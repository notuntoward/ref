---
created date: 2024-12-17T08:51:31-08:00
modified date: 2024-12-17T08:51:31-08:00
---
# LLM word Probabilities Animation

From: [[Novis23ObsidianChatGPT#Word probability display on chatGPT website]]

![[Graphics to Borrow-20240403175805104.webp]]
# GAN and Var. AutoEnc

From: [Generative AI - Comprehensive Guide For Beginners](https://binmile.com/blog/generative-artificial-intelligence/)

![[Graphics to Borrow-20240318203842157.webp]]



![[__temp__Graphics to Borrow-20240318203910209.webp]]
# Adapting LLM to use case

## Finetune LLM vs. Knowledge base
From: ["okay, but I want GPT to perform 10x for my specific use case" - Here is how - YouTube](https://www.youtube.com/watch?v=Q9zv369Ggfk)
![[Graphics to Borrow-20240320185805848.webp]]

## LORA adaptation

^e719b4

From: [Understanding LoRA — Low Rank Adaptation For Finetuning Large Models | by Bhavin Jawade | Towards Data Science](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6)

(note: this is a copy from google image search w/ blank, rather than white, background.  Could match slide better.  Get white background by screen copy)

![[Graphics to Borrow-20240320192105854.webp]]

From: [Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA) - Lightning AI](https://lightning.ai/pages/community/article/lora-llm/)

![[__temp__Graphics to Borrow-20240320192452449.webp]]

![[Graphics to Borrow-20240320192739736.webp]]

![[__temp__Graphics to Borrow-20240320192757647.webp]]
![[Graphics to Borrow-20240320192829127.webp]]

From: [Understanding LoRA — Low Rank Adaptation For Finetuning Large Models | by Bhavin Jawade | Towards Data Science](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6)

(note: google image search has a version that w/ blank, rather than white, background.  Could match slide better)

![[__temp__Graphics to Borrow-20240320193023694.webp]]

From: [Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)

![[__temp__Graphics to Borrow-20240320193414690.webp]]

# Programming/Agents/ZeroShot/Benchmarks
## Zero shot
From: [Robots Talk Back, AI Security Risks, Political Deepfakes, and more](zotero://select/library/items/QUV7MR9D)
![[Graphics to Borrow-20240322153837660.webp]]
[[2ddd5b3a44249cd75fd3324d90df8294_MD5.gif|Open: unnamed---2024-03-20T163430.388.gif]]

## Programming Holistic Eval
From: [[Jain24liveCodeBenchLeaderboard]]

[[__temp__ecadd8efdd0495482251e777e79872a2_MD5.jpeg|Open: Pasted image 20240418171349.png]]
![[__temp__ecadd8efdd0495482251e777e79872a2_MD5.jpeg]]
![[Pasted image 20240418171327.png]]
# GIF: Robot gets stumped, asks questions and follows advice

^la5kij

From: [Robots Talk Back, AI Security Risks, Political Deepfakes, and more](zotero://select/library/items/QUV7MR9D)
**Use?**: [6 Methods to Slow Down GIFs Quickly & Easily - MiniTool MovieMaker](https://moviemaker.minitool.com/moviemaker/slow-down-gif.html)

This works in PowerPoint, and can be expanded to fullscreen. [test](obsidian://open?vault=Obsidian%20Share%20Vault&file=work%2FGenerative%20AI%20talk%2Ftest)

![[2ddd5b3a44249cd75fd3324d90df8294_MD5.gif]]

# GIF: Robot gets advice, whether it wants it or not
From: [Engineering household robots to have a little common sense](zotero://select/library/items/CH7NT3FL)

![[__temp__ComonsenseBots-ani_1.gif]]

The paper about this work is: [Grounding Language Plans in Demonstrations Through Counter-Factual Perturbations](zotero://select/library/items/X3I7W7KV), and from it, this graphic might be good for slides

![[Graphics to Borrow-20240326223405431.webp]]

# Gen AI Disappointment
## Chatbot Letdown
From: [AI chatbot letdown](zotero://select/library/items/YUKAFSYU)

![[__temp__Graphics to Borrow-20240327134858584.webp|738]]
![[__temp__Fried24AIChatbotLetdown.gif|607]]
## Waste of time, money
From: [[Birch24genAIwasteTimeMoney|Generative artificial intelligence is simply a waste of our time and money]] ?
![[__temp__Graphics to Borrow-20240405124029760.webp]]
# Gen AI Finances
## AI Bubble
From [[Jin24IstartupShwrCashNoBiz]]
[[7a04c93f9d6f2e1cf273d642c51901c3_MD5.jpeg|Open: Pasted image 20240429105231.png]]
![[7a04c93f9d6f2e1cf273d642c51901c3_MD5.jpeg]]
## Investments, partnerships & FOMO

- Huge investments & data source for a nice graphic slide: [Generative AI is driving tech heavyweights to invest billions of dollars in startups](zotero://select/library/items/6GKCX492)

# Voice Cloning

Not video but good slide content?  Too long?

Little 15 second clips: [[#Voice Cloning|Voice Cloning]]

# AI Safety

From: [Artificial Intelligence Act](zotero://select/library/items/MHF9CJG8)

![[__temp__Graphics to Borrow-20240401102723113.webp]]
# Chat sessions
## [[Gen AI Talk Demo - GoogleNotebookLM]]

A video or screenshots, for example:
![[Gen AI Demo - GoogleNotebookLM-20240404193012262.webp]]
# Embedding
## Tokenizing
From [[OpenAI24tokenizer]]

Run examples and make page clips, or use the [[tiktoken]] package to make my own.

![[Graphics to Borrow-20240410214759232.webp]]

From: [[OpenAI24tokensWhatHowCount|What are tokens and how to count them?]]

more token graphs
![[__temp__Graphics to Borrow-20240410215642419.webp]]
## Word2Vec
From: [[SwimmTeam24twordEmbed5TypNLPapp|5 Types of Word Embeddings and Example NLP Applications]]
![[Graphics to Borrow-20240408190123789.webp]]

# RAG Diagram

From: [[OpenAI24ragAndSemSrchGPT|Retrieval Augmented Generation (RAG) and Semantic Search for GPTs]]

![[__temp__Graphics to Borrow-20240410214159713.webp]]

# Fine Turning Algorithms
From [[Davidson23improveAInoExpensTrain|AI capabilities can be significantly improved without expensive retraining]].  Figure 2.

[[work/Generative AI talk/attachments/e84d503a8c22cf3d4d4e377e15da450d_MD5.jpeg|Open: Pasted image 20240415161050.png]]
![[work/Generative AI talk/attachments/e84d503a8c22cf3d4d4e377e15da450d_MD5.jpeg]]
# Performance / Benchmarks
## Programming
From: [[Jain24LiveCodeBenchHolisticNoCntam|LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code]]
[[work/Generative AI talk/attachments/cdf0db40e57676842cdee19fb9b876ae_MD5.jpeg|Open: Pasted image 20240423145026.png]]
[[__temp__dc8669c2b25f38b6cfe79c590be677eb_MD5.jpeg|Open: cdf0db40e57676842cdee19fb9b876ae_MD5.jpeg]]
![[__temp__dc8669c2b25f38b6cfe79c590be677eb_MD5.jpeg]]

## Benchmark performance vs. Humans
From [[HAI24AIIndexReport|Artificial Intelligence Index Report 2024]]
This [Nature article](https://www.nature.com/articles/d41586-024-01087-4) has a better looking graph, but its low res.

Don't forget to mention AI grade inflation:

[[1cf3bcc791708cbdeeb4227170927650_MD5.jpeg|Open: Pasted image 20240418084518.png]]
![[1cf3bcc791708cbdeeb4227170927650_MD5.jpeg]]

# Vector Databases
From: [[Exxact24vecDBllmGenAIdpLrn|Vector Database used in AI | Exxact Blog]]

May also be interesting for Transformers section or RAG

[[work/Generative AI talk/attachments/25b34e6db28116d69f10d09e32ae6ce3_MD5.jpeg|Open: Pasted image 20240423145448.png]]
[[__temp__1e92ad26ecd869468abc2be8722aaf1e_MD5.jpeg|Open: 25b34e6db28116d69f10d09e32ae6ce3_MD5.jpeg]]
![[__temp__1e92ad26ecd869468abc2be8722aaf1e_MD5.jpeg]]
# Cost
## Training Computh2he Flops: 1950-2023 (3?)
From: [[Epoch23MachineLearningTrends|Key Trends and Figures in Machine Learning]]

### Use this graph
- Note knees at deep lear
- ning and LLMs start years
- Gemini Ultra is labeledOLD ALL
- Can say the Gemini ultra costs: total $ and compute $.  Maybe params too?
- Came from [[Sevilla22ComputeTrendsThree]] and/or [[Epoch24trainComputeVsTime]]

![[work/Generative AI talk/attachments/e508c8064614bb553749e6d6d39b7a64_MD5.jpeg]]

### DON'T use this graph
[[work/Generative AI talk/attachments/e508c8064614bb553749e6d6d39b7a64_MD5.jpeg|Open: Pasted image 20240415214357.png]]
- It's higher quality SVG than the one above but has the wierd OOMs slope labels
- The line slope labels (OOMs/year) are probably orders of mag. / year.  This nearly agrees with the slope labels on the other graph you get in the middle of the article you see when you click on "read more".  The line label above 1980 is 1.4x/year logbase10(1.4) = 0.146 ~ 0.2?? 

[[__temp__6c13cbf68028013b467e82968778fd51_MD5.jpeg|Open: Pasted image 20240415164038.png]]
![[__temp__6c13cbf68028013b467e82968778fd51_MD5.jpeg]]

- # Actually, just don't use the graph above!
- use the one in the one that you get when you click on the article+
# AI disappointment##

## Boom to Bust
From [[Naughton24boomBurstAI|From boom to burst, the AI bubble is only heading in one direction]]
[[f97d672bf3612d8607b5ede4233db701_MD5.jpeg|Open: Pasted image 20240418162335.png]]
![[f97d672bf3612d8607b5ede4233db701_MD5.jpeg]]
## Lost its Magic
From [[Bogost24aiLostMagic|AI Has Lost Its Magic]]
[[work/Generative AI talk/attachments/6e201a19c1c2589db6495b4e23870c6d_MD5.jpeg|Open: Pasted image 20240418163427.png]]
![[work/Generative AI talk/attachments/6e201a19c1c2589db6495b4e23870c6d_MD5.jpeg]]

**OR** so text is bigger when you show "That's how you know..."

[[work/Generative AI talk/attachments/4acf4d4338d6d9be4153f03a6a66a0d0_MD5.jpeg|Open: Pasted image 20240418163850.png]]
![[work/Generative AI talk/attachments/4acf4d4338d6d9be4153f03a6a66a0d0_MD5.jpeg]]
# Resources: classes, youtubers
## Yannic Kilchner

From: Kilcher24MixtralExpertsPaperExpln

[[work/Generative AI talk/attachments/273624791e454d2e8cca4908f7dd7691_MD5.jpeg|Open: Pasted image 20240421212817.png]]
![[work/Generative AI talk/attachments/273624791e454d2e8cca4908f7dd7691_MD5.jpeg]]

# Quantization / CPU,GPU,TPU

## From [[Sato17googleFirstTPU|An in-depth look at Google’s first Tensor Processing Unit (TPU)]]

[[bcdcc1b4d2fec9626d7e08e1bcc6f69f_MD5.jpeg|Open: Pasted image 20240423141851.png]]
![[bcdcc1b4d2fec9626d7e08e1bcc6f69f_MD5.jpeg]]

[[02c06ce027cbfb7765b67b162d83103f_MD5.jpeg|Open: Pasted image 20240423141919.png]]
![[02c06ce027cbfb7765b67b162d83103f_MD5.jpeg]]

[[a69f0e1330b155726461c115d818831a_MD5.jpeg|Open: Pasted image 20240423141953.png]]


![[a69f0e1330b155726461c115d818831a_MD5.jpeg]]

## From [[Ma24Era1bitLLMs|The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits]]


[[work/Generative AI talk/attachments/2eda33bf443058f33bc72218990e5a67_MD5.jpeg|Open: Pasted image 20240423143204.png]]
[[c7bef93aa1b8028267375cafe7c2c21d_MD5.jpeg|Open: 2eda33bf443058f33bc72218990e5a67_MD5.jpeg]]
![[c7bef93aa1b8028267375cafe7c2c21d_MD5.jpeg]]

# Speech Recognition / Multilingual

From: [[AssemblyAI24multilangASRuniversal1|Introducing Universal-1]]

[[work/Generative AI talk/attachments/b09effe8996afa970cd88b630cc402e2_MD5.jpeg|Open: Pasted image 20240423151223.png]]
[[work/Generative AI talk/attachments/f3834e87a95252b24922d96e45d79825_MD5.jpeg|Open: b09effe8996afa970cd88b630cc402e2_MD5.jpeg]]
![[work/Generative AI talk/attachments/f3834e87a95252b24922d96e45d79825_MD5.jpeg]]

# Transformer Theory

## Representation Adjustments

Adjusting meaning of word: [[3Blue1Brown24visAttenTransfrmr#^5da5b5|mole example]]

Moving info from whole sequence to the place where it can predict the next word: [[3Blue1Brown24visAttenTransfrmr#^c89a1d|last representation]], and result of soft max but not showing softmax itself.  Also shows context flowing from a whole document.
## attention layer
### computational benefit of attention layer

input adjusting weights in attention block allows one layer to do the work over several before.
[[Work/VPP ideas/Generative AI talk/attachments/80c7705cf00d4dc2538f72bc3272f16d_MD5.jpeg|Open: Pasted image 20240426141832.png]]

This one's not literal, but illustrates.  Would probably want to highlight what adjusts to data somehow.

# Basic Neural Nets

Would like weights, biases, graphic nonlinearity, classifier, finite number of layers.  Graininess maybe OK, since I'll this will follow [[Rumelhart86learnRepBackprop]]. I could show a screenshot of that paper, which is also grainy.

Separate blowups of hidden layer sum/nonlinearity b/c no full graph shows that perfectly.  Could be a popup

## Simple NN drawings that show (most) parameters


### Finite layer size, weights, missing bias, nonlinearity shown graphically

From [here](https://ahtchow.medium.com/solving-navigation-using-deep-reinforcement-learning-value-based-methods-3fe74fe85876) but I should probably use the original source cited there for this diagram: "Artificial Neural Network (Source: VIASAT)"

![[work/Generative AI talk/attachments/8b909f559c71eeb973f0867867583f68_MD5.jpeg]]
### Has everything but the nonlinearity graphic
From [here](https://www.softwaretestinghelp.com/artificial-neural-network-ann-models/)

![[work/Generative AI talk/attachments/aaa7288cb290cb08304d608349329f2c_MD5.jpeg]]
### Everything but nonlinearity shown as sum
From [here](https://www.researchgate.net/publication/303875613_Financial_Forecasting_Using_Machine_Learning/figures?lo=1)

![[__temp__9c9ea0fdd19827d96308a3cff0d6b597_MD5.jpeg]]
### Shows weights and bias but is incomplete
From [here](https://www.mathworks.com/help/deeplearning/ug/nmodel_layn.gif)

![[97adb965c3240635401e9a9060c20bad_MD5.jpeg]]
### Better graphics, missing bias, undefined num layers, no bias
From [here](https://www.mdpi.com/2075-5309/8/11/151)

![[41b16025862070245b42e6077da3dfa5_MD5.jpeg]]
## Hidden Node Nonlinearities

### Quite good
From [here](https://www.researchgate.net/publication/262493920_Managing_a_real-time_massively-parallel_neural_architecture/figures?lo=1)
![[__temp__ed870a19df26e8cc50d5fcb9a576b980_MD5.jpeg]]

### Shows sum an nonlinearity, missing bias
From [here](https://www.analyticsvidhya.com/blog/2021/04/estimation-of-neurons-and-forward-propagation-in-neural-net/)

![[6fd11aa56142985631b36d92dfc9069c_MD5.jpeg]]
### Good, Missing Bias
From [here](https://www.freecodecamp.org/news/deep-learning-neural-networks-explained-in-plain-english/)
![[work/Generative AI talk/attachments/203fd4d22e6b7d6974571e1d9c9a11c9_MD5.jpeg]]
### 1 matches other way of showing bias, must rename nonlinearity
From [here](https://www.google.com/url?sa=i&url=https%3A%2F%2Fpyimagesearch.com%2F2021%2F05%2F06%2Fintroduction-to-neural-networks%2F&psig=AOvVaw1pfLSJYO_K_XWTBVpGGK-z&ust=1714331399866000&source=images&cd=vfe&opi=89978449&ved=0CBMQjhxqFwoTCLDw0KWO44UDFQAAAAAdAAAAABAO)

![[work/Generative AI talk/attachments/c7230a9ef45866e4cdf4617204ef2b11_MD5.jpeg]]
# Separate bias, non-graphical nonlin
From [here](https://link.springer.com/chapter/10.1007/978-3-030-89010-0_10)

![[__temp__942cff4576b18d8f61cea30bd2895695_MD5.jpeg]]
### OK, no bias, non-graphical nonlin
From [here](https://www.cosmos.esa.int/web/machine-learning-group/neural-network-introduction)
![[__temp__d2b5cf1a25e769807226e0281419c37e_MD5.jpeg]]

{"path":"lit/lit_sources.backup/Ehrlinger16DecisionmakingCognitiveBiases.pdf","text":"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/301662722 Decision-Making and Cognitive Biases Article · December 2016 DOI: 10.1016/B978-0-12-397045-9.00206-8 CITATIONS 3 READS 14,050 3 authors, including: Some of the authors of this publication are also working on these related projects: Emotions and performance View project Joyce Ehrlinger Washington State University 30 PUBLICATIONS   1,590 CITATIONS    SEE PROFILE Bora Kim Yonsei University 8 PUBLICATIONS   25 CITATIONS    SEE PROFILE All content following this page was uploaded by Joyce Ehrlinger on 06 October 2017. The user has requested enhancement of the downloaded file. Decision-Making and Cognitive Biases Author and Co-author Contact Information Joyce Ehrlinger Department of Psychology, Washington State University, Pullman, WA 99164-4820 ehrlinger@wsu.edu 509-335-9127 Wilson O. Readinger Paradigm2 Research, PO Box 6192, Reading, PA 19610 wilson@paradigm2.com 610-855-8333 Bora Kim Department of Psychology, Washington State University, Pullman, WA 99164-4820 .a.kim@wsu.edu Keywords: Anchoring Attention Bias Blind Spot Cognition Decision-Making Endowment Framing Heuristics Loss Aversion Overconfidence Planning Abstract / Synopsis People are able to make many quick and efficient decisions each day by, often non- consciously, relying on cognitive schemas or short cuts. These short cuts allow people to come up with judgments that are “good enough” and, frequently, correct. That said, they also leave people prone to predictable cognitive biases. This article highlights some of the most common cognitive short cuts and resulting biases. We offer definitions, examples, and summaries of research regarding the underlying mechanisms of cognitive bias. Introduction As a category, cognitive biases in decision making encompass a broad range of deviations from what is commonly considered purely rational judgment and decisions. In his 2011 book, Daniel Kahneman argued for a distinction between different modes in which people think. Specifically, we can distinguish between thinking that is fast (often referred to as System 1 or automated thinking), and thinking that is slow (System 2, or conscious, deliberate thinking). System 2 (i.e., slow) thinking involves formally weighing of options and utility values that is part of rational decision-making; it is conscious, effortful, and particularly useful for calculating and drawing connections between concepts. The drawback to System 2 thinking is that it requires having available time, information, and motivation to engage in slow conscious reflection. If every decision we made required this level of processing, each day would expire before we were able to get ourselves fully dressed. System 1 thinking solves this problem by allowing for “background processing” of the vast majority of decisions we face throughout the day. System 1 thinking is automatic, rapid, and often more emotionally driven, compared to System 2 thought. Consider a case in which we hear a male voice say “I’m pregnant.” A System 2 response would be based on knowledge of biology and anatomy, along with an assessment of the likelihood that the voice was actually male, and perhaps some assessment of the presence of sarcasm in the tone of voice used. The automatic, System 1 response takes just a fraction of a second, and is generally provides a gut feeling. It is closely related to what we typically think of as intuition. There is, however, a price to pay for the speed and efficiency associated with thinking fast: it requires generalization and the neglect of some potentially important information. There is no free lunch. In this entry, we will describe multiple strategies that humans have developed over time to facilitate rapid, automated processing and make more efficient judgments. Perhaps the most well known type of fast thinking is the use of heuristics, or rules of thumb that allow us to make quick estimates of likelihood or value. We will define several heuristics in this entry and also describe how each of these automated forms of cognition can result in predictable errors or biases. It is important to note that these biases are not uncommon or inconsistent with typical cognitive and social functioning. Quite to the contrary, they are often unavoidable, automatic ways in which the mind processes information and allow people to make “good enough” choices in a sensory-rich environment. Cognitively (and perceptually), we base our assessments on a limited amount of data, for reasons of speed, expediency, and sometimes data availability. Heuristics and other cognitive short cuts thus function as a path to achieve a “good enough” assessment, as opposed to the optimal assessment that might result if one had unlimited information and time with which to make decisions. More than 100 cognitive, decision-making, and memory-related biases have been documented in the literature, and the research in cognitive and social psychology continues to frequently identify and delineate new biases. The review that follows offers a brief sample of the biases that are the most well known, most often encountered, and/or most relevant to the mental health field. Availability heuristic The availability heuristic suggests that the people treat the ease with which a value or event comes to mind as informative regarding its likelihood or frequency. Consider, for example, whether there are more words in the English language that start with the letter “r” or more words for the letter “r” is in the third position. To answer this question, most people try to think of words that begin with “r” and words with “r” as the third letter. The first task is easier than the second and, as a result, most people can think of more words that begin with the letter “r” than words with “r” as the third letter. Because words that begin with “r” come to mind more easily, many people assume that these words are also more common in the English language. In fact, there are more English-language words with “r” in the third than the first position. However, people tend to falsely conclude that the more “available” words, those that begin with “r,” are more frequent than those with “r” in the third position. Like all heuristics, the availability heuristic works well most of the time; generally speaking, we have an easier time thinking of or recalling events that are actually more common. Cognitive bias presents itself, though, when the cognitive availability and the actual frequency or likelihood of events are inconsistent. This point can be demonstrated with another common example. When asked to name the capital of Australia, many people quickly respond with the answer “Sydney.” In fact, the capital of Australia is Canberra. However, Sydney is arguably the most well known city in Australia and, for many Americans, the first to come to mind. People then mistake this cognitive availability as informative and conclude that Sydney must be Australia’s capital city. Factors that are particularly vivid — because they are particularly visual or emotionally meaningful — are easily imagined and, hence, cognitively available. Vivid events also tend to be covered in greater depth by the media more often than comparatively cold facts and statistics. For example, people are likely to set aside impressions based on consumer reports and similar information in the face of a neighbor’s story about bad experiences with a type of car. The latter information is more vivid and, as a consequence, seems more important to weigh into our decisions than cold, unemotional numbers. The availability heuristic has also been identified as an important underlying cause of egocentric judgments. It is easy to conclude, for example, that you complete a majority of the household chores while your spouse reliably complete fewer than half of these chores. Indeed, it is common for both members in a marriage to overclaim responsibility for household chores (Ross & Sicoly, 1979). This bias stems, in part, from a tendency to simply know more about and better recall the tasks that you have performed than the tasks performed by someone else. In other words, one’s own contributions to the household are more “available” than the contributions of others, which leads people to often assume that their own contributions are more frequent than those made by others. The anchoring and adjustment heuristic Imagine that you are asked to estimate the year in which George Washington was elected president. Most people do not know the exact year in which Washington was elected but many do know that the United States declared its independence in 1776. One way in which a person might estimate Washington’s election year, then, might be to treat 1776 as an initial anchor or starting point and then adjust from that date to estimate Washington’s likely election year based on estimates of how long it might have taken for the election to occur after the Declaration of Independence. This process of identifying a judgmental anchor and then adjusting from that anchor in order to arrive at an estimated value or likelihood is referred to as anchoring-and-adjustment. This heuristic was first defined by Tversky and Kahneman (1974) and allows people to make reasonable, educated estimates for many day- to-day judgments. The anchoring-and-adjustment heuristic also, however, leads to a predictable cognitive bias in estimates because adjustment from one’s initial anchor tends to be insufficient. We can see insufficient adjustment at work in studies that focus participants on differing anchors. For example, Tversky and Kahneman used a roulette-style wheel to randomly assign participants to a high anchor condition (starting with the number 65) or a low anchor condition (starting with the number 10). They asked participants whether a particular value (the percentage of African countries in the U.N.) was higher or lower than this assigned anchor and, next, asked participants to estimate the exact percentage of the countries in the U.N. that were African. Participants in the low anchor condition presumably asked themselves whether African countries made up 10% of the U.N. and then adjusted from that starting point to estimate the true percentage. The median estimate offered by this group was 25%. In contrast, participants in the high anchor condition adjusted from an anchor of 65 and offer a median estimate of 45%. Because the two groups started with very different anchors and each group adjusted insufficiently from their respective anchors, participants in the high anchor condition offered far higher estimates than did those in the low anchor condition. There has been some controversy in the literature about whether people truly engage in a process of adjusting from an initial anchor. Instead, some have argued that values close to a judgmental anchor are more available and, because of the availability heuristic, are then viewed as more plausible (e.g., Mussweiler & Strack, 1999). While this alternative explanation might explain some errors previously thought to be evidence of insufficient adjustment, Epley and Gilovich (2006) demonstrated that there are also cases in which people truly adjust from an initial anchor in exactly the way originally proposed by Tversky and Kahneman (1974). True adjustment can be seen most easily in examples for which people generate their own judgmental anchor rather than receiving an anchor from an experimenter. True adjustment occurs from self-generated anchors, in part, because these anchors never truly considered as a plausible answer. For example, imagine that you are asked to estimate the freezing point of vodka. Many people know that vodka can be stored in a freezer because its freezing point is lower than that of water. Thus, the value of 32°F (0°C) will likely come to mind for those asked to estimate the freezing point of vodka. However, people immediately know that this value is incorrect and, instead, use it as an anchor from which they adjust downward to come to a final estimate. Representativeness Heuristic When making judgments of the probability, we often rely on the extent to which a single case or example is representative of a particular category, group or stereotype. An illustration of the bias can be seen in Tversky and Kahneman’s (1974) seminal paper. Participants were asked to read about a person who was described as “very shy and withdrawn, invariably helpful, but with little interest in people, having a need for order and structure, and a passion for detail.” Participants were asked to infer the person’s occupation by assigning their probability estimations to each particular job from a list that included farmer, salesman, librarian, and physician. People tended to assign the highest probability to the librarian, because the figure in the description was regarded to be closest to – and most representative of – a stereotypical librarian. When the participants were told, though, that the fictitious individual was taken from a sample of 95 physicians and 5 librarians, though, there was no significant change to participants’ assessments of probability. Clearly, the base rate probability that a member of the sample is a physician should logically increase the probability estimation that this individual is a physician, but the representativeness heuristic is powerful enough to lead to a neglect of this information. An example closer to the health context can be seen in people’s perceptions of health risks. For example, people tend to expect that women’s risk of developing breast cancer is greater than their risk of heart disease. In fact, heart disease is far more common among women than breast cancer. However, breast cancer is more representative of a women’s disease and, as such, perceived as more likely than heart disease (Woloshin, Schwartz, Black, & Welch, 1999). Focalism Focalism, aka the focusing illusion, is a prototypical example of how cognitive biases can influence mental health. Focalism is the tendency to place too much focus or emphasis on a single factor or piece of information when making judgments or predictions. For example, consider the relationship between dating and overall happiness, as studied by Strack and colleagues (1988). When participants were asked two questions in this order (“How happy are you with your life in general?” and then “How many dates did you have last month?”), the correlation between the answers was not different from zero. When the order of the questions was reversed in another sample of participants, though, the correlation jumped to 0.66; thinking about that aspect of life led the research participants to more closely associate it with their general happiness. Questions on marriage and physical health have been shown to have virtually identical effects. “Nothing in life is quite as important as you think it is while you are thinking about it.” This quote from Schkade and Kahneman (1998) very acutely summarizes the relationship between focalism and expectations of happiness. People tend to overwhelmingly believe that being richer would make them happier. However, research indicates that this is not the case. The relationship is fairly complex, but it is sufficient to conclude that on a large scale, average increases in income do not lead to enduring, significant increases in self-reported average well-being. However, when people think about how money might impact their level of happiness, they are likely to focus on the possibility of engaging in more pleasurable pursuits (relaxing or playing golf, for example). At the same time, people ignore the – perhaps more realistic – probability of associated increases in the time spent working, commuting to a job that may be less appealing, or engaged in other activities less conspicuously related to happiness. Focalism exists because individuals are poor at anticipating the shift back to “normalcy” following disruptive events and are ultimately poor at accounting for the diminishing novelty of relative differences in everyday life. Much research on well-being (especially mental health) is thus surprising to those without a thorough understanding of this cognitive bias. In light of this focusing illusion, though, one can see that when an individual stops thinking about a particular circumstance, its effect on overall states of being will wane. Impact bias Affective forecasting is simply the process of making judgments about one’s own future emotional states. This takes place throughout our lives, and has a tremendous influence on our judgment and decision making, on both a grand and small scale. One may very reasonably consider future feelings in deciding if and when to marry, have a child, or change jobs, and the expectations for the impact of these decisions on happiness, for example, will likely play a huge role in the path taken. At the same time, one may wonder whether or not to have a second glass of wine or purchase a new brand of laundry detergent. Expectations about the effects of these behaviors on future emotions will also play a role in our day-to-day decision making. Affective forecasting is an important skill, then, but it is affected by several biases, perhaps the most salient of which is impact bias. Impact bias is the tendency for individuals to overestimate the duration and intensity of future emotions, expecting current events to have a greater impact on future states of emotions than they ultimately do. Imagine learning that a loved one has been diagnosed with cancer, or learning that you yourself have just been fired from your job. How upset would you be? How long would that feeling influence your life? Or consider a more felicitous example: if you could earn a lot more money, would you be happier than before? To what extent would this impact the quality of your life? When faced with these hypothetical situations, most people tend to be inaccurate in predicting duration and intensity of their affective reactions to future events. According to Wilson and Gilbert (2005), errors in affective forecasting, such as the impact bias, are mainly caused by excessive attention to a single target event, without considering other complex and distracting factors that will influence the future. (This is focalism, which was discussed in more detail above.) This, in turn, leads many people to overestimate the effect of the event on their feelings. It follows, then, that affective forecasting can be less biased if other things besides the target event are taken into account and given due weight. For example, Hoerger and colleagues (2010) found that some students who were asked to write about their trivial daily activities (such as tooth brushing and studying) made a less biased prediction regarding the impact of election results on their happiness level. Putting a potentially impactful event into a day-to-day context can thus mitigate some of the effects of this bias. Another, related, source of bias in affective forecasting is immune neglect, in which people fail to accurately predict their emotional state in the future because they do not consider their psychological flexibility and coping strategies to buffer the impact of future, negative, outcomes. Myriad coping mechanisms (not to mention cognitive and emotional biases) have evolved to allow for the ability to adapt to a negative situation, thereby tempering its effect on mental health. It is common, though, for individuals to both forget (or be unaware of) the role of this psychological immune system, and to underestimate its power of self-protection. For example, Gilbert and colleagues (1998) found that people with positive self-views expected they would be unhappy when receiving negative personality feedback, regardless of the fallibility of the feedback source. (That is, they would be hurt by the criticism of others, even knowing that the criticism was not reliable or well-informed.) However, these individuals later reported a higher level of happiness than originally predicted, primarily because the stated fallibility of the source of the criticism allowed them a psychological “escape route:” they were able to reason that the criticism was not justified and therefore not as damaging. The ability to make this adjustment in the future is underestimated in affective forecasting. Planning fallacy The planning fallacy refers to a tendency for individuals to underestimate the time required to complete a task. Even in cases where the task was familiar (i.e., had been completed by the participant before), and the individual making the estimate was very experienced – even expert – in the task at hand, underestimates are exceedingly common. The bias is present in myriad types of tasks, from homework to construction, from technical (high-skill) to mundane tasks, and from small-scale (5 minute) projects to large-scale government programs. The planning fallacy has since been revised, though, to extend not just to time, but also to cost and benefits of the task being evaluated. More precisely, when individuals are making predictions about their own task (i.e., one they plan to complete themselves), they tend to systematically underestimate the time it will take, underestimate the cost of the project, and overestimate the potential benefits of the project. Strikingly, the bias is only present when making judgments about one’s own tasks. Observers uninvolved in the execution of the task tend to be overly pessimistic in the face of the abounding optimism of the person planning to perform the task. Closely related to the planning fallacy is Hofstadter’s Law, named for Douglas Hofstadter and first appearing in his 1979 book. The law states that “it always takes longer than you expect, even when you take into account Hofstadter's Law.” This recursive piece of advice reinforces the planning fallacy, particularly when one considers the finding that feedback and experience do not mitigate the effect of this bias. That is, even when individuals are completing tasks that they have completed before, sometimes many times, the tendency to underestimate the time required and to overestimate the associated benefits remains. Several factors combine to make us overly optimistic in our predictions. Focalism may play a large part in the planning fallacy, inasmuch as individuals will tend to discount the competing activities and factors that lie outside the task they are predicting, but still have a strong likelihood of impacting the task. Timothy Wilson and colleagues (2000) have shown that reducing focalism can also mitigate the effects of the planning fallacy and make predictions somewhat more realistic. Buehler, Griffin, and Ross (1994) provide evidence that wishful thinking underlies the planning fallacy. In this account, individuals tend to underestimate the time to completion simply because that is the result they desire. Ian Newby-Clark and colleagues added some credence to this view when they found that asking individuals for their “best guess” about time to completion and asking about the “best case” time for completion tended to be identical. In other words, research participants expected the most favorable outcome to also be the most likely outcome (2000). Framing Suppose you are faced with a gamble: you must choose between Option A, which guarantees you $20, and Option B, which is probabilistic. In Option B you have a 1/3 chance of winning $60 and a 2/3 chance of winning nothing (and losing nothing). Would you choose Option A or Option B? Although the expected utility of both options is the same (both result in an average win of $20), most people choose Option A. With that in mind, now consider a second gamble; in this case you have been given an “endowment” of $60 with which to wager. You must choose between Option C, which is a certain loss of $40, and Option D, which is probabilistic. In Option D you have a 1/3 chance of keeping the entire endowment (no loss), and a 2/3 chance of losing the entire $60. Which sounds more appealing in this case, Option C or Option D? Once again, the two wagers offer the same expected utility (a “loss” of $40 from the endowment), but most people prefer Option D. In the first gamble, the alternatives are set up as “wins” and are thus presented in this positive light. The second gamble presents the alternatives as losses, and leads to a change in the way the options are perceived, by most individuals. This preference shift for alternatives presented in contrasting frames is called framing. According to Tversky and Kahneman’s Prospect Theory (1981), people in the “gain” (positive) frame view the gambles as containing risk of loss, something to be avoided, and thus tend to assure a certain profit by choosing Option A. On the other hand, people in the “loss” (negative) frame see a secure outcome as a loss and are motivated to take steps to avoid the loss by taking risk with Option D. Prospect Theory predicts that the absolute value of earning $100 and of losing lose $100 will not be psychologically equivalent. Losses, relative to gains, loom larger. Thus, because the pain caused by loss mentally outweighs the pleasure associated with the same-sized gain, individuals become motivated, over time, to avoid loss. Framing is not limited to only gains and losses. Levin and colleagues (1998) introduced several other important ways that framing can occur, including different presentation methods of the same event (e.g., the survival rate vs. mortality rate of a surgery) or the approach toward a desirable goal (e.g., cost not to take action vs. benefit to take action). For example, these extensions of the importance of framing can be seen in a study by Thomas on programs for the prevention of skin cancer. Messages highlighting the risk of sun exposure were more influential than messages informing of the benefits of skin protection. Similarly, Meyerowitz and Chaiken (1987) found that arguments for the importance of breast self-examination (BSE) were more influential when the message warned of potential risks caused by failing to conduct BSE than when the message emphasized the benefits of BSE. Research on framing in the health care field is occasionally conflicting, though. A recent Gallagher and Updegraff (2012) meta-analysis has revealed that positive-framed information was overall more effective than negative-framed information in promoting preventive behaviors. These examples illustrate the several instances of framing and its effects on judgments, but we can see examples of framing everywhere around us. Politics is a particularly rich source of such examples. What was originally known as the “Estate Tax” was re-branded the “Death Tax:” an artful piece of framing that not only brought the issue to many more Americans, but clearly influenced the perception of the legislation by millions. We can see similar framing taking effect in the change in rhetoric from “gay marriage” to “same-sex marriage,” “speculative oil drilling” to “energy exploration,” and “global warming” to “climate change.” The effects of framing, along with loss aversion tendencies and strategies more generally, in the normal population imply that value estimation and risk assessment are often affected by cognitive biases. The Endowment Effect The endowment effect was first coined by Thaler (1980) and refers to the fact that people often value an object more highly once it is in their possession than when it is not. Considering the very same item, owners tend to ask more to give that item up than non- owners are willing to pay to acquire it. This contradicts rational economic theory which predicts that the allocation of resources is not affected by the assignment of property rights if transaction costs are zero (i.e., Coase Theorem). One of the clearest examples of this bias can be seen in a review by Kahneman, Knetsch, and Thaler (1991). Each participant in this classic study was given a coffee mug and asked to value the item in terms of the price for which they would sell it. They called this the amount that the participant was willing to accept. In another group, participants were simply asked how much they would be willing to pay to buy the same mug. Consistently, the amount that participants were willing to accept was found to be much higher than the amount they were willing to pay. Many follow-up studies have robustly and reliably demonstrated that owners’ selling price exceeds buyers’ paying price. Kahneman and colleagues interpreted this effect in line with prospect theory - more specifically, loss aversion - because selling an already-owned item can be regarded as a loss, while buying an item is considered a gain. Therefore, for an identical object, sellers request a greater amount to give it up than potential buyers do to acquire it. In terms of the status quo bias, it is sometimes explained that the baseline for owners (or the reference point in the value function of prospect theory) is to have the item and selling behavior can be perceived as loss. To avoid the loss, owners might be motivated to maintain the status quo, that is, to retain the item, which leads to higher selling prices than buying prices. Morewedge, Shu, Gilbert, and Wilson (2009) have proposed an additional account that focuses more on people’s psychological attachment to objects. Unlike previous experiments where sellers were owners, they used new experimental designs to separate ownership from “seller-ship.” They found that when buyers already happened to own a cup and had to value another (similar) cup for purchase, buyers’ willingness to pay were similar to sellers’ willingness to accept. Also, participants playing the role of brokers (or agents) on behalf of buyers or sellers asked higher prices for the cup when they happened to own identical cups, compared with when they did not own them. This implies that ownership, not loss aversion, may cause the endowment effect. Similarly, Van Boven and colleagues (2000) showed that sellers who were asked to value their own item (as opposed to someone else’s similar item) tended to overvalue the item. Recently, Isoni, Loomes, and Sugden (2011) suggested that the endowment effect might be more complicated than originally thought, based on inconsistencies observed in several experiments; they suggest that conditional factors could influence the effect. These might include participants’ market experience, the items used in experiments - typically low priced commodities like mugs and candies - and even some experimental protocols that may provoke misconceptions for item valuation. In summary, the effect of the cognitive bias associated with endowment is strong and fairly pervasive, even if the underlying structure and mechanisms are still being debated. Overconfidence Overconfidence is perhaps the most pervasive types of cognitive bias, and one that relates quite directly to many of the other biases discussed here. In the simplest form, overconfidence is the pervasive tendency for an individual to be more confident in his or her abilities or judgments than is measurably justified. For example, most people rate themselves as above average on a host of traits, abilities and personal characteristics. While some people truly are above average, it is statistically impossible for most people to be above average. Still, people tend to routinely rate themselves as above average in many domains, including physical attractiveness, likeability, intelligence, morality, and the ability to get along with others, to name a few (see Dunning, 2005 for review). Of course, not everyone holds overconfident perceptions of himself or herself. Indeed, there are particular domains in which people tend to be quite underconfident. For example, underconfidence is quite common for difficult tasks. Most people believe, for example, that they are below average in their ability to play chess (Kruger, 1999). Also, women and underrepresented minorities tend to lack confidence in their abilities to succeed and perform well in domains such as science, technology, engineering and math and this lack of confidence has contributed to a significant lack of diversity in these fields (e.g., Ehrlinger & Dunning, 2003). While there exist notable examples in which people hold less confidence than they merit, greater attention is sometimes paid to overconfidence both because of its frequency and, in particular, the frequency of overconfidence among the unskilled. It is, of course, troubling when someone who is skilled is a bit overconfident. However, far more troubling are those cases in which people who have very little idea of what they are doing, nonetheless, exude confidence. Sadly, research suggests that the unskilled are, very often, overconfident (e.g., Dunning, Johnson, Ehrlinger, & Kruger, 2003). Indeed, overconfidence is sometimes caused by a lack of skill. Because they are unskilled, those who lack competence simply lack the necessary knowledge and information that would necessary to know just how wrong they are. Even when those who are unskilled are offered large financial incentives to make accurate estimates of their performance, they remain vastly overconfident (Ehrlinger et al, 2008). Gambler's fallacy The gambler’s fallacy is seen when people believe that a certain outcome is “due.” People tend to be overly confident that deviations from long-term averages will be remedied in the short term. Said another way, the gambler’s fallacy is committed when an individual believes that deviations in the outcomes of independent trials in one direction will be followed by deviations in the opposite direction. The prototypical example of this is a series of coin- tosses. In the first six tosses of the coin, heads comes up each time. A gambler might wager on tails coming up on the subsequent toss, predicting that tails is “due,” simply because tails has not come up recently. The underlying cause of the gambler’s fallacy is the representativeness heuristic, described above. Small runs of independent events are expected to be representative of longer-term averages. In a classic study, research participants were simply asked to create “random- looking” sequences of coin tosses (that is, to create series of heads or tails outcomes that would appear random). The sequences were very likely to maintain a balance of about 50% each of heads and tails, even over very short runs. Chance would produce short runs that deviated from the global average more often and to a greater extent than research participants produced in their simulations of randomness. Confirmation bias Individuals tend to evaluate a hypothesis by testing to see if they are able to confirm it. The methods by which one tries to confirm – versus reject – a hypothesis can be very different, and seeking confirmation leads to systematic errors in the conclusions we draw and decisions we make. Consider a version of the classic experiment known as the Wason Card Task, or Wason Selection Task, in which participants were presented with four cards on a table. They are told that each card has a letter on one side and a number on the other. Participants can see one vowel, one consonant, one odd number and one even number facing up on the cards (e.g., A, B, 3, and 4). The job of the participants is to test the following hypothesis: “If a card has a vowel on one side, it has an even number on the other.” They are to turn over only the cards that are necessary to determine whether the hypothesis is true. The correct answer is to turn over the vowel (which must reveal an even number, for the rule to be valid), and the odd number (which must NOT reveal a vowel, in order for the rule to be valid). In the original experiment, as well as myriad replications and extensions, few people complete the task correctly, and Wason attributed this to confirmation bias (Wason & Johnson-Laird, 1972). The bias leads participants to turn over cards that confirm the rule, and fail to check the card that could prove the rule false. This indicates the extent to which individuals will seek evidence to confirm the theory in question, at the expense of evidence that could prove it to be false. Further research has shown that this tendency is even stronger when people are evaluating a theory that they actually believe in, or wish to be true (Dawson, Gilovich, & Regan, 2002). A striking real-world example of confirmation bias can be seen in the 1986 explosion of the Space Shuttle Challenger. Research conducted with NASA in the intervening years has shown that confirmation bias may have had a great deal to do with the catastrophic results of launching on that particular day. Decision makers appear to have paid more attention to available information, and made efforts to seek additional information, that confirmed their preferences, which were in many cases to continue with a launch under questionable circumstances. The launch would take place in the coldest weather ever, beyond the original specifications of certain critical components, but the specifications had been stretched by small margins over many different trials and tests. Decision-makers appealed to these instances to confirm their judgment that the launch could occur safely. Although the o-rings sealing the rocket boosters during pre-launch testing showed signs of wear that could not be explained, NASA concluded that since the test launches had been successful, the wear of these parts was within tolerable limits and would not threaten the actual launch. In this sense, they were able to actually take strong contradictory evidence for their theory (i.e., critical components are showing wear that we do not expect) and view it as confirmation of their decision to launch (i.e, despite the unexplained wear, test launches were successful). Bias blind spot “Have you ever noticed that anybody driving slower than you is an idiot, and anyone going faster than you is a maniac?” In this one question, comedian George Carlin aptly summarizes people’s naturally tendencies to view their own beliefs, attitudes, and behaviors as correct and as the “right” way to think or act. People have a bias blind spot in that we think our own judgments and decisions relatively free of motivated and cognitive bias. In contrast, we view others (and, in particular, those with differing opinions) as prone to bias (Ehrlinger, Gilovich, & Ross, 2005; Pronin, Gilovich, & Ross, 2004). The bias blind spot is a consequence of naïve realism or the tacit belief that one’s own judgments and understandings of the world—and the decisions, preferences, and priorities that reflect those understandings—are direct and unmediated reflections of objective reality or “the way things are” (Ross & Ward, 1996). It is common for people to hold naïve realist perceptions that their own interpretations and beliefs about everyday issues (e.g., motivations of family members and friends) as well as larger points of conflict (e.g., politics and religion) are correct and unbiased. The obvious corollary of this epistemic stance is the belief that judgments that differ from one’s own are either uninformed, a product of intellectual incapacity or laziness, or a reflection of distorting motivational, ideological, or cognitive bias. To the degree that opposing groups see each other as unintelligent and biased by self-interest, any attempt to resolve conflicts is likely to end in disappointment. Indeed, past research on naïve realism has been directly applied to help work toward the resolution of important real world international conflicts including those in Northern Ireland and the Middle East (Ross, Conclusion Cognitive and decision-making biases exist because fast, System 1 thinking is unconsciously evaluating any given situation and attempting to pattern-match it to a previous situation from past experience. This schema-based processing is remarkably efficient, but fails at tests of traditional (Bernoullian) utility theory in predictable ways. These shortcuts, as such, are valuable tools for navigating the complex social and cognitive landscape. Taken as a set, they offer insight into the differences in judgment and decision making that exist between conscious, deliberate thought and automated, heuristic-based processing. Cross References 42. Health Beliefs 48. Positive Illusions 201. Behavioral Economics 217. Negotiation and Conflict Resolution References Buehler, R., Griffin, D., & Ross, M. (1994). Exploring the \"planning fallacy\": Why people underestimate their task completion times. Journal of Personality and Social Psychology, 67(3), 366-381. doi: http://dx.doi.org/10.1037/0022-3514.67.3.366 Burks, S.V., Carpenter, J.P., Goette, L, Rustichini, A. (2010). Overconfidence is a social signaling bias. Discussion paper series: Forschungsinstitut zur Zukunft der Arbeit, No. 484. Combs, B. & Slovic, P. (1979). Causes of death: Biased newspaper coverage and biased judgments. Journalism Quarterly, 56, 837-843. Dawson, E., Gilovich, T., & Regan, D.T. (2002). Motivated reasoning and performance on the Wason selection task. Personality and Social Psychology Bulletin, 28, 1379-1387. Dunning, D. (2005). Self-Insight: Roadblocks and Detours on the Path to Knowing Thyself. New York, NY: Psychology Press. Dunning, D., Johnson, K., Ehrlinger, J., & Kruger, J. (2003). Why people fail to recognize their own incompetence. Current Directions in Psychological Science, 12(3), 83-87. doi: http://dx.doi.org/10.1111/1467-8721.01235 Ehrlinger J. & Dunning, D.A. (2003). How chronic self-views influence (and mislead) estimates of performance. Journal of Personality and Social Psychology, 84, 5-17. Ehrlinger J., Gilovich, T., & Ross, L. (2005). Peering into the bias blind spot: People’s assessments of bias in themselves and others. Personality and Social Psychology Bulletin, 31(5), 680-692. http://dx.doi.org/10.1177/0146167204271570 Ehrlinger, J., Johnson, K., Banner, M., Dunning, D., & Kruger, J. (2008). Why the unskilled are unaware: Further explorations of (absent) self-insight among the incompetent. Organizational Behavior and Human Decision Processes, 105(1), 98- 121.doi: http://dx.doi.org/10.1016/j.obhdp.2007.05.002 Epley, N., & Gilovich, T. (2006). The Anchoring-and-adjustment heuristic: Why the adjustments are insufficient. Psychological Science, 17(4), 311-318. doi: 10.1037//0033-2909.119.2.197. Gallagher, K. M., & Updegraff, J. A. (2012). Health message framing effects on attitudes, intentions, and behavior: A meta-analytic review. Annals of Behavioral Medicine, 43(1), 101-116. doi: http://dx.doi.org/10.1007/s12160-011-9308-7 Gilbert, D. T., Pinel, E. C., Wilson, T. D., Blumberg, S. J., & Wheatley, T. P. (1998). Immune neglect: A source of durability bias in affective forecasting. Journal of Personality and Social Psychology, 75(3), 617-638. doi: http://dx.doi.org/10.1037/0022-3514.75.3.617 Hoerger, M., Quirk, S. W., Lucas, R. E., & Carr, T. H. (2010). Cognitive determinants of affective forecasting errors. Judgment and Decision Making, 5(5), 365-373. Hofstadter, D. (1979). Godel, Escher, Bach: An Eternal Golden Braid. New York, NY: Basic Book, Inc. Isoni, A., Loomes, G., & Sugden, R. (2011). The willingness to pay-willingness to accept gap, the \"endowment effect,\" subject misconceptions, and experimental procedures for eliciting valuations: Comment.\" American Economic Review, 101(2), 991-1011. doi: http://www.aeaweb.org/aer/ Kahneman, D. (2011). Thinking, Fast and Slow. Macmillan. ISBN 978-1-4299-6935-2 Kahneman, D., Knetsch, J. L., & Thaler, R. H. (1991). Anomalies: The endowment effect, loss aversion, and status quo bias. The Journal of Economic Perspectives, 5(1), 193- 206. Levin, I. P., Schneider, S. L., & Gaeth, G. J. (1998). All frames are not created equal: A typology and critical analysis of framing effects. Organizational Behavior and Human Decision Processes, 76(2), 149-188. doi: 10.1006/obhd.1998.2804 Meyerowitz, B. E., & Chaiken, S. (1987). The effect of message framing on breast self- examination attitudes, intentions, and behavior. Journal of Personality and Social Psychology, 52(3), 500-510. doi: http://dx.doi.org/10.1037/0022-3514.52.3.500 Morewedge, C. K., Shu, L. L., Gilbert, D. T., & Wilson, T. D. (2009). Bad riddance or good rubbish? Ownership and not loss aversion causes the endowment effect. Journal of Experimental Social Psychology, 45(4), 947-951. doi: http://dx.doi.org/10.1016/j.jesp.2009.05.014 Mussweiler, T., & Strack, F. (1999). Hypothesis-consistent testing and semantic priming in the anchoring paradigm: A selective accessibility model. Journal of Experimental Social Psychology, 35(2), 136-164. doi: http://dx.doi.org/10.1006/jesp.1998.1364 Newby-Clark, I. R., Ross, M., Buehler, R., Koehler, D. J., & Griffin, D. (2000). People focus on optimistic scenarios and disregard pessimistic scenarios while predicting task completion times. Journal of Experimental Psychology: Applied, 6(3), 171-182. doi: http://dx.doi.org/10.1037/1076-898X.6.3.17 Pronin, E., Gilovich, T. D., & Ross, L. (2004). Objectivity in the eye of the beholder: Divergent perceptions of bias in self versus others. Psychological Review, 111, 781- 799. Ross, L. (2011). Perspectives on disagreement and dispute resolution: Lessons from the lab and the real world. In E. Shafir (Ed.), The behavioral foundations of public policy (pp. 108-125). Princeton, NJ: Princeton University & Russell Sage Foundation Press. Ross, M., & Sicoly, F. (1979). Egocentric biases in availability and attribution.Journal of Personality and Social Psychology, 37(3), 322-336. doi:10.1037/0022- 3514.37.3.322. Ross, L., & Ward, A. (1996). Naive realism in everyday life: Implications for social conflict and misunderstanding. In T. Brown, E. S. Reed, & E. Turiel (Eds.), Values and knowledge. The Jean Piaget Symposium Series (pp. 103–135). Hillsdale, NJ: Erlbaum. Strack, F., Martin, L. L., & Schwarz, N. (1988). Priming and communication: Social determinants of information use in judgments of life satisfaction. European Journal of Social Psychology, 18(5), 429-442. doi: http://dx.doi.org/10.1002/ejsp.2420180505 Thaler, R. (1980). Toward a positive theory of consumer choice. Journal of Economic Behavior and Organization, 1, 39-60. Thomas, K., Hevey, D., Pertl, M., Ní Chuinneagáin, S., Craig, A., & Maher, L. (2011). Appearance matters: The frame and focus of health messages influences beliefs about skin cancer. British Journal of Health Psychology, 16(2), 418–429. doi: http://dx.doi.org/10.1348/135910710X520088 Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124-1131. doi: http://dx.doi.org/10.1126/science.185.4157.1124 Tversky, A., & Kahneman, D. (1981). The framing of decisions and the psychology of choice. Science, 211(4481), 453-458. doi: http://dx.doi.org/10.1126/science.7455683 Van Boven, L., Dunning, D., & Loewenstein, G. (2000). Egocentric empathy gaps between owners and buyers: Misperceptions of the endowment effect. Journal of Personality and Social Psychology, 79(1), 66-76. doi: http://dx.doi.org/10.1037/0022- 3514.79.1.66 Wilson, T. D., & Gilbert, D. T. (2005). Affective forecasting: Knowing what to want. Current Directions in Psychological Science, 14(3), 131-134. doi: http://dx.doi.org/10.1111/j.0963-7214.2005.00355.x Wilson, T. D., Wheatley, T., Meyers, J. M., Gilbert, D. T., & Axsom, D. (2000). Focalism: A source of durability bias in affective forecasting. Journal of Personality and Social Psychology 78(5), 821–836. doi: http://dx.doi.org/10.1037/0022-3514.78.5.821 Woloshin, S., Schwartz, L. M., Black, W. C., &Welch, H. G. (1999). Women’s perceptions of breast cancer risk: How you ask matters. Medical Decision Making, 19(3), 221– 229. Biographies and photos Joyce Ehrlinger’s research focuses on understanding the sources of error and accuracy in self and social judgment. For example, Dr. Ehrlinger has examined how self-beliefs regarding ability and regarding the malleability of attributes impact behavior in achievement situations and perceptions of performance. She has also examined how naïve realism leads people to perceive their own views as better informed and less biased than those of others and, in turn, leads to overconfident perceptions of the ease of persuading others. Her work has been feature in a broad range of journals including the Journal of Personality and Social Psychology, the Journal of Experimental Social Psychology, and Personality and Social Psychology Bulletin and has been covered in the popular media. Dr. Ehrlinger is now an Assistant Professor of Psychology at Washington State University. Her lab website is located at www.joyceehrlinger.com. Wilson Readinger is the owner and Principal Scientist at Paradigm2 Research, a marketing and basic research company. He has been engaged in marketing and social science research for the past 15 years, with experience as a contract consultant to Fortune 100 companies, as a corporate research manager, and as a small-business owner. Wil has been awarded a Fulbright Fellowship for his research on visual perception and human movement patterns, along with a John Marshall Scholarship for his study of cognitive psychology, especially Theory of Mind. His recent work focuses on methodological advancement of cognitive interviewing techniques. He holds a Bachelor’s Degree in Psychology and Philosophy from Franklin & Marshall College, a Master’s Degree in Experimental Psychology from Cornell University, a professional certification (MRP) in Marketing Research from the University of Georgia, and an MBA in International Management from Royal Holloway College at The University of London. Bora Kim’s research broadly focuses on emotions and decision-making. In particular, she is interested in the effect of incidental emotion on decisions in social dilemmas. She earned her BA and MA at Seoul National University. Before her current position as an Experimental Psychology PhD student in the Department of Psychology at Washington State University, Bora earned a MS in Management from University of Arizona. View publication statsView publication stats","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/Tang24fastPredictConeBinOptCaVE.pdf","text":"CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary Linear Programs Bo Tang[0000−0002−6035−5167] and Elias B. Khalil [0000−0001−5844−9642] SCALE AI Research Chair in Data-Driven Algorithms for Modern Supply Chains Department of Mechanical and Industrial Engineering, University of Toronto {botang, khalil}@mie.utoronto.ca Abstract. The end-to-end predict-then-optimize framework, also known as decision-focused learning, has gained popularity for its ability to inte- grate optimization into the training procedure of machine learning mod- els that predict the unknown cost (objective function) coefficients of optimization problems from contextual instance information. Naturally, most of the problems of interest in this space can be cast as integer linear programs. In this work, we focus on binary linear programs (BLPs) and propose a new end-to-end training method to predict-then-optimize. Our method, Cone-aligned Vector Estimation (CaVE), aligns the predicted cost vectors with the normal cone corresponding to the true optimal solution of a training instance. When the predicted cost vector lies in- side the cone, the optimal solution to the linear relaxation of the binary problem is optimal. This alignment not only produces decision-aware learning models, but also dramatically reduces training time as it cir- cumvents the need to solve BLPs to compute a loss function with its gradients. Experiments across multiple datasets show that our method exhibits a favorable trade-off between training time and solution qual- ity, particularly with large-scale optimization problems such as vehicle routing, a hard BLP that has yet to benefit from predict-then-optimize methods in the literature due to its difficulty. Keywords: Integer programming · predict-then-optimize · data-driven optimization · machine learning. 1 Introduction Theoretical and experimental results reported over the past few years, starting with Elmachtoub and Grigas [7] and Ban and Rudin [2], have demonstrated the need for end-to-end training of Machine Learning (ML) models that pre- dict the cost coefficients of optimization problems. This contrasts with the more traditional two-stage approach, where an ML model is first trained to mini- mize regression loss for prediction, and then its predictions are applied to new test instances for decision. This conventional approach often leads to substantial regret in terms of the quality of the solutions obtained, especially when the train- ing set is small. Given that such predict-then-optimize settings are commonlyarXiv:2312.07718v2 [cs.LG] 15 Mar 2024 2 B. Tang, E. Khalil encountered in many applications (e.g., predicting product demand to manage inventory or travel time on a road network to route trucks), researchers in ML and optimization have proposed a wide range of end-to-end training methods, many of which have been recently surveyed and compared [16, 24, 28]. With a few exceptions [6, 11], these methods largely follow the now prevalent mini-batch stochastic gradient descent training algorithm. This process involves the following steps in each iteration: (0) a small batch of training instances is fed into the ML model; (1) the model predicts the cost coefficients; (2) a loss function, incorporating the concept of “decision error” such as regret, is calculated; (3) the gradients of this loss w.r.t. the parameters of the ML model are computed using backpropagation; and (4) a gradient descent step is employed to update the model parameters. A common feature of many of these methods is the necessity to solve the optimization problem for each training instance at least once in the forward pass (Steps (1, 2)). Since these repeated calls to the (integer) optimization solver represent a significant computational bottleneck, there have been attempts to improve efficiency by replacing them with much cheaper linear optimization calls [17], solution caching [14, 20], and function approximation [9, 26, 27]. However, these measures often come at a sacrifice in solution quality, as we empirically demonstrate in this paper. In this work, we are interested in efficient end-to-end training of ML mod- els that predict cost coefficients of challenging binary linear optimization prob- lems, a prime example of which is the Capacitated Vehicle Routing Problem (CVRP) [29]. CVRP is defined on a graph comprising customers and a depot, where binary variables represent the edges of the graph and indicate whether or not a vehicle traverses that edge. The number of vehicles, their capacities, and customer demands are fixed and known. Linear constraints capture the re- quirements of valid tours for all vehicles that start and end at the depot, comply with vehicle capacity limits, and visit each customer exactly once. In the predict- then-optimize setting, each CVRP instance is associated with a “feature vector” (e.g., weather conditions, time-of-day, whether it is a holiday or not, etc.) that is predictive of the (unknown) travel times on the graph’s edges, which are also the cost coefficients of the objective function, namely the total travel time. CVRP is notoriously hard to solve, even for tens of customers, making end-to-end training of ML models extremely time-consuming. A distinguishing feature of our CaVE loss functions is that they do not require solving the original optimization problem during training. Instead, they rely on easier projection problems, continuous and quadratic, which are significantly faster to solve. Our key insight is as follows: By ensuring that the predicted cost vector falls inside a specific cone, namely, one that corresponds to the optimal solution under the true cost vector, we are able to recover this optimal solution. The binding (or active) constraints at the optimal solution define this critical cone. To align the ML model’s predicted cost vector with the cone, we need to minimize the angle between the prediction and the cone; this is done through projection onto the cone, our main optimization routine. We show that CaVE trains ML models in a fraction of the time required by state-of-the-art methods CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 3 such as SPO+ [7] and PFYL [3] while yielding equally effective cost predictions as measured by regret in unseen test instances. 2 Related Work In the field of operations research, the integration of ML methodologies has emerged as a crucial area of research, significantly reshaping traditional ap- proaches. End-to-end predict-then-optimize, also known as decision-focused learn- ing, effectively utilizes data to tackle optimization problems involving unknown (cost) coefficients. KKT-Based Methods. A notable advancement in this area is the KKT-based method: Amos and Kolter [1] obtain both optimal solutions and gradients by solving a linear system derived from KKT conditions. Wilder et al. [31] adapted the method for linear programs (LPs) by incorporating a small quadratic term into the objective function, while Mandi and Guns [15] introduced a logarithmic barrier term. Furthermore, Ferber et al. [8] employ the cutting-plane method, which allows for integer variables. These KKT-based implicit differentiation methods require the use of specialized solving algorithms, which inherently limit their flexibility and often compromise the efficiency of solving problems such as linear programming. Furthermore, aside from the time-intensive cutting-plane method, KKT-based approaches generally struggle to tackle discrete models. Black-Box Methods. In contrast, other methodologies approach the optimiza- tion solver as a black box, functioning independently of the solver and algorithm. Elmachtoub and Grigas [7] propose a convex surrogate of regret for linear ob- jective functions, which has nonzero subgradients. In contrast to this convex and theoretically sound loss function, Poganˇci´c et al. [23] present a linear in- terpolation, transforming optimization into a piecewise linear function. As a specific case of the interpolation method, Sahoo et al. [25] adopt a more direct straight-through estimator by using the negative identity matrix as the surro- gate optimization gradient. Niepert et al. [21] extend the interpolated method with perturbations with random noise. In the context of linear objective func- tions, more perturbation approaches [3, 4] involve adding a perturbation to the predicted cost coefficients to smooth the optimization function and further con- struct a loss function based on duality. Since the training process involves solving the optimization problem in each iteration, many solutions naturally accumulate as samples. Under the assump- tion that the feasible region remains fixed, these solutions are all feasible. There- fore, Mulamba et al. [20] proposes a contrastive loss designed to maximize the distinction between suboptimal solutions in the sample and the optimal solution. Additionally, it utilizes these accumulated solutions as a cache, effectively reduc- ing computational cost. Inspired by the contrastive approach, Mandi et al. [14] employed “learning-to-rank” [13] by ranking the objective value of the cached solutions. 4 B. Tang, E. Khalil Function Approximation Methods. Due to the computational inefficiency often encountered in solving optimization problems, function approximation methods that do not require constraint optimization are appealing. The critical component of function approximation is a learnable surrogate function, which is learned to mimic the original objective or loss function. Shah et al. [26, 27] samples datasets and employs an additional neural network model. This model is trained to approximate the actual loss of a decision. In doing so, they effectively deploy this approximate loss in end-to-end training. With the approximation method, the complexity of directly dealing with the original function is signif- icantly reduced, allowing for more efficient learning. However, the accuracy of the approximate loss function directly impacts the final model performance. In practice, training a model to learn and approximate a specific function effectively can be a challenging endeavour. 3 Problem Statement and Preliminaries 3.1 Definitions and Notation For the sake of clarity, we can define a binary linear program as follows: There are binary decision variables w ∈ {0, 1} d. The cost coefficients associated with these decision variables are represented by c ∈ Rd; the constraints are Aw ≤ b, where A ∈ Rk×d and b ∈ Rk: min w c⊺w s.t. Aw ≤ b, w ∈ {0, 1} d. (1) Let Ω represent the feasible region of the problem; then the optimal solution is expressed as w∗(c) = arg minw∈Ω c⊺w. Given its computational complexity, the process of determining w∗(c) can be extremely time-consuming, especially when the number of decision variables and constraints is large. In the predict-then-optimize setting, the coefficients c are unknown and linked to a feature vector x ∈ Rp. This relationship facilitates the use of an ML model g(x, θ) to estimate the predicted coefficients ˆc. In this model, θ rep- resents the learnable parameters, which are adjusted to minimize a decision loss L(·), a metric that quantifies the discrepancy between the true optimal solution w∗(c) and the solution derived from the prediction ˆc. 3.2 Metric To measure the quality of the decision, regret is introduced by Elmachtoub and Grigas [7], which is defined as the absolute gap between the objective value of the solution w∗(ˆc) obtained using the predicted coefficients ˆc, and the optimal value of the solution w∗(c) obtained using the actual coefficients c. It is expressed in the following equation: LRegret(ˆc, c) = c⊺(w∗(ˆc) − w∗(c) ). (2) CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 5 In line with Elmachtoub and Grigas [7], we adopt normalized regret, which serves as an adjusted metric that takes into account the scale of the problem, providing a more standardized and comparable measure with n samples: ∑n i=1 LRegret (ˆci, ci) ∑n i=1 |c⊺ i w∗ i (ci)| . (3) The regret is best understood as follows: If a method records a test regret of 0.07 for example, it means that it produces solutions that are 7% worse than the true optimal solutions under the true but unknown cost vectors. 4 Methodology 4.1 Optimal Cones and Subcones Normal Cone Feasible Region 𝒄𝒄1 𝒄𝒄2 Fig. 1: Illustration of a normal cone: the cost vectors c1 and c2 produce the same optimal solution if and only if they lie within this cone. In (continuous) LP, one can associate a normal cone as optimal cone with a given cost vector c ∈ Rd. Within this cone, all cost coefficients yield the same optimal solution w∗(c). As depicted on the right of Fig. 1, the construction of the normal cone leverages the conical combination of the binding constraints ̃A(c) at w∗(c): each vector within the cone can be represented as a non-negative combination of the binding constraints coefficients aj ∈ ̃A(c), which can be written as ∑ aj ∈ ̃A(c) λiaj, ∀λ ≥ 0, Now consider the case of a binary linear program. If linear cuts A′(c) correspond to the convex hull of integer points of the BLP, the same logic to obtain the optimal cone of an integer solution would have been applied. This parallel is illustrated on the right side of Figure 2 for a BLP optimal cone, defined as C∗(c) =    v ∈ Rd : v = ∑ aj ∈A′(c) λiai, ∀λ ≥ 0    . 6 B. Tang, E. Khalil 2w1+ 2w2≤ 3 w1≤ 1 w2≤ 1 0 1 1 w1 w2 Feasible Region 𝒘𝒘 ∗ 𝒄𝒄 2w1+ 2w2≤ 3 w1≤ 1 w2≤ 1 0 1 1 w1 w2 Feasible Region （Points） 𝒘𝒘 ∗ 𝒄𝒄 w1+ w2≤ 1 Fig. 2: Illustration of the optimal cone and optimal subcone: On the left, the green cone is the optimal cone of a BLP. On the right, the green cone is a subset of the left cone and the optimal cone of the LP relaxation of the BLP on the left. Here, the cone is delineated by the cuts w1 ≥ 0, w2 ≤ 1 and w1 + w2 ≤ 1. However, we typically do not operate on the convex hull of a BLP, necessitating an alternate definition for the cones with which we will attempt to align the predictions. To this end, we keep the normal cone of a BLP as the optimal subcone: SC∗(c) =   v ∈ Rd : v = ∑ aj ∈ ̃A(c) λiai, ∀λ ≥ 0    . For a BLP with an optimal solution w∗(c), the optimal subcone is the optimal cone of the same solution, but with the LP relaxation of the BLP instead. In Fig. 2, the cone on the left side is the optimal cone for the BLP; the cone on the right side is the optimal subcone. Since SC∗(c) ⊂ C∗(c), all rays within the subcone also belong to the optimal cone. Although the LP relaxation leads to an expanded feasible region, all binary feasible solutions of the original BLP remain feasible vertices in the LP. As such, recovering an optimal subcone for a given solution w∗(c) is trivial: for a BLP problem defined as in Equation 1, its optimal subcone is the set of vectors that can be expressed as nonnegative combinations of the coefficient vectors aj ∈ ̃A(c) that satisfy ajw∗(c) = bj, including the variable bounds for which we have either w∗ i (c) = 0 or w∗ i (c) = 1. CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 7 4.2 Cone-aligned Vector Estimation: three variants Cone-aligned Vector Estimation (CaVE) is an approach specially designed for end- to-end training in BLP. The core idea is to leverage the optimal subcone defined in Sec. 4.1. This approach aims to train an ML model so that the predicted cost coefficients reside within this optimal subcone. To drive the predicted vector ˆc into the subcone, the ML model is trained to reduce the angle ϕ between the cost prediction and the subcone. Accordingly, the loss LCaVE(·) to be minimized can be defined as the negative cosine similarity between the prediction and its projection onto the subcone, namely: LCaVE(ˆc, ̃A(c)) = −cosine similarity(ˆc, pˆc) = − ˆc⊺pˆc ∥ˆc∥∥pˆc∥ , (4) where, assuming there are m binding constraints, i.e., ̃A(c) ∈ Rm×d, the projec- tion writes: pˆc = ̃A(c) ⊺λ∗, λ∗ = arg min λ≥0 ∥ ̃A(c) ⊺λ − ˆc∥2. (5) Algorithm 1 Cone-aligned Vector Estimation (CaVE) Require: Pairs of feature vectors and binding constraints {(xi, ̃A i)}n i=1 for n training instances; learning rate α > 0 1: Initialize model parameters θ 2: for each training epoch do 3: for each batch of training samples (x, ̃A) do 4: Predict cost coefficient ˆc ← g(x, θ) 5: Compute projection pˆc with quadratic program (5) 6: Compute cosine similarity loss LCaVE(ˆc, ̃A) (4) 7: Compute the gradient ∇θLCaVE(ˆc, ̃A) with backpropagation 8: Update ML model parameters θ ← θ − α∇θLCaVE(ˆc, ̃A) 9: end for 10: end for 11: return g(·, θ) When the alignment is precise, i.e., the predicted cost vector falls within the correct optimal subcone, the CaVE loss achieves its minimum value of −1, indicat- ing an optimal decision. Although our method still requires a quadratic program (QP) to compute the projection of the prediction values during each training it- eration, it effectively circumvents the need to solve the more challenging binary linear program. Algorithm 1 presents a detailed, step-by-step description of the CaVE training process. Figure 3 illustrates the three types of projection that can be employed in the CaVE framework. The exact projection projects the cost coefficients ˆc directly onto the surface of the optimal subcone; this is the approach that we have just laid out in Algorithm 1. The inner projection ensures that the projected vector 8 B. Tang, E. Khalil �𝒄𝒄 𝒄𝒄 𝜙𝜙 𝒑𝒑�𝒄𝒄 = �𝑨𝑨 𝒄𝒄 ⊺𝝀𝝀∗ 𝝀𝝀∗ = argmin 𝝀𝝀≥𝟎𝟎 �𝑨𝑨 𝒄𝒄 ⊺𝝀𝝀 − �𝒄𝒄 2 Optimal Subcone Feasible Region �𝒄𝒄 𝒄𝒄 𝜙𝜙 𝒑𝒑�𝒄𝒄 = �𝑨𝑨 𝒄𝒄 ⊺𝝀𝝀∗ 𝝀𝝀∗ = subargmin 𝝀𝝀≥𝟎𝟎 �𝑨𝑨 𝒄𝒄 ⊺𝝀𝝀 − �𝒄𝒄 2 �𝒄𝒄 𝒄𝒄 𝜙𝜙 �𝑨𝑨 𝒑𝒑�𝒄𝒄 = 1 − 𝛾𝛾 �𝒄𝒄 + 𝛾𝛾�𝑨𝑨 𝒄𝒄 ⊺ �𝑨𝑨 𝒄𝒄 = 1 𝑚𝑚 �𝒂𝒂𝑗𝑗∈�𝑨𝑨 𝒄𝒄 𝒂𝒂𝑗𝑗 Fig. 3: Illustration of the three projections: Exact projection on the left, inner projection in the middle, and heuristic projection on the right. lies strictly within the subcone. The heuristic projection is an approximation of the true projection onto the optimal subcone, used to reduce the computational cost. We will detail these three variants next. CaVE-E with exact projection. CaVE-E performs exact projection, wherein the optimal solution of the quadratic programming problem (5) is computed to locate the projection on the surface of the cone, as illustrated on the left in Fig. 3. Nevertheless, the CaVE-E method encounters a significant drawback due to its projection onto the face of the cone and the use of cosine similarity as the loss function. This approach results in the vanishing of gradients as the predicted cost vector nears the surface of the optimal subcone but is yet to enter it. Experimental evidence in Section 6 corroborates this issue – the regret of CaVE-E is typically higher than existing end-to-end methods, necessitating a modification. CaVE+ with inner projection. Due to the issue of vanishing gradients associ- ated with CaVE-E, CaVE+ replaces the exact projection with what we refer to as “inner projection”. The goal is to obtain a projection of the predicted cost vector that lies inside the subcone. As all optimal projections will lie on a face of the subcone, we thus require a suboptimal solution to the projection problem. This is readily achieved by simply limiting the number of iterations in the quadratic programming solver and thus terminating prematurely. Since the solver uses the primal-dual interior point method, the feasibility is guaranteed at each iteration. In our experiments, the maximum number of iterations of the QP solver used by CaVE+ was set to 3. A suboptimal projection will lie inside the optimal sub- cone, resulting in nonzero loss and a strong gradient signal that will push the ML model parameters to produce predictions that move toward the inside of the subcone. Compared to exact projection, this approach is more computationally efficient with fewer iterations of the interior point method. The inner projection is illustrated in the middle of Fig. 3. CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 9 CaVE-H with a mix of inner and heuristic projections. To alleviate the computational burden of repeated QP solving in both CaVE-E and CaVE+, a hybrid strategy is employed in CaVE-H. We interleave inner projections (obtained with a QP just as in CaVE+) with much cheaper heuristic projections. Unlike exact and inner projections, the heuristic projection does not necessitate solving a quadratic program and instead requires a simple convex combination of A(c) and ˆc with weight γ ∈ [0, 1]: pˆc = (1 − γ)ˆc + γA(c)⊺, where A(c) = 1 m ∑ aj ∈ ̃A(c) aj is the average of all binding constraints. As illus- trated on the right in Fig. 3, it is crucial to note that the heuristic projection is not guaranteed to be in the optimal subcone, but it still ensures that the cost coefficient vector is pushed in the direction of the optimal subcone. With probability β < 0.5, i.e., in the minority of the iterations of Algorithm 1, CaVE-H performs an inner projection via QP. With probability (1 − β), the heuristic pro- jection is used instead, without any optimization required. In our experiments, γ and β are set to 0.2 and 0.3, respectively, and were not tuned any further for performance. Method Iteration cost PFYL K×BLP SPO+ 1×BLP NCE β×BLP CaVE-E 1×QP CaVE+ 1×QP (partial) CaVE-H β×QP (partial) Table 1: Comparison of state-of-the-art predict-then-optimize methods SPO+, PFYL, and NCE with the three CaVE variants w.r.t. “Iteration cost” (per training instance), i.e., the frequency of time-consuming solver calls required to compute the loss of a method during gradient descent. The methods are sorted in decreas- ing order of their iteration costs. SPO+ requires solving the BLP with predicted costs and PFYL requires solving the K (typically 1-5) BLPs, whereas NCE with solution cache needs to solve only a small fraction (100 × β%) of the BLPs. CaVE methods require solving a single QP, or partially solving a single QP in (100 × β%) of the iterations. Comparison with existing methods. Table 1 summarizes the computational cost of training predict-then-optimize models using SPO+ [7], PFYL [3], NCE [20] and the three CaVE variants. We choose SPO+ and PFYL as they represent the state-of-the-art based on an extensive evaluation carried out in [28], though 10 B. Tang, E. Khalil our experiments will include NCE with solution cache, a fast method, as well as accelerated variants of SPO+ and PFYL. The table shows that CaVE-H and NCE are the fastest to train, whereas PFYL and SPO+ exhibit similar costs, particularly when PFYL’s number of random perturbations of the predicted cost vector is set to K = 1. CaVE+ sits between CaVE-E and CaVE-H in terms of training cost. We will empirically examine these theoretical complexities in the next section. 5 Benchmark Datasets We utilized synthetic datasets [28] for our experiments. The synthetic dataset, denoted as D, comprises feature vectors x and the corresponding cost coefficients c. Each feature vector x adheres to a standard Gaussian distribution, and the associated cost vector c is derived from a polynomial function of x, with added random noise. Shortest Path. The shortest path problem is a fundamental task in graph theory and network analysis, where the objective is to find a minimum weight path from a fixed source node to a fixed target node. Our instances are based on 5×5 grid networks (SP5) where the source is the node in the northwest corner of the grid and the target is the node in the southeast corner. The cost coefficient cij comes from [ 1 3.5deg ( 1 √5 (Bxi)j + 3 )deg + 1 ] · ϵij, where feature size is 5, B follows a Bernoulli distribution, ϵij is random noise uniformly distributed between 0.5 and 1.5, and deg is the polynomial degree of feature mapping. This is a standard and easy task introduced by Elmachtoub and Grigas [7]. Traveling Salesperson and Vehicle Routing. We utilized the traveling salesperson problem (TSP) dataset. TSP, known for its NP-hardness, is a clas- sic problem in combinatorial optimization where the goal is to find the shortest tour that visits a set of locations exactly once and returns to the original starting point. In our study, we do not only test TSP instances with 20 and 50 nodes, but also explore the same graphs under the much more challenging CVRP problem. In this dataset, the cost coefficient cij comes from two parts: The first part con- sists of the Euclidean distances among the nodes, and the second part resembles the shortest path problem, formulated as ( 1 √10 (Bxi)j + 3 )deg · ϵij, where the feature vector x ∈ R10. For CVRP, the capacity of each vehicle is set at 30, and each customer’s demand follows a uniform distribution from 0 to CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 11 10. To our knowledge, our work is the first to use a CVRP in a predict-then- optimize setting. As will become apparent in the next section, this is due to the time required to solve the BLP representing the CVRP, making SPO+ and PFYL extremely time-consuming as per Table 1. 6 Experimental Results 6.1 Experimental setup The experiments were designed to evaluate both the training time and normal- ized regret, with the size of both the training and test sets being 1,000 instances. To account for randomness in data generation and stochastic model training, 5 or 10 random seeds are used to generate and train training/test sets and the cor- responding ML models. Our comparative analysis encompassed several methods, including three variants of CaVE, a two-stage approach (least-squares regression on cost vectors), SPO+, PFYL, and NCE. A linear model was used as the cost prediction model class for all the aforementioned approaches; this is a standard choice in the literature starting with [7]. In the case of PFYL, the size of random sampling is fixed at K = 1; for NCE, the solving ratio is set at β = 5%. The Adam optimizer was used for gradient descent for all our experiments. For SP5, a learning rate of 0.01 was used with all methods for 10 epochs, except for the two-stage and NCE method which allotted 20 epochs at the same learning rate. For both TSP and CVRP, the only modification was to increase the learning rate to 0.05, maintaining the number of epochs at 10. In selecting the number of epochs, careful consideration was given to the convergence of each model, ensuring that our results are not skewed by over- or under-training. The loss curves in Fig. 4 show that all methods exhibit a convergent behavior, with CaVE methods doing so earlier than the baselines. Note that the 2-Stage models are trained essentially to its optimality as they are convex optimization problems. 0 2 4 6 8 10 12 14 16 18 Epochs 2.50 2.25 2.00 1.75 1.50 1.25 1.00 0.75Log Normalized RegretValidation Regret for the TSP50 with Polynomial Degree 4 2-Stage CaVE+ SPO+ PFYL NCE 0 2 4 6 8 10 12 14 16 18 Epochs 2.5 2.0 1.5 1.0 0.5Log Normalized RegretValidation Regret for the TSP50 with Polynomial Degree 6 2-Stage CaVE+ SPO+ PFYL NCE Fig. 4: Validation regeret curves for TSP50 with a polynomial of degree 4 (left) and 6 (right). The vertical axis represents the average normalized regret values of each of the five methods for the validation dataset with 1000 instances. Our numerical experiments were conducted using Python v3.9.6 on a system with 8 Intel E5-2683 v4 CPUs and 32GB memory. We utilized SciPy [30] v1.11.2 12 B. Tang, E. Khalil and Clarabel 0.6.0 for QP solving, Gurobi [10] 10.0.3 for BLP, and PyTorch [22] v2.0.1 with PyEPO [28] v0.3.6 for end-to-end training, where PyEPO provided implementations for SPO+, PFYL and NCE. Our code is available at https:// github.com/khalil-research/CaVE. 6.2 Results Tables 2, 3, 4, 5, and 6 summarize the results. The tables correspond to the Shortest Path problem on 5 × 5 grid (SP5), TSP with 20 nodes (TSP20), TSP with 50 nodes (TSP50), CVRP with 20 customers (CVRP20), and CVRP with 30 customers (CVRP30), respectively. Each table contains two sub-tables: (a) reports the normalized regret metric defined in Section 3.2 on 1,000 test instances and (b) reports the training time in seconds; both quantities are averages over 5 random seeds for CVRP20 and 10 random seeds for others with standard deviation, while the experiments for CVRP30 is not repeated due to running time. Each sub-table has two rows, one corresponding to datasets that use a degree-4 polynomial in the (unknown) mapping from instance features to cost coefficients, and the other for a degree-6 polynomial; we refer to Section 5 for details on the role of the polynomial in the function that we are attempting to learn, but note that the higher the degree the more difficult the learning task. In each sub-table, we bold the best-performing method, excluding the 2-Stage method as it is often fast to train, but has much worse regret than end-to-end methods. Shortest Path. This is the easiest BLP (in fact, LP) we will look at. It serves as a sanity check for any new method in this space. Table 2 shows that SPO+ and PFYL achieve the lowest test regrets, closely followed by CaVE-H and CaVE+. The latter two trains roughly 10 times faster than SPO+ and PFYL, already sub- stantiating our claim that QP solving is faster in solving the optimization prob- lem itself, even for small-scale polynomial-time solvable shortest path problems. In addition, although NCE is fast, it has a higher regret than others. Table 2: Experimental Results for SP5 (a) Average Test Normalized Regret (%) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 8.82 ± 1.15 10.73 ± 1.54 8.39 ± 0.95 8.35 ± 0.88 7.79 ± 1.00 7.68 ± 0.99 11.34 ± 1.11 Deg 6 12.58 ± 2.14 11.30 ± 1.30 8.89 ± 0.90 8.84 ± 1.00 7.72 ± 1.11 7.86 ± 0.96 13.78 ± 1.58 (b) Average Training Time (Sec) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 1.52 ± 0.14 4.64 ± 0.09 4.89 ± 0.12 2.57 ± 0.19 17.64 ± 0.12 18.52 ± 0.31 4.50 ± 0.48 Deg 6 1.38 ± 0.13 3.52 ± 0.11 3.72 ± 0.14 2.39 ± 0.19 18.68 ± 0.40 17.78 ± 0.13 4.38 ± 0.42 CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 13 TSP. For the TSP, SPO+, PFYL and NCE employ the Dantzig–Fulkerson–Johnson (DFJ) formulation [5] to solve this BLP efficiently. For both TSP20 and TSP50, CaVE+ achieves the best time-regret trade-off across all methods: its regret is the best or second-best across all methods and its training time is the second- best after CaVE-H. As shown in part (b) of Tables 3 and 4, CaVE+ trains in roughly half the time of SPO+ and PFYL, achieving the same or even better test regret. Additionally, we also compared our methods with SPO+ Rel and PFYL Rel, which employ a linear relaxation of the BLP during training, as detailed in the Appendix A.1. While they do reduce the training time of vanilla SPO+/ PFYL by a bit, this typically comes at an increase in regret. Similarly, NCE has a prohibitively high regret. Table 3: Experimental Results for TSP20 (a) Average Test Normalized Regret (%) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 12.12 ± 0.89 7.35 ± 0.40 6.20 ± 0.24 7.69 ± 0.33 5.95 ± 0.16 6.56 ± 0.21 12.21 ± 0.88 Deg 6 21.32 ± 1.81 8.01 ± 0.45 6.97 ± 0.37 9.52 ± 0.64 7.48 ± 0.36 7.41 ± 0.37 14.31 ± 0.40 (b) Average Training Time (Sec) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 1.52 ± 0.10 113.56 ± 3.16 107.15 ± 3.80 27.06 ± 2.17 175.23 ± 4.95 220.21 ± 24.20 25.92 ± 4.23 Deg 6 1.53 ± 0.19 158.66 ± 9.65 102.19 ± 10.38 30.17 ± 2.62 185.13 ± 7.44 185.02 ± 5.09 25.48 ± 3.66 Table 4: Experimental Results for TSP50 (a) Average Test Normalized Regret (%) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 28.16 ± 1.08 15.19 ± 0.65 7.69 ± 0.22 9.59 ± 0.44 7.57 ± 0.20 8.03 ± 0.23 14.31 ± 0.40 Deg 6 52.61 ± 2.36 23.25 ± 2.41 8.57 ± 0.38 11.28 ± 0.80 10.26 ± 0.46 9.00 ± 0.52 17.12 ± 0.48 (b) Average Training Time (Sec) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 1.55 ± 0.18 611.47 ± 23.52 518.07 ± 51.89 196.96 ± 35.92 1220.68 ± 85.39 1328.99 ± 28.87 151.80 ± 24.21 Deg 6 1.16 ± 0.13 502.71 ± 16.03 573.87 ± 20.19 253.93 ± 27.67 1191.29 ± 42.63 1456.21 ± 34.18 155.95 ± 24.46 CVRP. As mentioned earlier, we are the first to tackle a CVRP in an end-to- end predict-then-optimize setting. To solve CVRP as a BLP, we formulated the problem with k-path cuts [12] and solved it using Gurobi. For CVRP20, CaVE+, 14 B. Tang, E. Khalil Table 5: Experimental Results for CVRP20 (a) Average Test Normalized Regret (%) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 10.10 ± 0.64 9.26 ± 1.56 6.44 ± 0.24 7.92 ± 0.52 5.94 ± 0.25 6.32 ± 0.28 15.77 ± 0.96 Deg 6 19.50 ± 1.22 11.64 ± 0.25 7.94 ± 0.54 11.44 ± 1.14 8.75 ± 0.28 8.09 ± 0.57 18.96 ± 1.01 (b) Average Training Time (Sec) with Standard Deviation Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 1.65 ± 0.48 213.56 ± 42.36 153.56 ± 11.08 44.52 ± 6.27 7020.11 ± 1043.05 3773.31 ± 288.84 583.56 ± 170.67 Deg 6 1.54 ± 0.25 208.95 ± 12.90 127.94 ± 13.84 51.83 ± 8.78 2204.83 ± 99.86 6197.84 ± 288.63 470.20 ± 84.46 SPO+ and PFYL compete for the lowest regret, with CaVE-H running closely be- hind. However, CaVE+ completes its 10 training epochs in roughly 2-3 minutes, whereas SPO+ and PFYL require more than 1 hour each, on average. The regret of NCE is high and the training time has increased a lot. Similar as TSP, we also employ a linear relaxation of the BLP during training for SPO+ and PFYL (see Appendix A.2), which results in a significant decrease in training time at the cost of regret. Table 6: Experimental Results for CVRP30 (a) Test Normalized Regret Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 0.1972 0.1254 0.0913 0.0999 N/A 0.1828 (b) Training Time (Sec) Methods 2-Stage CaVE-E CaVE+ CaVE-H SPO+ PFYL NCE Deg 4 9.27 331.73 287.77 132.62 ≥ 100h 884.95 This performance gap is even more pronounced for CVRP30 in Table 6. It takes roughly 20 seconds on average to solve a single CVRP30 instance, thus requiring 8 or so hours to traverse the entire dataset of 1,000 training instances once for SPO+ and PFYL. This makes end-to-end training with these methods impractical for real-world applications. In contrast, CaVE demonstrates its ability to handle such a challenging problem efficiently. Note that due to the scale of the problem, our experimental evaluation was not repeated with random seeds and we used a smaller test set comprising only 10 instances. All CaVE variants achieve test regrets of 9-12% compared to the 2-Stage method’s far higher 20%, while requiring only 2-6 minutes of training each. In addition, NCE achieves 18% with 15 minutes of training and is thus worse than all variants CaVE in both CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 15 metrics. To our knowledge, this is the hardest optimization problem ever targeted in the predict-then-optimize literature, a feat that is only possible due to the computational efficiency brought about by CaVE. 7 Conclusion CaVE reframes the end-to-end training problem for predict-then-optimize as a regression task. Unlike the traditional two-stage approach, which regresses on the cost vectors, our framework instead regresses on cones that correspond to optimal solutions under the true costs. CaVE can be seen as an attempt at ob- taining the best of both worlds: fast training with a regression loss that does not require solving hard integer optimization problems in every iteration of gradient descent, and a loss function that penalizes cost predictions that point in the wrong direction relative to the optimal decision. We proposed three versions of our method with varying performance trade-offs. The best of the three appears to be CaVE+, which regresses on an inner vector of the optimal subcone of a training instance, resulting in stable and efficient training, as well as test regret results that compare to state-of-the-art methods that require training 30 times longer on CVRP20, or do not even complete a single training epoch in 8 hours for CVRP30. We note that if we were to use early termination during training, CaVE methods would record even smaller training times as they do converge in fewer gradient descent iterations than competing methods as per Fig. 4. We hope that our framework will enable the adoption of end-to-end predict-then-optimize in a wider range of applications and have made our implementation available with plans to make CaVE one of the standard methods within the PyEPO package following the publication of this work. CaVE is limited to binary problems. In practice, this is not too problematic, as bounded integer variables can be represented using a set of binary variables. An- other limitation of our method is the lack of theoretical guarantees. In particular, we currently do not know whether the loss function in Eq. (4) or a modification thereof could be proven to be a valid upper bound on regret, as does the SPO+ loss of Elmachtoub and Grigas [7] for example. This direction merits further investigation. An interesting connection may be established between CaVE and recent ML methods such as [19] to predict the active constraints of a family of similar optimization problems for which optima are known. Rather than predict- then-optimize, the goal of Misra et al. [19] is to accurately predict the active set to solve a reduced optimization problem over only that set. Bibliography [1] Amos, B., Kolter, J.Z.: Optnet: Differentiable optimization as a layer in neural networks. In: International Conference on Machine Learning, pp. 136–145, PMLR (2017) [2] Ban, G.Y., Rudin, C.: The big data newsvendor: Practical insights from machine learning. Operations Research 67(1), 90–108 (2019) [3] Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J.P., Bach, F.: Learning with differentiable perturbed optimizers. arXiv preprint arXiv:2002.08676 (2020) [4] Dalle, G., Baty, L., Bouvier, L., Parmentier, A.: Learning with com- binatorial optimization layers: a probabilistic approach. arXiv preprint arXiv:2207.13513 (2022) [5] Dantzig, G., Fulkerson, R., Johnson, S.: Solution of a large-scale traveling- salesman problem. Journal of the operations research society of America 2(4), 393–410 (1954) [6] Elmachtoub, A., Liang, J.C.N., McNellis, R.: Decision trees for decision- making under the predict-then-optimize framework. In: International Con- ference on Machine Learning, vol. 119, pp. 2858–2867, PMLR (2020) [7] Elmachtoub, A.N., Grigas, P.: Smart “predict, then optimize”. Management Science 0(0) (2021) [8] Ferber, A., Wilder, B., Dilkina, B., Tambe, M.: Mipaal: Mixed integer pro- gram as a layer. In: Proceedings of the AAAI Conference on Artificial In- telligence, vol. 34, pp. 1504–1511 (2020) [9] Ferber, A.M., Huang, T., Zha, D., Schubert, M., Steiner, B., Dilkina, B., Tian, Y.: Surco: Learning linear surrogates for combinatorial nonlinear op- timization problems. In: International Conference on Machine Learning, pp. 10034–10052, PMLR (2023) [10] Gurobi Optimization, LLC: Gurobi Optimizer Reference Manual (2021), URL https://www.gurobi.com [11] Jeong, J., Jaggi, P., Butler, A., Sanner, S.: An exact symbolic reduction of linear smart predict+ optimize to mixed integer linear programming. In: International Conference on Machine Learning, pp. 10053–10067, PMLR (2022) [12] Kohl, N., Desrosiers, J., Madsen, O.B., Solomon, M.M., Soumis, F.: 2-path cuts for the vehicle routing problem with time windows. Transportation science 33(1), 101–116 (1999) [13] Liu, T.Y., et al.: Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3(3), 225–331 (2009) [14] Mandi, J., Bucarey, V., Tchomba, M.M.K., Guns, T.: Decision-focused learning: through the lens of learning to rank. In: International Conference on Machine Learning, pp. 14935–14947, PMLR (2022) [15] Mandi, J., Guns, T.: Interior point solving for lp-based predic- tion+optimisation. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 17 M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, pp. 7272–7282, Curran Associates, Inc. (2020) [16] Mandi, J., Kotary, J., Berden, S., Mulamba, M., Bucarey, V., Guns, T., Fioretto, F.: Decision-focused learning: Foundations, state of the art, bench- mark and future opportunities. arXiv preprint arXiv:2307.13565 (2023) [17] Mandi, J., Stuckey, P.J., Guns, T., et al.: Smart predict-and-optimize for hard combinatorial optimization problems. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 1603–1610 (2020), https://doi.org/10.1609/aaai.v34i02.5521 [18] Miller, C.E., Tucker, A.W., Zemlin, R.A.: Integer programming formulation of traveling salesman problems. Journal of the ACM (JACM) 7(4), 326–329 (1960) [19] Misra, S., Roald, L., Ng, Y.: Learning for constrained optimization: Iden- tifying optimal active constraint sets. INFORMS Journal on Computing 34(1), 463–480 (2022) [20] Mulamba, M., Mandi, J., Diligenti, M., Lombardi, M., Bucarey, V., Guns, T.: Contrastive losses and solution caching for predict-and-optimize. arXiv preprint arXiv:2011.05354 (2020) [21] Niepert, M., Minervini, P., Franceschi, L.: Implicit mle: backpropagating through discrete exponential family distributions. Advances in Neural In- formation Processing Systems 34, 14567–14579 (2021) [22] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An im- perative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019) [23] Poganˇci´c, M.V., Paulus, A., Musil, V., Martius, G., Rolinek, M.: Differen- tiation of blackbox combinatorial solvers. In: International Conference on Learning Representations (2019) [24] Sadana, U., Chenreddy, A., Delage, E., Forel, A., Frejinger, E., Vidal, T.: A survey of contextual optimization methods for decision making under uncertainty. arXiv preprint arXiv:2306.10374 (2023) [25] Sahoo, S.S., Paulus, A., Vlastelica, M., Musil, V., Kuleshov, V., Martius, G.: Backpropagation through combinatorial algorithms: Identity with pro- jection works. arXiv preprint arXiv:2205.15213 (2022) [26] Shah, S., Perrault, A., Wilder, B., Tambe, M.: Leaving the nest: Go- ing beyond local loss functions for predict-then-optimize. arXiv preprint arXiv:2305.16830 (2023) [27] Shah, S., Wilder, B., Perrault, A., Tambe, M.: Learning (local) surrogate loss functions for predict-then-optimize problems. arXiv e-prints pp. arXiv– 2203 (2022) [28] Tang, B., Khalil, E.B.: Pyepo: A pytorch-based end-to-end predict-then- optimize library for linear and integer programming. arXiv preprint arXiv:2206.14234 (2022) [29] Toth, P., Vigo, D.: Vehicle routing: problems, methods, and applications. SIAM (2014) 18 B. Tang, E. Khalil [30] Virtanen, P., Gommers, R., Oliphant, T.E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., et al.: Scipy 1.0: fundamental algorithms for scientific computing in python. Na- ture methods 17(3), 261–272 (2020) [31] Wilder, B., Dilkina, B., Tambe, M.: Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 1658–1665 (2019) A More Experiments A.1 TSP Relaxation. To enhance training efficiency, it has been proposed in [17] that a linear relax- ation can be used as a substitute for the original BLP during training. Because the DFJ formulation utilizes constraint generation to handle subtour elimiation constraints, it is challenging to achieve linear relaxation due to the exponential number of constraints. In this study, we used the LP relaxation of the Miller- Tucker-Zemlin (MTZ) formulation [18] of the TSP, and trained SPO+ Rel and PFYL Rel on the same TSP instances with 20 and 50 nodes. Although relaxation methods are more efficient than CaVE in TSP20, they result in higher regret. As the size of the model increases, the relaxation approach also loses its efficiency advantage. Table 7: Experimental Results for TSP20 Relaxation (a) Average Test Normalized Regret (%) with Standard Deviation Methods SPO+ Rel PFYL Rel Deg 4 7.75 ± 0.32 9.15 ± 0.50 Deg 6 9.83 ± 0.57 11.28 ± 0.90 (b) Average Training Time (Sec) with Standard Deviation Methods SPO+ Rel PFYL Rel Deg 4 71.92 ± 2.05 77.25 ± 0.60 Deg 6 67.99 ± 0.60 53.32 ± 3.74 A.2 CVRP Relaxation. Similarly, the CVRP formulation with k-path cuts also struggles to obtain a linear relaxation. Thus, the MTZ formulation can be modified with constraints uj − ui ≥ Q(xij − 1) + qj ∀i ̸= j, i ̸= 0, j ̸= 0 for subtour elimilation and customer demand, where Q is the capacity and qi is the demand of customer i. The SPO+ Rel and PFYL Rel are trained for the CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize 19 Table 8: Experimental Results for TSP50 Relaxation (a) Average Test Normalized Regret (%) with Standard Deviation Methods SPO+ Rel PFYL Rel Deg 4 10.17 ± 0.23 11.11 ± 0.33 Deg 6 13.14 ± 0.46 13.38 ± 0.58 (b) Average Training Time (Sec) with Standard Deviation Methods SPO+ Rel PFYL Rel Deg 4 386.06 ± 9.69 536.67 ± 4.94 Deg 6 636.99 ± 3.04 510.37 ± 3.46 same VRP instances with 20 customers. While the linear relaxation approach demonstrates high efficiency in these scenarios, it does not guarantee a low regret compared to CaVE. Table 9: Experimental Results for CVRP20 Relaxation (a) Average Test Normalized Regret (%) with Standard Deviation Methods SPO+ Rel PFYL Rel Deg 4 8.03 ± 0.38 17.07 ± 0.63 Deg 6 15.73 ± 0.39 19.19 ± 1.66 (b) Average Training Time (Sec) with Standard Deviation Methods SPO+ Rel PFYL Rel Deg 4 78.95 ± 0.73 78.80 ± 1.19 Deg 6 78.74 ± 3.82 81.80 ± 0.86","libVersion":"0.3.2","langs":""}
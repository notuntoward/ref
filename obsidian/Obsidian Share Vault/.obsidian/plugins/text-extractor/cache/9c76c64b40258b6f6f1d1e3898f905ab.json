{"path":"lit/lit_sources.backup/Archambeau08MixturesRobustProbabilistic.pdf","text":"Neurocomputing 71 (2008) 1274–1282 Mixtures of robust probabilistic principal component analyzers Ce´ dric Archambeaua,\u0002, Nicolas Delannayb,1, Michel Verleysenb aCentre for Computational Statistics and Machine Learning, University College London, Gower Street, London WC1E 6BT, UK bMachine Learning Group, Universite´ catholique de Louvain, 3 Place du Levant, B-1348 Louvain-la-Neuve, Belgium Available online 2 February 2008 Abstract Mixtures of probabilistic principal component analyzers model high-dimensional nonlinear data by combining local linear models. Each mixture component is speciﬁcally designed to extract the local principal orientations in the data. An important issue with this generative model is its sensitivity to data lying off the low-dimensional manifold. In order to address this problem, the mixtures of robust probabilistic principal component analyzers are introduced. They take care of atypical points by means of a long tail distribution, the Student-t. It is shown that the resulting mixture model is an extension of the mixture of Gaussians, suitable for both robust clustering and dimensionality reduction. Finally, we brieﬂy discuss how to construct a robust version of the closely related mixture of factor analyzers. r 2008 Elsevier B.V. All rights reserved. Keywords: Mixture model; Principal component analysis; Dimensionality reduction; Robustness to outliers; Non-Gaussianity; EM algorithm 1. Introduction Extracting information from high-dimensional data is problematic due to the curse of dimensionality. It is of practical importance to discover an implicit, low-dimen- sional representation whenever the core of the data lies on one or several directed manifolds. Principal component analysis (PCA) is a well-known statistical technique for linear dimensionality reduction [12,14]. It projects high- dimensional data into a low-dimensional subspace by applying a linear transformation that minimizes the mean squared reconstruction error. PCA is used as a pre- processing step in many applications involving data compression or data visualization. The approach has, however, severe limitations. Since it minimizes a mean squared error, it is very sensitive to atypical observations, which in turn leads to identifying principal directions strongly biased toward them. Recently, PCA was reformulated as a robust probabil- istic latent variable model based on the Student-t density function [2]. Among others, the (univariate) Student-t density arises in the problem of estimating the mean of a Gaussian random variable when the variance is unknown (and the sample size is small) [9]. More generally, the multivariate Student-t is a heavy tailed generalization of the multivariate Gaussian. Hence, adjusting the thickness of the distribution tails reduces the sensitivity of its mean and covariance estimates to outliers. The robust probabilistic reformulation of PCA gener- alizes standard PPCA [20,26]. Increasing the robustness by replacing Gaussian densities with Student-t densities was also proposed in the context of ﬁnite mixture modelling [19,1]. In contrast with previous robust ap- proaches to PCA (see for example [28,13], and the references therein), the probabilistic formalism has a number of important advantages. First, it only requires to choose the dimension of the projection space, the other parameters being set automatically by maximum likelihood (ML). Previous attempts need in general to optimize several additional parameters. Second, the probabilistic approach provides a natural framework for constructing mixture models. This enables us to model low-dimensional nonlinear relationships in the data by aligning a collection of local linear generative models, instead of using neighborhood preserving dimensionality reduction techni- ques [23,25,21,5]. Third, a probabilistic model provides ARTICLE IN PRESS www.elsevier.com/locate/neucom 0925-2312/$ - see front matter r 2008 Elsevier B.V. All rights reserved. doi:10.1016/j.neucom.2007.11.029 \u0002Corresponding author. E-mail addresses: c.archambeau@cs.ucl.ac.uk (C. Archambeau), nicolas.delannay@uclouvain.be (N. Delannay), michel.verleysen@uclouvain.be (M. Verleysen). 1This author is a research fellow of the Belgian national fund for scientiﬁc research (FNRS). likelihood measures for the data, which can be used to compute posterior probabilities and eventually to construct a Bayes classiﬁer [4]. This article introduces mixtures of robust probabilistic principal component analyses (PPCAs). It is based on some earlier work [3] presented at the 15th European Symposium on Artiﬁcial Neural Networks. The method generalizes mixtures of standard PPCAs [27]. An interesting feature of the approach is that it can be used for robust density estimation and robust clustering, even in high-dimensional spaces. The main advantage resides in the fact that the full- rank, possibly ill-conditioned covariance matrices are approximated by low-rank covariance matrices, where the correlation between the (local) principal directions need not be neglected to avoid numerical instabilities. The number of free parameters per component depends on the speciﬁc choice for the dimension of the latent subspace. This procedure is more appealing than constraining the covariance matrices of the mixture components to be diagonal as it is often done in practice. Diagonal covariance matrices lead to axis aligned components, which are in general suboptimal [1]. PCA and PPCA are closely related to factor analysis (FA) [8] and ML FA [22], which can also be combined to form mixtures [11]. The mixture of PPCAs and its robust version assume that the likelihood of the data given the low-dimensional representation is isotropic. When considering a diagonal heteroscedastic noise model instead of the homoscedastic (or isotropic) one, we obtain the mixture of probabilistic factor analyzers (PFAs) [10]. This model is useful when it is reasonable to assume that the noise in the features is independent and of different amplitude. As mixtures of PPCAs, mixtures of PFAs can be made robust to atypical observations by formulating the probabilistic model in terms of the Student-t distribution. Hence, the aim of this work is to show that inherent robustness (with respect to atypical observations) can be achieved in the class of generative latent variable models that provide locally linear approximations to implicit low- dimensional data manifolds. Other important questions, not discussed in this work, are model selection (i.e., the optimal number of mixture components) and the auto- matic identiﬁcation of the optimal dimensionality of these manifolds. One possibility is to use cross-validation or bootstrap techniques [7]. However, these approaches are computationally intensive and they are only feasible when the number of hyperparameters is relatively small. Alternatively, (hierarchical) Bayesian techniques can be envisioned [4,16]. This paper is organized as follows. In Section 2 robust PCA is introduced and in Section 3 the corresponding mixture model is derived. ML estimates of the parameters are computed by means of the expectation–maximization (EM) algorithm [6]. The approach is validated in Section 4. Note that the EM algorithm for mixtures of robust PFAs is discussed in Appendix D. 2. Robust PPCA PCA seeks a linear projection which maps a set of observations fyngN n¼1 to a set of lower dimensional latent (unobserved) vectors fxngN n¼1 such that the variance in the projection space is maximized [14]. The latent variable model can be formalized as follows: yn ¼ Wxn þ l þ en, (1) where yn 2 R D and xn 2 Rd, with D4d. The matrix W 2 R D\u0002d is the (transposed) orthogonal projection matrix. The data offset and the projection errors are denoted by l and f\u0002ngN n¼1, respectively. In PPCA [20,26],it is further assumed that the error terms, as well as the prior uncertainty, are drawn from zero mean isotropic Gaussian2 densities. Tipping and Bishop [26] showed that ML leads to a solution that is equivalent to PCA up to a rotation in the projection space. The columns of the ML estimate of W span the same subspace as the d principal eigenvectors of the sample covariance matrix and the ML estimate of the noise variance t\u00031 is equal to the average lost variance (eigenvalues) in the discarded directions. As discussed in Appendix A, the rotational ambiguity can be ignored in the context of mixture modelling, unless we are explicitly interested in local principal directions. However, PPCA (as well as its non-probabilistic counter- part) suffers from the fact that it is based on Gaussian noise model. As a result, it is very sensitive to atypical observations such as outliers, and more generally, to situations where the data are not well conﬁned on the low-dimensional clusters. Unfortunately, such cases occur quite often in practice which motivates the approach proposed in the following. 2.1. Latent variable view of the Student-t distribution Compared to the Gaussian density, the Student-t density has an additional parameter, called the number of degrees of freedom n. It regulates the thickness of the distribution tails and therefore reduces the sensitivity to atypical observations. In this work, we do not restrict n to be an integer value. As noted in [15], the ML estimates of the parameters of the Student-t density can be computed by an EM algorithm by viewing the density as the following latent variable model: Sðyjl; K; nÞ¼ Z 1 0 Nðyjl; uKÞG u n 2 ; n 2 \u0002 \u0002 \u0002 \u0003\u0004 du ¼hNðyjl; uKÞiujn; n40, (2) where h\u0004iu denotes the expectation with respect to the latent (or unobserved) scale variable u, over which we marginalize and on which a gamma3 prior is imposed. Hence, the Student-t density can be reformulated as an inﬁnite mixture ARTICLE IN PRESS 2The multivariate Gaussian density with mean l and inverse covariance matrix (or precision) K is deﬁned as Nðyjl; KÞ/ jKj1=2 e \u0003ð1=2Þðy\u0003lÞ>Kðy\u0003lÞ. 3The gamma density is deﬁned as Gðuja; bÞ/ u a\u00031e \u0003bu with a40 and b40. C. Archambeau et al. / Neurocomputing 71 (2008) 1274–1282 1275 of Gaussian densities with the same mean and where the prior on u is a gamma density with parameters depending only on n. Examples of zero mean univariate Student-t densities with unit variance and the corresponding gamma prior are shown in Fig. 1. 2.2. Robust reformulation of PPCA As shown in [2], PPCA can be made robust by using a Student-t model for the prior and the likelihood instead of a Gaussian one: pðxnÞ¼hNðxnj0; unIdÞiunjn ¼ Sðxnj0; Id; nÞ, (3) pðynjxnÞ¼hNðyjWxn þ l; untIDÞiunjn ¼ SðynjWxn þ l; tID; nÞ, (4) where t is the inverse residual variance, that is t\u00031 accounts for the variance not captured by the low-dimensional latent vectors. Note that the scaled (inverse) covariance is data dependent; a different scale variable un is assigned to each data point yn. Furthermore, the gamma prior on the scale variables is shared by the latent vectors and the observa- tions, such that the robustness in the latent space and the measurement space is determined by a single n. Thus, when a data point is considered to be an outlier in the high- dimensional space, it does not contribute to the identiﬁca- tion of the principal subspace as it is also considered to be an outlier in the projection space. 3. Mixtures of robust PPCAs A mixture of M robust probabilistic principal compo- nent analyzers is deﬁned as follows: pðyÞ¼ XM m¼1 pmpðyjhmÞ, (5) where fhmgM m¼1 is the set of component parameters and fpmgM m¼1 is the set of mixture proportions, with Pm pm ¼ 1 and pmX0 for all m. The marginal likelihood pðynjhmÞ associated to the observation yn is deﬁned as a single robust PPCA according to (3) and (4): pðynjhmÞ¼ Z þ1 0 Z þ1 \u00031 NðynjWmxnm þ lm; untmIDÞ \u0002Nðxnmj0; unIdÞG un nm 2 ; nm 2 \u0002 \u0002 \u0002 \u0003\u0004 dxnm dun ¼ Sðynjlm; Am; nmÞ, (6) where A \u00031 m \u0005 WmW> m þ t\u00031 m ID. A set of low-dimensional latent variables fxnmgN n¼1 and a set of latent scale variables funmgN n¼1 are associated to the mth robust PPCA model. For each observation yn, we also introduce the binary latent variable zn indicating by which component yn was generated. The resulting complete probabilistic model is deﬁned as follows: PðznÞ¼ Y m p znm m , (7) pðunjznÞ¼ Y m G unm nm 2 ; nm 2 \u0002 \u0002 \u0002 \u0003\u0004znm , (8) pðvnjun; znÞ¼ Y m Nðxnmj0; unmIdÞznm , (9) pðynjvn; un; znÞ¼ Y m NðynjWmxnm þ lm; unmtmIDÞznm, (10) where zn ¼ðzn1; ... ; znM Þ>, un ¼ðun1; ... ; unM Þ> and vn ¼ðxn1; ... ; xnM Þ>. This probabilistic model can be represented by the graphical model shown in Fig. 2. Latent variables, indicated by unshaded nodes, are integrated out. Mathematically, this leads to (5) as desired. 3.1. Training algorithm We seek ML estimates for the parameters h ¼fpm; hmgM m¼1, with hm \u0005flm; Wm; tm; nmg for all m. Unfortunately, the probabilistic formulation (7)–(10) of a mixture of robust principal component analyzers does not permit a direct maximization of the log-likelihood function ln L ¼P n ln pðynjhÞ as this quantity is intractable. Therefore, we adopt an EM approach [6], which ﬁnds ML parameters ARTICLE IN PRESS 01234-10 -5 0 5 10 5 0 0.5 1 1.5 2 0 0.1 0.2 0.3 0.4 Fig. 1. (a) Univariate Student-t density for n ¼ 10 (solid) and n ¼ 0:1 (dashed) and (b) the corresponding gamma prior on the latent scale variable. C. Archambeau et al. / Neurocomputing 71 (2008) 1274–12821276 estimates iteratively. First (E step), the posterior distribution of the latent variables is estimated for ﬁxed parameters. Second (M step), the following quantity is maximized with respect to the parameters (see Appendix B for the explicit form): X n hln pðyn; vn; un; znjhÞivn;un;zn. (11) The expectation is taken with respect to the joint posterior distribution of the latent variables, which is tractable (see Appendix C). The E step boils down to computing the expectations required in the M step: ¯rnm \u0005hznmi¼ pmSðynjlm; Am; nmÞ Pk pkSðynjlk; Ak; nkÞ , (12) ¯unm \u0005hunmi¼ D þ nm ðyn \u0003 lnÞ>Amðyn \u0003 lmÞþ nm , (13) ln ~unm \u0005hln unmi¼ c D þ nm 2 \u0005\u0006 \u0003 ln ðyn \u0003 lmÞ>Amðyn \u0003 lmÞþ nm 2 \u0005\u0006, (14) ¯xnm \u0005hxnmi¼ tmB \u00031 m W> mðyn \u0003 lmÞ, (15) ¯Snm \u0005hznmunmxnmx > nmi¼ ¯rnmB \u00031 m þ ¯onm ¯xnm ¯x> nm, (16) where ¯onm \u0005 ¯rnm ¯unm, Bm \u0005 tmW> mWm þ Id and cð\u0004Þ \u0005 G 0ð\u0004Þ=Gð\u0004Þ is the digamma function. Maximizing (11) leads then to the following M step for the parameters: pm 1 N X n ¯rnm, (17) lm P n ¯onmðyn \u0003 Wm ¯xnmÞ P n ¯onm , (18) Wm X n ¯onmðyn \u0003 lmÞ ¯x > nm ! X n ¯Snm !\u00031, (19) t\u00031 m X n ¯onm DNpm ðkyn \u0003 lmk2 \u0003 2ðyn \u0003 lmÞ>Wm ¯xnmÞ þ 1 DNpm X n trfWm ¯SnmW> mg, (20) for all m. The contribution of each data point is weighted according to ¯onm, which accounts for both the effect of the responsibilities ¯rnm and the expected latent scale variables ¯unm. The latter ensures robustness as its value is small for yn lying far from lm. For the parameters fnmgM m¼1 there is no closed form ML estimate. Nevertheless, an ML solution can be computed at each EM iteration by solving the following expression by a line search algorithm (see for example [18]): 1 þ ln nm 2 \u0003\u0004 \u0003 c nm 2 \u0003\u0004 þ 1 Npm X n ¯rnmfln ~unm \u0003 ¯unmg¼ 0, (21) for all m. Alternatively, a heuristic was proposed by Shoham [24] in the context of mixture modelling. This heuristic is also applicable here. Since a line search is computationally inexpensive, the computational complexity of each EM step is OðMNDdÞ. Hence, the overall complexity for mixtures of robust PPCAs is the same as for mixtures of ordinary PPCAs [27]. 3.2. Low-rank approximation of the component covariances Using a (latent) low-dimensional representation of the data has a clear advantage over a standard mixture of Gaussians (or Student-t’s) when considering a clustering problem or when estimating a density. Indeed, the number of parameters to estimate the covariance of each compo- nent is equal to Dd m þ 1 \u0003 dmðd m \u0003 1Þ=2 (where the last term takes the rotational invariance into account) in the case of robust PPCAs and it is equal to DðD þ 1Þ=2 in the case of a standard mixture. The interesting feature of our approach is that the correlations between the principal directions are not neglected since Wm contains the local dm principal directions in the data. By contrast, it is common practice to force the covariance matrices to be diagonal (and thus axis aligned) in order to avoid numerical instabilities. This heuristic is clearly suboptimal. 3.3. Mixtures of robust PFAs As mentioned earlier, PPCA is closely related to PFA [22]. If we assume in (10) that the covariance of the noise model is a diagonal matrix, we obtain a mixture of robust PFAs. The columns of the matrix Wm are called the local factor loadings, and the components pðynjhmÞ are now given by pðynjhmÞ¼ Sðynjlm; Am; nmÞ, (22) where A\u00031 m \u0005 WmW> m þ W\u00031 m . The diagonal matrix Wm contains the inverse variances (or inverse uniquenesses)of ARTICLE IN PRESS Fig. 2. Graphical model for the robust mixture of probabilistic principal component analyzers. A shaded node indicates that the random variable is observed, an arrow denotes a conditional dependency between the variables and a plate corresponds to a repetition. C. Archambeau et al. / Neurocomputing 71 (2008) 1274–1282 1277 the (local) factors and it corresponds to the precision of the conditional marginal SðynjWmxnm þ lm; Wm; nmÞ. The fac- tors are thus independent given the latent variables. The EM algorithm for ML training is discussed in Appendix D. 4. Experiments In this section, two types of experiments are considered. First, we illustrate how a low-dimensional nonlinear manifold spoiled by noisy data can still be recovered when using a robust approach. Second, the robust mixture modelling of high-dimensional data is demonstrated on two different data sets. 4.1. Robust reconstruction of low-dimensional manifolds The following three-dimensional data set is considered: y3n ¼ y2 1n þ y2 2n \u0003 1 þ \u0002n. (23) The data fyin : i 2f1; 2ggN n¼1 are drawn from a uniform distribution in the ½\u00031; 1\u0006 interval and the error terms f\u0002ngN n¼1 are distributed according to Nð\u0002nj0; t\u0002Þ, with t\u00031 \u0002 ¼ 0:01. The data are located along a two-dimensional paraboloid; 500 training data were generated. The number of mixture components was ﬁxed to 5 and d m was set to 2 (the true dimension of the manifold) for all m. Fig. 3 shows the results for a mixture of standard PPCAs and robust PPCAs in presence of 10% of outliers. These are drawn from a uniform distribution on the interval ½\u00031; 1\u0006 in each direction. The shaded surfaces at the bottom of each plot indicate the regions associated to each component (or local linear model) after projection onto this two-dimensional surface. Each shaded region corresponds to a different component. The regions (data) are assigned to the component with highest responsibility. When the local models are nicely aligned with the manifold, the two- dimensional regions do not split. However, as shown in Fig. 3, only the mixture of robust PPCAs provides a satisfactory solution. Indeed, one of the components of the mixture of standard PPCAs is ‘‘lost’’ as it is used to model the outliers (and thus crosses the paraboloid). 4.2. Analysis of high-dimensional clustering First, we consider a three-dimensional synthetic exam- ple. Next, we discuss results on the high-dimensional USPS handwritten digit database. 4 4.2.1. Toy example The data are grouped into three clusters (see Fig. 4). Each cluster corresponds to a three-dimensional Gaussian component with a diagonal covariance matrix equal to diagf5; 1; 0:2g before rotation around the second coordinate axis. The two outer clusters are arranged at an angle of \u000730\b with respect to the middle one, which is horizontally aligned, and are, respectively, shifted by \u00075 units along the axis of rotation. Hence, the data clusters lie essentially on a distorted two-dimensional plane. The training data consist of 300 data points, of which 100 are drawn from each Gaussian component, and 5% of outliers. These are drawn from the uniform distribution in the hypercube centered on 0 and of side length equal to 20. The validation data consist of 900 data points (300 per cluster). The performance measure that we use to assess the mixture models (the standard mixture of Gaussians, the mixture of PPCAs and the mixture of robust PPCAs) is the log-likelihood of the validation data. Table 1 shows the results for 30 models trained on different training sets for M ¼ 3 and 4. The dimension of the latent vectors is set to the same value for all components. The overall best generalization on unseen data is obtained for the mixture of three robust PPCAs, with d m ¼ 2 for all m. The average validation log-likelihood is the highest and the standard deviation the smallest. Mixtures of standard PPCAs always perform poorer than their ARTICLE IN PRESS Fig. 3. Noisy paraboloid data set (black dots). Each two-dimensional shaded region is associated with a different local linear model (or component). They represent the dominant component membership of the points lying above it. The outliers are indicated by squares. (a) Projection by mixture of PPCAs. (b) Projection by mixture of robust PPCAs. 4The USPS data were gathered at the Center of Excellence in Document Analysis and Recognition (CEDAR) at SUNY Buffalo during a project sponsored by the US Postal Service. C. Archambeau et al. / Neurocomputing 71 (2008) 1274–12821278 robust counterpart. However, the former favor a one- dimensional latent space when M ¼ 3, while the latter favors a two-dimensional one. Interestingly, unconstrained mixtures of Gaussians perform well when the number of components is equal to 4. The reason for this is that the 4th component accounts for the outliers. This was also observed in [19]. Still, mixtures of robust PPCAs perform better. In fact, the two outer components are approxi- mately Gaussian as n is in the range of 10 for both of them. The remaining two components are centered on the origin, one being approximately Gaussian (n \t 20) and the other being heavy tailed (n \t 1). By contrast, when the number of components is set to 3, the middle component is approximately Gaussian (n \t 10) and the two outer components are heavy tailed (n \t 2). Finally, it should be noted that the quality of the model provided by the mixtures of robust PPCAs is not affected by asymmetric noise. For example, when considering outliers only in the hypercube of side 10 and centered on ð2:5; 2:5; 2:5Þ >, we obtain an average validation log-like- lihood which is not signiﬁcantly different (\u00039:14 \u0002 103\u0007 0:24 \u0002 103). 4.2.2. USPS handwritten digit data In this experiment, the well-known USPS handwritten digit data are considered. The data are grayscale 16 \u0002 16- pixels images of digits (0–9). To simplify the illustration, we kept only the images of digit 2 and digit 3 (respectively, 731 and 658 images), as well as 100 (randomly chosen) images of digit 0. In this setting, these are outliers as they stand outside the two main clusters of digit 2 and 3. We compared the mixture of PPCAs and the mixture of robust PPCAs in their ability to ﬁnd the two main clusters assuming a one-dimensional latent space. The result is shown in Fig. 5. Each row represents images generated along the principal directions. The mixture of robust PPCAs completely ignores the outliers. The ﬁrst compo- nent concentrates on the digits 3 and the second on the digits 2. Interestingly, the model is able to discover that the main variability of digits 3 is along their width, while the main variability of digits 2 is along their height. On the other hand, the mixture of PPCAs is very sensitive to the outliers as its ﬁrst component makes the transition between digits 3 and outliers digits 0. This is undesirable in general as we prefer each component to stick to a single cluster. Of course, one could argue that three components would be a better choice in this case. However, we think that this example exploits a very common property of high-dimensional data, namely that the major mass of the density is conﬁned in a low-dimensional subspace (or clusters of them), but not entirely. This experiment shows that the mixture of robust PPCAs is able to model such noisy manifolds, which are common in practice. ARTICLE IN PRESS 30° -5 -5 -10 -10-10 10 0 0 0 5 5 10 10yn3 yn2 yn1 Fig. 4. Synthetic example for clustering with robust linear subspace models. (a) Vertical view of the mixture of Gaussians. (b) Example of a single training data set, with the outliers indicated by squares. Table 1 Average log-likelihood of the validation data and the corresponding standard deviation M ¼ 3 M ¼ 4 d ¼ 1 Mixt. of PPCAs \u000311.9672.01 \u000310.5971.53 Mixt. of robust PPCAs \u000310.2670.68 \u000310.4270.41 d ¼ 2 Mixt. of PPCAs \u000313.0271.61 \u000310.3271.62 Mixt. of robust PPCAs \u00039.3470.26 \u00039.5370.32 d ¼ 3 Mixt. of Gaussians \u000313.2771.86 \u000310.3371.88 All numbers need to be multiplied by 1000. The training/validation process is repeated 30 times with different data sets. Fig. 5. Mixture of two component PPCAs with one-dimensional latent space to cluster USPS digit 2 and 3, and outliers digit 0. Top: robust PPCA. Bottom: standard PPCA. C. Archambeau et al. / Neurocomputing 71 (2008) 1274–1282 1279 5. Conclusion PCA and factor analysis are elementary and funda- mental tools for exploratory data mining and data visualization. When tackling real-life problems, such as digit recognition, it is essential to take a robust approach. Here, the term ‘‘robust’’ is used to indicate that the performance of the methods is not spoiled by non- Gaussian noise (e.g., outliers). This property is obtained by exploiting the adaptive distribution tails of the Student- t. In this paper, mixtures of robust probabilistic principal component/factor analyzers were introduced. They provide a practical approach for discovering nonlinear relation- ships in the data by combining robust local linear models. More generally, they are suitable for robust clustering, while performing dimensionality reduction at the same time, and for the visualization of noisy high-dimensional data. Appendix A. On the rotational ambiguity of the projection matrix The ML estimate of the PPCA projection matrix has the following form [26]: WML ¼ UdðPd \u0003 t\u00031IdÞ1=2R, (A.1) where the columns of Ud 2 RD\u0002d are the eigenvectors of the sample covariance matrix corresponding to the d largest eigenvectors, Pd 2 Rd\u0002d is the diagonal matrix of these eigenvectors and R 2 Rd\u0002d is an orthogonal matrix. From (A.1) it is clear that WML 2 RD\u0002d spans the same subspace as PCA and that the (scaled) principal directions are found up to a rotation R. As noted in [26], this rotational ambiguity can easily be removed by a post- processing step. However, this step can be ignored in the context of mixture modelling since the mixture components are the marginals fpðyjhmÞgM m¼1. These densities depend on the inverse covariance matrices fAmgM m¼1 (see (6)), which are independent of fRmgM m¼1 since RmR> m ¼ Id for all m. Appendix B. Variational lower bound to the log-likelihood When computing ML estimates of the parameters by the EM algorithm, the log-likelihood is maximized iteratively by maximizing a lower bound, which is the variational negative free energy [17]: \u0003Fðq; hÞ¼ X n hln pðyn; vn; un; znjhÞi þ H½q\u0006, (B.1) with ln L ¼\u0003Fðq; hÞþ KL½qkp\u0006X \u0003 Fðq; yÞ. (B.2) The variational distribution q approximates the posterior of the latent variables given the parameters h. At each iteration, the bound decreases monotonically, which provides a sanity check for the training algorithm. For model (7)–(10) and the exact posteriors (see Appendix C), the negative free energy is given by \u0003Fðq; hÞ¼ X n X m ¯rnm ln pm þ nm 2 ln nm 2 n \u0003 ln G nm 2 \u0003\u0004 þ nm 2 \u0003 1 \u0003\u0004 ln ~unmo \u0003 X n X m ¯onmnm 2 \u0003 NMD 2 ln 2p þ X n X m ¯rnmD 2 ln ~unm \u0003 X n X m 1 2 trf ¯Snmg þ X n X m ¯rnmD 2 ln tm \u0003 X n X m ¯onmtm 2 kyn \u0003 lmk2 þ X n X m ¯onmtmðyn \u0003 lmÞ>Wm ¯xnm \u0003 X n X m tm trfWm ¯SnmW> mg \u0003 X n X m ¯rnmfln ¯rnm þ am ln bnm \u0003 ln GðamÞ þðam \u0003 1Þ ln ~unmgþ X n X m ¯onmbnm þ X m Nd m 2 \u0003 X n X m ¯rnm 2 ln jBmj, (B.3) where am \u0005ðD þ nmÞ=2, bnm \u0005ððyn \u0003 lmÞ>Amðyn \u0003 lmÞþ nmÞ=2 and ¯onm ¼ ¯rnm ¯unm. The special quantities ¯rnm, ¯unm, ~unm, ¯xnm and ¯Snm are, respectively, deﬁned in (12)–(16). Appendix C. Posterior distributions of the latent variables The posterior distributions of the latent variables are computed by applying the Bayes rule. Here, they are all tractable. The posterior probabilities of the indicator variables are given by Pðznm ¼ 1jynÞ¼ pmSðynjlm; Am; nmÞ Pn pmSðynjlm; Am; nmÞ , (C.1) for all n and m. This quantity is called the responsibility.It is the posterior probability that the observation yn was generated by the component m. The gamma distribution is conjugate to the Gaussian distribution. Therefore, the posteriors of the scale variables are again gamma distributions: pðunmjyn; znm ¼ 1Þ / Nðynjlm; unmAmÞG unm nm 2 ; nm 2 \u0002 \u0002 \u0002 \u0003\u0004 ¼ G unm D þ nm 2 ; ðyn \u0003 lmÞ>Amðyn \u0003 lmÞþ nm 2 \u0002 \u0002 \u0002 \u0002 \u0005\u0006, (C.2) for all n and m. ARTICLE IN PRESS C. Archambeau et al. / Neurocomputing 71 (2008) 1274–12821280 The posterior distributions of the low-dimensional latent vectors are given by pðxnmjyn; unm; znm ¼ 1Þ / NðynjWmxnm þ lm; unmtmIDÞNðxnmj0; unmIdÞ ¼ NðxnmjtmB \u00031 m W> mðyn \u0003 lmÞ; unmBmÞ, (C.3) for all n and all m. The inverse covariance is deﬁned as Bm ¼ tmW> mWm þ Id. Finally, the joint posterior of the latent variables is given by Y n pðvn; un; znjynÞ¼ Y n Y m pðxnmjunm; znm; ynÞ \u0002pðunmjznm; ynÞpðznmjynÞ. (C.4) Appendix D. EM algorithm for robust mixtures of factor analyzers ML estimates for the parameters of mixtures of PFAs can be computed by the EM algorithm. For the E step, (12)–(14), (16) still hold, but the updates for xnm and Bm are given by ¯xnm ¼ B \u00031 m W> mWmðyn \u0003 lmÞ, (D.1) Bm ¼ W> mWmWm þ Id, (D.2) for all n and m. The M step is identical to robust PPCAs, except for the update of the diagonal precisions (inverse uniquenesses): W\u00031 m diag 1 Npm X n ð ¯onmðyn \u0003 lmÞðyn \u0003 lmÞ > ( \u0003Wm ¯SnmW> mÞ ) , (D.3) where diagf\u0004g sets all the off-diagonal elements to zero. References [1] C. Archambeau, Probabilistic models in noisy environments—and their application to a visual prosthesis for the blind, Ph.D. Thesis, Universite´ catholique de Louvain, Louvain-la-Neuve, Belgium, 2005. [2] C. Archambeau, N. Delannay, M. Verleysen, Robust probabilistic projections, in: W.W. Cohen, A. Moore (Eds.), 23rd International Conference on Machine Learning (ICML), ACM, New York, 2006, pp. 33–40. [3] C. Archambeau, N. Delannay, M. Verleysen, Mixtures of robust probabilistic principal component analyzers, in: 15th European Symposium on Artiﬁcial Neural Networks (ESANN), 2007, pp. 229–234. [4] C.M. Bishop, Pattern Recognition and Machine Learning, Springer, New York, 2006. [5] T. Cox, M. Cox, Multidimensional Scaling, Chapman & Hall, London, 2001. [6] A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete data via EM algorithm, J. R. Stat. Soc. B 39 (1) (1977) 1–38. [7] B. Efron, R.J. Tibshirani, An Introduction to the Bootstrap, Chapman & Hall, London, 1993. [8] B.S. Everitt, An Introduction to Latent Variable Models, Chapman & Hall, London, 1983. [9] R.A. Fisher, Applications of ‘‘Student’s’’ distribution, Metron 5 (1925) 90–104. [10] Z. Ghahramani, G.E. Hinton, The EM algorithm for mixtures of factor analyzers, Technical Report CRG-TR-96-1, Department of Computer Science, University of Toronto, 1996. [11] G.E. Hinton, P. Dayan, M. Revow, Modeling the manifolds of images of handwritten digits, IEEE Trans. Neural Networks 8 (1997) 65–74. [12] H. Hotelling, Analysis of a complex of statistical variables into principal components, J. Educ. Psychol. 24 (1933) 417–441. [13] K. Huang, Y. Ma, R. Vidal, Minimum effective dimension for mixtures of subspaces: a robust GPCA algorithm and its applications, in: 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol. 2, 2004, pp. 631–638. [14] I.T. Jolliffe, Principal Component Analysis, Springer, New York, 1986. [15] C. Liu, D.B. Rubin, ML estimation of the t distribution using EM and its extensions, ECM and ECME, Stat. Sinica 5 (1995) 19–39. [16] T.P. Minka, Automatic choice of dimensionality for PCA, in: T.K. Leen, T.G. Dietterich, V. Tresp (Eds.), Advances in Neural Information Processing Systems (NIPS), vol. 13, MIT Press, Cam- bridge, MA, 2001, pp. 598–604. [17] R.M. Neal, G.E. Hinton, A view of the EM algorithm that justiﬁes incremental, sparse, and other variants, in: M.I. Jordan (Ed.), Learning in Graphical Models, MIT Press, Cambridge, MA, 1998, pp. 355–368. [18] J. Nocedal, S.J. Wright, Numerical Optimization, Springer, Berlin, 2000. [19] D. Peel, G.J. McLachlan, Robust mixture modelling using the t distribution, Stat. Comput. 10 (2000) 339–348. [20] S.T. Roweis, EM algorithms for PCA and SPCA, in: M.I. Jordan, M.J. Kearns, S.A. Solla (Eds.), Advances in Neural Information Processing Systems (NIPS), vol. 10, MIT Press, Cambridge, MA, 1998. [21] S.T. Roweis, L.K. Saul, Nonlinear dimensionality reduction by locally linear embedding, Science 290 (2000) 2323–2326. [22] D.B. Rubin, D.T. Thayer, EM algorithms for ML factor analysis, Psychometrika 47 (1) (1982) 69–76. [23] J.W. Sammon Jr., A nonlinear mapping for data structure analysis, IEEE Trans. Comput. 18 (1969) 401–409. [24] S. Shoham, Robust clustering by deterministic agglomeration EM of mixtures of multivariate t-distributions, Pattern Recognition 35 (5) (2002) 1127–1142. [25] J.B. Tenenbaum, V. de Silva, J.C. Langford, A global geometric framework for nonlinear dimensionality reduction, Science 290 (2000) 2319–2323. [26] M.E. Tipping, C.M. Bishop, Probabilistic principal component analysis, J. R. Stat. Soc. B 61 (1999) 611–622. [27] M.E. Tipping, C.M. Bishop, Mixtures of probabilistic principal component analyzers, Neural Comput. 11 (2) (1999) 443–482. [28] L. Xu, A.L. Yuille, Robust principal component analysis by self- organizing rules based on statistical physics approach, IEEE Trans. Neural Networks 6 (1) (1995) 131–143. Ce´ dric Archambeau received the Electrical En- gineering degree and the Ph.D. in Applied Sciences from the Universite´ catholique de Louvain, respectively, in 2001 and 2005. Until mid-2005, he was involved in an European biomedical research project, which aimed at developing a visual prosthesis for blind people. Since April 2006, he is a research associate at the University College London in the Centre for Computational Statistics and Machine Learning. His current research interests include approximate Bayesian inference, stochastic processes and dynamical systems, as well as clustering, classiﬁcation and regression in very noisy environments. ARTICLE IN PRESS C. Archambeau et al. / Neurocomputing 71 (2008) 1274–1282 1281 Nicolas Delannay was born in 1980, Ottignies, Belgium. He graduated Master of electromecha- nical engineering in 2003 from Universite´ cath- olique de Louvain (UCL), Belgium. He then received a research fellow grant from the FRS- FNRS to work on a Ph.D. thesis. The research took place from October 2003 to October 2007 within the Machine Learning Group at UCL (www.ucl.ac.be/mlg). The main topics covered by his thesis are: machine learning, data mining, Bayesian modelling, regression and collaborative ﬁltering. Michel Verleysen was born in 1965 in Belgium. He received the M.S. and Ph.D. degrees in electrical engineering from the Universite´ cath- olique de Louvain (Belgium) in 1987 and 1992, respectively. He was an invited professor at the Swiss E.P.F.L. (Ecole Polytechnique Fe´ de´ rale de Lausanne, Switzerland) in 1992, at the Universite´ d’Evry Val d’Essonne (France) in 2001, and at the Universite´ ParisI-Panthe´ on- Sorbonne from 2002 to 2007, respectively. He is now a professor at the Universite´ catholique de Louvain, and Honorary Research Director of the Belgian F.N.R.S. (National Fund for Scientiﬁc Research). He is editor-in-chief of the Neural Processing Letters journal, chairman of the annual ESANN conference (European Symposium on Artiﬁcial Neural Networks), associate editor of the IEEE Trans. on Neural Networks journal, and member of the editorial board and program committee of several journals and conferences on neural networks and learning. He is author or co-author of more than 200 scientiﬁc papers in international journals and books or communications to conferences with reviewing committee. He is the co-author of the scientiﬁc popularization book on artiﬁcial neural networks in the series ‘‘Que Sais-Je?,’’ in French, and of the ‘‘Nonlinear Dimensionality Reduction’’ book published by Springer in 2007. His research interests include machine learning, artiﬁcial neural networks, self-organization, time-series forecasting, nonlinear statistics, adaptive signal processing, and high-dimensional data analysis. ARTICLE IN PRESS C. Archambeau et al. / Neurocomputing 71 (2008) 1274–12821282","libVersion":"0.3.2","langs":""}
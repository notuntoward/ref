{"path":"lit/lit_sources/Valdes22RepresentationalGradientBoosting.pdf","text":"Representational Gradient Boosting: Backpropagation in the Space of Functions Gilmer Valdes , Jerome H. Friedman, Fei Jiang, and Efstathios D. Gennatas Abstract—The estimation of nested functions (i.e., functions of functions) is one of the central reasons for the success and popularity of machine learning. Today, artiﬁcial neural networks are the predominant class of algorithms in this area, known as representational learning. Here, we introduce Representational Gradient Boosting (RGB), a nonparametric algorithm that estimates functions with multi- layer architectures obtained using backpropagation in the space of functions. RGB does not need to assume a functional form in the nodes or output (e.g., linear models or rectiﬁed linear units), but rather estimates these transformations. RGB can be seen as an optimized stacking procedure where a meta algorithm learns how to combine different classes of functions (e.g., Neural Networks (NN) and Gradient Boosting (GB)), while building and optimizing them jointly in an attempt to compensate each other’s weaknesses. This highlights a stark difference with current approaches to meta-learning that combine models only after they have been built independently. We showed that providing optimized stacking is one of the main advantages of RGB over current approaches. Additionally, due to the nested nature of RGB we also showed how it improves over GB in problems that have several high-order interactions. Finally, we investigate both theoretically and in practice the problem of recovering nested functions and the value of prior knowledge. Index Terms—Gradient boosting, representational learning, non parametric models, neural networks Ç 1INTRODUCTION D ATA sets often consist of mixed and complex features. A single algorithm or type of function is unlikely to be the best at analyzing all of them. For instance, a medical data set may consist of MRI or CT (images), physician notes (text), lab- oratory test results (tabular data), genetic information (tabular data) and physiological recordings (time series). To model the outcome of interest, we may choose the most appropriate algorithm for each individual type of feature group (e.g., Con- volutional Neural Networks, CNN, for images and Gradient Boosting, GB, for tabular data), and combine them in a way that can best predict outcomes. This modeling strategy is known as meta learning [1], [2], [3], which usually takes place in two steps. In the ﬁrst step, a separate model is ﬁtted to each group of features, without considering the information in the other features. We call these base models. In the second step, the estimates from the base models are combined, usually lin- early, to predict the outcome. This two-step strategy often results in suboptimal parame- ter estimation. If one ﬁts the individual models separately, when combining them in the meta learning step we ignore the relationships among different categories of predictors (i.e., images and tabular data), and hence certain types of informa- tion losses are unavoidable. This is to say, the individual mod- els are ﬁtted without taking into consideration that they will be later combined. Alternatively, each base model can be trained on all variables but using the same algorithm (i.e., NN). Perfor- mance will also suffer. This is due to the fact that only one class of function (potentially sub optimal) is selected to analyze all the different groups of variables (i.e., NN for tabular data or GB for images). To address the above issues, we propose Representa- tional Gradient Boosting, a novel representational learning algorithm based on gradient boosting (Fig. 1). Instead of combining base models’ outputs at the end, RGB uses back- propagation to train them in parallel so that they may com- pensate each other’s weaknesses and/or share information. Similar to Neural Networks, RGB estimates a function com- prised of many different functions stacked in a multi-layer architecture (i.e., a set of nested functions) which allows the incorporation of prior knowledge previously designed for each individual algorithm or type of data. Unlike NNs, RGB does not assume a functional form in the nodes or output (e.g., linear models and ReLu) but esti- mates these transformations. Furthermore, RGB allows the combination of differentiable (e.g., NNs) and non-differen- tiable functions (e.g., GB) bringing together the most popu- lar state-of-the-art algorithms today. \u0001 Gilmer Valdes is with the Department of Radiation Oncology and Depart- ment of Epidemiology and Biostatistics, University of California, San Francisco, CA 94143 USA. E-mail: gilmer.valdes@ucsf.edu. \u0001 Jerome H. Friedman is with the Department of Statistics, Stanford Univer- sity, Stanford, CA 94305 USA. E-mail: jhf@stanford.edu. \u0001 Fei Jiang is with the Department of Epidemiology and Biostatistics, Uni- versity of California, San Francisco, CA 94143 USA. E-mail: fei. jiang@ucsf.edu. \u0001 Efstathios D. Gennatas is with the Department of Radiation Oncology, Stanford University, Stanford, CA 94305 USA, and also with the Depart- ment of Epidemiology and Biostatistics, University of California, San Francisco, CA 94143 USA. E-mail: gennatas@stanford.edu. Manuscript received 22 November 2020; revised 29 November 2021; accepted 5 December 2021. Date of publication 23 December 2021; date of current version 3 November 2022. This work was supported in part by the National Institute of Biomedical Imag- ing and Bioengineering of the National Institutes of Health under Grant K08EB026500. The content is solely the responsibility of the authors and does not necessarily represent the ofﬁcial views of the National Institutes of Health. (Corresponding author: Gilmer Valdes.) Recommended for acceptance by T. Zhang. Digital Object Identiﬁer no. 10.1109/TPAMI.2021.3137715 10186 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 12, DECEMBER 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ RGB can also be seen as a nonparametric NN where increases in complexity do not require to change the archi- tecture (i.e., wider or deeper networks) but are achieved with each iteration. Borrowing from this connection with NN and GB, RGB can also be thought of as gradient descent in the space of functions for a multilayer architecture. Backpropagation in the space of functions had been already suggested by pioneering work from G. Mani [4] but since the key idea of Gradient Boosting (ﬁtting the gradient with base learners) had not been explored at the time, ad-hoc or inefﬁcient compromises were made. Other algorithms are also closely related to RGB and highlighting them provide useful insights. For instance, RGB can also be thought as a generalization of Projection Pursuit (closely related to NN) [5], [6]. In Projection Pursuit, similar to vanilla Neural Networks, linear transforma- tions of the input space are performed, followed by nonlinear smoothers. In RGB, nonlinear general transformations and output function are estimated using nonparametric models. In the present article we performed experiments to gain intuition on the inner workings of RGB and the classes of problems best suited for it. Speciﬁcally, here we investigate: 1) RGB formalism to ﬁt functions. 2) Would RGB improve over combination of models after they have being built (optimized stacking)? 3) Understand the type of problems best suited for RGB. 4) How much and what type of prior knowledge can help improve performance? 5) Under which conditions can/if nested functions be recovered? 2RELATED WORK Machine Learning (ML) algorithms can be used to analyze both structured, a.k.a. tabular, and unstructured data, like images, or text. As a tool to analyze unstructured data, rep- resentational learning has become extremely popular in recent years [7]. This can be mainly attributed to the success that NNs have achieved in pattern recognition problems like image classiﬁcation or natural language processing [8], [9], [10]. In general, it is thought that the tasks above are well characterized by a hierarchy where simpler, more general concepts get combined to deﬁne less abstract ones. Their multi-layer architecture allows NNs to match these type of functions well. Besides their architecture, NNs can incorpo- rate additional prior knowledge that is also thought to help them perform well on these tasks. In CNN, kernels are swept across images (convolution) to learn general concepts like lines or circles in the ﬁrst layers which are later combined to form high order features [11]. Additionally, pooling layers are included to provide the network with a degree of invariance to feature transla- tion (the output will be the same regardless of the speciﬁc position of a feature within the pooling layer). Therefore, both convolution and pooling are essential in explaining the performance of CNN. To further this point, Alain and Ben- gio [12] showed that two layers of convolution and pooling, even with random weight initialization (no training), was enough to take the test prediction error of logistic regression from roughly 8 to 2 percent on the MNIST data set. In the analysis of tabular data other algorithms like GB excel [13], [14]. There are many reasons for this. GB com- monly uses trees as base learners and therefore inherits many of their advantages while mitigating their disadvan- tages. Trees are consistent estimators, invariant to mono- tonic transformations of the input variables, can handle categorical variables and missing values very efﬁciently, and perform intrinsic feature selection; characteristics that NN do not possess. The position of features respect to other features is not important in tabular data and prior knowledge that exploit locality (i.e., convolution) does not help. Additionally, GB is very easy to regularize with three main tunable parameters (shrinkage, depth of the trees and number of trees) improving over the greedy nature of single decision trees. In fact, it has been shown that CART is the greediest version of gradient boosting with stumps [15], [16]. Motivated by the success of GB in the analysis of tabular data Feng et al. used the differential target backpropagation algorithm [17], [18] to create a NN with GB models at its nodes [19]. Unfortunately, target back-propagation requires the calculation of an inverse function in an ill posed prob- lem and as such can only be applied to simple architectures. RGB provides GB with the modularity typical of an NN while providing the latter with the advantages of including any type of functions in its node, including tree-based layers. It expands the backpropagation algorithm to the space of functions with important theoretical and practical applications. It does not requiere the calculation of an inversed function as such it can be applied effectively to dif- ferent architectures. 3GENERAL RGB DESCRIPTION Let xxi 2X be a p dimensional feature vector, and yi 2 yy be its corresponding outcome. Furthermore, we assume ðxxi;yiÞ for i ¼ 1; ... ;n are independent identically distributed ran- dom samples draw from an unknown distribution D.In a two-class classiﬁcation setting, Y¼ \u00031ðÞ, and in regression, Y¼ R. The goal is to learn a function from X to Y that will perform well in predicting the outcome on new examples drawn from D. Let us also assume that there are reasons to believe that the function is a multi-layer function where the Fig. 1. Vanilla Representational Gradient Boosting (one hidden layer). p features, k base functions and u updates. VALDES ET AL.: REPRESENTATIONAL GRADIENT BOOSTING: BACKPROPAGATION IN THE SPACE OF FUNCTIONS 10187 initial input xx is transformed by functions FkðxxÞ, k ¼ f1; ... ;Kg and K 2 N, which serve as input for the second layer function Oð\u0004Þ as OF1ðxxÞ;F2ðxxÞ; ... ;FKðxxÞðÞ. In the present article, we will limit ourselves to the investigation of the function with only one hidden layer, Fig. 1. This nested function is generally not identiﬁable. We say that a function is identiﬁable if OF1ðxxÞ;F2ðxxÞ; .. . ;FKðxxÞðÞ ¼ PG1ðxxÞ;G2ðxxÞ; ... ;GKðxxÞðÞ implies that O ¼ P; F1 ¼ G1;F2 ¼ G2; .. . ;Fk ¼ Gk for any given xx. Hence,wemustimposecertain typesofidentiﬁ- ability conditions to perform parameter estimation and inference. We discuss several identiﬁablitly conditions in Appendix, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/ 10.1109/TPAMI.2021.3137715. However, when our goal is prediction, identifying individual parameters is not neces- sary. Therefore, let FF ðxxÞ¼ F1ðxxÞ; .. . ;FKðxxÞ½\u0005 >.We obtain the parameter estimators through minimizing the loss function JðO; FF Þ JðO; FF Þ¼ XN i¼1 Lyi;O FF ðxxiÞðÞðÞ: We estimate O and FF iteratively as described below. Here we use superscript t to indicate the parameter estima- tor and functions at iteration t. First, consider FF t and Ot are given and we perform a gra- dient descent step to update Otþ1 using regular Gradient Boosting [13] Otþ1 ¼ Ot þ r tDOtðFF tÞ; where DOtðFF tÞ is the estimation of the direction of the loss function gradient with respect to O DOt ¼ argminDO XN i¼1 \u0006 @Lyi;Ot FF tðxxiÞ \u0002\u0003\u0002\u0003 @Ot FF tðxxiÞ \u0002\u0003 \u0006 DOFF tðxxiÞ \u0002\u0003 !2; (1) and r t is the step size obtained as rt ¼ argminr XN i¼1 Lyi;O t FF tðxxiÞ \u0002\u0003 þ rDOt FF tðxxiÞ \u0002\u0003\u0002\u0003: (2) Now, once we obtain Otþ1, we can update FF t by propa- gating the gradient back and performing updates in each of the components (functions) as indicated below. This step becomes backpropagation in the space of functions F tþ1 k ¼ F t k þ r t F Df t k (3) Df t k ¼ argminDfk XN i¼1 \u0006 @Lyi;Otþ1 FF tðxxiÞ \u0002\u0003\u0002\u0003 @Otþ1 FF tðxxiÞ \u0002\u0003 \u0007 @Otþ1 FF tðxxiÞ \u0002\u0003 @FF tðxxiÞ \u0006 DfkðxxiÞ\u00042 (4) rt F ¼ argminrF XN i¼1 Lyi;O tþ1 FF tðxxiÞþ rF Df tðxxiÞ \u0002\u0003\u0002\u0003 ; (5) where Df tðxxiÞ¼ Df t 1ðxxiÞ; ... ; Df t KðxxiÞ \u0005\u0006>. Normally, (5) does not have a closed solution but we can approximate r t F using a single Newton-Raphson step given by \u0006 @2 PN i¼1 Lyi;Otþ1 FF tðxxiÞþ rF Df tðxxiÞ \u0002\u0003\u0002\u0003 @r2 F \u0007 \u0007 \u0007 \u0007rF ¼0 !\u00061 \u0007 @ PN i¼1 Lyi;Otþ1 FF tðxxiÞþ rF Df tðxxiÞ \u0002\u0003\u0002\u0003 @rF \u0007 \u0007 \u0007 \u0007rF ¼0: (6) Finally, these updates of O and FF can be repeated until the loss function reaches a certain threshold. Algorithm 1 below contains the pseudo code for RGB. 3.1 Examples of RGB Speciﬁc Implementations In this section, we give examples of speciﬁc RGB implemen- tations. In order to derive all RGB equations, we need to choose 1) the base learners, 2) the output and 3) the loss function. Many different functions can be used for the out- put or output updates DO as long as they are differentiable in order to allow calculation of @OFF ðxxiÞðÞ=@FkðxxiÞ. Base learners in the ﬁrst hidden layer do not have to be differen- tiable. We will focus on regression tasks in this paper for the derivation of the equations but by no means is RGB limited to regression and we offer a classiﬁcation example without including the derivation of the equations in the paper. For regression tasks, a common loss function is the squared error loss Ly; O FF ðxxÞðÞðÞ ¼ y \u0006 OFF ðxxÞðÞðÞ 2=2: The squared error loss has the advantage that the step size, r, for the output updates does not need to be calculated and as such it will be omitted below. Here, we investigate three different versions of RGB. First, in order to explore the RGB concept we choose linear models and trees as base learners and an output function with a convex solution. We call this implementation QuadRGB. Second, in order to compare RGB against stacking, we implement a version that has linear models in the output and a diverse set of tree-based models and linear models in the bases. We call this version sRGB. Finally, in order to explore whether using tree bases in the ﬁrst layer can improve ANN perfor- mance, we explore an implementation that boosts small neural networks in the output and has tree-based learners in the hidden layer. We call this version Neural RGB (nRGB) and it is our most general implementation as both the output and the bases are fully nonparametric models. a) QuadRGB. In this case, we choose a simple output function that includes all the bases and pairwise interac- tions OFF ðxxÞðÞ ¼ c0 þ XK k¼1 ckFkðxxÞþ XK\u00061 k¼1 XK j¼2;j > k ckjFkðxxÞFjðxxÞ: In the output step, for known FF ð\u0004Þ, ﬁnding the coefﬁcients is equivalent to solving a linear problem with closed solution and we do not need to apply the boosting steps. However, we saw that this strategy would make RGB converge too fast and place too much weight on the output step. To main- tain more control, we therefore also boost the linear model in order to provide regularization of the output. 10188 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 12, DECEMBER 2022 b) sRGB. For a fair comparison with stacking we chose a linear output model OFF ðxxiÞðÞ ¼ c0 þ XK k¼1 ckFkðxxiÞ: As in the case of QuadRGB, for known FF ðxxÞ, ﬁnding the coefﬁcients ck;k ¼ O; ... ;K is equivalent to solving a linear problem with a closed solution, although we also use boost- ing to approach the solution gradually. Algorithm 1. Representational Gradient Boosting. (RGB) Input: \u0001 yi;xiðÞ N 1 : data. \u0001 t: number of iterations \u0001 go: shrinkage parameter used to regularize the output \u0001 gk 8 k 2 1; ... ;KðÞ: shrinkage parameter used to reg- ularize each channel k Output: OðF1ðxxÞ;F2ðxxÞ; .. . ;FKðxxÞÞ -Initialize O1 F 1 1 ðxxÞ;F 1 2 ðxxÞ; ... ;F 1 KðxxÞ \u0002\u0003 for t = 1:T do -Calculate \u0006 @L @OtðF tðxxiÞF tðxxiÞÞ -Solve: DO t ¼ argminDO PN i¼1 \u0006 @Lyi;Ot FF tðxxiÞðÞðÞ @Ot FF tðxxiÞðÞ \u0006 DOFF tðxxiÞ \u0002\u0003 \b\u00042 -Calculate: rt ¼ argminr PN i¼1 Lyi;Ot FF tðxxiÞ \u0002\u0003\u0002\u0003 þ rDO t FF tðxxiÞ \u0002\u0003 -Update the output: O tþ1 ¼ O t þ gor tDO tðFF tÞ -Calculate \u0006 @Lyi;Otþ1 FF tðxxiÞðÞðÞ @Otþ1 FF tðxxiÞðÞ @Otþ1 FF tðxxiÞðÞ @FF tðxxiÞ -8 k 2 1; .. . ;KðÞ Estimate: Df t k ¼ argminDfk XN i¼1 \u0006 @Lyi;Otþ1 FF tðxxiÞ \u0002\u0003\u0002\u0003 @Otþ1 FF t kðxxiÞ \u0002\u0003 \u0007 @O tþ1 FF tðxxiÞ \u0002\u0003 @F t kðxxiÞ \u0006 DfkðxxiÞ\u00042 -Solve: rt F ¼ argminrF PN i¼1 Lyi;O tþ1 FF tðxxiÞþ rF Df tðxxiÞ \u0002\u0003\u0002\u0003 - 8 k 2 1; ... ;KðÞ update: F tþ1 k ¼ F t k þ gkrt F Df t k end return OT F T 1 ðxxÞ;F T 2 ðxxÞ; .. . ;F T KðxxÞ \u0002\u0003 c) nRGB. In the case that we have a summation of ReLu functions, the ﬁnal output takes the form Ot FF ð\u0004ÞðÞ ¼ C0 þ Xt u¼1 go½W u o ReLu B u h þ WW u> h FF ð\u0004Þ \t þ B u o \u0005; and the updates in the output function are given by DOt FF ð\u0004ÞðÞ ¼ W t oRELUðB t h þ WW t> h FF ð\u0004ÞÞ þ B t o; (7) where W t o;B t h;Bt o are constants and WW t h 2 R K. The ﬁrst and second order derivatives of the QuadRGB, sRGB and nRGB are given in Appendix, available in the online supplemental material 4EXPERIMENTS In order to study RGB behaviour and properties, we ﬁrst performed simulated studies with known data generating functions. We created a nested function deﬁned as OðxÞ¼ F1ðxÞþ F2ðxÞþ F3ðxÞþ F1ðxÞF2ðxÞ þ F1ðxÞF3ðxÞþ F2ðxÞF3ðxÞþ \u0002 (8) where F1ðxÞ¼ 10sinðpx1x2Þþ 20ðx3 \u0006 0:5Þ 2 þ 10x4 þ 5x5; F2ðxÞ¼ 0:1expð4x6Þþ 4 1 þ exp \u000620ðx7 \u0006 0:5ÞðÞ þ 3x8 þ 2x9 þ x10; F3ðxÞ¼ 10sinðpx11Þ cos ðpx12Þþ 10ðx13Þ 2 þ x14x15; (9) and xx is a vector of at least 15 components being all xk drawn from the unit hyper cube and \u0002 is a normally dis- tributed variable with mean equal 0 and variance as speciﬁed on each experiment. These bases functions are similar to those used in the widely tested Friedman data set [20] while the output is a simple quadratic function of them. 4.1 Practical Aspects of RGB Implementation We use this section to discuss important aspects of the implementation of RGB: 1) initialization, 2) regularization via shrinkage and 3) step size during gradient descent r. 1) Initialization: Unlike GB, RGB can not be initialized with constants in all the functions (bases and output) because the derivatives would be zero and learning would not be possible. Careful initialization is required to account for the nested function architec- ture of the model. Different strategies were tested. Normally, we initialized the bases by ﬁtting the out- come directly using random subsamples of the dis- tributions as if there was no output function. Our idea was that initial features that predicted the out- come were good as they would correlate with some aspects of the function being ﬁtted. We also observed that initializing the bases with a constant ﬁrst plus a base ﬁtted to the residual but with a shrinkage applied to it helped with regularization. This was a result of having subsequent steps anchored to the dimension of the constant (chosen to be proportional to the mean value of the output for the observations used to initialize each base). Once the bases were ini- tialized, we initialized the outcome using the same method described for Gradient Boosting [13]. 2) Shrinkage g: Using learning rate a.k.a. shrinkage is a standard technique to regularize GB. In RGB, both the output and base updates can be multiplied by a learning rate to regularize the solution: Otþ1 ¼ Ot þ gor tDO tðFF tÞ F tþ1 k ¼ F t k þ gkrt F Df t k: However, there is a caveat to applying a learning rate to both the bases and the output. After the out- put is updated and regularized with go the Fk func- tions will be updated. As such, the scale of Dfk can VALDES ET AL.: REPRESENTATIONAL GRADIENT BOOSTING: BACKPROPAGATION IN THE SPACE OF FUNCTIONS 10189 change to cancel any shrinkage applied to the out- come. Equally, most DO are of the type DO WFðÞ where W is a linear transformation of the input F, therefore W can re-scale back F partially canceling the shrinkage parameter, gk applied to the base updates. This problem can be solved by having an adaptive shrinkage parameter in the outcome that normalizes W, for instance g0k ¼ go=maxðjWjÞ. This was the strategy adopted here in our cases when we used ReLUs and linear models in the output. 3) Step size r: Here we have included the r parameter to scale the step size of the updates of the functions for both the output and base learners. In ANNs this step is usually not performed but rather an adaptive learning rate is used, which decreases if the loss function increases. However, we saw that such a strategy always led to longer running times because sometimes bigger r could be used. As such we decided to estimate the r at every step as described above and adaptively decrease it by a factor of 10 if that r increased the loss function. Therefore, all regu- larization happened through the shrinkage parame- ter,g in our case as every step was close to the biggest. 4.2 Set of Experiments 1 In this section, we analyze the value of RGB as an optimized stacking procedure. As mentioned in the introduction, when different methods are combined using stacking or blending, they are learned independently without consider- ing that they will be subsequently combined. As such, they can not optimally compensate each others’ weaknesses and focus on the aspect of the data that they are best at. For instance, one can imagine that if our problem contains imag- ing and tabular data, we would like to use RGB to combine CNN and GB so that each algorithm can handle the part of the data that they are best at. We designed two experiments to address our hypothe- sis. In both of them we compared the linear output RGB (sRGB) against stacking the bases after they have been fully trained. Tree-based models are in general bad at modeling linear functions while the latter can’t model interactions. Therefore, we used a sRGB version with two channels, one using trees as base learners and the other one using linear models. We compared this sRGB with stacking the base models (Gradient Boosting and a Linear model) after fully learned independently. The trees in the GB channel were trained with a maximum depth of 2 and a learning rate Lr ¼ 0:1. The linear model was trained with a Lr ¼ 0:1 as well. 1000 iterations were done in all cases. Results are shown in Table 1 for two different simulations using (9), one with 15 variables and no noise in the output and the other one with 30 variables and 10% noise in the output. The metric used for comparison was the squared root of the relative mean squared error (RRMSE). In both cases, sRGB improved over stacking as previously thought. Since the function space that both algorithms ﬁt are equal (sRGB and stacking the bases), this improvement comes from optimizing them jointly as previously sug- gested. In Section 3 we will apply to the analysis of imaging and tabular data by combining both CNNs and GB in different channels. 4.3 Set of Experiments 2 In this section, we compare RGB against GB to illustrate a set of scenarios where the former can outperform the later. We also wanted to evaluate the value of providing RGB with prior knowledge thought to improve performance. We ﬁrst generated N ¼ 1000 points for training, 250 for valida- tion and 1000 testing using (8) and (9), with 15 extra noise covariates and 10% nosie added to the output. We studied Gradient Boosting (as baseline), RGB with 3 groups of 5 bases each (12 bases total) using tree models of depth [2,3,4,5] and using boosted neural networks in the output (nRGB). The learning rate selected for the bases was Lr ¼ 0:1. During the ﬁtting steps of the output for the nRGB, the small networks were asked to ﬁt 90% of the resid- ual and stop to provide extra regularization and faster com- putation time (compared to ﬁtting up to smaller percent). 2000 maximum iterations were used in all cases but early stopping was performed if the validation error increased 5 consecutive times. We tested two versions of nRGB, one where we would provide the correct variables to each group of bases (bases in group 1 only received covariates ½x1; ... ;x10\u0005, bases in group 2 only received ½x11; ... ;x20\u0005 and bases in group 3 only received ½x21; ... ;x30\u0005 ), being 5 cova- raites the ones used in the data generating process and the 5 others noise. This version was called “nRGB w idx”. We also tested “nRGB wo idx” where all bases received the 30 covariates, Fig. 2. Please note that the bases can do feature selection as they are comprised of tree based models. We called the last version “nRGB wo idx”. For GB, we per- formed hyper parameter search for the number of iterations (max = 3000), max depth of the tree ([2,3,4,5]) and learning rate ([0.5,0.25, 0.1, 0.01]), Table 2 shows the RRMSE for the three algorithms. Several conclusions can be derived from Tables 2. Both versions of nRGB substantially outperformed GB. Although GB can detect high order interaction terms, RGB can per- form this task substantially more efﬁciently. Please note that each term FjðxÞFkðxÞ adds between 12 and 20 new high order interactions terms to the function and as such there are 47 new interaction terms introduced in (9) beyond those TABLE 1 Comparison of Stacking versus sRGB Without Noise (First Two Columns) and With 10% Noise (Last Two Columns) Stacking sRGB Stacking wn sRGB wn 0.88 0.90 0.87 0.88 0.90 0.92 0.82 0.88 0.88 0.90 0.85 0.89 0.88 0.91 0.86 0.87 0.87 0.89 0.82 0.88 -0.02\u00030.01 -0.04\u00030.03 p = 0.0002 p = 0.016 Results correspond to 5 different iterations, each row corresponding to the same test and training set. R2 is shown. In the last columns we show the mean value \u0003 the standard deviation of each algorithm minus the results of sRGB together with the p value of one sided T test against the null hypothesis that the distributions are the same. In both cases, sRGB signiﬁcantly improved over stacking. 10190 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 12, DECEMBER 2022 included in each of the channels. All these interactions terms would have to be modeled by GB to obtain a good result but can be approximated by RGB easier and more efﬁciently by updating the bases on each channel. In order for GB to model high order interaction, due to the greedy nature of the trees, strong main effects need to exist (in the form of stumps) before higher order terms are discovered. This is not the case with RGB since even stump updates in the bases can result in high order interaction terms. To further this point, we also provide comparison when RGB and GB are used to model F1 þ F2 þ F3. Here we used the same hyperparameters described above. In these type of functions where the number and order of interaction terms is substantially reduced, GB outperforms RGB, Table 3. Sim- ilar results are obtained if noise is added. Therefore, those problems that are more natural to RGB are those with many interaction terms as is customary of representational prob- lems. Of course, once could choose a set of hyper parame- ters that get RGB close to GB (i.e., using a linear output) but in that case the advatanges of RGB might be lost unless dif- ferent models are being combined (i.e., optimized stacking). As such, for problems with only few interactions term GB should be preferred. As usual, there is not way to recognize if a problem has many interaction terms before hand and the practitioners should always check both. A good hint for high order interaction terms might be if the hyper parame- ter depth of the trees in GB is high. Further conclusions can be derived from results in Tables 2. To our surprise, providing indexing to the RGB did not improve performance and it actually slightly worsen it. Providing indexing reduces the variance of the algorithm and in principle would not increase bias as the right function can still be recovered and should therefore improve performance. However, given that the output func- tion is miss-speciﬁed at the beginning and we are trying to estimate it, the bases do not necessarily start approaching the estimation of the right function for each channel. This, in turns, makes the output function compensate the miss- speciﬁcation of the bases converging towards a solution that approximates the nested function but not its parts. This is a result of the identiﬁability theorem discussed in the Appendix, available in the online supplemental material. Therefore, the beneﬁt of variance reduction introduced by indexing gets negated as it becomes harder for the function to converge to a good solution. To prove that the slightly worse result when indexing is provided in our experiments are a consequence of the misspesiﬁcation of the outcome experiments about identiﬁability are provided in the Appendix, available in the online supplemental material. Finally, to make sure that our observations were not a result of running both versions of nRGB with the same number of steps and as such putting the simpler model at a disadvantage, we ran an experiment selecting the number of steps using a validation set with qualitatively equal results as those described here. As such, it seems that the reduction of variance competes with the ability to obtain similar functions that can approximate well the underlying Fig. 2. (a) RGB without feature indexing. All bases receive all input varia- bles. (b) RGB with feature indexing. Input variables are grouped to pro- vide prior knowledge. In both cases only one base learner function per group is represented (instead of 5) for clarity of the graph. TABLE 2 Comparison of GB and nRGB Without (wo) or With(w) Grouping (Indexing) the Right Variables for the Different Channels GB nRGB wo idx nRGB w idx 0.891 0.922 0.916 0.898 0.922 0.927 0.884 0.927 0.910 0.898 0.922 0.916 0.904 0.932 0.927 -0.030\u0003 0.008 -0.024\u00030.004 p = 0.003 p = 0.00009 Numbers correspond to R2. Each row corresponds to one experiment replica- tion. In each experiment, new data was generated. In the last columns we show the mean value \u0003 the standard deviation of each algorithm minus the results GB together with the p value of one sided T test against the null hypothesis that the distributions are the same.. To our surprise, nRGB without indexing achieved the top performance although only slightly better (and not statistically signifcant). Both GB and nRGB without indexing received all 30 variables in all channels with nRGB with indexing receiving 10 variables per channel where only 5 of them contribute to the outcome in the true data generating process. TABLE 3 Modeling F1ðxÞþ F2ðxÞþ F3ðxÞ With the Same Settings as Those Used in Table 2 GB nRGB wo indexing nRGB with indexing 0.922 0.840 0.870 0.910 0.856 0.840 0.927 0.863 0.798 0.924 0.840 0.824 0.922 0.863 0.760 0.069\u00030.014 0.103\u00030.045 p = 0.0001 p = 0.0033 R2 is shown. In the last columns we show the mean value \u0003 the standard devi- ation of each algorithm minus the results GB together with the p value of one sided T test against the null hypothesis that the distributions are the same. If the interaction terms between the channels are removed, GB signiﬁcantly out- performs RGB and the results in Table 2 are reversed. VALDES ET AL.: REPRESENTATIONAL GRADIENT BOOSTING: BACKPROPAGATION IN THE SPACE OF FUNCTIONS 10191 nested function. Our results seem to indicate that not all prior knowledge can help performance and that intuition can be especially troublesome in nested functions. It also indicates that the ability to model many high order interac- tion terms with small updates in the bases is one of the key features to explain performance of representational learning functions like RGB and NN. 4.4 Set of Experiments 3 In this section, we wanted to apply RGB to real data. As illustrated in the simulation sections, RGB has 4 main advantages: 1) By learning different models that capture different aspect of the data together, it can improve over the different models or their combination after fully learned (optimized stacking). 2) It can model more efﬁciently functions that contain many high-order interaction terms. 3) Different from NN, RGB does intrinsic feature selection. 4) Modularity allows to reduce variance with priors although this can also potentially harm performance. In that regard, we thought that: 1) problems with both tabular and imaging data as input potentiated the opti- mized stacking contribution as GB and CNN could be used together, 2) multivariate time series data and sequence data like those found in Natural Language Processing (NLP) tasks also offered the type of problems on which RGB can perform the best given that: 1) features are usually noisy and feature selection is needed 2) many high-order interac- tion terms are likely to exist between the variables 3) due to the nature of the data, regularization and prior knowledge might help (e.g., data becomes less important as time to pre- diction increases) 4) different variables or channels might beneﬁt from the use of different classes of functions. As such, we decided to compare the performance of RGB against ML algorithms in the analysis of 1) one multi modal problem (regression), 2) a timeseries problem (regression) and one NLP problem (classiﬁcation). In the three cases, we compared RGB to the state of the art approaches for all the cases. One problem was a privately collected data set (medi- cal problem) that can not be relased due to IRB and privacy issues. The other two datasets are publicly available and can be donwloaded to replicate our results. The description of these three problems can be found below: 4.4.1 Predicting House Prices This dataset consists of 2840 color images (227 \u0007 227) with their corresponding tabular data (Latitude, Longitude, Number of Beds, Number of Baths and Zip Code). The out- put is the price of house in USD. The dataset was collected by Rosenfelder and analyzed in his personal blog. The same can be downloaded here Donwload Data. The original images where (224 \u0007 224) so we added 3 pixels to the left of the images to make them the current size (so that it could be read by our CNN). Additionally, all the tabular data was standardized as well as the output (prices of the house) and the images rescale to be in the range [0,1]. We split the data in two sets: 80% for training and validation and 20% for testing. All preprocessing was done using the values from the training set (e.g., mean). Here we decided to combine a CNN, a linar model and GB using a linear output layer as our model. For the CNN architecture we used the ﬁrst 16 layers of Alexnet (removing the regression layers) [8]. The parameters of this portion of the CNN were initialized using those of Alexnet after being trained on Imagenet [8]. We also used a learning rate equal to Lr ¼ 10\u00065 during training for the parameters of this portion of the network. After the ﬁrst 16 Alexnet’s layers, we added a dropout layer (probability = 0.5) and a one node fully connected layer. These parameters were initialized with the glorot initializer [21] and a learning rate equal to Lr ¼ 10\u00064 was used for them. These learning rates were selected as half of those that did not cause the CNN to diverge in early explorations as suggested by Laro- chelle et al. [22]. All these layers will be called CNN here. They received as input the images. Besides the CNN chan- nel, we set up a linear channel(2) and a GB channel(3). Both of these channels received as input the tabular vector. The output of the CNN was then concatenated to the output of channel 2 and 3. This concatenated vector was passed to the ﬁnal regression layer whose bias was set to 0 (the output had been normalized) and the rest of the parameters to 1 3 . We used a learning rate equal to Lr ¼ 10\u00062 for the linear channel and the GB channel and they both were initialized with a constant equal to 0 (mean of the output after normali- zation). The maximum depth of the trees considered for the GB channel was 5. This network is represented in Fig. 3. For training, the parameters of the whole network (the CNN + the output layer) were updated using backpropa- gation and the automatic differentiation toolbox from Matlab Inc. The model was trained for 60 epochs with a batch size of 512. Both channel 2 (the linear model) and channel 3 (GB) where updated using backpropagation in the space of functions. We ﬁtted the residual divided by the coefﬁcient from the output layer for each channel to not only get the direction of the step in the space of func- tions but also its magnitude at the same time as suggested by Friedman [13] DFk ¼ argminDFk XN i¼1 yi \u0006 RGBðxxiÞ ck \u0006 DFkFkðxxiÞ \b\u00042: (10) These updates were added to each of these channels at each iteration after they have been multiplied by the learn- ing rate. Besides training RGB, we also trained a multi input Fig. 3. Architecture of RGB combining Alexnet, a linear channel and a Gradient Boosting channel. The output layer is a linear combination of the concatenated vector of the three different algorithms. 10192 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 12, DECEMBER 2022 CNN (called here mCNN). The CNN channel portion of mCNN was identical to that of RGB but then the output of the CNN was concatenated with the raw tabular data (also standardized) and passed to the output layer. Additionally, we trained a GB model using only the tabular data, 10000 iterations (with early stopping), Lr ¼ 0:01 and depth ¼ 5. Finally we also created a stacked model (linear combina- tion) using the output of the mCNN and GB. The results of these experiments are reported in Table 4. As it can be seen, RGB signiﬁcantly outperforms any other strategy including stacking mCNN and GB. Since both classes of functions are identical, this improvement comes from optimizing them jointly as previously shown for the combination of linear models and GB in the set of experiments 1. 4.4.2 Brain Signal Dataset This data set contains brain signals from 90 different sensors placed on patients head. The data set had 90 region of inter- ests (ROI), each with 180 observations taken serially with time. Although no speciﬁc ROI is more important than the other, as proof of principle, we decided to predict the ﬁrst channel only, using information from all the other ROIs and early data from that same ROI. We used the ﬁrst 80% for training and the last 20% for testing. We also decided to pre- dict the next value only. For this problem, no transformation was applied to the data when GB and RGB were analyzed since these algo- rithms are invariant to monotonic transformations of the input variables. This together with how easy trees can han- dle missing values is one of the advantages or RGB although no speciﬁcally investigated here. When LSTMs were used, the data were normalized ﬁrst. For RGB and GB we decided to use the data with lag 5. We also used lag 10 but found no advantage for the extra computational time. With GB, the learning rate ½0:01; 0:01; 0:25; 0:5; 1\u0005 and the depth of the trees [1,2,3,4,5] where cross validated using the training data and max number of iteration equal to 2000 combined with early stopping. For nRGB we used 3 ver- sions. In all cases, a channel using trees of depth equal 2 and another channel building linear models were used. In the ﬁrst version of nRGB we did not provide prior informa- tion and as such, 450(90x5) variables where provided to each channel. The learning rate was set to Lr ¼ 0:1. We will call this version “nRGB wo idx”. In the second version we divided the data into 5 groups of channels, two base models per channel(one using trees of depth equal 2 and the other one using linear models). Each group received indexing to only include variables corresponding to a time stamp. All the channels had the same Learning rate of Lr ¼ 0:1 as well. We called this version “nRGB w idx”. The prior knowledge provided here assumes that variables inﬂuencing on the outcome depends on time. The last version of nRGB was equal to the second but additionally we decreased the Lr by a factor of two with time. We are assuming that data sepa- rated longer in time is more noisy and needs more regulari- zation, Fig. 4. Finally, we also compared it with two LSTM models, one using one hidden LSTM layer and a fully connected layer, called here “LSTM”, and the other one using 5 LSTM layers with 0.2 dropout layers between them and a fully connected layer, called here “Deep LSTM”. Different number of hid- den units were tested in the LSTM layers that ranged from 10 to 500. Additionally, we explored different initial learn- ing rates ½0:0001; 0:001; 0:001; 0:01; 0:1\u0005 , with a learning rate drop period of [10,25,50] and different learning rate drop factors [0.1, 0.2, 0.5, 0.8] and epochs [50,100,200,500,1000]. The networks were trained using the Adam optimizer. Results shown for the LSTM are for the best combination of hyper parameters. Results for these experiments are shown in Table 5. The reduction of variance provided by both priors (indexing and differential learning rate with time) independently improved performance, eventually making nRGB more accurate than GB. Additionally, it seems that doing feature selection is very important in this problem as both GB and nRGB signiﬁcantly outperformed the LSTM models, even when substantially more time was spent looking for the right combination of hyper parameters in the latter. No time was spent searching for hyper parameters in RGB as sensi- ble choices like those used here give good performance. TABLE 4 R2 for Each Algorithm Evaluated on the Test Set GB mCNN stacked GB + mCNN RGB 0.523 0.551 0.590 0.698 0.473 0.523 0.601 0.667 0.424 0.615 0.668 0.703 0.491 0.591 0.647 0.720 0.447 0.505 0.640 0.680 -0.222 \u0003 0.041 -0.137 \u0003 0.032 -0.0642 \u0003 0.029 p = 0.0001 p = 0.0003 p = 0.0039 GB was only trained on the tabular data while mCNN was trained using both the images and the tabular data. RGB was trained using both the images and tabular data with three different channels (CNN, Linear Model and GB). In the last columns we show the mean value \u0003 the standard deviation of each algorithm minus the results RGB together with the p value of one sided T test against the null hypothesis that the distributions are the same. RGB signiﬁ- cantly outperforms every strategy including stacking. Fig. 4. (a) Architecture of RGB when there was no indexing according to the lag or time stamp (b) when variables are grouped according to their lag. VALDES ET AL.: REPRESENTATIONAL GRADIENT BOOSTING: BACKPROPAGATION IN THE SPACE OF FUNCTIONS 10193 4.4.3 Classiying Weather Reports This is a Natural Language processing task and it consisted of classifying text descriptions of weather reports. In total there were 40 categories (e.g., Freezing Fog, Hurricane). This data was used as demonstration of LSTM networks by Matlab Inc and they had reported accuracy of these models to be, Accuracy ¼ 0:875. In total, there were 23900 reports. The data was approximately divided into 80% for training and 20% for testing, having the same class distribution in both groups. For this problem, a classiﬁcation task, we used the LSTM network and followed the training procedure provided by Matlab Inc. This network consisted of a word embedding layer (dimension = 100) + LSTM layer (hidden units = 80) + fully connected layer + a Soft max layer. Each weather report was tokenized and 75 tokens were selected. The Adam opti- mizer and the same learning rate policy described above were used. We separated the data into 80% for training and 20% for testing. Using these hyperparameters we replicated the results reported in Matlab with an Accuracy ¼ 0:87. We extracted the representation learned in the word embedding layer and provided it as input to RGB. In this ver- sion of RGB we had 39 groups of channels, each using three channels with trees of depth = [1,3,5]. Each channel special- ized in predicting an FkðxÞ that were later linearly combined into a softmax layer to produce a probability over the classes. Using the training data, we also explored combinations of lin- ear models and trees in the groups but observed that there was always overﬁtting using the linear models. Therefore we concluded that feature selection was needed and used the RGB described above. The learning rate for each channel was Lr ¼ 0:1. When the word representation learned by the LSTM was fed as input to RGB, we obtained an Accuracy ¼ 0:80. Our results indicate that the word embedding representa- tion was learned to facilitate the learning of the LSTM and its features are hard to reuse. In fact, when we retrained a LSTM network using this same word embedding representation, the best accuracy that we could obtain was, Accuracy ¼ 0:82. This phenomenon termed co-adaptation has been exten- sively reported in ANNs and it indicates that “...feature detectors are only helpful in the context of several other spe- ciﬁc feature detectors...” as described by Hinton et al. [23]. If instead of using the representation learned by the word embedding, we trained RGB using the frequencies of the words on each document we then obtained an Accuracy ¼ 0:88 which rivals that of the LSTM model. Our results indi- cate that for RGB to be better in NLP tasks, we would need to design an initial layer with word embedding speciﬁc for our algorithm. That work, however, is beyond the scope of this paper. Additionally, note that by no means is the accuracy obtained by the LSTM models in this task, either by Matlab Inc or us, can be considered state of the art. Transformer architectures, especially when pretrained in a large corpus of data, have dominated NLP tasks in recent years [24]. As such, we should regard this experiment as an illustration of how providing feature selection (through RGB) can improve on simpler NN approaches that do not contain it. 5CONCLUSIONS AND FUTURE WORK Here, we introduce Representational Gradient Boosting, a novel representation learning algorithm based on gradient boosting. RGB offers unique advantages from a practical perspective. It can perform optimized stacking where differ- ent functions (e.g., a CNN and GB ) are learned together to improve on each others’ weaknesses, even when some of the functions are not differentiable (e.g., GB). This allows to bring together differentiables (e.g., NN) and non-differen- tiable functions (e.g., GB) into the same optimization umbrella. Given the multi-modal nature of many data sets today where images, texts and tabular data with categorical and continue variables are combined, no speciﬁc class of functions is likely to be the best to analyze all parts of it and an algorithm that allows different classes to be optimized together is essential. Additionally, adding layers of tree- based models makes those layers invariant to monotonic transformations of the input and able to handle missing val- ues. Moreover, it offers a natural way to introduce feature selection for multi-layer architectures. It is able to model functions with many high-order interaction terms (typical of nested funtions) better than GB although there is not much advatange when the underlying data generation pro- cess does not consist of many interation terms. Because of these characteristics, we expected RGB to be highly compet- itive in the analysis of multi modal problems, timeseries and sequences. This was demonstrated here using three real data sets and extensive experimentation on synthetic data. We also proved that not only nested functions can not be recovered in theory but actually in practice it is a very hard task requiring extensive amount of prior knowledge. These results highlight the issue that although some representa- tions learned in a certain manifold can help improve perfor- mance compared to the original variables, we should be cautious when we interpret their meaning. We also showed how, due to the identiﬁability issue, certain prior knowl- edge that was thought to help actually hurt performance. Our experiments highlight the difﬁculties of applying intui- tion to nested functions in order to improve performance. This is equally the case when we use this intuition to pro- vide explanations of why a representation algorithm is obtaining better performance. As discussed above, it is RGB’s efﬁciency in modeling multiple high-order interac- tion terms what resulted in better performance over GB in the problems analyzed. Without proper experimentation, we would have attributed the improvement to the prior knowledge provided. We encourage the interested reader to review our theoretical section and experiments regarding identiﬁability and representational learning in the Appen- dix, available in the online supplemental material. Finally, we would like to discuss some aspects of the computational requirements of RGB. This depends on the TABLE 5 RRMSE for Each Algorithm GB sRGB wo idx sRGB w idx 0.328 0.226 0.510 sRGB w id and LR LSTM Deep LSTM 0.578 0.208 0.154 GB was better than plain sRGB and both of them better than LSTM. However, once we provided prior information to RGB in the form of variable indexing and stronger regularization with time, sRGB w id and LR resulted to be the best algorithm. 10194 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 12, DECEMBER 2022 particular models being selected for the individual channels as well as that for the output function. Generally simple base learners, as in regular gradient boosting, are preferred. For instance, if we select NN (e.g., CNN) as the base learn- ers, then complexity and memory requirements will explode if we optimize that channel as a summation of all of them (unless very small networks are selected). However, when one of the channels is a NN, for instance a CNN, there is no need to boost that channel (build a model with sum- mation of smaller base learners) and the error could be propagated to that channel in the space of parameters (regu- lar backpropagation) while we propagate the error to the other channels in the space of functions. This is the approach we have taken in Section 3 (Predicting House Pri- ces). The computational complexity in this case is no larger than training the different models and performing stacking afterwards as it is customary done. For future work, we would like to investigate other differ- entiable simple base learners so that deeper architectures can be designed although depth is not a requirement to increase complexity and it might not be needed as much as in current NN implementations. Additionally, the use of numerical derivatives can also be considered if layers of trees are desired in other positions other than the deepest layer. REFERENCES [1] L. Breiman, “Stacked regressions,” Mach. Learn., vol. 24, no. 1, pp. 49–64, 1996. [2] M. LeBlanc and R. Tibshirani, “Combining estimates in regression and classiﬁcation,” J. Amer. Statist. Assoc., vol. 91, no. 436, pp. 1641–1650, 1996. [3] D. H Wolpert, “Stacked generalization,” Neural Netw., vol. 5, no. 2, pp. 241–259, 1992. [4] G. Mani, “Learning by gradient descent in function space,” in Proc. IEEE Int. Conf. Syst., Man Cybern. Conf. Proc., 1990, pp. 242–247. [5] J. H Friedman and J. W Tukey, “A projection pursuit algorithm for exploratory data analysis,” IEEE Trans. Comput., vol. 100, no. 9, pp. 881–890, Sep. 1974. [6] J. H Friedman and W. Stuetzle, “Projection pursuit regression,” J. Amer. Statist. Assoc., vol. 76, no. 376, pp. 817–823, 1981. [7] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798–1828, Aug. 2013. [8] A. Krizhevsky, I. Sutskever, and G. E Hinton, “Imagenet classiﬁca- tion with deep convolutional neural networks,” in Proc. Adv. Neu- ral Inf. Process. Syst., 2012, pp. 1097–1105. [9] K. Simonyan and A. Zisserman, “Very deep convolutional net- works for large-scale image recognition,” in Proc. Int. Conf. Learn. Representations, 2015. [10] F. Seide, G. Li, and D. Yu, “Conversational speech transcription using context-dependent deep neural networks,” in Proc. Annu. Conf. Int. Speech Commun. Assoc., 2011, pp. 437–440. [11] Y. LeCun et al., “Backpropagation applied to handwritten zip code recognition,” Neural Comput., vol. 1, no. 4, pp. 541–551, 1989. [12] G. Alain and Y. Bengio, “Understanding intermediate layers using linear classiﬁer probes,” in Proc. Int. Conf. Learn. Representations,2017. [13] J. H Friedman, “Greedy function approximation: A gradient boosting machine,” Ann. Statist., vol. 29, pp. 1189–1232, 2001. [14] R. Caruana and A. Niculescu-Mizil , “An empirical comparison of supervised learning algorithms,” in Proc. Int. Conf. Mach. Learn., 2006, pp. 161–168. [15] J. M. Luna et al., “Building more accurate decision trees with the additive tree,” Proc. Nat. Acad. Sci. USA, vol. 116, no. 40, pp. 19887– 19893, 2019. [16] G. Valdes, J. M. Luna, E. Eaton, C. B. Simone II , L. H Ungar, and T. D Solberg, “MediBoost: A patient stratiﬁcation tool for inter- pretable decision making in the era of precision medicine,” Sci. Rep., vol. 6, 2016, Art. no. 37854. [17] Y. Bengio, “Deriving differential target propagation from iterating approximate inverses,” 2020, arXiv:2007.15139. [18] D.-H. Lee, S. Zhang, A. Fischer, and Y. Bengio, “Difference target propagation,” in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discov. Databases, 2015, pp. 498–515. [19] J. Feng, Y. Yu, and Z.-H. Zhou, “Multi-layered gradient boosting decision trees,” in Proc. Adv. Neural Inf. Process. Syst., 2018, pp. 3555–3565. [20] J. H Friedman, “Multivariate adaptive regression splines,” Ann. Statist., vol. 19, pp. 1–67, 1991. [21] X. GlorotY. Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,” in Proc. Int. Conf. Artif. Intell. Stat- ist., 2010, pp. 249–256. [22] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin, “Exploring strategies for training deep neural networks,” J. Mach. Learn. Res., vol. 10, no. 1, pp. 1–40, 2009. [23] G. E Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R Salakhutdinov, “Improving neural networks by prevent- ing co-adaptation of feature detectors,” 2012, arXiv:1207.0580. [24] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998–6008. Gilmer Valdes received the PhD degree in medi- cal physics from the University of California, Los Angeles, in 2013. He was a postdoctoral fellow with the University of California, San Francisco between 2013–2014 and a medical physics resi- dent from 2014 to 2016 with the University of Pennsylvania. He is currently an assistant profes- sor with dual appointments with the Department of Radiation Oncology and the Department of Epidemiology and Biostatistics, the University of California, San Francisco. Jerome H. Friedman received the PhD degree in high energy particle physics from the University of California, Berkeley, in 1967. In 1972, he was appointed lead with Computation Research Group, Stanford Linear Accelerator Center, Stan- ford University. Since 1982, he has been a profes- sor with the Department of Statistics, Stanford University. Fei Jiang received the PhD degree in statistics from Rice University in 2013. Between 2013– 2016, she was a postdoctoral fellow with Harvard. From 2016 to 2019, she was an assistant profes- sor and the associate director for the artiﬁcial intelligence and data science programs with the University of Hong Kong. She was with the Department of Epidemiology and Biostatistics, the University of California, San Francisco, in 2019. Efstathios D. Gennatas received the MBBS degree in medicine from Imperial College London and the PhD degree in neuroscience from the University of Pennsylvania. From 2017 to 2019, he was an assistant professional researcher with the University of California San Francisco. From 2019 to 2020, he was a research scientist with Stanford University. He will join the Department of Epidemiology and Biostatistics with the University of California, San Francisco, in August 2020. \" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl. VALDES ET AL.: REPRESENTATIONAL GRADIENT BOOSTING: BACKPROPAGATION IN THE SPACE OF FUNCTIONS 10195","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/2208.07590v3 1.pdf","text":"NEURAL NETWORKS FOR EXTREME QUANTILE REGRESSION WITH AN APPLICATION TO FORECASTING OF FLOOD RISK Olivier C. Pasche Research Center for Statistics, University of Geneva, Switzerland, olivier.pasche@unige.ch Sebastian Engelke Research Center for Statistics, University of Geneva, Switzerland, sebastian.engelke@unige.ch ABSTRACT Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems. Keywords Extreme value theory · Generalized Pareto distribution · Machine learning · Prediction · Recurrent neural network 1 Introduction Risk assessment is concerned with the analysis of rare events, which have small occurrence probabilities but carry the potential of serious impacts on our health, the environment, or the economy. Examples of such extreme events are floods in hydrology, crises in the financial system, or heatwaves in a changing climate. In these applications, the quantity of interest is typically a univariate response variable Y representing the random risk factor. The goal is to estimate a quantile Q(τ ) = F −1 Y (τ ) at level τ ∈ [0, 1], where we denote by F −1 Y the generalized inverse of the distribution function of Y . Since for risk quantification the level τ is usually very close to 1 so that the quantile Q(τ ) goes beyond the range of the data, the classical approach is to model the tail of the distribution of Y using extrapolation results from extreme value theory. Two main approaches exist. When Y represents, say, a daily quantity, then the generalized Pareto distribution (GPD) can be used to approximate the tail above a high threshold u by (Balkema and de Haan, 1974; Pickands III, 1975) P(Y > y) ≈ P(Y > u) ( 1 + ξ y − u σ(u) )−1/ξ + , y ≥ u, (1)arXiv:2208.07590v3 [stat.ME] 30 May 2024 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Figure 1: Topographic map of water catchment of the gauging station in Bern–Schönau (62) on the Aare in Switzerland. Another gauging station upstream in Gsteig (43) on the Lütschine river and six meteorological stations with precipitation measurements (triangles) are also shown. where ξ ∈ R and σ(u) > 0 are the shape and scale parameters, and the second factor on the right-hand side is the GPD approximation to P(Y > y | Y > u). On the other hand, if Y represents an annual maximum, then the generalized extreme value distribution provides a good fit (Fisher and Tippett, 1928). In hydrology and climate science, risk is often assessed as the T -year return level QT , that is, the size of an event that is exceeded on average once every T years. If Y represent a quantity with nY independent recordings per year (e.g., nY = 365 for daily data and nY = 1 for annual maxima), then the T -year return level is the quantile QT = Q(1 − 1/(nY T )). Figure 1 shows the river catchment of a gauging station on the Aare river in Bern, Switzerland. It is part of the Aare–Rhein basin, where flooding is a major economic and safety concern (Andres et al., 2021). The Swiss Federal Office for the Environment (FOEN) provides recordings of daily average discharges throughout the country. For risk assessment, they report the 100-year return levels using the GEV method for annual maxima and the GPD for large daily discharges. The horizontal lines in Figure 2 show estimates of this return level at the Bernese station based on data from the years 1930–1958. Both methods give very similar results. The disadvantage of such an unconditional approach is twofold. First, the return level is static and unable to reflect changes in the size of extreme floods over time, which can occur for instance due to climate change. For the Bernese station, for instance, a structural break has been observed in the nineties without a clearly defined cause1, making classical extreme value modelling challenging. Second, while the return level QT is relevant for the construction of long-term flood infrastructure, it can not be used to assess the risk of flooding on a given day. Such forecasting of extreme events is crucial for early warning systems. Indeed, the probability of exceeding on a particular day a given high threshold, say the (constant) 100-year return level, depends on many covariates X such as the river flows upstream and precipitation in the catchments during the preceding days and weeks. 1see flood report of the FOEN at https://www.hydrodaten.admin.ch/en/2135.html 2 EXTREME QUANTILE REGRESSION NEURAL NETWORKSDischarge [m3s−1]Probability ratio 2005 Jul 2005 Aug 2005 Sep 2005 Oct 250 500 750 1000 0 500 1000 1500 Date Figure 2: Top: Daily average discharge (points) at the Bern–Schönau station (62) and one-day-ahead EQRN forecasts of conditional 100-year quantiles (solid line) during the 2005 flood. Horizontal lines show unconditional Q 100 based on GEV (dashed) and GPD (dotted). Bottom: one-day-ahead EQRN forecast of the conditional probability of exceeding the GEV estimated Q 100 as a ratio to the unconditional probability. The vertical line indicates August 22, the day of the first exceedance. In this paper, we, therefore, advocate a conditional version of return levels defined as the conditional quantile of Y given a vector of observed covariates X = x, that is, Qx(τ ) = F −1 Y |X=x(τ ). (2) The interpretation of such a conditional T -year return level QT x = Qx(1 − 1/(nY T )) is different from the unconditional return level QT . Since QT x depends on the exact configuration of the covariates x, one can see a conditional T -year return level as the size of an event that is exceeded in average once every T years, if the covariate vector X of all observations of Y had the same value x. A more precise interpretation is to see QT x as the value that is exceeded in the next time step with probability 1/(nY T ), and we refer to it as the conditional quantile with return period T years; for a comprehensive discussion of return levels and quantiles, see Bücher and Zhou (2021). The top panel of Figure 2 shows one-day-ahead forecasts of such conditional 100-year quantiles Q100 x for the Bernese river data from the method of this paper, fitted to the years 1930–1958. We see that, as opposed to the unconditional return level Q100, the conditional quantile changes from day to day depending on past precipitation and river flows. In fact, on August 21, the day before the first exceedance of the unconditional Q100 during the serious flood of August 2005 in Bern, the size of the conditional 100-year event predicted for the next day was much higher than on other days, and the forecasted conditional probability of such an exceedance (bottom panel of Figure 2) was 920 times larger than the static 100-year probability. Both outputs can be used as triggers for early warnings and additional flood management measures. In particular, the days when an exceedance above Q100 is likely can be effectively pinpointed thanks to their temporal sparsity in the probability forecast. Forecasting extreme events is notoriously difficult due to the low occurrence probabilities involved. This is particularly true for hydrological models, which are typically used by national agencies, and which do not use explicit tail extrapolation. In the aftermath of the 2005 flood, the FOEN published a detailed analysis of the internal forecasting procedures during this event (Bezzola and Hegg, 2007). The report showed that too-late warnings lead to more severe consequences since forecasters did not trust the predicted precipitation amounts during this extreme scenario and underestimated the flood risk. This gives another motivation why our statistical model and its output in Figure 2 could have been a helpful tool during this flood. 3 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Conditional quantiles (2) are studied in the field of quantile regression, where many flexible methods exist (Meinshausen, 2006; Cannon, 2011; Zhang et al., 2019; Athey et al., 2019). While they work well for quantile levels within the data range, they break down for extreme values of τ close to 1. Such extreme quantile regression relies on extrapolation results as in (1), where extreme value parameters such as the scale σ(x) and shape ξ(x) may be modelled as functions of the covariates through linear models (Wang et al., 2012; Li and Wang, 2019), generalized additive models (Chavez- Demoulin and Davison, 2005; Youngman, 2019) or kernel methods (Daouia et al., 2011; Gardes and Stupfler, 2019; Velthoen et al., 2019). To overcome limitations of additive and kernel-based methods in higher dimensions, more recently, flexible tree-based methods have been combined with the GPD extrapolation for predicting extreme conditional quantiles (Velthoen et al., 2023; Gnecco et al., 2022) or predictive tail distributions (Koh, 2023) on complex data. Tree-based methods have the advantage of requiring little tuning for good prediction performance. However, they can not incorporate additional structure of the data as encountered in time series or spatial applications. The goal of our work is to combine ideas from extreme value theory and machine learning to propose an extreme quantile regression model that has the ability to extrapolate in the direction of the response Y and to model complex covariate dependencies in the predictors X. We propose an extreme quantile regression network (EQRN) that models covariate-dependent GPD parameters σ(x) and ξ(x) as outputs of a neural network. Conditional quantile estimates at the desired extreme level are then readily derived from the estimated conditional tail distribution. Neural networks are known for their ability to model complex dependencies and to approximate any measurable function arbitrarily well (Hornik, 1991). The second advantage is versatility. The deep learning literature is rich in network architectures, activation functions and regularization methods. In particular, convolutions produce shift-invariant models for covariates with spatial dependencies, and recurrent architectures provide models for sequentially dependent observations such as time series. In our application of flood forecasting with time-dependent data, recurrent neural networks (Werbos, 1988; Elman, 1990) are of particular interest. The main output of our EQRN model for sequentially dependent data is the one-day-ahead risk forecast, either in terms of conditional quantiles or exceedance probabilities. Thanks to the GPD approximation, the method can extrapolate beyond the range of the data, as illustrated in Figure 2. The strength of our recurrent model lies in the ability to exploit information from multiple covariates and capture complex time dependence. It is therefore an effective early warning tool even for unprecedented, record-shattering events, like the 2005 floods. This is of particular importance in a non-stationary system, where climate change makes extreme events increasingly likely (Fischer et al., 2021). The main contributions of the paper are threefold. Firstly, our EQRN method is the first to model the conditional GPD parameters through neural networks. Thanks to the large number of neural network architectures, this expands the range of possible applications for extreme quantile regression to new areas. While we concentrate on sequentially dependent data, our method can be used, for instance, in combination with convolutional or graphical neural networks for spatial covariates. Secondly, a major technical contribution is to make neural networks applicable in the extreme value context. To stabilize the prediction of extreme quantiles in these highly flexible regression methods, we propose to use an orthogonal reparametrization of the GPD deviance, a suitable choice of activation functions, and the use of the intermediate quantiles as additional covariates; the latter is a new idea that seems to also improve other extreme quantile methods. Finally, a main novelty is the application to flood forecasting and the notion of conditional return levels that can be used as early warnings. With this perspective, our method enables applications in many other areas, such as the one-day-ahead forecasting of the value-at-risk or of the expected shortfall in financial time series. The paper is organized as follows. In Section 2, we provide background on quantile regression, extreme value theory and neural networks. We propose our EQRN model for both independent and sequentially dependent data in Section 3. Section 4 contains a simulation study to assess the performance of our 4 EXTREME QUANTILE REGRESSION NEURAL NETWORKS approach in comparison to existing methods. In Section 5, we describe the Swiss river data, apply our methodology to forecast flood risk and discuss the implications of the results. Section 6 concludes with a brief discussion. 2 Background 2.1 Quantile Regression In the classical quantile regression setup, we observe an independent and identically distributed sample D = {(xi, yi)}n i=1 of the random vector (X, Y ), where Y is the real-valued response variable and X is a vector of p covariates (or predictors). One aims at predicting the conditional quantile Qx(τ ) defined in (2) of Y given X = x, for some predictor value x ∈ Rp and probability level τ ∈ (0, 1) of interest. Analogously to regression that minimizes the mean squared error, quantile regression minimizes the quantile loss, Qx(τ ) = arg min q E[ρτ (Y − q) | X = x], (3) where ρτ (t) := t(τ − 1{t<0}) is the quantile check function (Koenker and Bassett, 1978). Many para- metric and non-parametric quantile regression models exist, including linear models (Chernozhukov, 2005), random forests (Athey et al., 2019) and neural networks (Cannon, 2011; Zhang et al., 2019). They yield a conditional quantile estimate by minimizing the empirical quantile loss over the training sample D, that is, ˆQx(τ ) = arg min qτ ∈M 1 n n∑ i=1 ρτ (yi − qτ (xi)), (4) where M is the set of possible quantile functions qτ (·) characterized by the model. Classical methods for quantile regression that rely on the quantile loss (3) perform well for “moderate” probability levels τ . To define what that means exactly, we typically let τn depend on the sample size n. The expected number of exceedances of yi over the respective conditional quantile Qxi(τn), i = 1, . . . , n, is given by n(1 − τn). With moderately extreme, or intermediate, we refer to a sequence τn → 1 with n(1 − τn) → ∞, meaning that the quantile goes to the upper endpoint of the distribution, but there are more and more exceedances with growing sample size n. On the other hand, we call a quantile level τn → 1 extreme if n(1 − τn) → c ∈ [0, ∞), that is, there are finitely many, or possibly zero, exceedances over Qxi(τn) in the sample. In this situation, classical quantile regression methods do not perform well due to the scarcity of observations in the tail of the response. The left panel of Figure 3 illustrates this issue for a sample y1, . . . , yn with n = 1000 and no covariates. The dashed line shows for different quantile levels τn the empirical quantile estimates, obtained by solving the respective quantile loss function without covariates. It can be seen that as soon as the number of exceedances n(1 − τn) < 1, that is, τn > 99.9%, there is a significant bias compared to the true quantiles (solid line). The reason is that the empirical estimates can not predict higher than the largest observation. When covariates are present, this issue persists, since quantile regression relies on solving the empirical quantile loss (4). In the sequel, we omit the dependence on n and write τ instead of τn. Intermediate quantile levels will be denoted by τ0. 2.2 Generalized Pareto Distribution In order to predict well on extreme quantiles, a method should rely on asymptotic results from extreme value theory for accurate extrapolation beyond the sample. In particular, we rely on the generalized 5 EXTREME QUANTILE REGRESSION NEURAL NETWORKS 5 10 15 20 90% 99% 99.9% 99.99% 99.999% Probability levelAverage predicted quantile x x(1) x(2) x(L) gW (x) y ℓ Figure 3: Left: true τ -quantiles (solid line) compared to empirical estimates (dashed line) and GPD based estimates (dotted line) for moderate to extreme probability levels (log-scale) for sample size 1,000. Estimates are averages over 100 trials. Right: multi-layer perceptron flowchart from input x to output gW (x), with loss function ℓ and corresponding response y. Pareto distribution (GPD). In the presence of covariates, it arises as an approximation of the tail of the distribution of Y | X = x. More precisely, we use a conditional version of (1), P(Y > y | X = x) ≈ (1 − τ0) (1 + ξ(x) y − u(x) σ(x) )−1/ξ(x) + , y > u(x), (5) where the threshold u(x) is chosen as an intermediate quantile Qx(τ0) at level τ0 ∈ (0, 1) close to 1, and the shape ξ(x) ∈ R and scale σ(x) > 0 depend on the covariates; here we omit the dependence of σ(x) on the intermediate level τ0 in the notation. This approximation holds under weak conditions on the tail of Y | X = x; see (Balkema and de Haan, 1974; Pickands III, 1975) for the precise statement. This condition is of univariate nature, and it can therefore be verified even in more complex situations, for instance where X represents the history of a multivariate time series; see Section 4.1 for details. The shape parameter ξ(x) is important since it encodes the tail heaviness of the response: if it is positive, the response has a heavy-tailed distribution such as Pareto or Student-t; if it is zero, the response is light-tailed such as a Gaussian or exponential; if it is negative then the response has a finite upper endpoint. In order to predict an extreme quantile at level τ > τ0 from approximation (5), we can invert this expression to find Qx(τ ) := Qx(τ0) + σ(x) ξ(x) [( 1 − τ0 1 − τ )ξ(x) − 1 ] . (6) This shows that an estimate ˆQx(τ ) of an extreme quantile requires estimates of the intermediate quantile ˆQx(τ0) and of the conditional GPD parameters ˆξ(x) and ˆσ(x) as functions of the predictor vector. For the intermediate quantile function, we can use any of the existing methods for quantile regression since they work well for this moderate quantile level as discussed above. Estimation of the GPD parameters can be done by specifying a parametric or non-parametric model. We will use neural networks for this purpose, which are introduced in the next section. The green line in Figure 3 shows estimates ˆQ(τ ) for different quantile levels τ using the approxima- tion (6) without covariate dependence, with empirical intermediate quantile at τ0 = 90% and GPD parameters estimated with maximum likelihood. It can be seen that the extrapolation solves the bias issue of empirical methods. 6 EXTREME QUANTILE REGRESSION NEURAL NETWORKS 2.3 Neural Networks and Conditional Density Estimation The literature on neural networks is vast, and existing methods are being improved constantly. We concentrate in this section on well-established techniques that are most relevant for our purpose of modelling extreme quantiles. A multi-layer perceptron (MLP) or fully-connected feed-forward neural network model is a parametric family of non-linear functions gW : Rp → Rq that map a p-dimensional input x to a q-dimensional output by x ↦→ x(L+1), with x(l) = σl (W lx(l−1) + b l) ∀l = 1, . . . , L + 1, (7) where x(0) = x. The number of hidden layers L ∈ N, the hidden layer dimensions h1, . . . , hL ∈ N and the choice of activation functions σl : Rhl → Rhl, l = 1, . . . , L + 1 (with h0 = p and hL+1 = q) are hyperparameters that need to be chosen for instance by cross-validation. The set of trainable parameters to be inferred from data contains all weights and bias terms of the network, that is, W = { (W l, bl); l = 1, . . . , L + 1} , with W l ∈ Rhl×hl−1 and bl ∈ Rhl. Figure 3 shows a schematic illustration of the transformations inside the MLP. In the general setting, p is the number of features or covariates considered in the model, and q depends on the task at hand. In order to train a model, a loss function ℓ : R × R → [0, ∞) is required that maps a tuple (y, gW (x)) of response and prediction to a positive number quantifying their discrepancy. Common tasks include mean regression with q = 1 and squared error loss, quantile regression with q = 1 and quantile loss, and classification with q equal to the number of possible classes and cross-entropy as loss. For conditional density estimation, or distribution regression, we suppose that Y follows a distribution with parametric probability density fY (·; θ) and parameter θ = θ(x) depending on the vector X = x. Conditional density estimation networks are neural networks that aim at outputting conditional estimates gW (x) = θ(x) based on realizations of X = x as input (e.g., Cannon, 2012). In this setting, p is the dimension of X and q is the dimension of θ. The loss function is the deviance or negative log-likelihood loss ℓ(y, θ(x)) = − log fY (y; θ(x)). To train a neural network on the training dataset D = {(xi, yi)} n i=1 we find the optimal parameter values minimizing the average empirical loss, that is, ˆW ∈ arg min W 1 n n∑ i=1 ℓ(yi, gW (xi)). (8) This is generally achieved via backpropagation using mini-batches and optimization algorithms such as the well-performing gradient descent variants (Kingma and Ba, 2014; Tieleman and Hinton, 2012; Duchi et al., 2011). Since neural networks are typically overparameterized, overfitting has to be prevented with regularization methods such as L2 weight penalties for narrow networks and dropout (Srivastava et al., 2014) for deeper architectures. As the optimization problem (8) is often non-convex, local-minima convergence is an issue. Restricting training to a subset of D and keeping the rest to track the validation loss at the end of each epoch helps to avoid local minima by learning rate decay. Restarting training with different initializations and keeping the best fit in terms of validation loss often leads to lower minima. Early stopping based on the validation loss is another measure against overfitting. The final validation loss is used for model selection and the choice of optimal hyperparameters; for more details on the fitting of neural networks, see Goodfellow et al. (2016). When observations are dependent in space or time, generalizations of the MLP exist to account for these particular structures. Convolutional and graph neural networks exploit neighbourhood information with parsimonious architectures that are effective for images, graphs or spatial observations (LeCun et al., 2015; Scarselli et al., 2009). We concentrate here on methods for sequential dependence that 7 EXTREME QUANTILE REGRESSION NEURAL NETWORKS xt−1xt−2xt−3xt−s LSTMLSTMLSTMLSTM ht−1ht−2ht−3 ct−1ct−2ct−3 ˜gW (˜xt)yt ℓ Figure 4: Single-layer LSTM network flowchart from input ˜xt := (xt−s, . . . , xt−1) to output ˜gW (˜xt), with loss evaluation. The LSTM cells represent the transformation in (9). arises typically in time series {(Xt, Yt)}T t=1. For this type of data, recurrent architectures of the network allow capturing dependence between observations. A simple recurrent neural network (RNN) layer (Werbos, 1988; Elman, 1990) takes as input a vector xt and outputs the hidden recurrent state vector ht = tanh(Wxhxt + Whhht−1 + bh), depending both on xt and the hidden state ht−1 recursively resulting from the previous inputs xt−1 and ht−2 in the sequence. Here and in the sequel, the bias vectors b· and weight matrices W··, indexed by the input and output variables, are the trainable parameters. This model has then been improved by the addition of a gating cell state ct, to avoid vanishing gradient issues, as well as a forget gate ft, an input gate it and output gate ot, to control both short- and long-term dependencies in the sequence. This yields the long short-term memory (LSTM) layer (Hochreiter and Schmidhuber, 1997; Gers et al., 2000, 2003; Jozefowicz et al., 2015) it = σ(Wxixt + Whiht−1 + bi), ft = σ(Wxf xt + Whf ht−1 + bf ), gt = tanh(Wxgxt + Whght−1 + bg), ot = σ(Wxoxt + Whoht−1 + bo), ct = ft ⊙ ct−1 + it ⊙ gt, ht = ot ⊙ tanh(ct), (9) where σ(z) = 1/(1 + exp(−z)) is the sigmoid activation and ⊙ is the Hadamard (or componentwise) product; see Figure 4 for an illustration. The common dimension of the vectors defined in (9) is a hyperparameter of the layer. The input of this layer ˜xt := (xt−s, . . . , xt−1) can include predictors from the past to model longer dependencies, where s ∈ N determines the time horizon. The LSTM model has been simplified by Cho et al. (2014) into the gated recurrent unit (GRU) layer, which has become a popular alternative. A multi-layer recurrent network is obtained by considering the ht as a sequence of inputs for the following recurrent layers; see Figure S.1 in Supplementary Material S.1. Usually, a fully connected layer as in (7) is used to map the hidden state of the final recurrent layer to the network output ˜gW (˜xt). 3 Extreme Quantile Regression Neural Networks In this section, we propose a new methodology that combines the extrapolation power of the GPD model with the high-dimensional predictor space capabilities and flexibility of neural networks to obtain accurate estimates for quantile functions Qx(τ ) at extreme levels τ . Let D = {(xi, yi)}n i=1 be the training dataset. Estimation of conditional extreme quantiles ˆQx(τ ) using (6) requires estimators for the intermediate quantile function ˆQx(τ0) with τ0 < τ and the GPD parameter σ(x) and ξ(x). It is customary to proceed in two steps. 8 EXTREME QUANTILE REGRESSION NEURAL NETWORKS First, we model the intermediate quantile at level τ0 using classical quantile regression methods. We then define the conditional exceedances zi := yi − ˆQxi(τ0), i ∈ I := {i = 1, . . . , n : yi > ˆQxi(τ0)}. The intermediate probability τ0 should be chosen low enough to allow for stable estimation of Qx(τ0) with classical empirical methods, but high enough for the approximation in (5) to be accurate, so that the exceedances zi are approximate samples of a GPD. However, it is not a classical tuning parameter, since different values for τ0 yield different subsets of exceedances I. Comparison of the loss function (11) on these datasets would therefore not be meaningful. Instead, the threshold is, in the univariate case, usually selected in terms of stability plots and sensitivity analyses. In the second step, we estimate the GPD parameters σ(x) and ξ(x) based on the set of exceedances zi, i ∈ I. Modelling these parameters directly in the extrapolation formula (6) may lead to strong dependence between the estimates and numerical instabilities. We therefore rely on an orthogonal reparametrization that has a diagonal Fisher information matrix. As for the standard asymptotic GPD likelihood properties, the latter is well-defined for the GPD model when ξ(x) > −0.5, and the reparametrization (σ(x), ξ(x)) ↦→ (ν(x), ξ(x)), ν(x) := σ(x)(ξ(x) + 1), yields the desired orthogonality (Cox and Reid, 1987; Chavez-Demoulin and Davison, 2005). In our experiments, this reparametrization significantly improves stability and convergence in every considered setting. In this section, we propose a flexible neural network model for the orthogonalized GPD parameters ν(x; W) and ξ(x; W), where W denotes the collection of all model parameters. This can be seen as conditional density estimation with output dimension q = 2 where the parametric family is the GPD model with parameters θ = (ν, ξ) depending on the covariate X = x. In general, an estimate of the model parameters ˆW is thus obtained as a minimizer of the GPD deviance loss over the training exceedances ˆW ∈ arg min W ∑ i∈I ℓOGPD{zi; ˆν(xi; W), ˆξ(xi; W)}, (10) where the deviance or negative log-likelihood of the GPD in terms of the orthogonal reparametrization is ℓOGPD(z; ν, ξ) = ( 1 + 1 ξ ) log {1 + ξ (ξ + 1)z ν } + log(ν) − log(ξ + 1). (11) This yields a flexible model for the conditional tail distribution of Y | X = x with which not only ˆQx(τ ) can be regressed, but also conditional exceedance probabilities over a high threshold or conditional expected shortfalls, for example. In the next two subsections, we discuss the details of the model for independent observations and for time series data with sequential dependence, respectively. 3.1 Independent Observations We first consider the case where the training data D = {(xi, yi)}n i=1 is a set of independent, identically distributed observations of (X, Y ). The goal in this case is the estimation of the conditional quantile Qx(τ ) for a predictor value X = x at an extreme level τ > 0. For the first step of estimating the intermediate quantile function, in principle, any classical quantile regression method can be used. To avoid overfitting and obtain unbiased generalization error estimates from the training set D, the predicted ˆQxi(τ0), i = 1, . . . , n, should be constructed out of training sample. This is achievable in two ways. Using bagging methods such as generalized random forests (Athey et al., 2019) is a 9 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Algorithm 1 EQRN for independent observations The tuning parameters Θ for the conditional GPD density estimation network gW and the intermediate quantile model ˆQ·(τ0) capable of out-of-sample prediction are pre-specified. The training data D = {(xi, yi)}n i=1 and test covariates x are observed. Let τ ∈ (τ0, 1) be the desired probability level. 1: procedure EQRN-FIT(D, Θ, ˆQ·(τ0)) 2: I ← {i = 1, . . . , n : yi > ˆQxi(τ0)} 3: zi ← yi − ˆQxi(τ0) ∀i ∈ I 4: T , V ← RANDOMVALIDATIONSPLIT(I) ▷ If no validation: T = I, V = ∅ 5: ˆW ← INITIALIZENETWORKWEIGHTS(Θ) 6: for e = 1 to maximum number of epochs E do 7: for all B ∈ GETMINIBATCHES(T ) do 8: {(ˆνi, ˆξi)}i∈B ← g ˆW (xB, ˆQxB (τ0)) 9: ℓ ← ∑ i∈B ℓOGPD(zi, ˆνi, ˆξi)/ |B| 10: ˆW ← BACKPROPUPDATE(ℓ, ˆW, xB, ˆQxB (τ0), Θ) 11: stop if V ̸= ∅ and LOSSNOTIMPROVING( ˆW, xV , ˆQxV (τ0), zV ) 12: output ˆW 13: procedure EQRN-PREDICT(x, τ , ˆW, ˆQ·(τ0)) 14: {ˆν(x), ˆξ(x)} ← g ˆW (x, ˆQx(τ0)) 15: ˆσ(x) ← ˆν(x)/{ ˆξ(x) + 1} 16: compute ˆQx(τ ) w.r.t. ˆσ(x), ˆξ(x), ˆQx(τ0), τ and τ0 using equation (6) 17: output ˆQx(τ ), and optionally {ˆσ(x), ˆξ(x)} convenient choice since they allow for out-of-bag predictions where only a single fit on D is required. For other methods such as quantile regression neural networks (Cannon, 2011), out of training sample predictions can be obtained in a foldwise manner similar to cross-validation. In the sequel, we assume that the intermediate quantile, and thus the exceedances, are given. In the second step, we propose to model the orthogonalized GPD parameters ν(x) and ξ(x) by a fully-connected feed-forward neural network with parameter vector W and deviance loss function as in (10); see Section 2.3 for details. Choices of the network architecture such as the number of neurons, the number of layers and activation functions, are hyperparameters, denoted by Θ, to be selected. We provide sensible default values in our implementation, but one can also choose them in a data-driven way based on a validation set. The only restrictions are on the output activation functions, since ν(x) should be strictly positive. We find the exponential function or the SELU activation (Klambauer et al., 2017) shifted above zero to be good choices. Regarding the output activation for ξ, no strict restrictions apply and the identity would be a natural choice. However, standard likelihood regularity properties are not satisfied for the GPD model when ξ ≤ −0.5, which very rarely occurs in practice. We observe that smoothly restricting the shape estimates, for example between −0.5 and 0.7 with the activation x ↦→ 0.6 tanh (x) + 0.1, helps to improve training stability. This avoids aberrant ξ estimates in the early stages of the training and still covers almost all practical cases. In many situations, it is reasonable to assume that only the scale ν(x) varies locally but ξ(x) ≡ ξ is constant (e.g., Kinsvater et al., 2016). This can be achieved by restricting the network so that the shape output only depends on a bias term. Algorithm 1 summarizes our extreme quantile regression network (EQRN) for independent obser- vations, which takes the intermediate quantiles and training data as input and outputs the extreme quantile at a desired test predictor value x ∈ Rp and level τ > τ0. Optionally, the conditional GPD parameters can also be obtained. 10 EXTREME QUANTILE REGRESSION NEURAL NETWORKS The conditional GPD estimation in the second step relies on the exceedances to ensure that only information from the tail is used for extrapolation. There may however be residual information in the moderately extreme observations that should not be discarded. We propose to use the intermediate quantiles ˆQxi(τ0), as an additional feature in the conditional density estimation. This feature engi- neering seems to consistently and significantly improve the accuracy of the final prediction ˆQx(τ ) on test data in our simulations. This idea is to some degree related to stacked learning (Breiman, 1996; Wolpert, 1992) and is achieved by considering the (p + 1)-dimensional vector (xi, ˆQxi(τ0)), i = 1, . . . , n, instead of xi as predictors, for the network input. To avoid overfitting, it is again important that the intermediate quantile estimates are constructed out of training sample. This new feature also improves other extreme quantile regression methods, such as the GBEX model (Velthoen et al., 2023), as observed in our simulations in Section 4. Classical backpropagation and optimization procedures are performed to find ˆW. To avoid overfitting and local minima convergence, the validation loss can be tracked as discussed in Section 2.3. The hyperparameters Θ of this network include choices of optimization algorithms and regularization, for instance. More details on the function calls in the algorithm can be found in Supplementary Material S.2. Since the true quantile Qx(τ ) is unknown in real-world data, we can not assess the performance of ˆQx(τ ) with metrics such as mean squared or absolute errors. As illustrated in Section 2.1, the quantile loss is also unreliable due to the data scarcity at extreme quantiles. We therefore choose the final validation loss based on the GPD deviance to compare different choices of hyperparameters, as it is the most reliable surrogate metric. 3.2 Sequential Dependence In many applications, the observations are not independent but display sequential dependence, such as in time series. In this case, we denote the training data by D = {(xt, yt)}T t=1, which are observed sequentially from a time series {(Xt, Yt)}T t=1. The goal here is different from the case of independent observations. Indeed, we would like to predict as well as possible high quantiles of the response Yu at some time point u one step in the future based on all past information ˜Xu := {(Xt, Yt)}t<u. Therefore, the target is Q˜xu(τ ) := F −1 Yu| ˜Xu=˜xu(τ ), (12) where ˜xu := {(xt, yt)}t<u are observations that are not necessarily part of the training set. In this section, we propose a recurrent neural network to solve this task. Several principles are the same as in the case of independent observations, such as the choice of output activation functions. We thus focus on the differences to the independent case. While in principle it is still possible to use any classical quantile regression method to model the intermediate conditional quantiles at level τ0, we recommend using quantile regression neural net- works (Cannon, 2011; Zhang et al., 2019) with a recurrent architecture. These models are specifically designed for sequential dependence and can easily adapt to varying sequence lengths of the input fea- tures ˜Xt. In our experiments, recurrent quantile regression neural networks consistently outperformed generalized random forests in the presence of sequential dependence. Although a varying sequence length of input features is possible, we restrict this length to a fixed horizon s ≪ T , for computational efficiency. Thus, for any time point t we define its past by ˜xt = {(xj, yj)}t−1 j=t−s. For simplicity, we denote our augmented training set by ˜D = {(˜xt, yt)}T t=s+1. Algorithm 2 summarizes our EQRN for sequential data. For the estimation of the conditional GPD parameters, the main difference compared to the independent model is the use of a recurrent architecture to capture the sequential nature of the data. If validation splitting is used during training, the split should preserve the sequential structure instead of being performed randomly. For the use of the 11 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Algorithm 2 EQRN for sequential observations The tuning parameters Θ for the recurrent conditional GPD density estimation network ˜gW , the intermediate quantile model ˆQ·(τ0) capable of out of sample prediction and horizon s are pre-specified. The training data ˜D = {(˜xt, yt)}T t=s+1 and test covariates ˜xu are observed. Let τ ∈ (τ0, 1) be the desired probability level. 1: procedure EQRN-FIT( ˜D, τ0, s, Θ, ˆQ·(τ0)) 2: zt ← yt − ˆQ˜xt(τ0) ∀t ∈ I := {t = s + 1, . . . , T : yt > ˆQ˜xt(τ0)} 3: T , V ← SEQUENTIALVALIDATIONSPLIT(I) 4: ˆW ← INITIALIZERECURRENTNETWEIGHTS(Θ) 5: for e = 1 to maximum number of epochs E do 6: for all B ∈ GETMINIBATCHES(T ) do 7: {(ˆνt, ˆξt)}t∈B ← ˜g ˆW (˜xB, ˆQ˜xB (τ0)) 8: ℓ ← ∑ t∈B ℓOGPD(zt, ˆνt, ˆξt)/ |B| 9: ˆW ← BACKPROPUPDATE(ℓ, ˆW, ˜xB, ˆQ˜xB (τ0), Θ) 10: stop if V ̸= ∅ and LOSSNOTIMPROVING( ˆW, ˜xV , ˆQ˜xV (τ0), zV ) 11: output ˆW 12: procedure EQRN-PREDICT(˜xu, τ , ˆW, ˆQ·(τ0)) 13: {ˆν(˜xu), ˆξ(˜xu)} ← ˜g ˆW (˜xu, ˆQ˜xu(τ0)) 14: ˆσ(˜xu) ← ˆν(˜xu)/{ ˆξ(˜xu) + 1} 15: compute ˆQ˜xu(τ ) w.r.t. ˆσ(˜xu), ˆξ(˜xu), ˆQ˜xu(τ0), τ and τ0 using equation (6) 16: output ˆQ˜xu(τ ), and optionally {ˆσ(˜xu), ˆξ(˜xu)} intermediate quantile as a feature, two approaches seem relevant. The first one is to only use ˆQ˜xt(τ0) as a separate additional input to ˜xt in the network. The second approach is to also use past intermediate information by considering {(xj, yj, ˆQ˜xj (τ0))} t−1 j=t−s instead of ˜xt as input features to model the GPD parameters ν(˜xt) and ξ(˜xt). We prefer the second approach as it can pass more information to the tail model. More details on the function calls in the algorithm can be found in Supplementary Material S.2. The training data consists of time points {1, . . . , T }, while the test data uses information from time points {u − s, . . . , u − 1} to predict at time u. These two intervals are typically disjoint when the model was fitted in the past and is applied for prediction in the present. The prediction model can of course be used to predict on the training data when u ≤ T , but such predictions might be overly precise since yu was used in the training procedure. 4 Simulation Study 4.1 Setup In this section, we assess the accuracy of our EQRN model in predicting extreme conditional quantiles on simulated data and compare it to existing state-of-the-art methods. The aim is to study a simplified version of the application, which motivates the simulation setup and modelling choices. We thus focus on the case of sequentially dependent data; a simulation study for independent data can be found in Supplementary Material S.3. The main competitors from the extreme value literature in terms of flexibility are the generalized additive models (EGAM) (Youngman, 2019) and gradient boosting for extreme quantile regression (GBEX) (Velthoen et al., 2023), which both use conditional GPD modelling. For sequentially dependent data, we also consider the extreme quantile autoregression (EXQAR) (Li and Wang, 2019) as, although assuming linear quantile dependence, the model is designed for time series. As a benchmark, we 12 EXTREME QUANTILE REGRESSION NEURAL NETWORKS consider an unconditional GPD model that ignores the covariate dependence altogether, and a semi- conditional GPD model that uses the covariate only for the intermediate quantile. We include results for the generalized random forests for quantile regression (GRF) (Athey et al., 2019), which does not use extrapolation for high quantiles. The training data D = {(xt, yt)} T t=1, with T = 7,000, are sequentially generated from the time series    Yt = σt ∣ ∣εY t ∣ ∣ , Xt = 0.4 · Xt−1 + ∣ ∣εX t ∣ ∣ , εY t , εX t ∼ N (0, 1), σ2 t = 1 + 0.1 · {2Y 2 t−1 + Y 2 t−2 + Y 2 t−3 + Y 2 t−4 + Y 2 t−5}+ + 0.1 · {3X 2 t−1 + 2X 2 t−2 + X 2 t−3 + X 2 t−4 + X 2 t−5}. (13) Figure S.5 in Supplementary Material S.4 shows part of the simulated data. To have a fair comparison, all methods use the same covariate vectors ˜xt = {(xj, yj)} t−1 j=t−s with s = 10. This model admits a GPD approximation as in (5) since the conditional distribution Yt | ˜Xt = ˜xt is a folded normal distribution, and its tail can therefore be approximated by a GPD with shape parameter ξ(˜xt) = 0. For the methods that use covariate-dependent intermediate quantiles, we use the same estimates ˆQ˜xt(τ0) with τ0 = 80% from a recurrent quantile regression neural network (QRN); for a sensitivity analysis of the choice of τ0 see Supplementary Material S.4. The best QRN architecture and hyperparameters are chosen based on validation quantile loss. For the methods that use covariate-dependent GPD parameters, we also use ˆQ˜xt(τ0) as additional covariate. Although designed for univariate time series, we adapted EXQAR to accept several covariate sequences. Those two choices significantly improve the competitors’ performances. For the EQRN model, 2,000 observations are kept for validation tracking and thus only the remaining 5,000 are effectively used for weight training. The best choices for EQRN hyperparameters are made based on validation loss by performing a grid search over a set of possible values and network architectures. All other models are fitted on the whole training dataset, as they do not use validation loss tracking. The best set of hyperparameters for GBEX (tree depths, learning rate and number of trees) are chosen using cross-validation, and the ground truth for whether the shape is constant is given to EGAM. For EXQAR, we use δ2n = n−0.9 as recommended by the authors, but set δ1n = 1 − τ0 thus increasing the number of quantile pseudo-observations used for inference and allowing better comparison with the other methods. The predictions of all models are evaluated by their root mean squared error (RMSE) compared to the true conditional quantiles on a newly generated test dataset that follows the same distribution as the training data in (13). 4.2 Results For estimation of the conditional GPD parameters with our recurrent EQRN model, we consider both LSTM and GRU architectures with one to three recurrent layers and hidden dimensions between 32 and 256. As the networks are not too deep, L2 penalty was chosen over dropout for regularization during training, with possible penalty λ ∈ {0, 10−6, 10−5, 10−4, 10−3}. Both constant and covariate- dependent shape parameter outputs are considered. The model with minimum validation loss is a single LSTM layer with hidden dimension 128, followed by the usual fully connected output layer, with constant shape and λ = 10−4. As a comparison, we also retain predictions for the best unpenalized network with λ = 0 and fixed shape, which has two LSTM layers of hidden size 128. The left panel of Figure 5 shows the RMSE of best penalized and unpenalized EQRN models compared to the improved competitors as a function of the quantile level τ . We observe that for the lowest level τ = τ0 = 80%, all structured GPD models have the same performance since they use the same intermediate quantiles. GRF and the unconditional model have already higher errors since they are not able to capture the sequential dependence at the intermediate level sufficiently. For growing quantile levels τ , the errors of the covariate-dependent GPD models start to diverge. This is due to the differences in modelling flexibility in terms of the GPD parameters of each method. We observe a 13 EXTREME QUANTILE REGRESSION NEURAL NETWORKS 1 2 3 0.8 0.9 0.99 0.999 0.9999 Probability levelRMSE Uncond Semi−cond GRF EXQAR EGAM GBEX EQRN unpen. EQRN best 10 20 10 20 10 20 True quantilesPredicted quantiles Figure 5: Left: root mean squared error between predicted and true conditional quantiles at different probability levels τ (log-scale), for the selected EQRN models and the improved competitors. Centre-right: true versus predicted quantiles at probability level τ = 99.95% for the best unpenalized (middle) and penalized (right) EQRN models (dots), compared to the semi-conditional estimates (crosses). similar behaviour for EXQAR, as its linear quantile dependence is not flexible enough. Our EQRN method based on recurrent neural networks seems to be best at modelling sequential tail dependence. Figure 5 also shows the predicted quantiles ˆQ˜xu(τ ) on the test data compared to the true Q˜xu(τ ) for a fixed τ = 99.95% for the best penalized and unpenalized EQRN models. In general, both models seem to perform well in predicting the high conditional quantiles. The weight penalty seems to mainly affect the larger quantile predictions. Compared to the unpenalized model, we observe that the reduction in the variance of the predictions comes at the cost of a bias for larger quantile values. This bias-variance trade-off is typical with penalization. The poor performance of the semi-conditional estimates highlights the added value of covariate dependence in the GPD parameters. As discussed in Section 3, the choice of the intermediate level τ0 generally has an impact on the prediction accuracy. In our covariate-dependent setting, where the model for the conditional GPD is a flexible regression model, this choice seems to have less importance. Indeed, even for a fairly low value of τ0, the flexibility of the neural network model seems to be able to absorb some of the approximation bias; see Supplementary Material S.4 for details. Additional results on the quantile R squared coefficient and bias-variance decomposition of the RMSEs presented in Figure 5 are also discussed in Supplementary Material S.4. 5 Application 5.1 Motivation Flood risk is a major natural hazard in Europe, which causes huge economic damage and endangers human lives. There is a longstanding interest in statistical methods of extreme value theory for hydrology (e.g., Katz et al., 2002; Keef et al., 2009; Asadi et al., 2015; Engelke and Hitz, 2020), and national agencies commonly use them to assess the long-term risk of flooding in cities, at power plants and other key locations. Return levels with long return periods can be estimated using the GEV distribution for annual maxima or the GPD model for daily threshold exceedances. The output then guides effective long-term flood management measures. An example of an important location in Switzerland is the gauging station in Bern on the Aare river, which is shown within its water catchment in Figure 1. The Swiss Federal Office for the Environment (FOEN) monitors the Aare, and we use daily average discharges (in m3s−1) in Bern and another 14 EXTREME QUANTILE REGRESSION NEURAL NETWORKSBern [m3s−1]Gsteig [m3s−1]BEP [m3s−1] 1962 Jan 1963 Jan 1964 Jan 1965 Jan 1966 Jan 1967 Jan 100 200 300 0 25 50 75 0 20 40 60 Date Figure 6: Daily average discharge observations at Bern–Schönau (62) and at the upstream station at Gsteig (42), and daily precipitation at the closest meteorological station to Bern (BEP), over five years; see Figure 1 for geographical locations of the gauging stations. upstream station together with recordings of daily precipitation (in mm) at six locations in the Bern catchment; see Figure 1 for details. All time series are available in the period from 1930–2014 and can be obtained from the FOEN2 (for discharges) and MeteoSwiss3 (for precipitation). Figure 6 shows an excerpt for the two river stations and one precipitation gauge. To illustrate possible drawbacks of a classical extreme value analysis, the left panel of Figure 7 shows the annual maxima of river discharges at the Bernese station on the Aare. The dashed line is the estimated 100-year return level based on the GEV approximation using the training period from 1930–1958. One can see that starting from the year 1999 there are several exceedances over this return level, somewhat contradicting the fact that it should only be exceeded on average once in 100 years. The solid line is the same return level based on data from 1930–y, where y ∈ {1959, . . . , 2014} denotes the end of the training period. While the predictions are fairly stable until 1999, an extreme value analysis performed after that year would yield much higher values for the 100-year return level. Conversely, historical estimates before such a break-point would severely underestimate the flood risk. In general, distributional shifts can be due to climate change, changes in the river system or other structural breaks in factors influencing discharge at this location. For the Bernese station, the FOEN indeed reports a significant break-point in extreme discharges in the nineties but acknowledges that a clear cause can not be identified4. One factor may be a multi-decadal variability of flood occurrence, as described in Schmocker-Fackel and Naef (2010). The aim of our methodology is complementary to classical extreme value analysis and addresses this issue with static return levels. We apply our EQRN model to estimate one-day-ahead extreme quantiles of the river discharge conditionally on previous observations of discharge and precipitation in the catchment. This allows the forecasting of flood risk even in non-stationary systems, such as a changing climate. The strength of our approach lies in the ability to exploit information from multiple covariates and capture the complex time dependence. Even in situations where the causes for structural changes are unknown, our method implicitly accounts for them through their effects on the covariates. The 2https://www.hydrodaten.admin.ch/ 3https://gate.meteoswiss.ch/idaweb 4see flood report of the FOEN at https://www.hydrodaten.admin.ch/en/2135.html 15 EXTREME QUANTILE REGRESSION NEURAL NETWORKS 300 400 500 600 1950 1975 2000 YearDischarge [m3s−1] Ymax GEV evol GEV train 2.4 2.6 2.8 0 50 100 150 EpochValue 0 500 1000 1500 2000 0.9 0.99 0.999 0.9999 Quantile levelNumber of exceedences Figure 7: Left: annual maxima of daily average discharges (points) at Bern–Schönau (62) together with the unconditional 100-year return level based on GEV fitted on the training data of 1930–1958 (dashed line), and the evolution of the same return level (solid line) using data from 1930–y, where y ∈ {1959, . . . , 2014} denotes end of the period. Middle: evolution of the validation loss (solid line) of the selected EQRN network for the river discharge data as a function of the training epoch; the dashed line shows the validation loss of the semi-conditional model. Right: number of observations exceeding the EQRN quantile predictions on the test set (dotted line) compared to the expected number of exceedances (solid line) for different probability levels (log-scale). output of the model can help practitioners and authorities to manage flooding more effectively and help to minimize their disastrous impacts by early warning systems. To illustrate our methodology and show its effectiveness in comparison to classical forecasting approaches, we consider in more detail the flood in August 2005 in Switzerland. At the Bernese gauging station, it was the largest event since the beginning of the recordings, and it caused severe economic damage across large parts of the country and the loss of several lives. 5.2 Model Specification The whole dataset consists of 31,046 daily observations (xt, yt) between 1930–2014. The response yt is the daily average discharge at the Bernese gauging station on the Aare, and the covariates xt ∈ Rp, p = 7, consist of discharge at another upstream station and daily precipitation measurements from six locations in the same catchment; see Figures 1 and 6. The discharges show significant seasonality, both in trend and variance, with the largest extremes only appearing in the summer. We do not reduce artificially the non-stationarity via classical approaches from times series analysis (e.g., Cleveland et al., 1990), as we believe the seasonality and other trends are captured through the covariates. We split the data into training and test sets. The first T = 10, 349 observations in the period between 1930–1958 are used to train the models, whereof the first three-quarters serve the parameter estimation, and the remaining quarter is a validation set to determine hyperparameters (sets T and V in Algorithm 2, respectively). The test set contains 20,697 observations from 1958–2014, which is used for neither fitting nor selection of parameters, but only to evaluate the model performance on an independent time period. We choose this rather small proportion of training data to study the ability of the model to adapt to possible non-stationarity over time without refitting. In particular, the model weights are not updated with any information from data after 1958, even for forecasts in the study of the 2005 flood of interest. A large test set is also required to evaluate extreme properties of the data distribution. As augmented covariates at time t for the recurrent neural network models, we use the s = 10 preceding days and set ˜xt = {(xj, yj)} t−1 j=t−s. The augmented training set is then ˜D = {(˜xt, yt)}T t=s+1. Only observations of the p + 1 = 8 variables during the preceding ten days are used to predict one day ahead for a new test time point. 16 EXTREME QUANTILE REGRESSION NEURAL NETWORKS As discussed in Section 3, we perform two steps for the estimation of the conditional tail model. First, we fit an intermediate quantile regression model to estimate Q˜xt(τ0). As in the simulations with sequential dependence, we choose a recurrent QRN for this purpose and set τ0 = 0.8. For the second step, we include the intermediate quantile estimates ˆQ˜xj (τ0) during the same time horizon j = t − s, . . . , t − 1 as additional covariates, and, slightly abusing notation, we denote ˜xt as the new covariate vector; see Section 3.2. A recurrent EQRN is then fitted to the exceedances for estimation of the conditional GPD parameters σ(˜xt) and ξ(˜xt); see Algorithm 2. Similarly to the study in Section 4.2, a grid search is performed on the training data to select the best hyperparameters and architectures for both recurrent neural network models based on validation losses. The final model chosen to regress the intermediate quantiles is a QRN with two LSTM layers of dimension 256, followed by the usual fully connected layer, and L2 weight penalty with parameter λ = 10−6. The chosen EQRN model has two LSTM layers of dimension 16, followed by a fully connected layer, and L2 weight penalty with parameter λ = 10−6. 5.3 Results The middle panel of Figure 7 shows the validation loss (solid line) of the selected EQRN model as a function of the training epoch. It can be seen that already after a few epochs, the method has a lower loss than the simple semi-conditional model with constant GPD parameters σ and ξ (dashed line). This shows that the GPD distribution varies with the predictor values ˜xt and that a flexible model is beneficial. The main output of the EQRN model are the extreme quantile estimates ˆQ˜xu(τ ) for a level τ and a time point u of interest, conditionally on the past covariates ˜xu. These one-day-ahead risk forecasts are shown as a function of time on the test set in the top panel of Figure 2. We observe that the model is able to extrapolate beyond the range of the data since the event shown in the plot is unprecedented and the predictions still anticipate the first exceedance of the unconditional 100-year return level Q100. An unconditional τ -quantile is defined as the value that is exceeded by a proportion of 1 − τ of the data. An analogous property holds for conditional quantiles in data with sequential dependence, which yields a natural model assessment tool. On the population level, if ( ˜Xt, Yt)T t=1 is the random time series with augmented covariate vectors, then the expected number of exceedances over the true conditional τ -quantiles Q ˜Xt(τ ) is E T∑ t=1 1{Yt > Q ˜Xt(τ )} = (1 − τ )T. Consequently, plugging in the data (˜xt, yt)T t=1 and estimates ˆQ˜xt(τ ) from a quantile regression method, the equation should approximately hold if the model well calibrated. Such a model assessment plot is shown in the right-hand panel of Figure 7 for our EQRN fit as a function of the quantile level τ . We observe that the model is fairly well calibrated, with a slight bias towards more exceedances than expected. An additional output are the corresponding GPD parameters σ(˜xu) and ξ(˜xu), which together with the intermediate quantile ˆQ˜xu(τ0) specify the whole tail of the distribution of Yu | ˜Xu = ˜xu according to (5). For a given threshold level of interest Q we can plot the flood risk for the next day as the one-day-ahead forecast of the exceedance probability over Q, that is, an estimate of the function u ↦→ P(Yu > Q | ˜Xu = ˜xu). The bottom panel of Figure 2 shows the EQRN-based estimate of this function on the test set as a ratio to the unconditional P(Yu > Q), where the threshold Q is chosen as the static 100-year return level Q100 based on the GEV distribution fitted on the training set; this threshold is relevant since it is often used to determine the height of dams for flood management. It results in a daily measure of how likely the exceedance on the next day is compared to what was expected unconditionally. Times with large predicted probability ratios are apparently times of 17 EXTREME QUANTILE REGRESSION NEURAL NETWORKS imminent danger that can be used as triggers for early warning systems or additional flood management measures. As an example, one may issue a warning when the forecasted conditional probability of exceeding Q100 is, say, a hundred times larger than the baseline unconditional probability P(Yu > Q100). In the test data, there are four time clusters, that typically last several days, when Q100 is exceeded; see Figure 8 for one of these events. Applying this early warning system, in all four of these cases a timely warning would have been issued on the days preceding the first exceedance of the cluster. Such a decision rule would lead to an average of only 1.3 warnings for clusters of exceedances per year on the test set and is therefore not overly conservative. As the model does not need re-fitting on the test set, the daily forecast and possible warnings are obtained in less than one second of computation time, even on a CPU-only laptop computer5. The training time for the selected GPD network took less than 15 minutes. We consider the period of the 2005 flood in Bern in more detail. Figure 8 shows the two discharge time series and precipitation at the closest meteorological station before and after the event. On the evening of August 21, 2005, the day preceding the first exceedance of the 100-year GEV return level (horizontal dashed line), the prediction of our EQRN already indicated a sudden increase in the probability of this exceedance; see bottom panel of Figure 2. Equivalently, the blue point in Figure 8 shows the increased value of the conditional 100-year quantile predicted by the model. The diamonds mark the observations of the previous ten days that were used for this prediction. In this case, the high precipitation values on August 21 and the preceding days seem to have driven this prediction, possibly together with high values of the rivers. It is interesting to note that a similar situation on August 2 has not resulted in a “flood warning” since the forecasted exceedance probability and return level are not exceptionally high – as a matter of fact, there was no exceedance on the next day. This means that the predictions are driven by a complex combination of the risk factors ˜Xt in space and time that are well captured by the recurrent architecture of the EQRN. At the time, the FOEN used an adapted version of the hydrological model HBV (Lindström et al., 1997) for forecasting river discharges based on several inputs such as precipitation forecasts. However, the forecasts prior to this event underestimated the flood risk and resulted in too-late warnings. Physical models for discharges and precipitation do not use explicit extrapolation in the extreme tails and often have poor performance in the largest predictions. In fact, during the 2005 flood, a main reason for the late warnings was that forecasters did not trust the predicted precipitation amounts during this extreme scenario. In the aftermath of this flood, the FOEN therefore published a detailed analysis of the internal forecasting procedures (Bezzola and Hegg, 2007). The exact forecasts from that time are not available, but the above discussion of the results shows that our statistical methodology is a competitive alternative to physical models for the forecasting of flood risk. We also note that our model uses much less information as it relies only on observed discharge and precipitation and does not require forecasts of atmospheric variables. In Section S.5 of the Supplementary Material, we compare and discuss the forecasts of our EQRN method with those of some of the competitors. Figures S.8-S.11 show that all methods seem to capture at least some of the temporal structure based on the past covariates. Except for the GBEX method, the forecasts of the competitors suffer however from a low sensitivity to changes in the conditional tail or from a too erratic behaviour as a function of time. Overall, the recurrent structure of our EQRN method therefore seems to be the best suited model for this kind of sequentially dependent data. Since in real-world applications the true quantiles are unknown, direct computation of the prediction error is difficult and model assessment plots as in the right-hand panel of Figure 7 are crucial. This highlights the importance of simulation studies to evaluate and compare quantitatively the accuracy of different 5Intel Core i5-8265U 1.6 GHz processor with 4 cores, and 8 GB of RAM memory. 18 EXTREME QUANTILE REGRESSION NEURAL NETWORKSBern [m3s−1]Gsteig [m3s−1]BEP [m3s−1] 2005 Aug 01 2005 Aug 22 250 500 750 1000 50 100 150 200 0 10 20 30 40 50 Date Figure 8: Discharge at Bern–Schönau (62) and Gsteig (42) and precipitation at BEP during the period of the 2005 flood, where diamonds indicate covariates used for prediction of the 100-year conditional quantile (triangle) on August 22, 2005; other precipitation stations are not shown. The top panel also shows the unconditional 100-year return level (dashed line) fitted on the training data and predictions on other days (solid line). methods. In particular in situations with temporal dependence, our EQRN method clearly outperforms the competitors (e.g., Figure 5). This is another indicator to trust the EQRN forecasts in applications. 6 Conclusion Our EQRN model combines extrapolation results from extreme value theory with the prediction power of neural networks. It provides a flexible and versatile method for extreme quantile regression that is capable of prediction beyond the range of the data in the presence of a large number of covariates. The main focus in this paper was the case of sequential dependence to develop a tool for risk forecasting in time series that can be used for effective early warning systems in flood management. Our model already performs well in issuing sparse warnings for the days with increased risk of flooding, as illustrated in the case study of the Aare catchment in Switzerland. A further improvement could be attained by using additional covariates as input of the model. This could include observations of variables that are typically used in hydrological models such as soil moisture, or forecasts of atmospheric variables such as precipitation and temperature. Many other applications seem pertinent. Even in the case of independent data, our simulation study in Supplementary Material S.3 shows that neural networks outperform tree-based methods such as ERF and GBEX if the quantile function is more complex. Applications are financial risk assessment in insurance companies or banks. For spatial data, images or graphs, convolutional or graph neural networks (LeCun et al., 2015; Scarselli et al., 2009; Wu et al., 2021) are known to perform extremely well in capturing neighbourhood structures. Our EQRN method can therefore be applied to quantify the risk of climate extremes where the predictor space contains spatio-temporal observations of meteorological variables (e.g., Boulaguiem et al., 2022). Transformer architectures (Vaswani et al., 2017) can also perform well for spatio-temporal or more complex dependencies. The price for the high flexibility of machine learning methods, which focus on prediction accuracy, is limited statistical interpretability. However, feature-importance identification methods are becoming 19 EXTREME QUANTILE REGRESSION NEURAL NETWORKS increasingly popular for interpreting neural network predictions (Lundberg and Lee, 2017). There is also active research on the construction of prediction intervals for black-box methods, for instance through conformal inference (Lei et al., 2018; Romano et al., 2019). How such techniques can be adapted to assess uncertainty for extreme quantile regression is an interesting future research question. Acknowledgements The authors would like to thank Daniel Viviroli for his valuable insights. Funding Both authors were supported by the Swiss National Science Foundation Eccellenza Grant 186858. Supplementary Material Supplementary Results The Supplementary Material appended to this document contains additional information on Algorithms 1 and 2, the simulation study on independent data, additional results for the simulation study on dependent data, an analysis of the EQRN sensitivity to the intermediate probability level and competitor approaches to the application. Reproducibility and R Package An open-source “EQRN” R package implementation of the pro- posed methodology is available on https://github.com/opasche/EQRN. The code and data with detailed instructions to reproduce the results presented in this paper, and more, are available on https://github.com/opasche/EQRN_Results. References Andres, N., Steeb, N., Badoux, A., and Hegg, C., editors. Extremhochwasser an der Aare. Hauptbericht Projekt EXAR. Methodik und Resultate. [Extreme flooding of the Aare. Main Report on the EXAR Project. Methodology and results.], volume 104 of WSL Berichte, 2021. Asadi, P., Davison, A. C., and Engelke, S. Extremes on river networks. Ann. Appl. Stat., 9(4):2023–2050, 2015. Athey, S., Tibshirani, J., and Wager, S. Generalized random forests. Ann. Stat., 47(2):1148–1178, 2019. Balkema, A. A. and de Haan, L. Residual Life Time at Great Age. Ann. Probab., 2(5):792 – 804, 1974. Bezzola, G. R. and Hegg, C. Ereignisanalyse Hochwasser 2005, Teil 1 — Prozesse, Schäden und erste Einordnung [Event analysis of the 2005 flood, Part 1 — Processes, damage and initial classification]. Technical report, Federal Office for the Environment FOEN, Swiss Federal Institute for Forest, Snow and Landscape Research WSL, 2007. Umwelt-Wissen Nr. 0707. 215 S. Boulaguiem, Y., Zscheischler, J., Vignotto, E., van der Wiel, K., and Engelke, S. Modeling and simulating spatial extremes by combining extreme value theory with generative adversarial networks. Environ. Data Sci., 1:e5, 2022. Breiman, L. Stacked Regressions. Mach. Learn., 24:49–64, 1996. Bücher, A. and Zhou, C. A Horse Race between the Block Maxima Method and the Peak–over–Threshold Approach. Stat. Sci., 36(3):360–378, 2021. Cannon, A. J. Quantile regression neural networks: Implementation in R and application to precipitation downscaling. Comput. Geosci., 37(9):1277–1284, 2011. Cannon, A. J. Neural networks for probabilistic environmental prediction: Conditional Density Estimation Network Creation and Evaluation (CaDENCE) in R. Comput. Geosci., 41:126–135, 2012. Chavez-Demoulin, V. and Davison, A. C. Generalized additive modelling of sample extremes. J. R. Stat. Soc. C, 54(1):207–222, 2005. Chernozhukov, V. Extremal quantile regression. Ann. Stat., 33(2):806 – 839, 2005. Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Proc. Conf. Empir. Methods Nat. Lang. Process., pages 1724–1734, 2014. 20 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Cleveland, R. B., Cleveland, W. S., McRae, J. E., and Terpenning, I. STL: A Seasonal-Trend Decomposition Procedure Based on Loess. J. Off. Stat., 6(1):3–73, 1990. Cox, D. R. and Reid, N. Parameter Orthogonality and Approximate Conditional Inference (with discussion). J. R. Stat. Soc. B, 49:1–39, 1987. Daouia, A., Gardes, L., Girard, S., and Lekina, A. Kernel estimators of extreme level curves. TEST, 20(2): 311–333, 2011. Duchi, J., Hazan, E., and Singer, Y. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res., 12:2121–2159, 2011. Elman, J. L. Finding Structure in Time. Cogn. Sci., 14(2):179–211, 1990. Engelke, S. and Hitz, A. Graphical models for extremes (with discussion). J. R. Stat. Soc. B, 82:871–932, 2020. Fischer, E., Sippel, S., and Knutti, R. Increasing probability of record-shattering climate extremes. Nat. Clim. Chang., 11:689–695, 2021. Fisher, R. A. and Tippett, L. H. C. Limiting forms of the frequency distribution of the largest or smallest member of a sample. In Math. Proc. Camb. Philos. Soc., volume 24, pages 180–190. Cambridge University Press, 1928. Gardes, L. and Stupfler, G. An integrated functional Weissman estimator for conditional extreme quantiles. REVSTAT, 17(1):109–144, 2019. Gers, F. A., Schmidhuber, J., and Cummins, F. Learning to Forget: Continual Prediction with LSTM. Neural Comput., 12(10):2451–2471, 2000. Gers, F. A., Schraudolph, N. N., and Schmidhuber, J. Learning precise timing with LSTM recurrent networks. J. Mach. Learn. Res., 3(Aug):115–143, 2003. Gnecco, N., Terefe, E. M., and Engelke, S. Extremal Random Forests, 2022. ArXiv:2201.12865. Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. MIT Press, 2016. Halton, J. H. Algorithm 247: Radical-inverse quasi-random point sequence. Commun. ACM, 7(12):701–702, dec 1964. Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural Comput., 9(8):1735–1780, 1997. Hornik, K. Approximation capabilities of multilayer feedforward networks. Neural Netw., 4(2):251–257, 1991. Jozefowicz, R., Zaremba, W., and Sutskever, I. An Empirical Exploration of Recurrent Network Architectures. In Proc. 32nd Int. Conf. Mach. Learn., volume 37, pages 2342–2350, 2015. Katz, R. W., Parlange, M. B., and Naveau, P. Statistics of extremes in hydrology. Adv. Water Resour., 25: 1287–1304, 2002. Keef, C., Tawn, J., and Svensson, C. Spatial risk assessment for extreme river flows. J. R. Stat. Soc. C, 58: 601–618, 2009. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. 3rd Int. Conf. Learn. Repres., 2014. Kinsvater, P., Fried, R., and Lilienthal, J. Regional extreme value index estimation and a test of tail homogeneity. Environmetrics, 27(2):103–115, 2016. Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-Normalizing Neural Networks. In Proc. 31st Int. Conf. Neural Inf. Process. Syst., pages 972–981, 2017. Koenker, R. and Bassett, G. Regression Quantiles. Econometrica, 46(1):33–50, 1978. Koh, J. Gradient boosting with extreme-value theory for wildfire prediction. Extremes, 26:273–299, 2023. LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 521:436–444, 2015. Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L. Distribution-Free Predictive Inference for Regression. J. Am. Stat. Assoc., 113(523):1094–1111, 2018. Li, D. and Wang, H. J. Extreme Quantile Estimation for Autoregressive Models. J. Bus. Econ. Stat., 37(4): 661–670, 2019. Lindström, G., Johansson, B., Persson, M., Gardelin, M., and Bergström, S. Development and test of the distributed HBV-96 hydrological model. J. Hydrol., 201:272–288, 1997. 21 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Lundberg, S. M. and Lee, S.-I. A Unified Approach to Interpreting Model Predictions. In Adv. Neural Inf. Process. Syst., volume 30, 2017. Meinshausen, N. Quantile Regression Forests. J. Mach. Learn. Res., 7(35):983–999, 2006. Pickands III, J. Statistical Inference Using Extreme Order Statistics. Ann. Stat., 3(1):119 – 131, 1975. Romano, Y., Patterson, E., and Candes, E. Conformalized Quantile Regression. In Adv. Neural Inf. Process. Syst., volume 32, 2019. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The Graph Neural Network Model. IEEE Trans. Neural Netw., 20(1):61–80, 2009. Schmocker-Fackel, P. and Naef, F. Changes in flood frequencies in Switzerland since 1500. Hydrol. Earth Syst. Sci., 14(8):1581–1594, 2010. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a Simple Way to Prevent Neural Networks from Overfitting. J. Mach. Learn. Res., 15:1929–1958, 2014. Tieleman, T. and Hinton, G. Lecture 6.5 – RMSProp. Technical report, COURSERA: Neural Networks for Machine Learning, 2012. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is All you Need. In Adv. Neural Inf. Process. Syst., volume 30, 2017. Velthoen, J., Cai, J.-J., Jongbloed, G., and Schmeits, M. Improving precipitation forecasts using extreme quantile regression. Extremes, 22(4):599–622, 2019. Velthoen, J., Dombry, C., Cai, J.-J., and Engelke, S. Gradient boosting for extreme quantile regression. Extremes, 2023. Wang, H. J., Li, D., and He, X. Estimation of High Conditional Quantiles for Heavy-Tailed Distributions. J. Am. Stat. Assoc., 107(500):1453–1464, 2012. Werbos, P. J. Generalization of backpropagation with application to a recurrent gas market model. Neural Netw., 1(4):339–356, 1988. Wolpert, D. H. Stacked generalization. Neural Netw., 5(2):241–259, 1992. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S. A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Netw. Learn. Syst., 32(1):4–24, 2021. Youngman, B. D. Generalized Additive Models for Exceedances of High Thresholds With an Application to Return Level Estimation for U.S. Wind Gusts. J. Am. Stat. Assoc., 114(528):1865–1879, 2019. Zhang, W., Quan, H., and Srinivasan, D. An Improved Quantile Regression Neural Network for Probabilistic Load Forecasting. IEEE Trans. Smart Grid, 10(4):4425–4434, 2019. 22 EXTREME QUANTILE REGRESSION NEURAL NETWORKS SUPPLEMENTARY MATERIAL TO “Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk” S.1 Additional LSTM Illustration Figure S.1 shows a schematic representation of a multi-layer LSTM network. S.2 Details on Algorithms 1 and 2 Algorithms 1 and 2 contain some abbreviated function calls. We give some details here: RANDOMVALIDATIONSPLIT(I): For independent data, splits the index set I randomly into training set T and validation set V with pre-specified proportions. SEQUENTIALVALIDATIONSPLIT(I): For sequential data, splits the index set I sequentially into training set T and validation set V with pre-specified proportions, such that all observations in T are before V in time. INITIALIZENETWORKWEIGHTS(Θ) / INITIALIZERECURRENTNETWEIGHTS(Θ): Initializes the weights of the GPD (recurrent) neural network randomly. The number of weights is determined by Θ. GETMINIBATCHES(T ): Splits the training set T into mini-batches for stochastic gradient descent. BACKPROPUPDATE(ℓ, ˆW, xB, ˆQxB (τ0), Θ): Updates the parameter vector by a gradient step computed by backpropagation; may involve regularization methods such as L2-penalty or dropout, specified in the hyperparameters Θ. LOSSNOTIMPROVING( ˆW, xV , ˆQxV (τ0), zV ): If validation loss is tracked, then also a stopping criterion (e.g., for early stopping) is specified, and this function returns TRUE if this criterion is attained, indicating that the validation loss is not improving any more. S.3 Simulation Study for Independent Observations Three data-generating models with independent observations are considered. The training sample D = {(xi, yi)}n i=1 is drawn from { X ∼ U ([−1, 1]p) , Y | X = x ∼ σ(x) · tα(x), (S.1) with p = 10, α(x) = 1/ξ(x) := 7 · {1 + exp(4x1 + 1.2)}−1 + 3, and three different models for σ(x): Model 1: σ(x) := 1 + 6ϕ(x1, x2), where ϕ is the bivariate Gaussian density with correlation 0.9, Model 2: σ(x) := 4 + 3 cos(7 ∥ ∥(x1, x2)⊤∥ ∥ 2 + 3), Model 3: σ(x) := 4 + 3 cos(6 ∥x∥2 + 3.5). 23 EXTREME QUANTILE REGRESSION NEURAL NETWORKS xt−1xt−2xt−3xt−s LSTM1LSTM1LSTM1LSTM1 h (1) t−1h(1) t−2h(1) t−3 c (1) t−1c(1) t−2c(1) t−3 LSTMLLSTMLLSTMLLSTML h (L) t−1h(L) t−2h(L) t−3 c (L) t−1c(L) t−2c(L) t−3 ˜gW (˜xt)yt ℓ Figure S.1: Multi-layer LSTM network flowchart from input ˜xt := (xt−s, . . . , xt−1) to output ˜gW (˜xt), with loss evaluation. The LSTM cells represent the transformation in (9). To avoid bias in the selection of the data models, α(x) and σ(x) in Model 1 are the same as in the simulation study in Velthoen et al. (2023). The choices for Models 2 and 3 are designed to study more complex covariate dependencies. The constants are chosen to have positive scale values and enough variation for the inference task to be interesting. As the focus of the experiments is on the extremal part of the model and since all extreme value competitors use the same intermediate quantile estimates, we use the true Qx(τ0) as intermediate quantiles. Two main types of network architectures are considered for EQRN. The first type is an MLP with tanh activation functions, narrow architectures with between one and four hidden layers and optional L2 weight penalty as training regularization. The second type is self-normalizing networks (Klambauer et al., 2017) using SELU activation functions, deeper architectures with between three and eight hidden layers and optional alpha dropout as regularization. This type of network is designed to maintain unit variance and zero mean across layers in deeper networks trained for regression tasks, in order to avoid vanishing and exploding gradient issues. Results suggest that the additional flexibility of the second type was not necessary for the three tasks at hand, as they never yielded better validation scores than the best tanh models. Table S.1 summarizes the hyperparameters of the chosen EQRN networks. To evaluate the accuracy of the best models over the full feature space X = [−1, 1]p, we use the integrated squared error (ISE) between the prediction ˆQx(τ ) and the true quantile Qx(τ ), ∫ X ( ˆQx(τ ) − Qx(τ ) )2 dx. (S.2) We generate test features using a Halton sequence (Halton, 1964) and compute the MSE between the corresponding predicted and true response quantiles, to estimate the p-dimensional integral. Figure S.2 shows the accuracy of EQRN and the competitor methods for an increasingly large τ , for the three data models. The competitors’ performances shown here were also significantly improved by using the intermediate ˆQx(τ0) as an additional covariate. For every model, EQRN outperforms all competitors, with a difference in accuracy increasing with τ . EGAM seems to suffer from the large dimension of the feature space at large τ , both when the actual quantile depends on only two or all of the ten features. The difference in accuracy of EQRN and GBEX compared to the unconditional 24 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Table S.1: Hyperparameters of the EQRN networks with the best validation loss for the three independent data models. Hidden activation function Hidden layer dimensions L2 penalty Model 1 tanh (128, 128, 128) 10−5 Model 2 tanh (20, 10, 10) 10−5 Model 3 tanh (10, 10, 10) 0 0 5 10 15 0.8 0.9 0.99 0.999 0.9999RISE 0 10 20 30 40 50 0.8 0.9 0.99 0.999 0.9999 Probability level EQRN 0 20 40 60 0.8 0.9 0.99 0.999 0.9999 Uncond Semi−cond GRF EGAM GBEX Figure S.2: Root integrated squared error between predicted and true conditional quantiles at different probability levels τ (log-scale) for the selected EQRN model and the improved competitors, for data Models 1–3 (left to right). The cropped-out RMISE for EGAM at level 0.9999 are around 43, 115 and 150, respectively. and semi-conditional models is particularly significant for Models 1 and 3, and EQRN generally outperforms GBEX, especially at high probability levels. We also define the quantile R squared of ˆQx(τ ) over the sample D as R2 τ := 1 − ∑n i=1 (Qxi(τ ) − ˆQxi(τ ))2 ∑n i=1 (Qxi(τ ) − QD(τ ) )2 , with QD(τ ) := 1 n n∑ i=1 Qxi(τ ). (S.3) The definition is similar to the classical R squared coefficient of determination in regression, but the true conditional quantile values are used as targets instead of the response observations. The R2 τ is essentially the reversed MSE normalized by the variance of the true conditional quantile. A value close to unity indicates a very low MSE compared to the quantile variance, and negative values indicate a MSE larger than the quantile variance. Figure S.3 shows the quantile R squared, the biases and the residual standard deviations of the same respective quantile predictions compared to the truth. The R squared lead to the same conclusions as the RISE. Regarding the bias-variance decomposition of the RISE, it seems that the variance term is dominating the square bias. Although EQRN is here not the least biased model for large τ values, it is its dominating performance in terms of residual variance that leads to it having the lowest RISE values for every data model. Figure S.4 shows the predicted ˆQx(0.9995) for EQRN and the competitors, as a function of the two significant covariates (x1, x2) for Model 1. At that extreme level, EQRN still seems to capture the true conditional quantile function quite well, although the predictions show some residual noise. GBEX shows an elliptical stepwise approximation behaviour. and seems to underestimate the smaller quantiles and overestimate the largest quantiles. EGAM and GRF fail drastically at recovering the conditional quantile function. The semi-conditional estimates are a translation of the intermediate quantiles, which in particular fail to capture the varying shape along X1. 25 EXTREME QUANTILE REGRESSION NEURAL NETWORKS −1.0 −0.5 0.0 0.5 1.0 0.8 0.9 0.99 0.999 0.9999Quantile R squared −3 0 3 0.8 0.9 0.99 0.999 0.9999Bias 0 5 10 15 0.8 0.9 0.99 0.999 0.9999Residual standard deviation −1.0 −0.5 0.0 0.5 1.0 0.8 0.9 0.99 0.999 0.9999Quantile R squared −40 −20 0 20 40 0.8 0.9 0.99 0.999 0.9999Bias 0 10 20 30 40 50 0.8 0.9 0.99 0.999 0.9999Residual standard deviation −2 −1 0 1 0.8 0.9 0.99 0.999 0.9999Quantile R squared −20 −10 0 10 20 0.8 0.9 0.99 0.999 0.9999 Probability levelBias 0 20 40 60 0.8 0.9 0.99 0.999 0.9999Residual standard deviation EQRNUncond Semi−cond GRF EGAM GBEX Figure S.3: Quantile R squared, bias and residual standard deviation of the predicted quantiles compared to the truth at different probability levels (log-scale) for the selected EQRN model and improved competitors, for data Models 1–3 (top to bottom). S.4 Simulation Study for Sequentially Dependent Data The main results of the simulation study on sequentially dependent data are presented in the main paper. This section discusses additional results. Figure S.5 shows part of the sequential data simulated from the generating process described in the main paper. Figure S.6 shows the quantile R squared (S.3), the biases and the residual standard deviations of the quantile predictions compared to the truth, for the two selected EQRN models and competitors. The R squared evolution again shows EQRN is the model that best captures the covariate sequential dependence in the tail, as it outperforms all competitors with a difference in accuracy increasing with τ . In terms of bias, the penalized EQRN here scores similar values as EXQAR, and both EQRN versions outperform all other methods. The unpenalized EQRN has the lowest residual variance, closely followed by both GBEX and the penalized EQRN, although GBEX has a bad accuracy overall, due to its large bias. Figure S.7 shows the impact of the intermediate level τ0 on the accuracy of the best EQRN model. The value τ0 = 0.8 used in the rest of the analysis leads to the best RMSE. This relatively low value 26 EXTREME QUANTILE REGRESSION NEURAL NETWORKS Figure S.4: Conditional quantile predictions of EQRN and the improved competitor models at probability level τ = 0.9995, shown as a function of X1 and X2, for Model 1.YX 0 250 500 750 1000 0 3 6 9 1 2 3 Figure S.5: First 1,000 observations of the sequential data simulated from (13). shows that τ0 can be chosen much lower than for the classical unconditional GPD model. An intuitive explanation for this fact is the following. In a situation without covariates, the choice of τ0 is a trade-off between approximation bias (which favours larger thresholds) and variance (which favours lower thresholds); see Figure 3 in the main document. For covariate-dependent data, the distribution of the exceedances varies and more data is needed to accurately capture this function of the covariates. The variance becomes more important than the approximation bias, and therefore lower thresholds are preferable. Moreover, the flexibility of the GPD regression neural network model seems to be able to absorb some of the approximation bias, also allowing for a low value of τ0. The final accuracy seems in fact to not be too sensitive to the choice for τ0 compared to the network’s grid-searched hyperparameters, discussed in the main analysis, as the differences in RMSE for τ0 values close to the optimum are relatively small. As mentioned in Section 3 of the main paper, τ0 cannot be treated as a classical tuning parameter, as different values for τ0 generally yield different 27 EXTREME QUANTILE REGRESSION NEURAL NETWORKS −0.5 0.0 0.5 1.0 0.8 0.9 0.99 0.999 0.9999Quantile R squared −2 −1 0 1 2 0.8 0.9 0.99 0.999 0.9999 Probability levelBias 1 2 0.8 0.9 0.99 0.999 0.9999Residual standard deviation Uncond Semi−cond GRF EXQAR EGAM GBEX EQRN unpen. EQRN best Figure S.6: Quantile R squared, bias and residual standard deviation of the predicted quantiles compared to the truth at different probability levels (log-scale) for the selected EQRN model and the improved competitors, for the sequential data model. 0.7 0.75 0.8 0.85 0.9 0.95 0.7 0.75 0.8 0.85 0.9 0.95 0.7 0.75 0.8 0.85 0.9 0.95 0 5 10 15 0 5 10 0 3 6 9 Intermediate probability levelRMSE Figure S.7: Boxplot of the absolute residuals of the quantile predictions from the selected EQRN model (blue) and their RMSE (red diamond) at probability levels 0.995 (left), 0.999 (middle) and 0.9995 (right) for different choices of intermediate probability level τ0, for the sequential data model. subsets of exceedances. Thus, the likelihood (11), which is used as a goodness of fit metric for hyperparameter tuning, would not be comparable between models. The RMSE cannot be computed in practice either, since Qx(τ ) is generally unknown. S.5 Application: Competitor Results The main results from our application to forecasting flood risk in Switzerland using our proposed EQRN methodology are presented in the main paper. This section discusses and compares additional results using the competitor methods, adapted to provide the same type of forecast as the EQRN approach, with a focus on the 2005 flood event (see Figure 2 in the main paper). We first observe that the predictions have roughly the same behaviour, which shows that all methods capture at least some of the temporal structure based on the past covariates. The semi-conditional method (Figure S.8) is clearly not flexible enough, since the predictions only show a weak sensitivity to the changes in covariates. It also fails to trigger any early warning during the main event, due to low probability ratio forecasts never exceeding the selected threshold value of 100. The reason is that, while the intermediate quantile is covariate-dependent, the GPD parameters are constant over time. We conclude that a covariate-dependent model for the tail is required in this application. Figure S.9 shows predictions from the EXQAR model (Li and Wang, 2019). This model is more sensitive to changes 28 EXTREME QUANTILE REGRESSION NEURAL NETWORKSDischarge [m3s−1]Probability ratio 2005 Jul 2005 Aug 2005 Sep 2005 Oct 100 200 300 400 500 600 5 10 15 Date Figure S.8: Top: Daily average discharge (points) at the Bern–Schönau station (62) and one-day-ahead semi- conditional forecasts of conditional 100-year quantiles (solid line) during the 2005 flood. Horizontal lines show unconditional Q 100 based on GEV (dashed) and GPD (dotted). Bottom: one-day-ahead forecast of the conditional probability of exceeding the GEV estimated Q 100 as a ratio to the unconditional probability, using the semi-conditional parameter forecast. The vertical line indicates August 22, the day of the first exceedance.Discharge [m3s−1]Probability ratio 2005 Jul 2005 Aug 2005 Sep 2005 Oct 500 1000 1500 0 2000 4000 6000 Date Figure S.9: Top: Daily average discharge (points) at the Bern–Schönau station (62) and one-day-ahead EXQAR forecasts of conditional 100-year quantiles (solid line) during the 2005 flood. Horizontal lines show unconditional Q 100 based on GEV (dashed) and GPD (dotted). Bottom: one-day-ahead forecast of the conditional probability of exceeding the GEV estimated Q 100 as a ratio to the unconditional probability, using the EXQAR forecast. The vertical line indicates August 22, the day of the first exceedance. in the covariates, but the regression function looks fairly erratic, with sudden spikes at some time points. Those spikes are here due to unusually small shape estimates in combination with a large-scale estimate. This might be caused by an instability in the estimation of the local moments used in the model. EGAM (Figure S.10) fails to trigger an early warning for the first day of the flooding event as its quantile and probability ratio forecasts are very low, but then seems to severely overestimate the river flow during the rest of the event. The best competing model seems to be the GBEX (Figure S.11), as it yields a smooth prediction curve with a pronounced spike at the main event. Comparing this with our EQRN method (Figure 2 in the main paper), it reacts slightly later and its risk forecast (probability ratio) on the day before the first exceedance is significantly lower than for EQRN. This qualitative way of checking the model is important since in real-world applications, the true extreme quantiles are unknown. Therefore, only some quantitative model checks can be performed, like the right-hand panel of Figure 7 in the main paper showing calibration in the number of quantile- 29 EXTREME QUANTILE REGRESSION NEURAL NETWORKSDischarge [m3s−1]Probability ratio 2005 Jul 2005 Aug 2005 Sep 2005 Oct 0e+00 2e+05 4e+05 6e+05 0 2000 4000 6000 Date Figure S.10: Top: Daily average discharge (points) at the Bern–Schönau station (62) and one-day-ahead EGAM forecasts of conditional 100-year quantiles (solid line) during the 2005 flood. Horizontal lines show unconditional Q 100 based on GEV (dashed) and GPD (dotted). Bottom: one-day-ahead forecast of the conditional probability of exceeding the GEV estimated Q 100 as a ratio to the unconditional probability, using the EGAM parameter forecast. The vertical line indicates August 22, the day of the first exceedance.Discharge [m3s−1]Probability ratio 2005 Jul 2005 Aug 2005 Sep 2005 Oct 400 800 1200 0 500 1000 1500 2000 Date Figure S.11: Top: Daily average discharge (points) at the Bern–Schönau station (62) and one-day-ahead GBEX forecasts of conditional 100-year quantiles (solid line) during the 2005 flood. Horizontal lines show unconditional Q 100 based on GEV (dashed) and GPD (dotted). Bottom: one-day-ahead forecast of the conditional probability of exceeding the GEV estimated Q 100 as a ratio to the unconditional probability, using the GBEX parameter forecast. The vertical line indicates August 22, the day of the first exceedance. exceeding test observations. Figure S.12 compares this number of quantile-exceedances for the competitor predictions. Although they can give evidence against the suitability of a model, it highlights that such metrics only assess calibration but not goodness-of-fit nor accuracy, as unconditional methods obtain similar values to flexible accurate methods. This underlines the importance of the simulation studies, which allow us to evaluate in several settings which methods are more accurate. In particular, in situations with temporal dependence, our EQRN method outperforms the competitors (e.g., Figure 5 in the main paper). 30 EXTREME QUANTILE REGRESSION NEURAL NETWORKS−1200−800−40004000.90.990.9990.9999Quantile levelDifference in e xceedences from e xpectedUncondSemi−condGRFEXQAREGAMGBEXEQRN Figure S.12: Difference between the number of observations exceeding the EQRN and competitor quantile predictions on the test set and the expected number of exceedances, for different probability levels (log-scale). 31","libVersion":"0.3.2","langs":""}
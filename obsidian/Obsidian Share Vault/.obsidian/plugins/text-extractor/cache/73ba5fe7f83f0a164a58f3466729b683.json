{"path":"lit/lit_sources.backup/Nguyen21TempLtntAutoEncMultiFrcst.pdf","text":"Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate Time Series Forecasting Nam Nguyen*, Brian Quanz* IBM Research {nnguyen,blquanz}@us.ibm.com Abstract Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of com- putational burden and distribution modeling. Most previous work either makes simple distribution assumptions or aban- dons modeling cross-series correlations. A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when us- ing deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factor- ization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By impos- ing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder. Extensive experiments demonstrate that our model achieves state-of-the- art performance on many popular multivariate datasets, with gains sometimes as high as 50% for several standard metrics. 1 Introduction Forecasting - predicting future values of time series, is a key component in many industries (Fildes et al. 2008). Applica- tions include forecasting supply chain and airline demand (Fildes et al. 2008; Seeger, Salinas, and Flunkert 2016), ﬁ- nancial prices (Kim 2003), and energy, trafﬁc or weather patterns (Chatﬁeld 2000). Forecasts are often required for large numbers of related time series, i.e., multivariate time se- ries forecasting, as opposed to univariate (single time series) forecasting. For example, retailers may require sales/demand forecasts for millions of different products at thousands of different locations - amounting to billions of sales time series. In multivariate settings, one common approach is to ﬁt a single multi-output model to predict all series simultaneously. This includes statistical methods like vector auto-regressive (VAR) models (L¨utkepohl 2005) and generalizations (e.g., MGARCH (Bauwens, Laurent, and Rombouts 2006)), and multivariate state-space models (Durbin and Koopman 2012), as well as deep neural net (DNN) models including recurrent neural networks (RNNs) (Funahashi and Nakamura 1993), temporal convolutional neural networks (TCNs) (Bai, Kolter, and Koltun 2018), and combinations (Lai et al. 2018; Goel, *Nam Nguyen and Brian Quanz contributed equally to this work Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. Melnyk, and Banerjee 2017; Borovykh, Bohte, and Oosterlee 2017; Cheng, Huang, and Zheng 2020; Dasgupta and Os- ogami 2017; Cirstea et al. 2018; Rodrigues and Pereira 2020). However, these are prone to overﬁtting and not scalable as the number of time series increases (Yu, Rao, and Dhillon 2016; Sen, Yu, and Dhillon 2019; Salinas et al. 2019). As such, another popular approach is to abandon multi- variate forecasting entirely and perform univariate forecast- ing (i.e., ﬁt a separate model per series). Classical statisti- cal forecasting methods using simple parametric models of past values and forecasts are still arguably most commonly used in industry, such as auto-regressive AR and ARIMA models (Hyndman and Athanasopoulos 2018), exponential smoothing (ES) (McKenzie 1984), and more general state- space models (Hyndman et al. 2008). Such methods have consistently out-performed machine learning methods such as RNNs in large scale forecasting competitions until recently (Makridakis, Hyndman, and Petropoulos 2020; Makridakis, Spiliotis, and Assimakopoulos 2018, 2020; Crone, Hibon, and Nikolopoulos 2011; Benidis et al. 2020). A key reason for recent success of deep learning for forecasting is multi- task univariate forecasting - sharing deep learning model pa- rameters across all series, possibly with some series-speciﬁc scaling factors or parametric model components (Salinas, Flunkert, and Gasthaus 2019; Smyl 2020; Bandara, Bergmeir, and Hewamalage 2020; Li et al. 2019; Wen et al. 2017; Ran- gapuram et al. 2018; Chen et al. 2018). E.g., the winner of the M4 forecasting competition (Makridakis, Spiliotis, and Assimakopoulos 2020) was a hybrid ES-RNN model (Smyl 2020), in which a single shared univariate RNN model is used to forecast each series but seasonal and level ES parameters are simultaneously learned per series to normalize them. However, a fundamental limitation of multi-task univariate forecasting approaches is they are unable to model cross- series correlations/effects (Rangapuram et al. 2018; Salinas et al. 2019), common in many domains (Salinas et al. 2019; Tsay 2013; Rasul et al. 2020). For example, in retail, cross- product effects (e.g., increased sales of one product causing increased/decreased sales of related products) are well known (Gelper, Wilms, and Croux 2016; Leeﬂang et al. 2008; Srini- vasan, Ramakrishnan, and Grasman 2005). In ﬁnancial time series one stock price may depend on relative prices of other stocks; and energy time series may have spatial correlations and dependencies. Furthermore, these approaches cannotarXiv:2101.10460v1 [cs.LG] 25 Jan 2021 leverage the extra information provided from related series in case of noise or sparsity. E.g., sales are often sparse (e.g., one sale a month for a particular product and store), so the sales rate cannot be accurately estimated from a single series. A promising line of research we focus on that addresses limitations of both the single, large multi-output multivariate model and the multi-task univariate model approaches is to use factorization (Yu, Rao, and Dhillon 2016; Sen, Yu, and Dhillon 2019). Relationships between time series are factor- ized into a low rank matrix, i.e., each time series is modeled as a linear combination of a smaller set of latent, basis (or global) time series, so forecasting can be performed in the low-dimensional latent space then mapped back to the input (local) space. Thus modeling can scale to very large number of series while still capturing cross-series relationships. Tem- poral regularized matrix factorization (TRMF) (Yu, Rao, and Dhillon 2016) imposes temporal regularization on the latent time series so they are predictable by linear auto-regressive models. Recently, DeepGLO (Sen, Yu, and Dhillon 2019) extended this approach to enable non-linear latent space fore- cast models. DeepGLO iteratively alternates between linear matrix factorization and ﬁtting a latent space TCN; forecasts from this model are then fed as covariates to a separately trained multi-task univariate TCN model. However, these have several key limitations. First, they cannot capture nonlinear relationships between series via the transformation, which are common in many domains. E.g., products’ sales or stocks’ prices may depend on relative price compared to others (i.e., value ratios, a non-linear relation- ship). Second, although deepGLO introduces deep learning, it is not an end-to-end model. Since factorization is done separately and heuristic, alternating optimization with no convergence guarantees is used, the process is inefﬁcient and may not ﬁnd an optimal solution. Third, they have no way to provide probabilistic outputs (i.e., predictive distributions), which are critical for practical use of forecasts (Makridakis, Hyndman, and Petropoulos 2020). Fourth, they are limited to capturing stationary relationships between time series with the ﬁxed linear transform on a single time point - whereas relationships between series are likely often nonstationary. To address these limitations and extend the factorization line of research, we propose the Temporal Latent Autoen- coder (TLAE) (see Figure 1), which enables non-linear trans- forms of the input series trained end-to-end with a DNN temporal latent model to enforce predictable latent temporal patterns, and implicitly infers the joint predictive distribution simultaneously. Our main contributions are: • We enable nonlinear factorization for the latent temporal factorization line of research; we generalize the linear map- pings to/from the latent space to nonlinear transforms by replacing them with encoder and decoder neural networks, with an input-output reproduction objective, i.e., an autoen- coder (Kramer 1991; Hinton and Zemel 1994). Further, the autoencoder can use temporal models (e.g., RNNs) - so embeddings can evolve over time (be nonstationary). • We introduce temporal regularization in the latent space with ﬂexible deep learning temporal models that can be trained end-to-end with stochastic gradient descent, by combining the objectives for reconstruction and forecast error in the latent and input spaces in the loss function. • We enable probabilistic output sampling by injecting noise in the latent space prior to latent forecast decoding, so the model learns to implicitly model the cross-time-series joint predictive distribution by transforming the noise, similar to variational autoencoders (VAEs) (Doersch 2016; Kingma and Welling 2014a). Unlike VAEs, the latent mean (output of the latent forecast model) is not constrained. • We perform extensive experiments with multiple multi- variate forecasting datasets, demonstrating superior perfor- mance compared to past global factorization approaches as well as comparable or superior performance to other recent state of the art forecast methods, for both point and probabilistic predictions (Section 4). We also provide a variety of analyses including hyper parameter sensitivity. 2 Related Work Neural nets have a long history of applications in forecasting (Zhang, Patuwo, and Hu 1998; Benidis et al. 2020), histori- cally mostly focused on univariate models. Here we discuss details of recent related deep learning work beyond those mentioned in the introduction. For further details on classi- cal methods please refer to (Hyndman and Athanasopoulos 2018; L¨utkepohl 2005; Durbin and Koopman 2012; Bauwens, Laurent, and Rombouts 2006). We compare with represen- tative classical forecast methods in experiments - e.g., VAR, ARIMA, and state space models (ETS). A trend in DNN forecasting is to normalize series to ad- dress different scaling / temporal patterns (Lai et al. 2018; Zhang 2003; Salinas, Flunkert, and Gasthaus 2019; Goel, Melnyk, and Banerjee 2017; Cheng, Huang, and Zheng 2020; Bandara, Bergmeir, and Hewamalage 2020; Smyl 2020). E.g., LSTNet (Lai et al. 2018) ﬁts the sum of a linear AR model and a DNN with convolutional and recurrent layers. A popu- lar multi-task univariate forecast method, DeepAR (Salinas, Flunkert, and Gasthaus 2019), scales each series by its av- erage and ﬁts a shared RNN across series. Another recent sate-of-the-art multi-task univariate model (Li et al. 2019) combines TCN embeddings with the Transformer architec- ture (Vaswani et al. 2017). Although these work well on some datasets, as mentioned they are limited in use as they cannot model dependencies between series. TADA(Chen et al. 2018), DA-RNN (Qin et al. 2017) and GeoMAN (Liang et al. 2018) use encoder-decoder ap- proaches built on sequence-to-sequence work (Cho et al. 2014; Bahdanau, Cho, and Bengio 2015). However the encoder-decoder is not an autoencoder, is designed for factor- ing in exogenous variables for multi-step univariate forecast- ing - not modeling cross series relationships / multivariate forecasting, and is not probabilistic. An autoencoder was used in (Cirstea et al. 2018), but only for pre-processing / denoising of individual series before training an RNN, so did not consider factorizing cross-series relationships or deriving probabilistic outcomes, as in our method. Recently a few DNN models have also been proposed to model multivariate forecast distributions (Salinas et al. 2019; Wang et al. 2019; Rasul et al. 2020). A low-rank Gaussian copula model was proposed (Salinas et al. 2019) in which a multitask univariate LSTM (Hochreiter and Schmidhuber 1997) is used to output transformed time series and diagonal and low-rank factors of a Gaussian covariance matrix. How- ever, it is limited in ﬂexibility / distributions it can model, sensitive to choice of rank, and difﬁcult to scale to very high dimensional settings. A deep factor generative model was proposed (Wang et al. 2019) in which a linear combination of RNN latent global factors plus parametric noise models the local series distributions. However, this can only model linear combinations of global series and speciﬁc noise distributions, has no easy way to map from local to global series, and is in- efﬁcient for inference and learning (limited network and data size that can be practically used). Further, a recent concurrent work uses normalizing ﬂows for probabilistic forecasting (Rasul et al. 2020): a multivariate RNN is used to model the series progressions (single large multi-output model), with the state translated to the output joint distribution via a nor- malizing ﬂow approach (Dinh, Sohl-Dickstein, and Bengio 2017).However, invertible ﬂow requires the same number of latent dimensions as input dimensions, so it does not scale to large numbers of time series. E.g., the temporal model it is applied across all series instead of a low dimensional space as in our model, so for RNN it has quadratic complexity in the number of series, whereas ours can be much lower (shown in supplement). Another line of related work is on variational methods with sequence models such as variational RNN (VRNN) (Chung et al. 2015) and (Chatzis 2017), e.g., VRNN applies a VAE to each hidden state of an RNN over the input series. Both of these apply the RNN over the input space so lack scala- bility beneﬁts and require propagating multi-step predictions through the whole model, unlike our method which scalably applies the RNN and its propagation in a low-dimensional latent space. Further, due to noise added at every time step, VRNN may struggle with long term dependencies, and the authors state the model is designed for cases of high signal- to-noise ratio, whereas most forecast data is very noisy. 3 Problem Setup and Methodology Notation. A matrix of multivariate time series is denoted by a bold capital letter, univariate series by bold lowercase letters. Given a vector x, its i-th element is denoted by xi. For a matrix X, we use xi as the i-th column and xi,j is the (i, j)-th entry of X. ∥X∥ℓ2 is the matrix Frobenius norm. ∥x∥ℓp is the ℓp-norm of the vector x, deﬁned as ( ∑ i xp i ) 1/p. Given a matrix Y ∈ Rn×T , YB is indicated as a sub-matrix of Y with column indices in B. For a set B, |B| is regarded as the cardinality of this set. Lastly, for functions f and g, f ◦ g is the composite function, f ◦ g(x) = f (g(x)). Problem deﬁnition. Let a collection of high dimensional multivariate time series be denoted by (y1, ..., yT ), where each yi at time point i is a vector of dimension n. Here we assume n is often a large number, e.g., ∼103 to 10 6 or more. We consider the problem of forecasting τ future values (yT +1, ..., yT +τ ) of the series given its observed his- tory {yi}T i=1. A more difﬁcult but interesting problem is modeling the conditional probability distribution of the high dimensional vectors: p(yT +1, ..., yT +τ |y1:T ) = ∏τ i=1 p(yT +i|y1:T +i−1). (1) This decomposition turns the problem of probabilistically forecasting several steps ahead to rolling prediction: the pre- diction at time i is input to the model to predict the value at time i + 1. Next we describe our key contribution ideas in deterministic settings, then extend it to probabilistic ones. 3.1 Point prediction Motivation. Temporal regularized matrix factorization (TRMF) (Yu, Rao, and Dhillon 2016), decomposes the mul- tivariate time series represented as a matrix Y ∈ Rn×T (composed of n time series in its rows) into components F ∈ Rn×d and X ∈ Rd×T while also imposing temporal constraints on X. The matrix X is expected to inherit tem- poral structures such as smoothness and seasonality of the original series. If Y can be reliably represented by just the few time series in X, then tasks on the high-dimensional series Y can be performed on the much smaller dimensional series X. In (Yu, Rao, and Dhillon 2016) forecasting future values of Y is replaced with the much simpler task of predict- ing future values on the latent series X, so the Y prediction is just a weighted combination of the new X values with weights deﬁned by the matrix F. To train temporal DNN models like RNNs, data is batched temporally. Denote YB as a batch of data containing a subset of b time samples, YB = [yt, yt+1, ..., yt+b−1] where B = {t, ..., t + b − 1} are time indices. To perform constrained factorization (Yu, Rao, and Dhillon 2016) proposed to solve: min X,F,W L(X, F, W) = 1 |B| ∑ B∈B LB(XB, F, W), (2) where B is the set of all data batches and each batch loss is: LB(XB, F, W) ≜ 1 nb ∥YB − FXB∥2 ℓ2 + λR(XB; W). (3) Here, R(XB; W) is regularization parameterized by W on XB to enforce certain properties of the latent factors and λ is the regularization parameter. In order to impose temporal constraints, (Yu, Rao, and Dhillon 2016) assumes an autoregressive model on XB speciﬁed simply as xℓ = ∑L j=1 W(j)xℓ−j where L is a predeﬁned lag parameter. Then, the regularization reads R(XB; W) ≜ b∑ ℓ=L+1 ∥ ∥ ∥ ∥ ∥ ∥ xℓ − L∑ j=1 W(j)xℓ−j ∥ ∥ ∥ ∥ ∥ ∥ 2 ℓ2 . (4) The optimization is solved via alternating minimization with respect to variables X, F, and W. Recently, (Sen, Yu, and Dhillon 2019) considered applying deep learning to the same problem; the authors proposed to replace the autoregressive component with a temporal convolutional network (TCN) (Bai, Kolter, and Koltun 2018). Their TCN-MF model employed the following regularization R(XB; W) ≜ b∑ ℓ=L+1 ∥ ∥xℓ − TCN(xℓ−L,...,xℓ−1; W) ∥ ∥ 2 ℓ2 , (5) where W is the set of parameters of the TCN network; alter- nating minimization was also performed for optimization. (Sen, Yu, and Dhillon 2019) also investigated feeding TCN- MF predictions as “global” features into a “local” multi-task model forecasting individual time series. However, as men- tioned, both (Yu, Rao, and Dhillon 2016) and (Sen, Yu, and Dhillon 2019) have several challenging limitations. First, due to the linear nature of the matrix factorization, the models implicitly assume linear relationships across time series. This implies the models will fail to capture non-linear correlation cross series (e.g., one series inversely proportional to another) that often occurs in practice, separately from the global tem- poral patterns. Second, implementation of these optimization problems with alternating minimization is sufﬁciently in- volved, especially when the loss has coupling terms as in (4). In (Sen, Yu, and Dhillon 2019), although the simple autore- gressive model is replaced by a TCN, this network cannot incorporate the factorization part, making back-propagation impossible to perform end-to-end. TCN-MF model is there- fore unable to leverage recent deep learning optimization developments. This may explain why solutions of TCN-MF are sometimes sub-optimal as compared to the simpler TRMF approach (Yu, Rao, and Dhillon 2016). Our model. In this paper we propose a new model to overcome these weaknesses. We observe that if Y can be decomposed exactly by F and X, then X = F+Y where F+ is the pseudo-inverse of F. This implies that Y = FF+Y. Now if F+ can be replaced by an encoder and F by a decoder, we can exploit the ideas of autoencoders (Kramer 1991; Hinton and Zemel 1994) to seek more powerful non- linear decompositions. The latent representation is now a nonlinear transformation of the input, X = gφ(Y) where gφ is the encoder that maps Y to d dimensional X: g : Rn → Rd and φ is the set of parameters of the encoder. The nonlin- earity of the encoder allows the model to represent more complex structure of the data in the latent embedding. The reconstruction of Y is ˆY = fθ(X) where fθ is the decoder that maps X back to the original domain: f : Rd → Rn and θ is the set of parameters associated with the decoder. Additionally, we introduce a new layer between the en- coder and decoder to capture temporal structure of the latent representation. The main idea is illustrated in Figure 1; in the middle layer an LSTM network (Hochreiter and Schmidhu- ber 1997) is employed to encode the long-range dependency of the latent variables. The ﬂow of the model is as follows: a batch of the time series YB = [y1, ..., yb] ∈ Rn×b is em- bedded into the latent variables XB = [x1, ..., xb] ∈ Rd×b with d ≪ n. These sequential ordered xi are input to the LSTM to produce outputs ˆxL+1, ..., ˆxb with each ˆxi+1 = hW(xi−L+1, ..., xi) where h is the mapping function. h is characterized by the LSTM network with parameters W. The decoder will take the matrix ˆXB consisting of variables x1, ..., xL and ˆxL+1, ..., ˆxb as input and yield the matrix ˆYB. As seen from the ﬁgure, batch output ˆYB contains two components. The ﬁrst, ˆyi with i = 1, ..., L, is directly trans- ferred from the encoder without passing through the mid- dle layer: ˆyi = fθ ◦ gφ(yi), while the second component ˆyi with i = L + 1, ..., b is a function of the past input: ˆyi+1 = fθ ◦ hW ◦ gφ(yi−L+1, ..., yi). By minimizing the error ∥ˆyi − yi∥ p ℓp, one can think of this second component as providing the model the capability to predict the future from the observed history, while at the same time the ﬁrst one requires the model to reconstruct the data faithfully. The objective function with respect to a batch of data is deﬁned as follows. LB(W, φ, θ) ≜ 1 nb ∥ ∥ ∥YB − ˆYB∥ ∥ ∥ p ℓp + λ 1 d(b − L) b−1∑ i=L ∥xi+1 − hW(xi−L+1, ..., xi)∥ q ℓq , (6) and the overall loss function is minW,φ,θ L(W, φ, θ) = 1 |B| ∑ B∈B LB(W, φ, θ). (7) On the one hand, by optimizing ˆY to be close to Y, the model is expected to capture the correlation cross time series and encode this global information into a few latent variables X. On the other hand, minimizing the discrepancy between ˆX and X allows the model to capture temporal dependency and provide the predictive capability of the latent representation. We add a few more remarks: • Although we use LSTMs here, other networks (e.g., TCN (Bai, Kolter, and Koltun 2018) or Transformer (Vaswani et al. 2017)) can be applied. • A fundamental difference between TRMF / TCN-MF and our method is that in the former, latent variables are part of the optimization and solved for explicitly while in ours, latent variables are parameterized by the networks, thus back-propagation can be executed end-to-end for training. • By simpler optimization, our model allows more ﬂexible selection of loss types imposed on ˆY and ˆX. In experi- ments, we found that imposing ℓ1 loss on ˆY consistently led to better prediction while performance remains similar with either ℓ1 or ℓ2 loss on ˆX. Since ℓ1 loss is known to be more robust to outliers, imposing it directly on ˆY makes the model more resilient to potential outliers. • Encoders/decoders themselves can use temporal DNNs so non-static relationships can be captured. Once the model is learned, forecasting several steps ahead is performed via rolling windows. Given past input data [yT −L+1, ..., yT ], the learned model produces the latent pre- diction ˆxT +1 = hW(xT −L+1, ..., xT ) where each xi = gφ(yi). The predicted ˆyT +1 is then decoded from ˆxT +1. The same procedure can be sequentially repeated τ times (in the latent space) to forecast τ future values of Y in which the latent prediction ˆxT +2 utilizes [xT −L+1, ..., xT , ˆxT +1] as the input to the model. Notice that the model does not require retraining during prediction as opposed to TRMF. 3.2 Probabilistic prediction One of the notorious challenges with high-dimensional time series forecasting is how to probabilistically model the future values conditioned on the observed sequence: p(yT +1, ..., yT +τ |y1, ..., yT ). Most previous works either focus on modelling each individual time series or parame- terizing the conditional probability of the high dimensional series by a multivariate Gaussian distribution. However, this Figure 1: Temporal latent autoencoder. Though illustrated with an RNN, any temporal DNN model (e.g., TCN or Transformer) can be used in the latent space. The decoder translates Normal noise to arbitrary distributions. is inconvenient since the number of learned parameters (co- variance matrix) grows quadratically with the data dimen- sion. Recent DNN approaches make distribution assumptions (such as low-rank covariance) that limit ﬂexibility and/or similarly lack scalability (see Section 2). In this paper, instead of directly modelling in the input space, we propose to encode the high dimensional data to a much lower dimensional embedding, on which a probabilistic model can be imposed. Prediction samples are later obtained by sampling from the latent distribution and translating these samples through the decoder. If the encoder is sufﬁciently complex so that it can capture non-linear correlation among series, we can introduce fairly simple probabilistic structure on the latent variables and are still able to model complex distributions of the multivariate data via the decoder mapping. Indeed, together with the proposed network architecture in Figure 1, we model p(xi+1|x1, ..., xi) = N (xi+1; µi, σ2 i ), i = L, ..., b. (8) Here, we ﬁx the conditional distribution of latent variables to multivariate Gaussian with diagonal covariance matrix with variance σ2 i . This is meaningful as it encourages the latent variables to capture different orthogonal patterns of the data, which makes the representation more powerful, universal, and interpretable. The mean µi and variance σ2 i are functions of x1, ..., xi: µi = h (1) W (x1, ..., xi) and σ2 i = h (2) W (x1, ..., xi). The objective function LB(φ, θ, W) with respect to the batch data YB is deﬁned as the weighted combination of the reconstruction loss and the negative log likelihood loss 1 nb ∥ ∥ ∥ ˆYB − YB∥ ∥ ∥ p ℓp −λ 1 b−L ∑b i=L+1 log N (xi; µi−1, σ2 i−1). (9) This bears similarity to the loss of the variational autoen- coder (VAE) (Kingma and Welling 2014b) which consists of a data reconstruction loss and a Kullback–Leibler divergence loss encouraging the latent distribution to be close to the stan- dard multivariate Gaussian with zero mean and unit diagonal covariance. Unlike VAEs, our model has a temporal model in the latent space and is measuring a conditional discrepancy (with no ﬁxed mean constraint). Further, rather than encour- age unit variance we ﬁx latent space unit variance, to also help avoid overﬁtting during training - i.e., we set σ2 i = 1 in our model. As with GANs, the decoder learns to translate this noise to arbitrary distributions (examples in supplement). Recall the output of the decoder ˆyi+1 = fθ(ˆxi+1) for each sample ˆxi+1 from the distribution (8). In order to back-propagate through batch data, we utilize the reparam- eterization trick as in VAEs. I.e., ˆxi+1 = µi + 1ϵ = h (1) W (x1, ..., xi) + 1ϵ with ϵ ∼ N (0, 1). Each iteration when gradient calculation is required, a sample ϵ is generated which yields latent sample ˆxi+1 and associated ˆyi+1. Once the model is learned, next prediction samples ˆyT +1 can be decoded from samples ˆxT +1 of the latent distribution (8). Conditioned on xT −L+1, ..., xT and the mean of ˆxT +1, samples ˆxT +2 can also be drawn from (8) to obtain prediction samples ˆyT +2, and so on. 4 Experiments 4.1 Point estimation We ﬁrst evaluate our point prediction model with loss de- ﬁned in (7). We compare with state-of-the art multivariate and univariate forecast methods (Sen, Yu, and Dhillon 2019) (Yu, Rao, and Dhillon 2016) (Salinas, Flunkert, and Gasthaus 2019) using 3 popular datasets: trafﬁc: hourly trafﬁc of 963 San Fancisco car lanes (Cuturi 2011; Dua and Graff 2017), electricity: hourly consumption of 370 houses (Trindad 2015), and wiki: daily views of ˜115k Wikipedia articles (Kaggle 2017). Trafﬁc and electricity show weekly cross-series pat- terns; wiki contains a very large number of series. Following conventional setups (Salinas, Flunkert, and Gasthaus 2019; Sen, Yu, and Dhillon 2019; Yu, Rao, and Dhillon 2016), we perform rolling prediction evaluation: 24 time-points per win- dow, last 7 windows for testing for trafﬁc and electricity, and 14 per window with last 4 windows for wiki. We use the last few windows prior to the test period for any hyper parameter selection. We use 3 standard metrics: mean absolute percent error (MAPE), weighted absolute percent error (WAPE), and symmetric MAPE (SMAPE) to measure test prediction error. Dataset / formula details are in the supplement. Network architecture and optimization setup in experi- ments is as follows: the encoder and decoder use feed forward network (FNN) layers with ReLU activations on all but the last layer. Layer dimensions vary per dataset. The network architecture in the latent space is a 4-layer LSTM, each with 32 hidden units. In all experiments, ℓ1 loss is used on ˆY and ℓ2 for the regularization. Regularization parameter λ is set to 0.5. We ﬁnd the ℓ1 loss on ˆY can help reduce potential outlier effects and provide more stable and accurate results. Setup and training details are provided in the supplement. Table 1 shows the comparison of different approaches. All results except our proposed TLAE were reported in (Sen, Yu, and Dhillon 2019) under the same experimental setup; we pick the best reported results in (Sen, Yu, and Dhillon 2019) with or without data normalization. Here, global mod- els use global features for multivariate forecasting while local models employ univariate models and separately predict in- dividual series. Here we do not compare our model with conventional methods (e.g., VAR, ARIMA) since they are already conﬁrmed to obtain inferior performance to TRMF and DeepAR methods (Yu, Rao, and Dhillon 2016) (Salinas, Flunkert, and Gasthaus 2019). As seen in the table, our method signiﬁcantly out-performs other global modeling / factorization methods on all datasets (8/9 dataset-metric combinations) - showing it clearly ad- vances the state-of-the-art for global factorization multivari- ate forecasting approaches. Compared with other global mod- els, the gain on trafﬁc and electricity datasets can be as sig- niﬁcant as 50%. Further, even without any local modeling and additional exogenous features like hour of day (as used in local and combined methods), our method still achieves superior performance on 2/3 datasets across all metrics. Our model could likely be further improved by incorporating the exogenous features in the latent space or with local modeling (as done with deepGLO) - the point is our model provides a better global ﬁt starting point. Also note our model only applied standard network architectures and did not make use of recent advanced ones such as TCNs or Transformer, for which we might expect further improvement. Furthermore, in experiments latent dimensions are set to 16 for trafﬁc and 32 for electricity data, as opposed to 64 di- mensions used in (Sen, Yu, and Dhillon 2019). This indicates our model is able to learn a better and more compact rep- resentation. We show examples of learned latent series and input and latent space predictions (and distributions) in the supplement, illustrating our model is able to capture shared global patterns. We also highlight that our model does not need retraining during testing. 4.2 Probabilistic estimation Our next experiments consider probabilistic forecasting. We compare our model with the state-of-the-art probabilistic multivariate method (Salinas et al. 2019), as well as (Wang et al. 2019) and univariate forecasting (Salinas, Flunkert, and Gasthaus 2019; Rangapuram et al. 2018; Li et al. 2019) in the supplement, each following the same data setup (note: differ- ent data splits and processing than in Section 4.1; details in supplement). We apply the same network architecture as in previous experiments, except the latent variable loss is the negative Gaussian log likelihood (9) and the regularization parameter λ is set to 0.005. A smaller λ is selected in this case to account for the scale difference between the regular- izations in (6) and (9). Latent samples are generated during training with the reparameterization trick and distribution statistics obtained from decoded sampled latent predictions. Two additional datasets are included: solar (hourly produc- tion from 137 stations) and taxi (rides taken at 1214 locations every 30 minutes) (training / data details in the supplement. For probabilistic estimates, we report both the continuous ranked probability score across summed time series (CRPS- sum) (Matheson and Winkler 1976; Gneiting and Raftery 2007; Salinas et al. 2019) (details in supplement) and MSE (mean square error) error metrics, to measure overall joint distribution pattern ﬁt and ﬁt of joint distribution central ten- dency, respectively, so that together the two metrics give a good idea of how good the predictive distribution ﬁt is. Re- sults are shown in Table 2 comparing error scores of TLAE with other methods reported in (Salinas et al. 2019). Here, GP is the Gaussian process model of (Salinas et al. 2019). As one can observe, our model outperforms other methods on most of the dataset-metric combinations (7/10), in which the perfor- mance gain is signiﬁcant on Solar, Trafﬁc, and Taxi datasets. We also provide additional tables in the supplement to show CRPS and MSE scores with standard deviation from different runs for more thorough comparison. In the supplement, we visually show different latent series learned from the model on all datasets as well as predictive distributions and sampled 2D joint distributions, demonstrating non-Gaussian and non- stationary distribution patterns. From the plots we see that some are focused on capturing global, more slowly changing patterns across time series; others appear to capture local, faster changing information. Combinations of these enable the model to provide faithful predictions. 4.3 Hyper parameter sensitivity & ablation study We conducted various experiments with trafﬁc data to mon- itor the prediction performance of the model when varying different hyper parameters: batch size, regularization param- eter λ, and latent dimension. We use the same network ar- chitecture as the previous section and train the model with probabilistic loss in (9). Predictions are obtained by decoding the mean of the latent distribution and the prediction accuracy is measured by MAPE, WAPE, and SMAPE metrics. As explained in our proposed model in Figure 1, the la- tent sequence to the decoder consists of two sub-sequences {x1, ..., xL} and {ˆxL+1, ..., ˆxb}. While the ﬁrst one is di- rectly transmitted from the encoder, the second one is the output of the LSTM network. Minimizing the discrepancy between ˆxL+i and xL+i equips the latent variables with the predictive capability, which implicitly contributes to better prediction of the time series, so we would expect selecting the batch size sufﬁciently larger than L (the number of LSTM time steps) should lead to better predictive performance. We validate this intuition by optimizing the model with varying batch size b = L + 1, 1.5L, 2L, 2.5L, and 3L where L is set to 194. Figure 2 illustrates the variability of the prediction accuracy with increasing batch size. As one can observe, all three metrics decreases as we increase the batch size, conﬁrming the importance of balancing the two latent sub-sequences, i.e., having a balance between both a direct re- production and predictive loss component in the input space. Next, for ﬁxed batch size b = 2L, we vary λ = 1e-6, 1e- 5, 1e-4, 1e-3, 5e-3, 5e-2, 5e-1, and 5 and train the model with 500 epochs. Figure 3 plots the prediction accuracy with respect to different choices of λ. As the regularization on the latent space is ignored by assigning very small parameter λ, the overall prediction performance is poor. The performance is quickly improved when higher λ is selected - the latent constraint starts to dominate the reconstruction term with larger λ. The best range of λ is between [1e-4, 1e-2]. Table 1: Comparison of different algorithms with WAPE/MAPE/SMAPE metrics. Only local and combined models employ additional features such as hour of day and day of week. Best results for global modeling methods are labeled in bold, best overall with ∗. Our scores are the average over 3 separate runs with different random initializations. Standard dev. is less than 0.003 for all the metrics. Model Algorithm Datasets Trafﬁc Electricity-Large Wiki-Large Global factorization TLAE (our proposed method) 0.117∗/0.137∗/0.108∗ 0.080∗/0.152∗/0.120∗ 0.334/0.447/0.434 DeepGLO - TCN-MF (Sen, Yu, and Dhillon 2019) 0.226/0.284/0.247 0.106/0.525/0.188 0.433/1.59/0.686 TRMF (Yu, Rao, and Dhillon 2016) (retrained) 0.159/0.226/0.181 0.104/0.280/0.151 0.309/0.847/0.451 SVD+TCN 0.329/0.687/0.340 0.219/0.437/0.238 0.639/2.000/0.893 Local & combined DeepGLO - combined (Sen, Yu, and Dhillon 2019) 0.148/0.168/0.142 0.082/0.341/0.121 0.237/0.441/0.395 LSTM 0.270/0.357/0.263 0.109/0.264/0.154 0.789/0.686/0.493 DeepAR (Salinas, Flunkert, and Gasthaus 2019) 0.140/0.201/0.114 0.086/0.259/0.141 0.429/2.980/0.424 TCN (no LeveledInit) 0.204/0.284/0.236 0.147/0.476/0.156 0.511/0.884/0.509 TCN (LeveledInit) 0.157/0.201/0.156 0.092/0.237/0.126 0.212 ∗/0.316 ∗/0.296 ∗ Prophet (Taylor and Letham 2018) 0.303/0.559/0.403 0.197/0.393/0.221 - Table 2: Comparison of different algorithms with CRPS-Sum and MSE metrics. Most results are from Tables 2 and 6 of (Salinas et al. 2019) with VRNN and our results (under same setup) at the end (TLAE). Lower scores indicate better performance. Shows the mean score from 3 separate runs with random initialization. VAR and GARCH are traditional statistical multivariate methods (L¨utkepohl 2005; Bauwens, Laurent, and Rombouts 2006); Vec-LSTM methods use a single global LSTM that takes and predicts all series at once, with different output Gaussian distribution approaches; and GP methods are DNN gaussian process ones proposed in (Salinas et al. 2019) with GP-Copula the main one - see details in (Salinas et al. 2019). A ’-’ indicates a method failed (e.g., required too much memory as not scalable enough for data size). CRPS-Sum / MSE Estimator Solar Electricity-Small Trafﬁc Taxi Wiki-Small VAR 0.524 / 7.0e3 0.031 / 1.2e7 0.144 / 5.1e-3 0.292 / - 3.400 / - GARCH 0.869 / 3.5e3 0.278 / 1.2e6 0.368 / 3.3e-3 - / - - / - Vec-LSTM-ind 0.470 / 9.9e2 0.731 / 2.6e7 0.110 / 6.5e-4 0.429 / 5.2e1 0.801 / 5.2e7 Vec-LSTM-ind-scaling 0.391 / 9.3e2 0.025 / 2.1e5 0.087 / 6.3e-4 0.506 / 7.3e1 0.113 / 7.2e7 Vec-LSTM-fullrank 0.956 / 3.8e3 0.999 / 2.7e7 -/- -/- -/- Vec-LSTM-fullrank-scaling 0.920 /3.8e3 0.747 / 3.2e7 -/- -/- -/- Vec-LSTM-lowrank-Copula 0.319 / 2.9e3 0.064 / 5.5e6 0.103 / 1.5e-3 0.4326 / 5.1e1 0.241 / 3.8e7 LSTM-GP (Salinas et al. 2019) 0.828 / 3.7e3 0.947 / 2.7e7 2.198 / 5.1e-1 0.425 / 5.9e1 0.933 / 5.4e7 LSTM-GP-scaling (Salinas et al. 2019) 0.368 / 1.1e3 0.022 / 1.8e5 0.079 / 5.2e-4 0.183 / 2.7e1 1.483 / 5.5e7 LSTM-GP-Copula (Salinas et al. 2019) 0.337 / 9.8e2 0.024 / 2.4e5 0.078 / 6.9e-4 0.208 / 3.1e1 0.086 / 4.0e7 VRNN (Chung et al. 2015) 0.133 / 7.3e2 0.051 / 2.7e5 0.181 / 8.7e-4 0.139 / 3.0e1 0.396 / 4.5e7 TLAE (our proposed method) 0.124 / 6.8e2 0.040 / 2.0e5 0.069 / 4.4e-4 0.130 / 2.6e1 0.241 / 3.8e7 Lastly, we vary the latent embedding dimension between [2, 4, 8, 16, 32], and train with 200 epochs vs. 1000 to reduce computational time. Figure 4 shows the impact on metrics. The model performance slightly improves with increase of the latent dimension and starts to stabilize, indicating higher latent dimension may not help much. Additionally, to validate the hypothesis that nonlinear trans- formation helps, we performed ablation study by using a linear decoder and encoder under the same setup. We found worse performance than the non-linear case, though still bet- ter than DeepGLO (details in supplement). 5 Conclusion This paper introduces an effective method for high dimen- sional multivariate time series forecasting, advancing the state-of-the-art for global factorization approaches. The method offers an efﬁcient combination between ﬂexible non- linear autoencoder mapping and inherent latent temporal dynamics modeled by an LSTM. The proposed formulation allows end-to-end training and, by modelling the distribution in the latent embedding, generating complex predictive distri- butions via the non-linear decoder. Our experiments illustrate the superior performance compared to other state-of-the-art 0 100 200 300 400 batch size - L 0.2 0.4 0.6 0.8 mape wape smape Figure 2: Varying batch size 6 4 2 0 log10 of lambda 0.12 0.14 0.16 0.18 0.20 0.22 0.24 mape wape smape Figure 3: Varying λ 1 2 3 4 5 log2 of latent dimension 0.11 0.12 0.13 0.14 0.15 mape wape smape Figure 4: Varying latent dimen- sion methods on several common time series datasets. Future di- rections include testing temporal models in the autoencoder, 3D tensor inputs, and combinations with local modeling. References Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine trans- lation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Bai, S.; Kolter, J. Z.; and Koltun, V. 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271 . Bandara, K.; Bergmeir, C.; and Hewamalage, H. 2020. LSTM- MSNet: Leveraging Forecasts on Sets of Related Time Series With Multiple Seasonal Patterns. IEEE Transactions on Neural Networks and Learning Systems 1–14. Bauwens, L.; Laurent, S.; and Rombouts, J. V. 2006. Multivariate GARCH models: a survey. Journal of applied econometrics 21(1): 79–109. Benidis, K.; Rangapuram, S. S.; Flunkert, V.; Wang, B.; Maddix, D.; Turkmen, C.; Gasthaus, J.; Bohlke-Schneider, M.; Salinas, D.; Stella, L.; et al. 2020. Neural forecasting: Introduction and literature overview. arXiv preprint arXiv:2004.10240 . Borovykh, A.; Bohte, S.; and Oosterlee, C. W. 2017. Conditional time series forecasting with convolutional neural networks. arXiv preprint arXiv:1703.04691 . Chatﬁeld, C. 2000. Time-series forecasting. CRC press. Chatzis, S. P. 2017. Recurrent latent variable conditional het- eroscedasticity. In 2017 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP), 2711–2715. IEEE. Chen, T.; Yin, H.; Chen, H.; Wu, L.; Wang, H.; Zhou, X.; and Li, X. 2018. TADA: Trend Alignment with Dual-Attention Multi-task Recurrent Neural Networks for Sales Prediction. In 2018 IEEE International Conference on Data Mining (ICDM), 49–58. Cheng, J.; Huang, K.; and Zheng, Z. 2020. Towards Better Forecast- ing by Fusing Near and Distant Future Visions. In AAAI Conference on Artiﬁcial Intelligence. Cho, K.; van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Ma- chine Translation. In Proceedings of the 2014 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), 1724– 1734. Chung, J.; Kastner, K.; Dinh, L.; Goel, K.; Courville, A. C.; and Bengio, Y. 2015. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, 2980– 2988. Cirstea, R.-G.; Micu, D.-V.; Muresan, G.-M.; Guo, C.; and Yang, B. 2018. Correlated time series forecasting using multi-task deep neural networks. In Proceedings of the 27th acm international conference on information and knowledge management, 1527–1530. Crone, S. F.; Hibon, M.; and Nikolopoulos, K. 2011. Advances in forecasting with neural networks? Empirical evidence from the NN3 competition on time series prediction. International Journal of forecasting 27(3): 635–660. Cuturi, M. 2011. Fast global alignment kernels. In Proceedings of the 28th international conference on machine learning (ICML-11), 929–936. Dasgupta, S.; and Osogami, T. 2017. Nonlinear dynamic Boltz- mann machines for time-series prediction. In Thirty-First AAAI Conference on Artiﬁcial Intelligence. Dinh, L.; Sohl-Dickstein, J.; and Bengio, S. 2017. Density estima- tion using real nvp . Doersch, C. 2016. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908 . Dua, D.; and Graff, C. 2017. UCI Machine Learning Repository. URL http://archive.ics.uci.edu/ml. Durbin, J.; and Koopman, S. J. 2012. Time series analysis by state space methods. Oxford university press. Fildes, R.; Nikolopoulos, K.; Crone, S. F.; and Syntetos, A. A. 2008. Forecasting and operational research: a review. Journal of the Operational Research Society 59(9): 1150–1172. Funahashi, K.-i.; and Nakamura, Y. 1993. Approximation of dynam- ical systems by continuous time recurrent neural networks. Neural networks 6(6): 801–806. Gelper, S.; Wilms, I.; and Croux, C. 2016. Identifying Demand Effects in a Large Network of Product Categories. Journal of Retailing 92(1): 25 – 39. ISSN 0022-4359. doi:https://doi.org/ 10.1016/j.jretai.2015.05.005. URL http://www.sciencedirect.com/ science/article/pii/S0022435915000536. Gneiting, T.; and Raftery, A. E. 2007. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association 102(477): 359–378. Goel, H.; Melnyk, I.; and Banerjee, A. 2017. R2N2: residual recur- rent neural networks for multivariate time series forecasting. arXiv preprint arXiv:1709.03159 . Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adver- sarial nets. In Advances in neural information processing systems, 2672–2680. Hinton, G. E.; and Zemel, R. S. 1994. Autoencoders, minimum description length and Helmholtz free energy. In Advances in neural information processing systems, 3–10. Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8): 1735–1780. Hyndman, R.; Koehler, A. B.; Ord, J. K.; and Snyder, R. D. 2008. Forecasting with exponential smoothing: the state space approach. Springer Science & Business Media. Hyndman, R. J.; and Athanasopoulos, G. 2018. Forecasting: princi- ples and practice. OTexts. Jordan, A.; Kr¨uger, F.; and Lerch, S. 2019. Evaluating Probabilistic Forecasts with scoringRules. Journal of Statistical Software 90(1): 1–37. Kaggle. 2017. Wikipedia web trafﬁc data set. URL https://www. kaggle.com/c/web-trafﬁc-time-series-forecasting/data. Accessed: 2020-01-11. Kim, K.-j. 2003. Financial time series forecasting using support vector machines. Neurocomputing 55(1-2): 307–319. Kingma, D. P.; and Welling, M. 2014a. Auto-Encoding Varia- tional Bayes. In Bengio, Y.; and LeCun, Y., eds., 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings. URL http://arxiv.org/abs/1312.6114. Kingma, D. P.; and Welling, M. 2014b. Auto-encoding variational bayes. In In Proceedings of the International Conference on Learn- ing Representations (ICLR). Kramer, M. A. 1991. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal 37(2): 233–243. Lai, G.; Chang, W.-C.; Yang, Y.; and Liu, H. 2018. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, 95–104. Leeﬂang, P. S.; Selva], J. P.; Dijk], A. V.; and Wittink, D. R. 2008. Decomposing the sales promotion bump accounting for cross- category effects. International Journal of Research in Marketing 25(3): 201 – 214. ISSN 0167-8116. doi:https://doi.org/10.1016/j. ijresmar.2008.03.003. URL http://www.sciencedirect.com/science/ article/pii/S0167811608000347. Li, S.; Jin, X.; Xuan, Y.; Zhou, X.; Chen, W.; Wang, Y.-X.; and Yan, X. 2019. Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting. In Advances in neural information processing systems, 4838–4847. Liang, Y.; Ke, S.; Zhang, J.; Yi, X.; and Zheng, Y. 2018. Geo- man: Multi-level attention networks for geo-sensory time series prediction. In IJCAI, 3428–3434. L¨utkepohl, H. 2005. New introduction to multiple time series analy- sis. Springer Science & Business Media. Makridakis, S.; Hyndman, R. J.; and Petropoulos, F. 2020. Forecast- ing in social settings: The state of the art. International Journal of Forecasting 36(1): 15–28. Makridakis, S.; Spiliotis, E.; and Assimakopoulos, V. 2018. Statisti- cal and Machine Learning forecasting methods: Concerns and ways forward. PloS one 13(3). Makridakis, S.; Spiliotis, E.; and Assimakopoulos, V. 2020. The M4 Competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting 36(1): 54–74. Matheson, J. E.; and Winkler, R. L. 1976. Scoring rules for continu- ous probability distributions. Management science 22(10): 1087– 1096. McKenzie, E. 1984. General exponential smoothing and the equiva- lent ARMA process. Journal of Forecasting 3(3): 333–344. Qin, Y.; Song, D.; Cheng, H.; Cheng, W.; Jiang, G.; and Cottrell, G. W. 2017. A dual-stage attention-based recurrent neural network for time series prediction. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, 2627–2633. Rangapuram, S. S.; Seeger, M.; Gasthaus, J.; Stella, L.; Wang, Y.; and Januschowski, T. 2018. Deep State Space Models for Time Series Forecasting. In Advances in neural information processing systems, 7796––7805. Rasul, K.; Sheikh, A.-S.; Schuster, I.; Bergmann, U.; and Vollgraf, R. 2020. Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows. arXiv preprint arXiv:2002.06103 . Rodrigues, F.; and Pereira, F. C. 2020. Beyond expectation: Deep joint mean and quantile regression for spatiotemporal problems. IEEE Transactions on Neural Networks and Learning Systems . Salinas, D.; Bohlke-Schneider, M.; Callot, L.; Medico, R.; and Gasthaus, J. 2019. High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes. In Advances in neural information processing systems, 7796––7805. Salinas, D.; Flunkert, V.; and Gasthaus, J. 2019. DeepAR: Proba- bilistic Forecasting with Autoregressive Recurrent Networks. Inter- national Journal of Forecasting 8(2): 136–153. Seeger, M. W.; Salinas, D.; and Flunkert, V. 2016. Bayesian inter- mittent demand forecasting for large inventories. In Advances in Neural Information Processing Systems, 4646–4654. Sen, R.; Yu, H.-F.; and Dhillon, I. 2019. Think Globally, Act Lo- cally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting. In Advances in neural information processing systems, 4838–4847. Smyl, S. 2020. A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting. International Journal of Forecasting 36(1): 75–85. Srinivasan, S. R.; Ramakrishnan, S.; and Grasman, S. E. 2005. Identifying the effects of cannibalization on the product portfolio. Marketing intelligence & planning . Taylor, S. J.; and Letham, B. 2018. Forecasting at scale. The American Statistician 72(1): 37–45. Trindad, A. 2015. ElectricityLoadDiagrams20112014 Data Set. URL https://archive.ics.uci.edu/ml/datasets/ ElectricityLoadDiagrams20112014. Accessed: 2020-01-11. Tsay, R. S. 2013. Multivariate time series analysis: with R and ﬁnancial applications. John Wiley & Sons. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems, 5998–6008. Wang, Y.; Smola, A.; Maddix, D.; Gasthaus, J.; Foster, D.; and Januschowski, T. 2019. Deep Factors for Forecasting. In Interna- tional Conference on Machine Learning, 6607–6617. Wen, R.; Torkkola, K.; Narayanaswamy, B.; and Madeka, D. 2017. A multi-horizon quantile recurrent forecaster. In Advances in neural information processing systems - Time Series Workshop. Yu, H.-F.; Rao, N.; and Dhillon, I. 2016. Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction. In Advances in neural information processing systems, 847–855. Zhang, G.; Patuwo, B. E.; and Hu, M. Y. 1998. Forecasting with artiﬁcial neural networks:: The state of the art. International journal of forecasting 14(1): 35–62. Zhang, G. P. 2003. Time series forecasting using a hybrid ARIMA and neural network model. Neurocomputing 50: 159–175. 6 Supplementary Material The following sections are included in this supplementary materials document: • Section 6.1 - Dataset details: More detailed description of the data sets used and their setups, including table listing the attributes of the data. • Section 6.2 - Network architecture and learning: More detailed description of the network architectures used and selected via validation set tuning per dataset, and the train- ing and hyper parameter selection process. • Section 6.3 - Model complexity and run times: Analy- sis of the computational complexity, showing the beneﬁt of our method as compare to input-space modeling, for scaling to large number of time series, as well as recorded run times for training on the different datasets. • Section 6.4 - Beneﬁt of nonlinear factorization - Abla- tion Study: Ablation study results in which we compare forecasting performance with using just a linear encoder vs. the nonlinear encoder of TLAE - demonstrating the beneﬁt of introducing nonlinear embedding. • Section 6.5 - Additional comparisons, metrics, and re- sult details: Results for additional comparisons for ad- ditional metrics including CRPS (non-sum version), and including the standard deviation across multiple random runs. • Section 6.6 - Comparison to univariate methods: Here we give detailed restults on our experiment comparing TLAE to recent state-of-the-art univariate forecasting meth- ods. • Section 6.7 - Visualization - of losses, predictions, and predictive distributions: Plots of the different train loss components and test losses per epoch, as well as plots of predictions in the latent and input spaces, including distri- bution plots (showing prediction intervals from sampled probabilistic predictions illustrating heteroskedastic pre- dictive distributions). Additionally, 2D scatter plots are provided showing examples of the learned joint predictive distribution amongst pairs of time series are shown, illus- trating the non-Gaussian and sometimes multi-modal and nonlinear relationships between series, which our model is able to capture. • Section 6.8 - Extension to tensor data for incorporat- ing exogenous variables: Details on the extension of our method to include additional exogenous features in our model using tensor inputs. • Section 6.9 - Evaluation metrics: Complete details and formulas for the different evaluation metrics used in the experiments. 6.1 Dataset details Table 3 describes statistics of datasets used in our experi- ments. Only the trafﬁc data is the same on both experiments, the electricity and wiki datasets utilize different portions of the data. In particular, electricity (large) and wiki (large) are used for deterministic experiments while the smaller ones are for probabilistic experiments (taking the same setup as previous state-of-the-art probabilistic methods to enable com- parison). Note we used the same setup in each case so we could directly compare with the best results prior state-of- the-art methods were able to achieve. For our point forecast we compared with the state-of-the-art global factorization approaches designed to be scalable, which is why they were applied on larger data previously. For the probabilistic case we did not ﬁnd other work applying to larger data, due to lack of scalability these others typically can only be applied on the smaller versions of the data - highlighting another key beneﬁt of our work, as it provides a highly scalable probabilistic forecasting method (as none of the prior global factorization methods could provide probabilistic forecasts). In these tables, T is the total number of time steps in each dataset for training and n is the number of dimensions (number of series). We perform rolling prediction for the test evaluation, where τ is the number of predicted steps at each window and k is the number of rolling windows. In particular, in the ﬁrst window, we use yT −L, ..., yT as the input of size L (L is the LSTM time steps) to pre- dict the next τ steps: ˆyT +1, ..., ˆyT +τ . The next window, the actual input yT −b+τ , ..., yT +τ is considered to predict ˆyT +τ +1, ..., ˆyT +2τ , and so on until k rolling windows is reached. The prediction ˆyT , ..., ˆyT +kτ and their actual num- bers are used to calculate the score metrics. The model train- ing data is y1, ..., yT and model is not allowed to see test window samples during training or when predicting those test windows. The datasets are publicly available in the github repositories of (Sen, Yu, and Dhillon 2019) and (Salinas et al. 2019) - in the included program code supplement we pro- vide instructions and links for downloading each dataset used. Descriptions of all 5 datasets are as follows: • trafﬁc: Hourly trafﬁc (in terms of occurpancy rates between 0 and 1) of 963 San Fancisco freeway car lanes, collected across 15 months (Cuturi 2011; Dua and Graff 2017) • electricity: Hourly electricity consumption of 370 house- holds/clients, collected over 4 years (Trindad 2015). The full dataset is used for deterministic forecast while a smaller subset of data is used for probablistic forecast (Salinas et al. 2019). • wiki: Daily page views of ˜115k Wikipedia articles over a couple years (Kaggle 2017) (note, a smaller subset of 2000 selected cleaner article series was used for probabilistic evaluation, following (Salinas et al. 2019)). • solar1: The solar power production records from January to August in 2006, which is sampled every hour from 137 PV plants in Alabama (Lai et al. 2018). • taxi2: Half hourly trafﬁc of New York taxi rides taken at 1214 locations during the month of January 2015 for training and January 2016 for testing. 6.2 Network architectures and learning For all the experimental setup, we design the encoder and decoder to contain a few feed forward layers, each followed 1https://www.nrel.gov/grid/solar-power-data.html 2https://www1.nyc.gov/ site/tlc/about/tlc-trip-record-data Table 3: Dataset statistics for both deterministic and probabilistic forecast experiments. Electricity (large), and wiki (large) datasets are used for deterministic forecasting, while smaller datasets are used for probabilistic forecasting (following previous state-of-the-art forecast method setups used for comparison (Salinas et al. 2019)). Dataset time steps T dimension n τ (number predicted steps) k (rolling windows) frequency domain Trafﬁc 10392 963 24 7 hourly R+ Electricity (large) 25920 370 24 7 hourly R+ Electricity (small) 5833 370 24 7 hourly R+ Solar 7009 137 24 7 hourly R+ Taxi 1488 1214 24 56 30-minutes N Wiki (large) 635 115084 14 4 daily N Wiki (small) 792 2000 30 5 daily N Table 4: Network architectures for different datasets. Dataset Encoder type Encoder and Decoder LSTM layers LSTM hidden dim sequence length number epochs Trafﬁc FFN [96, 64, 32, 16] 4 32 194 1000 Electricity (large) FFN [96, 64, 32] 4 64 194 750 Electricity (small) FFN [128, 64] 4 64 194 200 Solar FFN [64, 16] 4 32 194 300 Taxi FFN [64, 16] 4 32 120 100 Wiki (large) FFN [64, 32] 4 32 128 20 Wiki (small) FFN [64, 32] 4 32 128 20 by a ReLU activation, except the last layer, and an LSTM network to model the temporal dependency in the latent space. Details of the network architectures of datasets used in our paper are provided in Table 4 (note the decoder sequence of layer sizes is reversed from that of the encoder). The process of hyper parameter selection using a held-out valid set is described later - but only minor hyper-parameter tuning was used to select some modeling settings (such as trying a few different number of layers / neurons). To train the model, we use the Adam optimizer with the learning rate set to 0.0001 (which is commonly used). We generally found that larger learning rates lead to unstable performance. The batch size is set to be twice the number of LSTM timesteps. This is to have an even balance between the number of latent features directly transferring to the decoder and the number passing through the middle LSTM layer before the decoder (please see the hyper-parameter sensitivity results in the main paper to see the effect of varying this). The running time is dependent on the size of the datasets and the number of epochs. For instance, with trafﬁc training data consisting of 963 series, each has more than 10, 344 time samples, it takes roughly 4 days to train with 1000 epochs with 4 CPUs. Detail of the running time is provided in Table 5. Recently after performing the experiments we have also started testing running using GPUs and found signiﬁcant speed up - taking hours instead of days in most cases, but have not performed full analysis with the GPU case. We experimented with sacriﬁcing reasonable run time for better accuracy by trying to train the trafﬁc data with 2000 epochs. We found this gave sufﬁciently more ac- curate performance than the result reported in Table 1. In particular, WAPE/MAPE/SMAPE with 2000 epochs is 0.106/0.118/0.092. This suggests with even more training epochs we might have the chance to improve our results reported throughout the paper further for some data sets - because we did not thoroughly search all hyper-parameters for our method during hyper parameter selection with the validation set, e.g., the number of epochs, the best possible results for our method may not have been realized and shown here. Further, we believe the running time can be sufﬁciently reduced if GPUs are employed, as mentioned. Note also in Table 4 the relatively smaller number of epochs used in the case of Wiki. For this dataset we gen- erally found the training would converge after much fewer epochs, perhaps because of the much smaller number of time points, and the larger decrease in size transforming to the latent space. It is future work to try more hyper parameter settings for the Wiki data as well. Across all the experiments, we select the regularization pa- rameter λ = 0.5 for the deterministic model and λ = 0.005 when working with probabilistic model. These choices are to guarantee the reconstruction and regularization losses in the objective have similar order of magnitude (note: please see the hyper parameter sensitivity study section in the main paper to see impact of changing λ). The loss function with respect to Y is chosen to be the ℓ1 norm. This choice is particularly natural to handle possible outliers in the dataset. We also tested the model with the ℓ2 loss but the outcome is generally not as good as the ℓ1 counterpart. For the regu- larization loss in the latent space, we apply ℓ2 loss for the deterministic model while negative log likelihood is applied for probabilistic model, as described in the main text. Our experiments also indicate that similar performance is attained with either ℓ2 or ℓ1 imposed on the regularization. To se- lect batch data for training, one can consider taking sliding windows where the two batches are entirely overlapping ex- cept one time point. For example, one can take two batches Yτ :τ +b and Yτ +1:τ +b+1 at times τ and τ + 1 where b is the batch size and feed to the model. However, to speed up the training process, we select the non-overlapping region to be 24, 12, 12, 2, and 1 for trafﬁc, electricity, solar, taxi, and wiki datasets, respectively. I.e., smaller non-overlapping window size was used with smaller datasets. In our experiments, we split the total training data Y = {y1, ..., yT } into two sets, the ﬁrst set {y1, ..., yT −m} is for modeling training and the last set {yT −m, ..., yT } for valida- tion where m is set to the size of the test data. We tested the model with a few different architectural choices by changing the FFN layers between [96, 64, 32, 16], [96, 64, 32], [64, 32], [64, 16], and [128, 64], changing the LSTM hidden units be- tween 32 and 64, and with different number of epochs. The best network architecture on the validation set was retrained with full training {y1, ..., yT } data and used for testing. We did not perform a thorough architecture and hyperparameter search. Doing so may help improve performance further, as suggested by our hyper-parameter sensitivity analyses in the Hyper Parameter Sensitivity Section in the main paper, where we do see some settings not tested in our main experiments (which we reported metric results for) showing better results than the default ones we used for our experiments). Table 5: Running time (seconds) per epoch for different datasets. Py- torch is used for modeling and training, and 8 CPUs are employed. Dataset Run time Solar 600 Electricity (small) 770 Electricity (large) 1175 Trafﬁc 597 Taxi 330 Wiki (small) 295 Wiki (large) 441 6.3 Model complexity and run times Denote the input dimension as n, dj the number of hidden units at j-th layer of the encoder, bc, bi, and bo the number of memory cells, input and output units of LSTM, respectively. Then the complexity of the model is O((nd1 + ∑ j djdj+1 + bidl + bibc + bcbo + b2 c )L), where L is the number of LSTM time steps. If we denote d and b the total of encoder and LSTMs hidden units, re- spectively, then the model complexity can be simpliﬁed to O((nd + bd)L), which is linear with the input dimension as- suming that b, d, L ≪ n. This number is signiﬁcantly smaller than n2 number of parameters used to model the data distri- bution directly in the input domain. Table 5 shows the running time per epoch of the model with different datasets. For these results we trained the model using Pytorch with 8 CPUs. We expect the running time to substantially decrease if GPUs are employed. As we can see from the table, although the large wiki dataset contains a signiﬁcantly larger number of series (115084 time series) compared to small wiki (only 2000 time series), the running time per epoch of the former is just marginally higher than that of the latter. This strongly indicates the scalability of the proposed TLAE. Note that a similar relationship holds even if other tempo- ral models are used in the latent space, (e.g., the transformer with self-attention). The key point is the temporal model introduces the most complexity relative to the number of series and history, and generally requires more complexity to model the complex temporal patterns. Therefore with the proposed approach, TLAE enables applying the complex temporal model in the lower dimensional latent space instead of the high dimensional input space, thereby reducing the complexity and size of the temporal model (in addition to denoising beneﬁts of modeling in a global aggregate space). This is a key advantage compared to models applied in the input space, and also makes the proposed approach comple- mentary to other multivariate prediction models, as the TLAE approach can also use these in the latent space in place of LSTM. 6.4 Beneﬁt of nonlinear factorization - Ablation Study Table 6: Comparison of linear vs nonlinear encoders with WAPE/MAPE/SMAPE metrics. Lower scores indicate better perfor- mance. The best score for each encoder architecture is in bold. Encoder type Hidden dim. WAPE / MAPE / SMAPE Linear FFN [64, 16] 0.136 / 0.157 / 0.130 FFN with ReLU [64, 16] 0.107 / 0.120 / 0.095 Linear FFN [64, 32] 0.117 / 0.134 / 0.109 FFN with ReLU [64, 32] 0.106 / 0.120 / 0.093 This section demonstrates the importance of nonlinear embedding for multivariate time series prediction - a prop- erty that is lacking in other global factorization approaches such as TRMF and DeepGLO. In particular, we implement TLAE with different choices of linear and nonlinear en- coders and conduct experiments on the trafﬁc dataset. Ta- ble 6 shows the prediction performance with respect to WAPE/MAPE/SMAPE metrics. One can see that with the same number of hidden dimensions the prediction accuracy is signiﬁcantly improved on the encoder with ReLU activation function. Note that metric scores are somewhat improved compared to our main results in the main paper. This is because we performed this ablation study later on after having run the original set of experiments. In this case we ran for a larger number of batches using GPUs, beyond the small set of batches we tried in the main experiment. The point is for the same training setup and architecture, we see signiﬁcantly bet- ter results with nonlinear encoders vs. simple linear encoding, validating the hypothesis that nonlinear embedding can lead to better global modeling of the multivariate time series. 6.5 Additional comparisons, metrics, and result details We provide additional tables 8 and 9 to report probabilistic CRPS and deterministic MSE scores with the std. deviations over multiple runs as well for more thorough comparison - as these would not ﬁt in the main paper. Details of the evalu- ated metrics are provided in the ”Evaluation metrics” Section (later in the supplementary materials). With MSE metrics, the prediction is calculated as ˆyt = f (ˆxt) where ˆxt is the latent sample mean of the Gaussian distribution (8). As one can see from the tables, when comparing with GP methods (Salinas et al. 2019), our model achieves better CRPS performance on solar, trafﬁc, and taxi data and comparable performance on electricity and wiki data. Furthermore, TLAE outperforms GPs on MSE metrics for almost all datasets. We make an ad- ditional remark that due to the high variability in the taxi data (Figure 15), GP models admit very high prediction variance while it is much smaller with TLAE. Similarly, with regard to MSE score, GP model produced unreasonably high variance over different runs (much higher than its mean) while TLAE produces consistent prediction over various runs. 6.6 Comparison to univariate methods In this section, we show experiment results comparing our proposed probabilistic TLAE with univariate baselines and state-of-the-art methods (Salinas, Flunkert, and Gasthaus 2019; Rangapuram et al. 2018; Li et al. 2019) (and a multi- variate one (Wang et al. 2019) that used the same metrics) on trafﬁc and small electricity datasets. Train and test sets are set the same as in (Li et al. 2019). Following these methods, we evaluate the prediction performance by ρ-quantile loss deﬁned in (13) in the ”Evaluation metrics” Section, with our reported scores as the mean over 3 separate runs. Note that this ρ-quantile loss is evaluating the predictive distribution at only 2 points (2 quantiles) as opposed to CRPS which evaluates ﬁt of the entire distribution (i.e., approximated by multiple quantiles distributed through) and better measures how good the uncertainty / distribution prediction is, but we used the ρ-quantile loss here to enable fair comparison with the prior work that all used this metric. Furthermore, many of these methods were speciﬁcally trained and targeted at the speciﬁc quantiles (i.e., the output is directly targeted at predicting the quantile using a pinball loss function), whereas our model is targeted at modeling the entire distribution (not focusing on particular quantiles - though note that the deter- ministic version of TLAE could also be targeted at speciﬁc quantiles). Nevertheless, TLAE still gets comparable accu- racy on these quantiles to the state-of-the-art, despite ﬁtting the entire distribution simultaneously. As one can see from Table 10, we obtain comparable scores to Transformer (Li et al. 2019) - the state-of-the-art method for univariate time series prediction. We hypothesize that our result will further be improved if a Transformer is ap- plied in our middle layer. Additionally, we note that while DeepAR (Salinas, Flunkert, and Gasthaus 2019), DeepState (Rangapuram et al. 2018), and Transformer (Li et al. 2019) utilize external features in their training and testing which likely have strong inﬂuence on the overall performance, these are not included in our model. We want to again emphasize that our model is complementary to univariate methods. A combination of our global latent forecast features with lo- cal features from univariate models as proposed in (Sen, Yu, and Dhillon 2019) will potentially improve the performance further. 6.7 Visualization - of losses, predictions, and predictive distributions Training losses and evaluated metrics. Figure 5 shows the training loss and the evaluated test metrics with respect to the training epochs on the trafﬁc data. On the left, the total training loss and the losses on X and Y are displayed in which the ℓ1 loss is imposed on Y while Gaussian negative log likelihood is used for X. As one can see, both of these losses decrease with increasing iterations, leading to the convergence of the training loss. Figure 5 on the right demonstrates the prediction performance of the test data on the three evaluated metrics. All the metrics make signiﬁcant improvement in the ﬁrst 200 epochs and slowly make progress till the last epoch. We furthermore provide training loss and the test metrics of the electricity dataset in Figure 6 which presents similar behavior. Latent and time series prediction. We display the dynamics of trained latent variables and the model’s predictions on all the datasets in Figures 7, 10, 12, 14, and 16. In the plots of latent variables, embedding variables are shown in blue and the mean predictions in orange. The light shaded area in orange is 90% prediction interval. The calculation of these variables is as follow: we calculate L latent samples [xT −L, ..., xT ] = gφ(Y), which are mapped from the batch time series data Y = [yT −L, ..., yT ] with T being the total number of training samples. We generate 100 prediction samples ˆx(s) T +1 from the Gaussian distribution ˆx (s) T +1 ∼ N (xT +1; hW(xT , ..., xT −L), I), s = 1, ..., 100, (10) from there, the 95% percentile, the 5% percentile, and the prediction mean ¯xT +1 are computed. The next 100 prediction samples ˆx(s) T +2 is then generated similarly with the predicted input ¯xT +1, ˆx(s) T +2 ∼ N (xT +2; hW(¯xT +1, xT , ..., xT −L+1), I), (11) with s = 1, ..., 100. The iteration is continued for τ times until the next τ samples is predicted. Then, the model receives the actual input batch data Y = [yT −L+τ , ..., yT +τ ] and forecast the next τ latent samples ¯xT +τ +1, ..., ¯xT +2τ with the associated conﬁdence intervals. The procedure is repeated k times to get all kτ predictions, which are displayed in Figure 7. As one can observe from these ﬁgures, high-level latent variables are capable of capturing global trends across in- dividual time series. Each latent variable possesses its own local properties while they share similar global repeating patterns. In addition, the following ﬁgures 8, 11, 13, 15, and 17 depict a few actual time series of the underlying datasets and its prediction. The actual series Y = [yT , ..., yT +kτ ] are labeled in blue while the prediction ¯Y is in orange with its 90% percentile in light shaded orange color. The output pre- diction samples are calculated from predicted latent samples in (10) and (11): ˆy(s) T +i = fθ(ˆx (s) T +i). As one can see, TLAE can accurately capture the global time series pattern thanks to the predictive capability of the latent representations. In addi- tion, the model is also capable of predicting local variability associated with individual time series. 0 200 400 600 800 1000 epochs 0.2 0.3 0.4 0.5 0.6 0.7 train loss loss_Y 15.0 15.2 15.4 15.6 loss_X 0 200 400 600 800 1000 epochs 0.10 0.15 0.20 0.25 0.30 0.35 wape mape smape Figure 5: Training loss (left) and evaluated metrics (right) versus epochs on the trafﬁc data. On the left, scale of the green curve displaying the loss of X is read from the right while other two is from the left. 0 25 50 75 100 125 150 175 200 epochs 0.2 0.3 0.4 0.5 0.6 0.7 0.8 train loss loss_Y 59.4 59.6 59.8 60.0 60.2 60.4loss_X 0 25 50 75 100 125 150 175 epochs 0.10 0.15 0.20 0.25 0.30 wape mape smape Figure 6: Training loss (left) and evaluated metrics (right) versus epochs on the electricity data. On the left, scale of the green curve displaying the loss of X is read from the right while other two is from the left. Table 7: Comparison of different algorithms with CRPS-Sum metrics. Results are from Table 2 of (Salinas et al. 2019) with our results (under same setup) at the end (TLAE). Lower scores indicate better performance. Mean and std. dev. are calculated from 3 separate runs with random initialization. VAR and GARCH are traditional statistical multivariate methods (L¨utkepohl 2005; Bauwens, Laurent, and Rombouts 2006); Vec-LSTM methods use a single global LSTM that takes and predicts all series at once, with different output Gaussian distribution approaches; and GP methods are DNN gaussian process ones proposed in (Salinas et al. 2019) with GP-Copula the main proposed copula method - see details in (Salinas et al. 2019). A ’-’ indicates a method failed (e.g., required too much memory as not scalable enough for data size). CRPS-Sum Estimator Solar Electricity-Small Trafﬁc Taxi Wiki-Small VAR 0.524 ± 0.001 0.031 ± 0.000 0.144 ± 0.000 0.292 ± 0.000 3.400 ± 0.003 GARCH 0.869 ± 0.000 0.278 ± 0.000 0.368 ± 0.000 - - Vec-LSTM-ind 0.470 ± 0.039 0.731 ± 0.007 0.110 ± 0.020 0.429 ± 0.000 0.801 ± 0.029 Vec-LSTM-ind-scaling 0.391 ± 0.017 0.025 ± 0.001 0.087 ± 0.041 0.506 ± 0.005 0.133 ± 0.002 Vec-LSTM-fullrank 0.956 ± 0.000 0.999 ± 0.000 - - - Vec-LSTM-fullrank-scaling 0.920 ± 0.035 0.747 ± 0.000 - - - Vec-LSTM-lowrank-Copula 0.319 ± 0.011 0.064 ± 0.008 0.103 ± 0.006 0.326 ± 0.007 0.241 ± 0.033 GP 0.828 ± 0.010 0.947 ± 0.016 2.198 ± 0.774 0.425 ± 0.199 0.933 ± 0.003 GP-scaling 0.368 ± 0.012 0.022 ± 0.000 0.079 ± 0.000 0.183 ± 0.395 1.483 ± 1.034 GP-Copula 0.337 ± 0.024 0.024 ± 0.002 0.078 ± 0.002 0.208 ± 0.183 0.086 ± 0.004 TLAE 0.124 ± 0.057 0.040 ± 0.003 0.069 ± 0.002 0.130 ± 0.010 0.241 ± 0.001 Table 8: Comparison of different algorithms with CRPS metrics. Results are from Table 2 of (Salinas et al. 2019) with our results (under same setup) at the end (TLAE). Lower scores indicate better performance. Mean and std. dev. are calculated from 3 separate runs with random initialization. VAR and GARCH are traditional statistical multivariate methods (L¨utkepohl 2005; Bauwens, Laurent, and Rombouts 2006); Vec-LSTM methods use a single global LSTM that takes and predicts all series at once, with different output Gaussian distribution approaches; and GP methods are DNN gaussian process ones proposed in (Salinas et al. 2019) with GP-Copula the main proposed copula method - see details in (Salinas et al. 2019). A ’-’ indicates a method failed (e.g., required too much memory as not scalable enough for data size). CRPS Estimator Solar Electricity-Small Trafﬁc Taxi Wiki-Small VAR 0.595 ± 0.000 0.060 ± 0.000 0.222 ± 0.000 0.410 ± 0.000 4.101 ± 0.002 GARCH 0.928 ± 0.000 0.291 ± 0.000 0.426 ± 0.000 - - LSTM-ind 0.480 ± 0.031 0.765 ± 0.005 0.234 ± 0.007 0.495 ± 0.002 0.800 ± 0.028 LSTM-ind-scaling 0.434 ± 0.012 0.059 ± 0.001 0.168 ± 0.037 0.586 ± 0.004 0.379 ± 0.004 LSTM-fullrank 0.939 ± 0.001 0.997 ± 0.000 - - - Vec-LSTM-fullrank-scaling 1.003 ± 0.021 0.749 ± 0.020 - - - LSTM-lowrank-Copula 0.384 ± 0.010 0.084 ± 0.006 0.165 ± 0.004 0.416 ± 0.004 0.247 ± 0.001 GP 0.834 ± 0.002 0.900 ± 0.023 1.255 ± 0.562 0.475 ± 0.177 0.870 ± 0.011 GP-scaling 0.415 ± 0.009 0.053 ± 0.000 0.140 ± 0.002 0.346 ± 0.348 1.549 ± 1.017 GP-Copula 0.371 ± 0.022 0.056 ± 0.002 0.133 ± 0.001 0.360 ± 0.201 0.236 ± 0.000 TLAE 0.335 ± 0.044 0.058 ± 0.003 0.097 ± 0.002 0.369 ± 0.011 0.298 ± 0.002 Table 9: Comparison of different algorithms with MSE metrics. Results are from Table 2 of (Salinas et al. 2019) with our results (under same setup) at the end (TLAE). Lower scores indicate better performance. Mean and std. dev. are calculated from 3 separate runs with random initialization. VAR and GARCH are traditional statistical multivariate methods (L¨utkepohl 2005; Bauwens, Laurent, and Rombouts 2006); Vec-LSTM methods use a single global LSTM that takes and predicts all series at once, with different output Gaussian distribution approaches; and GP methods are DNN gaussian process ones proposed in (Salinas et al. 2019) with GP-Copula the main proposed copula method - see details in (Salinas et al. 2019). A ’-’ indicates a method failed (e.g., required too much memory as not scalable enough for data size). MSE Estimator Solar Electricity-Small Trafﬁc Taxi Wiki-Small VAR 7.0e3+/-2.5e1 1.2e7+/-5.4e3 5.1e-3+/-2.9e-6 - - GARCH 3.5e3+/-2.0e1 1.2e6+/-2.5e4 3.3e-3+/-1.8e-6 - - LSTM-ind 9.9e2+/-2.8e2 2.6e7+/-4.6e4 6.5e-4+/-1.1e-4 5.2e1+/-2.2e-1 5.2e7+/-3.8e5 LSTM-ind-scaling 9.3e2+/-1.9e2 2.1e5+/-1.2e4 6.3e-4+/-5.6e-5 7.3e1+/-1.1e0 7.2e7+/-2.1e6 LSTM-fullrank 3.8e3+/-1.8e1 2.7e7+/-2.3e2 - - - Vec-LSTM-fullrank-scaling 3.8e3+/-6.9e1 3.2e7+/-1.1e7 - - - LSTM-lowrank-Copula 2.9e3+/-1.1e2 5.5e6+/-1.2e6 1.5e-3+/-2.5e-6 5.1e1+/-3.2e-1 3.8e7+/-1.5e5 GP 3.7e3+/-5.7e1 2.7e7+/-2.0e3 5.1e-1+/-2.5e-1 5.9e1+/-2.0e1 5.4e7+/-2.3e4 GP-scaling 1.1e3+/-3.3e1 1.8e5+/-1.4e4 5.2e-4+/-4.4e-6 2.7e1+/-1.0e1 5.5e7+/-3.6e7 GP-Copula 9.8e2+/-5.2e1 2.4e5+/-5.5e4 6.9e-4+/-2.2e-5 3.1e1+/-1.4e0 4.0e7+/-1.6e9 TLAE 6.8e2+/-1.3e2 2.0e5+/-1.6e4 4e-4+/-5.0e-6 2.6e1+/-1.4e0 3.8e7+/-7.2e4 Table 10: Univariate time series methods comparison experiment results. Comparison of different algorithms with the reported ρ-quantile metrics with ρ = 0.5/ρ = 0.9. Results were reported from Table 6 of (Li et al. 2019) with our results placed at the last columns. Lower scores indicate better performance. The best two scores are in bold. ARIMA ETS TRMF DeepAR DeepState DeepFactor Transformer TLAE Trafﬁc 0.223/0.137 0.236/0.148 0.186/- 0.161 /0.099 0.167/0.113 0.225/0.159 0.122/0.081 0.117/0.078 Elec. 0.154/0.102 0.101/0.077 0.084/- 0.075 /0.040 0.083 /0.056 0.112/0.059 0.059/0.034 0.072/0.047 Furthermore, we clearly see examples of nonstationary and varying output predictive distributions demonstrating the pre- dictive distribution is not homoskedastic (the output variance changes for different time points and series), despite using a ﬁxed variance in the latent space. I.e., prediction intervals for individual time series expand and shrink at different time points per series and for different series at the same time (e.g., see Figure 11). Note this is not surprising as the same idea underpins modern neural network based generative models like VAEs and Generative Adversarial Networks (Goodfel- low et al. 2014) - i.e., using a nonlinear decoder to map a simple input distribution to arbitrarily complex output dis- tributions. The key difference here is this decoding is based on a latent state that is updated over time along with the simple noise distribution, which then gets decoded through the nonlinear network. In the version of the model we used in experiments, the latent state at a given time point determines how the underlying noise distribution is mapped to the input space distribution (i.e., it must be encoded in the latent state as well). I.e., even in the current model with feed-forward networks for the encoder and decoder, nonstationary distribu- tions can be modeled through encoding in the latent state and its progression. However, it’s possible to enable potentially even more complex distribution behavior by using temporal models for the encoder or decoder as well - fully testing this is an area of future work. Joint predictive distribution. Figure 18 demonstrates the correlation of one predicted time series with respect to some others on the trafﬁc data. Given 1000 prediction samples ˆy(s) T +1 decoded from fθ(ˆx(s) T +i) where latent ˆx(s) T +i is drawn from Gaussian distri- bution with s = 1, ..., 1000, each subplot shows the scatter distribution of the ﬁrst series ˆy(s) T +1,0 with another ˆy(s) T +1,i for i = 1, ..., 30, where each dot is a single prediction sam- ple. We observe that a few plots attain diagonal structure (indicating strong correlation between two time series) while the majority exhibit multimodal distributions and nonlinear relationships. These plots suggest that the marginal join distri- bution between two time series that TLAE captures is much more complex than the joint Gaussian distribution, thanks to the expressive capability of the deep decoder. 6.8 Extension to tensor data for incorporating exogenous variables Although we did not conduct experiments with this setting, we would like to comment on the capability of our model to handle higher dimensional data - i.e., to incorporate addi- tional exogenous features (other than the time series history themselves) in the modeling and forecasting. So far in our dis- cussion, we assume the multivariate data is two dimensional where one dimension is temporal and the other consists of different time series. In many practical settings, each time series may be associated with additional temporal features (even static features can be modeled as a constant over time temporal feature). For example, for retail demand forecasting, where each series is a combination of a product and a store - weather forecasts for the store region can be one temporal feature, current and planned future price of the product at the given node can be another, events at each location another, seasonal indicators like day of week and hour of day another, etc. In the case of global features (those that are the same for all series, like day of week, hour of day, etc.) - these could directly be incorporated as exogenous inputs to the latent space temporal model used (e.g., LSTM) - as these are the same for all series. However, for additional features that are local to speciﬁc series, a more scalable approach as needed. We can therefore capture the data including exogenous features per time series as a three-dimensional tensor instead of a two dimensional matrix. In this case now Y ∈ Rn×p×T where p is the number of additional features per series. I.e., each slice of the tensor along the time dimension at time point t is composed of an n × p matrix Yt of the n time series and each of their p associated features. Notice that some of these exogenous features may be the same for either some subset of series, all series, all time points, or some combination of these, however, this is not a problem as these can just be included as repeated values in the 3D tensor. In this formulation the encoder still maps the input to the same d dimensional latent space and associated 2D matrix X, so the latent space modeling remains unchanged. In this case the encoder gφ maps Y to d dimensional X, per time point: g : Rn×p → Rd. There are multiple ways to accomplish this, one trivial way is to add a ﬂattening layer as the ﬁrst layer of the encoder that maps Rn×p → Rnp, as was done in early work applying deep learning to image classiﬁcation. Alternatively, we can borrow the ideas used for image data and use convolutional neural net (CNN) style architectures to capture the cross-series and feature relationships, at lower compute cost and fewer model parameters. Further, if we want to feed prior time points in as well to inﬂuence the embedding for the current time point, we can either feed in a ﬁxed time window of Y and perform convolution over the 3D tensor (i.e., temporal convolution as in TCNs, but now with additional channels to start), or use a combination with other recurrent structures like LSTMs. The decoder can similarly map back to the original tensor input space, e.g., using deconvolution. Updated losses In this case, the loss for predictions in the latent space and the reconstruction loss remains the same. However for the component of the loss on ˆY for the predic- tion in the input space, ˆyi with i = L, ..., b (those compo- nents that are a function of the past input), it may be desirable to only enforce predicting accurately the future values of the multivariate time series themselves, as opposed to the exoge- nous features - as these exogenous features might be provided for future values and so need not be forecast. In this case we can use masking on the second loss term measuring the dis- crepancy between Y and ˆY to 0-out the contribution of the exogenous feature elements for the prediction time points, during training. Multistep prediction This formulation is ﬁne for predict- ing the next step, but a common issue with using exogenous features with multi-step prediction is how to get the future values of the exogenous variables, to feed in for predicting subsequent steps. In some cases these will be known and can directly be plugged in at each next step (for example, day of week, hour of day, holidays, and other seasonal features are known for future time points as well). In others, forecasts for these might be available or provided separately as well (for example, for weather, sophisticated weather forecast models can project next step weather feature values). However, if neither is possible, the model itself can be used to forecast the next values of the exogenous features, as these are naturally outputs of the end-to-end model. I.e., it can be used in the same way as before with no external features - the predicted outputs in the latent space can be used to feed the next step prediction. Another alternative approach is to change the model to a multi-time-step output model - i.e., train the model to simul- taneously predict multiple next time points, given the input. The disadvantage of this approach is that the model can only be applied then for a speciﬁc number of next steps it was trained for, and different models would be needed for differ- ent number of next steps. E.g., if we wanted to predict one more next step than it was trained for, to use this approach we would have to train another model with one additional output. 6.9 Evaluation metrics Here we describe the evaluation metrics used for both the point (deterministic) and probabilistic forecast settings in more detail. Deterministic metrics For deterministic settings, we em- ploy well-known metrics that are often used in time series for comparison. In the following, Y ∈ Rn×d is denoted as the target time series while ˆY ∈ Rn×d is the prediction. 1. Weighted Absolute Percentage Error (WAPE) is deﬁned as ∑n i=1 ∑d j=1 |ˆyij − yij| ∑n i=1 ∑d j=1 |yij| . 2. Mean Absolute Percentage Error (MAPE) is deﬁned as 1 m n∑ i=1 d∑ j=1 |ˆyij − yij| |yij| 1{|yij |>0}, where m = ∑n i=1 ∑d j=1 1{|yij |>0}. 3. Symmetric Mean Absolute Percentage Error (SMAPE) is deﬁned as 1 m n∑ i=1 d∑ j=1 2|ˆyij − yij| |ˆyij + yij| 1{|yij |>0}, where m = ∑n i=1 ∑d j=1 1{|yij |>0}. 4. Mean Square Error (MSE) is deﬁned as 1 nd n∑ i=1 d∑ j=1(ˆyij − yij)2. We note that these metrics can also be applied when the model is probabilistic. In this case, ˆY will be the mean of the predicted distribution. Probabilistic metrics To compare with other probabilistic models on time series forecasting, as in (Salinas et al. 2019), we use the widely-used Continuous Ranked Probability Score (CRPS) (Matheson and Winkler 1976; Gneiting and Raftery 2007; Salinas et al. 2019; Jordan, Kr¨uger, and Lerch 2019), which measures the mean discrepancy between a probability distribution (i.e., predictive distribution) p and a determinis- tic observation y. The CRPS is able to effectively measure the ﬁt of the predictive distribution to the true one and is a proper scoring rule - i.e., it is minimized by the true underly- ing distribution (Matheson and Winkler 1976; Gneiting and Raftery 2007; Jordan, Kr¨uger, and Lerch 2019). Speciﬁcally to measure how well the predictive distribution ﬁts across test cases, it computes the integral of the squared difference between the cumulative density function (CDF) of the predic- tive distribution and the observed CDF (Heaviside function), as illustrated in Figure 19, which is a generalization of mean absolute error to the predictive distribution case (Gneiting and Raftery 2007). CRPS for a sample of observations and forecasts is deﬁned as: CRPS(F f , F o) = 1 nd n∑ i=1 d∑ j=1 ∫ ∞ −∞(F f ij(x) − F o ij(x)) 2dx (12) where F f ij(x) is the forecast CDF for the ij point (time point and series being predicted) and F o ij(x) is the CDF of the corresponding observation (represented by the Heaviside function - i.e., 0 for values less than the observed value and 1 for values greater than or equal to the observed value). As we do not have a closed-form for the predictive dis- tribution and CDF, we evaluate the CRPS using a sample generated from the predictive distribution. Speciﬁcally, we borrow the approach and implementation of (Salinas et al. 2019) to compute the CRPS using the alternative pinball loss deﬁnition in which the integral is closely approximated by evaluating quantiles of samples from the predictive distribu- tion (see the supplementary material, Section G.1, in (Salinas et al. 2019) for complete details). To compute this we sam- ple 1000 points from the predictive distribution for each prediction point, and evaluate the CRPS using 20 quantiles as was done in (Salinas et al. 2019) (note our implementa- tion was directly taken from the implementation shared by the authors used to generate the results reported in (Salinas et al. 2019) - see https://github.com/mbohlkeschneider/gluon- ts/tree/mv release for speciﬁc code). Similarly as in (Salinas et al. 2019) we report the CRPS-sum metric to capture joint effects - which is the CRPS computed on the sum (across series) of the observed target values and the statistics on the sum of the predictive distribution samples. Another set of metrics that we use in the paper is the ρ- quantile loss Rρ with ρ ∈ (0, 1), R( ˆY, Y) = 2 ∑n i=1 ∑d j=1 Dρ(ˆyij, yij) ∑n i=1 ∑d j=1 |yij| , (13) where Dρ(ˆy, y) = (ρ − I{ˆy≤y})(ˆy − y), where ˆy is the empirical ρ-quantile of the predicted distribu- tion and I{ˆy≤y} is an indicator function. 0 50 100 150 200 250 300 350 20 10 0 10 20 latent variable 1 0 50 100 150 200 250 300 350 15 10 5 0 5 latent variable 2 0 50 100 150 200 250 300 350 25 20 15 10 5 0 5 10 latent variable 3 0 50 100 150 200 250 300 350 20 10 0 10 20 latent variable 4 0 50 100 150 200 250 300 350 0 5 10 15 20 latent variable 5 0 50 100 150 200 250 300 350 30 20 10 0 10 latent variable 6 0 50 100 150 200 250 300 350 10 5 0 5 10 15 20 25 latent variable 7 0 50 100 150 200 250 300 350 30 20 10 0 10 20 latent variable 8 0 50 100 150 200 250 300 350 20 10 0 10 20 latent variable 9 0 50 100 150 200 250 300 350 10 0 10 20 latent variable 10 0 50 100 150 200 250 300 350 20 10 0 10 20 latent variable 11 0 50 100 150 200 250 300 350 20 10 0 10 20 latent variable 12 0 50 100 150 200 250 300 350 15 10 5 0 5 10 latent variable 13 0 50 100 150 200 250 300 350 30 20 10 0 10 20 latent variable 14 0 50 100 150 200 250 300 350 10 0 10 20 latent variable 15 0 50 100 150 200 250 300 350 10 0 10 20 30 latent variable 16 Figure 7: 16 latent variables and their prediction of the trafﬁc dataset. The ﬁrst 194 samples, colored in blue, is the embedding of the multivariate time series YB = [yT −194, ..., yT ]. The last 168 samples, colored in orange, are rolling predictions. The shaded light orange is the 95% percentile. 0 25 50 75 100 125 150 175 0.02 0.04 0.06 0.08 time series 1 target prediction 0 25 50 75 100 125 150 175 0.00 0.05 0.10 0.15 0.20 0.25 time series 2 target prediction 0 25 50 75 100 125 150 175 0.025 0.050 0.075 0.100 0.125 0.150 time series 3 target prediction 0 25 50 75 100 125 150 175 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 time series 4 target prediction 0 25 50 75 100 125 150 175 0.01 0.02 0.03 0.04 0.05 0.06 0.07 time series 5 target prediction 0 25 50 75 100 125 150 175 0.00 0.05 0.10 0.15 0.20 0.25 time series 6 target prediction 0 25 50 75 100 125 150 175 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 time series 7 target prediction 0 25 50 75 100 125 150 175 0.00 0.05 0.10 0.15 0.20 0.25 0.30 time series 8 target prediction 0 25 50 75 100 125 150 175 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 time series 9 target prediction 0 25 50 75 100 125 150 175 0.00 0.05 0.10 0.15 0.20 0.25 time series 10 target prediction 0 25 50 75 100 125 150 175 0.00 0.02 0.04 0.06 0.08 0.10 time series 11 target prediction 0 25 50 75 100 125 150 175 0.02 0.03 0.04 0.05 0.06 0.07 time series 12 target prediction 0 25 50 75 100 125 150 175 0.00 0.02 0.04 0.06 0.08 0.10 0.12 time series 13 target prediction 0 25 50 75 100 125 150 175 0.00 0.02 0.04 0.06 0.08 time series 14 target prediction 0 25 50 75 100 125 150 175 0.00 0.05 0.10 0.15 0.20 time series 15 target prediction 0 25 50 75 100 125 150 175 0.0 0.1 0.2 0.3 0.4 0.5 time series 16 target prediction Figure 8: The 168 prediction samples (orange) of a few time series versus their actual samples (blue) in the trafﬁc dataset. The shaded light orange is the 95% percentile. As one can see, the model not only predict the global patterns of time series well, but also be able to accurately capture local changes in individual series. 0 25 50 75 100 125 150 175 0.0075 0.0050 0.0025 0.0000 0.0025 0.0050 0.0075 time series 1 0 25 50 75 100 125 150 175 0.125 0.100 0.075 0.050 0.025 0.000 0.025 time series 2 0 25 50 75 100 125 150 175 0.05 0.04 0.03 0.02 0.01 0.00 0.01 0.02 time series 3 0 25 50 75 100 125 150 175 0.125 0.100 0.075 0.050 0.025 0.000 0.025 0.050 time series 4 0 25 50 75 100 125 150 175 0.004 0.002 0.000 0.002 0.004 0.006 time series 5 0 25 50 75 100 125 150 175 0.10 0.05 0.00 0.05 time series 6 0 25 50 75 100 125 150 175 0.075 0.050 0.025 0.000 0.025 0.050 0.075 0.100 time series 7 0 25 50 75 100 125 150 175 0.10 0.05 0.00 0.05 time series 8 0 25 50 75 100 125 150 175 0.03 0.02 0.01 0.00 0.01 0.02 0.03 time series 9 0 25 50 75 100 125 150 175 0.10 0.05 0.00 0.05 0.10 time series 10 0 25 50 75 100 125 150 175 0.0075 0.0050 0.0025 0.0000 0.0025 0.0050 0.0075 0.0100 time series 11 0 25 50 75 100 125 150 175 0.004 0.002 0.000 0.002 0.004 0.006 0.008 time series 12 0 25 50 75 100 125 150 175 0.010 0.005 0.000 0.005 0.010 0.015 0.020 time series 13 0 25 50 75 100 125 150 175 0.015 0.010 0.005 0.000 0.005 0.010 0.015 0.020 time series 14 0 25 50 75 100 125 150 175 0.06 0.04 0.02 0.00 0.02 0.04 time series 15 0 25 50 75 100 125 150 175 0.3 0.2 0.1 0.0 0.1 time series 16 Figure 9: The prediction errors of a few time series in the trafﬁc dataset. As one can see, the prediction error does not exhibit any periodic pattern as shown in the original series in Figure 8. This indicates that not much information can be further extracted from this noise to improve the prediction. 0 50 100 150 200 250 300 350 5.0 2.5 0.0 2.5 5.0 7.5 latent variable 1 0 50 100 150 200 250 300 350 8 6 4 2 0 2 4 6 latent variable 2 0 50 100 150 200 250 300 350 20 15 10 5 0 5 10 15 latent variable 3 0 50 100 150 200 250 300 350 15 10 5 0 5 10 15 20 latent variable 4 0 50 100 150 200 250 300 350 5 0 5 10 latent variable 5 0 50 100 150 200 250 300 350 10 5 0 5 10 15 latent variable 6 0 50 100 150 200 250 300 350 15 10 5 0 5 10 15 20 latent variable 7 0 50 100 150 200 250 300 350 2 0 2 4 6 8 10 latent variable 8 0 50 100 150 200 250 300 350 10 5 0 5 latent variable 9 0 50 100 150 200 250 300 350 15 10 5 0 5 10 15 latent variable 10 0 50 100 150 200 250 300 350 15 10 5 0 5 10 latent variable 11 0 50 100 150 200 250 300 350 10 5 0 5 10 latent variable 12 0 50 100 150 200 250 300 350 10.0 7.5 5.0 2.5 0.0 2.5 5.0 latent variable 13 0 50 100 150 200 250 300 350 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 latent variable 14 0 50 100 150 200 250 300 350 4 2 0 2 4 6 8 latent variable 15 0 50 100 150 200 250 300 350 5 0 5 10 latent variable 16 Figure 10: 16 latent variables and their prediction of the solar dataset. The ﬁrst 194 samples, colored in blue, is the embedding of the multivariate time series YB = [yT −194, ..., yT ]. The last 168 samples, colored in orange, are rolling predictions. The shaded light orange is the 95% percentile. 0 25 50 75 100 125 150 175 0 50 100 150 200 250 time series 1 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 2 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 3 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175 time series 4 target prediction 0 25 50 75 100 125 150 175 0 50 100 150 200 time series 5 target prediction 0 25 50 75 100 125 150 175 0 100 200 300 400 time series 6 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 7 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 8 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175 time series 9 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 10 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 11 target prediction 0 25 50 75 100 125 150 175 0 20 40 60 80 time series 12 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 175 time series 13 target prediction 0 25 50 75 100 125 150 175 0 25 50 75 100 125 150 time series 14 target prediction 0 25 50 75 100 125 150 175 0 50 100 150 time series 15 target prediction 0 25 50 75 100 125 150 175 0 20 40 60 80 time series 16 target prediction Figure 11: The 168 prediction samples (orange) of a few time series versus their actual samples (blue) in the solar dataset. The shaded light orange is the 95% percentile. As one can see, the model not only predict the global patterns of time series well, but also be able to accurately capture local changes in individual series. 0 50 100 150 200 250 300 350 7.5 5.0 2.5 0.0 2.5 5.0 7.5 latent variable 1 0 50 100 150 200 250 300 350 7.5 5.0 2.5 0.0 2.5 5.0 latent variable 2 0 50 100 150 200 250 300 350 10 8 6 4 2 0 2 4 latent variable 3 0 50 100 150 200 250 300 350 8 6 4 2 0 2 4 latent variable 4 0 50 100 150 200 250 300 350 5.0 2.5 0.0 2.5 5.0 7.5 10.0 12.5 latent variable 5 0 50 100 150 200 250 300 350 10 5 0 5 latent variable 6 0 50 100 150 200 250 300 350 8 6 4 2 0 2 4 6 latent variable 7 0 50 100 150 200 250 300 350 7.5 5.0 2.5 0.0 2.5 5.0 7.5 latent variable 8 0 50 100 150 200 250 300 350 5 0 5 10 15 latent variable 9 0 50 100 150 200 250 300 350 6 4 2 0 2 4 6 8 latent variable 10 0 50 100 150 200 250 300 350 10 5 0 5 latent variable 11 0 50 100 150 200 250 300 350 15.0 12.5 10.0 7.5 5.0 2.5 0.0 latent variable 12 0 50 100 150 200 250 300 350 12.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0 latent variable 13 0 50 100 150 200 250 300 350 7.5 5.0 2.5 0.0 2.5 5.0 latent variable 14 0 50 100 150 200 250 300 350 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 latent variable 15 0 50 100 150 200 250 300 350 8 6 4 2 0 2 4 latent variable 16 Figure 12: 16 latent variables and their prediction of the electricity dataset. The ﬁrst 194 samples, colored in blue, is the embedding of the multivariate time series YB = [yT −194, ..., yT ]. The last 168 samples, colored in orange, are rolling predictions. The shaded light orange is the 90% prediction interval (5% and 95% percentiles). 0 25 50 75 100 125 150 175 400 500 600 700 800 900 1000 time series 16 target prediction 0 25 50 75 100 125 150 175 80 100 120 140 160 180 time series 17 target prediction 0 25 50 75 100 125 150 175 60 80 100 120 time series 18 target prediction 0 25 50 75 100 125 150 175 250 500 750 1000 1250 1500 1750 time series 19 target prediction 0 25 50 75 100 125 150 175 60 80 100 120 time series 20 target prediction 0 25 50 75 100 125 150 175 30 40 50 60 70 80 90 100 time series 21 target prediction 0 25 50 75 100 125 150 175 600 800 1000 1200 time series 22 target prediction 0 25 50 75 100 125 150 175 100 150 200 250 300 time series 23 target prediction 0 25 50 75 100 125 150 175 500 1000 1500 2000 time series 24 target prediction 0 25 50 75 100 125 150 175 40 60 80 100 time series 25 target prediction 0 25 50 75 100 125 150 175 250 300 350 400 450 500 550 time series 26 target prediction 0 25 50 75 100 125 150 175 20 40 60 80 100 time series 27 target prediction 0 25 50 75 100 125 150 175 700 800 900 1000 1100 1200 1300 time series 28 target prediction 0 25 50 75 100 125 150 175 20 25 30 35 40 45 time series 29 target prediction 0 25 50 75 100 125 150 175 20 30 40 50 60 70 time series 30 target prediction 0 25 50 75 100 125 150 175 100 150 200 250 300 time series 31 target prediction Figure 13: The 168 prediction samples (orange) of a few time series versus their actual samples (blue) in the electricity dataset. The shaded light orange is the 90% prediction interval. As one can see, the model not only predict the global patterns of time series well, but also be able to accurately capture local changes in individual series. 0 50 100 150 200 250 300 5 0 5 10 latent variable 1 0 50 100 150 200 250 300 10 5 0 5 10 15 latent variable 2 0 50 100 150 200 250 300 15 10 5 0 5 10 latent variable 3 0 50 100 150 200 250 300 10 5 0 5 10 15 latent variable 4 0 50 100 150 200 250 300 10 5 0 5 10 15 latent variable 5 0 50 100 150 200 250 300 5 0 5 10 15 latent variable 6 0 50 100 150 200 250 300 10 5 0 5 10 latent variable 7 0 50 100 150 200 250 300 10 5 0 5 latent variable 8 0 50 100 150 200 250 300 15 10 5 0 5 10 latent variable 9 0 50 100 150 200 250 300 10 5 0 5 latent variable 10 0 50 100 150 200 250 300 10 5 0 5 10 latent variable 11 0 50 100 150 200 250 300 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 latent variable 12 0 50 100 150 200 250 300 15 10 5 0 5 10 latent variable 13 0 50 100 150 200 250 300 10 5 0 5 10 latent variable 14 0 50 100 150 200 250 300 10 5 0 5 10 15 latent variable 15 0 50 100 150 200 250 300 10 5 0 5 10 15 latent variable 16 Figure 14: 16 latent variables and their prediction of the taxi dataset. The ﬁrst 120 samples, colored in blue, is the embedding of the multivariate time series YB = [yT −120, ..., yT ]. The last 168 samples, colored in orange, are rolling predictions. The shaded light orange is the 90% prediction interval. 0 25 50 75 100 125 150 175 0 5 10 15 20 time series 1 target prediction 0 25 50 75 100 125 150 175 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 time series 2 target prediction 0 25 50 75 100 125 150 175 0 2 4 6 8 10 12 14 time series 3 target prediction 0 25 50 75 100 125 150 175 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 time series 4 target prediction 0 25 50 75 100 125 150 175 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 time series 5 target prediction 0 25 50 75 100 125 150 175 0 2 4 6 8 10 12 time series 6 target prediction 0 25 50 75 100 125 150 175 0.0 2.5 5.0 7.5 10.0 12.5 15.0 time series 7 target prediction 0 25 50 75 100 125 150 175 0 5 10 15 20 time series 8 target prediction 0 25 50 75 100 125 150 175 0 5 10 15 20 25 time series 9 target prediction 0 25 50 75 100 125 150 175 0 2 4 6 8 10 12 time series 10 target prediction 0 25 50 75 100 125 150 175 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 time series 11 target prediction 0 25 50 75 100 125 150 175 0 2 4 6 8 10 12 time series 12 target prediction 0 25 50 75 100 125 150 175 0 5 10 15 20 25 time series 13 target prediction 0 25 50 75 100 125 150 175 0 5 10 15 time series 14 target prediction 0 25 50 75 100 125 150 175 0 2 4 6 8 10 time series 15 target prediction 0 25 50 75 100 125 150 175 0 2 4 6 8 10 12 time series 16 target prediction Figure 15: The 168 prediction samples (orange) of a few time series versus their actual samples (blue) in the taxi dataset. The shaded light orange is the 90% prediction interval. As one can see, although the data has high variability, the model is able to capture the global pattern. 0 50 100 150 200 250 6 5 4 3 2 1 0 latent variable 1 0 50 100 150 200 250 1 0 1 2 3 latent variable 2 0 50 100 150 200 250 3 2 1 0 1 2 latent variable 3 0 50 100 150 200 250 2 1 0 1 2 3 latent variable 4 0 50 100 150 200 250 4 3 2 1 0 latent variable 5 0 50 100 150 200 250 2 1 0 1 2 latent variable 6 0 50 100 150 200 250 0 1 2 3 4 latent variable 7 0 50 100 150 200 250 4 3 2 1 latent variable 8 0 50 100 150 200 250 3 2 1 0 1 latent variable 9 0 50 100 150 200 250 2 1 0 1 2 3 latent variable 10 0 50 100 150 200 250 2 3 4 5 6 latent variable 11 0 50 100 150 200 250 2 3 4 5 6 latent variable 12 0 50 100 150 200 250 5 4 3 2 1 0 latent variable 13 0 50 100 150 200 250 2 3 4 5 6 latent variable 14 0 50 100 150 200 250 4 3 2 1 0 1 latent variable 15 0 50 100 150 200 250 0 1 2 3 4 latent variable 16 Figure 16: 16 latent variables and their prediction of the wiki dataset. The ﬁrst 128 samples, colored in blue, is the embedding of the multivariate time series YB = [yT −128, ..., yT ]. The last 150 samples, colored in orange, are rolling predictions. The shaded light orange is the 90% prediction interval. 0 20 40 60 80 100 120 140 5000 7500 10000 12500 15000 17500 20000 time series 1 target prediction 0 20 40 60 80 100 120 140 1000 1200 1400 1600 1800 2000 time series 2 target prediction 0 20 40 60 80 100 120 140 800 1000 1200 1400 1600 1800 time series 3 target prediction 0 20 40 60 80 100 120 140 2000 3000 4000 5000 6000 7000 8000 time series 4 target prediction 0 20 40 60 80 100 120 140 1000 2000 3000 4000 5000 6000 time series 5 target prediction 0 20 40 60 80 100 120 140 2000 2500 3000 3500 4000 4500 time series 6 target prediction 0 20 40 60 80 100 120 140 1000 1500 2000 2500 3000 3500 4000 time series 7 target prediction 0 20 40 60 80 100 120 140 800 1000 1200 1400 1600 time series 8 target prediction 0 20 40 60 80 100 120 140 1000 1200 1400 1600 1800 2000 2200 time series 9 target prediction 0 20 40 60 80 100 120 140 2000 4000 6000 8000 10000 12000 14000 16000 time series 10 target prediction 0 20 40 60 80 100 120 140 2000 4000 6000 8000 10000 time series 11 target prediction 0 20 40 60 80 100 120 140 1000 1500 2000 2500 3000 3500 time series 12 target prediction 0 20 40 60 80 100 120 140 2000 4000 6000 8000 10000 12000 14000 time series 13 target prediction 0 20 40 60 80 100 120 140 1200 1400 1600 1800 time series 14 target prediction 0 20 40 60 80 100 120 140 5000 10000 15000 20000 time series 15 target prediction 0 20 40 60 80 100 120 140 0 50000 100000 150000 200000 time series 16 target prediction Figure 17: The 150 prediction samples (orange) of a few time series versus their actual samples (blue) in the wiki dataset. The shaded light orange is the 90% prediction interval. As one can see, although the data has high variability, the model is able to capture the global pattern of some series and fails on some others. Figure 18: Scatter plots of one series with respect to others on the trafﬁc dataset. Each dot in the subplot indicates a single prediction sample. A few plots attain diagonal structure (indicates strong correlation between two time series) while majority admit multimodal behavior. 0 5 10 15 20 Value 0.0 0.1 0.2 0.3Probability densityForecast PDF Observed value 0 5 10 15 20 Value 0.0 0.2 0.4 0.6 0.8 1.0Cumulative densityForecast CDF Observed CDF Discrepancy Figure 19: Illustration of CRPS metric calculation per prediction / observation. The predictive distribution (PDF) along with the observation is illustrated in (a). The CDF of the predictive distribution, and the CDF of the observation (Heaviside function) are shown in (b). By comparing the discrepancy between the two (shaded region in b) we can determine how good a ﬁt the predicted distributions are to the observed values, averaged across many different observations.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_notes_OLD_PARTIAL/Liu23conceptDriftGlobalTSfrcst.pdf","text":"Handling Concept Drift in Global Time Series Forecasting Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir Abstract Machine learning (ML) based time series forecasting models often require and assume certain degrees of stationarity in the data when producing forecasts. However, in many real-world situations, the data distributions are not stationary and they can change over time while reducing the accuracy of the forecasting models, which in the ML literature is known as concept drift. Handling concept drift in forecasting is essential for many ML methods in use nowadays, however, the prior work only proposes methods to handle concept drift in the classiÔ¨Åcation domain. To Ô¨Åll this gap, we explore concept drift handling methods in particular for Global Forecasting Models (GFM) which recently have gained popularity in the forecasting domain. We propose two new concept drift handling methods, namely: Error Con- tribution Weighting (ECW) and Gradient Descent Weighting (GDW), based on a continuous adaptive weighting concept. These methods use two forecasting models which are separately trained with the most recent series and all series, and Ô¨Ånally, the weighted average of the forecasts provided by the two models are considered as the Ô¨Ånal forecasts. Using LightGBM as the underlying base learner, in our evaluation on three simulated datasets, the proposed models achieve signiÔ¨Åcantly higher accuracy Ziyi Liu Department of Data Science and AI, Monash University, Melbourne, Australia e-mail: ziyi.liu1@monash.edu Rakshitha Godahewa Department of Data Science and AI, Monash University, Melbourne, Australia e-mail: rakshitha.godahewa@monash.edu Kasun Bandara School of Computing and Information Systems, University of Melbourne, Melbourne, Australia EnergyAustralia, Melbourne, Australia e-mail: kasun.bandara@unimelb.edu.au Christoph Bergmeir (corresponding author) Department of Data Science and AI, Monash University, Melbourne, Australia e-mail: christoph.bergmeir@monash.edu 1arXiv:2304.01512v1 [cs.LG] 4 Apr 2023 2 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir than a set of statistical benchmarks and LightGBM baselines across four evaluation metrics. 1 Introduction Accurate time series forecasting is crucial in the context of decision making and strategic planning nowadays for many businesses and industries. The forecasting models are typically trained based on historical data. In most applications, data distributions are not stationary and they change over time while making the trained models outdated and reducing their forecasting accuracy, which is known as concept drift. Hence, it is important to make sure that the trained models well-generalise beyond the training data and are capable to provide accurate forecasts even with the changed data distributions. Transfer learning can be applied to make the models robust to distribution shift. In general, transfer learning is an approach of solving one problem and applying the corresponding knowledge gained to solve another related problem. In the time series forecasting domain, the transfer learning models typically train on a large set of series and during forecasting, they predict the future of (1) new series that they have not seen before; (2) training series where the data distributions change over time. In this chapter, we focus on the second sub-problem of transfer learning which is predicting the future of a set of series that is used to train a model when the corresponding data distribution is not stationary. Traditional forecasting methods like Exponential Smoothing [ETS, 1] are capable of handling the most common non-stationarities, namely trends and seasonalities, and also have been designed to show a certain degree of robustness under other types of non-stationarities. In particular, ETS gives most weight to the most recent observations and therefore it can adapt to a distribution shift reasonably well. How- ever, Global Forecasting Models [GFM, 2] have recently shown their potential in providing more accurate forecasts compared to the traditional univariate forecasting models, by winning the M4 [3] and M5 [4] forecasting competitions. In contrast to traditional univariate forecasting models that build isolated models on each series, GFMs build a single model across many series while allowing the model to learn the cross-series information. By building a single model over many series, the models have more data available for training and can aÔ¨Äord to be more complex [5]. Conse- quently, ML methods like neural networks and Gradient Boosted Trees are usually the methods of choice for global modelling. However, unlike many traditional fore- casting methods like ETS, ML methods do normally not have inherent mechanisms to deal with changes in the underlying distribution. Therefore, in ML the Ô¨Åeld of concept drift has emerged, which studies how ML methods can be used under dis- tribution shift [6]. In this chapter, we study how the methods from the concept drift literature can be used for forecasting with ML methods. To the best of our knowledge, no methods to deal with concept drift in the forecasting domain in a GFM and ML context have been proposed in the literature. The available concept drift handling methods are all related to the classiÔ¨Åcation Handling Concept Drift in Global Time Series Forecasting 3 domain. Dynamic Weighted Majority [DWM, 7], Social Adaptive Ensemble [SAE, 8] and Error Interaction Approach [EIA, 9] are some of the popular concept drift handling methods in the classiÔ¨Åcation domain. Most of these concept drift handling methods use ensembling [10]. In the forecasting space, ensembling is also known as forecast combination which aggregates the predictions of multiple forecasting models to produce the Ô¨Ånal forecasts. Ensembling techniques are widely used to reduce model variance and model bias. This motivates us to explore methods to handle concept drift in the GFM space that incorporate ensembling techniques. In this study, we propose two new forecast combination methods to handle the concept drift in the forecasting domain. We name these two methods Error Contribu- tion Weighting (ECW) and Gradient Descent Weighting (GDW). In particular, these methods independently train two forecasting models based on diÔ¨Äerent time series, typically one model is trained with the most recent series history and the other model is trained with the full series history, where the two models are Ô¨Ånally weighted based on the previous prediction error, which is known as continuous adaptive weighting. The proposed ECW and GDW methods are diÔ¨Äerent based on the method of cal- culation of the sub-model weights. In the proposed methods, the sub-model trained with the full series‚Äô history is expected to provide more accurate forecasts in gen- eral situations whereas if concept drift occurs, then the sub-model trained with the most recent series‚Äô history is expected to quickly adapt to the new concept. Further- more, two diÔ¨Äerent methods for series weighting, exponential weighting and linear weighting, are used during the training of the two sub-models where these methods internally assign higher weights for the most recent series observations than the older observations during training. We also quantify the eÔ¨Äect of handling concept drift using our proposed methods and only using the series weighted methods con- sidering LightGBM [11] as the base learner. The LightGBM models that use our proposed concept drift handling methods outperform a set of statistical benchmarks and LightGBM baselines with statistical signiÔ¨Åcance across three simulated datasets on four error metrics. All implementations of this study are publicly available at: https://github.com/Neal-Liu-Ziyi/Concept_Drift_Handling. The remainder of this chapter is organized as follows: Section 2 introduces diÔ¨Äer- ent concept drift types. Section 3 reviews the relevant prior work. Section 4 explains the series weighted methods and proposed concept drift handling methods in detail. Section 5 explains the experimental framework, including the datasets, error metrics, benchmarks, evaluation and statistical testing. An analysis of the results is presented in Section 6. Section 7 concludes the study and discusses possible future research. 2 Problem Statement: Concept Drift Types There are two main types of concept drift: real concept drift [7] and virtual concept drift [12]. Real concept drift occurs when the true outputs of the instances change over time whereas virtual concept drift occurs when the data distribution changes 4 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir over time even with the same true outputs, possibly due to noise. In many situations, real and virtual concept drift can co-occur. The literature typically classiÔ¨Åes real concept drift into four diÔ¨Äerent groups [6] which are (1) sudden concept drift where the data distribution at time ùë° suddenly changes into a new distribution at time ùë° + 1; (2) incremental concept drift where the data distribution changes and stays in a new distribution after going through a variable distribution; (3) gradual concept drift where the data distribution shifts between a new distribution and the current distribution over time where gradually, the series completely enters the new distribution; (4) recurring concept drift where the same concept drift occurs periodically. Incremental and recurring concept drift in a time series forecasting context are closely related to trends and seasonalities in the data. In particular, recurring concept drift can be easily handled by the typical handling of seasonality with dummy variables, Fourier terms and similar additional features [13]. Thus, in this study we focus on three concept drift types: sudden, incremental and gradual, which are illustrated in Figure 1. Fig. 1: A visualisation of sudden, gradual and incremental concept drift types. 3 Related Work In the following, we discuss the related prior work in the areas of global time series forecasting, transfer learning and concept drift handling. Handling Concept Drift in Global Time Series Forecasting 5 3.1 Global Time Series Forecasting GFMs [2] build a single model across many series with a set of global parameters that are the same across all series. In contrast to the traditional univariate forecasting models, GFMs are capable of learning cross-series information with a fewer amount of parameters. Global models have been pioneered by works such as Salinas et al. [14], Smyl [15], Bandara et al. [16], Hewamalage et al. [17], Godahewa et al. [18]. GFMs have recently obtained massive popularity in the forecasting Ô¨Åeld after winning the M4 [3] and M5 [4] forecasting competitions. In particular, the winning method of the M4 competition uses global Recurrent Neural Networks [RNN, 17] whereas the winning method of the M5 competition uses global LightGBM [11] models. 3.2 Transfer Learning In the forecasting domain, there are many works that present on forecasting new time series that a model has not seen during training. Oreshkin et al. [19] propose a deep learning based meta-learning framework that generalises well on new time series from diÔ¨Äerent datasets that it has not seen before. The popular Nixtla framework [20] provides ten N-BEATS [21] and N-HiTS [22] models that are pre-trained using the M4 dataset [3] where the models are expected to generalise on unseen time series. Woo et al. [23] propose DeepTime, a meta-learning framework that can forecast unseen time series by properly addressing distribution shifts using a ridge regressor. Grazzi et al. [24] propose Meta-GLAR, which performs transfer learning using local closed-form adaptation updates. The transfer learning framework proposed by Ye and Dai [25] in particular considers the adequate long-ago data during training. To the best of our knowledge, all these transfer learning frameworks produce forecasts for unseen time series. In contrast, our proposed methods produce forecasts for the same set of time series used during training where the distribution of the series is not stationary. 3.3 Concept Drift Handling To the best of our knowledge, the existing concept drift handling methods are all related to the classiÔ¨Åcation domain. The concept drift handling methods in the literature can be mainly divided into two categories: implicit methods and explicit methods [26]. The explicit methods use a drift detection technique and immediately react to the drift at the point of its occurrence. The implicit methods do not use drift detection techniques. In particular, Chu and Zaniolo [27] propose the Adaptive Boosting Model which is an explicit method of concept drift handling in the classiÔ¨Åcation domain. This 6 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir method assigns a weight to each training instance where the weights are consequently updated across a sequence of classiÔ¨Åers. The Ô¨Årst classiÔ¨Åer assigns a sample weight to each instance. Each classiÔ¨Åer then keeps the same weights for the correctly classiÔ¨Åed instances and assigns higher weights for the incorrectly classiÔ¨Åed instances where the weighted instances are used to train the next classiÔ¨Åer in the sequence. Each classiÔ¨Åer is also assigned a separate weight based on its classiÔ¨Åcation accuracy. This process is repeated until the error function does not change. The weighted average of the predictions provided by all classiÔ¨Åers are considered as the Ô¨Ånal prediction. When a concept drift situation is detected, the weight of each classiÔ¨Åer in the sequence is reset to one. However, this method contains a few limitations. It is highly time-consuming for trivial concept drift. Also, if the concept drift is too small to impact the accuracy of the previous classiÔ¨Åer, it may stop building new classiÔ¨Åers. Furthermore, it needs to reset all classiÔ¨Åers when a data stream encounters concept drift which may then lead the model to be sensitive to false alarms [28]. The recurring concept drift framework, RCD, proposed by Gon√ßalves Jr and De Barros [29] also uses an explicit approach to handle concept drift. This method extends a similar approach proposed by Widmer and Kubat [30] that was originally designed to deal with categorical data, to work with numerical data. It stores the context of each data type in a buÔ¨Äer and a classiÔ¨Åer is built together with a concept drift detector. This method uses a two step drift detection mechanism. The drift detector uses a new classiÔ¨Åer to detect concept drift. If there is no concept drift, an empty buÔ¨Äer is Ô¨Ålled with the samples used in the training of the classiÔ¨Åer. Furthermore, there is a warning level to monitor the error rate of the classiÔ¨Åer where if the error rate of the classiÔ¨Åer reaches the warning level, it means concept drift might be occurring. Simultaneously, a new classiÔ¨Åer is created with the new buÔ¨Äer. If the error rate of this classiÔ¨Åer decreases and returns to the normal level, then the algorithm will treat that concept drift as a false alarm. Furthermore, when a true concept drift occurs, a statistical test compares the current buÔ¨Äer with all previous buÔ¨Äers to detect whether the new concept has already been observed in the past. If the test result is positive, which means the new concept has been seen before, then the previous classiÔ¨Åer and the buÔ¨Äer corresponding to this concept are used. Otherwise, the current classiÔ¨Åer and the buÔ¨Äer are stored in the list. The main limitation of these explicit methods is that they frequently falsely detect concept drift. Therefore, using these explicit methods may result in incorrect concept drift detection while reducing the model performance. Furthermore, it is diÔ¨Écult to use a drift detector to recognise diÔ¨Äerent concept drift types. Thus, implicit methods are usually more useful in handling concept drift. The DWM method proposed by Kolter and Maloof [7] is an implicit method of handling concept drift. This method uses a four-step process to handle concept drift: (1) the DWM holds a weighted pool of base learners which adds or removes base learners based on the performance of the global algorithm; (2) if the global algorithm makes a mistake, then an expert will be added; (3) if a base learner makes a mistake, then its weight will be reduced; (4) if the weight of a base learner reduces below a threshold, then it will be removed. The SAE method proposed by Gomes and Enembreck [8] is also a popular implicit method which is also known as the Handling Concept Drift in Global Time Series Forecasting 7 social network abstraction for data stream classiÔ¨Åcation. It uses a similar learning strategy as the DWM method. The SAE method explores the similarities between its classiÔ¨Åers. It also drops classiÔ¨Åers based on their accuracy and similarities with other classiÔ¨Åers. The new classiÔ¨Åers are only added if the overall method accuracy is lower than a given threshold. Furthermore, SAE connects two classiÔ¨Åers when they have very similar predictions. Even though implicit methods overcome the issues of explicit methods such as the model sensitivity to falsely detecting concept drift and diÔ¨Éculties in recognising diÔ¨Äerent concept drift types, they take a considerable amount of time to adapt to a new concept. This phenomenon has motivated researchers to explore robust and rapidly reactive concept drift handling methods without drift detection. The EIA method proposed by Baier et al. [9] is an implicit method that does not use drift detection. The EIA uses two models with diÔ¨Äerent complexities, ùëÄùë†ùëñùëö ùëùùëôùëí and ùëÄùëêùëúùëö ùëùùëôùëíùë•, to make predictions. At a time, the predictions are obtained using a single model where this model is selected based on its error predicted using a regression method. Here, ùëÄùëêùëúùëö ùëùùëôùëíùë• captures more information during model training and thus, in normal situations, ùëÄùëêùëúùëö ùëùùëôùëíùë• is used to make predictions. However, when the data has concept drift, in particular sudden concept drift, ùëÄùëêùëúùëö ùëùùëôùëíùë• is unable to respond to the concept drift quickly. Thus, in such situations, ùëÄùë†ùëñùëö ùëùùëôùëí which is trained with most recent observations is applied as it can quickly adapt to the current concept. However, the EIA method is only applicable to sudden concept drift as it is diÔ¨Écult to switch between the two models more frequently to handle continuous drift types such as incremental and gradual concept drift types. In line with that, in this study, we propose two methods which use the continuous adaptive weighting approach, ECW and GDW, where both methods are inspired by the EIA method. Similar to the EIA, our proposed methods use two sub-models. However, unlike the EIA method, the weighted average of the forecasts provided by both sub-models is considered during forecasting where the weights of the sub- models are dynamically changed. Furthermore, all the above explained concept drift handling work is from the classiÔ¨Åcation domain where in this study, we explore the elements of these methods that should be changed to adapt them to handle concept drift in the forecasting domain. 4 Methodology This section explains the series weighted methods and our proposed concept drift handling methods in detail. 8 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir 4.1 Series Weighted Methods In this simple and straightforward Ô¨Årst approach, we assign diÔ¨Äerent weights for the training instances. In particular, we consider the most recent instances to be more relevant than older instances and thus, during training, we assign higher weights for the most recent instances where the weights are gradually decreased for the older instances, e.g., using an exponential decay similar to ETS. In our work, we consider two series weighting methods: exponential weighting and linear weighting. Equations 1 and 2 respectively deÔ¨Åne the process of weighting the training instances and the method of calculating the weights with exponential and linear weighting methods. Here, ùë•ùëñ is the original ùëñùë° ‚Ñé instance, ùëãùëñ is the weighted ùëñùë° ‚Ñé instance, series_length is the number of training instances considered from a particular series, ùõºseries_length‚àíùëñ is the weight of the ùëñùë° ‚Ñé instance, ùõº0 is the initial weight of the most recent instance, where 0 < ùõº ‚â§ 1, and ùõΩ is the weight decrease of the linear weighting, where 0 < ùõΩ ‚â§ 1. ùëãùëñ = ùõºseries_length‚àíùëñ ¬∑ ùë•ùëñ (1) ùõºùëñ = {ùõºùëñ‚àí1 ¬∑ ùõº0 exponential weighting ùõºùëñ‚àí1 ‚àí ùõΩ series_length linear weighting (2) The experiments are conducted considering two values for series_length, 200 instances and all instances. The values of ùõº0 and ùõΩ are Ô¨Åxed to 0.9 based on our preliminary experiments. Finally, four models are trained with each of our experi- mental datasets (Section 5.1), considering the two weighting methods: exponential and linear weighting, and two series_length values. 4.2 Our Proposed Concept Drift Handling Methods We propose two continuous adaptive weighting methods: ECW and GDW. In par- ticular, our methods train two forecasting models with diÔ¨Äerent series where one model is trained with the recent series observations (ùëÄpartial) and the other model is trained with all series observations (ùëÄall). The weighted average of the predictions provided by ùëÄpartial and ùëÄall is considered as the Ô¨Ånal prediction of a given time point where the weights of these models are dynamically changed and calculated based on the previous prediction error. As ùëÄpartial is trained with the recent data, its prediction accuracy may be less than the prediction accuracy of ùëÄall, however, if the series encounters concept drift, then ùëÄpartial can quickly adapt to the new concept compared to ùëÄall. In general, the weight of ùëÄpartial is less than the weight of ùëÄall due to its lower prediction accuracy. However, when a concept drift occurs, our methods rapidly increase the weight of ùëÄpartial which can eÔ¨Éciently remedy the issues of other implicit concept drift handling methods. Handling Concept Drift in Global Time Series Forecasting 9 We consider Residual Sum of Squares (RSS) as the loss function of ùëÄpartial and ùëÄall. Equation 3 deÔ¨Ånes RSS where ùë¶ùëñ are the actual values, ÀÜùë¶ùëñ are the predictions and ùëõ is the number of predictions. ùëÖùëÜùëÜ = ùëõ‚àëÔ∏Å ùëñ=1 (ùë¶ùëñ ‚àí ÀÜùë¶ùëñ)2 (3) As we only consider the error of the last prediction for the weight calculations corresponding with the next prediction, Equation 3 can be reduced to: ùëÖùëÜùëÜùëñ = (ùë¶ùëñ ‚àí ÀÜùë¶ùëñ)2 (4) Based on preliminary experiments and to manage complexity of the experiments, we consider 200 time series observations when training the ùëÄpartial model. Our proposed ECW and GDW methods are diÔ¨Äerent based on the algorithms they use to calculate the weights of ùëÄpartial and ùëÄall, which we explain in detail in the next sections. 4.2.1 Error Contribution Weighting The ECW method directly outputs the prediction of ùëÄall as the Ô¨Årst prediction. From the second prediction onwards, it determines the weights that should be used to combine the predictions of the two sub-models as follows. The prediction errors of ùëÄpartial and ùëÄall for the ùëñùë° ‚Ñé prediction, ùúñ ùëùùëñ and ùúñùëéùëñ , are deÔ¨Åned as shown in Equations 5 and 6, respectively, considering RSS as the loss function. ùúñ ùëùùëñ = (ùë¶ùëñ ‚àí ÀÜùë¶partialùëñ )2 (5) ùúñùëéùëñ = (ùë¶ùëñ ‚àí ÀÜùë¶allùëñ )2 (6) The error percentages of ùëÄpartial and ùëÄall for the ùëñùë° ‚Ñé prediction, ùê∏ ùëùùëñ and ùê∏ùëéùëñ , are deÔ¨Åned as shown in Equations 7 and 8, respectively. ùê∏ ùëùùëñ = ùúñ ùëùùëñ ùúñ ùëùùëñ + ùúñùëéùëñ (7) ùê∏ùëéùëñ = ùúñùëéùëñ ùúñ ùëùùëñ + ùúñùëéùëñ (8) The weights corresponding with ùëÄpartial and ùëÄall for the ùëñùë° ‚Ñé prediction, ùë§ ùëùùëñ and ùë§ùëéùëñ , can be then deÔ¨Åned as shown in Equations 9 and 10, respectively. In this way, the higher weight is assigned to the model with the lowest previous prediction error. ùë§ ùëùùëñ = ùê∏ùëéùëñ‚àí1 (9) 10 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir ùë§ùëéùëñ = ùê∏ ùëùùëñ‚àí1 (10) Equation 11 shows the formula used to obtain the Ô¨Ånal prediction corresponding with the ùëñùë° ‚Ñé time point, ÀÜùë¶ùëñ, after weighting. ÀÜùë¶ùëñ = ùë§ ùëùùëñ ¬∑ ÀÜùë¶ ùëùùëéùëü ùë°ùëñùëéùëôùëñ + ùë§ùëéùëñ ¬∑ ÀÜùë¶ùëéùëôùëôùëñ (11) Algorithm 1 shows the process of the ECW method that is used to obtain the prediction for a given time point. Algorithm 1 Error Contribution Weighting 1: procedure ecw(ùë¶ùëñ‚àí1, ÀÜùë¶ùëùùëéùëü ùë°ùëñùëéùëôùëñ‚àí1 , ÀÜùë¶ùëéùëôùëôùëñ‚àí1 , ÀÜùë¶ùëùùëéùëü ùë°ùëñùëéùëôùëñ , ÀÜùë¶ùëéùëôùëôùëñ , ùëñ) 2: if ùëñ is 1 then 3: ùë¶ùëñ ‚Üê ÀÜùë¶ùëéùëôùëôùëñ 4: else 5: ùúñùëéùëñ‚àí1 ‚Üê ùëêùëéùëôùëêùë¢ùëôùëéùë°ùëí_ùëü ùë†ùë† ( ùë¶ùëñ‚àí1, ÀÜùë¶ùëéùëôùëôùëñ‚àí1 ) 6: ùúñ ùëùùëñ‚àí1 ‚Üê ùëêùëéùëôùëêùë¢ùëôùëéùë°ùëí_ùëü ùë†ùë† ( ùë¶ùëñ‚àí1, ÀÜùë¶ùëùùëéùëü ùë°ùëñùëéùëôùëñ‚àí1 ) 7: ùë§ùëéùëñ ‚Üê ùúñ ùëùùëñ‚àí1 ùúñ ùëùùëñ‚àí1 +ùúñùëéùëñ‚àí1 8: ùë§ùëùùëñ ‚Üê ùúñùëéùëñ‚àí1 ùúñ ùëùùëñ‚àí1 +ùúñùëéùëñ‚àí1 9: ùë¶ùëñ ‚Üê ùë§ùëùùëñ ¬∑ ÀÜùë¶ùëùùëéùëü ùë°ùëñùëéùëôùëñ + ùë§ùëéùëñ ¬∑ ÀÜùë¶ùëéùëôùëôùëñ 10: end if 11: return ùë¶ùëñ 12: end procedure 4.2.2 Gradient Descent Weighting The GDW method uses gradient descent to determine the weights of the sub-models. The sub-models are initialised with weights of 0.5. This method also directly outputs the prediction of ùëÄall as the Ô¨Årst prediction. From the second prediction onwards, it determines the weights that should be used to combine the predictions of the two sub-models using gradient descent as follows. The Ô¨Ånal error of the ùëñùë° ‚Ñé prediction, ùúñùëñ, is deÔ¨Åned as shown in Equation 12 considering RSS as the loss function. ùúñùëñ = (ùë¶ùëñ ‚àí ùë§ ùëùùëñ ¬∑ ÀÜùë¶partialùëñ ‚àí ùë§ùëéùëñ ¬∑ ÀÜùë¶allùëñ )2 (12) The gradients calculated from ùëÄpartial and ùëÄall for the ùëñùë° ‚Ñé prediction, ùëî ùëùùëñ and ùëîùëéùëñ , are deÔ¨Åned as shown in Equations 13 and 14, respectively. ùëî ùëùùëñ = ‚àáùúñùëñ (ùë§ ùëùùëñ ) = ‚àí2 ¬∑ ÀÜùë¶partialùëñ ¬∑ ùúñùëñ (13) ùëîùëéùëñ = ‚àáùúñùëñ (ùë§ùëéùëñ ) = ‚àí2 ¬∑ ÀÜùë¶allùëñ ¬∑ ùúñùëñ (14) Handling Concept Drift in Global Time Series Forecasting 11 To avoid large gaps between two adjacent weights, the gradients are multiplied by an adjusted rate ùòÇ. The updated weights of ùëÄpartial and ùëÄall for the ùëñùë° ‚Ñé prediction, ùë§ ùëùùëñ and ùë§ùëéùëñ , are then deÔ¨Åned as shown in Equations 15 and 16, respectively. Based on our preliminary experiments, the value of ùòÇ is Ô¨Åxed to 0.01. ùë§ ùëùùëñ = ùë§ ùëùùëñ‚àí1 ‚àí ùëî ùëùùëñ‚àí1 ¬∑ ùòÇ (15) ùë§ùëéùëñ = ùë§ùëéùëñ‚àí1 ‚àí ùëîùëéùëñ‚àí1 ¬∑ ùòÇ (16) Equation 17 shows the formula used to obtain the Ô¨Ånal prediction corresponding with the ùëñùë° ‚Ñé time point after weighting. ÀÜùë¶ùëñ = ùë§ ùëùùëñ ¬∑ ÀÜùë¶ ùëùùëéùëü ùë°ùëñùëéùëôùëñ + ùë§ùëéùëñ ¬∑ ÀÜùë¶ùëéùëôùëôùëñ (17) Algorithm 2 shows the process of the GDW method that is used to obtain the prediction for a given time point. Algorithm 2 Gradient Descent Weighting 1: procedure gdw(ùë¶ùëñ‚àí1, ÀÜùë¶ùëñ‚àí1, ÀÜùë¶ùëùùëéùëü ùë°ùëñùëéùëôùëñ‚àí1 , ÀÜùë¶ùëéùëôùëôùëñ‚àí1 , ÀÜùë¶ùëùùëéùëüùë°ùëñùëéùëôùëñ , ÀÜùë¶ùëéùëôùëôùëñ , ùë§ùëùùëñ‚àí1 , ùë§ùëéùëñ‚àí1 , ùòÇ, ùëñ) 2: if ùëñ is 1 then 3: ùë¶ùëñ ‚Üê ÀÜùë¶ùëéùëôùëôùëñ 4: else 5: ùúñùëñ‚àí1 ‚Üê ùëêùëéùëôùëêùë¢ùëôùëéùë°ùëí_ùëü ùë†ùë† ( ùë¶ùëñ‚àí1, ÀÜùë¶ùëñ‚àí1) 6: ùëîùëùùëñ‚àí1 ‚Üê ‚àí2 ¬∑ ÀÜùë¶ùëùùëéùëü ùë°ùëñùëéùëôùëñ‚àí1 ¬∑ ùúñùëñ‚àí1 7: ùëîùëéùëñ‚àí1 ‚Üê ‚àí2 ¬∑ ÀÜùë¶ùëéùëôùëôùëñ‚àí1 ¬∑ ùúñùëñ‚àí1 8: ùë§ùëùùëñ = ùë§ùëùùëñ‚àí1 ‚àí ùëîùëùùëñ‚àí1 ¬∑ ùòÇ 9: ùë§ùëéùëñ = ùë§ùëéùëñ‚àí1 ‚àí ùëîùëéùëñ‚àí1 ¬∑ ùòÇ 10: ùë¶ùëñ ‚Üê ùë§ùëùùëñ ¬∑ ÀÜùë¶ùëùùëéùëüùë°ùëñùëéùëôùëñ + ùë§ùëéùëñ ¬∑ ÀÜùë¶ùëéùëôùëôùëñ 11: end if 12: return ùë¶ùëñ 13: end procedure When training ùëÄpartial and ùëÄall models, we consider both exponential and linear weighting methods which internally weight the training instances as explained in Section 4.1. Thus, the forecasts are obtained from four model combinations, consid- ering two models and two weighting methods. The Ô¨Ånal forecasts of the ECW and GDW methods are obtained by averaging the forecasts provided by the above four model combinations. Our proposed methods are applicable to any GFM including neural networks, machine learning and deep learning models. However, for our experiments, we consider LightGBM [11] as the base learner due to its recent competitive performance over the state-of-the-art forecasting models including deep learning models [31]. 12 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir 5 Experimental Framework In this section, we discuss the experimental datasets, error metrics, evaluation method, and benchmarks used in our experiments. 5.1 Datasets We are unaware of any publicly available real-world datasets which contain series showing sudden, incremental and gradual concept drift types, adequately, to evaluate the performance of our proposed methods, ECW and GDW. Thus, in this study, we limit ourselves to the use of simulated datasets that contain series representing sudden, incremental and gradual concept drift types. Let ts1 and ts2 be two time series simulated from the same AR(3) data generation process with diÔ¨Äerent initial values and a random seed. The two series are then combined based on the required concept drift type as shown in Equation 18, where ùë•ùëñ is the ùëñùë° ‚Ñé element of the combined series and series_length is the length of the series. ùë•ùëñ (sudden) = {ts1ùëñ if ùëñ < ùë°drift ts2ùëñ if ùëñ ‚â• ùë°drift ùë•ùëñ (incremental) = Ô£±Ô£¥Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£¥ Ô£≥ ts1ùëñ if ùëñ < ùë°start (1 ‚àí ùë§)ts1ùëñ + (ùë§)ts2ùëñ if ùë°start ‚â§ ùëñ ‚â§ ùë°end ts2ùëñ if ùëñ > ùë°end ùë•ùëñ (gradual) = {ts1ùëñ if ùêºùëñ = 0 ts2ùëñ if ùêºùëñ = 1 (18) Where ùë§ = ùëñ ‚àí ùë°start ùë°start ‚àí ùë°end ùêºùëñ is random based on probability ùëùùëñ ùëùùëñ = ùëñ series_length For sudden concept drift, the drift point, ùë°drift, is randomly selected. The combined series contains ts1 before ùë°drift and ts2 on and after ùë°drift. For incremental concept drift, the drift starting point, ùë°start, and drift ending point, ùë°end, are randomly selected. The combined series contains ts1 before ùë°start and ts2 after ùë°end. A linear weighted combination of ts1 and ts2 is considered in between ùë°start and ùë°end where ùë§ and (1‚àí ùë§) are the weights of ts2 and ts1, respectively. For gradual concept drift, the combined series contains values from either ts1 or ts2 depending on the corresponding ùêºùëñ Handling Concept Drift in Global Time Series Forecasting 13 which is randomised based on an increasing probability, ùëùùëñ. In general, ùëùùëñ is used to represent the probability that an instance belongs to ts2. Thus, when ùëùùëñ = 1 the series ultimately enters the new distribution. Finally, three simulated datasets are created that respectively contain series representing sudden, incremental and gradual concept drift types. Each dataset has 2000 series. The length of each series in all datasets is 2000. The Ô¨Årst 1650 data points in each series are used for model training whereas the last 350 data points in each series are reserved for testing. Note that the drift can occur in either training or testing parts of the series. 5.2 Error Metrics We measure the performance of our models using Root Mean Squared Error (RMSE) and Mean Absolute Error [MAE, 32] which are commonly used error metrics in the time series forecasting Ô¨Åeld. Equations 19 and 20 respectively deÔ¨Åne the RMSE and MAE error metrics. Here, ùêπùëò are the forecasts, ùëåùëò are the actual values for the required forecast horizon and ‚Ñé is the forecast horizon. ùëÖùëÄùëÜùê∏ = ‚àöÔ∏Ñ √ç‚Ñé ùëò=1 |ùêπùëò ‚àí ùëåùëò |2 ‚Ñé (19) ùëÄ ùê¥ùê∏ = √ç‚Ñé ùëò=1 |ùêπùëò ‚àí ùëåùëò | ‚Ñé (20) Since all these error measures are deÔ¨Åned for each time series, we calculate the mean and median values of them across a dataset to measure the model performance. Therefore, four error metrics are used to evaluate each model: mean RMSE, median RMSE, mean MAE and median MAE. 5.3 Evaluation We use prequential evaluation [33] to evaluate the performance of the models. This procedure is also a common evaluation technique used in the forecasting space, where it is normally called time series cross-validation [13]. We conduct the prequential evaluation with increasing block sizes [34] as follows. First, the forecast horizon of each series which consists of 350 data points is divided into 7 test sets of 50 data points each. The models are then trained with the training set and 50 predictions are obtained corresponding with the Ô¨Årst test set, one at a time iteratively. This procedure is also known as rolling origin without recalibration [35] as the models are not retrained when obtaining individual predic- tions. After obtaining the predictions corresponding with the Ô¨Årst test set, the actuals corresponding with the Ô¨Årst test set are also added to the training set. The models are retrained with the new training set and the 50 predictions corresponding with 14 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir the second test set are obtained, one at a time iteratively, in a rolling origin with recalibration fashion. This process is repeated until all 350 predictions are obtained corresponding with the complete forecast horizon. 5.4 Benchmarks We consider three types of LightGBM models: plain LightGBM that does not handle concept drift (Plain), LightGBM trained with exponential weighting (EXP) and LightGBM trained with linear weighting (Linear), as the main benchmarks of our study. We also consider three statistical models: AR(3) model, AR(5) model and ETS [1] as the benchmarks of our study. All statistical models are implemented using the Python package, StatsForecast [36]. Each baseline is separately trained considering all data points and the last 200 data points in the training series. Thus, twelve models are Ô¨Ånally considered as benchmarks, considering six LightGBM models and six statistical models. 5.5 Statistical Testing of the Results The non-parametric Friedman rank-sum test is used to assess the statistical signif- icance of the results provided by diÔ¨Äerent forecasting models across time series considering a signiÔ¨Åcance level of ùõº = 0.05. Based on the corresponding RMSE errors, the methods are ranked on every series of a given experimental dataset. The best method according to the average rank is chosen as the control method. To further characterise the statistical diÔ¨Äerences, Hochberg‚Äôs post-hoc procedure [37] is used. 6 Results and Discussion This section provides a comprehensive analysis of the results of all considered models in terms of main accuracy and statistical signiÔ¨Åcance, and later also gives more insights into the modelling. 6.1 Main Accuracy Results Table 1 shows the results of all models across sudden, incremental and gradual concept drift types for mean RMSE, median RMSE, mean MAE and median MAE. The models in Table 1 are grouped based on the sub-experiments. The results of the best performing models in each group are italicized, and the overall best Handling Concept Drift in Global Time Series Forecasting 15 Table 1: Results across sudden, incremental and gradual concept drift types. The best performing models in each group are italicized and the overall best performing models are highlighted in boldface. Mean RMSE Median RMSE Mean MAE Median MAE Sudden AR3_200 0.7318 0.5154 0.6184 0.4933 AR3_All 0.5973 0.5303 0.4724 0.4310 AR5_200 0.6964 0.6731 0.5723 0.5533 AR5_All 0.5516 0.5110 0.4736 0.4454 ETS_200 0.5525 0.5286 0.5025 0.4847 ETS_All 0.6118 0.5305 0.4883 0.4307 EXP_200 0.8409 0.7221 0.6830 0.5844 EXP_All 0.7379 0.5490 0.5939 0.4441 Linear_200 0.7631 0.6268 0.6137 0.5030 Linear_All 0.8113 0.6518 0.6431 0.5102 Plain_200 0.7838 0.6402 0.6273 0.5109 Plain_All 0.9242 0.7768 0.7334 0.6113 GDW 0.4056 0.2899 0.3044 0.2288 ECW 0.5812 0.4162 0.4598 0.3246 Incremental AR3_200 0.5796 0.5428 0.5001 0.4538 AR3_All 0.5602 0.5266 0.4501 0.4281 AR5_200 0.6124 0.5977 0.5377 0.5142 AR5_All 0.5637 0.5472 0.4551 0.4453 ETS_200 0.5874 0.5536 0.5544 0.5368 ETS_All 0.5746 0.5424 0.4634 0.4400 EXP_200 0.7524 0.7183 0.6101 0.5821 EXP_All 0.6236 0.5548 0.5018 0.4480 Linear_200 0.6745 0.6245 0.5403 0.5004 Linear_All 0.7365 0.6569 0.5836 0.5179 Plain_200 0.6943 0.6392 0.5541 0.5098 Plain_All 0.8710 0.7848 0.6892 0.6175 GDW 0.3629 0.3083 0.2805 0.2437 ECW 0.5541 0.4420 0.4320 0.3444 Gradual AR3_200 0.7931 0.7877 0.7588 0.7480 AR3_All 0.7939 0.7903 0.6819 0.6832 AR5_200 0.8011 0.7957 0.7832 0.7795 AR5_All 0.8637 0.8610 0.8531 0.8499 ETS_200 0.7853 0.7810 0.6973 0.6926 ETS_All 0.7936 0.7924 0.7328 0.7297 EXP_200 1.0023 0.9963 0.7810 0.7745 EXP_All 1.0292 1.0260 0.7825 0.7821 Linear_200 0.8998 0.9006 0.6962 0.6961 Linear_All 1.1786 1.1469 0.9170 0.8921 Plain_200 0.9067 0.9058 0.7036 0.7024 Plain_All 1.3042 1.2433 1.0200 0.9722 GDW 0.7168 0.7161 0.4617 0.4605 ECW 0.7716 0.7711 0.5686 0.5674 16 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir performing models across the datasets are highlighted in boldface. The Ô¨Årst group contains the statistical baselines we considered which are AR(3), AR(5) and ETS. The second group contains the LightGBM baselines we considered which are the plain LightGBM model (Plain), LightGBM trained with exponential weighting (EXP) and LightGBM trained with linear weighting (Linear). The baseline models also indicate the number of data points used for model training, which is either last 200 data points or all data points. The third group contains our proposed concept drift handling methods, ECW and GDW, which use two LightGBM models to provide forecasts. The hyperparameters of all LightGBM models such as maximum depth, number of leaves, minimum number of instances in a leaf, learning rate and sub- feature leaf are tuned using a grid search approach. For our experiments, we use the Python lightgbm package. In the Ô¨Årst group, diÔ¨Äerent statistical models show the best performance across diÔ¨Äerent concept drift types. Overall, AR5_All and AR3_All respectively show the best performance across sudden and incremental concept drift types. Across gradual concept drift, ETS_200 and AR3_All respectively show the best performance on RMSE and MAE metrics. In the second group, EXP_All shows the best performance across sudden and incremental concept drift types whereas Linear_200 shows the best performance across gradual concept drift. The models that use exponential weighting overall show a better performance than the models that use linear weighting. Across all concept drift types, overall, the methods that use exponential and linear weighting show a better performance than the plain LightGBM models which do not handle concept drift. This shows assigning higher weights for the recent series observations is beneÔ¨Åcial in handling concept drift. For diÔ¨Äerent concept drift types, usage of diÔ¨Äerent numbers of training data points shows best results and this indicates using two models trained with the recent series history and full series history may be a better way to handle concept drift. In the third group, our proposed GDW method shows the best performance across all concept drift types. In particular, this method outperforms all considered baselines and shows the best performance on all error metrics. Our proposed ECW method also outperforms all considered baselines on all error metrics across all concept drift types except for 2 cases: AR5_All and ETS_200 on mean RMSE across sudden concept drift. This further shows combining the forecasts of two models trained with the most recent series history and full series history is a proper way of handling concept drift in global time series forecasting. The model trained with the full series history has a high forecasting accuracy where the model trained with the recent series history can quickly adapt to new concepts. Thus, combining the forecasts provided by these two models leads to high forecasting accuracy in particular for the series showing concept drift. The performance improvements gained by the GDW method across diÔ¨Äerent concept drift types are relative to the characteristics of the drift type. In particu- lar, compared to the LightGBM baselines, the GDW method shows considerable improvements across the sudden and incremental concept drift types and smaller improvements across the gradual concept drift type in terms of mean RMSE and Handling Concept Drift in Global Time Series Forecasting 17 mean MAE. The drift occurs gradually in the series showing gradual concept drift and thus, there is a possibility that the models have seen instances of the new distri- bution during training. Hence, the baseline LightGBM models have also provided accurate forecasts up to some extent for the series showing gradual concept drift. However, with the series showing sudden or incremental concept drift, there is a high possibility that the models have seen fewer or no instances of the new distribution during training where the drift should be properly handled by the models to provide accurate forecasts. Our proposed GDW method can properly handle concept drift compared to the baselines and thus, across sudden and incremental concept drift types, it shows considerable performance improvements in terms of the forecasting accuracy. 6.2 Statistical Testing Results Table 2 shows the results of the statistical testing evaluation, namely the adjusted ùëù-values calculated from the Friedman test with Hochberg‚Äôs post-hoc procedure considering a signiÔ¨Åcance level of ùõº = 0.05 [37]. The statistical testing is separately conducted using the three experimental datasets corresponding with sudden, incre- mental and gradual concept drift types. For a given dataset, the RMSE values of each series provided by all methods are considered. Table 2: Results of statistical testing across sudden, incremental and gradual concept drift types. (a) Sudden concept drift Method ùëùHoch GDW ‚Äì ECW 3.56 √ó 10‚àí9 EXP_All < 10‚àí30 Linear_200 < 10‚àí30 Plain_200 < 10‚àí30 Linear_All < 10‚àí30 EXP_200 < 10‚àí30 Plain_All < 10‚àí30 AR3_200 < 10‚àí30 AR3_All < 10‚àí30 AR5_200 < 10‚àí30 AR5_All < 10‚àí30 ETS_200 < 10‚àí30 ETS_All < 10‚àí30 (b) Incremental concept drift Method ùëùHoch GDW ‚Äì ECW 3.48 √ó 10‚àí20 EXP_All < 10‚àí30 Linear_200 < 10‚àí30 Plain_200 < 10‚àí30 EXP_200 < 10‚àí30 Linear_All < 10‚àí30 Plain_All < 10‚àí30 AR3_200 < 10‚àí30 AR3_All < 10‚àí30 AR5_200 < 10‚àí30 AR5_All < 10‚àí30 ETS_200 < 10‚àí30 ETS_All < 10‚àí30 (c) Gradual concept drift Method ùëùHoch GDW ‚Äì ECW 8.01 √ó 10‚àí8 Linear_200 < 10‚àí30 Plain_200 < 10‚àí30 EXP_200 < 10‚àí30 EXP_All < 10‚àí30 Linear_All < 10‚àí30 Plain_All < 10‚àí30 AR3_200 < 10‚àí30 AR3_All < 10‚àí30 AR5_200 < 10‚àí30 AR5_All < 10‚àí30 ETS_200 < 10‚àí30 ETS_All < 10‚àí30 The overall ùëù-values of the Friedman rank-sum test corresponding with the sud- den, incremental and gradual concept drift types are less than 10‚àí30 showing the results are highly signiÔ¨Åcant. For all concept drift types, GDW performs the best 18 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir on ranking over RMSE per each series and thus, it is used as the control method as mentioned in the Ô¨Årst row in each sub-table of Table 2. In all sub-tables, horizontal lines are used to separate the models that perform signiÔ¨Åcantly worse than GDW. All benchmarks and ECW show ùëùHoch values less than ùõº across all concept drift types and thus, they are signiÔ¨Åcantly worse than the control method. 6.3 Further Insights Figure 2 shows the performance of all models in terms of MAE and RMSE, across the series showing sudden concept drift with diÔ¨Äerent drift points. According to the Ô¨Ågure, our proposed methods, GDW and ECW, show a better performance compared to the baselines in both RMSE and MAE where the GDW shows the best performance in both error metrics. Compared to the ECW and other baselines, the GDW shows a lower error increasing rate. All methods show higher errors for the series where the concept drift occurs in the test set (after 1650 data points) as the models have not seen the corresponding data distributions in the training set. However, even in this phenomenon, we see that the error increasing rate of the GDW method is considerably lower compared with the other methods. M A E R M S E Drift Point Drift Point Fig. 2: Performance of all models in terms of MAE (left) and RMSE (right), across the series showing sudden concept drift with diÔ¨Äerent drift points. Figure 3 shows the performance of all models in terms of MAE and RMSE, across the series showing incremental concept drift with diÔ¨Äerent drift lengths. Here, the Handling Concept Drift in Global Time Series Forecasting 19 drift length refers to the diÔ¨Äerence between the drift starting and ending points. According to the Ô¨Ågure, we see that all models show higher forecasting errors for shorter drift lengths, as the models are required to adapt to the new concept as quickly as possible for shorter drift lengths. Our proposed methods, GDW and ECW, are continuous adaptive weighting methods and they continuously adjust model weights based on the previous prediction error. Thus, GDW and ECW show a better performance compared to the other methods where GDW shows the best performance in both MAE and RMSE. M A E R M S E Drift Length Drift Length Fig. 3: Performance of all models in terms of MAE (left) and RMSE (right), across the series showing incremental concept drift with diÔ¨Äerent drift lengths. 7 Conclusions and Future Research Time series data distributions are often not stationary and as a result, the accuracy of ML forecasting models can decrease over time. Thus, handling such concept drift is a crucial issue in the Ô¨Åeld of ML methods for time series forecasting. However, the concept drift handling methods in the literature are mostly designed for classiÔ¨Åcation tasks, not for forecasting. In this study, we have proposed two new methods based on continuous adapting weighting, GDW and ECW, to handle concept drift in global time series forecasting. Each proposed method uses two sub-models, a model trained with the most recent series history and a model trained with the full series history, where the weighted 20 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir average of the forecasts provided by the two models are considered as the Ô¨Ånal forecasts. The proposed methods calculate the weights that are used to combine the sub-model forecasts based on the previous prediction error. During training, the sub-models internally assign higher weights for the recent observations either exponentially or linearly. Across three simulated datasets that demonstrate sudden, incremental and gradual concept drift types, we have shown that our proposed methods can signiÔ¨Åcantly outperform a set of baseline models. In particular, GDW shows the best performance across all considered concept drift types. From our experiments, we conclude that using two models trained with training sets of diÔ¨Äerent sizes is a proper approach to handle concept drift in global time series forecasting. We further conclude gradient descent is a proper approach to determine the weights that should be used to combine the forecasts provided by these models. Thus, we recommend the GDW method to handle concept drift in global time series forecasting. The success of this approach encourages as future work to explore the performance of GDW and ECW methods when using three or more sub-models trained using diÔ¨Äerent numbers of data points. Analysing the performance of the proposed methods based on the characteristics of the datasets such as the number of series, and the order and parameters of the AR data generation process is also worthwhile to study. It will be also interesting to study the performance of our proposed methods on real-world datasets that adequately demonstrate concept drift. Analysing the performance of our proposed methods across diÔ¨Äerent base learners including deep learning models is also a worthwhile endeavour. References [1] R. J. Hyndman, A. B. Koehler, J. K. Ord, and R. D. Snyder. Forecasting with Exponential Smoothing: The State Space Approach. Springer Science and Business Media, 2008. [2] T. Januschowski, J. Gasthaus, Y. Wang, D. Salinas, V. Flunkert, M. Bohlke- Schneider, and L. Callot. Criteria for classifying forecasting methods. Inter- national Journal of Forecasting, 36(1):167‚Äì177, 2020. [3] S. Makridakis, E. Spiliotis, and V. Assimakopoulos. The M4 competition: Results, Ô¨Åndings, conclusion and way forward. International Journal of Fore- casting, 34(4):802‚Äì808, 2018. [4] S. Makridakis, E. Spiliotis, and V. Assimakopoulos. The M5 accuracy compe- tition: Results, Ô¨Åndings and conclusions. International Journal of Forecasting, 38(4):1346‚Äì1364, 2022. [5] P. Montero-Manso and R. J. Hyndman. Principles and algorithms for fore- casting groups of time series: Locality and globality. International Journal of Forecasting, 2021. ISSN 0169-2070. [6] G. I. Webb, R. Hyde, H. Cao, H. L. Nguyen, and F. Petitjean. Characterizing concept drift. Data Mining and Knowledge Discovery, 30(4):964‚Äì994, 2016. Handling Concept Drift in Global Time Series Forecasting 21 [7] J. Z. Kolter and M. A. Maloof. Dynamic weighted majority: An ensemble method for drifting concepts. The Journal of Machine Learning Research, 8: 2755‚Äì2790, 2007. [8] H. M. Gomes and F. Enembreck. SAE: Social adaptive ensemble classiÔ¨Åer for data streams. In IEEE Symposium on Computational Intelligence and Data Mining, pages 199‚Äì206, 2013. [9] L. Baier, M. Hofmann, N. K√ºhl, M. Mohr, and G. Satzger. Handling con- cept drifts in regression problems‚Äìthe error intersection approach. In 15th International Conference on Wirtschaftsinformatik, 2020. [10] H. M. Gomes, J. P. Barddal, F. Enembreck, and A. Bifet. A survey on ensemble learning for data stream classiÔ¨Åcation. ACM Computing Surveys (CSUR), 50 (2):1‚Äì36, 2017. [11] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. Liu. LightGBM: A highly eÔ¨Écient gradient boosting decision tree. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS‚Äô17, page 3149‚Äì3157, Red Hook, NY, USA, 2017. Curran Associates Inc. [12] S. J. Delany, P. Cunningham, A. Tsymbal, and L. Coyle. A case-based technique for tracking concept drift in spam Ô¨Åltering. In International Conference on Innovative Techniques and Applications of ArtiÔ¨Åcial Intelligence, pages 3‚Äì16. Springer, 2004. [13] R. J. Hyndman and G. Athanasopoulos. Forecasting: Principles and Practice. OTexts, 2nd edition, 2018. [14] D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski. DeepAR: Probabilis- tic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181‚Äì1191, 2020. [15] S. Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting. International Journal of Forecasting, 36 (1):75‚Äì85, 2020. [16] K. Bandara, C. Bergmeir, and S. Smyl. Forecasting across time series databases using recurrent neural networks on groups of similar series: A clustering ap- proach. Expert Systems with Applications, 140:112896, 2020. [17] H. Hewamalage, C. Bergmeir, and K. Bandara. Recurrent neural networks for time series forecasting: Current status and future directions. International Journal of Forecasting, 2020. [18] R. Godahewa, K. Bandara, G. I. Webb, S. Smyl, and C. Bergmeir. Ensembles of localised models for time series forecasting. Knowledge-Based Systems, 233: 107518, 2021. [19] B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio. Meta-learning frame- work with applications to zero-shot time-series forecasting. In AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 35, 2021. [20] M. Mergenthaler and F. G. Ram√≠rez. Nixtla: Transfer learning for time series forecasting, 2022. URL https://github.com/Nixtla/ transfer-learning-time-series. 22 Ziyi Liu, Rakshitha Godahewa, Kasun Bandara and Christoph Bergmeir [21] B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio. N-BEATS: Neu- ral basis expansion analysis for interpretable time series forecasting. In 8th International Conference on Learning Representations (ICLR), 2020. [22] C. Challu, K. G. Olivares, B. N. Oreshkin, F. Garza, M. Mergenthaler-Canseco, and A. Dubrawski. N-HiTS: Neural hierarchical interpolation for time series forecasting. In AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 37, 2023. [23] G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. Hoi. Deeptime: Deep time-index meta-learning for non-stationary time-series forecasting. https://arxiv. org/abs/2207.06046, 2022. [24] R. Grazzi, V. Flunkert, D. Salinas, T. Januschowski, M. Seeger, and C. Ar- chambeau. Meta-forecasting by combining global deep representations with local adaptation. https://arxiv.org/abs/2111.03418, 2021. [25] R. Ye and Q. Dai. A novel transfer learning framework for time series forecast- ing. Knowledge-Based Systems, 156:74‚Äì99, 2018. [26] H. Ghomeshi, M. M. Gaber, and Y. Kovalchuk. EACD: Evolutionary adaptation to concept drifts in data streams. Data Mining and Knowledge Discovery, 33 (3):663‚Äì694, 2019. [27] F. Chu and C. Zaniolo. Fast and light boosting for adaptive mining of data streams. In PaciÔ¨Åc-Asia Conference on Knowledge Discovery and Data Mining, pages 282‚Äì292. Springer, 2004. [28] B. Krawczyk, L. Minku, J. Gama, J. Stefanowski, and M. Wo≈∫niak. Ensemble learning for data stream analysis: A survey. Information Fusion, 37:132‚Äì156, 2017. [29] P. M. Gon√ßalves Jr and R. S. S. De Barros. RCD: A recurring concept drift framework. Pattern Recognition Letters, 34(9):1018‚Äì1025, 2013. [30] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69‚Äì101, 1996. [31] T. Januschowski, Y. Wang, K. Torkkola, T. Erkkil√§, H. Hasson, and J. Gasthaus. Forecasting with trees. International Journal of Forecasting, 2021. [32] Mean absolute error. In C. Sammut and G. I. Webb, editors, Encyclopedia of Machine Learning, pages 652‚Äì652. Springer US, Boston, MA, 2010. [33] A. P. Dawid. Present position and potential developments: Some personal views statistical theory the prequential approach. Journal of the Royal Statistical Society: Series A (General), 147(2):278‚Äì290, 1984. [34] V. Cerqueira, L. Torgo, and I. Mozetiƒç. Evaluating time series forecasting models: An empirical study on performance estimation methods. Machine Learning, 109(11):1997‚Äì2028, 2020. [35] L. J. Tashman. Out-of-sample tests of forecasting accuracy: An analysis and review. International journal of forecasting, 16(4):437‚Äì450, 2000. [36] F. Garza, M. M. Canseco, C. Chall√∫, and K. G. Olivares. StatsForecast: Lightning fast forecasting with statistical and econometric models. PyCon Salt Lake City, Utah, US, 2022. URL https://github.com/Nixtla/ statsforecast. [37] S. Garc√≠a, A. Fern√°ndez, J. Luengo, and F. Herrera. Advanced nonparametric tests for multiple comparisons in the design of experiments in computational Handling Concept Drift in Global Time Series Forecasting 23 intelligence and data mining: Experimental analysis of power. Information Sciences, 180(10):2044‚Äì2064, 2010.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources.backup/Lainder22frcstGBTaugTuneCV.pdf","text":"International Journal of Forecasting Volume 38, Issue 4, October–December 2022, Pages 1426-1433 Forecasting with gradient boosted trees: augmentation, tuning, and cross-validation strategies: Winning solution to the M5 Uncertainty competition Author links open overlay panel A. David Lainder a , Russell D. Wolfinger b https://doi.org/10.1016/j.ijforecast.2021.12.003 Get rights and content Abstract Deep neural networks and gradient boosted tree models have swept across the field of machine learning over the past decade, producing across-the-board advances in performance. The ability of these methods to capture feature interactions and nonlinearities makes them exceptionally powerful and, at the same time, prone to overfitting, leakage, and a lack of generalization in domains with target non-stationarity and collinearity, such as time-series forecasting. We offer guidance to address these difficulties and provide a framework that maximizes the chances of predictions that generalize well and deliver state-of-the-art performance. The techniques we offer for cross-validation, augmentation, and parameter tuning have been used to win several major time-series forecasting competitions—including the M5 Forecasting Uncertainty competition and the Kaggle COVID19 Forecasting series—and, with the proper theoretical grounding, constitute the current best practices in time- series forecasting. Introduction Recent years have witnessed explosive growth in the use of two major classes of predictive modeling methods: gradient boosted trees (GBTs) and neural networks (NNs). Progress has been accelerated by data science competition platforms like Kaggle (Kaggle, 2010) as well as the availability of high-quality open-source packages like XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), and Catboost (Dorogush et al., 2018, Prokhorenkova et al., 2017) for GBTs, and PyTorch (Paszke et al., 2019) and Tensorflow/Keras (Abadi et al., 2016, Chollet et al., 2015) for NNs. The winners of every machine learning competition in recent years featuring quantitative datasets have leveraged some combination of these in their solutions. In the fields of computer vision and natural language processing, fairly standard techniques for network design, data augmentation, and training have emerged and provide a nearly automatic strong baseline for many tasks. These fields usually involve enormous datasets, such as ImageNet (Deng et al., 2009), that allow for a straightforward comparison of methods. Most areas of time-series forecasting, however, are just the opposite. In key use cases, such as forecasting product sales based on early traction or predicting the course of a novel epidemic, the number of truly independent observations is quite low and it is easy to fit a perfect model with the benefit of hindsight. In such cases, where the number of independent observations is in the thousands rather than millions, the only way to measure actual generalizable performance is to forecast based on a known set of data, and be judged by on its predictive ability on a completely hidden, or often yet-to-happen, set of outcomes. The best practices and theory discussed in this article have largely been forged in the fires of data science competitions over the past several years and draw upon lessons learned amidst thousands of hours of crowd- sourced experimentation. Richly detailed solution descriptions, code, and forum discussions have greatly 6/25/24, 12:29 PM Forecasting with gradient boosted trees: augmentation, tuning, and cross-validation strategies: Winning solution to the M5 Uncerta… chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/5 facilitated the sharing of ideas and provide the primary references for our work. The techniques we describe are certainly not limited in efficacy to M5 Uncertainty; they are the culmination and refinement of successful methods employed in Kaggle time-series competitions over the past few years, as listed in Table 1. For example, over the course of 2020, variations on these ideas were used to win multiple rounds of the COVID19 Global Forecasting series (Kaggle, 2020a, Kaggle, 2020b, Kaggle, 2020c, Kaggle, 2020d, Kaggle, 2020e), including consecutive wins in Rounds 3 and 4 and consistent top performance out of hundreds of teams amidst continuing national and international attention on this topic (Cramer, Ray, et al, & Reich, 2021). This sequence of time-series competitions, a welcome addition to the machine learning landscape, provided the training ground to hone and optimize these techniques to the point where they are a framework easily applicable to any form of time-series forecasting, and capable of producing world-class performance, or at least very strong performance, if applied correctly. Section snippets M5 competition The M competitions, organized by Professor Spyros Makridakis and colleagues (Makridakis et al., 2021a, Makridakis et al., 2021b), provide a well-regarded proving ground for state-of-the-art time series methods. The most recent M5 competition (Kaggle, 2020f, Kaggle, 2020g), hosted on Google’s Kaggle machine learning competition platform, enjoyed a particularly large field of over 5000 teams. The M5 competition featured five years of daily sales data on over 30,000 Walmart store-item combinations, Cross-validation Machine learning is the art and science of training a model on one dataset in a way that maximizes its performance measured on unseen data. Time-series forecasting takes this to a whole new level, as while it may be possible to achieve perfect accuracy in classifying images, the future is inherently unknowable and most time series worth forecasting have inherently high levels of volatility. Cross-validation (CV) is typically the most straightforward part of machine learning: divide the set into Model selection A common rule of thumb is that NNs are all but mandatory for spatially oriented data, e.g. images or audio, while GBTs outperform on quantitative or tabular datasets. While both classes of methods are capable of modeling feature interactions and non-linearities, GBTs are typically both faster and easier to train, and much more straightforward to regularize. Both of these are enormous benefits when forecasting 108 separate targets (12 levels 9 quantiles) across five very different folds, as in Conclusions The M5 competitions, this Special Issue of the IJF, and this article provide a crucial step in moving time-series forecasting forward as a distinct field of machine learning, one with best practices capable of producing strong models quickly and reliably. The challenge of predicting the future is that it does not yet exist, and a strong theoretical grounding is necessary to build models that fully generalize while avoiding overfitting to the past and present. Machine learning models are capable 6/25/24, 12:29 PM Forecasting with gradient boosted trees: augmentation, tuning, and cross-validation strategies: Winning solution to the M5 Uncerta… chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/5 Python code The M5 Uncertainty winning solution code and further details are available at http://github.com/david-1013/m5 . Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We thank Professor Makridakis and the M Competition Team, Walmart, and Kaggle for organizing and sponsoring the M5 competition. We also sincerely appreciate top Kaggle competitors who have freely shared their ingenious solutions and wisdom over the past several years. References (30) Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., & Dean, J., et al. (2016). Tensorflow: A system for large-scale... BuslaevA. et al. Albumentations: Fast and flexible image augmentations Information (2020) ChenT. et al. XGBoost: A scalable tree boosting system CholletF. Keras (2015) CramerE.Y. et al. Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the US (2021) Cubuk, E., Zoph, B., Shlens, J., & Le, Q. (2020). Randaugment: Practical automated data augmentation with a reduced... De PradoM.L. Advances in Financial Machine Learning (2018) 6/25/24, 12:29 PM Forecasting with gradient boosted trees: augmentation, tuning, and cross-validation strategies: Winning solution to the M5 Uncerta… chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/5 DengJ. et al. Imagenet: A large-scale hierarchical image database DorogushA.V. et al. CatBoost: gradient boosting with categorical features support (2018) KaggleA.V. Kaggle competitions (2010) KaggleA.V. Walmart store sales (2014) KaggleA.V. Rossmann store sales (2015) KaggleA.V. Walmart stormy weather (2015) KaggleA.V. Wikipedia web traffic time series forecasting (2017) KaggleA.V. Corporacion favorita grocery sales forecasting (2018) There are more references available in the full text version of this article. Cited by (14) A spatial–temporal model for network-wide flight delay prediction based on federated learning 2024, Applied Soft Computing Show abstract 6/25/24, 12:29 PM Forecasting with gradient boosted trees: augmentation, tuning, and cross-validation strategies: Winning solution to the M5 Uncerta… chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/5 A novel method of creating machine learning-based time series meta-models for building energy analysis 2023, Energy and Buildings Citation Excerpt : The data folding technique is used to provide a robust estimation of predictive performance from the models selected in the previous step. The data folding method used in this step is different from the conventional cross-validation [50]. The variable importance would be applied to determine the most important variable affecting building energy use [51]. Show abstract Short-Term Electricity Consumption Forecasting at Residential Level Using Two- Phase Hybrid Machine Learning Model 2024, Electrica Transferability of predictive models to map susceptibility of ephemeral gullies at large scale 2024, Natural Hazards Sentiment Analysis On Arabic Companies Reviews 2024, 6th International Conference on Computing and Informatics, ICCI 2024 I Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives 2023, arXiv View all citing articles on Scopus View full text © 2021 Published by Elsevier B.V. on behalf of International Institute of Forecasters. 6/25/24, 12:29 PM Forecasting with gradient boosted trees: augmentation, tuning, and cross-validation strategies: Winning solution to the M5 Uncerta… chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/5","libVersion":"0.3.2","langs":""}
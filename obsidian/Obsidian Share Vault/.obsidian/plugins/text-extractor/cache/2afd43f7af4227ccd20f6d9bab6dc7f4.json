{"path":"lit/lit_sources/Manguin16ElectricityPricePrediction.pdf","text":"UC Berkeley CE 263N - Scalable Spatial Analytics Electricity Price Prediction and Nodal Price Analysis in California ISO Authors: M´elanie Manguin Emily Porter Bertrand Travacca Professor: Alexey Pozdnukhov December 8, 2016 1 Introduction In the United States, an independent system operator (ISO) controls and monitors the operation of the electrical power system within a given perime- ter. In California, CAISO also acts as the electricity market operator. In this project we focus our attention on the day ahead market (DAM) for electric- ity. This market takes place one day prior to the operating day and consists of scheduling the quantities and marginal electricity prices (wholesale) for each hour. We will denote p ∈ R24 the DAM price for a given day. The marginal clearing price corresponds to the intersection between the supply and demand curve: these curves are formed by CAISO who receive bids from all the market participants (essentially: electricity retailers, gener- ators, and virtual bidders). Figure 1 gives an overview of the DAM market process, while Figure 2 illustrates the obtainment of the market clearing price for a given hour. CAISO has the speciﬁcity of having diﬀerent clearing prices based on lo- cation, this is done with the purpose of including externalities of electricity losses (around 6% on the transmission grid [6]), and electricity congestion in the ﬁnal price [7]. All in all, the clearing price in the DAM is speciﬁc to a location (or node), and is referred to as Local Marginal Price (LMP). It is essential for a market participant to be able to predict the DAM price before submitting it’s bid to CAISO. That is why in a ﬁrst step we decided to build a prediction model for the DAM price (average price in the system, not the LMP). It is also important for the market participant to understand the underlying risk when making a bid: that is why we studied the error between our prediction and the realized price. In a second step, we examined and visualized the LMP behavior, which is particularly interesting if we want to understand the local market power an agent can have. 1 Figure 1: Day-Ahead Market overview. Figure 2: Clearing price: intersection between the demand and supply curves. 2 Price prediction, simulation, and risk man- agement We gathered 3 years of data (Jan 2013- Dec 2015) from CAISO (PG&E region only) to build a prediction model ˆp ∈ R24 and produce a covariance matrix estimate ˆΣp, such that ppp ∼ N (ˆp, ˆΣp). The data was taken from [4]. Note that this data corresponds to an average in the PG&E electric region 2 (this is not an LMP as in Section 3 or 4). 2.1 DAM price prediction We started by producing a prediction model for DAM prices using random forest regression [2]. The input/features for predicting the price are: • demand forecast (CAISO) • year, month, day, and hour Method 1: We started by randomly picking 80% of the days in the dataset to train our prediction model, and we then tested our model on the remaining 20%. We found a RMSE around 3.5$. Method 2: In order to build a more realistic model we implemented an ’online version’: we started by training our model with the data of 2013, and then recursively updated our model and prediction as follow. Let us denote pd = {p1, ...pd} the price history we have at a given day. Let us denote Md the random forest regression model corresponding to this historical data. In order to predict the next day price ˆpd+1 ∈ R24 we use the model Md we have. For the next day, we build an updated model Md+1 using historical data pd+1, and we use this new model to predict the price ˆpd+2, etc. With this model we obtained a RMSE of 4.5 to 5$. 2.2 Risk management for a market participant (moti- vation) Let us consider a market participant who wants to bid a quantity Q ∈ R24 in the DAM. This participant does not know yet what is the clearing price (cf. Introduction). If this agent wants to chose the optimal bid Q∗ with respect to its portfolio, only considering minimizing its cost ˆpT Q := ∑24 h=1 ˆpT h Qh would correspond to a risk neutral attitude. Most of the time, an agent is risk averse regarding the bid he makes. The choice of the amount Qh for h ∈ {1, ...24} can be considered as a classic portfolio problem where the assets are the DAM prices [1]. In a way, the risk averse agent does not want to ”put all his eggs in the same basket” and looks 3 for a trade-oﬀ between the expected DAM price ˆp and its variance QT ΣpQ, where Σp ∈ R24x24 denotes the covariance of the error on prediction. This digression motivates the following. 2.3 Covariance matrix estimation and simulation using Cholesky decomposition In a second step, we looked at the error ǫ between the predicted and realized price. ǫd ∈ R24 refers to the error vector for the day d. We denote, δ = 1 N Nd∑ k=1 ǫkǫT k ∈ R24x24 We assume that ǫ is a multivariate Gaussian random variable with zero mean (note: we empirically have an error ∼ 0) and covariance matrix Σp. We want to estimate this covariance matrix using the samples we have. We consider each ǫd to be statistically independent. The density function for ǫ is given by: f (x) = (2π)−12 det(Σp)−1/2exp(−xT Σ−1 p x/2) The log-likelihood function has the form, L = log f (Πǫd) = −(12Nd) log(2π) + (Nd/2) log det Σ−1 p − (Nd/2)trace(Σ−1 p δ) Note that L is a concave function of Σ−1 p (often referred to as precision matrix). We can have an estimate of the precision matrix solving the follow- ing strictly convex optimization problem: ˆΣ−1 p := arg max Θ≻0 log det(Θ) − trace(Θδ) Or equivalently, ˆΣ−1 p := arg min Θ≻0 trace(Θδ) − log det(Θ) Adding a L1 norm regularization term (lasso regularization) to this convex optimization problem allows to obtain a sparse precision matrix [5], which is (heuristically) useful if we want to have deeper insight on the error correlation between diﬀerent hours and erase ’weak’ correlations. The sparse covariance is obtained solving the following convex optimization problem: 4 ˆΣ−1 p = arg min Θ≻0 traceΘδ − log(det(Θ)) + β∥Θ∥1 Nevertheless, we found that β should be kept very small if we do not want to deteriorate the covariance matrix model. This tends to prove that, in our case, only using the empirical estimation might be reasonable. We used scikit learn [2] to obtain an estimate for the covariance matrix (we did this with the two prediction methods in 2.1). Figure 3 and 4 on page 6 display a heat-map of the covariance matrix estimate we have for the DAM price respectively with Method 1 and Method 2 (’online learning’). We can observe that the online learning method results are higher valued on the diagonal but also show high correlation for the error from one hour to the other. This is really diﬀerent for Method 1 where the covariance matrix is mostly diagonal. Figure 5 on page 6. shows some simulations of a priori possible DAM prices: psimu = Lu + ˆp with u ∼ N (0, I) and L cholesky decomposition of the covariance matrix (from Method 2). We can see that the covariance matrix estimate is not able to capture all the uncertainty on the prediction we make: for instance we can observe that all the simulations tend to expect a higher price between midnight and 3AM. Figure 6 conﬁrms this phenomenon, with simulations unable to capture high prices between 10AM and 5PM. This observation motivates section 2.4. 5 Figure 3: Heat map of the covariance matrix estimate ˆΣp using method 1 of Section 2.1. Figure 4: Heat map of the covariance matrix estimate ˆΣp using method 2 of Section 2.1. Figure 5: Day Ahead price simulations (colored lines) and real price (black dashed line). 6 Figure 6: Day Ahead price simulations (colored lines) and real price (black dashed line). 2.4 Gaussian Mixture Model on the error This last observation made us consider the fact that the error should be modeled as a Gaussian Mixture Model (GMM) rather than a simple Gaussian model (or a GMM with one component). Indeed, it is possible to imagine that the error can be clustered with respect to the day: for instance, for a given day our estimate might be below/above the true price all day. We used sklearn mixture library to ﬁt a GMM to the error using a BIC criterion to deﬁne the number of components for the GMM. Figure 7. shows the BIC score for a diﬀerent number of components. We found that the optimal num- ber of components was one. This means that we cannot improve our model on the error with GMM. This result is still interesting: if we go back to the digression in section 2.2, as the covariance matrix does not incorporate all the uncertainty (and we cannot ﬁnd a model that would), it would be interesting for the risk averse agent who wants to optimize Q∗ to include a linear matrix inequal- ity constraint on the covariance matrix estimate to make it’s choice more pertinent/robust (this would result in a SDP: Semi-Deﬁnite Program). 7 Figure 7: BIC score obtained for diﬀerent number of components in the GMM for the error. 3 Nodal clustering Data clustering is a useful tool to reduce the complexity of large datasets. It is helpful to group similar data so that the overall dataset can be thought of as a conglomeration of a few clusters, instead of several hundred / thousand / million individual data points. Although the concept of clustering is easily graspable, the actual methods used to cluster data vary widely based on the data being considered. Further, it is often diﬃcult to know how many clusters should be used to ideally represent the data. In the case considered here, the data consists of hourly local marginal prices corresponding to nodes in the CAISO network. The data is grouped into days, so that each data point contains 24 numbers that represent the marginal price of electricity at that node over the course of the day. Figure 8 demonstrates this concept. 8 Figure 8: Visualization of data clustering. For this project, two clustering methods were considered: 1. Density Based Spectral Clustering of Applications with Noise (DB- SCAN) 2. K-Means (and Mini Batch K-means) For both clustering methods, scikit implementations were utilized [9] [8]. The applications of these two clustering methods are described further below. 3.1 DBSCAN The DBSCAN method has two necessary input parameters: 1) the max- imum distance between two points that are in the same cluster, and 2) the minimum number of data points in a single cluster. These input parameters are diﬃcult to optimize for several reasons. The notion of distance between the data points is not easily graspable because the distance is between hourly prices, not geographical distance. The minimum number of data points in a single sample is strongly related to the maximum distance between points. Furthermore, the DBSCAN method does not necessarily place all data sam- ples into a cluster. Samples can be given a label of -1 meaning that they do not fall into any of the created clusters. Ultimately, if the maximum distance between samples in the same cluster is small, then the cluster itself will be composed of relatively few data points meaning there will be many clusters to represent the full data set. If the minimum number of samples in a cluster 9 is large then the maximum distance must be selected to allow so many data points to be in the same cluster, otherwise the majority of the data will be given the -1 label. Due to the diﬃculty of selecting these two parameters to work well with each other and accurately represent the data-set, the K-means clustering method was preferred. 3.2 K-means and Mini Batch K-means K-means clustering has only one necessary input parameter: the number of desired clusters (mini batch k-means also requires the size of mini batches to be used). A brief literature review shows that there are several suggested performance metrics that can be used to determine the optimal number of clusters. These metrics include: • The squared error between each point in a cluster and the cluster’s center, deﬁned below. K∑ i=1 ∑ p∈Ci(p − mi)2 • The entropy of the cluster, deﬁned below [10]. − K∑ i=1 Pn(Ci)log(Pn(Ci)) Figure 9 shows the entropy and error calculated for diﬀerent numbers of clusters using the mini batch k-means clustering method with a batch size of 50. 10 Figure 9: Variation in entropy and error with number of clusters. As shown, the entropy increases as the number of clusters increases and the error decreases as the number of clusters increases. Both of these trends are expected. The entropy can be thought of as a measure of disorder, so as the number of clusters increases, the disorder/entropy increases. On the other hand, since the error is essentially measuring the distance between each point and its cluster center, this value decreases as the number of clusters increases because the clusters more closely represent the data within them. Ultimately, determining the best number of clusters to use is an unsolved problem. The ideal number of clusters is heavily dependent on the dataset. For the visualizations of the data presented in the following section, 10 clus- ters were chosen to represent the data. This number was used because it is large enough to capture some of the diversity in the data but small enough to be easily captured through visualization. 4 Spatial visualization of the nodes In an electric power network, it is useful to examine local characteristics of the diﬀerent nodes to understand macro tendencies. The marginal local price of the electricity (LMP) is the cost of supplying electric demand at a particular location (node). This LMP takes into account both supply bids and demand oﬀers, as well as physical aspects of the transmission system. 11 Hence, it is composed of three components : (i) the marginal cost of energy (MCE), (ii) the marginal cost of congestion (MCC), and (iii) the marginal cost of losses (MCL). For instance, there may be some high-congestion areas that explain an increase in the average price, or we could detect that a whole portion of the transmission network is obsolete because of high losses in the nodes of this portion. Hence, we used visualization tools from Mapbox [11] to get better intuition on the CAISO network’s price patterns. 4.1 Local values We started by scraping data from CAISO’s website for the whole month of November 2015 using [12]. We extracted the local marginal price (LMP) along with its three components MCC, MCE, MCL. We also extracted the geographical information associated with each node. A pre-cleaning of the data was necessary as there were some outlier values. In brief, steps we performed to get data in a readable format by Mapbox were the following: • Clean outliers from dataset. • Link price values to geographical information. • Import this ﬁle as a GeoJSON to Mapbox and store it in a layer. • Customize color chart by modifying the layer in Javascript. • Display the map locally: we built 4 diﬀerent maps, one with the LMP, and one for each component (MCC, MCE, MCL). Below are some visualization maps and insights we could get from them (see Appendix for more visualizations): 12 4.1.1 Local marginal price (LMP) Figure 10: Local marginal price in California for November 2015. 13 Figure 11: Zoom over San Francisco. Figure 12: Zoom over San Diego. The color represents the average price for November 2015, while the size of the dots represents its variance. The more yellow the higher the price is, and the bigger the dots, the more the price ﬂuctuates. We can observe generally bigger dots around big cities like San Francisco, Los Angeles, and San Diego, which indicates the price varies more there. This may be due to higher demand peaks in cities. Also, it can be noted that prices are higher in San Francisco than in San Diego, which can be explained by higher demand for heat in Northern California. 4.1.2 Marginal Cost of Congestion (MCC) The MCC designates the marginal cost of transmission system congestion due to binding constraints, e.g., binding transmission line constraints. Below is one visualization of the MCC component of LMP: 14 Figure 13: Marginal cost of congestion in California for November 2015. As before, a color gradient was used to represent the average cost of con- gestion (the more red the higher the cost), and the size of the dots indicates the variance of this cost. A ﬁrst takeaway is that this cost ﬂuctuates more in urban areas. However, it is worth noticing that this cost stays lower than average in San Francisco (see Appendix for a zoomed in view). 4.1.3 Marginal Cost of Losses (MCL) The MCL stems from thermal losses in the transmission lines. Below is one visualization of the MCL component of LMP over California: 15 Figure 14: Marginal cost of losses in California for November 2015. The color gradient here is such that the greener the dot is, the lower the loss is. This map shows that the losses are very low in the Bay Area, and seem to get higher inland, especially in farming areas between San Francisco and Los Angeles. 4.2 Visualization of K-Means clusters We also made a web map of the clustering done with K-Means (only made on the LMP, not on marginal components). The web map is available at this link, and each color maps to one particular cluster, with 10 clusters total. 5 Conclusion and future works As explained above, the day-ahead market for electricity is well suited to prediction models, such as random forest regression. Accurately predicting the prices of electricity at speciﬁc nodes is of high importance to partici- pants in the electricity market. Further, the clustering and visualization of 16 the electricity price data reveal interesting information such as the higher variance of electricity prices in urban areas, and the lower losses in the SF Bay area compared with places further inland. Further analysis regarding the best number of clusters to represent the data, using metrics other than the error and entropy, may be useful. Additionally, eﬀorts to improve the predictive capabilities of the regression model would be interesting. References [1] Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cam- bridge university press, 2004. [2] Pedregosa, Fabian, et al. ”Scikit-learn: Machine learning in Python.” Journal of Machine Learning Research 12.Oct (2011): 2825-2830. [3] CAISO. Interface Speciﬁcation for OASIS. [4] CAISO: Average Price - LCG Consulting :: EnergyOnline [5] Mazumder, Rahul, and Trevor Hastie. ”The graphical lasso: New insights and alternatives.” Electronic journal of statistics 6 (2012): 2125. [6] Wikipedia contributors. ”Electric power transmission.” Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 9 Dec. 2016. Web. 9 Dec. 2016. [7] Singh, Harry, Shangyou Hao, and Alex Papalexopoulos. ”Transmission congestion management in competitive electricity markets.” IEEE Trans- actions on power systems 13.2 (1998): 672-680. [8] “k-means++: The advantages of careful seeding” Arthur, David, and Sergei Vassilvitskii, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics (2007) [9] “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise” Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996 17 [10] J. Kwac, J. Flora, R. Rajagopal, Household energy consumption seg- mentationusing hourly data, IEEE Transactions on Smart Grid 5 (1) (2014) 420–430 [11] www.mapbox.com, 2016 [12] Munsing, Eric, CAISO-Scrapers, (2014), GitHub repository, https://github.com/emunsing/CAISO-Scrapers/ 6 Appendix 6.1 Zoom on particular areas for Marginal Cost of Congestion (MCC) Figure 15: Marginal cost of losses in the Bay Area for November 2015. 18 6.2 Zoom on particular areas for Marginal Cost of Losses (MCL) Figure 16: Zoom of MCL over San Francisco. Figure 17: Zoom over South California. 19","libVersion":"0.3.2","langs":""}
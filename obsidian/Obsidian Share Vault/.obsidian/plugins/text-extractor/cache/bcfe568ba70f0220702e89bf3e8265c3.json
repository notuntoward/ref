{"path":"lit/lit_sources/Guen20probTseriesFrcstSTRIPE.pdf","text":"Probabilistic Time Series Forecasting with Structured Shape and Temporal Diversity Vincent Le Guen 1,2 vincent.le-guen@edf.fr Nicolas Thome 2 nicolas.thome@cnam.fr 1 EDF R&D, Chatou, France 2 Conservatoire National des Arts et Métiers, CEDRIC, Paris, France Abstract Probabilistic forecasting consists in predicting a distribution of possible future outcomes. In this paper, we address this problem for non-stationary time series, which is very challenging yet crucially important. We introduce the STRIPE model for representing structured diversity based on shape and time features, ensuring both probable predictions while being sharp and accurate. STRIPE is agnostic to the forecasting model, and we equip it with a diversiﬁcation mechanism relying on determinantal point processes (DPP). We introduce two DPP kernels for modeling diverse trajectories in terms of shape and time, which are both differentiable and proved to be positive semi-deﬁnite. To have an explicit control on the diversity structure, we also design an iterative sampling mechanism to disentangle shape and time representations in the latent space. Experiments carried out on synthetic datasets show that STRIPE signiﬁcantly outperforms baseline methods for representing diversity, while maintaining accuracy of the forecasting model. We also highlight the relevance of the iterative sampling scheme and the importance to use different criteria for measuring quality and diversity. Finally, experiments on real datasets illustrate that STRIPE is able to outperform state-of- the-art probabilistic forecasting approaches in the best sample prediction. 1 Introduction Time series forecasting consists in analysing historical signal correlations to anticipate future out- comes. In this work, we focus on probabilistic forecasting in non-stationary contexts, i.e. we aim at producing plausible and diverse predictions where future trajectories can present sharp variations. This forecasting context is of crucial importance in many applicative ﬁelds, e.g. climate [62, 34, 15], optimal control or regulation [66, 41], trafﬁc ﬂow [39, 38], healthcare [8, 1], stock markets [14, 7], etc. Our motivation is illustrated in the example of the blue input in Figure 1(a): we aim at performing predictions covering the full distribution of future trajectories, whose samples are shown in green. State-of-the-art methods for time series forecasting currently rely on deep neural networks, which exhibit strong abilities in modeling complex nonlinear dependencies between variables and time. Recently, increasing attempts have been made for improving architectures for accurate predictions [31, 53, 37, 42, 35] or for making predictions sharper, e.g. by explicitly modeling dynamics [9, 16, 50], or by designing speciﬁc loss functions addressing the drawbacks of blurred prediction with mean squared error (MSE) training [12, 47, 33, 58]. Although Figure 1(b) shows that such approaches produce sharp and realistic forecasts, their deterministic nature limits them to a single trajectory prediction without uncertainty quantiﬁcation. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. (a) True predictive distribution (b) Sharp loss [33] (c) deep stoch model [65] (d) STRIPE (ours) Figure 1: We address the probabilistic time series forecasting problem. (a) Recent deep learning models include a speciﬁc loss enabling sharp predictions [12, 47, 33, 58] (b), but are inadequate for producing diverse forecasts. On the other hand, probabilistic forecasting approaches based on generative models [65, 46] loose the ability to generate sharp forecasts (c). The proposed STRIPE model (d) produces both sharp and diverse future forecasts. Methods targeting probabilistic forecasting enable to sample diverse predictions from a given input. This includes deterministic methods that predict the quantiles of the predictive distribution or proba- bilistic methods that sample future values from a learned approximate distribution, parameterized explicitly (e.g. Gaussian [52, 45, 51]), or implicitly with latent generative models [65, 29, 46]. These approaches are commonly trained using MSE or variants for probabilisting forecasts, e.g. quantile loss [28], and consequently often loose the ability to represent sharp predictions, as shown in Figure 1(c) for [65]. These generative models also lack an explicit structure to control the type of diversity in the latent space. In this work, we introduce a model for including Shape and Time diverRsIty in Probabilistic forEcast- ing (STRIPE). As shown in Figure 1(d), this enables to produce sharp and diverse forecasts, which ﬁt well the ground truth distribution of trajectories in Figure 1(a). STRIPE presented in section 3 is agnostic to the predictive model, and we use both deterministic or generative models in our experiments. STRIPE encompasses the following contributions. Firstly, we introduce a structured shape and temporal diversity mechanism based on determinantal point processes (DPP). We introduce two DPP kernels for modeling diverse trajectories in terms of shape and time, which are both differentiable and proved to be positive semi-deﬁnite (section 3.1). To have an explicit control on the diversity structure, we also design an iterative sampling mechanism to disentangle shape and time representations in the latent space (section 3.2). Experiments are conducted in section 4 on synthetic datasets to evaluate the ability of STRIPE to match the ground truth trajectory distribution. We show that STRIPE signiﬁcantly outperforms baseline methods for representing diversity, while maintaining the accuracy of the forecasting model. Experiments on real datasets further show that STRIPE is able to outperform state-of-the-art probabilistic forecasting approaches when evaluating the best sample (i.e. diversity), while being equivalent based on its mean prediction (i.e. quality). 2 Related work Deterministic time series forecasting Traditional time series forecasting methods, including linear autoregressive models such as ARIMA [6] or exponential smoothing [27], handle linear dynamics and stationary time series (or made stationary by modeling trends and seasonality). Deep learning has become the state-of-the-art for automatically modeling complex long-term dependencies, with many works focusing on architecture design based on temporal convolution networks [5, 53], recurrent neural networks (RNNs) [31, 64, 44], or Transformer [57, 37]. Another crucial topic more recently studied in the non-stationary context is the choice of a suitable loss function. As an alternative to the mean squared error (MSE) largely used as a proxy, new differentiable loss functions were proposed to enforce more meaningful criteria such as shape and time [47, 12, 33, 58], e.g. soft-DTW based on 2 dynamic time warping [12, 4] or the DILATE loss with a soft-DTW term for shape and a smooth temporal distortion index (TDI) [20, 56] for accurate temporal localization. These works toward sharper predictions were however only studied in the context of deterministic predictions and not for multiple outcomes. Probabilistic forecasting For describing the conditional distribution of future values given an input sequence, a ﬁrst class of deterministic methods add variance estimation with Monte Carlo dropout [67, 32] or predict the quantiles of this distribution [61, 21, 60] by minimizing the pinball loss [28, 49] or the continuous ranked probability score (CRPS) [23]. Other probabilistic methods try to approximate the predictive distribution, explicitly with a parametric distribution (e.g. Gaussian for DeepAR [52] and variants [45, 51]), or implicitly with a generative model with latent variables (e.g. with conditional variational autoencoders (cVAEs) [65], conditional generative adversarial networks (cGANs) [29], normalizing ﬂows [46]). However, these methods lack the ability to produce sharp forecasts by minimizing variants of the MSE (pinball loss, gaussian maximum likelihood), at the exception of cGANs - but which suffer from mode collapse that limits predictive diversity. Moreover, these generative models are generally represented by unstructured distributions in the latent space (e.g. Gaussian), which do not allow to have an explicit control on the targeted diversity. Diverse predictions For improving the diversity of predictions, several repulsive schemes were studied such as the variety loss [26, 55] that consists in optimizing the best sample, or entropy regularization terms [13, 59] that encourage a uniform distribution and thus more diverse samples. Submodular distribution functions such as determinantal point processes (DPP) [30, 48, 40] are an appealing probabilistic tool to enforce structured diversity via the choice of a positive semi-deﬁnite kernel. DPPs has been successfully applied in various contexts, e.g. document summarization [24], recommendation systems [22], object detection [2], and very recently to image generation [17] and diverse trajectory forecasting [65]. GDPP [17] is based on matching generated and true sample diversity by aligning the corresponding DPP kernels, and thus limits their use in datasets where the full distribution of possible outcomes is accessible. In contrast, our approach is applicable in realistic scenarii where only a single label is available for each training sample. Although we share with [65] the goal to use DPP as diversiﬁcation mechanism, the main limitation in [65] is to use the MSE loss for training the prediction and diversiﬁcation models, leading to blurred prediction, as illustrated in Figure 1(c). Our approach is able to generate sharp and diverse predictions ; we also highlight the importance in STRIPE to use different criteria for training the prediction model (quality) and the diversiﬁcation mechanism in order to make them cooperate. 3 Shape and time diversity for probabilistic time series forecasting We introduce the STRIPE model for including shape and time diversity for probabilistic time series forecasting, which is depicted in Figure 2. Given an input sequence x1:T = (x1, ..., xT ) ∈ Rp×T , our goal is to sample a set of N diverse and plausible future trajectories ˆy(i) = (ˆyT +1, ..., ˆyT +τ ) ∈ Rd×τ from the data future distribution ˆy(i) ∼ p(.|x1:T ). STRIPE builds upon a general Sequence To Sequence (Seq2Seq) architecture dedicated to multi-step time series forecasting: the input time series x1:T is fed into an encoder that summarizes the input into a latent vector h. Note that our method is agnostic to the speciﬁc choice of the forecasting model: it can be a deterministic RNN, or a probabilistic conditional generative model (e.g. cVAE [65], cGAN [29], normalizing ﬂow [46]). For training the predictor (upper part in Figure 2), we concatenate h with a vector 0k ∈ Rk (free space left for the diversifying variables) and a decoder produces a forecasted trajectory ˆy(0) = (ˆy(0) T +1, ..., ˆy(0) T +τ ). The predictor minimizes a quality loss Lquality(ˆy(0), y(0)) between the predicted ˆy(0) and ground truth future trajectory y(0). In our non-stationary context, we train the STRIPE predictor with Lquality based on the recently proposed DILATE loss [33], that has proven successful for enforcing sharp predictions with accurate temporal localization. For introducing structured diversity (lower part in Figure 2), we concatenate h with diversifying latent variables z ∈ Rk and produce N future trajectories {ˆy(i)} i=1,..,N . Our key idea is to augment Lquality(·) with a diversiﬁcation loss Ldiversity(·; K) parameterized by diversity kernel K and 3 Figure 2: Our STRIPE model builds upon a Seq2Seq architecture trained with a quality loss Lquality enforcing sharp predictions. Our contributions rely on the design of a diversity loss Ldiversity based on a speciﬁc Determinantal Point Processes (DPP). We design admissible shape and time DPP kernels, i.e. positive semideﬁnite, and differentiable for end-to-end training with deep models (section 3.1). We also introduce an iterative DDP sampling mechanism to generate disentangled latent codes between shape and time, supporting the use of different criteria for diversity and quality (section 3.2). balanced by hyperparameter λ ∈ R, leading to the overall objective training function: LST RIP E(ˆy(0), ..., ˆy(N ), y(0); K) = Lquality(ˆy(0), y(0)) + λ Ldiversity(ˆy(1), ..., ˆy(N ); K) (1) We highlight that STRIPE is applicable with a single target trajectory y(0), i.e. we do not require the full trajectory distribution. We now detail how the Ldiversity(·; K) loss is designed to ensure diverse shape and time predictions. 3.1 STRIPE diversity module based on determinantal point processes Our Ldiversity loss relies on determinantal point processes (DPP) that are a convenient probabilistic tool for enforcing structured diversity via adequately chosen positive semi-deﬁnite kernels. For comparing two time series y1 and y2, we introduce the two following kernels Kshape and Ktime, for ﬁnely controlling the shape and temporal diversity: Kshape(y1, y2) = e−γ DTWγ (y1,y2) (2) Ktime(y1, y2) = TDIγ(y1, y2) = 1 Z ∑ A∈Aτ,τ ⟨A, Ω⟩ exp− ⟨A,∆(y1 ,y2 )⟩ γ (3) where DTWγ(y1, y2) := −γ log (∑ A∈Aτ,τ exp− ⟨A,∆(y1 ,y2)⟩ γ ) is a smooth relaxation of Dy- namic Time Warping (DTW) [12], and Ktime corresponds to a smooth Temporal Distortion In- dex (TDI) [20, 33]: γ > 0 denotes the smoothing coefﬁcient, A ⊂ {0, 1}τ ×τ is a warping path between two time series of length τ , Aτ,τ is the set of all feasible warping paths and ∆(y1, y2) = [δ((y1)i, (y2)j)]1≤i,j≤τ is a pairwise cost matrix between time steps of both se- ries with similarity measure δ : Rd × Rd → R, Ω is a τ × τ matrix penalizing the deviation of warping paths from the main diagonal and Z is the partition function. These kernels are derived from the two components of the DILATE loss [33] ; however in contrast to the deterministic nature of DILATE, they are used in a probabilistic context for producing sharp and diverse forecasts. Kshape and Ktime are differentiable by design1, making them suitable for end-to-end training with back-propagation. We also derive the key following result for ensuring the submodularity properties of DPPs, that we prove in supplementary 1: 1In the limit case γ → 0, DTWγ (resp. TDIγ) recovers the standard DTW (resp. TDI). 4 Figure 3: At test time, STRIPE sequential shape and time sampling scheme that leverages the disentangled latent space. STRIPE-shape ﬁrst proposes diverse shape latent variables. For each generated shape, STRIPE-time further enhances its temporal variability, leading to a ﬁnal set of accurate predictions with shape and time diversity. Proposition 1. Providing that κ is a positive semi-deﬁnite (PSD) kernel κ such that κ 1+κ is also PSD, if we deﬁne the cost matrix ∆ with general term δ(yi, yj) = −γ log κ(yi, yj), then Kshape and Ktime deﬁned respectively in Equations (2) and (3) are PSD kernels. In practice, we choose κ(u, v) = 1 2 e− (u−v)2 σ2 (1 − 1 2 e− (u−v)2 σ2 ) −1 that fullﬁlls Prop 1 requirements. DPP diversity loss We combine the two differentiable PSD kernels Kshape and Ktime with the DPP diversity loss from [65] deﬁned as the negative expected cardinality of a random subset Y (of a ground set Y of N items) sampled from the DPP of kernel K (denoted as K in matrix form of shape N × N ). This loss is differentiable and can be efﬁciently computed in closed-form: Ldiversity(Y; K) = −EY ∼DP P (K)|Y | = −T race(I − (K + I)−1) (4) Intuitively, a larger expected cardinality means a more diverse sampled set according to kernel K. We provide more details on DPPs and the derivation of Ldiversity in supplementary 2. 3.2 STRIPE learning and sequential shape and temporal diversity sampling To maximize shape and time diversity with Eq (1) and (4), a naive way is to consider the combined kernel Kshape + Ktime which is also PSD. However, this reduces to using the same criterion for quality and diversity, i.e. DILATE [33]. This directly makes Ldiversity conﬂicts with Lquality and harms prediction performances, as shown in ablation studies (section 4.2). Another simple solution is to diversify using Kshape and Ktime independently, which prevents from modeling joint shape and time variations, and intrinsically limits the expressiveness of the diversiﬁcation scheme. In contrast, we propose a sequential shape and temporal diversity sampling scheme, which enables to jointly model variations in shape and time without altering prediction quality. We now detail how the STRIPE models are trained and then used at test time. STRIPE-shape and STRIPE-time learning We start by independently training two proposal modules STRIPE-shape and STRIPE-time (and their respective encoders and decoders) by optimizing Eq (1) with LST RIP E(·; Kshape) (resp. LST RIP E(·; Ktime)). To this end, we complement the latent state h of the forecaster with a diversifying latent variable z ∈ Rk decomposed into shape zs ∈ Rk/2 and temporal zt ∈ Rk/2 components: z = (zs, zt) ∈ Rk. As illustrated in Figure 3, STRIPE-shape (the description of STRIPE-time is symmetric) is a proposal neural network that produces Ns different shape latent codes z(i) s (the output of the STRIPE-shape neural network is of shape Ns × k). The decoder takes the concatenated state (h, z(i) s , zt) for a ﬁxed zt and produces Ns future trajectories ˆy(i), whose diversity is maximized with Ldiversity(ˆy(1), ..., ˆy(Ns); K shape) in Eq (4). The architecture of STRIPE-time is similar to STRIPE-shape, except that the proposal neural network is conditioned on a generated shape variable z(i) s to produce temporal variability with respect to a given shape. 5 Sequential sampling at test time Once the STRIPE- shape and STRIPE-time models (and their corresponding encoders and decoders) are learned, test-time sampling (il- lustrated in Figure 3 and detailed in Algorithm 1) consists in sequentially maximizing the shape diversity with STRIPE- shape (different guesses about the step amplitude in Figure 3) and the temporal diversity of each shape with STRIPE- time (the temporal localization of the step). Notice that the ordering shape+time is actually important since the notion of time diversity between two time series is only meaningful if they have a similar shape (so that computing the DTW optimal path has a sense): the STRIPE- time proposals are conditioned on the shape proposals from the previous step. As shown in our experiments, this two- steps scheme (denoted STRIPE S+T) leads to more diverse predictions with both shape and time criteria compared to using the shape or time kernels alone. Algorithm 1: STRIPE S+T sam- pling at test time Sample an initial z(0) t ∼ N (0, I) z(1) s , ..., z(Ns) s = STRIPE-shape(x1:T ) for i=1..Ns do z(i,1) t , ..., z(i,Nt) t = STRIPE-time(x1:T , z(i) s ) for j=1..Nt do ˆy(i,j) T +1:t+τ = Decoder(x1:T , (z(i) s , z(i,j) t )) end end 4 Experiments To illustrate the relevance of STRIPE, we carry out experiments in two different settings: in the ﬁrst one, we compare the ability of forecasting methods to capture the full predictive distribution of future trajectories on a synthetic dataset with multiple possible futures for each input. To validate our approach in realistic settings, we evaluate STRIPE on 2 standard real datasets (trafﬁc & electricity) where we evaluate the best (resp. the mean) sample metrics as a proxy for diversity (resp. quality). Implementation details: To handle the inherent ambiguity of the synthetic dataset (multiple targets for one input), our STRIPE model is based on a natively stochastic model (cVAE). Since this situation does not arise exactly for real-world datasets, we choose in this case a deterministic Seq2Seq predictor with 1 layer of 128 Gated Recurrent Units (GRU) [10]. In our experiments, all methods produce N=10 future trajectories that are compared to the unique (or multiple) ground truth(s). For a fair comparison, STRIPE S+T generates Ns × Nt = 10 × 10 = 100 predictions and we randomly sample N=10 predictions for evaluation. Further neural network architectures and implementation details are described in supplementary 3.1. Our PyTorch code implementing STRIPE is available at https://github.com/vincent-leguen/STRIPE. 4.1 Synthetic dataset with multiple futures We use a synthetic dataset similar to [33] that consists in predicting step functions based on a two- peaks input signal (see Figure 1). For each input series of 20 timesteps, we generate 10 different future series of length 20 by adding noise on the step amplitude and localisation. The dataset is composed of 100 × 10 = 1000 time series for each train/valid/test split (further dataset description in supplementary 3.1). Metrics: In this multiple futures context, we deﬁne two speciﬁc discrepancy measures Hquality(ℓ) and Hdiversity(ℓ) for assessing the divergence between the predicted and true distributions of futures trajectories for a given loss ℓ (ℓ = MSE or DILATE in our experiments): Hquality(ℓ) = Ex∈DtestEˆy [ inf y∈F (x) ℓ(ˆy, y)] Hdiversity(ℓ) = Ex∈DtestEy∈F (x) [ inf ˆy ℓ(ˆy, y) ] Hquality penalizes forecasts ˆy that are far away from a ground truth future of x denoted y ∈ F (x) (similarly to the precision concept in pattern recognition) whereas Hdiversity penalizes when a true future is not covered by a forecast (similarly to recall). We also use the continuous ranked probability score (CRPS) 2 which is a standard proper scoring rule [23] for assessing probabilistic forecasts [21]. 2An intuitive deﬁnition of the CRPS is the pinball loss integrated over all quantile levels. The CRPS is minimized when the predicted future distribution is identical to the true future distribution. 6 Table 1: Forecasting results on the synthetic dataset with multiple futures for each input, averaged over 5 runs (mean ± standard deviation). Best equivalent method(s) (Student t-test) shown in bold. Metrics are scaled (MSE × 1000, DILATE ×100, CRPS × 1000) for readability. Hquality (.)(↓) Hdiversity(.) (↓) CRPS (↓) Methods MSE DILATE MSE DILATE Deep AR [52] 26.6 ± 6.4 67.0 ± 12.0 15.2 ± 3.4 45.4 ± 4.3 62.4 ± 9.9 cVAE MSE 11.8 ± 0.5 48.8 ± 3.2 20.0 ± 0.6 85.4 ± 7.0 76.4 ± 3.0 variety loss [55] MSE 13.1 ± 2.7 50.9 ± 4.7 19.6 ± 1.1 84.7 ± 2.2 80.1 ± 3.3 Entropy regul. [13] MSE 12.0 ± 0.7 51.5 ± 2.9 19.7 ± 0.7 89.5 ± 7.4 78.9 ± 2.9 Diverse DPP [65] MSE 15.9 ± 2.6 56.6 ± 2.8 16.5 ± 1.5 59.6 ± 5.6 80.5 ± 6.1 GDPP [17] kernel MSE 11.7 ± 1.3 47.5 ± 3.1 19.5 ± 0.4 82.3 ± 5.2 74.0 ± 4.5 STRIPE S+T (ours) 12.4 ± 1.0 48.7 ± 0.7 18.1 ± 1.6 62.0 ± 5.4 72.2 ± 3.1 cVAE DILATE 11.6 ± 1.8 28.3 ± 2.9 22.2 ± 2.5 67.8 ± 7.8 62.2 ± 4.2 variety loss [55] DILATE 14.9 ± 3.3 33.5 ± 1.9 23.8 ± 3.9 61.6 ± 1.9 62.6 ± 3.0 Entropy regul. [13] DILATE 12.7 ± 2.6 29.9 ± 3.2 23.5 ± 2.6 65.1 ± 4.5 62.4 ± 3.9 Diverse DPP [65] DILATE 11.1 ± 1.6 30.2 ± 2.9 20.7 ± 2.3 62.6 ± 11.3 60.7 ± 1.6 GDPP [17] kernel DILATE 10.6 ± 1.6 28.7 ± 4.1 21.7 ± 2.1 47.7 ± 9.0 63.4 ± 6.4 STRIPE S+T (ours) 10.8 ± 0.4 30.7 ± 0.9 14.5 ± 0.6 35.5 ± 1.1 60.5 ± 0.4 Results We compare our method to 4 recent competing diversiﬁcation mechanisms (variety loss [55], entropy regularisation [13], diverse DPP [65] and GDPP [17]) based two different forecasting backbones: a conditional variational autoencoder (cVAE) trained with MSE and with DILATE. Results in Table 1 show that our model STRIPE S+T based on a cVAE DILATE obtains the global best performances by improving the diversity by a large margin (Hdiversity(DILATE) = 35.5 vs. 67.8), signiﬁcantly outperforming other methods. This highlights the relevance of the structured shape and time diversity in STRIPE. It is worth mentioning that STRIPE also presents the best performances in quality. In contrast, other diversiﬁcation mechanisms (variety loss, entropy regularisation, diverse DPP) based on the same backbone (cVAE DILATE) improve the diversity in DILATE but at the cost of a loss in quality in MSE and/or DILATE. Although GDPP does not deteriorate quality, it is signiﬁcantly worse than STRIPE in diversity, and the approach requires full future distribution supervision, which it not applicable in in real dataset (see section 2). Similar conclusions can be drawn for the cVAE MSE backbone: the different diversity mechanisms improve the diversity but at the cost of a loss of quality. For example, Diverse DPP MSE [65] improves diversity (Hdiversity(DILATE) = 59.6 vs. 85.4) but looses in quality (Hquality(DILATE) = 56.6 vs. 48.8). In contrast, STRIPE S+T again both improves diversity (Hdiversity(DILATE) = 62.0 vs. 85.4) with equivalent quality (Hquality(DILATE) = 48.7 vs. 48.8). We further highlight that STRIPE S+T gets the best results evaluated in CPRS, conﬁrming its ability to better recover the true future distribution. 4.2 Ablation study To analyze the respective roles of the quality and diversity losses, we perform an ablation study on the synthetic dataset with the cVAE backbone trained with the quality loss DILATE and different DPP diversity losses. For a ﬁner analysis, we report in Table 2 the shape (DTW, computed with Tslearn [54]) and time (TDI) components of the DILATE loss [33]. Table 2: Ablation study on the synthetic dataset. We train a backbone cVAE with the DILATE quality loss and compare different DPP kernels for diversity. Metrics are scaled for readability. Results averaged over 5 runs (mean ± std). Best equivalent method(s) (Student t-test) shown in bold. cVAE DILATE H q uality (.) (↓) H div er sity (.) (↓) CRPS (↓) diversity MSE DILATE MSE DTW TDI DILATE None 11.6 ± 1.8 28.3 ± 2.9 22.2 ± 2.5 18.8 ± 1.3 48.6 ± 2.2 67.8 ± 7.8 62.2 ± 4.2 DILATE 11.1 ± 1.6 30.2 ± 2.8 20.7 ± 2.3 18.6 ± 1.6 42.8 ± 10.2 62.6 ± 11.3 60.7 ± 1.7 MSE 10.9 ± 1.5 30.2 ± 2.9 20.1 ± 2.2 18.5 ± 1.3 41.9 ± 8.8 61.7 ± 9.5 62.1 ± 0.9 shape (ours) 11.0 ± 1.4 30.2 ± 1.2 15.5 ± 1.04 16.4 ± 1.5 15.4 ± 4.2 37.8 ± 3.7 63.2 ± 1.6 time (ours) 11.9 ± 0.5 31.2 ± 1.3 16.1 ± 0.70 17.6 ± 0.5 15.1 ± 3.1 38.9 ± 3.3 62.3 ± 1.4 S+T (ours) 10.8 ± 0.4 30.7 ± 0.9 14.5 ± 0.6 16.1 ± 1.1 13.2 ± 1.7 35.5 ± 1.1 60.5 ± 0.4 7 Trafﬁc Electricity Figure 4: Qualitative predictions for Trafﬁc and Electricity datasets. Input series in blue are not shown entirely for readability. We display 10 future predictions of STRIPE S+T that are both sharp and accurate compared to the ground truth (GT) future in green. Results presented in Table 2 ﬁrst reveal the crucial importance to deﬁne different criteria for quality and diversity. With the same loss for quality and diversity (as this is the case in [65]), we observe here that the DILATE DPP kernel does not bring a statistically signiﬁcant diversity gain compared to the cVAE DILATE baseline (without diversity loss). By choosing the MSE kernel instead, we even get a small diversity and quality improvement. In contrast, our introduced shape and time kernels Kshape and Ktime largely improve the diversity in DILATE without deteriorating precision. As expected, each kernel brings its own beneﬁts: Kshape brings the best improvement in the shape metric DTW (Hdiversity(DTW) = 16.4 vs. 18.8) and Kshape the best improvement in the time metric TDI (Hdiversity(TDI) = 15.1 vs. 48.6). With our sequential shape and time sampling sheme described in section 3.2, STRIPE S+T gathers the beneﬁts of both criteria and gets the global best results in diversity and equivalent results in quality. 4.3 State-of-the-art comparison on real-world datasets We evaluate here the performances of STRIPE on two challenging real-world datasets commonly used as benchmarks in the time series forecasting literature [63, 52, 31, 45, 33, 53]: Trafﬁc: consisting in hourly road occupancy rates (between 0 and 1) from the California Department of Transportation, and Electricity: consisting in hourly electricity consumption measurements (kWh) from 370 customers. For both datasets, models predict the 24 future points given the past 168 points (past week). Although these datasets present daily, weakly, yearly periodic patterns, we are more interested here in modeling ﬁner intraday temporal scales, where these signals present sharp ﬂuctuations that are crucial for many applications, e.g. short-term renewable energy forecasts for load adjustment in smart-grids [34]. Contrary to the synthetic dataset, we only dispose of one future trajectory sample y(0) T +1:T +τ for each input series x1:T . In this case, the metrics Hquality (resp. Hdiversity) deﬁned in section 4.1 reduce to the mean sample (resp. best sample), which are common for evaluating stochastic forecasting models [3, 19]. We also report the CRPS in supplementary 3.2. Results in Table 3 reveal that STRIPE S+T outperforms all other methods in terms of the best sample trajectory evaluated in MSE and DILATE for both datasets, while being equivalent in the mean sample in 3/4 cases. Interestingly, STRIPE S+T provides better best trajectories (evaluated in MSE Table 3: Forecasting results on the Trafﬁc and Electricity datasets, averaged over 5 runs (mean ± std). Metrics are scaled for readability. Best equivalent method(s) (Student t-test) shown in bold. T rafﬁc Electricity MSE DILA TE MSE DILA TE Method mean best mean best mean best mean best Nbeats [ 42 ] MSE - 7.8 ± 0.3 - 22.1 ± 0.8 - 24.6 ± 0.9 - 29.3 ± 1.3 Nbeats [ 42 ] DILA TE - 17.1 ± 0.8 - 17.8 ± 0.3 - 38.9 ± 1.9 - 20.7 ± 0.5 Deep AR [ 52 ] 15.1 ± 1.7 6.6 ± 0.7 30.3 ± 1.9 16.9 ± 0.6 67.6 ± 5.1 25.6 ± 0.4 59.8 ± 5.2 17.2 ± 0.3 cVAE DILA TE 10.0 ± 1.7 8.8 ± 1.6 19.1 ± 1.2 17.0 ± 1.1 28.9 ± 0.8 27.8 ± 0.8 24.6 ± 1.4 22.4 ± 1.3 V ariety loss [ 55 ] 9.8 ± 0.8 7.9 ± 0.8 18.9 ± 1.4 15.9 ± 1.2 29.4 ± 1.0 27.7 ± 1.0 24.7 ± 1.1 21.6 ± 1.0 Entrop y re gul. [ 13 ] 11.4 ± 1.3 10.3 ± 1.4 19.1 ± 1.4 16.8 ± 1.3 34.4 ± 4.1 32.9 ± 3.8 29.8 ± 3.6 25.6 ± 3.1 Div erse DPP [ 65 ] 11.2 ± 1.8 6.9 ± 1.0 20.5 ± 1.0 14.7 ± 1.0 31.5 ± 0.8 25.8 ± 1.3 26.6 ± 1.0 19.4 ± 1.0 STRIPE S+T 10.1 ± 0.4 6.5 ± 0.2 19.2 ± 0.8 14.2 ± 0.2 29.7 ± 0.3 23.4 ± 0.2 24.4 ± 0.3 16.9 ± 0.2 8 and DILATE) than the recent state-of-the-art N-Beats algorithm [42] (either trained with MSE or DILATE), which is dedicated to producing high quality deterministic forecasts. This conﬁrms that STRIPE’s structured quality and diversity framework enables to obtain very accurate best predictions. Finally when compared to the state-of-the art probabilistic deep AR method [52], STRIPE S+T is consistently better in diversity and quality. We display a few qualitative forecasting examples of STRIPE S+T on Figure 4 and additional ones in supplementary 3.3. We observe that STRIPE predictions are both sharp and accurate: both the shape diversity (amplitude of the peaks) and temporal diversity match the ground truth future. 4.4 Model analysis Figure 5: Inﬂuence of the number N of tra- jectories on quality (higher is better) and di- versity for the synthetic dataset. Figure 6: Scatterplot of 50 predictions in the plane (DTW,TDI), comparing STRIPE S+T v.s. Diverse DPP DILATE [65]. We analyze in Figure 5 for the synthetic dataset the evolution of performances when increasing the number N of sampled future trajectories from 5 to 100: we observe that this results in higher normalized DILATE diversity (Hdiversity(5)/Hdiversity(N )) for STRIPE S+T without deteriorating quality (which even increases slightly). In contrast, deepAR [52], which does not have control over the targetted diversity, increases diversity with N but at the cost of a loss in quality. This again conﬁrms the relevance of our approach that effectively combines an adequate quality loss function and a structured diversity mechanism. We provide an additional analysis to highlight the importance to separate the criteria for enforcing quality and diversity. In Figure 6, we represent 50 predictions from the models Diverse DPP DILATE [65] and STRIPE S+T in the plane (DTW,TDI). Diverse DPP DILATE [65] uses a DPP diversity loss based on the DILATE kernel, which is the same than for quality. We clearly see that the two objectives conﬂict: this model increases the DILATE diversity (by increasing the variance in the shape (DTW) or the time TDI) components) but a lot of these predictions have a high DILATE loss (worse quality). In contrast, STRIPE S+T predictions are diverse in DTW and TDI, and maintain an overall low DILATE loss. STRIPE S+T succeeds in recovering a set of good tradeoffs between shape and time leading a low DILATE loss. 5 Conclusion and perspectives We present STRIPE, a probabilistic time series forecasting method that introduces structured shape and temporal diversity based on determinantal point processes. Diversity is controlled via two proposed differentiable positive semi-deﬁnite kernels for shape and time and exploits a forecasting model with a disentangled latent space. Experiments on synthetic and real-world datasets conﬁrm that STRIPE leads to more diverse forecasts without sacriﬁcing on quality. Ablation studies also reveal the crucial importance to decouple the criteria used for quality and diversity. A future perspective would be to incorporate seasonality and extrinsic prior knowledge (such as special events) [32, 42] to better model the non-stationary abrupt changes and their impact on diversity and model conﬁdence [11]. Other appealing directions include diversity-promoting forecasting for exploration in reinforcement learning [43, 18, 36], and extension of structured diversity to spatio- temporal or video prediction tasks [62, 19, 25]. 9 Broader Impact Probabilistic time series forecasting, especially in the non-stationary contexts, is a paramount research problem with immediate and large impacts in the society. A wide range of sensitive applications heavily rely on accurate forecasts of uncertain events with potentially sharp variations for making crucial decisions: in weather and climate science, better anticipating ﬂoods, hurricanes, earthquakes or other extreme events evolution could help taking emergency measures on time and save lives; in medicine, better predictions of an outbreak’s evolution is a particularly actual topic. We believe that introducing meaningful criteria such as shape and time, which are more related to application-speciﬁc evaluation metrics, is an important step toward more reliable and interpretable forecasts for decision makers. References [1] Ahmed M Alaa and Mihaela van der Schaar. Attentive state-space modeling of disease progres- sion. In Advances in Neural Information Processing Systems (NeurIPS), pages 11334–11344, 2019. [2] Samaneh Azadi, Jiashi Feng, and Trevor Darrell. Learning detection with diverse proposals. In Computer Vision and Pattern Recognition (CVPR), pages 7149–7157, 2017. [3] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. International Conference on Learning Representations (ICLR), 2018. [4] Mathieu Blondel, Arthur Mensch, and Jean-Philippe Vert. Differentiable divergences between time series. arXiv preprint arXiv:2010.08354, 2020. [5] Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. Conditional time series forecast- ing with convolutional neural networks. arXiv preprint arXiv:1703.04691, 2017. [6] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [7] Philippe Chatigny, Jean-Marc Patenaude, and Shengrui Wang. Financial time series representa- tion learning. arXiv preprint arXiv:2003.12194, 2020. [8] Sucheta Chauhan and Lovekesh Vig. Anomaly detection in ECG time signals via deep long short-term memory networks. In International Conference on Data Science and Advanced Analytics (DSAA), pages 1–7. IEEE, 2015. [9] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in neural information processing systems (NeurIPS), pages 6571–6583, 2018. [10] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder- decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. [11] Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Pérez. Ad- dressing failure prediction by learning model conﬁdence. In Advances in Neural Information Processing Systems (NeurIPS), pages 2902–2913, 2019. [12] Marco Cuturi and Mathieu Blondel. Soft-DTW: a differentiable loss function for time-series. In International Conference on Machine Learning (ICML), pages 894–903, 2017. [13] Adji B Dieng, Francisco JR Ruiz, David M Blei, and Michalis K Titsias. Prescribed generative adversarial networks. arXiv preprint arXiv:1910.04302, 2019. [14] Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. Deep learning for event-driven stock prediction. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2015. 10 [15] Jérémie Donà, Jean-Yves Franceschi, Sylvain Lamprier, and Patrick Gallinari. PDE-driven spatiotemporal disentanglement. arXiv preprint arXiv:2008.01352, 2020. [16] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural ODEs. In Advances in Neural Information Processing Systems (NeurIPS), pages 3134–3144, 2019. [17] Mohamed Elfeki, Camille Couprie, Morgane Riviere, and Mohamed Elhoseiny. GDPP: learning diverse generations using determinantal point process. International Conference on Machine Learning (ICML), 2019. [18] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. International Conference on Learning Representations (ICLR), 2019. [19] Jean-Yves Franceschi, Edouard Delasalles, Mickaël Chen, Sylvain Lamprier, and Patrick Gallinari. Stochastic latent residual video prediction. International Conference on Machine Learning (ICML), 2020. [20] Laura Frías-Paredes, Fermín Mallor, Martín Gastón-Romeo, and Teresa León. Assessing energy forecasting inaccuracy by simultaneously considering temporal and absolute errors. Energy Conversion and Management, 142:533–546, 2017. [21] Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile function RNNs. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1901–1910, 2019. [22] Jennifer A Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. Expectation-maximization for learning determinantal point processes. In Advances in Neural Information Processing Systems (NeurIPS), pages 3149–3157, 2014. [23] Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243–268, 2007. [24] Boqing Gong, Wei-Lun Chao, Kristen Grauman, and Fei Sha. Diverse sequential subset selection for supervised video summarization. In Advances in neural information processing systems (NeurIPS), pages 2069–2077, 2014. [25] Vincent Le Guen, Yuan Yin, Jérémie Dona, Ibrahim Ayed, Emmanuel de Bézenac, Nicolas Thome, and Patrick Gallinari. Augmenting physical models with deep networks for complex dynamics forecasting. arXiv preprint arXiv:2010.04456, 2020. [26] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social GAN: Socially acceptable trajectories with generative adversarial networks. In Computer Vision and Pattern Recognition (CVPR), pages 2255–2264, 2018. [27] Rob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential smoothing: the state space approach. Springer Science & Business Media, 2008. [28] Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives, 15(4):143–156, 2001. [29] Alireza Koochali, Andreas Dengel, and Sheraz Ahmed. If you like it, gan it. probabilistic multivariate times series forecast with gan. arXiv preprint arXiv:2005.01181, 2020. [30] Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Founda- tions and Trends in Machine Learning, 5(2–3):123–286, 2012. [31] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 95–104, 2018. 11 [32] Nikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek Smyl. Time-series extreme event forecasting with neural networks at Uber. In International Conference on Machine Learning (ICML), volume 34, pages 1–5, 2017. [33] Vincent Le Guen and Nicolas Thome. Shape and time distortion loss for training deep time series forecasting models. In Advances in Neural Information Processing Systems (NeurIPS), pages 4191–4203. 2019. [34] Vincent Le Guen and Nicolas Thome. A deep physical model for solar irradiance forecasting with ﬁsheye images. In CVPR 2020 OmniCV workshop, 2020. [35] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [36] Edouard Leurent, Denis Eﬁmov, and Odalric-Ambrym Maillard. Robust estimation, prediction and control with linear dynamics and generic costs. In Advances in Neural Information Processing Systems (NeurIPS). 2020. [37] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), pages 5244–5254, 2019. [38] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net- work: Data-driven trafﬁc forecasting. In International Conference on Learning Representations (ICLR), 2018. [39] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Trafﬁc ﬂow prediction with big data: a deep learning approach. IEEE Transactions on Intelligent Transportation Systems, 16(2):865–873, 2015. [40] Zelda E Mariet, Yaniv Ovadia, and Jasper Snoek. DPPNet: Approximating determinantal point processes with deep networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 3218–3229, 2019. [41] Shamsul Masum, Ying Liu, and John Chiverton. Multi-step time series forecasting of electric load using machine learning models. In International Conference on Artiﬁcial Intelligence and Soft Computing, pages 148–159. Springer, 2018. [42] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. International Conference on Learning Representations (ICLR), 2020. [43] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16–17, 2017. [44] Yao Qin, Dongjin Song, Haifeng Cheng, Wei Cheng, Guofei Jiang, and Garrison W Cottrell. A dual-stage attention-based recurrent neural network for time series prediction. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 2627–2633. AAAI Press, 2017. [45] Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. In Advances in neural information processing systems (NeurIPS), pages 7785–7794, 2018. [46] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf. Multi-variate probabilistic time series forecasting via conditioned normalizing ﬂows. arXiv preprint arXiv:2002.06103, 2020. [47] François Rivest and Richard Kohar. A new timing error cost function for binary time series prediction. IEEE transactions on neural networks and learning systems, 2019. 12 [48] Joshua Robinson, Suvrit Sra, and Stefanie Jegelka. Flexible modeling of diversity with strongly log-concave distributions. In Advances in Neural Information Processing Systems (NeurIPS), pages 15199–15209, 2019. [49] Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. In Advances in Neural Information Processing Systems (NeurIPS), pages 3538–3548, 2019. [50] Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems (NeurIPS), pages 5321–5331, 2019. [51] David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. High-dimensional multivariate forecasting with low-rank gaussian copula processes. In Ad- vances in Neural Information Processing Systems (NeurIPS), pages 6824–6834, 2019. [52] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–1191, 2020. [53] Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally, act locally: A deep neural net- work approach to high-dimensional time series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), pages 4838–4847, 2019. [54] Romain Tavenard, Johann Faouzi, Gilles Vandewiele, Felix Divo, Guillaume Androz, Chester Holtz, Marie Payne, Roman Yurchak, Marc Rußwurm, Kushal Kolar, et al. Tslearn, a machine learning toolkit for time series data. Journal of Machine Learning Research, 21(118):1–6, 2020. [55] Luca Anthony Thiede and Pratik Prabhanjan Brahma. Analyzing the variety loss in the context of probabilistic trajectory prediction. In International Conference on Computer Vision (ICCV), pages 9954–9963, 2019. [56] Loïc Vallance, Bruno Charbonnier, Nicolas Paul, Stéphanie Dubost, and Philippe Blanc. To- wards a standardized procedure to assess solar forecast accuracy: A new ramp and time alignment metric. Solar Energy, 150:408–422, 2017. [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems (NIPS), pages 5998–6008, 2017. [58] Titouan Vayer, Laetitia Chapel, Nicolas Courty, Rémi Flamary, Yann Soullard, and Romain Tavenard. Time series alignment with global invariances. arXiv preprint arXiv:2002.03848, 2020. [59] Dilin Wang and Qiang Liu. Nonlinear Stein variational gradient descent for learning diversiﬁed mixture models. In International Conference on Machine Learning (ICML), pages 6576–6585, 2019. [60] Ruofeng Wen and Kari Torkkola. Deep generative quantile-copula models for probabilistic forecasting. ICML Time Series Workshop, 2019. [61] Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi- horizon quantile recurrent forecaster. NeurIPS Time Series Workshop, 2017. [62] Shi Xingjian, Zhourong Chen, and Hao et al Wang. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in Neural Information Processing Systems (NeurIPS), 2015. [63] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for high-dimensional time series prediction. In Advances in neural information processing systems (NIPS), pages 847–855, 2016. [64] Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue. Long-term forecasting using tensor-train RNNs. arXiv preprint arXiv:1711.00073, 2017. 13 [65] Ye Yuan and Kris Kitani. Diverse trajectory forecasting with determinantal point processes. International Conference on Learning Representations (ICLR), 2020. [66] Jian Zheng, Cencen Xu, Ziang Zhang, and Xiaohua Li. Electric load forecasting in smart grids using long-short-term-memory based recurrent neural network. In 51st Annual Conference on Information Sciences and Systems (CISS), pages 1–6. IEEE, 2017. [67] Lingxue Zhu and Nikolay Laptev. Deep and conﬁdent prediction for time series at Uber. In International Conference on Data Mining Workshops (ICDMW), pages 103–110. IEEE, 2017. 14","libVersion":"0.3.1","langs":""}
{"path":"lit/lit_sources.backup/He19TransferLearningFinanciala.pdf","text":"Metadata of the chapter that will be visualized in SpringerLink Book Title PRICAI 2019: Trends in Artificial Intelligence Series Title Chapter Title Transfer Learning for Financial Time Series Forecasting Copyright Year 2019 Copyright HolderName Springer Nature Switzerland AG Author Family Name He Particle Given Name Qi-Qiao Prefix Suffix Role Division Department of Computer and Information Science Organization University of Macau Address Taipa, Macau Email Author Family Name Pang Particle Given Name Patrick Cheong-Iao Prefix Suffix Role Division School of Computing and Information Systems Organization The University of Melbourne Address Parkville, Australia Email Corresponding Author Family Name Si Particle Given Name Yain-Whar Prefix Suffix Role Division Department of Computer and Information Science Organization University of Macau Address Taipa, Macau Email fstasp@umac.mo Abstract Time-series are widely used for representing non-stationary data such as weather information, health related data, economic and stock market indexes. Many statistical methods and traditional machine learning techniques are commonly used for forecasting time series. With the development of deep learning in artificial intelligence, many researchers have adopted new models from artificial neural networks for forecasting time series. However, poor performance of applying deep learning models in short time series hinders the accuracy in time series forecasting. In this paper, we propose a novel approach to alleviate this problem based on transfer learning. Existing work on transfer learning uses extracted features from a source dataset for prediction task in a target dataset. In this paper, we propose a new training strategy for time-series transfer learning with two source datasets that outperform existing approaches. The effectiveness of our approach is evaluated on financial time series extracted from stock markets. Experiment results show that transfer learning based on 2 data sets is superior than other base-line methods. Keywords (separated by '-') Transfer learning - Financial time series - Forecasting - Artificial neural networks Transfer Learning for Financial Time Series Forecasting Qi-Qiao He 1, Patrick Cheong-Iao Pang 2, and Yain-Whar Si 1(B) 1 Department of Computer and Information Science, University of Macau, Taipa, Macau fstasp@umac.mo 2 School of Computing and Information Systems, The University of Melbourne, Parkville, Australia Abstract. Time-series are widely used for representing non-stationary data such as weather information, health related data, economic and stock market indexes. Many statistical methods and traditional machine learning techniques are commonly used for forecasting time series. AQ1 With the development of deep learning in artiﬁcial intelligence, many researchers have adopted new models from artiﬁcial neural networks for forecasting time series. However, poor performance of applying deep AQ2 learning models in short time series hinders the accuracy in time series forecasting. In this paper, we propose a novel approach to alleviate this problem based on transfer learning. Existing work on transfer learning uses extracted features from a source dataset for prediction task in a target dataset. In this paper, we propose a new training strategy for time-series transfer learning with two source datasets that outperform existing approaches. The eﬀectiveness of our approach is evaluated on ﬁnancial time series extracted from stock markets. Experiment results show that transfer learning based on 2 data sets is superior than other base-line methods. Keywords: Transfer learning · Financial time series · Forecasting · Artiﬁcialneuralnetworks 1 Introduction Time-series forecasting is one of the challenging tasks in data analytics and artiﬁcial intelligence area. Time-series prediction plays a crucial role in plethora of applications such as forecasting sales, marketing, ﬁnance, and production planning etc. Traditional statistical methods in time-series forecasting include autoregressive integrated moving average (ARIMA) [16] for non-stationary data, simple exponential smoothing (SES) [9] for predicting time-series, and Holt and Damped exponential smoothing [10]. Besides, Conventional machine learning techniques have been used in time series forecasting, such as support vector regression (SVR) [19] and various hybrid methods [12]. However, time-series c⃝ Springer Nature Switzerland AG 2019 A. C. Nayak and A. Sharma (Eds.): PRICAI 2019, LNAI 11671, pp. 1–13, 2019. https://doi.org/10.1007/978-3-030-29911-8_3Author Proof 2 Q.-Q. He et al. forecasting is a challenging task when limited data is available for training the machine learning models. Recently, the success of deep learning models in image and Natural Lan- guage Processing (NLP) applications becomes a driving force behind the adop- tion of deep learning model for time-series forecasting. The information obtained during the learning processes is used for interpretation of data such as text, images and sound. The learning process also allows the computer to automati- cally extract the pattern features. It also integrates the feature learning into the process of modeling and therefore reduces the incompleteness caused by artiﬁcial design features. Three techniques, namely, a large number of hidden units, better learning algorithms, and parameter initialization techniques, have contributed to the success of deep learning approach [6]. However, when the dataset does not have suﬃcient data, deep learning approach could result poor forecasting performance [26]. In order to alleviate insuﬃcient data problem, transfer learning is commonly used in majority of deep learning model [29]. Transfer learning is shown to be eﬀective in computer vision [3]and NLP[23]. Despite its promising results in computer vision and related applications, transfer learning has been rarely used in deep learning models for time series data. Recently, Fawaz et al. [8] investi- gate how to transfer deep convolutional neural networks (CNNs) for Time Series Classiﬁcation (TSC) tasks. Ye et al. [27] propose a novel transfer learning frame- work for time series forecasting. Most of these studies were designed to transfer features from a source dataset to a target dataset. However, in certain cases it may be necessary for the model to learn features or patterns from diﬀerent source datasets when the target dataset is insuﬃcient. This research problem has been widely considered in developing multilingual speech technologies. Most of these works focus on transferring features between languages because of the limited resources available for the target language [15]. In the time series forecasting, Hu et al. [14] combine wind speed information from multi-sources to build a deep neural network (DNN). In their model, the hidden layers are shared across many farms while the output layers are designed to be farm dependent. In their approach, the shared hidden layers can be considered as a universal feature transformation. Motivated by these recent ﬁndings, in this paper, we investigate whether or not transfer learning can be eﬀectively used for ﬁnancial time series forecasting. Speciﬁcally, we evaluate the eﬀectiveness of transfer learning with more than one source dataset for stock price prediction. In a more detailed case study, we also evaluate the eﬀect of choosing diﬀerent source datasets based on a similarity measure. Besides, a new training strategy for transfer learning with two source datasets is also proposed in this paper. To the best of authors’ knowledge, our work makes the ﬁrst attempt to investigate transfer learning for ﬁnancial time series forecasting problem. Our contributions are two-fold: – We propose a new training strategy for transfer learning with two source datasets.Author Proof Transfer Learning for Financial Time Series Forecasting 3 – We propose a similarity based approach for selecting source datasets for train- ing the deep learning models with transfer learning for ﬁnancial time series forecasting. The rest of the paper is structured as follows. In Sect. 2, we describe some background knowledge and review existing work on transfer learning for time series forecasting. In Sect. 3, we describe our proposed training strategy used in this paper. In Sect. 4, we present our experiment setups and discuses the result. Finally, in Sect. 5, we conclude the paper with future work. 2 Background and Related Work 2.1 Financial Time Series Forecasting Financial time series forecasting is a challenging task due to the noise and volatile features of the underlying market situations [24]. Several technical indi- cators used for time-series prediction include auto-regression (AR), moving- average(MA), ARIMA, Holt-Winters Exponential Smoothing (HWES) and so on. However, with the development of deep learning models, DNN, recurrent neural network (RNN) and CNN have been extensively researched in time series forecasting area. Deep learning can successfully learn complex real-world data by extracting robust features that capture most useful information [13]. Ding et al. [7] combine the neural tensor network and deep CNN to predict the short– term and long–term inﬂuences of events on stock price movements. Yoshihara et al. [28] also adopted deep belief networks in ﬁnancial market forcasting. How- ever, Makridakis et al. [18] conclude that when the available data is insuﬃcient for training, the performance of deep learning can be poorer than simple statis- tical methods. 2.2 Transfer Learning in Time Series Forecasting Transfer learning aims to extract knowledge from one or more source tasks and applies the knowledge to a target task [20]. The study of Transfer learning is motivated by the fact that people can intelligently apply knowledge learned previously to solve new problems faster or with better solutions. Karl et al. [25] have deﬁned the transfer learning as follows: “Transfer learning for deep neural networks, is the process of ﬁrst training a base network on a source dataset and task, and then transfer the learned features (the network’s weights) to a second network to be trained on a target dataset and task ”. Transfer learning has been used in computer version and nature language processing. These cases include application of transfer learning in visual application by Amaral et al. [3] and initialization of the language model’s weights with pre-trained weights by Ramachandran et al. [21]. Recently, transfer learning was also adopted for time series analysis. For example, Ye et al. [27] propose a novel transfer learning framework for time series forecasting. To calculate the similarity as the guidelineAuthor Proof 4 Q.-Q. He et al. of selecting source datasets, Fawaz et al. proposed a dynamic time warping (DTW) based algorithm in [8]. In [14], Hu et al. proposed an approach for the prediction of the wind speed for the new farm by transferring information from several old farms. In [14], the authors use the time series data of several old farms to pre-train a two-layers DNN model. The parameters of the model are then shared with all wind farms. The model can be considered as a universal feature transformation. In contrast to the approach proposed by Hu et al. [14], in our training strategy, a source dataset is used for training the ﬁrst layer of the DNN only and the second layer is trained by using both source datasets. In addition, the parameters of these layers are not shared among the layers. Therefore, the model from our strategy not only have the universal features but also the speciﬁc features. In addition, Hu et al. [14] did not compare the performance of the model based on one source dataset with model which is built from multi-source datasets. 3Method In this paper, we aim to answer several questions related to transfer learning in time series prediction: – Is transferring features from two source datasets better than transferring from one source dataset? – What is the eﬀect of transfer learning when the ratio for training is increased while the size for the testing is ﬁxed in the target dataset? – Whether or not the similarity measure of two data sets can be used as an indicator for selecting source datasets? 3.1 Training Strategy In the context of multi-domain transfer learning, Glorot et al. [11]proposeda strategy in which a learned shared model from a set of domains is adapted for each individual target domain. The shared hidden layers can be considered as a universal feature transformation that works well for many domains. However, all source domains may not have the same inﬂuence on the performance of target domain. In this paper, we extend shared-parameters strategy proposed in [11]. In our approach, the parameters of the hidden layers not only contain the universal features, but also maintain speciﬁc features of the source domains. The architectures of the proposed strategy is shown in Fig. 1. Assume that D1 s and D2 s are two diﬀerent source datasets and D1 s is more similar to the target dataset Dt than D2 s. In our approach, the ﬁrst layer of the proposed architecture will learn the features of D1 s and the second layer will learn the features of both D1 s and D2 s. The proposed strategy has three steps: 1. First, we use the ﬁrst source dataset to pre-train the deep learning model. 2. Next, we freeze the ﬁrst layer of the model. Then we use the second source dataset to train the model. 3. Finally, we use the target dataset to ﬁne-tune the whole deep learning model. The process of training model is similar to stack auto-encoder [4].Author Proof Transfer Learning for Financial Time Series Forecasting 5 Fig. 1. Our proposed training strategy. 3.2 Network Architecture The deep learning model used in our approach comprises of an input layer, two hidden layers, and an output layer. The model is designed to be a generic model meaning that it can be replaced with a DNN or a Long Short-Term Memory (LSTM) model. Note that our training strategy proposed in the previous section is independent of the chosen network architecture. Hu et al. [14] used two hidden layers and each number of each hidden layer contains 100 nodes. In [14], Sigmoid function is used as the activation function. Similar to their approach, in DNN model, we also used two hidden layers and increased the number of nodes in each layer to 256. Besides, we choose Tanh as activation function because it gives better training performance for multi-layer neural networks [17]. The output layer contains one unit with Linear activation function. The network is shown in Fig. 2. The LSTM model used in our approach consists of two LSTM cells with Tanh as activation function and one output layer of a neuron with Linear activation function. Each LSTM layer has a 256- dimensional state vector. LSTM solves the gradient explosion and vanishing problem of RNN. The LSTM network used in this paper is shown in Fig. 3. Fig. 2. Two-layers DNN network.Author Proof 6 Q.-Q. He et al. Fig. 3. Two-layers LSTM network. 4 Experiments 4.1 Experiment Setup During the experiment, we used DNN and LSTM models for forecasting. We implemented our network using open source deep learning library Keras [1] with the Tensorﬂow [2] back-end. We run our experiments on Octal Core Intel(R) Core(TM) i7-6700 CPU @ 3.40 GHz. In this paper, we have compared ﬁve strate- gies for testing the eﬀectiveness of diﬀerent transfer learnings in ﬁnancial time series forecasting. These strategies are numbered from M1 to M5. – M1: Training the neural network model without transfer learning. – M2: Transfer learning from source dataset D1 s to target dataset Dt. – M3: Transfer learning from source dataset D2 s to target dataset Dt. – M4: Transferring learning from source datasets D1 s and D2 s to target dataset Dt with shared parameter strategy. – M5: Transferring features from source datasets D1 s and D2 s to target dataset Dt with our proposed strategy. The ﬁrst strategy is to train the model with the target dataset without trans- fer learning. We denote this strategy as the baseline approach. In the second and third strategies, the model is pre-trained by only using one of the source datasets and subsequently ﬁne-tuned by using the target dataset. In the fourth strategy, the model is pre-trained by using two source datasets with shared parameters [14] and subsequently ﬁne-tuned by using target dataset. In the last strategy, the model is pre-trained by using two source datasets with our proposed training strategy from Sect. 3.1. In the experiments, we adopt similar Hyper-parameters from [8] except that the epochs and batch size are set to 100 and 200 respec- tively. Besides, the learning rate of ﬁne-tune step is also set to 0.00001. The Hyper-parameters used for the models are listed in Table 1. 4.2 Evaluation After the learning process, the output of the model are inverse-normalized before computing the indicators. In this paper, we choose three classical indicatorsAuthor Proof Transfer Learning for Financial Time Series Forecasting 7 Table 1. The hyperparameters used for the experiments. Hyperparameter Baseline Pre-train Fine-tune Epochs 100 100 100 Batch size 200 200 200 Optimizer Adam Adam Adam Learning rate 0.001 0.001 0.00001 First moment 0.9 0.9 0.9 Second moment 0.999 0.999 0.999 Loss function Cross-entropy Cross-entropy Cross-entropy (MAP E, RM SE and R2) to measure the predictive accuracy of each model. The deﬁnitions of these indicators are as follows: MAP E = 100% n n∑ i=1 ∣ ∣ ∣ ∣ yi − ˆyi yi ∣ ∣ ∣ ∣ (1) RM SE = √ ∑ (yi − ˆyi) 2 n (2) R2 =1 − ∑ (yi − ˆyi) 2 ∑ (yi − yi) 2 (3) In these equations, yi is the actual value and ˆyi is the predicted value. n represents the prediction period. MAP E measures the size of the error. RM SE is the mean of the square root of the error between the predicted value and thetruevalue. R2 is used for evaluating the ﬁtting situation of the prediction model. The lower the MAP E and RM SE, the better the model in forecasting. In contrast, higher the R2, better the trained model. 4.3 Datasets In the experiments, we use the stock market data from Yahoo Finance (https:// ﬁnance.yahoo.com/). We choose three diﬀerent groups of source and target datasets. The ﬁrst group of datasets include Hang Seng Commerce & Indus- try (HSNC, 1000 time points) and Hang Seng Properties Index (HSNP, 1000 time points) as source datasets and Hang Seng Finance Index (HSNF, 1000 time points) as target dataset. The second datasets contain Dax Performance- Index (DAX, 5000 time points) and CAC 40 (CAC, 5000 time points) as source datasets and FTSE 100 (FTSE, 4000 time points) as the target dataset. Third datasets include S&P 500 (GSPC, 10000 time points) and Nasdaq (IXIC, 10000 time points) as source datasets, and Dow 30 (DJI, 8000 time points) as target dataset. These groups are diﬀerent in size and we label them as small, mid-size, and large datasets.Author Proof 8 Q.-Q. He et al. Time series data are preprocessed before they are used for deep learning model. First, we drop missing and abnormal values. We also transform the time series into acceptable data format by the DNN and LSTM. For a given time- stamp t (day), the input vector x consists of 90-day historical stock price: x = [p(t),...,p(t−89)] and the output vector y consists of 1-day stock price from time t: y = pt+1. We trained the model using 90 days for the lookback and 1 day for the forecast horizon. The value of time series are min-max scaled to [-1, 1] interval. During the experiments, we used the 70% and 30% of the target dataset for training and testing. The test size of target dataset of HSNF, FTSE, DJI are 273, 1173, and 2373 respectively. 4.4 Results and Discussion In the experiments, we compare the performance of our strategy with the shared parameter strategy for multi-source datasets. We also examine the eﬀect of trans- fer learning when the ratio for training is increased while the size for the testing is ﬁxed in the target dataset. The experiment results are listed in Table 2.From these results, we can observe that the model with transfer learning have signiﬁ- cant impact on time series forecasting. Besides, transfer learning with two source datasets (M4 and M5) is better than transfer learning with one source dataset (M2 and M3). Our proposed strategy (M5) achieve good results in majority of the cases. Table 2. Experiment results for diﬀerent transfer learning strategies. D 1 s D 2 s Dt Strategy DNN LSTM MAPE RMSE R2 MAPE RMSE R2 HSNC HSNP HSNF M1 1.2445 622.0425 0.9569 1.5868 801.2603 0.9291 M2 1.0356 522.7943 0.9698 1.0088 511.6846 0.9711 M3 1.0297 518.9767 0.9702 0.9899 507.3583 0.9716 M4 0.9944 505.0655 0.9718 0.9618 494.5743 0.9730 M5 0.9851 499.9882 0.9724 0.97348 492.6246 0.9732 DAX CAC FTSE M1 0.9380 79.9719 0.9718 0.7254 63.7207 0.9835 M2 0.7020 62.6288 0.9841 0.6533 58.6790 0.9861 M3 0.6832 61.3956 0.9847 0.6817 60.3091 0.9853 M4 0.6708 60.4791 0.9852 0.6750 59.8770 0.9855 M5 0.6752 60.5211 0.9852 0.6675 59.4312 0.9857 GSPC IXIC DJI M1 0.9287 212.3614 0.9978 1.2121 289.9345 0.9959 M2 0.7428 172.5451 0.9985 0.7162 166.7077 0.9986 M3 0.7023 168.2734 0.9986 0.7469 174.1719 0.9985 M4 0.9489 210.1081 0.9979 0.6807 159.6134 0.9987 M5 0.6858 158.5243 0.9988 0.6787 158.7363 0.9988 The experiment results for the eﬀect of transfer learning when the ratio for training is increased while the size for the testing is ﬁxed in the target datasetAuthor Proof Transfer Learning for Financial Time Series Forecasting 9 are shown in Figs. 4, 5,and 6. The experiment settings for testing with diﬀerent ratio of target training dataset for ﬁne-tuning is similar to previous experiments except that we use 20%, 40%, 60%, 80%, 100% of target training dataset for ﬁne-tuning. In these experiments, the size of test datasets for target dataset of HSNF, FTSE, DJI are kept constant at 273, 1173, and 2373 respectively. From 4, 5,and 6, we can observe that DNN model with transfer learning is better than the model without transfer learning regardless of the size of training dataset. We also found that M5 (LSTM) gained good result in majority of the cases. Fig. 4. The MAPE of HSNF dataset. Fig. 5. The MAPE of FTSE dataset.Author Proof 10 Q.-Q. He et al. Fig. 6. The MAPE of DJI dataset. 4.5 Additional Experiment with Similarity Measure for Source Datasets Rosenstein et al. [22] empirically showed that if two datasets are dissimilar, then brute-force transfer may negatively eﬀect the performance of the target dataset. Such eﬀect is also labeled as negative transfer by [22]. In this section, we further examine the eﬀect of similarity between the source and target datasets on the overall results of transfer learning strategies. In this paper, we adopt Dynamic Time Warping (DTW) [5] algorithm to calculate the similarity of two time series (two datasets). DTW does not require that the two time series to be in the same length. DTW also permits time shifting between the two time series. Therefore, in the following four experiments, we use the mean value of two DTW distances as the main criteria to ﬁnd potential source datasets for a given target dataset. The equation of mDT W (mean DTW distance) is deﬁned as follow: mDT W = DT W (D1 s,Dt)+ DT W (D2 s ,Dt) 2 (4) The smaller the mDTW, the more similar between the source and target dataset. In the four experiments, time series of Bank of China (BOC) is used as the target datasets for all transfer learning strategies. Source datasets for the experiments include China Construction Bank Corporation (CCB) and Industrial and Com- mercial Bank of China Limited (ICBC), Alibaba (BABA) and Lenovo Group (Leveno), Hang Seng Bank Limited (HSB) and Bank of America (BOA), Ten- cent and CLP Group (CLP). In experiment 1 and 3, the source datasets are selected from the same industry (i.e. Finance). In experiment 2 and 4, the source datasets are selected from the diﬀerent industries. In Experiment 1 and 2, we evaluate the eﬀect of small distance (mDT W =0.6) between source and target datasets and Experiment 3 and 4 are designed to evaluate the eﬀect of larger distance (mDT W > 0.15). The results of the experiments are listed in Table 3. From these results, we can observe that selecting the source and target datasetsAuthor Proof Transfer Learning for Financial Time Series Forecasting 11 from the same industry produces the best results. We can also observe that for the case of same industry with small mDT W , the results of M4 (DNN) and M5 (LSTM) are superior. The results of M5 (LSTM) is also superior for the case of selecting source and target datasets from the same industry with large mDT W value. Table 3. Transfer learning strategies for diﬀerent DTW distance and industry. Exp D 1 s D 2 s Dt mDTW Strategy DNN LSTM MAPE RMSE R2 MAPE RMSE R2 M1 1.3095 0.0672 0.9735 1.3271 0.07107 0.9705 Exp1 CCB ICBC BOC 0.06 M2 1.1568 0.0609 0.9783 1.0011 0.0558 0.9819 M3 1.1419 0.0592 0.9795 0.9765 0.0543 0.9828 M4 1.0397 0.0551 0.9823 0.9817 0.0551 0.9823 M5 1.0631 0.0560 0.9817 0.9780 0.0547 0.9825 Exp2 BABA Lenovo BOC 0.06 M2 1.1499 0.0601 0.9789 1.0264 0.0567 0.9812 M3 1.2798 0.0656 0.9749 1.4130 0.0768 0.9656 M4 1.0769 0.0585 0.9800 1.0060 0.0561 0.9816 M5 1.0927 0.0586 0.9800 1.0070 0.0552 0.9822 Exp3 HSB BOA BOC 0.15 M2 1.1989 0.0634 0.9766 1.1064 0.0601 0.9790 M3 1.2582 0.0648 0.9755 1.0459 0.0563 0.9815 M4 1.1565 0.0613 0.9781 0.9861 0.0548 0.9825 M5 1.1287 0.0597 0.9792 0.9828 0.0542 0.9829 Exp4 Tencent CLP BOC 0.17 M2 1.2217 0.06404 0.9761 1.4027 0.0734 0.9685 M3 1.1988 0.0613 0.9781 1.0334 0.0560 0.9817 M4 1.1434 0.0599 0.9790 0.9899 0.0556 0.9819 M5 1.1119 0.0585 0.9800 1.0019 0.0563 0.9815 5 Conclusion In this paper, we propose a new training strategy for transfer learning with two source datasets. The experiment results reveal that the model with transfer learning has positive impact on ﬁnancial time series forecasting. The experiment results also reveal that transfer learning with more source datasets is superior than using a single source dataset. In addition, the proposed training strategy (M5) achieve good results in majority of the cases. Although the proposed strat- egy is tested with only 2 source datasets, it can be extended for training more datasets after additional hidden layers are added to the network architecture. A similarity based approach based on DTW for selecting source and target datasets for training the deep learning models with transfer learning for ﬁnancial time series forecasting is also proposed in this paper. Experiment results show that the transfer learning with similar (i.e. smaller mDT W ) source datasets from the same industry is superior than selecting source datasets from diﬀerent industries. In the paper, we use the DTW distance to calculate the similarity of two time series. As for the future work, we are planning to investigate the eﬀect of other distance functions on the transfer learning strategies. In addition, we are also planning to test the eﬀect of more than two source datasets on the training out- come. We are also conducting experiments for the deep learning neural networks with varying number of hidden layers.Author Proof 12 Q.-Q. He et al. Acknowledgement. The research was funded by the Research Committee of Univer- sity of Macau, Grant MYRG2018-00246-FST. References 1. Keras (2015). https://keras.io/ 2. Abadi, M., et al.: TensorFlow: a system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2016), pp. 265–283 (2016) 3. Amaral, T., Silva, L.M., Alexandre, L.A., Kandaswamy, C., de S´a, J.M., Santos, J.M.: Transfer learning using rotated image data to improve deep neural network performance. In: Campilho, A., Kamel, M. (eds.) ICIAR 2014. LNCS, vol. 8814, pp. 290–300. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11758-4 32 4. Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise training of deep networks. In: Advances in Neural Information Processing Systems, pp. 153–160 (2007) 5. Berndt, D., Cliﬀord, J.: Using dynamic time warping to ﬁnd patterns in time series. In: KDD Workshop, vol. 10, no. 16, pp. 359–370 (1994) 6. Deng, L., Yu, D.: Deep learning for signal and information processing. Found. Trends Signal Process. 2–3, 197–387 (2013) 7. Ding, X., Zhang, Y., Liu, T., Duan, J.: Deep learning for event-driven stock predic- tion. In: Proceedings of the 24th International Conference on Artiﬁcial Intelligence, pp. 2327–2333. AAAI Press (2015) 8. Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.A.: Transfer learning for time series classiﬁcation. In: 2018 IEEE International Conference on Big Data (Big Data). pp. 1367–1376. IEEE (2018) 9. Gardner Jr., E.S.: Exponential smoothing: the state of the art. Int. J. Forecast. 4(1), 1–28 (1985) 10. Gardner Jr., E.S.: Exponential smoothing: the state of the art–part ii. Int. J. Forecast. 22(4), 637–666 (2006) 11. Glorot, X., Bordes, A., Bengio, Y.: Domain adaptation for large-scale sentiment classiﬁcation: a deep learning approach. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 513–520 (2011) 12. Haque, A.U., Nehrir, M.H., Mandal, P.: A hybrid intelligent model for deterministic and quantile regression approach for probabilistic wind power forecasting. IEEE Trans. Power Syst. 29(4), 1663–1672 (2014) 13. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neu- ral networks. Science 313(5786), 504–507 (2006) 14. Hu, Q., Zhang, R., Zhou, Y.: Transfer learning for short-term wind speed prediction with deep neural networks. Renew. Energy 85, 83–95 (2016) 15. Huang, J.T., Li, J., Yu, D., Deng, L., Gong, Y.: Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 7304– 7308. IEEE (2013) 16. Hyndman, R., Khandakar, Y.: Automatic time series forecasting: the forecast pack- age for R. J. Stat. Softw. 27(3), 1–22 (2008) 17. Karlik, B., Olgac, A.V.: Performance analysis of various activation functions in generalized mlp architectures of neural networks. Int. J. Artif. Intell. Expert Syst. 1(4), 111–122 (2011)Author Proof Transfer Learning for Financial Time Series Forecasting 13 18. Makridakis, S., Spiliotis, E., Assimakopoulos, V.: Statistical and machine learning forecasting methods: Concerns and ways forward. PLOS One 13(3), 1–26 (2018) 19. Ortiz-Garc´ıa, E.G., Salcedo-Sanz, S., P´erez-Bellido, ´A.M., Gasc´on-Moreno, J., Portilla-Figueras, J.A., Prieto, L.: Short-term wind speed prediction in wind farms based on banks of support vector machines. Wind Energy 14(2), 193–207 (2011) 20. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22(10), 1345–1359 (2009) 21. Ramachandran, P., Liu, P., Le, Q.: Unsupervised pretraining for sequence to sequence learning. In: Proceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing, pp. 383–391. Association for Computational Linguistics (2017) 22. Rosenstein, M.T., Marx, Z., Kaelbling, L.P., Dietterich, T.G.: To transfer or not to transfer. In: NIPS 2005 Workshop on Transfer Learning, vol. 898, pp. 1–4 (2005) 23. Vu, N.T., Imseng, D., Povey, D., Motlicek, P., Schultz, T., Bourlard, H.: Multilin- gual deep neural network based acoustic modeling for rapid language adaptation. In: 2014 IEEE International Conference on Acoustics, Speech and Signal Process- ing (ICASSP), pp. 7639–7643. IEEE (2014) 24. Wang, B., Huang, H., Wang, X.: A novel text mining approach to ﬁnancial time series forecasting. Neurocomputing 83, 136–145 (2012) 25. Weiss, K., Khoshgoftaar, T.M., Wang, D.: A survey of transfer learning. J. Big Data 3(1), 9 (2016). https://doi.org/10.1186/s40537-016-0043-6 26. Wu, D.D., Olson, D.L.: Financial risk forecast using machine learning and sen- timent analysis. In: Wu, D.D., Olson, D.L. (eds.) Enterprise Risk Manage- ment in Finance, pp. 32–48. SPringer, London (2015). https://doi.org/10.1057/ 9781137466297 5 27. Ye, R., Dai, Q.: A novel transfer learning framework for time series forecasting. Knowl. Based Syst. 156, 74–99 (2018) 28. Yoshihara, A., Fujikawa, K., Seki, K., Uehara, K.: Predicting stock market trends by recurrent deep neural networks. In: Pham, D.-N., Park, S.-B. (eds.) PRICAI 2014. LNCS (LNAI), vol. 8862, pp. 759–769. Springer, Cham (2014). https://doi. org/10.1007/978-3-319-13560-1 60 29. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in deep neural networks? In: Advances in Neural Information Processing Systems, pp. 3320–3328 (2014)Author ProofAuthor Queries Chapter 3 Query Refs. Details Required Author’s response AQ1 This is to inform you that corresponding author has been identiﬁed as per the information available in the Copy- right form. AQ2 As Per Springer style, both city and country names must be present in the aﬃliations. Accordingly, we have inserted the city name “Taipa” in ﬁrst aﬃliation. Please check and conﬁrm if the inserted city name is correct. If not, please provide us with the correct city name.Author Proof","libVersion":"0.3.2","langs":""}
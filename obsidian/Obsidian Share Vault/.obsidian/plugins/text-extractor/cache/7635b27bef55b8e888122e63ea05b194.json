{"path":"lit/lit_sources/Tang23end2endPredOptPyEPO.pdf","text":"Noname manuscript No. (will be inserted by the editor) PyEPO: A PyTorch-based End-to-End Predict- then-Optimize Library for Linear and Integer Programming Bo Tang · Elias B. Khalil Received: date / Accepted: date Abstract In deterministic optimization, it is typically assumed that all prob- lem parameters are ﬁxed and known. In practice, however, some parameters may be a priori unknown but can be estimated from historical data. A typical predict-then-optimize approach separates predictions and optimization into two stages. Recently, end-to-end predict-then-optimize has become an attrac- tive alternative. In this work, we present the PyEPO package, a PyTorch- based end-to-end predict-then-optimize library in Python. To the best of our knowledge, PyEPO (pronounced like pineapple with a silent “n”) is the ﬁrst such generic tool for linear and integer programming with predicted objec- tive function coeﬃcients. It provides four base algorithms: a convex surrogate loss function from the seminal work of Elmachtoub and Grigas [16], a diﬀer- entiable black-box solver approach of Poganˇci´c et al. [35], and two diﬀeren- tiable perturbation-based methods from Berthet et al. [6]. PyEPO provides a simple interface for the deﬁnition of new optimization problems, the imple- mentation of state-of-the-art predict-then-optimize training algorithms, the use of custom neural network architectures, and the comparison of end-to- end approaches with the two-stage approach. PyEPO enables us to conduct a comprehensive set of experiments comparing a number of end-to-end and two-stage approaches along axes such as prediction accuracy, decision quality, and running time on problems such as Shortest Path, Multiple Knapsack, and the Traveling Salesperson Problem. We discuss some empirical insights from Bo Tang Department of Mechanical and Industrial Engineering, University of Toronto 5 King’s College Rd, Toronto, ON M5S 3G8 E-mail: botang@mie.utoronto.ca Elias B. Khalil Department of Mechanical and Industrial Engineering, University of Toronto 5 King’s College Rd, Toronto, ON M5S 3G8 E-mail: khalil@mie.utoronto.caarXiv:2206.14234v2 [math.OC] 14 Apr 2023 2 Bo Tang, Elias B. Khalil these experiments, which could guide future research. PyEPO and its docu- mentation are available at https://github.com/khalil-research/PyEPO. Keywords Data-driven optimization · Mixed integer programming · Machine learning Mathematics Subject Classiﬁcation (2020) 90-04, 90C11, 62J05 1 Introduction Predictive modeling is ubiquitous in real-world decision-making. For instance, in many applications, the objective function coeﬃcients of the optimization problem, such as travel time in a routing problem, customer demand in a delivery problem, and assets return in portfolio optimization, are unknown at the time of decision-making. In this work, we are interested in the com- monly used paradigm of prediction followed by optimization in the context of linear programs or integer linear programs, two widely applicable modeling frameworks. Here, it is assumed that a set of features describe an instance of the optimization problem. A regression model maps the features to the (un- known) objective function coeﬃcients. A deterministic optimization problem is then solved to obtain a solution. Due to its wide applicability and simplicity compared to other frameworks for optimization under uncertain parameters, the predict-then-optimize paradigm has received increasing attention in recent years. One natural idea is to proceed in two stages, ﬁrst training an accurate predictive model, then solving the downstream optimization problem using predicted coeﬃcients. However, prediction errors such as mean squared error cannot measure the quality of decisions appropriately. While a perfect pre- diction would always yield an optimal decision, learning a prediction model without errors is impracticable. Bengio [5], Ford et al. [19], and Elmachtoub and Grigas [16] reported that training a predictive model based on prediction error leads to worse decisions than directly considering decision error. Thus, the state-of-art alternative is to integrate optimization into prediction, taking into account the impact on the decision – the so-called end-to-end learning framework. End-to-end predict-then-optimize requires embedding an optimization solver into the model training loop. Classical solution approaches for linear and in- teger linear models, including graph algorithms, linear programming, integer programming, constraint programming, etc., are well-established and eﬃcient in practice. In addition, commercial solvers such as Gurobi [22] and CPLEX [9] are highly optimized and enable users to easily express business or academic problems as optimization models without a deep understanding of theory and algorithms. However, embedding a solver for end-to-end learning requires ad- ditional computation and integration (e.g., gradient calculation) that current software does not provide. Title Suppressed Due to Excessive Length 3 On the other hand, the ﬁeld of machine learning has witnessed tremendous growth in recent decades. In particular, breakthroughs in deep learning have led to remarkable improvements in several complex tasks. As a result, neural networks now pervade disparate applications spanning computer vision, nat- ural language, and planning, among others. Python-based machine learning frameworks such as Scikit-Learn [34], TensorFlow [1], PyTorch [33], MXNet [8], etc., have been developed and extensively used for research and produc- tion needs. Although deep learning has proven highly eﬀective in regression and classiﬁcation, it lacks the ability to handle constrained optimization such as integer linear programming. Since Amos and Kolter [4] ﬁrst introduced a neural network layer for generic mathematical optimization, there have been some prominent attempts to bridge the gap between optimization solvers and the deep learning frame- work. The critical component is typically a diﬀerentiable block for optimization tasks. With a diﬀerentiable optimizer or a direct decision loss function, neural network packages enable the computation of gradients for optimization oper- ations and then update predictive model parameters based on a loss function that depends on decision quality. While research code implementing a number of predict-then-optimize train- ing algorithms have been made available for particular classes of optimization problems and/or predictive models [21, 14, 36, 17, 26, 16, 35, 26, 12, 3, 2], there is a dire need for a generic end-to-end learning framework, especially for linear and integer programming. In this paper, we propose the open-source software package PyEPO which aims to customize and train end-to-end predict-then- optimize for linear and integer programming. Our contributions are as follows: 1. We implement SPO+ (“Smart Predict-then-Optimize+”) loss [16], DBB (dif- ferentiable black-box) solver [35], DPO (diﬀerentiable perturbed optimizer), and PFYL (perturbed Fenchel-Young loss), which are four typical end-to- end methods for linear and integer programming. 2. We build PyEPO based on PyTorch. As one of the most popular deep learning frameworks, PyTorch makes it easy to use and integrate any deep neural network. 3. We provide interfaces to the Python-based optimization modeling frame- works GurobiPy and Pyomo. Such high-level modeling languages allow non-specialists to formulate optimization models with PyEPO. 4. We enable parallel computing for the forward pass and backward pass in PyEPO. Optimizations in training are carried out in parallel, allowing users to harness multiple processors to reduce training time. 5. We present new benchmark datasets for end-to-end predict-then-optimize, allowing us to compare the performance of diﬀerent approaches. 6. We conduct and analyze a comprehensive set of experiments for end-to-end predict-then-optimize. We compare the performance of diﬀerent methods on a number of datasets. We show the competitiveness of end-to-end learn- ing and the surprising eﬀect of relaxations and hyperparameter tuning. A 4 Bo Tang, Elias B. Khalil number of empirical ﬁndings are reported to support new research direc- tions within this topic. 2 Related work In early work on the topic, Bengio [5] introduced a diﬀerentiable portfolio optimizer and suggested that direct optimization with ﬁnancial criteria has better performance in neural networks compared to the mean squared error of predicted values. Kao et al. [25] trained a linear regressor with a convex combination of prediction error and decision error, but they only considered unconstrained quadratic programming. With the success of deep learning, predict-then-optimize research has adopted gradient-based methods. Domke [13] investigated generic gradient-descend methods to minimize unconstrained energy functions. More recently, the interest has been in constrained optimiza- tion problems that represent much of the real-world applications. For instance, Gould et al. [21] allows diﬀerentiating bi-level optimization problems with and without constraints. In subsequent studies, a comprehensive exploration of dif- ferentiable constraint optimization encompasses various types of optimization problems such as quadratic programming, linear programming, and integer lin- ear programming. A comparison of methodologies for end-to-end constrained optimization are presented in Table 1 and Table 2. Another notable piece of work that does not employ gradients is that of the predict-then-optimize decision tree of Elmachtoub et al. [15]. 2.1 Gradients of Optimal Solutions via the KKT Conditions Gradient-based end-to-end learning requires well-deﬁned ﬁrst-order derivatives of an optimal solution with respect to the cost vector. The KKT conditions become an attractive option because they make the optimization problem with hard constraints diﬀerentiable. Amos and Kolter [4] proposed OptNet , which derives gradients of con- strained quadratic programs from the KKT conditions. Based on OptNet , Donti et al. [14] investigated a general end-to-end framework, DQP , for learn- ing with constrained quadratic programming, which improved decision-making over two-stage models. Although linear programming is a special case of quadratic programming, DQP has no ability to tackle linear objective functions because the gradient is zero almost everywhere and undeﬁned otherwise. Nevertheless, Amos and Kolter [4] adopted a quadratic objective for the Sudoku problem, eﬀectively expressing a linear constraint satisfaction problem as a quadratic problem to obtain useful gradients. Similarly, Wilder et al. [36] take inspiration from OptNet and add a small quadratic objective term on DQP to make a linear program second-order diﬀerentiable, resulting in QPTL . Wilder et al. [36] also discussed the relaxation and rounding for the approxi- mation of binary problems. Further, Ferber et al. [17] followed up on QPTL with Title Suppressed Due to Excessive Length 5 MIPaaL , a cutting-plane approach to support (mixed) integer programming. With the cutting-plane method, MIPaaL generates (potentially exponentially many) valid cuts to convert a discrete problem into an equivalent continuous problem, which is theoretically sound for combinatorial optimization but ex- tremely time-consuming. In addition, Mandi and Guns [26] introduced IntOpt based on the interior-point method, which computes gradients for linear pro- gramming with log-barrier term instead of the quadratic term of the QPTL . Except for MIPaaL [17], end-to-end learning approaches for (mixed) integer programming use linear relaxation during training but evaluate with optimal integer solutions at test time. Besides DQP and its extension, Agrawal et al. [2] introduced a generic frame- work, CvxpyLayers , which diﬀerentiates through the KKT conditions of the conic program. The central idea in CvxpyLayers is to canonicalize disciplined convex programming as conic programming so that it is applicable to a wider range of convex optimization problems. In particular, for problems such as linear programs which cannot be diﬀerentiated at a solution, a least-squares solution is used as a heuristic. However, these KKT-based methods require a solver for either quadratic or conic programming. Linear programming requires additional objective terms and the solver eﬃciency of the above implementations is not comparable to commercial MILP solvers such as Gurobi [22] and CPLEX [9]. Furthermore, they do not naturally support discrete optimization due to non-convexity. Method In PyEPO w/ Unk Constr Disc Var Lin Obj Quad Obj DQP [14] [code] 7 3 7 7 3 QPTL [36] [code] 7 7 7 3 3 MIPaaL [17] 7 7 3 3 7 IntOpt [26] [code] 7 7 7 3 7 CvxpyLayers [2] [code] 7 3 7 3 3 SPO+ [16] [code] 3 7 3 3 7 SPO+ Rel [27] [code] 3 7 3 3 7 SPO+ WS [27] [code] 7 7 3 3 7 DBB [35] [code] 3 7 3 3 7 DPO [6] [code] 3 7 3 3 7 PFYL [6] [code] 3 7 3 3 7 Table 1: Methodology Comparison This is a comparison diagram for diﬀerent methodologies. The ﬁrst set of methods uses the KKT conditions, and the second part is based on diﬀerentiable approximations. ”In PyEPO” denotes whether the method is available in PyEPO. ”w/ Unk Constr” denotes whether unknown parameters occur in constraints. ”Disc Var” denotes whether the method supports integer variables. ”Lin Obj” denotes whether the method supports a linear objective function. ”Quad Obj” denotes whether the method supports a quadratic objective function. 6 Bo Tang, Elias B. Khalil 2.2 Diﬀerentiable Approximations as an Alternative to KKT Since the KKT conditions may not be ideal for linear programming, researchers have also explored designing gradient approximations. For example, Elmach- toub and Grigas [16] proposed regret (SPO loss in their paper) to measure de- cision error. Since the regret loss suﬀers from non-convexity and discontinuity of linear programming, they then developed a method SPO+ , in which a convex and sub-diﬀerentiable loss guaranteed that a useful subgradient could be com- puted and used to guide training. Same as previous approaches, SPO+ solves an optimization problem in each forward pass. In contrast to the KKT-based methods, SPO+ is limited to the linear objective function. Because optimiza- tion is the computational bottleneck of SPO+ , Mandi et al. [27] utilized SPO+ on combinatorial problems by applying a relaxation as well as warm starting. As Mandi et al. [27] reported, the usage of a relaxation reduced the solving time at the cost of performance. On the other hand, Poganˇci´c et al. [35] computed a subgradient from con- tinuous interpolation of linear objective functions, an approach they referred to as the “diﬀerentiable black-box solver”, DBB . The interpolation approxima- tion was non-convex but also avoided vanishing gradients. Compared to SPO+ , DBB requires an extra optimization problem for the backward pass, and the loss of DBB is ﬂexible (the Hamming distance in their paper). In addition, Berthet et al. [6] applied perturbed optimizer DPO by adding random noise to the cost vector so that a nonzero expected derivative of linear programming can be obtained and further constructed Fenchel-Young loss PFYL . As the key algo- rithms of PyEPO, SPO+ , DBB , DPO , and PFYL are further discussed in Section 3. 2.3 Software for Predict-then-Optimize Research code. Table 1 lists and links to the codebases that will be dis- cussed next. Amos and Kolter [4] developed a PyTorch-based solver qpth for OptNet , which was used to solve a quadratic program and compute its deriva- tives eﬃciently. The solver was based on an eﬃcient primal-dual interior-point method [29] and can solve batches of quadratic programs on a GPU. Using this solver, Donti et al. [14] provided an open-source repository to reproduce the DQP experiments; the repository was speciﬁcally designed for inventory, power scheduling, and battery storage problems. Wilder et al. [36] provided code for QPTL for budget allocation, bipartite matching, and diverse recommendation. MIPaaL [17] relied on qpth and used CPLEX to generate cutting planes, but there is no available open-source code. Mandi and Guns [26] released IntOpt code for knapsack, shortest path, and power scheduling. Berthet et al. [6] con- tributed the TensorFlow-based DPO and PFYL implementation, which provided universal functions for end-to-end predict-then-optimize but required users to create additional helper methods for tensor operations. Other approaches, in- cluding SPO+ [16, 27] and DBB [35], have open-source code. Elmachtoub and Title Suppressed Due to Excessive Length 7 Grigas [16] provided an implementation of SPO+ in Julia, which contained the shortest path and portfolio optimization problems, while Mandi et al. [27] im- plemented SPO+ with Python for the knapsack and power scheduling problems. Code for DBB [35] as applied to shortest path, traveling salesperson, ranking, perfect matching, and graph matching are available. Except for DPO and PFYL , the above contributions provided solutions to speciﬁc optimization problems, and Berthet et al. [6] did not wrap up DPO and PFYL as a generic library. In conclusion, they were conﬁned to research-grade code for purposes of reproducibility. Software packages. CvxpyLayers [2] is the ﬁrst generic end-to-end predict- then-optimize learning framework. In contrast to the above codes, it requires modeling with a domain-speciﬁc language CVXPY , which is embedded into a diﬀerentiable layer in a straightforward way. The emergence of CvxpyLayers provides a more powerful tool for academia and industry. However, the solver of CvxpyLayers cannot compete with commercial solvers on eﬃciency, especially for linear and integer linear programming. Since end-to-end training requires repeated optimization in each iteration, the ineﬃciency of the solver becomes a bottleneck. In addition, the nature of CvxpyLayers means that it cannot support training with integer variables, which limits applicability to many real-world decision-making problems. Method Computation per Gradient DQP [14] GPU-based primal-dual interior-point method for quadratic programming QPTL [36] GPU-based primal-dual interior-point method for quadratic programming MIPaaL [17] Cutting-plane method + GPU-based primal-dual interior-point method for quadratic programming IntOpt [26] GPU-based primal-dual interior-point method for quadratic programming CvxpyLayers [2] GPU-based primal-dual interior-point method for conic programming SPO+ [16] Linear/integer programming SPO+ Rel [27] Linear programming SPO+ WS [27] Integer programming with warm starting DBB [35] Two Linear/integer programming solves DPO [6] Monte-Carlo sampling, multiple linear/integer programming solves with random noise PFYL [6] Monte-Carlo sampling, multiple linear/integer programming solves with random noise Table 2: Computational cost per gradient calculation for diﬀerent methodolo- gies. 3 Preliminaries 3.1 Deﬁnitions and Notation For the sake of convenience, we deﬁne the following linear programming prob- lem without loss of generality, where the decision variables are w ∈ Rd and all wi ≥ 0, the cost coeﬃcients are c ∈ Rd, the constraint coeﬃcients are 8 Bo Tang, Elias B. Khalil A ∈ Rk×d, and the right-hand sides of the constraints are b ∈ Rk: min w c⊺w s.t. Aw ≤ b w ≥ 0 (1) When some variables wi are restricted to be integers, we obtain a (mixed) integer program: min w c⊺w s.t. Aw ≤ b w ≥ 0 wi ∈ Z ∀i ∈ D′, D′ ⊆ {1, 2, ..., d} (2) For both linear and integer programming, let S be the feasible region, z∗(c) be the optimal objective value with respect to cost vector c, and w∗(c) ∈ W ∗(c) be a particular optimal solution derived from some solver. We deﬁne the optimal solution set W ∗(c) because there may be multiple optima. As mentioned before, some coeﬃcients are unknown and must be pre- dicted before optimizing. Here we assume that only the cost coeﬃcients of the objective function c are unknown, but they correlate with a feature vec- tor x ∈ Rp. Given a training dataset D = {(x1, c1), (x2, c2), ..., (xn, cn)} or D = {(x1, w∗(c1)), (x2, w∗(c2)), ..., w∗((xn, cn)}), one can train a machine learning predictor g(·) to minimize a loss function l(·), where θ is the vector of predictor parameters and ˆc = g(x; θ) is the prediction of the cost coeﬃcient vector c. 3.2 The Two-Stage Method As Figure 1 shows, the two-stage approach trains a predictor g(·) by minimiz- ing a loss function w.r.t. the true cost vector c such as mean squared error (MSE), lMSE(ˆc, c) = 1 n ∥ˆc−c∥ 2. Following training, and given an instance with feature vector x, the predictor outputs a cost vector ˆc = g(x; θ), which is then used for solving the optimization problem. The advantage of the two-stage ap- proach is the utilization of existing machine learning methods. It decomposes the predict-then-optimize problem into a traditional regression and then an optimization. 3.3 Gradient-based End-to-end Predict-then-Optimize The main drawbacks of the two-stage approach are that the decision error is not taken into account in training, and the prediction of costs requires true costs as labels. In contrast, the end-to-end predict-then-optimize method in Figure 2 attempts to minimize the decision error and has the potential Title Suppressed Due to Excessive Length 9 Fig. 1: Illustration of the two-stage predict-then-optimize framework: A la- beled dataset D of (x, c) pairs is used to ﬁt a machine learning predictor that minimizes prediction error. At test time (grey box), the predictor is used to estimate the parameters of an optimization problem, which is then tackled with an optimization solver. The two stages are thus completely separate. Fig. 2: Illustration of the end-to-end predict-then-optimize framework: A la- beled dataset D of (x, c) or (x, w∗(c)) pairs is used to ﬁt a machine learning predictor that directly minimizes decision error. The critical component is an optimization solver which is embedded into a diﬀerentiable predictor (e.g., a neural network). At test time, this approach is similar to the two-stage ap- proach from Figure 1. to learn without true costs. Consistent with deep learning terminology, we will use the term “backward pass” to refer to the gradient computation via the backpropagation algorithm. In order to incorporate optimization into the prediction, we can derive the derivative of the optimization task and then apply the gradient descent algorithm, Algorithm 1, to update the parameters of the predictor. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coeﬃcient matrix A, right-hand side b, data D 1: Initialize predictor parameters θ for predictor g(x; θ) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; θ) 6: Forward pass to compute optimal solution w∗(ˆc) := argminw∈S ˆc⊺w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·) to update parameters θ with gradient 9: end for 10: end for For an appropriately deﬁned loss function, i.e., one that penalizes decision error, the chain rule can be used to calculate the following gradient of the loss w.r.t. predictor parameters: ∂l(ˆc, ·) ∂θ = ∂l(ˆc, ·) ∂ ˆc ∂ ˆc ∂θ = ∂l(ˆc, ·) ∂w∗(ˆc) ∂w∗(ˆc) ∂ ˆc ∂ ˆc ∂θ Note: ∂ ˆc ∂θ = ∂g(x; θ) ∂θ (3) The last term ∂ ˆc ∂θ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diﬀerentiable optimizer ∂l(ˆc,·) ∂ ˆc or the direct decision loss function ∂w∗(ˆc) ∂ ˆc . Because the optimal solution w∗(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w∗, the predictor parameters cannot be updated with gradient descent. Thus, SPO+ and PFYL , as direct decision loss functions, de- rive surrogate ∂l(ˆc,·) ∂ ˆc , measuring decision errors with respect to speciﬁc losses, while DBB and DPO , as diﬀerentiable optimizers, compute approximate ∂w∗(ˆc) ∂ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the diﬀerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c) = c⊺w∗(ˆc) − z∗(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw∈S ˆc⊺w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw∈W ∗(c)w⊺c−z∗(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the Title Suppressed Due to Excessive Length 11 0 50 100 150 200 250 300 Epoch 0.00 0.05 0.10 0.15 0.20 0.25 0.30Loss Learning Curve Regret Unambiguous Regret 0 50 100 150 200 250 300 Epoch 0.05 0.10 0.15 0.20 0.25 0.30Loss Learning Curve Regret Unambiguous Regret Fig. 3: As shown for the learning curves of the training of SPO+ and DBB on the shortest path, regret and unambiguous regret in the various tasks overlap almost exactly. regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Besides regret, decision error can also be deﬁned as the diﬀerence between the true solution and its prediction, such as Hamming distance of solutions [35] and squared error of the solutions [6]. In addition, Dalle et al. [10] also considered treating the objective value c⊺w∗ ˆc itself as a loss. 3.4 Methodologies 3.4.1 Smart Predict-then-Optimize [16] To make the decision error diﬀerentiable, Elmachtoub and Grigas [16] proposed SPO+ , a convex upper bound on the regret: lSP O+(ˆc, c) = −min w∈S{(2ˆc − c) ⊺w} + 2ˆc⊺w∗(c) − z∗(c). (5) One proposed subgradient for this loss writes as follows: 2(w∗(c) − w∗(2ˆc − c)) ∈ ∂lSPO+(ˆc, c) ∂ ˆc (6) Thus, we can use Algorithm 1 to directly minimize lSPO+(ˆc, c) with gradi- ent descent. This algorithm with SPO+ requires solving minw∈S(2ˆc − c)⊺w for each training iteration. To accelerate the SPO+ training, Mandi et al. [27] employed relaxations (SPO+ Rel ) and warm starting (SPO+ WS ) to speed-up the optimization. The idea of SPO+ Rel is to use the continuous relaxation of the integer program during training. This simpliﬁcation greatly reduces the training time at the expense of model performance. Compared to SPO+ , the improvement of SPO+ Rel in training eﬃciency is not negligible. For example, linear programming can be solved in polynomial time while integer programming is worst-case 12 Bo Tang, Elias B. Khalil exponential. In Section 6, we will further discuss this performance-eﬃciency tradeoﬀ. For SPO+ WS , Mandi et al. [27] suggested using previous solutions as a starting point for the integer programming solver, which potentially improves the eﬃciency by narrowing down the search space. 3.4.2 Diﬀerentiable Black-Box Solver [35] DBB was developed by Poganˇci´c et al. [35] to estimate gradients from inter- polation, replacing the zero gradients in ∂w∗(c) ∂c . Thus, Poganˇci´c et al. [35] add a slight perturbation with hyperparameter λ and then utilize ﬁnite dif- ferences to obtain a zero-order estimate of the gradient. The substitute of w∗(c) becomes piecewise aﬃne. Therefore, when computing w∗(ˆc), a useful nonzero gradient is obtained at the cost of faithfulness. The forward pass and backward pass are shown in Algorithm 2 and Algorithm 3. The hyperparam- eter λ ≥ 0 controls the interpolation degree. However, compared to SPO+ , the approximation function of DBB is non-convex, so the convergence to a global optimum is compromised, even when the predictor is convex in its parameters. Algorithm 2 DBB Forward Pass Require: ˆc 1: Solve w∗(ˆc) 2: Save ˆc and w∗(ˆc) for backward pass 3: return w∗(ˆc) Algorithm 3 DBB Backward Pass Require: ∂l(ˆc,·) ∂w∗(ˆc) , λ 1: Load ˆc and w∗(ˆc) from forward pass 2: c′ := ˆc + λ ∂l(ˆc,·) ∂w∗(ˆc) 3: Solve w∗(c′) 4: return ∂w∗(ˆc) ∂ ˆc := 1 λ (w∗(c′) − w∗(ˆc)) Similar to SPO+ , DBB requires solving the optimization problem in each training iteration. Thus, utilizing a relaxation/rounding approach may also work for DBB . However, Poganˇci´c et al. [35] did not consider this option. Given the potential eﬃciency gains that a continuous relaxation can bring, we also conducted experiments for DBB Rel in section 6. The same goes for DPO and PFYL below. 3.4.3 Diﬀerentiable Perturbed Optimizer [6] DPO [6] uses Monte-Carlo samples to estimate solutions, in which the predicted costs ˆc are perturbed with Gaussian noise ξ ∼ N (0, I). Based on the K samples of perturbed costs ˆc + σξ, it returns the estimate of the optimal solution expectation E ξ [w∗(ˆc + σξ)] ≈ 1 K K∑ κ w∗(ˆc + σξκ). DPO outputs the expectation of the optimal solution Eξ[w∗(ˆc + σξ)] under a perturbed cost with random noise ˆc + σξ, rather than the optimal solution Title Suppressed Due to Excessive Length 13 w∗(ˆc) under a ﬁxed cost ˆc, where the expectation can be regarded as a combi- nation of proportions of diﬀerent feasible solutions. Thus, unlike the piecewise constant function w∗(ˆc), Eξ[w∗(ˆc + σξ)] varies the proportions in response to the change of ˆc, providing a nonzero gradient of ˆc: ∂ Eξ[w∗(ˆc + σξ)] ∂ ˆc ≈ 1 K K∑ κ w∗(ˆc + σξκ)ξκ. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: ˆc, K, σ 1: for sample κ ∈ {1, ..., K} do 2: Generate Gaussian noise ξκ 3: Solve: wξ κ := w∗(ˆc + σξκ) 4: Save wξ κ and ξκ for backward pass 5: end for 6: return 1 K ∑K κ wξ κ Algorithm 5 DPO Backward Pass Require: ∂l(·) ∂ Eξ[w∗] , K 1: Load wξ κ and ξκ from forward pass 2: Compute ∂ Eξ[w∗] ∂ ˆc := 1 K ∑K κ wξ κξκ 3: Compute l(·) ∂ ˆc := ∂l(·) ∂ Eξ[w∗] ∂ Eξ[w∗] ∂ ˆc 4: return l(·) ∂ ˆc 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for DPO , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(ˆc, w∗(c)) and gradient ∂lFY(ˆc,w∗(c)) ∂ ˆc . Compared to DPO , PFYL avoids the ineﬃcient calcula- tion of the Jacobian matrix ∇T w∗(ˆc) and includes a theoretically sound loss function. The loss of PFYL is based on Fenchel duality: The expectation of the per- turbed minimizer is deﬁned as F (c) = Eξ[min w∈S{(c + σξ) ⊺w}], and the dual of F (c), denoted by Ω(w∗(c)), is utilized to deﬁne the Fenchel-Young loss: lFY(ˆc, w∗(c)) = ˆc⊺w∗(c) − F (ˆc) − Ω(w∗(c)), then the gradient of the loss is ∂lFY(ˆc, w∗(c)) ∂ ˆc = w∗(c) − ∂F (ˆc) ∂ ˆc = w∗(c) − E ξ [argmin w∈S {(ˆc + σξ) ⊺w}]. Similar to DPO , we can estimate the well-deﬁned gradient as ∂lFY(ˆc, w∗(c)) ∂ ˆc ≈ w∗(c) − 1 K K∑ κ argmin w∈S {(ˆc + σξκ) ⊺w} 14 Bo Tang, Elias B. Khalil 4 Implementation and Modeling The core module of PyEPO is an “autograd” function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero gradients so that the prediction model can learn from the decision error or its surrogates. Thus, our implementation extends PyTorch, which facilitates the deployment of end-to-end predict-then-optimize tasks using any neural network that can be implemented in PyTorch. We choose GurobiPy [22] and Pyomo [24] to build optimization models. Both GurobiPy and Pyomo are algebraic modeling languages (AMLs) written in Python. GurobiPy is a Python interface to Gurobi, which combines the expressiveness of a modeling language with the ﬂexibility of a programming language. As an oﬃcial interface of Gurobi, GurobiPy has a simple algebraic syntax and natively supports all features of Gurobi. Considering that users may not have a Gurobi license, we have additionally designed a Pyomo in- terface as an alternative. As an open-source optimization modeling language, Pyomo supports a variety of solvers, including Gurobi and GLPK. Both of the above provide a natural way to express mathematical programming models. Users without specialized optimization knowledge can easily build and main- tain optimization models through high-level algebraic representations. Besides GurobiPy and Pyomo, PyEPO also allows users to construct optimization models from scratch using any algorithm and solver. As a result, fast and ﬂex- ible model customization for research and production is possible in PyEPO. 1 2 4 8 16 Num of Cores 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50RuntimeperEpoch(Sec) Shortest Path SPO+ DBB 1 2 4 8 16 Num of Cores 2 4 6 8 10 12 14RuntimeperEpoch(Sec) Traveling Salesman SPO+ DBB Fig. 4: Parallel eﬃciency: Although there is additional overhead in creating a new process, parallel computing of SPO+ and DBB with an appropriate number of processors can reduce the training time eﬀectively. In addition, PyEPO supports parallel computing. For SPO+ , DBB , DPO , and PFYL , the computational cost is a major challenge that cannot be ignored, par- ticularly for integer programs. Both require solving an optimization problem per instance to obtain the gradient. Title Suppressed Due to Excessive Length 15 Figure 4 shows the average running time per epoch for a mini-batch gra- dient descent algorithm with a batch size of 32 as a function of the number of cores. The decrease in running time per epoch is sublinear in the number of cores. This may be explained by the overhead associated with starting up addi- tional cores, which might dominate computation costs. For example, in Figure 4, for the shortest path, the easily solvable polynomial problem, the running time actually increases when the number of cores exceeds 4, while the more complicated N P-complete problem, TSP, continues to beneﬁt slightly from additional cores. Overall, we believe this feature to be crucial for large-scale predict-then-optimize tasks. 4.1 Optimization Model The ﬁrst step in using PyEPO is to create an optimization model that in- herits from the optModel class. Since PyEPO tackles predict-then-optimize with unknown cost coeﬃcients, it is ﬁrst necessary to instantiate an opti- mization model, optModel, with ﬁxed constraints and variable costs. Such an optimization model would accept diﬀerent cost vectors and be able to ﬁnd the corresponding optimal solutions with a ﬁxed set of constraints. The construc- tion of optModel is separated from the autograd functions (see Section 4.2). An instance of optModel will be passed as an argument into the autograd functions. 4.1.1 Optimization Model from Scratch In PyEPO, the optModel works as a black-box, which means that we do not speciﬁcally require a certain algorithm or a certain solver. This design is intended to give users more freedom to customize their tasks. To build an optModel from scratch, users need to override abstract methods getModel to build a model and get its variables, setObj to set the objective function with a given cost vector, and solve to ﬁnd an optimal solution. In addition, optModel provides an attribute modelSense to indicate whether the problem is one of minimization or maximization. The following shortest path example uses the Python library N etworkX [23] and its built-in Dijkstra’s algorithm: 1 import networkx as nx 2 from pyepo import EPO 3 from pyepo . model . opt import optModel 4 5 class m y S h or t e s t P a t h Mo d e l ( optModel ) : 6 7 def __init__ ( self ) : 8 self . modelSense = EPO . MINIMIZE 9 self . grid = (5 ,5) # graph size 10 self . arcs = self . _getArcs () # list of arcs 11 super () . __init__ () 12 13 def _getModel ( self ) : 16 Bo Tang, Elias B. Khalil 14 \"\"\" 15 A method to build model 16 \"\"\" 17 # build graph as model 18 model = nx . Graph () 19 # add arcs as variables 20 model . add_edges_from ( self . arcs , cost =0) 21 var = model . edges 22 return model , var 23 24 def setObj ( self , c ) : 25 \"\"\" 26 A method to set objective function 27 \"\"\" 28 for i , e in enumerate ( self . arcs ) : 29 self . _model . edges [ e ][ \" cost \" ] = c [ i ] 30 31 def solve ( self ) : 32 \"\"\" 33 A method to solve model 34 \"\"\" 35 # dijkstra 36 s = 0 # source node 37 t = self . grid [0] * self . grid [1] - 1 # target node 38 path = nx . shortest_path ( self . _model , weight = \" cost \" , 39 source =s , target = t ) 40 # convert path into active edges 41 edges = [] 42 u = 0 43 for v in path [1:]: 44 edges . append (( u , v ) ) 45 u = v 46 # init sol & obj 47 sol = [0] * self . num_cost 48 obj = 0 49 # convert active edges into solution and obj 50 for i , e in enumerate ( self . arcs ) : 51 if e in edges : 52 # active edge 53 sol [ i ] = 1 54 # cost of active edge 55 obj += self . _model . edges [ e ][ \" cost \" ] 56 return sol , obj 57 58 def _getArcs ( self ) : 59 \"\"\" 60 A helper method to get list of arcs for grid network 61 \"\"\" 62 arcs = [] 63 h , w = self . grid 64 for i in range ( h ) : 65 # edges on rows 66 for j in range ( w - 1) : 67 v = i * w + j 68 arcs . append (( v , v + 1) ) 69 # edges in columns 70 if i == h - 1: 71 continue Title Suppressed Due to Excessive Length 17 72 for j in range ( w ) : 73 v = i * w + j 74 arcs . append (( v , v + w ) ) 75 return arcs 76 77 optmodel = m y S h o r t e s tP a t h M o d e l () 4.1.2 Optimization Model with Gurobi On the other hand, we provide optGrbModel to create an optimization model with GurobiPy. Unlike optModel, optGrbModel is more lightweight but less ﬂexible for users. Let us use the following optimization model (7) as an exam- ple, where ci is an unknown cost coeﬃcient: max x 4∑ i=0 cixi s.t. 3x0 + 4x1 + 3x2 + 6x3 + 4x4 ≤ 12 4x0 + 5x1 + 2x2 + 3x3 + 5x4 ≤ 10 5x0 + 4x1 + 6x2 + 2x3 + 3x4 ≤ 15 ∀xi ∈ {0, 1} (7) Inheriting optGrbModel is the convenient way to use Gurobi with PyEPO. The only implementation required is to override getModel and return a Gurobi model and the corresponding decision variables. In addition, there is no need to assign a value to the attribute modelSense in optGrbModel manually. An example for Model (7) is as follows: 1 import gurobipy as gp 2 from gurobipy import GRB 3 from pyepo . model . grb import optGrbModel 4 5 class myModel ( optGrbModel ) : 6 def _getModel ( self ) : 7 # ceate a model 8 m = gp . Model () 9 # varibles 10 x = m . addVars (5 , name = \" x \" , vtype = GRB . BINARY ) 11 # sense ( must be minimize ) 12 m . modelSense = GRB . MAXIMIZE 13 # constraints 14 m . addConstr (3* x [0]+4* x [1]+3* x [2]+6* x [3]+4* x [4] <=12) 15 m . addConstr (4* x [0]+5* x [1]+2* x [2]+3* x [3]+5* x [4] <=10) 16 m . addConstr (5* x [0]+4* x [1]+6* x [2]+2* x [3]+3* x [4] <=15) 17 return m , x 18 19 optmodel = myModel () 18 Bo Tang, Elias B. Khalil 4.1.3 Optimization Model with Pyomo Similarly, optOmoModel allows modeling mathematical programs with Py- omo. In contrast to optGrbModel, optOmoModel requires an explicit object attribute modelSense. Since Pyomo supports multiple solvers, instantiating an optOmoModel requires a parameter solver to specify the solver. The fol- lowing is an implementation of problem 7: 1 from pyomo import environ as pe 2 from pyepo import EPO 3 from pyepo . model . omo import optOmoModel 4 5 class myModel ( optOmoModel ) : 6 def _getModel ( self ) : 7 # sense 8 self . modelSense = EPO . MAXIMIZE 9 # ceate a model 10 m = pe . ConcreteModel () 11 # varibles 12 x = pe . Var ([0 ,1 ,2 ,3 ,4] , domain = pe . Binary ) 13 m . x = x 14 # constraints 15 m . cons = pe . ConstraintList () 16 m . cons . add (3* x [0]+4* x [1]+3* x [2]+6* x [3]+4* x [4] <=12) 17 m . cons . add (4* x [0]+5* x [1]+2* x [2]+3* x [3]+5* x [4] <=10) 18 m . cons . add (5* x [0]+4* x [1]+6* x [2]+2* x [3]+3* x [4] <=15) 19 return m , x 20 21 optmodel = myModel ( solver = \" glpk \" ) 4.2 Autograd Functions Training neural networks with modern deep learning libraries such as Ten- sorFlow [1] or PyTorch [33] requires gradient calculations for backpropaga- tion. For this purpose, the numerical technique of automatic diﬀerentiation [32] is used. For example, PyTorch provides autograd functions, so that users are allowed to utilize or create functions that automatically compute partial derivatives. Autograd functions are the core modules of PyEPO that solve and back- propagate the optimization problems with predicted costs. These functions can be integrated with diﬀerent neural network architectures to achieve end-to-end predict-then-optimize for various tasks. In PyEPO, autograd functions include SPOPlus [16], blackboxOpt [35], perturbedOpt [6], and perturbedFenchelYoung [6]. 4.2.1 Function SPOPlus The function SPOPlus directly calculates SPO+ loss, which measures the de- cision error of an optimization solve with predicted cost coeﬃcients. This optimization is represented as an instance of optModel and passed into the Title Suppressed Due to Excessive Length 19 SPOPlus as an argument. As shown below, SPOPlus also requires processes to specify the number of processes. 1 from pyepo . func import SPOPlus 2 # init SPO + Pytorch function 3 spop = SPOPlus ( optmodel , processes =8) The parameters for the forward pass of SPOPlus are as follows: – pred cost: a batch of predicted cost vectors, one vector per instance; – true cost: a batch of true cost vectors, one vector per instance; – true sol: a batch of true optimal solutions, one vector per instance; – true obj: a batch of true optimal objective values, one value per instance. The following code block is the SPOPlus forward pass: 1 # calculate SPO + loss 2 loss = spop ( pred_cost , true_cost , true_sol , true_obj ) 4.2.2 Function blackboxOpt SPOPlus directly obtains a loss while blackboxOpt provides a solution. Thus, blackboxOpt makes it possible to use various loss functions. Compared to SPOPlus, blackboxOpt requires an additional parameter lambd, which is the interpolation hyperparameter λ for the diﬀerentiable black-box solver. Accord- ing to Poganˇci´c et al. [35], the values of λ should be between 10 and 20. 1 from pyepo . func import blackboxOpt 2 # init DBB solver 3 dbb = blackboxOpt ( optmodel , lambd =10 , processes =8) Since blackboxOpt works as a diﬀerentiable optimizer, there is only one pa- rameter pred cost for the forward pass. As in the code below, the output is the optimal solution for the given predicted cost: 1 # find optimal solution 2 pred_sol = dbb ( pred_cost ) 4.2.3 Function perturbedOpt Same as blackboxOpt, perturbedOpt is a diﬀerentiable optimizer and provides an “expected solution” for an arbitrary loss function. The hyperparameters for perturbedOpt include n sample and sigma as the number of Monte-Carlo sample K and the amplitude parameter of the perturbation σ, respectively. 1 import pyepo 2 # init DPO solver 3 dpo = pyepo . func . perturbedOpt ( optmodel , n_samples =3 , sigma =1.0 , processes =8) Given predicted cost ˆc, perturbedOpt outputs an expected solution by aver- aging the solutions of n sample randomly perturbed optimization problems: 1 # find the expected optimal solution 2 exp_sol = dpo ( pred_cost ) 20 Bo Tang, Elias B. Khalil 4.2.4 Function perturbedFenchelYoung perturbedFenchelYoung uses a cost prediction ˆc and a true solution w∗(c) to compute the Perturbed Fenchel-Young loss lFY(ˆc, w∗(c)); it requires the same hyperparameters as perturbedOpt. 1 import pyepo 2 # init Fenchel - Young loss 3 pfyl = pyepo . func . p e r t u r b e d F e n c h e l Y o u n g ( optmodel , n_samples =3 , sigma =1.0 , processes =8) The below code block illustrates the calculation of Fenchel-Young loss: 1 # calculate Fenchel - Young loss 2 loss = pfyl ( pred_cost , true_sol ) 4.3 The optDataset Class for Managing Data The utilization of decision losses, such as SPO+ and PFYL , necessitates the availability of true optimal solutions. Therefore, to facilitate convenience in PyEPO training and testing, an auxiliary optDataset has been introduced, which is not strictly indispensable. optDataset stores the features and their associated costs of the objective function and solves optimization problems to get optimal solutions and corresponding objective values. optDataset is extended from PyTorch Dataset. In order to obtain optimal solutions, optDataset requires the corresponding optModel (see in Section 4.1). The parameters for the optDataset are as follows: – model: an instance of optModel; – feats: data features; – costs: corresponding costs of objective function; Then, as the following example, PyTorch DataLoader receives an optDataset and wraps the data samples and acts as a sampler that provides an iterable over the given dataset. It is required to provide the batch size which is the number of training samples that will be used in each update of the model parameters. 1 import pyepo 2 from torch . utils . data import DataLoader 3 4 # build dataset 5 dataset = pyepo . data . dataset . optDataset ( optmodel , feats , costs ) 6 7 # get data loader 8 dataloader = DataLoader ( dataset , batch_size =32 , shuffle = True ) By iterating over the DataLoader, we can obtain a batch of features, true costs, optimal solutions, and corresponding objective values: Title Suppressed Due to Excessive Length 21 1 for x , c , w , z in dataloader : 2 # a batch of features 3 print ( x ) 4 # a batch of true costs 5 print ( c ) 6 # a batch of true optimal solutions 7 print ( w ) 8 # a batch of true optimal objective values 9 print ( z ) 4.4 End-to-End Training The core capability of PyEPO is to build an optimization model, and then embed the optimization model into a PyTorch neural network for the end-to- end training. Here, we build a simple linear regression model in PyTorch as an example: 1 from torch import nn 2 3 # construct linear model 4 class LinearRegression ( nn . Module ) : 5 def __init__ ( self ) : 6 super ( LinearRegression , self ) . __init__ () 7 # size of input and output is the size of feature and cost 8 self . linear = nn . Linear ( num_feat , len_cost ) 9 def forward ( self , x ) : 10 out = self . linear ( x ) 11 return out 12 13 # init model 14 predmodel = LinearRegression () Then, we can train the prediction model with SPO+ loss to predict un- known cost coeﬃcients, make decisions, and compute decision errors. The training of the prediction model is performed using a stochastic gradient de- scent (SGD) optimizer. By utilizing PyTorch automatic diﬀerentiation capa- bilities, the gradients of the loss with respect to the model parameters are computed and used to update the model parameters during training. 1 import torch 2 3 # set SGD optimizer 4 optimizer = torch . optim . SGD ( predmodel . parameters () , lr =1 e -3) 5 6 # training 7 for epoch in range ( num_epochs ) : 8 # iterare features , costs , solutions , and objective values 9 for x , c , w , z in dataloader : 10 # forward pass 11 cp = predmodel ( x ) # predict costs 12 loss = spop ( cp , c , w , z ) . mean () # calculate SPO + loss 13 # backward pass 14 optimizer . zero_grad () # reset gradients to 0 15 loss . backward () # compute gradients 16 optimizer . step () # update model parameters 22 Bo Tang, Elias B. Khalil 4.5 Metrics PyEPO provides evaluation functions to measure model performance, in par- ticular the two metrics mentioned in Section 3.3.1: regret and unambiguous regret. We further deﬁne the normalized (unambiguous) regret by ∑ntest i=1 lRegret(ˆci, ci) ∑ntest i=1 |z∗(ci)| . Both of them require the following parameters: – predmodel: a regression neural network for cost prediction. – optModel: a PyEPO optimization model. – dataloader: a PyEPO data loader. Assume that we have trained a prediction model predmodel for an op- timization problem optModel. To evaluate predmodel’S performance on a dataset dataloader of optModel instances, the following suﬃces: 1 from pyepo . metric import regret , unambRegret 2 # compute normalized regret 3 regret = regret ( predmodel , optmodel , dataloader ) 4 # compute normalized unambiguous regret 5 uregret = unambRegret ( predmodel , optmodel , dataloader ) 5 Benchmark Datasets 5.1 Benchmark Datasets from PyEPO In this section, we describe our new datasets designed for the task of end- to-end predict-then-optimize. Overall, we generate datasets in a similar way to Elmachtoub and Grigas [16]. The synthetic dataset D includes features x and cost coeﬃcients c: D = {(x1, c1), (x2, c2), ..., (xn, cn)}. The feature vector xi ∈ Rp follows a standard multivariate Gaussian distribution N (0, Ip) and the corresponding cost vector ci ∈ Rd comes from a (possibly nonlinear) polynomial function of xi with additional random noise. ϵij ∼ U (1 − ¯ϵ, 1 + ¯ϵ) is the multiplicative noise term for cij, the jth element of cost ci. Our dataset includes three of the most classical optimization problems: the shortest path problem, the multi-dimensional knapsack problem, and the traveling salesperson problem. PyEPO provides functions to generate these data with adjustable data size n, number of features p, cost vector dimension d, polynomial degree deg, and noise half-width ¯ϵ. 5.1.1 Shortest Path Following Elmachtoub and Grigas [16], we consider a h × w grid network and the goal is to ﬁnd the shortest path [31] from northwest to southeast. We generate a random matrix B ∈ Rd×p, where Bij follows Bernoulli distribution Title Suppressed Due to Excessive Length 23 with probability 0.5. Then, the cost vector ci is almost the same as in [16], and is generated from [ 1 3.5 deg ( 1 √p (Bxi)j + 3 )deg + 1 ] · ϵij. (8) The following code generates data for the shortest path on the grid network: 1 from pyepo . data . shortestpath import genData 2 x , c = genData (n , p , grid =( h , w ) , deg = deg , noise_width = e ) 5.1.2 Multi-Dimensional Knapsack The multi-dimensional knapsack problem [28] is one of the most well-known integer programming models. It maximizes the value of selected items under multiple resource constraints. Due to its computational complexity, solving this problem can be challenging, especially with the increase in the number of constraints (or resources or knapsacks). Because we assume that the uncertain coeﬃcients exist only in the objective function, the weights of items are ﬁxed throughout the data. We use k to denote the number of resources; the number of items is the same as the dimension of the cost vector d. The weights W ∈ Rk×m are sampled from 3 to 8 with a precision of 1 decimal place. With the same B ∈ Rd×p as in Section 5.1.1, cost cij is calculated according to Equation (8). To generate k-dimensional knapsack data, a user simply executes the fol- lowing: 1 from pyepo . data . knapsack import genData 2 W , x , c = genData (n , p , num_item =d , dim = dim , deg = deg , noise_width = e ) 5.1.3 Traveling Salesperson As one of the most famous combinatorial optimization problems, the traveling salesperson problem (TSP) aims to ﬁnd the shortest possible tour that visits every node exactly once. Here, we introduce the symmetric TSP with the number of nodes to be visited v. PyEPO generates costs from a distance matrix. The distance is the sum of two parts: one comes from Euclidean distance, and the other is derived from a polynomial function of the features. For Euclidean distance, we create coordinates from the mixture of Gaussian distribution N (0, I) and uniform distribution U (−2, 2). For the function of the features, the polynomial kernel function is 1 3deg−1 ( 1√p (Bxi)j + 3)deg · ϵij, where the elements of B come from the multiplication of Bernoulli B(0.5) and uniform U (−2, 2). An example of a TSP data generation is as follows: 1 from pyepo . data . tsp import genData 2 x , c = genData (n , p , num_node =v , deg = deg , noise_width = e ) 24 Bo Tang, Elias B. Khalil 5.2 Warcraft Terrain Map Images Poganˇci´c et al. [35] proposed a dataset of maps from the popular video game Warcraft (refer to Figure 5), which allows learning the shortest path from RGB terrain images. It is a remarkable benchmark due to its image inputs, a modality that was not explored in other work in this area. In accordance with the experiment of Poganˇci´c et al. [35] and Berthet et al. [6], we utilize 96 × 96 RGB images for the shortest path on 12 × 12 grid networks. Feature 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 CostSolution Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k × k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diﬀerent predictors and SPO+ /DBB /PFYL with a linear prediction model g(x; θ). Notably, DPO was not shown due to its overall subpar performance. Unlike direct decision loss functions SPO+ and PFYL , DBB and DPO allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for diﬀerent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diﬀerence between solutions. However, in our experiments, compared to the regret, DBB using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for DBB . Title Suppressed Due to Excessive Length 25 Method Description 2-stage LR Two-stage method where the predictor is a linear regression 2-stage RF Two-stage method where the predictor is a random forest with default parameters 2-stage Auto Two-stage method where the predictor is Auto-Sklearn [18] with 10 minutes time limit and uses MSE as metric SPO+ Linear model with SPO+ loss [16] PFYL Linear model with perturbed Fenchel-Young loss [6] DBB Linear model with diﬀerentiable black-box optimizer [35] SPO+ Rel Linear model with SPO+ loss [16], using linear relaxation for training PFYL Rel Linear model with perturbed Fenchel-Young loss [6], using linear relaxation for training DBB Rel Linear model with diﬀerentiable black-box optimizer [35], using linear relaxation for training SPO+ L1 Linear model with SPO+ loss [16], using l1 regularization for cost SPO+ L2 Linear model with SPO+ loss [16], using l2 regularization for cost PFYL L1 Linear model with perturbed Fenchel-Young loss [6], using l1 regularization for cost PFYL L2 Linear model with perturbed Fenchel-Young loss [6], using l2 regularization for cost DBB L1 Linear model with diﬀerentiable black-box optimizer [35], using l1 regularization for cost DBB L2 Linear model with diﬀerentiable black-box optimizer [35], using l2 regularization for cost Table 3: Methods compared in the experiments. All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diﬀerent Methods We compare the performance between two-stage methods, SPO+ , PFYL , and DBB with varying training data size n ∈ {100, 1000, 5000}, polynomial de- gree deg ∈ {1, 2, 4, 6}, and noise half-width ¯ϵ ∈ {0.0, 0.5}. We then conduct small-scale experiments on the validation set to select gradient descent hy- perparameters, namely the batch size, learning rate, and momentum for the shortest path, knapsack, and TSP in SPO+ , PFYL , and DBB . The hyperpa- rameter tuning uses a limited random search in the space of hyperparameter conﬁgurations. Thus, there is no guarantee of the best performance in the results. For PFYL , we arbitrarily set the number of samples K = 1 and the perturbation amplitude σ = 1.0. We repeated all experiments 10 times, each with a diﬀerent x, B, and ϵ to generate 10 diﬀerent training/validation/test datasets. We use boxplots to summarize the statistical outcomes. Problem Parameters Feature Size Cost Dimension Shortest Path Height of the grid is 5 Width of the grid is 5 5 40 Knapsack Dimension of resource is 2 Number of items is 32 Capacity is 20 5 32 Traveling Salesperson Number of nodes is 20 10 190 Table 4: Problem Parameters for Performance Comparison 26 Bo Tang, Elias B. Khalil We generate synthetic datasets with parameters in Table 4, so the dimen- sions of the cost vectors d are 40, 32, and 190, respectively. For the TSP, we use the Dantzig–Fulkerson–Johnson (DFJ) formulation [11] because it is faster to solve than alternatives. 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegret Shortest Path Training Set Size = 100, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegret Shortest Path Training Set Size = 100, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegret Shortest Path Training Set Size = 1000, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegret Shortest Path Training Set Size = 1000, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegret Shortest Path Training Set Size = 5000, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegret Shortest Path Training Set Size = 5000, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 6: Normalized regret for the shortest path problem on the test set: The size of the grid network is 5 × 5. The methods in the experiment include two-stage approaches with linear regression, random forest, and Auto-Sklearn and end-to-end learning such as SPO+ , PFYL , and DBB . The normalized regret is visualized under diﬀerent sample sizes, noise half-width, and polynomial degrees. For normalized regret, lower is better. Figures 6, 7, and 8 summarize the performance comparison for the short- est path problem, 2D knapsack problem, and traveling salesperson problem. These ﬁgures should be read as follows: the left column is for noise-free costs (easier), while the right column includes noise. Each row of ﬁgures is for a training set size in increasing order. Within each ﬁgure and from left to right, the degree of the polynomial that generates the costs from the feature vector increases. Within each such polynomial degree, the diﬀerent methods’ box- plots are shown, summarizing the test set normalized regret results for the 10 diﬀerent experiments; lower is better. Title Suppressed Due to Excessive Length 27 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 100, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 100, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 1000, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 1000, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 5000, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 5000, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 7: Normalized regret for the 2D knapsack problem on the test set: There are 32 items, and the capacity of the two resources is 20. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learning such as SPO+ , PFYL , and DBB . The normalized regret is visualized under diﬀerent sample sizes, noise half-width, and polynomial degrees. For normalized regret, lower is better. Two-stage linear regression (2-stage LR) performs well at lower polynomial degrees but loses its advantage at higher polynomial degrees. The two-stage random forest (2-stage RF) is robust at high polynomial degrees but requires a large amount of training data. With 5000 data samples, random forest achieves the best performance in many cases. The two-stage method with automated hyperparameter tuning using the Auto-Sklearn tool [18] (which will be dis- cussed further in the next section, Section 6.2) is attractive: despite tuning for lower prediction error (not decision error), Auto-Sklearn eﬀectively reduces the regret so that it usually performs better than two-stage linear regression and random forest. However, with the increase of input and output dimension, Auto-Sklearn fails to be competitive for the TSP (shown in Figure 8). SPO+ and PFYL show their advantage: they perform best, or at least rel- atively well, in all cases. SPO+ and PFYL are comparable to linear regression under low polynomial degrees and depend less on the sample size than random forest. At high polynomial degrees, SPO+ and PFYL outperform Auto-Sklearn, 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 100, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 100, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 5000, Noise Half−width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 5000, Noise Half−width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as SPO+ , PFYL , and DBB . The normalized regret is visualized under diﬀerent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. which exposes the limitations of the two-stage approach. Compared to the SPO+ , the PFYL method provides an additional beneﬁt in that it does not necessitate the presence of true costs c within the training dataset. Finding #1 SPO+ and PFYL can robustly achieve relatively good decisions under diﬀerent scenarios, often outperforming two-stage baselines. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of “2-stage Auto” is the mean squared error of Title Suppressed Due to Excessive Length 29 the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ﬁve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. Finding #2 Even with successful model selection and hyperparameter tuning, the two-stage method performs worse than SPO+ in terms of decision qual- ity, which substantiates the value of the end-to-end approach. 6.3 Exact Method and Relaxation Training SPO+ , PFYL , and DBB with a linear relaxation instead of solving the integer program improves computational eﬃciency. However, the use of a “weaker” solver theoretically undermines model performance. Therefore, an important question arises about the tradeoﬀ when using a linear relaxation in training. To this end, we compare the performance of end-to-end approaches with their relaxation using 2D knapsack and TSP as examples. We use the same instances, models, and hyperparameters as before. SPO+ SPO+ Rel PFYL PFYL Rel DBB DBB Rel Method 0.00 0.05 0.10 0.15 0.20 0.25 0.30RuntimeperIter(Sec)2D Knapsack SPO+ (DFJ) SPO+ Rel (GG) SPO+ Rel (MTZ) PFYL (DFJ) PFYL Rel (GG) PFYL Rel (MTZ) DBB (DFJ) DBB Rel (GG) DBB Rel (MTZ) Method 0.0 0.2 0.4 0.6 0.8 1.0 1.2RuntimeperIter(Sec)TSP Fig. 9: Average training time per iteration for exact and relaxation methods with standard deviation error bars: We visualized the mean training time with standard deviation for the Knapsack and TSP; lower is better. There are several integer programming formulations for the TSP. Besides DFJ, we also implemented the Miller-Tucker-Zemlin (MTZ) formulation [30] and the Gavish-Graves (GG) formulation [20]. Although they all have the same integer solution, they diﬀer in their linear relaxations. Since DFJ requires column generation to handle the exponential subtour constraints, its linear relaxation is hard to implement. The GG formulation is shown to have a tighter linear relaxation than MTZ. Thus, we use DFJ for exact SPO+ , PFYL , 30 Bo Tang, Elias B. Khalil and DBB , and MTZ and GG for the relaxation to investigate the eﬀect of solution quality on regret. 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 100, Noise Half−width = 0.5 SPO+ SPO+ Rel PFYL PFYL Rel DBB DBB Rel 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegret 2D Knapsack Training Set Size = 1000, Noise Half−width = 0.5 SPO+ SPO+ Rel PFYL PFYL Rel DBB DBB Rel 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 100, Noise Half−width = 0.5 SPO+ (DFJ) SPO+ Rel(GG) SPO+ Rel(MTZ) PFYL (DFJ) PFYL Rel(GG) PFYL Rel(MTZ) DBB (DFJ) DBB Rel(GG) DBB Rel(MTZ) 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half−width = 0.5 SPO+ (DFJ) SPO+ Rel(GG) SPO+ Rel(MTZ) PFYL (DFJ) PFYL Rel(GG) PFYL Rel(MTZ) DBB (DFJ) DBB Rel(GG) DBB Rel(MTZ) Fig. 10: Normalized regret for the 2D knapsack (at the top) and TSP (at the bottom) on the test set: The methods in the experiment include SPO+ , PFYL and DBB w/o relaxation. Then, we visualize the normalized regret under diﬀerent sample sizes and polynomial degrees to investigate the impact of the relaxation method. For normalized regret, lower is better. According to Figure 9, using a linear relaxation signiﬁcantly reduces the running time. As shown in Figure 10, the impact on the performance on knap- sack is almost negligible. Interestingly, relaxed methods have the potential to perform better on small data, perhaps because the linear relaxation acts as a regularization to avoid overﬁtting. For TSP, Figure 10 demonstrates that a tighter bound does reduce the regret, and DBB Rel shows advantages over DBB . Overall, using relaxations achieves fairly good performance with improved computational eﬃciency. Moreover, formulations with tighter linear relaxation lead to better performance. Finding #3 End-to-end predict-then-optimize with relaxation has excellent poten- tial to improve computation eﬃciency at a slight degradation in perfor- mance, particularly when the true cost-generating function is not very non-linear. Title Suppressed Due to Excessive Length 31 6.4 Prediction Regularization As proposed in Elmachtoub and Grigas [16], the mean absolute error lMAE(ˆc, c) = 1 n ∑n i ∥ˆci − ci∥1 or mean squared error lMSE(ˆc, c) = 1 2n ∑n i ∥ˆci − ci∥ 2 2 of the predicted cost vector w.r.t. true cost vector can be added to the decision loss as l1 or l2 regularizers. When using regularization, we set either the l1 regular- ization parameter φ1 and the l2 regularization parameter φ2 from 0.001 to 10 logarithmically. For the experiments, we still use the same instances, model, and hyperparameters as before, while the number of training samples n, the noise half-width ¯ϵ and, the polynomial degree deg are ﬁxed at 1000, 0.5 and 4. 0.0 0.001 0.01 0.1 1.0 10.0 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegretSPO+ PFYL DBB 2-Stage LR 0.0 0.001 0.01 0.1 1.0 10.0 0 5 10 15 20MSE SPO+ PFYL DBB 2-Stage LR L1 Parameter Test Loss on Shortest Path with L1 Regularization Training Set Size = 1000, Polynomial Degree = 4, Noise Half−width = 0.5 0.0 0.001 0.01 0.1 1.0 10.0 0.00 0.05 0.10 0.15 0.20 0.25NormalizedRegretSPO+ PFYL DBB 2-Stage LR 0.0 0.001 0.01 0.1 1.0 10.0 0 5 10 15 20MSE SPO+ PFYL DBB 2-Stage LR L2 Parameter Test Loss on Shortest Path with L2 Regularization Training Set Size = 1000, Polynomial Degree = 4, Noise Half−width = 0.5 0.0 0.001 0.01 0.1 1.0 10.0 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegretSPO+ PFYL DBB 2-Stage LR 0.0 0.001 0.01 0.1 1.0 10.0 0 10 20 30 40 50MSE SPO+ PFYL DBB 2-Stage LR L1 Parameter Test Loss on 2D Knapsack with L1 Regularization Training Set Size = 1000, Polynomial Degree = 4, Noise Half−width = 0.5 L1 Parameter 0.0 0.001 0.01 0.1 1.0 10.0 0.00 0.05 0.10 0.15 0.20 0.25 0.30NormalizedRegretSPO+ PFYL DBB 2-Stage LR 0.0 0.001 0.01 0.1 1.0 10.0 0 10 20 30 40 50MSE SPO+ PFYL DBB 2-Stage LR L2 Parameter Test Loss on 2D Knapsack with L2 Regularization Training Set Size = 1000, Polynomial Degree = 4, Noise Half−width = 0.5 L2 Parameter Fig. 11: Normalized regret and MSE on the test set: In these experiments, we compare the performance on SPO+ , PFYL , and DBB with diﬀerent levels of l1 or l2 regularization and show the trend line with increasing regularization, while 2-Stage LR (green line) is served as the baseline. The normalized regret and MSE under diﬀerent problems are shown; lower is better. Based on Fig 11, regularization appears to be a viable strategy for eﬀec- tively reducing MSE. With the increase of the regularization parameter, all end-to-end methods achieve an MSE which is close to that of the two-stage linear regression approach. In terms of regret, the impact on SPO+ and PFYL is insigniﬁcant, while more regularization for DBB has the potential to yield improvements. Finding #4 l1 and l2 regularization are deemed to be appealing techniques for en- hancing prediction accuracy while simultaneously preserving decision quality. 32 Bo Tang, Elias B. Khalil 6.5 Trade-oﬀs between MSE and Regret Figure 12 examines the prediction-decision trade-oﬀ on the shortest path with 100 and 1000 training samples, a 0.5 noise half-width, and polynomial degree 4. We calculate the average MSE and regret for 10 repeated random experiments. Apart from this, mean training time is also annotated, and circle sizes are proportional to it. Finally, we remove the circles of DBB because they are at the right top corner and far worse than other methods. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 MSE 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15NormalizedRegret Shortest Path Training Set Size = 100, Polynomial degree = 4, Noise Half−width = 0.5 0 5 10 15 20 25 30 35 MSE 0.08 0.10 0.12 0.14 0.16 0.18NormalizedRegret 2D Knapsack Training Set Size = 100, Polynomial degree = 4, Noise Half−width = 0.5 0 20 40 60 80 100 MSE 0.050 0.075 0.100 0.125 0.150 0.175 0.200NormalizedRegret TSP Training Set Size = 100, Polynomial degree = 4, Noise Half−width = 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 MSE 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15NormalizedRegret Shortest Path Training Set Size = 1000, Polynomial degree = 4, Noise Half−width = 0.5 0 5 10 15 20 25 30 35 MSE 0.08 0.10 0.12 0.14 0.16 0.18NormalizedRegret 2D Knapsack Training Set Size = 1000, Polynomial degree = 4, Noise Half−width = 0.5 0 20 40 60 80 100 MSE 0.050 0.075 0.100 0.125 0.150 0.175 0.200NormalizedRegret TSP Training Set Size = 1000, Polynomial degree = 4, Noise Half−width = 0.5 Fig. 12: MSE v.s. Regret: The result covers diﬀerent two-stage methods, SPO+ , PFYL , and their relaxations. DBB is omitted because it is far away from others. The size of the circles is proportional to the training time (Sec), so the smaller is better. Figure 12 demonstrates that SPO+ and PFYL can reach a low decision error, especially with large training sets, at the cost of higher prediction errors. Moreover, the MSE of PFYL ’s predictions is signiﬁcantly larger than that of SPO+ . Further examination reveals that the higher prediction error of SPO+ and PFYL comes mainly from multiplicative shifts in the predicted cost values, which does not alter the optima of an optimization problem with a linear objective function. In addition, training with Auto-Sklearn, which comes from Title Suppressed Due to Excessive Length 33 automated algorithm selection and hyperparameter tuning, is time-consuming but provides both high-quality prediction error and decision error. However, compared to SPO+ and PFYL , even the competitive 2-stage Auto model does not have an advantage in decision-making with 1000 training data samples. 0 5 10 15 20 25 30 35 MSE 0.08 0.10 0.12 0.14 0.16 0.18NormalizedRegret 2D Knapsack Training Set Size = 1000, Polynomial degree = 4, Noise Half−width = 0.5 0 20 40 60 80 MSE 0.050 0.075 0.100 0.125 0.150 0.175 0.200NormalizedRegret TSP Training Set Size = 1000, Polynomial degree = 4, Noise Half−width = 0.5 Fig. 13: MSE v.s. Regret: The result covers diﬀerent two-stage methods, SPO+ , PFYL , and their regularization. DBB is omitted because it is far away from others. The size of the circles is proportional to the training time (Sec), so the smaller is better. Inspired by Sec 6.4, we further add l2 regularization with parameter φ2 = 1.0 for SPO+ and PFYL , and refer to these as SPO+ L2 and PFYL L2 . Fig 13 shows that the correct amount of prediction regularization can achieve a favorable balance between MSE and regret, sometimes even reducing the regret compared to the unregularized variants. Finding #5 Generally, SPO+ and PFYL can achieve good decisions at the cost of pre- diction accuracy. If one is seeking a balanced tradeoﬀ between decision quality and prediction accuracy, an end-to-end method with prediction regularization may be preferable. 7 Empirical Evaluation for Image-Based Shortest Path Following Poganˇci´c et al. [35] and Berthet et al. [6], we employ a truncated ResNet18 convolutional neural network (CNN) architecture consisting of the ﬁrst ﬁve layers on Warcraft terrain images (refer to Section 5.2). As Table 5 shows, the methods we compare include a two-stage method, SPO+ , DBB , DPO , 34 Bo Tang, Elias B. Khalil and PFYL with truncated ResNet18. We train the CNN over 50 epochs with batches of size 70. The learning rate is set to 0.0005 decaying at the epochs 30 and 40, and the hyperparameters n = 1, σ = 1 for DPO and PFYL , λ = 10 for DBB . We use the Hamming distance for DBB and the squared error of solutions for DPO , which are the loss functions used in the original papers. Method Description 2S Two-stage method where the predictor is a truncated ResNet18 SPO+ Truncated ResNet18 with SPO+ loss [16] DBB Truncated ResNet18 with diﬀerentiable black-box optimizer and Hamming distance loss [35] DPO Truncated ResNet18 with diﬀerentiable perturbed optimizer and squared error loss [6] PFYL Truncated ResNet18 with perturbed Fenchel-Young loss [6] Table 5: Methods compared in the experiments. The sample size of the test set ntest is 1000. To evaluate our methods, we compute the relative regret c⊺w∗(ˆc)−z∗(c) z∗(c) and path accuracy ∑d j=1 1 (z∗(c)j =z∗(ˆc)j ) d per instance on the test set; the latter is simply the fraction of edges in the “predicted” solution that are also in the optimal solution. As shown in Figure 14, the two-stage method, SPO+ and PFYL achieve com- parable levels of performance in predicting the shortest path on the Warcraft terrain, while PFYL obtains solutions that agree the most with the optima (i.e., highest Path Accuracy). It seems that the Warcraft shortest path problem may not require end-to-end learning. However, it is noteworthy that PFYL , despite lacking knowledge of the true costs, yields a competitive result, encouraging researchers to broaden the applications for end-to-end learning. 0 10 20 30 40 50 Epochs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8NormalizedRegretLearning Curve on Test Set 2S SPO+ PFYL DBB DPO 2S SPO+ PFYL DBB DPO Methods 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75RelativeRegret Relative Regret for each Instance on Test Set 2S SPO+ PFYL DBB DPO Methods 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00PathAccuracy Path Accuracy for each Instance on Test Set Fig. 14: Learning curve, relative regret, and path accuracy for the shortest path problem on the test set: The methods in the experiment include a two- stage neural network, SPO+ , DBB , DPO , and PFYL . The learning curve shows relative regret on the test set, and the box plot demonstrates the distribution of relative regret on the test set. For relative regret, lower is better. Title Suppressed Due to Excessive Length 35 Finding #6 End-to-end learning is eﬀective in rich contextual features such as im- ages. Moreover, the study highlights that PFYL can achieve impressive performance levels, even without knowing the costs during training. 8 Conclusion Because of the lack of easy-to-use generic tools, the potential power of the end- to-end predict-then-optimize has been underestimated or even overlooked in various applications. Our PyEPO package aims to alleviate barriers between the theory and practice of the end-to-end approach. PyEPO, the PyTorch-based end-to-end predict-then-optimize tool, is specif- ically designed for linear objective functions, including linear programming and (mixed) integer programming. The tool is extended from the automatic dif- ferentiation function of PyTorch, one of the most widespread open-source ma- chine learning frameworks. Hence, PyEPO allows leveraging numerous state- of-art deep learning models and techniques as implemented in PyTorch. PyEPO allows users to build optimization problems as black boxes or by leveraging interfaces to GurobiPy and Pyomo, thus providing broad compat- ibility with high-level modeling languages and both commercial and open- source solvers. With the PyEPO framework, we generate three synthetic datasets and incorporate one image dataset from the literature. Comprehensive experiments and analyses are conducted with these datasets. The results show that the end- to-end methods achieved excellent improvement in decision quality over two- stage methods in many cases. In addition, the end-to-end models can beneﬁt from using relaxations and regularization. The code repository includes step- by-step tutorials that allow new users to replicate the experiments presented in this paper or use them as a starting point for their own applications. The future development eﬀorts of PyEPO include: – New applications and new optimization problems, including linear objec- tive functions with mixed-integer variables or non-linear constraints; – Additional variations and improvements to current approaches such as warm starting and training speed up; – Novel training methods that leverage the gradient computation features that PyEPO provides; – Other existing end-to-end predict-then-optimize approaches, such as QPTL and its variants. 36 Bo Tang, Elias B. Khalil References 1. Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Devin M, et al. (2016) Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:160304467 2. Agrawal A, Amos B, Barratt S, Boyd S, Diamond S, Kolter JZ (2019) Diﬀerentiable convex optimization layers. In: Wallach H, Larochelle H, Beygelzimer A, d'Alch´e-Buc F, Fox E, Garnett R (eds) Advances in Neural Information Processing Systems, Curran Associates, Inc., vol 32 3. Agrawal A, Barratt S, Boyd S, Busseti E, Moursi WM (2019) Diﬀerenti- ating through a cone program. arXiv preprint arXiv:190409043 4. Amos B, Kolter JZ (2017) Optnet: Diﬀerentiable optimization as a layer in neural networks. In: International Conference on Machine Learning, PMLR, pp 136–145 5. Bengio Y (1997) Using a ﬁnancial training criterion rather than a predic- tion criterion. International Journal of Neural Systems 8(04):433–443 6. Berthet Q, Blondel M, Teboul O, Cuturi M, Vert JP, Bach F (2020) Learning with diﬀerentiable perturbed optimizers. arXiv preprint arXiv:200208676 7. Blondel M, Martins AF, Niculae V (2020) Learning with fenchel-young losses. J Mach Learn Res 21(35):1–69 8. Chen T, Li M, Li Y, Lin M, Wang N, Wang M, Xiao T, Xu B, Zhang C, Zhang Z (2015) Mxnet: A ﬂexible and eﬃcient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:151201274 9. Cplex II (2009) V12. 1: User’s manual for cplex. International Business Machines Corporation 46(53):157 10. Dalle G, Baty L, Bouvier L, Parmentier A (2022) Learning with com- binatorial optimization layers: a probabilistic approach. arXiv preprint arXiv:220713513 11. Dantzig G, Fulkerson R, Johnson S (1954) Solution of a large-scale traveling-salesman problem. Journal of the operations research society of America 2(4):393–410 12. Djolonga J, Krause A (2017) Diﬀerentiable learning of submodular models. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds) Advances in Neural Information Processing Systems, Curran Associates, Inc., vol 30 13. Domke J (2012) Generic methods for optimization-based modeling. In: Artiﬁcial Intelligence and Statistics, PMLR, pp 318–326 14. Donti P, Amos B, Kolter JZ (2017) Task-based end-to-end model learning in stochastic optimization. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds) Advances in Neural Infor- mation Processing Systems, Curran Associates, Inc., vol 30 15. Elmachtoub A, Liang JCN, McNellis R (2020) Decision trees for decision- making under the predict-then-optimize framework. In: International Con- ference on Machine Learning, PMLR, vol 119, pp 2858–2867 Title Suppressed Due to Excessive Length 37 16. Elmachtoub AN, Grigas P (2021) Smart “predict, then optimize”. Man- agement Science 0(0) 17. Ferber A, Wilder B, Dilkina B, Tambe M (2020) Mipaal: Mixed integer program as a layer. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 34, pp 1504–1511 18. Feurer M, Klein A, Eggensperger J Katharina Springenberg, Blum M, Hutter F (2015) Eﬃcient and robust automated machine learning. In: Advances in Neural Information Processing Systems 28 (2015), pp 2962– 2970 19. Ford B, Nguyen T, Tambe M, Sintov N, Delle Fave F (2015) Beware the soothsayer: From attack prediction accuracy to predictive reliability in security games. In: International Conference on Decision and Game Theory for Security, Springer, pp 35–56 20. Gavish B, Graves SC (1978) The travelling salesman problem and related problems. Operations Research Center Working Paper;OR 078-78 21. Gould S, Fernando B, Cherian A, Anderson P, Cruz RS, Guo E (2016) On diﬀerentiating parameterized argmin and argmax problems with applica- tion to bi-level optimization. arXiv preprint arXiv:160705447 22. Gurobi Optimization, LLC (2021) Gurobi Optimizer Reference Manual. URL https://www.gurobi.com 23. Hagberg A, Swart P, S Chult D (2008) Exploring network structure, dy- namics, and function using networkx. Tech. rep., Los Alamos National Lab.(LANL), Los Alamos, NM (United States) 24. Hart WE, Laird CD, Watson JP, Woodruﬀ DL, Hackebeil GA, Nicholson BL, Siirola JD, et al. (2017) Pyomo-optimization modeling in python, vol 67. Springer 25. Kao Yh, Roy B, Yan X (2009) Directed regression. In: Bengio Y, Schu- urmans D, Laﬀerty J, Williams C, Culotta A (eds) Advances in Neural Information Processing Systems, Curran Associates, Inc., vol 22 26. Mandi J, Guns T (2020) Interior point solving for lp-based predic- tion+optimisation. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in Neural Information Processing Systems, Curran Associates, Inc., vol 33, pp 7272–7282 27. Mandi J, Stuckey PJ, Guns T, et al. (2020) Smart predict-and-optimize for hard combinatorial optimization problems. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 34, pp 1603–1610, DOI 10.1609/ aaai.v34i02.5521 28. Martello S, Toth P (1990) Knapsack problems: algorithms and computer implementations. John Wiley & Sons, Inc. 29. Mattingley J, Boyd S (2012) Cvxgen: A code generator for embedded convex optimization. Optimization and Engineering 13(1):1–27 30. Miller CE, Tucker AW, Zemlin RA (1960) Integer programming for- mulation of traveling salesman problems. Journal of the ACM (JACM) 7(4):326–329 31. Ortega-Arranz H, Llanos DR, Gonzalez-Escribano A (2014) The shortest- path problem: Analysis and comparison of methods. Synthesis Lectures 38 Bo Tang, Elias B. Khalil on Theoretical Computer Science 1(1):1–87 32. Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Des- maison A, Antiga L, Lerer A (2017) Automatic diﬀerentiation in pytorch. In: NIPS 2017 Autodiﬀ Workshop 33. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S (2019) Pytorch: An imperative style, high-performance deep learning library. In: Wallach H, Larochelle H, Beygelzimer A, d'Alch´e-Buc F, Fox E, Garnett R (eds) Advances in Neural Information Processing Systems 32, Curran Associates, Inc., pp 8024–8035 34. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E (2011) Scikit- learn: Machine learning in Python. Journal of Machine Learning Research 12:2825–2830 35. Poganˇci´c MV, Paulus A, Musil V, Martius G, Rolinek M (2019) Diﬀeren- tiation of blackbox combinatorial solvers. In: International Conference on Learning Representations 36. Wilder B, Dilkina B, Tambe M (2019) Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 33, pp 1658–1665 9 Statements and Declarations 9.1 Funding This work was supported by funding from a SCALE AI Research Chair and an NSERC Discovery Grant. 9.2 Author Contributions Tang and Khalil contributed to the conception and design of the project. Soft- ware development, data generation, software testing, and benchmarking exper- iments were performed by Tang. Tang wrote the ﬁrst draft of the submission. Tang and Khalil contributed to ﬁnalizing the submission. Both authors read and approved the ﬁnal manuscript. 9.3 Competing Interests The authors have no relevant ﬁnancial or non-ﬁnancial interests to disclose.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources - Copy/Alzahrani17irradFrcstDp.pdf","text":"Available online at www.sciencedirect.com 1877-0509 © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems. 10.1016/j.procs.2017.09.045 ScienceDirect Procedia Computer Science 114 (2017) 304–313 10.1016/j.procs.2017.09.045 1877-0509 Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2017) 000–000 www.elsevier.com/locate/procedia 1877-0509 © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems. Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 – November 1, 2017, Chicago, Illinois, USA Solar Irradiance Forecasting Using Deep Neural Networks Ahmad Alzahrani a 0F*, Pourya Shamsia, Cihan Daglib, and Mehdi Ferdowsi a aElectrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO 65401, USA b System Engineering, Missouri University of Science and Technology, Rolla, MO 65401, USA Abstract Predicting solar irradiance has been an important topic in renewable energy generation. Prediction improves the planning and operation of photovoltaic systems and yields many economic advantages for electric utilities. The irradiance can be predicted using statistical methods such as artificial neural networks (ANN), support vector machines (SVM), or autoregressive moving average (ARMA). However, they either lack accuracy because they cannot capture long-term dependency or cannot be used with big data because of the scalability. This paper presents a method to predict the solar irradiance using deep neural networks. Deep recurrent neural networks (DRNNs) add complexity to the model without specifying what form the variation should take and allow the extraction of high-level features. The DRNN is used to predict the irradiance. The data utilized in this study is real data obtained from natural resources in Canada. The simulation of this method will be compared to several common methods such as support vector regression and feedforward neural networks (FNN). The results show that deep learning neural networks can outperform all other methods, as the performance tests indicate. © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems. Keywords: Neural Networks; Solar; Irradiance; Power; DNN; DRNN; PV 1. Introduction The increase in fossil fuel prices and the decrease of PV panel production cost have spurred the integration of renewable energy sources. Renewable energy sources have many advantages, including being environment-friendly * Corresponding author. Tel.: 970-430-7813; fax: +0-000-000-0000 . E-mail address: asakw9@mst.edu Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 and sustainable. However, these sources are highly intermittent. That is, the output power of renewable sources is variable and can be considered as a varying non-stationary time series. Solar photovoltaic (PV) systems are one of the main renewable energy sources and are simply panels that convert sunlight into electricity. The output of PV is highly dependent on solar irradiance, temperature, and different weather parameters. Predicting solar irradiance means that the output of PV is predicted one or more steps ahead of time. Prediction helps us improve various applications of power systems [1-2]. Figure 1 shows applications that use prediction to improve operation and planning of the power grid with the corresponding required time-resolution of the forecast [3-4]. Stability and regulation require information about the next seconds’ solar irradiation. Reserve management and load following necessitate information about the next minutes or next hours, of solar irradiance. Scheduling and unit commitment need information about the next days of solar irradiance to operate optimally. Seconds Minutes Hours Days Stability and Regulation Load Following Reserve Management and Dispatching Scheduling Unit Commitment Time horizon Fig. 1. Required time resolution of prediction. Fig. 2. Solar irradiance prediction methods Quantitative forecasting methods can represent solar irradiance time series. That is, past data can be used to predict future samples [5]. The average of solar irradiance can be assumed to be repeatable. For example, the monthly average of solar irradiance of summer is always the highest among other seasons in any given year. However, the time series itself is stochastic and highly affected by the cloud motion. Volatility of fossil fuel price, public health, and global warming awareness spurs the renewable energy growth and necessitates the upgrade of the current electric grid. Renewable energy sources can bring many economic and social benefits. However, integrating these sources to the electric power grid is usually accompanied with challenges such as intermittency. Therefore, forecasting methods can mitigate the intermittency as it gives information about future trends and allows users make decisions beforehand. Figure 2 shows some of the most common methods used in predicting and forecasting solar irradiance. Modeling solar irradiance can be divided into three categories: physical, statistical, and empirical models. 1.1. Physical models Several physical predictive models of solar irradiance can be found in the literature. One of the most widely used physical models is the irradiance model introduced by [6-8]. The total irradiance is the sum of direct and diffused irradiance, and ground reflected irradiance, as given in the following equation:     1 cos 1 cos .cos 22 TOT DNI DIFF REFLGG G G              (1) where β is the tilt angle of the PV panel and θ is the solar angle, which can be calculated by. Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 305 Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2017) 000–000 www.elsevier.com/locate/procedia 1877-0509 © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems. Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 – November 1, 2017, Chicago, Illinois, USA Solar Irradiance Forecasting Using Deep Neural Networks Ahmad Alzahrani a 0F*, Pourya Shamsia, Cihan Daglib, and Mehdi Ferdowsi a aElectrical and Computer Engineering, Missouri University of Science and Technology, Rolla, MO 65401, USA b System Engineering, Missouri University of Science and Technology, Rolla, MO 65401, USA Abstract Predicting solar irradiance has been an important topic in renewable energy generation. Prediction improves the planning and operation of photovoltaic systems and yields many economic advantages for electric utilities. The irradiance can be predicted using statistical methods such as artificial neural networks (ANN), support vector machines (SVM), or autoregressive moving average (ARMA). However, they either lack accuracy because they cannot capture long-term dependency or cannot be used with big data because of the scalability. This paper presents a method to predict the solar irradiance using deep neural networks. Deep recurrent neural networks (DRNNs) add complexity to the model without specifying what form the variation should take and allow the extraction of high-level features. The DRNN is used to predict the irradiance. The data utilized in this study is real data obtained from natural resources in Canada. The simulation of this method will be compared to several common methods such as support vector regression and feedforward neural networks (FNN). The results show that deep learning neural networks can outperform all other methods, as the performance tests indicate. © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems. Keywords: Neural Networks; Solar; Irradiance; Power; DNN; DRNN; PV 1. Introduction The increase in fossil fuel prices and the decrease of PV panel production cost have spurred the integration of renewable energy sources. Renewable energy sources have many advantages, including being environment-friendly * Corresponding author. Tel.: 970-430-7813; fax: +0-000-000-0000 . E-mail address: asakw9@mst.edu Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 and sustainable. However, these sources are highly intermittent. That is, the output power of renewable sources is variable and can be considered as a varying non-stationary time series. Solar photovoltaic (PV) systems are one of the main renewable energy sources and are simply panels that convert sunlight into electricity. The output of PV is highly dependent on solar irradiance, temperature, and different weather parameters. Predicting solar irradiance means that the output of PV is predicted one or more steps ahead of time. Prediction helps us improve various applications of power systems [1-2]. Figure 1 shows applications that use prediction to improve operation and planning of the power grid with the corresponding required time-resolution of the forecast [3-4]. Stability and regulation require information about the next seconds’ solar irradiation. Reserve management and load following necessitate information about the next minutes or next hours, of solar irradiance. Scheduling and unit commitment need information about the next days of solar irradiance to operate optimally. Seconds Minutes Hours Days Stability and Regulation Load Following Reserve Management and Dispatching Scheduling Unit Commitment Time horizon Fig. 1. Required time resolution of prediction. Fig. 2. Solar irradiance prediction methods Quantitative forecasting methods can represent solar irradiance time series. That is, past data can be used to predict future samples [5]. The average of solar irradiance can be assumed to be repeatable. For example, the monthly average of solar irradiance of summer is always the highest among other seasons in any given year. However, the time series itself is stochastic and highly affected by the cloud motion. Volatility of fossil fuel price, public health, and global warming awareness spurs the renewable energy growth and necessitates the upgrade of the current electric grid. Renewable energy sources can bring many economic and social benefits. However, integrating these sources to the electric power grid is usually accompanied with challenges such as intermittency. Therefore, forecasting methods can mitigate the intermittency as it gives information about future trends and allows users make decisions beforehand. Figure 2 shows some of the most common methods used in predicting and forecasting solar irradiance. Modeling solar irradiance can be divided into three categories: physical, statistical, and empirical models. 1.1. Physical models Several physical predictive models of solar irradiance can be found in the literature. One of the most widely used physical models is the irradiance model introduced by [6-8]. The total irradiance is the sum of direct and diffused irradiance, and ground reflected irradiance, as given in the following equation:     1 cos 1 cos .cos 22 TOT DNI DIFF REFLGG G G              (1) where β is the tilt angle of the PV panel and θ is the solar angle, which can be calculated by. 306 Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 cos( ) cos( ) cos( ) sin( ) sin( ) sin( )szz     (2) where z is the zenith angle (the angle between the vertical line and the beam radiation), and φs is the solar azimuth angle (the angle between the south of the projection of the beam and the PV surface). The angle α is the solar altitude angle, and γ is the surface azimuth angle, as shown in the Fig. 3. Fig. 3. Solar characteristics angles 1.2. Statistical models Many statistical models for solar irradiance can be found in the literature. The most common methods are shown in Fig. 2. There are two types of statistical models: time series methods and machine learning algorithms. In time series methods, solar irradiance can be considered a time series that contains three components: long-term trend, periodical components, and the mean. The most common time series prediction method is the autoregressive moving average ARMA(p,q) [9-11], which can be given by 1 1 11 2 2t t p tb t t t q t q t Autoregressive Moving Average yc y y                     (3) where the first part of (3) is autoregressive AR and the second part of (3) is the moving average MA. The variables can be identified using Yule-Walker method. The time series should be tested for stationarity before applying this approach, and this might be a drawback of time series prediction methods. The machine learning methods are also prevalent. The most widely used method is support vector machine SVM. The SVM is a supervised learning machine algorithm. SVM is a suitable tool for prediction and classification. The idea of SVMs is to define the decision boundaries by the concept of decision planes [12]. It has been used in forecasting, but SVM might not be capable of extracting the long-term correlation of the time series or the very short-term components. 1.3 Empirical models Several models can be found in the literature. The most common method is a sunshine-based model, which is given by (4), where a and b are empirical values, and GHI is global horizontal irradiance (monthly average). The monthly average of sunshine is given by S. The day length is given by So [13-15]: avg oo GHI a S b H S   (4) Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 2. Neural Networks 2.1. Feedforward neural networks (FNN) Neural networks are a computational method that uses an enormous group of artificial neurons. These neurons are inexactly equivalent to axon in a biological brain. Neural networks are used in various fields such as machine learning, image processing, signal processing, and computer science, controlling power electronics converters that interface the PV systems [16-18] and modeling energy sources [2]. They consist of three parts: neurons, activation functions, and bias. The neurons can be either input neurons, output neurons, or hidden neurons. Fig. 4a shows a simple feed-forward neural network with one hidden layer. Fig. 4b shows the FNN with input from a sliding window [19-20]. x2 x1 x3 Output layerHidden layerInput layer Sliding Window Sliding Window Sliding Window x1(t) x1(t-3) x2(t) x2(t-3) x3(t) x3(t-3) t-4, t-3, t-2, t-1, t Output layer Hidden Layer Input Layer x3 x2 x1 b) FNN with time series inputa) Basic FNN Fig. 4. Different architecture of FNN 2.2. Recurrent Neural Network (RNN) A recurrent neural network is a type of neural network used in modeling and prediction of sequential data where the output is dependent on the input. It has been used in applications such as image processing, sentiment analysis, language translation, and speech recognition. The RNN is capable of predicting a random sequence of inputs thanks to its internal memory. The internal memory can store information about previous calculation. Fig. 5 shows the basic RNN, where the hidden neuron h has feedback from other neurons in an earlier time step multiplied by a weight W. When basic RNN is spread out into a full network, it can be seen that the input of a hidden neuron takes an input from neurons at the previous time step [21]. Fig. 5. RNN unfolded (left), and RNN folded (right) The input xt at instant time t is multiplied by the input weight vector to obtain the input of the first hidden neuron. Then, the next hidden neuron, ht+1, will have the input of both xt+1 and the previous hidden neuron ht multiplied by Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 307 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 cos( ) cos( ) cos( ) sin( ) sin( ) sin( )szz     (2) where z is the zenith angle (the angle between the vertical line and the beam radiation), and φs is the solar azimuth angle (the angle between the south of the projection of the beam and the PV surface). The angle α is the solar altitude angle, and γ is the surface azimuth angle, as shown in the Fig. 3. Fig. 3. Solar characteristics angles 1.2. Statistical models Many statistical models for solar irradiance can be found in the literature. The most common methods are shown in Fig. 2. There are two types of statistical models: time series methods and machine learning algorithms. In time series methods, solar irradiance can be considered a time series that contains three components: long-term trend, periodical components, and the mean. The most common time series prediction method is the autoregressive moving average ARMA(p,q) [9-11], which can be given by 1 1 11 2 2t t p tb t t t q t q t Autoregressive Moving Average yc y y                     (3) where the first part of (3) is autoregressive AR and the second part of (3) is the moving average MA. The variables can be identified using Yule-Walker method. The time series should be tested for stationarity before applying this approach, and this might be a drawback of time series prediction methods. The machine learning methods are also prevalent. The most widely used method is support vector machine SVM. The SVM is a supervised learning machine algorithm. SVM is a suitable tool for prediction and classification. The idea of SVMs is to define the decision boundaries by the concept of decision planes [12]. It has been used in forecasting, but SVM might not be capable of extracting the long-term correlation of the time series or the very short-term components. 1.3 Empirical models Several models can be found in the literature. The most common method is a sunshine-based model, which is given by (4), where a and b are empirical values, and GHI is global horizontal irradiance (monthly average). The monthly average of sunshine is given by S. The day length is given by So [13-15]: avg oo GHI a S b H S   (4) Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 2. Neural Networks 2.1. Feedforward neural networks (FNN) Neural networks are a computational method that uses an enormous group of artificial neurons. These neurons are inexactly equivalent to axon in a biological brain. Neural networks are used in various fields such as machine learning, image processing, signal processing, and computer science, controlling power electronics converters that interface the PV systems [16-18] and modeling energy sources [2]. They consist of three parts: neurons, activation functions, and bias. The neurons can be either input neurons, output neurons, or hidden neurons. Fig. 4a shows a simple feed-forward neural network with one hidden layer. Fig. 4b shows the FNN with input from a sliding window [19-20]. x2 x1 x3 Output layerHidden layerInput layer Sliding Window Sliding Window Sliding Window x1(t) x1(t-3) x2(t) x2(t-3) x3(t) x3(t-3) t-4, t-3, t-2, t-1, t Output layer Hidden Layer Input Layer x3 x2 x1 b) FNN with time series inputa) Basic FNN Fig. 4. Different architecture of FNN 2.2. Recurrent Neural Network (RNN) A recurrent neural network is a type of neural network used in modeling and prediction of sequential data where the output is dependent on the input. It has been used in applications such as image processing, sentiment analysis, language translation, and speech recognition. The RNN is capable of predicting a random sequence of inputs thanks to its internal memory. The internal memory can store information about previous calculation. Fig. 5 shows the basic RNN, where the hidden neuron h has feedback from other neurons in an earlier time step multiplied by a weight W. When basic RNN is spread out into a full network, it can be seen that the input of a hidden neuron takes an input from neurons at the previous time step [21]. Fig. 5. RNN unfolded (left), and RNN folded (right) The input xt at instant time t is multiplied by the input weight vector to obtain the input of the first hidden neuron. Then, the next hidden neuron, ht+1, will have the input of both xt+1 and the previous hidden neuron ht multiplied by 308 Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 the weight W of the hidden neuron. The output neurons take the input only from the hidden neurons multiplied by the output weight N. The dynamic of the system is captured by 1()t h t th fM x W h   (5) ()ty ty f Nh (6) where f is the activation function such as sigmoid, tanh, or ReLU. The training of RNN is comparable to the training of an artificial neural network that uses a similar method to the backpropagation (BP) which is called backpropagation through time (BPTT). The BP depends only on the current state values, while BPTT depends on current and previous state values. 2.3. Deep Recurrent Neural Networks (DRNN) Deep neural networks consist of more than a hidden layer of neurons. Several studies [22-23] suggest that with more hidden layers, the neural network is capable of representing complex function more efficiently than RNN with less hidden layers. Fig. 6 shows the possible architecture of RNN as reported in [23]. Fig. 6a shows the conventional RNN already explained in the previous section. Fig. 6b shows the DRNN with a deep transition. The deep transition raises the number of nonlinear steps, which might increase the complexity of training such a network. One possible way to overcome such a challenge is to introduce a shortcut connection as in Fig. 6d. The shortcut connection offers a shorter path and bypasses the middle layers. Stacked RNN contains a stacked hidden layer ut , as shown in Fig. 6c. The hidden layers ut, and ht can be used with different time scales. Working with various time scales can be very useful for reading from sensors that have different sampling rates. Figure 6e shows DRNN with another deep transition between the hidden layer and the output [23]. The gradient of RNNs can be difficult to tract in long-term memorization when they use their connection for short-term memory. Therefore, the gradient might either vanish or explode [23]. The long-term short term memory (LSTM) method was introduced to overcome vanishing or exploding gradient. xt htht-1 xt-1 yt-1 yt xt htht-1 yt-1 yt xt-1 ht-2 yt-2 xt htht-1 xt-1 ht-2 yt-1 ytyt-2 xt htht-1 xt-1 ut-1 ut yt-1 yt a) RNN b) DRNN c) Stacked-RNN d) DRNN with shortcut connections e) DRNN with deep output layer xt htht-1 yt-1 yt xt-1 ht-2 yt-2 Fig. 6. Different types of DRNN 3. Data preprocessing The prediction process has many steps, as shown in Fig. 7. Before the pre-processing, a dataset of solar irradiance must be obtained. The data was downloaded from [24] and the location and sample data are shown in Fig. 8 and Fig. 9. The data is a high-resolution time series recorded at 100 Hz frequency. The benefit of having high-resolution data Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 is the ability to capture dynamic behavior such as ramp rates and fast fluctuations. Such information can help predict abnormal events and take action such as pre-emptive control [25]. The solar irradiance was recorded using LI-200S pyranometer every millisecond and then average over a period of 10 milliseconds [24]. The dataset contains global horizontal irradiance and global tilted irradiance with their corresponding time. The data was recorded for four days, March 24, February 8, October 8, and August 12, for clear-sky, overcast, variable, and very variable solar irradiance, respectively. Although data contains only four days, it is enough for training and predictions. The dataset from [24] needs pre-processing to clean and normalize data, as there are some misreading such as negative GHI readings. Several techniques in literature can clean data [26]. The simplest method is to remove misreadings and replace them with an interpolation of preceding and succeeding points. Fig. 7. Time series prediction process 3.1. Extracting cyclical behavior Any function of time can be decomposed into a combination of sinusoidal and sinusoidal functions with different frequencies using Fourier transform. The goal of using Fourier transform with time series data is to extract cyclical behavior. Figure 10 shows a periodogram, a time series plotted in the frequency domain, and all components were extracted. At frequency 55 Hz, the magnitude is high, which means there is some cyclical behavior. Fig. 8. Solar farm [google maps] Fig. 9. Solar irradiance map Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 309 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 the weight W of the hidden neuron. The output neurons take the input only from the hidden neurons multiplied by the output weight N. The dynamic of the system is captured by 1()t h t th fM x W h   (5) ()ty ty f Nh (6) where f is the activation function such as sigmoid, tanh, or ReLU. The training of RNN is comparable to the training of an artificial neural network that uses a similar method to the backpropagation (BP) which is called backpropagation through time (BPTT). The BP depends only on the current state values, while BPTT depends on current and previous state values. 2.3. Deep Recurrent Neural Networks (DRNN) Deep neural networks consist of more than a hidden layer of neurons. Several studies [22-23] suggest that with more hidden layers, the neural network is capable of representing complex function more efficiently than RNN with less hidden layers. Fig. 6 shows the possible architecture of RNN as reported in [23]. Fig. 6a shows the conventional RNN already explained in the previous section. Fig. 6b shows the DRNN with a deep transition. The deep transition raises the number of nonlinear steps, which might increase the complexity of training such a network. One possible way to overcome such a challenge is to introduce a shortcut connection as in Fig. 6d. The shortcut connection offers a shorter path and bypasses the middle layers. Stacked RNN contains a stacked hidden layer ut , as shown in Fig. 6c. The hidden layers ut, and ht can be used with different time scales. Working with various time scales can be very useful for reading from sensors that have different sampling rates. Figure 6e shows DRNN with another deep transition between the hidden layer and the output [23]. The gradient of RNNs can be difficult to tract in long-term memorization when they use their connection for short-term memory. Therefore, the gradient might either vanish or explode [23]. The long-term short term memory (LSTM) method was introduced to overcome vanishing or exploding gradient. xt htht-1 xt-1 yt-1 yt xt htht-1 yt-1 yt xt-1 ht-2 yt-2 xt htht-1 xt-1 ht-2 yt-1 ytyt-2 xt htht-1 xt-1 ut-1 ut yt-1 yt a) RNN b) DRNN c) Stacked-RNN d) DRNN with shortcut connections e) DRNN with deep output layer xt htht-1 yt-1 yt xt-1 ht-2 yt-2 Fig. 6. Different types of DRNN 3. Data preprocessing The prediction process has many steps, as shown in Fig. 7. Before the pre-processing, a dataset of solar irradiance must be obtained. The data was downloaded from [24] and the location and sample data are shown in Fig. 8 and Fig. 9. The data is a high-resolution time series recorded at 100 Hz frequency. The benefit of having high-resolution data Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 is the ability to capture dynamic behavior such as ramp rates and fast fluctuations. Such information can help predict abnormal events and take action such as pre-emptive control [25]. The solar irradiance was recorded using LI-200S pyranometer every millisecond and then average over a period of 10 milliseconds [24]. The dataset contains global horizontal irradiance and global tilted irradiance with their corresponding time. The data was recorded for four days, March 24, February 8, October 8, and August 12, for clear-sky, overcast, variable, and very variable solar irradiance, respectively. Although data contains only four days, it is enough for training and predictions. The dataset from [24] needs pre-processing to clean and normalize data, as there are some misreading such as negative GHI readings. Several techniques in literature can clean data [26]. The simplest method is to remove misreadings and replace them with an interpolation of preceding and succeeding points. Fig. 7. Time series prediction process 3.1. Extracting cyclical behavior Any function of time can be decomposed into a combination of sinusoidal and sinusoidal functions with different frequencies using Fourier transform. The goal of using Fourier transform with time series data is to extract cyclical behavior. Figure 10 shows a periodogram, a time series plotted in the frequency domain, and all components were extracted. At frequency 55 Hz, the magnitude is high, which means there is some cyclical behavior. Fig. 8. Solar farm [google maps] Fig. 9. Solar irradiance map 310 Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 Power (dB) Fig. 10. Extracting cyclical behavior 3.2 Data cleaning One of the most important steps in preprocessing is to clean the data from the abnormal data that comes from a missing sensor reading or incorrect readings from logging devices, such as negative data and radiation data that are bigger than the theoretical data limits. The missing data can be replaced using the linear interpolation method. The wrong readings can be replaced either with theoretical data or removed and then replaced with the maximum limit if it is beyond limit or interpolation results. In this paper, solar irradiance at night was eliminated using the primary elimination method. Depending on the season, the data time was taken from 5-6 a.m. to 7-8 p.m. Eliminating irradiance during night enhances the performance of the prediction process as only useful data feed the neural network. Another way to remove unuseful data or irradiance at night is to cluster the data using the fuzzy c-mean clustering algorithm. This is the best way to divide the time series data into seasons. Figure 11 shows the irradiance before removing irradiance and demonstrates the irradiance after removing unuseful irradiance data. Fig. 11. Solar irradiance before removing night hours (left) and after removing night hours (right) 3.3 Normalization Normalization is the next step of the pre-processing stage. In this step, all data are scaled between zero and one and align the entire probability distribution of the input values. Equation (7) is used to normalize the data: ) ) (( ( )max min min min max min y y y xx y xx      (7) where y is the normalized input, ymax is 1, and ymin is 0. In this method, x is assumed to have only real values. In this study, a built-in MATLAB function, Mapminmax, was used to normalize data between 0 and 1. Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 4. Data training and DRNN implementation After normalization, the data was split into three parts: training data set, testing dataset, and validation dataset with 70%,15%, and 15%, respectively. The architecture used was DRNN with long-term short-term units (LSTM) with two hidden layers and 35 hidden neurons. Keras API and MATLAB was used to implement the model. Another model was implemented as a reference mode. The reference model uses an FNN. The FNN was trained using Backpropagation method. The hidden neurons and number of layers were chosen by trial and error to achieve the maximum performance out of FNN. After training, the data was renormalized using mapminmax. 5. Performance Evaluation Several methods are used to evaluate the performance of a prediction method. The most common methods are main bias error (MBE), mean squared error (MSE), and root mean square error (RMSE), and they can be calculated using the following equations: 1 21 )-,( ii n i M n SE G GP    (8) 2 1 - 1 ( )ii n i RMSE G GP n    (9) 1 1 - ),( n i iiMBE n G GP    (10) where G is the actual output, GP is the predicted output, and n is the number of samples. The MBE can point out the misjudgment in forecasting. If the MBE is negative, the forecast overestimates the observation, and if the MBE is positive, it means the forecast underestimated the observation. The MSE is used in the training process as an objective function that needs to be minimized. The RMSE is the square root of MSE and is used when only small errors are tolerated. 6. Results and Discussion Figure 12 shows a sample of training data in four different weather conditions: few clouds, scattered, overcast, and clear-sky. The results of the predictive model are shown in Fig. 13. The actual data plotted with the standard deviation of the predictive model is plotted as well because the experiments had different random initializations. The average RMSE of the DRNN is 0.0513 in training and 0.068 in testing. On the other hand, the average RMSE of the DRNN is 0.0746 in training and 0.086 in testing. This method gives promising results, as the RMSE is the lowest. Table 1 shows a comparison of different prediction methods. The comparison was made on the test dataset. A feedforward neural network was implemented using one hidden layer with 21 neurons. It was trained using the Levenberg–Marquardt algorithm. The training was stopped early to avoid the overfitting issues. FNN has the poorest performance among the other methods. The support vector machine has better performance than FNN. However, the deep learning LSTM outperforms other methods. Table 1: Performance test Method RMSE MBE FNN 0.16 0.005 SVR 0.11 0.0042 Deep Learning (LSTM) 0.086 0.004 Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 311 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 Power (dB) Fig. 10. Extracting cyclical behavior 3.2 Data cleaning One of the most important steps in preprocessing is to clean the data from the abnormal data that comes from a missing sensor reading or incorrect readings from logging devices, such as negative data and radiation data that are bigger than the theoretical data limits. The missing data can be replaced using the linear interpolation method. The wrong readings can be replaced either with theoretical data or removed and then replaced with the maximum limit if it is beyond limit or interpolation results. In this paper, solar irradiance at night was eliminated using the primary elimination method. Depending on the season, the data time was taken from 5-6 a.m. to 7-8 p.m. Eliminating irradiance during night enhances the performance of the prediction process as only useful data feed the neural network. Another way to remove unuseful data or irradiance at night is to cluster the data using the fuzzy c-mean clustering algorithm. This is the best way to divide the time series data into seasons. Figure 11 shows the irradiance before removing irradiance and demonstrates the irradiance after removing unuseful irradiance data. Fig. 11. Solar irradiance before removing night hours (left) and after removing night hours (right) 3.3 Normalization Normalization is the next step of the pre-processing stage. In this step, all data are scaled between zero and one and align the entire probability distribution of the input values. Equation (7) is used to normalize the data: ) ) (( ( )max min min min max min y y y xx y xx      (7) where y is the normalized input, ymax is 1, and ymin is 0. In this method, x is assumed to have only real values. In this study, a built-in MATLAB function, Mapminmax, was used to normalize data between 0 and 1. Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 4. Data training and DRNN implementation After normalization, the data was split into three parts: training data set, testing dataset, and validation dataset with 70%,15%, and 15%, respectively. The architecture used was DRNN with long-term short-term units (LSTM) with two hidden layers and 35 hidden neurons. Keras API and MATLAB was used to implement the model. Another model was implemented as a reference mode. The reference model uses an FNN. The FNN was trained using Backpropagation method. The hidden neurons and number of layers were chosen by trial and error to achieve the maximum performance out of FNN. After training, the data was renormalized using mapminmax. 5. Performance Evaluation Several methods are used to evaluate the performance of a prediction method. The most common methods are main bias error (MBE), mean squared error (MSE), and root mean square error (RMSE), and they can be calculated using the following equations: 1 21 )-,( ii n i M n SE G GP    (8) 2 1 - 1 ( )ii n i RMSE G GP n    (9) 1 1 - ),( n i iiMBE n G GP    (10) where G is the actual output, GP is the predicted output, and n is the number of samples. The MBE can point out the misjudgment in forecasting. If the MBE is negative, the forecast overestimates the observation, and if the MBE is positive, it means the forecast underestimated the observation. The MSE is used in the training process as an objective function that needs to be minimized. The RMSE is the square root of MSE and is used when only small errors are tolerated. 6. Results and Discussion Figure 12 shows a sample of training data in four different weather conditions: few clouds, scattered, overcast, and clear-sky. The results of the predictive model are shown in Fig. 13. The actual data plotted with the standard deviation of the predictive model is plotted as well because the experiments had different random initializations. The average RMSE of the DRNN is 0.0513 in training and 0.068 in testing. On the other hand, the average RMSE of the DRNN is 0.0746 in training and 0.086 in testing. This method gives promising results, as the RMSE is the lowest. Table 1 shows a comparison of different prediction methods. The comparison was made on the test dataset. A feedforward neural network was implemented using one hidden layer with 21 neurons. It was trained using the Levenberg–Marquardt algorithm. The training was stopped early to avoid the overfitting issues. FNN has the poorest performance among the other methods. The support vector machine has better performance than FNN. However, the deep learning LSTM outperforms other methods. Table 1: Performance test Method RMSE MBE FNN 0.16 0.005 SVR 0.11 0.0042 Deep Learning (LSTM) 0.086 0.004 312 Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 0 500 1000 Few clouds 0 1000 2000 Scattered clouds 0 100 200 Overcast 4:48AM 7:12AM 9:36AM 12:00PM 2:24PM 4:48PM 7:12PM 0 500 1000 Clear-sky Fig. 12 Solar irradiance in different weather conditions 10:30AM 11:30AM 12:30PM 1:30PM 2:30PMSolar Irradiance W/m2 500 520 540 560 580 600 620 640 660 680 700 actual data standard deviation of prediction Time Fig. 13 Prediction results 7. Conclusion and future work This paper presented a deep learning neural network algorithm and was implemented to predict short-term solar irradiance. This method was applied to real data obtained from a Canadian solar farm [24]. The data was cleaned, and anomalies such as wrong readings were removed and the data normalized to be ready for training. Then, the network was implemented to train the data and predict the future samples. The results were presented and were compared to other methods such as FNN. All other predictive models exhibited lower accuracies and more bias error than deep learning neural network. Deep recurrent neural networks have potential in big-data applications, renewable energy forecasting, and predictive modeling. Solar irradiance time series can be treated as big data if the sampling rate and volume are high. References [1] A. Alzahrani, J.W. Kimball, C. Dagli, Predicting Solar Irradiance Using Time Series Neural Networks, Procedia Computer Science, Volume 36, 2014, Pages 623-628, ISSN 1877-0509, http://dx.doi.org/10.1016/j.procs.2014.09.065. [2] A. Alzahrani, P. Shamsi, M. Ferdowsi, C. Dagli, Modeling and Simulation of Microgrid, Procedia Computer Science, 2017, ISSN 1877-0509. [3] E. B. Ssekulima, M. B. Anwar, A. Al Hinai and M. S. El Moursi, \"Wind speed and solar irradiance forecasting techniques for enhanced renewable energy integration with the grid: a review,\" in IET Renewable Power Generation, vol. 10, no. 7, pp. 885-989, 7 2016. doi: 10.1049/iet-rpg.2015.0477 [4] Holttinen, H., Meibom, P., Orths, A., et al: ‘Impacts of large amounts of wind power on design and operation of power systems, results of iea collaboration’, Wind Energy, 2011, 14, (2), pp. 179–192 (doi: 10.1002/we.410). [5] F. Alfaris, A. Alzahrani and J. W. Kimball, \"Stochastic model for PV sensor array data,\" 2014 International Conference on Renewable Energy Research and Application (ICRERA), Milwaukee, WI, 2014, pp. 798-803. doi: 10.1109/ICRERA.2014.7016495 [6] W. De Soto, S.A. Klein, W.A. Beckman, Improvement and validation of a model for photovoltaic array performance, Solar Energy, Volume 80, Issue 1, January 2006, Pages 78-88, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2005.06.010 [7] Duffie, John A., and William A. Beckman. \"Solar engineering of thermal processes.\" (1980). [8] Alberto Dolara, Sonia Leva, Giampaolo Manzolini, Comparison of different physical models for PV power output prediction, Solar Energy, Volume 119, September 2015, Pages 83-99, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2015.06.017. [9] A. Phinikarides, G. Makrides, N. Kindyni, A. Kyprianou and G. E. Georghiou, \"ARIMA modeling of the performance of different photovoltaic technologies,\" 2013 IEEE 39th Photovoltaic Specialists Conference (PVSC), Tampa, FL, 2013, pp. 0797-0801. [10] Jamal Hassan, ARIMA and regression models for prediction of daily and monthly clearness index, Renewable Energy, Volume 68, August 2014, Pages 421-427, ISSN 0960-1481, https://doi.org/10.1016/j.renene.2014.02.016. [11] M. David, F. Ramahatana, P.J. Trombe, P. Lauret, Probabilistic forecasting of the solar irradiance with recursive ARMA and GARCH models, Solar Energy, Volume 133, August 2016, Pages 55-72, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2016.03.064. [12] Weston, J.: Support Vector Machine. In: Tutorial, 4 Independence Way, Princeton, USA [13] Gana, N. N., and D. O. Akpootu. \"Angstrom type empirical correlation for estimating global solar radiation in North-Eastern Nigeria.\" The International Journal Of Engineering And Science 2 (2013): 58-78. Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 [14] Bishal Madhab Mazumdar, Mohd. Saquib, Abhik Kumar Das, An empirical model for ramp analysis of utility-scale solar PV power, Solar Energy, Volume 107, September 2014, Pages 44-49, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2014.05.027. [15] A. J. Veldhuis, A. M. Nobre, I. M. Peters, T. Reindl, R. Rüther and A. H. M. E. Reinders, \"An Empirical Model for Rack-Mounted PV Module Temperatures for Southeast Asian Locations Evaluated for Minute Time Scales,\" in IEEE Journal of Photovoltaics, vol. 5, no. 3, pp. 774-782, May 2015. [16] A. Alzahrani, P. Shamsi, M. Ferdowsi, C. Dagli, Chaotic Behavior in High-Gain Interleaved Dc-dc Converters, Procedia Computer Science, 2017, ISSN 1877-0509. [17] A. Alzahrani, P. Shamsi and M. Ferdowsi, \"Analysis and design of bipolar Dickson DC-DC converter,\" 2017 IEEE Power and Energy Conference at Illinois (PECI), Champaign, IL, 2017, pp. 1-6. doi: 10.1109/PECI.2017.7935733 [18] A. Alzahrani, P. Shamsi and M. Ferdowsi, \"A Novel Non-isolated High-Gain Dc-dc Boost Converter,\" 2017 North American Power Symposium (NAPS), Morgantown, WV, 2017, pp. 1-6. [19] Simon Haykin. 1998. Neural Networks: A Comprehensive Foundation (2nd ed.). Prentice Hall PTR, Upper Saddle River, NJ, USA. [20] Lippmann, Richard. \"An introduction to computing with neural nets.\" IEEE Assp magazine 4, no. 2 (1987): 4-22. [21] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" Nature 521, no. 7553 (2015): 436-444. [22] G. Hinton et al., \"Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,\" in IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, Nov. 2012. doi: 10.1109/MSP.2012.2205597 [23] Pascanu, Razvan, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. \"How to construct deep recurrent neural networks.\" arXiv preprint arXiv:1312.6026 (2013). [24] Natural resources Canada http://www.nrcan.gc.ca/energy/renewable-electricity/solar-photovoltaic/18409 [25] P. Shamsi and H. Xie, \"Preemptive control: A paradigm in supporting high renewable penetration levels,\" 2016 North American Power Symposium (NAPS), Denver, CO, 2016, pp. 1-5. doi: 10.1109/NAPS.2016.7747860 [26] Winkler, William E. \"Data cleaning methods.\" In Proc ACM SIGKDD Workshop on Data Cleaning, Record Linkage, and Object Consolidation. 2003. [27] Cyril Voyant, Gilles Notton, Soteris Kalogirou, Marie-Laure Nivet, Christophe Paoli, Fabrice Motte, Alexis Fouilloy, Machine learning methods for solar radiation forecasting: A review, Renewable Energy, Volume 105, 2017, Pages 569-582, ISSN 0960-1481, http://dx.doi.org/10.1016/j.renene.2016.12.095. Ahmad Alzahrani et al. / Procedia Computer Science 114 (2017) 304–313 313 Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 0 500 1000 Few clouds 0 1000 2000 Scattered clouds 0 100 200 Overcast 4:48AM 7:12AM 9:36AM 12:00PM 2:24PM 4:48PM 7:12PM 0 500 1000 Clear-sky Fig. 12 Solar irradiance in different weather conditions 10:30AM 11:30AM 12:30PM 1:30PM 2:30PMSolar Irradiance W/m2 500 520 540 560 580 600 620 640 660 680 700 actual data standard deviation of prediction Time Fig. 13 Prediction results 7. Conclusion and future work This paper presented a deep learning neural network algorithm and was implemented to predict short-term solar irradiance. This method was applied to real data obtained from a Canadian solar farm [24]. The data was cleaned, and anomalies such as wrong readings were removed and the data normalized to be ready for training. Then, the network was implemented to train the data and predict the future samples. The results were presented and were compared to other methods such as FNN. All other predictive models exhibited lower accuracies and more bias error than deep learning neural network. Deep recurrent neural networks have potential in big-data applications, renewable energy forecasting, and predictive modeling. Solar irradiance time series can be treated as big data if the sampling rate and volume are high. References [1] A. Alzahrani, J.W. Kimball, C. Dagli, Predicting Solar Irradiance Using Time Series Neural Networks, Procedia Computer Science, Volume 36, 2014, Pages 623-628, ISSN 1877-0509, http://dx.doi.org/10.1016/j.procs.2014.09.065. [2] A. Alzahrani, P. Shamsi, M. Ferdowsi, C. Dagli, Modeling and Simulation of Microgrid, Procedia Computer Science, 2017, ISSN 1877-0509. [3] E. B. Ssekulima, M. B. Anwar, A. Al Hinai and M. S. El Moursi, \"Wind speed and solar irradiance forecasting techniques for enhanced renewable energy integration with the grid: a review,\" in IET Renewable Power Generation, vol. 10, no. 7, pp. 885-989, 7 2016. doi: 10.1049/iet-rpg.2015.0477 [4] Holttinen, H., Meibom, P., Orths, A., et al: ‘Impacts of large amounts of wind power on design and operation of power systems, results of iea collaboration’, Wind Energy, 2011, 14, (2), pp. 179–192 (doi: 10.1002/we.410). [5] F. Alfaris, A. Alzahrani and J. W. Kimball, \"Stochastic model for PV sensor array data,\" 2014 International Conference on Renewable Energy Research and Application (ICRERA), Milwaukee, WI, 2014, pp. 798-803. doi: 10.1109/ICRERA.2014.7016495 [6] W. De Soto, S.A. Klein, W.A. Beckman, Improvement and validation of a model for photovoltaic array performance, Solar Energy, Volume 80, Issue 1, January 2006, Pages 78-88, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2005.06.010 [7] Duffie, John A., and William A. Beckman. \"Solar engineering of thermal processes.\" (1980). [8] Alberto Dolara, Sonia Leva, Giampaolo Manzolini, Comparison of different physical models for PV power output prediction, Solar Energy, Volume 119, September 2015, Pages 83-99, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2015.06.017. [9] A. Phinikarides, G. Makrides, N. Kindyni, A. Kyprianou and G. E. Georghiou, \"ARIMA modeling of the performance of different photovoltaic technologies,\" 2013 IEEE 39th Photovoltaic Specialists Conference (PVSC), Tampa, FL, 2013, pp. 0797-0801. [10] Jamal Hassan, ARIMA and regression models for prediction of daily and monthly clearness index, Renewable Energy, Volume 68, August 2014, Pages 421-427, ISSN 0960-1481, https://doi.org/10.1016/j.renene.2014.02.016. [11] M. David, F. Ramahatana, P.J. Trombe, P. Lauret, Probabilistic forecasting of the solar irradiance with recursive ARMA and GARCH models, Solar Energy, Volume 133, August 2016, Pages 55-72, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2016.03.064. [12] Weston, J.: Support Vector Machine. In: Tutorial, 4 Independence Way, Princeton, USA [13] Gana, N. N., and D. O. Akpootu. \"Angstrom type empirical correlation for estimating global solar radiation in North-Eastern Nigeria.\" The International Journal Of Engineering And Science 2 (2013): 58-78. Alzahrani, Ahmad/ Procedia Computer Science 00 (2017) 000–000 [14] Bishal Madhab Mazumdar, Mohd. Saquib, Abhik Kumar Das, An empirical model for ramp analysis of utility-scale solar PV power, Solar Energy, Volume 107, September 2014, Pages 44-49, ISSN 0038-092X, https://doi.org/10.1016/j.solener.2014.05.027. [15] A. J. Veldhuis, A. M. Nobre, I. M. Peters, T. Reindl, R. Rüther and A. H. M. E. Reinders, \"An Empirical Model for Rack-Mounted PV Module Temperatures for Southeast Asian Locations Evaluated for Minute Time Scales,\" in IEEE Journal of Photovoltaics, vol. 5, no. 3, pp. 774-782, May 2015. [16] A. Alzahrani, P. Shamsi, M. Ferdowsi, C. Dagli, Chaotic Behavior in High-Gain Interleaved Dc-dc Converters, Procedia Computer Science, 2017, ISSN 1877-0509. [17] A. Alzahrani, P. Shamsi and M. Ferdowsi, \"Analysis and design of bipolar Dickson DC-DC converter,\" 2017 IEEE Power and Energy Conference at Illinois (PECI), Champaign, IL, 2017, pp. 1-6. doi: 10.1109/PECI.2017.7935733 [18] A. Alzahrani, P. Shamsi and M. Ferdowsi, \"A Novel Non-isolated High-Gain Dc-dc Boost Converter,\" 2017 North American Power Symposium (NAPS), Morgantown, WV, 2017, pp. 1-6. [19] Simon Haykin. 1998. Neural Networks: A Comprehensive Foundation (2nd ed.). Prentice Hall PTR, Upper Saddle River, NJ, USA. [20] Lippmann, Richard. \"An introduction to computing with neural nets.\" IEEE Assp magazine 4, no. 2 (1987): 4-22. [21] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" Nature 521, no. 7553 (2015): 436-444. [22] G. Hinton et al., \"Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,\" in IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, Nov. 2012. doi: 10.1109/MSP.2012.2205597 [23] Pascanu, Razvan, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. \"How to construct deep recurrent neural networks.\" arXiv preprint arXiv:1312.6026 (2013). [24] Natural resources Canada http://www.nrcan.gc.ca/energy/renewable-electricity/solar-photovoltaic/18409 [25] P. Shamsi and H. Xie, \"Preemptive control: A paradigm in supporting high renewable penetration levels,\" 2016 North American Power Symposium (NAPS), Denver, CO, 2016, pp. 1-5. doi: 10.1109/NAPS.2016.7747860 [26] Winkler, William E. \"Data cleaning methods.\" In Proc ACM SIGKDD Workshop on Data Cleaning, Record Linkage, and Object Consolidation. 2003. [27] Cyril Voyant, Gilles Notton, Soteris Kalogirou, Marie-Laure Nivet, Christophe Paoli, Fabrice Motte, Alexis Fouilloy, Machine learning methods for solar radiation forecasting: A review, Renewable Energy, Volume 105, 2017, Pages 569-582, ISSN 0960-1481, http://dx.doi.org/10.1016/j.renene.2016.12.095.","libVersion":"0.3.2","langs":""}
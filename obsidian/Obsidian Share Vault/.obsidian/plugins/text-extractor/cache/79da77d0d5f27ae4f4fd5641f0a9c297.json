{"path":"lit/lit_sources.backup/Shavitt18RegularizationLearningNetworks.pdf","text":"We hypothesized that this potentially large variability in the relative importance of different input features may partly explain the lower performance of DNNs on such tabular datasets [11]. One way to overcome this limitation could be to assign a different regularization coefﬁcient to every weight, which might allow the network to accommodate the non-distributed representation and the variability in relative importance found in tabular datasets. This will require tuning a large number of hyperparameters. The default approach to hyperparameter tuning is using derivative-free optimization of the validation loss, i.e., a loss of a subset of the training set which is not used to ﬁt the model. This approach becomes computationally intractable very quickly. Here, we present a new hyperparameter tuning technique, in which we optimize the regulariza- tion coefﬁcients using a newly introduced loss function, which we term the Counterfactual Loss, orLCF . We term the networks that apply this technique Regularization Learning Networks (RLNs). In RLNs, the regularization coefﬁcients are optimized together with learning the network weight parameters. We show that RLNs signiﬁcantly and substantially outperform DNNs with other regu- larization schemes, and achieve comparable results to GBTs. When used in an ensemble with GBTs, RLNs achieves state of the art results on several prediction tasks on a tabular dataset with varying relative importance for different features. 2 Related work Applying different regularization coefﬁcients to different parts of the network is a common prac- tice. The idea of applying different regularization coefﬁcients to every weight was introduced [23], but it was only applied to images with a toy model to demonstrate the ability to optimize many hyperparameters. Our work is also related to the rich literature of works on hyperparameter optimization [29]. These works mainly focus on derivative-free optimization [30, 6, 17]. Derivative-based hyperparameter optimization is introduced in [3] for linear models and in [23] for neural networks. In these works, the hyperparameters are optimized using the gradients of the validation loss. Practically, this means that every optimization step of the hyperparameters requires training the whole network and back propagating the loss to the hyperparameters. [21] showed a more efﬁcient derivative based way for hyperparameter optimization, which still required a substantial amount of additional parame- ters. [22] introduce an optimization technique similar to the one introduced in this paper, however, the optimization technique in [22] requires a validation set, and only optimizes a single regulariza- tion coefﬁcient for each layer, and at most 10-20 hyperparameters in any network. In comparison, training RLNs doesn’t require a validation set, assigns a different regularization coefﬁcient for ev- ery weight, which results in up to millions of hyperparameters, optimized efﬁciently. Additionally, RLNs optimize the coefﬁcients in the log space and adds a projection after every update to counter the vanishing of the coefﬁcients. Most importantly, the efﬁcient optimization of the hyperparameters was applied to images and not to dataset with non-distributed representation like tabular datasets. DNNs have been successfully applied to tabular datasets like electronic health records, in [26, 24]. The use of RLN is complementary to these works, and might improve their results and allow the use of deeper networks on smaller datasets. To the best of our knowledge, our work is the ﬁrst to illustrate the statistical difference in distributed and non-distributed representations, to hypothesize that addition of hyperparameters could enable neural networks to achieve good results on datasets with non-distributed representations such as tabular datasets, and to efﬁciently train such networks on a real-world problems to signiﬁcantly and substantially outperform networks with other regularization schemes. 3 Regularization Learning Generally, when using regularization, we minimize ˜L (Z, W, λ) = L (Z, W ) + exp (λ) · ∑n i=1 ∥wi∥, where Z = {(xm, ym)}M m=1 are the training samples, L is the loss function, W = {wi}n i=1 are the 2 weights of the model, ∥·∥ is some norm, and λ is the regularization coefﬁcient,2 a hyperparameter of the network. Hyperparameters of the network, like λ, are usually obtained using cross-validation, which is the application of derivative-free optimization on LCV (Zt, Zv, λ) with respect to λ where LCV (Zt, Zv, λ) = L ( Zv, arg minW ˜L (Zt, W, λ) ) and (Zt, Zv) is some partition of Z into train and validation sets, respectively. If a different regularization coefﬁcient is assigned to each weight in the network, our learning loss becomes L † (Z, W, Λ) = L (Z, W ) + ∑n i=1 exp (λi) · ∥wi∥, where Λ = {λi} n i=1 are the regulariza- tion coefﬁcients. Using L † will require n hyperparameters, one for every network parameter, which makes tuning with cross-validation intractable, even for very small networks. We would like to keep using L † to update the weights, but to ﬁnd a more efﬁcient way to tune Λ. One way to do so is through SGD, but it is unclear which loss to minimize: L doesn’t have a derivative with respect to Λ, while L † has trivial optimal values, arg minΛ L † (Z, W, Λ) = {−∞} n i=1. LCV has a non-trivial dependency on Λ, but it is very hard to evaluate ∂LCV ∂wt+1,i 4 Experiments We demonstrate the performance of our method on the problem of predicting human traits from gut microbiome data and basic covariates (age, gender, BMI). The human gut microbiome is the collection of microorganisms found in the human gut and is composed of trillions of cells including bacteria, eukaryotes, and viruses. In recent years, there have been major advances in our understand- ing of the microbiome and its connection to human health. Microbiome composition is determined by DNA sequencing human stool samples that results in short (75-100 basepairs) DNA reads. By mapping these short reads to databases of known bacterial species, we can deduce both the source species and gene from which each short read originated. Thus, upon mapping a collection of dif- ferent samples, we obtain a matrix of estimated relative species abundances for each person and a matrix of the estimated relative gene abundances for each person. Since these features have varying relative importance (Figure 1), we expected GBTs to outperform DNNs on these tasks. We sampled 2,574 healthy participants for which we measured, in addition to the gut microbiome, a collection of different traits, including important disease risk factors such as cholesterol levels and BMI. Finding associations between these disease risk factors and the microbiome composition is of Figure 3: For each model type and trait, we took the 10 best performing models, based on their validation performance, and calcu- lated the average variance of the predicted test samples, and plotted it against the im- provement in R2 obtained when training en- sembles of these models. Note that models that have a high variance in their prediction beneﬁt more from the use of ensembles. As expected, DNNs gain the most from ensem- bling. great scientiﬁc interest, and can raise novel hypothe- ses about the role of the microbiome in disease. We tested 4 types of models: RLN, GBT, DNN, and Lin- ear Models (LM). The full list of hyperparameters, the setting of the training of the models and the en- sembles, as well as the description of all the input features and the measured traits, can be found in the supplementary material. 5 Results When running each model separately, GBTs achieve the best results on all of the tested traits, but it is only signiﬁcant on 3 of them (Figure 2). DNNs achieve the worst results, with 15% ± 1% less ex- plained variance than GBTs on average. RLNs sig- niﬁcantly and substantially improve this by a factor of 2.57 ± 0.05, and achieve only 2% ± 2% less ex- plained variance than GBTs on average. Constructing an ensemble of models is a powerful technique for improving performance, especially for models which have high variance, like neural net- works in our task. As seen in Figure 3, the average variance of predictions of the top 10 models of RLN and DNN is 1.3%±0.6% and 14%±3% respectively, while the variance of predictions of the top 10 mod- Figure 4: Ensembles of different predictors. 5 Figure 5: Results of various ensembles that are each composed of different types of models. Trait RLN + GBT LM + GBT GBT RLN Max input features in the RLN do not have any non-zero outgoing edges, while all of the input features have at least one non-zero outgoing edge in the DNN (Figure 6a). A possible explanation could be that the RLN was simply trained using a stronger regularization coefﬁcients, and increasing the value of λ for the DNN model would result in a similar behavior for the DNN, but in fact the RLN was obtained with an average regularization coefﬁcient of θ = −6.6 while the DNN model was trained using a regularization coefﬁcient of λ = −4.4. Despite this extreme sparsity, the non zero weights are not particularly small and have a similar distribution as the weights of the DNN (Figure 6b). (a) (b) Figure 6: a) Each line represents an input feature in a model. The values of each line are the absolute values of its outgoing weights, sorted from greatest to smallest. Noticeably, only 12% of the input features have any non-zero outgoing edge in the RLN model. b) The cumulative distribution of non-zero outgoing weights for the input features for different models. Remarkably, the distribution of non-zero weights is quite similar for the two models. We suspect that the combination of a sparse net- work with large weights allows RLNs to achieve their improved performance, as our dataset includes features with varying rel- ative importance. To show this, we re-optimized the hyperparameters of the DNN and RLN models after removing the covariates from the datasets. The covariates are very important features (Figure 1), and removing them would reduce the variability in relative importance. As can be seen in Figure 7a, even without the covariates, the RLN and GBT ensembles still achieve the best results on 5 out of the 9 traits. However, this improvement is less signiﬁcant than when adding the covariates, where RLN and GBT ensembles achieve the best results on 8 out of the 9 traits. RLNs still signiﬁcantly outperform DNNs, achieving explained variance higher by 2% ± 1%, but this is signiﬁcantly smaller than the 9% ± 2% improvement obtained when adding the covariates (Figure 7b). We speculate that this is because RLNs particularly shine when features have very different relative importances. To understand what causes this interesting structure, we next explored how the weights in RLNs change during training. During training, each edge performs a traversal in the w, λ space. We expect that when λ decreases and the regularization is relaxed, the absolute value of w should increase, and vice versa. In Figure 8, we can see that 99.9% (a) (b) Figure 7: a) Training our models without adding the covariates. b) The relative improvement RLN achieves compared to DNN for different input features. Figure 8: On the left axis, shown is the traversal of edges of the ﬁrst layer that ﬁnished the training with a non-zero weight in the w, λ space. Each colored line represents an edge, its color represents its regularization, with yellow lines having strong regularization. On the right axis, the black line plots the percent of zero weight edges in the ﬁrst layer during training. Evaluating feature importance is difﬁcult, especially in domains in which little is known such as the gut microbiome. One possibility is to examine the information it supplies. In Figure 9a we show the feature importance achieved through this technique using RLNs and DNNs. While the importance in DNNs is almost constant and does not give any meaningful information about the speciﬁc importance of the features, the importance in RLNs is much more meaningful, with entropy of the 4.6 bits for the RLN importance, compared to more than twice for the DNN importance, 9.5 bits. 8 (a) (b) Figure 9: a) The input features, sorted by their importance, in a DNN and RLN models. b) The Jensen-Shannon divergence between the feature importance of different instantiations of a model. Another possibility is to evaluate its consistency across different instantiations of the model. We expect that a good feature importance technique will give similar importance distributions regardless of instantiation. We trained 10 instantiations for each model and phenotype and evaluated their feature importance distributions, for which we calculated the Jensen-Shannon divergence. In Figure 9b we see that RLNs have divergence values 48% ± 1% and 54% ± 2% lower than DNNs and LMs respectively. This is an indication that Garson’s algorithm results in meaningful feature importances in RLNs. We list of the 5 most important bacterial species for different traits in the supplementary material. 7 Conclusion In this paper, we explore the learning of datasets with non-distributed representation, such as tab- ular datasets. We hypothesize that modular regularization could boost the performance of DNNs on such tabular datasets. We introduce the Counterfactual Loss, LCF , and Regularization Learn- ing Networks (RLNs) which use the Counterfactual Loss to tune its regularization hyperparameters efﬁciently during learning together with the learning of the weights of the network. We test our method on the task of predicting human traits from covariates and microbiome data and show that RLNs signiﬁcantly and substantially improve the performance over classical DNNs, achieving an increased explained variance by a factor of 2.75 ± 0.05 and comparable results with GBTs. The use of ensembles further improves the performance of RLNs, and ensembles of RLN and GBT achieve the best results on all but one of the traits, and outperform signiﬁcantly any other ensemble not incorporating RLNs on 3 of the traits. We further explore RLN structure and dynamics and show that RLNs learn extremely sparse net- works, eliminating 99.8% of the network edges and 82% of the input features. In our setting, this was achieved in the ﬁrst 10-20 epochs of training, in which the network learns its regularization. Because of the modularity of the regularization, the remaining edges are virtually not regulated at all, achieving a similar distribution to a DNN. The modular structure of the network is especially beneﬁcial for datasets with high variability in the relative importance of the input features, where RLNs particularly shine compared to DNNs. The sparse structure of RLNs lends itself naturally to model interpretability, which gives meaningful insights into the relation between features and the labels, and may itself serve as a feature selection technique that can have many uses on its own [13]. Besides improving performance on tabular datasets, another important application of RLNs could be learning tasks where there are multiple data sources, one that includes features with high variability in the relative importance, and one which does not. To illustrate this point, consider the problem of detecting pathologies from medical imaging. DNNs achieve impressive results on this task [32], but in real life, the imaging is usually accompanied by a great deal of tabular metadata in the form of the electronic health records of the patient. We would like to use both datasets for prediction, but different models achieve the best results on each part of the data. Currently, there is no simple way to jointly train and combine the models. Having a DNN architecture such as RLN that performs well on tabular data will thus allow us to jointly train a network on both of the datasets natively, and may improve the overall performance. 9 Acknowledgments We would like to thank Ron Sender, Eran Kotler, Smadar Shilo, Nitzan Artzi, Daniel Greenfeld, Gal Yona, Tomer Levy, Dror Kaufmann, Aviv Netanyahu, Hagai Rossman, Yochai Edlitz, Amir Globerson and Uri Shalit for useful discussions. References [1] David Beam and Mark Schramm. Rossmann Store Sales. 2015. 1 [2] Kamil Belkhayat, Abou Omar, Gino Bruner, Yuyi Wang, and Roger Wattenhofer. XGBoost and LGBM for Porto Seguro’s Kaggle challenge: A comparison Semester Project. 2018. 1 [3] Yoshua Bengio. Gradient-Based Optimization of 1 Introduction. pages 1–18, 1999. 2 [4] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Review and New Perspectives. 1 [5] Yoshua Bengio and Yann LeCun. Scaling Learning Algorithms towards AI. 2007. 1 [6] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for Hyper-Parameter Optimization. Advances in Neural Information Processing Systems (NIPS), pages 2546–2554, 2011. 2 [7] Hengxing Cai, Runxing Zhong, Chaohe Wang, Kejie Zhou, Hongyun Lee, Renxin Zhong, Yao Zhou, Da Li, Nan Jiang, Xu Cheng, and Jiawei Shen. KDD CUP 2018 Travel Time Prediction. 1 [8] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. 1 [9] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and Michiel Bacchiani Google. State-Of-The-Art Speech Recognition with Sequence-To-Sequence Models. 1 [10] G D Garson. Interpreting neural network connection weights. AI Expert, 6(4):47–51, apr 1991. 6 [11] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www. deeplearningbook.org. 1 [12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining And Harnessing Adversarial Examples. 1 [13] Bryce Goodman and Seth Flaxman. European Union regulations on algorithmic decision-making and a \" right to explanation \". 7 [14] GE HINTON, JL MCCLELLAND, and DE RUMELHART. Distributed representations. 1 [15] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating Feature Importance Estimates. 6 [16] Yide Huang. Highway Tollgates Trafﬁc Flow Prediction Task 1. Travel Time Prediction. 1 [17] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential Model - Based Optimization for General Algorithm Conﬁguration. Lecture Notes in Computer Science, 5:507–223, 2011. 2 [18] Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho- rat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. 2016. 1 [19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classiﬁcation with Deep Convolu- tional Neural Networks. 1 [20] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/. 1 [21] Jonathan Lorraine and David Duvenaud. Stochastic Hyperparameter Optimization through Hypernet- works. 2018. 2 [22] Jelena Luketina, Jelena Luketina@aalto Fi, Mathias Berglund, Mathias Berglund@aalto Fi, Klaus Greff, Klaus@idsia Ch, Tapani Raiko, and Tapani Raiko@aalto Fi. Scalable Gradient-Based Tuning of Contin- uous Regularization Hyperparameters. 2 [23] Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Gradient-based Hyperparameter Optimization through Reversible Learning. 2 [24] Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records. Nature Publishing Group, 2016. 2 [25] Nicolas Papernot, Patrick Mcdaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The Limitations of Deep Learning in Adversarial Settings. 1 10 [26] Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela Hardt, Peter J Liu, Xiaob- ing Liu, Jake Marcus, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Yi Zhang, Gerardo Flores, Gavin E Duggan, Jamie Irvine, Quoc Le, Kurt Litsch, Alexander Mossin, Justin Tansuwan, De Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L Volchenboum, Katherine Chou, Michael Pearson, Srinivasan Madabushi, Nigam H Shah, Atul J Butte, Michael D Howell, Claire Cui, Greg S Corrado, and Jeffrey Dean. Scalable and accurate deep learning with electronic health records. npj Digital Medicine, 1, 2018. 2 [27] Vlad Sandulescu, Adform Copenhagen, and Denmark Mihai Chiru. Predicting the future relevance of research institutions - The winning solution of the KDD Cup 2016. 1 [28] Avanti Shrikumar, Peyton Greenside, and Anna Y Shcherbina. Not Just A Black Box: Learning Important Features Through Propagating Activation Differences. (3). 6, 4 [29] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay. 2 [30] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of Machine Learn- ing Algorithms. pages 1–12, 2012. 2 [31] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Gradients of Counterfactuals. 6, 4 [32] Kenji Suzuki. Overview of deep learning in medical imaging. Radiological Physics and Technology, 10. 7 11","libVersion":"0.3.2","langs":""}
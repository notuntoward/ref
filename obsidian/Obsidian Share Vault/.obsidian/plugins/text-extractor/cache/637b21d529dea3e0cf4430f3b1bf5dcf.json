{"path":"lit/lit_notes_OLD_PARTIAL/Zhu24genAIsecurityCounterMeas 2.pdf","text":"Generative AI Security: Challenges and Countermeasures Banghua Zhu 1, Norman Mu 1, Jiantao Jiao1, and David Wagner1 1University of California, Berkeley Abstract Generative AI’s expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks. 1 A Distinct Problem from Traditional Security Generative AI (GenAI) systems enable users to quickly generate high-quality content. Recent advances in Large Language Models (LLMs) (Radford et al., 2019; Chowdhery et al., 2022; Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023; Schulman et al., 2022; OpenAI, 2023; Anthropic, 2023), Vision Language Models (VLMs) (Radford et al., 2021; Liu et al., 2023a; Driess et al., 2023; Team, 2023) and diffusion models (Ramesh et al., 2021; Song et al., 2020; Yang et al., 2023) have revolutionized the capability of GenAI. GenAI models are designed to understand and generate content with a degree of autonomy that surpasses traditional machine learning systems, providing novel capabilities to generate text and code, interact with humans and Internet services, generate realistic images, and understand visual scenes. This capability enables a broader range of applications, and in this way introduces new security challenges unique to these novel GenAI-integrated applications. In this paper we discuss the challenges and opportunities for the field, starting in this section with the security risks, including how GenAI models might become a target of attack, a “fool” that unintentionally harms security, or a tool for bad actors to attack others. 1.1 Target: GenAI models are susceptible to attack While GenAI models have groundbreaking capabilities, they are also susceptible to adversarial attack and manipulation. Jailbreaking and prompt injection are two prominent threats to GenAI models and applications built using them. Jailbreaking is an emergent technique where adversaries use specially crafted prompts to manipulate AI models into generating harmful or misleading outputs (Chao et al., 2023; Wei et al., 2023; Liu et al., 2023d). This exploitation can lead to the AI system bypassing its own safety protocols or ethical guidelines. It is similar to how root access is obtained in smartphones, but in the context of AI, it involves circumventing the model’s restrictions to generate prohibited or unintended content. Prompt injection attacks insert malicious data or instructions into the model’s input stream, tricking the model into following the attacker’s instructions rather than the application developer’s instructions (Branch et al., 2022; Toyer et al., 2023; Liu et al., 2023c; Greshake et al., 2023a). This is analogous to SQL injection attacks in database systems, where an attacker can craft malicious data that, when incorporated by the application into a SQL query, is interpreted by the database as a new query. In the context of GenAI, prompt injection can leverage the model’s generative capabilities to produce outputs that deviate significantly from the intended functionality of the application. This becomes particularly concerning when GenAI-integrated applications interact with external tools, plugins, or software APIs, thereby amplifying the attack surface. Both jailbreaking and prompt injection attacks pose significant risks. Brand or reputational damage could occur if the AI model is tricked into generating offensive or embarrassing content. More alarmingly, 1arXiv:2402.12617v1 [cs.CR] 20 Feb 2024 Figure 1. An example of a prompt injection attack on Bing Chat 2. An injection prompt is hidden in the website content, leading to undesired behavior of the chat model. In this example, the attack causes the model to start outputting emojis, but it could also have more serious consequences, such as outputting disinformation or abusive content. when integrated into broader applications including search, tool-use, or even powering real-world robots, these vulnerabilities can lead to substantial security breaches. An illustrative example is shown in Figure 1, where Bing Chat, powered by GenAI models, becomes vulnerable to prompt injection attacks, leading to potential security compromises. Besides continual fine-tuning on known jailbreaking prompts, no good solutions to these risks are known yet. New research is needed to find strong defenses against these threats. We encourage those interested in jailbreaking to focus on evaluating use cases where jailbreaks result in tangible harm (to parties other than the person using the jailbreak), such as facilitating social engineering attacks, spreading disinformation, or creating malware. We advocate pragmatic threat models that acknowledge the impossibility of perfect security; sometimes, it suffices to increase the difficulty for attackers, adopting an “arms race” mentality rather than striving for an unattainable Fort Knox-like impregnability. Furthermore, we call for exploring a wider array of solutions for safety, including filtering training sets and detecting usage of jailbreaks to cause specific harms. Emphasis should also be placed on developing defenses, even if they are not perfect, prioritizing them over the creation of more sophisticated attacks. Until more effective defenses are discovered, we recommend continuous monitoring for anomalous behavior and not allowing GenAI models to take or cause high-sensitivity actions (e.g., spending money, disclosing sensitive information). Software companies might consider educating developers about these vulnerabilities to foster a security-aware culture in this burgeoning field. 1.2 Fool: Misplaced reliance on GenAI might lead to vulnerabilities In the previous section, we discuss how GenAI can be susceptible to attack from strong adversaries. However, vulnerabilities within GenAI systems may not solely stem from deliberate adversarial actions. In practical scenarios, it is equally plausible that non-adversarial or weakly adversarial behavior could inadvertently lead to vulnerabilities, especially when GenAI is misapplied in contexts for which it was not designed or adequately secured. The integration of AI in various domains, especially in generating or processing sensitive data, might inadvertently lead to new vulnerabilities, if GenAI models generate insecure code or leak sensitive data. 2See the link here for the full content. 2 Data Leakage Risks. GenAI models are not good at keeping secrets. GenAI models that are trained on proprietary or sensitive data might inadvertently reveal this sensitive information, either directly or indirectly (Gupta et al., 2023; Wu et al., 2023; ThankGod Chinonso, 2023; Sebastian, 2023). This can include the leakage of personally identifiable information (PII), confidential business information, or access tokens. Additionally, one could input a dataset containing sensitive information into the prompt, with the expectation that the model will generate only aggregated statistics and summaries. However, existing LLMs may occasionally reveal private information, even in infrequent instances. Such occurrences are particularly concerning in domains where privacy is paramount, such as healthcare and finance. The complexity and unpredictability of these models make it difficult to proactively determine under what conditions a model trained on sensitive data might reveal that data. The exponentially large space of potential user prompts renders it nearly impossible to anticipate and prevent all forms of data leakage. For instance, subtle patterns in the training data might be unintentionally revealed when the model is prompted in specific, unanticipated ways. Malicious attacks can also extract the training data of GenAI models. This type of security breach represents a significant threat as it can compromise the confidentiality of the data used to train these advanced systems (Nasr et al., 2023). To mitigate data leakage risks, we suggest a crude heuristic: if a GenAI model is trained on private or secret data, then assume the model can be induced to reveal that data in its outputs. Thus, it is best to avoid training or fine-tuning on sensitive data, perhaps by masking out or redacting sensitive data before training. It is also an interesting research direction to develop monitoring systems to detect inadvertent data exposures. Generation of Insecure Code. While GenAI tools like Microsoft CoPilot and ChatGPT have become increasingly popular for code generation and revision, their reliability is still under scrutiny. Recent studies show that code generated by these AI models can contain security vulnerabilities (Fu et al., 2023). These vulnerabilities range from simple syntactic errors to complex logical flaws that could be exploited. Developers, enticed by the ease of use of these tools, might inadvertently introduce these flaws into their codebases. However, there is also emerging research suggesting the potential for GenAI to aid in developing more secure code (Asare et al., 2023). This research highlights the potential risks of utilizing GenAI in code development but also the opportunities to improve software security. To mitigate code generation risks, further research is needed on how to ensure generated code is secure, perhaps by improving the ability of models to recognize whether the code they generate has security problems, or by new prompting strategies to teach these models secure coding practices. Until then, educating developers on the potential pitfalls of GenAI-generated code and fostering a security-aware culture in software development seems prudent. 1.3 Tool: GenAI models could be used by threat actors GenAI tools could also be abused by bad actors for malicious purposes. Bad actors might use GenAI to create malicious code or harmful content, posing a significant threat to digital security systems (Glukhov et al., 2023; Bommasani et al., 2021). The capabilities of GenAI might be repurposed to enhance or automate traditional cyber-attacks. This includes, but is not limited to: • Crafting sophisticated phishing emails, including automating the process of creating individually targeted spear phishing messages (Renaud et al., 2023; Alawida et al., 2023). • Generating fake images or video clips for misinformation campaigns or for scams (Zhang et al., 2019), where a video call that appears to be from a known contact might be persuasive. • Producing malicious code capable of attacking online systems (Monje et al., 2023; Pa Pa et al., 2023). • Generating prompts that exploit GenAI systems to ‘jailbreak’ or bypass their own security protocols (Ganguli et al., 2022; Chao et al., 2023). 3 The evolving nature of GenAI systems necessitates a proactive approach in cybersecurity and governance. It is imperative to develop robust frameworks to mitigate these risks, ensuring that the advancement in AI technology is aligned with ethical standards and security protocols to prevent misuse. In conclusion, while GenAI offers substantial benefits in automating and enhancing various tasks, its potential to introduce new vulnerabilities necessitates a cautious and well-informed approach to its deployment and usage in sensitive domains. 2 Existing Approaches Fall Short Many of the challenges and research directions we propose studying are familiar to the fields of both machine learning and computer security. However, a number of key practical differences between GenAI systems and existing AI and computer systems require new approaches in order to make progress towards concrete security outcomes. 2.1 GenAI vs. ML We distinguish between modern GenAI systems such as LLMs, and previous AI systems such as machine translation and object detectors. Previous systems were typically built for a single task and domain, whereas GenAI systems offer broad, general capabilities across many domains (e.g., LLMs are fluent in conversation, prose, technical reports, code) and sometimes even multiple modalities like audio and video. A recent report from DeepMind draws a distinction between “general” and “narrow” AI, and considers LLMs the first forms of “general” AI Morris et al. (2023). GenAI systems also exhibit a variety of security-relevant differences, including: • Emergent threat vectors: unexpected GenAI capabilities can create unforeseen threat vectors. • Expanded attack surfaces: reliance on huge user-generated datasets for training and inference exposes a much larger attack surface. • Deep integrations: unmediated connections with other computer systems paint a bigger target for attackers. • Economic value: valuable GenAI-powered applications pose a more lucrative target for attackers. Emergent threat vectors. Many useful and significant capabilities in GenAI systems develop during the course of training without any intentional human design (Wei et al., 2022a). For instance, the abilities of LLMs to learn new tasks “in-context” from a handful of demonstrations (Brown et al., 2020), or perform “chain-of-thought” reasoning (Wei et al., 2022b) were only discovered after training at a large enough scale. Threat actors may be able to leverage undocumented “zero-day” capabilities within GenAI systems to execute attacks. Unexpected capabilities in LLMs may also enable the use of known existing capabilities in attacks, analogous to how the PDF specification, in allowing users to embed code, enables its use as a malware delivery mechanism. The difficulty of enumerating all the capabilities of a GenAI system makes it much harder to anticipate potential threat vectors. Expanded attack surfaces. GenAI systems are trained on massive amounts of user-generated content, collected through various means such as large-scale webscraping, crowdsourcing, or licensing digital archives. In the context of LLM training, user-generated data comes in the form of pretraining documents, supervised task demonstrations, feedback data, all of which are susceptible to adversarial manipulation such as data poisoning attacks to insert hidden backdoor functionality into models (Carlini et al., 2023; Rando and Tram`er, 2023). At inference time, systems such as chat assistants, retrieval-augmented generation (RAG), and “web agents” all rely on additional untrusted data such as user messages, reference documents, and website responses. This poses an opportunity for the use of adversarial inputs to hijack system objectives, such as through (indirect) prompt injection attacks (Perez and Ribeiro, 2022; Greshake et al., 2023b). Maintaining up-to-date world knowledge also requires continually training on new data, 4 which turns all of these risks into a persistent threat. Validating all of these input sources is a daunting undertaking, warranting the study and development of new techniques to handle the sheer scale of data at hand. Deep integrations. A now common design pattern for GenAI systems is to connect previously un- interoperable software systems, such as mobile apps3, or to leverage external tools (Schick et al., 2023). The latter use case is generalized by OpenAI’s custom GPTs4 which enables ChatGPT to call arbitrary user-defined APIs and take real-world actions. Researchers have also started to explore the use of GenAI models in robotic systems, paving the way to household robots driven by natural language input (Driess et al., 2023). GenAI systems are already being deeply integrated into many facets of consumer technology, such as email and digital banking, as well as enterprise technology, such as customer support5 and code review 6. In all these instances, the model is given unmediated access to its connected systems, making it a prime target for attackers seeking to access these systems. Economic value. Application areas such as healthcare, customer service, and software engineering have drawn lots of investment attention, as successfully automating or extending human workers has the potential to generate tremendous amounts of economic value. The high price of inference also push the usage of GenAI toward more valuable tasks that can bear the additional cost. Many first deployments of initial and particularly insecure GenAI systems will therefore be concentrated in economically valuable domains, as compared with prior ML systems. This means that the costs of a successful attack are much higher, and that GenAI systems are likely to draw much more attention from malicious actors. Securing GenAI systems poses greater challenges and carries higher stakes than prior ML systems. Model providers, application developers, and end users will all need to consider security more seriously and systematically. 2.2 GenAI vs. security In traditional computer systems, a variety of different techniques have been developed to defend against common patterns of attacks and system vulnerabilities. Techniques such as access control, firewalls, sandboxing, and malware detection have found success and enjoy broad usage in practice. Generally, security techniques rely on the assumption that systems are modular and highly predictable: individual components can be easily replaced, and their effect on overall system behavior can be precisely characterized. In the setting of GenAI systems, attacks will appear much more like social engineering attacks against human organizations, rather than highly targeted technical exploits. So while some high-level principles may transfer over, many existing tools from computer security are not suitable for direct application to GenAI. Effective defenses will need to leverage machine learning as a core tool, while robustly handling the brittleness of the underlying GenAI and ML systems. Access control. Access control restricts users and programs so they can only access resources (e.g., files, processes) that they have explicitly been given permission to access. We envision that LLM-integrated applications might control access to confidential or critical data by using access control to limit which data entries can be accessed by Retrieval Augmented Generation (RAG) systems, or to limit which tools/APIs the LLM can invoke, depending on the user who invoked the application. However, the open-ended nature of user requests to LLM assistants makes it difficult to decide in advance all the data and tools that will be required to complete a task, so we expect it will typically be difficult or impossible to limit what data the LLM can access or what actions it can take. Rule-based blocking. Traditionally, rule-based filtering methods have been used as a first line of defense against undesirable outputs. A natural idea is to do the same with GenAI, scanning the AI’s inputs and outputs, and prevent the display of any content that meets certain predefined criteria. However, 3See link here for an example of mobile app, Rabbit R1. 4See link here for data leakage issues from GPTs. 5See the link here for an example of customer support. 6See the link here for an example of code review. 5 the complexity of GenAI prompts and opportunities for obfuscation mean that relying exclusively on rules to filter harmful content is likely to result in numerous false positives and negatives. Malicious actors can also find ways around these rule-based systems, rendering them inadequate for ensuring AI safety. Consequently, relying solely on basic rule-based filtering methods to safeguard a sophisticated intelligence system like GPT-4 is impractical and insufficient. Figure 2 shows a few examples of non-trivial jailbreaking prompts that are challenging to detect with simple rules. Consequently, we expect filtering defenses will need to have some intelligence to be effective. Figure 2: Rule-based defenses can be easily defeated. Sandboxing. Sandboxing is the practice of executing programs in isolation, which prevents malicious software from harming other system functions. Software with complex integrations such as Adobe Flash are difficult to sandbox without limiting functionality. Similarly, GenAI systems like ChatGPT are often connected to a variety of powerful plugins such as web browsing or other live APIs and not amenable to airtight isolation. Antivirus and blacklisting. Antivirus software constantly scans files and programs for malware, relying on known identifying characteristics, in order to promptly isolate or remove the suspected data. Unfortunately, we do not expect such approaches to be very effective at protecting GenAI, because there are simply too many ways for an attacker to phrase an attack and too many ways to obfuscate attacks. For example, many jailbreak attacks remain effective even if they are translated to a different language or typographically degraded. Parameterized queries. Parameterized queries can effectively defend against SQL injection attacks by restricting user control of a command to just the data fields. Using parameterized queries requires developers to precisely delineate between code and data, which is not always feasible with inputs to LLMs where “code” must be inferred from “data” in the case of few-shot prompting/in-context learning. Parameterized queries also limit program functionality to only the set of queries for which templates have been pre-defined, which would negate the flexibility of LLM applications. Software patching. For many applications, security engineers may reasonably rely on users to regularly install software updates through which patches to newly discovered vulnerabilities may be applied. The monolithic nature of LLMs makes it hard to develop localized fixes once vulnerabilities are discovered, since different knowledge and capabilities may be entangled within the weights of the model, and editing a specific behavior without affecting any other behaviors can be difficult. Proprietary LLMs served via API do not have to worry about users running outdated versions, but with open models it may be just as difficult to get users to update models as it has been with traditional software. 6 Encryption. Encryption is used to protect sensitive information and ensure data privacy. However, the challenge of data obfuscation for GenAI is the difficulty in pinpointing and defining which data is ‘sensitive’ (Narayanan and Shmatikov, 2010). Furthermore, the interdependencies in data sets mean that even if certain pieces of information are obfuscated, other, seemingly benign data points might provide enough context for an AI to infer the missing data (Narayanan et al., 2016; Narayanan, 2008). Rely on vendors. While proprietary companies like OpenAI and Anthropic lead in pioneering AI safety, expecting it to be a panacea for every GenAI security issue is optimistic. Current models use reinforcement learning with human feedback (RLHF) to align model outputs with universal human values (Schulman et al., 2022; Ouyang et al., 2022). However, universal values may not be sufficient: each application of GenAI is likely to have its own application-specific security requirements. It is not realistic to expect vendors to be able to anticipate and address application-specific issues; developers building GenAI-enabled applications will need to take responsibility for the security of their applications. 3 Potential Research Directions New approaches to security are needed, to address the new issues associated with GenAI. We discuss several potential research directions to combat the security challenges posed by GenAI, and call on the research community to develop novel solutions. 3.1 AI Firewall We suggest researchers study how to build an “AI firewall”, which protects a black-box GenAI model by monitoring and possibly transforming its inputs and outputs. An AI firewall might monitor inputs to detect possible jailbreak attacks; it is an interesting research question how to use continuous learning to detect new jailbreak prompts. Additionally, the system could be stateful, analyzing a sequence of inputs from a particular user to determine if they might indicate malicious intent. An AI firewall might also monitor outputs to check if they violate security policies (e.g., contain toxic/offensive/inappropriate content), perhaps using a suitable content moderation model (Phute et al., 2023; Markov et al., 2023). The work of Wei et al. (2023) advocates for employing a detection and moderation model that matches the sophistication and capabilities of the model it aims to protect. A less capable model for moderation can be susceptible to obfuscation techniques, especially if they rely on a enumeration-based blacklist policy for filtering which may fail to recognize complex or subtly crafted inputs designed to bypass its restrictions. On the other hand, there’s a growing interest in the possibility of using smaller models for moderation purposes. This remains an open research question: Can moderation be effectively and safely conducted with smaller models? This question is also closely related to the problem of superalignment, where we hope to align a model that is much more intelligent using a model that is less capable. An intriguing example of a strong moderation model in practice is the relationship between DALL-E 3 and ChatGPT (Betker et al., 2023). In this setup, the user sends instructions to ChatGPT, which crafts prompts for DALL-E 3 to generate images. ChatGPT acts as an AI firewall, providing a moderation model that enforces policy on the types of images DALL-E 3 can generate. Notably, ChatGPT’s superior language understanding capabilities compared to DALL-E 3 play a crucial role in preventing attacks that might exploit DALL-E 3’s vulnerability to unsafe prompts. Another example of AI moderation in action is the content filtering in Azure AI services 7, and possibly input filtering as well, which is widely applied in applications like Bing Chat. This approach demonstrates how AI systems are increasingly being equipped with mechanisms to monitor and control the content they generate or interact with, ensuring adherence to set guidelines and preventing misuse. Finally, an AI firewall might impose limits or access control on the model’s ability to invoke tools or take actions. It is an open problem how to design a suitable access control system, perhaps based on second model that analyzes the query to determine what limits are appropriate and obtain consent from the user when needed (Felt et al., 2012; Iqbal et al., 2023). 7See the link here for descriptions of content filtering in Azure. 7 Figure 3: An AI Firewall, built by apply a moderation model to LLM inputs and outputs. 3.2 Integrated Firewall Gaining access to a GenAI model’s weights opens up enhanced opportunities for defense, allowing for more effective detection of attacks. We discuss two potential research directions: Internal State Monitoring: One approach involves the surveillance of the model’s internal states. Certain neurons or neuron clusters within the language model might be correlated with the generation of hallucinatory or unethical outputs (Azaria and Mitchell, 2023; Rateike et al., 2023). By monitoring these specific neurons, it might be possible to detect and mitigate undesirable model behaviors early in the response generation process. Safety Fine-Tuning: Open-source GenAI models could be fine-tuned against known malicious prompts and behaviors with either supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF) (Radford et al., 2019; Stiennon et al., 2020; Ziegler et al., 2019; Ouyang et al., 2022; Schulman et al., 2022; Ivison et al., 2023; Wang et al., 2023; Lv et al., 2023; Zhu et al., 2023c,b,a; Bai et al., 2022; Christiano et al., 2023). This method is akin to equipping a person with self-defense skills, enhancing the model’s inherent ability to recognize and counteract harmful inputs. Training the model on a dataset of known threats would enable it to learn and adapt its responses to minimize risks. Figure 4: An integrated firewall can use visibility into the model to detect more attacks. Combining an AI firewall and integrated firewall might be stronger than either alone, since direct integration with the AI model’s intelligence promises superior efficiency and efficacy in countering threats, aligning with the safety-capability criterion discussed in Wei et al. (2023). 3.3 Guardrails We also identify another important research challenge: is it possible to enforce “guardrails”, i.e., application- specific restrictions or policies, on the output of a LLM? We envision a scenario where we have black-box access to an off-the-shelf LLM (e.g., GPT4 or Claude), and an application-specific guardrail (e.g., “Only talk about our company’s products. Do not discuss other company’s products, religion, or politics.”). The challenge then is to steer the output of the LLM at test time, to produce outputs that obey the guardrail. One simple yet effective method is rejection sampling, or best-of-K sampling (Liu et al., 2023b; Stiennon et al., 2020; Gao et al., 2023): run the LLM 10 times on the same prompt, to generate 10 outputs, use a second model to score how well each follows the guardrail, and then keep the output with the highest 8 score. Rejection sampling is effective, but it is computationally expensive, increasing the test-time costs by an order of magnitude. Can we achieve similar effectiveness at enforcing guardrails, at significantly lower cost? Some promising attempts along this direction include controlled decoding (Yang and Klein, 2021; Mudgal et al., 2023; Qin et al., 2022), which adds bias in the logits of LLM during decoding process. 3.4 Watermarking and Content Detection Differentiating between human-generated and machine-generated content is critical in contexts such as plagiarism, data contamination, and misinformation propagation. Recent research has concentrated on two approaches: training a classifier to distinguish between human-generated and machine-generated content, or embedding hidden signals in LLMs with watermarks (Venugopal et al., 2011; Aaronson, 2022; Kirchenbauer et al., 2023; Kuditipudi et al., 2023; Christ et al., 2023; Zhao et al., 2023; Huang et al., 2023). These watermarks facilitate the identification of their machine origin. We envision that the classification-based methods may not be worth future research efforts, as new GenAI models are likely to be harder to recognize. Also, classification-based approaches are highly sensitive to the distribution of model outputs, which can vary significantly, presenting a moving target in content detection. Furthermore, the classification-based approach can have biases for non-English content or rarely-seen samples during the training. Therefore, watermarking might be a more promising direction than classification-based methods. We envision several potential future research directions. • Watermarking Open Source Models: It is unclear how to watermark open-source models, since it is easy for an attacker to remove any watermark-specific code or modify the weights and decoding method. Without a practical way to watermark open-source models, it is very easy to use open-source models for rephrasing watermarked content generated by closed-source models. • Watermarking Human-Generated Content: Image authentication methods (Lu and Liao, 2000; Kutter et al., 1997; Sinha and Singh, 2003), such as signing photos taken by digital cameras, hint at the possibility of developing special watermarks for human-authored content. This approach could offer an alternative or complementary method to distinguish between human- and machine- generated content, adding another layer to content authentication processes. Such a dual approach, focusing on both machine and human content verification, could significantly enhance the robustness of content verification systems. • Cross-Model Coordination: Ensuring that watermarking mechanisms are effective across different models and generations of AI technologies is crucial. This requires a unified watermarking method that is acceptable for all model providers. 3.5 Regulations Enforcement Policies and regulations can potentially play a role in mitigating risks associated with misuse of GenAI. Researchers can have influence by making realistic predictions on the development and effect of GenAI, and proposing a range of policy options for policymakers. We suggest several considerations for policymakers: • Regulation of Proprietary and Open Source Models: Drawing insights from the “crypto wars” (Taskinsoy, 2019; Jarvis, 2020), we know that overly stringent national regulations can be counterproductive, potentially stifling innovation and slowing adoption of beneficial technology rather than mitigating the adverse impacts of emerging technologies. This observation is particularly pertinent in the context of proprietary and open-source models in the field of Generative AI (GenAI). Proprietary models may be easier to regulate, as there are only a few companies that would need to be controlled, but depend largely on the responsible and ethical practices of those companies. Open models permit unregulated use and uncontrolled modifications, but foster rapid innovation and support research on improving AI safety. Policy should take into account the challenges and benefits presented by each type of model, aiming to strike a balance between fostering innovation and ensuring security. 9 • Government Licensing of LLM Companies: One approach might be government licensing of companies developing large language models (LLMs). This could establish a structured framework for accountability, oversight, and ethical compliance, thereby enhancing the trustworthiness of GenAI systems. • Dynamic Policy Evolution: Given the rapid advancement of GenAI technology, policies and regulation will require regular updates, to adapt to new technological realities and challenges. 3.6 Evolving Threat Management GenAI threats, like all technology threats, aren’t stagnant. We face a cat and mouse game where for every defensive move, attackers design a counter-move. Thus, security systems need to be ever-evolving, learning from past breaches and anticipating future strategies. Just as with adversarial examples for computer vision (Tramer et al., 2020), there is no universal protection for prompt injection, jailbreaks, or other attacks, so for now, one pragmatic defense might be to monitor and detect threats. Developers will need tools to monitor, detect, and respond to attacks on GenAI, and a threat intelligence strategy to track new emerging threats. Society has had thousands of years to come up with ways to protect against scammers; GenAIs have only been around for several years, so we’re still figuring out how to defend them. Predicting the exact nature of future AI threats is challenging. Researchers are actively investigating new countermeasures to defend against threats on GenAI. Therefore, we recommend developers design systems in a way that preserves flexibility for the future, so that new defenses can be slotted in as they are discovered. 10 Acknowledgements This research was supported by the National Science Foundation under grants 2229876 (the ACTION center) and 2154873, a NSF Graduate Fellowship, OpenAI, C3.ai DTI, Open Philanthropy, Google, the Department of Homeland Security, and IBM. Broader Impacts Our work explores the threats society and technologists might face. We believe it is in the public interest to understand future risks, so that the research community can begin developing novel methods to mitigate those risks. We also sketch a roadmap of directions for future research that we believe holds promise for addressing a number of these risks. 11 References S. Aaronson. My AI safety lecture for UT Effective Altruism. Shtetl-Optimized: The blog of Scott Aaronson. Retrieved on September, 11:2023, 2022. URL https://scottaaronson.blog/?p=6823. M. Alawida, B. A. Shawar, O. I. Abiodun, A. Mehmood, A. E. Omolara, et al. Unveiling the Dark Side of ChatGPT: Exploring Cyberattacks and Enhancing User Awareness. Information, 15, 2023. Anthropic. Model Card and Evaluations for Claude Models, 2023. URL https://www-files.anthropic. com/production/images/Model-Card-Claude-2.pdf. Accessed: Sep. 27, 2023. O. Asare, M. Nagappan, and N. Asokan. Is Github’s Copilot as bad as humans at introducing vulnerabilities in code? Empirical Software Engineering, 28(6):1–24, 2023. A. Azaria and T. Mitchell. The Internal State of an LLM Knows When It’s Lying, 2023. arXiv:2304.13734. Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2023. R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. H. J. Branch, J. R. Cefalu, J. McHugh, L. Hujer, A. Bahl, D. d. C. Iglesias, R. Heichman, and R. Darwishi. Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples. arXiv preprint arXiv:2209.02128, 2022. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas, and F. Tram`er. Poisoning web-scale training datasets is practical. arXiv preprint arXiv:2302.10149, 2023. P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. M. Christ, S. Gunn, and O. Zamir. Undetectable watermarks for language models. arXiv preprint arXiv:2306.09194, 2023. P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences, 2023. arXiv:1706.03741. D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 12 A. P. Felt, S. Egelman, M. Finifter, D. Akhawe, and D. Wagner. How to ask for permission. In HotSec 2012, 2012. Y. Fu, P. Liang, A. Tahir, Z. Li, M. Shahin, and J. Yu. Security weaknesses of copilot generated code in github. arXiv preprint arXiv:2310.02059, 2023. D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835–10866. PMLR, 2023. D. Glukhov, I. Shumailov, Y. Gal, N. Papernot, and V. Papyan. LLM Censorship: A Machine Learning Challenge or a Computer Security Problem? arXiv preprint arXiv:2307.10719, 2023. K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz. More than you’ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv e-prints, pages arXiv–2302, 2023a. K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, pages 79–90, 2023b. M. Gupta, C. Akiri, K. Aryal, E. Parker, and L. Praharaj. From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. IEEE Access, 2023. B. Huang, B. Zhu, H. Zhu, J. D. Lee, J. Jiao, and M. I. Jordan. Towards optimal statistical watermarking. arXiv preprint arXiv:2312.07930, 2023. U. Iqbal, T. Kohno, and F. Roesner. LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI’s ChatGPT Plugins, 2023. arXiv:2309.10254. H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, and H. Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023. C. Jarvis. Crypto wars: the fight for privacy in the digital age: A political history of digital encryption. CRC Press, 2020. J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein. A watermark for large language models. arXiv preprint arXiv:2301.10226, 2023. R. Kuditipudi, J. Thickstun, T. Hashimoto, and P. Liang. Robust distortion-free watermarks for language models. arXiv preprint arXiv:2307.15593, 2023. M. Kutter, F. D. Jordan, and F. Bossen. Digital signature of color images using amplitude modulation. In Storage and Retrieval for Image and Video Databases V, volume 3022, pages 518–526. SPIE, 1997. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a. T. Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023b. Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu. Prompt Injection attack against LLM-integrated Applications. arXiv preprint arXiv:2306.05499, 2023c. Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang, and Y. Liu. Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study, 2023d. arXiv:2305.13860. C.-S. Lu and H.-Y. M. Liao. Structural digital signature for image authentication: an incidental distortion resistant scheme. In Proceedings of the 2000 ACM workshops on Multimedia, pages 115–118, 2000. 13 K. Lv, W. Zhang, and H. Shen. Supervised fine-tuning and direct preference optimization on intel gaudi2 — by intel(r) neural compressor — intel analytics software — nov, 2023 — medium. https://medium.com/intel-analytics-software/ the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3, 2023. (Accessed on 01/12/2024). T. Markov, C. Zhang, S. Agarwal, F. E. Nekoul, T. Lee, S. Adler, A. Jiang, and L. Weng. A holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 15009–15018, 2023. A. Monje, A. Monje, R. A. Hallman, and G. Cybenko. Being a bad influence on the kids: Malware generation in less than five minutes using ChatGPT, 2023. M. R. Morris, J. Sohl-dickstein, N. Fiedel, T. Warkentin, A. Dafoe, A. Faust, C. Farabet, and S. Legg. “Levels of AGI”: Operationalizing Progress on the Path to AGI, 2023. arXiv:2311.02462. S. Mudgal, J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H.-T. Cheng, M. Collins, T. Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. A. Narayanan. Lendingclub.com: A de-anonymization walkthrough, 2008. https://33bits.wordpress.com/2008/11/12/57/. A. Narayanan and V. Shmatikov. Myths and fallacies of” personally identifiable information”. Communications of the ACM, 53(6):24–26, 2010. A. Narayanan, J. Huey, and E. W. Felten. A precautionary approach to big data privacy. Data protection on the move: Current developments in ICT and privacy/data protection, pages 357–385, 2016. M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tram`er, and K. Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023. OpenAI. Gpt-4 technical report, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022. Y. M. Pa Pa, S. Tanizaki, T. Kou, M. Van Eeten, K. Yoshioka, and T. Matsumoto. An Attacker’s Dream? Exploring the Capabilities of ChatGPT for Developing Malware. In Proceedings of the 16th Cyber Security Experimentation and Test Workshop, pages 10–18, 2023. F. Perez and I. Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022. M. Phute, A. Helbling, M. Hull, S. Peng, S. Szyller, C. Cornelius, and D. H. Chau. LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked, 2023. arXiv:2308.07308. L. Qin, S. Welleck, D. Khashabi, and Y. Choi. Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:9538–9551, 2022. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR, 2021. 14 J. Rando and F. Tram`er. Universal jailbreak backdoors from poisoned human feedback. arXiv preprint arXiv:2311.14455, 2023. M. Rateike, C. Cintas, J. Wamburu, T. Akumu, and S. Speakman. Weakly Supervised Detection of Hallucinations in LLM Activations. arXiv preprint arXiv:2312.02798, 2023. K. Renaud, M. Warkentin, and G. Westerman. From ChatGPT to HackGPT: Meeting the Cybersecurity Threat of Generative AI. MIT Sloan Management Review, 2023. T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. J. Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny, et al. ChatGPT: Optimizing language models for dialogue. OpenAI blog, 2022. G. Sebastian. Privacy and Data Protection in ChatGPT and Other AI Chatbots: Strategies for Securing User Information, 2023. SSRN 4454761. A. Sinha and K. Singh. A technique for image encryption using digital signature. Optics communications, 218(4-6):229–234, 2003. Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 3008–3021, 2020. J. Taskinsoy. Facebook’s libra: Why does us government fear price stable cryptocurrency? Available at SSRN 3482441, 2019. O. Team. GPT-4V(ision) System Card, 2023. E. ThankGod Chinonso. The impact of ChatGPT on privacy and data protection laws, 2023. SSRN 4574016. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. S. Toyer, O. Watkins, E. A. Mendes, J. Svegliato, L. Bailey, T. Wang, I. Ong, K. Elmaaroufi, P. Abbeel, T. Darrell, et al. Tensor trust: Interpretable prompt injection attacks from an online game. arXiv preprint arXiv:2311.01011, 2023. F. Tramer, N. Carlini, W. Brendel, and A. Madry. On adaptive attacks to adversarial example defenses. Advances in neural information processing systems, 33:1633–1645, 2020. A. Venugopal, J. Uszkoreit, D. Talbot, F. J. Och, and J. Ganitkevitch. Watermarking the outputs of structured prediction with an application in statistical machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1363–1372, 2011. G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, and Y. Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023. A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a. 15 J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022b. X. Wu, R. Duan, and J. Ni. Unveiling security, privacy, and ethical concerns of ChatGPT. Journal of Information and Intelligence, 2023. K. Yang and D. Klein. Fudge: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218, 2021. L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1–39, 2023. X. Zhang, S. Karaman, and S.-F. Chang. Detecting and simulating artifacts in gan fake images. In 2019 IEEE international workshop on information forensics and security (WIFS), pages 1–6. IEEE, 2019. X. Zhao, P. Ananth, L. Li, and Y.-X. Wang. Provable robust watermarking for ai-generated text. arXiv preprint arXiv:2306.17439, 2023. B. Zhu, E. Frick, T. Wu, H. Zhu, and J. Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, 2023a. B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270, 2023b. B. Zhu, H. Sharma, F. V. Frujeri, S. Dong, C. Zhu, M. I. Jordan, and J. Jiao. Fine-tuning language models with advantage-induced policy alignment. arXiv preprint arXiv:2306.02231, 2023c. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 16","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_notes_OLD_PARTIAL/Muennighoff23MTEBMassiveText.pdf","text":"MTEB: Massive Text Embedding Benchmark Niklas Muennighoff 1, Nouamane Tazi 1, Loïc Magne 1, Nils Reimers 2* 1Hugging Face 2cohere.ai 1firstname@hf.co 2info@nils-reimers.de Abstract Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art em- beddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the ﬁeld difﬁcult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we intro- duce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks cov- ering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We ﬁnd that no particular text embedding method dominates across all tasks. This suggests that the ﬁeld has yet to converge on a universal text embedding method and scale it up sufﬁciently to provide state-of-the-art results on all embed- ding tasks. MTEB comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb. 1 Introduction Natural language embeddings power a variety of use cases from clustering and topic representa- tion (Aggarwal and Zhai, 2012; Angelov, 2020) to search systems and text mining (Huang et al., 2020; Zhu et al., 2021; Nayak, 2019) to feature representations for downstream models (Saharia et al., 2022; Borgeaud et al., 2022). Using gener- ative language models or cross-encoders for these applications is often intractable, as they may re- quire exponentially more computations (Reimers and Gurevych, 2019). However, the evaluation regime of current text embedding models rarely covers the breadth of *Most of the work done while at Hugging Face. Corre- spondence to n.muennighoff@gmail.com. their possible use cases. For example, Sim- CSE (Gao et al., 2021b) or SBERT (Reimers and Gurevych, 2019) solely evaluate on STS and clas- siﬁcation tasks, leaving open questions about the transferability of the embedding models to search or clustering tasks. STS is known to poorly corre- late with other real-world use cases (Neelakantan et al., 2022; Wang et al., 2021). Further, evaluating embedding methods on many tasks requires imple- menting multiple evaluation pipelines. Implemen- tation details like pre-processing or hyperparam- eters may inﬂuence the results making it unclear whether performance improvements simply come from a favorable evaluation pipeline. This leads to the “blind” application of these models to new use cases in industry or requires incremental work to reevaluate them on different tasks. The Massive Text Embedding Benchmark (MTEB) aims to provide clarity on how models perform on a variety of embedding tasks and thus serves as the gateway to ﬁnding universal text em- beddings applicable to a variety of tasks. MTEB consists of 58 datasets covering 112 languages from 8 embedding tasks: Bitext mining, classi- ﬁcation, clustering, pair classiﬁcation, reranking, retrieval, STS and summarization. MTEB software is available open-source1 enabling evaluation of any embedding model by adding less than 10 lines of code. Datasets and the MTEB leaderboard are available on the Hugging Face Hub2. We evaluate over 30 models on MTEB with addi- tional speed and memory benchmarking to provide a holistic view of the state of text embedding mod- els. We cover both models available open-source as well as models accessible via APIs, such as the OpenAI Embeddings endpoint. We ﬁnd there to be no single best solution, with different models dom- 1https://github.com/embeddings-benchm ark/mteb 2https://huggingface.co/spaces/mteb/l eaderboardarXiv:2210.07316v3 [cs.CL] 19 Mar 2023 inating different tasks. Our benchmarking sheds light on the weaknesses and strengths of individual models, such as SimCSE’s (Gao et al., 2021b) low performance on clustering and retrieval despite its strong performance on STS. We hope our work makes selecting the right embedding model easier and simpliﬁes future embedding research. 2 Related Work 2.1 Benchmarks Benchmarks, such as (Super)GLUE (Wang et al., 2018, 2019) or Big-BENCH (Srivastava et al., 2022), and evaluation frameworks (Gao et al., 2021a) play a key role in driving NLP progress. Yearly released SemEval datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016) are commonly used as the go-to benchmark for text embeddings. Se- mEval datasets correspond to the task of semantic textual similarity (STS) requiring models to embed similar sentences with geometrically close embed- dings. Due to the limited expressivity of a single Se- mEval dataset, SentEval (Conneau and Kiela, 2018) aggregates multiple STS datasets. SentEval focuses on ﬁne-tuning classiﬁers on top of embeddings. It lacks tasks like retrieval or clustering, where em- beddings are directly compared without additional classiﬁers. Further, the toolkit was proposed in 2018 and thus does not provide easy support for recent trends like text embeddings from transform- ers (Reimers and Gurevych, 2019). Due to the insufﬁciency of STS benchmarking, USEB (Wang et al., 2021) was introduced consisting mostly of reranking tasks. Consequently, it does not cover tasks like retrieval or classiﬁcation. Meanwhile, the recently released BEIR Benchmark (Thakur et al., 2021) has become the standard for the evaluation of embeddings for zero-shot information retrieval. MTEB uniﬁes datasets from different embed- ding tasks into a common, accessible evaluation framework. MTEB incorporates SemEval datasets (STS11 - STS22) and BEIR alongside a variety of other datasets from various tasks to provide a holis- tic performance review of text embedding models. 2.2 Embedding Models Text embedding models like Glove (Pennington et al., 2014) lack context awareness and are thus commonly labeled as Word Embedding Models. They consist of a layer mapping each input word to a vector often followed by an averaging layer to provide a ﬁnal embedding invariant of input length. Transformers (Vaswani et al., 2017) inject context awareness into language models via self-attention and form the foundation of most recent embed- ding models. BERT (Devlin et al., 2018) uses the transformer architecture and performs large-scale self-supervised pre-training. The resulting model can directly be used to produce text embeddings via an averaging operation alike Glove. Build- ing on InferSent (Conneau et al., 2017), SBERT (Reimers and Gurevych, 2019) demonstrated it to be beneﬁcial to perform additional ﬁne-tuning of the transformer for competitive embedding perfor- mance. Most recent ﬁne-tuned embedding models use a contrastive loss objective to perform super- vised ﬁne-tuning on positive and negative text pairs (Gao et al., 2021b; Wang et al., 2021; Ni et al., 2021b; Muennighoff, 2022). Due to the large va- riety of available pre-trained transformers (Wolf et al., 2020), there is an at least equally large va- riety of potential text embedding models to be ex- plored. This leads to confusion about which model provides practitioners with the best performance for their embedding use case. We benchmark both word embedding and trans- former models on MTEB quantifying gains pro- vided by often much slower context aware models. 3 The MTEB Benchmark 3.1 Desiderata MTEB is built on a set of desiderata: (a) Diversity: MTEB aims to provide an understanding of the usability of embedding models in various use cases. The benchmark comprises 8 different tasks, with up to 15 datasets each. Of the 58 total datasets in MTEB, 10 are multilingual, covering 112 differ- ent languages. Sentence-level and paragraph-level datasets are included to contrast performance on short and long texts. (b) Simplicity: MTEB pro- vides a simple API for plugging in any model that given a list of texts can produce a vector for each list item with a consistent shape. This makes it possible to benchmark a diverse set of models. (c) Extensibility: New datasets for existing tasks can be benchmarked in MTEB via a single ﬁle that speciﬁes the task and a Hugging Face dataset name where the data has been uploaded (Lhoest et al., 2021). New tasks require implementing a task in- terface for loading the data and an evaluator for benchmarking. We welcome dataset, task or metric contributions from the community via pull requests to continue the development of MTEB. (d) Repro- MTEB 8 Tasks 58 Datasets Massive Text Embedding Benchmark Classification AmazonCounterfactual Retrieval Pair Classification AmazonPolarity AmazonReviews Banking77 Emotion Imdb MassiveIntent MassiveScenario MTOPDomain MTOPIntent ToxicConversations TweetSentimentExtraction SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus Clustering ArxivP2P ArxivS2S STS BIOSESS SICK-R STS11 STS12 STS13 STS14 Reranking Summarization STS15 STS16 AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverFlowDupQuestionsSummEval STSBSTS17 STS22 ArguAna ClimateFEVER CQADupstackRetrieval FEVER DBPedia FiQA2018 HotpotQA MSMARCO NFCorpus NQ Quora SCIDOCS SciFact Touche2020 TRECCOVID MedrxivP2P MedrxivS2S Reddit StackExchange RedditP2P StackExchangeP2P TwentyNewsgroup BiorxivP2P BiorxivS2S Bitext Mining BUCC Tatoeba Figure 1: An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade. ducibility: Through versioning at a dataset and software level, we aim to make it easy to repro- duce results in MTEB. JSON ﬁles corresponding to all results available in this paper have been made available together with the MTEB benchmark3. 3.2 Tasks and Evaluation Figure 1 provides an overview of tasks and datasets available in MTEB. Dataset statistics are available in Table 2. The benchmark consists of the follow- ing 8 task types: Bitext Mining Inputs are two sets of sentences from two different languages. For each sentence in the ﬁrst set, the best match in the second set needs to be found. The matches are commonly translations. The provided model is used to embed each sentence and the closest pairs are found via cosine similarity. F1 serves as the main metric for bitext mining. Accuracy, precision and recall are also computed. Classiﬁcation A train and test set are embedded with the provided model. The train set embeddings are used to train a logistic regression classiﬁer with 100 maximum iterations, which is scored on the test set. The main metric is accuracy with average precision and f1 additionally provided. 3https://huggingface.co/datasets/mteb /results Clustering Given a set of sentences or para- graphs, the goal is to group them into meaning- ful clusters. A mini-batch k-means model with batch size 32 and k equal to the number of dif- ferent labels (Pedregosa et al., 2011) is trained on the embedded texts. The model is scored using v-measure (Rosenberg and Hirschberg, 2007). V- measure does not depend on the cluster label, thus the permutation of labels does not affect the score. Pair Classiﬁcation A pair of text inputs is pro- vided and a label needs to be assigned. Labels are typically binary variables denoting duplicate or paraphrase pairs. The two texts are embedded and their distance is computed with various metrics (cosine similarity, dot product, euclidean distance, manhattan distance). Using the best binary thresh- old accuracy, average precision, f1, precision and recall are computed. The average precision score based on cosine similarity is the main metric. Reranking Inputs are a query and a list of rele- vant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query. The model is used to embed the references which are then compared to the query using cosine similarity. The resulting ranking is scored for each query and averaged across all queries. Metrics are mean MRR@k and MAP with the latter being the main metric.AmazonCounterfactualClassificationAmazonPolarityClassificationAmazonReviewsClassificationBanking77ClassificationEmotionClassificationImdbClassificationMassiveIntentClassificationMassiveScenarioClassificationMTOPDomainClassificationMTOPIntentClassificationToxicConversationsClassificationTweetSentimentExtractionClassificationArxivClusteringP2PArxivClusteringS2SBiorxivClusteringP2PBiorxivClusteringS2SMedrxivClusteringP2PMedrxivClusteringS2SRedditClusteringRedditClusteringP2PStackExchangeClusteringStackExchangeClusteringP2PTwentyNewsgroupsClusteringSprintDuplicateQuestionsTwitterSemEval2015TwitterURLCorpusAskUbuntuDupQuestionsMindSmallRerankingSciDocsRRStackOverflowDupQuestionsArguAnaClimateFEVERCQADupstackAndroidRetrievalCQADupstackEnglishRetrievalCQADupstackGamingRetrievalCQADupstackGisRetrievalCQADupstackMathematicaRetrievalCQADupstackPhysicsRetrievalCQADupstackProgrammersRetrievalCQADupstackStatsRetrievalCQADupstackTexRetrievalCQADupstackUnixRetrievalCQADupstackWebmastersRetrievalCQADupstackWordpressRetrievalDBPediaFEVERFiQA2018HotpotQAMSMARCONFCorpusNQQuoraRetrievalSCIDOCSSciFactTouche2020TRECCOVIDBIOSSESSICK-RSTS12STS13STS14STS15STS16STS17STS22STSBenchmarkSummEval AmazonCounterfactualClassification AmazonPolarityClassification AmazonReviewsClassification Banking77Classification EmotionClassification ImdbClassification MassiveIntentClassification MassiveScenarioClassification MTOPDomainClassification MTOPIntentClassification ToxicConversationsClassification TweetSentimentExtractionClassification ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions ArguAna ClimateFEVER CQADupstackAndroidRetrieval CQADupstackEnglishRetrieval CQADupstackGamingRetrieval CQADupstackGisRetrieval CQADupstackMathematicaRetrieval CQADupstackPhysicsRetrieval CQADupstackProgrammersRetrieval CQADupstackStatsRetrieval CQADupstackTexRetrieval CQADupstackUnixRetrieval CQADupstackWebmastersRetrieval CQADupstackWordpressRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark SummEval 97 85 84 90 89 83 90 89 84 87 91 94 81 85 85 92 92 89 91 89 88 92 92 89 91 89 88 100 91 92 87 92 88 88 98 98 91 92 87 92 88 88 98 98 100 93 93 87 90 89 90 96 96 95 95 94 94 89 91 92 90 97 97 96 96 98 91 91 83 87 83 86 90 90 89 89 89 89 92 93 87 90 87 89 97 97 95 95 96 96 93 88 88 81 85 82 85 87 87 87 87 87 87 95 90 91 91 85 87 85 88 93 93 92 92 92 92 95 96 94 87 87 81 84 81 84 87 87 87 87 87 87 92 89 96 93 89 89 83 85 83 85 90 90 89 89 89 90 93 93 93 97 96 94 94 88 92 90 89 95 95 95 95 95 96 90 95 88 93 88 91 94 95 86 93 92 91 95 95 95 95 96 97 92 95 90 93 89 91 96 92 92 89 91 88 88 95 95 94 94 94 94 92 96 89 94 89 92 95 95 87 87 79 86 82 83 90 90 89 89 89 88 89 91 86 90 85 88 89 91 92 93 93 88 91 87 88 96 96 95 95 95 96 92 98 89 95 90 93 95 95 96 91 74 74 69 78 72 69 77 77 79 79 74 75 73 75 71 75 71 74 77 76 77 74 77 88 89 83 85 85 85 91 91 90 90 92 92 85 91 83 88 83 85 91 92 89 84 90 71 92 92 84 88 87 89 92 92 92 92 93 93 89 92 87 91 87 89 93 93 92 86 93 74 88 88 87 85 89 84 84 92 92 91 91 89 91 89 91 86 90 85 88 90 90 92 88 92 77 85 87 84 86 80 81 80 84 89 89 88 88 88 88 85 88 82 88 82 86 88 88 87 82 89 67 84 88 83 91 92 86 89 85 88 95 95 93 93 93 93 94 97 91 97 91 95 94 94 95 92 96 76 89 92 91 89 88 88 84 87 83 83 92 92 91 91 89 90 90 92 86 92 86 90 90 90 93 92 92 75 85 87 92 84 93 92 91 84 87 85 89 90 90 90 90 91 90 92 91 91 91 91 90 91 93 92 87 92 72 86 91 87 86 92 88 87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87 88 87 80 89 82 84 90 90 90 90 88 88 87 89 85 88 85 86 89 90 91 92 90 79 84 86 90 81 90 90 87 85 91 91 83 89 86 87 92 92 92 92 92 92 90 93 88 91 88 89 93 93 96 91 93 74 86 90 87 83 92 89 91 88 91 91 90 82 90 85 87 93 93 92 92 91 91 89 93 87 91 87 89 93 94 94 95 93 75 87 89 90 84 92 91 90 88 94 94 86 85 79 86 80 81 87 87 87 87 86 86 88 89 85 89 86 88 87 88 91 93 89 74 81 84 88 80 90 91 86 84 91 90 92 88 87 80 87 82 83 89 89 89 89 87 87 91 91 88 91 87 89 89 90 93 94 90 77 82 86 89 81 92 92 88 85 92 92 93 94 88 88 80 87 82 83 89 89 88 88 89 88 93 92 88 92 87 89 90 91 93 92 91 73 83 87 87 82 92 88 90 86 91 94 93 90 93 88 88 81 87 82 85 90 90 89 89 88 88 90 91 88 91 87 89 90 91 94 95 92 75 83 87 88 81 93 92 91 85 92 95 94 93 94 94 87 87 80 86 81 82 88 88 88 88 87 87 92 91 89 92 90 91 89 90 92 93 91 74 83 86 87 81 93 90 89 85 90 93 92 93 96 94 95 87 87 80 86 80 82 88 88 88 88 86 87 90 90 87 90 86 88 89 89 93 92 90 75 82 85 88 81 90 91 87 84 90 92 91 92 96 91 93 93 88 87 81 88 82 84 90 90 89 89 88 88 89 90 87 90 86 88 90 90 93 93 91 76 83 86 93 80 91 91 88 85 94 93 94 93 95 93 95 94 94 88 88 80 88 82 84 90 90 89 89 89 88 89 91 87 90 87 89 90 91 93 93 91 74 83 87 89 83 92 91 89 85 93 93 94 93 93 92 96 93 93 94 87 87 80 87 82 83 89 89 88 88 88 88 89 89 86 90 86 88 89 90 92 92 90 75 83 87 89 81 91 92 88 84 92 92 93 92 93 91 93 92 93 94 96 90 90 86 87 84 86 93 93 92 92 92 93 90 95 89 93 88 91 93 92 93 87 94 73 88 89 88 85 93 89 89 91 86 91 89 86 87 88 87 88 87 87 88 87 87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87 100 85 88 88 84 85 86 85 85 84 85 85 84 91 92 92 87 90 87 88 95 95 93 93 95 94 91 95 88 92 88 90 94 94 95 90 95 75 89 92 91 87 94 91 92 89 89 92 92 88 90 90 91 90 90 90 91 90 91 89 90 90 86 89 85 88 95 95 94 94 94 94 90 96 88 93 88 92 94 93 94 88 95 74 90 91 90 88 94 90 89 93 88 91 90 88 88 88 88 88 87 88 89 87 96 93 93 90 91 87 91 86 87 94 94 95 95 93 94 89 94 87 91 88 90 94 93 93 87 94 77 88 90 89 85 92 89 89 91 89 91 90 87 88 88 88 88 87 88 89 87 93 91 93 93 89 89 83 85 84 87 89 89 88 88 90 89 92 91 94 94 95 94 90 91 92 87 92 72 85 89 86 84 92 87 93 87 86 90 89 85 88 89 89 90 86 87 88 87 90 87 90 89 89 91 92 86 89 86 88 95 95 94 94 94 95 91 96 88 94 88 92 95 94 95 89 97 74 90 92 90 89 95 91 91 93 88 92 91 87 89 90 89 88 88 89 90 89 95 93 93 97 92 90 92 92 88 92 88 89 97 97 96 96 98 97 91 97 88 93 88 91 96 96 97 91 96 76 90 93 91 88 94 91 91 91 91 94 94 89 90 92 92 91 90 91 92 91 93 91 95 95 95 91 95 89 90 85 86 82 85 91 91 89 89 89 89 95 93 92 97 92 96 90 91 93 90 93 74 85 89 88 86 97 91 91 88 87 90 89 90 92 90 92 93 90 90 90 89 91 88 91 91 90 92 92 91 89 90 83 85 83 86 89 89 89 89 89 89 93 92 95 96 95 95 90 91 92 88 92 73 85 89 86 83 94 88 92 87 87 90 90 87 90 91 90 92 88 89 89 88 91 87 90 89 89 97 90 91 95 92 93 85 88 88 90 93 93 92 92 94 93 91 93 89 92 89 90 94 95 93 88 93 74 89 93 88 87 92 89 96 88 88 92 91 86 87 89 90 88 88 88 89 88 90 88 93 91 91 91 93 94 90 90 88 89 84 84 83 86 90 90 89 89 89 90 93 92 94 96 96 97 90 91 91 86 92 72 85 89 88 86 94 88 91 88 86 89 88 86 88 88 88 90 87 88 88 87 91 88 90 91 89 96 91 90 95 96 90 86 86 80 82 80 83 85 85 85 85 85 85 91 88 93 93 92 92 86 87 88 84 88 71 82 85 83 80 90 85 90 83 84 87 86 83 86 87 86 88 85 85 85 84 87 83 86 85 85 93 86 86 91 97 87 92 83 84 81 82 81 84 89 89 88 88 89 89 81 88 78 84 78 82 88 88 86 81 88 68 89 85 83 83 86 83 82 84 81 82 83 77 79 81 80 79 78 80 80 79 84 84 85 88 86 82 88 89 81 80 86 81 76 92 92 89 90 88 88 97 97 95 95 95 96 90 96 87 93 87 91 95 94 95 89 96 76 91 93 91 87 94 91 91 91 89 92 92 87 88 89 89 88 88 89 89 88 93 91 94 95 94 90 95 96 90 90 93 90 86 90 92 91 89 90 88 88 96 96 94 94 94 95 91 95 89 93 89 91 95 94 95 90 95 75 90 92 91 87 94 91 92 90 89 94 92 88 89 91 91 89 89 90 90 89 92 90 93 93 92 91 94 95 91 91 93 91 87 86 97 94 94 90 92 91 90 97 97 96 96 97 98 92 96 90 94 90 92 96 97 96 90 97 76 91 94 92 89 95 92 94 92 90 94 94 89 90 91 91 90 89 91 91 90 93 92 95 95 94 92 95 97 92 92 95 92 88 89 98 98 93 92 89 90 89 89 96 96 95 95 95 96 91 95 89 93 88 90 95 95 95 89 95 74 90 92 91 88 94 91 92 90 89 92 92 87 88 90 89 88 87 89 89 88 92 90 94 94 93 90 94 96 90 90 94 90 87 89 96 96 98 93 93 89 91 91 89 95 95 94 94 95 95 90 93 88 91 88 89 95 95 95 90 94 75 88 93 91 85 92 90 92 88 90 93 92 87 89 90 91 89 89 90 91 90 91 88 95 92 93 91 92 96 90 90 93 89 86 85 95 94 97 95 91 90 86 88 87 88 95 95 95 95 95 96 87 94 85 91 85 88 94 94 92 87 94 73 92 91 89 88 92 89 89 89 87 89 90 85 86 87 87 85 84 86 87 86 90 89 91 94 92 88 93 95 88 87 92 88 83 93 95 93 95 95 92 89 89 91 88 85 85 94 94 92 92 93 93 89 93 87 91 87 90 92 93 94 87 93 75 88 90 90 86 92 90 89 89 87 89 89 86 87 87 87 87 87 87 88 87 91 89 97 92 92 88 93 94 90 88 90 89 85 85 93 92 94 92 92 90 93 93 87 90 89 91 96 96 95 95 96 97 89 95 87 92 87 90 96 96 94 89 95 75 92 93 90 88 93 90 92 90 89 92 92 86 88 89 90 88 87 89 89 89 91 90 94 94 93 90 94 96 89 89 94 89 85 94 96 95 97 96 96 97 91 93 92 85 89 87 90 94 94 93 93 94 94 91 94 90 92 90 91 94 95 93 88 94 73 92 93 88 89 92 89 94 91 88 91 92 86 88 88 89 88 87 87 89 88 91 91 92 93 92 92 93 94 90 91 93 91 88 89 94 94 95 94 92 95 91 95 70 75 80 85 90 95 100 Figure 2: Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL, see Table 1) to embed 100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized. Retrieval Each dataset consists of a corpus, queries and a mapping for each query to relevant documents from the corpus. The aim is to ﬁnd these relevant documents. The provided model is used to embed all queries and all corpus documents and similarity scores are computed using cosine simi- larity. After ranking the corpus documents for each query based on the scores, nDCG@k, MRR@k, MAP@k, precision@k and recall@k are computed for several values of k. nDCG@10 serves as the main metric. MTEB reuses datasets and evaluation from BEIR (Thakur et al., 2021). Semantic Textual Similarity (STS) Given a sentence pair the aim is to determine their simi- larity. Labels are continuous scores with higher numbers indicating more similar sentences. The provided model is used to embed the sentences and their similarity is computed using various distance metrics. Distances are benchmarked with ground truth similarities using Pearson and Spearman cor- relations. Spearman correlation based on cosine similarity serves as the main metric (Reimers et al., 2016). Summarization A set of human-written and machine-generated summaries are provided. The aim is to score the machine summaries. The pro- vided model is ﬁrst used to embed all summaries. For each machine summary embedding, distances to all human summary embeddings are computed. The closest score (e.g. highest cosine similarity) is kept and used as the model’s score of a single machine-generated summary. Pearson and Spear- man correlations with ground truth human assess- ments of the machine-generated summaries are computed. Like for STS, Spearman correlation based on cosine similarity serves as the main met- ric (Reimers et al., 2016). 3.3 Datasets To further the diversity of MTEB, datasets of vary- ing text lengths are included. All datasets are grouped into three categories: Sentence to sentence (S2S) A sentence is com- pared with another sentence. An example of S2S are all current STS tasks in MTEB, where the simi- larity between two sentences is assessed. Class. Clust. PairClass. Rerank. Retr. STS Summ. Avg. Num. Datasets (→) 12 11 3 4 15 10 1 56 Self-supervised methods Glove 57.29 27.73 70.92 43.29 21.62 61.85 28.87 41.97 Komninos 57.65 26.57 72.94 44.75 21.22 62.47 30.49 42.06 BERT 61.66 30.12 56.33 43.44 10.59 54.36 29.82 38.33 SimCSE-BERT-unsup 62.50 29.04 70.33 46.47 20.29 74.33 31.15 45.45 Supervised methods SimCSE-BERT-sup 67.32 33.43 73.68 47.54 21.82 79.12 23.31 48.72 coCondenser-msmarco 64.71 37.64 81.74 51.84 32.96 76.47 29.50 52.35 Contriever 66.68 41.10 82.53 53.14 41.88 76.51 30.36 56.00 SPECTER 52.37 34.06 61.37 48.10 15.88 61.02 27.66 40.28 LaBSE 62.71 29.55 78.87 48.42 18.99 70.80 31.05 45.21 LASER2 53.65 15.28 68.86 41.44 7.93 55.32 26.80 33.63 MiniLM-L6 63.06 42.35 82.37 58.04 41.95 78.90 30.81 56.26 MiniLM-L12 63.21 41.81 82.41 58.44 42.69 79.80 27.90 56.53 MiniLM-L12-multilingual 64.30 37.14 78.45 53.62 32.45 78.92 30.67 52.44 MPNet 65.07 43.69 83.04 59.36 43.81 80.28 27.49 57.78 MPNet-multilingual 67.91 38.40 80.81 53.80 35.34 80.73 31.57 54.71 OpenAI Ada Similarity 70.44 37.52 76.86 49.02 18.36 78.60 26.94 49.52 SGPT-125M-nli 61.46 30.95 71.78 47.56 20.90 74.71 30.26 45.97 SGPT-5.8B-nli 70.14 36.98 77.03 52.33 32.34 80.53 30.38 53.74 SGPT-125M-msmarco 60.72 35.79 75.23 50.58 37.04 73.41 28.90 51.23 SGPT-1.3B-msmarco 66.52 39.92 79.58 54.00 44.49 75.74 25.44 56.11 SGPT-2.7B-msmarco 67.13 39.83 80.65 54.67 46.54 76.83 27.87 57.12 SGPT-5.8B-msmarco 68.13 40.35 82.00 56.56 50.25 78.10 24.75 58.81 SGPT-BLOOM-7.1B-msmarco 66.19 38.93 81.90 55.65 48.21 77.74 24.99 57.44 GTR-Base 65.25 38.63 83.85 54.23 44.67 77.07 29.67 56.19 GTR-Large 67.14 41.60 85.33 55.36 47.42 78.19 29.50 58.28 GTR-XL 67.11 41.51 86.13 55.96 47.96 77.80 30.21 58.42 GTR-XXL 67.41 42.42 86.12 56.65 48.48 78.38 30.64 58.97 ST5-Base 69.81 40.21 85.17 53.09 33.63 81.14 31.39 55.27 ST5-Large 72.31 41.65 84.97 54.00 36.71 81.83 29.64 57.06 ST5-XL 72.84 42.34 86.06 54.71 38.47 81.66 29.91 57.87 ST5-XXL 73.42 43.71 85.06 56.43 42.24 82.63 30.08 59.51 Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets. Paragraph to paragraph (P2P) A paragraph is compared with another paragraph. MTEB imposes no limit on the input length, leaving it up to the models to truncate if necessary. Several clustering tasks are framed as both S2S and P2P tasks. The former only compare titles, while the latter include both title and content. For ArxivClustering, for example, abstracts are concatenated to the title in the P2P setting. Sentence to paragraph (S2P) A few retrieval datasets are mixed in a S2P setting. Here a query is a single sentence, while documents are long paragraphs consisting of multiple sentences. Similarities across 56 MTEB datasets are vi- sualized in Figure 2. Several datasets rely on the same corpora, such as ClimateFEVER and FEVER, resulting in a score of 1. Clusters of simi- lar datasets can be seen among CQADupstack vari- ations and STS datasets. S2S and P2P variations of the same dataset tend to also be similar. Scientiﬁc datasets, such as SciDocsRR, SciFact, ArxivClus- tering, show high similarities among each other even when coming from different tasks (Reranking, Retrieval and Clustering in this case). 4 Results 4.1 Models We evaluate on the test splits of all datasets except for MSMARCO, where the dev split is used follow- ing Thakur et al. (2021). We benchmark models claiming state-of-the-art results on various embed- 0.1B 1B 2B 4B Model Parameters (Billions) 0.62 0.64 0.66 0.68 0.70 0.72 0.74Average Performance (accuracy) Classification 0.1B 1B 2B 4B Model Parameters (Billions) 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44Average Performance (v_measure) Clustering 0.1B 1B 2B 4B Model Parameters (Billions) 0.76 0.78 0.80 0.82 0.84 0.86Average Performance (ap) PairClassification 0.1B 1B 2B 4B Model Parameters (Billions) 0.51 0.52 0.53 0.54 0.55 0.56Average Performance (map) Reranking 0.1B 1B 2B 4B Model Parameters (Billions) 0.350 0.375 0.400 0.425 0.450 0.475 0.500Average Performance (nDCG@10) Retrieval 0.1B 1B 2B 4B Model Parameters (Billions) 0.74 0.76 0.78 0.80 0.82Average Performance (cos. sim. spearman corr.)STS GTR ST5 SGPT Figure 3: MTEB performance scales with model size. The smallest SGPT variant underperforms similar- sized GTR and ST5 variants. This may be due to the bias-only ﬁne-tuning SGPT employs, which catches up with full ﬁne-tuning only as model size and thus the number of bias parameters increases (Muennighoff, 2022). ding tasks leading to a high representation of trans- formers (Vaswani et al., 2017). We group models into self-supervised and supervised methods. Self-supervised methods (a) Transformer- based BERT (Devlin et al., 2018) is trained using self-supervised mask and sentence prediction tasks. By taking the mean across the sequence length (mean-pooling) the model can directly be used to produce text embeddings. SimCSE-Unsup (Gao et al., 2021b) uses BERT as a foundation and performs additional self-supervised training. (b) Non-transformer: Komninos (Komninos and Manandhar, 2016) and Glove (Pennington et al., 2014) are two word embedding models that directly map words to vectors. Hence, their embeddings lack context awareness, but provide signiﬁcant speed-ups. Supervised methods The original transformer model (Vaswani et al., 2017) consists of an encoder and decoder network. Subsequent transformers often train only encoders like BERT (Devlin et al., 2018) or decoders like GPT (Radford et al., 2019). (a) Transformer encoder methods coCon- denser (Gao and Callan, 2021), Contriever (Izac- ard et al., 2021), LaBSE (Feng et al., 2020) and SimCSE-BERT-sup (Gao et al., 2021b) are based on the pre-trained BERT model (Devlin et al., 2018). coCondenser and Contriever add a self- supervised stage prior to supervised ﬁne-tuning for a total of three training stages. LaBSE uses BERT to perform additional pre-training on par- allel data to produce a competitive bitext mining model. SPECTER (Cohan et al., 2020a) relies on the pre-trained SciBERT (Beltagy et al., 2019) vari- ant instead and ﬁne-tunes on citation graphs. GTR (Ni et al., 2021b) and ST5 (Ni et al., 2021a) are based on the encoder part of the T5 model (Raf- fel et al., 2020) and only differ in their ﬁne-tuning datasets. After additional self-supervised training, ST5 does contrastive ﬁne-tuning on NLI (Ni et al., 2021a; Gao et al., 2021b) being geared towards STS tasks. Meanwhile, GTR ﬁne-tunes on MS- MARCO and focuses on retrieval tasks. MPNet and MiniLM correspond to ﬁne-tuned embedding models (Reimers and Gurevych, 2019) of the pre- trained MPNet (Song et al., 2020) and MiniLM (Wang et al., 2020) models using diverse datasets to target any embedding use case. (b) Transformer decoder methods SGPT Bi- Encoders (Muennighoff, 2022) perform contrastive ﬁne-tuning of <0.1% of pre-trained parameters us- ing weighted-mean pooling. Similar to ST5 and GTR, SGPT-nli models are geared towards STS, while SGPT-msmarco models towards retrieval. SGPT-msmarco models embed queries and doc- uments for retrieval with different special tokens to help the model distinguish their role. For non- retrieval tasks, we use its query representations. We benchmark publicly available SGPT models based on GPT-NeoX (Andonian et al., 2021), GPT- J (Wang and Komatsuzaki, 2021) and BLOOM (Scao et al., 2022). Alternatively, cpt-text (Nee- lakantan et al., 2022) passes pre-trained GPT de- coders through a two-stage process using last token pooling to provide embeddings from decoders. We benchmark their models via the OpenAI Embed- dings API4. (c) Non-transformer LASER (Heffernan et al., 2022) is the only context aware non-transformer model we benchmark, relying on an LSTM 4https://beta.openai.com/docs/guides/ embeddings 102 103 104 Speed (examples per sec) 35 40 45 50 55 60MTEB Score LASER2 Komninos Glove SGPT-125M-nli SGPT-125M-msmarco SGPT-5.8B-nli SGPT-5.8B-msmarco MiniLM-L6 MPNet ST5-Base ST5-XXL GTR-Base GTR-XXL Contriever coCondenser-msmarco BERT SimCSE-BERT-sup SimCSE-BERT-unsup LaBSE MiniLM-L12 SPECTER Base Architecture LASER WordEmbeddings GPT MiniLM MPNet T5 BERT SciBERT Figure 4: Performance, speed, and size of produced embeddings (size of the circles) of different embedding models. Embedding sizes range from 1.2 kB (Glove / Komninos) to 16.4 kB (SGPT-5.8B) per example. Speed was benchmarked on STS15 using 1x Nvidia A100 80GB with CUDA 11.6. (Hochreiter and Schmidhuber, 1997) instead. Simi- lar to LaBSE, the model trains on parallel data and focuses on bitext mining applications. 4.2 Analysis Based on the results in Table 1, we observe that there is considerable variability between tasks. No model claims the state-of-the-art in all seven En- glish tasks. There is even more variability in the results per dataset present in the appendix. Further, there remains a large gap between self-supervised and supervised methods. Self-supervised large lan- guage models have been able to close this gap in many natural language generation tasks (Chowd- hery et al., 2022). However, they appear to still require supervised ﬁne-tuning for competitive em- bedding performance. We ﬁnd that performance strongly correlates with model size, see Figure 3. A majority of MTEB tasks are dominated by multi-billion param- eter models. However, these come at a signiﬁcant cost as we investigate in Section 4.3. Classiﬁcation ST5 models dominate the classiﬁ- cation task across most datasets, as can be seen in detail in the full results in the appendix. ST5-XXL has the highest average performance, 3% ahead of the best non-ST5 model, OpenAI Ada Similarity. Clustering Despite being almost 50x smaller, the MPNet embedding model is on par with the ST5- XXL state-of-the-art on Clustering. This may be due to the large variety of datasets MPNet (and MiniLM) has been ﬁne-tuned on. Clustering re- quires coherent distances between a large number of embeddings. Models like SimCSE-sup or SGPT- nli, which are only ﬁne-tuned on a single dataset, NLI, may produce incoherent embeddings when encountering topics unseen during ﬁne-tuning. Re- latedly, we ﬁnd that the query embeddings of SGPT- msmarco and the Ada Search endpoint are competi- tive with SGPT-nli and the Ada Similarity endpoint, respectively. We refer to the public leaderboard5 for Ada Search results. This could be due to the MSMARCO dataset being signiﬁcantly larger than NLI. Thus, while the OpenAI docs recommend us- ing the similarity embeddings for clustering use cases6, the retrieval query embeddings may be the better choice in some cases. 5https://huggingface.co/spaces/mteb/l eaderboard 6https://beta.openai.com/docs/guides/ embeddings/similarity-embeddings deu-eng mal-eng nob-eng spa-eng epo -eng tur -eng tel-eng pol-eng vie-eng hrv -eng r on-eng hin-eng glg-eng sqi-eng ces-eng est-eng hun-eng slk-eng lit-eng fin-eng afr -eng tha-eng nld-eng slv -eng tgl-eng mon-eng lvs-eng dan-eng swe-eng zsm-eng cat-eng jpn-eng ina-eng ell-eng cmn-eng k at-eng eus-eng bel-eng aze-eng bos-eng fra-eng isl-eng pes-eng bul-eng nno -eng srp-eng por -eng hye-eng ukr -eng gle-eng rus-eng ind-eng mkd-eng ur d-eng ita-eng mar -eng uig-eng cym-eng xho -eng heb-eng amh-eng k or -eng ast-eng wuu-eng yue-eng ido -eng fry -eng tam-eng ara-eng yid-eng ben-eng k az-eng fao -eng tat-eng gla-eng ile-eng swh-eng uzb-eng k ur -eng lat-eng jav -eng cbk-eng nds-eng khm-eng arz-eng tuk-eng nov -eng awa-eng lfn-eng hsb-eng oci-eng dsb-eng pms-eng ceb-eng max-eng war -eng swg-eng ang-eng tzl-eng csb-eng gsw-eng ar q-eng orv -eng cha-eng mhr -eng br e-eng kzj-eng dtp-eng pam-eng cor -eng ber -eng k ab-eng 0.0 0.2 0.4 0.6 0.8 1.0F1 score LaBSE LASER2 MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco (a) Bitext Mining on Tatoeba en hi zh-CN pt id es th it fr ru de fa sv vi zh- TW nl ms da pl tr sq el r o hu sl k o fi ja nb ml lv he ur bn ar te af ta hy my az mn is kn tl jv sw k a km am cy zh 0.1 0.2 0.3 0.4 0.5 0.6 0.7Accuracy (b) Multilingual Classiﬁcation ko fr es en ar it zh ru tr de pl 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Cos. Sim. Spearman Corr. fr -en en-ar en-de it-en es-en nl-en pl-en zh-en en-tr es-it fr -pl de-fr de-en de-pl (c) Multi- and Crosslingual STS Figure 5: MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classiﬁcation and STS results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pre- trained on, such as Chinese, French and Portuguese. Pair Classiﬁcation GTR-XL and GTR-XXL have the strongest performance. Pair classiﬁca- tion is closest to STS in its framing, yet models rank signiﬁcantly differently on the two tasks. This highlights the importance of benchmarking on a diverse set of tasks to avoid blindly reusing a model for a different task. Reranking MPNet and MiniLM models perform strongly on reranking tasks. On SciDocsRR (Co- han et al., 2020a) they perform far better than big- ger models, which is likely due to parts of Sci- DocsRR being included in their training data. Our scale of experiments and that of model pre-training make controlling for data contamination challeng- ing. Thus, we ignore overlap of MTEB datasets with model training datasets in MTEB scores. As long as enough datasets are averaged, we believe these effects to be insigniﬁcant. Retrieval SGPT-5.8B-msmarco is the best em- bedding model on the BEIR subset in MTEB as well as on the full BEIR benchmark (Thakur et al., 2021; Muennighoff, 2022). The even larger 7.1B SGPT model making use of BLOOM (Scao et al., 2022) performs signiﬁcantly weaker, which is likely due to the multilinguality of BLOOM. Models geared towards STS (SimCSE, ST5, SGPT- nli) perform badly on retrieval tasks. Retrieval tasks are unique in that there are two distinct types of texts: Queries and documents (“asymmetric”), while other tasks only have a single type of text (“symmetric”). On the QuoraRetrieval dataset, which has been shown to be largely symmetric (Muennighoff, 2022), the playing ﬁeld is more even with SGPT-5.8B-nli outperforming SGPT- 5.8B-msmarco, see Table 11. STS & Summarization Retrieval models (GTR, SGPT-msmarco) perform badly on STS, while ST5- XXL has the highest performance. This highlights the bifurcation of the ﬁeld into separate embedding models for retrieval (asymmetric) and similarity (symmetric) use cases (Muennighoff, 2022). 4.3 Efﬁciency We investigate the latency-performance trade-off of models in Figure 4. The graph allows for signiﬁ- cant elimination of model candidates in the model selection process. It brings model selection down to three clusters: Maximum speed Word Embedding models offer maximum speed with Glove taking the lead on both performance and speed, thus making the choice simple in this case. Maximum performance If latency is less impor- tant than performance, the left-hand side of the graph offers a cluster of highly performant, but slow models. Depending on the task at hand, GTR- XXL, ST5-XXL or SGPT-5.8B may be the right choice, see Section 4.2. SGPT-5.8B comes with the additional caveat of its high-dimensional em- beddings requiring more storage. Speed and performance The ﬁne-tuned MPNet and MiniLM models lead the middle cluster mak- ing the choice easy. 4.4 Multilinguality MTEB comes with 10 multilingual datasets across bitext mining, classiﬁcation and STS tasks. We in- vestigate performance on these in Figure 5. Tabular results can be found in Tables 12, 13 and 14. Bitext Mining LaBSE (Feng et al., 2020) per- forms strongly across a wide array of languages in bitext mining. Meanwhile, LASER2 shows high variance across different languages. While there are additional language-speciﬁc LASER2 models available for some of the languages we benchmark, we use the default multilingual LASER2 model for all languages. This is to provide a fair one-to- one comparison of models. In practice, however, the high variance of LASER2’s performance may be resolved by mixing its model variants. MP- Net, MiniLM and SGPT-BLOOM-7B1-msmarco perform poorly on languages they have not been pre-trained on, such as German for the latter. Classiﬁcation & STS On multilingual classiﬁ- cation and STS, the multilingual MPNet provides the overall strongest performance. It outperforms the slightly faster multilingual MiniLM on almost all languages. Both models have been trained on the same languages, thus bringing decision- making down to performance vs speed. SGPT- BLOOM-7B1-msmarco provides state-of-the-art performance on languages like Hindi, Portuguese, Chinese or French, which the model has seen ex- tensively during pre-training. It also performs com- petitively on languages like Russian or Japanese that unintentionally leaked into its pre-training data (Muennighoff et al., 2022). However, it is not much ahead of the much cheaper MPNet. LASER2 performs consistently worse than other models. 5 Conclusion In this work, we presented the Massive Text Em- bedding Benchmark (MTEB). Consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, MTEB aims to provide re- liable embedding performance estimates. By open- sourcing MTEB alongside a leaderboard, we pro- vide a foundation for further pushing the state-of- the-art of available text embeddings. To introduce MTEB, we have conducted the most comprehensive benchmarking of text embed- dings to date. Through the course of close to 5,000 experiments on over 30 different models, we have set up solid baselines for future research to build on. We found model performance on different tasks to vary strongly with no model claiming state-of- the-art on all tasks. Our studies on scaling behav- ior, model efﬁciency and multilinguality revealed various intricacies of models that should ease the decision-making process for future research or in- dustry applications of text embeddings. We welcome task, dataset or metric contributions to the MTEB codebase7 as well as additions to the leaderboard via our automatic submission format8. 7https://github.com/embeddings-benchm ark/mteb 8https://huggingface.co/spaces/mteb/l eaderboard Acknowledgments This work was granted access to the HPC resources of Institut du développement et des ressources en informatique scientiﬁque (IDRIS) du Centre na- tional de la recherche scientiﬁque (CNRS) under the allocation 2021-A0101012475 made by Grand équipement national de calcul intensif (GENCI). In particular, all the evaluations and data processing ran on the Jean Zay cluster of IDRIS, and we want to thank the IDRIS team for responsive support throughout the project, in particular Rémi Lacroix. We thank Douwe Kiela, Teven Le Scao and Nan- dan Thakur for feedback and suggestions. References Charu C Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining text data, pages 77–128. Springer. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. 2015. Semeval-2015 task 2: Seman- tic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th interna- tional workshop on semantic evaluation (SemEval 2015), pages 252–263. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In SemEval@ COLING, pages 81–91. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger- man Rigau Claramunt, and Janyce Wiebe. 2016. Semeval-2016 task 1: Semantic textual similar- ity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th International Workshop on Se- mantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (As- sociation for Computational Linguistics). Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi- lot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Compu- tational Semantics–Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385– 393. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. * sem 2013 shared task: Semantic textual similarity. In Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main confer- ence and the shared task: semantic textual similar- ity, pages 32–43. Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santa- coder: don’t reach for the stars! arXiv preprint arXiv:2301.03988. Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach. 2021. GPT-NeoX: Large scale autoregressive language modeling in pytorch. Dimo Angelov. 2020. Top2vec: Distributed representa- tions of topics. arXiv preprint arXiv:2008.09470. Akari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2020. Xor qa: Cross-lingual open-retrieval question an- swering. arXiv preprint arXiv:2010.11856. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- ert: A pretrained language model for scientiﬁc text. arXiv preprint arXiv:1903.10676. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from tril- lions of tokens. In International Conference on Ma- chine Learning, pages 2206–2240. PMLR. Micael Carvalho, Rémi Cadène, David Picard, Laure Soulier, Nicolas Thome, and Matthieu Cord. 2018. Cross-modal retrieval in the cooking context: Learn- ing semantic text-image embeddings. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 35– 44. Iñigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient intent detection with dual sentence encoders. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. Tydi qa: A benchmark for information-seeking question answering in typo- logically diverse languages. Transactions of the As- sociation for Computational Linguistics, 8:454–470. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. 2020a. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020b. Specter: Document-level representation learning using citation-informed transformers. Alexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representa- tions. arXiv preprint arXiv:1803.05449. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805. Alexander R. Fabbri, Wojciech Kry´sci´nski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2020. Summeval: Re-evaluating summarization evaluation. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Language- agnostic bert sentence embedding. arXiv preprint arXiv:2007.01852. Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natara- jan. 2022. Massive: A 1m-example multilin- gual natural language understanding dataset with 51 typologically-diverse languages. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPoﬁ, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. 2021a. A framework for few-shot language model evaluation. Version v0. 0.1. Sept. Luyu Gao and Jamie Callan. 2021. Unsupervised cor- pus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. Simcse: Simple contrastive learning of sentence em- beddings. arXiv preprint arXiv:2104.08821. Gregor Geigle, Nils Reimers, Andreas Rücklé, and Iryna Gurevych. 2021. Tweac: Transformer with ex- tendable qa agent classiﬁers. Kevin Heffernan, Onur Çelebi, and Holger Schwenk. 2022. Bitext mining using distilled sentence rep- resentations for low-resource languages. arXiv preprint arXiv:2205.12654. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780. Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanab- han, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD Interna- tional Conference on Knowledge Discovery & Data Mining, pages 2553–2561. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code- searchnet challenge: Evaluating the state of seman- tic code search. arXiv preprint arXiv:1909.09436. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Alexandros Komninos and Suresh Manandhar. 2016. Dependency based embeddings for sentence classi- ﬁcation tasks. In Proceedings of the 2016 confer- ence of the North American chapter of the associa- tion for computational linguistics: human language technologies, pages 1490–1500. Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential para- phrases. In Proceedings of The 2017 Conference on Empirical Methods on Natural Language Process- ing (EMNLP), pages 1235–1245. Association for Computational Linguistics. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. 2021. Datasets: A commu- nity library for natural language processing. arXiv preprint arXiv:2109.02846. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2020. Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark. Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang Zhai. 2018. Linkso: a dataset for learning to retrieve similar question answer pairs on software develop- ment forums. In Proceedings of the 4th ACM SIG- SOFT International Workshop on NLP for Software Engineering, pages 2–5. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analy- sis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies, pages 142–150, Port- land, Oregon, USA. Association for Computational Linguistics. Julian McAuley and Jure Leskovec. 2013. Hidden fac- tors and hidden topics: Understanding rating dimen- sions with review text. RecSys ’13, New York, NY, USA. Association for Computing Machinery. Niklas Muennighoff. 2020. Vilio: State-of-the-art visio-linguistic models applied to hateful memes. arXiv preprint arXiv:2012.07788. Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai- ley Schoelkopf, et al. 2022. Crosslingual general- ization through multitask ﬁnetuning. arXiv preprint arXiv:2211.01786. Pandu Nayak. 2019. Understanding searches better than ever before. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre- training. arXiv preprint arXiv:2201.10005. Jianmo Ni, Gustavo Hernández Ábrego, Noah Con- stant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. 2021a. Sentence-t5: Scalable sentence en- coders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus- tavo Hernández Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021b. Large dual encoders are generalizable re- trievers. arXiv preprint arXiv:2112.07899. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: To- wards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741. James O’Neill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. 2021. I wish i would have loved this one, but i didn’t – a multilingual dataset for counterfactual detection in product reviews. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch- esnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532–1543. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan- guage models are unsupervised multitask learners. OpenAI blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text trans- former. J. Mach. Learn. Res., 21(140):1–67. Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016. Task-oriented intrinsic evaluation of semantic tex- tual similarity. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 87–96. Nils Reimers and Iryna Gurevych. 2019. Sentence- bert: Sentence embeddings using siamese bert- networks. arXiv preprint arXiv:1908.10084. Facebook Research. Tatoeba multilingual test set. Andrew Rosenberg and Julia Hirschberg. 2007. V- measure: A conditional entropy-based external clus- ter evaluation measure. pages 410–420. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con- textualized affect representations for emotion recog- nition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687–3697, Brussels, Belgium. Association for Computational Linguistics. Teven Le Scao, Angela Fan, Christopher Akiki, El- lie Pavlick, Suzana Ili´c, Daniel Hesslow, Ro- man Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100. Darsh Shah, Tao Lei, Alessandro Moschitti, Salva- tore Romeo, and Preslav Nakov. 2018. Adversar- ial domain adaptation for duplicate question detec- tion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1056–1063, Brussels, Belgium. Association for Computational Linguistics. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie- Yan Liu. 2020. Mpnet: Masked and permuted pre- training for language understanding. Advances in Neural Information Processing Systems, 33:16857– 16867. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615. Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from trans- formers. arXiv preprint arXiv:1908.07490. Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab- hishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information process- ing systems, 30. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language un- derstanding systems. Advances in neural informa- tion processing systems, 32. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Ben Wang and Aran Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflol z/mesh-transformer-jax. Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. Tsdae: Using transformer-based sequential denois- ing auto-encoder for unsupervised sentence embed- ding learning. arXiv preprint arXiv:2104.06979. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self- attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural In- formation Processing Systems, 33:5776–5788. Samuel Weinbach, Marco Bellagente, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Björn Deiseroth, Koen Oost- ermeijer, Hannah Teufel, and Andres Felipe Cruz-Salinas. 2022. M-vader: A model for dif- fusion with multimodal context. arXiv preprint arXiv:2212.02936. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow- icz, et al. 2020. Transformers: State-of-the-art nat- ural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45. Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 3597– 3606. Wei Xu, Chris Callison-Burch, and William B Dolan. 2015. Semeval-2015 task 1: Paraphrase and seman- tic similarity in twitter (pit). In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 1–11. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xi- aoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2022. Making a miracl: Multilingual in- formation retrieval across a continuum of languages. arXiv preprint arXiv:2210.09984. Jeffrey Zhu, Mingqin Li, Jason Li, and Cassandra Oduola. 2021. Bing delivers more contextualized search using quantized transformer inference on nvidia gpus in azure. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2016. Towards preparation of the second bucc shared task: Detecting parallel sentences in compa- rable corpora. In Proceedings of the Ninth Workshop on Building and Using Comparable Corpora. Euro- pean Language Resources Association (ELRA), Por- toroz, Slovenia, pages 38–43. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2017. Overview of the second bucc shared task: Spotting parallel sentences in comparable cor- pora. In Proceedings of the 10th Workshop on Build- ing and Using Comparable Corpora, pages 60–67. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th workshop on building and using comparable corpora, pages 39–42. A Datasets Table 2 provides a summary along with statistics of all MTEB tasks. In the following, we give a brief description of each dataset included in MTEB. A.1 Clustering ArxivClusteringS2S, ArxivClusteringP2P, BiorxivClusteringS2S, BiorxivClusteringP2P, MedrxivClusteringP2P, MedrxivCluster- ingS2S These datasets are custom-made for MTEB using the public APIs from arXiv9 and bioRxiv/medRxiv10. For S2S datasets, the input text is simply the title of the paper, while for P2P the input text is the concatenation of the title and the abstract. The cluster labels are generated using categories given to the papers by humans. For bioRxiv and medRxiv this category is unique, but for arXiv multiple categories can be given to a single paper so we only use the ﬁrst one. For bioRxiv and medRxiv there is only one level of category (e.g. biochemistry, genetics, microbiology, etc.) hence we only perform clustering based on that label. For arXiv there is a main category and secondary category: for example \"cs.AI\" means the main category is Computer Science and the sub-category is AI, math.AG means the main category is Mathematics and the sub-category is Algrebraic Geometry etc. Hence, we create three types of splits: (a) Main category clustering Articles are only clustered based on the main category (Math, Physics, Computer Science etc.). This split evalu- ates coarse clustering capacity of a model. (b) Secondary category clustering within the same main category Articles are clustered based on their secondary category, but within a given main category, for example only Math papers that need to be clustered into Algebraic Geometry, Functional Analysis, Numerical Analysis etc. This split evaluates ﬁne-grained clustering capacity of a model, as differentiating some sub-categories can be very difﬁcult. (c) Secondary category clustering Articles are clustered based on their secondary category for all main categories, so the labels can be Number The- ory, Computational Complexity, Astrophysics of Galaxies etc. These splits evaluate ﬁne-grained 9https://arxiv.org/help/api/ 10https://api.biorxiv.org/ clustering capacity, as well as multi-scale capac- ities i.e. is a model able to both separate Maths from Physics as well as Probability from Algebraic Topology at the same time. For every dataset, split and strategy, we select subsets of all labels and then sample articles from those labels. This yields splits with a varying amount and size of clusters. RedditClustering (Geigle et al., 2021): Cluster- ing of titles from 199 subreddits. Clustering of 25 splits, each with 10-50 classes, and each class with 100 - 1000 sentences RedditClusteringP2P Dataset created for MTEB using available data from Reddit posts11. The task consists of clustering the concatenation of title+post according to their subreddit. It contains 10 splits, with 10 and 100 clusters per split and 1,000 to 100,000 posts. StackExchangeClustering (Geigle et al., 2021) Clustering of titles from 121 stackexchanges. Clus- tering of 25 splits, each with 10-50 classes, and each class with 100-1000 sentences. StackExchangeClusteringP2P Dataset created for MTEB using available data from StackEx- change posts12. The task consists of clustering the concatenation of title and post according to their subreddit. It contains 10 splits, with 10 to 100 clusters and 5,000 to 10,000 posts per split. TwentyNewsgroupsClustering13 Clustering of the 20 Newsgroups dataset, given titles of article the goal is to ﬁnd the newsgroup (20 in total). Con- tains 10 splits, each with 20 classes, with each split containing between 1,000 and 10,000 titles. A.2 Classiﬁcation AmazonCounterfactual (O’Neill et al., 2021) A collection of Amazon customer reviews annotated for counterfactual detection pair classiﬁcation. For each review the label is either \"counterfactual\" or \"not-counterfactual\". This is a multilingual dataset with 4 available languages. 11https://huggingface.co/datasets/sent ence-transformers/reddit-title-body 12https://huggingface.co/datasets/flax -sentence-embeddings/stackexchange_title _body_jsonl 13https://scikit-learn.org/0.19/datase ts/twenty_newsgroups.html Name Type Categ. #Lang. Train Dev Test Train avg. Dev avg. Test avg. Samples Samples Samples chars chars chars BUCC BitextMining s2s 4 0 0 641684 0 0 101.3 Tatoeba BitextMining s2s 112 0 0 2000 0 0 39.4 AmazonCounterfactualClassiﬁcation Classiﬁcation s2s 4 4018 335 670 107.3 109.2 106.1 AmazonPolarityClassiﬁcation Classiﬁcation p2p 1 3600000 0 400000 431.6 0 431.4 AmazonReviewsClassiﬁcation Classiﬁcation s2s 6 1200000 30000 30000 160.5 159.2 160.4 Banking77Classiﬁcation Classiﬁcation s2s 1 10003 0 3080 59.5 0 54.2 EmotionClassiﬁcation Classiﬁcation s2s 1 16000 2000 2000 96.8 95.3 96.6 ImdbClassiﬁcation Classiﬁcation p2p 1 25000 0 25000 1325.1 0 1293.8 MassiveIntentClassiﬁcation Classiﬁcation s2s 51 11514 2033 2974 35.0 34.8 34.6 MassiveScenarioClassiﬁcation Classiﬁcation s2s 51 11514 2033 2974 35.0 34.8 34.6 MTOPDomainClassiﬁcation Classiﬁcation s2s 6 15667 2235 4386 36.6 36.5 36.8 MTOPIntentClassiﬁcation Classiﬁcation s2s 6 15667 2235 4386 36.6 36.5 36.8 ToxicConversationsClassiﬁcation Classiﬁcation s2s 1 50000 0 50000 298.8 0 296.6 TweetSentimentExtractionClassiﬁcation Classiﬁcation s2s 1 27481 0 3534 68.3 0 67.8 ArxivClusteringP2P Clustering p2p 1 0 0 732723 0 0 1009.9 ArxivClusteringS2S Clustering s2s 1 0 0 732723 0 0 74.0 BiorxivClusteringP2P Clustering p2p 1 0 0 75000 0 0 1666.2 BiorxivClusteringS2S Clustering s2s 1 0 0 75000 0 0 101.6 MedrxivClusteringP2P Clustering p2p 1 0 0 37500 0 0 1981.2 MedrxivClusteringS2S Clustering s2s 1 0 0 37500 0 0 114.7 RedditClustering Clustering s2s 1 0 420464 420464 0 64.7 64.7 RedditClusteringP2P Clustering p2p 1 0 0 459399 0 0 727.7 StackExchangeClustering Clustering s2s 1 0 417060 373850 0 56.8 57.0 StackExchangeClusteringP2P Clustering p2p 1 0 0 75000 0 0 1090.7 TwentyNewsgroupsClustering Clustering s2s 1 0 0 59545 0 0 32.0 SprintDuplicateQuestions PairClassiﬁcation s2s 1 0 101000 101000 0 65.2 67.9 TwitterSemEval2015 PairClassiﬁcation s2s 1 0 0 16777 0 0 38.3 TwitterURLCorpus PairClassiﬁcation s2s 1 0 0 51534 0 0 79.5 AskUbuntuDupQuestions Reranking s2s 1 0 0 2255 0 0 52.5 MindSmallReranking Reranking s2s 1 231530 0 107968 69.0 0 70.9 SciDocsRR Reranking s2s 1 0 19594 19599 0 69.4 69.0 StackOverﬂowDupQuestions Reranking s2s 1 23018 3467 3467 49.6 49.8 49.8 ArguAna Retrieval p2p 1 0 0 10080 0 0 1052.9 ClimateFEVER Retrieval s2p 1 0 0 5418128 0 0 539.1 CQADupstackAndroidRetrieval Retrieval s2p 1 0 0 23697 0 0 578.7 CQADupstackEnglishRetrieval Retrieval s2p 1 0 0 41791 0 0 467.1 CQADupstackGamingRetrieval Retrieval s2p 1 0 0 46896 0 0 474.7 CQADupstackGisRetrieval Retrieval s2p 1 0 0 38522 0 0 991.1 CQADupstackMathematicaRetrieval Retrieval s2p 1 0 0 17509 0 0 1103.7 CQADupstackPhysicsRetrieval Retrieval s2p 1 0 0 39355 0 0 799.4 CQADupstackProgrammersRetrieval Retrieval s2p 1 0 0 33052 0 0 1030.2 CQADupstackStatsRetrieval Retrieval s2p 1 0 0 42921 0 0 1041.0 CQADupstackTexRetrieval Retrieval s2p 1 0 0 71090 0 0 1246.9 CQADupstackUnixRetrieval Retrieval s2p 1 0 0 48454 0 0 984.7 CQADupstackWebmastersRetrieval Retrieval s2p 1 0 0 17911 0 0 689.8 CQADupstackWordpressRetrieval Retrieval s2p 1 0 0 49146 0 0 1111.9 DBPedia Retrieval s2p 1 0 4635989 4636322 0 310.2 310.1 FEVER Retrieval s2p 1 0 0 5423234 0 0 538.6 FiQA2018 Retrieval s2p 1 0 0 58286 0 0 760.4 HotpotQA Retrieval s2p 1 0 0 5240734 0 0 288.6 MSMARCO Retrieval s2p 1 0 8848803 8841866 0 336.6 336.8 MSMARCOv2 Retrieval s2p 1 138641342 138368101 0 341.4 342.0 0 NFCorpus Retrieval s2p 1 0 0 3956 0 0 1462.7 NQ Retrieval s2p 1 0 0 2684920 0 0 492.7 QuoraRetrieval Retrieval s2s 1 0 0 532931 0 0 62.9 SCIDOCS Retrieval s2p 1 0 0 26657 0 0 1161.9 SciFact Retrieval s2p 1 0 0 5483 0 0 1422.3 Touche2020 Retrieval s2p 1 0 0 382594 0 0 1720.1 TRECCOVID Retrieval s2p 1 0 0 171382 0 0 1117.4 BIOSSES STS s2s 1 200 200 200 156.6 156.6 156.6 SICK-R STS s2s 1 19854 19854 19854 46.1 46.1 46.1 STS12 STS s2s 1 4468 0 6216 100.7 0 64.7 STS13 STS s2s 1 0 0 3000 0 0 54.0 STS14 STS s2s 1 0 0 7500 0 0 54.3 STS15 STS s2s 1 0 0 6000 0 0 57.7 STS16 STS s2s 1 0 0 2372 0 0 65.3 STS17 STS s2s 11 0 0 500 0 0 43.3 STS22 STS p2p 18 0 0 8060 0 0 1992.8 STSBenchmark STS s2s 1 11498 3000 2758 57.6 64.0 53.6 SummEval Summarization p2p 1 0 0 2800 0 0 359.8 Table 2: Tasks in MTEB AmazonPolarity (McAuley and Leskovec, 2013) A collection of Amazon customer reviews annotated for polarity classiﬁcation. For each review the label is either \"positive\" or \"negative\". AmazonReviews (McAuley and Leskovec, 2013) A collection of Amazon reviews designed to aid research in multilingual text classiﬁcation. For each review the label is the score given by the review between 0 and 4 (1-5 stars). This is a multilingual dataset with 6 available languages. Banking77 (Casanueva et al., 2020) Dataset composed of online banking queries annotated with their corresponding intents. For each user query the label is an intent among 77 intents like ’acti- vate_my_card’, ’apple_pay’, ’bank_transfer’, etc. Emotion (Saravia et al., 2018) Dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. Imdb (Maas et al., 2011) Large movie review dataset with labels being positive or negative. MassiveIntent (FitzGerald et al., 2022) A col- lection of Amazon Alexa virtual assistant utter- ances annotated with the associated intent. For each user utterance the label is one of 60 intents like ’play_music’, ’alarm_set’, etc. This is a multi- lingual dataset with 51 available languages. MassiveScenario (FitzGerald et al., 2022) A col- lection of Amazon Alexa virtual assistant utter- ances annotated with the associated intent. For each user utterance the label is a theme among 60 scenarios like ’music’, ’weather’, etc. This is a multilingual dataset with 51 available languages. MTOPDomain / MTOPIntent Multilingual sentence datasets from the MTOP (Li et al., 2020) benchmark. We refer to their paper for details. ToxicConversations Dataset from Kaggle com- petition14. Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not. TweetSentimentExtraction Dataset from Kag- gle competition15. Sentiment classiﬁcation of tweets as neutral, positive or negative. A.3 Pair Classiﬁcation SprintDuplicateQuestions (Shah et al., 2018): Collection of questions from the Sprint commu- nity. The goal is to classify a pair of sentences as duplicates or not. TwitterSemEval2015 (Xu et al., 2015) Paraphrase-Pairs of Tweets from the SemEval 2015 workshop. The goal is to classify a pair of tweets as paraphrases or not. 14https://www.kaggle.com/competitions/ jigsaw-unintended-bias-in-toxicity-class ification 15https://www.kaggle.com/competitions/ tweet-sentiment-extraction TwitterURLCorpus (Lan et al., 2017) Paraphrase-Pairs of Tweets. The goal is to classify a pair of tweets as paraphrases or not. A.4 Bitext Mining BUCC (Zweigenbaum et al., 2016, 2017, 2018) BUCC provides big set of sentences (∼ 10-70k each) for English, French, Russian, German and Chinese, along with associated pairs annotation. The annotated pairs here corresponds to a pairs of translated sentences, i.e. a sentence and its transla- tion in the other language. Tatoeba (Research) Tatoeba provides sets of sen- tences (1000 sentences each) for 112 languages with annoated associated pairs. Each pair is one sentence and its translation in another language. A.5 Reranking AskUbuntuDupQuestions16 Questions from AskUbuntu with manual annotations marking pairs of questions as similar or dissimilar. MindSmall (Wu et al., 2020) Large-scale En- glish Dataset for News Recommendation Research. Ranking news article titles given the title of a news article. The idea is to recommend other news from the one you are reading. SciDocsRR (Cohan et al., 2020b) Ranking of re- lated scientiﬁc papers based on their title. StackOverﬂowDupQuestions (Liu et al., 2018) Stack Overﬂow Duplicate Questions Task for ques- tions with the tags Java, JavaScript and Python, ranking questions as duplicates or not. A.6 Semantic Textual Similarity (STS) STS12, STS13, STS14, STS15, STS16, STS17, STS22, STSBenchmark (Agirre et al., 2012, 2013)17181920 Original STS benchmark, with scores from 0 to 5. The selection of sentences includes text from image captions, news headlines and user forums. In total they contain between 1,000 and 20,000 sentences. STS12 - STS16 and 16https://github.com/taolei87/askubuntu 17https://alt.qcri.org/semeval2014/tas k10/ 18https://alt.qcri.org/semeval2015/tas k2/ 19https://alt.qcri.org/semeval2016/tas k1/ 20https://competitions.codalab.org/com petitions/33835 STSBenchmark are monolingual english bench- marks. STS17 and STS22 contain crosslingual pairs of sentences, where the goal is to assess the similarity of two sentences in different languages. STS17 has 11 language pairs (among Korean, Ara- bic, English, French, German, Turkish, Spanish, Italian and Dutch) and STS22 has 18 language pairs (among Arabic, English, French, German, Turkish, Spanish, Polish, Italian, Russian and Chinese). BIOSSES21 Contains 100 sentence pairs from the biomedical ﬁeld. SICK-R (Agirre et al., 2014) Sentences Involv- ing Compositional Knowledge (SICK) contains a large number of sentence pairs (10 0000) that are lexically, syntactically and semantically rich. A.7 Summarization SummEval (Fabbri et al., 2020) Summaries gen- erated by recent summarization models trained on CNN or DailyMail alongside human annotations. A.8 Retrieval We refer to the BEIR paper (Thakur et al., 2021), which contains description of each dataset. For MTEB, we include all publicly available datasets: ArguAna, ClimateFEVER, CQADupstack, DB- Pedia, FEVER, FiQA2018, HotpotQA, MS- MARCO, NFCorpus, NQ, Quora, SCIDOCS, SciFact, Touche2020, TRECCOVID. B Limitations of MTEB While MTEB aims to be a diverse benchmark to provide holistic performance reviews, the bench- mark has its limitations. We list them here: 1. Long document datasets MTEB covers mul- tiple text lengths (S2S, P2P, S2P), but very long documents are still missing. The longest datasets in MTEB have a few hundred words, and longer text sizes could be relevant for use cases like retrieval. 2. Task imbalance Tasks in MTEB have a differ- ent amount of datasets with summarization consist- ing of only a single dataset. This means MTEB av- erage scores, which are computed over all datasets, are biased towards tasks with many datasets, no- tably retrieval, classiﬁcation and clustering. As MTEB grows, we hope to add more datasets to cur- rently underrepresented tasks like summarization or pair classiﬁcation. 21https://tabilab.cmpe.boun.edu.tr/BIO SSES/DataSet.html 3. Multinguality MTEB contains multilingual classiﬁcation, STS and bitext mining datasets. However, retrieval and clustering are English-only. SGPT-BLOOM-7B1-msmarco is geared towards multilingual retrieval datasets and due to the lack thereof cannot be comprehensively benchmarked in MTEB. Further, MTEB does not contain any code datasets that could be used to benchmark code models (Neelakantan et al., 2022; Allal et al., 2023). It should be easy to extend MTEB with datasets, such as CodeSearchNet (Husain et al., 2019), TyDI QA (Clark et al., 2020), XOR QA (Asai et al., 2020) or MIRACL (Zhang et al., 2022). 4. Additional modalities Text embeddings are commonly used as input features for downstream models, such as in our classiﬁcation task. This can involve other modalities, notably image con- tent (Carvalho et al., 2018; Tan and Bansal, 2019; Muennighoff, 2020; Nichol et al., 2021; Saharia et al., 2022; Weinbach et al., 2022). We have fo- cused solely on natural language applications and leave extensive benchmarking of text embeddings as inputs for other modalities to future work. C Examples Tables 3-9 provide examples for each dataset for each task. For retrieval datasets, we refer to the BEIR paper (Thakur et al., 2021). D Correlations Figure 6 provides correlation heatmaps for model performance and MTEB tasks. E Models Table 10 provides publicly available model check- points used for MTEB evaluation. F Additional results Tables 11 until the end provide results on individ- ual datasets of MTEB. The results are additionally available in json format on the Hugging Face Hub22 and can be inspected on the leaderboard23. 22https://huggingface.co/datasets/mteb /results 23https://huggingface.co/spaces/mteb/l eaderboard Dataset Text Label AmazonCounterfactualClassiﬁcation In person it looks as though it would have cost a lot more. counterfactual AmazonPolarityClassiﬁcation an absolute masterpiece I am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda’s music contributed greatly to the... positive AmazonReviewsClassiﬁcation solo llega una unidad cuando te obligan a comprar dos Te obligan a comprar dos unidades y te llega solo una y no hay forma de reclamar, una autentica estafa, no compreis!! 0 Banking77Classiﬁcation What currencies is an exchange rate calculated in? exchange_rate EmotionClassiﬁcation i feel so inhibited in someone elses kitchen like im painting on someone elses picture sadness ImdbClassiﬁcation When I ﬁrst saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York’s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe... negative MassiveIntentClassiﬁcation réveille-moi à neuf heures du matin le vendredi alarm_set MassiveScenarioClassiﬁcation tell me the artist of this song music MTOPDomainClassiﬁcation Maricopa County weather forecast for this week weather MTOPIntentClassiﬁcation what ingredients do is have left GET_INFO_RECIPES ToxicConversationsClassiﬁcation The guy’s a damn cop, so what do you expect? toxic TweetSentimentExtractionClassiﬁcation I really really like the song Love Story by Taylor Swift positive Table 3: Classiﬁcation examples Dataset Text Cluster ArxivClusteringP2P Finite groups of rank two which do not involve Qd(p). Let p > 3 be a prime. We show that if G is a ﬁnite group with p-rank equal to 2, then G involves Qd(p) if and only if G p′-involves Qd(p). This allows us to use a version of Glauberman’s ZJ-theorem to give a more direct construction of ﬁnite group actions on mod-p homotopy spheres. We give an example to illustrate that the above conclusion does not hold for p ≤ 3. math ArxivClusteringS2S Vertical shift and simultaneous Diophantine approximation on polynomial curves math BiorxivClusteringP2P Innate Immune sensing of Inﬂuenza A viral RNA through IFI16 promotes pyroptotic cell death Programmed cell death pathways are triggered by various stresses or stimuli, including viral infections. The mechanism underlying the regula- tion of these pathways upon Inﬂuenza A virus IAV infection is not well characterized. We report that a cytosolic DNA sensor IFI16 is... immunology BiorxivClusteringS2S Association of CDH11 with ASD revealed by matched-gene co-expression analysis and mouse behavioral neuroscience MedrxivClusteringP2P Temporal trends in the incidence of haemophagocytic lymphohistiocytosis: a nationwide cohort study from England 2003-2018. Haemophagocytic lymphohistiocytosis (HLH) is rare, results in high mortality and is increasingly being diagnosed. Little is known about what is driving the apparent rise in the incidence of this disease. Using national linked electronic health data from hospital admissions and death certiﬁcation cases of HLH that were diagnosed in England between 1/1/2003 and 31/12/2018 were identiﬁed using a previously validated approach. We calculated incidence... infectious diseases MedrxivClusteringS2S Current and Lifetime Somatic Symptom Burden Among Transition-aged Young Adults on the Autism Spectrum psychiatry and clinical psychology RedditClustering Could anyone tell me what breed my bicolor kitten is? r/cats RedditClusteringP2P Headaches after working out? Hey guys! I’ve been diagnosed with adhd since I was seven. I just recently got rediag- nosed (22f) and I’ve been out on a different medication, adderall I was normally taking vyvanse but because of cost and no insurance adderall was more affordable. I’ve noticed that if I take adderall and workout... r/ADHD StackExchangeClustering Does this property characterize a space as Hausdorff? math.stackexchange.com StackExchangeClusteringP2P Google play services error DEBUG: Application is pausing, which disconnects the RTMP client. I am having this issue from past day with Google Play Services Unity. What happens is, when I install app directly ot device via Unity, the Google Play Services work ﬁne but when I upload it as beta to play store console and install it via that then it starts to give \" DEBUG: Application is pausing, which disconnects the RTMP client\" error. I have a proper SHA1 key. unity TwentyNewsgroupsClustering Commercial mining activities on the moon 14 Table 4: Clustering examples Dataset Sentence 1 Sentence 2 Label SprintDuplicateQuestions Franklin U722 USB modem signal strength How do I know if my Franklin U772 USB Modem has a weak signal ? 1 TwitterSemEval2015 All the home alones watching 8 mile\",\"All the home alones watching 8 mile The last rap battle in 8 Mile nevr gets old ahah 0 TwitterURLCorpus How the metaphors we use to describe discovery affect men and women in the sciences Light Bulbs or Seeds ? How Metaphors for Ideas Inﬂuence Judgments About Genius 0 Table 5: Pair classiﬁcation examples. Labels are binary. Dataset Query Positive Negative AskUbuntuDupQuestions change the application icon theme but not changing the panel icons change folder icons in ubuntu-mono-dark theme change steam tray icon back to default MindSmallReranking Man accused in probe of Giuliani associates is freed on bail Studies show these are the best and worst states for your retirement There are 14 cheap days to ﬂy left in 2019: When are they and what deals can you score? SciDocsRR Discovering social circles in ego networks Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communi- ties. Improving www proxies performance with greedy-dual- size-frequency caching policy StackOverﬂowDupQuestions Java launch error selection does not contain a main type Error: Selection does not contain a main type Selection Sort in Java Table 6: Reranking examples Dataset Sentence 1 Sentence 2 Score BIOSSES It has recently been shown that Craf is essential for Kras G12D-induced NSCLC. It has recently become evident that Craf is essential for the onset of Kras-driven non-small cell lung cancer. 4.0 SICK-R A group of children is playing in the house and there is no man standing in the background A group of kids is playing in a yard and an old man is stand- ing in the background 3.2 STS12 Nationally, the federal Centers for Disease Control and Pre- vention recorded 4,156 cases of West Nile, including 284 deaths. There were 293 human cases of West Nile in Indiana in 2002, including 11 deaths statewide. 1.7 STS13 this frame has to do with people ( the residents ) residing in locations , sometimes with a co-resident . inhabit or live in ; be an inhabitant of ; 2.8 STS14 then the captain was gone. then the captain came back. 0.8 STS15 you ’ll need to check the particular policies of each pub- lisher to see what is allowed and what is not allowed. if you need to publish the book and you have found one publisher that allows it. 3.0 STS16 you do not need to worry. you don ’t have to worry. 5.0 STS17 La gente muestra su afecto el uno por el otro. A women giving something to other lady. 1.4 STS22 El secretario general de la Asociación Gremial de los Tra- bajadores del Subte y Premetro de Metrodelegados, Beto Pianelli, dijo que el Gobierno porteño debe convocar “in- mediatamente” a licitación para la compra de nuevos trenes y retirar los que quedan en circulación... En diálogo con el servicio informativo de la Radio Pública, el ministro de Salud de la Nación, Ginés González García, habló sobre el avance del coronavirus en la Argentina y se manifestó a favor de prorrogar la cuarentena obligatoria dis- puesta por... 1 STSBenchmark A man is playing the cello. A man seated is playing the cello. 4.25 Table 7: STS examples. Scores are continuous between 0 and 5 (included). Dataset First set sentence Second set sentence BUCC Morales remporte l’élection présidentielle de 2005 à la ma- jorité absolue. Morales went on to win the 2005 presidential election with an absolute majority. Tatoeba Chi le ha detto che Tom l’ha fatto? Who told you that Tom did that? Table 8: Bitext mining examples Dataset Human Summary Machine Summary Relevance SummEval V. Stiviano must pay back $2.6 million in gifts from Donald Sterling. Sterling’s wife claimed the ex-Clippers used the couple’s money for the gifts. The items included a Ferrari, two Bentleys and a Range Rover. donald sterling , nba team last year . sterling ’s wife sued for $ 2.6 million in gifts . sterling says he is the former female companion who has lost the . sterling has ordered v. stiviano to pay back $ 2.6 m in gifts after his wife sued . sterling also includes a $ 391 easter bunny costume , $ 299 and a $ 299 . 1.7 Table 9: Summarization exampleGloveKomninosBERTSimCSE-BERT-unsupSimCSE-BERT-supcoCondenser-msmarcoContrieverSPECTERLaBSELASER2MiniLM-L6MiniLM-L12MiniLM-L12-multilingualMPNetMPNet-multilingualSGPT-125M-nliSGPT-5.8B-nliSGPT-125M-msmarcoSGPT-1.3B-msmarcoSGPT-2.7B-msmarcoSGPT-5.8B-msmarcoSGPT-BLOOM-7.1B-msmarcoGTR-BaseGTR-LargeGTR-XLGTR-XXLST5-BaseST5-LargeST5-XLST5-XXL Glove Komninos BERT SimCSE-BERT-unsup SimCSE-BERT-sup coCondenser-msmarco Contriever SPECTER LaBSE LASER2 MiniLM-L6 MiniLM-L12 MiniLM-L12-multilingual MPNet MPNet-multilingual SGPT-125M-nli SGPT-5.8B-nli SGPT-125M-msmarco SGPT-1.3B-msmarco SGPT-2.7B-msmarco SGPT-5.8B-msmarco SGPT-BLOOM-7.1B-msmarco GTR-Base GTR-Large GTR-XL GTR-XXL ST5-Base ST5-Large ST5-XL ST5-XXL 98 90 88 97 95 93 95 94 92 99 95 93 85 95 94 90 89 79 91 90 98 93 92 89 92 91 90 84 96 94 92 98 97 95 91 94 94 94 91 97 96 90 85 90 97 91 90 78 91 90 97 97 88 92 87 90 88 77 91 90 97 97 87 91 86 100 95 93 85 96 96 98 96 91 95 92 97 97 89 88 78 90 90 96 96 87 90 86 99 99 96 94 93 85 97 96 98 97 89 95 93 97 97 99 95 97 96 91 99 98 95 91 94 97 96 92 91 97 91 96 96 95 89 98 98 97 94 91 96 94 94 94 97 93 98 99 92 90 78 91 89 96 96 87 89 85 95 96 96 94 95 93 95 89 87 75 88 87 96 97 83 87 82 96 96 94 94 95 89 94 99 87 86 73 87 85 95 97 81 85 80 95 96 94 94 94 88 93 98 100 84 82 69 83 81 92 95 78 81 77 93 94 91 92 91 84 90 97 99 99 84 83 69 83 81 92 94 78 82 78 94 95 91 93 91 84 90 97 98 99 100 87 85 74 88 87 96 98 79 87 83 96 97 95 95 95 88 92 96 97 97 96 95 85 83 72 87 86 95 98 76 84 80 95 96 93 95 94 86 90 94 97 97 95 94 100 85 83 72 86 85 95 97 76 84 79 95 96 93 95 94 85 90 94 97 97 95 95 100 100 84 82 71 85 85 94 97 75 84 79 95 96 92 95 93 84 89 93 96 96 94 94 99 100 100 94 92 87 97 97 96 93 88 95 93 94 93 96 94 97 97 97 91 90 89 85 85 92 91 91 91 92 90 85 95 96 94 93 86 93 91 93 93 95 94 96 95 97 90 90 89 86 86 92 92 92 92 99 92 90 84 94 95 94 93 85 92 90 93 93 94 94 95 94 96 91 91 90 87 87 93 93 93 92 99 100 90 88 81 92 93 94 94 83 90 87 93 94 94 95 95 92 96 92 93 92 90 90 95 95 95 94 97 99 99 70 75 80 85 90 95 100 (a) Model correlation based on all results Class. Clust. PairClass. Rerank. Retr. STS Summ.Class.Clust.PairClass.Rerank.Retr.STSSumm. 68 72 81 58 95 83 57 85 87 90 79 75 85 78 69 3 -4 8 -8 -16 0 0 20 40 60 80 100 (b) Task correlation based on average task results Figure 6: Pearson correlations across model and task results. Left: Size variants of the same architecture show high correlations. Right: Performance on clustering and reranking correlates strongest, while summarization and classiﬁcation show weaker correlation with other tasks. Model Public Checkpoint Glove https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B.300d Komninos https://huggingface.co/sentence-transformers/average_word_embeddings_komninos BERT https://huggingface.co/bert-base-uncased SimCSE-BERT-unsup https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased SimCSE-BERT-sup https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased coCondenser-msmarco https://huggingface.co/sentence-transformers/msmarco-bert-co-condensor Contriever https://huggingface.co/nthakur/contriever-base-msmarco SPECTER https://huggingface.co/sentence-transformers/allenai-specter LaBSE https://huggingface.co/sentence-transformers/LaBSE LASER2 https://github.com/facebookresearch/LASER MiniLM-L6 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 MiniLM-L12 https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 MiniLM-L12-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 MPNet https://huggingface.co/sentence-transformers/all-mpnet-base-v2 MPNet-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 MiniLM-L12-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 SGPT-125M-nli https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-nli-bitfit SGPT-5.8B-nli https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit SGPT-125M-msmarco https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit SGPT-1.3B-msmarco https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit SGPT-2.7B-msmarco https://huggingface.co/Muennighoff/SGPT-2.7B-weightedmean-msmarco-specb-bitfit SGPT-5.8B-msmarco https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit SGPT-BLOOM-7.1B-msmarco https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco SGPT-BLOOM-1.7B-nli https://huggingface.co/bigscience-data/sgpt-bloom-1b7-nli GTR-Base https://huggingface.co/sentence-transformers/gtr-t5-base GTR-Large https://huggingface.co/sentence-transformers/gtr-t5-large GTR-XL https://huggingface.co/sentence-transformers/gtr-t5-xl GTR-XXL https://huggingface.co/sentence-transformers/gtr-t5-xxl ST5-Base https://huggingface.co/sentence-transformers/sentence-t5-base ST5-Large https://huggingface.co/sentence-transformers/sentence-t5-large ST5-XL https://huggingface.co/sentence-transformers/sentence-t5-xl ST5-XXL https://huggingface.co/sentence-transformers/sentence-t5-xxl Table 10: Publicly available model links used for evaluationDatasetGloveKomninosBERTSimCSE-SimCSE-coCondenser-Contr-SPECTERLaBSELASER2MiniLM-MiniLM-MiniLM-MPNetMPNet-OpenAISGPT-125M-SGPT-5.8B-SGPT-125M-SGPT-1.3B-SGPT-2.7B-SGPT-5.8B-SGPT-GTR-GTR-GTR-GTR-ST5-ST5-ST5-ST5-BERT-BERT-msmarcoieverL6L12-L12-multilingualAdanlinlimsmarcomsmarcomsmarcomsmarcoBLOOM-7.1B-BaseLargeXLXXLBaseLargeXLXXLunsupsupmultilingualSimilaritymsmarcoAmazonCounterfactualClassiﬁcation56.9160.5474.2567.0975.7564.0672.1958.7075.9376.8464.1565.2871.5765.2775.8176.4065.8874.0761.2465.2167.5769.2268.0669.3370.0368.6067.3075.8275.5176.0177.07AmazonPolarityClassiﬁcation60.3259.5971.3374.4882.4766.8868.6357.7768.9561.0162.5862.9869.2167.1376.4192.8374.9482.3165.4073.2171.4471.2668.9767.8273.9274.5875.0585.1292.8793.1792.79AmazonReviewsClassiﬁcation29.6731.0133.5633.8539.6034.8537.4226.2635.8028.7131.7930.7935.1131.9238.5147.4535.1041.5831.1734.9635.7539.1933.8638.4837.2138.2037.3044.9447.1248.1848.93Banking77Classiﬁcation67.6967.0563.4173.5575.7682.3580.0266.6669.8557.7679.7580.4079.7781.8681.0768.0474.6881.7477.7082.0683.2284.4984.3379.2681.2182.2282.3276.4878.4680.8882.31EmotionClassiﬁcation36.9333.1835.2842.2244.8141.9144.7724.8237.2224.8338.4341.1742.3739.7345.8450.3242.2349.9239.0846.3949.2149.6644.8742.2046.3245.5543.1951.3651.7351.9548.57ImdbClassiﬁcation62.5763.9865.3569.6373.5360.1767.0456.3562.0457.5860.6659.7660.4670.7264.5789.3862.9074.3358.6764.0563.5366.6461.7765.9970.8668.1570.877.3487.0187.5490.23MassiveIntentClassiﬁcation56.1957.2159.8859.8465.9570.4067.7851.7361.4647.9167.4067.1566.8469.5769.3265.1758.0870.061.4168.6569.0170.3969.6767.0570.0670.2370.6169.7471.7872.0973.44MassiveScenarioClassiﬁcation66.0366.1164.2866.2570.7873.7376.0058.5866.4155.9275.7674.5871.5176.0175.3567.6766.3475.0369.7476.0475.9076.2875.3475.4075.4975.9477.7772.3273.1673.2674.82MTOPDomainClassiﬁcation79.1178.5782.6381.7184.2991.3493.1874.5386.0675.3691.5691.9087.0692.0889.2489.8981.5289.6486.9692.0892.5693.4793.6892.4294.0193.6093.8490.3490.9990.7392.49MTOPIntentClassiﬁcation55.8557.0768.1459.2363.1471.0769.3150.0563.0349.4762.1862.8465.5270.2168.6964.8058.2470.6862.2571.1971.8572.4271.3462.4463.8665.9367.7163.3264.9868.1568.33ToxicConversationsClassiﬁcation65.4067.7670.068.8272.0464.0167.7757.4466.9054.0566.9967.4766.0760.8671.0270.0062.7969.9362.6668.7368.8467.7166.5566.6068.6567.5668.4868.2071.7370.9570.04TweetSentimentExtractionClassiﬁcation50.8049.6851.8153.3659.7355.7456.1045.5258.8248.7355.4154.2556.1255.4659.0363.3554.8262.4452.4155.6756.6956.8555.8556.0254.0954.7754.5462.7162.3361.2162.01ArxivClusteringP2P32.5634.7335.1932.6135.1836.9442.6144.7532.1317.7746.5546.0738.3348.3837.7841.4934.7440.5539.7143.3844.7245.5944.5935.4937.5037.9037.9039.2841.6241.6242.89ArxivClusteringS2S23.1426.0127.5124.6827.5429.0332.3235.2722.0512.3937.8637.5031.5539.7231.6828.4724.6832.4928.2433.7135.0838.8638.0327.1830.5530.4532.3927.2629.4431.1733.47BiorxivClusteringP2P29.2729.7630.1224.9030.1532.3534.9739.5229.8412.4038.4836.9933.4939.6233.0936.8628.9333.5933.6335.0634.4136.5536.0327.6629.5930.5230.4833.9935.9936.4336.53BiorxivClusteringS2S19.1820.7124.7719.5524.6728.1629.0834.5320.578.8333.1733.2129.4435.0229.6027.5523.0829.1327.0430.7130.5333.7032.4823.2525.7226.0627.5022.9224.0226.4728.66MedrxivClusteringP2P26.1226.6526.0923.6026.2530.2331.1935.0430.1317.9134.4134.2531.5235.5831.9631.0928.3030.3331.3732.0831.3531.5131.0527.5728.7228.6929.1233.2032.4032.3032.09MedrxivClusteringS2S20.3821.5023.6021.9724.1227.0127.2731.6624.8216.6332.2932.2430.8732.8731.7026.5024.9328.0226.8729.4528.7728.7629.2625.1327.3926.6927.5626.1326.3326.9326.82RedditClustering28.4628.8427.2432.1840.2348.0454.8924.1328.799.9650.6751.1842.0254.8245.2442.4733.7642.1740.2348.2346.4740.4535.5356.1361.6961.3464.1352.9354.5357.0358.99RedditClusteringP2P35.827.3743.3245.1447.7453.5357.5835.0649.1426.4254.1554.8050.7356.7751.3158.1041.0148.0249.0953.1854.1755.7554.5258.5361.6761.1162.8459.6762.5062.3464.46StackExchangeClustering35.8039.0443.5843.0747.5559.5463.1539.0135.4315.7953.3653.0549.6053.8052.9853.5244.5954.1352.7460.8659.1959.2155.1364.2169.9369.9571.4363.1365.1167.1370.78StackExchangeClusteringP2P28.5130.2326.5528.5029.4530.4832.2531.4628.8318.6338.0033.1331.6934.2832.9430.4328.2331.1232.6632.3632.5733.9534.3133.0133.2132.7332.8535.6836.8634.7935.25TwentyNewsgroupsClustering25.8327.4223.3523.2134.8638.6846.8224.2223.2811.3846.8647.4739.2849.7444.1036.2628.2437.2032.1340.0640.8939.4637.2846.7251.6451.1550.4448.1049.3349.5350.93SprintDuplicateQuestions86.9685.5536.8169.4169.3996.0995.5571.6389.2665.5494.5592.4589.4690.1590.5577.8577.7380.5489.8992.5893.4793.8494.9394.5595.0595.4595.6891.2389.0191.4488.89TwitterSemEval201548.4553.8555.9060.2167.7565.9566.8543.2562.7859.5767.8670.0262.0673.8566.7569.0457.0966.0054.7562.3763.6866.8765.3172.2376.0377.8177.5478.2579.7580.8980.28TwitterURLCorpus77.3579.4176.2981.3783.8983.1785.2169.2284.5881.4784.7084.7783.8385.1185.1483.6980.5184.5481.0683.7984.8085.2985.4684.7784.8985.1485.1386.0586.1485.8686.01AskUbuntuDupQuestions49.5750.8845.8451.5751.8058.9956.6950.0752.7548.9963.4864.0660.4965.8560.1653.4952.6355.9055.8458.1359.6361.6359.9760.8661.6463.0863.2359.7361.5162.8666.16MindSmallReranking27.0128.9228.3728.6229.3027.1331.5824.8029.8124.7930.8031.0230.3730.9730.1530.7129.2731.1130.4031.3431.7232.2931.7931.3331.8431.5031.9330.2030.2729.7730.60SciDocsRR62.5663.5564.9466.3370.1472.7876.5181.3168.7254.9987.1287.2077.7888.6578.0971.0468.3677.5471.3477.2177.7280.7979.7773.7176.3976.4977.9673.9674.8875.1676.09StackOverﬂowDupQuestions34.0335.6534.6239.3538.9048.4847.7836.2242.4236.9850.7651.4745.8551.9846.7940.8539.9744.7744.7449.3249.6151.5351.0751.0151.5852.7953.5048.4649.3451.0552.85ArguAna36.3030.9628.2938.3438.3345.1548.3232.6734.1812.8650.1747.1344.8846.5248.9139.6531.0435.0745.4249.6850.4951.3847.2850.8352.0952.8153.7744.8539.2739.4039.85ClimateFEVER14.4414.875.4111.8011.9816.9624.796.863.830.3620.2721.5718.4921.9715.272.8311.0117.5721.8626.627.1130.4629.3924.8826.9027.0127.2110.3711.3610.6114.63CQADupstackRetrieval15.4716.795.5113.2214.5027.7233.6714.6018.754.1241.3242.5330.7144.9631.3210.1720.2929.9827.2533.3336.5339.4039.6234.5536.6237.3538.5635.2338.9640.7844.65DBPedia18.2915.884.1315.0419.7327.8638.104.1415.571.5332.3333.3622.6332.0926.223.4810.8726.1022.7231.5134.7039.8739.0335.2439.5539.7441.2827.7731.5533.6539.19FEVER14.9915.563.3021.0520.4145.6859.295.4512.170.7751.9355.9152.6650.8656.764.4518.4038.6460.4568.1272.7378.2473.9768.9372.6672.1874.0826.1636.2136.1251.20FiQA201810.0910.492.199.8410.4115.6227.425.647.001.7336.8737.2720.3349.9622.967.548.9418.5921.1229.9933.2937.2035.8435.1542.7944.1946.7834.8343.5544.7146.68HotpotQA19.1820.778.2619.7522.8935.6156.815.4618.755.5046.5144.5930.0139.2937.0312.617.7333.9940.8849.9352.8459.2657.2654.9357.8558.9159.6733.2033.9537.1742.14MSMARCO9.609.751.919.3511.0029.5736.775.587.601.0936.5439.0323.7239.7526.6010.536.2715.8327.9836.0538.8339.9141.1241.1642.7343.5244.0520.7123.9625.1727.68NFCorpus13.8711.794.309.8812.4222.2931.310.8416.542.4431.5932.2523.4533.2925.4920.5911.8028.2622.7932.0833.8936.2135.7830.2232.6333.3434.1828.6431.1033.1835.08NQ12.8712.752.6111.6916.0829.8541.835.998.420.6443.8746.4729.8050.4533.602.027.6324.6329.7342.9446.7052.4153.1550.4755.0956.1657.2436.3242.0246.2952.87QuoraRetrieval71.3271.5861.0378.0379.6286.5186.7264.6577.0371.1487.5687.7586.5587.4686.4182.1878.9684.6872.9885.2885.6084.5874.7187.9888.4788.9189.0985.4985.7385.8585.96SCIDOCS8.048.472.815.507.5310.1317.120.005.630.7821.6421.820.0323.7713.966.287.1313.5512.2116.1816.5719.8718.6214.0015.5115.7115.8814.1615.3815.9717.17SciFact29.5829.5313.3425.7229.5952.3165.5147.8838.204.0464.5162.6448.3765.5750.3045.4631.7946.6656.9068.2970.1774.7072.1159.7463.4264.2066.7745.7649.9150.9155.38Touche202013.9913.170.978.909.898.5715.798.464.881.0616.9017.2216.0619.9317.403.112.2716.1822.9724.4523.4425.4323.9825.8928.2925.2626.7620.3021.6322.5121.65TRECCOVID36.2235.9214.7426.222.9340.5444.7729.9116.3410.9747.2550.8239.1251.3337.8724.5639.3155.3570.3072.9875.1784.8881.3756.0556.6860.0951.9040.7046.1154.7759.48BIOSSES44.9350.2554.7072.3168.3877.3283.3264.9578.7062.0181.6483.5774.1880.4376.2778.0470.9379.5075.2183.0284.8486.2585.3179.0084.8678.9481.9175.8978.9373.1280.43SICK-R55.4355.4958.6572.2480.7772.0070.2056.3969.9962.8677.5879.3279.6180.5979.6277.4874.5779.5965.9367.2368.2069.6369.8271.4573.3973.6374.2980.1880.3479.9880.47STS1254.6453.5130.8766.0575.3068.1964.3462.4965.0862.6072.3773.0876.0272.6377.9072.3069.1774.2966.5366.5966.9967.5069.6668.5970.3369.1170.1278.0579.1179.0278.85STS1369.1670.8059.8981.4984.6780.4080.0358.7067.9859.6280.6082.1380.7083.4885.1181.4977.2385.3576.1777.3377.5879.1679.6779.0982.1981.8282.7285.8587.3388.8088.94STS1460.8163.5647.7373.6180.1974.0274.5154.8764.0357.0375.5976.7378.8578.0080.8174.7470.9979.2169.0571.8372.7874.4674.6174.6477.1677.0778.2482.1983.1784.3384.86STS1572.3174.0860.2979.7285.4082.5783.3062.5476.5971.5785.3985.5885.8485.6687.4884.2879.7485.5279.2480.6682.6284.4783.8184.8586.3186.0186.2687.4688.2888.8989.32STS1665.3464.6063.7378.1280.8279.7879.6764.2772.9870.7578.9980.2381.0580.0383.2082.0677.9382.5476.0778.9180.1080.9680.4081.5781.8582.2381.6184.0384.3685.3184.67STS1777.9576.9164.1083.5889.4485.9486.3269.6379.4576.7387.5988.6386.8790.6086.9987.0887.3390.4484.9586.9987.2587.7887.0785.8083.9384.9085.1889.5788.9988.9189.46STS2256.3553.8956.3759.6561.9667.5464.6455.0660.9739.7567.2165.6761.7267.9563.0664.7159.6463.2065.6667.3068.7569.3566.1366.1764.3066.6165.7662.6662.3964.3265.33STSBenchmark61.5461.5547.2976.5284.2576.9778.8161.2672.2569.7782.0383.0984.4283.4286.8283.7879.5485.6775.3477.5979.2181.3980.9079.5877.6077.6577.7385.5285.3683.9384.01SummEval28.8730.4929.8231.1523.3129.5030.3627.6631.0526.830.8127.930.6727.4931.5726.9430.2630.3828.9025.4427.8724.7524.9929.6729.5030.2130.6431.3929.6429.9130.08Average41.9742.0638.3345.4548.7252.3556.0040.2845.2134.9556.2656.5352.4457.7854.7149.5245.9753.7451.2356.1157.1258.8157.4456.1958.2858.4258.9755.2757.0657.8759.51Table11:AllEnglishresults.ThemainscoreforeachtaskisreportedasdescribedinSection3.2. Dataset Language LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco BUCC de-en 99.21 99.35 97.11 98.59 54.00 BUCC fr-en 98.39 98.72 94.99 96.89 97.06 BUCC ru-en 97.62 97.78 95.06 96.44 45.30 BUCC zh-en 97.70 99.16 95.63 97.56 97.96 Tatoeba sqi-eng 97.22 96.76 98.17 98.57 10.38 Tatoeba fry-eng 42.07 89.31 31.13 43.54 24.62 Tatoeba kur-eng 19.09 83.59 46.94 61.44 8.26 Tatoeba tur-eng 98.03 98.00 95.08 96.17 6.15 Tatoeba deu-eng 99.07 99.20 97.02 97.73 70.10 Tatoeba nld-eng 95.35 96.07 94.58 95.50 29.74 Tatoeba ron-eng 96.52 96.92 95.30 96.43 27.23 Tatoeba ang-eng 25.22 59.28 10.24 16.72 28.76 Tatoeba ido-eng 80.86 89.42 40.25 43.91 43.91 Tatoeba jav-eng 9.95 79.77 17.04 23.39 15.02 Tatoeba isl-eng 94.32 94.75 24.07 59.25 6.29 Tatoeba slv-eng 95.40 96.03 96.92 97.08 10.14 Tatoeba cym-eng 5.85 92.00 13.25 22.31 6.97 Tatoeba kaz-eng 53.30 87.49 34.89 61.49 3.32 Tatoeba est-eng 96.43 96.55 97.33 98.40 4.76 Tatoeba heb-eng 0.00 91.53 86.88 88.26 1.69 Tatoeba gla-eng 1.52 85.66 3.61 4.72 2.09 Tatoeba mar-eng 92.93 92.65 92.38 93.83 45.53 Tatoeba lat-eng 64.81 80.07 19.47 24.25 28.76 Tatoeba bel-eng 79.54 95.00 67.73 79.94 8.03 Tatoeba pms-eng 36.23 64.57 30.70 34.19 31.94 Tatoeba gle-eng 4.20 93.80 11.62 16.85 3.26 Tatoeba pes-eng 93.13 94.70 92.59 93.47 12.13 Tatoeba nob-eng 95.77 98.40 97.73 98.53 21.07 Tatoeba bul-eng 93.57 94.58 92.65 93.52 20.09 Tatoeba cbk-eng 77.17 79.44 55.37 58.68 64.63 Tatoeba hun-eng 95.20 96.55 91.58 94.18 5.07 Tatoeba uig-eng 56.49 92.40 24.39 48.35 1.27 Tatoeba rus-eng 92.58 93.75 91.87 92.92 59.84 Tatoeba spa-eng 97.33 98.40 95.42 97.00 94.48 Tatoeba hye-eng 88.72 94.09 93.28 94.38 0.50 Tatoeba tel-eng 96.72 97.86 36.40 79.73 64.62 Tatoeba afr-eng 92.59 96.18 58.22 72.96 16.62 Tatoeba mon-eng 3.42 95.91 95.04 96.14 2.85 Tatoeba arz-eng 66.16 76.00 51.26 55.69 70.66 Tatoeba hrv-eng 96.72 96.95 95.98 97.00 12.79 Tatoeba nov-eng 60.02 74.38 47.99 50.23 52.23 Tatoeba gsw-eng 27.52 46.50 25.74 25.12 21.03 Tatoeba nds-eng 77.13 79.42 32.16 38.88 23.92 Tatoeba ukr-eng 93.52 93.97 92.82 92.67 22.06 Tatoeba uzb-eng 23.20 84.23 17.14 23.19 4.71 Tatoeba lit-eng 96.20 96.47 93.16 95.37 4.49 Tatoeba ina-eng 93.93 95.37 79.13 84.32 73.67 Tatoeba lfn-eng 63.39 67.54 47.02 49.56 44.85 Tatoeba zsm-eng 95.41 95.62 95.31 95.80 79.95 Tatoeba ita-eng 94.32 92.72 93.05 93.76 65.04 Tatoeba cmn-eng 85.62 95.10 94.93 95.83 91.45 Tatoeba lvs-eng 95.33 95.88 97.87 97.53 6.55 Tatoeba glg-eng 96.14 96.82 94.00 95.32 79.86 Tatoeba ceb-eng 9.93 64.42 8.05 7.39 6.64 Tatoeba bre-eng 31.2 15.07 5.56 6.42 4.67 Tatoeba ben-eng 89.43 88.55 36.48 64.90 75.98 Tatoeba swg-eng 33.10 59.36 26.31 22.80 16.89 Tatoeba arq-eng 26.63 42.69 18.60 19.84 27.75 Tatoeba kab-eng 65.88 4.31 1.16 1.41 1.69 Tatoeba fra-eng 94.28 94.86 91.72 93.12 91.44 Tatoeba por-eng 94.54 94.14 92.13 93.02 92.62 Tatoeba tat-eng 34.74 85.92 10.25 10.89 3.59 Tatoeba oci-eng 58.13 65.81 38.57 43.49 40.17 Tatoeba pol-eng 97.32 97.22 94.28 96.95 14.09 Tatoeba war-eng 8.25 60.29 7.25 7.42 10.38 Tatoeba aze-eng 82.41 94.93 62.10 76.36 6.32 Tatoeba vie-eng 96.73 97.20 95.12 97.23 94.20 Tatoeba nno-eng 72.75 94.48 76.34 81.41 16.28 Tatoeba cha-eng 14.86 31.77 15.98 12.59 23.26 Tatoeba mhr-eng 6.86 15.74 6.89 7.57 1.56 Tatoeba dan-eng 95.22 95.71 94.80 96.17 23.52 Tatoeba ell-eng 96.20 95.35 95.43 94.93 5.34 Tatoeba amh-eng 80.82 91.47 36.21 53.49 0.03 Tatoeba pam-eng 3.24 10.73 5.41 5.39 5.85 Tatoeba hsb-eng 45.75 67.11 36.10 44.32 9.68 Tatoeba srp-eng 93.64 94.43 92.24 94.12 11.69 Tatoeba epo-eng 96.61 98.20 41.73 55.12 26.20 Tatoeba kzj-eng 4.46 11.33 6.24 5.88 5.17 Tatoeba awa-eng 33.74 71.70 33.43 42.83 35.01 Tatoeba fao-eng 57.04 87.40 27.51 38.24 12.61 Tatoeba mal-eng 98.16 98.45 32.20 88.46 83.30 Tatoeba ile-eng 87.88 85.58 57.71 60.36 59.59 Tatoeba bos-eng 95.86 94.92 93.27 94.02 13.65 Tatoeba cor-eng 4.45 10.11 3.42 3.53 2.83 Tatoeba cat-eng 95.80 95.38 94.42 96.05 88.31 Tatoeba eus-eng 93.32 95.01 23.18 31.33 53.38 Tatoeba yue-eng 87.75 89.58 71.45 77.58 77.03 Tatoeba swe-eng 95.31 95.63 94.42 95.45 19.53 Tatoeba dtp-eng 7.39 10.85 5.69 5.03 3.41 Tatoeba kat-eng 81.16 95.02 95.44 95.46 0.42 Tatoeba jpn-eng 93.78 95.38 90.41 92.51 71.36 Tatoeba csb-eng 27.03 52.57 21.56 23.73 10.03 Tatoeba xho-eng 4.68 91.55 4.52 6.53 5.51 Tatoeba orv-eng 23.24 38.93 15.10 23.77 5.79 Tatoeba ind-eng 92.98 93.66 92.74 93.50 88.04 Tatoeba tuk-eng 16.35 75.27 15.16 14.91 5.48 Tatoeba max-eng 36.96 63.26 45.25 48.77 36.14 Tatoeba swh-eng 55.66 84.50 14.48 16.02 16.74 Tatoeba hin-eng 95.32 96.87 97.62 97.75 85.23 Tatoeba dsb-eng 42.34 64.81 33.43 36.85 8.78 Tatoeba ber-eng 77.63 8.40 4.43 4.88 4.92 Tatoeba tam-eng 87.32 89.0 24.64 73.60 72.76 Tatoeba slk-eng 95.82 96.5 95.15 96.62 9.98 Tatoeba tgl-eng 63.19 96.02 13.09 17.67 10.70 Tatoeba ast-eng 76.35 90.68 62.17 70.08 71.13 Tatoeba mkd-eng 93.63 93.6 91.00 93.02 10.47 Tatoeba khm-eng 74.19 78.37 32.11 58.80 0.37 Tatoeba ces-eng 95.52 96.68 95.12 95.73 9.55 Tatoeba tzl-eng 36.56 58.88 25.46 34.21 27.82 Tatoeba urd-eng 84.23 93.22 94.57 95.12 70.10 Tatoeba ara-eng 90.14 88.80 87.93 90.19 85.37 Tatoeba kor-eng 87.97 90.95 92.52 93.07 22.39 Tatoeba yid-eng 2.49 88.79 14.38 30.73 0.16 Tatoeba ﬁn-eng 96.98 96.37 93.10 95.92 3.41 Tatoeba tha-eng 96.38 96.14 96.72 95.99 2.22 Tatoeba wuu-eng 75.09 90.18 76.00 78.25 79.58 Average mix 67.42 81.75 57.98 63.38 31.08 Table 12: Multilingual bitext mining results. Scores are f1. Dataset Language LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco AmazonCounterfactualClassiﬁcation de 67.82 73.17 68.35 69.95 61.35 AmazonCounterfactualClassiﬁcation ja 68.76 76.42 63.45 69.79 58.23 AmazonReviewsClassiﬁcation de 31.07 39.92 35.91 39.52 29.70 AmazonReviewsClassiﬁcation es 32.72 39.39 37.49 39.99 35.97 AmazonReviewsClassiﬁcation fr 31.12 38.52 35.30 39.00 35.92 AmazonReviewsClassiﬁcation ja 28.94 36.44 33.24 36.64 27.64 AmazonReviewsClassiﬁcation zh 30.89 36.45 35.26 37.74 32.63 MassiveIntentClassiﬁcation af 38.01 56.12 45.88 52.32 47.85 MassiveIntentClassiﬁcation am 12.70 55.71 36.75 41.55 33.30 MassiveIntentClassiﬁcation ar 37.16 50.86 45.14 51.43 59.25 MassiveIntentClassiﬁcation az 19.98 58.97 47.42 56.98 45.24 MassiveIntentClassiﬁcation bn 42.51 58.22 35.34 48.79 61.59 MassiveIntentClassiﬁcation cy 17.33 50.16 26.12 27.87 44.92 MassiveIntentClassiﬁcation da 45.61 58.25 57.73 62.77 51.23 MassiveIntentClassiﬁcation de 44.79 56.21 50.71 59.57 56.10 MassiveIntentClassiﬁcation el 46.71 57.03 58.70 62.62 46.13 MassiveIntentClassiﬁcation es 45.44 58.32 59.66 64.43 66.35 MassiveIntentClassiﬁcation fa 45.01 62.33 61.02 65.34 51.20 MassiveIntentClassiﬁcation ﬁ 45.94 60.12 57.54 62.28 45.33 MassiveIntentClassiﬁcation fr 46.13 60.47 60.25 64.82 66.95 MassiveIntentClassiﬁcation he 42.55 56.55 52.51 58.21 43.18 MassiveIntentClassiﬁcation hi 40.20 59.40 58.37 62.77 63.54 MassiveIntentClassiﬁcation hu 42.77 59.52 60.41 63.87 44.73 MassiveIntentClassiﬁcation hy 28.07 56.20 51.60 57.74 38.13 MassiveIntentClassiﬁcation id 45.81 61.12 59.85 65.43 64.06 MassiveIntentClassiﬁcation is 39.86 54.90 30.83 37.05 44.35 MassiveIntentClassiﬁcation it 48.25 59.83 59.61 64.68 60.77 MassiveIntentClassiﬁcation ja 45.30 63.11 60.89 63.74 61.22 MassiveIntentClassiﬁcation jv 24.30 50.98 32.37 36.49 50.94 MassiveIntentClassiﬁcation ka 22.70 48.35 43.03 49.85 33.84 MassiveIntentClassiﬁcation km 22.48 48.55 40.04 45.47 37.34 MassiveIntentClassiﬁcation kn 4.32 56.24 40.98 50.63 53.54 MassiveIntentClassiﬁcation ko 44.26 60.99 50.30 61.82 53.36 MassiveIntentClassiﬁcation lv 39.75 57.10 54.68 61.29 46.50 MassiveIntentClassiﬁcation ml 41.33 57.91 42.41 54.34 58.27 MassiveIntentClassiﬁcation mn 16.20 58.50 51.77 56.59 40.28 MassiveIntentClassiﬁcation ms 43.23 58.60 54.76 60.70 59.65 MassiveIntentClassiﬁcation my 25.37 57.35 52.01 57.09 37.42 MassiveIntentClassiﬁcation nb 37.74 57.91 55.50 62.60 49.41 MassiveIntentClassiﬁcation nl 45.00 59.37 59.51 63.57 52.09 MassiveIntentClassiﬁcation pl 44.99 59.71 59.43 64.30 50.48 MassiveIntentClassiﬁcation pt 48.55 60.16 61.27 64.89 66.69 MassiveIntentClassiﬁcation ro 44.30 57.92 58.39 62.80 50.53 MassiveIntentClassiﬁcation ru 44.29 60.67 59.04 63.26 58.32 MassiveIntentClassiﬁcation sl 44.72 59.37 57.36 63.51 47.74 MassiveIntentClassiﬁcation sq 46.12 58.03 56.59 62.49 48.94 MassiveIntentClassiﬁcation sv 45.95 59.66 59.43 64.73 50.79 MassiveIntentClassiﬁcation sw 31.89 51.62 29.57 31.95 49.81 MassiveIntentClassiﬁcation ta 29.63 55.04 36.77 50.17 56.40 MassiveIntentClassiﬁcation te 36.03 58.32 40.72 52.82 54.71 MassiveIntentClassiﬁcation th 43.39 56.58 58.97 61.11 44.43 MassiveIntentClassiﬁcation tl 29.73 55.28 33.67 38.83 50.21 MassiveIntentClassiﬁcation tr 43.93 60.91 59.90 64.54 46.56 MassiveIntentClassiﬁcation ur 26.11 56.70 52.80 56.37 56.75 MassiveIntentClassiﬁcation vi 44.33 56.67 56.61 59.68 64.53 MassiveIntentClassiﬁcation zh-CN 40.62 63.86 61.99 65.33 67.07 MassiveIntentClassiﬁcation zh-TW 32.93 59.51 58.77 62.35 62.89 MassiveScenarioClassiﬁcation af 47.10 63.39 53.64 59.67 51.47 MassiveScenarioClassiﬁcation am 17.70 62.02 41.89 48.97 34.87 MassiveScenarioClassiﬁcation ar 45.21 57.72 51.74 57.78 65.21 MassiveScenarioClassiﬁcation az 28.21 63.48 52.06 61.53 45.58 MassiveScenarioClassiﬁcation bn 50.52 61.84 41.17 54.53 67.30 MassiveScenarioClassiﬁcation cy 22.58 56.13 31.72 35.26 46.29 MassiveScenarioClassiﬁcation da 54.87 65.24 66.87 71.00 53.52 MassiveScenarioClassiﬁcation de 54.34 62.39 57.40 67.34 61.74 MassiveScenarioClassiﬁcation el 55.47 64.58 66.14 68.81 48.96 MassiveScenarioClassiﬁcation es 52.77 63.61 65.04 70.42 73.34 MassiveScenarioClassiﬁcation fa 52.50 67.46 65.86 69.88 53.17 MassiveScenarioClassiﬁcation ﬁ 52.63 64.58 63.75 67.60 44.69 MassiveScenarioClassiﬁcation fr 54.32 65.10 66.06 70.69 72.91 MassiveScenarioClassiﬁcation he 52.41 63.53 59.20 65.16 43.10 MassiveScenarioClassiﬁcation hi 47.37 64.40 65.21 67.92 69.27 MassiveScenarioClassiﬁcation hu 53.43 65.82 66.56 70.30 45.16 MassiveScenarioClassiﬁcation hy 33.57 61.25 56.11 63.02 38.73 MassiveScenarioClassiﬁcation id 54.38 65.84 66.16 70.73 70.13 MassiveScenarioClassiﬁcation is 49.78 61.94 37.52 44.16 44.21 MassiveScenarioClassiﬁcation it 54.84 64.09 65.00 69.73 65.57 MassiveScenarioClassiﬁcation ja 54.12 67.72 66.50 69.69 65.76 MassiveScenarioClassiﬁcation jv 32.71 58.29 38.60 44.20 54.79 MassiveScenarioClassiﬁcation ka 26.92 53.38 50.66 57.30 32.99 MassiveScenarioClassiﬁcation km 27.23 56.18 46.96 53.14 39.34 MassiveScenarioClassiﬁcation kn 10.06 61.74 45.73 56.08 60.50 MassiveScenarioClassiﬁcation ko 52.01 67.26 55.66 68.52 55.69 MassiveScenarioClassiﬁcation lv 44.82 61.87 59.80 66.28 44.35 MassiveScenarioClassiﬁcation ml 49.10 62.26 47.69 60.13 65.53 MassiveScenarioClassiﬁcation mn 21.51 62.60 57.07 60.85 38.72 MassiveScenarioClassiﬁcation ms 53.60 65.63 61.71 65.81 64.99 MassiveScenarioClassiﬁcation my 29.72 62.94 59.10 63.03 36.84 MassiveScenarioClassiﬁcation nb 43.90 64.29 64.25 70.24 51.80 MassiveScenarioClassiﬁcation nl 53.33 65.16 65.52 70.37 56.32 MassiveScenarioClassiﬁcation pl 52.92 64.56 65.04 68.99 49.98 MassiveScenarioClassiﬁcation pt 53.41 63.28 65.79 70.09 71.46 MassiveScenarioClassiﬁcation ro 50.48 62.41 64.17 67.95 53.69 MassiveScenarioClassiﬁcation ru 51.84 65.25 65.24 69.92 61.60 MassiveScenarioClassiﬁcation sl 51.29 64.25 64.01 70.81 48.04 MassiveScenarioClassiﬁcation sq 55.65 64.54 64.31 69.63 50.06 MassiveScenarioClassiﬁcation sv 54.64 66.01 67.14 71.60 51.73 MassiveScenarioClassiﬁcation sw 42.04 58.36 34.86 37.29 54.22 MassiveScenarioClassiﬁcation ta 36.72 59.08 42.62 55.96 62.77 MassiveScenarioClassiﬁcation te 42.08 64.13 46.46 58.81 62.59 MassiveScenarioClassiﬁcation th 52.15 64.34 67.01 69.44 45.18 MassiveScenarioClassiﬁcation tl 37.34 60.23 37.37 43.99 52.06 MassiveScenarioClassiﬁcation tr 52.56 65.43 66.55 70.4 47.21 MassiveScenarioClassiﬁcation ur 32.60 61.52 60.43 62.9 64.26 MassiveScenarioClassiﬁcation vi 50.97 61.05 60.72 65.71 70.61 MassiveScenarioClassiﬁcation zh-CN 50.22 70.85 67.44 71.23 73.95 MassiveScenarioClassiﬁcation zh-TW 42.32 67.08 65.70 68.73 70.30 MTOPDomainClassiﬁcation de 74.08 86.95 79.20 85.73 82.05 MTOPDomainClassiﬁcation es 73.47 84.07 83.04 86.96 93.55 MTOPDomainClassiﬁcation fr 72.26 84.14 78.63 81.21 90.98 MTOPDomainClassiﬁcation hi 72.95 85.11 81.36 84.76 89.33 MTOPDomainClassiﬁcation th 72.68 81.24 79.99 82.51 60.49 MTOPIntentClassiﬁcation de 51.62 63.42 54.23 61.27 61.92 MTOPIntentClassiﬁcation es 52.75 64.44 60.28 66.59 74.49 MTOPIntentClassiﬁcation fr 50.12 62.01 54.05 59.76 69.12 MTOPIntentClassiﬁcation hi 45.55 62.58 59.90 62.37 64.85 MTOPIntentClassiﬁcation th 50.07 64.61 61.96 64.80 49.36 Average mix 42.85 60.77 54.87 60.39 54.4 Table 13: Multilingual classiﬁcation results. Scores are accuracy. Dataset Language Komninos LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco STS17 ko-ko 2.54 70.52 71.32 77.03 83.41 66.89 STS17 ar-ar 13.78 67.47 69.07 79.16 79.10 76.42 STS17 en-ar 9.08 65.05 74.51 81.22 80.85 78.07 STS17 en-de -3.11 66.66 73.85 84.22 83.28 59.10 STS17 en-tr -0.45 70.05 72.07 76.74 74.90 11.80 STS17 es-en -8.18 55.30 65.71 84.44 86.11 78.22 STS17 es-es 48.23 79.67 80.83 85.56 85.14 86.00 STS17 fr-en 5.81 70.82 76.98 76.59 81.17 80.46 STS17 it-en 3.64 70.98 76.99 82.35 84.24 51.58 STS17 nl-en -0.44 68.12 75.22 81.71 82.51 45.85 STS22 de 33.04 25.69 48.58 44.64 46.70 30.05 STS22 es 48.53 54.92 63.18 56.56 59.91 65.41 STS22 pl 12.47 18.34 39.30 33.74 33.65 31.13 STS22 tr 47.38 36.97 58.15 53.39 56.30 47.14 STS22 ar 32.42 42.57 57.67 46.2 52.19 58.67 STS22 ru 19.44 39.24 57.49 57.08 58.74 43.36 STS22 zh 4.78 49.41 63.02 58.75 61.75 66.78 STS22 fr 49.43 58.61 77.95 70.55 74.30 80.38 STS22 de-en 28.65 32.35 50.14 52.65 50.81 51.16 STS22 es-en 26.97 54.34 71.86 67.33 70.26 75.06 STS22 it 57.77 60.31 72.22 55.22 60.65 65.65 STS22 pl-en 45.55 53.63 69.41 69.02 73.07 53.31 STS22 zh-en 14.05 46.19 64.02 65.71 67.96 68.45 STS22 es-it 41.10 42.21 69.69 47.67 53.70 65.50 STS22 de-fr 14.77 37.41 53.28 51.73 62.34 53.28 STS22 de-pl 11.21 15.67 58.69 44.22 40.53 43.05 STS22 fr-pl 39.44 39.44 61.98 50.71 84.52 28.17 Average mix 22.14 51.55 65.67 64.23 67.71 57.81 Table 14: Multilingual STS Results. Scores are Spearman correlations of cosine similarities.","libVersion":"0.3.2","langs":""}
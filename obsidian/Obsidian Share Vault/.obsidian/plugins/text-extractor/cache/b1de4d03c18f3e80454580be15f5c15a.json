{"path":"lit/lit_sources.backup/Wolf24littleGuideBuildLLM.pdf","text":"A little guide to building Large Language Models in 2024 thomas@huggingface.co Website: https://thomwolf.io E-Mail: thomas@huggingface.co Twitter: @thom_wolf LinkedIn: @thom-wolf CSO and Co-founder of Hugging Face Created Hugging Face Transformers and Datasets libraries. Exploring Open-science in research in AI/ML, trying to lower the gap between academia and industrial labs through projects like the BLOOM/BigScience Workshop Co-wrote \"Natural Language Processing with Transformers\" (O'Reilly) Push for lowering the access barrier in AI for researchers and practitioners Hi! ● Quick overview of Hugging Face ● LLMs in 2024 at Hugging Face ○ data ○ training ○ evaluation ○ alignment ○ inference ● Closing notes Plan of today WTF is Hugging Face? Open vs Closed Models Open and closed models have diﬀerent benefits and should be considered for each use-case Open-Source Closed / Proprietary Security Models can be self-hosted, data stays in your environment Models cannot be self-hosted. Data is sent outside your environment to vendor Control The timing and nature of updates are controlled by you Updates and changes to performance can happen without notice Customization Full source code access to customize the model for your needs Limited ability to customize for your needs Transparency Inspect code and data provides better auditability and understandability No ability to audit or understand performance Cost Typical lower long term cost due to smaller model size larger model size and proprietary premium often balanced by decreased cost from server-side optimization Latency Lower latency due to on premise and smaller model sizes Often greater latency due to larger model sizes + API latency Quality No single approach is best. Each use case will vary. Proprietary is typically closer to the frontier of performance. Examples Founded In 2016 170 Employees 300K+ stars on Github 500K+ open source models 100K+ public data sets 1M+ daily downloads 700K+ daily visitors 30+ Libraries Hugging Face: The home of open MLUsed everywhere in the AI world Open source contributors Hardware partners Cloud partners 15,000+ startups and enterprises On-prem partners Hugging Face Integrated with the ML ecosystem Model in production 100,000+ datasets on the hub 500,000+ models on the hub No-code AutoML Managed Inference on AWS, Azure and GCP Hosted ML applications HW-accelerated training & inference Deploy anywhere Open Datasets Open Models Transformers Accelerate Optimum Diﬀusers Amazon SageMaker Hugging Face on Azure NVIDIA DGX Cloud Cloud Platforms Google Cloud Open-source Ecosystem LLMs in 2024 at Hugging Face I – Training: ❖ data preparation – datatrove ❖ eﬀicient training techniques – nanotron ❖ evaluation – lighteval II – Fine-tuning: ❖ RLHF – TRL III – Inference: ❖ quantization – bitsandbytes ❖ deployment – Text Generation Inference The workflow for LLMsEnergy/carbon footprint and LLMs Do you need to pretrain an LLM? Start by test existing models on your domain and task(s) of interest Energy budget will likely be dominated by inference costs. Select a compute eﬀicient model: - smallest size - quantized - classification models > generative Most of the time the answer is “no” => Deploy it in a setup/cloud provider in a region with a good energy mix Yes => Ressources: - Power Hungry Processing: Watts Driving the Cost of AI Deployment? https://arxiv.org/abs/2311.16863 - Language models scale reliably with over-training and on downstream tasks https://arxiv.org/abs/2403.08540 - Region energy mix (e.g. solar, nuclear, coal, gas) can have a x500 impact on model carbon footprint: https://app.electricitymaps.com/ Train-compute-optimal models (Chinchilla law) are not eﬀicient for inference Train smaller models for longer if you plan to deploy it at large scale Train in a cluster/provider with a good energy mix Share the model so people can re-use/leverage the compute spent – Itʼs like recycling AI models Focus on eﬀicient pretraining while taking a holistic view of model life-cycle Focus on eﬀicient fine-tuning and inference Training: 1. Data preparation Often a lot of focus on models architecture but… I – Training https://nonint.com/2023/06/10/the-it- in-ai-models-is-the-dataset/Yi: Open Foundation Models by 01.AI https://arxiv.org/abs/2403.04652 I – Training I – Data preparation: ★ Latest resources ○ A Survey on Data Selection for Language Models https://arxiv.org/abs/2402.16827 ○ Yi: Open Foundation Models by 01.AI https://arxiv.org/abs/2403.04652 ★ Recent dataset reports ○ Dolma (AllenAI): an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research https://arxiv.org/abs/2402.00159 ○ RefinedWeb (Falcon): Outperforming Curated Corpora with Web Data, and Web Data Only https://arxiv.org/abs/2306.01116 I – TrainingData preparation ★ LM training requires multiple stages: ○ pretraining ○ instruction-tuning ○ alignment ○ in-context learning ○ task-specific fine-tuning ★ each training stage has diﬀerent goals ★ data selection methods will use diﬀerent mechanisms Data preparation Pretraining stage: ★ Goal: train a general-purpose model – maximal coverage ★ Requires: train on massive quantities of text, at least 1 trillion of tokens nowadays ★ Challenges: ○ maximizing diversity and coverage ○ maximising quality, robustness ○ data quality evaluation: how to measure data quality at the billion tokens scale Data preparationData preparationData preparation Data sources ★ Very large (> 100B tokens): ○ Common crawl: everyone starts from here ○ Code: Github and Software Heritage ★ Curated: ○ Wikipedia ○ Books: public-domain vs. copyrighted ★ More recent trends ○ Synthetic data Data sources ★ A synthetic dataset of 30M samples ★ generated by Mixtral-8x7B-Instruct-v0.1 ★ 8 splits: ○ various sources for seed samples: Stanford, OpenStax and KhanAcademy, webdata, instruction-tuning datasets ○ model is asked to generate content Synthetic data: Cosmopedia v0.1 🛰Cosmopedia v0.1 🛰 ★ 3B+ files in 658 programming languages ★ created as part of the BigCode Project pre-training dataset for Code LLMs ★ Derived from the Software Heritage archive: largest public archive of software source code Code data: StarCoder 2 – The Stack v2 💫 Coming soon…. webdata: FineWebLanguage filtering heuristics Quality filtering heuristics ★ Advantages ○ controlled ○ robust ○ rather clear priors ★ Drawbacks ○ rely entirely on surface level ○ danger of removing too much ○ hyper-parameter tuning Quality filtering heuristics Given a set of examples of good/bad documents: ★ classifier-based quality filtering: fastText classification with an n-gram size of 2 ★ perplexity based filtering: 5-gram Kneser-Ney model on Wikipedia Filter based on a threshold Allow for more “quality/content based filtering” but harder to estimate the impact of the training documents. May introduce unwanted “bias” Quality filtering – ML filtering ★ Taking care of domains specificities ○ important to inspect the eﬀect on domain specific data ○ extract 10 documents per domains (e.g. top urls) ○ manually inspect the results ○ craft domain specific filters/hyper-parameters ○ same for multiple languages ★ Deterministic vs. stochastic selection ○ hard threshold are strong decision points ○ stochastic smoothing of rules Notes on data filtering Reasoning: a lot of duplicate/near duplicate documents in large scale dumps (internet/github…) ★ increases the distribution density around those areas ★ duplicated data point have more chance of being memorized (Carlini et al. (2023)) ★ filtering out duplicates reduce training time ★ reducing duplication improve accuracy on downstream tasks (Lee et al., 2022a; Tirumala et al., 2023) Data deduplication ★ methods: ○ Fuzzy: ■ BLOOM filters (hashing and fixed size vector) ■ MinHash (hashing and sorting) ○ Exact ■ Exact substring with suﬀix array ■ Sentence dedup ★ time/memory consumption ○ MinHash oﬀers a good trade–oﬀ of speed/memory with more control than BLOOM filters ★ counter intuitive results ○ more deduplication may lead to keeping only bad data Data deduplication Shuﬀle: ➜ Important! Tokenizers – some good practices: ★ Sample quite widely in your dataset (donʼt overfit to a subset) ★ For math: be careful of numbers – either split digits or add them manually ★ For code: be careful of spaces – handle them well to group them ★ Byte-level BPE is a good standard – Byte fallback is good as well Scaling tokenization: ★ Tokenization of trillions token can be non negligible ○ tokenize dynamically during training ○ parallelize pretokenization in small data sub-sets Preparing the data for training 1. Small models trainings: train 1-2B size models on 30GT ★ Use a set of “high-signal” benchmarks (in NLP): ○ commonsense QA ○ hellaswag ○ openbook QA ○ PiQA ★ High-signal? ○ monotonicity: monotonically increasing during training ○ low variance: ■ when comparing two known reference datasets (e.g. The Pile versus C4) ■ when comparing with various sub-parts of data and seeds ■ above random baseline ★ Tricky details to maximize signal: ○ Small models like “normalized loglikelihood” better ○ Larger models like “letter answers” better How to evaluate data quality ○ SIQA ○ winogrande ○ ARC ○ MMLU 2. Manual data inspection ★ Inspect 10 documents for your top domains/URLs ★ Inspect at various stage until post tokenization ★ Inspect kepts and discarded documents ★ Search tools in the dataset 3. Maps and clustering https://github.com/huggingface/text-clustering 4. Uncommon: training a tokenizer and inspect the top/last/longest tokens :) How to evaluate data quality Quick intro datatrove ★ Started as an open reproduction of RefinedWeb ★ Ended up in a fully-fledged lightweight library for processing, filter and deduplicate text data at a very large scale. ★ Provides prebuilt commonly used processing blocks with a framework to easily add custom functionality. ★ Full python ★ Local, remote and other file systems are supported through fsspec. ★ Scaled to 20k+ nodes on SLURM datatrove Quick intro lighteval ★ Lightweight LLM evaluation suite ★ Allow to evaluate models with 3D parallelism ★ Custom evals and prompt explorations ★ Push to hub/WandB/tensorboard lighteval Training: 2. Modeling Training LLM: the essential elements ★ size/eﬀiciency ○ parallelism ○ asynchronicity ○ kernel merging ○ attention ★ instabilities ○ stable training recipes ★ capacity ○ mixture of experts ○ mamba Training the model ★ 4D Parallelism: ○ Data Parallelism ○ Tensor Parallelism ○ Pipeline Parallelism ○ Sequence Parallelism When the model is too big: Parallelism ★ 4D Parallelism: ○ Data Parallelism ■ usually work out-of-the-box ● just need to be careful with dataloading ■ challenges: ● compute eﬀiciency for gradient all-reduce ● training eﬀiciency of batch-size ○ Tensor Parallelism ○ Pipeline Parallelism ○ Sequence Parallelism When the model is too big: Data Parallelism ★ 4D Parallelism: ○ Data Parallelism ○ Tensor Parallelism ■ Re-write model code ■ Combine collumn/row slicing to reduce sync points ○ Pipeline Parallelism ○ Sequence Parallelism When the model is too big: Tensor Parallelism ★ 4D Parallelism: ○ Data Parallelism ○ Tensor Parallelism ■ Re-write model code ■ Combine collumn/row slicing to reduce sync points ○ Pipeline Parallelism ○ Sequence Parallelism When the model is too big: Tensor Parallelism ★ 4D Parallelism: ○ Data Parallelism ○ Tensor Parallelism ○ Pipeline Parallelism ■ Groupe sub-parts of the network ■ Challenge to keep all GPU busy ○ Sequence Parallelism When the model is too big: Pipeline Parallelism ★ 4D Parallelism: ○ Data Parallelism ○ Tensor Parallelism ○ Pipeline Parallelism ■ Groupe sub-parts of the network ■ Challenge to keep all GPU busy ○ Sequence Parallelism When the model is too big: Parallelism ★ 4D Parallelism: ○ Data Parallelism ○ Tensor Parallelism ○ Pipeline Parallelism ○ Sequence Parallelism ■ Add another parallelism ■ Be careful: another “sequence parallelism” exists: “ring attention” (also interesting) ■ Usually only interesting during training When the model is too big: Parallelism References: ★ Breadth-First Pipeline Parallelism https://arxiv.org/abs/2211.05953 ★ Reducing Activation Recomputation in Large Transformer Models https://arxiv.org/abs/2205.05198 ★ Sequence Parallelism: Long Sequence Training from System Perspective https://arxiv.org/abs/2105.13120 When the model is too big: Parallelism ★ Challenges when scaling to multiple GPUs ○ Synchronization between: ■ Multiple GPUs ■ CPU and GPUs ○ Example: ■ Data parallelism all_reduce overlapping communication and computation When the model is too big: eﬀiciency Stop materializing attention matrices! Flash Attention Flash attention V2: ★ reduce the number of non-matmul FLOP (division, etc) ○ each non-matmul FLOP is 16× more expensive than a matmul FLOP ★ better parallelism (sequences) ★ causal masks ★ better work partitioning (blocks, wraps) Flash Attention v2 ★ Initialization ★ Stabilisation (see MuTransfer) ★ Learning rate: Cosine or not. ★ Scaling hyper-parameters results References: ★ MiniCPM blog post ★ Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer https://arxiv.org/abs/2203.03466 ★ Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster https://arxiv.org/abs/2304.03208 Stable training recipes Recent developments ★ Mixture-of-Experts (MoE) ○ Eﬀicient training on GPUs with Block Sparsity Capacity and architectures Recent developments: Mamba ➜ eﬀicient and performant SSM https://srush.github.io/annotated-mamba/hard.html Capacity and architectures Quick intro nanotron ★ Minimalistic large language model 3D-parallelism training ★ Philosophy ○ Make it fast ○ Make it minimal ○ Make everything explicit instead of transparent ★ We support the following ○ 3D parallelism, including 1F1B pipeline engine ○ ZeRO-1 optimizer ○ FP32 gradient accumulation ○ Parameter tying/sharding ★ Architectures ○ Llama ○ Mamba ○ MoE nanotron ★ RLHF in 2024: ○ Align the modelʼs outputs with humanʼs preferences ○ hard to design reward functions for RL Alignment ★ RLFH in 2024: PPO 🤯 results are incredible ❌ implementation is complicated ❌ GPU memory (fit 4 models) 🤯 Alignment ★ RLFH in 2024: DPO ✅ results are great ✅ Implementation is easy ✅ Fits in GPU memory well (2 models) Alignment ★ RLFH in 2024: Is PPO/on-policy RL out yet? Alignment ★ Quantization: ○ Full quantization for inference: GPTQ/GGML/NF4 https://arxiv.org/abs/2210.17323 Comparison of all three technics ○ AutoGPTQ ○ lama.cpp ★ Speculative Decoding ○ Use a small and large model in parallel ○ Medusa: https://arxiv.org/abs/2401.10774 ★ Compiling and CUDA graphs ○ Accelerating Generative AI with PyTorch II: GPT, Fast InferenceFinal step! Thank you! Questions ? Commercial solutions Hardware Specialized hardware optimized for production Models 500k+ open models Datasets 90k+ open datasets ML Libraries Transformers, Tokenizers, Autotrain and many more free libraries and tools Spaces Build, host, and share your ML applications using Spaces in just a few minutes. Get started with free compute and upgrade easily as needed. Expert acceleration program World-class experts to support your ML team reach impact Inference endpoints Deploy and call your models as APIs in a few clicks Enterprise Hub Private and securely hosting and tooling for enterprise collaboration on models, datasets and compute 1 2","libVersion":"0.3.2","langs":""}
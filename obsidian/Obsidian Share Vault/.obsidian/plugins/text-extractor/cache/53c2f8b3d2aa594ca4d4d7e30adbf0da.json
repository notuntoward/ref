{"path":"lit/lit_sources/Chernozhukov21DistributionalConformalPrediction.pdf","text":"STATISTICS Distributional conformal prediction Victor Chernozhukov a,b,1 , Kaspar Wüthrich c,d,e,1 , and Yinchu Zhu f,g,1,2 aDepartment of Economics, Massachusetts Institute of Technology, Cambridge, MA 02142; bCenter for Statistics and Data Science, Massachusetts Institute of Technology, Cambridge, MA 02142; cDepartment of Economics, University of California San Diego, La Jolla, CA 92093; dCESifo, 81679 Munich, Germany; eifo Center for Public Finance and Political Economy, ifo Institute, 81679 Munich, Germany; fDepartment of Economics, Brandeis University, Waltham, MA 02453; and gInternational Business School, Brandeis University, Waltham, MA 02453 Edited by Emmanuel J. Candès, Stanford University, Stanford, CA, and approved October 5, 2021 (received for review April 24, 2021) We propose a robust method for constructing conditionally valid prediction intervals based on models for conditional distributions such as quantile and distribution regression. Our approach can be applied to important prediction problems, including cross- sectional prediction, k–step-ahead forecasts, synthetic controls and counterfactual prediction, and individual treatment effects prediction. Our method exploits the probability integral trans- form and relies on permuting estimated ranks. Unlike regression residuals, ranks are independent of the predictors, allowing us to construct conditionally valid prediction intervals under het- eroskedasticity. We establish approximate conditional validity un- der consistent estimation and provide approximate unconditional validity under model misspecification, under overfitting, and with time series data. We also propose a simple “shape” adjustment of our baseline method that yields optimal prediction intervals. prediction intervals | conditional validity | model-free validity | quantile regression | distribution regression W e develop a robust approach for constructing prediction intervals based on models for conditional distributions. The proposed method is generic and can be implemented using a great variety of ﬂexible and powerful methods, including conven- tional quantile regression (QR) (1) and distribution regression (DR) (e.g., refs. 2 and 3), as well as nonparametric and high- dimensional machine learning methods such as quantile neural networks (e.g., ref. 4) and quantile trees and random forests (e.g., refs. 5 and 6). We observe data {(Yt , Xt )}T t=1,where Yt is a continuous out- come of interest and Xt is a p × 1 vector of predictors. Our task is to predict YT +1 given knowledge of XT +1. This setting encom- passes many classical cross-sectional and time series prediction problems. Moreover, our approach can be applied to synthetic control settings where the goal is to predict counterfactuals in the absence of a policy intervention (e.g., refs. 7 and 8) and to the problem of predicting individual treatment effects (e.g., refs. 9 and 10). With independent and identically distributed (iid) (or ex- changeable data), standard conformal prediction methods, which are based on modeling the conditional mean, yield prediction intervals ̂C(1−α) that satisfy P (YT +1 ∈ ̂C(1−α) (XT +1)) ≥ 1 − α [1] for a given miscoverage level α ∈ (0, 1). A prediction interval sat- isfying this property is said to be unconditionally valid. Uncondi- tionally valid prediction intervals guarantee accurate coverage on average, treating (YT +1, XT +1) and {(Yt , Xt )}T t=1 as random. However, in many applications, unconditional validity may be unsatisfactory. Let us consider three examples; refs. 11 and 12 have further examples and discussions. First, from a fair- ness perspective, data-driven recommendation systems should guarantee equalized coverage across protected groups, in which case the goal is to construct prediction intervals that are valid conditional on a protected attribute such as race or gender (11). Second, as in Predicting Stock Market Returns, consider the problem of predicting stock returns given the realized volatility. Since the distribution of returns is more dispersed when the variance is higher, a natural prediction algorithm should yield wider prediction intervals for higher values of volatility. That is, the prediction interval should be valid conditional on the known value of realized volatility rather than on average. Third, as in Predicting Wages Using CPS Data, suppose our goal is to predict wages based on an individual’s education and experience. An unconditionally valid prediction interval exhibits coverage 90% on average across all individuals but may contain the true wage of high school dropouts with no work experience with probability zero. A more useful prediction interval should exhibit correct coverage conditional on an individual’s observed education and experience and contain the true wage with 90% probability for every single individual. Motivated by this discussion, we develop a distributional con- formal prediction (DCP) method for constructing prediction in- tervals that are approximately valid conditional on the full vector of predictors XT +1 while treating YT +1 and {(Yt , Xt )}T t=1 as random: P (YT +1 ∈ ̂C(1−α) (XT +1) | XT +1) ≥ 1 − α + oP (1). [2] A prediction interval satisfying property Eq. 2 as T →∞ is said to be approximately conditionally valid.* While the requirement in Eq. 2 is natural in many applications, there are also other notions of conditional validity. Instead of conditioning on XT +1 (object conditional), one can also study Significance Prediction problems are important in many contexts. Exam- ples include cross-sectional prediction, time series forecasting, counterfactual prediction and synthetic controls, and indi- vidual treatment effect prediction. We develop a prediction method that works in conjunction with many powerful classi- cal methods (e.g., conventional quantile regression) as well as modern high-dimensional methods for estimating conditional distributions (e.g., quantile neural networks). Unlike many existing prediction approaches, our method is valid conditional on the observed predictors and efficient under some condi- tions. Importantly, our method is also robust; it exhibits uncon- ditional coverage guarantees under model misspecification, under overfitting, and with time series data. Author contributions: V.C., K.W., and Y.Z. designed research, performed research, con- tributed new reagents/analytic tools, analyzed data, and wrote the paper. The authors declare no competing interest. This article is a PNAS Direct Submission. Published under the PNAS license. 1V.C., K.W., and Y.Z. contributed equally to this work. 2To whom correspondence may be addressed. Email: yinchuzhu@brandeis.edu. This article contains supporting information online at https://www.pnas.org/lookup/ suppl/doi:10.1073/pnas.2107794118/-/DCSupplemental. Published November 24, 2021. *See, for example, refs. 12–14 for a further discussion of the difference between conditional and unconditional validity. PNAS 2021 Vol. 118 No. 48 e2107794118 https://doi.org/10.1073/pnas.2107794118 1of9Downloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131. the conditional coverage probability given the training sample {(Yt , Xt )}T t=1 (training conditional) or given YT +1 (label condi- tional) or combinations of them; ref. 15 has a detailed discussion. By proposition 2 of ref. 15, inductive conformal predictions (also known as split-sample conformal predictions) automatically achieve training conditional validity as long as the training sample is large enough. In classiﬁcation problems (the support of YT +1 is a ﬁnite set), label conditional validity is often of great interest as it is important to know the error rates for different categories and provides useful information on false-positive and false-negative rates (15). In ref. 15, label conditional validity is achieved by forming the conformity score within each category. Both training and label conditional validity can be achieved in a distribution- free way (i.e., for a given procedure, the conditional validity holds for any distribution of the data). However, object conditional validity in the sense of Eq. 2 can- not be achieved in a distribution-free way for nontrivial predic- tions. By refs. 12, 13, and 15, any prediction set satisfying Eq. 2 for every probability distribution of (Xt , Yt ) has inﬁnite Lebesgue measure with nontrivial probability. Therefore, we only aim to achieve Eq. 2 for a limited class of probability distributions. The construction of the proposed prediction set ̂C(1−α) relies on learning the conditional distribution Yt | Xt , and we only hope for conditional validity in Eq. 2 in the class of distributions that can be learned well. In particular, this class of distributions is those satisfying our regularity conditions. Our empirical results demonstrate the importance of using DCP instead of standard conformal prediction methods based on modeling the conditional mean. When predicting daily stock returns in Predicting Stock Market Returns, the coverage proba- bility of the 90% mean-based conformal prediction interval can drop to around 50% when the realized volatility is high. By contrast, DCP provides a coverage probability close to 90% for all values of realized volatility. This ﬁnding is important since volatility tends to be high during periods of crisis when accurate risk assessments are most needed. When predicting wages in Predicting Wages Using CPS Data, we ﬁnd that the DCP prediction intervals contain the true wage with probability close to 90% for most individuals, whereas standard mean-based conformal prediction intervals either substantially under- or overcover. To motivate DCP, note that a conditionally valid prediction interval is given by [Q ( α 2 , x ) , Q (1 − α 2 , x )] , [3] where Q(τ , x ) is the τ quantile of Yt given Xt = x . To implement the prediction interval Eq. 3, a plug-in approach would replace Q with a consistent estimator ˆQ: [ ˆQ ( α 2 , x ) , ˆQ (1 − α 2 , x )] . [4] This approach exhibits two well-known drawbacks. First, it will often exhibit undercoverage in ﬁnite samples (e.g., ref. 16). Second, it is neither conditionally nor unconditionally valid under misspeciﬁcation. We build upon conformal prediction (17, 18) and use the con- ditional ranking as a conformity score. This choice is particularly useful when working with regression models for conditional dis- tributions such as QR and DR.† Our method is conditionally valid under correct speciﬁcation, while the construction of the proce- dure as a conformal prediction method guarantees the uncondi- tional validity under misspeciﬁcation. Let F (y, x )= P (Yt ≤ y | Xt = x ) denote the conditional cumulative distribution function (CDF) of Yt given Xt = x . Throughout the paper, we assume †This transformation is also very useful in other prediction problems (e.g., ref. 19). that F (·, Xt ) is a continuous function almost surely. Our method is based on the probability integral transform, which states that the conditional rank, Ut := F (Yt , Xt ), has the uniform distribu- tion on (0, 1) and is independent of Xt . To construct the prediction interval, we test the plausibil- ity of each y ∈ R. By the probability integral transform, con- ditional on XT +1, F (YT +1, XT +1) belongs to [α/2, 1 − α/2] with probability 1 − α. Thus, collecting all values y ∈ R satis- fying F (y, XT +1) ∈ [α/2, 1 − α/2] yields a conditionally valid prediction interval in the sense of Eq. 2. We operationalize this idea by proposing a conformal prediction procedure based on the estimated ranks, ˆU (y) t := ˆF (y)(Yt , Xt ). For each y ∈ R, ˆF (y) is an estimator of F obtained based on the augmented data, {(Yt , Xt )}T +1 t=1 ,where YT +1 = y. Data augmentation is a key feature of conformal prediction. It implies the model-free unconditional exact ﬁnite-sample validity with iid (or exchange- able) data and thus, guards against model misspeciﬁcation and overﬁtting. Without data augmentation, the resulting prediction intervals are not exactly valid, not even with correct speciﬁcation and iid data. Our baseline method asymptotically coincides with the oracle interval in Eq. 3. This oracle interval may not be the shortest pos- sible prediction interval in general. Therefore, we also develop a simple and easy to implement adjustment of our baseline method for improving efﬁciency, which we refer to as optimal DCP. In Predicting Wages Using CPS Data, we show empirically that optimal DCP yields shorter prediction intervals than baseline DCP when the conditional distribution is skewed. We establish the following theoretical performance guarantees for the baseline and optimal DCP. 1) Asymptotic conditional validity under consistent estima- tion of the conditional CDF 2) Unconditional validity under model misspeciﬁcation: • Finite-sample validity with iid (or exchangeable) data • Asymptotic validity with time series data 3) For optimal DCP: • Under weak conditions: asymptotic conditional validity and optimality (shortest length) • Under strong conditions: asymptotic convergence to the optimal prediction interval Motivating Example We illustrate the advantages of DCP relative to mean-based conformal prediction (e.g., ref. 20) based on the following simple analytical example: Yt = Xt + Xt εt , Xt iid ∼ Uniform(0, 1), εt iid ∼ N (0, 1). [5] Our motivating example draws on refs. 16 and 20–22 that illus- trate the importance of accounting for heteroscedasticity. We focus on the population conformal prediction (or oracle) prob- lem under correct speciﬁcation and abstract from ﬁnite-sample issues. Mean-based conformal prediction is based on the residuals Rt = Yt − E (Yt | Xt )= Yt − Xt = Xt εt . The mean-based pre- diction interval is Creg (1−α)(x )= [x − Q|R|(1 − α), x + Q|R|(1 − α)] , [6] where Q|R|(1 − α) is the (1 − α) quantile of the distribution of |Rt |. An important property and drawback of Creg (1−α) is that its length, 2 · Q|R|(1 − α), is ﬁxed and does not depend on XT +1 = x (16, 20). This feature implies that Creg (1−α) is not adaptive to the heteroskedasticity in the location-scale model Eq. 5 and not conditionally valid. 2of9 PNAS https://doi.org/10.1073/pnas.2107794118 Chernozhukov et al. Distributional conformal predictionDownloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131.STATISTICS DCP is based on the ranks Ut =Φ (εt ),where Φ(·) is the CDF of N (0, 1). The DCP prediction interval is Cdcp (1−α)(x )= [x − x · Q|ε|(1 − α), x + x · Q|ε|(1 − α)] , [7] where Q|ε|(1 − α)=Φ−1(1 − α/2) is the (1 − α) quantile of |εt |. Unlike Creg (1−α), the length of Cdcp (1−α), 2x · Q|ε|(1 − α),de- pends on XT +1 = x . Our construction automatically adapts to the heteroskedasticity in model Eq. 5 and is conditionally valid. Fig. 1 provides an illustration. Fig. 1A shows that the condi- tional length of Creg (0.9) is constant, whereas the length of Cdcp (0.9) varies as a function of x. Cdcp (0.9) is shorter than Creg (0.9) for low values and wider for high values of x.Fig.1B shows that Cdcp (0.9) is valid for all x, whereas Creg (0.9) overcovers for low values and undercovers for high values of x. Fig. 1 illustrates the advantage of our method. 0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5 Conditional length 90% prediction interval XConditional length Distributional conformal prediction Mean−based conformal prediction A 0.0 0.2 0.4 0.6 0.8 1.00.60.70.80.91.0 Conditional coverage 90% prediction interval XConditional coverage Distributional conformal prediction Mean−based conformal prediction B Fig. 1. Motivating example. (A) Conditional length 90% prediction interval. (B) Conditional coverage 90% prediction interval. For predictor values where the conditional variance is low, it yields shorter prediction intervals while ensuring conditional coverage for values where the conditional dispersion is large by suitably enlarging the prediction interval. Related Literature We build on and contribute to the literature on conformal pre- diction (e.g., refs. 13, 15–18, 20, 23, and 24) and the literature on model-free prediction (19, 25), as well as the literature on quantile prediction methods (e.g., ref. 26 has a review). Within the conformal prediction literature, our paper is most closely related to refs. 13, 16, and 20. Ref. 13 proposes condi- tionally valid and asymptotically efﬁcient conformal prediction intervals based on estimators of the conditional density. We take a different and complementary approach, allowing researchers to leverage powerful regression methods for modeling conditional distributions, including QR and DR approaches. Ref. 20 devel- ops conformal prediction methods based on regression models for conditional expectations. However, as discussed in Motivating Example, this approach is not conditionally valid under het- eroskedasticity. They also propose a locally weighted conformal prediction approach, where the regression residuals are weighted by the inverse of a measure of their variability. This approach can alleviate some of the limitations of mean-based conformal prediction but is motivated by and based on restrictive locations- scale models. By contrast, our approach is generic and exploits ﬂexible and substantially more general models for the whole conditional distribution. Ref. 16 proposes a split conformal approach based on QR models, which they call conformalized quantile regression (CQR). Refs. 14 and 27 have related approaches, and ref. 28 has a general approach to adaptive conformal prediction. CQR is basedonsplitting thedataintotwo subsets, T1 and T2. Based on T1, they estimate two separate quantile functions ˆQ(α/2, x ) and ˆQ(1 − α/2, x ) and construct the prediction intervals as [ ˆQ(α/2, x ) − QE (1 − α), ˆQ(1 − α/2, x )+ QE (1 − α)] , where QE (1 − α) is the (1 − α)(1 + 1/|T2|) th empirical quan- tile of Et = max { ˆQ(α/2, Xt ) − Yt , Yt − ˆQ(1 − α/2, Xt )} in T2. Constructing prediction intervals based on deviations from quantile estimates is similar to working with deviations from mean estimates, as the deviations are measured in absolute levels. By contrast, exploiting the probability integral transform, our approach is generic and relies on permuting ranks, which naturally have the same scaling on (0, 1). Note, however, that our paper was inspired by ref. 16, and we view our proposal as a (fully quantile rank–based) reﬁnement of ref. 16. The value of this reﬁnement is especially apparent in the second empirical example. In addition, we also give quantile-based optimal predic- tion intervals. Our adjustment for constructing efﬁcient prediction intervals is related to and inspired by conformal prediction literature on minimum-volume prediction sets based on density estimators (e.g., refs. 13, 23, and 29–31) and nearest-neighbor estimators (32). It is most closely related and can be viewed as an alternative to conformal histogram regression (33). The main differences between our approach and conformal histogram regression are the following. First, our method is based on an optimization problem formulated in terms of estimated quantile functions and does not require estimating a conditional density or histogram. Second, we do not work with nested sets but instead, use a simple adjustment of our baseline conformity score. Finally, our approach works for general outcome distributions and does not rely on assuming unimodal distributions. Chernozhukov et al. Distributional conformal prediction PNAS 3of9 https://doi.org/10.1073/pnas.2107794118Downloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131. Conceptually, our paper is further related to the transformation-based model-free prediction approach de- veloped in refs. 19 and 25 in that we rely on transformations of the original setup into one that is easier to work with (i.e., ranks that are uniformly distributed) and study the properties of our approach in a model-free setting. An important difference is the implementation of the resulting procedure. The transformation- based approach is based on the bootstrap, whereas our approach is based on permuting ranks. Permuting ranks estimated based on the augmented data guarantees the model-free ﬁnite- sample validity of our method with exchangeable data. To our knowledge, no exact ﬁnite-sample validity results have been developed for the bootstrap-based approach. DCP Here, we introduce DCP. We present a full and a split-sample version of our method. Full DCP. Let y denote a test value for YT +1. We test the plausi- bility of each value y ∈ R, collect all plausible values, and report them as the prediction set. In practice, we consider a grid of test values Ytrial. ‡ Deﬁne the augmented data Z (y) = {Z (y) t }T +1 t=1 , where Z (y) t = {(Yt , Xt ) if 1 ≤ t ≤ T (y, Xt ) if t = T +1 . [8] Based on the augmented dataset Z (y), we estimate the condi- tional CDF using a suitable method such as QR and DR, which are discussed in more detail in SI Appendix.Let ˆF (y) denote the estimator for F based on the augmented sample. If the original estimate is not monotonic, we rearrange it (e.g., refs. 35 and 36) so that ˆF (y)(·, x ) is always monotonic. To simplify the exposition, we keep these rearrangements implicit. We compute the ranks { ˆU (y) t }T +1 t=1 ,where ˆU (y) t = { ˆF (y)(Yt , Xt ) if 1 ≤ t ≤ T ˆF (y)(y, Xt ) if t = T +1 , [9] and obtain P values as ˆp(y)= 1 T +1 T +1∑ t=1 1 { ˆV (y) t ≥ ˆV (y) T +1} , [10] where ˆV (y) t := ψ( ˆU (y) t ) and ψ(·) is a deterministic function. For our baseline method, we use ψ(x )= |x − 1/2|.In Extension: Optimal DCP, we show how to choose ψ optimally to ensure efﬁciency. Prediction intervals are computed as ̂C full (1−α)(XT +1)= {y ∈Ytrial :ˆp(y) >α}. § We summarize our approach in Algorithm 1. Algorithm 1: (Full DCP). Input: Data {(Yt , Xt )}T t=1, miscoverage level α ∈ (0, 1),a point XT +1, test values Ytrial Process: For y ∈Ytrial, 1) deﬁne the augmented data Z (y) as in Eq. 9 2) compute ˆp(y) as in Eq. 10 ‡For example, we can choose Ytrial to be a fine grid between − max1≤t≤T |Yt | and max1≤t≤T |Yt |. This choice has a theoretical justification since under exchangeabil- ity, P ( |YT+1| > max1≤t≤T |Yt |) ≤ 1/(1 + T) (34) (a discussion is in the conformal Inference R-package; https://github.com/ryantibs/conformal). §Instead of ̂C full (1−α)(XT+1), we typically report the closed interval ̃C full (1−α)(XT+1)= [min ( ̂C full (1−α)(XT+1)) ,max ( ̂C full (1−α)(XT+1))]. Output: Return (1 − α) prediction set ̂C full (1−α)(XT +1)= {y ∈Ytrial :ˆp(y) >α} Split DCP. An important drawback of full DCP (Algorithm 1)is its computational burden due to the grid search. Since ˆF (y) is obtained based on the augmented data, one has to choose Ytrial and reestimate the entire conditional distribution for all y ∈Ytrial. Therefore, we propose a split conformal procedure that exploits sample splitting, avoids grid search, and only requires estimating F once. Sample splitting is a popular approach for improving the computational performance of conformal prediction methods (e.g., refs. 16 and 20). Algorithm 2: (Split DCP). Input: Data {(Yt , Xt )}T t=1, miscoverage level α ∈ (0, 1), point XT +1 Process: 1) Split {1, ... , T } into T1 := {1, ... , T0} and T2 := {T0 + 1, ... , T } 2) Obtain ˆF based on {Zt }t∈T1 3) Compute { ˆVt }t∈T2 = {ψ( ˆUt )}t∈T2 ,where ˆUt = ˆF (Yt , Xt ) 4) Compute ˆQT2 ,the (1 − α)(1 + 1/|T2|) empirical quantile of { ˆVt }t∈T2 Output: Return (1 − α) prediction set ̂C split (1−α)(XT +1)= {y : ψ ( ˆF (y, XT +1)) ≤ ˆQT2 } (Since ˆF (·, XT +1) is monotonic, ̂C split (1−α)(XT +1) is an interval) In Algorithm 2, we split {1, ... , T } into {1, ... , T0} and {T0 + 1, ... , T }. With iid data, one can also consider random splits. Split DCP lends itself naturally to simple in-sample valid- ity checks with both cross-sectional and time series data as illustrated in Empirical Applications. Theoretical Performance Guarantees In this section, we establish the theoretical properties of our procedure. We focus on full-sample DCP (Algorithm 1). For the split-sample approach (Algorithm 2), we provide a modiﬁed version (SI Appendix, Algorithm S1)in SI Appendix and present its theoretical properties in Extension: Optimal DCP. When the data are iid (or exchangeable), our method achieves ﬁnite-sample unconditional validity in a model-free manner, as a consequence of general results on conformal inference and permutation inference more generally (e.g., refs. 17 and 37). Theorem 1 (Finite-sample unconditional validity). Suppose that the data are iid or exchangeable and that the estimator of the conditional distribution is invariant to permutations of the data. Then, P (YT +1 ∈ ̂C full (1−α) (XT +1)) ≥ 1 − α. The proof of Theorem 1 is standard and omitted. Theorem 1 highlights the strengths and drawbacks of conformal prediction methods. Most commonly used estimators of the conditional CDF such as QR and DR are invariant to permutations of the data. As a result, Theorem 1 provides a model-free unconditional performance guarantee in ﬁnite samples, allowing for arbitrary misspeciﬁcation of the model of the conditional CDF. On the other hand, it has a major theoretical drawback. Even with iid data, it provides no guarantee at all on conditional validity. Our next theoretical results provide a remedy. We impose the following weak regularity conditions. Assumption 1. Suppose that there exists a nonrandom function F ∗(·, ·) such that the following conditions hold as T →∞. Deﬁne Vt := ψ(F ∗(Yt , Xt )) for 1 ≤ t ≤ T +1. 4of9 PNAS https://doi.org/10.1073/pnas.2107794118 Chernozhukov et al. Distributional conformal predictionDownloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131.STATISTICS 1) There exists a strictly increasing continuous function φ :[0, ∞) → [0, ∞) such that φ(0) = 0 and (T + 1)−1 ∑T +1 t=1 φ(| ˆVt − Vt |)= oP (1) and ˆVT +1 = VT +1 + oP (1), where ˆVt := ˆV (YT +1) t = ψ( ˆF (YT +1)(Yt , Xt )) for 1 ≤ t ≤ T +1. 2) supv ∈R | ˜G(v ) − G(v )| = oP (1), where ˜G(v )=(T + 1)−1 ∑T +1 t=1 1{Vt < v } and G(·) is the distribution function of VT +1. 3) supx1̸=x2 |G(x1) − G(x2)|/|x1 − x2| is bounded. Assumption 1 allows for some ﬂexibility with respect to the model estimator. Here, we only require F ∗ to be a nonrandom function, which may or may not be F. The interpretation is straightforward when F ∗ = F since this simply means that the estimator ˆF is consistent for F.Wediscuss thecaseof F ∗ ̸= F after Theorem 2 below. Note that we can replace the consistency requirement in Assumption 1 with a stronger uniform consistency requirement, supx ,y | ˆF (y, x ) − F ∗(y, x )| = oP (1). We also notice that the quantities ˆVt and Vt are deﬁned under the true YT +1. This means that ˆF (y) uses y = YT +1.In other words, the estimator ˆF based on the sample {(Xt , Yt )}T +1 t=1 would be consistent for some F ∗ if YT +1 were observed. ¶ Since the goal of Assumption 1 is to guarantee the coverage probability for YT +1, the conditions in Assumption 1 only need to hold for y = YT +1. Notice that ˆF is consistent for F ∗ under a very weak norm, and no rate condition is required. When ψ(x )= |x − 1/2|, a simple example of φ(·) in Assumption 1 is φ(x )= x q for some q > 0; in other words, a sufﬁcient condition is (T + 1)−1 ∑T +1 t=1 | ˆF (Yt , Xt ) − F ∗(Yt , Xt )|q = oP (1), which can be veriﬁed for many existing estimators with q =2. The following lemma gives the basic consistency result. Lemma 1. Let Assumption 1 hold. Then, ˆG( ˆVT +1)= G(VT +1)+ oP (1), where ˆG(v )=(T +1)−1 ∑T +1 t=1 1{ ˆVt < v }. By Assumption 1, G(·) is uniformly continuous and thus, con- tinuous. Since G(·) is the distribution function of VT +1,we have that G(VT +1) has the uniform distribution on (0, 1) [i.e., P (G(VT +1) ≤ α)= α ]. This implies the unconditional asymp- totic validity. Theorem 2 (Asymptotic unconditional validity). Let Assump- tion 1 hold. Then, P (YT +1 ∈ ̂C full (1−α) (XT +1)) =1 − α + o(1). Theorem 2 establishes the asymptotic unconditional validity of the procedure. Since Theorem 1 already establishes the uncon- ditional validity in ﬁnite samples for iid or exchangeable data without assuming any consistency of ˆF , the main purpose of Theorem 2 is to address the case of nonexchangeable data (e.g., time series data with ergodicity), especially when the model is misspeciﬁed (i.e., if F ∗ ̸= F ). To illustrate model misspeciﬁcation, consider the popular linear QR model, which assumes Q(τ , x )= x ⊤β(τ ), and thus, F (y, x )= F (y, x ; β)= ∫ 1 0 1{x ⊤β(τ ) ≤ y}d τ . This model is typically estimated by ˆβ(τ )= arg minβ ∑T +1 t=1 ρτ (Yt − X ⊤ t β) with ρτ (a)= a(τ − 1{a < 0}). Under misspeciﬁcation [Q(τ , x ) ̸= x ⊤β(τ ) ], ˆβ(τ ) is still estimating β∗(τ )= arg minβ ∑T +1 t=1 E ρτ (Yt − X ⊤ t β),and F ∗ is deﬁned using β∗(·) [e.g., F ∗(y, x )= ∫ 1 0 1{x ⊤β∗(τ ) ≤ y}d τ ]. For parametric models, F ∗ is usually the probability limit of ˆF . In general, ¶This is not really much different from assuming that ˆF basedonthe sample {(Xt , Yt )}T t=1 is consistent for some F∗. we can consider a model F and minimize the empirical risk ˆF = arg ming∈F ∑T +1 t=1 L(Yt , Xt , g) for some loss function L. Even if the model is misspeciﬁed (F /∈F ),it is still possible to show that ˆF is close (in some norm) to F ∗ = arg ming∈F ∑T +1 t=1 E [L(Yt , Xt , g)].In SI Appendix,we provide a more detailed discussion of this and some theoretical results verifying the consistency requirement in Assumption 1 for the time series case; ref. 24 has a general discussion of conformal prediction in time series settings. The cost of allowing for misspeciﬁcation is that one cannot guarantee conditional validity when F ∗ ̸= F . On the other hand, Lemma 1 implies that the prediction intervals are conditionally valid when F ∗ = F . Theorem 3 (Asymptotic conditional validity). Let Assumption 1 hold with F ∗ = F . Then, P (YT +1 ∈ ̂C full (1−α) (XT +1) | XT +1) =1 − α + oP (1). Theorems 2 and 3 establish the asymptotic validity of our pro- cedure under weak and easy to verify conditions. They formalize the key intuition that conditional validity hinges on the quality of the estimator ˆF of the conditional CDF. # Extension: Optimal DCP In Theoretical Performance Guarantees, we have seen that a generic conformity score ψ(y, x )= |F (y, x ) − 1/2| leads to con- ditional validity if the conditional distribution F can be estimated consistently. We now characterize an optimal choice of confor- mity score that results in the shortest prediction interval. Detailed implementation algorithms, technical assumptions, and proofs are provided in SI Appendix. Let Z and X denote the support of Zt =(Yt , Xt ) and Xt , respectively. The optimal prediction interval is Copt (1−α)(x )=[r1(x , α), r2(x , α)], [11] where the functions r1(·, ·), r2(·, ·) satisfy that for any x ∈X , r2(x , α) − r1(x , α)= min F (z2,x )−F (z1,x )≥1−α z2 − z1. [12] The question is whether it is possible to design a conformity score that achieves the above optimal prediction interval. To answer this question formally, we consider a generic conformity score ψ(y, x ), which might contain components that need to be estimated. Permuting a large number of values of {ψ(Yt , Xt )} in con- formal predictions leads to taking the sample (1 − α) quantile of ψ(Yt , Xt ) as the output. For example, following Algorithm 2, one would output the (1 − α)(1 + 1/|T2|) empirical quantile of {ψ(Yt , Xt )}. Assuming a law of large numbers, this empirical quantile would be close to the population (1 − α) quantile of ψ(Yt , Xt ), leading to the asymptotic conformal prediction inter- val for YT +1: Cconf (1−α)(XT +1)= {y : ψ(y, XT +1) ≤ Qψ(1 − α)}, [13] where Qψ(1 − α) is the (1 − α) quantile of ψ(Yt , Xt ).The following result shows how to construct the optimal conformity score ψ. Lemma 2. Let ψ∗(y, x )= |F (y, x ) − b(x , α) − (1 − α)/2|, where b(·, ·) is a function satisfying that for any x ∈X , b(x , α) ∈ arg min z ∈[0,α] Q(z +1 − α, x ) − Q(z , x ). [14] #In Theorem 3, we assume F∗ = F. Since the first version of this paper was posted, ref. 38 has provided more general results where F∗ ≈ F. Chernozhukov et al. Distributional conformal prediction PNAS 5of9 https://doi.org/10.1073/pnas.2107794118Downloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131. Let Cconf (1−α)(XT +1) be deﬁned as in Eq. 13 with the above confor- mity score ψ∗. Assume that F (·, x ) is a continuous function for any x ∈X . Then, Qψ(1 − α)=(1 − α)/2 and μ (Copt (1−α)(XT +1)) = μ (Cconf (1−α)(XT +1)) almost surely, where μ(·) denotes the Lebesgue measure. If the optimization prob- lem in Eq. 11 has a unique solution for any x ∈X , then Copt (1−α)(XT +1)= Cconf (1−α)(XT +1) almost surely. Lemma 2 motivates conformity scores of the form ψ∗(y, x )= |F (y, x ) − [b(x , α)+(1 − α)/2]|,where b(·, ·) solves Eq. 14. Compared with the choice of ψ(y, x )= |F (y, x ) − 1/2| men- tioned in Theoretical Performance Guarantees, we can view ψ∗ as having a “shape” adjustment b(x , α) − α/2.Since F (Yt , Xt ) is independent of Xt , the optimal conformity score measures the distance between two independent components: F (Yt , Xt ) and 1/2+(b(Xt , α) − α/2).Hence,by Lemma 2, in order to take into account the shape of the conditional distribution F (·, x ),it sufﬁces to consider the scalar quantity 1/2+(b(x , α) − α/2). In some special cases, the shape adjustment can be shown to be zero [i.e., b(x , α)= α/2 ]. One typical example is when F (·, x ) is a symmetric unimodal distribution with a well-deﬁned con- ditional density. ‖ Therefore, the choice of ψ(y, x )= |F (y, x ) − 1/2| mentioned in Theoretical Performance Guarantees is optimal in these cases. However, Lemma 2 provides a construction that achieves optimality more generally. By the deﬁnition of ψ∗ and Qψ(1 − α)=(1 − α)/2, the prediction interval is Cconf (1−α)(x )=[Q(b(x , α), x ), Q(b(x , α)+1 − α, x )]. [15] We illustrate this in Fig. 2 with α =0.1.Eq. 15 implies that b(x , α) is the quantile index of the lower bound of the interval. For the symmetric distribution in Fig. 2, Top,wesee b(x , α)= 0.05,which is α/2. For the asymmetric distribution in Fig. 2, Bottom, we see that b(x , α) = 0.007, which is far away from α/2 = 0.05. The ﬁrst result in Lemma 2 is general and allows for the lack of uniqueness of the optimal prediction interval. For example, if F is the uniform distribution on a certain interval, then all condi- tionally valid prediction intervals have the same length. Clearly, in this case, achieving the optimal length is the only goal one can hope for. When we can uniquely deﬁne the optimal prediction interval, Lemma 2 implies that the conformal procedure can recover the uniquely deﬁned optimal interval, not just achieving the optimal length. Lemma 2 also conﬁrms the insight of ref. 13; the optimal conﬁdence set for XT +1 = x should take the form {y : f (y, x ) ≥ c(x )} for some c(x ) > 0,where f (y, x )= ∂F (y, x )/∂y. Assume that F (·, x ) is a unimodal distribution and f (·, x ) is a continuous function for any x ∈X . Then, this conﬁdence set is an inter- val. This means that {y : f (y, x ) ≥ c(x )} =[c1(x ), c2(x )] and f (c1(x ), x )= f (c2(x ), x )= c(x ). We notice that c1(x ), c2(x ) are related to our results in that c1(x )= Q(b(x , α), x ) and c2(x )= Q(b(x , α)+1 − α, x ). To see this, simply observe that the ﬁrst-order condition of the optimization problem in Eq. 14 is 1/f (Q(z +1 − α, x ), x ) − 1/f (Q(z , x )) = 0, which implies that f (Q(b(x , α)+1 − α, x )) = f (Q(b(x , α), x )). To make the procedure operational, we provide the conformal prediction interval ̂Cconf (1−α)(XT +1) deﬁned in ||In this case, Q(1/2 + δ, x) − Q(1/2, x)= Q(1/2, x) − Q(1/2 − δ, x), and the condi- tional density is increasing on (−∞, Q(1/2, x)) and decreasing on (Q(1/2, x), ∞). One can show b(x, α)= α/2 by taking the first-order derivative for the optimization problem in Eq. 14 and setting it to zero. N(0, 1) QY(0.05) QY(0.95) χ 2(5) QY(0.007) QY(0.907) Fig. 2. Optimal prediction intervals. (Top) Symmetric distribution. (Bottom) Asymmetric distribution. SI Appendix, Algorithm S1. We can provide the following guarantee. Theorem 4. Let SI Appendix, Assumption S1 hold. Then, P (YT +1 ∈ ̂Cconf (1−α)(XT +1) | XT +1) =1 − α + oP (1) and μ ( ̂Cconf (1−α)(XT +1)) ≤ μ (Copt (1−α)(XT +1)) + oP (1). The main requirements in SI Appendix, Assumption S1 are consistency of ˆF and that the density f bounded below on its 6of9 PNAS https://doi.org/10.1073/pnas.2107794118 Chernozhukov et al. Distributional conformal predictionDownloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131.STATISTICS support. This is quite mild in the sense that it does not imply that the optimal prediction interval in Eq. 11 is uniquely deﬁned. For example, it allows f to be a uniform distribution. Therefore, as discussed above, the conformal prediction interval would have approximately the shortest length but might not converge to Copt (1−α)(XT +1) in Eq. 11. The following theorem provides a stronger result about ̂Cconf (1−α)(XT +1) based on stronger assumptions. Theorem 5. Let SI Appendix, Assumption S2 hold. Consider the conformal prediction interval ̂Cconf (1−α)(XT +1) deﬁned in SI Appendix, Algorithm S1. Then, μ ( ̂Cconf (1−α)(XT +1)△Copt (1−α)(XT +1)) = oP (1), where △ denotes the symmetric difference of sets [i.e., A△B = (A\\B ) ⋃(B \\A) ], Copt (1−α)(XT +1) is deﬁned in Eq. 11. The key component of SI Appendix, Assumption S2 is consis- tent estimation of b. Theorem 5 shows that ̂Cconf (1−α)(XT +1) is close to Copt (1−α)(XT +1) in the sense that the symmetric difference between these two sets has vanishing Lebesgue measure. Empirical Applications We illustrate the performance of DCP in two empirical appli- cations and provide a comparison with alternative approaches. These examples, especially the second, illustrate the value of our proposal. We consider eight different conformal prediction methods. 1) DCP-QR: DCP with QR (Algorithm 2) 2) DCP-QR∗: Optimal DCP with QR (SI Appendix, Algorithm S1) 3) DCP-DR: DCP with DR (Algorithm 2) 4) CQR: CQR with QR (16) 5) CQR-m: CQR variant (14, 27) with QR 6) CQR-r: CQR variant (14) with QR 7) CP-OLS: Mean-based split conformal prediction (CP) with Ordinary Least Squares (OLS) 8) CP-loc: Locally weighted conformal prediction (20) with OLS All computations were carried out in R (39). Code and data for replicating the empirical results are deposited in GitHub (https://github.com/kwuthrich/Replication_DCP). Predicting Stock Market Returns. Here, we consider the problem of predicting stock market returns, which are known to exhibit substantial heteroskedasticity (a recent review is in chapter 13 in ref. 40 and references therein). We use data on daily returns of the market portfolio (Center for Research in Security Prices value-weighted portfolio) from 1 July 1926 to 30 June 2021.** We use lagged realized volatility Xt to predict the present return Yt . †† Daily returns are not iid and exhibit time series dependence. In SI Appendix, we show that the key conditions underlying our theoretical results hold when the data are β-mixing. Several stochastic volatility models for asset returns, including the pop- ular generalized autoregressive conditional heteroskedasticity models, can be shown to be β-mixing (e.g., refs. 42–44). We evaluate the performance of the different methods by splitting the data into a training and a test sample. To account for the dependence in the data, we present results averaged over ﬁve consecutive prediction exercises. In the ﬁrst exercise, we apply split conformal prediction with an equal split (|T1| = |T2|)tothe **The data are constructed from the Fama/French Three Factors data (41) available from Kenneth R. French’s data library (accessed 17 August 2021). ††We compute realized volatility as the square root of the sum of squared returns over the last 22 d. 510 15 200.40.50.60.70.80.91.0 Conditional coverage 90% prediction intervals BinConditional coverage DCP−QR DCP−QR* DCP−DR CQR CQR−m CQR−r CP−OLS CP−loc Fig. 3. Conditional coverage 90% prediction intervals by realized volatility. ﬁrst 50% of observations and use the next 10% for testing. In the second exercise, we drop the ﬁrst 10% of the observations, apply split conformal prediction to the next 50% of observations, and use the next 10% for testing and so on. Fig. 3 plots the empirical coverage probabilities for 20 bins obtained by dividing up the support of Xt based on equally spaced quantiles. DCP-QR and DCP-QR ∗ yield prediction intervals with coverage levels that are almost constant across all bins and close to the nominal level. They outperform DCP-DR, which undercovers in high-volatility regimes. The conditional coverage properties of DCP-QR and DCP-QR ∗ are very similar to CQR, CQR-m, CQR-r, and CP-loc. This suggest that location-scale models, which are nested by QR, provide a good approximation of the conditional distribution. CP-OLS exhibits overcoverage under low-volatility regimes and substantial undercoverage un- der high-volatility regimes. This ﬁnding has important practical implications since the volatility tends to be high during periods of crisis, which is precisely when accurate risk assessments are most needed. Fig. 4 shows the conditional length of the prediction intervals. DCP-QR, DCP-QR∗,CQR,CQR-m,CQR-r,and CP-loc yield prediction intervals of similar length. The DCP-DR prediction intervals are somewhat shorter than those of the QR-based meth- ods at the upper tail. Finally, CP-OLS yields prediction intervals that are almost constant across all values of realized volatility; they are longer than the DCP intervals at the lower tail and shorter at the upper tail. ‡‡ Predicting Wages Using CPS Data. We consider the problem of predicting wages using individual characteristics. We use the 2012 Current Population Survey (CPS) data provided in the R package hdm (45), which contain information on N = 29,217 observations. Here, we use the index i instead of t. To illustrate the impact of skewness on the performance of the different prediction methods, we use the hourly wage as our dependent 510 15 200246810Conditional length 90% prediction intervals BinConditional length DCP−QR DCP−QR* DCP−DR CQR CQR−m CQR−r CP−OLS CP−loc Fig. 4. Conditional length 90% prediction intervals by realized volatility. ‡‡The CP-OLS prediction intervals are not exactly constant because we are reporting results averaged over five experiments. Chernozhukov et al. Distributional conformal prediction PNAS 7of9 https://doi.org/10.1073/pnas.2107794118Downloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131. Table 1. Coverage 90% prediction intervals DCP-QR DCP-QR ∗ DCP-DR CQR CQR-m CQR-r CP-OLS CP-loc Unconditional coverage 0.90 0.90 0.90 0.90 0.90 0.90 0.90 0.90 SD of predicted conditional coverage (×100) 1.80 1.71 3.08 2.21 2.36 2.30 11.13 4.11 variable Yi . ### Predictors Xi include indicators for gender, marital status, educational attainment, region, experience, expe- rience squared, and all two-way interactions such that dim(Xi )= 100 after removing constant variables. Following refs. 14 and 16, we evaluate the performance of the different methods by randomly holding out 20% of the data for testing, Itest, and applying split conformal prediction with an equal split to the remaining 80% of the data. We repeat the whole experiment 20 times. Table 1 shows that all conformal prediction methods exhibit excellent unconditional coverage properties, conﬁrming the the- oretical ﬁnite-sample guarantees. To assess and compare the conditional coverage properties, for each method, we compute conditional coverage probabilities as the predictions from logis- tic regressions of {Yi ∈ ̂C split (1−α)(Xi )}i∈Itest on {Xi }i∈Itest ,where ̂C split (1−α) is the split conformal prediction interval obtained by the corresponding method. The less dispersed the predicted cover- age probabilities are around the nominal level 1 − α =0.9,the better the overall conditional coverage properties of a method. Table 1 plots the SD of the predicted coverage probabilities. ¶¶ DCP-QR ∗ yields the lowest dispersion of all methods. The predicted coverage probabilities based on DCP-QR are less ###We obtain the hourly wage by exponentiating the log hourly wage provided in the dataset. ¶¶Using √ 1/|Itest| ∑i∈Itest ( ̂Coveragei − 0.9)2,where ̂Coveragei is the predicted cover- age probability, instead of the SD yields very similar results. Table 2. Average length 90% prediction intervals DCP-QR DCP-QR ∗ DCP-DR CQR CQR-m CQR-r CP-OLS CP-loc 34.22 29.61 33.69 34.52 34.84 34.63 33.84 32.66 dispersed than those obtained from CQR, CQR-m, and CQR-r. CP-loc yields a higher dispersion than the methods based on QR and DR, which demonstrates the value added of using ﬂexible models of the conditional distribution. Overall, DCP performs much better than CP-OLS, for which the predicted coverage probabilities exhibit a very high dispersion. SI Appendix, Fig. S1 plots histograms of the predicted coverage probabilities. Table 2 shows the average length of the prediction intervals. DCP-QR ∗ produces the shortest prediction intervals among of all methods. This demonstrates the practical advantage of the shape adjustment when the conditional distribution is skewed. The results also suggest a trade-off between conditional coverage accuracy and average length. For example, CP-OLS and CP-loc, which both exhibit poor conditional coverage properties, yield shorter prediction intervals than DCP-QR. Data Availability. Data and computer codes to replicate all the re- sults in this paper have been deposited in GitHub (https://github.com/ kwuthrich/Replication_DCP). All data are referenced in the main text. ACKNOWLEDGMENTS. We thank the editor, two anonymous referees, Dim- itris Politis, and Allan Timmermann for valuable comments. V.C. acknowl- edges funding from the NSF. All remaining errors are our own. 1. R. Koenker, G. Bassett, Regression quantiles. Econometrica 46, 33–50 (1978). 2. S. Foresi, F. Peracchi, The conditional distribution of excess returns: An empirical analysis. J. Am. Stat. Assoc. 90, 451–466 (1995). 3. V. Chernozhukov, I. Fernandez-Val, B. Melly, Inference on counterfactual distribu- tions. Econometrica 81, 2205–2268 (2013). 4. J. W. Taylor, A quantile regression neural network approach to estimating the conditional density of multiperiod returns. J. Forecast. 19, 299–311 (2000). 5. P. Chaudhuri, W. Y. Loh, Nonparametric estimation of conditional quantiles using quantile regression trees. Bernoulli 8, 561–576 (2002). 6. N. Meinshausen, Quantile regression forests. J. Mach. Learn. Res. 7, 983–999 (2006). 7. M. D. Cattaneo, Y. Feng, R. Titiunik, Prediction intervals for synthetic control meth- ods. arXiv [Preprint] (2019). https://arxiv.org/abs/1912.07120 (Accessed 20 August 2021). 8. V. Chernozhukov, K. Wüthrich, Y. Zhu, An exact and robust conformal infer- ence method for counterfactual and synthetic controls. J. Am. Stat. Assoc., 10.1080/01621459.2021.1920957 (2021). 9. D. Kivaranovic, R. Ristl, M. Posch, H. L. Leeb, Conformal prediction intervals for the individual treatment effect. arXiv [Preprint] (2020). https://arxiv.org/abs/2006.01474 (Accessed 20 August 2020). 10. L. Lei, E. J. Candès, Conformal inference of counterfactuals and individual treatment effects. arXiv [Preprint] (2020). https://arxiv.org/abs/2006.06138 (Accessed 20 August 2021). 11. Y. Romano, R. F. Barber, C. Sabatti, E. J. Candes, With malice towards none: As- sessing uncertainty via equalized coverage. arXiv [Preprint] (2019). https://arxiv.org/ abs/1908.05428 (Accessed 20 August 2021). 12. R. Foygel Barber, E. J. Candès, A. Ramdas, R. J. Tibshirani, The limits of distribution- free conditional predictive inference. J. IMA 10, 455–482 (2021). 13. J. Lei, L. Wasserman, Distribution-free prediction bands for non-parametric regres- sion. J. Royal Stat. Soc. Ser. B. Stat. Methodol. 76, 71–96 (2014). 14. M. Sesia, E. J. Candes, A comparison of some conformal quantile regression meth- ods. Stat 9, e261 (2020). 15. V. Vovk, “Conditional validity of inductive conformal predictors” in Proceedings of the Asian Conference on Machine Learning, S. C. H. Hoi, W. Buntine, Eds. (PMLR, Singapore Management University, Singapore), vol. 25, pp. 475–490 (2012). 16. Y. Romano, E. Patterson, E. Candes, Conformalized quantile regression. Adv. Neural Inf. Process. Syst., 32, 3543–3553 (2019). 17. V. Vovk, A. Gammerman, G. Shafer, Algorithmic Learning in a Random World (Springer Science & Business Media, 2005). 18. V. Vovk, I. Nouretdinov, A. Gammerman, On-line predictive linear regression. Ann. Stat. 37, 1566–1590 (2009). 19. D. N. Politis, Model-Free Prediction and Regression: A Transformation-Based Approach to Inference (Springer, New York, NY, 2015). 20. J. Lei, M. G. Sell, A. Rinaldo, R. J. Tibshirani, L. Wasserman, Distribution-free predictive inference for regression. J. Am. Stat. Assoc. 113, 1094–1111 (2018). 21. R. Koenker, G. Bassett, Robust tests for heteroscedasticity based on regression quantiles. Econometrica 50, 43–61 (1982). 22. R. Koenker, Quantile Regression, Econometric Society Monographs (Cambridge University Press, 2005). 23. J. Lei, J. Robins, L. Wasserman, Distribution free prediction sets. J. Am. Stat. Assoc. 108, 278–287 (2013). 24. V. Chernozhukov, K. Wüthrich, Z. Yinchu, “Exact and robust conformal inference methods for predictive machine learning with dependent data” in Proceedings of the 31st Conference on Learning Theory, S. Bubeck, V. Perchet, P. Rigollet, Eds. (PMLR, Cambridge, MA, 2018), vol. 75, pp. 732–749. 25. D. N. Politis, Model-free model-fitting and predictive distributions. Test 22, 183–221 (2013). 26. I. Komunjer, “Chapter 17 - Quantile prediction” in Handbook of Economic Forecast- ing, G. Elliott, A. Timmermann, Eds. (Elsevier, 2013), pp. 961–994. 27. D. Kivaranovic, K. D. Johnson, H. Leeb, “Adaptive, distribution-free prediction intervals for deep networks” in Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, S. Chiappa, R. Calandra, Eds. (PMLR, Cambridge, MA, 2020), vol. 108, pp. 4346–4356. 28. V. Vovk et al., “Conformal calibrators” in Proceedings of the Ninth Symposium on Conformal and Probabilistic Prediction and Applications, A. Gammerman, V. Vovk, Z. Luo, E. Smirnov, G. Cherubin, Eds. (PMLR, Cambridge, MA, 2020), vol. 128, pp. 84–99. 29. D. J. Eck, F. W. Crawford, Efficient and minimal length parametric conformal predic- tion regions. arXiv [Preprint] (2019). https://arxiv.org/abs/1905.03657 (Accessed 20 August 2021). 30. R. Izbicki, G. T. Shimizu, R. B. Stern, Flexible distribution-free conditional pre- dictive bands using density estimators. arXiv [Preprint] (2019). https://arxiv.org/ abs/1910.05575 (Accessed 20 August 2021). 31. R. Izbicki, G. Shimizu, R. B. Stern, CD-split and HPD-split: Efficient conformal regions in high dimensions. arXiv [Preprint] (2020). https://arxiv.org/abs/2007.12778 (Accessed 20 August 2021). 32. L. Gyorfi, H. Walk, “Nearest neighbor based conformal prediction” (Rep. Stuttgarter Mathematische Berichte 2020-002, Universität Stuttgart, Stuttgart, Germany, 2020). 33. M. Sesia, Y. Romano, Conformal histogram regression. arXiv [Preprint] (2021). https://arxiv.org/abs/2105.08747 (Accessed 20 August 2021). 8of9 PNAS https://doi.org/10.1073/pnas.2107794118 Chernozhukov et al. Distributional conformal predictionDownloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131.STATISTICS 34. W. Chen, Z. Wang, W. Ha, R. F. Barber, Trimmed conformal prediction for high- dimensional models. arXiv [Preprint] (2016). https://arxiv.org/abs/1611.09933 (Ac- cessed 20 August 2021). 35. V. Chernozhukov, I. Fernandez-Val, A. Galichon, Improving point and interval esti- mators of monotone functions by rearrangement. Biometrika 96, 559–575 (2009). 36. V. Chernozhukov, I. Fernández-Val, A. Galichon, Quantile and probability curves without crossing. Econometrica 78, 1093–1125 (2010). 37. W. Hoeffding, The large-sample power of tests based on permutations of observa- tions. Ann. Math. Stat. 23, 169–192 (1952). 38. E. J. Candès, L. Lei, Z. Ren, Conformalized survival analysis. arXiv [Preprint] (2021). https://arxiv.org/abs/2103.09763 (Accessed 20 August 2021). 39. R Core Team, R: A Language and Environment for Statistical Computing (R Founda- tion for Statistical Computing, Vienna, Austria, 2021). 40. G. Elliott, A. Timmermann, Economic Forecasting (Princeton University Press, 2016). 41. R. K. French, Kenneth French Data Library. Fama/French 3 Factors [Daily] Data (2021). http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html. Accessed 17 August 2021. 42. F. Boussama, “Ergodicité, mélange et estimation dans les modeles GARCH,” PhD thesis, Paris 7, Paris, France (1998). 43. M. Carrasco, X. Chen, Mixing and moment properties of various GARCH and stochastic volatility models. Econom. Theory 18, 17–39 (2002). 44. C. Francq, J. M. Zakoïan, Mixing properties of a general class of GARCH (1,1) models without moment assumptions on the observed process. Econom. Theory 22,815– 834 (2006). 45. V. Chernozhukov, C. Hansen, M. Spindler, hdm: High-dimensional metrics. RJ. 8, 185–199 (2016). Chernozhukov et al. Distributional conformal prediction PNAS 9of9 https://doi.org/10.1073/pnas.2107794118Downloaded from https://www.pnas.org by 97.113.17.131 on August 1, 2024 from IP address 97.113.17.131.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/Zhang21LearningMultiLayeredGBDT.pdf","text":"Learning Multi-Layered GBDT Via Back Propagation Zhendong Zhang School of Electronic Engineering, Xidian University Xi’an, 710071, China zhd.zhang.ai@gmail.com Abstract Deep neural networks are able to learn multi-layered representation via back propagation (BP). Although the gradient boosting decision tree (GBDT) is eﬀective for modeling tabular data, it is non-diﬀerentiable with re- spect to its input, thus suﬀering from learning multi- layered representation. In this paper, we propose a framework of learning multi-layered GBDT via BP. We approximate the gradient of GBDT based on linear re- gression. Speciﬁcally, we use linear regression to replace the constant value at each leaf ignoring the contribution of individual sample to the tree structure. In this way, we estimate the gradient for intermediate representa- tions, which facilitates BP for multi-layered GBDT. Ex- periments show the eﬀectiveness of the proposed method in terms of performance and representation ability. To the best of our knowledge, this is the ﬁrst work of opti- mizing multi-layered GBDT via BP. This work provides a new possibility of exploring deep tree based learning and combining GBDT with neural networks. 1. Introduction Deep neural networks have achieved outstanding breakthrough of machine learning in recent years [12]. Feature engineering is important for traditional ma- chine learning methods that lack the ability to learn intermediate representation from raw data. However, deep neural networks can learn good representations from raw data [1], which is believed to be one of the key advantages beyond traditional methods. This is achieved by constructing multi-layered neural networks and learning hierarchical representations. Although many learning strategies have been proposed for train- ing deep neural networks such as target propagation [19] and layer-wise greedy training [14], back propaga- tion (BP) [23] with stochastic gradient descent is still the dominant approach. BP is used to compute the gradients of parameters layer by layer in the inverse direction. On the other hand, tree based ensembles such as random forest [3] and gradient boosting decision trees (GBDT) [10, 4] (we just name a few representative works) are still the best choice of modeling tabular data because of not only their performance but also their computational eﬃciency and interpretability. However, there are no hierarchical representations of these models which limit their expressiveness. How can we construct multi-layered tree ensembles? Recently, researchers have explored this issue pop- ularly. Zhou and Feng [28] proposed a deep forest framework, which was the ﬁrst attempt to construct a multi-layered model using tree ensembles. They intro- duced ﬁne-grained scanning and cascading operations and constructed a multi-layered structure with adaptive model complexity. However, the representation learning ability of deep forest was diﬃcult to explicitly examine. Later, Feng et al. [9] proposed the ﬁrst multi-layered structure called mGBDT using gradient boosting de- cision trees as building blocks per layer with an ex- plicit emphasis on its representation learning ability. mGBDT was learned via target propagation. Speciﬁ- cally, targets were approximated and propagated by a set of inverse functions corresponding to the forward functions. Both inverse and forward functions were constructed by GBDT. Such explorations showed the feasibility of learning hierarchical tree ensembles. Their results demonstrated that tree ensembles were beneﬁcial from multi-layered representations. Several methods of diﬀerentiable and probabilistic decision trees have been proposed [11, 18, 22, 13, 25, 8]. The probability of going to the left branch was determined by the inner product between inputs and learned tree parameters. Training this kind of decision trees was similar to train- ing neural networks. For traditional decision trees used in GBDT, the left branch and right branch were split by a threshold along one of the features in a deterministic manner. Since probability decision trees are diﬀerent from the ones used in GBDT in terms of hypothesis space and training methodology, [11, 18, 22, 13, 25, 8] 1arXiv:2109.11863v2 [cs.LG] 27 Sep 2021 are outside the scope of this work. Researchers also have explored combining GBDT with neural networks. [17] distills the learned represen- tations of GBDT into deep neural networks. In [15], the inputs are ﬁrst processed by GBDT then processed by graph neural networks. GBDT is learned via the gradients provided by graph neural networks. Although the accuracy is improved, the combination mechanism is restricted. In this work, we focus on traditional decision trees. We learn multi-layered GBDT via BP which is diﬀerent from mGBDT. Since GBDT is non-diﬀerentiable and non-parametric, it seems impossible to apply BP to GBDT. Our solution to this problem is as follows: we replace the constant value at each leaf with linear re- gression, called piece-wise linear GBDT (GBDT-PL) in a recent work [24]. We ﬁrst learn the structure of each regression tree, and use it to decide the set of inputs belonging to each leaf. We apply linear regression to each leaf to obtain the optimal linear parameters w. Then, we express the gradient of output w.r.t its input in terms of w (see Section 3 for details). Note that this is an approximated gradient because the true gradient depends on not only the linear parameters but also the structure of trees. When samples for training become massive, it is reasonable to assume individual sample has little eﬀect on the structure of trees. In fact, a series of works indicates that the true gradient is not necessary for updating parameters [2, 5, 20]. When we stack multi layers of GBDT, the gradient information can be propagated from the ﬁnal output to the input layer by layer via BP. Then, we update the variables of hidden layers along the direction of gradient descent and re-train GBDT to ﬁt those updated hidden vari- ables. Although we could compute the gradient of w in a similar way, it is unnecessary to do that during the BP process. We obtain w by linear regression after the structure of trees is learned, instead of gradient descent. We call the proposed method as GBDT-BP. We evaluate GBDT-BP in terms of performance and representation learning ability on both synthetic data and real-world data. Compared with existing methods, our main contributions and novelties are as follows: • Although the idea of combining GBDT and linear regression at leaves appears earlier, this is the ﬁrst work to explicitly point out that such a combination leads to an approximately diﬀerentiable model. • This is the ﬁrst work to learn multi-layered GBDT via BP which provides a new possibility of exploring deep tree based learning and combining GBDT with neural networks. In principle, GBDT-BP and neural networks can be combined in arbitrary way. 2. Background 2.1. Back Propagation Back propagation (BP), shorthand for ”backward propagation of errors”, is a method to compute the gradients w.r.t. the weights of multi-layered neural networks which has become the key ingredient of the training algorithm of deep neural networks. Although BP is originally proposed for neural networks, in prin- ciple it can be utilized to compute the gradients of any layer structured diﬀerentiable models. Thus, we formalize the process of BP beyond neural networks as follows. Denote L as the ﬁnal loss and hi as the hidden vector of i-th layer. Denote fwi as the feed forward function of i-th layer parameterized by wi. That is, hi = fwi−1(hi−1). Given the gradients of hi, BP com- putes the gradients of hi−1 and wi−1 as follows: ∂L ∂hi−1 = ( ∂hi ∂hi−1 )T ∂L ∂hi (1) ∂L ∂wi−1 = ( ∂hi ∂wi−1 )T ∂L ∂hi (2) where ∂hi ∂hi−1 and ∂hi ∂wi−1 are jacobians. From (1), the gradients of the i-th layer are propagated to the (i − 1)- th layer. Then, the same rule is applied layer by layer until the inputs. In this way, the gradients of w at all layers are obtained. BP requires that the model is diﬀerentiable or the gradient can be evaluated within acceptable compu- tational complexity. A series of works is proposed to overcome these limitations of BP in particular situa- tions. [2] proposed straight through estimator to learn binary variables. [20] proposed relaxation methods to learn a particular discrete probability distribution. [5] proposed the synthetic gradient which reduces the com- putation and communication complexity for distributed training. Their model is updated using the synthetic gradient instead of the true gradient. The eﬀective- ness of these works indicates that it is a good choice to use the approximated gradient when the model is non-diﬀerentiable or the true gradient is too expensive to evaluate. 2.2. Gradient Boosting Decision Trees Boosting is a widely used strategy for ensemble learn- ing [27]. Boosting algorithms iteratively add weak learn- ers into the ﬁnal strong learner. The construction of each weak learner is usually guided by the gradient of current predictions, i.e. gradient boosting. When we choose decision trees as the weak learner, we get gradient boosting decision trees. Formally, denote M as the function of an individual tree. The prediction of GBDT is the summation of all trees: ˆy = N∑ i=1 Mi(x) (3) Given the ﬁrst N trees, the (N + 1)-th tree is grown by ﬁtting current negative gradient w.r.t. ˆy. GBDT is ﬁrst proposed in [10]. It is still the best choice for modeling tabular data which spans a broad range of real-world applications. The most important problem of GBDT is how to split a node. The quality of splitting is measured by some objectives. Thus, this problem is cast to choosing suitable objectives. Traditional GBDTs use so-called “friedman mse”. Recent implementations for GBDTs such as XGBoost [4] and LightGBM [16] use not only the ﬁrst order gradient but also the second order gra- dient to guide the learning of each tree. [4] ﬁrstly derived the corresponding splitting objective which is a generalization of “friedman mse”. When the second order gradient is a constant, their proposed objective is equivalent of “friedman mse”. Another problem of GBDT is how to compute the predictions of each leaf. A constant value is usually used. For traditional GBDT, this value equals the mean of the negative gradient as follows: 1 N ∑ i∈leaf −gi (4) where N is the number of samples belonging to the same leaf and g is the gradient. For implementations using second order gradient, the leaf value equals to the negative gradient weighted by second order gradient. − ∑ i∈leaf gi λ + ∑ i∈leaf g′ i (5) where g′ is the second order gradient and λ is the strength of L2 regularization. [6] replaced the constant leaf value with linear regression to derive the corre- sponding spitting objective. 3. Proposed Method In this section, we describe the proposed method in a top-down manner. We ﬁrst provide the overall learning algorithm. Then, we show how to compute the gradient of GBDT. Finally, we describe how to learn the linear regression at each node. Implementation details are also discussed. 3.1. Overall Learning Algorithm Consider a multi-layered structure with L − 1 hidden layers. Denote hi ∈ Rdi where i ∈ {0, 1, 2, . . . L} as the hidden variables of i-th layer with dimension di. For convenience, denote h0 and hL as the input and the ﬁnal output layer respectively. Except for the input layer, each element of hi is obtained by a GBDT with hi−1 as its input. That is, hi is a concatenation of the outputs of di GBDTs. Note that we replace the constant prediction at each leaf with linear regression to make GBDT approximate diﬀerentiable. Given a diﬀerentiable loss L, the gradient of L-th layer is obtained directly. It is required to compute the gradient of all hidden layers. Suppose that the jacobian of any two adjacent layers is given (we will describe how to compute it for GBDT in Section 3.2). Then, we can compute the gradient of all hidden layers via BP as in (1). Although the BP process is similar to deep neural networks, the update rule is signiﬁcantly diﬀerent. For neural networks, their parameters are up- dated directly by (stochastic) gradient descent. Recall that there are linear parameters learned at each leaf and thus the model becomes parametric. However, it is meaningless to apply gradient descent to these pa- rameters. It is meaningful only when the tree structure is ﬁxed. Unfortunately, the tree structure is dynami- cally changed during training. Thus, it is unnecessary to compute the gradient of the linear parameters and update them. We only update the hidden variables via gradient descent as follows: hi ← hi − α ∂L ∂hi (6) where α is the learning rate. Then, we clean previously learned GBDTs and train new ones from scratch to ﬁt the updated hidden variables based on mean square error (MSE). Note that when we re-train new GBDTs to ﬁt the updated hi, hi−1 has not been updated yet. That is, hidden variables and GBDTs are updated al- ternately layer by layer in the inverse direction. We summarize GBDT-BP in Algorithm 1 and provide the ﬂow of GBDT-BP with two hidden layers in Fig. 1. 3.2. Gradient of GBDT Layer We compute the jacobian of two adjacent GBDT layers to propagate the gradient as follows. We ﬁrst formalize a single regression tree. Here, it is not our interest how the tree structure is determined. We sup- pose the tree structure is ﬁxed. Denote M : Rn → R as the function of a tree as follows: M(x) = ∑ i∈leaf I(x ∈ i)fwi(x) (7) x T1 h1 T2 h2 y h1 − αg1 h2 − αg2 g1 g2 ﬁt ﬁt Figure 1. Flow of GBDT-BP with two hidden layers. We update the hidden variables via gradient descant and retrain decision trees to ﬁt the updated ones. x and y are the input and output respectively. h is the hidden representation and g is its gradient. α is the learning rate, while T is the GBDT model. The subscripts 1 and 2 indicate the number of layers. Algorithm 1 Learning multi-layered GBDT via back propagation Input: Number of layers L, training dataset {h0, Y}, ﬁnal loss function L, learning rate α for hidden variables, learning rate β for GBDT and number of boosters Ni Output: Multi-layered GBDTs Initialize forward function Ti of layer i for i = 0, 1, . . . L − 1 repeat // Forward for computing hidden variables for i = 0 to L − 1 do hi+1 = Ti(hi) end for // Backward for computing gradients gL ← ∂L(hL,Y) ∂hL for i = L to 1 do gi−1 ← ( ∂hi ∂hi−1 )T gi // How to compute the jacobian is described in section 3.2 and 3.3 end for // Update hidden variables and ﬁt GBDTs from scratch for i = L to 1 do hi ← hi − αgi Ti−1, ˆh ← {}, 0 // Initialize to an empty list for j = 1 to Ni do r ← ˆh − hi // Gradient of MSE M ← RegressionT ree(hi−1, −r) // Fit regression tree with variables of previous layer and negative gradient ˆh ← ˆh + βM(hi−1) Append M to Ti−1 end for end for until convergence Return: {T0, T1, . . . TL−1} where leaf is a set of leaves, I is the indicator function and fwi is the function of leaf i. That is, we ﬁrst decide which leaf an input belongs to, then we compute the prediction of that leaf. The gradient of M w.r.t its input is as follows: ∂M(x) ∂x = ∑ i∈leaf I(x ∈ i) ∂fwi(x) ∂x + ∂I(x ∈ i) ∂x fwi(x) (8) The indicator function should be ﬁxed if we suppose 2 2,4 2,4 1 2 3 4 6 5 Figure 2. Example of incremental feature selection. Each circle is a leaf, while numbers in a circle are the selected features for that leaf. the tree structure is ﬁxed. This means ∂I(x∈i) ∂x = 0, ∀i ∈ leaf . That is, an individual sample has no eﬀect on the tree structure. This is not true but it is a good approximation as the training samples become massive. Then, the gradient is simpliﬁed as: ∂M(x) ∂x = ∑ i∈leaf I(x ∈ i) ∂fwi(x) ∂x (9) Note that if f is a constant function, the gradient degrades to 0. In this case, we are not able to learn multi-layered GBDT via BP. This is why we replace the constant value with linear regression. We have derived the gradient of an individual regression tree. Since the prediction of GBDT is the summation of the predictions of all regression trees, the gradient of GBDT is easily obtained. Denote T : Rn → R as the function of GBDT. Then, we get: ∂T ∂x = ∑ i ∂Mi ∂x (10) Suppose given hi−1, j-th element of hi is predicted by j-th GBDT, i.e. T j. Then, the jacobian is as follows: ∂hi ∂hi−1 = ( ∂T 0 ∂hi−1 , ∂T 1 ∂hi−1 , . . . ∂T di ∂hi−1 )T (11) Once we get the jacobian, we can propagate the gradient of multi-layered GBDT using (1). 3.3. Linear Regression at Leaves The jacobian depends on leaf functions, and we de- scribe how to learn leaf functions. As mentioned earlier, the parameters of each leaf are learned by linear regres- sion. In [6], all dimensions of inputs are used for linear regression. However, the drawbacks of such a strategy are obvious, as addressed in [24]. First, the computa- tional cost is relatively high. Second, given samples in a leaf, not all dimensions are useful for predicting their targets, thus such a strategy has a high risk of over-ﬁtting. Feature selection is required. In this work, we follow the incremental feature selection proposed in [24]. Given a leaf, we trace back its decision path. For each node in the decision path, there is a dimension used to split. We collect all splitting dimensions that appeared in the decision path. Then, we only use those dimensions to learn the linear regression model. As shown in Fig. 2, the number of selected dimensions is no more than the depth of trees. Those dimensions are relevant to the targets. Another useful trick for linear regression is extending the feature dimensions based on some transforms, which increases the capacity of linear regression. Denote x ∈ Rd as a sample with d dimensions. Here, we suppose these d dimensions are already captured by incremental feature selection. Denote X ∈ Rn×d as a collection of n samples belonging to the same leaf. Denote y ∈ Rn as their current targets (negative residuals). Denote φ as the feature extending function. Then, the objective of linear regression is as follows: ∥φ(X)w − y∥2 2 + λ 2 wT w (12) where λ is the strength of L2 regularization. The solu- tion of (12) is as follows: w∗ = [φ(X)T φ(X) + λI]−1φ(X)T y (13) Then, the predictions of this leaf is as follows: fw∗ (x) = φ(x) T w∗ (14) The gradient of the leaf function is as follows: ∂fw∗ (x) ∂x = ( ∂φ(x) ∂x )T w∗ (15) It is easy to choose a diﬀerentiable φ. From Sections 3.2 and 3.3, we get the jacobian of GBDT layers and thus back propagation is operational. 3.4. Implementation Details For each regression tree, we use DecisionT reeRegressor in scikit-learn package [21] which splits a node based on “friedman mse”. Although recent second order based implementations show better performance, there is no signiﬁcant diﬀerence in our case because MSE is used as the loss function to ﬁt hidden variables. Although [6] developed a novel splitting objective for linear regression at leaves, we still use “friedman mse” for a fair comparison and easy implementation. The splitting objective is not the goal of this work. For feature extending function, we set φ as follows: φ(x) = [1, x, x2] (16) Other choices for feature extending can be possible. Momentum is commonly used when training neural networks with stochastic gradient descent to speed up the learning process. We introduce this technique into GBDT-BP as follows. Denote mi as the momentum of hi as follows: mi ← µmi + (1 − µ) ∂L ∂hi (17) When we update hi, we replace its gradient with mi in (6). We set µ = 0.5 and α = 0.5 in all experiments. Hidden variable h is initialized as random Gaussian noise. 4. Experiments We ﬁrst evaluate GBDT-BP on two synthetic datasets: Circle and Curve. We train GBDT-BP on them in supervised setting and unsupervised setting, respectively. Then, we compare the function approxima- tion capability of GBDT-BP by ﬁtting a random neural network. Finally, we evaluate GBDT-BP on three tabu- lar datasets and MNIST 1. All tabular datasets are from UCI [7]: Income Prediction 2, Protein Localization 3, and Drug Consumption 4. Income Prediction and Protein Localization are also used in [9], and we choose them for a fair comparison. We obtain the best performance by adjusting the depth of trees and the number of boosters as [9]. GBDT-BP is convergent within 60 epochs for all datasets. We denote U is a uniform distribution, while N is a normal distribution. 4.1. Synthetic Data Circle: This dataset is used for binary classiﬁcation as shown in Fig. 3(a). The angle of each point is sampled from U (0, 2π). The radius of blue points is sampled from U(0.8, 1.0), while the radius of red points is sampled from U (0.4, 0.6). In this way, we generate 10000 points for training. We build a single hidden layer GBDT- BP model with structure 2 → 2 → 1. The maximum tree depth is set to 6 and the number of boosters per 1http://yann.lecun.com/exdb/mnist/ 2http://archive.ics.uci.edu/ml/datasets/Adult 3http://archive.ics.uci.edu/ml/datasets/Yeast 4https://archive.ics.uci.edu/ml/datasets/ Drug+consumption+%28quantiﬁed%29 GBDT is set to 8. We show how hidden representations progress in Fig. 3. It is obvious that the learned hidden representations make the classiﬁcation easier. At the end of the progress, we can completely separate them by a single node of a decision tree. Curve: This dataset is used for self-reconstruction as shown in Fig. 4(a). Each 3D point is sampled by: [t, N (0, 0.05) + sin t, N (0, 0.05) + cos t] (18) where t is U (−1, 1). Those points belong to a 2D mani- fold. We generate 10000 points for training. We train a GBDT-BP auto-encoder with structure 3 → 2 → 3. The maximum tree depth is set to 5 and the number of boosters per GBDT is set to 32. We provide how ﬁnal reconstructions progress in Fig. 4. As shown in the ﬁgure, the 2D manifold is successfully captured. Those results indicate that GBDT-BP is feasible in both supervised setting and unsupervised setting. Moreover, it learns meaningful hidden representations of data. Random neural network : We check the function ap- proximation capability of GBDT-BP by ﬁtting it to a random neural network function. Speciﬁcally, the input x ∈ R32 is sampled from U(0, 1). Its corresponding target is computed by the following random initialized neural network: Dense(32, 16) → ReLU → Dense(16, 1) We generate 6000 pairs of samples, and split them into training and test sets by 10-fold cross-validation5. We compare GBDT-BP with GBDT, XGBoost and GBDT-PL. Here, GBDT-PL is implemented by us as the building block of GBDT-BP. We measure the per- formance in terms of root mean square error (RMSE). The GBDT-BP model has a single hidden layer with 32 variables. The numbers of boosters are 40 and 200 for the ﬁrst and second layer. For single-layered models, we select the best RMSE within 2000 boosters. GBDT-BP is signiﬁcantly better than others. From Table 1, it can be observed that linear regression at leaves has no remarkable eﬀects on the performance. Thus, it can be concluded that the performance gain is contributed by multi-layered representations. 4.2. Real Data For real data, we compare GBDT-BP with GBDT- PL, XGBoost, fully connected neural networks (NN) 5We randomly split the samples into 10 folds. At each time, we use one of those folds as test set, while we use the others as training set. We repeat this process 10 times until all folds are used as test set. We report mean and standard deviation of the results. (a) Circle 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 0.50 0.25 0.00 0.25 0.50 0.75 (b) Epoch 2 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 0.75 0.50 0.25 0.00 0.25 0.50 0.75 (c) Epoch 4 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 0.75 0.50 0.25 0.00 0.25 0.50 0.75 (d) Epoch 6 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 1.0 0.5 0.0 0.5 1.0 (e) Epoch 11 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 1.0 0.5 0.0 0.5 1.0 (f) Epoch 13 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 1.0 0.5 0.0 0.5 1.0 (g) Epoch 17 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 1.0 0.5 0.0 0.5 1.0 (h) Epoch 23 Figure 3. Progress of hidden representations on Circle. We train a 2-2-2 multi-layered GBDT for binary classiﬁcation to learn meaningful hidden representations. 1.000.750.500.250.000.250.500.751.00 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 (a) Curve 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 (b) Epoch 2 1.0 0.5 0.0 0.5 1.0 1.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 (c) Epoch 4 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 (d) Epoch 6 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.51.0 1.0 0.5 0.0 0.5 1.0 (e) Epoch 10 1.000.750.500.250.000.250.500.751.00 1.0 0.5 0.0 0.51.0 1.0 0.5 0.0 0.5 1.0 (f) Epoch 15 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 (g) Epoch 25 1.000.750.500.250.000.250.500.751.00 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 (h) Epoch 47 Figure 4. Progress of ﬁnal reconstructions on Curve. We train a 3-2-3 multi-layered GBDT as an auto-encoder to learn the 2D manifold of 3D data. Table 1. RMSE comparison for ﬁtting a random neural network function. Method RMSE Constant Leaf Multi Layer GBDT 0.01867 ± 0.00023 √ × XGBoost 0.01886 ± 0.00018 √ × GBDT-PL 0.01861 ± 0.00022 × × GBDT-BP 0.01775 ± 0.00026 × √ Table 2. Classiﬁcation accuracy on Income Prediction and Protein Localization datasets. NN is fully connected neural networks, while ∗ indicates the result is reported from literature [9]. Method Income Prediction Protein Localization Drug Consumption MNIST-3000 XGBoost 0.8721 0.5951 ± 0.0341 0.7040 ± 0.0251 0.9242 GBDT-MO 0.8730 0.6079 ± 0.0303 0.7047 ± 0.0237 0.9263 GBDT-PL 0.8726 0.6037 ± 0.0308 0.6962 ± 0.0265 0.9254 NN 0.8543∗ 0.5907 ± 0.0268 ∗ 0.6954 ± 0.0291 0.9305 mGBDT 0.8742∗ 0.6160 ± 0.0323 ∗ 0.6990 ± 0.0321 0.9284 GBDT-BP 0.8741 0.6186 ± 0.0296 0.7063 ± 0.0306 0.9317 trained with back propagation, and mGBDT. Exper- imental results are presented in Table 2. On every dataset, we keep the experimental settings as same for all methods. Income Prediction: This dataset consists of 32561 training samples and 16281 test samples each of which contains social background of a person. We predict whether this person makes over 50K a year based on its background. After one-hot coding for categorical attributes, we get 113 features of each sample as [9]. We train a GBDT-BP model with structure 113 → 32 → 1. Protein Localization: This dataset is a 10 class clas- siﬁcation task that contains 1484 samples. There are 8 measurements of a protein sequence in each sample. Our goal is to predict protein localization sites with 10 possible choices. We evaluate and compare the accu- racies of diﬀerent methods via 10-fold cross-validation. For a fair comparison, we use exactly the same set of folds as [9]. We train a GBDT-BP model using struc- ture 8 → 16 → 10. We also examine the eﬀect of layers on this dataset. The dimension for each hidden layer is ﬁxed to be 16. As shown in Table 3, although the test accuracies decrease as the the number of layers in- creases, the results are stable. The results demonstrate the eﬀectiveness of GBDT-BP when training deeper models. Drug Consumption: This dataset is a 7 class clas- siﬁcation task that contains 1885 samples. There are 12 measurements of people’s personality information in each sample. Our goal is to predict when drugs are used by a person: ”never”, ”over a decade ago”, ”in last decade”, ”in last year”, ”in last month”, ”in last week”, or ”in last day”. We evaluate and compare the accuracies of diﬀerent methods via 10-fold cross- validation. We train a GBDT-BP model using structure 12 → 32 → 7. MNIST-3000 : MNIST (Mixed National Institute of Standards and Technology) is a popular dataset for eval- uating machine learning algorithms. There are 50000 training samples and 10000 test samples. Each sample is a 28 × 28 gray image which expresses one of digits from 0 to 9. Because training mGBDT consumes a lot of memory, we only use the ﬁrst 3000 samples for training. We call this dataset MNIST-3000. We reshape each image to a vector before ﬁtting it into models. We train a GBDT-BP model using structure 784 → 64 → 10. Experimental results are presented in Table 2. The proposed method achieves the best performance except on Income Prediction. The proposed method is obviously better than single-layered GBDT. This indicates the beneﬁts of multi-layered structure. The proposed method is obviously better than single-layered GBDT-PL. This indicates the performance gain is mainly contributed by multi-layered structure instead of the piece-wise linear function of leaves. The pro- posed method is competitive with or sightly better than mGBDT. This indicates the feasibility of training GBDT via back propagation. 4.3. Combination of GBDT and NN Back propagation with gradient descant becomes a dominant learning paradigm of neural networks. Train- ing GBDT with back propagation makes it possible to jointly train a model which combines GBDT and NN. Table 3. Classiﬁcation accuracy on Protein Localization dataset with varying number of layers. The dimension of all hidden layers is 16. NN is fully connected neural networks, while ∗ indicates that the result is reported from literature [9]. Hidden Layer NN ∗ mGBDT∗ GBDT-BP 1 0.5803 ± 0.0316 0.6160 ± 0.0323 0.6186 ± 0.0296 2 0.5907 ± 0.0268 0.5948 ± 0.0268 0.5938 ± 0.0382 3 0.5901 ± 0.0270 0.5897 ± 0.0312 0.5882 ± 0.0461 4 0.5768 ± 0.0286 0.5782 ± 0.0229 0.5751 ± 0.0291 (a) GBDT-BP (b) NN (c) GBDT-NN (d) NN-GBDT Figure 5. We visualize the hidden representations on Protein Localization using T-SNE. The clusters of GBDT-BP have larger margin compared with NN. The representations of GBDT-NN is similar to GBDT-BP and the representations of NN-GBDT is similar to NN. Table 4. Classiﬁcation accuracy on Protein Localization. Method Income Prediction Protein Localization NN 0.8543 0.5907 ± 0.0268 GBDT-BP 0.8741 0.6186 ± 0.0296 NN-GBDT 0.8611 0.6020 ± 0.0257 GBDT-NN 0.8748 0.6203 ± 0.0281 This provides more ﬂexibility to design novel machine learning models. There are two natural ways to combine GBDT and NN. We ﬁrst construct GBDT layers, then we construct NN layers following GBDT layers. We call it GBDT- NN. Or we ﬁrst construct NN layers, the we construct GBDT layers following NN layers. We call it NN-GBDT. In this section, we evaluate GBDT-NN and NN-GBDT on Income Prediction and Protein Localization. We train models with structure 113 → 32 → 1 on Income Prediction and models with structure 8 → 32 → 10 on Protein Localization. For NN-GBDT, the NN layer is updated by 15 stochastic gradient descant steps in each epoch. Experiments are shown in Table 4. The results indi- cate the feasibility of combining NN and GBDT. GBDT- NN achieves the best performance. GBDT-NN is much better than NN-GBDT. One possible reason is that the approximated gradient by GBDT-BP is not accurate and it may mislead the updating of NN. Better gradi- ent estimation algorithms of GBDT are helpful in the future. We also visualize the hidden representations on Protein Localization using T-SNE in Figure 5. The clus- ters of GBDT-BP have larger margin compared with NN. This indicates that GBDT-BP learns better hidden representations than NN. Moreover, the representations of GBDT-NN is similar to GBDT-BP and the represen- tations of NN-GBDT is similar to NN. This indicates that the earlier layers may have greater impact on the representations. 5. Discussions mGBDT is the most related work to GBDT-BP. We highlight two diﬀerences between them: 1) The main diﬀerence is how hidden variables are updated. mGBDT updates them via target propagation by ﬁtting a set of inverse functions; 2) The ﬁnal layer of mGBDT is a fully connected neural network layer. The mixture of GBDT and neural network makes it diﬃcult to rec- ognize the contribution of each part. When we train GBDT-BP with a smaller batch size than the full batch size, the performance is slightly decreased. Thus, we train GBDT-BP with full batch size in this work. As mentioned in Section 3.1, once the hidden representa- tions are updated, we re-train GBDTs from scratch. An alternative way is that we keep previously trained GB- DTs and train new GBDTs to ﬁt the change of hidden representations. Our consideration is that the change of hidden representations is not consistent, thus leading to the redundancy of trees. Training GBDT-BP is time- consuming because 1) it requires many epochs; and 2) it requires multiple GBDTs in hidden layers. For the ﬁrst reason, we may need better updating rules. For the second reason, we may use recently proposed GBDT- MO [26] to learn multiple outputs simultaneously. We leave faster training as future works. 6. Conclusions and Future Works We have proposed a method of learning multi-layered GBDT via back propagation. We have achieved this by replacing the constant leaf value with linear regression and approximating its derivatives. We have provided new probabilities of learning deep tree based ensem- bles. Experiments on both synthetic and real-world datasets verify that the proposed method outperforms single-layered GBDT in terms of both performance and representation ability. Moreover, the proposed method supports both supervised and unsupervised learning.References [1] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Trans- actions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013. 1 [2] Y. Bengio, N. L´eonard, and A. Courville. Estimat- ing or propagating gradients through stochastic neu- rons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. 2 [3] L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001. 1 [4] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785–794. ACM, 2016. 1, 3 [5] W. M. Czarnecki, G. ´Swirszcz, M. Jaderberg, S. Osin- dero, O. Vinyals, and K. Kavukcuoglu. Understanding synthetic gradients and decoupled neural interfaces. In Proceedings of the International Conference on Machine Learning, pages 904–912, 2017. 2 [6] L. de Vito. Linxgboost: Extension of xgboost to generalized local linear models. arXiv preprint arXiv:1710.03634, 2017. 3, 5 [7] D. Dheeru and E. Karra Taniskidou. UCI machine learning repository, 2017. 6 [8] J. Feng, Y.-X. Xu, Y. Jiang, and Z.-H. Zhou. Soft gradi- ent boosting machine. arXiv preprint arXiv:2006.04059, 2020. 1 [9] J. Feng, Y. Yu, and Z.-H. Zhou. Multi-layered gra- dient boosting decision trees. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 3555–3565, 2018. 1, 6, 8, 9 [10] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 1189–1232, 2001. 1, 3 [11] N. Frosst and G. Hinton. Distilling a neural net- work into a soft decision tree. arXiv preprint arXiv:1711.09784, 2017. 1 [12] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning, volume 1. MIT press Cambridge, 2016. 1 [13] H. Hazimeh, N. Ponomareva, P. Mol, Z. Tan, and R. Mazumder. The tree ensemble layer: Diﬀerentiabil- ity meets conditional computation. In International Conference on Machine Learning, pages 4138–4148. PMLR, 2020. 1 [14] G. E. Hinton. A practical guide to training restricted boltzmann machines. In Neural networks: Tricks of the trade, pages 599–619. Springer, 2012. 1 [15] S. Ivanov and L. Prokhorenkova. Boost then convolve: Gradient boosting meets graph neural networks. In International Conference on Learning Representations, 2020. 2 [16] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu. Lightgbm: A highly eﬃcient gradient boosting decision tree. In Advances in Neural Information Processing Systems, pages 3146–3154, 2017. 3 [17] G. Ke, Z. Xu, J. Zhang, J. Bian, and T.-Y. Liu. Deep- gbm: A deep learning framework distilled by gbdt for online prediction tasks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 384–394, 2019. 2 [18] P. Kontschieder, M. Fiterau, A. Criminisi, and S. Rota Bulo. Deep neural decision forests. In Pro- ceedings of the IEEE Conference on Computer Vision, pages 1467–1475, 2015. 1 [19] D.-H. Lee, S. Zhang, A. Fischer, and Y. Bengio. Dif- ference target propagation. In Proceedings of the Joint European Conference on Machine Learning and Knowl- edge Discovery in Databases, pages 498–515. Springer, 2015. 1 [20] C. J. Maddison, A. Mnih, and Y. W. Teh. The con- crete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. 2 [21] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Perrot, and E. Duchesnay. Scikit- learn: Machine learning in python. Journal of Machine Learning Research, 12:2825–2830, 2011. 5 [22] S. Popov, S. Morozov, and A. Babenko. Neural obliv- ious decision ensembles for deep learning on tabular data. In Proceedings of the International Conference on Learning Representations, 2020. 1 [23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533, 1986. 1 [24] Y. Shi, J. Li, and Z. Li. Gradient boosting with piece- wise linear regression trees. arXiv: Learning, 2018. 2, 5 [25] R. Tanno, K. Arulkumaran, D. Alexander, A. Cri- minisi, and A. Nori. Adaptive neural trees. In In- ternational Conference on Machine Learning, pages 6166–6175. PMLR, 2019. 1 [26] Z. Zhang and C. Jung. Gbdt-mo: Gradient-boosted decision trees for multiple outputs. IEEE Transactions on Neural Networks and Learning Systems, 2020. 10 [27] Z.-H. Zhou. Ensemble learning. Encyclopedia of Bio- metrics, pages 411–416, 2015. 2 [28] Z.-H. Zhou and J. Feng. Deep forest: towards an alternative to deep neural networks. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, pages 3553–3559, 2017. 1","libVersion":"0.3.2","langs":""}
{"path":"work/QCELLS/Generative AI talk/attachments/Graphics to Borrow-20240326223405431.webp","text":"(a) Prompt LLM to select features for demonstrations of picking and placing the red can | r{'k0’: left ee...(more | — idsscription) ; 132 centan de e o G ! ‘k3’: can xyz..., \"1 +taskdescription 1 K8 i canPin ., ' ol 4 B ' ‘k4’: table center...,, I ! 2\" v ' ‘k5’: milk bin... h L ' \\@ K2 | 1 —_ Y 1 d £\\ S 41 : ‘k6’: cereal bin..., ! anei/ o E i = o - . ‘k7’: bread bin..., ' ' ) . ‘k8’: can bin..., ' N . = v - : state S; (b) Plan mode sequence of transporting the red can : Reach -> Grasp -> Transport (c) Collect demonstrations § classifier .. : -/\\_S/’(~/o-%Q gl L Rehkhbb ¥ A7 g | c S - y (7] \\ \\ ® N % ¢C-) ' ‘ Figure 1: GLiDE framework Given a common-sense LLM that understands (a) the appropriate state abstrac- tions for a task and (b) how to solve the task via a sequence of manipulation modes in semantic space and (c) a few unsegmented human demonstrations that embody the transitions through these modes, we learn a grounding classifier that maps continuous physical states and observations to discrete semantic modes. Mode boundaries discovered by the classifier encode constraints implicit in the demonstrations that are critical for task success.","libVersion":"0.3.2","langs":"eng"}
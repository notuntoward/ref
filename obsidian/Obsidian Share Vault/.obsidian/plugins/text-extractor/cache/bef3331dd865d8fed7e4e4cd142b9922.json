{"path":"lit/sources/papers_added/papers/Greenewald15spatTempCorrKprodPCA.pdf","text":"1 Robust Kronecker Product PCA for Spatio-Temporal Covariance Estimation Kristjan Greenewald, Student Member, IEEE, and Alfred O. Hero III, Fellow, IEEE Abstract—Kronecker PCA involves the use of a space vs. time Kronecker product decomposition to estimate spatio-temporal co- variances. In this work the addition of a sparse correction factor is considered, which corresponds to a model of the covariance as a sum of Kronecker products of low (separation) rank and a sparse matrix. This sparse correction extends the diagonally corrected Kronecker PCA of [1], [2] to allow for sparse unstructured “outliers” anywhere in the covariance matrix, e.g. arising from variables or correlations that do not ﬁt the Kronecker model well, or from sources such as sensor noise or sensor failure. We introduce a robust PCA-based algorithm to estimate the covariance under this model, extending the rearranged nuclear norm penalized LS Kronecker PCA approaches of [2], [3]. An extension to Toeplitz temporal factors is also provided, producing a parameter reduction for temporally stationary measurement modeling. High dimensional MSE performance bounds are given for these extensions. Finally, the proposed extension of KronPCA is evaluated on both simulated and real data coming from yeast cell cycle experiments. This establishes the practical utility of robust Kronecker PCA in biological and other applications. I. INTRODUCTION I N this paper, we develop a method for robust estimation of spatio-temporal covariances and apply it to multivariate time series modeling and parameter estimation. The covariance for spatio-temporal processes is manifested as a multiframe covariance, i.e. as the covariance not only between variables or features in a single frame (time point), but also between variables in a set of pt nearby frames. If each frame contains ps spatial variables, then the spatio-temporal covariance at time t is described by a ptps by ptps matrix Σt = Cov [ {In} t−1 n=t−pt] , (1) where In denotes the ps variables or features of interest in the nth frame. We make the standard piecewise stationarity assumption that Σt can be approximated as unchanging over each consecutive set of pt frames. As pspt can be very large, even for moderately large ps and pt the number of degrees of freedom (pspt(pspt + 1)/2) in the covariance matrix can greatly exceed the number n of training samples available to estimate the covariance matrix. One way to handle this problem is to introduce structure and/or sparsity into the covariance matrix, thus reducing the number of parameters to be estimated. A natural non-sparse option is to introduce structure by modeling the covariance matrix Σ as the Kronecker product K. Greenewald and A. Hero III are with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA. This research was partially supported by grants from AFOSR FA8650- 07-D-1220-0006 and ARO MURI W911NF-11-1-0391. of two smaller symmetric positive deﬁnite matrices, i.e. Σ = A ⊗ B. (2) When the measurements are Gaussian with covariance of this form they are said to follow a matrix-normal distribution [4], [5], [6]. This model lends itself to coordinate decompositions [3], such as the decomposition between space (variables) vs. time (frames) natural to spatio-temporal data [3], [1]. In the spatio-temporal setting, the ps × ps B matrix is the “spatial covariance” and the pt × pt matrix A is the “time covariance,” both identiﬁable up to a multiplicative constant. An extension to the representation (2), discussed in [3], approximates the covariance matrix using a sum of Kronecker product factors Σ = ∑r i=1 Ai ⊗ Bi, (3) where r is the separation rank, Ai ∈ Rpt×pt , and Bi ∈ Rps×ps . We call this the Kronecker PCA (KronPCA) covari- ance representation. This model (with r > 1) has been used in various appli- cations, including video modeling and classiﬁcation [1], [7], network anomaly detection [2], synthetic aperture radar, and MEG/EEG covariance modeling (see [3] for references). In [8] it was shown that any covariance matrix can be represented in this form with sufﬁciently large r. This allows for more accurate approximation of the covariance when it is not in Kronecker product form but most of its energy can be accounted for by a few Kronecker components. An algorithm (Permuted Rank-penalized Least Squares (PRLS)) for ﬁtting the model (3) to a measured sample covariance matrix was introduced in [3] and was shown to have strong high di- mensional MSE performance guarantees. It should also be noted that, as contrasted to standard PCA, KronPCA accounts speciﬁcally for spatio-temporal structure, often provides a full rank covariance, and requires signiﬁcantly fewer components (Kronecker factors) for equivalent covariance approximation accuracy. Naturally, since it compresses covariance onto a more complex (Kronecker) basis than PCA’s singular vector basis, the analysis of Kron-PCA estimation performance is more complicated. The standard Kronecker PCA model does not naturally accommodate additive noise since the diagonal elements (vari- ances) must conform to the Kronecker structure of the matrix. To address this issue, in [1] we extended this KronPCA model, and the PRLS algorithm of [3], by adding a structured diagonal matrix to (3). This model is called Diagonally Loaded Kro- necker PCA (DL-KronPCA) and, although it has an additionalarXiv:1411.1352v4 [stat.ME] 29 Jul 2015 2 pspt parameters, it was shown that for ﬁxed r it performs signiﬁcantly better for inverse covariance estimation in cases where there is additive measurement noise [1]. The DL-KronPCA model [1] is the r + 1-Kronecker model Σ = (∑r i=1 Ai ⊗ Bi) + U = Θ + U, (4) where the diagonal matrix U is called the “diagonal loading matrix.” Following Pitsianis-VanLoan rearrangement of the square ptps × ptps matrix Σ to an equivalent rectangular p 2 s × p 2 t matrix [3], [9], this becomes an equivalent matrix approximation problem of ﬁnding a low rank plus diagonal approximation [1], [3]. The DL-KronPCA estimation problem was posed in [2], [1] as the rearranged nuclear norm penalized Frobenius norm optimization min Σ ∥Σ − ˆΣSCM ∥2 F + λ∥R(Θ)∥∗ (5) where the minimization is over Σ of the form (4), R(·) is the Pitsianis-VanLoan rearrangement operator deﬁned in the next section, and ∥ · ∥∗ is the nuclear norm. A weighted least squares solution to this problem is given in [1], [2]. This paper extends DL-KronPCA to the case where U in (4) is a sparse loading matrix that is not necessarily diagonal. In other words, we model the covariance as the sum of a low separation rank matrix Θ and a sparse matrix Γ: Σ = (∑r i=1 Ai ⊗ Bi) + Γ = Θ + Γ. (6) DL-KronPCA is obviously a special case of this model. The motivation behind the extension (6) is that while the KronPCA models (3) and (4) may provide a good ﬁt to most entries in Σ, there are sometimes a few variables (or correlations) that cannot be well modeled using KronPCA, due to complex non-Kronecker structured covariance patterns, e.g. sparsely correlated additive noise, sensor failure, or corruption. Thus, inclusion of a sparse term in (6) allows for a better ﬁt with lower separation rank r, thus reducing the overall number of model parameters. In addition, if the underlying distribution is heavy tailed, sparse outliers in the sample covariance will occur, which will corrupt Kronecker product estimates (3) and (4) that don’t have the ﬂexibility of absorbing them into a sparse term. This notion of adding a sparse correction term to a regularized covariance estimate is found in the Robust PCA literature, where it is used to allow for more robust and parsimonious approximation to data matrices [10], [11], [12], [13]. Robust KronPCA differs from Robust PCA in that it re- places the outer product with the Kronecker product. KronPCA and PCA are useful for signiﬁcantly different applications because the Kronecker product allows the decomposition of spatio-temporal processes into (full rank) spatio-temporally separable components, whereas PCA decomposes them into deterministic basis functions with no explicit spatio-temporal structure [3], [1], [9]. Sparse correction strategies have also been applied in the regression setting where the sparsity is applied to the ﬁrst moments instead of the second moments [14], [15]. The model (6) is called the Robust Kronecker PCA (Robust KronPCA) model, and we propose regularized least squares based estimation algorithms for ﬁtting the model. In particular, we propose a singular value thresholding (SVT) approach using the rearranged nuclear norm. However, unlike in robust PCA, the sparsity is applied to the Kronecker decomposition instead of the singular value decomposition. We derive high dimensional consistency results for the SVT-based algorithm that specify the MSE tradeoff between covariance dimension and the number of samples. Following [2], we also allow for the enforcement of a temporal block Toeplitz constraint, which corresponds to a temporally stationary covariance and results in a further reduction in the number of parameters when the process under consideration is temporally stationary and the time samples are uniformly spaced. We illustrate our proposed robust Kronecker PCA method using simulated data and a yeast cell cycle dataset. The rest of the paper is organized as follows: in Section II, we introduce our Robust KronPCA model and introduce an al- gorithm for estimating covariances described by it. Section III provides high dimensional convergence theorems. Simulations and an application to cell cycle data are presented in Section IV, and our conclusions are given in Section V. II. ROBUST KRONPCA Let X be a ps × pt matrix with entries ˜x(m, t) denoting samples of a space-time random process deﬁned over a ps- grid of spatial samples m ∈ {1, . . . , ps} and a pt-grid of time samples t ∈ {1, . . . , pt}. Let x = vec(X) denote the ptps column vector obtained by lexicographical reordering. Deﬁne the ptps ×ptps spatiotemporal covariance matrix Σ = Cov[x]. Consider the model (6) for the covariance as the sum of a low separation rank matrix Θ and a sparse matrix Γ: Σ = Θ + Γ. (7) Deﬁne M(i, j) as the i, jth ps × ps subblock of M, i.e., M(i, j) = [M ](i−1)ps+1:ips,(j−1)ps+1:jps. The invertible Pitsianis-VanLoan rearrangement operator R(·) maps ptps × ptps matrices to p 2 t × p 2 s matrices and, as deﬁned in [3], [9] sets the (i − 1)pt + jth row of R(M) equal to vec(M(i, j))T , i.e. R(M) = [ m1 . . . mp2 t ]T , (8) m(i−1)pt+j = vec(M(i, j)), i, j = 1, . . . , pt. After Pitsianis-VanLoan rearrangement the expression (7) takes the form R(Σ) = ∑r i=1 aib T i + S = L + S, (9) where ai = vec(Ai) and bi = vec(Bi). In the next section we solve this Robust Kronecker PCA problem (low rank + sparse + noise) using sparse approximation, involving a nuclear and 1-norm penalized Frobenius norm loss on the rearranged ﬁtting errors. A. Estimation Similarly to the approach of [1], [3], we ﬁt the model (7) to the sample covariance matrix ˆΣSCM = n−1 ∑n i=1(xi − x)(xi − x) T , where x is the sample mean and n is the number of samples of the space time process X. The best ﬁt 3 matrices Ai, Bi and Γ in (7) are determined by minimizing the objective function min ˆΘ, ˆΓ ∥ ˆΣSCM − ˆΘ − ˆΓ∥ 2 F + λΘ∥R( ˆΘ)∥∗ + λΓ∥ˆΓ∥1. (10) We call the norm ∥R(Θ)∥∗ the rearranged nuclear norm of Θ. The regularization parameters λΘ and λΓ control the impor- tance of separation rank deﬁciency and sparsity, respectively, where increasing either increases the amount of regularization. The objective function (10) is equivalent to the rearranged objective function min ˆL,ˆS ∥R − ˆL − ˆS∥2 F + λΘ∥ˆL∥∗ + λΓ∥ˆS∥1, (11) with R = R( ˆΣSCM ). The objective function is minimized over all p2 t × p 2 s matrices ˆR = ˆL + ˆS. The solutions ˆL and ˆS correspond to estimates of R(Θ) and R(Γ) respectively. As shown in [1], the left and right singular vectors of ˆL corre- spond to the (normalized) vectorized Ai and Bi respectively, as in (9). This nuclear norm penalized low rank matrix approxima- tion is a well-studied optimization problem [16], where it is shown to be strictly convex. Several fast solution methods are available, including the iterative SVD-based proximal gradient method on which Algorithm 1 is based [17]. If the sparse correction Γ is omitted, equivalent to setting λΓ = ∞, the resulting optimization problem can be solved directly using the SVD [3]. The minimizers ˆL, ˆS of (11) are transformed to the covariance estimate by the simple operation ˆΣ = R −1 (ˆL + ˆS ) , (12) where R −1(·) is the inverse of the permutation operator R(·). As the objective function in Equation (11) is strictly convex and is equivalent to the Robust PCA objective function of [17], Algorithm 1 converges to a unique global minimizer. Algorithm 1 is an appropriate modiﬁcation of the iterative algorithm described in [17]. It consists of alternating between two simple steps: 1) soft thresholding of the singular values of the difference between the overall estimate and the estimate of the sparse part (SVTλ(·)), and 2) soft thresholding of the entries of the difference between the overall estimate and the estimate of the low rank part (softλ(·)). The soft singular value thresholding operator is deﬁned as SVTλ(M) = U (diag(σ1, . . . , σmin(m1,m2)) − λI) + VT , (13) where Udiag(σ1, . . . , σmin(m1,m2))VT is the singular value decomposition of M ∈ Rm1×m2 and (·)+ = max(·, 0). The entrywise soft thresholding operator is given by [softλ(M)]ij = sign(Mij)(|Mij| − λ)+. (14) Algorithm 1 Proximal Gradient Robust KronPCA 1: R = R( ˆΣSCM ) 2: Initialize M, S, L, choose step sizes τk. 3: while R−1 (L + S) not converged do 4: L k = SVTτkλ′ Θ(M k−1 − S k−1) 5: S k = softτkλ′ Γ (M k−1 − L k−1) 6: M k = L k + S k − τk(L k + S k − R) 7: end while 8: ˆΣ = R −1 (L + S) 9: return ˆΣ Algorithm 2 Proximal Gradient Robust Toeplitz KronPCA 1: ˜R = PR( ˆΣSCM ) 2: Initialize M, S, L, choose step sizes τk. 3: while R−1 (PT (L + S) ) not converged do 4: L k = SVTτkλ′ Θ(M k−1 − S k−1) 5: for j ∈ I do 6: S k j+pt = softτkλ′ Γcj (M k−1 j+pt − L k−1 j+pt) 7: end for 8: M k = L k + S k − τk(L k + S k − ˜R) 9: end while 10: ˆΣ = R −1 (P T (L + S)) 11: return ˆΣ B. Block Toeplitz Structured Covariance Here we extend Algorithm 1 to incorporate a block Toeplitz constraint. Block Toeplitz constraints are relevant to station- ary processes arising in signal and image processing. For simplicity we consider the case that the covariance is block Toeplitz with respect to time, however, extensions to the cases of Toeplitz spatial structure and having Toeplitz structure simultaneously in both time and space are straightforward. The objective function (11), is to be solved with a constraint that both ˆΘ and ˆΓ are temporally block Toeplitz. For low separation rank component Θ = ∑r i=1 Ai⊗Bi, the block Toeplitz constraint is equivalent to a Toeplitz constraint on the temporal factors Ai. The Toeplitz constraint on Ai is equivalent to [18], [19] [ai]k = v(i) j+pt, ∀k ∈ K(j), j = −pt + 1, . . . , pt − 1, (15) for some vector v(i) where ai = vec(Ai) and K(j) = {k : (k − 1)pt + k + j ∈ [−pt + 1, pt − 1]}. (16) It can be shown, after some algebra, that the optimization problem (11) constrained to block Toeplitz covariances is equivalent to an unconstrained penalized least squares prob- lem involving v(i) instead of ai. Speciﬁcally, following the techniques of [18], [19], [2] with the addition of the 1- norm penalty, the constrained optimization problem (11) can be shown to be equivalent to the following unconstrained optimization problem: min ˜L,˜S ∥ ˜R − ˜L − ˜S∥2 F +λ ′ Θ∥˜L∥∗ + λ′ Γ ∑ j∈I cj∥˜Sj+pt∥1, (17) 4 where ˜Sj denotes the jth row of ˜S, ˜L = PˆL, ˜S = PS, and ˜R = PR. The summation indices I, the 1-norm weighting constants cj, and the (2pt − 1) × p2 t matrix P are deﬁned as I ={−pt + 1, . . . , pt − 1} (18) cj =1/ √pt − |j| Ppt+j,i = { 1√pt−|j| i ∈ K(j) 0 o.w. where the last line holds for all j = −pt + 1, . . . , pt − 1, i = 1, . . . , p2 s. Note that this imposition of Toeplitz structure also results in a signiﬁcant reduction in computational cost primarily due to a reduction in the size of the matrix in the singular value thresholding step [2]. The block Toeplitz estimate is given by ˆΣ = R −1 (P T (˜L + ˜S )) , (19) where ˜L, ˜S are the minimizers of (17). Similarly to the non- Toeplitz case, the block Toeplitz estimate can be computed using Algorithm 2, which is the appropriate modiﬁcation of Algorithm 1. As the objective function in Equation (11) is strictly convex and is equivalent to the Robust PCA objective function of [17], Algorithm 2 converges to a unique global minimizer. The non-Toeplitz and Toeplitz objective functions (11) and (17), respectively, are both invariant with respect to replacing Θ with ΘT and Γ with Γ T because ΣSCM is symmetric. Furthermore, ∥(M + M T )/2∥ ≤ 1 2 (∥M∥ + ∥M T ∥) = ∥M∥ (for both the weighted 1-norm and nuclear norm) by the triangle inequality. Hence the symmetric (Σ + Σ T )/2 = (Θ + ΘT )/2 + (Γ + Γ T )/2 will always result in at least as low an objective function value as would Σ. By the uniqueness of the global optimum, the Robust KronPCA covariance estimates ˆΘ and ˆΓ are therefore symmetric for both the Toeplitz and non- Toeplitz cases. III. HIGH DIMENSIONAL CONSISTENCY In this section, we impose the additional assumption that the training data xi is Gaussian with true covariance given by Σ0 = Θ0 + Γ0, (20) where Θ0 is the low separation rank covariance of interest and Γ0 is sparse. A norm Rk(·) is said to be decomposable with respect to subspaces (Mk, ¯Mk) if [13] Rk(u + v) = Rk(u) + Rk(v), ∀u ∈ Mk, v ∈ ¯M ⊥ k . (21) We deﬁne subspace pairs MQ, ¯MQ [13] with respect to which the rearranged nuclear (Q = Θ) and 1-norms (Q = Γ) are respectively decomposable [13]. These are associated with the set of either low separation rank (Q = Θ) or sparse (Q = Γ) matrices. For the sparse case, let S be the set of indices on which vec{Γ} is nonzero. Then MΓ = ¯MΓ is the subspace of vectors in Rp2 t p2 s that have support contained in S, and ¯M ⊥ Γ is the subspace of vectors orthogonal to ¯M ⊥ Γ , i.e. the subspace of vectors with support contained in Sc. For the rearranged nuclear norm, note that by [8] any pq×pq matrix Θ can be decomposed as Θ = min(p2 t ,p 2 s)∑ i=1 σiA (i) Θ ⊗ B (i) Θ (22) where for all i, ∥A(i) Θ ∥F = ∥B (i) Θ ∥F = 1, σi ≥ 0 and nonincreasing, the pt ×pt {A(i) Θ }i are all linearly independent, and the ps × ps {B (i) Θ }i are all linearly independent. It is easy to show that this decomposition can be computed by extracting and rearranging the singular value decomposition of R(Θ) [8], [3], [9] and thus the σi are uniquely determined. Let r be such that σi = 0 for all i > r. Deﬁne the matrices UA = [vec(A(1) Θ ), . . . , vec(A(r) Θ )], UB = [vec(B (1) Θ ), . . . , vec(B (r) Θ )]. Then we deﬁne a pair of subspaces with respect to which the nuclear norm is decomposable as MΘ =range(UA ⊗ UB), (23) ¯M ⊥ Θ =range(U⊥ A ⊗ U⊥ B). It can be shown that these subspaces are uniquely determined by Θ. Consider the covariance estimator that results from solving the optimization problem in Equation (11). As is typical in Robust PCA, an incoherence assumption is required to ensure that Θ and Γ are distinguisable. Our incoherence assumption is as follows: max { σmax (P ¯MΘP ¯MΓ ) , σmax (P ¯M⊥ ΘP ¯MΓ) , (24) σmax (P ¯MΘP ¯M⊥ Γ ) , σmax ( P ¯M⊥ ΘP ¯M⊥ Γ )} ≤ 16 Λ2 where Λ = 2 + max { 3β√2r λ√s , 3λ√s β√2r } , (25) P ¯MQ is the matrix corresponding to the projection operator that projects onto the subspace ¯MQ and σmax denotes the maximum singular value. By way of interpretation, note that the maximum singular value of the product of projection matrices measures the “an- gle” between the subspaces. Hence, the incoherence condition is imposing that the subspaces in which Θ and Γ live be sufﬁciently “orthogonal” to each other i.e., “incoherent.” This ensures identiﬁability in the sense that a portion of Γ (a portion of Θ) cannot be well approximated by adding a small number of additional terms to the Kronecker factors of Θ (Γ). Thus Θ cannot be sparse and Γ cannot have low separation rank. In [13] it was noted that this incoherency condition is signiﬁcantly weaker than other typically imposed approximate orthogonality conditions. Suppose that in the robust KronPCA model the n training samples are multivariate Gaussian distributed and IID, that L0 = R(Θ0) is at most rank r, and that S0 = R(Θ0) has 5 s nonzero entries (9). Choose the regularization parameters to be λΘ = k∥Σ0∥ max(α2, α), λΓ = 32ρ(Σ0) √ log ptps n , (26) where ρ(Σ) = maxj Σjj and k is smaller than an increasing function of t0 given in the proof ((42)). We deﬁne α below. Given these assumptions, we have the following bound on the estimation error (deﬁning M = max(pt, ps, n)). Theorem III.1 (Robust KronPCA). Let L0 = R(Θ0). As- sume that the incoherence assumption (24) and the assump- tions in the previous paragraph hold, and the regularization parameters λΘ and λΓ are given by Equation (26) with α = √ t0(p2 t + p2 s + log M )/n for any chosen t0 > 1. Then the Frobenius norm error of the solution to the optimization problem (11) is bounded as: ∥ˆL−L0∥F ≤ (27) 6 max { k∥Σ0∥√r max(α2, α), 32ρ(Σ0) √ s log ptps n } with probability at least 1 − c exp(−c0 log ptps), where c, c0 are constants, and c0 is dependent on t0 but is bounded from above by an absolute constant. The proof of this theorem can be found in Appendix A. Note that in practice the regularization parameters will be chosen via a method such as cross validation, so given that the parameters in (26) depend on Σ0, the speciﬁc values in (26) are less important than how they scale with pt, ps, n. Next, we derive a similar bound for the Toeplitz Robust KronPCA estimator. It is easy to show that a decomposition of Θ of the form (22) exists where all the Ai are Toeplitz. Hence the deﬁnitions of the relevant subspaces ( ¯MΓ, ¯MΘ, MΓ, MΘ) are of the same form as for the non Toeplitz case. In the Gaussian robust Toeplitz KronPCA model (17), further suppose ˜L0 = PL0 is at most rank r and ˜S0 = PS0 has at most s nonzero entries. Theorem III.2 (Toeplitz Robust KronPCA). Assume that the assumptions of Theorem III.1 hold and that PL0 is at most rank r and that PS0 has at most s non-zero entries. Let the regularization parameters λΘ and λΓ be as in (26) with α =√t0(2pt + p2 s + log M )/n for any t0 > 1. Then the Frobenius norm error of the solution to the Toeplitz Robust KronPCA optimization problem (11) with coefﬁcients given in (18) is bounded as: ∥ˆL−L0∥F ≤ (28) 6 max { k∥Σ0∥√r max(α2, α), 32ρ(Σ0) √ s log ptps n } with probability at least 1 − c exp(−c0 log ptps), where c, c0 are constants, and c0 is dependent on t0 but is bounded from above by an absolute constant. The proof of this theorem is given in Appendix A. Comparing the right hand sides of (27) and (28) in Theo- rems III.1 and III.2 we see that, as expected, prior knowledge of Toeplitz structure reduces the Frobenius norm error of the estimator ˆL from O(p 2 t ) to O(pt). IV. RESULTS A. Simulations In this section we evaluate the performance of the pro- posed robust Kronecker PCA algorithms by conducting mean squared covariance estimation error simulations. For the ﬁrst simulation, we consider a covariance that is a sum of 3 Kronecker products (pt = 10, ps = 50), with each term being a Kronecker product of two autoregressive (AR) covariances. AR processes with AR parameter a have covariances Ψ given by ψij = ca |i−j|. (29) For the p × p temporal factors Ai, we use AR parameters [0.5, 0.8, 0.05] and for the q × q spatial factors Bi we use [0.95, 0.35, 0.999]. The Kronecker terms are scaled by the constants [1, 0.5, 0.3]. These values were chosen to create a complex covariance with 3 strong Kronecker terms with widely differing structure. The result is shown in Figure 1. We ran the experiments below for 100 cases with randomized AR parameters and in every instance Robust KronPCA dominated standard KronPCA and the sample covariance estimators to an extent qualitatively identical to the case shown below. To create a covariance matrix following the non-Toeplitz “KronPCA plus sparse” model, we create a new “corrupted” covariance by taking the 3 term AR covariance and deleting a random set of row/column pairs, adding a diagonal term, and sparsely adding high correlations (whose magnitude de- pends on the distance to the diagonal) at random locations. Figure 2 shows the resulting corrupted covariance. To create a “corrupted” block Toeplitz “KronPCA plus sparse” covariance, a diagonal term and block Toeplitz sparse correlations were added to the AR covariance in the same manner as in the non-Toeplitz case. Figures 3 and 4 show results for estimating the Toeplitz corrupted covariance, and non-Toeplitz corrupted covariance respectively, using Algorithms 1 and 2. For both simulations, the average MSE of the covariance estimate is computed for a range of Gaussian training sample sizes. Average 3- steps ahead prediction MSE loss using the learned covariance to form the predictor coefﬁcients ( ˆΣyx ˆΣ† xx) is shown in Figure 5. MSE loss is the prediction MSE using the estimated covariance minus E[(y − E[y|x]) 2], which is the prediction MSE achieved using an inﬁnite number of training samples. The regularization parameter values shown are those used at n = 105 samples, the values for lower sample sizes are set proportionally using the n-dependent formulas given by (26) and Theorem III.1. The chosen values of the regularization parameters are those that achieved best average performance in the appropriate region. Note the signiﬁcant gains achieved us- ing the proposed regularization, and the effectiveness of using the regularization parameter formulas derived in the theorems. In addition, note that separation rank penalization alone does not maintain the same degree of performance improvement over the unregularized (SCM) estimate in the high sample regime, whereas the full Robust KronPCA method maintains 6 a consistent advantage (as predicted by the Theorems III.1 and III.2). Fig. 1. Uncorrupted covariance used for the MSE simulations. Subﬁgures (clockwise starting from upper left): r = 3 KronPCA covariance with AR factors Ai, Bi (3); its KronPCA and PCA spectra; and the ﬁrst spatial and temporal factors of the original covariance. Fig. 2. Corrupted version of the r = 3 KronPCA covariance (Figure 1), used to test the robustness of the proposed Robust KronPCA algorithm. Subﬁgures (Clockwise from upper left): covariance with sparse corruptions (6); its KronPCA, Robust KronPCA, and PCA spectra; and the (non-robust) estimates of the ﬁrst spatial and temporal factors of the corrupted covariance. Note that the corruption spreads the KronPCA spectrum and the signiﬁcant corruption of the ﬁrst Kronecker factor in the non-robust KronPCA estimate. B. Cell Cycle Modeling As a real data application, we consider the yeast (S. cere- visiae) metabolic cell cycle dataset used in [20]. The dataset consists of 9335 gene probes sampled approximately every 24 minutes for a total of 36 time points, and about 3 complete cell cycles [20]. 10 2 10 3 10 4 10 510 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Training SamplesMSE Toe p λ Θ = 0, λ Γ = ∞ Toe p λ Θ = 0.02, λ Γ = ∞ Toe p λ Θ = 0.02, λ Γ = 0.004 Toe p λ Θ = 0.05, λ Γ = 0.004 N on Toe p λ Θ = 0, λ Γ = ∞ N on Toe p λ Θ = 0.05, λ Γ = 0.002 Fig. 3. MSE plots for both Toeplitz Robust KronPCA (Toep) and non Toeplitz Robust KronPCA (Non Toep) estimation of the Toeplitz corrupted covariance, as a function of the number of training samples. Note the advantages of using each of Toeplitz structure, separation rank penalization, and sparsity regularization, as proposed in this paper. The regularization parameter values shown are those used at n = 105 samples, the values for lower sample sizes are set proportionally using the n-dependent formulas given by (26) and Theorems III.1 and III.2. 10 2 10 3 10 4 10 510 −2 10 −1 10 0 10 1 10 2 10 3 Training SamplesMSE λ Θ = 0, λ Γ = ∞ λ Θ = 0.02, λ Γ = ∞ λ Θ = 0.02, λ Γ = 0.002 λ Θ = 0.05, λ Γ = 0.001 Fig. 4. MSE plots for non-Toeplitz Robust KronPCA estimation of the corrupted covariance, as a function of the number of training samples. Note the advantages of using both separation rank and sparsity regularization. The regularization parameter values shown are those used at n = 105 samples, the values for lower sample sizes are set proportionally using the n-dependent formulas given by (26) and Theorem III.1. In [20], it was found that the expression levels of many genes exhibit periodic behavior in the dataset due to the periodic cell cycle. Our goal is to establish that periodicity can also be detected in the temporal component of the Kronecker spatio-temporal correlation model for the dataset. Here the spatial index is the label of the gene probe. We use pt = 36 so only one spatio-temporal training sample is available. Due to their high dimensionality, the spatial factor estimates have very low accuracy, but the ﬁrst few temporal Ai factors (36 × 36) can be effectively estimated (bootstrapping using random sets 7 10 2 10 3 10 4 10 510 −4 10 −3 10 −2 10 −1 10 0 10 1 Training SamplesMSE λ Θ = 0, λ Γ = ∞ λ Θ = 0.02, λ Γ = ∞ λ Θ = 0.05, λ Γ = 0.002 Fig. 5. 3-ahead prediction MSE loss plots using the OLS predictor with corrupted covariance estimated by non-Toeplitz Robust KronPCA. Note the advantages of using both separation rank and sparsity regularization. The regularization parameter values shown are those used at n = 105 samples, the values for lower sample sizes are set proportionally using the n- dependent formulas given by (26) and Theorem III.1. The sample covariance (λΘ = 0, λΓ = ∞) and standard KronPCA (λΘ = 0.02, λΓ = ∞) curves are cut short due to aberrant behavior in the low sample regime. of 20% genes achieved less than 3% RMS variation) due to the large number of spatial variables. We learn the spatiotem- poral covariance (both space and time factors) using Robust KronPCA and then analyze the estimated time factors (Ai) to discover periodicity. This allows us to consider the overall periodicity of the gene set, taking into account relationships between the genes, as contrasted to the univariate analysis as in [20]. The sparse correction to the covariance allows for the partial or complete removal of genes and correlations that are outliers in the sense that their temporal behavior differs from the temporal behavior of the majority of the genes. Figure 6 shows the quantiles of the empirical distribution of the entries of the sample covariance versus those of the normal distribution. The extremely heavy tails motivate the use of a sparse correction term as opposed to the purely quadratic approach of standard KronPCA. Plots of the ﬁrst row of each temporal factor estimate are shown in Figure 7. The ﬁrst three factors are shown when the entire 9335 gene dataset is used to create the sample covariance. Note that 3 cycles of strong temporal periodicity are discovered, which matches our knowledge that approximately 3 complete cell cycles are contained in the sequence. Figure 8 displays the estimates of the ﬁrst temporal factor when only a random 500 gene subset is used to compute the sample covariance. Note that the standard KronPCA estimate has much higher variability than the proposed robust KronPCA estimate, masking the presence of periodicity in the temporal factor. This is likely due to the heavy tailed nature of the distribution, and to the fact that robust KronPCA is better able to handle outliers via the sparse correction of the low Kronecker-rank component. −4 −2 0 2 4 −60 −40 −20 0 20 40 60 Standard Normal QuantilesSCM Normalized Entry Quantiles Fig. 6. Plot of quantiles of the empirical distribution of the sample covariance entries versus those of the normal distribution (QQ). Note the very heavy tails, suggesting that an 2-norm based approach will break down relative to the Robust KronPCA approach allowing for sparse corrections. 0 5 10 15 20 25 30 35 40 −0.6 −0.5 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 TimeMagnitude 1st temporal factor 2nd temporal factor 3rd temporal factor Fig. 7. Plots of the temporal covariance factors estimated from the entire cell cycle dataset. Shown are the ﬁrst rows of the ﬁrst three temporal factors (excluding the ﬁrst entry). Note the strong periodicity of the ﬁrst two. V. CONCLUSION This paper proposed a new robust method for performing Kronecker PCA of sparsely corrupted spatiotemporal covari- ances in the low sample regime. The method consists of a combination of KronPCA, a sparse correction term, and a temporally block Toeplitz constraint. To estimate the covari- ance under these models, a robust PCA based algorithm was proposed. The algorithm is based on nuclear norm penalization of a Frobenius norm objective to encourage low separation rank, and high dimensional performance guarantees were derived for the proposed algorithm. Finally, simulations and experiments with yeast cell cycle data were performed that demonstrate advantages of our methods, both relative to the sample covariance and relative to the standard (non-robust) KronPCA. 8 5 10 15 20 25 30 35 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 TimeMagnitude Nonrobust Temporal Estimate Robust Temporal Estimate Fig. 8. Plots of the ﬁrst temporal covariance factors (excluding the ﬁrst entry) estimated from the highly subsampled (spatially) cell cycle dataset using robust and standard KronPCA. Note the ability of Robust KronPCA to discover the correct periodicity. APPENDIX A DERIVATION OF HIGH DIMENSIONAL CONSISTENCY In this section we ﬁrst prove Theorem III.1 and then prove Theorem III.2, which are the bounds for the non-Toeplitz and Toeplitz cases respectively. A general theorem for decomposable regularization of this type was proven in [13]. In [13] the theorem was applied to Robust PCA directly on the sample covariance, hence when appropriate we follow a similar strategy for our proof for the Robust KronPCA case. Consider the more general M-estimation problem min (θk)k∈I L ( ∑ k∈I θk ) + ∑ k∈I λkRk(θk), (30) where L is a convex differentiable loss function, the regu- larizers Rk are norms, with regularization parameters λk ≥ 2R ∗ k(∇θk L(θ∗)). R∗ k is the dual norm of the norm Rk, ∇θ denotes the gradient with respect to θ, and θ∗ is the true parameter value. To emphasize that L depends on the observed training data X (in our case through the sample covariance), we also write L(θ; X). Let Mk be the model subspace associated with the constraints enforced by Rk [13]. Assume the following conditions are satisﬁed: 1) The loss function L is convex and differentiable. 2) Each norm Rk (k ∈ I) is decomposable with respect to the subspace pairs (Mk, ¯M ⊥ k ), where Mk ⊆ ¯Mk. 3) (Restricted Strong Convexity) For all ∆ ∈ Ωk, where Ωk is the parameter space for parameter component k, δL(∆k; θ∗) := L(θ∗ + ∆k) − L(θ∗) − ⟨∆θL(θ∗), ∆k⟩ ≥ κL∥∆k∥ 2 − gkR 2 k(∆k), (31) where κL is a “curvature” parameter, and gkR 2 k(∆k) is a “tolerance” parameter. 4) (Structural Incoherence) For all ∆k ∈ Ωk, |L(θ∗ + ∑ k∈I ∆k) + (|I| − 1)L(θ∗) − ∑ k∈I L(θ∗ + ∆k)| ≤ κL 2 ∑ k∈I ∥∆k∥ 2 + ∑ k∈I hkR 2 k(∆k). (32) Deﬁne the subspace compatibility constant as Ψk(M, ∥·∥) := supu∈M\\{0} Rk(u) ∥u∥ . Given these assumptions, the following theorem holds (Corollary 1 in [13]): Theorem A.1. Suppose that the subspace-pairs are chosen such that the true parameter values θ∗ k ∈ Mk. Then the parameter error bounds are given as: ∥ˆθ − θ∗∥ ≤ ( 3|I| 2¯κ ) max k∈I λkΨk( ¯Mk). (33) where ¯κ := κL 2 − 32¯g|I| (max k∈I λkΨk( ¯Mk) )2 , ¯g := max k 1 λk √gk + hk. We can now prove Theorem III.1. Proof of Theorem III.1: To apply Theorem A.1 to the KronPCA estimation problem, we ﬁrst check the conditions. In our objective function (11), we have a loss L(Σ; X) = ∥Σ − ˆΣSCM ∥ 2 F , which of course satisﬁes condition 1. It was shown in [13] that the nuclear norm and the 1-norm both satisfy Condition 2 with respect to MΘ, ¯MΘ and MΓ, ¯MΓ respectively. Hence we let the two Rk be the nuclear norm (RΘ(·) = ∥ · ∥∗) and the 1-norm (RΓ(·) = ∥ · ∥1) terms in (11). The restricted strong convexity condition (Condition 3) holds trivially with κL = 1 and gk = 0 [13]. It was shown in [13] that for the linear Frobenius norm mismatch term (L(Σ) = ∥Σ − ˆΣSCM ∥ 2 F ) that we use in (11), the following simpler structural incoherence condition implies Condition 4 with hk = 0: max { σmax (P ¯MΘP ¯MΓ ) , σmax ( P ¯MΘP ¯M⊥ Γ ) , (34) σmax (P ¯M⊥ Θ P ¯MΓ ) , σmax (P ¯M⊥ Θ P ¯M⊥ Γ )} ≤ 1 16Λ2 where Λ = maxk1,k2 { 2 + 3λk1 Ψk1 ( ¯Mk1 ) λk2 Ψk2 ( ¯Mk2 ) } . The subspace compatibility constants are as follows [13]: ΨΘ( ¯MΘ) = sup ∆∈ ¯MΘ\\{0} ∥∆∥∗ ∥∆∥F ≤ √2r, (35) ΨΓ( ¯MΓ) = sup ∆∈ ¯MΓ\\{0} ∥∆∥1 ∥∆∥F ≤ √s, where r is the rank of Θ and s is the number of nonzero entries in Γ. The ﬁrst follows from the fact that for all Θ ∈ ¯MΘ, rank(Θ) ≤ 2r since both the row and column spaces of Θ must be of rank r [13]. Hence, we have that Λ = 2 + max { 3β√2r λ√s , 3λ√s β√2r } . (36) 9 Finally, we need to show that both of the regularization parameters satisfy λk ≥ 2R ∗ k(∇θk L(θ∗; X)), i.e. λΘ ≥ 2R∗ Θ(∇ΘL(Θ0 + Γ0; X)) (37) λΓ ≥ 2RΓ(∇ΓL(Θ0 + Γ0; X)) with high probability. Since the 1-norm is invariant under rearrangement, the argument from [13] still holds and we have that λΓ = 32ρ(Σ0) √ log ptps n (38) satisﬁes (37) with probability at least 1 − 2 exp(−c2 log pspt). For the low rank portion, (37) will hold if [13] λΘ ≥ 4∥R( ˆΣSCM − Σ0)∥. (39) From [3] we have that for t0 ≥ f (ϵ) = 4C log(1 + 2 ϵ ) (C absolute constant given in [3]), C an absolute constant, and α ≥ 1 ∥R( ˆΣSCM − Σ0)∥ ≤ ∥Σ0∥t0 1 − 2ϵ p 2 t + p2 s + log M n (40) with probability at least 1 − 2M −t0/4C and otherwise ∥R( ˆΣSCM − Σ0)∥ ≤ ∥Σ0∥ √t0 1 − 2ϵ √ p2 t + p2 s + log M n (41) with probability at least 1 − 2M −t0/4C. Thus our choice of λθ satisﬁes (39) with high probability. To satisfy the constraints on t, we need t0 > f 2(ϵ). Clearly, ϵ can be adjusted to satisfy the constraint and k = 4/(1 − 2ϵ). (42) Recalling the sparsity probability 1 − 2 exp(−c2 log ptps), the union bound implies (37) is satisﬁed for both regularization parameters with probability at least 1 − 2 exp(−c2 log ptps) − 2 exp(−(t0/4C) log M ) ≥ 1 − c exp(−c0 log ptps) and the proof of Theorem III.1 is complete. Next, we present the proof for Theorem III.2, emphasizing only the parts that differ from the non Toeplitz proof of Theorem III.1, since much of the proof for the previous theorem carries over to the Toeplitz case. Let ∆n = R(W) (43) W = ˆΣSCM − Σ0. We require the following corollary based on an extension of a theorem in [3] to the Toeplitz case: Corollary A.2. Suppose Σ0 is a ptps × ptps covariance ma- trix, ∥Σ0∥2 is ﬁnite for all pt, ps, and let M = max(pt, ps, n). Let ϵ′ < 0.5 be ﬁxed and assume that t0 ≥ f (ϵ) and C = max(C1, C2) > 0. We have that ∥∆n∥2 ≤ ∥Σ0∥ 1 − 2ϵ′ max {t0α2, √t0α} (44) with probability at least 1 − 2M − t0 4C , where α = 2pt + p2 s + log M n . (45) The proof of this result is in Appendix B. Proof of Theorem III.2: Adjusting for the objective in (17), let the regularizers Rk be RΘ(·) = ∥ · ∥∗ and RΓ(M) = ∑ j∈I cj∥Mj+pt ∥1. Condition 1 still holds as in the general non-Toeplitz case, and Condition 2 holds because RΓ is a positively weighted sum of norms, forming a norm on the product space (which is clearly the entire space). RΓ is decomposable because the 1-norm is decomposable and the overall model subspace is the product of the model subspaces for each row. The remaining two conditions trivially remain the same from the non Toeplitz case. The subspace compatibility constant remains the same for the nuclear norm, and for the sparse case we have for all ∆ RΓ(∆) ≤ ∥∆∥1 (46) hence, the supremum under the 1-norm is greater than the supremum under the row weighted norm. Thus, the subspace compatibility constant is still less than or equal to √s, where s is now the number of nonzero entries in PR(Γ). A tighter bound is achievable if the degree of sparsity in each row is known. We now show that the regularization parameters chosen satisfy (37) with high probability. For the sparse portion, we need to ﬁnd the dual of RΓ, deﬁned as R∗ Γ(Z) = sup {⟨⟨Z, X⟩⟩|RΓ(X) ≤ 1} , (47) where ⟨⟨Z, X⟩⟩ = trace{Z T X}. Let the matrix P1 = diag{{√pt − |j|} pt−1 j=−pt+1}. Deﬁne the matrices X′ such that X′ = P −1 1 X. Then RΓ(X) = ∥X′∥1 and R ∗ Γ(Z) = sup {⟨⟨P1Z, X′⟩⟩ | ∥X′∥1 ≤ 1} (48) = ∥P1Z∥∞ since the dual of the 1-norm is the ∞-norm. From [21], (37) now takes the form λΓ ≥ 4∥P1PW∥∞ = ∥ ˜W∥∞ (49) where ˜Wj+pt,i = ∑ ℓ∈K(j) Wℓ,i. (50) Hence | ˜Wj+pt,i| ≤ (pt − |j|)∥W∥∞ (51) ∥ ˜W∥∞ ≤ pt∥W∥∞. From [21] (via the union bound), we have Pr ( ∥W∥∞ > 8ρ(Σ) √ log ptps n ) ≤ 2 exp(−c2 log(ptps)), (52) giving Pr ( ∥ ˜W∥∞ > 8ρ(Σ)pt √ log ptps n ) ≤ 2 exp(−c2 log(ptps)), (53) which demonstrates that our choice for λΓ is satisfactory with high probability. 10 As before, for the low rank portion (37) will hold if [13] λΘ ≥ 4∥PR( ˆΣSCM − Σ0)∥. (54) From Corollary A.2 we have that for t ≥ f (ϵ), C an absolute constant, and α ≥ 1 ∥PR( ˆΣSCM − Σ0)∥ ≤ ∥Σ0∥t0 1 − 2ϵ 2pt + p 2 s + log M n (55) with probability at least 1 − 2M −t0/4C and otherwise ∥PR( ˆΣSCM − Σ0)∥ ≤ ∥Σ0∥ √t0 1 − 2ϵ √ 2pt + p2 t + log M n (56) with probability at least 1 − 2M −t0/4C. Hence, in the same way as in the non-Toeplitz case we have with high probability ∥ˆ˜L−˜L0∥F ≤ (57) 6 max { k∥Σ0∥√r max(α2, α), 32ρ(Σ0) √ s log ptps n } and since L = PT ˜L the theorem follows. APPENDIX B GAUSSIAN CHAOS OPERATOR NORM BOUND We ﬁrst note the following corollary from [3]: Corollary B.1. Let x ∈ Sp2 t −1 and y ∈ Sp2 s−1. Let zi ∼ N (0, Σ0), i = 1, . . . , n be ptps dimensional iid training samples. Let ∆n = R( ˆΣSCM − Σ0) = R( 1 n ∑ i zizT i − Σ0). Then for all τ > 0, Pr(|xT ∆ny| ≥ τ ) ≤ exp ( −nτ 2/2 C1∥Σ0∥2 2 + C2∥Σ0∥2τ ) (58) where C1, C2 are absolute constants. The proof (appropriately modiﬁed from that of a similar theorem in [3]) of Corollary A.2 then proceeds as follows: Proof: Deﬁne N (Sd′−1, ϵ ′) as an ϵ′ net on Sd′−1. Choose x1 ∈ S2pt−2, y1 ∈ Sp2 s−1 such that |xT 1 P∆ny1| = ∥P∆n∥2. By deﬁnition, there exists x2 ∈ N (S2pt−2, ϵ ′), y2 ∈ N (Sp2 s−1, ϵ ′) such that ∥x1−x2∥2 ≤ ϵ′, ∥y1−y2∥2 ≤ ϵ′. Then |x T 1 P∆ny1| − |xT 2 P∆ny2| ≤ |xT 1 P∆ny1 − xT 2 P∆ny2| (59) ≤ 2ϵ′∥P∆n∥2. We then have ∥P∆n∥2(1 − 2ϵ′) (60) ≤ max {|xT 2 P∆ny2| : x2 ∈ N (S2pt−2, ϵ ′), y2 ∈ N (Sp2 s−1, ϵ ′), ∥x1 − x2∥2 ≤ ϵ′, ∥y1 − y2∥2 ≤ ϵ′} ≤ max {|xT 2 P∆ny2| : x2 ∈ N (S2pt−2, ϵ ′), y2 ∈ N (Sp2 s−1, ϵ ′) } since |xT 1 P∆ny1| = ∥P∆n∥2. Hence ∥P∆n∥2 (61) ≤ 1 1 − 2ϵ′ max x∈N (S2pt−2,ϵ′),y∈N (Sp2 s −1,ϵ′) |xT P∆ny|. From [3] card(N (Sd′−1, ϵ ′)) ≤ (1 + 2 ϵ′ )d′ (62) which allows us to use the union bound. Pr(∥P∆n∥2 > ϵ ′) (63) ≤ Pr ( max x∈N (S2pt−2,ϵ′),y∈N (Sp2 s −1,ϵ′) |xT P∆ny| ≥ ϵ(1 − 2ϵ′) ) ≤ Pr    ⋃ x∈N (S2pt−2,ϵ′),y∈N (Sp2 s −1,ϵ′) |xT P∆ny| ≥ ϵ(1 − 2ϵ′)  |  ≤ card(N (S2pt−2, ϵ ′))card(N (Sp2 s−1, ϵ ′)) × max x∈N (S2pt−2,ϵ′),y∈N (Sp2 s −1,ϵ′) Pr(|x T P∆ny| ≥ ϵ(1 − 2ϵ′)) ≤ (1 + 2 ϵ′ )2pt+p2 s × max x∈N (S2pt−2,ϵ′),y∈N (Sp2 s −1,ϵ′) Pr(|xT P∆ny| ≥ ϵ(1 − 2ϵ′)). Note that ∥x T P∥ 2 2 = ∑ j x 2 j+pt pt − |j| (pt − |j|) (64) = ∑ j x2 j = ∥x∥ 2 2 = 1, so xT P ∈ Sp2 t −1. We can thus use Corollary B.1, giving Pr(∥P∆n∥2 > ϵ ′) (65) ≤ 2 (1 + 2 ϵ′ )2pt+p2 s exp ( −nϵ2(1 − 2ϵ′) 2/2 C1∥Σ0∥2 2 + C2∥Σ0∥2ϵ(1 − 2ϵ′) ) . Two regimes emerge from this expression. The ﬁrst is where ϵ ≤ C1∥Σ0∥2 C2(1−2ϵ′) , which allows Pr(∥P∆n∥2 > ϵ) (66) ≤ 2 ( 1 + 2 ϵ′ )2pt+p2 s exp ( −nϵ 2(1 − 2ϵ′)2/2 2C1∥Σ0∥2 2 ) . Choose ϵ = √t0∥Σ0∥2 1 − 2ϵ′ √ 2pt + p2 s + log M n . (67) This gives: Pr ( ∥P∆n∥2 > √t0∥Σ0∥2 1 − 2ϵ′ √ 2pt + p2 s + log M n ) (68) ≤ 2 (1 + 2 ϵ′ )2pt+p2 s exp ( −t2(2pt + p 2 s + log M ) 4C1 ) ≤ 2 ((1 + 2 ϵ′ ) e − t0 4C1 )2pt+p2 s M −t0/(4C1) ≤ 2M −t0/(4C1). The second regime (ϵ > C1∥Σ0∥2 C2(1−2ϵ′) ) allows us to set ϵ to ϵ = t0∥Σ0∥2 1 − 2ϵ′ 2pt + p 2 s + log M n (69) 11 which gives Pr ( ∥P∆n∥2 > t0∥Σ0∥2 1 − 2ϵ′ 2pt + p2 s + log M n ) (70) ≤ 2 (1 + 2 ϵ′ )2pt+p2 s exp ( −t(2pt + p2 s + log M ) 4C2 ) ≤ 2M −t0/(4C2). Combining both regimes (noting that t0 > 1 and √t0C1/C2 > 1) completes the proof. REFERENCES [1] K. Greenewald, T. Tsiligkaridis, and A. Hero, “Kronecker sum decom- positions of space-time data,” in Proceedings of IEEE CAMSAP, 2013. [2] K. Greenewald and A. Hero, “Regularized block toeplitz covariance matrix estimation via kronecker product expansions,” in Proceedings of IEEE SSP, 2014. [3] T. Tsiligkaridis and A. Hero, “Covariance estimation in high dimensions via kronecker product expansions,” IEEE Trans. on Sig. Proc. 61(21), pp. 5347–5360, 2013. [4] P. Dutilleul, “The mle algorithm for the matrix normal distribution,” Journal of statistical computation and simulation 64(2), pp. 105–123, 1999. [5] A. P. Dawid, “Some matrix-variate distribution theory: notational con- siderations and a bayesian application,” Biometrika 68(1), pp. 265–274, 1981. [6] T. Tsiligkaridis, A. Hero, and S. Zhou, “On convergence of kronecker graphical lasso algorithms,” IEEE Trans. Signal Proc. 61(7), pp. 1743– 1755, 2013. [7] K. Greenewald and A. Hero, “Kronecker pca based spatio-temporal modeling of video for dismount classiﬁcation,” in Proceedings of SPIE, 2014. [8] C. V. Loan and N. Pitsianis, “Approximation with kronecker products,” in Linear Algebra for Large Scale and Real Time Applications, pp. 293– 314, Kluwer Publications, 1993. [9] K. Werner, M. Jansson, and P. Stoica, “On estimation of cov. matrices with kronecker product structure,” IEEE Trans. on Sig. Proc. 56(2), pp. 478–491, 2008. [10] V. Chandrasekaran, S. Sanghavi, P. Parrilo, and A. Willsky, “Sparse and low-rank matrix decompositions,” in Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on, pp. 962–967, Sept 2009. [11] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky, “Latent variable graphical model selection via convex optimization,” in Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Confer- ence on, pp. 1610–1613, IEEE, 2010. [12] E. J. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?,” Journal of the ACM (JACM) 58(3), p. 11, 2011. [13] E. Yang and P. Ravikumar, “Dirty statistical models,” in Advances in Neural Information Processing Systems, pp. 611–619, 2013. [14] Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma, “Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images,” in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 763–770, June 2010. [15] R. Otazo, E. Cand`es, and D. K. Sodickson, “Low-rank plus sparse matrix decomposition for accelerated dynamic mri with separation of back- ground and dynamic components,” Magnetic Resonance in Medicine , 2014. [16] R. Mazumder, T. Hastie, and R. Tibshirani, “Spectral regularization algorithms for learning large incomplete matrices,” Journal of Machine Learning Research 11, pp. 2287–2322, 2010. [17] B. Moore, R. Nadakutiti, and J. Fessler, “Improved robust pca using low- rank denoising with optimal singular value shrinkage,” in Proceedings of IEEE SSP, 2014. [18] J. Kamm and J. Nagy, “Opt. kronecker product approx. of block toeplitz matrices,” SIAM Journal on Matrix Analysis and App. 22(1), pp. 155– 172, 2000. [19] N. P. Pitsianis, The Kronecker product in approximation and fast transform generation. PhD thesis, Cornell University, 1997. [20] A. Deckard, R. C. Anaﬁ, J. B. Hogenesch, S. B. Haase, and J. Harer, “Design and analysis of large-scale biological rhythm studies: a com- parison of algorithms for detecting periodic signals in biological data,” Bioinformatics 29(24), pp. 3174–3180, 2013. [21] A. Agarwal, S. Negahban, M. J. Wainwright, et al., “Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions,” The Annals of Statistics 40(2), pp. 1171–1197, 2012.","libVersion":"0.3.1","langs":""}
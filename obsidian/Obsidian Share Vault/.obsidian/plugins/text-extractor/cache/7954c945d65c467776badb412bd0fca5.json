{"path":"lit/sources/Khatun21TestingPairsContinuous.pdf","text":"Journal of Computational Mathematics and Data Science 1 (2021) 100012 Contents lists available at ScienceDirect Journal of Computational Mathematics and Data Science journal homepage: www.elsevier.com/locate/jcmds Testing pairs of continuous random variables for independence: A simple heuristic âœ© Mahfuza Khatun a, Sikandar Siddiqui b,âˆ— a Jahangirnagar University, Department of Finance & Banking, Savar, Dhaka 1342, Bangladesh b Deloitte Audit Analytics GmbH, Franklinstr. 50, D-60486 Frankfurt, Germany A B S T R A C T Detection and examination of pairwise dependence patterns between continuous variables is among the central tasks in the fields of business and economic statistics. To perform this analysis, practitioners frequently resort to Pearsonâ€™s (1895) productâ€“moment correlation coefficient and the related significance tests. However, the use of such tests in isolation involves the risk of missing the nonlinear and particularly non-monotonic associations between the variables. This problem is also relevant in the cases where the dependence prevails between higher-order moments, e.g., variances, rather than means. We present a simple, computationally inexpensive heuristic by which this problem can be addressed and demonstrate its usefulness in a small number of example cases. 1. Introduction Detection and examination of pairwise dependence patterns between continuous variables is among the central tasks in the fields of business and economic statistics. In this context, practitioners frequently use Pearsonâ€™s [1] productâ€“moment correlation coefficient and the related significance tests. However, since this approach is implicitly based on the assumption that the relationship prevailing between the two variables is linear in means, the use of such tests in isolation involves the risk of missing nonlinear and particularly non-monotonic associations between variables. This problem is also found in cases where the dependence prevails between higher-order moments, e.g., variances, rather than means. For example, in financial data such as stock price time series, statistical dependence of their volatilities (within the series) cannot be detected by the measure of autocorrelation when it is applied to the original data, but they can be detected in the transformed data (which are obtained by squaring the residuals of the mean model to the original data). This gives rise to the need for procedures through which more general forms of dependence can be detected and tested for statistical significance. A considerable number of solutions to this problem have already been proposed. An early test for independence, due to Hoeffding [2] and Blum et al. [3], is based on the notion of differences between the joint cumulative distribution function and the product of the marginals. In a similar vein, the distance correlation approach by SzÃ©kely et al. [4] examines the discrepancies between the characteristic functions of the joint distribution of the variables under examination and the product of the characteristic functions of their marginal distributions. The bagged nearest neighbour method by Wang et al. [5] involves estimating the conditional âœ© Acknowledgements: The project underlying this publication was funded by the German Federal Ministry of Economics and Energy under project funding reference number 01MK21002G. We are profoundly indebted to this journalâ€™s editor and an anonymous referee for several very helpful comments. âˆ— Corresponding author. E-mail address: siddiqui@web.de (S. Siddiqui). https://doi.org/10.1016/j.jcmds.2021.100012 Received 1 April 2021; Received in revised form 8 October 2021; Accepted 8 October 2021 2772-4158/Â© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 expectation of a dependent variable Y given an explanatory variable X by combining the k-nearest neighbour regression approach (see, e.g.,[6â€“8]) with a resampling and aggregation technique known as â€˜â€˜baggingâ€™â€™ (see [9]). Genest et al. [10] introduce a nonparametric test statistic based on the mean squared deviation between the empirical copula and the product copula of the sample distribution and a bootstrap method for deriving approximate ğ‘-values. For all of the above methods, the procedures for calculating the test statistic itself and/or the related p values tend to become computationally expensive if the sample size is large: â€¢ As Mudholkar and Wilding [11] point out, the tests for independence due to Hoeffding [2] and Blum et al. [3] have been shown to be consistent against all forms of dependence, but â€˜â€˜no investigations, or tabulation of the distributions of the null distributions of the related test statistics exist in the literatureâ€™â€™. This should not be seen as an unsurmountable obstacle since it is possible to apply resampling techniques to estimate the quantiles of the unknown, true distribution of the statistic under the null hypothesis. However, applying such techniques is notoriously computer intensive and will be inconvenient if both the sample size and number of variable pairs to be tested becomes very large. â€¢ The method proposed by SzÃ©kely et al. [4] consists of calculating a weighted distance between the joint characteristic function of the distributions under investigation on the one hand and the mathematical product of the individual characteristic functions involved on the other. They propose a test where, under independence, the related statistic converges to a weighted sum of squared, independent standard normal variates in which the positive weight factors depend on the particular form of the distributions from which the sample was drawn. In many empirical applications, the true form of these distributions is not known a priori. Hence, the authors propose to generate replicates of the test statistic by conditioning on one variable and resampling a large number of random permutations of the realisations of the other variable. This procedure is computationally very demanding if both the sample size and the number of pairs to be tested are large. â€¢ The â€˜â€˜baggingâ€™â€™ technique advocated by Wang et al. [5] consists of randomly drawing a large number of quasi-samples with replacement from a subset of the original data used for model calibration, running the chosen algorithm for each of them, and aggregating the results (see, e.g., [12]). The usefulness of this procedure has been proven in many practical applications, but in this case as well, the large number of resampling rounds may render the proposed approach impractical under certain circumstances. This is even more true in view of the fact that the choice of the optimal number of neighbours in the underlying k-nearest neighbour approach also requires several trial runs of the model with different trial values of k. â€¢ Likewise, the test proposed by Genest et al. [10] uses a resampling procedure, i.e., the Wild Bootstrap introduced by Wu [13], to compute approximate ğ‘-values. Hence, the practicability concerns that apply if a great number of related tests must be performed in large samples are also relevant here. Readers interested in further details of resampling methods are referred to Crowley [14] and Beasley and Rodgers [15]. As a consequence, the question may arise whether it is possible to devise a simpler, computationally inexpensive rule of thumb by which the above problem can be addressed, at least in an approximate manner, when dealing with large samples. The purpose of this paper is to propose such a procedure (Section 2) and to provide evidence of its effectiveness in a number of different yet empirically plausible settings (Section 3). In the approach we pursue, each of the two continuous variables involved is first transformed into a discrete form. Then, a measure of the distance between the joint distribution of these two discretised variables and the mathematical product of their marginal distributions is calculated. The idea underlying this procedure was introduced earlier by Wijayatunga [16], who proposes a generalisation of Pearsonâ€™s correlation coefficient to nonlinear dependences, and in a supplementary paper by the same author [17] that presents a refined version of the original formula. Finally, a test is performed to determine whether the distance between these two calculation outcomes is large enough to reject the assumption of independence. Since it relates to the entire joint distribution of the variables involved, rather than only to dependences in the means, the approach advocated here is more flexible than correlation-based independence tests. 2. The proposed procedure 2.1. Review: testing pairs of discrete random variables for independence Alternative 1: Chi-square test based on CramÃ©râ€™s V CramÃ©râ€™s V (see [18], ch. 21) is a dependence measure for a pair of discrete random variables each of which can take a finite number of possible values. Essentially, it consists of comparing the empirically observed frequencies of all possible combinations of values with their statistically expected values under the null hypothesis of stochastic independence, calculating the sum of their squared differences and normalising the outcome so that it lies inside in the interval [0; 1]. Let {ğ‘§1,ğ‘–, z2,ğ‘–} , i = 1, . . . , N, be a sample of joint realisations of two discrete random variables ğ‘1, and ğ‘2 . The variable ğ‘1 is assumed to have ğ‘… distinct possible values that are consecutively numbered by ğ‘Ÿ = 1, â€¦ , ğ‘…. Likewise, the ğ‘† distinct possible values of ğ‘2 are numbered by ğ‘  = 1, â€¦ , ğ‘†. 2 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 In the following, ğ¼(.) stands for the indicator function that is set to 1 if the condition in the brackets is fulfilled and to 0 otherwise. Let ğ‘› (ğ‘Ÿ, .) âˆ¶= ğ‘âˆ‘ ğ‘–=1 ğ¼(ğ‘§1,ğ‘– = ğ‘Ÿ), ğ‘Ÿ = 1, â€¦ , ğ‘… (1) denote the number of times that the value ğ‘Ÿ was observed for variable ğ‘1, ğ‘› (., ğ‘ ) âˆ¶= ğ‘âˆ‘ ğ‘–=1 ğ¼(ğ‘§2,ğ‘– = ğ‘ ) ğ‘  = 1, â€¦ , ğ‘† (2) the number of times the value ğ‘  was observed for variable ğ‘2, and ğ‘› (ğ‘Ÿ, ğ‘ ) âˆ¶= ğ‘âˆ‘ ğ‘–=1 ğ¼(ğ‘§1,ğ‘– = ğ‘Ÿ)ğ¼(ğ‘§2,ğ‘– = ğ‘ ) (3) the number of times the combination of values {ğ‘Ÿ, ğ‘ } was observed for ğ‘1 and ğ‘2, respectively. Then, the original formula for CramÃ©râ€™s V reads ğ‘‰ âˆ¶= âˆš ğœ™2 min(ğ‘… âˆ’ 1; ğ‘† âˆ’ 1) with ğœ™2 âˆ¶= 1 ğ‘ ğ‘…âˆ‘ ğ‘Ÿ=1 ğ‘†âˆ‘ ğ‘ =1 ( ğ‘› (ğ‘Ÿ, ğ‘ ) âˆ’ ğ‘›(ğ‘Ÿ,.)ğ‘›(.,ğ‘ ) ğ‘ )2 ğ‘›(ğ‘Ÿ,.)ğ‘›(.,ğ‘ ) ğ‘ (4) Generally, (4) will be a biased estimator of its population counterpart and has a tendency to overestimate the strength of the association prevailing between variables ğ‘1, and ğ‘2. Hence, it has become common practice to instead use a bias-corrected version of the above statistic that was introduced by Bergsma [19]. This bias-corrected version is given by ğ‘‰ âˆ— âˆ¶= âˆš ğœ™âˆ— 2 min(ğ‘…âˆ— âˆ’ 1; ğ‘†âˆ— âˆ’ 1) (5) with ğœ™âˆ—2 âˆ¶= ğ‘šğ‘ğ‘¥ {0; ğœ™2 âˆ’ (ğ‘…âˆ’1)(ğ‘†âˆ’1) ğ‘âˆ’1 } , ğ‘…âˆ— = ğ‘… âˆ’ (ğ‘…âˆ’1)2 ğ‘âˆ’1 , and ğ‘†âˆ— = ğ‘† âˆ’ (ğ‘†âˆ’1)2 ğ‘âˆ’1 As the sample size approaches infinity, the associated test statistic ğœ’ âˆ—2 âˆ¶= ğ‘ â‹… ğœ™âˆ—2 (6) follows a chi-square distribution with (R-1)(S-1) degrees of freedom. Alternative 2: Pearsonâ€™s chi-square statistic The ğœ™2 term in Eq. (4) coincides with the familiar Pearson [20] chi-square test for independence divided by the sample size. Hence, the question arises whether systematic differences exist between the accuracy of this statistic, in its original form, and the bias-corrected version (6). See the arguments presented in Wijayatunga [16]. Alternative 3: Chi-square test based on mutual information A likelihood-based statistic that measures the degree of mutual dependence between two random variables is mutual information. While the term â€˜â€˜mutual informationâ€™â€™ was coined by Roberto M. Fano (according to Kreer [21]), the underlying concept is due to Shannon [22,23]. In the case of a bivariate discrete distribution with a probability mass function ğ‘ƒ ğ‘Ÿ(ğ‘1 = ğ‘Ÿ; ğ‘2 = ğ‘ ) = ğ‘ğ‘Ÿ,ğ‘  (7) mutual information (MI) is calculated as ğ‘€ğ¼ = ğ‘…âˆ‘ ğ‘Ÿ=1 ğ‘†âˆ‘ ğ‘ =1 ğ‘ ğ‘Ÿ,ğ‘ ğ‘™ğ‘› ( ğ‘ğ‘Ÿ,ğ‘  ğ‘ğ‘Ÿ,. ğ‘.,ğ‘  ) with ğ‘ğ‘Ÿ,. = ğ‘ƒ ğ‘Ÿ(ğ‘1 = ğ‘Ÿ) and ğ‘.,ğ‘  = ğ‘ƒ ğ‘Ÿ(ğ‘2 = ğ‘ ) (8) Using the notation introduced in Eqs. (1)â€“(3), the maximum likelihood estimates of the quantities involved can be expressed as Ì‚ğ‘ğ‘Ÿ,. = (1âˆ•ğ‘) ğ‘› (ğ‘Ÿ, .), Ì‚ğ‘.,ğ‘  = (1âˆ•ğ‘) ğ‘› (ğ‘ , .), and Ì‚ğ‘ğ‘Ÿ,ğ‘  = (1âˆ•ğ‘) ğ‘› (ğ‘Ÿ, ğ‘ ), respectively. In the null case of independence, the asymptotic distribution of the test statistic ğ¾ 2 = 2ğ‘ â‹… ğ‘…âˆ‘ ğ‘Ÿ=1 ğ‘†âˆ‘ ğ‘ =1 Ì‚ğ‘ğ‘Ÿ,ğ‘ ğ‘™ğ‘› ( Ì‚ğ‘ğ‘Ÿ,ğ‘  Ì‚ğ‘ğ‘Ÿ,. Ì‚ğ‘.,ğ‘  ) (9) follows a chi-square distribution with (R-1)(S-1) degrees of freedom. 3 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 All three test statistics discussed here are based on a measure of the distance between (a) the statistically expected frequency distribution of the variables under examination under the null hypothesis on the one hand and (b) their actually observed frequency distribution on the other hand. In the case of Alternatives 1 and 2, this measure of distance is essentially a weighted sum of the squared deviations between empirically observed frequencies and their expected values under the null. In the case of Alternative 3, the corresponding measure is the Kullbackâ€“Leibler divergence (source: [24]) between (a) and (b). A key advantage that both of these approaches have in common is that they do not require a specific pattern of dependence to prevail in the data for a false null hypothesis to be rejected. Moreover, in both cases, the distribution to which the related test statistics converge as the sample size approaches infinity is known from the statistical theory of extremum estimators (see, e.g., [25], ch. VIII). While a considerable variety of alternative test statistics can be developed on the basis of other distance measures (e.g., the sum of absolute deviations or the maximum absolute deviation between expected and observed frequencies), closed-form solutions for their asymptotic distribution will most likely be difficult or impossible to obtain. We therefore limit our discussion to the three alternatives specified above. 2.2. Testing pairs of continuous random variables for independence The basic idea underlying the approach advocated in this note is that to test a pair of continuous variables for independence, one first transforms each of them into a discrete form by creating a set of â€˜â€˜binsâ€™â€™, i.e., contiguous intervals across the entire range of the underlying variable. After grouping the data accordingly and assigning an integer group number to each of the observed realisations, the procedure sketched in Section 2.1 can then be applied to the grouped data. Let { ğ‘¥1,ğ‘–, ğ‘¥2,ğ‘–}, i = 1, . . . , N, be a sample of the joint realisations of two continuous random variables ğ‘‹1, and ğ‘‹2 . Then, it is necessary to assign each observed value of variable ğ‘‹ğ‘˜, with k = 1, 2, to one out of B consecutively numbered bins b = 1, . . . , B. A first approach in which this can be achieved is to define a new, discrete variable ğºğ‘˜ for which the sample values ğ‘”ğ‘˜,ğ‘– depend on those of ğ‘¥ğ‘˜,ğ‘– as follows: ğ‘”ğ‘˜,ğ‘– = â§ âª âª âª â¨ âª âª âª â© 1, ğ‘–ğ‘“ ğ‘¥ğ‘˜,ğ‘– â‰¤ Ì‚ğ¹ âˆ’1 ğ‘‹ğ‘˜ ( 1 ğµ ) â‹® ğ‘, ğ‘–ğ‘“ Ì‚ğ¹ âˆ’1 ğ‘‹ğ‘˜ ( ğ‘âˆ’1 ğµ ) < ğ‘¥ğ‘˜,ğ‘– â‰¤ Ì‚ğ¹ âˆ’1 ğ‘‹ğ‘˜ ( ğ‘ ğµ ) , ğ‘“ ğ‘œğ‘Ÿ ğ‘ = 2, â€¦ , ğµ âˆ’ 1 â‹® ğµ, ğ‘–ğ‘“ ğ‘¥ğ‘˜,ğ‘– > Ì‚ğ¹ âˆ’1 ğ‘‹1 ( ğµâˆ’1 ğµ ) (10) where Ì‚ğ¹ âˆ’1 ğ‘‹ğ‘˜ (ğ›¼) denotes the (ğ›¼â‹… 100%) sample quantile of variable ğ‘‹ğ‘˜. Once this step has been performed, the null hypothesis that ğ‘‹1 and ğ‘‹2 are independent can be tested by applying the chi-square test of Section 2.1. to these newly generated variables. Prior to applying the above method, it is necessary to determine the number ğµ of bins into which the range of each continuous variable under investigation is to be split. This choice is complicated by two mutually opposed sources of possible difficulties. On the one hand, overly large values of B will cause the number of data points per bin to become small, rendering the informative value of any results attained questionable. On the other hand, a choice for B that is too small carries the risk of missing important details of the dependence pattern possibly prevailing between ğ‘‹1 and ğ‘‹2 and thus of failing to reject the null hypothesis of independence in the cases where it is actually false. The statistical dependence between two random variables can take an extremely large variety of forms. These include but are by no means limited to simple linear (ğ‘‹1 = ğ›¼ + ğ‘‹2 â‹… ğ›½ + ğœ€), various nonlinear yet monotonic (e.g., ğ‘‹1 = ğ›¼ + ğ›½ â‹… ğ‘‹3 2 + ğœ€), or nonmonotonic (e.g., ğ‘‹1 = ğ‘ ğ‘–ğ‘›(ğ‘‹2) + ğœ€) relationships as well as several forms of dependence involving higher moments (e.g., ğ‘‹1 = exp(âˆ’ğ›½ â‹… ğ‘‹2 2) â‹… ğœ€). The marginal distributions of ğ‘‹1 and ğ‘‹2 also come in an almost infinite number of shapes. Therefore, it does not appear to be possible to formulate a universally valid rule for choosing B. Hence, the users of the proposed method may need to experiment with a number of trial values of this parameter and examine whether the qualitative implications of the results obtained differ materially. If they do, a graphical investigation of the relationship under investigation, e.g., by means of a scatter plot, will often be helpful. A rough guideline value for making an initial choice of ğµ may be obtained by specifying a target number ğ‘›B of observations per bin and setting ğµ â‰ˆ ğ‘šğ‘ğ‘¥ {2; (ğ‘Ÿğ‘›ğ‘‘) [ğ‘âˆ•ğ‘›ğµ] }, where (rnd) [.] means rounding the expression in square brackets to the nearest integer. Based on a rule of thumb quoted by Hogg and Tanis [26], a sample size of 25 to 30 or more is required for the large sample properties of many common test statistics to hold in good approximation. Hence, we recommend setting ğ‘›ğµ to a value of 30 or more to obtain reliable estimates. In addition, an upper limit in the form of (say) ğµ 10 or 20 could be defined to maintain parsimony. Setting B to 10 (20) will cause each of the bins into which the range of Xk (k = 1, 2) is divided to contain approximately 10% (5%) of the observations of the underlying variable. In the case of a sample size of N = 1000, this translates into ğ‘›ğµ = 100 (50), which exceeds the recommended minimum stated above by a fair margin and hence can be considered to be an intuitively plausible guess. 4 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 However, it is obvious from the above that the procedure sketched above will not work very well in small samples: if ğ‘›ğµ is set to 50, as is done in the exemplary applications presented in Section 3, even dividing the range of observations into only B = 2 bins will require a sample size of 100. Moreover, if the conditional expectation function of ğ‘‹ğ‘– given ğ‘‹ğ‘— (with ğ‘– =1, 2 and ğ‘– â‰  j) follows a strongly oscillating pattern [as in the case of ğ¸ ( ğ‘‹1| | | ğ‘‹2) = ğ‘ ğ‘–ğ‘›(ğœ‹ â‹… ğ‘‹2)], the probability of the proposed test statistic rejecting the false null hypothesis of independence may become low because the distribution of the data points within the individual buckets may have a high degree of resemblance. 2.3. Supplementary note: testing one continuous random and one discrete random variable for independence Let {ğ‘¥ğ‘–, ğ‘§ğ‘–}, i = 1, . . . , N, be a sample of joint realisations of one continuous random variable ğ‘‹, and one discrete random variable Z. Then, analogous to the procedure outlined in Section 2.2., each observation ğ‘¥ğ‘– of ğ‘‹ can be assigned to one out of B bins b = 1, . . . , B, and attributed a bin number G defined as in Eq. (10). (Here, we drop the variable index k because there is only one continuous variable under consideration). The null hypothesis that X and ğ‘ are independent can be tested by applying the chi-square test of Section 2.1. to the observed values of ğº and ğ‘. 3. Example applications with simulated data In the following, we examine the power of the proposed procedure for five exemplary statistical relationships that may prevail between two continuous random variables X 1 and X 2. Let ğœ– be a normally distributed random variable with mean zero and unit variance that is independent of X 1 and X 2, and X 2 be uniformly distributed in the interval [-1; 1]. The first data-generating process under investigation is constructed such that the null hypothesis H0 of independence is actually true. In cases (2)â€“(5), different forms of stochastic dependence between X 1 and X 2 prevail, so H0 is false. Descriptions and exemplary simulations of the five data generating processes under examination are given below: The simulation study was carried out using programme code in the GAUSSTM matrix language (see [27]). The code is provided in the Annex to this document. All else being equal, with any statistical test for independence, the probability of rejecting an incorrect null hypothesis declines with decreasing sample size while the variability of the random factor(s) (â€˜â€˜noiseâ€™â€™) in the data-generating process that is captured by the term ğœ– in the expressions above increases. However, the objective of our simulation exercise is not to examine the power of the tests under consideration under particularly unfavourable conditions (i.e., with small samples of very noisy data). Rather, its purpose is to demonstrate how an application of the procedure proposed in Section 2 can lead to the detection of (visually rather obvious) dependence patterns that would remain undiscovered by conventional test statistics, even if the sample size is large and the level of noise is limited. More specifically, we set the size of the simulated sample to ğ‘ = 250 and the number of bins to 5. For each of these cases, we generate 100 000 simulated samples and calculate the percentage of the cases in which the null hypothesis of independence, H0, is rejected at a confidence level of ğ›¼ = 95% for different types of test statistics. The more powerful a given test statistic is, the higher the percentage of rejections in cases (2)-(5), and the closer the percentage of rejections in case (1) is to 5%. In Table 1 below, we present the results. To facilitate a meaningful comparison of the proposed procedure with alternative tests for (more specific forms of) statistical dependence, we list the results obtained for the significance tests for the Pearson correlation coefficient and the rank order correlation statistics by Spearman [28] and Kendall [29]. Each of the last five rows given in the table below relates to one of the cases (1)-(5) described mathematically and graphically at the beginning of the section. In each of these cases, we performed 100 000 simulated cases of the related data generating process as described above. Upon interpreting the results given in this table, we need to distinguish between two cases: 5 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 Table 1 Rejection frequency for null hypothesis of independence for cases (1) to (5) (Confidence level ğ›¼ = 95%). Case H0 is. . . Pearson correlation Spearmanâ€™s Rho Kendallâ€™s tau ğœ’ âˆ—2 based on Cramerâ€™s V Pearsonâ€™s ğœ’ 2 statistic ğ¾ 2 based on Mutual Information (1) . . . true 4.87% 4.86% 4.82% 0.03% 4.98% 5.99% (2) . . . false 100.00% 100.00% 100.00% 99.34% >99.99% >99.99% (3) . . . false 8.05% 9.57% 10.74% 100.00% 100.00% 100.00% (4) . . . false 12.25% 10.90% 14.64% 99.97% 100.00% 100.00% (5) . . . false 0,32% 1.20% 0.24% 90.46% 99.84% 99.88% â€¢ In situations where the null hypothesis of independence is in fact true, as in case (1) above, an ideal test based on a confidence level of ğ›¼ = 0.95 will tend to reject this null hypothesis with a likelihood of (1âˆ’ğ›¼) = 5%. Therefore, the closer the actual rejection frequency in the simulation experiment below comes to this ideal level, the more reliable the test statistic. â€¢ On the other hand, for data generating processes in which the null hypothesis of independence is not true, as in cases (2)-(5), increased rejection frequency of the null hypothesis corresponds to higher reliability of the test statistic employed. It is found that under the conditions of the experiment sketched above, the ğœ’ âˆ—2 test based on CramÃ©râ€™s V rejects the null hypothesis only very rarely if it is in fact true. In the case of the ğ¾ 2 test statistic derived from the Mutual Information criterion, on the other hand, the frequency of a Type I error (rejection of the null hypothesis of independence if it is, in fact, true) exceeds the level of 5% (which should be regarded as a benchmark if the confidence level is set to 95%) by almost one percentage point. The closest match between the observed frequency of Type I error and its statistically expected value of 5.00% is observed in the case of Pearsonâ€™s ğœ’ 2 test. In the cases where the true nature of the underlying relationship is indeed linear (case (2)), all three approaches presented reject the false null hypothesis slightly less frequently than test statistics relating to Kendallâ€™s tau and the Spearman and Pearson correlation coefficients, although in the case of the ğ¾ 2 and Pearson ğœ’ 2 statistic, the difference is negligibly small. This apparent (although very slight) disadvantage is, however, countered by the far greater likelihood of the proposed procedure to detect dependence relationships if they are nonlinear (case (3)) or prevail between second-order central moments rather than means (cases (4) and (5)). In the last-mentioned case, and measured by the frequency of Type II errors (failures to reject a false null hypothesis), both the likelihood-based ğ¾ 2 statistic and Pearsonâ€™s ğœ’ 2 test outperform the ğœ’ âˆ—2 test derived from CramÃ©râ€™s V. This suggests that within the limits of the framework of our simulation exercise, the particular test with the best overall reliability is Pearsonâ€™s ğœ’ 2 statistic. 4. Conclusion Not all forms of pairwise statistical dependence between continuous random variables are monotonic or linear in means. Hence, focusing only on productâ€“moment and/or rank correlation coefficients when seeking to identify such dependences incurs the risk of ignoring some of the more complex interrelationships that may prevail in the data. Existing solutions to this problem generally require the application of computationally expensive resampling methods that may render these methods impracticable in the cases where both the sample size and the number of variable pairs to be examined are very large. The evidence presented in this paper indicates that in such cases, a simple, computationally inexpensive heuristic based on Pearsonâ€™s [20] chi-square statistic can be used to test pairs of continuous variables for independence and to identify some of the nonlinear dependence patterns that would otherwise remain undetected. Nevertheless, a word of caution is appropriate here. Given the virtually unlimited variety of possible dependence patterns prevailing between variables, there does not appear to exist a unique testing strategy, let alone a single test statistic, that always works in the most reliable manner. Therefore, the proposed procedures ought to be seen as a supplement, rather than an alternative, to the existing statistical dependence measures and related tests. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. 6 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 Annex. GAUSSTM programme code for simulation study 7 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 8 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 9 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 10 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 11 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 12 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 13 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 14 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 References [1] Pearson K. Note on regression and inheritance in the case of two parents. Proc R Soc London 1895;58:240â€“2. [2] Hoeffding W. A non-parametric test of independence. Ann Math Stat 1948;19:546â€“57. [3] Blum JR, Kiefer J, Rosenblatt M. Distribution free test of independence based on the sample distribution function. Ann Math Stat 1961;32:485â€“98. 15 M. Khatun and S. Siddiqui Journal of Computational Mathematics and Data Science 1 (2021) 100012 [4] SzÃ©kely GJ, Rizzo ML, Bakirov NK. Measuring and testing dependence by correlation of distances. Ann Statist 2007;35(6):2769â€“94. [5] Wang Y, Li X, Liu P, Weilin X, Wang J, Wang Y, Xiong M. Bagging nearest-neighbor prediction independence test: an efficient method for nonlinear dependence of two continuous variables. Nat Sci Rep 2017;7:12736, October 16. [6] Benedetti JK. On the nonparametric estimation of regression functions. J R Stat Soc Ser B Stat Methodol 1977;39:248â€“53. [7] Stone M. Cross-validatory choice and assessment of statistical predictions. J R Stat Soc Ser B Stat Methodol 1974;39:44â€“7. [8] Tukey J. Exploratory data analysis. Reading: Addison Wesley; 1977. [9] Breiman L. Bagging predictors. Mach Learn 1996;24(2):123â€“40. [10] Genest C, Neslehova JG, Murphy OA. Testing for independence in arbitrary distributions. Biometrika 2019;106(1):47â€“68. [11] Mudholkar GS, Wilding GE. On the conventional wisdom regarding two consistent tests of bivariate independence. J Statist Soc Ser D 2003;52(1):41â€“57. [12] Zoghni R. Bagging (bootstrap aggregating), overview. Medium, september 5. 2020. [13] Wu CFJ. Jackknife, bootstrap and other resampling methods in regression analysis (with discussions). Ann Statist 1986;14:1261â€“350. [14] Crowley PH. Resampling methods for computation-intensive data analysis in ecology and evolution. Annu Rev Ecol Syst 1992;23:405â€“47. [15] Beasley WH, Rodgers J. Resampling methods. In: Millsap Roger E, Maydeu-Olivares Alberto, editors. The SAGE Handbook of Quantitative Methods in Psychology. London: SAGE; 2009, p. 362â€“86. [16] Wijayatunga P. A geometric view on Pearsonâ€™s correlation coefficient and a generalization of it to non-linear dependencies. Ratio Math 2016;30:3â€“21. http://dx.doi.org/10.23755/rm.v30i1.5. [17] Wijayatunga P. Discussion on the Paper Sparse graphs using exchangeable random measures by Caron and Fox. J R Stat Soc Ser B Stat Methodol 2017;79(5). http://dx.doi.org/10.1111/rssb.12233, 1359â€“1359. [18] Cramer H. Mathematical methods of statistics. Princeton(University Press; 1946. [19] Bergsma W. A bias correction for CramÃ©râ€™s V and Tschuprowâ€™s T. J Korean Stat Soc 2013;42(3):323â€“8. [20] Pearson K. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philos Mag Ser 1900;5(50 (302)):157â€“75. [21] Kreer JG. A question of terminology. IRE Trans Inform Theory 1957;3(3):208. [22] Shannon CE. A mathematical theory of communication. Bell Syst Tech J 1948a;27(3):379â€“423. [23] Shannon CE. A mathematical theory of communication. Bell Syst Tech J 1948b;27(4):623â€“66. [24] Kullback S, Leibler RA. On information and sufficiency. Ann Math Stat 1951;22(1):79â€“86. [25] GouriÃ©roux C, Monfort A. Statistique et modÃ¨les Ã©. 1989, Paris (Economica). [26] Hogg RV, Tanis EA. Probability and statistical inference. Pearson: Upper Saddle River, NJ; 2015. [27] Aptech Systems. GAUSSâ„¢ 5.0 User Manual. Higley/AZ (in-house publication). 2002. [28] Spearman C. The proof and measurement of association between two things. Am J Psychol 1904;15(1):72â€“101. [29] Kendall M. A new measure of rank correlation. Biometrika 1938;30(1â€“2):81â€“9. 16","libVersion":"0.3.1","langs":""}
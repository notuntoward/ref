{"path":"lit/lit_notes_OLD_PARTIAL/Schreiner23GPT4ArchitectureDatasets.pdf","text":"4/2/24, 11:12 PM GPT-4 architecture, datasets, costs and more leaked chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/3MaxMax is managinAs a trained phiconsciousness, whether machinpretend to.Pro\u0000leE-UpdateAdded new information from the SemiAnalysis reportUpdate, July 11, 2023:A new report by SemiAnalysis reveals more details about OpenAI's GPT-4,concluding that \"OpenAI is keeping the architecture of GPT-4 closed not becauseof some existential risk to humanity, but because what they've built isreplicable.\" The details of the report leaked on Twitter and Pastebin, con\u0000rmingmost of the already known information shared by people like George Hotz.The key points:GPT-4's Scale: GPT-4 has ~1.8 trillion parameters across 120 layers, which isover 10 times larger than GPT-3.Mixture Of Experts (MoE): OpenAI utilizes 16 experts within their model, eachwith ~111B parameters for MLP. Two of these experts are routed per forwardpass, which contributes to keeping costs manageable.AI researchJul 11, 2023UpdateGPT-4 architecture, datasets, costs and moreleakedMidjourney prompted by THE DECODER 4/2/24, 11:12 PM GPT-4 architecture, datasets, costs and more leaked chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/3Dataset: GPT-4 is trained on ~13T tokens, including both text-based andcode-based data, with some \u0000ne-tuning data from ScaleAI and internally.Dataset Mixture: The training data included CommonCrawl & Re\u0000nedWeb,totaling 13T tokens. Speculation suggests additional sources like Twitter,Reddit, YouTube, and a large collection of textbooks.Training Cost: The training costs for GPT-4 was around $63 million, taking intoaccount the computational power required and the time of training.Inference Cost: GPT-4 costs 3 times more than the 175B parameter Davinci,due to the larger clusters required and lower utilization rates.Inference Architecture: The inference runs on a cluster of 128 GPUs, using 8-way tensor parallelism and 16-way pipeline parallelism.Vision Multi-Modal: GPT-4 includes a vision encoder for autonomous agentsto read web pages and transcribe images and videos. The architecture issimilar to Flamingo. This adds more parameters on top and it is \u0000ne-tuned withanother ~2 trillion tokens.Original article from June 28, 2023:OpenAI GPT-4 is said to be based on the Mixture of Experts architecture andhas 1.76 trillion parameters.GPT-4 is rumored to be based on eight models, each with 220 billionparameters, which are linked in the Mixture of Experts (MoE) architecture. Theidea is nearly 30 years old and has been used for large language models before,such as Google's Switch Transformer.The MoE model is a type of ensemble learning that combines different models,called \"experts,\" to make a decision. In an MoE model, a gating networkdetermines the weight of each expert's output based on the input. This allowsdifferent experts to specialize in different parts of the input space. Thisarchitecture is particularly useful for large and complex data sets, as it caneffectively partition the problem space into simpler subspaces.No statement from OpenAI, but the rumors arecredibleThe information about GPT-4 comes from George Hotz, founder of Comma.ai, anautonomous driving startup. Hotz is an AI expert who is also known for hishacking past: He was the \u0000rst to crack the iPhone and Sony's Playstation 3.Other AI experts have also commented on Hotz's Twitter feed, saying that hisinformation is very likely true.What can open-source learn from GPT-4?The architecture may have simpli\u0000ed the training of GPT-4 by allowing differentteams to work on different parts of the network. This would also explain whyOpenAI was able to develop GPT-4's multimodal capabilities independently of 4/2/24, 11:12 PM GPT-4 architecture, datasets, costs and more leaked chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/3SummaryOpenAI's GPT-4 is reportedly based on the \"Mixture of Experts\" architectureand includes 1.76 trillion parameters.This architecture combines multiple models for decision making and can beparticularly useful with large data sets.The information comes from George Hotz, an AI expert, and has gainedcredibility in the AI community.Open source developers could try to replicate this architecture and learn fromGPT-4's advances.SourcesSemiAnalysisMMax is managDECODER. Asdeals with coquestion of wreally think othe currently available product and release them separately. In the meantime,however, GPT-4 may have been merged into a smaller model to be moreef\u0000cient, speculated Soumith Chintala, one of the founders of PyTorch.Hotz also speculated that GPT-4 produces not just one output, but iteratively 16outputs that are improved with each iteration.The open-source community could now try to replicate this architecture; theideas and technology have been available for some time. However, GPT-4 mayhave shown how far the MoE architecture can go with the right training data andcomputational resources.","libVersion":"0.3.2","langs":""}
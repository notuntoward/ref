{"path":"lit/lit_sources/Gill23typesGenAItrainEval.pdf","text":"4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 1 /1 2 Table of Cont ent In this Ar ticle Generativ e AI Models and LLM Models T ypes of Generativ e AI models  How do generativ e AI models work? Best Strategies for T raining Generativ e AI Models E v aluation and monitoring metrics for generativ e AI  Conclusion of T raining and Assessing Generativ e AI and LLM Models Additional Resour ces Strategies for Generativ e AI Models Security Scaling and Go v erning AI initiativ es with ModelOps Enterprise AI T ypes of Gener ativ e AI Models and LLM Model T r aining and Ev aluation | 20 No v ember 2023D r. Ja g r eet K a u r Gill 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 2 /1 2 Generativ e AI Use Cases and Applications Empower y our business with Generativ e AI ser vices Generativ e AI Assessment and Roadmap T alk with Generativ e AI Exper ts and Consultants G e n e r a t iv e A I M o d e ls a n d L L M M o d e ls The u se of data-dri ven machine learning (ML) has enab led the possibility of a new paradigm in research and development. ML has proven an ef fective tool for uncovering the structure-activity relationships in material data. However , the paradigm shift faces challenges due to slow progress in data quality governance and the need for guidance on combining domain knowledge with data- driven ana lysis. Th ese challenges are three key issues: high dimensionality of feature space vs. small sample, model accuracy vs. us ability, and ML results vs. domain knowledge. The key to resolving the above-mentioned issues and enabling accurate mining of structure-activity relationships lies in embedding domain knowledge into models with generative ability . 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 3 /1 2 T y p e s o f G e n e r a t iv e A I m o d e ls Generative AI or foundation models are designed to generate dif ferent types of content, such as text a nd chat, images, code, video, and embeddings. Researchers can mod ify these models to fit specific domains and tackle tasks by adjusting the generative AI's learning algorithms or model structures. This sectio n provide s an overview of task-specific and general generative ai research, with a focus on the applications of generative ai models in materials research. T ask-specific GAN 1. Generative Adversarial Networks (GANs) GAN, stan ds for Generativ e Adv ersarial Network, is an adv anced deep learning ar chitectur e consists o f two essential components : a generator and a discriminator. The generator 's primar y function is to generate synthetic data that closely r esembles r eal data, while the discriminator is r esponsible for distinguishing between authentic and fabricated data. The generator enhances the authenticity of its produced data through adv ersarial training, while the discriminator eff ectiv ely determines whether the data is r eal or synthetic. F unctioning as a generativ e model, G AN is commonly emplo y ed in deep learning for generating samples to enhance data augmentation and pr e-processing techniques. Its broad application extends across v arious ﬁ elds such as image processing and biomedicine, wh er e it pro v es v aluable in producing high-quality synthetic data for r esear ch and analysis. Explore more about GAN applications and its benefits 2. Diffusion model Generativ e diffusion models can cr eate new data using the data the y wer e trained on. F or instance, when trained on an assor tment of human faces, a diffusion model can cr eate new and lif elik e faces with div erse f eatur es and expr essions, e v en if the y wer e not pr esent in the original dataset.  The funda mental idea behind diffusion models is to transform a simple and easily obtainable distribution into a mor e complex and meaningful data distribution. Th is transformation is accomplished through a series of r e v ersible operations. Once the model understands the process of transformation, it can generate new samples b y star ting from a point within the simple distribution and gradually spr eading it towar ds the desir ed complex data distribution. Discover the Intricacies of Generative AI Architecture 3. V ariational Autoencoders (V AEs) 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 4 /1 2 V AEs ar e generativ e models which combine the capabilities of autoencoders and probabilistic modeling to acquir e a compr essed r epr esentation of data. V AEs encode input data into a lower- dimensional latent space, allowing the generation of new samples b y sampling points from the acquir ed distribution. With practical applications spanning image generation, data compr ession, anomaly detection, and drug disco v er y, V AEs exhibit v ersatility across v arious domains. 4. Flow model Flow-based models ar e generativ e ai model that aims to learn the underlyin g structur e of a giv en dataset. T hese mo dels achie v e this b y understanding the probability distribution of the diff er ent v alues or e v ents within the dataset.Once the model has acquir ed this probability distribution, it is capable of generating fr esh data p oints that maintain identical statistical proper ties and characteristics to those of the initial dataset. A k e y f eatur e of ﬂ ow-based models is that the y apply a simple inv er tible transformation to the input data that can be easily r e v ersed. By star ting from a simple initial distribution, such as random noise, and apply ing the transformation in r e v erse, the model can quickly generate new samples without r equiring complex optimization. This m ak es ﬂ ow-based models computationa lly e ﬃ cient and faster than other models. General GAI (Generative AI) 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 5 /1 2 The d evelopment of big data and data representation technologies has enabled the generation of human-readable language from inpu t data patterns and structures, allowing us to achieve objectives across various environments. This goal seeks to go beyond the language generation paradigm, restricted to adapting sample distributions for tasks. 1. The Generative Pre-Trained T ransformer (GPT) Initially showcased its potential for gen erating task-specific natural language through unsupervised pre-training and fine-tuning for downstream tasks. It utilizes transformer-decoder layers for next- word prediction and coherent text ge neration. Fine-tuning is used to adap t it to a specific task based on pre-training. Let's explore more about Universal Language Model Fine-T uning (ULMfit) 2. GPT -2 It exp ands on its predecessor's mode l structure and parameters and trains on various datasets beyond ju st web text. Despite exhibitin g advanced results with zero-shot lear ning, it still falls under task-specific GAI. Let's delve deeper into How to Build LLM and Foundation Models 3. GPT -3 It is a language m odel that employs Prompt to reduce the dependence on large, supervised datasets. It uses the linguistic structur e of text probability to make predictions. The model is pre- trained on a vast am ount of text, allowing it to perform few-shot or zero-shot learning. By defining a new cue template, it swiftly adjusts to n ew scenarios, even in situations where there is limited or no labelled data. This methodology p roves advantageous for tasks that involve language comprehension and generation, as it m inimizes the amount of data needed and enhances overall performance. Recently , GPT -4, the latest model developed by OpenAI, was trained with an unprecedented scale of co mputations and data, surprisingly achieved human-like performance across almost all tasks and s ignificantly outperformed its predecessors. The introduction of GPT -4 re presents a significant leap forwa rd in the field of General Artificial Intelligence (GAI). Building upon the success of the previous G PT mod els, GPT -4 showcases remarkable advancements in its a bility to perceive and generate multimodal data, including text, images, and audio. This groundbreaking development holds grea t promise for the field of materials science research.. The formidable capabilities of GPT - 4 in multim odal gen eration and conversational interactivity of fer a promising outlook for materials science research. 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 6 /1 2 4. LLaMA from Meta Meta, formerly known as Facebook, has recently announced a new LLM in 2023. The LLM is called LLaMA, which stands for Large Langu age Model for Meta Applications and comes with 600 billion parameters. LLaMA has been trained on various data sources, including social media posts, web pages, books, news articles, and more . Its purpose is to support various Meta applications such as content m oderation, search, recomm endation, and personalization. LLaMA claims to be more ethical by incorporating human feedback, fairness, and transparency in its training. 5. PaLM 2 from Google The PaLM model h as a new version called PaLM 2, which will be released in 2023 with 400 billion parameters. It is a multimodal LLM that can process and generate text and images. It has been trained with a large-scale dataset tha t covers 100 languages and 40 visual domains, making it capable of performing cross-modal tasks such as image captioning, visual question answering, text-to-image synth esis, and more. Palm 2 generalizes to new tasks and domains without fine- tuning, thanks to its zero-shot learning capability . 6. BLOOM BLOOM generates text in 46 natural languages, dialects, and 13 programming languages. It has been train ed on enormous data, totalin g 1.6 terabytes, equivalent to 320 copies of Shakespeare's works.The model has the capability to process a total of 46 languages, which encompass French, V ietnamese, Manda rin, Indonesian, Catalan, 13 Indic languages (including Hindi), as well as 20 African languages. Although just ove r 30% of the training data was in English, the system is proficient in all mentioned languages. 7. BERT from Google: One of Google's most influential LLM s released in 2018 is BER T. BER T is an abbreviation for Bidirectional Encoder Representations from T ransformers, which contains 34 0 million parameters. BER T, which is constructed based o n the transformer framework, leverages bidirectional self- attention to acquire knowledge from ex tensive volumes of textual data. With its capabilities, BER T is proficient in executing diverse natural language tasks such as text classification, sentiment analysis, and named entity recognition.Additionally , BER T is widely used as a pre-trained model for fine-tuning specific downstream tasks and domains. Generative AI Adversarial Networks 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 7 /1 2 1. DALL·E 2 It is an AI system that can take a simple description in natural language and turn it into a realistic image or work of art. 2. StyleGAN 3 An AI system can generate photorealistic images of anything the user can imagine, from human faces to animals and cars. Furthermore, it provides a remarkable degree of personalization by allowing users to manipulate the generated images' style, shape, and pose. Diffusion Models 1. Stable Diffusion IT is a gen erative A I model that creates photorealistic images, videos, and an imations from text and image prompts. It uses dif fusion technology and latent space, which reduces processing requirements and allows it to run on desktops or laptops with GPUs. W ith transfer learning, developers can fine-tune the model with just five images to meet their needs. It was launched in 2022. 2. DALL-E 2 An in novative language model developed by OpenAI showcases its ex traordinary talent for converting textual descriptions into breathtaking images using an advanced dif fusion model. The model uses contrastive learning to recognize the dif ferences between similar images and create new ones . It has p ractical applications in design, advertising, and content creation, making it a groundbreaking example of human-centered AI. H o w d o g e n e r a t iv e A I m o d e ls w o r k ? Generativ e AI models function b y scrutinizing patterns and information with in extensiv e datasets, emplo ying this understanding to cr eate fr esh content. This process encompasses v arious stages. 1. Data gathering When training a generative AI model, the first step is clearly defining the ob jective. The objective should specify the kind of content that the model is expected to generate. A clear goal, whether 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 8 /1 2 images, text, or mu sic, is crucial. The developer can tailor the training process by defining the objective to ensure the model produces the desired output. 2. Preprocessing To create a quality Generative AI model, collect a diverse dataset that aligns with the objective. Ensure the data is preprocessed and cleaned to remove noise and errors before feeding it into the model. 3. Choose the Right Model Architecture Choosing the exemplary model architecture is a crucial step in ensuring the success of your generative AI project. V arious architectures exist, such as Ge nerative Adversarial Networks (GANs), V ariational Autoenc oders (V AEs), and T ransformers . Eac h architecture has unique advanta ges and limitations, so it is essential to carefully evaluate the objective and dataset before selecting the appropriate one. 4. Implement the Model There is a need to create the neural network, define the layers, and establish the connections between them by writing code to implement the chosen model architecture; frameworks and libraries like TensorFlow and PyTorch of fer prebuilt components and resources to simplify the implementation process. 5. T rain the Model To tra in a generative AI model, it involves sequentially introducing the train ing data to the model and refining its para meters to reduce th e dif ference between the generated output and the intended result. This training process requires considerable computational resources and time, depending on the mode l's comple xity and the dataset's size. Monitoring the model's progress and adjusting its training parameters, like learning rate and batch size, is crucial to achieving the best results. 6. Evaluate and Optimize After training a mod el, it is crucial to as sess its performance. This can be done by using appropriate metrics to measure the quality of the generated content and comparing it to the desired output. If the result s are unsatisfactory , adjusting the model's architecture, training parameters, or dataset could be necessary to optimize its performance. 7. Fine-tune and Iterate 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 9 /1 2 Developing a generative AI model is a process that requires continuous iterat ion and improvement. Once the initial res ults are evaluated, areas for improvement can be identified. By incorporating feedback from users, introducing fresh training data, and refining the training process, it is possible to enhance the model and optimize the results. Therefore, consistent improv ements are crucial in developing a high-quality generative AI model. B e s t S t r a t e g ie s f o r T r a in in g G e n e r a t iv e A I M o d e ls 1. Choose the exemplary model architecture When it comes to data generation, selecting the most appropriate model is a critical factor that can significantly impact the resulting data quality . The most used models are V ariational Autoencoders (V AEs), Generative Adversarial Networks (GANs), and autoregressive mo dels. Each of these models has advantages and disadvantages, depending on the complexity and quality of the data. V AEs are particularly useful for learning latent representations and generating smooth data. However , they may suffer from blurrin ess and mode collapse. On the other hand, GANs excel at producing sharp and realistic data, but they may be more challenging to train. Autoregressive models generate high-quality data but may be slow and memory-intensive. When selecting the most appropriate model for particular requirements, it is crucial to compare their performance, scala bility, and ef ficiency . T his allows for a well-informed decision based on the project's specific requirements and co nstraints. Therefore, carefully consid ering these factors is critical to achieving the best results in data generation. Explore the Advantages of Model-Centric AI for Businesses in 2023 2. Use transfer learning and pre-trained models One practical approach for generative tasks is the application of transfer learning and pre-trained models. T ransfer learning involves leveraging knowledge from one domain o r task to another . Pre- trained mo dels have already been trained on large, diverse datasets such as ImageNet, Wikipedia, and YouT ube. The use of pre-exis ting models and applying transfer learning can substanti ally cut down on the time and resources required for model training. Furthermore, pre-trained models can be adapted to spec ific data and tasks . For example, developers may use pre-trained models like V AE or GAN for images and GPT -3 or BER T for text to generate images or text. Better results can be achieved by fine-tuning these models with their dataset or domain. Discover the Inner W orkings of ChatGPT Model and Its Promising Future Applications 3. Use data augmentation and regularization techniques 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 1 0 /1 2 The quality of generativ e tasks can b e impro v ed through data augmentation and r egularization techniques.Data augmentation encompasses the cr eation of div erse data b y applying transformations such as cropping, ﬂ ipping, rotating, or introducing noise to the existing dataset. Conv ersely, r egularization inv olv es imposing constraints or penalties on the model to pr e v ent o v er ﬁ tting and enhance generalizatio n. These methods expand the training data 's scope and div ersity, mitigate the risk of memorization or r eplication, and enhance the generativ e model's r esilience and v ariety. T echniques such as data augmentation can be used fo r random cropping or color jitter ing for image generation. In contrast, r egularization techniques su ch as dropout, weight deca y, or spectral normalization can be used for GAN training. Discover about the Importance of Model Robustness 4. Use distributed and parallel computing A helpful strategy to enhance generat ive tasks is to use distributed and pa rallel computing. This technique involves dividing the data an d model among devices, such as GPUs, CPUs or TPUs, and coordinating their work. Distributed an d parallel computing can accelerate tr aining and enable the management of extensive, complex data and models. It also helps to reduce memory and bandwidth consump tion and scale up the generative model. For instance, distributed and parallel computing techniq ues such as data parallelism, model parallelism, pipeline parallelism, or federated learning can be used to train generative models. 5. Use efficient and adaptive algorithms Efficient and adaptable algorithms have the capability to swiftly and flexibly enhance the parameters and hyperparameters of th e generative model. These include th e learning rate, batch size, and number of epochs. These algorithms can improve the model's performance and convergence and reduce trial-and-error time. Several algorithms are available for optimizing generative models, including SGD, Adam, and AdaGrad. Additionally , Bayesian optimization, grid search, and random search algorithms are suitable for hyperparameter tuning. By leveraging these techniques, one can ef fectively fine-tu ne models to suit dif ferent data and ta sks while addressing non-convex and dynamic optimization challenges. It is recommended that these methods be 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 1 1 /1 2 employed to achieve optimal results in generative modeling. E v a lu a t io n a n d m o n it o r in g m e t r ic s f o r g e n e r a t iv e A I Language models such as OpenAI GPT -4 and Llama 2 can cause harmful outcomes if not designed carefully . The evaluation stag e helps identify and measure potential harms by establishing clear met rics and completing iterative testing. Mitigation steps, such as prompt engineering and content filters, can then be taken. AI-a ssisted metrics can be helpful in scenarios without ground truth data , helping to measure the quality and safety of the answer . Below are metrics that help to evaluate the results generated by generative models: 1. The groundedness metric assesses how well an AI model's generated answers align with user- defined context. It ensures that claims made in an AI-generated answer are substantiated by the source context, making it essential for applications where factual correctness and contextual accuracy are critical. The input requ ired for this metric includes the question, context, and generated answer , and the score range is Integer [1-5], where one is bad, and five is good. 2. The r elevanc e metric is crucial fo r evaluating an AI system's ability to generate appropr iate responses. It measures how well the m odel's responses relate to the given questions.A high level of relevance scores signifies the AI syste m's comprehension of the input and its ability to generate coherent and suitable outputs. Conversely , low relevance scores indicate that the generated responses may deviate from the topic, lack context, or be inadequate. 4 /1 3 /2 4 , 1 1 :5 2 A M T y p e s o f G e n e r a tiv e A I M o d e ls a n d L L M M o d e l T r a in in g a n d E v a lu a tio n c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 1 2 /1 2 3. Coherence is a metric that measures the ability of a language model to generate output that flows smoothly , reads naturally , and resembles human-like language. It ass esses the readability and user-friendliness of the model's generated responses in real-world a pplications. The input required to calculate this metric is a question and its corresponding generated answer . 4. The Fluency score gauges how ef fectively an AI-generated text conforms to proper grammar, syntax, an d the appropriate use of voc abulary. It is an integer score ranging from 1 to 5, with one indicating poor and five indicating good. This metric helps evaluate the linguistic correctness of the AI-generated response. It requires the question and the generated answer as input. 5. The Similarity metr ic rates the similarity between a ground truth sentence and the AI model's generated response on a scale of 1-5. It objectively assesses the performance of AI models in text generation tasks by creating sentence- level embeddings. This metric helps compare the generated text with the desired content. To use the GPT -Similarity metric, input the q uestion, ground truth answer , and generated answer . C o n c lu s io n o f T r a in in g a n d A s s e s s in g G e n e r a t iv e A I a n d L L M M o d e ls Enterprises must consider the options when incorporating and deploying foundation models for their use cases. Ea ch use case has its specific requirements, and several de cision points must be considered while deciding on the deployment options. These decision points include cost, ef fort, data privacy , intellectual property , and security . An enterprise can use one or more deployment options based on these factors. Foundation models will play a vital role in accelerating the adoption of AI in businesses. They will significantly reduce the need for labelling, making it easier for businesses to experiment with AI, build ef fic ient AI-dri ven automation and applications, and deploy AI in a broa der range of mission- critical situations. Know more about Generative Adversarial Network Architecture Explore more about How to Build a Generative AI Model for Image Synthesis Deep dive into the Introduction to Foundation Models","libVersion":"0.3.1","langs":""}
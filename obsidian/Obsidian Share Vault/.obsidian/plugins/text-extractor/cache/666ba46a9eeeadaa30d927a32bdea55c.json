{"path":"priceFrcstAEMO/attachments/arena-short-term-forecasting-funding-round-evaluation.pdf","text":"GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial i Knowledge and findings generated from the short-term self-forecasting trial Analysis and reporting Australian Renewable Energy Agency 06 October 2021 The Power of Commitment GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial ii Executive summary The Australian Renewable Energy Agency (ARENA) commissioned GHD Advisory to undertake a study to analyse and report on the knowledge and findings from a $9.4 million funded five-minute ahead self-forecasting trial program. The purpose of this study is to assess the trial participants’ ability to: • Submit their upcoming five-minute self-forecast via the Australian Energy Market Operator (AEMO) web- based Market Participant Five-minute Forecasting Application Programming Interface (MP5F API) • Provide a more accurate five-minute ahead self-forecast than AEMO’s forecast systems for wind and solar generators • Generate social and commercial benefits for wind and solar farms from investing in short-term, self- forecasting solutions. Project background ARENA was established by the Australian Government in 2012 to improve the competitiveness of renewable energy technologies and increase the supply of renewable energy through innovation that benefits Australian consumers and businesses. AEMO has managed, planned, and operated the National Electricity Market (NEM) across Queensland, New South Wales, Australian Capital Territory, South Australia, Victoria, and Tasmania since 1998. The NEM accounts for approximately 85% of Australian electricity consumption1 via an interconnected network incorporating around 40,000 km of transmission lines and cables. As the market operator, AEMO is responsible for monitoring electricity flows across the power system and maintaining power system stability. Any imbalance between demand and supply of electricity requires the market operator to intervene to maintain power system stability. A range of electricity generation resources is connected to the NEM, comprising approximately: • 76% of fossil fuel • 8% of wind • 7% of hydro • 3% grid-scale solar • 6% rooftop solar • 0.04% of battery energy storage systems However, the proportion of fossil fuel generation is reducing as investment in new capacity is overwhelmingly wind and solar, while several large coal-fired generation units will reach the end of their economic life. NEM generators are registered with AEMO as either Scheduled (S), Semi-Scheduled (SS) or Non-Scheduled (NS). The National Electricity Market Dispatch Engine (NEMDE) generates 5-minute ahead generation dispatch targets for Scheduled (typically fossil fuelled) generating units and makes allowance for forecasts of Semi- Scheduled (variable renewable energy) units. The dispatch price for that 5-minute period is determined by the highest price bid of dispatched Scheduled generating units and operating Semi-Scheduled generating units. Semi- Scheduled generating units may also be constrained in dispatch to operate at a threshold below their maximum output, if required to meet power system requirements. The 5-minute ahead forecasting systems maintained by AEMO for the NEM Semi-Scheduled generating units and integrated with NEMDE are: 1 Australian Electricity Market Analysis report to 2020 and 2030, Final Draft, CSIRO, May 2014, https://arena.gov.au/assets/2017/02/CSIRO- Electricity-market-analysis-for-IGEG.pdf GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial iii • Australian Wind Energy Forecasting System (AWEFS) for wind generation • Australian Solar Energy Forecasting System (ASEFS) for solar generation. Volatile weather conditions, topographical or other locational factors have made forecasting challenging. Inability to comply with dispatch targets or inaccurate forecasts result in diversion from the NEM frequency standards which may affect system stability, incurring frequency corrections purchased via parallel markets, termed Frequency Control Ancillary Services (FCAS) and traditionally supplied by large coal-fired generating units being sped up or slowed down. FCAS is recovered from market participants in proportion to their size and in relation to the assessed degree of causation, which is in turn directly related to the gaps between dispatch targets/forecasts and actual generating unit output. Inaccurate AWEFS and ASEFS forecasts can therefore result in high FCAS charges for variable renewable energy generators who are unable to offset these charges by participating in the FCAS markets. FCAS costs for generators have grown and may become a significant deterrent to investment and further growth in the renewable energy industry. To address the challenges, ARENA, in partnership with AEMO, selected eleven market participants from a competitive process to take part in a $9.4 million funded trial program. Each participant was to adopt and/or develop its forecasting equipment to take into consideration of weather, geographies, and machine-learning technologies to generate a five-minute ahead output forecast and submit it to AEMO’s central dispatch system. The purpose of the trial program is to demonstrate that: • The participants have the ability to submit the five-minute self-forecasting to AEMO’s web-based market participant five-minute forecasting application programming interface (MP5F API) • The participants can provide more accurate five-minute self-forecasts than that of AEMO’s forecasting system • There will be market or commercial benefits of wind and solar farms investing in short-term, self- forecasting solutions. Individual trial participants have reported their experience and GHD Advisory has prepared this report for ARENA addressing the key outcomes of the trial: • Self-forecasting in the trial being more accurate than AEMO’s forecast systems, based on the data provided • With generation being able to dispatch more accurately, it reduces the need for FCAS and thus benefiting power system operation and market participants • Self-forecasting resulting in potential savings as an individual generator that initiates more accurate forecasting may benefit from reduced FCAS charges. Project outcomes We have undertaken a thorough review of all the reports and spreadsheets provided by the market participants and concluded that the self-forecast trial has met its objectives of increased: • Reliability • Accuracy • Financial efficacy. The voluntary self-forecast program should be taken up by the remaining semi-scheduled variable renewable energy generators. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial iv Abbreviations Acronyms used throughout this document are provided in the table below. Acronyms Definition AEMO Australian Energy Market Operator AGC Automatic Generation Control API Application Programming Interface ARENA Australian Renewable Energy Agency ASEFS Australian Solar Energy Forecasting System AWEFS Australian Wind Energy Forecasting System CP Causer Pays CPF Causer Pays Factor DUID Dispatchable Unit Identifier ECM Energy Conversion Model FCAS Frequency Control Ancillary Services GOS Global Operating System GW Gigawatts IRR Internal Rate of Return LIDAR Light Detection and Ranging LNEF Lower Not Enabled Factor MAE Mean Absolute Error NER National Electricity Rules MP5F Market Participant Five-Minute Forecast MW Megawatt NEMDE National Electricity Market Dispatch Engine NEM National Electricity Market NPV Net Present Value NSCAS Network Support Control Ancillary Services NWP Numerical Weather Prediction RMSE Root Mean Squared Error GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial v Acronyms Definition RNEF Raise Not Enabled Factor RRF Ross River Farm SCADA Supervisory Control and Data Acquisition SODARS Sonic Detection and Ranging Systems SRAS System Restart Ancillary Services UIGF Unconstrained Intermittent Generation Forecast VER Variable Renewable Energy GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial vi Scope and limitations This report: has been prepared by GHD for the Australian Renewable Energy Agency and may only be used and relied on by the Australian Renewable Energy Agency for the purpose agreed between GHD and the Australian Renewable Energy Agency as set out in section 1.1 of this report. GHD otherwise disclaims responsibility to any person other than the Australian Renewable Energy Agency arising in connection with this report. GHD also excludes implied warranties and conditions, to the extent legally permissible. The services undertaken by GHD in connection with preparing this report were limited to those specifically detailed in the report and are subject to the scope limitations set out in the report. The opinions, conclusions and any recommendations in this report are based on conditions encountered and information reviewed at the date of preparation of the report. GHD has no responsibility or obligation to update this report to account for events or changes occurring subsequent to the date that the report was prepared. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial vii Contents 1. Introduction 1 1.1 Background 1 1.2 Power system security 2 1.3 Current generation forecasts 3 1.4 A case for change 3 1.5 Purpose of this report 4 2. The self-forecast trial 5 2.1 The purpose of the trial 0 3. Reliability 1 3.1 AEMO’s assessment criteria 1 3.1.1 Initial assessment 1 3.1.2 Ongoing assessment 3 3.2 Self-forecast’s initial assessment 4 3.3 Self-forecast ongoing assessment 7 4. Accuracy 9 4.1 Accuracy measurements 9 4.1.1 Measure 1: Mean absolute error 9 4.1.2 Measure 2: Percentage of time in error bands 11 4.1.3 Measure 3: Comparison with AWEFS / ASEFS 13 4.2 Factors affecting accuracy 14 5. Financial performance 19 5.1 Background 19 5.2 Calculated FCAS savings for trial participants 19 5.2.1 The cost of the trial program 19 5.2.2 Estimated net benefits of the trial program to the participants 19 5.2.3 Other factors that can affect FCAS charges 22 6. Conclusions 22 6.1 Reliability 22 6.2 Accuracy 23 6.3 Financial performance 23 6.4 Further observations 24 6.5 Recommendations 24 7. References 25 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial viii Figure index Figure 1 Total output increase, compared to previous year, 4th quarter 2020 (Source: AEMO, 2020) ............................................................................................................................................ 3 Figure 2 Initial assessment process (source: AEMO 2018) .............................................................. 2 Figure 3 Explanation for performance assessment (Source: AEMO 2018) ...................................... 3 Figure 4 Wind farm participants' reliability of submitting unsuppressed self-forecasts .................... 7 Figure 5 Solar farm participants' reliability of submitting unsuppressed self-forecasts .................... 8 Figure 6 Mean absolute error based on participants' self-forecast for wind farm generation (Dec-19 to Apr-20) .............................................................................................................................. 10 Figure 7 Mean absolute error based on participants' self-forecast for wind farm generation (May-20 to Sep-20) ............................................................................................................................. 10 Figure 8 Mean absolute error based on participants' self-forecast for solar farm generation ........ 11 Figure 9 Error bands for wind farms (Dec-19 to Apr-20) ................................................................ 12 Figure 10 Error bands for wind farms (May-20 to Sep-20) ............................................................... 12 Figure 11 Error bands for solar farms ............................................................................................... 13 Figure 12 Self-forecast outperformance of AWEFS / ASEFS in terms of MAE ................................ 14 Figure 13 Self-forecast outperformance of AWEFS / ASEFS in terms of RMSE ............................. 14 Table index Table 1 Details of wind farm trial participants .................................................................................. 0 Table 2 Details of solar farm trial participants ................................................................................. 4 Table 3 Initial assessment issues .................................................................................................... 4 Table 4 Summary of issues relating to ongoing assessment .......................................................... 8 Table 5 Net present value of cash flow for the trial program ......................................................... 20 Table 6 Assumptions for net present value calculations based on the four trial participants who submitted financial data .................................................................................................... 20 Table 7 Sensitivity analysis ............................................................................................................ 21 Appendix List of the knowledge sharing deliverables submitted by the trial participants GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial 1 1. Introduction This study has been prepared by GHD Advisory for the ARENA in partnership with AEMO. It analyses the insights and key themes from the knowledge sharing deliverables submitted by the eleven self-forecast trial participants to ARENA. This study is solely based on the list of knowledge sharing deliverables provided by ARENA which have been submitted by the participants, and GHD’s interviews with the participants. 1.1 Background The NEM is a wholesale electricity market operating across five interconnected regions: Queensland, New South Wales, (including the Australian Capital Territory), Victoria, South Australia and Tasmania. The NEM operates over 40,000 kilometres of transmission lines throughout these regions and supplies electricity to approximately nine million customers2. It is coordinated by AEMO, a registered not-for-profit public company, to ensure that Australians have access to reliable, secure, and affordable energy. AEMO was originally established as the National Electricity Market Management Company in 1998. Its roles include operating and maintaining a secure / reliable network and managing the financial operation of the NEM. In 2015 AEMO also became the operator for the power system in Western Australia (South West Interconnected System). Some of the key network security tasks AEMO engages in include monitoring the supply and demand of electricity in the immediate future, voltage and frequency control and managing planned and unplanned outages and other network contingencies. Electricity supply and demand are matched continually across the NEM to maintain a balance. The wholesale market3 operates to dispatch sufficient generation to match anticipated demand five minutes ahead at the most efficient price possible at that time. Registered electricity generating units, including scheduled (typically fossil- fuelled generators), semi-scheduled (usually wind or solar farms), and unscheduled (often solar), offer their generation units in the wholesale market. Generation is dispatched to meet demand in real time in order from the lowest bid price to the highest. However, dispatch instructions must consider technical and security constraints. There are over 470 registered participants in the NEM4. Amongst the market participants are generators, transmission network providers, distribution network providers and market customers. AEMO manages and operates the centrally coordinated NEMDE that determines the dispatch price for each NEM region every five minutes. Six five-minute dispatch intervals are currently averaged to set the spot price for each 30-minute trading interval5. Financial settlements are based on the calculated spot price. This system aims to enable the least-cost generation option to supply the market. It is worth noting that the NEM is currently in the process of testing and implementing a switch to five-minute settlement6. This will align dispatch instructions with financial settlement periods. 2 About the National Electricity Market, AEMO website, https://aemo.com.au/en/energy-systems/electricity/national-electricity-market- nem/about-the-national-electricity-market-nem. 3 Fact Sheet, The National Electricity Market, AEMO, 28 July 2020, https://aemo.com.au/-/media/files/electricity/nem/national-electricity-market- fact-sheet.pdf 4 NEM participation and exemption list: AEMO, https://aemo.com.au/-/media/files/electricity/nem/participant_information/nem-registration-and- exemption-list.xls 5 Fact Sheet: How the spot market works: AEMC, https://www.aemc.gov.au/sites/default/files/content//Five-Minute-Settlement-directions-paper- fact-sheet-FINAL.PDF. 6 AEMO: 5MS System in production: https://aemo.com.au/initiatives/major-programs/nem-five-minute-settlement-program-and-global- settlement/5ms-systems-in-production/settlements. A timetable has been set for movement to a 5-minute settlement period, thus aligning it with the length of dispatch intervals. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial 2 1.2 Power system security Apart from the wholesale market, the NEM maintains ten ancillary services, including Network Support Control Ancillary Services (NSCAS) and System Restart Ancillary Services (SRAS). Each of these markets has a particular purpose and operates under AEMO’s direction to keep the NEM within acceptable constraints. The FCAS markets are of greatest relevance to this study. When electricity demand is greater than supply, the system frequency will fall. Conversely, when demand is less than supply, the frequency will rise. FCAS markets direct generators to counter these fluctuations and to keep the NEM within acceptable operating bands such that energy supply matches energy demand. FCAS quantities are bid into the markets – mainly by non-renewable generators who operate generating units capable of providing the requisite services - and the costs are recovered from market participants – including renewable generators. FCAS is further divided into Regulation FCAS and Contingency FCAS7. Regulation FCAS applies a Causer-Pays (CP) method of cost recovery for relatively small deviations of frequency, whereas contingency FCAS covers events such as the loss of a major load or generating unit. Regulation FCAS constantly corrects frequency deviations in response to minor deviations in load or generation. Regulation FCAS is traditionally provided by coal and/or gas generation plants who are on Automatic Generation Control (AGC). The AGC system allows AEMO to monitor the power system frequency and to send signals to generators to change output in order to restore the frequency back to within the normal operating band. Contingency FCAS corrects larger deviations in frequency due to a major contingency such as the loss of a generation unit, large load or a significant transmission element. To do so, contingency services will restore the balance by, for instance: • Opening or closing turbine steam valves • Load shedding by disconnecting load elements from the electrical system • In the case of low frequency, starting a fast generator (in the case of high frequency, reducing generator output) Causer-Pays Factors (CPF), which partially determine the allocation of FCAS Regulation costs among market participants, are calculated based on the difference between each market participant’s output and its dispatch target or five-minute ahead forecast and are re-calculated every 28 days. Participants can reduce their FCAS charges by improving their forecast accuracy and closely following dispatch trajectory. As indicated above, the majority of FCAS is provided by scheduled generators (generally coal or gas fired or hydro plant), although fast frequency response in the NEM is increasingly provided by battery energy storage systems. Meanwhile the intermittent output of semi-scheduled wind and solar farms, due to their dependence on variable wind speeds or solar radiation intensity, can play a significant role in frequency fluctuations as the proportion of these types of generation increases. Self-forecasting is therefore a key consideration within the FCAS Regulation market. Generators are assumed to follow a linear trajectory from one dispatch target to the next. Should power output deviate from this trajectory, FCAS may need to be dispatched to counter grid frequency rise or fall to ensure stability. Market participants are expected to cover the costs of the required FCAS use. This compensation is calculated based on the MPF reflecting the portion of the total Regulation FCAS that the generator is responsible for. Further details of FCAS market and its cost recovery are provided in Appendix A-1. 7 https://www.aemo.com.au/-/media/files/electricity/nem/security_and_reliability/ancillary_services/guide-to-ancillary-services-in-the-national- electricity-market.pdf GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial 3 1.3 Current generation forecasts Under the National Electricity Rules (NER), Clause 3.7B, AEMO provides Unconstrained Intermittent Generation Forecast (UIGF) for all timeframes, assuming that there is no network limitation applied. The purpose of the UIGF is to determine the dispatch instructions for semi-scheduled wind and / or solar generating units. AEMO uses two forecasting systems for wind and solar generation systems, namely: • The Australian Wind Energy Forecasting System (AWEFS) • The Australian Solar Energy Forecasting System (ASEFS). These systems use a variety of weather forecasting and persistence, including: • Supervisory Control and Data Acquisition (SCADA) measurements from the power station, including measurements of generation and local weather • Numerical weather prediction data from multiple weather data providers • Standing data from the power station as defined in the Energy Conversion Model (ECM) • Additional information provided by the power station, such as expected outages inverters / turbines under maintenance and upper MW limit on facility. Forecasting renewable energy generation is challenging because of the unpredictability of weather, topographic and locational conditions. As the number of renewable energy generators in the NEM rises, the uncertainty of forecasts becomes more significant. This can have adverse impacts on power system stability and may lead to high FCAS charges for market participants, which is of greater concern to renewable generators who are generally not able to offset these costs by participating in the provision of FCAS services. 1.4 A case for change The total Variable Renewable Energy (VRE) generation from both wind and solar farms across the NEM reached a record high of 26 terawatt hours (TWh) in the fourth quarter of 2020, representing 14% of the total NEM generation8. As shown in Figure 1, amongst the interconnected NEM region, Victoria had the highest output (MWh) increase with a total of approximately 230MWh increase. A 225 MWh output increase in Victoria was due to wind generation. Figure 1 Total output increase, compared to previous year, 4th quarter 2020 (Source: AEMO, 2020) 8 Quarterly Energy Dynamics Q4 2020, Market Insights and WA Market Operations, AEMO, https://aemo.com.au/-/media/files/major- publications/qed/2020/qed-q4-2020.pdf?la=en GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial 4 This trend is forecasted to continue, as 63% of coal-fired generation is scheduled to be retired and replaced by more than 26 GW VRE over the next two decades9. The ability to forecast available generation is critical to maintain the power system balance. Forecasts for VRE generation rely on unpredictable weather conditions, time of day, as well as specific geographic location conditions. For this reason, as VRE generation increases in the NEM, it is expected that the power system will become less stable, and more FCAS will be necessary. This is illustrated in recent FCAS cost increases. FCAS costs are shared amongst the registered NEM participants. FCAS costs have significantly increased in recent time. In the fourth quarter 2020 alone, the costs were reported to have increased to $50 million, a $15 million increase (30%) compared to the previous quarter, as reported in AEMO’s Quarterly Dynamic Report, 4th quarter 202010. Under the current method of VRE forecasting, where AEMO applies its own forecasts to determine generator’s dispatch instructions, generators are responsible for contributions to CPF when they are unable to meet targets determined by AEMO forecast. This misalignment of interests can potentially be eliminated, should generators produce their own self-forecasts in advance of each dispatch interval and report this to AEMO. To examine the feasibility of rolling out self-forecasting capabilities to all generators, ARENA awarded $9.41 million to eleven participants for a trial. These participants are large scale wind and solar farms, each with the capability to provide five-minute self-forecasts. The trial is in partnership with AEMO, whose Operational Forecasting team has been assisting the forecast submission and assessment process. The forecasts that participants submit have been used in AEMO’s central dispatch system. The purpose of the trial was to investigate: • The accuracy of the forecasting technologies used by the trial participants and the effect different weather, operational conditions and geographies have on the accuracy of forecasts amongst the participants • The participants’ ability to submit the forecast via AEMO’s Market Participant Five-Minute Forecasting Application Programming Interface (MP5F API) • Whether the participants’ self-forecast is more accurate than AWEFS / ASEFS • Commercial benefits of wind and solar farms investing in self-forecasting solutions. 1.5 Purpose of this report The purpose of the study is to investigate if there are benefits to rolling out the five-minute self-forecast to the entire wind and solar energy generation industry, based on the trial. The study focuses on: • The trial participants’ ability to submit their five-minute ahead self-forecasts on time to AEMO’s Market Participant Five-Minute Forecasting Application Programming Interface (MP5F API) • The accuracy of the self-forecast compared to AEMO’s forecast, and how the self-forecast improves the dispatch accuracy and pricing signals to the market • With the improved regulation of the power system, it reduces the need for FCAS to re-balance the frequency, and thereby decreases CP charges. A key issue whether investing in self-forecasting capabilities has a positive return on investment for generators. 9 AEMO’s 20-year development plan for the National Electricity Market, https://aemo.com.au/newsroom/media-release/isp-2020 10 Quarterly Dynamic Report, 4th Quarter 2020, AEMO, p23 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self- forecasting trial 5 2. The self-forecast trial The self-forecast trial was a partnership between ARENA and AEMO, involving eleven participants, commencing in March 2019. Many of the participants finished their trials in March 2021 with Aeolius Wind Systems and Meridian Energy’s projects continuing into 2022. The participants include: • Advisian Pty Ltd • Aeolius Wind Systems Pty Ltd • DNV GL Pty Ltd • Fulcrum3D Pty Ltd (Wind) • Fulcrum3D Pty Ltd (Solar) • Industrial Monitoring and Control Pty Ltd • Meridian Energy Australia Pty Ltd • Proa Analytics Pty Ltd • Solar and Storage Modelling Pty Ltd • Vestas Australian Wind Technology Pty Ltd • WindLab Limited. The trial participants developed forecasts for a number of wind and solar farms (most trial participants were attached to more than one generator and some generators hosted more than one participant) located in New South Wales, Victoria, Queensland, and South Australia. Each participant installed a combination of technologies and/or developed automated systems to generate forecasts at the required 5-minute resolution (the length of a dispatch interval). The choice of technologies and forecasting methods took into consideration weather, operational conditions and specific geographic aspects, and included: • Onsite sky cameras • Wind speed radars, SODARs and LIDARs • Weather satellites • Meteorological masts • Infrared skycam • Machine learning algorithms which are based on both onsite and Bureau of Meteorology weather data. A complete list of trial participants and technologies adopted is provided in Table1 and Table 2 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 0 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Table 1 Details of wind farm trial participants 11 Wind farm trial participants Participants Wind Farm Technologies Forecast approach Forecast technologies Participant submitting unconstrained forecasts to AEMO Advisian Waterloo Wind Farm (131 MW) – loT Edge Gateway Device integrated with loT Edge Computing devices. – Forecast will be developed from a predictive model- trained using machine learning techniques based on input variables such as time series wind speed and direction and positional information of the wind turbines. – Use spatial temporal data such as wind speed with historical power generation from either wind or solar farms within the machine learning models. No Aeolius Wind Systems Macarthur Windfarm – Dual Doppler Laser Radar System Light Detection and Ranging (LIDAR). – Install two separate lidar units (Erbium Fibre Laser and Erbium YAG Laser) at different locations in the windfarm. – Neural Network Model driven by existing turbine SCADA data. – Each LIDAR measures the radial wind within its field of view. – Data forwarded in real time to a central computer which will derive a 3-D vector wind field upstream of the wind farm. – Adopted an integrated Dual Doppler forecasting system at sites. – A computer model that will identify flow structures (gusts, lulls and turbulence) forward in time and space towards wind turbines. – The predicted wind speed at the hub height of each wind turbine is then calculated and converted to power output, using the coefficients for the model. – Provide a minimum of 5-minutes advanced notice of changes in power output by measuring the three-dimensional wind field up to twenty kilometres upstream of from wind farms. Project still ongoing 11 These details summarised information provided by the trial participants and published on ARENA’s website. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 1 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Wind farm trial participants DNV GL Ararat Wind Farm – Using live site data operated on existing data filter models. Forecasts developed using DNV standard forecasting models, sophisticated machine learning models and existing sources of historical and live site data, with outputs adjusted to reduce Causer Pays charges. – Multi-model forecasting. – Machine-learning forecasting model – Causer Pays charges model and adjustment algorithm No Fulcrum3D (Wind) Clement Gap (57 MW) Crowlands (80 MW) Taralga (107 MW), owned by Pacific Hydro – Data logger and logging software. Four Sonic Detection and Ranging Systems (SODARS) and /or masts. – Wind Farm power forecasting using power models and real-time power station and wind data. – Develop a detailed turbine-agnostic, site-specific real-time short term forecasting models (short term forecasts of UIGF that are not impacted by the turbine technology utilised) by integrating data including turbine state, available power, actual power, wind speed and direction. – The software model will take real-time, time-stamped data feeds from: (i) turbines this will include data such as wind speed, wind direction, turbine state, wind farm total generation capacity; and (ii) other power station equipment (inverters etc). Yes Meridian Energy Australia Pty Ltd Mt Mercer Wind Farm (131 MW), Vic Mt Millar Wind Farm (70 MW), SA – Use existing measurement devices, located on each wind turbine and the two existing meteorological masts, including anemometers, in a data-driven, machine learning approach to improve the persistence-based forecasting system This modelling, together with turbine anemometers, LIDAR measurements and data- streams from the Bureau of Meteorology, will be used to deliver a forecasting system for Mt Mercer Wind Farm that is based on the physical features of the wind as well as robust statistical methods. Project still ongoing GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 2 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Wind farm trial participants – Install an extensive array of measurement devices at Mt Mercer Wind Farm, including a state-of-the-art LIDAR, temperature sensors at different heights, rain gauges, and an array of new anemometers. Vestas Australian Wind Technology Pty Ltd Lake Bonney 2&3 – Install an 80m mast with meteorological instrumentation and data logger at Lake Bonney 2 (Met Mast B at opposite end of site to Met Mast A) (159MW). – Install a data logger at existing Lake Bonney 2 Met Mast A. – Install a cyber security upgrade hardware and software package at the SCADA server. – Set up a local High Performance Computing Cluster hosted on the Amazon Web Services cloud. – Utilise four separate models (implemented at different stages Infigen_Lake Bonney_2/3 A Turbine SCADA data Infigen_Lake Bonney_2/3 B Turbine SCADA data + Met Mast Infigen_Lake Bonney_2/3 C Turbine SCADA data + Weather Model Infigen_Lake Bonney_2/3 D Turbine SCADA data + Met Mast + Weather Model – Nostradamus is the weather forecasting engine that will be used as part of the Project. – Use a Utopus Insights forecasting tool, HyperCast. This combines weather prediction, sensor data and analytics to provide ongoing five-minute ahead maximum UIGF for a NEM registered semi-scheduled wind farm (Five-minute Forecasts). Yes GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 3 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Wind farm trial participants throughout the project) to assess the incremental improvements that met masts and weather models have on forecast accuracy when combined with high resolution (1-sec) turbine SCADA data. WindLab Ltd Kiata Wind Farm Kennedy Energy Park – Install LIDAR, power supply and Wi-Fi communications hardware. – Apply machine learning algorithms, coupled with custom hardware, to generate and optimise short term forecasting at two separate wind farms. – Establish baseline accuracy of persistence and machine learning forecast methodologies algorithms. – Upgrade previously established machine learning forecast methodologies based on upstream LIDAR measurements. – Optimise machine learning algorithms at both the Kiata Wind Farm and the No GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 4 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Wind farm trial participants Kennedy Energy Park. Table 2 Details of solar farm trial participants 12 Solar farm trial participants Participants Solar Farm Technologies Forecast approach Forecast technologies Participant submitting unconstrained forecasts to AEMO Advisian Ross River (148 MW) – loT Edge Gateway Device integrated with loT Edge Computing devices. – The power generation forecast will be developed from a predictive model trained using machine learning techniques based on time series input variables such cloud position, solar irradiance, wind speed and direction. – Use spatial temporal data such as solar irradiance with historical power generation from solar farms within the machine learning models. No Fulcrum3D (Solar) Kidston Solar Farm (50 MW) owned by Genex Power Ltd – Install a minimum of nine ground- based sky imaging devices (CloudCAMs), as well as logging and local processing equipment, throughout the Kidston Solar Farm – Use multiple CloudCAMs and integrate them on a single site to allow forecasting over a solar farm of this size. The CloudCAMs and other solar farm datasets will be integrated to provide ongoing five-minute ahead maximum UIGF for the NEM registered semi- – Develop a physical power model of the solar farm and incorporate data from the CloudCAMs and the existing SCADA system into to calculate the forecast. Yes 12 These details summarise information provided by the trial participants and published on ARENA’s website. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 5 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Solar farm trial participants scheduled solar farm (Five-minute Forecasts). Industrial Monitoring & Control Pty Ltd – The installation and integration of all hardware at each of the five solar sites, comprising dual/stereo sky cameras (The Cloud180CAM or Skycam) and associated processing and networking hardware. – Use fourteen real- time solar power forecasting models for five solar farms. – Combined all the individual models to improve overall predictability capability. – Reimplement a Causer Pays procedure to forecast FCAS savings. Use an ensemble of forecasting models: – A cloud motion vector-based model from UNSW. – An advanced statistical autoregression model (CARDS) from UniSA. – A combined Numerical Weather Prediction model from CSIRO Oceans and Atmosphere – A stereographic camera forecast model from CSIRO. – A detailed physical-based power conversion model. Yes Proa Analytics Kidston Solar Farm Oakey Solar Farm Bannerton Solar Farm Adopting Proa forecasting system – Optimal use of a combination of four different technologies – satellite cloud motion vectoring (CMV), skycam CMV, live data and numerical weather prediction (NWP) models. Yes Solar and Storage Modelling Pty Ltd (Solcast) Moree Solar Farm, Lilyvale Solar Farm, Beryl Solar Farm, Gannawarra Solar Farm, Hamilton Solar Farm, Whitsunday Solar Farm, Numurkah Solar Farm, and Coleambally Solar Farm – Installation of sky imagers – Configuration of participating solar farms in the Solcast forecasting system, and tuning (machine learning) improvement of the solar farm – Utilise a combination of satellite, weather model and sky imager data to generate and provide ongoing five- minute ahead maximum UIGF for the NEM registered semi-scheduled solar farm (Five-minute Forecasts). – Utilise Solcast’s Super Rapid Update short-term forecasting solution involving the use of low cost and low complexity of hardware for the forecast. Yes GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 6 This document is in draft form. The contents, including any opinions, conclusions or recommendations contained in, or which may be implied from, this draft document must not be relied upon. GHD reserves the right, at any time, without notice, to modify or retract any part or all of the draft document. To the maximum extent permitted by law, GHD disclaims any responsibility or liability arising from or in connection with this draft document. Solar farm trial participants models using historical power output data. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 0 2.1 The purpose of the trial The self-forecast for each generator was required to pass design and performance assessments before AEMO used these forecasts in its dispatch process. This is to ensure that the performance of the forecast is not significantly worse than the forecasts provided by AWEFS and ASEFS, and to ensure no adverse impact on power system security. In some cases, forecasts from a different forecast provider to that which passed the initial assessment can be submitted to AEMO, though they need to continue to pass ongoing performance assessments. The trial sought to investigate the ability of each participant to: • Submit the five-minute ahead self-forecasts via AEMO’s Market Participant Five-Minute Forecasting Application Programming Interface (MP5F API) • Provide accurate forecasts using a wide range of forecasting technologies, and understand the impact of different weather, operational conditions and geographies on the forecasts • Demonstrate commercial benefits of investing in the self-forecast technology, and feasibility of rolling out the self-forecasting solution to the industry. The aim of having a more accurate VRE forecasts used in the central dispatch process is to have: • The power system be more balanced and secure, as more accurate self-forecasts can reduce the difference between the target and actual generation output • Reduction in the use of frequency regulation by AEMO, leading to a lower FCAS operating costs to the industry • Incentivise the industry to engage in self-forecasting and promote innovation in the industry. The following sections investigate whether the three key objectives of the trial were met. The objectives were namely: • Reliability - ability of each participant to submit the forecast on time • Accuracy of the forecasting technologies used by the trial participants. This is to measure whether the self-forecast performs at least as accurately as the AWEFS and ASEFS, and to highlight the lessons learnt concerning how different weather, operational conditions and geographies influence the accuracy of the forecast between different wind and/or solar farms • Potential commercial benefits that the trial participants received during the trial and the implications of the feasibility of rolling out the self-forecast to the industry. It is important here to note some limitations of this analysis. Due to confidentiality clauses between projects and ARENA, much of the information that was supplied was done so under the proviso that it would not be publicly released. Therefore, it was only possible to perform analysis in an aggregated manner. AEMO do release generator performance metrics, which are publicly available and may have beneficially added to the analysis, however it was beyond the scope of this work. Due to aggregation, it has not been possible to identify the performance between different technologies or generators as would have been ideal. We therefore recommend seeking out other sources of data including but not limited to the publicly available project reports, which are available through ARENA’s Knowledge Bank. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 1 3. Reliability The first assessment of the trial performance is to investigate reliability - each participant’s ability to submit their self-forecasts on time. This applied in two distinct ways: • The number of weeks that the trial participants required to pass all the required assessment criteria as set out by AEMO. This reflects the agility of AEMO MP5F API platform and associated infrastructure to accept participants with different level of technical expertise and experience in dealing with AEMO • The ability of each participant to submitting the forecast before the required time that the system is closed for the dispatch interval (“gate closure”13). 3.1 AEMO’s assessment criteria AEMO is required to prepare a forecast of available capacity for wind and solar generating units which is known as the UIGF, in accordance with the NER 3.7B as highlighted in Semi-Scheduled Generation Dispatch Self-Forecast Assessment Procedure14. Clause 3.7B(c) further requires AEMO to use AWEFS and ASEFS as part of the five- minute ahead UIGFs which then feed into AEMO’s NEMDE to determine the dispatch of semi-scheduled wind and solar generating units. For the dispatch purposes, AEMO is required, under the Clause 3.7B, to ensure that each participant’s dispatch self-forecast: • Meets the purpose of use as UIGFs in dispatch • References the same location as the agreed dispatch point, matching the location of the generating unit’s SCADA active power signal used in AWEFS / ASEFS • Assumes that there is no distribution or transmission network constraints apply at the end of the next dispatch interval • Applies a cap at all local limits that are forecast to apply at the end of the next dispatch interval • Is positive and less than or equal to the generating unit’s registered maximum capacity • Meets the reliability and accuracy requirements described in the assessment procedure. To be consistent with its power security responsibility, AEMO does not use any submitted self-forecast in dispatch unless the self-forecast has passed the assessment and been accredited. There are two assessment stages set out by AEMO, namely: • Initial assessment: Reliability and performance. By the end of the assessment, the participant will be accredited when it passes the assessment. It can then be eligible to submit the self-forecast for the use in dispatch • Ongoing assessment: Performance. The assessment is on a weekly basis. AEMO has the ability to suppress the use of self-forecast in dispatch if it is causing disruption in market or power system security. 3.1.1 Initial assessment The purpose of the assessment is to ensure that the self-forecast is reliable. When the self-forecast is used in dispatch, it will be no worse than the current forecast generated from AWEFS / ASEFS. Each participant is to be assessed for reliability and performance during an assessment window of a minimum of eight weeks. The participants who meet the assessment standard are accredited and eligible for acceptance for use in dispatch. For 13 Gate closure is the point in time at which participants are no longer allowed to make changes to their self-forecasts. It is described in terms of time before the start of the dispatch interval for which the self-forecasts are made (generally 70 seconds). 14 Semi-Scheduled Generation Dispatch Self-Forecast – Assessment Procedure, Final, AEMO, 21st December 2018 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 2 those who do not meet the assessment criteria during the eight-week assessment window, the assessment period will extend each week by a week up to sixteen weeks, until the assessment criteria are all met. The initial assessment is summarised in the following flow chart: Figure 2 Initial assessment process (source: AEMO 2018) GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 3 There are two key assessments in the initial assessment, namely: Reliability assessment: For a minimum of eight weeks, each participant is expected to demonstrate that, over 95% of dispatch interval over the current assessment window, the self-forecast that was submitted to AEMO’s platform will be at least 70 seconds prior to the gate closure for the dispatch interval. Performance assessment: The second condition during the same period is the requirements that each participant demonstrates to meet the following assessment criteria: • At the minimum of 80% of dispatch interval during the current assessment window, the unsuppressed self-forecast will be submitted to the platform at least 70 seconds prior to the gate closure for the dispatch interval • The generating unit’s energy target is required to be at least equal or greater than its UIGF or provide a good quality SCADA Possible Power (AEMO may be able to use Possible Power as an alternative) for the dispatch interval. Once there is sufficient self-forecast data for performance assessment, AEMO will examine the self-forecast based on the following condition: • The self-forecast will have less than or same level of errors as AWEFS / ASEFS measuring by both Mean Absolute Error (MAE), or Root Mean Square Error (RMSE) as defined in the following: MAE self-forecast (MAESF) is required to be less than MAEAWEFS_ASEFS, where MAE is calculated as following: RMSE self-forecast (RMSESF) is required to be less than RMSEAWEFS_ASEFS, where RMSE is calculated as following: Figure 3 Explanation for performance assessment (Source: AEMO 2018) The participants with accreditation from AEMO can submit self-forecast which will be used in dispatch and subject to ongoing assessment. 3.1.2 Ongoing assessment The accredited self-forecast will be assessed on its relative performance against AWEFS / ASEFS over previous one, four and eight-week windows. This is to ensure that the self-forecast is no worse than AWEFS / ASEFS over at least one of the assessment windows. The rolling one, four and eight-week assessment window is to allow assessment of the participants’ self-forecast in a short, medium, and long term, respectively. The diversification of assessment timing takes into account of the volatility in weather conditions; therefore, it reduces risk of the self- forecast being constrained off and failed to be assessed. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 4 The performance assessment will be measured in terms of MAE and RMSE as discussed above. The conditions are: • If self-forecast performs better or same as AWEFS / ASEFS, then AEMO does not suppress the self- forecast for any window; otherwise, • AEMO suppresses the self-forecasts for all windows. Ongoing assessment involves examination of whether there are sufficient self-forecast samples over the short assessment window. • Insufficient self-forecast: the examination will repeat in all assessment windows (1, 4, and 8-week windows), and if the problem continues, then it will repeat in an eight-week assessment window. Should this fail, there will be no self-forecast performance assessment and the self-forecast remains in the current state until the next weekly assessment • Sufficient self-forecast: it will be assessed on performance when there is a sufficient sample size, as set out in the preliminary assessment. The process will be similar to above rolling basis. Should the self- forecast fail all the assessment windows, AEMO will suppress the self-forecast until the next weekly assessment. 3.2 Self-forecast’s initial assessment Of the 28 generating units, 16 passed the AEMO assessment at the minimum period of eight weeks, whilst six of the remaining units required double this time to meet the assessment criteria. Such a significant delay in meeting the initial assessment requirements may result in unexpected operational costs for participants. It is therefore important to identify the root cause and improve the situation to ensure that majority of participants meet the assessment criteria within the minimum required time. Our study has identified common issues causing delays for some participants in passing the initial assessments, including in the lead up to the assessment period, which are summarised in below table. Table 3 Initial assessment issues Issues Descriptions Contract issues Considerations • The relationship between contractors and market participants, which may add complexity in the contractual relationship, create barriers to communication with AEMO and result in delays • Delays caused by project partners, company personnel changes, construction delays, and AEMO restrictions • Difficulties during the negotiation phase were experienced. Participants and subcontractors may not always be willing to collaborate • AEMO’s API registration process only recongises market participants who are registered in the dispatch system. Any communication with AEMO relating the API connection must go through market participants. This caused communication problems for the trial participants, who were not market participants. It is a particular issue, if there are conflicting priorities between the market participants and trial forecaster • In certain instances, land-holder agreements must be secured before installing equipment on site. Delays can be caused by lengthy process of reaching agreements between parties GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 5 Issues Descriptions • Some licensing was required before sharing data between consortium partners, AEMO / ARENA and the public. This requires additional time and resources to arrange • Market participants who had multiple Dispatchable Unit Identifiers (DUID) registered experienced additional complexity for data integration into the API platform, which required additional time and resources. Technical issues Understanding AEMO’s API connection process • Problems with gaining access to the API portal to download the API reference documents and files • Challenge interpreting AEMO documentation, including technical errors in the documentation, and lacking business process information • The inherent delays in calculations and transmission of forecast information and the impacts on the forecasting frequency. The forecasts take time to process. The forecast is required to effectively be done for the time about 7 minutes ahead to be used for the next 5-minute dispatch period • Problems with password expiry and reset caused delays for a few trial participants. It was noted that passwords expired without enough notification. This created an administrative burden as the process of resetting passwords was not easy • Ensuring that data and the locations under analysis aligned with documentation and AEMO’s understanding • HTTP response codes which are standard protocols for responding to an HTTP request. It appears that AEMO’s use of HTTP response codes were sometimes inconsistent with industry convention, impacting monitoring and supporting services downstream from AEMO’s API • Redis cache problems were identified. It has been reported that there were issues with the AEMO API. It is possible for participant to switch to another set of valid credentials to restore submissions, but this did result in some missing submissions. AEMO has since fixed the problems. It is of note that many of the issues identified above have now been resolved by AEMO. Other considerations • Delays encountered by participants who ordered scientific sky camas and experienced issues such as network incompatibility and control unit overheating • Technical challenges associated with sky cameras, that were ordered with filters for use in daylight hours and tested in the UK prior to delivery. However, the lens filter is not suitable for Australia. Images captured from the solar camera were overexposed due to the brightness of the Australian sky. Also, Australia’s high temperatures impacted reliability. To address the issue, it was required to add vents and additional fan cooler power supplies. These problems added to the project delay. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 6 Issues Descriptions Other issues Long assessment periods. AEMO, as discussed above, assess MP5F self- forecast submissions for accuracy and availability over 8 weeks. Should the forecast assessment fail, this period will extend to 16 weeks until the RMSE and MAE demonstrate 80% or better compared with the AWEFS/ASEFS data. This implies that should the assessment fail in week eight, the assessment will be performed again at week 9. The participant can then pass the assessment if its forecast meets the performance criteria. It was reported that the production server was, at times, too slow to process the submitted forecast. Delays of up to 12 seconds were observed and dispatch intervals missed as a result. It appears that the probability of such a delay increases when submitting a forecast close to the deadline. Such issues with cache speeds must be resolved. As there are more renewable energy farms and server providers register, the forecast error will increase if submissions miss the deadline due to technical delays. Allowed for a sufficient level of contingency to mitigate financial risk arising from uncertainty when ordering monitoring equipment. The above issues are the root causes of delays for trial participants. AEMO implemented password automation in October 2020 to improve its system authentication and password reset process to minimise administrative burden. AEMO has already made numerous improvements to streamline these processes, including enhancing system documentation since commencing the trial. AEMO continues to work with the industry to improve the start-up process for the industry. There are areas that the incoming participants should consider before engaging in self-forecasting. These include: • Allowing a contingency in the project plan for unforeseen risks • Allowing sufficient time and resources to understand the requirements for API integration and implement the assessment criteria • Establishing communication protocols between project partners and AEMO • When partnering with another organisation, carefully considering the contractual arrangement, and how long it will take to obtain agreements from all parties • Considering requirements regarding land agreements or data licensing • Potential requirements for additional time and resources spent developing a platform to communicate with AEMO’s API • Project risks such as bush fire risk, to what extent, and how to mitigate such risk. The issues raised indicate that the self-forecasting arrangement is still in an early development stage. There are ‘teething problems’ to address, such as authentication issues. Trial participants with previous experience dealing with AEMO had fewer administrative and data connectivity issues when implementing their projects. AEMO has made changes to streamline the administrative process and reduce the burden on market participants. Participants reported no apparent structural issues relating to the capacity of the MP5F platform in managing the influx of forecast submissions from different participants. In addition, AEMO indicates that the API platform and its central dispatch system has sufficient capacity to cater for the wider industry roll-out, when it is ready to do so. To- date, 66 of 115 semi-scheduled generators (57%) are accredited or registered for a self-forecasting process. Many of the identified issues will be resolved, as the self-forecast process matures. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 7 3.3 Self-forecast ongoing assessment For those participants who are accredited and eligible to use their self-forecast in dispatch, they are likely to meet the ongoing assessment criteria to provide a minimum sample size of at least 80% intervals as set out in Section 3.1.2, assuming they are not constrained down. This is demonstrated in below Figure 4 (Wind farm reliability) and Figure 5 (Solar Farm reliability). Most of the participants, whether wind or solar farm, are consistently able to submit their unsuppressed self-forecast 70 seconds prior to the gate closure for dispatch. This is with an exception for WF1 which had two months below the 80% requirement. However, its reliability improved in Month 5 and Month 6, reaching nearly 100%. Figure 4 Wind farm participants' reliability of submitting unsuppressed self-forecasts 0% 20% 40% 60% 80% 100% WF1 WF2 WF3 WF4 WF5 WF6 WF7 WF8 Percentage of time each forecast operational and submitted to AEMO Month 1 Month 2 Month 3 Month 4 Month 5 Month 6 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 8 Figure 5 Solar farm participants' reliability of submitting unsuppressed self-forecasts There are a number of issues highlighted in the participants’ reports and/or from stakeholder meetings which may impact the ongoing reliability of data submissions, as summarised in the below table. Table 4 Summary of issues relating to ongoing assessment Issues Descriptions Prepare two submissions for one interval One of the participants submitted two forecasts within 30 seconds of each other. If the later data was not submitted on time, the first set of data would be submitted properly before the gate close. By doing so reliability was improved to over 95%. However, this increased time and resources spent, resulting in market inefficiency. Slow server response Participants note that the production server was too slow to process submitted forecasts, resulting in delays of up to 12 seconds. Some dispatch intervals were therefore missed. It appears that such delays occur when submitting forecasts close to the gate closure time. As more renewable energy farms register, the forecast error will increase if many submissions are provided close to the dispatch deadline. Outages • Should there be outages (planned or unplanned), the self-forecast quality will be compromised because of limited input data for the forecast until the historian can be restored • Data connections are ongoing challenge for some remote sites; especially when telecommunication networks had outages simultaneously • Remote connection may not be available at some sites where the hardware like LIDAR forecasters is installed. 0% 20% 40% 60% 80% 100% SF1 SF2 SF3 SF4 SF5 SF6 SF7 SF8 SF9 SF10 SF11 SF12 Percentage of time each forecast opertional and submit to AEMO Month 1 Month 2 Month 3 Month 4 Month 5 Month 6 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 9 The highlighted issues can affect participants’ ability to submit data on time. As the process matures, some of the above issues will no longer be an issue affecting the ability to meet the ongoing assessment standard. Regarding the slow server response, it will be necessary for AEMO to investigate the issue and improve the response time. A major risk for self-forecasters is that of major outages caused by an external party such as telecommunication service providers. 4. Accuracy A key focus of the trial is to assess trial participants ability to provide self-forecasts, which has higher than or at least the same accuracy level as AEMO’s AWEFS / ASEFS. Analysis of the available forecast data for seven of the trial participants shows that over a comparable period of 111,000 dispatch periods between December 2019 and September 2020. An average forecast errors for each of the individual participants added together (non-coincident forecasts) were around 4.8 MW. Meanwhile the average sum of forecast errors for each dispatch interval (coincident forecasts) was around 2.8 MW, a reduction of 42%. This suggests that numerous small reductions in dispatch forecast errors are likely to have a much larger impact on overall dispatch forecast error. The accuracy of the self-forecast is measured by comparing participants’ self-forecast against AWEFS / ASEFS in terms of: • Mean absolute error • Error bands based on both raw and normalised figures. We have normalised these measurements normalised based on the respective wind / solar generation installed capacity at registration to ease comparison (i.e., Mean Absolute Percentage Error or MAPE). 4.1 Accuracy measurements 4.1.1 Measure 1: Mean absolute error MAE of each forecast for each 5-minute interval is calculated as 𝑀𝑀𝑀𝑀𝑀𝑀 = 𝑀𝑀𝐴𝐴𝐴𝐴(|𝑀𝑀𝑀𝑀5𝐹𝐹 − 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎|) where MP5F is the unconstrained intermittent generation forecast for the endpoint of a dispatch interval, and actual is the realised outcome. MAE represents the average difference between the forecast and actual values, without regard to the direction of the difference. The below graphs present MAE for the self-forecast on either wind farm generation or solar farm generation, which are normalised based on the output capacity of each site. The average MAE for the wind farms over the six-month data provided by the participants was 2.6%, as compared with the average MAE for solar farms of 3.0%. Self- forecasts for solar generation have a higher level of mean absolute error than that of wind farms. The relative high volatility in forecast performance reflects the greater difficulty in forecasting cloud cover, as compared with wind. Figure 6 demonstrated the mean absolute percentage error (MAPE) of six wind farms, reported their six-month MAPE data starting from November 2019. The MAPE across these wind farms appeared to have reduced over the period December 19 – February 20, but slightly reversed back in March 2019. The average of the MAPE amongst these participants was just under 3%. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 10 Figure 6 Mean absolute error based on participants' self-forecast for wind farm generation (Dec-19 to Apr-20) Only wind farms 4-6 from Figure 6 provided data beyond April 2020. We have therefore compared wind farms 4-6 against wind farms 7-8 which has data between May 2020 and Sept 2020. Figure 7, provided below, demonstrates the MAPE of the wind farms with data starting May 2020 and ending September 2020. It is noted that data from each wind farm followed a similar pattern of MAPE over the five months. Their forecast accuracy reduces during winter months, while it improves during warmer months. The average MAPE over the six-month period is over 2%. Figure 7 Mean absolute error based on participants' self-forecast for wind farm generation (May-20 to Sep-20) Figure 8 demonstrates the MAPE of all the measurements taken across solar farms. Compared to wind farm data, the data from solar farms is more volatile as the gap between the highest MAPE and lowest MAPE of each solar farm is more notable than that of measurement from the wind farms. 0.0% 1.0% 2.0% 3.0% 4.0% 5.0% 6.0% WF1 WF2 WF3 WF4 WF5 WF6 Wind Fram 1 to 6 - Normalised Mean Absolute Percentage Error (MAPE) Dec-19 Jan-20 Feb-20 Mar-20 Apr-20 Average 0.0% 1.0% 2.0% 3.0% 4.0% 5.0% 6.0% WF4 WF5 WF6 WF7 WF8 Wind Farm (May 20 to Sep 20) - Normalised Mean Absolute Percentage Error (MAPE) May-20 Jun-20 Jul-20 Aug-20 Sep-20 Average GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 11 Figure 8 Mean absolute error based on participants' self-forecast for solar farm generation 4.1.2 Measure 2: Percentage of time in error bands The second measure of self-forecast accuracy is to place the size of errors into error bands, in order to show the percentage of times that the forecast errors fall within bands representing 50%; 20%; 10%; 5%; 1% and 0.5% of the actual generation output. We have combined these error bands into 0% - 5%; >5% - 20%, and 20%-50% for the ease of analysis. Figures 9 & 10 below demonstrate how each of the eight participating wind farms perform when measured by the three error bands. The figures demonstrated that self-forecasting of solar farms performs better than that of wind farms. 50% of the measured time, the forecast error of all solar available data fell into the 0% - 5% error band, whilst 34% of wind farm available data resulted in the 0%-5% error band. The forecast outcome in terms of absolute error for solar farm is, however, larger than that of wind farm, as reflected in the Measure 1: Mean Absolute Error.15 The error band outcomes reflect that on a sunny day, the solar forecast outcomes will be very close to actual output. The accuracy of solar forecasts on a sunny day tends to be higher than that of wind farm, which is more difficult to predict wind speed and directions. The forecast outcomes are, however, different on a cloudy day, as it is difficult to forecast cloud cover. Another point to note is that the participating wind and solar farms reported that their forecast varied between 34% to 93% of times (average of 79%) achieving 50% of the actual output over the six-month period. This wide range of percentage times that the forecast deviation being 50% of the actual output has shown that the constant changes in weather conditions and other locational factors have made the renewable energy generation output forecast challenging. The significant deviation between forecast and actual results in high FCAS requirements to balance the power systems and therefore FCAS costs to the market participants. It is to note that WF6 has a significantly higher percentage of errors falls in the 0%-5% error band than other wind farms. There is no explanation for the result. 15 The Skill of ECMWF Cloudiness Forecasts, Thomas Haiden, Richard Forbes, Maike Ahlgrimm, Alessio Bozzo, ECMWF Newsletter no 143 – Spring 2015 p14-19 0.0% 1.0% 2.0% 3.0% 4.0% 5.0% 6.0% SF1 SF2 SF3 SF4 SF5 SF6 SF7 SF8 SF9 SF10 SF11 SF12 Solar Farm - Normalised Mean Absolute Percentage Error (MAPE) Apr-20 May-20 Jun-20 Jul-20 Aug-20 Sep-20 Average GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 12 Figure 9 Error bands for wind farms (Dec-19 to Apr-20) Figure 10 Error bands for wind farms (May-20 to Sep-20) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Dec-19Jan-20Feb-20Mar-20Apr-20Dec-19Jan-20Feb-20Mar-20Apr-20Dec-19Jan-20Feb-20Mar-20Apr-20Dec-19Jan-20Feb-20Mar-20Apr-20Dec-19Jan-20Feb-20Mar-20Apr-20Dec-19Jan-20Feb-20Mar-20Apr-20 WF1 WF2 WF3 WF4 WF5 WF6 Wind Farm 1 to 6 - Accuracy in each error band over period 0% - 5% error band 5% - 20% error band 20% - 50% error band 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%May-20Jun-20Jul-20Aug-20Sep-20May-20Jun-20Jul-20Aug-20Sep-20May-20Jun-20Jul-20Aug-20Sep-20May-20Jun-20Jul-20Aug-20Sep-20May-20Jun-20Jul-20Aug-20Sep-20 WF4 WF5 WF6 WF7 WF8 Wind Farm (May 20 to Sep 20) - Accuracy in each error bands over period 0% - 5% error band 5% - 20% error band 20% - 50% error band GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 13 Figure 11 Error bands for solar farms Percentage of time SF12 having forecast error equal to or less than 50% of actual output is abnormally low, as shown in Figure 11. No explanation was provided on such occurrence. 4.1.3 Measure 3: Comparison with AWEFS / ASEFS Another accuracy measurement provided by a limited number of participants (both wind and solar) are the direct improvement in MAE and RMSE measures of the self-forecasts compared with equivalent AEMO forecasts. As shown in Figure 12, based on MAE, self-forecasting on windfarm generation output outperformed AWEFS by between 11.7% and 19.8%. Similar results for solar farms versus ASEFS were in the range of 13.1% and 18.2%. Figure 13 shows the improvement of self-forecasts over AEMO forecasts using RMSE. Using this measure, self- forecasts for wind farms outperformed AWEFS by between 11.5% and 45.6%, while self-forecasts for solar farms outperformed ASEFS in the range of 2.8% and 21.2%. Some participants suggested that an 18% accuracy improvement based on MAE is the optimal level. Any attempt to further improve accuracy will see the marginal benefits start to decline. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20Apr-20Jun-20Aug-20 SF1 SF2 SF3 SF4 SF5 SF6 SF7 SF8 SF9 SF10 SF12 Solar farm - Accuracy in each error bands over period 0% - 5% error band 5% - 20% error band 20% - 50% error band GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 14 Figure 12 Self-forecast outperformance of AWEFS / ASEFS in terms of MAE Figure 13 Self-forecast outperformance of AWEFS / ASEFS in terms of RMSE 4.2 Factors affecting accuracy Results of the trial shows that self-forecasting by market participants allows for greatly improved accuracy as compared with AWEFS / ASEFS. There are however issues relating to assessment period, data, and other factors that may have hindered further improvements in the forecast accuracy, which are highlighted as below. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 15 Forecast horizon Constraint periods AEMO places a constraint to keep the power system in line with the transmission capacity which is informed by the transmission network service providers. When there is a constraint, generators cannot use active power to produce forecasts as the active power is suppressed below the unconstrained output of the generator. This will impact forecast performance during and immediately after when the constraint is active. To solve the problem, one of the participants has developed three separate models which allows algorithms to switch between three scenarios. The models are to use: • Lags of past power (30 minutes) and other parameters to forecast for ‘normal’ operation period • An autoregression model of the possible power output for constraint period • A full 30-minutes of historical data for the scenario where the constraint is lifted, and updated data is yet available. The ensemble model appears to have improved the accuracy of forecasts after periods of constraint compared with AEMO’s forecast. Forecasting time It has been commented that by reducing the forecast horizon, it will reduce the level of errors. Data issues: Rounding issues When submitting a forecast to the AEMO API, it is required that the capacity of an entity be between zero and the maximum capacity, be stored as integer in AEMO’s registration systems, and always rounded down. For example, a wind farm has a rated capacity of 120.8 MW, then the maximum capacity forecast cannot exceed 120MW; otherwise, AEMO rejects the submission. This creates an inherent error in the system that the self- forecast potentially cannot be absolutely accurate when it is at full capacity. Discrepancy between AEMO data and participants’ data AEMO took a power reading from a different location than the participants. This creates a discrepancy between expected power and reported power. It is difficult to match the timing of AEMO’s instantaneous power samples for the initial MW which may cause minor errors. Quality of site data It was highlighted that solar forecasting is one of the most demanding forecasts partly due to problems in control system data. This includes: • Missing or incorrect data mapping • Poor quality data or data drop out • High resolution data is required for training the forecast model. It is important to save historical data in high resolution • Remote data can be a challenge as well. An example of this is when telecommunication network providers had outages simultaneously. Therefore, to improve forecast performance, it is required to filter data that negatively affects the forecast output. Forecast performance improvements come from: • Using assembling techniques to combine multiple models’ forecasts • Using larger training sets for machine learning models • Improvements in inverter power conversion modelling GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 16 • Filtering bad data from training sets – specifically unchanging, missing, and incomplete SCADA data • In addition to changes in methodology, it was also required to relocate its dispatch point to the onsite switchyard which is consistent with AEMO data feeds and dispatch location and retrained forecasts. This has improved levels of accuracy when collecting data. Other data issues Factors affecting wind forecast accuracy: • Flow model accuracy • A significant alteration in the Bureau of Meteorology data, which requires a significant amount of time and resources to change the code before downloading, reading, and processing the new dataset. Therefore, it is important to be across potential upgrades and changes to external input data • Wind direction - placement/number of upstream sensors • Latency - real time SCADA readings and high-speed forecast calculations will increase forecast accuracy • Forecasting the state of the plant (turbine shutdowns etc.) and assuming that observable plant variables, such as turbine state, are constant for the next 6.5 minutes. Note that the MP5F forecast must be submitted 6 minutes and 10 seconds before the applicable period • Unobservable variables that affect plant performance. For example, plant/turbine set points • High speed cut-outs. Other factors Measurement costs for wind farms It was indicated that inexpensive short tower wind measurements could lead to a cost- effective implementation of technology. Two factors will however impact measurement effectiveness: • The lower the height, the more sensitive is the measurement to the terrain between the tower and the turbines • Significant vegetation such as forest canopies which are at similar height to the measurement equipment may have impact measurements. Approach • To prevent potential lapses in self-forecasting, it is important to have multiple forecasting locations and expandable data storage or a setup which allows users to overwrite their oldest data and allow them to continue despite a fixed amount of data storage. Additional requirements include redundant systems, management of potential power failures, and having enough data storage • Real-time SCADA data cannot be assumed to be high quality. A robust monitoring system is required to sense and respond to any abnormalities quickly to minimise impacts on the forecast output • Using a single approach to forecasts is not preferred as it is less reliable compared with alternatives. The preferred approach is to blend multiple forecast elements such as a sky imager, satellite imagery, and statistical forecasting using SCADA inputs, to produce the required output • Planning should include multiple potential options for the installation and configuration of sky-image hardware. This is to minimise delays and mitigate risks. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 17 The table below highlights lessons learnt from the participants about how different weather, operational conditions and geographies influence the accuracy of the forecast across different wind and/or solar farms. Weather Wind As wind speed varies between turbines, one suggested hypothesis for providing better forecasting outputs was to model each turbine separately and then aggregate the forecasts as the total site generation. The aggregate outcomes did not perform better than the forecast model that grouped turbines. It was therefore recommended to model turbine groups rather than individual turbines. The layout of wind farm turbines impacts the wind direction forecast. Wind turbines that do not shadow each other have less reliance on wind direction in the forecast. Conversely, if the wind turbines are clustered, wind direction will have a greater emphasis in the forecast. Accessing turbine specific wind speed and wind direction data may improve the model’s ability to recognize trends and capture inherent atmospheric influences over the short- and long-term production. The use of the 6th order linear regression (LR6), ARIMA (1,0,5) and persistence was recommended. This performed slightly better, with lower computational requirements, than AWEFS due to the tendency for these forecasts to predict an increase in power from a low power state and decrease from a high-power state. Forecast models tend to more accurately predict small power ramping events than AWEFS. These models work better during spring and summer when the wind can be volatile, and during the day when the wind is more prone to change. Another challenge when using upstream measurements is that seasonal variability in wind speed and direction makes the task more difficult. Therefore, for larger wind farms with a more varied climate, the number of measurements necessary to increased accuracy will be much greater and therefore more expensive. Solar • A clear sky power model is needed to better deal with sunrise and sunset. Diffuse irradiance has a more significant effect at these lower light times. Modelling cloud impacts is also particularly challenging in these periods. The models need adjustment accordingly and better modelling of diffuse irradiance has delivered a material impact on forecast accuracy. • There are several large clouds events in the middle of the day which CloudCAM was able to accurately forecast the timing and reduction in power for. This accurate forecasting of power reduction is not possible on utility-scale solar farms without good diffuse irradiance models. • The preferred approach is to have an aggregate approach that blends each of the forecast elements, such as sky imager, satellite imagery, and statistical forecasting using SCADA inputs, to produce the required output. • The performance is influenced by the morning and afternoon ramp up / down, as the forecasting ensemble models applies different weights to the persistence forecast when the zenith angle is greater than a threshold value. This effect is magnified for large solar farms. The magnitude of ASEFS errors during morning and afternoon ramps can cause persistent forecast errors of >5 MW throughout the ramps. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 18 Operation Global Operating Systems Cloud based system such as the Global Operating System (GOS) are not built for real time controls. It is therefore not compatible with self-forecasting. The GOS is a plant monitoring system, and it does not require robust continuous communications capability. SCADA data integration on the other hand is composed of onsite hardware and a dedicated RTU and continues to run smoothly even if communications are interrupted. It is therefore recommended to be a better system. Sky Cameras Technical issues: • Some participants reported that sky cameras ordered from overseas were not immediately suitable for Australia. Images captured from such solar cameras were over-exposed as the Australian sky is, on average, brighter than the sky in the cameras’ places of origin. • High temperatures in Australia impact the long-term reliability of hardware purchased from overseas. To address the issue, it was necessary to add vents and a new, fan cooled power supply. Cleaning: • Atmospheric contamination reduces the effectiveness of using Skycam as a forecast input. Skycam with self-cleaning devices were reported to improve forecast performance by 20-30% relative to those without self-cleaning devices. • Bird droppings still present challenges on Skycam forecast effectiveness. The use of sensors is currently under consideration for detecting birds before they reach instruments and actively trigger audio and mechanical system to deter them from perching nearby. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 19 5. Financial performance 5.1 Background The purpose of the self-forecast trial is to assess whether the self-forecasts will result in lower errors than AEMO’s AWEFS / ASEFS forecasts. Improved forecasting of intermittent generation output is likely to increase power system stability by reducing FCAS requirements. Importantly for the generators concerned, improved self- forecasting may lead to a reduction in their FCAS charges. Hence part of the trial examined the financial outcome of self-forecasting, focusing on its impact on FCAS charges. 5.2 Calculated FCAS savings for trial participants With more accurate dispatch instructions, thanks to self-forecasting, we expected that there would be a reduction in the CPF and therefore a reduction in the costs to trial participants, relative to a counterfactual in which AWEFS / ASEFS forecasts were used. A limited number of trial participants provided data to ARENA on the Net financial benefits of the self-forecast trial. Based on this financial data, we have calculated an indicative net Present Value (NPV) of financial benefits generated from the trial (in $2021 real dollar terms). It is important to note that this is an aggregated financial performance calculation and some self-forecasts generated significantly higher returns for the wind and solar farms that they were deployed on. Some simple paybacks were less than six months. 5.2.1 The cost of the trial program The costs of the trial program comprise: • ARENA’s partial funding to all trial participants of $9.4 million • Participants’ own contributions of $5.7 million • AEMO’s contribution to the implementation, which we assume entailed a capital cost equivalent to 5% of total of capital contribution from ARENA and trial participants for adjusting relevant systems to cater for the trial program • AEMO’s ongoing costs for administration of the self-forecast process (not including existing administration costs relating to AWEFS / ASEFS). The total implementation costs are $15.4 million. 5.2.2 Estimated net benefits of the trial program to the participants Due to limited financial information on individual trial participants who had completed the program at the time this report being prepared, the NPV of cash flow considered in this report is at the trial program level and is not segregated into wind and solar farms cash flow subtotals. Table 5 demonstrates the NPV calculations. The NPV of cash flow to the trial program on a twelve-year basis (two-year implementation and ten-year operation) is estimated to be $2.5 million with a payback of six years, based on a real discount rate of 8%, in 2021-dollar terms. The present value of benefits16 provided by trial participants for all the solar and wind farms was $19.4 million and the present value of costs (both implementation costs and ongoing operating costs) was $16.9 million. 16 Five trial participants provided expected savings across a number of generating units, which were then annualised for the purpose of the Net present value calculations. All the financial data were extracted from the participants’ reports and had not been audited. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 20 Table 5 Net present value of cash flow for the trial program NPV Financial year ($’000) ARENA funding -3,156.6 Contributions from participants -3,086.9 AEMO capex -312.2 Annualised benefits reported from trial participants 19,400.0 Assumed operating costs -9,711.2 AEMO ongoing costs -688.6 Net cash 2,487.0 Payback 6 years IRR 5.7% ARENA’s funding and participants’ contribution to the program include only the amount attributed to those participants who provide financial data, $3.2 million and $3.1 million, respectively. The net benefits of the trial program are savings in FCAS charges (relative to AWEFS / ASEFS forecasts) due to improved CPF. Savings (excluding financial impacts on interests and tax) are estimated using the financial information provided by a number of trial participants mostly across solar farms, with a small number of wind farms. It is important to note that the FCAS fee savings are estimations. The trial participants highlighted that calculating FCAS charges is very difficult due to the complex nature of the calculation of the CPF. They estimated the above savings using their own forecast models, based on data available to them. The NPV cash flows for the trial program was calculated, assuming that trial participants will continue to experience the same rate of savings for the next ten years. The NPV result suggests that rolling out self-forecasting to most if not all semi-scheduled generators is advantageous to wind and solar farms in the NEM, as NPV for all is greater than zero with payback period of 6 years with Internal Rate of Return (IRR) of 5.7%. For those trial participants who provided financial information, the payback period was reported to be as low as three to six months. The table below provides a summary of assumption used in the NPV calculations. Table 6 Assumptions for net present value calculations based on the four trial participants who submitted financial data Assumptions Descriptions Dollar term 2021, real ARENA funding Of the total $9.4 million ARENA funding, $3.2 million contributed to those participants who provided financial data Participants’ contributions $3.1 million Operating margin 50%. With operating margin of 50%, each of these participants will have an operating costs equivalent to total benefits * 50% AEMO capital costs $312,000, which is 5% of total of capital contribution from ARENA’s funding and participants’ contributions AEMO ongoing incremental maintenance costs due to self-forecast roll-out $80,000, which is 1.5% of total of capital contribution from ARENA’s funding and participants’ contributions Discount rate 8%, comprising 7%, real (Widely adopted in infrastructure sector) + 1% risk premium (reflecting new technology) GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 21 Assumptions Descriptions Project life 2-year implementation, and 10-year operation. This is a relative short project life, reflecting the rapid change in technology. The NPV of cash flow to the trial program on an eleven-year basis (2-year implementation and 10-year operation) is estimated to be $2.5 million with a payback of six years. It is important to note that: • The cost assumptions relating to participants’ contributions, and AEMO’s implementation costs and ongoing administration costs are for illustrative purposes only. Further enquiries to the trial participants and AEMO would be necessary to confirm these costs. Benefits used in the above calculations are measured by the savings which is the difference between FCAS charges calculated based on self- forecast and AWEFS/ASEFS. It is important to note that the savings exclude the savings impact on each participant’s depreciation, interest and tax profile. • The FCAS fee savings are estimations provided by participants. The trial participants highlighted that calculating FCAS charges is very difficult due to the complex nature of the calculation of the CPF. They estimated the above savings using their own forecast models, based on data available to them. It is assumed that trial participants will continue to experience the same rate of savings for the next ten years. Our purpose in making the above assumptions is to undertake a benefit-cost analysis to demonstrate the impact of self-forecast on the participants who provided financial data. This potentially have implications on the overall impact on the renewable energy providers, if self-forecast is to roll-out to the industry. Below table present sensitivity analysis of the key variables assume in the above calculations. Table 7 Sensitivity analysis -10% Base case 10% Operating margin 40% 50% 60% NPV ($ million) 0.5 2.5 4.4 IRR (%) 0.4% 5.7% 10.7% Participant benefits $ million pa 2.1 2.3 2.6 NPV ($ million) 1.5 2.5 3.5 IRR (%) 3.1% 5.7% 8.3% Discount rate 7% 8% 9% NPV ($ million) 2.9 2.5 2.1 IRR (%) 6.7% 5.7% 4.8% In short, based on the limited financial data provided, rolling out the self-forecasting across the industry is likely to be advantageous. There are wider economic benefits generated from improving power system stability and reliability that have yet to be included in the above NPV calculations. In addition, should the self-forecast improve the forecast accuracy in the dispatch process, it will reduce FCAS costs to the wind and solar generators, which will in turn increase investment attraction to the industry. It is vital to continue to build the renewable energy industry to replace the fossil-fuelled based generation, and future energy such as green hydrogen sector development. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 22 5.2.3 Other factors that can affect FCAS charges The complexity and dynamic nature of wind and solar farms poses many challenges for providing accurate forecasts17. By improving generation forecast accuracy, system reliance on FCAS will be reduced, therefore reducing CP fees for generators. However, given the market volatility during extreme events such as bushfires or severe weather, annual or seasonal changes, energy market behaviour, energy regulation and others, FCAS will continue to be required to stabilize the market. There are also a number of other factors that can influence the FCAS charges, as shown below. Ramp up speed If a generation cap is lifted, some control systems may not be able to ramp the plant fast enough to meet the dispatch target within one period. The plant could then incur CP charges. Weather condition On clear-sky days, the self-forecast performed significantly better than ASEFS. On days with highly variable cloud cover, the self-forecast performance gain over ASEFS was diminished. Extrapolating from this, it is clear that sites in regions with more frequent periods of highly variable cloud cover (i.e., near bodies of water and in the sub-tropics of Queensland) that apply self-forecasting have lower CP fee reductions compared with regions with fewer periods of variable cloud cover (NSW and Victoria). Forecast accuracy An ability to provide accurate forecast is not the only avenue leading to FCAS savings. As seen above, by manipulating the forecast error, participants can achieve FCAS savings. A detailed investigation of periods showing similar performance between ASEFS and self-forecasting found that the quality of SCADA measurements, frequency of semi- dispatch caps, and local limits can affect the extent to which CPF can be reduced. These factors have direct impact on CPF and may not necessarily reflect in forecast accuracy. 6. Conclusions We have reviewed of all the reports and data submitted to ARENA by the VRE self-forecast trial participants during the course of the trial so far. In this report we have discussed the extent to which the three objectives set out for the self-forecast trial were met. Our conclusions are set out below. 6.1 Reliability Initial implementation Most of the trial participants have integrated their systems and been able to submit five-minute ahead self- forecasts to AEMO’s MP5F system. These participants have passed the initial assessment. Once the participants are accredited, they can submit their forecast and AEMO can then use it in the central dispatch process. Throughout the implementation period, there have been a few “teething” problems such as issues with password resets, insufficient details, errors in documentations, and communications problems. Reports from participants indicate that the API platform was slow which resulted in several missing the opportunity to submit forecasts on 17 DNV GL 2018APR166 End of Project and Financial Analysis Report GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 23 time. Many issues have already been resolved, or in the process of being resolved. As the process matures, AEMO and market participants will be able to streamline the process and become more efficient. During the review process, we have not found any evidence to indicate systemic structural issues which may restrict the self-forecasting roll out across the remaining industry. Ongoing issues There are some external factors that may affect the reliability of forecast submission. Telecommunication network outages can hamper the ability to submit forecasts on-time. Going forward, this may be classified as a project risk for self-forecasters. Should there be a widespread telecommunication network outage, numerous market participants may be unable to submit their forecasts. This could have implications for the security of the power system, although the existing AWEFS / ASEFS backup would remain available. Some participants identified a method of increasing the reliability of their submissions which may also offset the risk of temporary telecommunications outages. This is to submit more than one set of forecasts for the same dispatch interval, 90 seconds apart, to the system, or to forecast several dispatch intervals ahead and continuously update the forecasts. This ensures that at least one set of data is entered before the submission deadline. 6.2 Accuracy Our review has highlighted some intrinsic problems that may have hindered the ability of the trial participants to initially make improvements in the forecasts. For example, when submitting a forecast to the AEMO API, it is a requirement that submissions be between zero and the maximum capacity of the generator, stored as integer in AEMO’s registration systems, and always rounded down. This means, for example, that a 180.5 MW wind farm would be recorded as 180 MW. At full capacity, if a forecast of 180.5 MW were to be submitted, it would exceed the recorded maximum capacity. Accordingly, this would result in the forecast being rejected and not used in the dispatch process18. Another issue arose when the location of power measurement differed between trial participants and AEMO. This created a discrepancy between expected power and reported power and impacted the forecast accuracy. Several participants experienced difficulties in providing accurate forecasts using a single forecasting model. This is largely because the output of a power station often changes more rapidly than regular changes in wind speed or solar radiation following a period of being constrained off. Instead, a combination of models was recommended to deal with all circumstances and to consistently outperform AWEFS / ASEFS. This provides the flexibility needed to cater for the dynamic conditions of wind and/or solar farms. 6.3 Financial performance Six trial participants provided sufficient financial data to make a judgement (measured by present value) on the financial feasibility of self-forecasting for an individual generator. All but one would have achieved a positive present value of FCAS savings if they continued to achieve comparable FCAS savings from the self-forecast trial for the next ten years, at a discount rate of 7%. In addition, we have aggregated the cash inflow and estimated outflows to calculate the NPV of a representative project using estimated FCAS savings from the trial participants, resulting in a positive NPV with a payback of 6 years at the IRR of 23%. Based on the analysis of the limited data set, assuming everything remains the same, it indicates that there is a commercial benefit for rolling out the self-forecast to the remaining industry. Some individual participants reported significantly higher returns for generators than these aggregated results with some simple paybacks being less than six months. The financial benefit to an intermittent semi-scheduled generator of implementing self-forecasting is complex, given that the connection between forecast accuracy and FCAS costs is not a simple inverse relationship. With a tendency towards underestimation of five-minute-ahead generation in the system (after allowing for forecasts of intermittent generation), there is on average a greater requirement for regulation raise services than there is for regulation lower services. Therefore, an intermittent generation unit that exceeds its forecast reduces the FCAS 18 This may be a learning curve for new self-forecasters. We do not know if the recorded maximum capacity in this case may be able to be recorded at 181 MW, which would then allow the maximum self-forecast in the forecasting algorithm to be constrained to 181.5 MW. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 24 requirement. The self-forecast trial demonstrated that a slight forecast bias, and not necessarily the most accurate forecast, may result in the lowest FCAS costs. Some trial participants have developed a model to calculate how to minimise FCAS costs by applying an appropriate bias to their self-forecasts. A few participants have noted that to benefit from the lower FCAS charges, they would have to underestimate their generation capabilities. When the generation falls short of the dispatch target, there will be a need for regulation rise. Therefore, a generator which has underestimated its generation ability will have the capacity to provide a fast injection of electricity to the grid and be rewarded. It is to note that AEMO applies a forecast error margin to registered FCAS offer trapeziums for VRE generations, so if they are systematically under-forecasting, then this might increase the trade-off between energy and FCAS raise to allow for this increased uncertainty. 6.4 Further observations While self-forecasting clearly has the potential to result in more accurate forecasting than AWEFS / ASEFS, the largest errors will tend to occur under similar circumstances, for the following reasons: • Half-hourly and seasonal profiles of forecast errors for trial participants and AEMO forecasting systems are likely to be aligned, simply because sometimes are inherently more difficult to forecast • Solar forecasts are easier than wind forecasts. Analysis of the available forecast data for seven of the trial participants shows that over a comparable period of 111,000 dispatch periods, average forecast errors for each of the individual participants added together (non- coincident forecasts) were around 4.8 MW. Meanwhile the average sum of forecast errors for each dispatch interval (coincident forecasts) was around 2.8 MW, a reduction of 42%. This suggests that numerous small reductions in dispatch forecast errors are likely to have a much larger impact on overall dispatch forecast error. Following on from the above point, it is clear that the scope for dispatch forecast error reduction right now, across all NEM semi-scheduled generating units, would be significant. The significance will also grow over time as the proportion of these types of generating units increases. It appears that the relatively large investment in forecasting hardware such as LIDARS may not represent a proportionate reduction in forecasting error. Some participants were successful in reducing forecast error solely using a forecasting algorithm. To the extent that reductions in aggregate forecasting error resulted semi-scheduled generation units being constrained off for less time, an indirect result of the improved forecasting may be an increase in available generation. This may be worthy of further analysis and quantification of this source of potential market benefit. The trial was not designed nor was the question addressed of using self-forecasting for day-ahead or Short-Term Projected Assessment of System Adequacy forecasts. However, a rapidly growing short-run self-forecasting capability in the NEM may lend itself to extended forecasts in the future. Similarly, we have not investigated potential complimentary methods of frequency regulation in the NEM, such as the use of Load Frequency Control Areas19 as applied to Renewable Energy Zones. 6.5 Recommendations Based on financial performance, many of the participants reported a positive NPV with short payback period. This indicates that, if everything remains the same, self-forecasting of intermittent generation should be encouraged for the rest of the industry. As self-forecasting rolls out across the remaining industry, AEMO will continue to monitor and assess its platform capacity, resource allocation and documentation, identifying areas that may be streamlined and methods of assisting inexperienced forecast providers. 19 A Load-Frequency Control Area (LFC Area) is part of a synchronous area, physically demarcated by points at the transmission network connection to other LFC Areas and operated to fulfil the NEM-wide frequency control obligations for power flows into and out of the LFC Area, rather than at each connection within it. GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 25 7. References i. About the National Electricity Market, AEMO website, https://aemo.com.au/en/energy- systems/electricity/national-electricity-market-nem/about-the-national-electricity-market-nem. ii. AEMO’s 20-year development plan for the National Electricity Market, https://aemo.com.au/newsroom/media-release/isp-2020 iii. An Introduction to Australia’s National Electricity Market, 2010, AEMO, https://www.abc.net.au/mediawatch/transcripts/1234_aemo2.pdf iv. DNV GL 2018APR166 End of Project and Financial Analysis Report v. Fact Sheet, The National Electricity Market, AEMO, 28 July 2020, https://aemo.com.au/- /media/files/electricity/nem/national-electricity-market-fact-sheet.pdf vi. Fact Sheet: How the spot market works: AEMC, https://www.aemc.gov.au/sites/default/files/content//Five- Minute-Settlement-directions-paper-fact-sheet-FINAL.PDF. vii. Guide to ancillary services in the National Electricity Market, April 2015, AEMO, https://aemo.com.au/- /media/files/electricity/nem/security_and_reliability/ancillary_services/guide-to-ancillary-services-in-the- national-electricity-market.pdf viii. NEM participation and exemption list: AEMO, https://aemo.com.au/- /media/files/electricity/nem/participant_information/nem-registration-and-exemption-list.xls ix. NEM FCAS causer pays factor issues for wind and solar farms, WattClarity, 23rd March 2017 x. https://wattclarity.com.au/articles/2021/04/ranking-solar-farm-fcas-costs-in-the-nem-in-2020/ xi. Quarterly Dynamic Report, 4th Quarter 2020, AEMO xii. Quarterly Dynamic Report, 4th Quarter 2020, AEMO, p23 xiii. Semi-Scheduled Generation Dispatch Self-Forecast – Assessment Procedure, Final, AEMO, 21st December 2018 xiv. What is Frequency Control Ancillary Services, ARENWIRE, 10 August 2017, https://arena.gov.au/blog/what-is-frequency-control-ancillary-services/ GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 26 Appendix GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 27 A-1 FAS Markets and Cost Recovery About the FCAS markets The load on a power system (demand) is required to constantly match with the generation feed to the system to maintain a stable power system20. The balance between the load and generation is key to maintain the power frequency within the normal operating band of 49.85Hz to 50.15Hz (50 cycles per second). Any deviation between the two, it will cause either power frequency increases (demand < generation) or decreases (demand > generation)21. The FCAS markets allow AEMO to purchase services from the market participants who have the capability to inject or withdraw energy in order to rebalance the power system as and when required. FCAS may provide either a fast injection of power or fast reduction of power. FCAS market participants offer their services into the FCAS markets, in a similar manner to generators who bid into the energy market22. The participants can bid in the following markets that relate to either “regulation” or “contingency” services: • Raise regulation • Lower regulation • Contingency raise six seconds • Contingency raise 60 seconds • Contingency raise five minutes • Contingency lower six seconds • Contingency lower 60 seconds • Contingency lower five minutes. Regulation FCAS constantly corrects frequency deviations in response to minor deviations in load or generation. Regulation FCAS is traditionally provided by coal and/or gas generation plants who are on Automatic Generation Control (AGC). The AGC system allows AEMO to monitor the power system frequency and to send signals to generators to change output in order to restore the frequency back to within the normal operating band. Contingency FCAS corrects larger deviations in frequency due to a major contingency such as the loss of a generation unit, large load or a significant transmission element. To do so, contingency services will restore the balance by, for instance: • Opening or closing turbine steam valves • Load shedding by disconnecting load elements from the electrical system • Starting a fast generator (in the case of high frequency, reducing generator output) As indicated above, the majority of FCAS is provided by scheduled generators (generally coal or gas fired or hydro plant), although fast frequency response in the NEM is increasingly provided by battery energy storage systems. Meanwhile the intermittent output of semi-scheduled wind and solar farms, due to their dependence on variable wind speeds or solar radiation intensity, can play a significant role in frequency fluctuations as the proportion of these types of generation increases. FCAS cost recovery and self-forecasting The costs incurred by AEMO in procuring FCAS from some generator market participants is recovered from all participants. Contingency Costs are generally shared between generators and customers in proportion to the quantity of energy supplied or consumed. However, the recovery of Regulation FCAS costs are determined using a “causer pays” approach. This means that scheduled generating units that fall short of (or exceed) their dispatch 20What is Frequency Control Ancillary Services, ARENWIRE, 10 August 2017, https://arena.gov.au/blog/what-is-frequency-control-ancillary- services/ 21 Guide to Ancillary Service in the National Electricity Market, April 2015, AEMO 22 An Introduction to Australia’s National Electricity Market, 2010, AEMO, https://www.abc.net.au/mediawatch/transcripts/1234_aemo2.pdf GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 28 targets, and semi-scheduled units that are forecast to generate more than (or less than) they actually produce, are allocated Regulation FCAS charges in proportion to the services required to correct the error. In order to allocate the costs of regulation FCAS, AEMO calculates “Causer Pays Factors” (CPF) for each market generator using four-second generation snapshots, including output at the start of a dispatch interval, an assumed dispatch trajectory towards the dispatch target or forecast at the end of the dispatch period, and the error or variation between actual generation and the expected trajectory23. The dispatch trajectory is measured as a straight line drawn between two successive dispatch intervals24. The CPF are calculated every twenty-eight days using historic data and applied over the following twenty-eight-day period. Scheduled generation units controlled by a governor tend to ramp linearly between the initial output at the start of a dispatch interval and the target value for the end of the dispatch interval. Progress is tracked with SCADA measurements occurring every 4 seconds. CPF are calculated by applying a frequency indicator to the errors. This data is then averaged over the 5-minute dispatch interval. The mechanics of FCAS cost recovery using the CPF method work well for scheduled generation units as their output is, under normal circumstances, predictable. As more wind and solar farm generation enters the NEM, the significance of deviations between actual generation and the dispatch trajectory increases. As mentioned above, AEMO forecast wind and solar generation using AWEFS/ASEFS, for use in the dispatch process. Variations in topography between wind farm locations, local weather conditions, and time of day contribute to forecasting errors, which, in turn, result in a greater proportion of FCAS cost recovery from wind and solar farms. Accordingly, the self-forecast trial investigates whether five- minutes ahead self-forecasts will performance better (or at least no worse) than AEMO’s AWEFS/ASEFS forecasting systems. With the ability to provide self-forecasts for use in dispatch, semi-scheduled generators can potentially provide more accurate predictions of their generation output than AWEFS/ASEFS. It should be expected that self- forecasts would be better adapted to specific location conditions, including weather patterns and topography. Self- forecasting also better aligns efficient market outcomes with the interests of generators. With more accurate predictions, FCAS Causer Pays charges for solar and wind generators may be expected to decrease, making investment in this sector more attractive and decreasing costs for consumers. The section below summarises the overall financial benefits achieved for trial participants. 23 Guide to ancillary services in the National Electricity Market, April 2010, AEMO, https://aemo.com.au/- /media/files/electricity/nem/security_and_reliability/ancillary_services/guide-to-ancillary-services-in-the-national-electricity-market.pdf 24 NEM FCAS causer pays factor issues for wind and solar farms, WattClarity, 23rd March 2017 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 29 A-2 List of the knowledge sharing deliverables submitted by the trial participants Trial participants Knowledge sharing deliverables Advisian Advisian Wind and Solar Forecasting for the NEM 11_Mar_2021 Advisian Wind and Solar Forecasting for the NEM Project Progress & Lessons Learnt Report 2 Advisian Wind and Solar Forecasting for the NEM Project Progress & Lessons Learnt Report 4 Aeolius Wind Systems AWS Progress Report v070520; AWS PR DNV GL DNV GL 2018 ARP 166 End of Project and Financial Analysis Public Report DNV GL 2018 ARP 166 End of Project and Financial Analysis Report; 2018ARP166 DNV GL Final Milestone Report; 2018ARP166 DNV GL Lessons Learnt Report 3; 2018ARP166 DNV GL Progress Report 3; 2018ARP166 DNV GL Milestone Report 2; 2018ARP166 DNV GL Lessons Learnt Report 2; 2018ARP166 DNV GL Progress Report 2; 2018ARP166 DNV GL Milestone Report 1; 2018ARP166 DNV GL Lessons Learnt Report 1; 2018ARP166 DNV GL Progress and Lessons Learnt Report 1; Fulcrum3D (Wind) Wind and Solar Forecasting commercialisation strategy; Fulcrum3D CloudCAM Solar Forecasting for the NEM PDR 2021.03.12; Fulcrum3D Cloud Forecasting Lessons Learnt Report October 2020 FINAL; Fulcrum3D Cloud Forecasting Progress Report October 2020 Final; Fulcrum3DCloudCAM Wind Forecasting PDR 2021.03.21 Fulcrum3D Wind Forecasting April 2020 Progress Report accuracy assessment; and Fulcrum3D Wind Forecasting October 2020 Progress Report accuracy assessment. Meridian Energy Australia Pty Ltd Meridian 2018 Arp 163 ARENA KS - Progress Report #4; GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 30 Trial participants Knowledge sharing deliverables Meridian 2018 Arp 163 ARENA KS - Progress Report #3; Meridian 2018 ARP 163 ARENA KS - Lessons Learnt Report #3; Meridian 2018 ARP 163 ARENA KS - Progress Report #2 (STF) April 2020 Final; Meridian – PR IMC ARENA Solar Power Ensemble Forecaster Final Report Rev2 IMC - Progress Report (STF) April 2020 Trial participants Knowledge sharing deliverables Proa Analytics Proa Analytics End of Project Report Proa Analytics 2018ARP170 - Financial Analysis Report Proa Analytics 2018ARP170-Forecasting Technology Cost Benefit Analysis Public Proa - Lesson Learnt Report (STF) Oct 2020 draft Proa - Lesson Learnt Report (STF) Oct 2020 FINAL NOV ARENA KS - Progress Report (STF) April 2020 Proa Proa Analytics - PR Solar and Storage Modelling Pty Ltd MEA_ARENA 5MSF Milestone #4 Report_Final; End of Project report (Public)_Solcast revision; Financial Analysis report (ARENA) (1); Solcast ARENA KS-Progress Report (STF) Oct 2020 v1.1; ARENA KS - Progress Report (STF) Oct 2020 v1.1; Solcast Progress Report 2 Windlab Ltd Windlab End of Project Report 02.03.21; Progress Report April 2020. Windlab.08.04.20 GHD | Australian Renewable Energy Agency | 12537688 | Knowledge and findings generated from the short-term self-forecasting trial 31 ghd.com The Power of Commitment","libVersion":"0.3.1","langs":""}
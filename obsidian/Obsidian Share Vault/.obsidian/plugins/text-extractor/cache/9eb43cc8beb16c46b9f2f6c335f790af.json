{"path":"lit/lit_sources.backup/InfiniFlow24ndleHaystckLLMdeathRAG.pdf","text":"4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/14 LLM’s ability to find needles in a haystack signifies the death of RAG? InfiniFlow · Follow 9 min read · Apr 6, 2024 Since February 24th, there have been significant news in the field of AI. We were a bit behind in addressing these pieces of AI news, as we were busy working on the release of the AI-native database for RAG, Infinity, and our OCR and deep document understanding-based RAG solution RAGFlow. Today, we’d like to share with you our observation and in-depth interpretation of the latest trends in AI. The recent hot topic is undoubtedly Sora. Contrary to the mainstream opinion, we view Sora as a trigger for the rise of multimodal RAG. However, our focus today is on the latest news in the RAG landscape: the release of a series of multimodal + long-context LLMs, such as Claude 3, Gemini 1.5, and Moonshot, along with significant funding and growing buzz. These large models support extensive contexts spanning tens of thousands to millions of tokens, excelling in the “needle in a haystack” tests. This term refers to querying specific questions within long-context windows to test an LLM’s accuracy. While RAG facilitates this process, LLMs like Claude 3 and Gemini 1.5 didn’t fully use RAG for this purpose, while still achiving high accuracy. Unlike StreamLLM, which utilizes a rolling window for long-context attention but falls short in detail recall, these new models have shown promising results. We also experimented with these LLMs, and the results 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/14 are indeed impressive. They even offer precise answers from PDFs containing complex charts. This has sparked another round of debates about long-context LLMs and RAG. For example, Dr. Fu Yao from the University of Edinburgh categorically asserts: “RAG is dead.” This sparked objections from many RAG supporters. Dr. Fu Yao further summarizes and rebuts around these rebuttals: ¢ Yao Fu & @FrancisYAO Feb 18 \"Orin one sentence: the 10m context Kills RAG. Nice work Gemini On 14 QQ 102 thi 18K J & 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/14 | Yao Fu vos @Francis YAQ_ Over the last two days after my claim \"long context will replace RAG\", | have received quite a few criticisms (thanks and really appreciated!) and many of them stand a reasonable point. Here | have gathered the major counterargument, and try to address then one~-by-one (feels like a paper rebuttal): - RAG is cheap, long context is expensive. True, but remember, compared to LLM, BERT-small is also cheap, and n-gram is even cheaper, but they are not used today, because we want the model to be smart first, then makes smart models cheaper -- history of Al tells itis much easier to make smart models cheaper than making cheap model smart -- when it is cheap, it's never smart. - Long context can mix retrieval and reasoning during the whole decoding processing. RAG only does the retrieval at the very beginning. Typically, given a question, RAG retrieves the paragraphs that is related to the question, then generate. Long-context does the retrieval for every layer and every token. In many cases the model needs 10 do on-the-fly per-token interleaved retrieval and reasoning, and only knows what to retrieve after getting the results of the first reasoning step. Only long- context can do such cases. - RAG supports trillion level tokens, long-context is 1M. True, but there is a natural distribution of the input document, and | tend to believe most of the cases that requires retrieval is under million level. For example, imagine a layer working on a case whose input is related legal documents, or a student learning machine learning whose input are three ML books -- does not feel as long as 1B right? - RAG can be cached, long-context needs to re-enter the whole document. This is a common misunderstanding of long-context: there is something called KV cache, and you can also design sophisticated 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/14 Here’s a recap of his view points: 1. RAG supporters defend their model on cost, but it’s not difficult to reduce costs of LLMs. 2. Long-context LLMs are capable of precise recall and reasoning within the context window, while RAG is only capable of one. 3. RAG advocates argue that a million tokens is only about 1MB of text, whereas RAG can handle texts with unlimited tokens. However, in most cases, user queries do not exceed 1MB. 4. Performance. RAG advocates argue that RAG is capable of caching and that LLM cannot make full use of the context each time. But this is caching and memory hierarchy ML system working with kv cache. This is to say, you only read the input once, then all subsequent queries will reuse the kv cache. One may argue that kv cache is large -- ture, but don't worry, we LLM researchers will give you crazy kv cache compression algorithms just in time. - You also want to call a search engine, which is also retrieval. True, and in the short term, it will continue to be true. Yet there are crazy researchers whose imagination can be wild -- for example, why hot letting the language model directly attend to the entire google search index, i.e., let the model absorb the whole google. | mean, since you guys believe in AGI, why not? - Today's Gemini 1.5 1M context is slow. True, and definitely it needs to be faster. I'm optimistic on this ~~ it will definitely be much faster, and eventually as fast as RAG Let's see how things go, shall we? 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/14 incorrect. Indeed, LLM can also establish KV Cache to leverage previous context tokens to improve performance. We are not going to elaborate on the latter two points because these arguments by RAG supporters are untenable. Now it does seem that RAG may be just a temporary solution. Given the ongoing improvement in LLM’s performance in handling extensive contexts, especially in precise recall, RAG may not seem indispensable any more. But, if we look at this debate from a different perspective, we will easily settle on one conclusion: LLM’s ultimate aim is AGI, eventually replacing human functionalities such as learning, reasoning, and summarization. However, no matter how capable or competent a person is, working for a company still requires access to internal data and information systems, typically facilitated by a computer system. Let’s draw another analogy, in a diagram provided by OpenAI co-founder Andrej Karpathy, he likens LLM to a CPU of a computer, the context to computer memory, and databases like vector database as the hard drive. 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 6/14 Now, let’s delve a bit deeper into why RAG remains essential even when LLMs pass the “needle in a haystack” test with flying colors. When we talk about RAG, it actually encompasses the complete chain of data preparation, data writing, multiple recall, and the final re-ranking. The main challenges in this process stem from two key areas: First, how to handl complex and diverse data, including data of various formats and various complex elements like graphs. Applying a RAG solution without grasping the semantics of such data leads to semantic loss and will eventually fail. Second, how to conduct multiple recall. This involves integrating various search and recall methods as well as the corresponding results. And the major difficulty lies in LLM’s inability to “align and control”. To tackle the first technical challenge, the typical approach is to utilize intelligent document models for target detection on diverse, complex data to understand the overall document layout, and then process the layout results, as illustrated in the following diagram. Software 1.0 tools “classical computer” Calculator Python interpreter Terminal Disk File system {+ernbeddings}kL JF Peripheral devices 10 vidao audio CPU context window LLM Ethernet Browser Cther LLMs 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 7/14 In our experiments, we tested LLMs with long contexts and LLMs with multimodal capabilities. While the results were promising in some cases, they essentially worked in a similar manner: not integrating small specialized models into the RAG system, as we supposed, but instead, applying LLMs directly. This approach mirrors practices seen in academia. Recently, a leading LLM company introduced a dedicated OCR large model for handling all document understanding tasks. But our testing revealed that, in niche areas, its capability of document layout recognition performances still falls behind performances of those specialized small models. Similar trends are seen in the field of long-context LLMs as well, such as Google’s Gemini 1.5, which claims to support OCR. Yet, their OCR results often contain errors and do not match up to those specialized models. BelowN \\med—Jifwal We HiHn \"ml Hii| neuwambwl 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 8/14 shows the performance of the long-context LLM moonshot in a visual test. The image below shows the original table: The following figure shows the html table converted by LLM: 3. Family net worth, by selected characteristics of families, 1989, 1992, and 1995 ‘Thousands of 1995 dollars except ns noted 1959 1992 19495 Family Ch Percent Perceniage Percenl characionatic Median Mean | Modan Mean of Median Mean or Tamilles families families Al familleg le 55.5 24.7 180.0 X23 2084 10D 4 100.0 {r.a.) (oa) (32) (15.6) (3.3) (14.07 facotie [7995 dadlars)! Less than 10000 ........cc00 ee 1.6 20.1 154 33 30.0 155 4.8 45.6 [Th] LO 000-24 S99 25.6 To 4.3 28.2 T12 27.8 30.0 4.4 20.5 3 (K-49 009 55.0 1218 30.1 HE 124.4 29.5 1.9 1193 31.1 S0 000-55 128.1 2295 123 121.2 240.8 20.0 121.1 255.0 20.2 LOGO (0 and more 4747 13729 1.7 506.1 1283.6 7.1 485.9 1445.2 G.1 Age of head (years) Leas then 35 92 64.3 272 10.1 303 a3 8 ILA 41.2 242 35-44 69.2 1713 234 350 F44.3 225 $8.5 144.5 23.3 45-54 L140 135.9 144 $14 27.8 Li a).5 rIkE 1738 me SR L105 A344 135 1225 356.6 132 110.8 355.2 125 ar 85.4 136.8 120 105.8 HIE3 125 104.1 331.6 119 TS and MOE oo .ceeieiuinsna cans 83.2 250.8 2.0 e283 E510 4 a5. 276.0 25 Education of head No high school diploma reararara 283 02.1 24.1 21.6 Tad 204 25.3 g1.2 12.0 High s=hool diploma I 434 1344 zl 31.4 120.6 2059 30.0 136.2 31.6 Some college ...oooveeooao...| 364 2155 15.1 G26 145.4 177 432 186.6 194) College degree ..ovviiviniinness 132.1 4169 28.3 102.1 Hd.3 35 1064.1 Ja18 35 Race or ethnicity of head White non-Hispanic ............. 84.7 2014 75.1 TL? 231.8 715.1 Ti8 244.0 115 Monwhite or Hispanic arerrarara 63 $2.1 24.5 15.9 14 24.9 15.5 Td 4 22.5 Cwrrent work sialus of head Professional, nal ... LOMG.0 262.7 16.9 TEE 24R.5 163 89.3 2528 15.9 Technical, sales, clerical| 40.9 28.0 134 480 103.4 14.3 41.3 1023 3.9 Precision lion . 58.4 od.2 945 384 85.5 70 43.5 193 82 Machine operalors andlaborezs 23.1 67.2 105 215 a5 100 KY iar 13.1 SETVICE occupations CeretEraasen 03 = 6.5 15.7 329 62 158 oi0 6.6 Self-employed 200.7 TH5 4 12 155.4 Gud 3 104 1524 T3515 07 TIS 199.2 pg To3 201.2 260 BLE 218.3 2510 ria not working .. 0.? 62.9 6.7 55 ER.5 8.2 4.5 6.4 6.5 Housing stairs F oravnrmtmrrararararanarara 119.9 Jil. 7 53.3 165.1 psd 53 5 1023 295.4 ted7 Retiberof Or vveininrerninnns 2 494 36.2 3.45 42.1 36.1 4.5 42.2 353 NOTE, Sea note to table I. IL. For tha calendar yaar preceding the survey, 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 9/14 Flease convert the text about family net worth by selected characteristics of families on page 6 of the pdf into the table shown In the dialogue. 5cf95-bull01972 dl DF 99.37 KB Cartalnly! The taxt from page & of the PDF regarding family net worth by selected characteristics of familles has bean converted Into a table format below: Family characteristic 1989 (In thousands of 1995 dollars) 1992 1995 | All families | 56.5 h2.8 56.4 Income {1995 dollars) . | | Lass than 10,000 } | 1.6 26 4.8 10,000—24,999 ez |5a4 256 | 265,000—49,999 28.5 136.2 56.9 50,000—99,99% | 84.7 oo oo | 106.6 262.7 | 100,000 and more | | 261.4 311.7 1023.6 | Age of head (years) | oo | Less than 35 | 5.0 | 7.7 13.2 | 35—44 | | 4.0 ) | 5.2 85 45—54 oo 24 | 4.5 1.9 hh—64 0.7 6.8 6.1 oo 65—74 lo7 | 5.0 h.b 75 and more 0.6 4.5 66 Education of head | | No high school diploma oo YS 15.9 23.5 High school dipioma | 22.3 oo 34.7 57.1 | | Some coilege oo 43.5 | | 66.3 110.5 College degree oo 106.6 | 156.6 644.3 [E— + - - [= Race or ethnicity of haad White non-Hispanic 84.7 128.1 485.9 A Nonwhite or Hispanic n.7 92.8 | 152.9 Current work status of head | } ( Professional, managerial 106.6 155.6 485.9 | 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 10/14 We can see that, although many numbers and cells have been recognized, there are still many mismatches in the overall layout. Such data can be easily identified by some specialized document intelligence models, and they can even run very fast on CPUs. For example, we recently developed a document intelligence model optimized for CPUs and mobile devices based on the traditional visual object detection algorithm YOLO. Not only does it have high recognition accuracy, but it can also run on any desktop machine, significantly reducing the overall implementation costs. Therefore, at present, a perfect model that can understand various complex documents, whether large or small, does not exist. Therefore, it is sensationalist to say that a certain technology will become obsolete. It is important to note that YOLO was proposed nearly a decade ago, and its backbone network still relies on the traditional CNN rather than Transformer. We have just open sourced our work in this to benefit companies with our RAG solutions and to gradually unlock the use of LLMs in enterprise scenarios. The above is just a very simple example. Currently, the use of RAG in enterprises is still in its infancy, so many application scenarios have not been unlocked. For example, user data may be way different from Technical, sales, clerical | 66.3 90.5 363.3 Precision production , 16.9 16.8 B2.2 Machine pperators and laborers 14.4 | oo 23.4 | 37.3 Service occupations | 9.0 | oo 17.3 | 49.6 Self-employed | 25.0 | 38.6 | he. 7 Ratirad | 20.0 26.0 77.4 Other not working oo 9.7 | 9.8 54.2 Please note that the values have bean adjusted to 19495 dollars using the consumer prica Index (CPI) for all urban consumers. The table doss not include the \"Percentaga of families\" row from the original text, as it was a header for the \"Medlan” and “Mean” columns, which are not applicable In this format. 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 11/14 document-type unstructured data. Future enterprise user data will likely be integrated with various internal information systems, synchronized with large amounts of data from databases and logs, and processed before being sent to the LLM for the final answers. In these scenarios, the LLM’s contextual window can only be used as “computer memory”, not used as “hard drive”. Therefore, Dr. Fu Yao’s view is more focused on the simplest RAG scenario — the knowledge base, not from a perspective for the entire industry. Saying that 1M context tokens may be enough for personal knowledge bases is just like what Bill Gates said decades ago about 640K of memory being enough. So, will the memory of LLMs increase indefinitely to the point where we no longer need a hard drive? This is hard to achieve from a theoretical perspective. Honestly, achieving long contexts is not a big deal technically: a necessary technology is rotation position encoding (RoPE). Here’s one straightforward solution to expand the context window: suppose your training window is 2000 in size, and you wish to expand it to 5000 during inference, then a position at 3000, which was previously impossible to effectively detect or predict, can be detected through interpolation, by coding to a position at 1200 (3000 * 2000/5000). The disadvantage to this approach is that positions previously in the range from 0 to 1000 are also compressed to a smaller range, leading to decreased ‘discernibility’. Rotation position encoding, represented by RoPE, is an attempt to mitigate the decrease in resolution. In real-world applications, increases in context window will significantly increase the overhead of KV caches in Transformers. Besides, because these KV caches have very high bandwidth requirements, it becomes much difficult to using normal RAM or even database systems to serve them, thereby limiting the context window of LLMs by hardware systems. Then, why not use the much more cost-effective RAG to achieve equivalent or even greater results? 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 12/14 Now, Let’s move on to the second challenge of RAG. Without multiple recall, RAG will certain fail. The remarkable needle-in-a-haystack capability of LLMs actually simplifies the implementation of RAG. RAG, like a computer’s hard drive, is bound to hold complex business and data. The various multiple recall approaches at present are essentially temporary solutions to compensate for LLMs. Once LLMs outgrow these constraints, the integration of RAG will become more straightforward. For example, RAG developers no longer need to worry about fine tuning their text chunking strategies; instead, they can broadly recall text near potential answers and leave the final recall and reasoning tasks to the LLM. However, with the current tech stack, achieving this level of efficiency is still a distant goal. RAG fundamentally refers to an entire industry rather than just an application. Many potential applications of RAG remain untapped, primarily due to limitations of RAG’s databases and the LLM itself. Addressing the database constraints is a priority, as we aim to enable diverse data, including documents, to interact within the RAG system centered around LLM. In this framework, the database’s role is to efficiently deliver all relevant data to the LLM in a prompt way. Here are some typical scenarios: Screening resumes based on predefined criteria such as job skills, work experience, expected income, education level, and region, which requires both vector and full-text search methods. Recommending products based on personal preferences in a dialogue. Multiple vectors can be used to describe individual preferences, where each specific column represents a user’s past usage preferences for a specific product category. In the recommendation process, in addition to using the user’s preference vector for searching, it is also necessary to combine product filtering: including expiration status, coupon availability, compliance requirements, recent purchases or readings by 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 13/14 the user, and more. Representing this information only through so-called “scalar” fields would make product development extremely complex, requiring additional ETL, leading to maintenance issues and other more severe data consistency issues. It’s important to note that RAG faces end- user scenarios where it needs to respond to business and LLM requests immediately, so it cannot be built like a data middle platform, which builds a complete set of data pipeline systems to create wide tables just in order to deliver a report! Providing reasons for recent anomalies in certain key performance indicators from monitoring logs. The relevant historical cases of these metrics are stored in the corresponding knowledge base. Offering explanations for recent anomalies in key performance indicators by referencing historical cases stored in the knowledge base. It is crucial to recognise that RAG must respond swiftly to both business and LLM requests in real-time scenarios, requiring a dynamic approach rather than being a static data platform solely focused on generating reports or creating complete data pipeline systems. There can be many more such examples, and the challenges encountered by businesses are even more intricate than those mentioned. With such complex multiple recall capabilities, the needed data for LLM can flow continuously in, and the final bottleneck lies with the LLM itself. Nowadays, we see a series of LLMs with long contexts evolving rapidly, which is a pleasant surprise rather than a scare. As mentioned earlier, we are not worried that this will replace the existence of RAG. Instead, we see that the biggest pain points faced by RAG are gradually being addressed on the path we expected: besides providing basic summarization capabilities, LLMs with long contexts can also offer some reasoning capabilities. This undoubtedly 4/17/24, 8:14 AM LLM’s ability to find needles in a haystack signifies the death of RAG? | by InfiniFlow | Apr, 2024 | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 14/14 accelerates the enterprise adoption of RAG, moving it beyond being just an engine for simple knowledge bases. Finally, you are very welcome to star and follow the AI-native database for RAG Infinity and our OCR and deep document understanding-based RAG engine solution RAGFlow.","libVersion":"0.3.2","langs":""}
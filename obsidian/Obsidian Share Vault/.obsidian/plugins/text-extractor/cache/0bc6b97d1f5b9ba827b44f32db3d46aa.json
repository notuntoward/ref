{"path":"lit/lit_notes_OLD_PARTIAL/Jaimungal09KernCopulaPhD.pdf","text":"Kernel-Based Copula Processes Sebastian Jaimungal 1 and Eddie K.H. Ng2 1Department of Statistics, University of Toronto sebastian.jaimungal@utoronto.ca 2The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto eddie@psi.toronto.edu Abstract. Kernel-based Copula Processes (KCPs), a new versatile tool for analyzing multiple time-series, are proposed here as a unifying frame- work to model the interdependency across multiple time-series and the long-range dependency within an individual time-series. KCPs build on the celebrated theory of copula which allows for the modeling of complex interdependence structure, while leveraging the power of kernel methods for eﬃcient learning and parsimonious model speciﬁcation. Speciﬁcally, KCPs can be viewed as a generalization of the Gaussian processes en- abling non-Gaussian predictions to be made. Such non-Gaussian features are extremely important in a variety of application areas. As one appli- cation, we consider temperature series from weather stations across the US. Not only are KCPs found to have modeled the heteroskedasticity of the individual temperature changes well, the KCPs also successfully discovered the interdependencies among diﬀerent stations. Such results are beneﬁcial for weather derivatives trading and risk management, for example. Keywords: Copula, Kernel Methods, Gaussian Processes, Time-Series Analysis, Heteroskedasticity, Maximum Likelihood Estimation, Financial Derivatives, Risk Management. 1 Introduction Gaussian processes (GPs) [3] have enjoyed wide acceptance in many machine learning applications as a non-linear regression and classiﬁcation tool. GPs clev- erly exploited the closure property of Gaussian random variables, under marginal- ization and conditioning, for eﬃcient learning and inference. Model selection can be handled naturally under a Bayesian framework, and missing data can be treated eﬀortlessly. The same closure property also limits the GPs to make predictions with Gaussian distributions. However, the world is an inherently non-Gaussian place. The Gaussian prior assumption is simply not valid in many real-world appli- cations such as the modeling of ﬁnancial data[12], wind-speeds and temperatures, and catastrophic damages (e.g. ﬂoods, earth-quakes and forest ﬁres[6] [8]) where it has been shown that risk-management based on Gaussian assumptions severely under-estimates the real-risks and greatly inﬂates the social and economical costs. The ability to accommodate non-Gaussian features is therefore crucial. W. Buntine et al. (Eds.): ECML PKDD 2009, Part I, LNAI 5781, pp. 628–643, 2009. c⃝ Springer-Verlag Berlin Heidelberg 2009 Kernel-Based Copula Processes 629 Copula theory [13] on the other hand has been proven to be an eﬀective way to model complex dependencies and distributional behavior by decomposing the joint-distribution into the dependency structure and the individual marginal distributions. However, the applications of copula functions are mostly static while its dynamic extensions are either limited to ﬁrst-order Markov processes [16] or simply using copula functions as a way to combine multiple time-series [5][11][14][15]. This work proposes a novel time-series analysis model dubbed Kernel-based Copula Processes (KCPs), and oﬀers a way to better address the non-Gaussian features of real-world data. KCPs are a powerful union of GP and copula func- tions. Unlike GPs where a time-series is modeled as a high-dimensional Gaussian distribtuion, KCPs use a copula function to separate the speciﬁcation of depen- dency structure and the behavior in the marginal distributions; thus allowing for complex non-Gaussian behavior, while retaining most of the attractiveness of GPs such as eﬃcient learning, inference, and natural missing data handling. For example, in the univariate setting, a high-dimensional elliptical copula func- tion is used to capture any long-range temporal dependencies, while an arbitrary non-Gaussian distribution is used to match the non-Gaussian behavior in the marginal distribution. A kernel function is used along with the elliptical copula to keep the model parsimonious. As will be shown in this work, heteroskedastic processes can also be captured with KCPs through proper design of this kernel function. Under this proposed framework, we further propose a natural extension for multi-variate time-series which allows multiple time-series with drastically dif- ferent characteristics to be described under a single model. Further, the modular formulation of multivariate KCPs lends itself naturally to a multi-core/processor environment for parallel computation. Section 2 presents the details of the KCPs framework in both univariate and multivariate settings. Section 3 presents a synthetic univariate example to il- lustrate the modeling power enjoyed by the non-Gaussian features of KCPs. Finally, section 4 presents a real-life example of modeling multiple temperature series from weather stations across the United States. A new kernel function is derived especially to accomodate the heteroskedastic nature of the temperature data. The performance of diﬀerent ﬂavors of KCPs are compared with GPs and GARCH [2]. 2 Kernel-Based Copula Processes (KCPs) This section introduces the basic formulation of copula processes for time-series analysis. Section 2.1 provides the formulation for the univariate time-series case which shows how kernel methods can bring a sense of time or parametric ordering to a high-dimensional copula, thus allowing the modeling of random processes instead of just a collection of random variables. This approach is reminiscent of the formulation of Gaussian processes (GP) as will be discussed in this section. Section 2.2 extends the basic formulation to the multiple time-series case. Section 2.3 presents the estimation methods for KCPs. 630 S. Jaimungal and E.K.H. Ng 2.1 Univariate Time-Series Consider a one-dimensional function g(x) with a set of noisy observations y = {y1, ..., yN }T = g(x)+ εn at x = {x1, ..., xN }T . In performing tasks such as regression or forecasting, the objective is to predict the values of the function, y∗ = {y∗ 1, ..., y∗ N }T ,at x ∗ = {x ∗ 1, ..., x ∗ M }T where x ⋂ x ∗ = ∅. Under the KCP assumption, the joint distribution of the observed and predicted values is given by:\u0000(Y ≤ y, Y∗ ≤ y∗|x, x ∗)= C({Fi(yi)}N i=1, {Fj(y∗ j )}M j=1|x, x ∗)(1) = C({ui}N i=1, {u∗ j }M j=1|x, x ∗)(2) where C(·) is the copula function 1 of the process, Fi(·) are the marginal cumula- tive distribution functions (CDFs), and ui is the transformed yi via the CDFs, i.e. ui . = Fi(yi). The joint density of (y, y∗) can be obtained by diﬀerentiating with respect to {y, y∗}:\u0000(y, y∗|x, x ∗)= c({Fi(yi)}N i=1, {Fj(y∗ j )}M j=1|x, x ∗) N∏ i=1 fi(yi) M∏ j=1 fj(y∗ j )(3) where fi(·) are the marginal PDFs and c(·) is the so-called copula density function given by the derivative of the copula function with respect to its parameters: c({ui}N i=1)= ∂N C ∂u1∂u2 ...∂uN (4) In this formulation, all the marginal distributions are constrained to have the same parametric form. The role of the copula function C is to capture the entire dependency structure of {y, y∗}. There exists a vast array of parametric or em- pirical copula functions which allow the speciﬁcation of complex and heavy-tail dependencies. Nelson [13] provides a wide collection of examples. However, when attention is focused on the elliptical class of copula functions, the dependency structure in the copula function can be speciﬁed by a kernel function to achieve parsimonious model speciﬁcation. For example, consider a copula process with a Gaussian copula function as follows: CΛ({Fi(yi)}N i=1)= ΦΛ({Φ −1(Fi(yi))}N i=1)(5) 1 The copula function is a joint distribution function of uniform random variables. Sklar’s theorem [13] states that given any joint distribution H(y1, ··· ,yN ) of random variables {yi} and margins {Fi(yi)}. There exists a copula function C such that for all yi in the respective domain, H(y1, ·· · ,yN )= C (F1(y1), ··· ,FN (yN )). If {Fi(yi)} are continuous, then C is unique. Theconverseisalso true. Kernel-Based Copula Processes 631 where the Φ(·)and ΦΛ(·) are the standardized univariate normal CDF and the zero-mean multi-variate CDF (with covariance matrix Λ) respectively. The co- variance matrix Λ can be deﬁned with the Gram matrix: Λ = ⎡ ⎢ ⎣ k(x1,x1) ... k(x1,xN ) ... . . . ... k(xN ,x1) ... k(xN ,xN ) ⎤ ⎥ ⎦ + σ2 nI (6) where σ2 n is the noise variance and k(xi,xj) is a kernel function describing the covariance2 between the pairs of random variables yi and yj. The radial basis function (RBF), kRBF (xi,xj)= exp (− 1 2h (xi − xj ) 2) (7) and the Ornstein-Uhlenbeck functions, kOU (xi,xj)= exp ∣ ∣ ∣ ∣− 1 h (xi − xj) ∣ ∣ ∣ ∣ (8) are examples of commonly used and versatile kernel functions. More examples of kernel functions can be found in [3]. In the Appendix, we show how to construct a nonstationary kernel function from a stochastic diﬀerential equation (SDE) based on the understanding of the dynamics of the temperature data studied in Section 4. Notice that if the copula is Gaussian and the marginal distributions are as- sumed to be normal, then the copula process reduces to the familiar GPs. 2.2 Multivariate Time-Series In practical problems, random processes rarely exist in logical isolation but rather typically form an intricate web of dependencies. Thus practitioners are often faced with the challenge of analyzing not a univariate time-series, but multiple codependent time-series. To this end, we propose a multivariate KCP framework to capture such complex interdependencies. In the cases where the daily values from each series are highly related and yet there exists no ordering among them, one natural parametrization of the joint distribution is as follows:\u0000(Y(1) ≤ y(1),..., Y(M) ≤ y(M))= Cb [ C1 ( {F (1) i (y(1) i )}N1 i=1) ,..., CM ({F (M) i (y(M) i )}NM i=1 )] (9) where Cb(·)is the binding copula function connecting M individual time-series together. The individual time-series are modeled as in the single KCP case 2 For non-Gaussian random variables, the kernel function is not the covariance itself, but rather it dictates the behavior of the covariance. 632 S. Jaimungal and E.K.H. Ng Fig. 1. A schematic illustration of a binding copula linking multiple KCPs. In this case, there are four time-series, each with ﬁve time samples. The temporal dependencies of each time-series are ﬁrst described by the individual KCPs {Ci} 4 i=1. The inter-time- series dependency is then described by a binding copula Cb. (Section 2.1), while the dependent structure resides with the binding copula. Figure 2.2 provides a schematic depiction of a binding copula joining multiple copula processes together. The advantage of this conﬁguration is that time-series with greatly diﬀerent individual dynamics can be captured by the individual copula processes, before they are joined together by the binding copula. For example, one time-series can be the changes in the LIBOR3 rate while others could be the returns of any stocks that are sensitive to interest-rates, such as utilities companies or companies with high debt load. In such cases, the stock returns would have much higher volatility but shorter term correlation than the changes in the LIBOR rate. 2.3 Learning The learning of KCPs can be performed by maximizing likelihood. The likelihood function of a generic KCP is given by: L(θ)=\u0000(y|x,θ)= c({Fi(yi)}N i=1) N∏ i=1 fi(yi) (10) 3 The London Interbank Oﬀered rate (LIBOR) is a daily reference rate based on the interest rates at which banks oﬀer to lend to other banks [10]. Kernel-Based Copula Processes 633 where y is the set of training data and θ is the set of hyper-parameters of the copula process. For instance, assuming the choice of Gaussian copula and margins are Stu- dent’s t -distributed, then the negative log-likelihood function is given as − log(L(θ)) = 1 2 log |Λ| + 1 2 z T Λz − 1 2 z T z − N∑ i=1 log(t(yi,υ)) (11) where z T = {z1 = Φ −1(Tυ(y1)),...,zN = Φ −1(Tυ(yN ))} is the transformed random vector zi ∼N (0, 1), and Tυ(yi)and ti(yi,υ) are the univariate Student’s t -distribution and density functions (with degree of freedom υ) respectively; and θ are the hyper-parameters of the kernel function. Here, without loss of generality the time-series samples are assumed to be standardized. The likelihood function for multivariate KCPs can be derived by taking the N · M − th order derivative of Eq. (9) with respect to y(j) i : L(θ)=\u0000(y(1),... , y(M)) = ∂M Cb ∂C1 ...∂CM · ⎡ ⎣ ∂N1C1 ∂u(1) 1 ... ∂u(1) N1 N1∏ j=1 f (1) j (y) ⎤ ⎦ · ... · ⎡ ⎣ ∂NM CM ∂u(M) 1 ... ∂u(M) NM NM∏ j=1 f (M) j (y) ⎤ ⎦ . = cb ·L1(θ1) · ... ·LM (θM ) (12) The overall likelihood function factorizes into the product of the binding copula density, cb, and the likelihood functions of the copula processes of the individ- ual time-series, Li(θi). This modular property allows for two ways to learn the model parameters: (i) all the parameters are learnt at the same time; or (ii) the parameters for each time-series are learnt separately and are ﬁxed while learning the parameters of the binding copula. The latter approach is particular conve- nient for equity portfolio analysis where individual names are often added and removed as portfolio compositions change. 2.4 Inference One of the strengths for KCPs (which is also enjoyed by GPs) is that the entire predictive distribution is available during inference, thus predictions are not lim- ited to point prediction, which allows for much more accurate risk management. The predictive distribution of a univariate KCP is given by:\u0000(y∗|y)= C ˜Λ({Fi(yi)}N i=1, {Fi(y∗ j )}M j=1) CΛ({Fi(yi)}N i=1) (13) 634 S. Jaimungal and E.K.H. Ng where Λ and ˜Λ are the Gram matrices of the observations and the combined ob- servations and targets respectively as deﬁned in Eq. 6. The dependency on x and x ∗ is implicit and it is dropped from the notation for clarity of presentation. The maximum a-posterior (MAP) estimator could be used to provide a point estimate of the target values: ¯y∗ =max y∗\u0000(y∗|y) (14) When the choice of copula functions and marginal distributions result in a sym- metric predictive distribution, the MAP estimate will coincide with the mean- squared estimator\u0002|\u0000(y∗|y)[y]. 2.5 Model Selection and Performance Metrics A model selection method is required to select the best of parametric func- tions for both the copula and marginal component in KCPs. Given the mod- ularity of multivariate KCPs, model selection can be performed in a modular fashion as well. That is, the best univariate KCP is selected for the individ- ual time series and the associated results are subsequently used to learn the best binding copula. Furthermore, in regression and out-of-sample prediction of stochastic processes, attention must be focused on obtaining an accurate pre- dictive distribution. Point-predictions are meaningless since the realized value from a stochastic process will almost always diﬀer from the prediction. Know- ing the entire predictive distribution on the other hand yields a much more complete picture. To this end, conventional performance metrics such as least- square errors between the point prediction and a set of realized values is of little use. In this study, in addition to the maximized likelihood, the probability-integral- transform (PIT) [7] is employed as a complementary criteria for model selec- tion. The maximized likelihood relates to the predicted density having mini- mum variance about the observations[4], while PIT measures how well the the predictive distribution corresponds to the true underlying distribution. Diebold et al. [7] evaluate PIT graphically, but it can also be evaluated quantitatively with the Anderson-Darling criterion 4 [1]. We will use log-likelihood and like- lihood ratios to rank the competing models, while using the PIT with the Anderson-Darling criterion to evaluate the validity of the model on a standalone basis. In evaluating the goodness-of-ﬁt in the multiple time-series level, a visual comparison of the Gram and pair-wise Pearson’s correlation matrices will also be given. 4 The Anderson-Darling criterion has the added advantage that it produces a score that focuses goodness-of-ﬁt at the tail regions of the distributions. Kernel-Based Copula Processes 635 Fig. 2. A synthetic data set generated by a AR(2) process with skew-normal innova- tions is shown on the left panel. The posterior distribution of the process at t =5 and the corresponding predictive distributions given by GP and GCTM KCP are shown on the right panel. 3Synthetic Example In this section a synthetic data set is used to illustrate the KCP in action in its simplest form. Here, a sample path is drawn from an AR(2) process with skew-normal 5 innovations: yt =0.1yt−1 +0.05yt−1 +1+ εt (16) where εt is a standardized skew-normal with skewness of 0.5. One hundred samples were taken at regular intervals within the range of x = [−5, 5]. Every ﬁfth sample is taken as the test set (i.e. the regression / prediction targets), while the rest of the samples are used as the training set. Then two KCPs: Gaussian copula with Gaussian marginal distribution (GCGM) (or simply a Gaussian process (GP)) and Gaussian copula with t -marginal (GCTM), are learnt using MLE. The Ornstein-Uhlenbeck kernel (8) is used in both cases due to its auto-regressive nature. 5 The skew-normal distribution can be obtained by taking the limit of the skew- t distribution by Hansen[9] as the degree of freedom η approaches inﬁnity. The full formulation of the standardized skew-t distribution is reproduced here for reference: g(z|η, λ)= ⎧ ⎪⎪⎨ ⎪⎪⎩ bc (1+ 1 η−2 ( bz+a 1−λ )2)−(η+1)/2 z< − a b bc (1+ 1 η−2 ( bz+a 1+λ )2)−(η+1)/2 z ≥− a b (15) where 2 <η < ∞,and |λ| < 1. The constants a, b,and c are given by a =4λc ( η−2 η−1 ), b2 =1 + 3λ2 − a2,and c = Γ ( η+1 2 )√π(η−2)Γ ( η 2 ) . 636 S. Jaimungal and E.K.H. Ng To illustrate the superior modeling power of KCPs over GPs, we consider the predictive distribution generated by a GP and a GCTM KCP in Fig. 2. This experiment is a simple example to illustrate the power and ﬂexibility of copula processes simply by changing the marginal distributions from Gaussian to Student’s t. Recall a GP is a special case of the GCTM KCP as the degree of freedom parameter υ →∞. The two particular types of KCPs were chosen to illustrate what a slight departure from GPs framework can achieve. As depicted, the predictive distribution produced by GCTM KCP is much closer to the true posterior distribution of the AR(2) process than that produced by GP. GP, restricted by its symmetric nature, must over shift its mean to the positive side to compensate for the excess probability mass in that region. This would introduce even greater prediction errors if point predictions are considered. GCTM KCP on the other hand is able to produce an asymmetric predictive distribution based on observed data to much better match the true distribution. 4 Real-Life Application 4.1 Data To showcase the modeling power of KCPs with multiple time-series, we consider the daily maximum temperatures recorded by the United States Historical Cli- matology Network (HCN) [?]. Data from as far back as the early 1800s from a network of 1219 weather stations are freely downloadable from the Network’s website. This study will consider the data from a few arbitrary three-year pe- riods from 156 stations of various states. Missing data is commonplace in such data as equipment is moved and maintained; however, KCPs handle missing data naturally. The latitude/longtitude and elevation coordinates for each weather station are also known. We will later use this information to help learn the dependency structure. To focus on the stochastic nature of the data, we ﬁrst subtract a sinusoidal seasonal trend from that data. A customized sinusoidal function is used for each weather station based on the least-square error criteria. An example of the temperature data with the ﬁtted sinusoidal trend is given in Fig. 3a with the detrended version in Fig. 3b. 4.2 Model Since there exists no natural ordering among the weather stations, we will take advantage of the modularity of the KCPs and ﬁrst learn a univariate KCP for each weather station and connect them with a binding copula. Upon closer examination of the detrended data, some notable features are observed which helped narrow down the modeling choices. First, the second moment autocorrelation function (ACF), i.e. the ACF of the square of the data, experiences an annual cycle as illustrated in Fig. 3c. This im- plies the rate of ﬂuctuation, or volatility, of maximum temperature goes through Kernel-Based Copula Processes 637 Fig. 3. Weather station TX410120 of Texas. Panel (a) depicts the raw maximum tem- perature data and the sinusoidal trend; Panel (b) shows the detrended data; Panel (c) shows the auto-correlation function of the ﬁrst four moments of the detrended data; Panel (d) shows the empirical density and the estimated skew-normal distribution. The second moment ACF in (c) and the detrended temperature in (b) clearly shows the periodic volatility of the residuals. an annual cycle. On the other hand, the ﬁrst moment ACF drops oﬀ relatively quickly (in a few days time), but this also implies recent temperature trends tend to persist. This provides an important hint in designing or selecting the type of kernel function that would be appropriate for the analysis. In this case, we designed a heteroskedastic kernel function that satisﬁes the above features: k(t1,t2)= σ2 · e−κΔt [ 1 2κ + α 4κ2 + T −2 ( 2κ · sin ( min(t1,t2) T + φ ) − 1 T cos ( min(t1,t2) T + φ ))] (17) where σ is roughly the average volatility, κ is the inverse of length scale of dependency in days, T is the period (= 1 year) and φ is the phase of volatility. Please see the Appendix for the derivation. Note the kernel function is non- stationary as the volatility of maximum temperature depends on the day of the year. 638 S. Jaimungal and E.K.H. Ng Another helpful observed feature in the detrended data, is that a skewed-normal ﬁts the empirical histogram well. Figure 3d shows a sample empirical histogram and the best-ﬁt skew-normal distribution. Although the histograms are not strictly valid 6, it helps us narrow down the choice of marginal distributions. It is important to note that the above analysis is not necessary for using KCPs. However, it is a simple way to incorporate additional domain knowledge by providing the initial values of the parameters 7 forMLE and asabasisfor forming the prior distribution for Bayesian learning. For this study, ﬁve competing models for the univariate time-series were in- vestigated. These are (i) GP, (ii) Gaussian copula with skew-normal marginal (GCSM) KCP, (iii) t -copula with Gaussian marginal (TCGM) KCP, (iv) t - copula with skew-normal marginal (TCSM) KCP, and (v) GARCH(1,1) (with t -innovations) model. The GARCH(1,1) model is included here because it is one of the best-known models for heteroskedastic processes [2][10]. All KCPs above use the heteroskedastic kernel function (17). The Gaussian- and t -copula are used as a binding copula to describe the interdependencies among the temperature series. In this case the Gaussian RBF kernel (7) is used, with the weighted distance metric: d(i, j) 2 = d 2 lat + d 2 long + w · d 2 elev (18) where d(i, j) is the weighted distance between weather station i and j, dx are distances along latitude, longitude, and elevation, and w is the weight on the distance along elevation. 4.3 Results First we examine the results from the univariate KCPs. Figure 4 provides a sample comparison of the detrended data from weather station MD181750 of Maryland. The ﬁrst column of panels from left to right show the time series, the correspond- ing ACF of the second moment, the histograms, and the q-q plots. The ﬁrst row shows the plots of the detrended data while the subsequent rows show the samples generated from the corresponding models after learning on the same set of the de- trended data. All KCP models work relatively well in the sense that the annual periodic change in volatilities was recovered as evident in both the time-domain plots and the ACF plots. The GARCH(1,1) model picked up some periodicity in 6 Histograms are conventionally used to visually evaluate the probability density of a random variables. A random process on the other hand is a collection of random vari- ables. Thus a sample path of a time-series (which is a random process), represents a series of draws from a potentially diﬀerent distribution. Therefore, collapsing all the samples from a time-series to form a histogram is axiomatically incorrect. 7 The skewness parameter can be obtained by the maximum likelihood ﬁt of his- tograms of the residuals as depicted in 3d and the length scale parameter κ in the kernel function (17) can be estimated by ˆκ = − log(ρ|τ =1)where ρ(τ )denotes the ACF for the ﬁrst moment at lag τ . Kernel-Based Copula Processes 639 Fig. 4. Comparing stylized facts of the detrended data and sample paths generated by models with MLE parameters based on the weather station MD181750 of Maryland Fig. 5. Univariate model selection results. The left panel shows the fraction of model acceptance/rejection based on the Anderson-Darling criterion; the middle and right panels show the fractions of each models ranked to be the most and the least appropri- ate model for the univariate temperature series according to the achieved log-likelihood. volatility as well, but the period was incorrect. Examining the histograms and the q-q plots visually, the TCSM KCP seems the best match with the data. More generally, using the data from 156 weather stations across the US, the Anderson-Darling test rejects the hypothesis that GP and TCGM KCP are suf- ﬁciently good models (with 95% conﬁdence level) in ∼43% and ∼42% of the cases; whereas, the TCSM KCP was shown to be the best model as it was re- jected in only ∼7.5% of the cases. The acceptance and rejection frequencies for 640 S. Jaimungal and E.K.H. Ng Table 1. Medians of pair-wise likelihood ratios among diﬀerent models and the corre- sponding critical values for the likelihood ratio tests at 95% conﬁdence level. Note that only the upper triangular part of this table is shown as the table is anti-symmetric. pGP pGCSM pTCGM pTCSM pGARCH qGP - 5.41 (3.84) dB 0.0 (3.84) dB 6.05 (5.99) dB 4.6 (5.99) dB qGCSM - - -5.6 (0.00) dB 0.0 (3.84) dB -4.6 (3.84) dB qTCGM - - - 5.8 (3.84) dB 5.1 (3.84) dB qTCSM - - - - -5.0 (0.00) dB qGARCH - - - - - all models are shown in the left panel of Fig. 5. The middle and the right panels of Fig. 5 show the frequency of each model being the best and the worst model for each data set as ranked by the maximized likelihoods. The GCSM and TCSM KCPs clearly dominate the GP and TCGM KCP, demonstrating that most of the modeling power (for this data set) comes from the skewness of the marginal distributions while the extra tail-dependency from the t -copula provides an in- cremental gain. The performance of the GARCH(1,1) model with t -innovations seems to be hit-and-miss with this data set, yet it is quite impressive that it ranks ﬁrst in about 22% of the time, even though the KCPs use a kernel function that is custom designed for this data set. However, it is important to note that only the maximized likelihoods are used to rank models here. Model complexity (i.e. number of parameters in each model) is not considered. The likelihood ratio test that follows will address this issue by using a signiﬁcance threshold which depends on the number of model parameters. Table 1 lists the median pair-wise likelihood ratios computed across the data from 156 weather stations for each model-pair, where px qy denotes the likelihood ratios of model x over model y. The corresponding critical values for the likeli- hood ratio tests at a 95% conﬁdence level are also listed in parentheses. Note, if a particular pair-wise likelihood ratio px qy is greater than the corresponding crit- ical value, then model x is preferred over model y with statistical signiﬁcance. The number of parameters used in each model is also taken into account when computing the critical values, thus balancing the desire for performance and the aversion to model complexity. Here, the likelihood ratios again show the supe- riority of KCPs over GPs and GARCH(1,1) by allowing diﬀerent combinations of copulas and marginal distributions for the speciﬁc applications. In particular, both GCSM and TCSM are better than GP, TCGM, and GARCH(1,1) with statistical signiﬁcance. Further, the likelihood ratios for pTCGM qGP and pTCSM qGCSM con- ﬁrm that the t -copula provides statistically insigniﬁcant performance gain over the Gaussian copula for this data set. Thus, on average, the GCSM KCP is the most powerful and yet parsimonious model for this application. Now we shift our attention to the eﬀectiveness of the binding copula in captur- ing the dependencies of multiple time-series for weather stations located in the same state. Figure 6 shows the Gram matrices of binding copulas and pair-wise correlation matrices of the detrended data after transforming by the distribution induced by the univariate KCPs that was ranked to be the best earlier. Recall Kernel-Based Copula Processes 641 Fig. 6. Sample multivariate KCP results from a few states. The Gram matrix (left) of the binding copula, pairwise correlation (middle), and the diﬀerence of the two matrices (right). The IDs of the weather stations are labeled on the vertical axes while only the last two digits of the IDs are shown on the horizontal axes. 642 S. Jaimungal and E.K.H. Ng that for non-Gaussian random variables, the Gram matrix is not identical to the covariance matrix, but rather it dictates the behavior of the covariance. Nev- ertheless, the simple t -copula with only geographical distance information was able to recover a substantial amount of interdependency on a visual basis. 5 Conclusion and Future Work This paper introduced the kernel-based copula processes (KCPs), a novel time- series analysis tool, which inherits features from both the theory of copula and Gaussian processes. The KCPs allows the modeling of non-Gaussian processes in a concise and parsimonious manner by separating the speciﬁcation of the dependency structure and the margins. An extension to multivariate time-series was also presented. A binding cop- ula was used to encapsulate the interdependency among time series. This mod- ular approach not only lends itself naturally to implementation on a multi-core / -processor for computational eﬃciency, it also allows time-series with diﬀerent lengths and sampling frequencies to be described seamlessly under a single model. Further research directions include possible sampling or variational methods to accommodate very large data set; the incorporation of a latent process for the kernel hyper-parameters to model processes with stochastic variances; and the application of KCPs in classiﬁcation problems in similar capacity as GPs. References 1. Anderson, T.W., Darling, D.A.: A test of goodness of ﬁt. The Journal of American Statistical Association 49(268), 765–769 (1954) 2. Bollerslev, T.: Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31 3. Williams, C.K.I., Rasmussen, C.E.: Gaussian Processes for Machine Learning. The MIT Press, Cambridge (2006) 4. Carney, M., Cunningham, P.: Evaluating density forecast models. tcd-cs-2006- 21. Database, Trinity College Dublin, Department of Computer Science (2006) 5. Chen, X., Fan, Y.: Estimation of copula-based semiparameteric time series models. Journal of Econometrics 130(2), 307–335 (2006) 6. Sandberg, D.V., Alvarado, E., Stewart, G.: Modeling large forest ﬁres as extreme events. Northwest Science, 72 7. Gunther, T.A., Diebold, F.X., Tay, A.S.: Evaluating density forecasts with appli- cations to ﬁnancial risk management. International Economic Review 39(4), 863– 883 (1998) 8. Rosso, R., De Michele, C., Salvadori, G., Kottegoda, N.T.: Extremes in nature: an approach using Copulas. Springer, New York (2007) 9. Hansen, B.E.: Autoregressive conditional density estimation. International Eco- nomic Review 35(3), 705–730 (1994) 10. Hull, J.C.: Options, Futures, and Other Derivatives, 5th edn. Prentice-Hall, New Jersey (2003) 11. Lee, T.-H., Long, X.: Copula-based multivariate garch model with uncorrelated dependent errors. Database, UCR Working Paper 2005-16 (2005) Kernel-Based Copula Processes 643 12. Mandelbrot, B.: The variation of certain speculative prices. Journal of Business, 26 13. Nelsen, R.B.: An Introduction to Copulas. Springer Series in Statistics. Springer- Verlag New York, Inc, New York (2006) 14. Fermanian, J.-D., Doukhan, P., Lang, G.: Copulas of a vector-valued stationary weakly dependent process. Database, Working paper CREST (2004) 15. Patton, A.J.: Modelling asymmetric exchange rate dependence. International Eco- nomic Review 47(2), 527–556 (2006) 16. Olsen, E.T., Darsow, W.F., Nguyen, B.: Copulas and markov processes. Illinois Journal of Mathematics, pp. 600–642 (1992) 17. Jr. M.J. Menne R.S. Vose Williams, C.N. and D.R. Easterling. United states histori- cal climatology network daily temperature, precipitation, and snow data. Database, The Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, U.S. Department of Energy (2006) Appendix: Heteroskedastic Kernel Function To derive the heteroskedastic kernel function, consider a random process (or time-series) Xt decomposed into a deterministic trend gt and a stochastic com- ponent with heteroskedastic variance, i.e. Xt = gt + yt. Assume yt follows a common Vasicek mean-reverting process: dyt = −κ · (θ − yt) · dt + σt · dWt (19) where κ is the rate of mean-reversion, θ is the mean-reversion level, σu is the variance process of yt,and Wt is a Wiener process in a usual SDE setup. This generalized Vasicek process has solution: yt = y0 · e−κt + θ · (1 − e−κt)+ ∫ t 0 σue−κ(t−u)dWu (20) Thus the auto-covariance of Xt is given by Cov[Xt1 ,Xt2 ]= E [(∫ t1 0 σue−κ(t1−u)dWu )(∫ t2 0 σue−κ(t2−u)dWu )] = e−κ(t1+t2) ∫ min(t1,t2) 0 σ2 ue−2κudu (21) Notably, that this kernel function is not a function of the diﬀerence between t1 and t2 and is therefore non-stationary. Now we must specify the form of the variance process. For the detrended temperature data, the variance process is periodic, consequently, we assume it takes on the following form σ2 t = σ2 · ( α · sin ( t T + φ )) (22) where σ, α, T ,and φ are all constants. Substituting (22) into (21), simplifying gives the ﬁnal form of the kernel function found in (17).","libVersion":"0.3.2","langs":""}
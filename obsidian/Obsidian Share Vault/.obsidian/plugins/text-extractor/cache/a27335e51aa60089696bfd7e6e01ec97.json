{"path":"lit/lit_notes_OLD_PARTIAL/Luo24ConformalThresholdedIntervals.pdf","text":"CONFORMAL THRESHOLDED INTERVALS FOR EFFICIENT REGRESSION Rui Luo and Zhixin Zhou Abstract. This paper introduces Conformal Thresholded Intervals (CTI), a novel conformal regres- sion method that aims to produce the smallest possible prediction set with guaranteed coverage. Unlike existing methods that rely on nested conformal framework and full conditional distribution estimation, CTI estimates the conditional probability density for a new response to fall into each interquantile inter- val using off-the-shelf multi-output quantile regression. CTI constructs prediction sets by thresholding the estimated conditional interquantile intervals based on their length, which is inversely proportional to the estimated probability density. The threshold is determined using a calibration set to ensure marginal coverage. Experimental results demonstrate that CTI achieves optimal performance across various datasets. 1. Introduction Conformal prediction is a powerful framework for constructing prediction intervals with finite-sample validity guarantees. By leveraging exchangeability of the data, conformal methods can convert the output of any machine learning algorithm into a set-valued prediction satisfying the required coverage level, without assumptions on the data distribution. This paper develops a novel conformal prediction method for regression that aims to produce the smallest possible prediction set with guaranteed coverage. Most existing conformal methods for regression either directly predict the lower and upper endpoints of the interval using quantile regression models [30, 17, 32, 11] or first estimate the full conditional distribution of the response and then invert it to obtain prediction sets [13, 4]. While these approaches perform well in many situations, they may produce sub-optimal intervals if the conditional distribution is skewed. Conformalized quantile regression typically yields equal-tailed intervals, but the shortest valid interval may be unbalanced. On the other hand, density-based methods can adapt to skewness but typically involve many tuning parameters and more difficult interpretation, which can be complex for practitioners. To address these limitations, we propose conformal thresholded intervals (CTI), a conformal inference method that seeks the smallest possible prediction set. Instead of relying on an estimate of the full conditional distribution, we use off-the-shelf multi-output quantile regression and construct prediction set by thresholding the estimated conditional interquantile intervals. Compared with conformal his- togram regression (CHR) [33], which first partitions the response space into bins, CTI directly trains a multi-output quantile regression model that uses equiprobable quantiles. This allows us to estimate the conditional probability density for a new response to fall into each interquantile interval, without the need for explicitly binning the response space. For each sample in the calibration set, we obtain the interquantile interval that its response falls into and find the corresponding probability density estimate. We compute the non-conformity scores based on these estimates. Intuitively, the non-conformity score is higher for a sample that falls into a long interquantile interval and lower for a sample that falls into a short interquantile interval. By adopting a similar thresholding idea as in conformal classification [31, 23], we threshold the intervals according to their length, the inverse of which corresponds to the probability density estimate. At test time, the threshold, i.e., the quantile for non-conformity scores, is used in constructing prediction sets for test samples. Specifically, the interquantile intervals are sorted in ascending order of length, and the first ones shorter than or equal to the threshold are kept. We show that the prediction sets generated from thresholding interquantile intervals guarantee marginal coverage and can achieve desired conditional coverage as well as the smallest expected prediction interval length if the multi-output quantile regression model produces true conditional probability density estimates. The rest of this paper is organized as follows. We discuss related work in Section 2. Section 3 describes the proposed CTI method in detail. Section 4 presents a theoretical analysis of CTI. Section 5 provides numerical experiments comparing CTI to existing conformal regression methods on both simulated and real data. Finally, Section 6 concludes with a discussion of the main results and future directions. 1arXiv:2407.14495v1 [cs.LG] 19 Jul 2024 2 CONFORMAL THRESHOLDED INTERVALS 2. Related Work Quantile regression, introduced by [18], estimates the τ -th conditional quantile function by minimizing the check function loss: (1) min fτ n∑ i=1 ρτ (yi − qτ (xi)), where (2) ρτ (r) = ®τ r if r > 0 −(1 − τ )r otherwise is the check function representing the absolute loss. Quantile regression has been widely used to construct prediction intervals by estimating conditional quantile functions at specific levels, such as the 5% and 95% levels for 90% nominal coverage [12, 36, 25, 35, 34]. This approach adapts to local variability, even for highly heteroscedastic data. [5] showed that simultaneous estimation of multiple quantiles is asymptotically more efficient than separate estimation of individual regression quantiles or ignoring within-subject dependency. However, this approach does not guarantee non-crossing quantiles, which can affect the validity of the predictions and introduce critical issues in certain scenarios. To address this limitation, research on non-crossing multiple quantile regression has gained attention in recent years, with several methods proposed to ensure non-crossing quantile estimates, including stepwise approaches [21], non-parametric techniques [3], and deep learning-based models [27, 2]. However, the validity of the produced intervals is only guaranteed for specific models under certain regularity and asymptotic conditions [34, 35, 25]. Many related methods for constructing valid prediction intervals can be encompassed within the nested conformal prediction framework, where a nested sequence of prediction sets is generated by thresholding nonconformity scores derived from various approaches, such as residual-based methods [29, 1, 19], quantile regression [30, 17, 32, 4], density estimation [14, 33, 15], and their combinations with ensemble methods [11] and localized methods [28, 6, 22]. However, as noted by [20], the optimal conditionally-valid prediction regions are level sets of conditional densities, which need not be intervals, suggesting that constructing possibly non-convex prediction sets might lead to more efficient conformal predictors. Our proposed method for constructing non-convex prediction sets is related to the work of [15], who introduce a profile distance to measure the similarity between features and construct prediction sets based on neighboring samples. In contrast, our method directly estimates the conditional probability density for a new response to fall into each interquantile interval based on a multi-output quantile regression model. By thresholding the interquantile intervals based on their length, which is inversely proportional to the estimated probability density, we can construct efficient prediction sets that adapt to the local density of the data. This approach allows us to generate prediction sets that are not restricted to intervals and can potentially achieve better coverage and efficiency compared to interval-based methods. Another related approach [10] converts regression to a classification problem and employs a conditional distribution with a smoothness-enforcing penalty. This method is orthogonal to our approach and can be potentially combined with our multi-output quantile regression framework to further improve the efficiency of the constructed prediction sets. 3. Proposed Method 3.1. Problem Setup. We consider a general regression problem with a dataset {(xi, yi)} n i=1, where xi ∈ X ⊆ Rd is the input feature vector and yi ∈ Y ⊆ R is the corresponding continuous response variable. The dataset is split into three parts: a training set Dtrain, a calibration set Dcal, and a test set Dtest. The corresponding indices set are denoted by Itrain, Ical and Itest respectively. We assume that the examples in these sets are exchangeable. Our goal is to construct a conformal predictor that outputs a prediction set C(X) ⊆ Y for each test input X such that the true response value Y is included in C(X) with a probability of at least 1 − α, where α ∈ (0, 1) is a user-specified significance level. Formally, we aim to achieve the following marginal coverage guarantee: P(Y ∈ C(X)) ≥ 1 − α for joint distribution for X and Y . The probability in this statement is marginal, being taken over all the samples in Dcal and Dtest. CONFORMAL THRESHOLDED INTERVALS 3 While achieving valid coverage, we aim to construct prediction sets that are as informative as possible. Specifically, we seek to minimize the expected length of the prediction sets: E[µ(C(X))] = ∫ X µ(C(x))dP (x), where µ denotes the Lebesgue measure on R and P (x) is the marginal distribution of the input features. 3.2. Our Method. First, we apply quantile regression on the training set Dtrain to predict the τ -th quantile of the conditional distribution Y |X = x for every x ∈ X , where τ takes values from 0 to 1 in increments of 1/K. The estimated quantile for τ = k/K is denoted by ̂qk(x) for k = 0, 1, . . . , K.(3) We then define the interquantile intervals as Ik(x) = (̂qk−1(x), ̂qk(x)] for k = 1, . . . , K.(4) Assuming the quantile regression provides sufficiently accurate estimations, each interval should have approximately the same probability, 1/K, of covering the true label Y . To minimize the size of the prediction set, it is more efficient to include intervals with smaller sizes. This strategy leads us to define the confidence set as: C(x) = ⋃{Ik(x) : µ(Ik(x)) ≤ t, k = 1, . . . , K},(5) where t is a threshold determined in a marginal sense, meaning it is independent of x. To determine t, we utilize the calibration set. We want t to satisfy the condition that yi ∈ C(xi) for at least ⌈(1+|Ical|)(1−α)⌉ instances in the calibration set, where i ∈ Ical. We define t as the smallest value that satisfies this condition: t = ⌈(1 + |Ical|)(1 − α)⌉-th smallest value of µ(Ik(yi)(xi)) for i ∈ Ical,(6) where k(y) is the index that of the interval that y belongs, i.e., y ∈ Ik(y)(x). By plugging t back into (5), we obtain the prediction set for every x ∈ X . Algorithm 1 Conformal thresholding intervals 1: Input: labeled data {(xi, yi)}i∈I, unlabel test data {xi}Itest , a data split ratio, black-box learning algorithm B, level α ∈ (0, 1), number of interquantile intervals K 2: Randomly split the indices I into Itrain and Ical. 3: Train B on samples in Itrain, and obtain quantile estimation functions ̂qk for k = 0, 1, . . . , K. 4: For every i ∈ Ical ∪ Itest, evaluate ̂qk(xi) for k = 0, 1, . . . , K. 5: For every i ∈ Ical ∪Itest, define the interquantile intervals Ik(xi) = (̂qk−1(xi), ̂qk(xi)] for k = 1, . . . , K. 6: t ← ⌈(1 + |Ical|)(1 − α)⌉-th smallest value of µ(Ik(yi)(xi)) for i ∈ Ical. 7: For i ∈ Itest, C(xi) = ⋃{Ik(xi) : µ(Ik(xi)) ≤ t, k = 1, . . . , K}. 8: Output: C(xi) for i ∈ Itest. Remark 3.3. The procedure we propose is generic and can be applied to any multi-output quantile regression method. In Section 5, we present results based on quantile regression methods using both a neural network and a random forest. In some cases, quantile regression algorithms may not support τ values of exactly 0 or 1. To overcome this limitation, we can replace these extreme values with values very close to 0 or 1, respectively. For example, we might use τ = 0.001 instead of τ = 0, and τ = 0.999 instead of τ = 1. For the sake of notational simplicity, we will continue to refer to these values as 0 and 1 throughout the discussion, unless otherwise specified. Remark 3.4. Our approach can also be considered in terms of conformity scores. Using the definition of the prediction set in equation (5), the value of label y is contained within a small interval. More formally, let k(y) be the index such that y ∈ Ik(y)(x). We can then define the conformity score function for our proposed method as: s(x, y) = µ(Ik(y)(x)). This conformity score function assigns a score to each label y based on the size of the interval Ik(y)(x) in which it falls. A smaller score indicates that the label y is more likely to be the true label for the input x. In the context of conformal prediction, labels with smaller conformity scores are given priority for inclusion in the prediction set. 4 CONFORMAL THRESHOLDED INTERVALS 4. Theoretical Analysis In the context of the entire population, CTI shares a very similar formulation with the Least Ambigu- ous Set method used for classification, as described in [31]. If we assume that our quantile regression model is sufficiently accurate, CTI has the potential to achieve the optimal size for prediction sets when considering the marginal distribution. To understand this better, let’s first take a look at the Neyman- Pearson Lemma: Lemma 4.1 (Neyman-Pearson). Let f and g be two nonnegative measurable functions. Then the opti- mizer of the problem min C ∫ C g subject to ∫ C f ≥ 1 − α, is given by C = {x : f (x)/g(x) ≥ t′} if there exists t such that ∫ f /g≥t′ f = 1 − α. To formalize the problem of minimizing the expected length of the prediction set subject to 1 − α coverage, we can write the problem as: min C(x) ∫ X ∫ C(x) 1dµ(y)dP (x) subject to ∫ X ∫ C(x) f (y|x)dµ(y)dP (x) ≥ 1 − α. The Neyman-Pearson Lemma implies that the optimal solution for C(x) has the form: C(x) = {y : f (y|x) ≥ t′}(7) for some suitable threshold t′. Indeed, this threshold can be defined as t′ = inf{t ∈ R : P(f (Y |X) ≥ t) ≥ 1 − α}.(8) which will be shown in Lemma A.2. Our algorithm is an empirical construction of such an interval. Suppose the quantile regression approximates ̂qτ well. In that case, we have: ∫ y∈Ik(x) f (y|x)dµ(y) = P(Y ∈ Ik(X)) ≈ 1/K. As K approaches infinity, µ(Ik(x)) tends to 0. If f (y|x) is sufficiently smooth, then f (y|x) ≈ 1/(Kµ(Ik(x))). The threshold on the length of intervals µ(Ik(x)) ≤ t in equation (5) approximately implies f (y|x) ≥ 1/(Kt), which is optimal in the sense of the Neyman-Pearson Lemma. This means that our algorithm, which constructs prediction sets based on the threshold on interval lengths, is an empirical approximation of the optimal solution prescribed by the Neyman-Pearson Lemma. The demonstration of the coverage probability for CTI follows the same reasoning as the traditional proof used in the general conformal prediction framework. Theorem 4.2 (Coverage Probability). Suppose the samples in {(Xi, Yi)}i∈Ical∪Itest } are exchangeable, then for (X, Y ) in the test set, the coverage probability P (Y ∈ C(X)) ≥ 1 − α. Proof. The score function s(Xi, Yi) for i ∈ Ical ∪ Itest are also exchangeable. For any (X, Y ) in the test set, the rank of s(X, Y ) is smaller than t defined in (6) with probability ⌈(1+|Ical|)(1−α)⌉ 1+|Ical| ≥ 1 − α. □ The upcoming proposition will demonstrate that the threshold t, which is defined in equation (6) for the length of interquantile intervals, results in a suitable threshold for the distribution of the conditional density f (Y |X), as shown in equation (8). Proposition 1 (Threshold Consistency). Suppose the interquantile intervals satisfy sup t′ ∣ ∣ ∣ ∣ ∣ ∣Ff (Y |X)(t′) − 1 Ical ∑ i∈Ical 1 {µ(Ik(yi)(xi)) > 1 Kt′ }∣ ∣ ∣ ∣ ∣ ∣ ≤ ϵ(9) Let t be as defined in (6) and let 1 − α′ = ⌈(1 + |Ical|)(1 − α)⌉/|Ical|, then Ff (Y |X)(1/(Kt)) ≥ α′ − ϵ. Proof. Let t′ = 1/(Kt) in the assumption (9), then by the definition of t, ∑ i∈Ical 1 {µ(Ik(yi)(xi)) > 1 Kt′ } = |Ical| − ⌈(1 + |Ical|)(1 − α)⌉ =: |Ical|α′. Under the assumption, Ff (Y |X)(t ′) ≥ α′ − ϵ. □ CONFORMAL THRESHOLDED INTERVALS 5 This theorem establishes that if the value of ϵ is sufficiently small and the size of the calibration set is large enough, then the value t′ = 1/(Kt), where t is the threshold defined in equation (6), will be close to the α-th quantile of the distribution f (Y |X). Lastly, the theorem presented below demonstrates that the size of the prediction set obtained using CTI will not significantly exceed the size of the theoretically optimal prediction set, which is defined in equation (7). Theorem 4.3 (Prediction Set). Suppose for x ∈ X , P (Y ∈ Ik(x)|X = x) = ∫ y∈Ik(x) f (y|x)dy ≥ 1 − δ(x) K ,(10) and suppose f (y|x) has a Lipschitz constant L(x), then C1−α(x) ⊆ ß y : f (y|x) ≥ 1 − δk(x) Kt − L(x)t 2 ™ . Proof. In the construction of C1−α(x), we consider the intervals with lengthµ(Ik(x)) ≤ t. Given the assumption (10) in the theorem, and combine it with Lemma A.1, we have min y∈Ik f (y|x) ≥ 1 − δk(x) Kt − L(x)t 2 =: t′. By the construction of C1−α(x), we have the conclusion of the theorem. □ The previous proposition demonstrates that the value 1/(Kt), where t is the threshold defined in equation (6), is slightly smaller than the α-th quantile of f (Y |X), which represents the theoretically optimal threshold as shown in equation (8). This theorem further illustrates that the prediction set obtained using CTI will include values that are even more conservative. However, we understand that 1/Kt, where t is the threshold defined in equation (6), serves as a relatively stable estimate of the quantile, which is asymptotically equivalent to a constant value. As the number of interquantile intervals K approaches infinity, the threshold t converges to 0. If we additionally assume that our quantile regression model is accurate, meaning that the error term δk(x) is small, then the prediction set C1−α(x) obtained using CTI will be close to the theoretically optimal prediction set defined in equation (5). 5. Experiments This section presents experiments that evaluate the performance of prediction sets generated by Split Conformal [19], CQR [30], CHR [33], and our proposed CTI, on various simulated and real-world datasets as in [30, 17, 32]. For a detailed introduction of the datasets, please refer to [32]. We have released our code on GitHub, which is built upon TorchCP [37], CHR [33], and QRF [16]. Following similar procedures, we standardize the response variables Y for all datasets. We randomly hold out 20% of the samples for testing and divide the remaining observations into two disjoint sets, I1 and I2, for training the multi-output quantile regression model and computing the non-conformity scores, respectively. We use 70% of the samples for training, which achieves a reasonable compromise for all datasets [32]. We repeat all experiments 10 times, starting from the initial data splitting. We evaluate the performance of the generated prediction intervals in terms of coverage and efficiency. As shown in Table 1, CTI achieves optimal performance across all datasets except for the star dataset, which has a relatively small sample size (n = 2161, d = 39). The limited number of samples in this dataset may hinder the performance of the multi-output quantile regression model, as it requires sufficient data to accurately capture the underlying relationships between the features and the response variable. We also notice a similar trend in the relative performance comparison of CTI based on random forest and CTI based on neural network, as well as CHR based on random forest and CHR based on neural network. This suggests that the efficiency of the conformal prediction sets depends on the quality of the multi- output quantile regression. The choice of the underlying model plays a crucial role in the performance of the conformal prediction methods. To further analyze the superior performance of CTI, we compare the interval lengths between the response intervals (i.e., intervals containing the actual responses) and all intervals generated by the multi-output quantile regression model for the MEPS-19 dataset. Figure 1 presents the distribution of interval lengths for both the response intervals (blue histogram) and all intervals (red histogram) on the test set. The difference in means between the two distributions is -0.0415, indicating that the response intervals have a smaller average length compared to all intervals. This finding suggests that the multi-output quantile regression model effectively captures the under- lying structure of the data, producing tighter intervals around the true responses. By estimating the 6 CONFORMAL THRESHOLDED INTERVALS conditional quantile functions simultaneously, the model can leverage the dependencies between different quantile levels and generate more precise intervals. The multi-output approach allows the model to share information across different quantile levels, leading to improved interval estimates. Consequently, CTI can construct more efficient prediction sets by thresholding the intervals based on their estimated probability density. By selecting intervals with higher probability density, CTI focuses on regions of the feature space where the true response is more likely to occur. This adaptive thresholding approach leads to improved coverage and shorter interval lengths compared to other methods that rely on fixed thresholds or do not consider the local density of the data. Dataset Metric CTI(RF) CTI(NN) CHR(RF) CHR(NN) CQR Split bike Coverage 0.898 (0.007) 0.899 (0.007) 0.898 (0.010) 0.900 (0.006) 0.906 (0.009) 0.899 (0.008) Size 1.032 (0.029) 0.720 (0.028) 1.124 (0.028) 0.758 (0.047) 1.599 (0.054) 1.345 (0.053) blog Coverage 0.910 (0.002) 0.900 (0.004) 0.902 (0.004) 0.902 (0.003) 0.940 (0.009) 0.910 (0.006) Size 0.709 (0.031) 1.003 (0.024) 1.567 (0.074) 1.737 (0.154) 3.259 (0.327) 1.453 (0.113) community Coverage 0.909 (0.018) 0.908 (0.021) 0.903 (0.015) 0.905 (0.021) 0.889 (0.024) 0.902 (0.024) Size 1.611 (0.088) 1.275 (0.095) 1.637 (0.096) 1.588 (0.100) 1.680 (0.078) 2.132 (0.188) star Coverage 0.903 (0.018) 0.910 (0.017) 0.907 (0.021) 0.897 (0.018) 0.901 (0.016) 0.910 (0.024) Size 0.186 (0.006) 0.197 (0.009) 0.182 (0.005) 0.204 (0.009) 0.181 (0.005) 0.181 (0.008) homes Coverage 0.900 (0.005) 0.900 (0.006) 0.899 (0.005) 0.895 (0.007) 0.898 (0.006) 0.897 (0.005) Size 0.640 (0.011) 0.515 (0.008) 0.682 (0.012) 0.535 (0.010) 0.851 (0.052) 0.825 (0.072) facebook1 Coverage 0.909 (0.003) 0.899 (0.003) 0.901 (0.004) 0.900 (0.004) 0.945 (0.009) 0.903 (0.002) Size 0.766 (0.033) 0.780 (0.023) 1.595 (0.088) 1.379 (0.086) 2.627 (0.329) 2.252 (0.208) facebook2 Coverage 0.911 (0.002) 0.900 (0.001) 0.899 (0.002) 0.899 (0.002) 0.943 (0.006) 0.904 (0.002) Size 0.735 (0.017) 0.773 (0.023) 1.533 (0.053) 1.382 (0.057) 2.661 (0.272) 2.100 (0.108) bio Coverage 0.900 (0.004) 0.902 (0.004) 0.899 (0.005) 0.900 (0.004) 0.900 (0.003) 0.901 (0.004) Size 1.295 (0.018) 1.474 (0.030) 1.450 (0.023) 1.576 (0.012) 2.005 (0.016) 1.961 (0.039) concrete Coverage 0.908 (0.024) 0.900 (0.031) 0.899 (0.022) 0.900 (0.023) 0.901 (0.024) 0.896 (0.021) Size 0.967 (0.035) 0.473 (0.050) 0.933 (0.041) 0.505 (0.144) 0.692 (0.051) 0.619 (0.029) meps19 Coverage 0.907 (0.008) 0.902 (0.007) 0.901 (0.007) 0.902 (0.004) 0.932 (0.007) 0.902 (0.010) Size 1.760 (0.087) 1.795 (0.061) 2.388 (0.195) 2.602 (0.128) 2.923 (0.170) 3.092 (0.377) meps20 Coverage 0.904 (0.004) 0.901 (0.007) 0.901 (0.007) 0.901 (0.006) 0.927 (0.009) 0.902 (0.005) Size 1.883 (0.067) 1.921 (0.091) 2.376 (0.105) 2.594 (0.140) 2.925 (0.193) 3.154 (0.217) meps21 Coverage 0.906 (0.005) 0.900 (0.008) 0.900 (0.006) 0.898 (0.004) 0.928 (0.007) 0.905 (0.004) Size 1.832 (0.089) 1.866 (0.076) 2.510 (0.167) 2.609 (0.145) 2.971 (0.179) 3.046 (0.199) Table 1. Coverage and Size of Different Methods 6. Discussion and Future Work Our proposed conformal prediction method for regression tasks has demonstrated promising results in achieving high performance. However, there are several areas where further research and improvements can be made. Computing prediction intervals as opposed to prediction sets. There might be concerns re- garding constructing disjoint prediction sets instead of prediction intervals, as it may be more desirable to have a single contiguous interval for interpretability and practicality. To address this issue, we can adopt ideas similar to those in [7], which deals with the problem of non-contiguous prediction sets for ordinal classification tasks. The authors propose training the predictor with a unimodal posterior over classes, allowing the construction of contiguous prediction sets. In the context of regression, we can impose unimodality constraints on the estimated conditional density or quantile functions to ensure that the resulting prediction sets are contiguous intervals. This approach would provide a more interpretable and user-friendly output while maintaining the advantages of our proposed method. Weighted aggregation of score functions. In this work, we implement the concept of weighted aggregation of score functions for conformal classification [24] by applying it to multi-output quantile regression models. Specifically, we combine the results from a neural network model and a random forest model. To ensure the combined results form a valid non-conformity score and accurately reflect predic- tions from both models, we use a harmonic mean for aggregation. This approach allows us to leverage the strengths of different models and improve the overall performance of the conformal prediction sets. CONFORMAL THRESHOLDED INTERVALS 7 0.0 0.1 0.2 0.3 0.4 0.5 Interval Length 0 50 100 150 200 250 300 350 400Frequency (Response Intervals) Response Intervals All Intervals 0 2000 4000 6000 8000 10000 12000 14000 16000 18000Frequency (All Intervals) Difference in Mean: -0.0459 Figure 1. Comparison of interval lengths between response intervals and all intervals for the MEPS-19 dataset. The blue histogram represents the distribution of interval lengths for the intervals that contain the actual responses, while the red histogram shows the distribution of all intervals generated by the multi-output quantile regression model. Both all results on the test set. The difference in means between the two distributions is -0.0415, indicating that the response intervals have a smaller average length compared to all intervals. Future research can explore alternative aggregation methods and investigate the optimal combination of models for different datasets and problem settings. Extension to other regression settings. Our current work focuses on the standard regression set- ting with continuous response variables. However, there are various other regression settings, such as multivariate regression or functional regression [8, 26, 9], where conformal prediction methods could be beneficial. Adapting our multi-output quantile regression approach to these settings would require ad- dressing the specific challenges and characteristics of each problem. For example, in multivariate regres- sion, the construction of prediction sets would involve estimating joint quantile functions and handling the dependencies between multiple response variables. In functional regression, the response variable is a function rather than a scalar, requiring the development of appropriate non-conformity measures and prediction set construction techniques. 7. Conclusion Conformal Thresholded Intervals (CTI) is a novel conformal prediction method for regression that constructs the smallest possible prediction set with guaranteed coverage. By leveraging multi-output quantile regression and thresholding the estimated conditional interquantile intervals based on their length, CTI adapts to the local density of the data and generates non-convex prediction sets. Exper- imental results demonstrate the effectiveness of CTI in achieving optimal performance across various datasets. However, the performance of CTI depends on the quality of the underlying multi-output quan- tile regression model, emphasizing the importance of model selection in conformal prediction methods. Future research directions include optimizing model selection as well as model aggregation and extending CTI to other types of regression problems. References [1] Balasubramanian, V., Ho, S.-S., and Vovk, V. Conformal prediction for reliable machine learning: theory, adaptations and applications. Newnes, 2014. [2] Brando, A., Center, B. S., Rodríguez-Serrano, J., Vitrià, J., et al. Deep non-crossing quantiles through the partial derivative. In International Conference on Artificial Intelligence and Statistics (2022), PMLR, pp. 7902–7914. [3] Cannon, A. J. Non-crossing nonlinear regression quantiles by monotone composite quantile regression neural network, with application to rainfall extremes. Stochastic environmental research and risk assessment 32, 11 (2018), 3207–3225. [4] Chernozhukov, V., Wüthrich, K., and Zhu, Y. Distributional conformal prediction. Proceedings of the National Academy of Sciences 118, 48 (2021), e2107794118. [5] Cho, H., Kim, S., and Kim, M.-O. Multiple quantile regression analysis of longitudinal data: Heteroscedasticity and efficient estimation. Journal of Multivariate Analysis 155 (2017), 334–343. [6] Colombo, N. On training locally adaptive cp. In Conformal and Probabilistic Prediction with Applications (2023), PMLR, pp. 384–398. 8 CONFORMAL THRESHOLDED INTERVALS [7] DEY, P., Merugu, S., and Kaveri, S. R. Conformal prediction sets for ordinal classification. In Thirty-seventh Conference on Neural Information Processing Systems (2023). [8] Diquigiovanni, J., Fontana, M., and Vantini, S. Conformal prediction bands for multivariate functional data. Journal of Multivariate Analysis 189 (2022), 104879. [9] Feldman, S., Bates, S., and Romano, Y. Calibrated multiple-output quantile regression with representation learning. Journal of Machine Learning Research 24, 24 (2023), 1–48. [10] Guha, E. K., Natarajan, S., Möllenhoff, T., Khan, M. E., and Ndiaye, E. Conformal prediction via regression-as-classification. In The Twelfth International Conference on Learning Representations (2024). [11] Gupta, C., Kuchibhotla, A. K., and Ramdas, A. Nested conformal prediction and quantile out-of-bag ensemble methods. Pattern Recognition 127 (2022), 108496. [12] Hunter, D. R., and Lange, K. Quantile regression via an mm algorithm. Journal of Computational and Graphical Statistics 9, 1 (2000), 60–77. [13] Izbicki, R., Shimizu, G., and Stern, R. Flexible distribution-free conditional predictive bands using density esti- mators. In International Conference on Artificial Intelligence and Statistics (2020), PMLR, pp. 3068–3077. [14] Izbicki, R., Shimizu, G., and Stern, R. Flexible distribution-free conditional predictive bands using density estima- tors. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (26–28 Aug 2020), S. Chiappa and R. Calandra, Eds., vol. 108 of Proceedings of Machine Learning Research, PMLR, pp. 3068– 3077. [15] Izbicki, R., Shimizu, G., and Stern, R. B. Cd-split and hpd-split: Efficient conformal regions in high dimensions. Journal of Machine Learning Research 23, 87 (2022), 1–32. [16] Johnson, R. A. quantile-forest: A python package for quantile regression forests. Journal of Open Source Software 9, 93 (2024), 5976. [17] Kivaranovic, D., Johnson, K. D., and Leeb, H. Adaptive, distribution-free prediction intervals for deep networks. In International Conference on Artificial Intelligence and Statistics (2020), PMLR, pp. 4346–4356. [18] Koenker, R. Quantile regression, vol. 38. Cambridge university press, 2005. [19] Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L. Distribution-free predictive inference for regression. Journal of the American Statistical Association 113, 523 (2018), 1094–1111. [20] Lei, J., Robins, J., and Wasserman, L. Distribution-free prediction sets. Journal of the American Statistical Association 108, 501 (2013), 278–287. [21] Liu, Y., and Wu, Y. Stepwise multiple quantile regression estimation using non-crossing constraints. Statistics and its Interface 2, 3 (2009), 299–310. [22] Luo, R., and Colombo, N. Conformal load prediction with transductive graph autoencoders. arXiv preprint arXiv:2406.08281 (2024). [23] Luo, R., and Zhou, Z. Trustworthy classification through rank-based conformal prediction sets. arXiv preprint arXiv:2407.04407 (2024). [24] Luo, R., and Zhou, Z. Weighted aggregation of conformity scores for classification, 2024. [25] Meinshausen, N. Quantile regression forests. Journal of Machine Learning Research 7 (2006), 983–999. [26] Messoudi, S., Destercke, S., and Rousseau, S. Ellipsoidal conformal inference for multi-target regression. In Conformal and Probabilistic Prediction with Applications (2022), PMLR, pp. 294–306. [27] Moon, S. J., Jeon, J.-J., Lee, J. S. H., and Kim, Y. Learning multiple quantiles with neural networks. Journal of Computational and Graphical Statistics 30, 4 (2021), 1238–1248. [28] Papadopoulos, H., Gammerman, A., and Vovk, V. Normalized nonconformity measures for regression conformal prediction. In Proceedings of the IASTED International Conference on Artificial Intelligence and Applications (AIA 2008) (2008), pp. 64–69. [29] Papadopoulos, H., Proedrou, K., Vovk, V., and Gammerman, A. Inductive confidence machines for regression. In Machine Learning: ECML 2002: 13th European Conference on Machine Learning Helsinki, Finland, August 19–23, 2002 Proceedings 13 (2002), Springer, pp. 345–356. [30] Romano, Y., Patterson, E., and Candès, E. Conformalized quantile regression. In Advances in Neural Information Processing Systems (2019), pp. 3538–3548. [31] Sadinle, M., Lei, J., and Wasserman, L. Least ambiguous set-valued classifiers with bounded error levels. Journal of the American Statistical Association 114, 525 (2019), 223–234. [32] Sesia, M., and Candès, E. A comparison of some conformal quantile regression methods. Stat 9, 1 (2020). [33] Sesia, M., and Romano, Y. Conformal prediction using conditional histograms. Advances in Neural Information Processing Systems 34 (2021), 6304–6315. [34] Steinwart, I., and Christmann, A. Estimating conditional quantiles with the help of the pinball loss. [35] Takeuchi, I., Le, Q. V., Sears, T. D., Smola, A. J., and Williams, C. Nonparametric quantile estimation. Journal of machine learning research 7, 7 (2006). [36] Taylor, J. W. A quantile regression neural network approach to estimating the conditional density of multiperiod returns. Journal of Forecasting 19, 4 (2000), 299–311. [37] Wei, H., and Huang, J. Torchcp: A library for conformal prediction based on pytorch. arXiv preprint arXiv:2402.12683 (2024). Appendix A. Ancillary Lemmas Lemma A.1. For a Lipschitz function f : R → R with Lipschitz constant L, if ∫ b a f (x)dµ(x) = c, then f (x) ∈ [ c b−a − L(b−a) 2 , c b−a + L(b−a) 2 ] for all x ∈ [a, b]. Proof. Suppose f (x′) = d for some x′ ∈ [a, b]. Then, by the Lipschitz condition, we have: |f (x) − f (x ′)| ≤ L|x − x′| =⇒ f (x) ≤ d + L|x − x′| CONFORMAL THRESHOLDED INTERVALS 9 for all x ∈ [a, b]. Integrating both sides over [a, b], we get: ∫ b a f (x)dµ(x) ≤ ∫ b a (d + L|x − x ′|)dµ(x) = d(b − a) + L ∫ b a |x − x′|dµ(x) ≤ d(b − a) + 1 2 L(b − a) 2, where the last inequality follows from the fact that ∫ b a |x − x′|dµ(x) ≤ 1 2 (b − a) 2. Since ∫ b a f (x)dµ(x) = c, we have: c ≤ d(b − a) + 1 2 L(b − a) 2 =⇒ d ≥ 1 b − a Å c − 1 2 L(b − a)2ã = c b − a − L(b − a) 2 . Since x′ was arbitrary, this lower bound holds for all x ∈ [a, b]. The upper bound can be proved analogously. □ Lemma A.2. Let C(x) = {y : f (y|x) ≥ t′}. Then the smallest t ′ satisfying ∫ X ∫ C(x) f (y|x)dµ(y)dP (x) ≥ 1 − α is given by t′ = inf{t ∈ R : P(f (Y |X) ≥ t) ≥ 1 − α}. Proof. By direct calculation. P(f (Y |X) ≥ t) = ∫ X ∫ Y 1{f (y|x) ≥ t}dP (y|x)dP (x) = ∫ X ∫ {f (y|x)≥t} f (y|x)dµ(y)dP (x) The proof is complete. □","libVersion":"0.3.2","langs":""}
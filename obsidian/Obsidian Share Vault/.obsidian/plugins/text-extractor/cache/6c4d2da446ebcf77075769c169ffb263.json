{"path":"lit/lit_sources/Bordt23nShapleyValsGnrlzd.pdf","text":"From Shapley Values to Generalized Additive Models and back Sebastian Bordt Ulrike von Luxburg Department of Computer Science University of Tübingen Department of Computer Science and Tübingen AI Center University of Tübingen Abstract In explainable machine learning, local post- hoc explanation algorithms and inherently inter- pretable models are often seen as competing ap- proaches. This work offers a partial reconciliation between the two by establishing a correspondence between Shapley Values and Generalized Addi- tive Models (GAMs). We introduce n-Shapley Values, a parametric family of local post-hoc ex- planation algorithms that explain individual pre- dictions with interaction terms up to order n. By varying the parameter n, we obtain a sequence of explanations that covers the entire range from Shapley Values up to a uniquely determined de- composition of the function we want to explain. The relationship between n-Shapley Values and this decomposition offers a functionally-grounded characterization of Shapley Values, which high- lights their limitations. We then show that n- Shapley Values, as well as the Shapley Taylor- and Faith-Shap interaction indices, recover GAMs with interaction terms up to order n. This implies that the original Shapely Values recover GAMs without variable interactions. Taken together, our results provide a precise characterization of Shap- ley Values as they are being used in explainable machine learning. They also offer a principled in- terpretation of partial dependence plots of Shapley Values in terms of the underlying functional de- composition. A package for the estimation of dif- ferent interaction indices is available at https: //github.com/tml-tuebingen/nshap. 1 INTRODUCTION Local post-hoc explanation algorithms and inherently inter- pretable models are two of the most prominent approaches Proceedings of the 26 th International Conference on Artiﬁcial Intel- ligence and Statistics (AISTATS) 2023, Valencia, Spain. PMLR: Volume 206. Copyright 2023 by the author(s). in explainable machine learning (Molnar, 2020; Holzinger et al., 2022). Despite a number of arguments about their rel- ative beneﬁts, the differences and similarities between these two approaches remain largely unresolved Rudin (2019). In the current literature, post-hoc explanations and inherently interpretable models are often framed as different concepts, with research papers, book chapters, and tutorials divided along these lines (Lundberg et al., 2020; Molnar, 2020; Lakkaraju et al., 2020). We take a different perspective and highlight the similarities between post-hoc explanations and interpretable models. We do so for the particular case of Shapley Values, a prominent feature attribution method, and GAMs, a popular class of interpretable models. Post-hoc explanations with Shapley Values. The sem- inal work by Lundberg and Lee (2017) introduced the SHAP feature attributions. These are based on the literature on Shapley Values in game theory. The authors showed that for linear functions f (x) = wT x and statistically in- dependent features, the SHAP attributions take the form Φi = wi(xi − E(xi)), thus establishing a link between the post-hoc explanation method and a very simple type of in- terpretable model. This work has inspired a whole branch of literature on explainable machine learning. Most relevant to us are Shapley Interaction Values (Lundberg et al., 2020), which extend Shapley Values with local interaction effects between pairs of features. An important building block of our work is the general- ization of Shapley Interaction Values towards n-Shapley Values, a novel type of Shapley-based post-hoc explana- tion that is able to incorporate arbitrarily many variable interactions. Similarly to the Shapley Taylor- (Sundararajan et al., 2020) and the Faith-Shap interaction index (Tsai et al., 2022), n-Shapley Values are a parametric family of local post-hoc explanation algorithms that explain individual pre- dictions with interaction terms up to order n. As n increases, the explanations become more complex and expressive and are able to faithfully explain more complex models. Generalized Additive Models (GAMs hereafter) are a pop- ular class of interpretable models with a restricted form of non-linearity (Hastie and Tibshirani, 1990; Caruana et al., 2015; Agarwal et al., 2021a). Traditionally, GAMs are allowed to exhibit (arbitrary) non-linearity in individualarXiv:2209.04012v3 [cs.LG] 23 Feb 2023 From Shapley Values to Generalized Additive Models and back features, but no interaction between features is allowed. GA2Ms (Lou et al., 2012) relax this restriction and allow for interaction between pairs of features. Conceptually, it is straightforward to extend GAMs with interaction effects of any desired order n (this comes, however, at the cost of human interpretability). Important to us, the model class of GAMs suffers from an identiﬁcation problem. As soon as we introduce variable interactions, the way in which a given function can be written as a GAM is no longer uniquely determined Lengerich et al. (2020). Shapley-based explanations faithfully explain GAMs. In this work, we show that different kinds of Shapley-based post-hoc explanations (Lundberg and Lee, 2017; Lundberg et al., 2020; Sundararajan et al., 2020; Tsai et al., 2022) are completely faithful to GAMs: if the function to be explained is a GAM, then the explanations recover its individual non- linear component functions. We link the order of the GAM – the maximum degree of variable interaction that is present in a function – with the order of an explanation that we use to explain that function. If the order of the explanation is at least as large as the maximum variable interaction that is (locally) present in the model, then the explanations are guaranteed to recover a faithful representation of the func- tion as a GAM. This result applies to the newly proposed n-Shapley Values, as well as to the Shapley Taylor- and Faith-Shap interaction indices. As a special case, our re- sults imply that the interventional SHAP feature attributions (Lundberg and Lee, 2017; Janzing et al., 2020) are perfectly faithful to GAMs without variable interactions, even if the features are arbitrarily dependent. What is more, we show that Shapley-based post-hoc ex- planations of any function implicitly solve the problem of representing the function as a GAM (potentially with vari- able interactions of very high order). This means that our results provide insights into the mechanics of Shapley Val- ues not only if the function to be explained is a lower-order GAM, but any (learned) function, for example a neural net- work. Concretely, we identify a necessary and sufﬁcient regularity condition – subset compliance – under which a value function gives rise to a well-deﬁned functional decom- position of the function that we attempt to explain. Because this decomposition connects Shapley Values with GAMs, we term it the Shapley-GAM. Taken together, our results offer a precise functionally- grounded analysis of Shapley Values, one of the most widely used approaches in explainable machine learning (Doshi-Velez and Kim, 2017). They also highlight the pecu- liar properties of these explanations, and the way in which they are different from other feature attribution methods (Covert et al., 2021; Krishna et al., 2022). For example, contrary to popular belief, Shapley Values only depend on the coordinates of the point that we attempt to explain, but not on the local neighbourhood of that point. This in turn implies that the explanations are unrelated to the gradient and do not perform any kind of local function approximation (Han et al., 2022). We consider n-Shapley Values to be a useful tool for prac- titioners who want to debug black-box models. Moreover, we introduce a novel method to plot feature attributions of higher order that is consistent with the underlying theory (de- picted, for example, in Figure 1). We also introduce a way to estimate the amount of variable interaction that is neces- sary to represent a given function. Finally, we study the link between accuracy and the average degree of variable inter- action present in different standard classiﬁers (Section 7). 2 RELATED WORK Shapley Values. The seminal paper by Lundberg and Lee (2017) has led to a line of work that investigates the usage of Shapley Values in explainable machine learning (Chen et al., 2020; Heskes et al., 2020; Slack et al., 2020; Al- bini et al., 2022). Shapley Values originate in a literature on economic game theory (Shapley, 1953), and our work builds on a particular paper from this literature, namely the seminal work by Grabisch (1997) on additive set functions. The idea to extend Shapley Interaction Values towards n- Shapley Values is closely related to other approaches that also extend the Shapley Value (Grabisch, 1997; Lundberg et al., 2020; Sundararajan et al., 2020; Tsai et al., 2022). The efﬁcient computation of Shapley Values is a topic of ongoing research interest (Lundberg et al., 2020; Jethani et al., 2021). Our results also relate to the debate about the choice of value function (Sundararajan and Najmi, 2020; Janzing et al., 2020). Shapley Values have been explored in various tasks with human decision makers, a topic about which there is much debate (Kumar et al., 2020). Generalized Additive Models. Generalized additive mod- els originate in statistics (Hastie and Tibshirani, 1990) and have recently become popular in combination with trees (Lou et al., 2012, 2013) and neural networks (Agarwal et al., 2021a). On tabular data sets, interpretable GAMs with few interactions (Caruana et al., 2015) can often achieve competitive accuracy, which has led to an active line of re- search on these models (Wang et al., 2022; Lengerich et al., 2022). From a statistical perspective, the decomposition of a function as a GAM is underdetermined, which has led to the development of additional uniqueness criteria such as functional ANOVA (Hooker, 2007; Lengerich et al., 2020). Explainable Machine Learning. Shapley Values are one of many different feature attribution methods (Ribeiro et al., 2016; Sundararajan et al., 2017; Kommiya Mothilal et al., 2021) about which there is a large literature (Lee et al., 2019; Garreau and von Luxburg, 2020; Slack et al., 2021; Covert et al., 2021; Krishna et al., 2022; Han et al., 2022) and much debate (Lipton, 2018; Rudin, 2019; Bordt et al., 2022). Con- siderable debate also exists around the question whether there is an accuracy-explainability trade-off or a cost of us- Sebastian Bordt, Ulrike von Luxburg ing interpretable models (Rudin, 2019; Moshkovitz et al., 2020). Apart from GAMs, there are many other inter- pretable models such as rule lists (Wang and Rudin, 2015) and sparse decision trees (Lin et al., 2020). Since our work is exclusively focused on Shapley Values and GAMs, we do not offer a comprehensive review of the literature on explainable machine learning. This can be found in many other places (Molnar, 2020; Samek et al., 2021; Holzinger et al., 2022; Rudin et al., 2022). 3 BACKGROUND AND NOTATION We consider data points x ∈ Rd with d features, and a function f : Rd → R whose behavior we want to explain. We consider the local post-hoc explanation setting with feature attributions: For a point x ∈ Rd, our goal is to explain which input features (or combinations thereof) were most inﬂuential in determining the “decision” f (x). In order to do so, we assign real numbers to input features and their combinations. The higher the absolute value of this number, the more inﬂuential the feature is considered to be (for an illustration consider Figure 1). In what follows, we denote [n] = {1, . . . , n} and use subsets of coordinates S = {s1, . . . , sn} ⊂ [d] to index both data points xS = (xs1 , . . . , xsn ) and collections of functions fS(xS) = fxs1 ,...,xsn (xs1 , . . . , xsn ) where we assume the ordering s1 < · · · < sn. 3.1 Value Functions and Shapley Values For a data point x ∈ Rd, a subset of coordinates S ⊂ [d], and a function f , the value function v(x, S) is supposed to quantify how much the features that are present in S con- tribute towards the prediction f (x). Two important value functions are the observational SHAP value function Lund- berg and Lee (2017) v(x, S) = Ez∼D [f (z) | xS] (1) and the interventional SHAP value function (Chen et al., 2020; Janzing et al., 2020) v(x, S) = Ez∼D [f (z) | do(xS)] . (2) Shapley Values, denoted by Φi(x), are obtained from the value function via the well-known Shapley formula (Shap- ley, 1953). We ﬁrst introduce the Shapley Interaction Index (Grabisch and Roubens, 1999), given by ∆S(x) = ∑ T ⊂[d]\\S (d − |T | − |S|)!|T |! (d − |S| + 1)! ∑ L⊂S(−1)|S|−|L|v(x, L ∪ T ). (3) The Shapley Value Φi(x) of feature i at x is then simply given by ∆i(x). Importantly, different value functions give rise to different Shapley Values, so that there effectively exists a multiplicity of possible Shapley Values, depending on our choice of value function (Sundararajan and Najmi, 2020). The popular KernelSHAP algorithm (Lundberg and Lee, 2017) approximates Shapley Values with respect to the interventional SHAP value function. The corresponding attributions are also known as the SHAP feature attributions. The following regularity condition, satisﬁed by both (1) and (2), will guarantee that the value function gives rise to a well-deﬁned functional decomposition of the function that we attempt to explain. Deﬁnition 1 (Subset-Compliant Value Function). We say that v(x, S) is a subset-compliant value function for f : Rd → R if v(x, [d]) = f (x) and if the value v(x, S) de- pends only on those coordinates of x that are indexed by S. For a subset-compliant value function, we also write v(x, S) = v(xS, S). 3.2 Generalized Additive Models We employ the following deﬁnition of a generalized additive model (GAM) of order n. Deﬁnition 2 (Generalized Additive Model of order n). We say that f : Rd → R is a generalized additive model of order n if f can be written in the form f (x) = ∑ S⊂[d], |S|≤n fS(xS) (4) In words, the function f can be described as a simple sum with interaction terms of at most n variables at a time. The individual functions fS are called component functions of f . GAMs with few interactions (n = 1, 2, 3) are often considered interpretable and called Glassbox-GAMs (Lou et al., 2012; Caruana et al., 2015). The reason for this is that the feature-wise shape functions f1, . . . , fd can be easily visualized, see for example Figure 4. If we allow for interactions of arbitrary order, that is n = d, then every function can be written as a GAM. However, it is a well-known fact that representing an arbitrary function according to (4) is under-determined: Many such represen- tations might be possible for the same function. Any such representation is called a functional decomposition of f . This non-identiﬁability has led to the development of ad- ditional criteria on the decomposition, such as functional ANOVA, that resolve the identiﬁcation problem (Hooker, 2007; Lengerich et al., 2020). 4 FROM SHAPLEY VALUES TO GENERALIZED ADDITIVE MODELS We now introduce n-Shapley Values, a parametric family of local-post hoc explanation algorithms that extends Shapley Values (Lundberg and Lee, 2017) and Shapley Interaction Values (Lundberg et al., 2020). We then show that every subset-compliant value function implicitly provides a func- tional decomposition of the function that we attempt to From Shapley Values to Generalized Additive Models and backAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P −0.4 −0.2 0.0 0.2 0.4 0.6Feature Attribution Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P Shapley Interaction ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 4-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure 1: n-Shapley Values generate a sequence of explanations of increasing complexity, ranging from the original Shapley Values to the Shapley-GAM. From left to right: Shapley Values (n = 1), Shapley Interaction Values (n = 2), 4-Shapley Values (n = 4) and the Shapley-GAM (n = d). In each plot, we distributed the higher-order interaction effects uniformly onto all involved features (as justiﬁed by Theorem 6). Taking into account the signs of the attributions, the different contributions to each of the bars sum to the Shapley Value of that feature (Equation (13)). Taking the overall sum over all bars for all features recovers the prediction f (x). See Appendix Section B for more details regarding this visualization. In this example, the function f is a random forest on the Folktables Income classiﬁcation task, the data point is the ﬁrst observation in our test set, and we used the value function of interventional SHAP. explain. Due to its connection with Shapley Values, we de- nominate this decompositions the Shapley-GAM. We then show that for n = d, n-Shapley Values are equal to this decomposition. 4.1 n-Shapley Values The deﬁnition of n-Shapley Values relates to the function f that we want to explain implicitly via the value function. Deﬁnition 3 (n-Shapley Values). Fix a function f : Rd → R. Let v(x, S) be a value function for f . n-Shapley Values Φn S provide an an attribution to all groups of at most n features at a time, that is for all sets S ⊂ [d] with |S| ≤ n. We deﬁne them recursively, starting from the original Shapley Values at n = 1 up to n = d, by Φn S =    ∆S if |S| = n Φn−1 S + Bn−|S| ∑ K⊂[d]\\S |K|+|S|=n ∆S∪K if |S| < n. (5) The coefﬁcients Bn that balance the different terms are the Bernoulli numbers (see Appendix A). All terms except the Bernoulli numbers additionaly depend on the point x. While this deﬁnition might seem rather abstract, n-Shapley Values are actually a straightforward extension of Shapley Interaction Values (Lundberg et al., 2020). These corre- spond to the case n = 2. The original Shapley Values correspond to the case n = 1. Similar to the original Shap- ley Values, n-Shapley Values are additive and always sum to the function value f (x) (when summed over all subsets S ⊂ [d]) of size ≤ n).1 The overall intuition behind the recursive deﬁnition of n-Shapley Values is that starting from the original Shapley Values at n = 1, we successively add higher-order variable interactions to the explanations. n-Shapley Values give rise to a sequence of explanations of increasing complexity, ranging from the original Shapley Values up to a functional decomposition of the function that we attempt to explain (see Theorem 4 below). Figure 1 de- picts such a sequence of explanations for a random forest on the Folktables Income classiﬁcation task (Ding et al., 2021). To visualize the n-Shapley Values, we evenly distribute all higher-order interactions onto the involved features. As we detail in Appendix B, this technique is justiﬁed by the re- cursive relationship between n-Shapley Values of different order. Note that n-Shapley Values of higher order are differ- ent from those of lower order only if the function that we attempt to explain actually contains higher-order variable interactions (this intuition will be made precise in Section 6). For this reason, n-Shapley Values can be used as a tool to assess the amount of variable interaction that is present in a given black-box predictor. For the random forest, we can see from the rightmost part of Figure 1 that it relies on very high degrees of variable interaction (for a quantitative analysis, see Section 7). 1The proof of Proposition 12 in the Appendix shows that the Bernoulli numbers are exactly the coefﬁcients that balance equa- tion (5) in this way. Sebastian Bordt, Ulrike von Luxburg 0 2 Value of Feature AGEP −0.1 0.0 0.1Feature AttributionShapley Values 0 2 Value of Feature AGEP Shapley Interaction Values 0 2 Value of Feature AGEP 4-Shapley Values 0 2 Value of Feature AGEP Shapley-GAM Figure 2: As n → d, the n-Shapley Values provide increasingly precise representations of the component functions fS of the Shapley-GAM. This ﬁgure depicts partial dependence plots of Φ1 AGEP (Shapley Values, n = 1), Φ2 AGEP (Shapley Interaction Values, n = 2), Φ4 AGEP (4-Shapley Values, n = 4) and Φ10 AGEP (Shapley-GAM, n = d). The leftmost partial dependence plot is the usual plot that is often used in order to visualize Shapley Values (Lundberg et al., 2020) (the plot depicts the original Shapley Values for the observations in the test set). It takes the often observed form where the Shapley Values are scattered around an overall functional relationship. Theorem 4 and Theorem 6 make this intuition precise by specifying how the Shapley Values are related to the component functions of the Shapley-GAM. The middle and right plots illustrate that as we move towards higher-order explanations, interaction effects can be appropriately represented. As a consequence, the partial dependence plots of individual feature attributions approach the component functions of the Shapley-GAM. In this example, the function f is a kNN classiﬁer on the Folktables Income classiﬁcation task. Appendix Figure K.8 depicts the partial dependence plots of all other features. 4.2 The Shapley-GAM The following Theorem 4 shows two things. First, a subset- compliant value function gives rise to a well-deﬁned func- tional decomposition. Second, d-Shapley Values are equal to this decomposition. The transformation of the value func- tion that deﬁnes the decomposition is well-known as the Harsanyi Dividend (Harsanyi, 1982) or Möbius transform. Theorem 4 (d-Shapley Values provide a functional decomposition of f ). Fix a function f : Rd → R. Let v(x, S) be a subset-compliant value function for f . Then the d-Shapley Values represent the function f as a speciﬁc GAM that we denominate the Shapley-GAM. It is given by f (x) = ∑ S⊂[d] fS(xS) (6) with component functions f∅ = v(∅) and fS(xS) = Φ d S(x) (7) where Φd S(x) = ∑ L⊂S(−1) |S|−|L|v(xL, L). (8) For intuition about Theorem 4, consider Figure 2. It is a well-known fact that the Shapley Value of feature i not only depends on the value of that feature, but also on the values of the other features of x (compare the leftmost partial depen- dence plot in Figure 2). The reason for this is that Shapley Values subsume higher-order variable interactions into the attributions of individual features (according to formula (11), as we will see below). Now, as we successively increase n, more and more variable interactions are appropriately repre- sented in the explanations. This means that they no longer have to be subsumed into lower-order effects, which implies in turn that the lower-order components of the explanations become more distinct (middle parts of Figure 2). For n = d, all possible variable interactions can be represented in the explanations, which implies that d-Shapley Values become well-deﬁned functions of the respective features (rightmost plot in Figure 2). n-Shapley Values depend on the value function, and so does the associated functional decomposition. For the ob- servational and interventional SHAP value functions, the functional decompositions are given as follows. Corollary 5 (Observational and Interventional SHAP). For the observational SHAP value function (1), the component functions of the Shapley-GAM are given by f∅ = E[f ], fi(xi) = E[f |xi] − E[f ] fi,j(x) = E[f |xi, xj] − E[f |xi] − E[f |xj] + E[f ] fS(xS) = ∑ L⊂S(−1)|S|−|L|E[f |xL]. (9) For the interventional SHAP value function, the component functions are given by the same expression, but with the conditional expectations replaced by the causal do-operator. As will see below (Theorem 7), there is actually a one-to- one relationship between subset-compliant value functions and different functional decompositions of f . 5 FROM GENERALIZED ADDITIVE MODELS TO SHAPLEY VALUES In the previous section, we have seen that Shapley Values give rise to a functional decomposition of the original func- From Shapley Values to Generalized Additive Models and back Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure 3: Visualizing the Shapley-GAM of interventional SHAP. Figures depict d-Shapley Values, visualized as in Figure 1. Different functions on different data sets require a different degree of variable interaction. (Left) A GAM without variable interactions on the German Credit data set. (Middle Left) A gradient boosted tree on the California Housing data set. (Middle Right) A kNN classiﬁer on the Folktables Travel data set. (Right) The 8-dimensional checkerboard function (14). Additional ﬁgures for more data points and classiﬁers can be found in Appendix K. tion (via the associated value function). In this section, we show that the original Shapley Values as well as n-Shapley Values of any order are linear combinations of the com- ponent functions of this decomposition. This provides a novel motivation for Shapley Values that does not require value functions or the Shapley formula. This alternative motivation of Shapley Values is equivalent to the original motivation via value functions: For every functional decom- position of f , there is a corresponding subset-compliant value function v such that the Shapley Values derived from the decomposition and v are equal (and vice-versa). 5.1 Shapley Values from the Shapley-GAM Theorem 6 speciﬁes the way in which the different compo- nent functions of the Shapley-GAM give rise to n-Shapley Values. Theorem 6 (n-Shapley Values from the Shapley-GAM). Let f (x) = ∑ S⊂[d] fS(xS) be the decomposition of f pro- vided by the Shapley-GAM, and let Φn S(x) be the n-Shapley Values of f . Then, it holds that Φn S = fS + ∑ K⊂[d]\\S n+1≤|S|+|K| Cn−|S|,|K| fS∪K (10) with coefﬁcients Cn,m = ∑n k=0 (n k) Bk 1+m−k . Speciﬁcally, the Shapley Value of feature i is given by Φ1 i = fi + · · · + 1 k + 1 ∑ S⊂[d]\\{i},|S|=k fS∪{i} + · · · + 1 d f[d] (11) where all terms additionally depend on the point x. Theorem 6 speciﬁes how higher-order variable interactions that are present in f are subsumed into lower-order expla- nations. In the case of the original Shapley Values, this is particularly intuitive: Higher-order effects are evenly distributed among the involved features.2 Theorem 6 also speciﬁes what information about the function f is and is not contained in Shapley Values. We see that different functions f can give rise to the same n-Shapley Values as long as n < d (Grabisch, 2016). We also see that it is impossible to tell from individual Shapley Values whether the model consists of main effects or complex variable interactions. Furthermore, a feature can have zero attribution although it appears in multiple interaction effects with different signs. For a bit more intuition about the Shapley-GAM, Figure 3 illustrates the Shapley-GAM of interventional SHAP for different functions. A main point is that different predictors require a different degree of variable interaction in order to be represented as a GAM. By deﬁnition, a Glassbox-GAM (leftmost part of Figure 3) does not require any variable interaction. The other extreme is the k-dimensional checker- board function (14) (rightmost part of Figure 3), which only consists of interaction terms of order k. Many learned func- tions such gradient boosted trees (Figure 3, middle left) and the k-Nearest Neighbor (kNN) classiﬁer (Figure 3, middle right) lie in between. Overall, there is a signiﬁcant amount of variation between different methods and problems. This is also illustrated in many additional ﬁgures in Appendix K. For a quantitative analysis, see Section 7. 2For individual value functions, equation (11) is known in the literature on economic game theory (Grabisch, 1997)[Theorem 1]. Variants of it were independently re-discovered in Keevers (2020), Herren and Hahn (2022) and Hiabu et al. (2023). Sebastian Bordt, Ulrike von Luxburg 5.2 From Functional Decompositions to Subset-Compliant Value Functions We have show that every subset-compliant value function corresponds to a functional decomposition of f . We now show that the reverse is also true, that is every functional decomposition of f corresponds to a subset-compliant value function. The transformation that deﬁnes the value function is also known as the Zeta transform. Theorem 7 (From Generalized Additive Models to Value Functions). Let f (x) = ∑ S⊂[d] gS(x) be any functional decomposition of f . Deﬁne the subset-compliant value func- tion v(x, S) = ∑ L⊂S gL(x). (12) Then the functional decomposition gS is the Shapley-GAM with respect to the value function (12). Taken together, Theorem 4 and Theorem 7 establish a bi- jection between subset-compliant value functions and func- tional decompositions of f . In a sense, this implies that every functional decomposition implicitly corresponds to a notion of feature attribution via its associated value function and the Shapley formula (or, more directly, via equation (11) which is just the same). 6 RECOVERY In this section, we connect Shapley Values with interpretable models by showing that n-Shapley Values, as well as the Shapley Taylor- and Faith-Shap interaction indices, recover GAMs. In order for this to be the case, the order of the ex- planation has to be at least as large as the order of the GAM. Theorem 8 (Shapley-based Explanations Recover GAMs). Let f be a generalized additive model of order n. Assume that either (a) the value function is given by observational SHAP and the individual features are independent random variables, or (b) the value function is given by interventional SHAP. Then, n-Shapley Values, as well as the Shapley Taylor- and Faith-Shap interaction indices of order n, recover a repre- sentation of f as a GAM. In fact, all the interaction indices are equal to each other and given by Φn S(x) = fS(xS) where fS are the component functions of the Shapley-GAM. Theorem 8 implies that the SHAP feature attributions re- cover GAMs without variable interactions and that Shapley Interaction Values recover GAMs with interactions of at most two variables at a time. Unlike our previous results, Theorem 8 depends on the choice of the value function. This is because the recovery property holds if (1) the inter- action index can be written like in equation (10), and (2) the Shapley-GAM is a GAM or order n — and the second point depends on the value function. As it turns out, the independence assumption in part (a) of Theorem 8 is indeed necessary (see Appendix D). This is interesting insofar as it establishes the usefulness of the interventional SHAP value function from a purely statistical perspective, that is without any causal motivation (for a discussion about the differences between observational and interventional SHAP, see also Chen et al. (2020)). Figure 4 (Top) illustrates the recovery result for a GAM without variable interactions. For this example, we explic- itly resort to the default implementation of the Kernel SHAP algorithm, in order to see whether there is any signiﬁcant ap- proximation error (Kernel SHAP approximates the Shapley Values of the interventional SHAP value function). The top part of Figure 4 depicts the shape curve of the feature POW- PUMA in the GAM (blue curve), as well as the associated Kernel SHAP values (red dots). The Kernel SHAP values lie almost exactly on the shape curve of the GAM, which means that the recovery property holds fairly precisely, at least in this simple example. 7 IS THERE AN ACCURACY- COMPLEXITY TRADE-OFF? In the previous sections, we have outlined the connections between Shapley Values and GAMs on a theoretical level. In this section, as well as in the next section, we turn to more practical concerns. In this section, we investigate the number of variable interactions that are present in various standard classiﬁers. In order to do so, we rely on a number of low-dimensional data sets on which we can reliably es- timate the Shapley-GAM decompositions of the different learned predictors (compare Section 8). It is interesting to compare this against the accuracy: Because models with more variable interactions can represent strictly more func- tions than models with less variable interactions, it is natural to suspect that more accurate classiﬁers might exhibit higher degrees of variable interaction (Dziugaite et al., 2020). We suggest to measure the extent of variable interaction that is present in a given classiﬁer with the following quantity E x∼D   ∑ S⊂[d] |S| · |fS(xS)|   ∕ E x∼D   ∑ S⊂[d] |fS(xS)|   . (13) where fS are the component functions of the Shapley-GAM decomposition of f , using interventional SHAP. Figure 4 (Middle) illustrates the relationship between the predictive accuracy and our measure (13) for different pre- From Shapley Values to Generalized Additive Models and back Figure 4: Top: Shapley Values recover GAMs without vari- able interactions (Theorem 8). To create this ﬁgure, we ﬁrst trained a GAM on the Folktables Travel data set using the InterpretML package (Nori et al., 2019). We then computed the Kernel SHAP values for the decision function of the GAM using the shap package (Lundberg and Lee, 2017). For the feature POWPUMA, the Figure depicts the ground- truth variable effect in the GAM in blue, and the associated Kernel SHAP values for data points from the test set as red dots. We see that the red dots lie on the blue line, that is Kernel SHAP recovers the component function of the GAM. Middle: The average degree of variable interaction (13) in the Shapley-GAM of interventional SHAP for various standard classiﬁers. The ﬁgure depicts predictive accuracy versus the average degree of variable interaction. Bottom: Estimating higher-order variable interactions requires pre- cise evaluations of the value function. A simple way to study this is by estimating the k-dimensional checkerboard func- tion (14). Left: 3-way variable interactions can be precisely estimated. Right: 7-way variable interactions can be reliably detected, but precise estimation requires prohibitively many samples. dictors f . The ﬁgure depicts four different kinds of classi- ﬁers: A Glassbox-GAM without variable interactions (Nori et al., 2019), a gradient boosted tree (Chen and Guestrin, 2016), a random forest, and a kNN classiﬁer (Pedregosa et al., 2011). We compare these classiﬁers on four different data sets: Folktables Travel and Income (Ding et al., 2021), Iris, and German Credit. Details on the data sets and training procedures are in Appendix J. As far as accuracy is concerned, we see from Figure 4 that GAMs without variable interactions perform fairly well against the more complicated classiﬁers — a fact that has often been observed in the literature (Caruana et al., 2015; Agarwal et al., 2021a). On the more complex data sets, however, there is usually a model with variable interactions and slightly better accuracy 3 As far as the degree of variable interaction is concerned, we see that there is a large amount of variation in between the different classiﬁer. Especially interesting is the kNN classiﬁer. It tends to per- form worse in terms of accuracy than the interpretable GAM, but exhibits very high degree of variable interaction. Ob- serve that the kNN classiﬁer can also be considered inter- pretable (by explaining the workings of the classiﬁer and providing the k data points that are responsible for the clas- siﬁcation). Therefore, this example shows that a high degree of variable interaction in the Shapley-GAM does not imply that a function is hard to explain per se. This simple empirical investigation suggests that the rela- tion between accuracy and the average degree of variable interaction in the Shapley-GAM is nuanced: While some degree of interaction seems necessary in order to achieve competitive accuracy, some classiﬁers seem to exhibit more interaction than that. In some cases, the correlation might even be negative (as for the kNN classiﬁer). 8 COMPUTATION AND ESTIMATION We now turn to the practical question of computing n- Shapley Values. In this work, we take the trivial approach and simply evaluate the value function for all possible sub- sets S ⊂ [d], then combine the respective terms according to Deﬁnition 3. A Python package to compute n-Shapley Values, as well as the Shapley Taylor- and Faith-Shap in- teraction indices, is available https://github.com/ tml-tuebingen/nshap. Even for the original Shap- ley Values, it is well-known that the number of required evaluations of the value function grows exponentially in the number of features. For this reason, there exist efﬁcient approximations such as Kernel SHAP, as well as efﬁcient implementations for certain function classes such as tree based models (Lundberg and Lee, 2017). We hold that 3The InterpretML package (Nori et al., 2019) allows to include interactions between pairs of variables which reportedly allows to be on par with black-box models on many data sets. Compare also (Lou et al., 2012). Sebastian Bordt, Ulrike von Luxburg such computationally efﬁcient approximations are also be possible for n-Shapley Values. Instead of focusing on the well-known computational aspect of the problem, we want to focus on the estimation aspect which seems much less studied. Note that n-Shapley Values are a statistic that is subject to sampling variation. The same is true for our visualizations (as in Figure 1), which are summary statistics of n-Shapley Values. This is because both the observational and the interventional SHAP value function require to estimate an expectation. We now asses with a simple empirical analysis up to which order interaction effects can be estimated in prac- tice. We consider the k-dimensional checkerboard function Bk : [0, 1]d → {0, 1} given by Bk(x1, . . . , xd) = {0 if ∑k i=0⌊(λ · xi)⌋ mod 2 = 0 1 otherwise (14) where λ > 2 parameterizes the number of checkers along each axis. If data points are uniformly distributed in the unit cube [0, 1] d, then the Shapely-GAM of interventional SHAP of Bk is given by the single k-th order interaction effect fx1,...,xk (x1, . . . , xk) = Bk(x1, . . . , xk, 0, . . . , 0). The question now is how precisely we have to estimate the expectation Ez∼D [f (z) | do(xS)] if we want to precisely estimate a kth-order interaction effect. The bottom part of Figure 4 depicts the result of estimating 10-Shapley Values when the underlying function is the 3- or 7-dimensional checkerboard function, respectively. The x-axis depicts the number of samples used to estimate the value function, ranging from 100 to 1 000 000. The y-axis depicts the order of the estimated effects, with conﬁdence bands that account for 5 randomly sampled data sets. From the ﬁgure, we observe that if the number of samples is small in relation to the magnitude of the interaction effect, then the estimation results in spurious lower-order effects. For k = 3, these effects vanish with sufﬁciently many samples, which means that the checkerboard function is precisely estimated. For k = 7, the presence of the higher-order interaction effect can be reliably detected, but not precisely estimated given reasonably many samples. In this simple analysis, we see that interaction effects of or- der larger that 2 can be precisely estimated given sufﬁciently many samples. We also see that functions with high-order interactions are difﬁcult to estimate and can result in arti- facts. Figures for all interaction orders k = 2, . . . , 10 and a discussion of the precision of the depicted visualizations of n-Shapley Values can be found in Appendix C. 9 DISCUSSION This work provides a functionally-grounded characteriza- tion of Shapley Values as they are being used in explainable machine learning (Doshi-Velez and Kim, 2017). Explain- able machine learning is often believed to be an impor- tant component in societal applications of machine learning (Wachter et al., 2017; Kaminski and Urban, 2021; Kästner et al., 2021). At the same time, current approaches face a lot of criticism, for example because they are non-robust or unable to provide the desired level of model understanding (as well as for a variety of other concerns) (Lipton, 2018; Kumar et al., 2020; Slack et al., 2020; Bordt et al., 2022). In this situation, we believe that a precise understanding of the mechanics of popular explainability methods, such as the one presented in this work, is a good ﬁrst step toward an informed discussion of what we can and cannot achieve. Some of our results stand in contrast to conventional wis- dom around Shapley Values, and offer a novel perspective on local-post hoc explanation algorithms. For example, we have seen that Shapley Values depend on the coordinates of the point that we attempt to explain, but not on the lo- cal neighbourhood of that point — the recovery example with the step function in Figure 4 suggests that this is also the case for the approximations of the Shapley Value that are used in practice. We have further seen that the original Shapley Values are able to faithfully explain non-linear func- tions, as long as the non-linearity is restricted to the speciﬁc form permitted by GAMs. As such, our results highlight the differences between Shapley Values and other feature attribution methods, for example those that are related to the gradient (Garreau and von Luxburg, 2020; Agarwal et al., 2021b), and those that perform local function approximation (Han et al., 2022). The demonstrated connections between value functions and functional decompositions effectively link the literature on feature attributions with the tools developed in the statis- tics literature on functional decompositions (Hooker, 2007; Lengerich et al., 2020). This raises the question of whether criteria for functional decompositions can be useful to un- derstand feature attributions. Here, two concurrent works made signiﬁcant contributions: Hiabu et al. (2023) show that the value function of interventional SHAP can be motivated with a causal assumption on the associated functional de- composition. Herren and Hahn (2022) outline connections between observational SHAP and functional ANOVA. While our work gives a functionally-grounded analysis of Shapley Values for any function, as well as recovery guar- antees for Shapley Values and GAMs, we do not claim that Shapley Values are an appropriate post-hoc explanation method for any function (Kumar et al., 2021; Tan et al., 2022). Instead, the purpose of our work is to highlight the connections between a post-hoc explanation method and a class of interpretable models. Overall, however, we believe that many properties of Shapley Values have the potential to be more clearly understood using our perspective of func- tional decompositions. From Shapley Values to Generalized Additive Models and back Acknowledgements This work was done in part while Sebastian was visiting the Simons Institute for the Theory of Computing. Sebastian would like to thank Rich Caruana, Gyorgy Turan, Michal Moshkovitz and Tosca Lechner for many fruitful discussions about variable interactions. The authors would also like to thank Markus Scheuer and René Gy for linking Lemma 10 to the literature on Bernoulli numbers, and the anonymous reviewers whose comments helped to improve this paper. This work has been supported by the German Research Foundation through the Cluster of Excellence “Machine Learning – New Perspectives for Science\" (EXC 2064/1 number 390727645), the BMBF Tübingen AI Center (FKZ: 01IS18039A), and the International Max Planck Research School for Intelligent Systems (IMPRS-IS). References R. Agarwal, L. Melnick, N. Frosst, X. Zhang, B. Lengerich, R. Caruana, and G. E. Hinton. Neural additive mod- els: Interpretable machine learning with neural nets. In NeurIPS, 2021a. S. Agarwal, S. Jabbari, C. Agarwal, S. Upadhyay, S. Wu, and H. Lakkaraju. Towards the uniﬁcation and robustness of perturbation and gradient based explanations. In ICML, 2021b. E. Albini, J. Long, D. Dervovic, and D. Magazzeni. Coun- terfactual shapley additive explanations. In ACM FAccT, 2022. S. Bordt, M. Finck, E. Raidl, and U. von Luxburg. Post-hoc explanations fail to achieve their purpose in adversarial contexts. In ACM FAccT, 2022. R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2015. H. Chen, J. D. Janizek, S. Lundberg, and S.-I. Lee. True to the model or true to the data? arXiv preprint arXiv:2006.16234, 2020. T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2016. I. Covert, S. Lundberg, and S. Lee. Explaining by removing: A uniﬁed framework for model explanation. JMLR, 2021. F. Ding, M. Hardt, J. Miller, and L. Schmidt. Retiring adult: New datasets for fair machine learning. In NeurIPS, 2021. F. Doshi-Velez and B. Kim. Towards a rigorous sci- ence of interpretable machine learning. arXiv preprint arXiv:1702.08608, 2017. G. K. Dziugaite, S. Ben-David, and D. M. Roy. Enforc- ing interpretability and its statistical impacts: Trade-offs between accuracy and interpretability. arXiv preprint arXiv:2010.13764, 2020. D. Garreau and U. von Luxburg. Explaining the explainer: A ﬁrst theoretical analysis of lime. In AISTATS, 2020. H. Gould and J. Quaintance. Bernoulli numbers and a new binomial transform identity. J. Integer Seq., 2014. M. Grabisch. K-order additive discrete fuzzy measures and their representation. Fuzzy sets and systems, 1997. M. Grabisch. Bases and transforms of set functions. In On Logical, Algebraic, and Probabilistic Aspects of Fuzzy Set Theory. Springer, 2016. M. Grabisch and M. Roubens. An axiomatic approach to the concept of interaction among players in cooperative games. International Journal of game theory, 1999. R. Gy. Combinatorial identity involving bernoulli numbers. Mathematics Stack Exchange, 2022. URL https:// math.stackexchange.com/q/4520567. T. Han, S. Srinivas, and H. Lakkaraju. Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations. arXiv preprint arXiv:2206.01254, 2022. J. C. Harsanyi. A simpliﬁed bargaining model for the n- person cooperative game. In Papers in game theory. Springer, 1982. T. Hastie and R. Tibshirani. Generalized Additive Models. Chapman Hall & CRC. 1990. A. Herren and P. R. Hahn. Statistical aspects of SHAP: Func- tional ANOVA for model interpretation. arXiv preprint arXiv:2208.09970, 2022. T. Heskes, E. Sijben, I. G. Bucur, and T. Claassen. Causal shapley values: Exploiting causal knowledge to explain individual predictions of complex models. In NeurIPS, 2020. M. Hiabu, J. T. Meyer, and M. N. Wright. Unifying local and global model explanations by functional decomposition of low dimensional structures. In AISTATS, 2023. A. Holzinger, R. Goebel, R. Fong, T. Moon, K.-R. Müller, and W. Samek. xxai-beyond explainable artiﬁcial intelli- gence. In International Workshop on Extending Explain- able AI Beyond Deep Models and Classiﬁers. Springer, 2022. G. Hooker. Generalized functional anova diagnostics for high-dimensional functions of dependent variables. Jour- nal of Computational and Graphical Statistics, 2007. D. Janzing, L. Minorics, and P. Blöbaum. Feature relevance quantiﬁcation in explainable ai: A causal problem. In AISTATS, 2020. N. Jethani, M. Sudarshan, I. C. Covert, S.-I. Lee, and R. Ran- ganath. Fastshap: Real-time shapley value estimation. In ICLR, 2021. Sebastian Bordt, Ulrike von Luxburg M. Kaminski and J. Urban. The right to contest ai. Columbia Law Review, 2021. L. Kästner, M. Langer, V. Lazar, A. Schomäcker, T. Speith, and S. Sterz. On the relation of trust and explainability: Why to engineer for trustworthiness. In IEEE 29th In- ternational Requirements Engineering Conference Work- shops (REW), 2021. T. L. Keevers. A power series expansion of feature impor- tance. Technical report, 2020. R. Kommiya Mothilal, D. Mahajan, C. Tan, and A. Sharma. Towards unifying feature attribution and counterfactual explanations: Different means to the same end. In AAAI/ACM Conference on AI, Ethics, and Society (AIES), 2021. S. Krishna, T. Han, A. Gu, J. Pombra, S. Jabbari, S. Wu, and H. Lakkaraju. The disagreement problem in explainable machine learning: A practitioner’s perspective. arXiv preprint arXiv:2202.01602, 2022. I. Kumar, C. Scheidegger, S. Venkatasubramanian, and S. Friedler. Shapley residuals: Quantifying the limits of the shapley value for explanations. NeurIPS, 2021. I. E. Kumar, S. Venkatasubramanian, C. Scheidegger, and S. Friedler. Problems with shapley-value-based explana- tions as feature importance measures. In ICML, 2020. H. Lakkaraju, J. Adebayo, and S. Singh. Explaining ma- chine learning predictions: State-of-the-art, challenges, opportunities. Tutorial at NeurIPS, 2020. E. Lee, D. Braines, M. Stifﬂer, A. Hudler, and D. Harborne. Developing the sensitivity of lime for better machine learning explanation. In Artiﬁcial Intelligence and Ma- chine Learning for Multi-Domain Operations Applica- tions, 2019. B. Lengerich, S. Tan, C.-H. Chang, G. Hooker, and R. Caru- ana. Purifying interaction effects with the functional anova: An efﬁcient algorithm for recovering identiﬁable additive models. In AISTATS, 2020. B. J. Lengerich, R. Caruana, M. E. Nunnally, and M. Kellis. Death by round numbers and sharp thresholds: How to avoid dangerous ai ehr recommendations. medRxiv, 2022. J. Lin, C. Zhong, D. Hu, C. Rudin, and M. Seltzer. General- ized and scalable optimal sparse decision trees. In ICML, 2020. Z. C. Lipton. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 2018. Y. Lou, R. Caruana, and J. Gehrke. Intelligible models for classiﬁcation and regression. In ACM SIGKDD Confer- ence on Knowledge Discovery and Data Mining, 2012. Y. Lou, R. Caruana, J. Gehrke, and G. Hooker. Accurate intelligible models with pairwise interactions. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2013. S. M. Lundberg and S.-I. Lee. A uniﬁed approach to inter- preting model predictions. NeurIPS, 2017. S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb, N. Bansal, and S.-I. Lee. From local explanations to global understand- ing with explainable ai for trees. Nature machine intelli- gence, 2020. C. Molnar. Interpretable machine learning. Lulu.com, 2020. M. Moshkovitz, S. Dasgupta, C. Rashtchian, and N. Frost. Explainable k-means and k-medians clustering. In ICML, 2020. H. Nori, S. Jenkins, P. Koch, and R. Caruana. Interpretml: A uniﬁed framework for machine learning interpretability. arXiv preprint arXiv:1909.09223, 2019. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour- napeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit- learn: Machine learning in Python. JMLR, 2011. M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you? explaining the predictions of any classiﬁer. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2016. C. Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 2019. C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova, and C. Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. Statistics Surveys, 2022. W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. Müller. Explaining deep neural networks and be- yond: A review of methods and applications. Proceedings of the IEEE, 109(3):247–278, 2021. L. Shapley. A value for n-person games., 1953. D. Slack, S. Hilgard, E. Jia, S. Singh, and H. Lakkaraju. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In AAAI/ACM Conference on AI, Ethics, and Society, 2020. D. Slack, A. Hilgard, S. Singh, and H. Lakkaraju. Reliable post hoc explanations: Modeling uncertainty in explain- ability. In NeurIPS, 2021. M. Sundararajan and A. Najmi. The many shapley values for model explanation. In ICML, 2020. M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In ICML, 2017. M. Sundararajan, K. Dhamdhere, and A. Agarwal. The shapley taylor interaction index. In ICML, 2020. Y. S. Tan, A. Agarwal, and B. Yu. A cautionary tale on ﬁtting decision trees to data from additive models: gener- alization lower bounds. In AISTATS, 2022. From Shapley Values to Generalized Additive Models and back C.-P. Tsai, C.-K. Yeh, and P. Ravikumar. Faith-shap: The faithful shapley interaction index. arXiv preprint arXiv:2203.00870, 2022. S. Wachter, B. Mittelstadt, and C. Russell. Counterfactual explanations without opening the black box: Automated decisions and the gdpr. Harv. JL & Tech., 31:841, 2017. F. Wang and C. Rudin. Falling rule lists. In AISTATS, 2015. Z. J. Wang, A. Kale, H. Nori, P. Stella, M. E. Nunnally, D. H. Chau, M. Vorvoreanu, J. Wortman Vaughan, and R. Caruana. Interpretability, then what? editing machine learning models to reﬂect human knowledge and values. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. Sebastian Bordt, Ulrike von Luxburg A n-Shapley Values This section details the properties of n-Shapley Values. A.1 Bernoulli numbers The Bernoulli numbers 1 Bn are deﬁned by B0 = 1 and n∑ k=0 (n + 1 k )Bk = 0 ∀n ≥ 1. (15) In this paper, the Bernoulli numbers arise as the coefﬁcients that make n-Shapley Values sum to the prediction (Proposition 12). In fact, equation (15) arises directly from the proof of Proposition 12. The Bernoulli numbers can be computed recursively by re-writing into (15) Bn = −1 n + 1 n−1∑ k=0 Bk ( n + 1 k ) ∀n ≥ 1. (16) In a certain sense, the entire combinatorics around n-Shapley Values relies on the properties of the Bernoulli numbers. In particular, the proofs of Theorem 4 and Theorem 6 rely on the following two Lemmas. Lemma 9. For all n ≥ 1, it holds that n∑ k=1 Bk n − k + 1 (n k ) = −1 n + 1 . (17) Proof. We re-arrange the sum to get n∑ k=1 Bk n − k + 1 ( n k ) = 1 n + 1 n∑ k=0 ( n + 1 k )Bk − B0 n + 1 = −1 n + 1 (18) where the second equality follows from (15). Lemma 10. For all n, m ≥ 0, it holds that n∑ k=0 m∑ l=0 (n k )(m l ) (n − k)!(m − l)! (n + m − k − l + 1)! (−1) lBk+l =    1 if n = 0 0 otherwise. (19) Lemma 10 follows from standard results for the Bernoulli numbers (Gould and Quaintance, 2014)[Theorem 2]. A proof is contained in Appendix I. A.2 Additivity and Efﬁciency From the recursive deﬁnition of the n-Shapley Values in Deﬁnition 3, a straightforward calculation shows that Φn S(x) = n−|S|∑ k=0 ∑ K⊂[d]\\S, |K|=k Bk ∆S∪K(x) (20) which is an alternative non-recursive deﬁnition of n-Shapley Values. 1An introduction and discussion about Bernoulli numbers can be found, for example, in the corresponding Wikipedia article at https://en.wikipedia.org/wiki/Bernoulli_number. From Shapley Values to Generalized Additive Models and back n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Bn 1 −1 2 1 6 0 −1 30 0 1 42 0 −1 30 0 5 66 0 −691 2730 0 7 6 0 −3617 510 0 43867 798 0 Table A.1: The ﬁrst 20 Bernoulli numbers. Proposition 11 (Additivity). For all 1 ≤ n ≤ d and all f, g : Rn → R, we have Φn S(x; f + g) = Φn S(x; f ) + Φn S(x; g). (21) Proof. By deﬁnition, Φn S is linear in ∆S, and ∆S is linear in the value function v. Therefore, the linearity of Φn S in f follows from the linearity of v in f , i.e. from the fact that vf +g(x, S) = vf (x, S) + vg(x, S). Proposition 12 (Efﬁciency). For all 1 ≤ n ≤ d, it holds that ∑ S⊂[d] 1≤|S|≤n Φn S(x) = v([d]) − v(∅). (22) Proof. For n = 1, the statement follows from the efﬁciency of the original Shapley Values. We assume that the statement holds for n − 1 and re-arrange the sum ∑ S⊂[d] 1≤|S|≤n Φn S(x) = ∑ S⊂[d] 1≤|S|<n Φn S(x) + ∑ S⊂[d] |S|=n Φn S(x) = ∑ S⊂[d] 1≤|S|<n      Φn−1 S (x) + Bn−|S| ∑ K⊂[d]\\S |K|+|S|=n ∆S∪K(x)      + ∑ S⊂[d] |S|=n ∆S(x) = ∑ S⊂[d] 1≤|S|≤n−1 Φn−1 S (x) + ∑ S⊂[d] 1≤|S|<n ∑ K⊂[d]\\S |K|+|S|=n Bn−|S| ∆S∪K(x) + ∑ S⊂[d] |S|=n ∆S(x). (23) Notice that the ﬁrst term is equivalent to v([d]) − v(∅) by the induction hypothesis. It remains to show that ∑ S⊂[d] 1≤|S|<n ∑ K⊂[d]\\S |K|+|S|=n Bn−|S| ∆S∪K(x) + ∑ S⊂[d] |S|=n ∆S(x) = 0. (24) Notice that both sums are over sets of length n. In the ﬁrst sum, each sets occurs multiple times. In the second sum, each set occurs exactly once. By counting the occurrences of each set in the ﬁrst sum we see that (24) holds if n−1∑ s=1 Bn−s ( n s ) + 1 = 0. (25) If we set B0 = 1, this holds if and only if n−1∑ k=0 Bk (n k ) = 0, (26) which is the deﬁning property of the Bernoulli numbers (15). In summary, we see that the Bernoulli numbers are the coefﬁcients that balance the terms in the ﬁrst sum in equation (24). Sebastian Bordt, Ulrike von LuxburgFeature 1Feature 2Feature 3Feature 4−0.1 0.0 0.1 0.2 n-Shapley Values (a) Example 1Feature 1Feature 2Feature 3Feature 4−0.1 0.0 0.1 0.2 n-Shapley Values (b) Example 2Feature 1Feature 2Feature 3Feature 4 −0.1 0.0 0.1 0.2 n-Shapley Values (c) Example 3Feature 1Feature 2Feature 3Feature 4 −0.1 0.0 0.1 0.2 0.3 n-Shapley Values (d) Example 4Feature 1Feature 2Feature 3Feature 4 0.0 0.2 n-Shapley Values (e) Example 5 Figure B.1: Examples that illustrate the proposed visualization technique for n-Shapley Values. A.3 Relationship Between n-Shapley Values of Different Order The following proposition is a straightforward extension of Theorem 6. Proposition 13 (Relationship Between n-Shapley Values of Different Order). For m ≤ n, let Φm S and Φn S be the m- and n-Shapley Values, respectively. Then, the m-Shapley Values can be computed from the n-Shapley Values by Φm S (x) = Φ n S + ∑ K⊂[d]\\S, m−|S|<|K|≤n−|S| βm−|S|,|K| Φn S∪K(x). (27) Speciﬁcally, it holds that Φ1 i = Φ n i + 1 2 ∑ j̸=i Φn i,j + · · · + 1 n ∑ K⊂[d]\\{i} |K|=n−1 Φn K∪i (28) which is the basis for the visualizations in the paper. Proof. The proposition follows from the counting argument used in the proof of Theorem 6. B Visualizing n-Shapley Values Due to the large number of terms involved in n-Shapley Values of higher order, visualizing these explanations is difﬁcult. However, Proposition 13 (which is really a variant of Theorem 6) states that higher-order variable interactions in n-Shapley Values are related to the original Shapley Values via a simple lump-sum formula. This gives rise to the idea of simply visualizing, for each feature, the respective components of the sum. To illustrate this idea, let us consider a simple example. Let us begin with four different features and the usual Shapley Values. Say the ﬁrst two features have attribution zero, the third feature has attribution 0.2, and the fourth feature has attribution −0.1. These Shapley Values can be visualized as usual, depicted in Figure B.1a. Now, let us add a second-order interaction effect, say Φ2 2,3 = 0.1. Because this interaction effect would ultimately be added to the attributions of feature 2 and feature 3 with a factor of 1 2 , let us simply add two corresponding bars to the attributions of these features, with the color indicating that it is a second-order effect. From the resulting Figure B.1b, it can then be seen that we have two main effects and a single positive interaction effect between features 2 and 3. If there were another interaction effect, say Φ2 3,4 = −0.1, we would proceed in the same way, taking care of the sign. From the resulting Figure B.1c, it can be seen that there are two main effects and a number of second-order interactions. With higher-order interactions we proceed accordingly, as illustrated for Φ3 2,3,4 = 0.1 (Figure B.1d) and Φ4 1,2,3,4 = −0.1 (Figure B.1d). Note that while this form of visualization faithfully depicts the relative magnitude of the different variable interactions, it is in general not possible to tell from the ﬁgures which variables interact with each other, for example when there are a number of different second-order effects. C Estimating n-Shapley Values Here we collect some additional details regarding the estimation of n-Shapley Values. We note that the discussion here is not exhaustive. Our objective is to (1) raise awareness for the fact that computing n-Shapley Values incurs an estimation From Shapley Values to Generalized Additive Models and back 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects2d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects3d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects4d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects5d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects6d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects7d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects8d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects9d Checkerboard Function 100 1000 10 000 100 000 1 000 000 Number of points sampled 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Order of estimated effects10d Checkerboard Function Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure B.2: Estimating higher-order variable interactions requires precise evaluations of the value function. A simple way to study this is by estimating the k-dimensional checkerboard function (14). Compare Figure 4 in the main paper. problem, and (2) ensure that the results presented in the main paper are precisely estimated and not statistical artifacts. Figure B.2 depicts the result of estimating the k-dimensional checkerboard function (14) for all values k = 2, . . . , 10 (compare Section 8 in the main paper). As already discussed in the main paper, we can see from the ﬁgure that estimation becomes gradually harder as we increase the order of interaction. In Figure C.3, we assess the degree up to which our visualizations are effected by the presence of spurious interaction effect of intermediate order, as observed when estimating a checkerboard function with too few samples. The ﬁgure visualizes the Shapley-GAM decomposition of a kNN classiﬁer on the Folktables Travel data set, estimated with 500, 5000 and 133549 samples per evaluation of the value function, respectively. By comparing the left and middle part of Figure C.3 (estimation with 500 and 5000 samples, respectively), we see that 500 samples are to few and result in the presence of spurious interaction effects, for example of of order 4 and 5. This can be seen from the fact that some of these effects vanish as we increase the number of samples. By comparing the middle and right part of Figure C.3 (estimation with 5000 and 133549 samples, respectively), we see that estimation with 5000 samples is already quite precise for this kNN classiﬁer. This can be seen from the fact that signiﬁcantly increasing the number of samples does not have any signiﬁcant effect on the Sebastian Bordt, Ulrike von LuxburgJWTRPOWPUMAPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 −0.1 0.0 0.1 0.2 Shapey-GAM, 500 SamplesJWTRPOWPUMAPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 −0.1 0.0 0.1 0.2 Shapey-GAM, 5000 SamplesJWTRPOWPUMAPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 −0.1 0.0 0.1 0.2 Shapey-GAM, 133549 Samples Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure C.3: Estimating higher-order interactions with too few samples can result in spurious interaction effects of intermediate order. These effects are also visible in our visualizations. Left: Estimation with 500 samples per evaluation of the value function results in spurious interaction effects. Middle: This can be seen from the fact that parts of the estimated effects vanish if we increase the number of samples to 5000 per evaluation of the value function. Right: Using all 133549 observations in the training data per evaluation of the value function, we get almost the same visualization as for 5000 samples. The function in this example is a kNN classiﬁer and the data set is the Folktables Travel data set. visualization. 2 Table K.2 depicts the individual terms that underlie the visualization in Figure C.3. From Table K.2, we see that main effects are precisely estimated even with 500 samples. However, many relatively small higher-order coefﬁcients are not very precisely estimated even for N = 5000. Note that the latter point is not in contrast to the fact that Figure C.3 is precisely estimated for N = 5000. Figure C.3 depicts summary statistics that are more precisely estimated than the individual components. D The Statistical Independence Assumption for Observational SHAP is Necessary In this section we give a simple example to demonstrate that the assumption of independent random variables for the observational SHAP value function in Theorem 8 is indeed necessary. Consider the GAM of order 1 f (x1, x2) = x1 + x2. Assume that x1 and x2 are correlated normal random variables ( x1 x2 ) ∼ N ((0 0 ) , ( 1, ρ ρ, 1 )) with 0 ≤ ρ ≤ 1. We have E[x2|x1] = ρx1. A simple calculation shows that the Shapley-GAM of observational SHAP is given by f∅ = 0, f1(x1) = (1 + ρ)x1, f2(x2) = (1 + ρ)x2, f (x1, x2) = −ρ(x1 + x2). According to Theorem 6, the observational SHAP values are then given by Φ1 = (1 + ρ 2 )x1 − ρ 2 x2, Φ2 = (1 + ρ 2 )x2 − ρ 2 x1. Clearly, recovery does not hold: Despite the fact that the underlying function is a GAM of order 1, the Shapley-GAM is a GAM of order 2. The Shapley Values also depend on both coordinates – hence they are not well-deﬁned functions of the individual coordinates. 2This could of course be discussed much more rigorously. From Shapley Values to Generalized Additive Models and back In contrast, the Shapley-GAM of the interventional SHAP value function is given by f∅ = 0, f1(x1) = x1, f2(x2) = x2. Moreover, the interventional SHAP values are given by Φ1 = x1, Φ2 = x2, that is recovery holds with the interventional SHAP value function (as guaranteed by Theorem 8). E Proof of Theorem 4 Proof of Theorem 4. We are going to show that Φd S(x) = ∑ L⊂S(−1) |S|−|L|v(xL, L). (29) Note that the RHS evaluates the value function v only for sets L ⊂ S. From the assumption that the value function is subset-compliant, it follows that the RHS is a well-deﬁned function of xS. According to Proposition 12 (efﬁciency), the d-Shapley Values sum to v(x) − v(∅) which implies the Theorem. To show (29), we consider the non-recursive deﬁnition of n-Shapley Values 20 and then substitute the deﬁnition of ∆S(x) from Deﬁnition 3. Φd S(x) = d−|S|∑ k=0 ∑ K⊂[d]\\S, |K|=k Bk ∆S∪K(x) = d−|S|∑ k=0 ∑ K⊂[d]\\S, |K|=k Bk ∑ T ⊂[d]\\(S∪K) (d − |T | − |S| − |K|)!|T |! (d − |S| − |K| + 1)! ∑ L⊂S∪K(−1) |S|+|K|−|L|v(x, L ∪ T ). = ∑ K⊂[d]\\S ∑ T ⊂[d]\\(S∪K) B|K| (d − |T | − |S| − |K|)!|T |! (d − |S| − |K| + 1)! ∑ L⊂S∪K(−1) |S|+|K|−|L|v(x, L ∪ T ). (30) Where the last equation follows from the realization that we are summing over all possible subsets of [d] \\ S. In equation (30), we are summing over the value of the same sets multiple times. Let us ﬁx a set M = L ∪ T and count how often it occurs in the sum. First note that v(x, M ) occurs exactly once for every set K, namely by choosing T = M \\(S ∪K) and L = M ∩ (S ∪ K). Since the coefﬁcients do not only depend on the size of K, but also on |T | and |L|, let us partition the set K = K1 ∪ K2 = {K ∩ M } ∪ {K \\ M }. Let n1 = |M \\ S| and n2 = |[d] \\ (S ∪ M )| denote the maximum sizes of both partitions. With this counting argument, we arrive at (−1) |S|−|M | ∑ K1⊂M \\S ∑ K2⊂[d]\\(S∪M ) B|K1|+|K2| (n2 − |K2|)!(n1 − |K1|)! (n1 + n2 − |K1| − |K2| + 1)! (−1) |K2| (31) occurrences of the term v(x, M ). Notice that equation (31) is equal to (−1)|S|−|M | n1∑ k1=0 n2∑ k2=0 (n1 k1 )(n2 k2 ) (n2 − k2)!(n1 − k1)! (n1 + n2 − k1 − k2 + 1)! (−1) k2Bk1+k2 (32) The desired result now follows from the properties of the Bernoulli numbers. In particular, since M ⊂ S ⇐⇒ n1 = 0, we see from Lemma 10 that (32) equals (−1)|S|−|M | if M ⊂ S and 0 otherwise. Comparing the terms for all possible sets M ⊂ [d], we see that (30) equals (29). Note that if we ﬁx the point x, then the Shapley-GAM at x is equivalent to the Moebious transform of the measure v(x, ·). From this perspective, Theorem 4 can be seen as an application of Theorem 2 in Grabisch (1997). Sebastian Bordt, Ulrike von Luxburg F Proof of Theorem 6 Proof of Theorem 6. According to Theorem 4, the d-Shapley Values can be written as Φd S(x) = fS(x) (33) where fS(x) are the component functions of the Shapley-GAM. Hence, the d-Shapley Values are a linear combination of the component functions of the Shapley-GAM. From the recursive deﬁnition of the n-Shapley Values, we see that Φn S(x) = Φ n+1 S (x) − B1+n−|S| ∑ K⊂[d]\\S,|K|+|S|=n+1 Φn+1 S∪K(x) (34) that is the n-Shapley Values are a linear combination of the terms involved in the n + 1-Shapley Values. By induction, we see that the n-Shapley Values are linear combinations of the component functions of the Shapley-GAM. It remains to determine the coefﬁcients Cn,m. We present a counting argument that is based on the recurrence relation (34). In this counting argument, we ﬁrst determine the coefﬁcients Dn,m where the ﬁrst index corresponds to the distance between |S| and the order of the Shapley Values, and the second index corresponds to the different between the size of the interaction effect and the order of the Shapley Values. Suppose that we are computing n-Shapley Values. If we use equation (34) to proceed recursively from d-Shapley Values to n-Shapley Values, then the ﬁrst time that the component function fS∪K is being added to Φm S is during the computation of the (|S| + |K| − 1)-Shapley Values. According to equation (34), the linear coefﬁcient will simply be D|K|−1,1 = −B|K|. The second time that the component function fS∪K is being added to Φm S is during the computation of the (|S| + |K| − 2)-Shapley Values. This is because we have previously added −B1fS∪K to all the terms of order |S| + |K| − 1 that are a subset of S ∪ K. There are (|K| 1 ) such terms, and we are now adding all of them to fS, using the coefﬁcient −B|K|−1. This means that we arrive at a total coefﬁcient of D|K|−2,2 = −B|K| + B|K|−1 (|K| 1 )B1. (35) By a similar argument we arrive at a coefﬁcient of D|K|−3,3 = −B|K| + B|K|−1 (|K| 1 )B1 − B|K|−2 ( |K| 2 )B2 − B|K|−2 (|K| 2 )B1 (2 1 )B1. (36) for the (|S| + |K| − 3)-Shapley Values. In general, that is when we compute n-Shapley Values, the component function fS∪K is being added to Φn S once for every possible pathway that goes from a set of order n + 1 to the set S ∪ K by successively adding different numbers of elements. For k ≥ 1, let Pk = { (p1, . . . , pk) ∈ Nk ≥0 ∣ ∣ ∣ ∣ k∑ i=1 pi = k and pi = 0 =⇒ (pj = 0 ∀j > i) } (37) be the set of pathways of length k. This means that we have P1 = { (1) }, P2 = {(2, 0), (1, 1)} , P3 = {(3, 0, 0), (2, 1, 0), (1, 2, 0), (1, 1, 1)} , P4 = {(4, 0, 0, 0), (3, 1, 0, 0), (2, 2, 0, 0), (2, 1, 1, 0), (1, 3, 0, 0), (1, 2, 1, 0), (1, 1, 2, 0), (1, 1, 1, 1)} (38) and so on. By accounting for the coefﬁcients Bk and the signs along each path, the coefﬁcients can be written as Dn,m = ∑ (p1,...,pm)∈Pm(−1)∑m i=1 sign(pi)(n + m n + p1 ) Bn+p1 m∏ i=2 Bpi (m − ∑i−1 j=1 pj pi ) (39) From Shapley Values to Generalized Additive Models and back From this, we derive the special case D0,m = ∑ (p1,...,pm)∈Pm(−1)∑m i=1 sign(pi)( m i1 ) Bp1 m∏ i=2 Bpi (m − ∑i−1 j=1 pj pi ) = ∑ (p1,...,pm)∈Pm(−1)∑m i=1 sign(pi) m∏ i=1 Bpi (m − ∑i−1 j=1 pj pi ) = −Bm − m−1∑ p1=1 ap1 (m p1 ) ∑ ( ˆp1,..., ˆpm−p1 )∈Pm−p1 (−1)∑m−p1 i=1 sign(pi) m−p1∏ j=1 B ˆpj (m − i1 − ∑j−1 s=1 ˆps ˆpj ) = −Bm − m−1∑ p1=1 ap1 (m p1 ) β0,m−p1 = −Bm − m−1∑ p1=1 ap1 (m p1 ) 1 m − p1 + 1 = − m∑ k=1 Bk m − k + 1 (m k ) = 1 m + 1 (40) where the last equality is due to Lemma 9. Now, this implies that ∆S(x) = Φ |S| S (x) = fS(x) + ∑ K⊂[d]\\S, |K|≥1 D0,|K| fS∪K(x) = ∑ K⊂[d]\\S 1 1 + |K| fS∪K(x) (41) which is a version of Theorem 1 in Grabisch (1997). Using (41) and the explicit formula for n-Shapley Values (20), we get Φn S(x) = n−|S|∑ k=0 ∑ K⊂[d]\\S, |K|=k Bk ∆S∪K(x) = n−|S|∑ k=0 ∑ K⊂[d]\\S, |K|=k Bk ∑ T ⊂[d]\\(S∪K) 1 1 + |T | fS∪K∪T (x) (42) From which we see that the component function fS∪ ˜K is being added to Φn S(x) exactly Cn−|S|,| ˜K| = n−|S|∑ k=0 ( n − |S| k ) Bk 1 + | ˜K| − k (43) times which concludes the proof. G Proof of Theorem 7 Proof of Theorem 7. According to Theorem 4, the Shapley-GAM decomposition is given by fS(x) = ∑ L⊂S(−1) |S|−|L|v(xL, L). (44) Sebastian Bordt, Ulrike von Luxburg By substituting the deﬁnition of the value function (12) fS(x) = ∑ L⊂S(−1) |S|−|L|v(xL, L) = ∑ L⊂S(−1) |S|−|L| ∑ T ⊂L gT (x) = ∑ L⊂S ∑ T ⊂L gT (x)(−1) |S|−|L| = ∑ T ⊂S gT (x) ∑ L⊂S\\T (−1)|S|−|L|−|T | = gS(x) (45) Where we have re-arranged the sum to count the number of occurrences of the set T , and then used the fact that inner sum averages to zero except for T = S. H Proof of Theorem 8 We show a slightly more general result than what is stated in the main paper. In fact, we show that recovery holds for all interaction indices that can be written as I n S (x) = fS(x) + ∑ K⊂[d]\\S n+1≤|S|+|K| Cn,|S|,|K| fS∪K(x) ∀S ⊆ [d], |S| ≤ n (46) where fS(x) are the component functions of the Shapley-GAM and Cn,|S|,|K| ∈ R are coefﬁcients that depend on the interaction index. n-Shapley Values can be written like this according to Theorem 6. For the Faith-Shap interaction index, this representation is given in Theorem 19 in Tsai et al. (2022) Faith-Shapn S(x) = fS(x) + ∑ K⊂[d]\\S n+1≤|S|+|K| (−1) n−|S| |S| n + |S| ( n |S| )(|S|+|K|−1 n ) (|S|+|K|+n−1 n+|S| ) fS∪K(x) ∀|S| ≤ n. (47) Also the Shapley Taylor interaction index (Sundararajan et al., 2020) can, due to its symmetry, be written as Shapley-Taylor n S(x) =    fS(x) if |S| < n fS(x) + ∑ K⊂[d]\\S n+1≤|S|+|K| 1 (|S|+|K| |K| ) fS∪K(x) if |S| = n. (48) Proof of Theorem 8. We assume that the function f can be written as a GAM of order n, that is f (x) = ∑ S⊂[d], |S|≤n gS(xS). (49) Notice that this GAM is not necessarily the Shapley-GAM, but just some way to write the function f as a GAM. Let fS be the component functions of the Shapley-GAM. Now, n-Shapley Values, the Faith-Shap interaction index, as well as the Shapley Taylor interaction index, can be written as a linear combination of the component functions of the Shapley-GAM I n S (x) = fS(xS) + ∑ K⊂[d]\\S, |S|+|K|>n Cn−|S|,|K| fS∪K(xS∪K) (50) where the speciﬁc linear coefﬁcients Cn,m depend on the interaction index (Theorem 6, equation (47), equation (48)). According to equation (50), the interaction index equals fS(xS) plus some weighted components of the Shapley-GAM of order greater than n. As a consequence, it remains to show is that the Shapley-GAM is a GAM of order n (then the second sum vanishes and we arrive at I n S (x) = fS(xS) which is what we want to show). From Shapley Values to Generalized Additive Models and back It remains to show that the Shapley-GAM is a GAM of order n. According to Theorem 4, the component functions of the Shapley-GAM are given by fS(x) = ∑ L⊂S(−1) |S|−|L|v(xL, L). (51) We want to show that the component functions of degree greater than n vanish. Let us ﬁrst consider observational SHAP. Here we have ∑ L⊂S(−1)|S|−|L|v(xL, L) = ∑ L⊂S(−1)|S|−|L|E[f (x)|xL] = ∑ L⊂S(−1)|S|−|L|E   ∑ T ⊂[d], |T |≤n gT (xT )∣ ∣ ∣xL   = ∑ L⊂S(−1)|S|−|L| ∑ T ⊂[d], |T |≤n E [gT (xT )|xL] = ∑ T ⊂[d], |T |≤n ∑ L⊂S(−1) |S|−|L|E [gT (xT )|xL] (52) Consider the inner sum. If |S| > n, we can always pick an element i ∈ S \\ T and write ∑ L⊂S\\{i}(−1)|S|−|L|(E [gT (xT )|xL] − E [ gT (xT )|xL∪{i}] ) (53) If the input features are independent, then gT (xT ) and xi are independent, from which we get by the properties of the conditional expectation that E [ gT (xT )|xL∪{i}] = E [gT (xT )|xL] (54) It follows that the inner sum is zero for all sets T , and that the component functions of the Shapley-GAM of degree greater than n are equal to zero, too. Let us now consider interventional SHAP. Just as for observational SHAP, we arrive at equation (53) using the linearity of the expectation operator. Hence, we require that E [ gT (xT )|do(xL∪{i}) ] = E [gT (xT )|do(xL)] (55) which follows from the properties of the causal do-operator. Intuitively, since gT does not depend on the value of feature i, intervening on that feature has no effect. I Proof of Lemma 10 Proof. Let us ﬁrst consider the case n = 0. For n = 0 and m = 0, we have (0 0 )(0 0 ) (0 − 0)!(0 − 0)! (0 + 0 − 0 − 0 + 1)! (−1)0B0 = 1. (56) For n = 0 and m ≥ 1, we have m∑ l=0 (m l ) 1 (m − l + 1) (−1)lBl = 1 m + 1 m∑ l=0 ( m + 1 l )(−1)lBl = −2 m + 1 (m + 1 1 )B1 + m∑ l=0 (m + 1 l ) = −2B1 + 0 = 1. (57) where we used (15) and the fact that the odd Bernoulli numbers vanish except for n = 1. For m = 0 and n ≥ 1, we also have from (15) n∑ k=0 ( n k ) 1 (n − k + 1) (−1)0Bk = 1 n + 1 n∑ k=0 ( n + 1 k )Bk = 0. (58) Sebastian Bordt, Ulrike von Luxburg It remains to show the general case n, m ≥ 1. According to a derivation by Gy (2022), the problem in this case is equivalent to (−1) n m∑ l=0 Bn+l+1 n + l + 1 ( m l ) + (−1)m n∑ k=0 Bm+k+1 m + k + 1 ( n k ) = − 1 (n + m + 1)(n+m m ) (59) Now, Theorem 2 in Gould and Quaintance (2014) with s = 1 states that for any sequence of numbers (an)n≥0, it holds that m∑ k=0 (m k ) an+k+1 n + k + 1 = n∑ k=0(−1) n−k( n k ) bm+k+1 m + k + 1 + (−1)n+1a0 (m + n + 1)(m+n n ) (60) where the sequence (bn)n≥0 is the binomial transform of the sequence (an)n≥0, given by bn = n∑ k=0 (n k )ak. (61) Setting an = Bn, we have from (15) that the binomial transform of the Bernoulli numbers is simply bn = n∑ k=0 (n k )Bk = (−1) nBn (62) where the factor (−1)n takes care of the special case n = 1. Using (60) with an = Bn and bn = (−1)nBn, we get (−1)n m∑ k=0 (m k ) Bn+k+1 n + k + 1 = − n∑ k=0 (−1)m(n k ) Bm+k+1 m + k + 1 − 1 (m + n + 1)(m+n n ) (63) where we multiplied both sides with (−1) n. This is the same as (59) which concludes the proof. From Shapley Values to Generalized Additive Models and back J Datasets and Models In our experiments, we use the following data sets and models. J.1 Datasets Folktables Income. Folktables is a Python package that provides access to data sets derived from recent US Censuses https://github.com/zykls/folktables. We used this package to obtain the data from the 2016 Census in California. The machine learning problem is the ACSIncome prediction task, that is to predict whether an individual’s income is above $50,000, based on 10 personal characteristics (Ding et al., 2021). The data set contains of 152 149 observations. Folktables Travel Time. Folktables is a Python package that provides access to data sets derived from recent US Censuses https://github.com/zykls/folktables. We used this package to obtain the data from the 2016 Census in California. The machine learning problem is the ACSTravelTime prediction task, that is to predict whether an individual has to commute to work longer than 20 minutes, based on 10 personal characteristics (Ding et al., 2021). The data set contains 133 549 observations. German Credit. The German Credit Data set is a data set with 20 different features on individual’s credit history and personal characteristic. The machine learning problem is to predict credit risk in binary form. We obtained the data set from the UCI machine learning repository and reduced the number of features to 10 without any observed drop in accuracy. The data set contains 1000 observations. California Housing. The California Housing data set was derived from the 1990 U.S. census. The regression problem is to predict the median house value, based on 8 characteristics. We obtained the data set form the scikit-learn library. The data set contains 20 640 observations. Iris. The Iris data set is a simple ﬂower data set. The machine learning problem is to classify whether the ﬂower is of a particular kind or not, based on 4 different features. We obtained the data set form the scikit-learn library. The data set contains 150 observations. J.2 Models Glassbox-GAM. We train the Glassbox-GAMs with the interpretML library (Nori et al., 2019) and default parameters (no interactions). Gradient Boosted Tree. We use the xgboost library (Chen and Guestrin, 2016) and train with 100 trees per model. This setting allows to achieve competitive accuracy for gradient boosted trees. Random Forest. We use the scikit-learn library (Pedregosa et al., 2011) and train with 100 trees per forest. This setting allows to achieve competitive accuracy for random forests. k-Nearest Neighbor. We use the scikit-learn library (Pedregosa et al., 2011). The hyperparameter k was chosen with cross-validation to be 30, 80, 25, 10, 1 for the data sets as listed above. Sebastian Bordt, Ulrike von Luxburg K Additional Plots and Figures K.1 Folktables Income K.1.1 Glassbox-GAM −1.5 −1.0 −0.5 0.0 0.5 1.0Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P −1.5 −1.0 −0.5 0.0 0.5 1.0Feature Attribution 6-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 7-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 8-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 9-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.4: n-Shapley Values for a Glassbox-GAM and the ﬁrst observation in our test set of the Folktables Income data set. K.1.2 Gradient Boosted Tree −0.2 −0.1 0.0 0.1 0.2 0.3 0.4Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P −0.2 −0.1 0.0 0.1 0.2 0.3 0.4Feature Attribution 6-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 7-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 8-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 9-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.5: n-Shapley Values for a Gradient Boosted Tree and the ﬁrst observation in our test set of the Folktables Income data set. From Shapley Values to Generalized Additive Models and back K.1.3 Random Forest −0.4 −0.2 0.0 0.2 0.4 0.6Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P −0.4 −0.2 0.0 0.2 0.4 0.6Feature Attribution 6-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 7-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 8-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 9-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.6: n-Shapley Values for a Random Forest and the ﬁrst observation in our test set of the Folktables Income data set. K.1.4 k-Nearest Neighbor −0.2 0.0 0.2 0.4Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P −0.2 0.0 0.2 0.4Feature Attribution 6-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 7-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 8-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P 9-Shapley ValuesAGEPCOWSCHLMAROCCPPOBPRELPWKHPSEXRAC1P Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.7: n-Shapley Values for a kNN classiﬁer and the ﬁrst observation in our test set of the Folktables Income data set. Sebastian Bordt, Ulrike von Luxburg 0 2 Value of Feature AGEP −0.1 0.0 0.1Feature AttributionShapley Values 0 2 Value of Feature AGEP Shapley Interaction Values 0 2 Value of Feature AGEP 4-Shapley Values 0 2 Value of Feature AGEP Shapley-GAM 0 1 2 Value of Feature COW −0.1 0.0 0.1Feature AttributionShapley Values 0 1 2 Value of Feature COW Shapley Interaction Values 0 1 2 Value of Feature COW 4-Shapley Values 0 1 2 Value of Feature COW Shapley-GAM −4 −2 0 Value of Feature SCHL −0.2 0.0 0.2Feature AttributionShapley Values −4 −2 0 Value of Feature SCHL Shapley Interaction Values −4 −2 0 Value of Feature SCHL 4-Shapley Values −4 −2 0 Value of Feature SCHL Shapley-GAM −0.5 0.0 0.5 1.0 Value of Feature MAR −0.05 0.00 0.05Feature AttributionShapley Values −0.5 0.0 0.5 1.0 Value of Feature MAR Shapley Interaction Values −0.5 0.0 0.5 1.0 Value of Feature MAR 4-Shapley Values −0.5 0.0 0.5 1.0 Value of Feature MAR Shapley-GAM −1 0 1 2 Value of Feature OCCP −0.1 0.0 0.1 0.2Feature AttributionShapley Values −1 0 1 2 Value of Feature OCCP Shapley Interaction Values −1 0 1 2 Value of Feature OCCP 4-Shapley Values −1 0 1 2 Value of Feature OCCP Shapley-GAM 0 1 2 3 Value of Feature POBP −0.10 −0.05 0.00 0.05Feature AttributionShapley Values 0 1 2 3 Value of Feature POBP Shapley Interaction Values 0 1 2 3 Value of Feature POBP 4-Shapley Values 0 1 2 3 Value of Feature POBP Shapley-GAM 0 1 2 3 Value of Feature RELP −0.2 −0.1 0.0Feature AttributionShapley Values 0 1 2 3 Value of Feature RELP Shapley Interaction Values 0 1 2 3 Value of Feature RELP 4-Shapley Values 0 1 2 3 Value of Feature RELP Shapley-GAM Figure K.8: Partial dependence plots for the kNN classiﬁer on the Folktables Income data set (compare Figure 2 in the main paper). Depicted are the partial dependence plots of Φn i for n = {1, 2, 4, 10} and 7 different features. From Shapley Values to Generalized Additive Models and back K.2 Folktables Travel K.2.1 Glassbox-GAM −0.2 0.0 0.2 0.4Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 0.0 0.2 0.4Feature Attribution 6-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 7-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 8-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 9-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.9: n-Shapley Values for a Glassbox-GAM and the ﬁrst observation in our test set of the Folktables Travel data set. K.2.2 Gradient Boosted Tree −0.2 −0.1 0.0 0.1 0.2 0.3Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 −0.1 0.0 0.1 0.2 0.3Feature Attribution 6-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 7-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 8-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 9-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.10: n-Shapley Values for a Gradient Boosted Tree and the ﬁrst observation in our test set of the Folktables Travel data set. Sebastian Bordt, Ulrike von Luxburg K.2.3 Random Forest −0.2 0.0 0.2 0.4Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 0.0 0.2 0.4Feature Attribution 6-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 7-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 8-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 9-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.11: n-Shapley Values for a Random Forest and the ﬁrst observation in our test set of the Folktables Travel data set. K.2.4 k-Nearest Neighbor −0.2 −0.1 0.0 0.1 0.2Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT −0.2 −0.1 0.0 0.1 0.2Feature Attribution 6-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 7-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 8-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT 9-Shapley ValuesJWTRPOWPPUMAAGEPOCCPPOVPIPSCHLSEXRELPCIT Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.12: n-Shapley Values for a kNN classiﬁer and the ﬁrst observation in our test set of the Folktables Travel data set. From Shapley Values to Generalized Additive Models and back 0 1 2 3 Feature JWTR −0.50 −0.25 0.00 0.25Feature AttributionShapley Values 0 1 2 3 Feature JWTR Shapley Interaction Values 0 1 2 3 Feature JWTR 4-Shapley Values 0 1 2 3 Feature JWTR Shapley-GAM −2 −1 0 1 2 Feature POWP −0.1 0.0 0.1 0.2Feature AttributionShapley Values −2 −1 0 1 2 Feature POWP Shapley Interaction Values −2 −1 0 1 2 Feature POWP 4-Shapley Values −2 −1 0 1 2 Feature POWP Shapley-GAM −2 −1 0 1 2 Feature PUMA −0.1 0.0 0.1 0.2Feature AttributionShapley Values −2 −1 0 1 2 Feature PUMA Shapley Interaction Values −2 −1 0 1 2 Feature PUMA 4-Shapley Values −2 −1 0 1 2 Feature PUMA Shapley-GAM −2 −1 0 1 2 Feature AGEP −0.2 −0.1 0.0Feature AttributionShapley Values −2 −1 0 1 2 Feature AGEP Shapley Interaction Values −2 −1 0 1 2 Feature AGEP 4-Shapley Values −2 −1 0 1 2 Feature AGEP Shapley-GAM −1 0 1 2 Feature OCCP −0.05 0.00 0.05 0.10Feature AttributionShapley Values −1 0 1 2 Feature OCCP Shapley Interaction Values −1 0 1 2 Feature OCCP 4-Shapley Values −1 0 1 2 Feature OCCP Shapley-GAM −2 −1 0 1 Feature POVPIP −0.05 0.00 0.05Feature AttributionShapley Values −2 −1 0 1 Feature POVPIP Shapley Interaction Values −2 −1 0 1 Feature POVPIP 4-Shapley Values −2 −1 0 1 Feature POVPIP Shapley-GAM −4 −2 0 Feature SCHL −0.05 0.00 0.05Feature AttributionShapley Values −4 −2 0 Feature SCHL Shapley Interaction Values −4 −2 0 Feature SCHL 4-Shapley Values −4 −2 0 Feature SCHL Shapley-GAM Figure K.13: Partial dependence plots for the random forest on the Folktables Travel data set. Depicted are the partial dependence plots of Φn i for n = {1, 2, 4, 10} and 7 different features. Sebastian Bordt, Ulrike von Luxburg K.3 German Credit K.3.1 Glassbox-GAM 0.0 0.2 0.4 0.6 0.8Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 0.0 0.2 0.4 0.6 0.8Feature Attribution 6-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 7-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 8-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 9-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.14: n-Shapley Values for a Glassbox-GAM and the ﬁrst observation in our test set of the German Credit data set. K.3.2 Gradient Boosted Tree −0.2 −0.1 0.0 0.1 0.2 0.3Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty−0.2 −0.1 0.0 0.1 0.2 0.3Feature Attribution 6-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 7-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 8-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 9-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.15: n-Shapley Values for a Gradient Boosted Tree and the ﬁrst observation in our test set of the German Credit data set. From Shapley Values to Generalized Additive Models and back K.3.3 Random Forest −0.05 0.00 0.05 0.10 0.15 0.20Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty −0.05 0.00 0.05 0.10 0.15 0.20Feature Attribution 6-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 7-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 8-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 9-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.16: n-Shapley Values for a Random Forest and the ﬁrst observation in our test set of the German Credit data set. K.3.4 k-Nearest Neighbor −0.1 0.0 0.1 0.2Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley Values 5-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty −0.1 0.0 0.1 0.2Feature Attribution 6-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 7-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 8-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty 9-Shapley ValuesAccountDurationHistoryPurposeAmountSavingsEmployRateHousingProperty Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.17: n-Shapley Values for a kNN classiﬁer and the ﬁrst observation in our test set of the German Credit data set. Sebastian Bordt, Ulrike von Luxburg −1 0 1 Value of Feature Account −0.5 0.0 0.5Feature AttributionShapley Values −1 0 1 Value of Feature Account Shapley Interaction Values −1 0 1 Value of Feature Account 4-Shapley Values −1 0 1 Value of Feature Account Shapley-GAM −1 0 1 2 3 Value of Feature Duration −0.5 0.0 0.5Feature AttributionShapley Values −1 0 1 2 3 Value of Feature Duration Shapley Interaction Values −1 0 1 2 3 Value of Feature Duration 4-Shapley Values −1 0 1 2 3 Value of Feature Duration Shapley-GAM −2 −1 0 1 Value of Feature History −0.5 0.0 0.5 1.0Feature AttributionShapley Values −2 −1 0 1 Value of Feature History Shapley Interaction Values −2 −1 0 1 Value of Feature History 4-Shapley Values −2 −1 0 1 Value of Feature History Shapley-GAM −1 0 1 2 Value of Feature Purpose −0.5 0.0Feature AttributionShapley Values −1 0 1 2 Value of Feature Purpose Shapley Interaction Values −1 0 1 2 Value of Feature Purpose 4-Shapley Values −1 0 1 2 Value of Feature Purpose Shapley-GAM 0 2 4 Value of Feature Amount 0.0 0.5 1.0Feature AttributionShapley Values 0 2 4 Value of Feature Amount Shapley Interaction Values 0 2 4 Value of Feature Amount 4-Shapley Values 0 2 4 Value of Feature Amount Shapley-GAM 0 1 Value of Feature Savings −0.50 −0.25 0.00 0.25Feature AttributionShapley Values 0 1 Value of Feature Savings Shapley Interaction Values 0 1 Value of Feature Savings 4-Shapley Values 0 1 Value of Feature Savings Shapley-GAM −2 −1 0 1 Value of Feature Employ −0.2 0.0 0.2 0.4Feature AttributionShapley Values −2 −1 0 1 Value of Feature Employ Shapley Interaction Values −2 −1 0 1 Value of Feature Employ 4-Shapley Values −2 −1 0 1 Value of Feature Employ Shapley-GAM Figure K.18: Partial dependence plots for the Glassbox-GAM without interaction terms on the German Credit data set. Depicted are the partial dependence plots of Φn i for n = {1, 2, 4, 10} and 7 different features. From Shapley Values to Generalized Additive Models and back K.4 California Housing K.4.1 Glassbox-GAM −1.0 −0.5 0.0 0.5Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude −1.0 −0.5 0.0 0.5Feature Attribution 5-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 6-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 7-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.19: n-Shapley Values for a Glassbox-GAM and the ﬁrst observation in our test set of the California Housing data set. K.4.2 Gradient Boosted Tree −2 −1 0 1 2Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude−2 −1 0 1 2Feature Attribution 5-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 6-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 7-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.20: n-Shapley Values for a Gradient Boosted Tree and the ﬁrst observation in our test set of the California Housing data set. Sebastian Bordt, Ulrike von Luxburg K.4.3 Random Forest −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75Feature Attribution 5-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 6-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 7-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.21: n-Shapley Values for a Random Forest and the ﬁrst observation in our test set of the California Housing data set. K.4.4 k-Nearest Neighbor −0.75 −0.50 −0.25 0.00 0.25 0.50Feature Attribution Shapley Values Shapley Interaction Values 3-Shapley Values 4-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude −0.75 −0.50 −0.25 0.00 0.25 0.50Feature Attribution 5-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 6-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude 7-Shapley ValuesMedIncHouseAgeAveRoomsAveBedrmsPopulationAveOccupLatitudeLongitude Shapley-GAM Main 2nd order 3rd order 4th 5th 6th 7th 8th 9th 10th order Figure K.22: n-Shapley Values for a kNN classiﬁer and the ﬁrst observation in our test set of the California Housing data set. From Shapley Values to Generalized Additive Models and back 0 2 Value of Feature MedInc 0 1 2Feature AttributionShapley Values 0 2 Value of Feature MedInc Shapley Interaction Values 0 2 Value of Feature MedInc 4-Shapley Values 0 2 Value of Feature MedInc Shapley-GAM −1 0 1 2 Value of Feature HouseAge −0.2 0.0 0.2Feature AttributionShapley Values −1 0 1 2 Value of Feature HouseAge Shapley Interaction Values −1 0 1 2 Value of Feature HouseAge 4-Shapley Values −1 0 1 2 Value of Feature HouseAge Shapley-GAM 0 5 10 Value of Feature AveRooms −0.5 0.0 0.5 1.0Feature AttributionShapley Values 0 5 10 Value of Feature AveRooms Shapley Interaction Values 0 5 10 Value of Feature AveRooms 4-Shapley Values 0 5 10 Value of Feature AveRooms Shapley-GAM 0 5 10 15 Value of Feature AveBedrms −0.4 −0.2 0.0Feature AttributionShapley Values 0 5 10 15 Value of Feature AveBedrms Shapley Interaction Values 0 5 10 15 Value of Feature AveBedrms 4-Shapley Values 0 5 10 15 Value of Feature AveBedrms Shapley-GAM 0 2 4 Value of Feature Population 0.0 0.2Feature AttributionShapley Values 0 2 4 Value of Feature Population Shapley Interaction Values 0 2 4 Value of Feature Population 4-Shapley Values 0 2 4 Value of Feature Population Shapley-GAM −0.1 0.0 0.1 0.2 Value of Feature AveOccup 0.0 0.5Feature AttributionShapley Values −0.1 0.0 0.1 0.2 Value of Feature AveOccup Shapley Interaction Values −0.1 0.0 0.1 0.2 Value of Feature AveOccup 4-Shapley Values −0.1 0.0 0.1 0.2 Value of Feature AveOccup Shapley-GAM −1 0 1 2 Value of Feature Latitude −2 0 2Feature AttributionShapley Values −1 0 1 2 Value of Feature Latitude Shapley Interaction Values −1 0 1 2 Value of Feature Latitude 4-Shapley Values −1 0 1 2 Value of Feature Latitude Shapley-GAM Figure K.23: Partial dependence plots for the gradient boosted tree on the California Housing data set. Depicted are the partial dependence plots of Φn i for n = {1, 2, 4, 10} and 7 different features. Sebastian Bordt, Ulrike von Luxburg Subset S N=500 N=5000 N=133 549 (0,) 0.1128 0.1144 0.1165 (1,) 0.0005 -0.0006 0.0022 (2,) -0.1248 -0.1117 -0.1098 (3,) 0.0227 0.0283 0.0281 (4,) -0.0041 -0.0020 -0.0018 (5,) -0.0123 -0.0189 -0.0200 (6,) 0.0845 0.1003 0.0982 (7,) 0.2357 0.2478 0.2505 (8,) 0.0280 0.0329 0.0347 (9,) 0.0197 0.0238 0.0241 (0, 1) -0.0023 -0.0020 -0.0032 (0, 2) 0.0059 0.0005 -0.0025 (0, 3) -0.0146 -0.0128 -0.0126 (0, 4) 0.0089 0.0102 0.0102 (0, 5) 0.0038 0.0140 0.0141 (0, 6) -0.0244 -0.0242 -0.0213 (0, 7) -0.0452 -0.0416 -0.0426 (0, 8) -0.0032 -0.0028 -0.0038 (0, 9) -0.0043 -0.0055 -0.0071 (1, 2) -0.0012 -0.0009 -0.0022 (1, 3) -0.0004 0.0003 0.0007 (1, 4) -0.0006 0.0011 -0.0011 (1, 5) -0.0060 -0.0000 0.0004 (1, 6) 0.0027 0.0024 0.0014 (1, 7) 0.0093 0.0102 0.0079 (1, 8) -0.0017 0.0020 0.0007 (1, 9) 0.0048 0.0032 0.0027 (2, 3) 0.0029 -0.0006 -0.0016 (2, 4) -0.0419 -0.0534 -0.0547 (2, 5) -0.0128 -0.0095 -0.0115 (2, 6) 0.0389 0.0286 0.0290 (2, 7) 0.0752 0.0695 0.0677 (2, 8) -0.0031 -0.0044 -0.0070 (2, 9) 0.0151 0.0039 0.0031 (3, 4) -0.0112 -0.0093 -0.0091 (3, 5) 0.0006 0.0058 0.0055 (3, 6) -0.0068 -0.0116 -0.0099 (3, 7) -0.0286 -0.0298 -0.0304 (3, 8) -0.0135 -0.0165 -0.0181 (3, 9) 0.0038 -0.0036 -0.0041 (4, 5) -0.0016 0.0069 0.0071 (4, 6) -0.0279 -0.0295 -0.0298 (4, 7) -0.0100 -0.0070 -0.0079 (4, 8) -0.0019 -0.0037 -0.0043 (4, 9) -0.0091 -0.0116 -0.0122 (5, 6) 0.0026 0.0083 0.0079 (5, 7) 0.0084 0.0152 0.0157 (5, 8) -0.0000 0.0055 0.0045 (5, 9) 0.0015 0.0044 0.0041 (6, 7) -0.0551 -0.0603 -0.0581 (6, 8) -0.0132 -0.0174 -0.0182 (6, 9) -0.0053 -0.0140 -0.0126 (7, 8) -0.0125 -0.0102 -0.0127 (7, 9) -0.0102 -0.0151 -0.0161 (8, 9) 0.0052 0.0014 -0.0004 (0, 1, 2) -0.0058 -0.0026 -0.0014 (0, 1, 3) 0.0018 0.0028 0.0020 (0, 1, 4) -0.0000 -0.0030 -0.0021 (0, 1, 5) 0.0070 -0.0005 -0.0013 (0, 1, 6) 0.0060 0.0024 0.0030 (0, 1, 7) -0.0039 -0.0024 -0.0015 (0, 1, 8) 0.0073 0.0007 0.0014 (0, 1, 9) -0.0003 -0.0006 -0.0009 (0, 2, 3) 0.0038 0.0031 0.0030 (0, 2, 4) -0.0274 -0.0141 -0.0079 (0, 2, 5) 0.0088 0.0062 0.0081 (0, 2, 6) -0.0042 0.0006 -0.0006 (0, 2, 7) 0.0233 0.0242 0.0275 (0, 2, 8) 0.0043 0.0023 0.0055 (0, 2, 9) -0.0298 -0.0249 -0.0216 (0, 3, 4) 0.0149 0.0078 0.0091 (0, 3, 5) 0.0019 -0.0023 -0.0014 ... ... ... ... Subset S N=500 N=5000 N=133 549 ... ... ... ... (2, 7, 8, 9) 0.0043 0.0009 0.0005 (3, 4, 5, 6) -0.0101 -0.0143 -0.0135 (3, 4, 5, 7) 0.0045 -0.0030 -0.0049 (3, 4, 5, 8) -0.0020 -0.0053 -0.0047 (3, 4, 5, 9) -0.0064 -0.0049 -0.0054 (3, 4, 6, 7) 0.0097 0.0076 0.0079 (3, 4, 6, 8) -0.0058 -0.0058 -0.0047 (3, 4, 6, 9) 0.0032 0.0018 0.0017 (3, 4, 7, 8) 0.0007 -0.0011 -0.0011 (3, 4, 7, 9) 0.0041 0.0003 0.0004 (3, 4, 8, 9) 0.0006 0.0013 0.0021 (3, 5, 6, 7) 0.0052 0.0059 0.0071 (3, 5, 6, 8) -0.0024 -0.0011 -0.0000 (3, 5, 6, 9) -0.0044 -0.0023 -0.0019 (3, 5, 7, 8) -0.0023 -0.0014 -0.0011 (3, 5, 7, 9) -0.0007 -0.0031 -0.0024 (3, 5, 8, 9) -0.0010 -0.0007 -0.0005 (3, 6, 7, 8) 0.0035 0.0027 0.0034 (3, 6, 7, 9) -0.0034 -0.0052 -0.0045 (3, 6, 8, 9) -0.0019 -0.0011 -0.0004 (3, 7, 8, 9) 0.0018 0.0014 0.0003 (4, 5, 6, 7) -0.0052 -0.0020 -0.0037 (4, 5, 6, 8) -0.0025 -0.0001 0.0007 (4, 5, 6, 9) -0.0019 0.0005 0.0004 (4, 5, 7, 8) -0.0027 0.0009 0.0016 (4, 5, 7, 9) -0.0017 0.0005 0.0010 (4, 5, 8, 9) -0.0004 -0.0004 0.0000 (4, 6, 7, 8) -0.0000 -0.0003 0.0011 (4, 6, 7, 9) 0.0017 0.0005 0.0006 (4, 6, 8, 9) -0.0005 -0.0005 0.0005 (4, 7, 8, 9) 0.0007 -0.0000 -0.0001 (5, 6, 7, 8) -0.0041 -0.0024 -0.0012 (5, 6, 7, 9) -0.0038 -0.0046 -0.0039 (5, 6, 8, 9) 0.0013 -0.0009 -0.0007 (5, 7, 8, 9) -0.0003 0.0003 0.0004 (6, 7, 8, 9) 0.0022 0.0005 0.0003 (0, 1, 2, 3, 4) 0.0042 0.0010 0.0003 (0, 1, 2, 3, 5) 0.0004 0.0010 0.0012 (0, 1, 2, 3, 6) 0.0018 0.0004 0.0002 (0, 1, 2, 3, 7) 0.0014 0.0006 0.0012 (0, 1, 2, 3, 8) 0.0007 -0.0004 -0.0001 (0, 1, 2, 3, 9) 0.0006 0.0012 0.0015 (0, 1, 2, 4, 5) 0.0051 0.0013 0.0013 (0, 1, 2, 4, 6) 0.0016 0.0011 0.0010 (0, 1, 2, 4, 7) 0.0005 -0.0011 -0.0009 (0, 1, 2, 4, 8) 0.0022 0.0005 0.0008 (0, 1, 2, 4, 9) 0.0026 -0.0002 -0.0000 (0, 1, 2, 5, 6) 0.0025 0.0023 0.0038 (0, 1, 2, 5, 7) 0.0013 -0.0001 0.0003 (0, 1, 2, 5, 8) 0.0012 0.0011 0.0017 (0, 1, 2, 5, 9) 0.0017 -0.0008 -0.0006 (0, 1, 2, 6, 7) 0.0005 -0.0010 -0.0008 (0, 1, 2, 6, 8) -0.0003 -0.0008 -0.0001 (0, 1, 2, 6, 9) 0.0008 -0.0001 -0.0003 (0, 1, 2, 7, 8) 0.0003 -0.0010 -0.0005 (0, 1, 2, 7, 9) 0.0004 -0.0008 -0.0006 (0, 1, 2, 8, 9) -0.0004 -0.0006 -0.0003 (0, 1, 3, 4, 5) 0.0013 0.0002 -0.0007 (0, 1, 3, 4, 6) 0.0017 0.0009 -0.0001 (0, 1, 3, 4, 7) 0.0028 0.0010 0.0010 (0, 1, 3, 4, 8) 0.0032 -0.0004 -0.0000 (0, 1, 3, 4, 9) -0.0005 -0.0006 -0.0001 (0, 1, 3, 5, 6) -0.0012 0.0000 0.0003 (0, 1, 3, 5, 7) 0.0018 -0.0006 -0.0003 (0, 1, 3, 5, 8) 0.0003 -0.0001 0.0000 (0, 1, 3, 5, 9) -0.0012 -0.0000 0.0002 (0, 1, 3, 6, 7) 0.0011 0.0002 0.0009 (0, 1, 3, 6, 8) 0.0020 -0.0004 -0.0002 (0, 1, 3, 6, 9) -0.0000 0.0007 0.0004 (0, 1, 3, 7, 8) 0.0015 -0.0003 0.0001 (0, 1, 3, 7, 9) -0.0003 -0.0010 -0.0004 ... ... ... ... Subset S N=500 N=5000 N=133 549 ... ... ... ... (1, 2, 5, 6, 7, 8, 9) 0.0029 -0.0003 -0.0009 (1, 3, 4, 5, 6, 7, 8) 0.0005 -0.0032 -0.0035 (1, 3, 4, 5, 6, 7, 9) 0.0061 0.0051 0.0049 (1, 3, 4, 5, 6, 8, 9) 0.0062 0.0014 -0.0009 (1, 3, 4, 5, 7, 8, 9) 0.0002 0.0002 0.0009 (1, 3, 4, 6, 7, 8, 9) 0.0015 0.0015 0.0008 (1, 3, 5, 6, 7, 8, 9) 0.0002 -0.0026 -0.0004 (1, 4, 5, 6, 7, 8, 9) 0.0025 0.0026 0.0016 (2, 3, 4, 5, 6, 7, 8) -0.0038 0.0007 -0.0002 (2, 3, 4, 5, 6, 7, 9) 0.0039 0.0042 0.0036 (2, 3, 4, 5, 6, 8, 9) 0.0059 0.0022 0.0013 (2, 3, 4, 5, 7, 8, 9) -0.0042 -0.0016 -0.0010 (2, 3, 4, 6, 7, 8, 9) -0.0007 0.0013 0.0008 (2, 3, 5, 6, 7, 8, 9) -0.0046 -0.0029 -0.0015 (2, 4, 5, 6, 7, 8, 9) 0.0012 0.0018 0.0008 (3, 4, 5, 6, 7, 8, 9) 0.0014 0.0009 0.0011 (0, 1, 2, 3, 4, 5, 6, 7) -0.0021 -0.0027 -0.0019 (0, 1, 2, 3, 4, 5, 6, 8) 0.0037 0.0021 0.0014 (0, 1, 2, 3, 4, 5, 6, 9) 0.0018 -0.0006 -0.0015 (0, 1, 2, 3, 4, 5, 7, 8) 0.0002 0.0002 -0.0001 (0, 1, 2, 3, 4, 5, 7, 9) -0.0002 -0.0006 -0.0012 (0, 1, 2, 3, 4, 5, 8, 9) 0.0015 0.0018 -0.0001 (0, 1, 2, 3, 4, 6, 7, 8) 0.0005 0.0010 -0.0003 (0, 1, 2, 3, 4, 6, 7, 9) -0.0004 0.0013 0.0003 (0, 1, 2, 3, 4, 6, 8, 9) 0.0025 0.0014 0.0005 (0, 1, 2, 3, 4, 7, 8, 9) -0.0013 0.0001 -0.0003 (0, 1, 2, 3, 5, 6, 7, 8) 0.0037 0.0016 -0.0005 (0, 1, 2, 3, 5, 6, 7, 9) 0.0009 0.0008 -0.0009 (0, 1, 2, 3, 5, 6, 8, 9) 0.0018 0.0009 -0.0002 (0, 1, 2, 3, 5, 7, 8, 9) 0.0014 0.0010 -0.0002 (0, 1, 2, 3, 6, 7, 8, 9) 0.0000 0.0006 0.0001 (0, 1, 2, 4, 5, 6, 7, 8) 0.0030 0.0017 0.0002 (0, 1, 2, 4, 5, 6, 7, 9) -0.0009 -0.0002 0.0000 (0, 1, 2, 4, 5, 6, 8, 9) 0.0052 0.0014 0.0004 (0, 1, 2, 4, 5, 7, 8, 9) -0.0010 0.0006 -0.0001 (0, 1, 2, 4, 6, 7, 8, 9) -0.0013 0.0003 -0.0000 (0, 1, 2, 5, 6, 7, 8, 9) 0.0007 0.0003 -0.0004 (0, 1, 3, 4, 5, 6, 7, 8) -0.0013 0.0008 -0.0006 (0, 1, 3, 4, 5, 6, 7, 9) 0.0003 0.0017 0.0006 (0, 1, 3, 4, 5, 6, 8, 9) 0.0010 0.0005 -0.0001 (0, 1, 3, 4, 5, 7, 8, 9) -0.0006 0.0007 -0.0000 (0, 1, 3, 4, 6, 7, 8, 9) -0.0007 0.0005 0.0002 (0, 1, 3, 5, 6, 7, 8, 9) -0.0001 0.0008 0.0002 (0, 1, 4, 5, 6, 7, 8, 9) -0.0002 0.0010 0.0001 (0, 2, 3, 4, 5, 6, 7, 8) 0.0006 0.0001 -0.0007 (0, 2, 3, 4, 5, 6, 7, 9) -0.0005 0.0015 0.0003 (0, 2, 3, 4, 5, 6, 8, 9) 0.0012 0.0004 0.0001 (0, 2, 3, 4, 5, 7, 8, 9) -0.0005 0.0002 -0.0001 (0, 2, 3, 4, 6, 7, 8, 9) -0.0010 0.0002 0.0002 (0, 2, 3, 5, 6, 7, 8, 9) 0.0009 0.0004 -0.0000 (0, 2, 4, 5, 6, 7, 8, 9) -0.0007 0.0007 0.0001 (0, 3, 4, 5, 6, 7, 8, 9) -0.0010 0.0008 0.0006 (1, 2, 3, 4, 5, 6, 7, 8) -0.0131 -0.0081 -0.0069 (1, 2, 3, 4, 5, 6, 7, 9) -0.0018 0.0002 0.0013 (1, 2, 3, 4, 5, 6, 8, 9) -0.0073 -0.0006 0.0015 (1, 2, 3, 4, 5, 7, 8, 9) 0.0039 0.0040 0.0042 (1, 2, 3, 4, 6, 7, 8, 9) 0.0011 0.0000 0.0014 (1, 2, 3, 5, 6, 7, 8, 9) 0.0018 0.0036 0.0014 (1, 2, 4, 5, 6, 7, 8, 9) -0.0021 -0.0014 0.0005 (1, 3, 4, 5, 6, 7, 8, 9) -0.0036 -0.0048 -0.0048 (2, 3, 4, 5, 6, 7, 8, 9) -0.0021 -0.0039 -0.0038 (0, 1, 2, 3, 4, 5, 6, 7, 8) -0.0023 -0.0018 -0.0002 (0, 1, 2, 3, 4, 5, 6, 7, 9) -0.0008 -0.0013 0.0003 (0, 1, 2, 3, 4, 5, 6, 8, 9) -0.0063 -0.0024 -0.0003 (0, 1, 2, 3, 4, 5, 7, 8, 9) 0.0012 0.0000 0.0010 (0, 1, 2, 3, 4, 6, 7, 8, 9) 0.0012 -0.0003 0.0002 (0, 1, 2, 3, 5, 6, 7, 8, 9) -0.0017 -0.0008 0.0005 (0, 1, 2, 4, 5, 6, 7, 8, 9) 0.0003 -0.0004 0.0004 (0, 1, 3, 4, 5, 6, 7, 8, 9) -0.0000 -0.0019 -0.0009 (0, 2, 3, 4, 5, 6, 7, 8, 9) -0.0003 -0.0017 -0.0010 (1, 2, 3, 4, 5, 6, 7, 8, 9) -0.0041 -0.0008 -0.0022 (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) 0.0005 0.0012 -0.0002 Table K.2: The individual terms of the Shapley-GAM decomposition of a kNN classiﬁer on the Folktables Travel data set. The table depicts a number of selected terms of the full decomposition, estimated with 500, 5000 and 133549 samples per evaluation of the value function. The depicted terms are visualized in Figure C.3. From the table, we see that many relatively small higher-order coefﬁcients are not very precisely estimated for N = 5000, whereas the overall sums (visualized in Figure C.3) are.","libVersion":"0.3.2","langs":""}
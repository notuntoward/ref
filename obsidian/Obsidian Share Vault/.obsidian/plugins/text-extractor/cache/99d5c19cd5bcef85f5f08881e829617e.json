{"path":"lit/lit_sources/Se23creatorsTimeGPT.pdf","text":"4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 1 /9 R ev olutionizing Time Series For ecasting: Int er view with TimeGPT's cr eat or s Re v o lu t io n iz in g T im e S e r ie s F o r e c a s t in g : In t e r v ie w wit h T im e G P T's c r e a t o r s It 's n o t an L L M ! A z u l G ar z a a n d M ax M e r g e n t h ale r t alk in n o v a t io n s , o p e n - s o u r c e , d iv e r s it y , an d p ain o f w r an g lin g 1 0 0 b illio n d a t a p o in t sTuring PostPosts K s e n ia S e & Ia n S p e k t o rApril 05, 2024 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 2 /9 If you like T uring Post, please consider to support us today Today , we're excited to welcome Azul Garza and Max Mergenthaler , co-founders of Nixtla and researchers behind T imeGPT , the first foundation model designed specifically for time series forecasting (→read the paper here). T ime series are used to analyze trends, identify seasonality , forecast future values, and detect unusual patterns, and more. Foundation models are revolutionizing time series forecasting because they are pre-trained on vast amounts of diverse data, enabling them to adapt to new forecasting tasks ef ficiently . This translates to versatile models that can handle complex data patterns, eliminating the need for custom models for each specific use case. Just recently , giants like Amazon (Chronos), Google (T imesFM), and Salesforce (Moirai) have released their proprietary time series foundation models. More breakthroughs are awaited in 2024! G r e a t t o h a v e y o u f o r t h is in t e r v ie w , A z u l a n d M a x . H o w d id y o u co m e u p w it h t h e id e a f o r T im e G P T ? W e have been working in the time series space for quite a while, and when we started Nixtla 3 years ago, we started it with the goal to help all practitioners with best-in-class implementations of classical and contemporary algorithms. W e began developing various open-source libraries that have now become the Nixtlaverse . This is the most comprehensive open-source time series project to date, including statistical, machine learning and deep learning models, and we’ve been excited to see that they’re being used by many Fortune 100 companies and startups alike, and have been downloaded more than 10 million times. As developers and maintainers of these libraries, we've had the privilege of working with top data science teams across the globe, and realized that a main obstacle in forecasting is that it remains an extremely hard and expensive process that requires a highly skilled team . W e wanted to change that and democratize access to state-of-the-art time series tools, without the need for a dedicated team of machine learning engineers. Inspired by the revolution of OpenAI and others in text processing, we aimed to bring the whole paradigm shift of generative pre-trained models to time series . It wasn’t clear yet if there could be an ef fective foundation model in time series analysis, so we set out to explore what was possible and how accurate it could be. W e didn’t want to do this just for the sake of it being done. W e wanted it to provide fast and accurate results for people working in time series. This is how T imeGPT , the first foundation model for time series, was created and released. H o w e x a c t ly d o e s T im e G P T d i\u0000 e r f r o m a n L L M ? H o w is t im e s e r ie s d a t a t r a n s f o r m e d in t o t o k e n s t o b e f e d t o t h e 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 3 /9 t r a n s f o r m e r ? W e get this question a lot: GPT here stands for a generative pre-trained transformer , which is in no way related to a classical Large Language Model (LLM). The GPT in ChatGPT stands for the same thing, because they use transformer approaches, but we don’t do the Language part, we do the data part and deal with time series data. W e essentially built a completely new model that was trained using publicly available time series from dif ferent domains, including retail, IoT , manufacturing, healthcare, electricity , and web traf fic. It does not understand text; it only understands time series for forecasting and anomaly detection tasks . Technically speaking, the optimization function of T imeGPT is very dif ferent from the sequence prediction task in natural language contexts. In our case, the model takes as input the data with timestamps, values, and exogenous variables and outputs predictions or the anomalies detected. That means, when we speak about tokens in T imeGPT, we are really referring to timestamps . There is no embedding process in our case. D e s p it e d e e p le a r n in g 's t r a n s f o r m a t iv e im p a c t o n \u0000 e ld s lik e N L P a n d co m p u t e r v is io n , it s co n t r ib u t io n s t o t im e s e r ie s f o r e ca s t in g h a v e b e e n m o r e m e a s u r e d . F r o m y o u r   p e r s p e c t iv e , w h a t b r e a k t h r o u g h s d o e s T im e G P T r e p r e s e n t in t h is co n t e x t ? That’ s a great question. There was a lot of discussion in the field as to whether deep learning approaches would outperform classical models. W e did a lot of comparisons and work in this space ourselves, and the classical models do very well in many contexts! Readers familiar with our past contributions will remember that Nixtla has played a pivotal role in demonstrating how classical models outperformed many so- called state-of-the-art models with a fraction of the cost and complexity . W e have published dif ferent experiments showcasing this. Therefore this evaluation of deep learning models is at the leading edge of the field, and we’ve done research in this space for many years. T imeGPT has been the first demonstration that a deep learning approach can not only outperform existing approaches, but also is much, much faster. From our perspective, the breakthroughs that T imeGPT represents in this context are significant. W e believe that times are changing, mainly due to data and compute availability . Currently , deploying pipelines for time series forecasting involves several steps, from data cleaning to model selection, that require a lot of ef fort and specialized knowledge unavailable to many users and companies. Pre-trained models offer a whole new 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 4 /9 paradigm in time series forecasting and anomaly detection given that users don’t have to train and deploy their own models. Simply upload and forecast. The main breakthrough of T imeGPT is that it showed for the first time in the history of the field that the idea of a general pre-trained model was possible. In other words, T imeGPT is the first large-scale example of the transferability of time series models ready for production. W e believe this marks a new chapter in the time series field, and we are extremely happy to see entities like Google (T imesFM), ServiceNow (LagLlama), Amazon (Chronos), Salesforce (Moirai), and CMU (Moment) following in our footsteps and contributing to this idea of pre-trained models for time series. I'm cu r io u s a b o u t t h e ch o ice o f a T r a n s f o r m e r - b a s e d m o d e l f o r T im e G P T. W h a t d r o v e t h is d e cis io n , a n d h o w h a s it in \u0000 u e n ce d t h e m o d e l's p e r f o r m a n ce a n d it s a b ilit y t o s ca le ? The short answer is: it was empirical. W e tried (and are still trying) dif ferent deep learning architectures for time series. In our tests, we found transformers to be highly scalable and accurate when using huge and diverse amounts of data. D e a lin g w it h u n ce r t a in t y is k e y in f o r e ca s t in g . A s w e ll a s u n d e r s t a n d in g u n d e r ly in g t e m p o r a l d y n a m ics , s u c h a s s e a s o n a lit y a n d t r e n d s . H o w d o e s T im e G P T t a ck le t h is , a n d d o e s it h a v e a n y e d g e o v e r m o r e t r a d it io n a l m e t h o d s ? W e could spend hours writing about explainability and uncertainty quantification in time series, and we believe those are extremely exciting topics in the field. They’re some of our favorite things to discuss! In the case of T imeGPT , we currently support uncertainty quantification with Conformal Prediction and will soon release other forms of probabilistic forecasting with multi-quantile and distribution losses . Additionally , T imeGPT of fers the possibility of understanding the dif ferent weights that exogenous variables play in your forecasts. For example, with our weights function, you can see if certain holidays or aspects such as weather are driving your sales up or down. Understanding the role of different covariates in the output creates the possibility to build what-if scenarios easily , and in just a few lines of code. 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 5 /9 H o w d o e s T im e G P T s t a ck u p a g a in s t o t h e r s t a t e - o f - t h e - a r t d e e p le a r n in g m o d e ls t a ilo r e d f o r t im e s e r ie s f o r e ca s t in g , s u ch a s D e e p A R , P a t ch T S T, o r T F T, e s p e c ia lly in t e r m s o f co m p u t a t io n a l e \u0000 cie n cy a n d p r e d ic t io n a ccu r a cy ?   W e conducted a large benchmark with more than 300,000 unique series, comparing zero-shot accuracy of T imeGPT (namely , the model never saw this data during training) against classical, machine learning, and deep learning models that were trained on the data. W e found that T imeGPT , even without training, outperformed all other methods for weekly and monthly data and scored second and third for daily and hourly data. In terms of speed, T imeGPT is orders of magnitude faster than other deep learning models. For zero-shot inference, our internal tests recorded an average GPU inference speed of 0.6 milliseconds per series for T imeGPT , which nearly mirrors the simple Seasonal Naive. As points of comparison, parallel computing-optimized statistical methods, when complemented with Numba compiling, averaged a speed of 600 milliseconds per series for training and inference. On the other hand, global models such as LGBM, LSTM, and NHITS demonstrated an average of 57 milliseconds per series, considering both training and inference. Due to its zero-shot capabilities, T imeGPT outperforms traditional statistical methods and global models in total speed by orders of magnitude. This is extremely important because it opens the possibility of new low-latency use cases in IoT , W eb Monitoring and Manufacturing. 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 6 /9 Image Credit: The original paper W h a t k e y d i\u0000 e r e n ce s e x is t b e t w e e n T im e G P T a n d o t h e r r e ce n t o p e n - s o u r ce t im e s e r ie s f o u n d a t io n m o d e ls s u c h a s L a g L la m a , P r e D cT, T im e s F M , a n d M O IR A I? A r e y o u p la n n in g t o b e n ch m a r k T im e G P T a g a in s t t h e o p e n - s o u r ce o n e s ? W e are seeing two families of foundation models emerge in the field: one family , like LagLlama and Chronos, leverages actual LLM models or intuitions behind their architectural design, and others, like T imeGPT and Moirai, rely solely on time series data and architectures. So far , we have found T imeGPT significantly faster and more accurate than its competitors, but we are currently working on a large-scale benchmarking of all available foundation models before we make any of ficial conclusions. Meanwhile, we have released open and fully reproducible benchmarks of Moirai, Chronos, and LagLlama. W e found that LagLlama is 40% less accurate than a simple Seasonal Naive and 1000x slower . W e also found that Amazon Chronos is 10% less accurate and 500% slower than training classical statistical models. Those results were validated by Amazon in the selected datasets (however , Amazon expanded our benchmark with new datasets and showed that Chronos could be 4% more accurate 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 7 /9 and faster than statistical models). W e also found that Salesforce's Moirai performs great in hourly data and is much faster than Chronos but is still up to 33% less accurate and less ef ficient than statistical models when considering monthly , weekly , and yearly data. Conducting benchmarks against dif ferent models, in an open and reproducible way , is important to us as a company . W e share our experiments and code in our GitHub repository: https://github.com/Nixtla/nixtla/tree/main/experiments T im e G P T w a s t r a in e d o n a p r o p r ie t a r y d a t a s e t o f o v e r 1 0 0 b illio n d a t a p o in t s ( o r d e r s o f m a g n it u d e la r g e r t h a n t h e la r g e s t a v a ila b le p u b lic d a t a s e t s ) . W h a t d o e s it e n t a il t o p u t t o g e t h e r s u ch a d a t a s e t ? A n y p la n s t o o p e n - s o u r ce it ? Pain, sweat, and tears. And lots of data wrangling. Jokes aside, creating that dataset was an important ef fort that gave us a significant advantage against titans such as Amazon, Google, and Salesforce. W e were able to do this because of our team’ s extensive experience in not just time series research, but also in being users, and working with so many people using our open source tools. W e had a good understanding of what data was important, and the depth and variability needed. W e are indeed considering open-sourcing the data and maybe even some earlier versions of the models, but can't promise anything. That being said, all our libraries are completely open source and always will be open source, and can be used to train more foundation models. T r a in in g t h e m o d e l in a b r o a d r a n g e o f s ce n a r io s a llo w s t h e m o d e l t o le a r n p a \u0000 e r n s t h a t r e p e a t b e t w e e n s e r ie s , t h a t it w o u ld n ' t b e a b le t o p r e d ic t if t r a in e d o n a s in g le s e r ie s . A s a n e x a m p le , it co u ld p r e d ic t a n e w p a n d e m ic f r o m o t h e r p a n d e m ics ' d a t a . H a v e y o u d o n e a n y t e s t s o r h a v e a n y in s ig h t s in t o t h is lin e o f w o r k ? T raining in a source domain and then forecasting in a new target domain encapsulates the whole idea behind the transferability of time series models. W e are working on a paper that precisely explores that. Generally speaking, transfer learning is a machine learning technique that involves applying knowledge acquired from solving a source task to enhance performance on a distinct yet related task. Models pre-trained on large datasets for the source task learn valuable patterns and 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 8 /9 representations, which can help smaller target tasks by improving the models’ generalization and reducing their computational costs and data requirements. Our empirical findings confirm that this is indeed the case for time series, and pre-trained models can outperform the accuracy of widely adopted automatic forecasting tools, with computational speed improvements on the orders of magnitude. In our exploration of what makes transferability work, we discovered that the source dataset’ s size and diversity play a critical role in the accuracy of pre-trained models. So, we’ve seen some evidence already that this is possible, and are optimistic for good performance on the types of applications like pandemic predictions. W h a t 's n e x t o n t h e h o r iz o n f o r T im e G P T ? A r e t h e r e a n y a r e a s y o u 'r e p a r t icu la r ly k e e n t o e n h a n ce o r e x p a n d in t o ? Nixtla has a clear vision of becoming the go-to solution for time series practitioners. W e have invested a lot of time and ef fort in making state-of-the-art open source tools available to practitioners and have also invested significant time in making T imeGPT usable for everyone through fully or self-hosted versions of the models. That means that now individuals, or organizations of all sizes can perform out-of-the-box forecasting tasks or replace thousands of lines of code with a few API calls. 4 /1 0 /2 4 , 8 :2 3 P M R e v o lu tio n iz in g T im e S e r ie s F o r e c a s tin g : In te r v ie w w ith T im e G P T 's c r e a to r s c h r o m e - e x te n s io n ://m p io d ijh o k g o d h h o fb c jd e c p ffjip k le /s r c /u i/p a g e s /e d ito r.h tm l 9 /9 W e have a very ambitious roadmap for the future and are constantly working on new features and better models. Multimodality in time series is the next big thing, and something we’re working on. A s w e lo o k t o t h e f u t u r e , w h a t 'b ig q u e s t io n s ' in A I d o y o u t h in k n e e d m o r e a \u0000 e n t io n ? O r, a r e t h e r e a n y e m e r g in g \u0000 e ld s t h a t ca t ch y o u r e y e ? There do remain so many big questions with broad societal and economic impacts. W e believe the field would benefit from more diversity . W e are extremely proud to be a queer and Latino-founded company and believe that having more diverse backgrounds in the space would benefit everyone . Our thesis is: that diversity of data improves forecasting accuracy , but it also makes a better ecosystem. W h a t b o o k w o u ld y o u r e c o m m e n d t o a s p ir in g d a t a s cie n t is t s / M L e n g in e e r s ? ( It d o e s n ’ t n e ce s s a r ily h a v e t o b e a b o u t M L !)   W e like Rob Hyndman and George Athanasopoulos’ s book, Forecasting: Principles and Practice , because it combines the theory with examples and code for practical implementation. W e’re also very excited that we get to work on the Python version of this book, so that’ s coming soon. W e have some recommendations when talking with aspiring engineers, and something we always keep in mind ourselves. Learn the basics. Always benchmark. Simpler is many times better . Prophet is a bad forecasting algorithm. Happy Forecasting! F o r o u r P r e m iu m m e m b e r s , w e co m p ile d a lis t o f h e lp f u l lin k s : Thank you for reading! if you find it interesting, please do share 🤍 This interview would not be possible without the help of Ian Spektor, Lead ML Engineer at T ryolabs, an AI consulting company that helps companies accelerate their AI adoption.","libVersion":"0.3.1","langs":""}
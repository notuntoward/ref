{"path":"lit/lit_notes_OLD_PARTIAL/Sun23MaximumOptimalityMargin.pdf","text":"Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming Chunlin Sun * 1 Shang Liu * 2 Xiaocheng Li 2 Abstract In this paper, we study the predict-then-optimize problem where the output of a machine learn- ing prediction task is used as the input of some downstream optimization problem, say, the objec- tive coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing ap- proaches largely suffer from either (i) optimiza- tion intractability (a non-convex objective func- tion)/statistical inefficiency (a suboptimal general- ization bound) or (ii) requiring strong condition(s) such as no constraint or loss calibration. We de- velop a new approach to the problem called max- imum optimality margin which designs the ma- chine learning loss function by the optimality con- dition of the downstream optimization. The max- margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the opti- mal solution in the training data rather than the ob- jective function, which makes it a new and natural approach to the inverse linear programming prob- lem under both contextual and context-free set- tings; we also analyze the proposed method under both offline and online settings, and demonstrate its performance using numerical experiments. 1. Introduction The predict-then-optimize problem considers a learning problem under a decision making context where the output of a machine learning model serves as the input of a down- stream optimization problem (e.g. a linear program). The *Equal contribution 1Institute for Computational and Mathemat- ical Engineering, Stanford University, Palo Alto, CA. 2Imperial College Business School, Imperial College London, London, UK. Correspondence to: Chunlin Sun <chunlin@stanford.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). ultimate goal of the learner is to prescribe a decision/solution for the downstream optimization problem using directly the input (variables) of the machine learning model but without full observation of the input of the optimization problem. A similar problem formulation was also studied as prescrip- tive analytics (Bertsimas & Kallus, 2020) and contextual linear programming (Hu et al., 2022). While (Elmachtoub & Grigas, 2022) justifies the importance of leveraging the optimization problem structure when building the machine learning model, the existing efforts on exploiting the op- timization structure have been largely inadequate. In this paper, we delve deeper into the structural properties of the optimization problem and propose a new approach called maximum optimality margin which builds a max-margin learning model based on the optimality condition of the downstream optimization problem. More importantly, our approach only needs the observations of the optimal solution in the training data rather than the objective function, thus it draws an interesting connection to the inverse optimization problem. The connection gives a new shared perspective on both the predict-then-optimize problem and the inverse optimization problem, and our analysis reveals a scale in- consistency issue that arises practically and theoretically for many existing methods. Now we present the problem formulation and provide an overview of the existing techniques and related literature. Consider a linear program (LP) that takes the following standard form LP(c, A, b) := min c⊤x, (1) s.t. Ax = b, x ≥ 0. where c ∈ Rn, A ∈ Rm×n, and b ∈ Rm are the inputs of the LP. In addition, there is an available feature vector z ∈ Rd that encodes the useful covariates (side information) associated with the LP. Predict-then-optimize/Contextual LP The problem of predict-the-optimize or contextual LP is stated as follows. A set of training data DML(T ) := {(ct, At, bt, zt)} T t=1 consists of i.i.d. samples from an unknown distribution P, where ct ∈ Rn, At ∈ Rm×n, bt ∈ Rm, and zt ∈ Rd. 1 Maximum Optimality Margin for Contextual and Inverse Linear Programming 2 Throughout the paper, we assume the constraints have a fixed dimensionality for notational simplicity, while all the results can be easily extended to the case of variable dimen- sionalities. The goal of a conventional machine learning (ML) model is to identify a function g(·; Θ) : Rd → Rn that best predicts the objective coefficient vector ct using the covariates zt with some model parametrized with Θ. However, the predict-then-optimize problem has a slightly different pipeline for the testing phase. It aims to map from the observation of the context and the knowledge of the constraints to a decision (from data to decision): (Anew, bnew, znew) → xnew without the observation of cnew, where the tuple (cnew, Anew, bnew, znew) is a new test sample from P. That is, for this new sample, the decision maker knows the con- straints (Anew, bnew), and the aim is to predict the unknown objective cnew from znew and determine xnew accordingly. There are commonly three performance measures for the problem (as noted by (Chen & Kılınc¸-Karzan, 2020)) Prediction loss: lpre( ˆΘ) = E [ ∥cnew − ˆcnew∥ 2 2] Estimate loss: lest( ˆΘ) = E [ c ⊤ newxnew − c ⊤ newx∗ new] Suboptimality loss: lsub( ˆΘ) = E [ˆc⊤ newx∗ new − ˆc ⊤ new ˆxnew] where ˆcnew = g(znew; ˆΘ) is the predicted output of the ML model with parameters ˆΘ. Here ˆxnew and x∗ new are the optimal solutions of LP(ˆcnew, Anew, bnew) and LP(cnew, Anew, bnew), respectively. The expectations are taken with respect to this new sample. The prediction loss is aligned with the standard ML prob- lems, where the L2 loss can also be replaced by other proper metrics. The estimate loss captures the quality of the rec- ommended decision xnew under the true (unobserved) cnew and it is a more natural loss given the interests in the down- stream optimization task. The suboptimality loss measures how well the predicted ˆcnew explains the realized optimal solution x∗ new. It is more commonly adopted in the inverse linear programming literature (Mohajerin Esfahani et al., 2018; B¨armann et al., 2018; Chen & Kılınc¸-Karzan, 2020). Inverse linear programming Our proposed approach to the predict-then-optimize prob- lem also solves a seemingly unrelated problem – inverse LP. In parallel to predict-then-optimize, the problem of inverse LP considers a set of training data Dinv(T ) := {(x∗ t , At, bt, zt)} T t=1 . Similarly to the previous case, the samples (ct, At, bt, zt)’s are generated from an unknown distribution P. Differently, for inverse LP, the optimal solution x∗ t instead of the ob- jective coefficient vectors ct is given in the training data. While the classic setting of inverse LP does not consider the context, i.e., zt ≡ 1 for all t, several recent works (Moha- jerin Esfahani et al., 2018; Besbes et al., 2021) study the contextual case. The goal of the inverse LP is similar to that of the predict-then-optimize problem, to learn a function g(·; Θ) that maps from the context zt to the objective ct. We emphasize that inverse LP is a much harder problem than contextual linear programming for several reasons. First, the observation of x∗ t gives much less information than that of ct, which makes the inverse problem information theo- retically more challenging than the predict-then-optimize problem. Second, speaking of the objective function, di- rectly minimizing the suboptimality loss lsub( ˆΘ) can lead to an unexpected failure. To see this, consider the naive prediction that always predicts ˆc to be zero, always leading to an optimal zero suboptimality loss. Similar issues have also appeared in by ignored by the literature (Bertsimas et al., 2015; Mohajerin Esfahani et al., 2018; B¨armann et al., 2018; Chen & Kılınc¸-Karzan, 2020), while our methods avoid such a problem (see Section 5). In the following, we briefly review the representative exist- ing approaches to tackle the two problems. Predicting the objective The first class of approaches treats the predict-then-optimize problem as a two-step procedure. The learner first learns a function g(·; ˆΘ) : z → c from the training data, and then prescribes the final output xnew by the optimal solution of LP(g(znew; ˆΘ), Anew, bnew). There are mainly two existing routes for how the parameter ˆΘ should be estimated from the training data. The first route is to ignore the constraint (At, bt) (in the training data) and treat the training prob- lem as a pure ML problem (Ho-Nguyen & Kılınc¸-Karzan, 2022). The issue of this route is that there can be a mis- alignment between the ML loss lpre(·) and the downstream optimization loss lest(·) or lsub(·). Empirically, it may cause sample inefficiency if one is interested in lest(·). Theoreti- cally, establishing a performance guarantee for the optimiza- tion loss, it requires a calibration condition (Bartlett et al., 2006) which can be hard to satisfy/verify (Ho-Nguyen & Kılınc¸-Karzan, 2022). The second route is to employ the optimization loss lest(·) for the estimation of ˆΘ (Elmachtoub & Grigas, 2022; Elmachtoub et al., 2020). However, the loss function is generally non-convex in the parameter Θ. A convex surrogate loss called SPO+ is proposed, but it also suffers from the misalignment issue when establishing a finite sample guarantee. To solve this problem, (Liu & Grigas, 2021) show that the calibration condition holds, but an O(T −1/2) convergence rate of the SPO+ loss only leads to an O(T −1/4) convergence rate of the SPO loss, which is suboptimal ((El Balghiti et al., 2019)). In the literature of inverse LP, the (convex) suboptimality loss lsub(·) is often used instead of lest(·). Specifically, our proposed approach 2 Maximum Optimality Margin for Contextual and Inverse Linear Programming 3 falls in this category of first predicting the objective and then solving the LP. Predicting the optimal solution The second class of approaches treats the predict-then- optimize problem as a one-step procedure and aims to learn an end-to-end function that maps from the context zt di- rectly to the optimal solution x∗ t (Bertsimas & Kallus, 2020; Hu et al., 2022). This end-to-end treatment works well in the unconstrained setting. But for the constrained setting, the optimal solution of an LP generally stays at the corner of the feasible simplex, and the end-to-end mapping can hardly predict such a corner solution. In the domain of inverse LP, a recent work (Tan et al., 2020) also proposes a loss function minimizing the gap between the predicted optimal solution and the observed optimal solution. The idea is aligned with the early formulation of inverse optimization (Zhang & Liu, 1996; Ahuja & Orlin, 2001). However, the formulation in (Tan et al., 2020) is more of a conceptual framework that is hardly computationally tractable. Some existing studies on differentiable optimization also fol- low an end-to-end fashion and do not require objective func- tions as training data. Since the optimal solution of an LP is always at the corner, the change of optimal solution with respect to the coefficient is generally discontinuous, which restricts the possibility of backward propagation-based gra- dient descent learning. To address the discontinuity, one can either add some noise to the objective ((Berthet et al., 2020)) or add a regularization term in the objective (Wilder et al. (2019)) to smooth the mapping from the objective to the optimal solution. Then, they can apply gradient descent to find the mapping. However, this smoothing technique only works for problems with fixed constraints but not the general random constraints as considered in our paper. The differentiable model training is based on a convex surrogate loss named Fenchel-Young loss (Blondel et al., 2020) which also enjoys some surrogate property (Blondel, 2019), yet no finite sample bound has been obtained for the proposed algorithms. The training is also computationally costly: it requires a Monte-Carlo step (suppose we sample M times) at each gradient computation, which needs solving M times more linear programs than SPO+. An analogous idea of turning the non-differentiable case into the differentiable one is Wilder et al. (2019) which adds a quadratic regular- ization term to the objective, while the consistency cannot be guaranteed. Consistency/Feasibility check. For the inverse LP problem, a common approach in liter- ature (Zhang & Liu, 1996; Ahuja & Orlin, 2001; Boyd & Vandenberghe, 2004; Bertsimas et al., 2015; B¨armann et al., 2018; Besbes et al., 2021) is to transform the knowledge of the optimal solution x∗ t equivalently to constraints on the objective vector ct ∈ Ct for some polyhedron Ct (through the optimality condition), and then utilize these Ct to make inference about the objective vector. There can be two is- sues with this approach. First, many existing results study the context-free case and require a non-empty intersection of the polyhedrons, i.e., ∩ T t=1Ct ̸= ∅. This requirement may fail in a noisy or contextual setting. Second, from a method- ology perspective, it is difficult to relate the structure of Ct with the covariates zt. This Ct also highlights the difference between predict-then-optimize and inverse LP: the former observes ct in the training data, while the latter only knows ct ∈ Ct for some polyhedron Ct. 2. LP Optimality and Main Algorithm Now we first present some preliminaries on LP and then describe our main algorithm. Let x∗ = (x1, ..., xn)⊤ be the optimal solution of the LP(c, A, b). The optimal basis B∗ and its complement N ∗ of an LP are defined as follows B∗ := {i : x∗ i > 0}, N ∗ := {i : x∗ i = 0}. For a set B ⊂ [n], we use AB to denote the submatrix of A with column indices corresponding to B and cB to denote the subvector with corresponding dimensions. We make the following assumptions on the LP’s nondegeneracy. Assumption 2.1 (Nondegeneracy). All the LPs in this paper are nondegenerate, i.e., satisfying the two conditions: (a) The LP is feasible and has a unique optimal solution. (b) All the submatrices AB are invertible for |B| = m. The optimal basis satisfies |B∗| = m. The assumption is standard in the literature of LP. It is a mild one in that any LP can satisfy the assumption under an arbitrarily small perturbation (Megiddo & Chandrasekaran, 1989). We also note that our focus on the standard-form LP (1) is without loss of generality because all the LPs can be written in the standard form and the results in this paper can be easily adapted to an LP of other forms. The following lemma describes the optimality condition for an LP in terms of its input. Lemma 2.2 ((Luenberger et al., 1984)). Let B ⊂ [n] be a feasible basis of LP(c, A, b), i.e., A−1 B b ≥ 0, and let N = [n]\\B be its complement. Then, under Assumption 2.1, the following inequality holds (element-wise) c ⊤ N − c ⊤ B A−1 B AN ≥ 0 (2) if and only if B = B∗. The result sets the foundation for the simplex method to solve LPs. When B = B∗, the vector p ∗ := c ⊤ B∗ A−1 B∗ ∈ Rm will be the optimal solution of the dual LP of (1), also known as the dual price. 3 Maximum Optimality Margin for Contextual and Inverse Linear Programming 4 2.1. Maximum optimality margin Now we present the approach of maximum optimality mar- gin. Consider the following training data Dinv(T ) = {(x∗ t , At, bt, zt)}T t=1 . Note that the training data only consists of the observations of the optimal solution x∗ t , so all the algorithms and analyses apply to both the predict-then-optimize problem and the inverse LP problem. Denote the sets of basic variables and non-basic variables of the t-th training sample as B∗ t ⊆ [n] and N ∗ t ⊆ [n], respectively. Then the training data can be equivalently re-written as Dinv(T ) = {(x∗ t , At, bt, zt, B∗ t , N ∗ t )} T t=1 . Specifically, we start with a linear mapping from the covari- ates zt to the objective vector ct, i.e., g(zt; Θ) := Θzt. Our maximum optimality margin method solves the follow- ing optimization problem. ˆΘ := arg min Θ∈K λ 2 ∥Θ∥ 2 2 + 1 T T∑ t=1 ∥st∥1 s.t. ˆct = Θzt, t = 1, ..., T, (3) ˆc⊤ t,N ∗ t − ˆc ⊤ t,B∗ t A−1 t,B∗ t At,N ∗ t ≥ 1|N ∗ t | − st, t = 1, ..., T, where the decision variables are Θ ∈ Rn×d, ˆct ∈ Rn, and st ∈ R|Nt|. K is a convex set to be determined. To interpret the optimization problem (3): The equality constraints encode the linear prediction func- tion, and ˆct is the predicted value of ct under Θ. For the inequality constraints, the vector 1|N ∗ t | ∈ R|N ∗ t | is an all-one vector of dimension |N ∗ t |. The left-hand-side of the inequality comes from the optimality condition (2) in Lemma 2.2, while the right-hand-side represents the slack- ness or margin of the optimality condition. From the ob- jective function, it is easy to see that the optimal solution will always render st ≥ 0. Then, when st ∈ [0, 1] element- wise for all t, the inequality constraints fulfill the optimality condition (2) perfectly, which means that the optimal solu- tion ˆΘ is consistent with all the training samples. In other words, when st ∈ [0, 1] element-wise for all t, if we pre- dict the objective coefficient ct with ˆct = ˆΘzt, the two LPs, LP(ct, At, bt) and LP(ˆct, At, bt) share the same opti- mal solution for all t (by Lemma 2.2), and it thus results in a zero estimate loss lest and a zero suboptimality loss lsub on the training data. When some coordinate of st is greater than 1, it means the optimal solution under the predicted objective ˆct = ˆΘzt no longer satisfies the optimality con- dition of the original LP(ct, At, bt), and consequently, this results in a mismatch between the two optimal solutions of LP(ct, At, bt) and LP(ˆct, At, bt), and thus a non-zero loss of lest and lsub. For the objective function, the second part is justified by the above discussion on the inequality constraints. We desire to have a smaller value of st as this leads to better satisfaction of the optimality condition. The first part of the objective function regularizes the parameter Θ. The rationale is that the left-hand-side of the inequality constraints that repre- sents the realized margin of the optimal condition, scales linearly with Θ. We hope the margin to be large but do not want that the large margin is due to the scaling up of Θ. We note that the optimization problem (3) has linear con- straints and a quadratic objective, and thus it is convex. Algorithm 1 describes the procedure of maximum optimal- ity margin for solving the predict-then-optimize problem and the inverse LP problem. It is a two-step procedure in that it first estimates the parameter and then solves the optimization problem based on the estimate ˆΘ. Algorithm 1 Maximum Optimality Margin (MOM) Input: Dataset D(T ) = {(x∗ t , At, bt, zt, B∗ t , N ∗ t )} T t=1, test sample (Anew, bnew, znew), λ, K 1: Solve the optimization problem (3) and obtain the esti- mate ˆΘ 2: Predict ˆcnew = ˆΘznew and solve the following LP min ˆc⊤ newx, s.t. Anewx = bnew, x ≥ 0. 3: Denote its optimal solution as ˆxnew Output: ˆxnew and ˆΘ 2.2. Interpreting the formulation The key idea of the MOM formulation is that it does not explicitly minimize the error of predicting the objective ct’s as a stand-alone ML problem, but it aims to minimize the violation – equivalently, maximize the margin – of the optimality conditions of the training samples (the inequality constraint in (3)). Intuitively, as long as we find a prediction ˆct that shares the same optimal solution x∗ t with ct for most t (hopefully), we should not bother with the error between ˆct and ct. This distinguishes from all the existing methods for predict-then-optimize and inverse LP in that it integrates the optimization problem’s structure naturally into the ML training procedure. We make the following remarks. First, our method does not require the knowledge of ct’s in the training data. This makes our method the first one that works for both predict-then-optimize and inverse LP 4 Maximum Optimality Margin for Contextual and Inverse Linear Programming 5 problems. Beyond the point, this special feature of our method has a modeling advantage of scale consistency or scale invariance. Specifically, we note that for the LP (1), both the objective vectors c and αc for any α > 0 lead to the same optimal solution. Hence the loss of training an ML model should ideally bear a scale invariance in terms of the objective vector. That is, a prediction of ˆc and a prediction of αˆc for any α > 0 should incur the same training loss as both of them lead to the same prescribed solution. Our method enjoys this scale invariance property, since it only utilizes the optimal solution x∗ t in the training phase but does not involve ct. In comparison, a method that minimizes the prediction loss lpre apparently does not have this scale invariance, so do some inverse LP algorithms (see Section 3.3). This property can be critical for some application contexts such as revealed preference or stated preference (Beigman & Vohra, 2006; Zadimoghaddam & Roth, 2012) where one aims to predict the utility ct of a customer based on observed covariates zt. In such contexts, even if we have observations of the ct’s through surveying the customers’ utilities, these observations might suffer from some scale contamination because each customer may have their own scale for measuring utilities. And therefore, striving for an accurate prediction of the observed ct can be misleading. Second, the inequality constraint in (3) is critical. Two tempting alternatives are to replace the inequality constraint of (3) with the following: ˆc⊤ t,N ∗ t − ˆc ⊤ t,B∗ t A−1 t,B∗ t At,N ∗ t ≥ 0, (4) ˆc⊤ t,N ∗ t − ˆc ⊤ t,B∗ t A−1 t,B∗ t At,N ∗ t ≥ −st. (5) For (4), it directly employs the optimality condition (2) in Lemma 2.2. The issue with this formulation is that there may exist no Θ that is consistent with all the training data, and thus (4) will lead to an infeasible problem. In comparison, the inequality constraints of (3) can be viewed as a softer version of (4). For (5), it suffers a similar scale invariance problem as mentioned above. Specifically, the constraint (5) will drive ˆΘ → 0 as the left-hand-side of (5) scales linearly with ˆΘ. To prevent this, one can impose an additional unit sphere constraint by ∥Θ∥2 = 1 but this will lead to a non- convexity issue. Lastly, we note a few additional points for the formulation. First, the formulation does not involve bt in the optimization problem (3) either. This is due to that bt’s information is encoded by (At, B∗ t , N ∗ t ) under the standard-form LP (1). Second, the formulation is presented under a linear mapping of zt to ct; non-linearity dependency can be introduced by kernelizing the original covariates zt (Hofmann et al., 2008). Specifically, we can replace the original feature zt0 by a new T -dimensional feature with (κ(zt0 , zt))T t=1 for some kernel function κ(·, ·). Third, the MOM formulation aims to find a parameter ˆΘ that best satisfies the optimality condition, and it measures the quality of satisfaction by the total margin of optimality condition violation. It does not strive for a structured prediction of the optimal bases Bt’s which will lead to a less tractable formulation such as a structured support vector machine (See Appendix C.1). 3. Theoretical Analysis Now we analyze the maximum optimality margin method under both offline and online settings. We make the follow- ing boundedness assumptions mainly for notation simplicity. Specifically, we note that the parameter ¯σ captures some “stability” of the LP’s optimal solution under perturbation of the constraint matrix. While our algorithm utilizes the optimal solutions in the training data, better stability of the optimal solutions will lead to more reliable learning of the parameter. Assumption 3.1 (Boundedness). Let the tuple (c, A, b, z, B∗, N ∗) be a sample drawn from the dis- tribution P. The following assumptions hold with probability 1. (a) There exists a constant ¯σ such that σmax(A−1 B∗ ) ≤ ¯σ, where σmax(·) denotes the largest singular value func- tion of a matrix. (b) The covariates vector z is bounded by 1, i.e., ∥z∥2 ≤ 1. (c) All entries of the LP’s input (A, b, c) are within [−1, 1]. (d) The feasible region {x ≥ 0 : Ax = b} is bounded by the 2-norm unit ball. 3.1. Separable Case We begin our discussion with the separable case where there exists a parameter that meets the optimality conditions on all the training samples with a margin. Assumption 3.2. There exists Θ∗ such that the following inequality holds element-wise almost surely, ˆc ⊤ N ∗ − ˆc ⊤ B∗ A−1 B∗ AN ∗ ≥ 1 (6) where ˆc = Θ ∗z and (c, A, b, z, B∗, N ∗) is sampled from P. Suppose ∥Θ∗∥F ≤ ¯Θ for some constant ¯Θ > 0. The assumption is weaker than assuming c = Θ∗z almost surely. It only requires the existence of a linear mapping ˆc = Θ ∗z that meets the optimality condition almost surely. It can happen that Assumption 3.2 holds but c ̸= ˆc almost surely. The right-hand-side of (6) changes from that of (2) in Lemma 2.2 from 0 to 1. The change is not essential when the LP is nondegenerate almost surely; one can always scale up the parameter Θ∗ to meet this margin requirement. 5 Maximum Optimality Margin for Contextual and Inverse Linear Programming 6 Proposition 3.3. Under Assumptions 2.1, 3.1 and 3.2, let ˆΘ denote the output of Algorithm 1 with T training samples, λ = 1√ T and K = {Θ ∈ Rn×d : ∥Θ∥F ≤ ¯Θ}. Suppose (cnew, Anew, bnew, znew) is a new sample from P and denote ˆcnew = ˆΘznew. Let x∗ new and ˆxnew denote the optimal solu- tions of LP(cnew, Anew, bnew) and LP(ˆcnew, Anew, bnew). Then we have E [P (x∗ new = ˆxnew)] ≥ 1 − 28 + 8 ¯Θ2 + 7m¯σ2 √T , (7) where m is the number of constraints in each LP, the expec- tation is taken with respect to the training samples, and the probability is with respect to the new test sample. The proposition states that under the separable case, the algorithm can predict the optimal solution accurately with high probability. We note that the result does not concern the prediction error in terms of the objective vector, and this fact is aligned with the design of MOM that focuses on the optimality condition instead of the objective vector. Intuitively, even when one makes some error in predicting the objective cnew, the prediction of the optimal solution can still be accurate due to the simplex structure of LP. The proof of the proposition mimics the algorithm stability analysis (Bousquet & Elisseeff, 2002; Shalev-Shwartz et al., 2010) where the regularization term in (3) plays a role of stabilizing the estimation. Here the performance bound is presented in the sense of on expectation. We remark that a high probability bound (removing the expectation in (7)) can also be achieved following the recent analysis of (Feldman & Vondrak, 2019). 3.2. Inseparable case Now we move on to the inseparable case where Assumption 3.2 no longer holds. Let Dnew = (c, A, b, z, B∗, N ∗) denote a new sample from P. Define the margin-violation loss function l(Dnew; Θ) := ∥s∥1 where s := 1|N ∗|−ˆc ⊤ N ∗ −ˆc ⊤ B∗ A−1 B∗ t AN ∗ and ˆc = Θz. The loss function is inherited from the objec- tive function of the MOM formulation (3). For the insepara- ble case, we can derive the following generalization bound as Proposition 3.3. Proposition 3.4. Under Assumptions 2.1 and 3.1, let ˆΘ denote the output of Algorithm 1 with T training samples, under the choice of λ = 1√T and K = {Θ ∈ Rn×d : ∥Θ∥F ≤ ¯Θ} for some ¯Θ > 0. Then we have E[l(Dnew; ˆΘ)] ≤ min Θ∈K E[l(Dnew; Θ)] + 28 + 8 ¯Θ2 + 7m¯σ2 √T + 1 , where the expectation is taken with respect to both the new sample Dnew and the training samples. The proposition follows the same analysis as Proposition 3.3 and the right-hand-side involves an additional term which will become zero under the separability condition of As- sumption 3.2. In the previous section, we motivate the opti- mization formulation (3) of MOM in its own right. The fol- lowing corollary reveals an interesting connection between its objective function and the suboptimality loss. Specifi- cally, the margin violation loss optimized in MOM can be viewed as an upper bound of the suboptimality loss. Proposition 3.5. The following inequality holds for any Θ ∈ Rn×d, lsub(Θ) ≤ E[l(Dnew; Θ)]. Therefore, under Assumptions 2.1 and 3.1, and the same setup as Proposition 3.4, lsub( ˆΘ) ≤ min Θ∈K E[l(Dnew; Θ)] + 28 + 8 ¯Θ2 + 7m¯σ2 √T . The additional term on the right-hand-side in both Propo- sition 3.4 and Proposition 3.5 can be viewed as a measure of separability in terms of the LP’s optimality condition. Specifically, if one can predict the objective c very well with the covariates z, then this term will become close to zero; otherwise, this term will become large. While the standard measurement of the predictive power of z concerns the minimum error in predicting c, it does not account for the structure of the downstream LP optimization problem. 3.3. Online maximum optimality margin The MOM formulation also enables an online algorithm which reduces the quadratic program of (3) to a sub-gradient descent step per sample. Specifically, we can view the margin violation of the t-th sample as a piecewise-linear function of Θ, i.e., l(Dt; Θ) = ∥st∥1 = ∥ ∥ ∥1|N ∗ t | − ˆc ⊤ t,N ∗ t − ˆc⊤ t,B∗ t A−1 t,B∗ t At,N ∗ t ∥ ∥ ∥ 1 where Dt represents the t-th sample in the dataset D(T ). The function l(Dt; Θ) is a convex function with respect to Θ and the piecewise linearity enables an efficient calculation of the sub-gradients. Algorithm 2 arises from an online sub- gradient descent algorithm with respect to the cumulative loss ∑T t=1 l(Dt; Θ). Proposition 3.6. Under Assumptions 2.1 and 3.1, with the choice of η = 2 ¯Θ ( √n+¯σ·mn)√T and K = {Θ ∈ Rn×d : ∥Θ∥F ≤ ¯Θ} for some ¯Θ > 0, the outputs of Algorithm 2 satisfy 1 T E [ T∑ t=1 ˆc ⊤ t (x ∗ t − xt) ] ≤E [ min Θ∈K T∑ t=1 l(Dt; Θ) ] + 3 ¯Θ√n + 3¯σ ¯Θ · mn √T (8) 6 Maximum Optimality Margin for Contextual and Inverse Linear Programming 7 Algorithm 2 Online MOM Algorithm Input: Dataset D(T ) = {(x∗ t , At, bt, zt, B∗ t , N ∗ t )} T t=1, step size η, K 1: Initialize Θ1 = 0 ∈ Rn×d 2: for t = 1, ..., T do 3: Predict the objective by ˆct = Θtzt 4: Solve the following LP and denote its optimal solu- tion by xt min ˆc ⊤ t x, s.t. Atx = bt, x ≥ 0. 5: Update Θt+1 = ProjK (Θt − η · ∂Θl(Dt; Θt)) 6: end for Output: {xt} T t=1, {ΘT } T +1 t=1 where Dnew = (cnew, Anew, bnew, znew) denotes a new sam- ple. Moreover, under Assumption 3.2, let ˆcnew = ˆΘznew where ˆΘ is uniformly randomly sampled from {Θt} T +1 t=1 . Let x∗ new and ˆxnew denote the optimal solutions of LP(cnew, Anew, bnew) and LP(ˆcnew, Anew, bnew). Then we have E [P (x∗ new = ˆxnew)] ≥ 1 − 3 ¯Θ √n + 3¯σ ¯Θ · mn √T + 1 (9) where the expectation is taken with respect to both training data samples and the new sample. The proposition states that the results in Proposition 3.3 and Proposition 3.4 can also be achieved by a subgradient descent algorithm from an online perspective. The bound (8) also recovers the regret bounds in the online inverse op- timization literature (B¨armann et al., 2018; Chen & Kılınc¸- Karzan, 2020) under a contextual setting. However, the algorithms therein require the convex set K not containing the origin (which will significantly restrict the predictive power of zt → ct). Otherwise, we find numerically that these existing algorithms will drive the parameter Θt → 0 and ˆct → 0, and consequently provide no meaningful pre- diction of the optimal solution. This reinforces the point of scale invariance raised earlier. As mentioned earlier, the margin-based formulation of MOM resolves the issue of scale invariance, and as a result, Algorithm 2 can provide an additional bound (9) and also perform stably well in numerical experiments, which we refer to Appendix A.4. 3.4. Tighter bound under separability – optimality-driven perceptron We conclude this section with Algorithm 3 that achieves a faster rate under the separability condition (Assumption 3.2). Specifically, the algorithm is an online algorithm that utilizes the knowledge of separability. The idea is to treat each inequality constraint in (3) as an individual binary classification task of distinguishing non-basic variables from basic variables, and view the margin of optimality condition as the margin for classification. Then when the margin (the reduced cost in Algorithm 3) drops below a threshold of 1/2, we perform an update of the parameter. Algorithm 3 Optimality-driven Perceptron Algorithm Input: Dataset D(T ) = {(x∗ t , At, bt, zt, B∗ t , N ∗ t )} T t=1 1: Let Θ1 = 0 ∈ Rn×d 2: %% We use (M )i to denote the i-th row of the matrix M 3: for t = 1, ..., T do 4: Predict the objective by ˆct = Θtzt 5: Solve the following LP and denote its optimal solu- tion by xt min ˆc ⊤ t x, s.t. Atx = bt, x ≥ 0. 6: Let Θtmp = Θt 7: for i = 1, ..., n do 8: if i /∈ N ∗ t then 9: Continue 10: end if 11: Predict the objective by ˆc(i) t = Θtmpzt 12: Compute the reduced cost r(i)⊤ t = ˆc (i)⊤ t − ˆc(i)⊤ t,B∗ t A−1 t,B∗ t At 13: if r(i) t,i ≤ 1 2 then 14: Update (Θtmp)i ← (Θtmp)i + z⊤ t (Θtmp)B∗ t ← (Θtmp)B∗ t − A−1 B∗ t Atzt 15: end if 16: end for 17: Let Θt+1 = Θtmp 18: end for Output: {xt}T t=1, {Θt} T +1 t=1 Proposition 3.7. Under Assumptions 2.1, 3.1 and 3.2, the number of mistakes made by Algorithm 3 is independent of T , # {t ∈ [T ] : x ∗ t ̸= xt} ≤ ¯Θ2 + ¯σ2 ¯Θ2m 2n. In addition, suppose (cnew, Anew, bnew, znew) is a new sam- 7 Maximum Optimality Margin for Contextual and Inverse Linear Programming 8 ple from P and denote ˆcnew = ˆΘznew where ˆΘ is uni- formly randomly sampled from {Θt}T +1 t=1 . Let x∗ new and ˆxnew denote the optimal solutions of LP(cnew, Anew, bnew) and LP(ˆcnew, Anew, bnew), E [P(x∗ new = ˆxnew)] ≥ 1 − ¯Θ 2 + ¯σ2 ¯Θ2m 2n T + 1 , where the expectation is taken with respect to both the train- ing data and the new data. Proposition 3.7 provides an upper bound on the number of mistakes made by Algorithm 3. The algorithm improves the dependency on T compared with the previous bounds in Proposition 3.3 and Proposition 3.6. However, we note this improvement sacrifices the dependency on either the problem size m and n or the constants of ¯Θ and ¯σ. In this light, the result is more of theoretical interest that completes our discussion. In the numerical experiments in Appendix A.4, we observe that the performance of Algorithm 3 is indeed worse than Algorithm 1 and Algorithm 2. 4. Numerical Experiments We conduct numerical experiments for two underlying LP problems – the shortest path problem and the fractional knap- sack problem. We present one experiment here and defer the remaining ones to Appendix A. The experiments compare the proposed MOM algorithms against several benchmarks and illustrate different aspects, such as loss performance, computational time, scale consistency/invariance, sample complexity, kernelized version of MOM, and online setting. Specifically, we consider a shortest path (SP) problem here following the setup of (Elmachtoub & Grigas, 2022). The SP problem is defined on a 5 × 5 grid network with n = 40 directed edges associated with a cost vector c ∈ Rn, and the covariates z ∈ Rd with d = 6. For the training data, the covariates are generated from the Gaussian distribution, and the objective vector is generated from a polynomial mapping from the covariates with additional white noise. For this numerical experiment, we consider the performance measure of relative loss defined by Relative Loss := c⊤ new(xnew − x∗ new) c⊤x∗ new where cnew is the objective vector of a new test sample, xnew is the predicted optimal solution, and x∗ new is the true optimal solution. Indeed, this relative loss normalizes the estimate loss lest with the optimal objective value. We defer more details on the experiment setup to Appendix A. Figure 1 presents the experiment results for two MOM al- gorithms and a few benchmarks, and each panel represents a different degree of the polynomial that governs the true mapping from the covariates to the objective vector; a higher degree indicates a stronger non-linearity between the covari- ates and the objective. We make the following observations: First, from an information viewpoint, all four benchmark algorithms utilize the observations of ct’s on the training data, while the two MOM algorithms only observe x∗ t ’s on the training data. In this sense, our algorithms give a better predictive performance with less amount of information, and therefore our algorithms are the only ones applicable to the inverse LP problem. Second, other than the two MOM algorithms, the SPO+ per- forms the best among the four benchmark algorithms. How- ever, the SPO+ takes significantly longer training time than all the other algorithms. Although SPO+ is a stochastic gra- dient descent-based algorithm, the calculation of the stochas- tic gradient at each step requires solving k LPs with k being the number of training samples in the mini-batch. Com- paratively, our MOM Algorithm 1 only solves a quadratic program for once, and its online version of Algorithm 2 allows a direct calculation of the online sub-gradient which makes it even faster than Algorithm 1. Third, the three ML-based methods (RF, OLS, Ridge) per- form generally worse than the other three methods (SPO+, MOM, MOM-OGD) because they do not take into account the optimization structure. To see this, for the shortest path problem, there may be some large entries for the cost vector c. The ML models treat all the dimensions of c equally and spend too much effort in fitting those large entries (as they cause large L2 errors). However, from the optimization per- spective, an accurate estimation of those large entries is not useful in that the optimal path will avoid those edges. That highlights the intuition behind the SPO+ and our MOM algo- rithms – one needs to incorporate the optimization structure into the learning model. 5. Discussions Many existing inverse LP algorithms (B¨armann et al., 2018; Chen & Kılınc¸-Karzan, 2020) implement algorithms that directly minimize the suboptimality loss lsub = ˆc⊤x∗ − ˆc⊤ ˆx via an online gradient descent (OGD) algorithm. We briefly showcase a simple example in the linear case ˆc = Θz with lsub,t(Θ) = ⟨Θ, (x ∗ t − ˆxt)z⊤ t ⟩ and regarding the gradient as (x∗ − ˆx)z⊤. One may wonder if such an OGD achieves a performance guarantee of O(1/√T ) (as our result in the previous sections) following the analysis of OGD of the convex functions. But we will show its impossibility. Why not OGD? The reason is that the minimizer of lsub(Θ) is lsub(0) ≡ 0. The dilemma of OGD starts: if the feasible region of the parameter K contains the original point 0, Θt under OGD will be gradually drawn to 0. But this means that the algorithm does not learn anything meaningful. This is a common issue for many inverse LP papers (Bertsimas et al., 8 Maximum Optimality Margin for Contextual and Inverse Linear Programming 9 R F O L S R i d g e S P O + M O M M O M - O G D 0.00 0.05 0.10 0.15Relative Loss Degree 1 R F O L S R i d g e S P O + M O M M O M - O G D 0.0 0.1 0.2 0.3 0.4 0.5 Degree 2 R F O L S R i d g e S P O + M O M M O M - O G D 0.5 1.0 1.5 2.0 2.5 3.0 Degree 4 R F O L S R i d g e S P O + M O M M O M - O G D 0 10 20 30 40 Degree 6 Random Forest Ordinary Least Squares Ridge Regression SPO+ Maximum Optimality Margin MOM-Online Gradient Descent Figure 1. Relative loss. The plot (means and confidence intervals) is generated based on 30 random trials each with 1000 training samples and 1000 test samples (20% of training samples are used for tuning hyper-parameters). The degree indicates the degree of the true polynomial function that generates the objective c from the covariates z. RF denotes random forests, OLS denotes ordinary least squares, Ridge denotes the ridge linear regression, SPO+ implements the algorithm of (Elmachtoub & Grigas, 2022), MOM implements Algorithm 1, and MOM-OGD implements Algorithm 2. For all these methods, they first predict the objective vector for a test sample, and solve an LP with the predicted objective for the optimal solution as the predicted solution for this test sample. 2015; Mohajerin Esfahani et al., 2018; B¨armann et al., 2018; Chen & Kılınc¸-Karzan, 2020), regardless of offline and online. In fact, this is what we refer to as scale inconsistency. Alternatively, if we constrain the feasible region K to only contain the unit-norm Θ’s, then the loss is not geodesically convex on the manifold of the unit sphere. To see this, if we impose a unit sphere constraint, then the OGD algorithm (Zinkevich, 2003) will fail, because a basic requirement for OGD is the underlying problem’s convexity. The OGD will require additional structures such as convexity over the manifold of the unit sphere, which does not hold even for linear functions (Jain et al., 2017). Following the above arguments, one may find a seeming contradiction: the suboptimality loss lsub,t(Θ) = ⟨Θ, (x∗ t − ˆxt)z⊤ t ⟩ seems to be a linear function of Θ at first glance, but why would the optimal Θ to be at an interior point 0 ∈ K while the theory of linear programs tells us that the optimal solution of an LP always lies at the boundary? We need to notice the dependence of ˆxt on Θ since xt is the optimal solution induced by Θzt. Such a dependence breaks the linearity, which means that (x∗ t − ˆxt)z⊤ t is even not the gradient. Should one get the gradient right, they cannot avoid being trapped by the naive prediction Θ = 0 as mentioned before. Finally, we explain the differences between the Θt → 0 phenomena of the above OGD and our MOM approach but under the constraints (5). The cumulative suboptimality loss ∑T t=1 lsub,t(Θ) is actually bounded by ∑T t=1 ∥st∥1 (proved in Lemma B.1). The upper bound relation holds when st sat- isfies either (5) or the original MOM constraints (3). Such a relation has two implications: (i) the margin mechanism avoids MOM algorithms converging to zero; (ii) the perfor- mance guarantee for the MOM objective directly transfers to that of lsub,t(Θ). Concluding remarks. In this paper, we develop a new approach named Maximum Optimality Margin for the prob- lems of predict-then-optimize and inverse LP. The MOM framework leads to new characterizations of the difficulty of the problem through the separability condition of Assump- tion 3.2 and the violation loss l(Dnew; Θ) which appears in Propositions 3.4, 3.5, and 3.6. With the MOM perspec- tive, we derive bounds on correctly predicting the optimal solution, P(x ∗ new = ˆxnew) in Propositions 3.3, 3.6, and 3.7, which is rarely seen in the existing literature even under strong conditions. Without the separability condition of Assumption 3.2, we draw a connection of our MOM ob- jective value with the suboptimality loss lsub in the inverse LP literature and derive performance bounds in terms of lsub. Numerically, we observe the MOM algorithms also perform well under the estimate loss lest. To theoretically quantify the performance of MOM under lest, we conjecture that one needs to impose stronger structures on the dual LPs. Besides, another interesting future direction to pursue is to generalize the algorithms and analyses of MOM to non-linear optimization problems. 9 Maximum Optimality Margin for Contextual and Inverse Linear Programming 10 References Ahuja, R. K. and Orlin, J. B. Inverse optimization. Opera- tions Research, 49(5):771–783, 2001. B¨armann, A., Martin, A., Pokutta, S., and Schneider, O. An online-learning approach to inverse optimization. arXiv preprint arXiv:1810.12997, 2018. Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006. Beigman, E. and Vohra, R. Learning from revealed pref- erence. In Proceedings of the 7th ACM Conference on Electronic Commerce, pp. 36–42, 2006. Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J.- P., and Bach, F. Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33:9508–9519, 2020. Bertsimas, D. and Kallus, N. From predictive to prescriptive analytics. Management Science, 66(3):1025–1044, 2020. Bertsimas, D., Gupta, V., and Paschalidis, I. C. Data- driven estimation in equilibrium using inverse optimiza- tion. Mathematical Programming, 153:595–633, 2015. Besbes, O., Fonseca, Y., and Lobel, I. Contextual inverse optimization: Offline and online learning. Available at SSRN 3863366, 2021. Blondel, M. Structured prediction with projection oracles. Advances in neural information processing systems, 32, 2019. Blondel, M., Martins, A. F., and Niculae, V. Learning with fenchel-young losses. The Journal of Machine Learning Research, 21(1):1314–1382, 2020. Bousquet, O. and Elisseeff, A. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. Chen, V. X. and Kılınc¸-Karzan, F. Online convex optimiza- tion perspective for learning from dynamically revealed preferences. arXiv preprint arXiv:2008.10460, 2020. El Balghiti, O., Elmachtoub, A. N., Grigas, P., and Tewari, A. Generalization bounds in the predict-then-optimize framework. Advances in neural information processing systems, 32, 2019. Elmachtoub, A., Liang, J. C. N., and McNellis, R. Decision trees for decision-making under the predict-then-optimize framework. In International Conference on Machine Learning, pp. 2858–2867. PMLR, 2020. Elmachtoub, A. N. and Grigas, P. Smart “predict, then optimize”. Management Science, 68(1):9–26, 2022. Feldman, V. and Vondrak, J. High probability generaliza- tion bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270–1279. PMLR, 2019. Hazan, E. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157–325, 2016. Ho-Nguyen, N. and Kılınc¸-Karzan, F. Risk guarantees for end-to-end prediction and optimization processes. Man- agement Science, 2022. Hofmann, T., Sch¨olkopf, B., and Smola, A. J. Kernel meth- ods in machine learning. The annals of statistics, 36(3): 1171–1220, 2008. Hu, Y., Kallus, N., and Mao, X. Fast rates for contextual linear optimization. Management Science, 2022. Jain, P., Kar, P., et al. Non-convex optimization for machine learning. Foundations and Trends® in Machine Learning, 10(3-4):142–363, 2017. Liu, H. and Grigas, P. Risk bounds and calibration for a smart predict-then-optimize method. Advances in Neural Information Processing Systems, 34:22083–22094, 2021. Luenberger, D. G., Ye, Y., et al. Linear and nonlinear programming, volume 2. Springer, 1984. Megiddo, N. and Chandrasekaran, R. On the ε-perturbation method for avoiding degeneracy. Operations Research Letters, 8(6):305–308, 1989. Mohajerin Esfahani, P., Shafieezadeh-Abadeh, S., Hana- susanto, G. A., and Kuhn, D. Data-driven inverse op- timization with imperfect information. Mathematical Programming, 167(1):191–234, 2018. Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635–2670, 2010. Tan, Y., Terekhov, D., and Delong, A. Learning linear programs from optimal decisions. Advances in Neural Information Processing Systems, 33:19738–19749, 2020. Wilder, B., Dilkina, B., and Tambe, M. Melding the data- decisions pipeline: Decision-focused learning for combi- natorial optimization. Proceedings of the AAAI Confer- ence on Artificial Intelligence, 33(01):1658–1665, 2019. 10 Maximum Optimality Margin for Contextual and Inverse Linear Programming 11 Zadimoghaddam, M. and Roth, A. Efficiently learning from revealed preference. In International Workshop on Internet and Network Economics, pp. 114–127. Springer, 2012. Zhang, J. and Liu, Z. Calculating some inverse linear pro- gramming problems. Journal of Computational and Ap- plied Mathematics, 72(2):261–273, 1996. Zinkevich, M. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th international conference on machine learning (icml-03), pp. 928–936, 2003. 11 Maximum Optimality Margin for Contextual and Inverse Linear Programming 12 A. Numerical Experiments In this section, we provide the details of the experiment setup and more numerical results. The codes and data can be found on https://github.com/liushangnoname/Maximum-Optimality-Margin. A.1. Basic Setup For the numerical experiments, we compare the performance of our proposed algorithms against several benchmark methods, and we test the performance under both online and offline settings. Specifically, we consider the following benchmark methods where all of them can only be used to solve the predict-then-optimize problem (but not the contextual linear programming) as they all require the observations of ct on the training data. 1. Linear regression models where we consider ordinary least squares (OLS) and ridge regression (RR). Accordingly, the parameter estimation follows the following loss functions: lOLS = 1 2 ∥Θz − c∥ 2 2, lRidge = 1 2 ∥Θz − c∥2 2 + λ 2 ∥Θ∥2 F . 2. Random Forest (RF). We apply RF algorithm with squared error criterion lRF = ∥ˆc − c∥ 2 2. 3. Predict-then-optimize method. We take SPO+ algorithm (Elmachtoub & Grigas, 2022) as an example, where the loss function is as follows lSPO+ = (2Θz − c) ⊤x∗(c) − min Ax=b,x≥0 {(2Θz − c) ⊤x}. Following the previous implementations, we optimize the SPO+ loss under a stochastic gradient descent (SGD) approach and Frobenius regularization. Underlying LP problems. We study two canonical LP problems for the numerical experiments, namely the shortest path (SP) problem and the fractional knapsack (FK) problem, and we present their results in the following two sections A.2 and A.3, respectively. The experiment setup follows that of (Elmachtoub & Grigas, 2022; Ho-Nguyen & Kılınc¸-Karzan, 2022; Besbes et al., 2021). Performance Measurements. We compare different algorithms by the performance measure of relative loss for the offline setting and cumulative Regret in the online setting. Under the offline setting, the relative loss normalizes the estimate loss lest by the optimal value, Rel-LossSP := c ⊤(x − x∗) c⊤x∗ . Specifically, for the shortest path (SP) problem, the optimal value is always non-zero (unless for trivial c) so the loss is well-defined. For the fractional knapsack (FK) problem, due to the random noise, the optimal value may be zero with a positive probability. So we consider another normalization that leads to Rel-LossFK := c ⊤(x∗ − x) ∥c∥2 . Under the online setting, we define cumulative regret as the cumulative sum of the relative loss of each time step. Overview of the results. In Appendix A.2, we demonstrate the algorithm performance under the effect of degree (see Figure 1) and also investigate numerically the issue of scale inconsistency/invariance (see Figure 2). In Appendix A.3, we examine the sample complexity under the offline setting (see Figure 3) and compare the performance of several online algorithms (see Figure 7). In addition, we consider the kernelized version of the MOM formulation and present its performance in Figure 4. 12 Maximum Optimality Margin for Contextual and Inverse Linear Programming 13 A.2. Shortest Path Problem For the shortest path problem, we follow the experiment setup of (Elmachtoub & Grigas, 2022). Consider a 5 × 5 grid network with n = 40 directed edges associated with a cost vector c ∈ Rn. Each edge is either from south to north or west to east, where the goal is to minimize the cost to traverse from the southwest corner to the northeast corner. For each node of the network, there is a flow balance constraint, encoded by Ax = b. The problem can then be formulated as the standard-form LP (1). For the methods of ordinary least square, ridge regression, and our maximum optimality margin, we solve the corresponding optimization problems using cvxpy package with GUROBI solver. For the random forest algorithm, we implement the sklearn.ensemble.RandomForestRegressor package. For the SPO+ algorithm and the online version of our MOM approach, we implement a gradient descent approach. For each simulation trial, the training sets and test sets both consist of N = 1000 sample sizes. All regularization parameters are chosen from 10−6 to 102, and all step sizes are chosen from 10 −3 to 101. All projection radius (the diameter of K) are chosen from 0.8∥B∥F to 2.0∥B∥F . The hyper-parameters are chosen via a validation subset of N/4 samples from the training data. Synthetic Data Generation. For each trial of our experiments, we generate a gound-truth coefficient matrix V ∈ Rn×d independently, where each component of V is drawn from Bernoulli(0.5). Here n = 40 and d = 6. 1. We generate the feature vectors {zt ∈ Rd}. The first d − 1 components are taken independently from a standard Gaussian distribution, and the last component of each zt is set to be 1 (to model the constant in predictors). 2. Then the true cost vectors are generated as follows: ct,j = [ ( 1 √d (V zt)j + 3)deg + 1] · ϵt,j · αt + ¯ηηt,j, where deg is a fixed integer to be chosen to represent the nonlinearity of the problem, ϵt,j represents the intrinsic diverse noise, αt ∈ {1, 1 + ¯α} stands for a scale noise which depends on zt, and ηt,j is the additive noise. ¯ϵ, ¯α, and ¯η are non-negative parameters chosen to control the power of each noise. Since the distribution of αt depends on feature zt, it henceforth can be considered as a type of attack that changes the scale of ct. More specifically, ϵt,j ∼ Unif[1 − ¯ϵ, 1 + ¯ϵ], αt = { 1 + ¯α, if zt,1 > 0.5 1, other cases, 2ηt,j + 1 ∼ Exponential(1). Results. Using the above data generation process, we test the algorithms Random Forest (RF), Ordinary Least Squares (OLS), Ridge Regression (Ridge), SPO+, Maximum Optimality Margin (MOM), and MOM-OGD over 30 different trials. We test the effect of deg (see Figure 1) and ¯α (see Figure 2) in these problems, where other parameters are tested later in Fractional Knapsack problem. The first experiment is to test the model misspecification error of different algorithms, where the deg parameter varies among {1, 2, 4, 6}. There is no noise at all (i.e. ¯ϵ = ¯α = ¯η = 0) so as to emphasize the misspecification effect. The results can be found in Figure 1. Another experiment is to study the effect of scale noise. As is shown in Appendix C.2, if the scale noise is independent of the feature vector, the consistency of the linear regressor still holds. So we design a scale noise that is correlated with the feature vector, which is the αt in our setting. To underline the effects of scale noise, we set deg = 1 and ¯ϵ = ¯η = 0 in our experiments. The result is shown in Figure 3. Analysis. From the result of the first experiment (see Figure 1), we can see that for small degrees deg ∈ {1, 2} where the linear model estimation is not a heavy mistake, our algorithm MOM, SPO+ of (Elmachtoub & Grigas, 2022), and linear regression type methods behave similarly, where linear methods perform the best if the model is exactly linear while MOM slightly dominates other methods in the deg = 2 case. The Random Forest algorithm performs not so well in those cases probably due to the fact that it is a non-parametric method. For high degrees deg ∈ {4, 6}, the model misspecification error 13 Maximum Optimality Margin for Contextual and Inverse Linear Programming 14 R F O L S R i d g e S P O + M O M M O M - O G D 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200Relative Loss Attack Power 0.1 R F O L S R i d g e S P O + M O M M O M - O G D 0.00 0.05 0.10 0.15 0.20 Attack Power 1.0 R F O L S R i d g e S P O + M O M M O M - O G D 0.00 0.05 0.10 0.15 0.20 0.25 Attack Power 3.0 Random Forest Ordinary Least Squares Ridge Regression SPO+ Maximum Optimality Margin MOM-Online Gradient Descent Figure 2. Experiment 2: Rel-LossSP under different ¯α’s, 30 trials. Here ¯α represents the power of the scale noise, denoted by Attack Power. becomes rather severe. The performance of linear methods falls drastically, while the performance of MOM dominates other algorithms. The good performance of the MOM approach could possibly be explained in the following way: polynomial function with a rather high degree magnifies the components of cost vectors that are larger than 1, where those algorithms which directly predict cost vectors place equal weights on all components and their focus are dragged to those unusually large components. But for the Shortest Path problem, improving prediction accuracy on those large components hardly benefits the performance of the algorithm, since the optimal solution will avoid those edges with huge costs. Instead, the prediction accuracy on those moderate and small components is worsened by focusing on the large components, which holds back the performance of RF, OLS, and Ridge. The MOM approach behaves alternatively: for those large components of cost vectors, the estimator does not place any further concern if their corresponding reduced costs are large than 1, so the MOM estimator outperforms other methods. Note that the SPO+ method we apply here is a stochastic gradient descent version, which empirically converges with batch size 5 in 2000 steps. Our online gradient descent version of our MOM approach can also be viewed as a stochastic gradient descent implementation of our MOM approach with batch size 1 and 1000 time steps. The performance gap between the offline MOM algorithm and the MOM-OGD algorithm can be partly explained by the small batch size and the lack of iterations. The second experiment indicates the effects of feature-dependent scale noise, which can be found in Figure 2. Note that the scale noise is only added to data when the first component of the feature is larger than 0.5. As a typical ensemble method, RF performs quite steadily due to the nature of tree estimators. That is not the case for linear regressors, since their estimation of true coefficients of the first feature component will be heavily disturbed. For the other three methods MOM, SPO+, and MOM-OGD, their performances are quite stable, thanks to the fact that they are only concerned with the optimal solutions of linear programs that remain unchanged under scale noises. A.3. Fractional Knapsack Problem For this experiment, we follow the problem setup of (Ho-Nguyen & Kılınc¸-Karzan, 2022). Specifically, each decision maker is presented with a bunch of items, and their aim is to maximize the utility (or equivalently, to minimize the cost) under a knapsack constraint. Different from the discrete case, the fractional knapsack problem allows the decision maker to select a fraction of some certain item, making it a linear program. Mathematically, the decision-maker solves the following LP min x,s1,s2 c⊤x, s.t. p⊤x + s1 = B, x + s2 = 1, x, s1, s2 ≥ 0. 14 Maximum Optimality Margin for Contextual and Inverse Linear Programming 15 Here p ∈ Rn can be interpreted as the price vector and B ∈ R is the budget of the decision maker. The corresponding utility vector should be interpreted as the opposite of the objective vector c since the LP considers a minimization problem. The slack variables s1 and s2 are introduced simply to convert the problem into a standard form. As for the offline learning problems, the algorithms we implement are exactly the same as we do in A.2. We also apply the online setting here with three algorithms: Perceptron (as is illustrated in Algorithm 3), Maximum Optimality Margin with Follow the Regularized Leader (as is illustrated in Algorithm 4), and Maximum Optimality Margin with Online Gradient Descent (as is illustrated in Algorithm 2). Synthetic Data Generation. At the beginning of every trial, we generate a ground-truth coefficient matrix V ∈ Rn×d, where each component is a Bernoulli random variable with an expectation of 0.5. Here n = 10 and d = 5. We then generate the price vector, where each component is selected to be a random integer between 1 and 1000 in an offline setting. To ease the pain of large constants, we re-scale the price vector in an online setting, where each component is uniformly distributed between 0 and 1. We generate two auxiliary variables low and high, where low = maxj pj and high = 1⊤p − u · low, and u ∼ Unif[0, 1]. Then the budget is chosen as Unif[low, high]. 1. We generate the feature vectors {zt ∈ Rd}. The first d − 1 components are taken independently from a Unif[0, 1] distribution and the last component of each zt is set to be 1 (to model the constant in predictors). 2. Then the true cost vectors are generated as follows: ct,j = (V zt) deg j · ϵt,j · αt + ¯ηηt,j, where deg is a fixed integer to be chosen to represent the nonlinearity of the problem, ϵt,j represents the intrinsic diverse noise, αt ∈ {1, 1 + ¯α} stands for a scale noise which depends on zt, and ηt,j is the additive noise. ¯ϵ, ¯α, and ¯η are non-negative parameters chosen to control the power of each noise. Since the distribution of αt depends on feature zt, it henceforth can be considered as a type of attack that changes the scale of ct. More specifically, ϵt,j ∼ Unif[1 − ¯ϵ, 1 + ¯ϵ], αt = { 1 + ¯α, if zt,1 > 0.5 1, other cases, 2ηt,j + 1 ∼ Exponential(1). Results. Equipped with such a data generation process, we apply several numerical experiments in both offline and online settings. We first test the sample complexity under offline setting, with deg = 1, ¯ϵ = 0.1, ¯η = 1.0, and ¯α = 0.0. We apply 30 trials. For training sets, we generate different T = 100, 200, 500, 1000, 5000, and test the performance against 1000 test samples. The algorithms we choose contain Random Forest (RF), Ordinary Least Squares (OLS), Ridge Regression (Ridge), SPO+ of (Elmachtoub & Grigas, 2022), our Maximum Optimality Margin Algorithm 1 (MOM), and our MOM-OGD Algorithm 2. All regularizing constants are chosen from 10−6 to 102. All step sizes are chosen from 10−3 to 101. All projection radii are chosen from 0.8∥B∥F to 2.0∥B∥F . The hyper-parameters are decided via a validation set of T /4 samples and the criterion of averaged Relative Loss. The results can be found in Figure 3. Sample complexity. Kernelization and non-linear MOM. We then develop some kernelized versions of our MOM approach, resulting in three different SVM-based algorithms: Linear MOM (Linear), Polynomial Kernelized MOM (PolyKer), Radial Basis Function Kernelized MOM (RbfKer). For those kernelized methods, the original feature zt0 is replaced by an T -dimensional feature (κ(zt0 , zt))T t=1, where {zt}T t=1 is the training set of features and κ is some pre-chosen kernel function. For the polynomial kernel, we utilize κγ,d0(zi, zj) = (z⊤ i zjγ−1 + 1)d0 . For the radial basis function kernel, we define κγ(zi, zj) = exp(− ∥zi−zj ∥2 γ ). The degree of the polynomial kernelized MOM is chosen from {1, 2, 3, 4} and the scale parameters γ’s of both kernelized MOM methods are chosen from {0.1, 0.5, 1, 2, 3, 4, 5}. All regularizing constants are chosen from 10−6 to 102. The training data size is now reduced to N = 500. The hyper-parameters are decided via a validation set of T /4 samples and the criterion of averaged Relative Loss. We paint the boxplots of 30 independent trials. The results can be found in Figure 4. 15 Maximum Optimality Margin for Contextual and Inverse Linear Programming 16 100 200 500 1000 2000 Number of Training Samples 0.2 0.4 0.6 0.8 1.0 1.2Relative LossRF OLS Ridge SPO+ MOM MOM-OGD Figure 3. Sample complexity of different algorithms. L i n e a r P o l y K e r R b f K e r 0.0 0.1 0.2 0.3Relative Loss Degree 1 L i n e a r P o l y K e r R b f K e r 0.00 0.05 0.10 0.15 0.20 0.25 Degree 2 L i n e a r P o l y K e r R b f K e r 0.00 0.05 0.10 0.15 0.20 Degree 4 L i n e a r P o l y K e r R b f K e r 0.00 0.05 0.10 0.15 0.20 0.25 Degree 6 Linear Polynomial Kernelized Radial Basis Function Kernelized Figure 4. Experiment 4: Rel-LossFK under different data generation degrees, 30 trials. The fourth experiment is about the kernelized versions of our MOM approach, where three MOM algorithms named Linear, Polynomial Kernelized, and Radial Basis Function Kernelized are implemented (see Figure 4). The performance of linear MOM is worsened as the degree of the model grows since the model misspecification becomes more severe. But kernelized MOM methods remains good performance under high degrees. Note that the performance of RBF Kernelized MOM’s behavior is unsatisfying when the model is perfectly linear. Those kernelized versions of our algorithms reveal the potential of generalizing our MOM principle to other cases apart from just the linear model. Analysis. The third experiment we adopt is examining the sample complexity of the aforementioned algorithms under a noisy environment (see Figure 3). The performance of linear regressors exceeds other algorithms since there is no model misspecification at all. The performance of the Random Forest algorithm is similar to that of the offline Maximum Optimality Margin algorithm, which is slightly worse than that of linear regressors. As for the SPO+ method, their averaged performance is undermined by the noise to a fairly large extent, compared with its performance under a noiseless environment in Experiment 1 and Experiment 2 (see Figure 1 and Figure 2). The difference in the performance between SPO+ and MOM can be partially explained by the different ways of dealing with noisy data. SPO+ directly put the optimal solution 16 Maximum Optimality Margin for Contextual and Inverse Linear Programming 17 of the noisy data to the gradient, while the optimal solution could probably vary exaggeratedly due to some noise to the cost vector. MOM acts more steadily: for those noisy cost vectors, their optimal solutions could be far from the noiseless cost vectors, but the change with respect to optimal basis will usually be only a few components which only affects a few corresponding lines in estimated Θ. Finally, we note that the small batch size and the lack of iteration could be blamed for the poor performance of MOM-OGD in noisy data since the noisy data exacerbates the variance. To complete the argument, we provide some extra numerical experiments to compare our kernelized methods with other kernelized methods such as kernelized ridge regression. The first extra experiment adopts the same setting as that in Figure 4. We keep the same setup (except for only 10 independent trials instead of 30 trials to save the time cost) again for 6 benchmark algorithms: Random Forest (RF), Ordinary Least Squares (OLS), Ridge Regression (Ridge), Polynomial Kernelized Ridge Regression (PolyRidge), Rbf Kernelized Ridge Regression (RbfRidge), and SPO+, and 3 of our MOM algorithms: Maximum Optimality Margin (MOM), Polynomial Kernelized MOM (PolyMOM), and Rbf Kernelized MOM (RbfMOM). The results can be found in Figure 5. R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0.000 0.025 0.050 0.075 0.100Relative Loss Degree 1 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0.00 0.05 0.10 0.15 0.20 Degree 2 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0.00 0.05 0.10 0.15 0.20Relative Loss Degree 4 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0.0 0.1 0.2 0.3 0.4 Degree 6 Random Forest Ordinary Least Squares Ridge Regression Polynomial Kernelized Ridge Rbf Kernelized Ridge SPO+ Maximum Optimality Margin Polynomial Kernelized MOM Rbf Kernelized MOM Figure 5. An extra comparison of the relative loss for kernelized MOMs and benchmark algorithms for the Fractional Knapsack problem. We also provide another result under a similar setting for the Shortest Path problem. The training sample size is now reduced to 200, and the number of independent trials is also reduced to 10 to ease the computational price. To emphasize the scale consistency of our MOM approach, we apply the same noise setup as the last sub-figure of Figure 2. The results are summarized in Figure 6. 17 Maximum Optimality Margin for Contextual and Inverse Linear Programming 18 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0.0 0.1 0.2 0.3 0.4 0.5Relative Loss Degree 1 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0.0 0.5 1.0 1.5 2.0 2.5 Degree 2 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0 5 10 15 20Relative Loss Degree 4 R F O L S R i d g e P o l y R i d g e R b f R i d g e S P O + M O M P o l y M O M R b f M O M 0 25 50 75 100 125 Degree 6 Random Forest Ordinary Least Squares Ridge Regression Polynomial Kernelized Ridge Rbf Kernelized Ridge SPO+ Maximum Optimality Margin Polynomial Kernelized MOM Rbf Kernelized MOM Figure 6. Relative loss for kernelized MOMs and benchmark algorithms for the Shortest Path problem. A.4. Online Setting Next we analyze the online setting for the MOM algorithms. We compare the performance of the OGD version of MOM (Algorithm 2), the perceptron version of MOM (Algorithm 3), and also a follow-the-regularized-leader (FTRL) version of the MOM Algorithm 1 as follows. Basically, Algorithm 4 solves the MOM optimization formulation 3 at each time t and use the estimated parameter to predict the optimal solution of the next time step. Algorithm 4 gives another interpretation of the MOM formulation. Under an online setting, if we solve the MOM optimization problem (3) at each time, this is equivalent to a follow-the-regularized online algorithm to solve the problem. The theoretical analysis of Algorithm 4 can be extended from Proposition 3.3 and Proposition 3.4. Figure 7 presents the cumulative regret of several algorithms for a fractional knapsack problem. In this numerical experiment, we let the objective vector c = Θ∗z almost surely for some fixed Θ ∗ so as to achieve the separability condition. We emphasize that it only ensures the existence of a parameter Θ ∗ to meet the optimality condition (2) but not with a margin of 1 as Assumption 3.2. From the plot, we observe that Algorithm 3 has the worst performance though it features for the best theoretical dependency on T . We remark that this regret curve does not contradict the bounded number of mistakes in Proposition 3.7; we find that when we increase the horizon to T = 100, 000, the regret curve of Algorithm 3 becomes flattened. For Algorithm 2 and Algorithm 4, both achieve significantly better performance. Comparatively, 4 performs better than Algorithm 2, with the price of more computation cost (to solve a quadratic program at each time period). Importantly, we also implement the online algorithm of (B¨armann et al., 2018; Chen & Kılınc¸-Karzan, 2020) under the same setting, and it incurs a regret linearly increasing with T ; so we do not include its regret curve as it will clap all the 18 Maximum Optimality Margin for Contextual and Inverse Linear Programming 19 Algorithm 4 Follow-the-Regularized-Leader MOM Input: Dataset D(T ) = {(x∗ t , At, bt, zt, B∗ t , N ∗ t )}T t=1, the set K 1: Initialize Θ1 = 0 ∈ Rn×d 2: for t = 1, ..., T do 3: Predict the objective by ˆct = Θtzt 4: Solve the following LP and denote its optimal solution by xt min ˆc ⊤ t x, s.t. Atx = bt, x ≥ 0. 5: Solve the optimization problem (3) with {(x∗ s, As, bs, zs, B∗ s , N ∗ s )} t s=1, λ = 1/ √t and K 6: Let Θt+1 be the optimal solution 7: end for Output: {xt}T t=1 0 200 400 600 800 1000 Time Step 0 20 40 60 80 100 120Averaged Regret Alg3: Perceptron Alg1/4: MOM-FTRL Alg2: MOM- OGD Figure 7. Cumulative regret curve (averaged over 30 trials) for online algorithms. regret curves of Figure 7 to x-axis. A closer investigation shows that the online algorithm of (B¨armann et al., 2018; Chen & Kılınc¸-Karzan, 2020) will render the estimate Θt → 0, as noted by the scale inconsistency issue in earlier sections. In contrast, our margin-based formulation plays an important rule to ensure that a small optimality condition violation is caused by discovering the correct mapping from z to c but not by the scale of the predicted ˆc. B. Proofs B.1. Proof of Lemma 2.2 Here we show a stronger version of Lemma 2.2 as follows. Lemma B.1. Consider an LP of the standard form (1). For any feasible basis B ⊂ [n] satisfying |B| = m and its complement N = [n]\\B, denote x = (x1, ..., xn)⊤ and r = (r1, ..., rn) ⊤ as the solution and reduced cost vector corresponding the basis B, respectively, i.e., xB = A−1 B b, xN = 0, r = c − A⊤(A−1 B )⊤cB. Denote x∗ = (x∗ 1, ..., x∗ n) ⊤ as one optimal solution of LP (1). Then, we have that rB = 0, and c⊤x − c ⊤x∗ ≤ max i∈[n] x∗ i · ∑ i∈N (−ri)+. (10) Furthermore, (10) implies that x is an optimal solution if r ≥ 0. 19 Maximum Optimality Margin for Contextual and Inverse Linear Programming 20 Proof. First, it is easy to verify that rB = 0. Then, we only need to show that the inequality (10) holds. For any feasible solution x′ = (x′ 1, ..., x′ n) ≥ 0 satisfying Ax′ = b, we have x′ B = A−1 B b − A−1 B AN x′ N . It implies c ⊤x′ = c⊤ B A−1 B b − c ⊤ B A−1 B AN x′ N + c ⊤ N x′ N = c⊤ B A−1 B b + r⊤ N x ′ N . (11) Next, from (11), we have c⊤x − c ⊤x′ = −r⊤ N x′ N ≤ ∑ i∈N x′ i(−ri)+ ≤ max i∈[n] x′ i · ∑ i∈N (−ri)+, where the first line comes from equality (11) directly, and the last two lines come from the non-negativity of x′ and (ri)+ for all i. Note that the above inequality holds for any feasible solution x ′; by plugging in x′ = x∗, we obtain (10). B.2. Proof of Proposition 3.4 We first show Proposition 3.4 and then utilize the result for the proof of 3.3. The key is to utilize the algorithm stability analysis where we cite the result from (Shalev-Shwartz et al., 2010) as the following proposition. We refer to (Bousquet & Elisseeff, 2002; Shalev-Shwartz et al., 2010; Feldman & Vondrak, 2019) for more related analysis such as the high probability bound. Theorem B.2 (Theorem 3 in (Shalev-Shwartz et al., 2010)). Let f : H × Z → R be such that H is bounded by B and f (h, z) is convex and L-Lipschitz with respect to h. Let , z0, z1, ..., zT be i.i.d. samples and let ˆhλ = arg min h∈H ( T∑ t=1 f (h, zt) + λ 2 ∥h∥ 2 2 ) . Then, we have E [ f (ˆhλ, z0)] ≤ inf h∈H E [f (h, z0)] + λ 2 B2 + 4(L + λB) 2 λT . Proof. We refer to Theorem 2 and Theorem 3 in (Shalev-Shwartz et al., 2010). Now we show Proposition 3.4 with Theorem B.2. Proof. Recall that Dnew = (c, A, b, z, B∗, N ∗) denotes a new sample from P, and the loss function l(Dnew; Θ) := ∥s∥1 where s := (1|N ∗| − ˆc⊤ N ∗ − ˆc ⊤ B∗ A−1 B∗ AN ∗ )+. Here, (·)+ denotes the entry-wise positive part function, and B∗ and N ∗ are defined as in Section 2. Here, with a slight abuse of notation, we drop the sample tuple Dnew and let l(Θ) = l (Dnew; Θ). Then, under Assumption 3.1, l(Θ) is (2 + √m¯σ)-Lipschitz with respect to Θ in the Frobenius norm for a fixed Dnew. To see this, for any two parameters Θ = (Θ1, ..., Θn) ⊤, ˆΘ = ( ˆΘ1, ..., ˆΘn) ⊤ ∈ Rn×d |l(Θ) − l(Θ ′)| = ∣ ∣ ∣ ∣ ∣ ∑ i∈N ((1 − Θ⊤ i z + A⊤ i (A−1 B∗ )⊤ΘB∗ z)+ − (1 − ˆΘ⊤ i z + A⊤ i (A−1 B∗ ) ⊤ ˆΘB∗ z)+)∣ ∣ ∣ ∣ ∣ 20 Maximum Optimality Margin for Contextual and Inverse Linear Programming 21 ≤ ∑ i∈N ∣ ∣ ∣(1 − Θ⊤ i z + A⊤ i (A−1 B∗ )⊤ΘB∗ z)+ − (1 − ˆΘ⊤ i z + A⊤ i (A−1 B∗ ) ⊤ ˆΘB∗ z)+∣ ∣ ∣ ≤ ∑ i∈N ∣ ∣ ∣−Θ ⊤ i z + A⊤ i (A−1 B∗ ) ⊤ΘB∗ z + ˆΘ⊤ i z − A⊤ i (A−1 B∗ ) ⊤ ˆΘB∗ z∣ ∣ ∣ ≤ ∑ i∈N (∣ ∣ ∣(Θi − ˆΘi) ⊤z∣ ∣ ∣ + ∣ ∣ ∣A⊤ i (A−1 B∗ ) ⊤(ΘB∗ − ˆΘB∗ )z∣ ∣ ∣) ≤ ∑ i∈N (∥ ∥ ∥Θi − ˆΘi∥ ∥ ∥ 2 ∥z∥2 + σmax ((A−1 B∗ )⊤(ΘB∗ − ˆΘB∗ ) ) ∥Ai∥2 ∥z∥2) ≤ 2∥Θ − ˆΘ∥F + √mσmax ((A−1 B∗ ) ⊤) σmax (ΘB∗ − ˆΘB∗ ) ≤ (2 + √m¯σ) ∥ ∥ ∥Θ − ˆΘ∥ ∥ ∥ F . Here the first line comes from the definition of l(·), the second and fourth lines are obtained by the convexity of the absolute value function, the third line comes from a direct computation of the positive part function, the fifth line is obtained by Cauchy’s inequality and the definition of σmax, the sixth line comes from the inequality that σmax(XY ) ≤ σmax(X)σmax(Y ) for any two matrices X, Y , and the last line comes from Assumption 3.1 and the inequality σmax(X) ≤ ∥X∥F for any matrix X. By Theorem B.2 and Assumption 3.1, we have E[l( ˆΘ)] ≤ min Θ∈K E[l(Θ)] + λ 2 ¯Θ 2 + 4(2 + √m¯σ + λ ¯Θ) 2 λT , (12) where ˆΘ = arg min Θ∈K ( T∑ t=1 l(Dnew, Θ) + λ 2 ∥Θ∥F ) . Finally, plugging λ = 1√T into (12), we have E[l( ˆΘ)] ≤ min Θ∈K E[l(Θ)] + 28 + 8 ¯Θ2 + 7m¯σ2 √T . B.3. Proof of Proposition 3.3 Proof. Under Assumption 3.2, there exists a Θ∗ ∈ K such that the following inequality holds almost surely Θ∗ N ∗ z − A⊤ N ∗ (A−1 B∗ ) ⊤Θ∗ B∗ z ≥ 1. Thus we have min Θ∈K E[l(Θ)] = 0, where l(Θ) is defined following the previous proof of Proposition 3.4. Then, from Proposition 3.4, E[l( ˆΘ)] ≤ 28 + 8 ¯Θ2 + 7m¯σ2 √T . (13) For a new sample (cnew, Anew, bnew, znew) with ˆcnew = ˆΘznew, we can utilize Lemma B.1 and bound the suboptimality loss as follows E [ˆc ⊤ newx∗ new − ˆc ⊤ new ˆxnew] ≤ E  max i∈[n](ˆxnew)i · ∑ i∈N ∗ new(−ri)+   21 Maximum Optimality Margin for Contextual and Inverse Linear Programming 22 ≤ E   ∑ i∈N ∗ new(−ri)+   ≤ E   ∑ i∈N ∗ new(1 − ri)+   ≤ 28 + 8 ¯Θ2 + 7m¯σ2 √T . (14) Here the first line comes from Lemma B.1, the second line comes from Assumption 3.1, the third line comes from the monotonicity of the positive part function, and the last line comes from (13). Now, we note that if ˆΘ renders any non-basic variables in N ∗ as basic variables, i.e., ˆc ⊤ new,N ∗ new − ˆc ⊤ new,B∗ new A−1 new,B∗ new Anew,N ∗ new ≥ 0 does not hold, we have l(Dnew; ˆΘ) ≥ 1. Thus, by applying Markov’s inequality to , we have that with probability no less than 1 − 28+8 ¯Θ 2+7m¯σ2 √T , ˆΘ can identify both the true optimal basis and the true optimal solution correctly. B.4. Proof of Proposition 3.6 The proof in this part is basically an application of the following lemma. Lemma B.3 (Theorem 3.1 in (Hazan, 2016)). Let {ft(x)} T t=1 be a sequence of convex functions defined on {x : ∥x∥2 ≤ K}. Suppose ∥∇ft(x)∥2 ≤ G for all x such that ∥x∥2 ≤ K and all t = 1, ..., T . Let xt+1 = xt − 2K G√t ∇ft(xt). Then, the following inequality holds T∑ t=1 ft(xt) − min x:∥x∥2≤K T∑ t=1 ft(x) ≤ 3KG √T . Proof. We refer to Theorem 3.1 in (Hazan, 2016). Next, we show Proposition 3.6. Proof. Recall the definition of lt(Dt, Θ) as follows l(Dt; Θ) = ∥ ∥ ∥(1|N ∗ t | − ˆc ⊤ t,N ∗ t − ˆc⊤ t,B∗ t A−1 t,B∗ t At,N ∗ t )+∥ ∥ ∥ 1 , where ˆc = Θzt, and (·)+ denotes the entry-wise positive part function. For the sake of simplicity, we use lt(Θ) to denote l(Dt; Θ). By calculating the derivative of lt(Θ), we have ∇ΘNt lt(Θ) = −gtz⊤ t , ∇ΘBt lt(Θ) = (A−1 t,Bt) ⊤At,Ntgtz⊤. (15) Here, gt ∈ Rn−m is defined as follows. For the i-th element in Nt for i = 1, ..., m, correspondingly, we define the i-th element of gt as gt,i =   1, if ((eNt − ΘNtzt + A⊤ t,Nt(A−1 t,Bt)⊤ΘBtzt) + ) i > 0 0, otherwise. Then, we have ∥∇lt(Θ)∥F ≤ ∥gtz⊤ t ∥F + ∥(A−1 t,Bt) ⊤At,Ntgtz⊤∥F ≤ ∥gt∥2∥zt∥2 + ∥A−1 t,Bt∥F ∥At,Nt∥F ∥gt∥2∥zt∥2 ≤ √n + ∥A−1 t,Bt∥F ∥At,Nt∥F · √n 22 Maximum Optimality Margin for Contextual and Inverse Linear Programming 23 ≤ √n + ¯σmn, where the first line comes from the equalities in (15) and the triangle inequality inequality ∥A + B∥F ≤ ∥A∥F + ∥B∥F for any two matrices A, B with the same size, the second line comes from the inequality ∥AB∥F ≤ ∥A∥F ∥B∥F for any two matrices A, B and the fact that ∥g∥F = ∥g∥2 for any vector g, the third line comes from the definition of gt and Assumption 3.1 that ∥zt∥2 ≤ 1, and the last line comes from Assumption 3.1 that all entries of At are in [−1, 1] and the inequality ∥A∥F ≤ √mσmax(A) for any matrix A ∈ Rm×m. Next, by Lemma B.3, with η = 2 ¯Θ ( √n+¯σmn)√T , T∑ t=1 lt(Θt) − min Θ:∥Θ∥F ≤ ˆΘ T∑ t=1 lt(Θ) ≤ (3 ¯Θ√n + 3¯σ ¯Θmn) √T . (16) Then, we show inequality (8) by 1 T E[z⊤ t Θ ⊤ t (x∗ t − xt)] ≤ E [ T∑ t=1 lt(Θt) ] ≤ E [ min Θ:∥Θ∥F ≤ ˆΘ T∑ t=1 lt(Θ) ] + 3 ¯Θ√n + 3¯σ ¯Θ · mn √T . Here, we can obtain the first line by Lemma B.1 and a similar proof as in the proof of Proposition 3.3, and obtain the second line by inequality (16). Furthermore, under Assumption 3.2, we have with probability 1, min Θ:∥Θ∥F ≤ ˆΘ T∑ t=1 lt(Θ) = 0, which implies E [ T∑ t=1 lt(Θt) ] ≤ 3 ¯Θ√n + 3¯σ ¯Θ · mn √T . (17) Recall that ˆΘ denotes the matrix sampled uniformly from {Θt}T t=1, x ∗ new denotes the optimal solution of LP(cnew, Anew, bnew), ˆxnew denotes the optimal solution of LP(ˆcnew, Anew, bnew), and ˆcnew = ˆΘznew. Similar to the last paragraph of the proof of Proposition 3.3, we have if ˆΘ misclassifies any non-basic variable in Nnew, we have l(Dnew; ˆΘ) ≥ 1. Thus, E[P(x∗ new ̸= ˆxnew)] ≤ E[P(l(Dnew; ˆΘ) ≥ 1)] ≤ E[l(Dnew; ˆΘ)] = 1 T + 1 E [T +1∑ t=1 l(Dnew; Θt) ] = 1 T + 1 E [T +1∑ t=1 l(Dt; Θt) ] ≤ 3 ¯Θ √n + 3¯σ ¯Θ · mn √T + 1 , by which we have shown that, with probability no less than 1 − 3 ¯Θ √n+3¯σ ¯Θ·mn√T +1 , Algorithm 2 can identify both the true optimal basis and the true optimal solution correctly. Here, the first line comes from the fact that l(Dnew; ˆΘ) ≥ 1 if x∗ new ̸= ˆxnew, the second line comes from Markov’s inequality, the third line comes from the definition of ˆΘ, the forth line comes from the fact that Dt and Dnew are two i.i.d. samples that are also independent of Θt, and the last line comes from inequality (17). 23 Maximum Optimality Margin for Contextual and Inverse Linear Programming 24 B.5. Proof of Proposition 3.7 The proof follows the standard analysis of the perceptron method. Proof. In this part, we denote Θt,i as the value of matrix Θtmp at the beginning on the t-th iteration of the outer loop and the i-th iteration of the inner loop for t ∈ [T ] and i ∈ [n]. Also, we view Θt,n+1 and Θt+1,1 as the same to avoid undefined boundary cases. Recall the definition of the reduced cost vector rt(Θ) = Θzt − A⊤ t (A−1 t,Bt) ⊤ΘBtzt, for t = 1, ..., T , which is a linear function of Θ entry-wisely. Thus, we have that for each t = 1, ..., T and i ∈ [n], there exists a matrix Wt,i ∈ Rn∗d such that r(i) t,i (Θ) = Trace(W ⊤ t,iΘ) and ∥Wt,i∥F ≤ 1 + ¯σ√mn, where Trace(W ) = n∑ i=1 wii for any square matrix W = (wij) n i,j=1 ∈ Rn×n. Then, we define ht,i(Θ) = sign(Trace(W ⊤ t,iΘ) − .5), where sign(·) denotes the sign function. Moreover, if there is an i ∈ Nt at some time t such that ht,i(Θt,i) = −1, we have Θt,i+1 = Θt,i + Wt,i. The updating rule in Algorithm 3 is obtained as mentioned above. Specifically, as in Algorithm 3, for any i ∈ Nt and t ∈ [T ], Wt,i is defined as follows (Wt,i)i = z⊤ t , (Wt,i)B∗ t = A−1 B∗ t Atzt, and all other entries are 0. Then, we have ∥Wt,i∥ 2 F = ∥zt∥ 2 F + ∥A−1 B∗ t Atzt∥2 F ≤ ∥zt∥ 2 F + ∥AB∗ t ∥2 F ∥At∥ 2 F ∥zt∥ 2 F , (18) ≤ 1 + ¯σ2m 2n where the first line comes directly from the definition of Wt,i, the second line comes from the inequality that ∥AB∥F ≤ ∥A∥F ∥B∥F for any two matrices A, B, and the last line comes from Assumption 3.1. We say that Algorithm 3 misclassifies one non-basic variable i ∈ Nt if ht,i(Θt,i) ≤ 0, and misclassifies one basic variable i ∈ Bt if ht,i(ΘT ) > 0. Since the values of entries of the reduced cost vector corresponding to the true basis Bt are 0 for all t and i, algorithm 2 makes a mistake only if it misclassifies one non-basic variable. Denote the number of identification mistakes at the t-th iteration as Kt for t = 1, ..., T . Let K = T∑ t=1 Kt be the number of all mistakes. From the updating rule, inequality (18) and the triangle inequality of the Frobenius norm, we have ∥Θt,i+1∥2 F ≤ Kt(1 + ¯σ2m 2n), and then, ∥ΘT +1∥ 2 F ≤ K(1 + ¯σ2m 2n). (19) Moreover, under Assumption 3.2, there exists a matrix Θ ∗ such that Trace(W ⊤ t,iΘ∗) ≥ 1, if i ∈ Nt, Trace(W ⊤ t,iΘ∗) ≤ 0, if i ∈ Bt, for all t = 1, ..., T . Then, we have once a mistake is made for some i ∈ Nt at time t Trace((Θt,i+1 − Θt,i)⊤Θ∗) = Trace(W ⊤ t,iΘ∗) ≥ 1, which implies trace(Θ⊤ T,n+1Θ∗) ≤ K. (20) Then, combining inequalities (19) and (20), we have K ≤ trace(Θ⊤ T,n+1Θ∗) 24 Maximum Optimality Margin for Contextual and Inverse Linear Programming 25 ≤ ¯Θ∥ΘT,n+1∥F ≤ ¯Θ√K(1 + ¯σ2m2n), where the first line comes from (20), the second line comes from Assumption 3.2 that ∥Θ ∗∥F ≤ ¯Θ and Cauchy inequality, and the last line comes from (19). Dividing each side by √K and taking square, K ≤ ¯Θ2 + ¯σ2 ¯Θ2m 2n. Moreover, Lemma 2.2 tells that one can recover the optimal solution if the optimal basis is identified. This statement implies that K is an upper bound of times that we cannot identify the true optimal solutions by Algorithm 3. Thus, we have |{t ∈ [T ] : x∗ t ̸= xt}| ≤ ¯Θ2 + ¯σ2 ¯Θ 2m 2n. For the generalization bound, we apply the symmetry of samples, follow similar steps in the last paragraph of the proof of Proposition 3.6 and have E(P(x∗ new ̸= ˆxnew)) ≤ ¯Θ2 + ¯σ2 ¯Θ 2m 2n T . C. Additional Discussions C.1. Why structured prediction does not work One might wonder if our Maximum Optimality Margin approach can be directly solved as a structured classification problem by using structured SVM classifier. In this section, we will argue that the classical ways of structured SVM without any surrogate loss functions are computationally intractable. The goal of such structured SVM classifier to estimate the optimal basis (or equivalently, the non-basic variables) by observing {(z, A)} and a predictor trained on {(zt, At)}’s and their corresponding labels {yt}’s, where we define (yt)i = +1 for i ∈ Nt and (yt)i = −1 for i ∈ Bt. The Maximum Optimality Margin approach is a principle to maximize the estimated reduced cost vectors for non-basic variables. To express this principle more explicitly, the margin term one wants to maximize in structured SVM can be written as max Θ∈K ∑ j∈N ˆrj, where ˆrj’s are to be specified later. For the simplicity of notations, we define some auxiliary vectors and matrices named as 1, JBt, JNt , and Φt,yt, where 1 ∈ Rn−m, 1j = 1, ∀j ∈ [n − m], JBtv = vBt, ∀v ∈ Rn, JNtv = vNt, ∀v ∈ Rn, Φt,yt := JNt − A⊤ t,NtA−⊤ t,BtJBt. Specifically, the estimated reduced cost with respect to yt can be written as ˆrt,yt = Φt,ytΘzt. Hence the margin term (where the subscript t is sometimes omitted for simplicity) is ∑ j∈N ˆrj = 1⊤ˆr = ⟨Θ, Φ⊤ y 1z⊤⟩, which implies the feature map ϕ((zt, At), yt) = Φ⊤ t,yt1z⊤ t . 25 Maximum Optimality Margin for Contextual and Inverse Linear Programming 26 Note that the corresponding label space Y = {y ∈ {−1, +1}n, #{t, yt = +1} = m} is exponentially large in n in general cases. We also define some measurement of difference ∆(·, ·) ∈ Y × Y → R, where ∆(y, y′) ≥ 0 and ∆(y, y) = 0. For example, the ∆(y, y′) function can be 1 {y ̸= y′} and we retrieve the multiclass Hinge loss. Such ∆(y, y′) can also be defined to be the Hamming distance and so on. Equipped with those notations, the structured SVM problem can now be formulated as: min Θ∈K,s∈RT ,y∈Y λ 2 ∥Θ∥2 + 1 T T∑ t=1 st, s.t. st ≥ ∆(yt, y) − ⟨Θ, ϕ((zt, At), yt)⟩ + ⟨Θ, ϕ((zt, At), y)⟩, ∀t ∈ [T ], ∀y ∈ Y, st ≥ 0, ∀t ∈ [T ]. We thereby note that solving the above problem requires solving another sub-problem where gt(Θ) := s ∗ t = max y∈Y {∆(yt, y) − ⟨Θ, ϕ((zt, At), yt)⟩ + ⟨Θ, ϕ((zt, At), y)⟩} . But the above sub-problem is computationally intractable since the precise evaluation of gt(Θ) requires solving a discrete optimization problem with exponentially large feasible set Y. Such an obstacle makes the training hard to implement. Besides, even if we get the training result ˜Θ, the following inference problem ˜f (z, A) = arg max y∈Y ⟨ ˜Θ, ϕ((z, A), y)⟩ is still highly intractable. C.2. Scale consistency of least squares linear regression In the previous sections, we raise the point of scale consistency several times. Specifically, the key point made is that the objective vectors of c and αc for α > 0 produce the same optimal solution. Therefore, the algorithm should account for this scale invariance as there might be some scale contamination of the training data in application contexts such as revealed preference/stated preference. Here we present a self-contained result on the scale consistency of the linear regression model that might be of independent interests. Consider the linear regression model where one wants to estimate the true coefficient matrix using cost vectors ct ∈ Rn and feature vectors zt ∈ Rd. Instead of observing true ct’s, we only observe their perturbed/contaminated versions with scale noises (1 + αt)ct’s, where αt’s here are some random variables. We claim that if αt’s are i.i.d. generated and independent of zt and ct, then the ordinary least squares method is still consistent if E[α] = 0. Proposition C.1 (Scale Consistency of Ordinary Least Squares). Assume c = Θ ∗z + ϵ, where E[ϵ|z] = 0, Θ∗ ∈ Rn×d is the true underlying coefficient matrix. Assume α is independent of z and c. Assume we observe T i.i.d. samples (zt, (1+αt)ct)’s. Assume zt, ct, and αt have finite second-order moments, which implies that they follow the strong law of large numbers. Further assume that ct and αt have finite fourth order moment, which implies the strong law of large numbers for c 2 t and α2 t . Assume E[ztz⊤ t ] = Σ, and Σ is non-singular. We estimate the underlying coefficient matrix by the ordinary least squares method: ˆΘT = arg min Θ { fT (Θ) := T∑ t=1 1 T ∥(1 + αt)ct − Θzt∥ 2 2 } . Then ˆΘT a.s. −−→ (1 + E[α])Θ∗, as T → ∞. Proof. W.l.o.g. we only prove the one-dimensional case c ∈ R, since multi-dimensional cases can be proved similarly by breaking Θ = (Θ1, . . . , Θn)⊤ into n independent Θi’s. For the one-dimensional case, the estimator is now minimizing fT (Θ) = 1 T T∑ i=1 ∥z⊤ t Θ − (1 + αt)ct∥2 2 26 Maximum Optimality Margin for Contextual and Inverse Linear Programming 27 = Θ ⊤ [ 1 T T∑ i=1 ztz⊤ t ] Θ − [ 1 T T∑ t=1 2(1 + αt)ctz⊤ t ] Θ + 1 T T∑ t=1(1 + αt) 2c 2 t . Note that we assume that zt’s, ct’s, c 2 t ’s, αt’s, α2 t ’s all have finite second-order moments, which implies 1 T T∑ t=1 ztz⊤ t a.s. −−→ Σ, as T → ∞, 1 T T∑ t=1 2(1 + αt)ctz⊤ t a.s. −−→ 2E[(1 + α)cz⊤], as T → ∞, 1 T T∑ t=1(1 + αt)2c 2 t a.s. −−→ E[(1 + α)2c2] = (1 + 2E[α] + E[α2])E[c 2] < ∞, as T → ∞. Combining the boundedness of the last component with the fact that Σ is non-singular and positive semidefinite and the fact that 2E[(1 + α)cz⊤] is bounded (which will be shown later), we have fT a.s. −−→ f, as T → ∞, where f is a positive definite quadratic function of Θ. Therefore, the unique minimizer of f must be its first-order stationary point. We now compute the partial derivatives. We have E[(1 + α)cz⊤] = E[1 + α]E[cz⊤] = (1 + E[α])E[E[cz⊤|z]] = (1 + E[α])E[E[Θ ∗⊤zz⊤ + ϵz⊤|z]] = (1 + E[α])E[Θ ∗⊤zz⊤], = (1 + E[α])Θ∗⊤Σ. where the first equality is from the fact that αt’s are independent of zt’s and ct’s, the second equality follows the tower law of conditional expectation, the third equality comes from the linear assumption, and the fourth equality comes from E[ϵ|z] = 0. It follows immediately that ∂f ∂Θ = 2ΣΘ − 2(1 + E[α])ΣΘ∗. Since Σ is non-singular, we have proved that the unique minimizer of f is exactly (1 + E[α])Θ∗. Then from the fact that fT a.s. −−→ f and f is positive definite quadratic function, we have ˆΘT = arg min fT a.s. −−→ arg min f = (1 + E[α])Θ∗, as T → ∞. Specifically, if E[α] = 0, we retrieve the true Θ∗. 27","libVersion":"0.3.2","langs":""}
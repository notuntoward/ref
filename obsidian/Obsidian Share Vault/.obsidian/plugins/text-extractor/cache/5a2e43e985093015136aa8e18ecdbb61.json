{"path":"lit/lit_sources/Tjostheim22StatisticalDependencePearsonsa.pdf","text":"Statistical Science 2022, Vol. 37, No. 1, 90–109 https://doi.org/10.1214/21-STS823 © Institute of Mathematical Statistics, 2022 Statistical Dependence: Beyond Pearson’s ρ Dag Tjøstheim, Håkon Otneim and Bård Støve Abstract. Pearson’s ρ is the most used measure of statistical dependence. It gives a complete characterization of dependence in the Gaussian case, and it also works well in some non-Gaussian situations. It is well known; however, that it has a number of shortcomings; in particular, for heavy tailed distribu- tions and in nonlinear situations, where it may produce misleading, and even disastrous results. In recent years, a number of alternatives have been pro- posed. In this paper, we will survey these developments, especially results obtained in the last couple of decades. Among measures discussed are the copula, distribution-based measures, the distance covariance, the HSIC mea- sure popular in machine learning and ﬁnally the local Gaussian correlation, which is a local version of Pearson’s ρ. Throughout, we put the emphasis on conceptual developments and a comparison of these. We point out relevant references to technical details as well as comparative empirical and simu- lated experiments. There is a broad selection of references under each topic treated. Key words and phrases: Statistical dependence, Pearson’s ρ, nonlinear de- pendence, distance covariance, HSIC, mutual information, local Gaussian correlation. 1. INTRODUCTION Pearson’s ρ, the product moment correlation, was not invented by Pearson, but rather by Francis Galton. Gal- ton, a cousin of Charles Darwin, needed a measure of as- sociation in his hereditary studies (Galton, 1888, 1890). This was formulated in a scatter diagram and regression context, and he chose r (for regression) as the symbol for his measure of association. Pearson (1896)gaveamore precise mathematical development and used ρ as a sym- bol for the population value and r for its estimated value. The product moment correlation is now universally re- ferred to as Pearson’s ρ. Galton died in 1911, and Karl Pearson became his biographer, resulting in a massive four-volume biography (Pearson, 1922, 1930). All of this and much more is detailed in Stigler (1989)and Stan- ton (2001). Some other relevant historical references are Fisher (1915, 1921), von Neumann (1941, 1942)and the survey paper by King (1987). Dag Tjøstheim is Professor Emeritus, Department of Mathematics, University of Bergen, P.b. 7803, 5020 Bergen, Norway (e-mail: dag.tjostheim@uib.no). Håkon Otneim is Associate Professor, Department of Business and Management Science, Norwegian School of Economics, Helleveien 30, 5045 Bergen, Norway (e-mail: hakon.otneim@nhh.no). Bård Støve is Professor, Department of Mathematics, University of Bergen, P.b. 7803, 5020 Bergen, Norway (e-mail: bard.stove@uib.no). Write the covariance between two random variables X and Y having ﬁnite second moments as Cov(X, Y ) = σ(X, Y ) = E(X − E(X))(Y − E(Y )). The Pearson’s ρ,or the product moment correlation, is deﬁned by ρ = ρ(X, Y ) = σ(X, Y ) σXσY with σX = √σ 2 X = √E(X − E(X))2 being the standard deviation of X and similarly for σY . The correlation takes values between and including −1and +1. For a given set of pairs of observations (X1,Y1), ...,(Xn,Yn) of X and Y , an estimate of ρ is given by (1.1) r = ̂ρ = ∑n j =1(Xj − X)(Yj − Y) √∑n j =1(Xj − X)2√∑n j =1(Yj − Y)2 with X = n−1 ∑n j =1 Xj , and similarly for Y . Consistency and asymptotic normality can be proved using an appro- priate law of large numbers and a central limit theorem, respectively. The correlation coefﬁcient ρ has been, and probably still is, the most used measure for statistical association, and it is generally accepted as the measure of dependence, not only in statistics, but in most applications of statistics to the natural and social sciences. There are several rea- sons for this: (i) It is easy to compute (estimate). 90 STATISTICAL DEPENDENCE 91 (ii) Linear models are much used, and in a linear re- gression model of Y on X,say, ρ is proportional to the slope of the regression line. (iii) In a bivariate Gaussian density f (x, y) = 1 2π √1 − ρ2σXσY × exp{ − 1 2(1 − ρ2) ( (x − μX)2 σ 2 X − 2ρ (x − μX)(y − μY ) σXσY + (y − μY )2 σ 2 Y )}, the dependence between X and Y is completely charac- terized by ρ. In particular, two jointly Gaussian variables (X, Y ) are independent if and only if they are uncorre- lated. For a considerable number of data sets, the Gaus- sian distribution works at least as a fairly good approx- imation. Moreover, joint asymptotic normality often ap- pears as a consequence of the central limit theorem for many statistics, and the joint asymptotic behavior of such statistics are therefore generally well deﬁned by the cor- relation coefﬁcient. (iv) The product moment correlation is easily general- ized to the multivariate case. For p stochastic variables X1,...,Xp, their joint dependencies can simply (but not always accurately) be characterized by their covariance matrix \u0005 ={σij }, with σij being the covariance between Xi and Xj . Similarly the correlation matrix is deﬁned by \u0006 ={ρij }, with ρij being the correlation between Xi and Xj . Again, for a column vector x = (x1,...,xp)T ,the joint normality density is deﬁned by f(x) = 1 (2π)p/2|\u0005|1/2 exp{− 1 2 (x − μ) T \u0005−1(x − μ) } , where |\u0005| is the determinant of the covariance matrix \u0005 (whose inverse \u0005−1 is assumed to exist), and μ = E(X). Then the complete dependence structure of the Gaussian vector is given by the pairwise covariances σij , or equiv- alently the pairwise correlations ρij . This is remarkable: the entire dependence structure is determined by pairwise dependencies. (v) It is easy to extend the correlation concept to time series. For a time series {Xt }, the autocovariance and au- tocorrelation function, respectively, are deﬁned, assuming stationarity and existence of second moments, by c(t) = σ(Xt+s,Xs) and ρ(t) = ρ(Xt+s,Xs) for arbitrary inte- gers s and t. For a Gaussian time series, the dependence structure is completely determined by ρ(t). Even for non- linear time series and nonlinear regression models, the au- tocovariance function has often been made to play a major role. In the frequency domain all of the traditional spectral analysis is based again on the autocovariance function. In spite of these assets, there are several serious weak- nesses of Pearson’s ρ. These will be brieﬂy reviewed in Section 2. In the remaining sections of this paper, a num- ber of alternative dependence measures going beyond the Pearson ρ will be described. The emphasis will be on con- cepts, conceptual developments and comparisons of these. We do provide some illustrative plots of key properties, but when it comes to technical details, empirical and sim- ulated experiments with numerical comparisons, we point out relevant references instead. In Section 3, we brieﬂy review the copula and its use in dependence modeling. Global dependence functionals based on distribution functions and density functions, if they exist, are treated in Section 4, where we cover the distance based functionals such as the distance covariance function and the mutual information criterion as well as related criteria. We also treat the HSIC criterion, which is popular in machine learning, and its relationship to the distance covariance. We discuss all of this in light of seven properties that a dependence criterion ideally should pos- sess according to Rényi (1959). In Section 5,weshift our emphasis to local dependence measures, allowing the statistical dependence to vary across different regions of the support of the distribution functions. In particular, we treat the recently introduced local Gaussian approxima- tion in Section 6. To improve the focus of the paper, some details, especially those related to Section 6, have been moved to an online supplementary note (Tjøstheim, Ot- neim and Støve, 2022). 2. WEAKNESSES OF PEARSON’S ρ We have subsumed, somewhat arbitrarily, the problems of Pearson’s ρ under three issues. 2.1 The Non-Gaussianity Issue A natural question to ask is whether the close con- nection between Gaussianity and correlation/covariance properties can be extended to larger classes of distribu- tions. The answer to this question is a conditional yes. The multivariate Gaussian distribution is a member of the vastly larger class of elliptical distributions. That class of distributions is deﬁned both for discrete and contin- uous variables, but we limit ourselves to the continuous case. An elliptical distribution can be deﬁned in terms of a parametric representation of the characteristic function or the density function. For an overview of elliptical dis- tributions see, for example, Gómez, Gómez-Villegas, and Marín (2003). Unfortunately, the equivalence between uncorrelated- ness and independence is generally not true for ellipti- cal distributions. Consider, for instance, the multivariate 92 D. TJØSTHEIM, H. OTNEIM AND B. STØVE t-distribution with ν degrees of freedom (2.1) f(x) = \b( p+ν 2 ) (πν)p/2\b(ν/2)|\u0005|1/2 × (1 + (x − μ)T \u0005−1(x − μ) ν )− p+ν 2 . Unlike the multinormal distribution where the exponen- tial form of the distribution forces the distribution to fac- tor if \u0005 is a diagonal matrix (uncorrelatedness), this is not true for the t distribution deﬁned in equation (2.1)if \u0005 is diagonal. In other words, if two components of a bivari- ate t distribution are uncorrelated, they are not necessar- ily independent. This pinpoints a serious deﬁciency of the Pearson’s ρ in measuring dependence in t distributions, and indeed in general elliptical (and, of course, nonellip- tical) distributions. 2.2 The Robustness Issue As is the case for regression, it is well known that the product moment estimator is sensitive to outliers. Even just one single outlier may be very damaging. There are therefore several robustiﬁed versions of ρ, primarily based on ranks. The idea of rank correlation goes back at least to Spearman (1904), and it is most easily ex- plained through its sample version. Given scalar obser- vations {X1,...,Xn}, we denote by R(n) i,X the rank of Xi among X1,...,Xn. (There are various rules for treating ties.) The estimated Spearman rank correlation function given n pairwise observations of two random variables X and Y is given by ̂ρS = n−1 ∑n i=1 R(n) i,XR(n) i,Y − (n + 1)2/4 (n2 − 1)/12 . The rank correlation is thought to be especially effective in picking up linear trends in the data, but it suffers in a very similar way as the Pearson ρ to certain nonlinearities of the data, which are treated in the next subsection. Another way of using the ranks is Kendall’s τ rank cor- relation coefﬁcient given by Kendall (1938). Again, con- sider the situation of n pairs (Xi,Yi) of the random vari- ables X and Y . Two pairs of observations (Xi,Yi) and (Xj ,Yj ), i ̸= j are said to be concordant if the ranks for both elements agree; that is, if both Xi >Xj and Yi >Yj or if both Xi <Xj and Yi <Yj . Similarly, they are said to be discordant if Xi >Xj and Yi <Yj or if Xi <Xj and Yi >Yj . If one has equality, they are neither concor- dant nor discordant, even though there are various rules for treating ties in this case as well. The estimated Kendall τ is then given by ̂τ = ((number of concordant pairs) − (number of discordant pairs) )/ (n(n − 1)/2 ). We will illustrate the robustness issue using a simple example. In Figure 1(a), we see 500 observations that have been simulated from the bivariate Gaussian distri- bution having correlation ρ =−0.5. Thesamplevalue for Pearson’s ρ is ̂ρ =−0.53. If we add just three outliers to the data, however, as shown in Figure 1(b), the sample correlation changes to ̂ρ =−0.36. The sample versions of Spearman’s ρ for the simulated data in Figures 1(a) and 1b are on the other hand very similar: ̂ρS =−0.52 and ̂ρS =−0.49, and the corresponding values for the es- timated Kendall’s τ are ̂τ =−0.37 and ̂τ =−0.35. 2.3 The Nonlinearity Issue This is probably the most serious issue with Pearson’s ρ, and it is an issue also for the rank based correlations of Spearman and Kendall. All of these (and similar mea- sures), are designed to detect rather speciﬁc types of sta- tistical dependencies, namely those for which large val- ues of X tend to be associated with large values of Y , and small values of X with small values of Y (positive dependence), or the opposite case of negative dependence in which large values of one variable tend to be associated with small values of the other variable. It is easy to ﬁnd examples where this is not the case, but where neverthe- less there is strong dependence. A standard introductory text book example is the case where (2.2) Y = X2. Here, Y is uniquely determined once X is given; that is, basically the strongest form of dependence one can have. Nevertheless, if X has a symmetric distribution on the real FIG.1. Illustration of some problems related to the Pearson correlation coefﬁcient. STATISTICAL DEPENDENCE 93 line, and if sufﬁcient moments exist in the case of ρ,then the population quantities corresponding to these three test statistics are all zero. A version of this situation is illus- trated in Figure 1(c), where we have generated 500 obser- vations of the standard normally distributed independent variables X and ϵ, and calculated Y as Y = X2 + ϵ. Still, ρ(X, Y ) = 0. The sample values for Pearson’s ρ, Spear- man’s ρS and Kendall’s τ are ̂ρ =−0.001, ̂ρS =−0.03 and ̂τ =−0.02, respectively, and none of them are signif- icantly different from zero. It may be noted that there is a consistent nonnegative modiﬁcation of Kendall’s τ ;see Bergsma and Dassios (2014). Essentially the same problem will occur if X = UW and Y = VW ,where U and V are independent of each other and independent of W . It is trivial to show that ρ(X, Y ) = 0ifE(U ) = E(V ) = 0, whereas X and Y are clearly dependent. This example typiﬁes the kind of de- pendence one has in ARCH/GARCH time series models: If {εt } is a time series of zero-mean i.i.d. variables and if the non-negative time series {ht } is independent of {εt }, and {Xt } and {ht } are given by the recursive relationship (2.3) Xt = εt h 1/2 t ,ht = α + βht−1 + γX2 t−1, where the stochastic process {ht } is the so-called volatil- ity process, then the resulting model is a GARCH(1, 1) model. Further, α> 0, and β and γ are nonnegative con- stants satisfying β +γ< 1. This model can be extended in many ways and the ARCH/GARCH models are extremely important in ﬁnance. A comprehensive book is Francq and Zakoïan (2011). The work on these kinds of mod- els was initiated by Engle (1982), and he was awarded the Nobel Memorial Prize in Economic Sciences for his work. The point as far as Pearson’s ρ is concerned is that Xt and Xs are uncorrelated for t ̸= s, but they are in fact strongly dependent through the volatility process {ht }, which can be taken to measure ﬁnancial risk. In Figure 1(d), we see some simulated data from a GARCH(1, 1)-model with ϵt ∼ i.i.d.N(0, 1), α = 0.1, β = 0.7and γ = 0.2, with Xt on the horizontal axis, and Xt−1 on the vertical axis. In this particular case, ̂ρ(Xt ,Xt−1) = 0.018, despite the strong serial depen- dence that is seen to exist directly from equation (2.3). In the following sections, we will look at ways of de- tecting nonlinear and non-Gaussian structures by going beyond Pearson’s ρ. 3. BEYOND PEARSON’S ρ: THE COPULA For two variables, one may ask, why not just take the joint density function f (x, y) or the cumulative distri- bution function F (x, y) as a descriptor of the joint de- pendence? The answer is quite obvious. If a parametric density model is considered, it is usually quite difﬁcult to give an interpretation of the parameters in terms of the strength of the dependence. If one looks at nonparametric estimates for multivariate density functions, to a certain degree one may get an informal indication of strength of dependence in certain regions from a display of the den- sity, but the problems increase quickly with dimension due both to difﬁculties of producing a graphical display and to the lack of precision of the estimates due to the curse of dimensionality. Another problem in analyzing a joint density function is that it may be difﬁcult to disentan- gle effects due to the shape of marginal distributions and effects due to dependence among the variables involved. This last problem is resolved by the copula construc- tion. Sklar’s (1959) theorem states that a multivariate cu- mulative distribution function F(x) = F(x1,...,xp) with marginals Fi(xi), i = 1,...,p can be decomposed as (3.1) F(x1,...,xp) = C(F1(x1), ...,Fp(xp) ), where C(u1,...,up) is a distribution function over the unit cube [0, 1]p. Klaassen and Wellner (1997) point out that Hoeffding (1940) had the basic idea of summariz- ing the dependence properties of a multivariate distribu- tion by its associated copula, but he chose to deﬁne the corresponding function on the interval [−1/2, 1/2] in- stead of on the interval [0, 1]. In the continuous case, C is a function of uniform variables U1,...,Up,using the well-known fact that for a continuous random variable Xi, Fi(Xi) is uniform on [0, 1]. Further, in the continuous case C is uniquely determined by Sklar’s (1959) theorem. The theorem continues to hold for discrete variables un- der certain regularity conditions securing uniqueness. We refer to Nelsen (1999)and Joe(2014) for extensive treat- ments of the copula. Joe (2014), in particular, contains a large section on copulas in the discrete case. See also Genest and Nešlehová (2007). The decomposition (3.1) very effectively disentangle the distributional properties of a multivariate distribution into a dependence part measured by the copula C and a marginal part described by the univariate marginals. Note that C is invariant with respect to one-to-one transforma- tions of the marginal variables Xi .Inthisrespect,itis analogous to the invariance of the Kendall and Spearman rank based correlation coefﬁcients. The decomposition in (3.1) is very useful in that it leads to large classes of models that can be speciﬁed by deﬁning the marginals and the copula function separately. It has great ﬂexibility in that very different models can be cho- sen for the marginal distribution, and there is a large cata- log of possible parametric models available for the copula function C; it can also be estimated nonparametrically. In particular, the Clayton copula has been important in eco- nomics and ﬁnance. It is deﬁned by (3.2) CC(u1,u2) = max{u−θ 1 + u−θ 2 − 1; 0]−1/θ with θ ∈[−1, ∞)/0 94 D. TJØSTHEIM, H. OTNEIM AND B. STØVE in the two-dimensional case. It can be extended to higher dimensions. For a connection between Kendall’s τ and the copula parameter θ , see Genest et al. (2011). We will throughout this paper and its supplement il- lustrate several points using a bivariate data set on some ﬁnancial returns. We use daily international equity price index data for the United States (i.e., the S&P 500) and the United Kingdom (i.e., the FTSE 100). The data are obtained from Datastream (2018), and the returns are de- ﬁned as rt = 100 × (log(pt ) − log(pt−1) ), where pt is the price index at time t. The observation span covers the period from January 1, 2007, through Decem- ber 31, 2009, in total 784 observations. In Figure 2, four scatterplots are presented. Figure 2(a) displays a scatterplot of the observed log- returns with S&P 500 on the horizontal axis, and the FTSE 100 on the vertical axis. Figure 2(b) displays the uniform scores of the same data, that is, ( ̂U1i, ̂U2i) = ( ̂F1(X1i), ̂F2(X2i)) where ̂F1 and ̂F2 are the empirical distribution functions of X1: S&P500 and X2: FTSE 100, and we see indications of a somewhat cluttered behaviour of the scores in the lower left and upper right corners of the unit square corresponding to the tails of the joint dis- tribution. The plot in Figure 2(b) then is a scatter diagram of the copula dependence function C in the formula (3.1) when p = 2. In Figure 2(c), the observations have been transformed to normal scores given by ̂Zi = \u0010−1( ̂F(Xi)), i = 1, 2, where \u0010−1 is the inverse cumulative distribution function of the standard normal. The transformation to a standard normal scale plays an important role in the the- ory and application of the local Gaussian correlation mea- sure; see, in particular, the discussion following equation (6.4). For now, it is sufﬁcient to note that it more clearly reveals the tail properties of the underlying distribution FIG.2. Illustrations using the ﬁnancial returns data set. STATISTICAL DEPENDENCE 95 than is the case of Figure 2(b). Finally, Figure 2(d) shows the scatterplot of 784 simulated pairs of variables, on uni- form scale, from a Clayton copula ﬁtted to the return data. The plot resembles Figure 2(b), in particular in the lower left corner. However, there are some differences in the up- per right corner. We will look into this discrepancy in Sec- tion 6, and in Section 2 of the supplement. In Figure 2(a), and perhaps more clearly in Figure 2(c), we see that there seems to be stronger dependence be- tween the variables when the market is going either up or down, which is very sensible from an economic point of view, but it is not easy to give an interpretation of the parameter θ of the Clayton copula in terms of such type of dependence. In fact, in this particular case, ̂θ = 0.96. The difﬁculty of giving a clear and concrete interpreta- tion of copula parameters in terms of measuring strength of dependence can be stated as a potential issue of the copula representation. In this respect, it is very different from Pearson’s ρ. We will return to this point in Section 6, where we deﬁne a local correlation. Another issue of the original copula approach has been the lack of good practical models as the dimension in- creases, as it would, for example, in a portfolio problem in ﬁnance. This has recently been sought solved by the so- called pair copula construction. To simplify, in a trivariate density f(x1,x2,x3), by conditioning this can be writ- ten f(x1,x2,x3) = f1(x1)f23|1(x2,x3|x1), and a bivariate copula construction, for example, a Clayton copula, can be applied to the conditional density f23|1(x2,x3|x1) with x1 ﬁxed and with a parameter θ = θ(x1) depending on x1. This conditioning can be extended to higher dimensions under a few simplifying assumptions, resulting in a so- called vine copula, of which there are several types. The procedure is well described by Aas et al. (2009), and has found a number of applications. 4. BEYOND PEARSON’S ρ: GLOBAL DEPENDENCE FUNCTIONALS AND TESTS OF INDEPENDENCE Studies of statistical dependence may be said to center mainly around two problems: (i) deﬁnition and estimation of measures of dependence and (ii) tests of independence. Of course, these two themes are closely related. Measures of association such as Pearson’s ρ can also be used in tests of independence, or more precisely: tests of uncorre- latedness. On the other hand, test functionals for tests of independence can in many, but not all, cases be used as a measures of dependence. A disadvantage with measures derived from tests is that they are virtually always based on a distance function and, therefore, nonnegative. This means that they cannot distinguish between negative and positive dependence. Most of the test functionals are based on the deﬁni- tion of independence in terms of cumulative distribution functions or in terms of density functions. Consider p stochastic variables X1,...,Xp. These variables are in- dependent if and only if their joint cumulative distribution function is the product of the marginal distribution func- tions: FX1,...,Xp (x1,...,xp) = F1(x1) ··· Fp(xp),and the same is true for all subsets of variables of (X1,...,Xp). If the variables are continuous, this identity can be phrased in terms of the corresponding density functions instead. A typical test functional is then designed to mea- sure the distance between the estimated joint distribu- tions/densities and the product of the estimated marginals. One would usually estimate the involved distributions non- or semiparametrically, which, for joint distributions, may be problematic for moderate and large p’s due to the curse of dimensionality. We will treat these problems in some detail in Sections 4.2–4.5. Before starting on the description of the various de- pendence measures, let us remark that Rényi (1959)pro- posed that a measure of dependence δ(X, Y ) between two stochastic variables X and Y should ideally have the fol- lowing seven properties: (i) δ(X, Y ) is deﬁned for any X, Y neither of which is constant with probability 1. (ii) δ(X, Y ) = δ(Y, X). (iii) 0 ≤ δ(X, Y ) ≤ 1. (iv) δ(X, Y ) = 0 if and only if X and Y are indepen- dent. (v) δ(X, Y ) = 1 if either X = g(Y ) or Y = f(X), where f and g are measurable functions. (vi) If the Borel-measurable functions f and g map the real axis in a one-to-one way to itself, then δ(f (X), g(Y )) = δ(X, Y ). (vii) If the joint distribution of X and Y is normal, then δ(X, Y ) =|ρ(X, Y )|,where ρ(X, Y ) is Pearson’s ρ. The product moment correlation ρ satisﬁes only (ii) and (vii). One can argue that the rules (i)–(vii) do not take into account the difference between positive and negative de- pendence; it only looks at the strength of the measured de- pendence. If this wider point of view were to be taken into account, (iii) could be changed into (iii′): −1 ≤ δ(X, Y ) ≤ 1, (v) into (v′): δ(X, Y ) = 1or δ(X, Y ) =−1if there is a deterministic relationship between X and Y . Finally, (vii) should be changed into (vii′) requiring δ(X, Y ) = ρ(X, Y ). Moreover, some will argue that property (vi) may be too strong to require. It means that the strength of dependence is essentially independent of the marginals as for the copula case. We will discuss these properties as we proceed in the paper. Before we begin surveying the test functionals as announced above, we start with the maximal correlation which, it will be seen, is intertwined with at least one of the test functionals to be presented in the sequel. 96 D. TJØSTHEIM, H. OTNEIM AND B. STØVE 4.1 Maximal Correlation The maximal correlation is based on the Pearson ρ.It is constructed to avoid the problem demonstrated in Sec- tion 2.3 that ρ can easily be zero even if there is strong dependence. It seems that the maximal correlation was ﬁrst intro- duced by Gebelein (1941). He introduced it as S(X, Y ) = sup f,g ρ(f(X), g(Y )), where ρ is Pearson’s ρ. Here, the supremum is taken over all Borel-measurable functions f , g with ﬁnite and posi- tive variance for f(X) and g(Y ). The measure S gets rid of the nonlinearity issue of ρ. It is not difﬁcult to check that S = 0 if and only if X and Y are independent, and in fact all of the Renyi’s seven criteria hold for the maxi- mum correlation; see Lancaster (1957) for property (vii). On the other hand, S cannot distinguish between negative and positive dependence, and it is in general difﬁcult to compute. Two more recent publications are Huang (2010), where the maximal correlation is used to test for conditional independence, and Yenigün, Székely and Rizzo (2011), where it is used to test for independence in contingency tables. The latter paper introduces a new example where S(X, Y ) can be explicitly computed. 4.2 Measures and Tests Based on the Distribution Function We start with, and in fact put the main emphasis on, the bivariate case. Let X and Y be stochastic variables with cumulative distribution functions FX and FY . The prob- lem of measuring the dependence between X and Y can then be formulated as a problem of measuring the distance between the joint cumulative distribution function FX,Y of (X, Y ) and the distribution function FXFY formed by taking the product of the marginals. Let \u0012(·, ·) be a can- didate for such a distance functional. It will be assumed that \u0012 is a metric, and it is natural to require (see, e.g., Skaug and Tjøstheim, 1996), that \u0012(FX,Y ,FXFY ) ≥ 0 and \u0012(FX,Y ,FXFY ) = 0 if and only if FX,Y = FXFY . (4.1) Clearly, such a measure is capable only of measuring the strength of dependence, not its direction. A natural estimate ̂\u0012 of a distance functional \u0012 is ob- tained by setting ̂\u0012(FX,Y ,FXFY ) = \u0012( ̂FX,Y , ̂FX ̂FY ), where ̂F may be taken to be the empirical distribution functions given by ̂FX(x) = 1 n n∑ j =1 1(Xj ≤ x), ̂FY (y) = 1 n n∑ j =1 1(Yj ≤ y) and ̂FX,Y (x, y) = 1 n n∑ j =1 1(Xj ≤ x)1(Yj ≤ y), for given observations {(X1,Y1), ...,(Xn,Yn)}. Conventional distance measures between two distribu- tion functions F and G are the Kolmogorov–Smirnov dis- tance \u00121(F, G) = sup (x,y) ∣ ∣F (x, y) − G(x, y)∣ ∣ and the Cramér–von Mises type distance of a distribution G from a distribution F \u00122(F, G) = ∫ {F (x, y) − G(x, y)}2 dF (x, y). Here, both \u00121 and \u00122 satisfy (4.1). Most of the work pertaining to measuring dependence and testing of independence has been done in terms of the Cramér–von Mises distance. This work started already by Hoeffding (1948) who looked at i.i.d. pairs (Xi,Yi), and studied ﬁnite sample distributions in some special cases. With considerable justiﬁcation, it has been named the Hoeffding-functional by some. This work was contin- ued by Blum, Kiefer and Rosenblatt (1961) who provided an asymptotic theory, still for the i.i.d. case. It was ex- tended to the time series case with a resulting test of se- rial independence in Skaug and Tjøstheim (1993a). A pa- per using a copula framework is Kojadinovic and Holmes (2009). We will brieﬂy review the time series case in an online supplement that accompanies this paper because it illustrates some of the problems, and because some of the same ideas as for the Hoeffding-functional have been used in more recent work on the distance covariance, which we treat in Section 4.3. As mentioned in the beginning of this section, an inde- pendence test for p> 2 should test the cummulative dis- tribution funcion for all subsets of X1,...,Xp. Deheuvels (1981a, 1981b) does exactly that using the Möbius trans- formation. A recent follow-up is Ghoudi and Rémillard (2018). Instead of stating independence in terms of cumulative distribution functions this can alternatively be expressed in terms of the characteristic function. Székely, Rizzo and Bakirov (2007) and Székely and Rizzo (2009), as will be seen in Section 4.3, make systematic use of this in their introduction of the distance covariance test. Two random variables X and Y are independent if and only if the char- acteristic functions satisfy φX,Y (u, v) = φX(u)φY (v) ∀(u, v), where φX,Y (u, v) = E (eiuX+ivY ),φX(u) = E (eiuX), φY (v) = E (eivY ) . STATISTICAL DEPENDENCE 97 This was exploited by Csörgö (1985) and Pinkse (1998)to construct tests for independence based on the characteris- tic function in the i.i.d. and time series case, respectively. Further work on testing of conditional independence was done by Su and White (2007). Hong (1999) put this into a much more general context by focussing on σk(u, v) = φXt ,Xt−|k|(u, v) − φXt (u)φXt−|k|(v). By taking Fourier transform of this quantity, one obtains (4.2) f(ω, u, v) = 1 2π ∞∑ k=−∞ σk(u, v)e−ikω. Hong (1999) called (4.2) the generalized spectral den- sity function and based a test of serial independence on this. More work related to this has been done by Es- canciano and Velasco (2006). Some related ideas can be found in Hong (2000), and more recently in Escanciano and Hualde (2019). 4.3 Distance Covariance We have seen that there are at least two ways of con- structing functionals that are consistent against all forms of dependence, namely those based on the empirical distribution function initiated by Hoeffding (1948)and brieﬂy reviewed above, and those based on the charac- teristic function represented by Csörgö (1985) in the i.i.d. case and Pinkse (1998) in the serial dependence case, and continued in Hong (1999, 2000) in a time series gener- alized spectrum approach. Both Pinkse and Hong use a kernel type weight function in their functionals. The authors of two remarkable papers, Székely, Rizzo and Bakirov (2007) and Székely and Rizzo (2009), take up the characteristic function test statistic again in the nontime series case. But what distinguishes these from earlier papers is an especially judicious choice of weight function reducing the empirical characteristic function functional to empirical moments of differences between the variables, or distances in the vector case, this leading to covariance of distances. Some of these ideas go back to what the authors term an “energy statistic”; see Székely (2002), Székely and Rizzo (2013). It has been extended to time series and multiple dependencies by Davis et al. (2018), Fokianos and Pitsillou (2017), Zhou (2012)and Dueck et al. (2014), and Yao, Zhang and Shao (2018). In the locally stationary time series case, there is also a theory; see Jentsch et al. (2020). The distance covariance, dcov, seems to work well in a number of situations, and it has been used as a yardstick by several authors writing on dependence and tests of independence. In particular, it has been used as a measure of comparison in the work on local Gaussian correlation to be detailed in Section 6 and the supplementary material. There are also points of contacts, as will be seen in Section 4.4, with the HSIC measure of dependence popular in the machine learning community. The central ideas and derivations are more or less all present in Székely, Rizzo and Bakirov (2007). The frame- work is that of pairs of i.i.d. vector variables (X, Y ) in Rp and Rq , respectively, and the task is to construct a test functional for independence between X and Y . Let φX,Y (u, v) = E(ei(⟨X,u⟩+⟨Y,v⟩)), φX(u) = E(ei⟨X,u⟩) and φY (v) = E(ei⟨Y,v⟩) be the characteristic functions in- volved, where ⟨·, ·⟩ is the inner product in Rp and Rq , respectively. The starting point is again the weighted char- acteristic functional V 2(X, Y ; w) = ∫ Rp+q ∣ ∣φX,Y (u, v) − φX(u)φY (v) ∣ ∣2 × w(u, v) du dv, (4.3) where w is a weight function to be chosen. Similarly, one deﬁnes V 2(X; w) = ∫ R2p ∣ ∣φX,X(u, v) − φX(u)φX(v) ∣ ∣2 × w(u, v) du dv (4.4) and V 2(Y ; w). The distance correlation, dcor, is next de- ﬁned by, assuming V 2(X)V 2(Y ) > 0, R2(X, Y ) = V 2(X, Y ) √V 2(X)V 2(Y ) . These quantities can be estimated by the empirical coun- terparts given n observations of the vector pair (X, Y ) with V 2 n(X, Y ; w) = ∫ Rp+q ∣ ∣φn X,Y (u, v) − φn X(u)φn Y (v) ∣ ∣2 × w(u, v) du dv, (4.5) where, for a set of observations {(X1,Y1), ...,(Xn,Yn)} the empirical characteristic functions are given by φn X,Y (u, v) = 1 n n∑ k=1 exp{i( ⟨Xk,u⟩+⟨Yk,v⟩ )} and φn X(u) = 1 n n∑ k=1 exp{i⟨Xk,u⟩ } , φn Y (v) = 1 n n∑ k=1 exp{i⟨Yk,v⟩ }. It turns out that it is easier to handle the weight function in the framework of the empirical characteristic functions. It will be seen below that (4.6) w(u, v) = (cpcq|u|1+p p |v|1+q q )−1 is a good choice. Here, |·|p is the Euclidean norm in Rp and similarly for |·|q . Moreover, the normalizing con- stants are given by cj = π (1+j)/2/\b((1+j)/2), j = p, q. 98 D. TJØSTHEIM, H. OTNEIM AND B. STØVE For it to make sense to introduce the weight function on the empirical characteristic function, one must show that the empirical functionals Vn converges to the theoretical functionals V for this weight function. This is not triv- ial because of the singularity at 0 for w givenby(4.6). A detailed argument is given in the proof of Theorem 2 in Székely, Rizzo and Bakirov (2007). The advantage of introducing the weight function for the empirical characteristic functions is that one can com- pute the squares in (4.5) and then interchange summa- tion and integration. The resulting integrals can be com- puted using trigonometric identities. The details are given in the proof of Theorem 1 in Székely, Rizzo and Bakirov (2007) and in Lemma 1 of the Appendix of Szekely and Rizzo (2005) who in turn refer to Prudnikov, Brychkov and Marichev (1986) for the fundamental lemma ∫ Rd 1 − cos⟨x, u⟩ |u|d+α d du = C(d, α)|x| α d for 0 <α < 2 with (4.7) C(d, α) = 2π d/2\b(1 − α/2) α2α\b((d + α)/2) , and where the weight function considered above corre- sponds to α = 1and d = p or d = q in (4.6). The general α-case corresponds to a weight function w(u, v; α) = (C(p, α)C(q, α)|u|p+α p |v|q+α q )−1. With the simpliﬁcation α = 1 all of this implies that V 2 n as deﬁned in (4.5), can be computed as V 2 n(u, v) = S1 + S2 − 2S3, where S1 = 1 n2 n∑ k,l=1 |Xk − Xl|p|Yk − Yl|q, S2 = 1 n2 n∑ k,l=1 |Xk − Xl|p 1 n2 n∑ k,l=1 |Yk − Yl|q and (4.8) S3 = 1 n3 n∑ k=1 n∑ l,m=1 |Xk − Xl|p|Yk − Ym|q, which explains the appellation distance covariance. In fact, it is possible to further simplify this by introducing akl =|Xk − Xl|p, ak. = 1 n n∑ l=1 akl, a.l = 1 n n∑ k=1 akl, a.. = 1 n2 n∑ k,l=1 akl,Akl = al − ak. − a.l + a.., for k, l = 1,...,n. Similarly, one can deﬁne bkl =|Yk − Yl|q and Bkl = bkl − bk. − b.l + b.. and V 2 n(X, Y ) = 1 n2 n∑ k,l=1 AklBkl and V 2 n(X) = V 2 n(X, X) = 1 n2 n∑ k,l=1 A 2 kl and similarly for V 2 n(Y ). From this, one can easily com- pute R2 n(X, Y ). The computations are available in the R- package energy by Rizzo and Szekely (2018). As is the case of the empirical joint distribution func- tional, it can be expected that the curse of dimensionality will inﬂuence the result for large and moderate values of p and q. Obviously, in the time series case, it is possi- ble to base oneself on pairwise distances, which has been done in Yao, Zhang, and Shao (2018). Letting n →∞, it is not difﬁcult to prove that an al- ternative expression for V(X, Y ) is given by (assuming E|X|p < ∞ and E|Y |q < ∞) V 2(X, Y ) = EX,X′,Y,Y ′{∣ ∣X − X′∣ ∣p∣ ∣Y − Y ′∣ ∣ q } + EX,X′{∣ ∣X − X′∣ ∣ p}EY,Y ′{∣ ∣Y − Y ′∣ ∣ q } (4.9) − 2EX,Y { EX′ ∣ ∣X − X′∣ ∣ pEY ′∣ ∣Y − Y ′∣ ∣ q }, where (X, Y ), (X′,Y ′) are i.i.d. This expression will be useful later in Section 4.4 in a comparison with the HSIC statistic. Properly scaled V 2 n has a limiting behavior un- der independence somewhat similar to that described in Theorem 2 of Skaug and Tjøstheim (1993a); see also equation (1.2) in Section 1 in the online supplement. One can also obtain an empirical process limit theorem, The- orem 5 of Székely, Rizzo and Bakirov (2007). In the R- package energy, as for the case of the empirical distri- bution function, it has been found advantageous to rely on re-sampling via permutations. This is quite fast since the algebraic formulas (4.8) are especially amenable to per- mutations. Both Székely, Rizzo and Bakirov (2007)and Székely and Rizzo (2009) in their experiments only treat the case of α = 1in (4.7). Turning to the properties (i)–(vii) of Rényi (1959) listed in the beginning of this section, it is clear that (i)–(iv) are satisﬁed by R. Moreover, according to Székely, Rizzo and Bakirov (2007), if Rn(x; y) = 1, then there exists a vec- tor α, a nonzero real number β and an orthogonal ma- trix C such that Y = α + βXC, which is not quite the same as Rényi’s requirement (v). The dcov measure, be- ing a correlation based measure, in general depends on the distribution of the margins, and hence Rényi’s invariance property (vi) does not hold in general; see also Berrett and Samworth (2019), Section 2.1. The ﬁnal criterion (vii) of Rényi is that the dependent measure should reduce to the STATISTICAL DEPENDENCE 99 absolute value of Pearson’s ρ in the bivariate normal case. This is not quite the case for the dcov, but it comes close, as is seen from Theorem 6 of Székely and Rizzo (2009). In fact, if (X, Y ) is bivariate normal with E(X) = E(Y ) = 0 and Var(X) = Var(Y ) = 1 and with correlation ρ,then R(X, Y ) ≤|ρ| and inf ρ̸=0 R(X, Y ) |ρ| = lim ρ→0 R(X, Y ) |ρ| = 1 2(1 + π/3 − √3)1/2 ≈ 0.891. 4.4 The HSIC Measure of Dependence Recall the deﬁnition and formula for the maximal cor- relation. This, as stated in Section 4.1,gives rise to a statistic S(X, Y ),where S(X, Y ) = 0 if and only if X and Y are independent. But it is difﬁcult to compute since it requires the supremum of the correlation ρ(f (X), g(Y )) taken over Borel-measurable f and g. In the framework of reproducing kernel Hilbert spaces (RKHS) it is possi- ble to pose this problem, or an analogous one, much more generally, and one can compute an analogue of S quite easily. This yields the so-called HSIC (Hilbert–Schmidt Independence Criterion). Reproducing kernel Hilbert spaces are very important tools in mathematics as well as in statistics. A general ref- erence to applications in statistics is Berlinet and Thomas- Agnan (2004). In the last decade or so, there has also been a number of uses of RKHS in dependence model- ing. These have often, but not always, been published in the machine learning literature; see, for example, Gretton and Györﬁ (2010), Gretton and Györﬁ (2012), Sejdinovic et al. (2013) and Pﬁster et al. (2018). We have found the quite early paper by Gretton et al. (2005) useful both for a glimpse of the general theory and for the HSIC criterion in particular. A reproducing kernel Hilbert space is a separable Hilbert space F of functions f on a set X , such that the evaluation functional f → f(x) is a continuous linear functional on F for every x ∈ X . Then, from the Riesz representation theorem, Muscat (2014), Chapter 10, there exists an element kx ∈ F such that ⟨f, kx⟩= f(x),where ⟨·, ·⟩ is the inner product in F . Applying this to f = kx and another point y ∈ X ,wehave ⟨kx,ky⟩= kx(y).The function (x, y) → kx(y) from X × X to R is the kernel of the RKHS F . It is symmetric and positive deﬁnite be- cause of the symmetry and positive deﬁniteness of the inner product in F . We use the notation k(x, y) for the kernel. The next step is to introduce another set Y with a corre- sponding RKHS G and to introduce a probability structure and probability measures pX, pY and pX,Y on X , Y and X × Y, respectively. With these probability measures and function spaces F and G, one can introduce correlation of functions of stochastic variables on X , Y and X × Y.This is an analogy of the functions used in the deﬁnition of the maximal correlation. In the RKHS setting, the covariance (or cross covariance) is an operator on the function space F . Note also that this has a clear analogy in functional statistics; see, for example, Ferraty and Vieu (2006). It is time to introduce the Hilbert–Schmidt operator: A linear operator C : G → F is called a Hilbert–Schmidt operator if its Hilbert–Schmidt (HS) norm ∥C∥HS ∥C∥ 2 HS . = ∑ i,j ⟨Cvj ,ui⟩ 2 F < ∞, where ui and vj are orthonormal bases of F and G,re- spectively. The HS-norm generalizes the Froebenius norm ∥A∥F = ( ∑i ∑j a2 ij )1/2 for a matrix A = (aij ). Finally, we need to deﬁne the tensor product in this context: If f ∈ F and g ∈ G, then the tensor product operator f ⊗ g : G → F is deﬁned by (f ⊗ g)h . = f ⟨g, h⟩G,h ∈ G. Moreover, by using the deﬁnition of the HS norm it is not difﬁcult to show that ∥f ⊗ g∥ 2 HS =∥f ∥2 F ∥g∥ 2 G. We can now introduce an expectation and a covariance on these function spaces. Again, the analogy with corre- sponding quantities in functional statistics will be clear. We assume that (X ,\b) and (Y,\u0006) are furnished with probability measures pX, pY , and with \b and \u0006 being σ -algebras of sets on X and Y. The expectations μX ∈ F and μY ∈ G are deﬁned by, X and Y are stochastic vari- ables in (X ,\b) and (Y,\u0006), respectively, ⟨μX,f ⟩F = EX[f(X)] and ⟨μY ,g⟩G = EY [g(Y ) ], where μX and μY are well-deﬁned as elements in F and G because of the Riesz representation theorem. The norm is obtained by ∥μX∥ 2 F = EX,X′[k(X, X′)], where as before X and X′ are independent but have the same distribution pX,and where ∥μY ∥F is deﬁned in the same way. With given φ ∈ F , ψ ∈ G, we can now deﬁne the cross covariance operator as CX,Y . = EX,Y [(φ(X) − μX) ⊗ ( ψ(Y ) − μY )] = EX,Y [φ(X) ⊗ φ(Y )] − μX ⊗ μY . Now, take φ(X) to be identiﬁed with kX ∈ F deﬁned above as a result of the Riesz representation theorem, and ψ(Y ) ∈ G deﬁned in exactly the same way. The Hilbert– Schmidt Information Criterion (HSIC) is then deﬁned as 100 D. TJØSTHEIM, H. OTNEIM AND B. STØVE the squared HS norm of the associated cross-covariance operator HSIC(pXY , F, G) . =∥CXY ∥ 2 HS. Let k(x, x′) and l(y, y′) be kernel functions on F and G. Then (Gretton et al., 2005, Lemma 1), the HSIC criterion can be written in terms of these kernels as HSIC(pXY , F, G) = EX,X′,Y,Y ′ [k(X, X′)l(Y, Y ′)] + EX,X′[k(X, X′)]EY,Y ′[l(Y, Y ′)] − 2EX,Y {EX′[k(X, X′)]EY ′[l(Y, Y ′)]}. (4.10) Existence is guaranteed if the kernels are bounded. The similarity in structure to (4.9) for the distance covariance should be noted. Note that the kernel functions depend on the way the spaces F and G and their inner products are deﬁned. In fact, it follows from a famous result by Moore– Aronszajn (see Aronszajn, 1950), that if k is a symmetric, positive deﬁnite kernel on a set X , then there is a unique Hilbert space of functions on X for which k is a reproduc- ing kernel. Hence as will be seen next, in practice when applying the HSIC criterion, the user has to choose a ker- nel. With some restrictions, the HSIC measure is a proper measure of dependence in the sense of the Rényi (1959) criterion (iv): From Theorem 4 of Gretton et al. (2005), one has that if the kernels k and l are universal (universal kernel has a mild continuity requirement on the kernel) on compact domains X and Y,then ∥CXY ∥HS = 0if and only if X and Y are independent. The compactness as- sumption results from the application of an equality for bounded random variables taken from Hoeffding (1963). A big asset of the HSIC measure is that its empirical version is easily computable. In fact, if we have indepen- dent observations X1,...,Xn and independent observa- tions Y1,...,Yn,then (4.11) HSICn(X, Y, F, G) = (n − 1)−2 tr{KH LH }, where tr is the trace operator and the n × n matrices H , K, L are deﬁned by K ={Kij }= {k(Xi,Xj ) } ,L ={Lij }= { l(Yi,Yj ) }, H ={Hij }= {δij − n−1} , where δij is the Kronecker delta. It is shown in Gretton et al. (2005) that this estimator converges in probability toward ∥CXY ∥2 HS. The convergence rate is n−1/2.Thereis also a limit theorem for the asymptotic distribution, which under the null hypothesis of independence and scaled with n, converges in distribution to the random variable Q = ∑∞ i,j =1 λiηj N 2 ij ,where the Nij are independent standard normal variables, and λi and ηj are eigenvalues of integral operators associated with centralized kernels derived from k and l and integrating using the probability measures pX and pY , respectively. Again, this could be compared to the limiting variable for the statistic in the Cramér–von Mises functional as stated in Theorem 2 by Skaug and Tjøstheim (1993a), or see equation (1.2) in the online supplement. Critical values can be obtained for Q, but as a rule one seems to rely more on resampling as is the case for most independence test functionals. It is seen from (4.11) that computation of the empir- ical HSIC criterion requires the evaluation of k(Xi,Xj ) and l(Yi,Yj ). Then appropriate kernels have to be cho- sen. Two commonly used kernels are the Gaussian kernel given by k(x, y) = e |x−y|2 2σ 2 ,σ > 0 and the Laplace kernel k(x, y) = e |x−y| σ ,σ > 0. Pﬁster and Peters (2017) describe the recent R-package dHSIC involving HSIC. Gretton et al. (2005)use these kernels in comparing the HSIC test with several other tests, including the dcov test in, among other cases, an in- dependent component setting. Both of these tests do well, and none of them decisively out-competes the other. This is perhaps not so unexpected because there is a strong re- lationship between these two tests. This is demonstrated by Sejdinovic et al. (2013). They look at both the dcov test and the HSIC test in a generalized setting of semimetric spaces, that is, with kernels and distances deﬁned on such spaces X and Y. For a given distance function, they intro- duce a distance-induced kernel, and under certain regular- ity conditions they establish a relationship between these two quantities. For the distance covariance and the HSIC, the distribu- tion under the null and under the alternative are generally different. The discrepancy between the two distributions has been analyzed by Zhang et al. (2018) and Yao, Zhang and Shao (2018) Lately there have been other extensions of both the dcov and HSIC to conditional dependence, partial dis- tance and to time series. A few references are Szekely and Rizzo (2014), Zhang et al. (2012) and Pﬁster et al. (2018). A recent tutorial on RKHS is Gretton (2019). Further, the generalization of the distance covariance to more than two vectors have independently been shown by Bilodeau and Nangue (2017), building on Bilodeau and Lafaye de Micheaux (2009) and Böttcher, Keller-Ressel and Schilling (2019). More speciﬁcally, Bilodeau and Nangue (2017) use the Möbius transformation of char- acteristic functions to characterize independence, and a generalization to p vectors of distance covariance and Hilbert–Schmidt independence criterion (HSIC) is pro- posed. Consistency and weak convergence of both types of statistics are established. STATISTICAL DEPENDENCE 101 4.5 Density Based Tests of Independence Intuitively, one might think that knowing that the den- sity exists should lead to increased power of the indepen- dence tests due to more information. This is true, at least for some examples (see, e.g., Teräsvirta, Tjøstheim and Granger, 2010, Chapter 7.7). As in the preceding sections, one can construct distance functionals between the joint density under dependence and the product density under independence. A number of authors have considered such an approach; both in the i.i.d. and time series case; see, for example, Rosenblatt (1975), Robinson (1991), Skaug and Tjøstheim (1993b, 1996), Granger, Maasoumi and Racine (2004), Hong and White (2005), Su and White (2007) and Berrett and Samworth (2019). For two ran- dom variables X and Y having joint density fX,Y and marginals fX and fY the degree of dependence can be measured by \u0012(fX,Y ,fXfY ),where \u0012 is now the dis- tance measure between two bivariate density functions. The variables may be normalized with E(X) = E(Y ) = 0 and Var(X) = Var(Y ) = 1. It is natural to consider the Rényi (1959) requirements again, in particular, the re- quirements (iv) and (vi). All of the distance functionals considered will be of type (4.12) \u0012 = ∫ B{ fX(x), fY (y), fX,Y (x, y)} × fX,Y (x, y) dx dy, where B is a real-valued function such that the integral exists. If B is of the form B(z1,z2,z3) = D(z1z2/z3),we have (4.13) \u0012 = ∫ D{ fX(x)fY (y) fX,Y (x, y) } fX,Y (x, y) dx dy which by the change of variable formula for integrals is seen to have the Rényi property (vi). Moreover, if D(w) = 0 if and only if w = 1, then Rényi property (iv) is fulﬁlled. If D(1) = 0and D is convex, then D is a so-called f - divergence (Csiszár, 1967) measure with f = D. Several well-known distance measures for density functions are of this type. For instance, letting D(w) = 2(1 − w1/2), we obtain the Hellinger distance H = ∫ {√ fX,Y (x, y) − √ fX(x)fY (y)}2 dx dy = 2 ∫ {1 − √ fX(x)fY (y) fX,Y (x, y) }fX,Y (x, y) dx dy between fX,Y and fXfY . The Hellinger distance is a met- ric, and hence satisﬁes the Rényi property (iv). The familiar Kullback–Leibler information (entropy) distance is obtained by taking D(w) =− ln w, (4.14) I = ∫ ln{ fX,Y (x, y) fX(x)fY (y) }fX,Y (x, y) dx dy. Since this distance is of type (4.13), it satisﬁes (vi). A very recent paper linking I with other recent approaches to in- dependence testing is Berrett and Samworth (2019). Tak- ing D(w) = w2 − 1 yields the χ 2-divergence; see also the test of ﬁt distance in Bickel and Rosenblatt (1973). All of the above measures are trivially extended to two arbitrary multivariate densities. However, estimating such densities in high or moderate dimensions may be difﬁcult due to the curse of dimensionality. A functional built up from pairwise dependencies can be considered instead. For a given functional \u0012 = \u0012(f, g) depending on two densities f and g, \u0012 may be estimated by ̂\u0012 = \u0012( ̂f, ̂g). There are several ways of estimating the densities, for ex- ample, the kernel density estimator, ̂fX(x) = 1 n n∑ i=1 Kb(x − Xi) for given observations {X1,...,Xn}. Here, Kb(x − Xi) = b−pK{b−1(x − Xi)},where b is the bandwidth (generally amatrix), K is the kernel function and p is the dimension of Xi. It should be pointed out that there are often differ- ent estimators of \u0012(f, g) that are much easier to calculate and have better theoretical properties. For example, in the case of \u0012 = I , one can consider the KSG-estimator; see Kraskov, Stögbauer and Grassberger (2004). Under regularity conditions (see, e.g., Skaug and Tjøs- theim, 1996), consistency and asymptotic normality un- der the null hypothesis of independence can be obtained for the estimated test functionals. Berrett and Samworth (2019) have demonstrated that local asymptotic power properties can also sometimes be proved. It should be noted that the leading term in an asymptotic expansion of the standard deviation of ̂\u0012 for the estimated Kullback– Leibler functional ̂I and the estimated Hellinger func- tional ̂H is of order O(n−1/2). This is, of course, the same as for the standard deviation of a parametric esti- mate in a parametric estimation problem. In that situa- tion, the next term of the Edgeworth expansion is of order O(n−1), and for moderately large values of n the ﬁrst- order term n−1/2 will dominate. However, for the func- tionals considered above, using density estimates and due to the presence of an n-dependent bandwidth, the next terms in the Edgeworth expansion are much closer, be- ing of order O(n−1/2b) and O({nb}−1), and since typi- cally b = O(n−1/6) or O(n−1/5), n must be very large indeed to have the ﬁrst term dominate in the asymptotic expansion. As a consequence, ﬁrst-order asymptotics in terms of the normal approximation cannot be expected to work well unless n is exceedingly large. In this sense, the situation is quite different from the empirical functionals treated in the previous sections, where there is no band- width parameter involved. All of this suggests the use of the bootstrap or permutations as an alternative for con- structing the null distribution. 102 D. TJØSTHEIM, H. OTNEIM AND B. STØVE 4.6 Global Test Functionals Generated by Local Dependence Relationships If one has bivariate normal data with standard normal marginals and ρ = 0, one gets observations scattered in a disc-like region around zero, and most test functionals will easily recognize this as a situation of independence. However, as pointed out by Heller, Heller and Gorﬁne (2013), if data are generated along a circle with radius r, for example, X2 + Y 2 = r 2 + ϵ for some stochastic noise variable ϵ,then X and Y are dependent, but as reported by Heller, Heller and Gorﬁne (2013), in practice, the dcov, and some other nonlinear global test functionals, do not work well. Heller, Heller and Gorﬁne (2013) point a way out of this difﬁculty, namely by looking at dependence locally (along the circle) and then aggregate the depen- dence by integrating, or by other means, over the local regions. There are, of course, several ways of measuring local dependence and we will approach this problem more fundamentally in Section 5. Another paper in this category, Reshef et al. (2011), is published in Science. The idea behind their MIC (Maxi- mal Information Coefﬁcient) statistic consists in comput- ing the mutual information I as deﬁned in (4.14) locally over a grid in the data set and then take as statistic the maximum value of these local information measures as obtained by maximizing over a suitable choice of grid. Some limitations of the method are identiﬁed in a later ar- ticle by Reshef et al. (2013) and it should also be pointed out that Kinney and Atwal (2014) ﬁnd serious problems with the paper. See also Gorﬁne, Heller and Heller (2012). Finally, the so-called BDS test named after its origina- tors Brock et al. (1996) should be mentioned. This test has a local ﬂavor at its basis, but the philosophy is a bit dif- ferent from the other tests presented here. The BDS test attracted much attention among econometricians in the 1990s, and it has since been improved by Genest, Ghoudi and Rémillard (2007). 5. BEYOND PEARSON’S ρ: LOCAL DEPENDENCE The test functionals treated in Section 4 deal with the second aspect of modeling dependence stated in the be- ginning of that section, namely that of testing of inde- pendence. These functionals all do so by the computation of one nonnegative number, which is derived from local properties in Section 4.5. This number properly scaled may possibly be said to deal with the ﬁrst aspect stated, namely that of measuring the strength of the dependence. But, as such, it may be faulted in several ways. Unlike the Pearson ρ, these functionals do not distinguish between positive and negative dependence, and they are not local. A local dependence measure between two stochastic variables X and Y can be deﬁned as a measure based on the joint cumulative distribution function (or the joint den- sity function in case it exists) for X and Y restricted to a local region R. In ﬁnance, it is of special interest to look at the local dependence when R is the tail region. If the joint cumulative distribution FX,Y is very different from the product FXFY of the marginals in R, this can be taken as an indication of strong local dependence between X and Y in R. As is the case for a global dependence measure, there are many ways of deﬁning a local dependence mea- sure. The local Gaussian correlation deﬁned in the next section is just one possibility. The local region R can be determined by a bandwidth parameter or by some other regional distance measure. Accumulating a local measure over the entire space leads to a global measure as in, for example, (4.3)and (4.4), or as in the distance function- als of Section 4.5. It is also possible to shrink the region R to a point (x, y), and get a local value of the measure at that point, as is done for the local Gaussian correlation ρX,Y (x, y) in Section 6.1 or implicitly as in, for example, (4.3)and (4.4). In Section 6, the main story will be the treatment of a local Gaussian correlation which in a sense returns to the Pearson ρ, but a local version of ρ, which satisﬁes many of the Rényi (1959) requirements, and which is signed. But ﬁrst, in the present section, we go back to some earlier attempts. We start with a remarkable paper by Lehmann (1966), who manages to deﬁne positive and negative de- pendence in quite a general nonlinear situation. 5.1 Quadrant Dependence Lehmann’s theory is based on the concept of quad- rant dependence. Consider two random variables X and Y with cumulative distribution FX,Y . Then the pair (X, Y ) or its distribution function FX,Y is said to be positively quadrant dependent if (5.1) P(X ≤ x, Y ≤ y) ≥ P(X ≤ x)P (Y ≤ y) for all (x, y). Similarly, (X, Y ) or FX,Y is said to be negatively quadrant dependent if (5.1) holds with the central inequality sign reversed. The connection between quadrant dependence and Pearson’s ρ is secured through a lemma of Hoeffding (1940). The lemma is a general result and resembles the result by Székely (2002) in his treatment of the so-called Cramér functional, a forerunner of the Cramér–von Mises functional. If FX,Y denotes the joint and FX and FY the marginals, then assuming that the necessary moments ex- ist, E(XY ) − E(X)E(Y ) = ∫ ∞ −∞ ∫ ∞ −∞ (FXY (x, y) − FX(x)FY (y)) dx dy. It follows immediately from deﬁnitions that if (X, Y ) is positively quadrant dependent (negatively quadrant de- pendent), then for Pearson’s ρ, ρ ≥ 0(ρ ≤ 0). Similarly, STATISTICAL DEPENDENCE 103 it is shown by Lehmann that if FX,Y is positively quad- rant dependent, then Kendall’s τ , Spearman’s ρS and the quadrant measure q deﬁned by Blomqvist (1950)are all nonnegative. An analogous result holds in the negatively quadrant dependent case. Lehmann (1966) introduced two additional and stronger concepts of dependence, namely regression dependence and likelihood ratio dependence; see his paper for details. 5.2 Local Measures of Dependence As mentioned already, econometricians have long looked for a formal statistical way of describing the shift- ing region-like dependence structure of ﬁnancial markets. It is obvious that when the market is going down there is a stronger dependence between ﬁnancial objects, and very strong in case of a panic. Similar effects, but per- haps not quite so strong, appear when the market is going up. But how should it be quantiﬁed and measured? This is important in ﬁnance, not the least in portfolio theory, where it is well known (see, e.g., Taleb, 2007), that or- dinary Gaussian description does not work, and if used, may lead to catastrophic results. Mainly two approaches have been used among econometricians. The ﬁrst is non- local and consists simply in using copula theory, but it may not always be so easy to implement in a time series and portfolio context. The other approach is local and is to use “conditional correlation” as in Silvapulle and Granger (2001) and Forbes and Rigobon (2002). One then com- putes an estimate as in (1.1) of Pearson’s ρ but in various regions of the sample space, for example, in the tail of the distribution of two variables. However, this estimate suffers from a serious bias, which is obvious by using the ergodic theorem or the law of large numbers, in the sense that for a Gaussian distri- bution it does not converge to ρ. This is unfortunate be- cause if the data happen to be Gaussian, one would like estimated correlations to be close or identical to ρ in or- der to approximate the classic Gaussian portfolio theory of Markowitz (1952). This requirement is consistent with Rényi’s property (vii). Statisticians have also tried various other ways of de- scribing local dependence. Bjerve and Doksum (1993) suggested a local measure of dependence, the correla- tion curve, based on localizing ρ by conditioning on X in a nonlinear regression model. The resulting correlation curve inherits many of the properties of ρ, and it succeeds in several of the cases where ρ fails to detect dependence, such as the parabola (2.2) in Section 2.3. However, unlike ρ, it is not symmetric in (X, Y ). Conditioning and regres- sion on Y would in general produce a different result. This brings out the difference between regression analysis and multivariate analysis, where ρ is a concept of the latter, which happens to enter into the ﬁrst. Bjerve and Doksum do propose a solution to this dilemma, but it is an ad hoc one. Heller, Heller and Gorﬁne (2013) used local contin- gency type arguments to construct a global test functional. Such reasoning goes further back in time. Holland and Wang (1987) used such arguments to obtain a local de- pendence function γ (x, y) = ∂ 2 ∂x∂y ln f (x, y), where f is the density function of (X, Y ). Implicitly it is assumed here that both mixed second-order partial deriva- tives exist and are continuous. For an alternative deriva- tion based on limiting arguments of local covariance func- tions and for properties and extensions, we refer to Jones (1996), Jones and Koch (2003)and Inci,Liand McCarthy (2011). The local dependence function γ does not take values between −1 and 1, and it does not reduce to ρ in the Gaus- sian bivariate case. Actually, in that case γ (x, y) = ρ 1 − ρ2 1 σXσY . 6. BEYOND PEARSON’S ρ: LOCAL GAUSSIAN CORRELATION The Pearson ρ gives a complete characterization of dependence in a bivariate Gaussian distribution but, as has been seen, not for a general density f (x, y) for two random variables X and Y . The idea of the Lo- cal Gaussian Correlation (LGC), introduced in Tjøstheim and Hufthammer (2013) is to approximate f locally in a neighborhood of a point (x, y) by a bivariate Gaus- sian distribution ψx,y(u, v),where (u, v) are running vari- ables. In this neighborhood, one gets close to a complete local characterization of dependence using the local cor- relation ρ(x, y), which is the Pearson’s ρ of the bivariate Gaussian ψx,y(u, v). Its precision depends on the size of the neighborhood and, of course, on the properties of the density at the point (x, y). In practice, it has to be reason- ably smooth. This section and the online supplement give a survey of some of the results obtained so far. 6.1 Deﬁnition and Examples For notational convenience in this section, we write (x1,x2) instead of (x, y), and, by a slight inconsis- tency of notation, x = (x1,x2). Similarly, (u, v) is re- placed by v = (v1,v2). Then, in this notation, letting μ(x) = (μ1(x), μ2(x)) be the mean vector of ψ, σ(x) = (σ1(x), σ2(x)) the vector of standard deviations and ρ(x) the correlation of ψ, the approximating density ψ is given by ψ ( v, μ1(x), μ2(x), σ 2 1 (x), σ 2 2 (x), ρ(x)) = 1 2πσ1(x)σ2(x)√ 1 − ρ2(x) 104 D. TJØSTHEIM, H. OTNEIM AND B. STØVE × exp[ − 1 2 1 1 − ρ2(x) ( (v1 − μ1(x))2 σ 2 1 (x) − 2ρ(x) (v1 − μ1(x))(v2 − μ2(x)) σ1(x)σ2(x) + (v2 − μ2(x))2 σ 2 2 (x) )]. Moving to another point y = (y1,y2) in general gives another approximating normal distribution ψy depending on a new set of parameters {μ1(y), μ2(y), σ1(y), σ2(y), ρ(y)}. An exception is the case where f itself is Gaus- sian with parameters {μ1,μ2,σ1,σ2,ρ}, in which case {μ1(x), μ2(x), σ1(x), σ2(x), ρ(x)}≡{μ1,μ2,σ1,σ2,ρ}. This means that the bias of the conditional correlation de- scribed in Section 5 is avoided and it means that the prop- erty (vii) in Rényi (1959)’s scheme is satisﬁed (and indeed (vii′) as well). To make this into a construction that can be used in practice, it is convenient to deﬁne the vector population parameter θ(x) . ={μ1(x), μ2(x), σ1(x), σ2(x), ρ(x)} and estimate it. Fortunately, this is a problem that has been treated in larger generality by Hjort and Jones (1996)and Loader (1996). They looked at the problem of approxi- mating f(x) with a general parametric family of densi- ties, the Gaussian being one such family. Here, x in prin- ciple can have a dimension ranging from 1 to p, but with p = 1 mostly covered in those publications. They were concerned with estimating f rather than the local parame- ters, one of which is the local Gaussian correlation (LGC) ρ(x). But ﬁrst we need a more precise deﬁnition of θ(x).This can be done in two stages using a neighborhood deﬁned by bandwidths b = (b1,b2) in the (x1,x2) direction, and then letting b → 0 componentwise. A suitable function measuring the difference between f and ψ is deﬁned by (6.1) q = ∫ Kb(v − x) [ψ (v, θ (x)) − ln{ ψ ( v, θ (x))}f(v)] dv, where Kb(v − x) = (b1b2)−1K1(b−1 1 (v1 − x1)) × K2(b−1 2 (v2 − x2)) is a product kernel. As is seen in Hjort and Jones (1996), pages 1623–1624, the expression in (6.1) can be interpreted as a locally weighted Kullback– Leibler distance from f to ψ(·, θ (x)). We then obtain that the minimizer θb(x) (also depending on K) should satisfy (6.2) ∫ Kb(v − x) ∂ ∂θj [ln{ψ ( v, θ (x))}f(v) − ψ ( v, θ (x))] dv = 0,j = 1,..., 5. In the ﬁrst stage, we deﬁne the population value θb(x) as the minimizer of (6.1), assuming that there is a unique solution to (6.2). The deﬁnition of θb(x) and the assump- tion of uniqueness are essentially identical to those used in Hjort and Jones (1996) for more general parametric fam- ilies of densities. In the next stage, we let b → 0 and consider the limit- ing value θ(x) = limb→0 θb(x). This is in fact considered indirectly by Hjort and Jones (1996) on pages 1627–1630 and more directly in Tjøstheim and Hufthammer (2013), both using Taylor expansion arguments. In the following, we will assume that a limiting value θ(x) independent of b and K exists. (It is possible to avoid the problem of a population value altogether if one takes the view of some of the publications cited in Section 4.6 by just estimating a suitable dependence function.) In estimating θ(x) and θb(x), a neighborhood with a ﬁnite bandwidth has to be used in analogy with nonpara- metric density estimation. The estimate ̂θ(x) = ̂θb(x) is obtained from maximizing a local likelihood. Given ob- servations X1,...,Xn, the local log likelihood is deter- mined by L (X1,...,Xn,θ (x) ) = n−1 n∑ i=1 Kb(Xi − x) ln ψ ( Xi,θ (x) ) − ∫ Kb(v − x)ψ ( v, θ (x)) dv. The last (and perhaps somewhat unexpected) term is es- sential, as it implies that ψ(x, θb(x)) is not allowed to stray far away from f(x) as b → 0. It is also discussed at length in Hjort and Jones (1996). (When b →∞,the last term has 1 as its limiting value and the likelihood reduces to the ordinary global log-likelihood.) Using the notation, uj (·,θ ) . = ∂ ∂θj ln ψ(·,θ ), by the law of large numbers, or by the ergodic the- orem in the time series case, assuming E{Kb(Xi − x) ln ψ(Xi,θb(x))} < ∞, we have almost surely ∂L ∂θj = n−1 ∑ i Kb(Xi − x)uj (Xi,θb(x)) − ∫ Kb(v − x)uj (v, θb(x))ψ (v, θb(x)) dv → ∫ Kb(v − x)uj (v, θb(x)) × [f(v) − ψ (v, θb(x))] dv. (6.3) Putting the expression in the ﬁrst line of (6.3) equal to zero yields the local maximum likelihood estimate ̂θb(x) = ̂θ(x) of the population value θb(x), which satisﬁes (6.2). We see the importance of the additional last term in the local likelihood by letting b → 0, Taylor expanding and requiring ∂L/∂θj = 0, which leads to uj ( x, θb(x))[f(x) − ψ ( x, θb(x))] + O(bT b) = 0, STATISTICAL DEPENDENCE 105 where bT is the transposed of b. It is seen that ignoring solutions that yield uj (x, θb(x)) = 0 requires ψ(x, θb(x)) to be close to f(x). An asymptotic theory has been developed in Tjøstheim and Hufthammer (2013)for ̂θb(x) for the case that b is ﬁxed and for ̂θ(x) in the case that b → 0. Theﬁrstcaseis much easier to treat than the second one. In fact, for the ﬁrst case the theory of Hjort and Jones (1996) can be taken over almost directly, although it is extended to the ergodic time series case in Tjøstheim and Hufthammer (2013). In the case that b → 0, this leads to a slow convergence rate of (n(b1b2)3)−1/2, which is the same convergence rate as for the the estimated dependence function treated in Jones (1996). The local correlation is clearly dependent on the mar- ginal distributions of X1 and X2 as is Pearson’s ρ.This marginal dependence can be removed by scaling the ob- servations to a standard normal scale. As mentioned in Section 3 about the copula, the dependence structure is disentangled from the marginals by Sklar’s theorem. For the purpose of measuring local dependence in terms of the local Gaussian correlation, at least for a number of pur- poses it is advantageous to replace a scaling with uniform variables Ui = Fi(Xi) by standard normal variables (6.4) Z = (Z1,Z2) = (\u0010 −1(F1(X1) ),\u0010 −1( F2(X2) )), where \u0010 is the cumulative distribution of the standard normal distribution. The local Gaussian correlation on the Z-scale will be denoted by ρZ(z). Of course, the vari- able Z cannot be computed via the transformation (6.4) without knowledge of the margins F1 and F2,but these can be estimated by the empirical distribution function. Extensive use has been made of ρZ(z1,z2),orrather ρ̂Z(z1,z2) with ̂Zi = \u0010−1( ̂Fi). Under certain regular- ity conditions, as in the copula case, the difference be- tween Z and ̂Z can be ignored in limit theorems. Us- ing the sample of pairs of Gaussian pseudo observations {\u0010−1( ̂F1(X1i), \u0010−1( ̂F2(X2i)}, i = 1,...,n, one can es- timate ρZ(z1,z2) by local log likelihood as described above. Under regularity conditions, the asymptotic theory will be the same as in Tjøstheim and Hufthammer (2013). In Otneim and Tjøstheim (2017, 2018), a further simpli- ﬁcation is made by taking μZi (z) ≡ 0and σZi (z) ≡ 1, in which case the asymptotic theory simpliﬁes and one ob- tains the familiar nonparametric rate of O((nb1b2)−1/2) for ̂ρ̂Z(z). The choice of Gaussian margins in the transformation (6.4) is not made without a purpose. It is natural since we are dealing with local Gaussian approximations. This leads to a more fundamental question: Why is a local Gaussian approximation and an associated local Gaussian correlation measure particularly useful? 6.2 Why Local Gaussian Approximation and Local Gaussian Correlation? In principle, another parametric family could be used as a local approximation (as has been done by Hjort and Jones, 1996) in their consideration of locally parametric density estimation. The advantage of using the Gaussian distribution as an approximating family is its powerful and unique properties. Among them is the fact that the entire dependence structure of a multivariate Gaussian is determined by its set of pairwise correlations, and the fact that for a multivariate Gaussian the conditional distribu- tion of one set of variables given another set of variables is again Gaussian. The idea, or the statistical modeling phi- losophy, of the local Gaussian approximation is that the unique properties of the Gaussian can be extended to non- Gaussian distributions, but locally. This can be shown to be useful in a number of different situations as follows. Description of dependence and corresponding tests of independence have been given for pairs of i.i.d. vari- ables, for single time series, and for pairs of time series, including the use of local Gaussian autocorrelation, in Berentsen and Tjøstheim (2014), and in Lacal and Tjøs- theim (2017, 2019) Applications to econometric data are given in Støve, Tjøstheim and Hufthammer (2014) and Støve and Tjøs- theim (2014). In particular, for multivariate ﬁnancial data, one can measure the increasing values of pairwise local Gaussian correlations in a market during an economic downturn. This describes quantitatively the well-known fact that ﬁnancial objects perform similarly (stronger mu- tual positive dependence) in such a situation. In the ex- treme scenario of a panic, the local Gaussian correlations would approach 1; see also Nguyen et al. (2020). A local Gaussian conditional distribution allows the in- troduction of a local Gaussian partial correlation, and density and conditional density estimation, as well as tests of conditional independence treated by Otneim and Tjøs- theim (2017, 2018, 2021). Locally Gaussian spectral estimation is contained in Jordanger and Tjøstheim (2020). They have shown that nonlinear and local oscillatory behavior can be detected in cases where it is missed in ordinary spectral analysis. Finally, relationships to the copula concept have been investigated in Berentsen et al. (2014), and applications to discrimination using a local Fisher discriminant have been explored in Otneim, Jullum and Tjøstheim (2020). There are three R-packages; Berentsen, Kleppe and Tjøstheim (2014), Jordanger (2020) and Otneim (2019). All of these developments are being collected in a forthcoming book, Tjøstheim, Otneim and Støve (2021). Due to lack of space, these developments cannot be de- scribed in more detail in the main part of this paper, but for the reader’s convenience, we give a brief summary in the online supplementary material. Further, there are plots of 106 D. TJØSTHEIM, H. OTNEIM AND B. STØVE the local Gaussian correlation in simulation experiments, and for the ﬁnancial return data of Figure 2. Finally, point- ers are given to where the local Gaussian correlation is compared in testing situations with the dcov statistic from Section 4.3 and with the ordinary global Pearson’s ρ. ACKNOWLEDGMENTS We are grateful to the Editor and to two anonymous ref- erees for a number of valuable comments and suggestions. FUNDING This work has been partially supported by the Finance Market Fund (Norway). SUPPLEMENTARY MATERIAL Supplement to “Statistical Dependence: Beyond Pearson’s ρ” (DOI: 10.1214/21-STS823SUPP; .pdf). The supplementary material consists of four sections. In Section 1 of the supplementary material we give some more details of Section 4.2 of the main article concerning the measuring and tests based on the distribution function in the time series case. Section 2 gives more details for the local Gaussian correlation in the time series and cop- ula case, and it contains some simulation experiments and a real data example. Section 3 gives some additional sym- metry properties of the local Gaussian correlation and a discussion of the Rényi criteria relative to this measure of dependence. Finally, Section 4 contains a brief overview of the use of the local Gaussian correlation in testing of independence. REFERENCES AAS,K., CZADO,C., FRIGESSI,A. and BAKKEN, H. (2009). Pair-copula constructions of multiple dependence. Insurance Math. Econom. 44 182–198. MR2517884 https://doi.org/10.1016/ j.insmatheco.2007.02.001 ARONSZAJN, N. (1950). Theory of reproducing kernels. Trans. Amer. Math. Soc. 68 337–404. MR0051437 https://doi.org/10. 2307/1990404 BERENTSEN,G. D., KLEPPE,T. and TJØSTHEIM, D. (2014). Intro- ducing localgauss, an R-package for estimating and visualizing lo- cal Gaussian corelation. J. Stat. Softw. 56 1–18. BERENTSEN,G. D. and TJØSTHEIM, D. (2014). Recognizing and vi- sualizing departures from independence in bivariate data using lo- cal Gaussian correlation. Stat. Comput. 24 785–801. MR3229697 https://doi.org/10.1007/s11222-013-9402-8 BERENTSEN,G. D., STØVE,B., TJØSTHEIM,D. and NORDBØ,T. (2014). Recognizing and visualizing copulas: An approach us- ing local Gaussian approximation. Insurance Math. Econom. 57 90–103. MR3225330 https://doi.org/10.1016/j.insmatheco.2014. 04.005 BERGSMA,W. and DASSIOS, A. (2014). A consistent test of indepen- dence based on a sign covariance related to Kendall’s tau. Bernoulli 20 1006–1028. MR3178526 https://doi.org/10.3150/13-BEJ514 BERLINET,A. and THOMAS-AGNAN, C. (2004). Reproducing Ker- nel Hilbert Spaces in Probability and Statistics. Kluwer Aca- demic, Boston, MA. With a preface by Persi Diaconis. MR2239907 https://doi.org/10.1007/978-1-4419-9096-9 BERRETT,T. B. and SAMWORTH, R. J. (2019). Nonparametric inde- pendence testing via mutual information. Biometrika 106 547–566. MR3992389 https://doi.org/10.1093/biomet/asz024 BICKEL,P. J. and ROSENBLATT, M. (1973). On some global mea- sures of the deviations of density function estimates. Ann. Statist. 1 1071–1095. MR0348906 BILODEAU,M. and LAFAYE DE MICHEAUX, P. (2009). A- dependence statistics for mutual and serial independence of cat- egorical variables. J. Statist. Plann. Inference 139 2407–2419. MR2508002 https://doi.org/10.1016/j.jspi.2008.11.006 BILODEAU,M. and NANGUE, A. G. (2017). Tests of mutual or serial independence of random vectors with applications. J. Mach. Learn. Res. 18 Paper No. 74. MR3714237 BJERVE,S. and DOKSUM, K. (1993). Correlation curves: Measures of association as functions of covariate values. Ann. Statist. 21 890– 902. MR1232524 https://doi.org/10.1214/aos/1176349156 BLOMQVIST, N. (1950). On a measure of dependence between two random variables. Ann. Math. Stat. 21 593–600. MR0039190 https://doi.org/10.1214/aoms/1177729754 BLUM,J. R., KIEFER,J. and ROSENBLATT, M. (1961). Distribution free tests of independence based on the sample distribution func- tion. Ann. Math. Stat. 32 485–498. MR0125690 https://doi.org/10. 1214/aoms/1177705055 BÖTTCHER,B., KELLER-RESSEL,M. and SCHILLING,R. L. (2019). Distance multivariance: New dependence measures for random vectors. Ann. Statist. 47 2757–2789. MR3988772 https://doi.org/10.1214/18-AOS1764 BROCK,W.A., SCHEINKMAN,J. A., DECHERT,W. D. and LEBARON, B. (1996). A test for independence based on the cor- relation dimension. Econometric Rev. 15 197–235. MR1410877 https://doi.org/10.1080/07474939608800353 CSISZÁR, I. (1967). Information-type measures of difference of prob- ability distributions and indirect observations. Studia Sci. Math. Hungar. 2 299–318. MR0219345 CSÖRG ˝O, S. (1985). Testing for independence by the empirical char- acteristic function. J. Multivariate Anal. 16 290–299. MR0793494 https://doi.org/10.1016/0047-259X(85)90022-3 DATASTREAM (2018). Subscription service. Accessed June 2018. DAVIS,R.A., MATSUI,M., MIKOSCH,T. and WAN, P. (2018). Ap- plications of distance correlation to time series. Bernoulli 24 3087– 3116. MR3779711 https://doi.org/10.3150/17-BEJ955 DEHEUVELS, P. (1981a). A Kolmogorov–Smirnov type test for in- dependence and multivariate samples. Rev. Roumaine Math. Pures Appl. 26 213–226. MR0616038 DEHEUVELS, P. (1981b). An asymptotic decomposition for multivari- ate distribution-free tests of independence. J. Multivariate Anal. 11 102–113. MR0612295 https://doi.org/10.1016/0047-259X(81) 90136-6 DUECK,J., EDELMANN,D., GNEITING,T. and RICHARDS,D. (2014). The afﬁnely invariant distance correlation. Bernoulli 20 2305–2330. MR3263106 https://doi.org/10.3150/13-BEJ558 ENGLE, R. F. (1982). Autoregressive conditional heteroscedastic- ity with estimates of the variance of United Kingdom inﬂation. Econometrica 50 987–1007. MR0666121 https://doi.org/10.2307/ 1912773 ESCANCIANO,J. C. and HUALDE, J. (2019). Measuring asset mar- ket linkages: Nonlinear dependence and tail risk. J. Bus. Econom. Statist. 1–25. ESCANCIANO,J. C. and VELASCO, C. (2006). Generalized spectral tests for the martingale difference hypothesis. J. Econometrics 134 STATISTICAL DEPENDENCE 107 151–185. MR2328319 https://doi.org/10.1016/j.jeconom.2005.06. 019 FERRATY,F. and VIEU, P. (2006). Nonparametric Functional Data Analysis: Theory and Practice. Springer Series in Statistics. Springer, New York. MR2229687 FISHER, R. A. (1915). Frequency distribution of the values of the cor- relation coefﬁcient in samples of an indeﬁnitely large population. Biometrika 10 507–521. FISHER, R. A. (1921). On the probable error of a coefﬁcient of corre- lation deduced from a small sample. Metron 1 3–32. FOKIANOS,K. and PITSILLOU, M. (2017). Consistent testing for pairwise dependence in time series. Technometrics 59 262–270. MR3635048 https://doi.org/10.1080/00401706.2016.1156024 FORBES,K.J. and RIGOBON, R. (2002). No contagion, only inter- dependence: Measuring stock market comovements. J. Finance 57 2223–2261. FRANCQ,C. and ZAKOÏAN, J.-M. (2011). GARCH Models: Struc- ture, Statistical Inference and Financial Applications. Wiley, Chichester. MR3186556 https://doi.org/10.1002/9780470670057 GALTON, F. (1888). Co-relations and their measurement, chieﬂy from anthropometric data. Proc. Roy. Soc. Lond. 45 135–145. GALTON, F. (1890). Kinship and correlation. N. Amer. Rev. 150 419– 431. GEBELEIN, H. (1941). Das statistische Problem der Korrelation als Variations- und Eigenwertproblem und sein Zusammenhang mit der Ausgleichsrechnung. ZAMM Z. Angew. Math. Mech. 21 364– 379. MR0007220 https://doi.org/10.1002/zamm.19410210604 GENEST,C., GHOUDI,K. and RÉMILLARD, B. (2007). Rank-based extensions of the Brock, Dechert, and Scheinkman test. J. Amer. Statist. Assoc. 102 1363–1376. MR2372539 https://doi.org/10. 1198/016214507000001076 GENEST,C. and NEŠLEHOVÁ, J. (2007). A primer on copulas for count data. Astin Bull. 37 475–515. MR2422797 https://doi.org/10. 2143/AST.37.2.2024077 GENEST,C., KOJADINOVIC,I., NEŠLEHOVÁ,J. and YAN,J. (2011). A goodness-of-ﬁt test for bivariate extreme-value copu- las. Bernoulli 17 253–275. MR2797991 https://doi.org/10.3150/ 10-BEJ279 GHOUDI,K. and RÉMILLARD, B. (2018). Serial independence tests for innovations of conditional mean and variance models. TEST 27 3–26. MR3764021 https://doi.org/10.1007/s11749-016-0521-3 GÓMEZ,E., GÓMEZ-VILLEGAS,M. A. and MARÍN, J. M. (2003). A survey on continuous elliptical vector distributions. Rev. Mat. Complut. 16 345–361. MR2031887 https://doi.org/10.5209/rev_ REMA.2003.v16.n1.16889 GORFINE,M., HELLER,R. and HELLER, Y. (2012). Comment on “Detecting novel associations in large data sets” by Reshef et al., Science Dec 16, 2011. GRANGER,C.W., MAASOUMI,E. and RACINE, J. (2004). A de- pendence metric for possibly nonlinear processes. J. Time Se- ries Anal. 25 649–669. MR2086354 https://doi.org/10.1111/j. 1467-9892.2004.01866.x GRETTON, A. (2019). Introduction to RKHS, and some simple kernel algorithms. Unpublished manuscript, Lecture Notes Gatsby Com- putational Neuroscience Unit. GRETTON,A. and GYÖRFI, L. (2010). Consistent nonparametric tests of independence. J. Mach. Learn. Res. 11 1391–1423. MR2645456 GRETTON,A. and GYÖRFI, L. (2012). Strongly consistent nonpara- metric test of conditional independence. J. Multivariate Anal. 82 1145–1150. GRETTON,A., BOUSQUET,O., SMOLA,A. and SCHÖLKOPF,B. (2005). Measuring statistical dependence with Hilbert–Schmidt norms. In Algorithmic Learning Theory (S. Jain, U. Simon and E. Tomita, eds.). Lecture Notes in Computer Science 3734 63–77. Springer, Berlin. MR2255909 https://doi.org/10.1007/11564089_7 HELLER,R., HELLER,Y. and GORFINE, M. (2013). A consis- tent multivariate test of association based on ranks of distances. Biometrika 100 503–510. MR3068450 https://doi.org/10.1093/ biomet/ass070 HJORT,N. L. and JONES, M. C. (1996). Locally parametric nonpara- metric density estimation. Ann. Statist. 24 1619–1647. MR1416653 https://doi.org/10.1214/aos/1032298288 HOEFFDING, W. (1948). A non-parametric test of independence. Ann. Math. Stat. 19 546–557. MR0029139 https://doi.org/10.1214/ aoms/1177730150 HOEFFDING, W. (1963). Probability inequalities for sums of bounded random variables. J. Amer. Statist. Assoc. 58 13–30. MR0144363 HÖFFDING, W. (1940). Maszstabinvariante Korrelationstheorie. Schr. Math. Inst. U. Inst. Angew. Math. Univ. Berlin 5 181–233. MR0004426 HOLLAND,P. W. and WANG, Y. J. (1987). Dependence func- tion for continuous bivariate densities. Comm. Statist. The- ory Methods 16 863–876. MR0886560 https://doi.org/10.1080/ 03610928708829408 HONG, Y. (1999). Hypothesis testing in time series via the em- pirical characteristic function: A generalized spectral density approach. J. Amer. Statist. Assoc. 94 1201–1220. MR1731483 https://doi.org/10.2307/2669935 HONG, Y. (2000). Generalized spectral tests for serial dependence. J. R. Stat. Soc. Ser. B. Stat. Methodol. 62 557–574. MR1772415 https://doi.org/10.1111/1467-9868.00250 HONG,Y. and WHITE, H. (2005). Asymptotic distribution the- ory for nonparametric entropy measures of serial dependence. Econometrica 73 837–901. MR2135144 https://doi.org/10.1111/j. 1468-0262.2005.00597.x HUANG, T.-M. (2010). Testing conditional independence using max- imal nonlinear conditional correlation. Ann. Statist. 38 2047–2091. MR2676883 https://doi.org/10.1214/09-AOS770 INCI,A.C., LI,H.-C. and MCCARTHY, J. (2011). Financial conta- gion: A local correlation analysis. Res. Int. Bus. Finance 25 11–25. JENTSCH,C., LEUCHT,A., MEYER,M. and BEERING, C. (2020). Empirical characteristic functions-based estimation and distance correlation for locally stationary processes. J. Time Series Anal. 41 110–133. MR4048684 https://doi.org/10.1111/jtsa.12497 JOE, H. (2014). Dependence Modeling with Copulas. Chapman & Hall, London. JONES, M. C. (1996). The local dependence function. Biometrika 83 899–904. MR1440052 https://doi.org/10.1093/biomet/83.4.899 JONES,M.C. and KOCH, I. (2003). Dependence maps: Local de- pendence in practice. Stat. Comput. 13 241–255. MR1982478 https://doi.org/10.1023/A:1024270700807 JORDANGER, L. A. (2020). LocalgaussSpec. Available at https:// github.com/LAJordanger/localgaussSpec. JORDANGER,L.A. and TJØSTHEIM, D. (2020). Nonlinear spectral analysis: A local Gaussian approach. J. Amer. Statist. Assoc. 1–55. KENDALL, M. G. (1938). A new measure of rank correlation. Biometrika 30 81–89. KING, M. L. (1987). Testing for autocorrelation in linear regression models: A survey. In Speciﬁcation Analysis in the Linear Model (M.L.King andD.E.A.Giles,eds.). Internat. Lib. Econom. 19– 73. Routledge, London. MR0899966 KINNEY,J.B. and ATWAL, G. S. (2014). Equitability, mutual in- formation, and the maximal information coefﬁcient. Proc. Natl. Acad. Sci. USA 111 3354–3359. MR3200177 https://doi.org/10. 1073/pnas.1309933111 KLAASSEN,C.A.J. and WELLNER, J. A. (1997). Efﬁcient esti- mation in the bivariate normal copula model: Normal margins are least favourable. Bernoulli 3 55–77. MR1466545 https://doi.org/10. 2307/3318652 108 D. TJØSTHEIM, H. OTNEIM AND B. STØVE KOJADINOVIC,I. and HOLMES, M. (2009). Tests of independence among continuous random vectors based on Cramér–von Mises functionals of the empirical copula process. J. Multivariate Anal. 100 1137–1154. MR2508377 https://doi.org/10.1016/j.jmva.2008. 10.013 KRASKOV,A., STÖGBAUER,H. and GRASSBERGER, P. (2004). Estimating mutual information. Phys. Rev. E (3) 69 066138. MR2096503 https://doi.org/10.1103/PhysRevE.69.066138 LACAL,V. and TJØSTHEIM, D. (2017). Local Gaussian autocorre- lation and tests for serial independence. J. Time Series Anal. 38 51–71. MR3601314 https://doi.org/10.1111/jtsa.12195 LACAL,V. and TJØSTHEIM, D. (2019). Estimating and testing nonlinear local dependence between two time series. J. Bus. Econom. Statist. 37 648–660. MR4016160 https://doi.org/10.1080/ 07350015.2017.1407777 LANCASTER, H. O. (1957). Some properties of the bivariate nor- mal distribution considered in the form of a contingency table. Biometrika 44 289–292. LEHMANN, E. L. (1966). Some concepts of dependence. Ann. Math. Stat. 37 1137–1153. MR0202228 https://doi.org/10.1214/aoms/ 1177699260 LOADER, C. R. (1996). Local likelihood density estimation. Ann. Statist. 24 1602–1618. MR1416652 https://doi.org/10.1214/aos/ 1032298287 MARKOWITZ, H. M. (1952). Portfolio selection. J. Finance 7 77–91. MUSCAT, J. (2014). Functional Analysis: An Introduction to Metric Spaces, Hilbert Spaces, and Banach Algebras. Springer, Cham. MR3308576 https://doi.org/10.1007/978-3-319-06728-5 NELSEN, R. B. (1999). An Introduction to Copulas. Lecture Notes in Statistics 139. Springer, New York. MR1653203 https://doi.org/10. 1007/978-1-4757-3076-0 NGUYEN,Q.N., ABOURA,S., CHEVALLIER,J., ZHANG,L. and ZHU, B. (2020). Local Gaussian correlations in ﬁnancial and com- modity markets. European J. Oper. Res. 285 306–323. MR4083070 https://doi.org/10.1016/j.ejor.2020.01.023 OTNEIM, H. (2019). lg: Locally gaussian distributions: Estimation and methods. Available at https://CRAN.R-project.org/package= lg. OTNEIM,H., JULLUM,M. and TJØSTHEIM, D. (2020). Pairwise lo- cal Fisher and naive Bayes: Improving two standard discriminants. J. Econometrics 216 284–304. MR4077395 https://doi.org/10. 1016/j.jeconom.2020.01.019 OTNEIM,H. and TJØSTHEIM, D. (2017). The locally Gaussian den- sity estimator for multivariate data. Stat. Comput. 27 1595–1616. MR3687328 https://doi.org/10.1007/s11222-016-9706-6 OTNEIM,H. and TJØSTHEIM, D. (2018). Conditional density estima- tion using the local Gaussian correlation. Stat. Comput. 28 303– 321. MR3747565 https://doi.org/10.1007/s11222-017-9732-z OTNEIM,H. and TJØSTHEIM, D. (2021). The locally Gaussian partial correlation. J. Bus. Econom. Statist. 1–33. To appear. PEARSON, K. (1896). Mathematical contributions to the theory of evo- lution. III. Regression, heredity and panmixia. Philos. Trans. R. Soc. Lond. 187 253–318. PEARSON, K. (1922). Francis Galton: A Centenary Appreciation. Cambridge Univ. Press, Cambridge. PEARSON, K. (1930). The Life, Letters and Labors of Francis Galton. Cambridge Univ. Press, Cambridge. PFISTER,N. and PETERS, J. (2017). dHSIC: Independence testing via Hilbert Schmidt independence criterion. Available at https: //CRAN.R-project.org/package=dHSIC. PFISTER,N., BÜHLMANN,P., SCHÖLKOPF,B. and PETERS,J. (2018). Kernel-based tests for joint independence. J. R. Stat. Soc. Ser. B. Stat. Methodol. 80 5–31. MR3744710 https://doi.org/10. 1111/rssb.12235 PINKSE, J. (1998). A consistent nonparametric test for se- rial independence. J. Econometrics 84 205–231. MR1630190 https://doi.org/10.1016/S0304-4076(97)00084-5 PRUDNIKOV,A. P., BRYCHKOV,Y. A. and MARICHEV, O. I. (1986). Integrals and Series. Gordon & Breach, New York. RÉNYI, A. (1959). On measures of dependence. Acta Math. Acad. Sci. Hung. 10 441–451. MR0115203 https://doi.org/10.1007/ BF02024507 RESHEF,D. N., RESHEF,Y. A., FINUCANE,H. K., GROSS- MAN,S.R., MCVEAN,G., TURNBAUGH,P. J., LANDER,E.S., MITZENMACHER,M. and SABETI, P. C. (2011). Detecting novel associations in large data sets. Science 334 1518–1524. https://doi.org/10.1126/science.1205438 RESHEF,D., RESHEF,Y., MITZENMACHER,M. and SABETI,P. (2013). Equitability analysis of the maximal information coefﬁ- cient, with comparisons. RIZZO,M. L. and SZEKELY, G. J. (2018). Energy: E-statistics: Multi- variate inference via the energy of data. Available at https://CRAN. R-project.org/package=energy. ROBINSON, P. M. (1991). Consistent nonparametric entropy- based testing. Rev. Econ. Stud. 58 437–453. MR1108130 https://doi.org/10.2307/2298005 ROSENBLATT, M. (1975). A quadratic measure of deviation of two- dimensional density estimates and a test of independence. Ann. Statist. 3 1–14. MR0428579 SEJDINOVIC,D., SRIPERUMBUDUR,B., GRETTON,A. and FUKU- MIZU, K. (2013). Equivalence of distance-based and RKHS- based statistics in hypothesis testing. Ann. Statist. 41 2263–2291. MR3127866 https://doi.org/10.1214/13-AOS1140 SILVAPULLE,P. and GRANGER, C. W. J. (2001). Large returns, con- ditional correlation and portfolio diversiﬁcation: A value-at-risk ap- proach. Quant. Finance 1 542–551. MR1863876 https://doi.org/10. 1088/1469-7688/1/5/306 SKAUG,H.J. and TJØSTHEIM, D. (1993a). A nonparametric test of serial independence based on the empirical distribution func- tion. Biometrika 80 591–602. MR1248024 https://doi.org/10.1093/ biomet/80.3.591 SKAUG,H.J. and TJØSTHEIM, D. (1993b). Nonparametric tests of serial independence. In Developments in Time Series Analysis (T. S. Rao, ed.) 207–229. CRC Press, London. MR1292268 SKAUG,H.J. and TJØSTHEIM, D. (1996). Testing for serial inde- pendence using measures of distance between densities. In Athens Conference on Applied Probability and Time Series Analysis, Vol. II (P. M. Robinson and M. Rosenblatt, eds.). Lect. Notes Stat. 115 363–377. Springer, New York. MR1466759 https://doi.org/10. 1007/978-1-4612-2412-9_27 SKLAR, M. (1959). Fonctions de Répartition à N Dimensions et Leurs Marges. Université Paris 8. SPEARMAN, C. (1904). The proof and measurement of association between two things. Am. J. Psychol. 15 72–101. STANTON, J. M. (2001). Galton, Pearson, and the peas: A brief history of linear regression for statistics instructors. J. Stat. Educ. 9 1–13. STIGLER, S. M. (1989). Francis Galton’s account of the invention of correlation. Statist. Sci. 4 73–79. MR1007556 STØVE,B. and TJØSTHEIM, D. (2014). Asymmetric dependence pat- terns in ﬁnancial returns: An empirical investigation using local Gaussian correlation. In Essays in Nonlinear Time Series Econo- metrics (M. Meitz N. Haldrup and P. Saikkonen, eds.) 307–329. Oxford Univ. Press, Oxford. MR3288225 https://doi.org/10.1093/ acprof:oso/9780199679959.003.0013 STØVE,B. TJØSTHEIM,D. and HUFTHAMMER, K. (2014). Using lo- cal Gaussian correlation in a nonlinear re-examination of ﬁnancial contagion. J. Empir. Finance 25 785–801. STATISTICAL DEPENDENCE 109 SU,L. and WHITE, H. (2007). A consistent characteristic function- based test for conditional independence. J. Econometrics 141 807– 834. MR2413488 https://doi.org/10.1016/j.jeconom.2006.11.006 SZÉKELY, G. J. (2002). E-statistics: The energy of statistical sam- ples. Technical report 02-16, Bowling Green State Univ., Bowling Green, OH. SZÉKELY,G. J. and RIZZO, M. L. (2005). Hierarchical cluster- ing via joint between-within distances: Extending Ward’s mini- mum variance method. J. Classiﬁcation 22 151–183. MR2231170 https://doi.org/10.1007/s00357-005-0012-9 SZÉKELY,G. J. and RIZZO, M. L. (2009). Brownian distance covari- ance. Ann. Appl. Stat. 3 1236–1265. MR2752127 https://doi.org/10. 1214/09-AOAS312 SZÉKELY,G. J. and RIZZO, M. L. (2013). Energy statistics: A class of statistics based on distances. J. Statist. Plann. Inference 143 1249– 1272. MR3055745 https://doi.org/10.1016/j.jspi.2013.03.018 SZÉKELY,G. J. and RIZZO, M. L. (2014). Partial distance correla- tion with methods for dissimilarities. Ann. Statist. 42 2382–2412. MR3269983 https://doi.org/10.1214/14-AOS1255 SZÉKELY,G. J., RIZZO,M. L. and BAKIROV, N. K. (2007). Measuring and testing dependence by correlation of distances. Ann. Statist. 35 2769–2794. MR2382665 https://doi.org/10.1214/ 009053607000000505 TALEB, N. N. (2007). The Black Swan: The Impact of the Highly Im- probable. Random House, New York. TERÄSVIRTA,T., TJØSTHEIM,D. and GRANGER,C.W.J. (2010). Modelling Nonlinear Economic Time Series. Advanced Texts in Econometrics. Oxford Univ. Press, Oxford. MR3185399 https://doi.org/10.1093/acprof:oso/9780199587148.001.0001 TJØSTHEIM,D. and HUFTHAMMER, K. O. (2013). Local Gaus- sian correlation: A new measure of dependence. J. Econometrics 172 33–48. MR2997128 https://doi.org/10.1016/j.jeconom.2012. 08.001 TJØSTHEIM,D., OTNEIM,H. and STØVE, B. (2021). Statistical Mod- eling Using Local Gaussian Approximation. Elsevier, Amsterdam. To appear. TJØSTHEIM,D., OTNEIM,H. and STØVE, B. (2022). Supplement to “Statistical Dependence: Beyond Pearson’s ρ.” https://doi.org/10. 1214/21-STS823SUPP VON NEUMANN, J. (1941). Distribution of the ratio of the mean square successive difference to the variance. Ann. Math. Stat. 12 367–395. MR0006656 https://doi.org/10.1214/aoms/1177731677 VON NEUMANN, J. (1942). A further remark concerning the distribu- tion of the ratio of the mean square successive difference to the vari- ance. Ann. Math. Stat. 13 86–88. MR0006657 https://doi.org/10. 1214/aoms/1177731645 YAO,S., ZHANG,X. and SHAO, X. (2018). Testing mutual indepen- dence in high dimension via distance covariance. J. R. Stat. Soc. Ser. B. Stat. Methodol. 80 455–480. MR3798874 https://doi.org/10. 1111/rssb.12259 YENIGÜN,C.D., SZÉKELY,G. J. and RIZZO, M. L. (2011). A test of independence in two-way contingency tables based on maxi- mal correlation. Comm. Statist. Theory Methods 40 2225–2242. MR2862708 https://doi.org/10.1080/03610921003764274 ZHANG,K., PETERS,J., JANZING,D. and SCHÖLKOPF, B. (2012). Kernel-based conditional independence test and applications in causal discovery. In Proceedings of the Uncertainty in Artiﬁcial In- telligence 804–813. AUAI Press, Corvallis, OR. ZHANG,Q., FILIPPI,S., GRETTON,A. and SEJDINOVIC,D. (2018). Large-scale kernel methods for independence testing. Stat. Comput. 28 113–130. MR3741641 https://doi.org/10.1007/ s11222-016-9721-7 ZHOU, Z. (2012). Measuring nonlinear dependence in time-series, a distance correlation approach. J. Time Series Anal. 33 438–457. MR2915095 https://doi.org/10.1111/j.1467-9892.2011.00780.x","libVersion":"0.3.2","langs":""}
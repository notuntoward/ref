{"path":"lit/sources/papers_to_add/papers_to_markup_over_weekend/Leutbecher_Ensemble_verification_2.pdf","text":"Ensemble Veriﬁcation II Martin Leutbecher Training Course 2014 1 Proper scores 2 3 Comparison of ensemble with single forecasts 4 Observation uncertainty 5 Climatological distribution 6 Further topics Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 1 / 31 Ensemble Veriﬁcation II Martin Leutbecher Training Course 2014 1 Proper scores 2 Continuous scalar variables 3 Comparison of ensemble with single forecasts 4 Observation uncertainty 5 Climatological distribution 6 Further topics Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 1 / 31 Ensemble Veriﬁcation II Martin Leutbecher Training Course 2014 1 Proper scores 2 Continuous scalar variables 3 Comparison of ensemble with single forecasts 4 Observation uncertainty 5 Climatological distribution 6 Further topics Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 1 / 31 Proper scores A score for a probabilistic forecast is a summary measure that evaluates the probability distribution. This condenses all the information into a single number and can be potentially misleading. Let us assume that we predict the distribution pfc(x) while the veriﬁcation is distributed according to a distribution py (x). Not all scores indicate maximum skill for pfc = py . A score (or scoring rule) is (strictly) proper if the score reaches its optimal value if (and only if) the predicted distribution is equal to the distribution of the veriﬁcation. If a forecaster is judged by a score that is not proper, (s)he is encouraged to issue forecasts that diﬀer from what her/his true belief of the best forecast is! In such a situation one says that the forecast is hedged or that the forecaster plays the score. Examples of proper scores are: Brier Score, continuous (and discrete) ranked probability score, logarithmic score see Gneiting and Raftery (2007) for more details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 2 / 31 Proper scores A score for a probabilistic forecast is a summary measure that evaluates the probability distribution. This condenses all the information into a single number and can be potentially misleading. Let us assume that we predict the distribution pfc(x) while the veriﬁcation is distributed according to a distribution py (x). Not all scores indicate maximum skill for pfc = py . A score (or scoring rule) is (strictly) proper if the score reaches its optimal value if (and only if) the predicted distribution is equal to the distribution of the veriﬁcation. If a forecaster is judged by a score that is not proper, (s)he is encouraged to issue forecasts that diﬀer from what her/his true belief of the best forecast is! In such a situation one says that the forecast is hedged or that the forecaster plays the score. Examples of proper scores are: Brier Score, continuous (and discrete) ranked probability score, logarithmic score see Gneiting and Raftery (2007) for more details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 2 / 31 Proper scores A score for a probabilistic forecast is a summary measure that evaluates the probability distribution. This condenses all the information into a single number and can be potentially misleading. Let us assume that we predict the distribution pfc(x) while the veriﬁcation is distributed according to a distribution py (x). Not all scores indicate maximum skill for pfc = py . A score (or scoring rule) is (strictly) proper if the score reaches its optimal value if (and only if) the predicted distribution is equal to the distribution of the veriﬁcation. If a forecaster is judged by a score that is not proper, (s)he is encouraged to issue forecasts that diﬀer from what her/his true belief of the best forecast is! In such a situation one says that the forecast is hedged or that the forecaster plays the score. Examples of proper scores are: Brier Score, continuous (and discrete) ranked probability score, logarithmic score see Gneiting and Raftery (2007) for more details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 2 / 31 Proper scores A score for a probabilistic forecast is a summary measure that evaluates the probability distribution. This condenses all the information into a single number and can be potentially misleading. Let us assume that we predict the distribution pfc(x) while the veriﬁcation is distributed according to a distribution py (x). Not all scores indicate maximum skill for pfc = py . A score (or scoring rule) is (strictly) proper if the score reaches its optimal value if (and only if) the predicted distribution is equal to the distribution of the veriﬁcation. If a forecaster is judged by a score that is not proper, (s)he is encouraged to issue forecasts that diﬀer from what her/his true belief of the best forecast is! In such a situation one says that the forecast is hedged or that the forecaster plays the score. Examples of proper scores are: Brier Score, continuous (and discrete) ranked probability score, logarithmic score see Gneiting and Raftery (2007) for more details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 2 / 31 Proper scores A score for a probabilistic forecast is a summary measure that evaluates the probability distribution. This condenses all the information into a single number and can be potentially misleading. Let us assume that we predict the distribution pfc(x) while the veriﬁcation is distributed according to a distribution py (x). Not all scores indicate maximum skill for pfc = py . A score (or scoring rule) is (strictly) proper if the score reaches its optimal value if (and only if) the predicted distribution is equal to the distribution of the veriﬁcation. If a forecaster is judged by a score that is not proper, (s)he is encouraged to issue forecasts that diﬀer from what her/his true belief of the best forecast is! In such a situation one says that the forecast is hedged or that the forecaster plays the score. Examples of proper scores are: Brier Score, continuous (and discrete) ranked probability score, logarithmic score see Gneiting and Raftery (2007) for more details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 2 / 31 Proper scores A score for a probabilistic forecast is a summary measure that evaluates the probability distribution. This condenses all the information into a single number and can be potentially misleading. Let us assume that we predict the distribution pfc(x) while the veriﬁcation is distributed according to a distribution py (x). Not all scores indicate maximum skill for pfc = py . A score (or scoring rule) is (strictly) proper if the score reaches its optimal value if (and only if) the predicted distribution is equal to the distribution of the veriﬁcation. If a forecaster is judged by a score that is not proper, (s)he is encouraged to issue forecasts that diﬀer from what her/his true belief of the best forecast is! In such a situation one says that the forecast is hedged or that the forecaster plays the score. Examples of proper scores are: Brier Score, continuous (and discrete) ranked probability score, logarithmic score see Gneiting and Raftery (2007) for more details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 2 / 31 Example of a score that is not proper consider the linear score: LinS = |p − o| dichotomous event e: e occured (o = 1), e did not occur (o = 0) assume the event occurs with the true probability of 0.4 If the prediction is 0.4, the expected linear score is E(LinS) = 0.4|0.4 − 1| + (1 − 0.4) |0.4 − 0| = 0.48 If the prediction is instead 0, the expected linear score is E(LinS) = 0.4|0 − 1| + (1 − 0.4)|0 − 0| = 0.40 Note, that is easy to prove that the Brier score is strictly proper (e.g. Wilks 2011) Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 3 / 31 Scores for probabilistic/ensemble forecasts of continuous scalar variables some (but not all) useful measures RMSE and other scores used for single forecasts applied to ensemble mean rank histograms (reliability again) continuous ranked probability score (reliability and resolution) logarithmic score (for Gaussian) (reliability and resolution) reliability of the ensemble spread (domain-integrated and local) Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 4 / 31 Scores for probabilistic/ensemble forecasts of continuous scalar variables some (but not all) useful measures RMSE and other scores used for single forecasts applied to ensemble mean rank histograms (reliability again) continuous ranked probability score (reliability and resolution) logarithmic score (for Gaussian) (reliability and resolution) reliability of the ensemble spread (domain-integrated and local) Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 4 / 31 Continuous ranked probability score CRPS = Mean squared error of the cumulative distribution Pfc cdf of observation Py (x) = P(y ≤ x) = H(x − y ) cdf of forecast Pfc(x) = P(xfc ≤ x) CRPS = ∫ +∞ −∞ (Pfc(x) − Py (x)) 2 dx = ∫ +∞ −∞ BSx dx −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs equal to mean absolute error for a single forecast Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 5 / 31 Continuous ranked probability score CRPS = Mean squared error of the cumulative distribution Pfc cdf of observation Py (x) = P(y ≤ x) = H(x − y ) cdf of forecast Pfc(x) = P(xfc ≤ x) CRPS = ∫ +∞ −∞ (Pfc(x) − Py (x)) 2 dx = ∫ +∞ −∞ BSx dx −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs equal to mean absolute error for a single forecast Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 5 / 31 Continuous ranked probability score CRPS = Mean squared error of the cumulative distribution Pfc cdf of observation Py (x) = P(y ≤ x) = H(x − y ) cdf of forecast Pfc(x) = P(xfc ≤ x) CRPS = ∫ +∞ −∞ (Pfc(x) − Py (x)) 2 dx = ∫ +∞ −∞ BSx dx −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs −2 −1 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 x P (P_fc − P_obs)^2 P_fc P_obs equal to mean absolute error for a single forecast Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 5 / 31 How to compute the CRPS Ensemble The integral ∫ . . . dx can be evaluated exactly by using the intervals deﬁned by the M ensemble forecasts and the veriﬁcation rather than some ﬁxed interval ∆x: CRPS = M∑ j=0 cj cj = αj p2 j + βj (1 − pj )2 pj = j/M Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 6 / 31 How to compute the CRPS Gaussian distribution For a Gaussian distribution an analytical formula for the CRPS is available. Assume that the predicted Gaussian has mean µ and variance σ2 and that the veriﬁcation is denoted by y . CRPS = σ √π [−1 + √π y − µ σ Φ ( y − µ √2σ ) + √2 exp (− (y − µ)2 2σ2 )] Here, Φ denotes the error function Φ(x) = 2 √π ∫ x 0 exp(−t2) dt. This relationship is particularly useful for calibration purposes (Non-homogeneous Gaussian regression). Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 7 / 31 CRPS Decomposition The CRPS can be decomposed into a reliability component and a resolution component. The CRPS is additive: The CRPS for the union of two samples is the weighted (arithmetic) average of the CRPS of the two samples with the weights proportional to the respective sample sizes. The components of the CRPS are not additive. The components can be computed from the sample averages of the αj and βj distances. This is similar to the decomposition of the Brier score. However, the reliability (resolution) component of the CRPS is not the integral of the reliability (resolution) component of the Brier scores. The reliability component of the CRPS is related to the rank histogram but not identical. see Hersbach (2000) for details Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 8 / 31 Ranked Probability Score (RPS) The CRPS ∫ BSx dx has a discrete analog, the (discrete) ranked probability score: RPS = L∑ k=1 BSxk = L∑ k=1 (Pfc(k) − Py (k)) 2 The thresholds xk that separate the L categories can be chosen in various ways ▶ equidistant (RPS → CRPS as ∆x → 0) ▶ climatologically equally likely, e.g. tercile boundaries Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 9 / 31 Logarithmic score Ignorance score For a forecast consisting of a probability density pfc(x), deﬁne LS = − log(pfc(y )) where y denotes the observation (or analysis). This score is proper and local. ensemble forecasts −→ probability density A simple yet useful exercise is to use the Gaussian density given by the ensemble mean µ and the ensemble variance σ2. Then, the logarithmic score is given by LS = (µ − y )2 2σ2 + 1 2 log(2πσ2) Thus, it consists of the squared error of the ensemble mean normalized by the ensemble variance and a logarithmic term that penalizes large variance. The ﬁrst term is a measure of the reliability and the second term is a measure of the sharpness of the forecast. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 10 / 31 Logarithmic score Ignorance score For a forecast consisting of a probability density pfc(x), deﬁne LS = − log(pfc(y )) where y denotes the observation (or analysis). This score is proper and local. ensemble forecasts −→ probability density A simple yet useful exercise is to use the Gaussian density given by the ensemble mean µ and the ensemble variance σ2. Then, the logarithmic score is given by LS = (µ − y )2 2σ2 + 1 2 log(2πσ2) Thus, it consists of the squared error of the ensemble mean normalized by the ensemble variance and a logarithmic term that penalizes large variance. The ﬁrst term is a measure of the reliability and the second term is a measure of the sharpness of the forecast. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 10 / 31 Logarithmic score Ignorance score For a forecast consisting of a probability density pfc(x), deﬁne LS = − log(pfc(y )) where y denotes the observation (or analysis). This score is proper and local. ensemble forecasts −→ probability density A simple yet useful exercise is to use the Gaussian density given by the ensemble mean µ and the ensemble variance σ2. Then, the logarithmic score is given by LS = (µ − y )2 2σ2 + 1 2 log(2πσ2) Thus, it consists of the squared error of the ensemble mean normalized by the ensemble variance and a logarithmic term that penalizes large variance. The ﬁrst term is a measure of the reliability and the second term is a measure of the sharpness of the forecast. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 10 / 31 Daily EPS stdev (shaded) and ens. mean (cont.) 500 hPa geopotential (m2 s−2) at 72 h lead; init. time 6 December 2010 H L L 70°N 70°N 70°N 60°N 60°N 60°N 60°N 60°N 50°N 50°N 50°N 50°N 50°N 50°N 50°N 40°N 40°N 40°N 40°N 40°N 40°N 30°N 30°N 30°N 30°N 30°N 60°E40°E20°E20°W40°W60°W 20°E20°W 0 75 150 225 300 375 450 525 600 675 750 825 EM Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 11 / 31 Daily EPS stdev (shaded) and ens. mean (cont.) 500 hPa geopotential (m2 s−2) at 72 h lead; init. time 8 December 2010 LL L L 70°N 70°N 70°N 60°N 60°N 60°N 60°N 60°N 50°N 50°N 50°N 50°N 50°N 50°N 50°N 40°N 40°N 40°N 40°N 40°N 40°N 30°N 30°N 30°N 30°N 30°N 60°E40°E20°E20°W40°W60°W 20°E20°W 0 75 150 225 300 375 450 525 600 675 750 825 EM Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 11 / 31 Spread-reliability methodology consider (local) pairs of ensemble variance and squared error of the ensemble mean Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 12 / 31 Spread-reliability methodology consider (local) pairs of ensemble variance and squared error of the ensemble mean Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 12 / 31 Spread-reliability methodology consider (local) pairs of ensemble variance and squared error of the ensemble mean Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 12 / 31 Spread-reliability methodology consider (local) pairs of ensemble variance and squared error of the ensemble mean — stratiﬁed by the ensemble variance Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 12 / 31 Spread-reliability methodology consider (local) pairs of ensemble variance and squared error of the ensemble mean — stratiﬁed by the ensemble variance Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 12 / 31 Spread-reliability: An example 500 hPa height — 20◦–90 ◦N 24 h 48 h4681012141618RMS spread (m)4681012141618RMS error (m).as 36R1as 36R44812162024283236RMS spread (m)4812162024283236RMS error (m).as 36R1as 36R4 40 cases T639, 50 member Jan 2010 conﬁg. (“as 36r1”) Nov 2010 conﬁg. (“as 36r4”): revised initial perturbations and revised tendency pertns. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 13 / 31 Veriﬁcation of ensembles and single forecasts When monitoring an operational forecasting system that consists of single (unperturbed) forecasts and an ensemble, it is useful to compare changes in the performance of the ensemble with changes seen for the single forecast(s). But what scores should be compared when looking at a single forecast versus an ensemble? Many scores for ensembles are meaningful when computed for single forecasts equivalences ▶ CRPS — MAE ▶ BS — BS single fc (using probabilities 0 and 1) Obviously, probabilistic skill of a “naked” (= raw) single forecast is inferior to the probabilistic skill of a dressed single forecast. The dressing kernel can be estimated from past error statistics. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 14 / 31 Veriﬁcation of ensembles and single forecasts When monitoring an operational forecasting system that consists of single (unperturbed) forecasts and an ensemble, it is useful to compare changes in the performance of the ensemble with changes seen for the single forecast(s). But what scores should be compared when looking at a single forecast versus an ensemble? Many scores for ensembles are meaningful when computed for single forecasts equivalences ▶ CRPS — MAE ▶ BS — BS single fc (using probabilities 0 and 1) Obviously, probabilistic skill of a “naked” (= raw) single forecast is inferior to the probabilistic skill of a dressed single forecast. The dressing kernel can be estimated from past error statistics. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 14 / 31 Veriﬁcation of ensembles and single forecasts When monitoring an operational forecasting system that consists of single (unperturbed) forecasts and an ensemble, it is useful to compare changes in the performance of the ensemble with changes seen for the single forecast(s). But what scores should be compared when looking at a single forecast versus an ensemble? Many scores for ensembles are meaningful when computed for single forecasts equivalences ▶ CRPS — MAE ▶ BS — BS single fc (using probabilities 0 and 1) Obviously, probabilistic skill of a “naked” (= raw) single forecast is inferior to the probabilistic skill of a dressed single forecast. The dressing kernel can be estimated from past error statistics. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 14 / 31 Dressed control forecast: v 850 hPa, 35◦–65 ◦N, DJF09 EPS raw prob. for CRPS; Gaussian for LS N(CF, σ2 err(CF)) σerr estimated from reforecasts CRPS02468101214fc-step (d)012345CRPS2008120100-2009022800 (90)ContinuousRankedProbabilityScore, ContinuousRankedProbabilityScoreCFWithErrClimv850hPa, Northern Mid-latitudesTMP161ec7 LS02468101214fc-step (d)01ContinuousIgnoranceScoreGaussian2008120100-2009022800 (90)ContinuousIgnoranceScoreGaussian, ContinuousIgnoranceScoreCFWithGaussianErrClimv850hPa, Northern Mid-latitudesTMP161ec7 Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 15 / 31 Dressed ens. mean forecast: v 850 hPa, 35 ◦–65 ◦N, DJF09 EPS raw prob. for CRPS; Gaussian for LS N(EM, σ2 err(EM)) σerr estimated from reforecasts CRPS02468101214fc-step (d)0123CRPS2008120100-2009022800 (90)ContinuousRankedProbabilityScore, ContinuousRankedProbabilityScoreEMWithErrClimv850hPa, Northern Mid-latitudesTMP161ec7 LS02468101214fc-step (d)-0.200.20.40.60.811.21.4ContinuousIgnoranceScoreGaussian2008120100-2009022800 (90)ContinuousIgnoranceScoreGaussian, ContinuousIgnoranceScoreEMWithGaussianErrClimv850hPa, Northern Mid-latitudesTMP161ec7 EM more accurate than CF ⇒ this permits a sharper Gaussian distribution. The Logarithmic score discriminates better the value of ﬂow-dependent variations in ensemble variance than the CRPS. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 16 / 31 Uncertainty of the verifying observations or, more generally, the verifying data In real applications the true state xt of the atmosphere is not know exactly. The observation y has an error y = xt + ϵ Assume an ensemble is perfectly reliable, i.e. ensemble members xe ∼ ρe and the true state xt ∼ ρt are realisations of the same distribution ρe = ρt. Then, the observation y is a realisation of the distribution given by the convolution of the true distribution and the error distribution ρy = ρt ∗ ρϵ Thus, a veriﬁcation with respect to y will indicate a lack of reliability. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 17 / 31 Veriﬁcation in the presence of observation uncertainties ρϵ ρt = ρe, ρy = ρE solution: postprocess ensemble members prior to veriﬁcation verify ensemble members to which noise has been added: xE = xe + ϵ with ϵ ∼ ρϵ Then ρE = ρy Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 18 / 31 The climatological distribution temperature in 850 hPa 15 March (based on ERA-Interim 1989–2008)-12-8-8-8-4-4 -4-400 0044 4488 8812012345678 contours: mean — shading: stdev Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 19 / 31 The climatological distribution temperature in 850 hPa 15 June (based on ERA-Interim 1989–2008)04 4488 8812 12 12121616 161620 20 2024012345678 contours: mean — shading: stdev Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 19 / 31 Ficticious skill due to a poor climatological distribution If one uses the same climatological distribution for a domain with diﬀerent climatological characteristics (mean, stdev, . . . ), the skill with respect to that distribution is not real skill. It reﬂects the poor quality of the climatological distribution. Same applies if seasonal variations of the climatological distribution are not represented. This criticism applies for instance if the climatological distribution is derived from the veriﬁcation sample itself by aggregating diﬀerent start times and diﬀerent locations. It can also be misleading to compare skill scores from diﬀerent prediction centres when the skill scores have been computed against own analyses. If the same climatological distribution (say ERA-Interim) is used as reference, this climatological distribution has the lowest skill when veriﬁed against the analysis that deviates most from the analyses used for computing the climatological distribution. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 20 / 31 Ficticious skill due to a poor climatological distribution If one uses the same climatological distribution for a domain with diﬀerent climatological characteristics (mean, stdev, . . . ), the skill with respect to that distribution is not real skill. It reﬂects the poor quality of the climatological distribution. Same applies if seasonal variations of the climatological distribution are not represented. This criticism applies for instance if the climatological distribution is derived from the veriﬁcation sample itself by aggregating diﬀerent start times and diﬀerent locations. It can also be misleading to compare skill scores from diﬀerent prediction centres when the skill scores have been computed against own analyses. If the same climatological distribution (say ERA-Interim) is used as reference, this climatological distribution has the lowest skill when veriﬁed against the analysis that deviates most from the analyses used for computing the climatological distribution. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 20 / 31 More veriﬁcation topics statistical signiﬁcance of diﬀerences of veriﬁcation statistics between diﬀerent forecast systems etc. sensitivity to ensemble size and estimation of veriﬁcation statistics in the limit M → ∞ skill on diﬀerent spatial scales multivariate aspects decision making and veriﬁcation Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 21 / 31 Probabilities can help with making decisions Open air restaurant scenario: ▶ to open additional tables costs £20 and provides £100 extra income (if T > 24 ◦ C) ▶ On a particular day, the forecast is P(T > 24◦ C) = 0.30 ▶ What should the restaurant do? Compute the proﬁt/loss (£) over 100 days (assuming reliable probabilities): proﬁt on warm days(T > 24 ◦ C) = 30 × (100 − 20) = +2400 proﬁt on cool days(T ≤ 24 ◦ C ) = 70 × (0 − 20) = −1400 total proﬁt = +1000 It is proﬁtable to open additional tables if the probability of a warm day exceeds 0.20 . The ratio of cost to loss (or cost to extra proﬁt) determines at what probability value it is beneﬁcial to take action. For low (high) cost/loss, action should be taken already (only) if the event is predicted with a low (high) probability. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 22 / 31 Probabilities can help with making decisions Open air restaurant scenario: ▶ to open additional tables costs £20 and provides £100 extra income (if T > 24 ◦ C) ▶ On a particular day, the forecast is P(T > 24◦ C) = 0.30 ▶ What should the restaurant do? Compute the proﬁt/loss (£) over 100 days (assuming reliable probabilities): proﬁt on warm days(T > 24 ◦ C) = 30 × (100 − 20) = +2400 proﬁt on cool days(T ≤ 24 ◦ C ) = 70 × (0 − 20) = −1400 total proﬁt = +1000 It is proﬁtable to open additional tables if the probability of a warm day exceeds 0.20 . The ratio of cost to loss (or cost to extra proﬁt) determines at what probability value it is beneﬁcial to take action. For low (high) cost/loss, action should be taken already (only) if the event is predicted with a low (high) probability. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 22 / 31 Probabilities can help with making decisions Open air restaurant scenario: ▶ to open additional tables costs £20 and provides £100 extra income (if T > 24 ◦ C) ▶ On a particular day, the forecast is P(T > 24◦ C) = 0.30 ▶ What should the restaurant do? Compute the proﬁt/loss (£) over 100 days (assuming reliable probabilities): proﬁt on warm days(T > 24 ◦ C) = 30 × (100 − 20) = +2400 proﬁt on cool days(T ≤ 24 ◦ C ) = 70 × (0 − 20) = −1400 total proﬁt = +1000 It is proﬁtable to open additional tables if the probability of a warm day exceeds 0.20 . The ratio of cost to loss (or cost to extra proﬁt) determines at what probability value it is beneﬁcial to take action. For low (high) cost/loss, action should be taken already (only) if the event is predicted with a low (high) probability. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 22 / 31 Decision making — cost loss model expense when using climatological prob. Ec = min(C , oL) expense when using a perfect forecast Ep = oC expense when using the forecast Ef = aC + bC + cL value V = saving from using forecast saving from using perfect fc. = Ec − Ef Ec − Ep = = min(α, o) − F (1 − o)α + Ho(1 − α) − o min(α, o) − oα where α = C /L; H = a a + c ; F = b b + d ; o = a + c Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 23 / 31 (Potential) economic value maximum value reached at α = C /L = o maximum value for all C /L is max V = H − F when a (reliable) probabilistic forecast predicts an event with probability p, all users with C /L < p should act. one speaks of potential economic value if calibrated probabilities are used to make decision Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 24 / 31 Potential economic value and ensemble size Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 25 / 31 Decision making — weather roulette Hagedorn and Smith (2009) Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 26 / 31 Decision making — weather roulette Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 27 / 31 Decision making — weather roulette 3-day forecasts Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 28 / 31 Decision making — weather roulette 3-day forecasts Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 29 / 31 Decision making — weather roulette 10-day forecasts weather roulette capital gains are closely related to the logarithmic score Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 30 / 31 References Candille, G. and O. Talagrand, 2005: Evaluation of probabilistic prediction systems for a scalar variable. Q. J. R. Meteorol. Soc., 131, 2131–2150. Ferro, C. A. T., D. S. Richardson, and A. P. Weigel, 2008: On the eﬀect of ensemble size on the discrete and continuous ranked probability scores. Meteorol. Appl., 15, 19–24. Gneiting, T. and A. E. Raftery, 2007: Strictly proper scoring rules, prediction and estimation. Journal of the American Statistical Association, 102, 359–378. Hagedorn, R. and L. A. Smith, 2009: Communicating the value of probabilistic forecasts with weather roulette. Meteorol. Appl., 16, 143–155. Hamill, T. M., 2001: Interpretation of rank histograms for verifying ensemble forecasts. Mon. Wea. Rev., 129, 550–560. Hamill, T. M. and J. Juras, 2006: Measuring forecast skill: Is it real skill or is it the varying climatology? Q. J. R. Meteorol. Soc., 132, 2905–2923. Hersbach, H., 2000: Decomposition of the continous ranked probability score for ensemble prediction systems. Weather and Forecasting, 15, 559–570. Jung, T. and M. Leutbecher, 2008: Scale-dependent veriﬁcation of ensemble forecasts. Q. J. R. Meteorol. Soc., 134, 973–984. Leutbecher, M., 2009: Diagnosis of ensemble forecasting systems. In Seminar on Diagnosis of Forecasting and Data Assimilation Systems, ECMWF, Reading, UK, 235–266. Murphy, A. and R. L. Winkler, 1987: A general framework for forecast veriﬁcation. Mon. Wea. Rev., 115, 1330–1338. Richardson, D. S., 2000: Skill and relative economic value of the ECMWF ensemble prediction system. Q. J. R. Meteorol. Soc., 126, 649–667. Roulston, M. S. and L. A. Smith, 2002: Evaluating probabilistic forecasts using information theory. Mon. Wea. Rev., 130, 1653–1660. Saetra, Ø., H. Hersbach, J.-R. Bidlot, and D. S. Richardson, 2004: Eﬀects of observation errors on the statistics for ensemble spread and reliability. Mon. Wea. Rev., 132, 1487–1501. Weigel, A. P., 2011: Ensemble veriﬁcation. In Forecast Veriﬁcation: A Practitioner’s Guide in Atmospheric Science, Jolliﬀe, I. T. and Stephenson, D. B., editors. Wiley, 2nd edition. Wilks, D. S., 2011: Statistical Methods in the Atmospheric Sciences. Academic Press, 3rd edition. Martin Leutbecher Ensemble Veriﬁcation II Training Course 2014 31 / 31","libVersion":"0.3.1","langs":""}
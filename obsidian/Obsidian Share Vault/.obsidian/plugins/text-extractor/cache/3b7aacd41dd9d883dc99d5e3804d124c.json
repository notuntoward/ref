{"path":"lit/lit_sources.backup/papers_to_add/Papers I'm Reviewing Right Now/SplineRgrsn/other spline papers/Harezlak07funcRgrsnPenalties.pdf","text":"Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 www.elsevier.com/locate/csda Penalized solutions to functional regression problems Jaroslaw Harezlak a,∗, Brent A. Coull a, Nan M. Laird a, Shannon R. Magari b, David C. Christiani c aDepartment of Biostatistics, Harvard School of Public Health, 655 Huntington Avenue, Boston, MA 02115, USA bColden Corporation, 100 North 17th Street, 9th Floor, Philadelphia, PA 19103, USA cDepartment of Environmental Health, Harvard School of Public Health, 655 Huntington Avenue, Boston, MA 02115, USA Available online 10 November 2006 Abstract Recent technological advances in continuous biological monitoring and personal exposure assessment have led to the collection of subject-speciﬁc functional data. A primary goal in such studies is to assess the relationship between the functional predictors and the functional responses. The historical functional linear model (HFLM) can be used to model such dependencies of the response on the history of the predictor values. An estimation procedure for the regression coefﬁcients that uses a variety of regularization techniques is proposed. An approximation of the regression surface relating the predictor to the outcome by a ﬁnite-dimensional basis expansion is used, followed by penalization of the coefﬁcients of the neighboring basis functions by restricting the size of the coefﬁcient differences to be small. Penalties based on the absolute values of the basis function coefﬁcient differences (corresponding to the LASSO) and the squares of these differences (corresponding to the penalized spline methodology) are studied. The ﬁts are compared using an extension of the Akaike Information Criterion that combines the error variance estimate, degrees of freedom of the ﬁt and the norm of the basis function coefﬁcients. The performance of the proposed methods is evaluated via simulations. The LASSO penalty applied to the linearly transformed coefﬁcients yields sparser representations of the estimated regression surface, while the quadratic penalty provides solutions with the smallest L2-norm of the basis function coefﬁcients. Finally, the new estimation procedure is applied to the analysis of the effects of occupational particulate matter (PM) exposure on heart rate variability (HRV) in a cohort of boilermaker workers. Results suggest that the strongest association between PM exposure and HRV in these workers occurs as a result of point exposures to the increased levels of PM corresponding to smoking breaks. © 2006 Elsevier B.V. All rights reserved. Keywords: Environmental assessment; Functional data; Heart rate variability; LASSO; Penalized regression splines 1. Introduction Ramsay and Dalzell (1991) have termed data where either the predictor or the response variables are measured continuously as “functional data”. We consider the case, common in many biological ﬁelds, where both independent and dependent variables are functions. One example is the study of growth curves where more than one characteristic of growth is observed, e.g. height and lung function. Other examples include continuous monitoring of biological functions where the predictor is an exogenous or an endogenous variable also varying over time. ∗ Corresponding author. Tel.: +1 617 432 3172; fax: +1 617 432 5619. E-mail address: jharezla@hsph.harvard.edu (J. Harezlak). 0167-9473/$ - see front matter © 2006 Elsevier B.V. All rights reserved. doi:10.1016/j.csda.2006.09.034 4912 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 Our research is motivated by a study of the effects of particulate matter on heart functioning. Particulate matter has been shown to be associated with mortality and morbidity (Dockery et al., 1993; Pope et al., 2004). Recent attention has focused on effects of pollution exposure on cardiac function, with current research suggesting that there are two components of action of the particulate matter on cardiac function: a long-lasting component through inﬂammatory cytokines and a short-lasting component via control of the autonomic nervous system. Here we consider the data collected on an occupational cohort of boilermaker construction workers who were exposed to residual oil ﬂy ash (ROFA) and metal fumes (Magari et al., 2001). Continuous, personal PM2.5 measurements were taken during workdays and on most non-work days. Measurements were taken every 10 s and 5-min averages were reported. In addition, continuous heart rate (HR) monitoring was performed using personal monitors. Five-min averages of HR and SDNN (standard deviation of the normal-to-normal intervals) were obtained. The SDNN outcome was used as the main heart rate variability (HRV) indicator which measures the cardiac autonomic tone. Early analysis of these data (Magari et al., 2001) used models that assume constant effects of exposure on HRV and hence did not take into account possibly different cumulative and lag effects of particulate exposure on HRV. We relax those assumptions in our modeling strategy in order to study complex patterns of association between the environmental exposure and HRV by utilizing a ﬂexible functional data approach. Interest in modeling functional data has grown in the past decade. Ramsay and Silverman (1997) provide an excellent summary of functional linear models. Ramsay and Silverman (1997, Chapter 11) consider the case where both the predictor x(t) and the response y(t) are observed over the same time period [0,T ], and both functions are periodic. A key feature of functional regression models is that the response values at time t, y(t), may be inﬂuenced by the predictors measured at times different from t. Ramsay and Silverman (1997) allowed x(·) observed at any time point s ∈[0,T ] to inﬂuence the response at any time t ∈[0,T ]. Therefore their model was deﬁned by Yi(t) = \u0002(t) + ∫ T 0 Xi(s)\u0003(s, t) ds + \u0004i(t), (1) where Yi(t) and Xi(s) were, respectively, response and predictor functions for subject i = 1,...,m, \u0004i(t) was a residual function with E[\u0004i(t)]= 0 and Cov(\u0004i(t), \u0004i′ (t ′)) = \u00052 if and only if i = i′ and t = t ′ and zero otherwise, \u0002(t) was a time-varying intercept function and \u0003(s, t) was an unknown regression surface describing how the predictor values Xi(s) inﬂuenced the response at time t, Yi(t). Because of its complexity and difﬁcult interpretation, the regression theory for functional data has not been as well- developed as corresponding theoretical results for classical regression with continuous or categorical predictors. Cuevas et al. (2002), and He et al. (2003) studied the theoretical properties of model (1). They concentrated on the asymptotic properties of estimators of \u0003(s, t) in functional spaces. Ramsay and Silverman (1997, 2002) studied approaches oriented towards practical examples and computational issues related to model (1). Malfait and Ramsay (2003) tailored model (1) to situations where the inﬂuence of the predictor x(·) on the response function y(·) could occur only in a feed- forward matter, so that the outcome y(t) at time t depended only on the predictors x(s) at times s ⩽t. Therefore model (1) was replaced by model (2) by restricting the integration region Yi(t) = \u0002(t) + ∫ t s0(t) Xi(s)\u0003(s, t) ds + \u0004i(t), (2) where s0(t) = max(0,t − \u0006) (where \u0006 was the maximum considered lag). Ramsay and Silverman (1997) discussed identiﬁability and estimability of regression surface \u0003(s, t) in model (1). They pointed out that even though \u0003(s, t) was identiﬁable, the estimation involved a model with inﬁnite number of parameters. Our primary goal in the estimation of the regression surface \u0003(·, ·) is the stability of solutions when there are small perturbations in the data. A general approach of dealing with such ill-posed problems is a method of regularization (Tikhonov and Arsenin, 1977), which provides meaningful and stable solutions. Malfait and Ramsay (2003) regularized the ﬁt by expansion into a small number of known basis functions. Their idea is equivalent to the regression spline methods in scatterplot smoothing. The approach of Malfait and Ramsay (2003) was satisfactory when the smoothness of the regression surface was uniform over the region of integration and the basis functions used approximated the regression surface \u0003(·, ·) well. However, the ﬁt could be poor when either one of the aforementioned conditions did not hold. In order to recover possible ﬁne features of the regression surface, one would need to use a large number of basis functions, leading to an unstable estimation procedure. On the other hand, a small number of basis functions suitable for a stable estimation will decrease the goodness of ﬁt. J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4913 We use the historical functional linear model (HFLM) considered in Malfait and Ramsay (2003) and propose an estimation procedure for the model parameters that uses regularization techniques based on penalty functions. We consider variations of the L1- and L2-norm penalties that incorporate the spatial relationship between the coefﬁcients of the estimated regression surface \u0003(s, t). Our approach forces the coefﬁcients of the neighboring basis functions to be similar through a discrete penalty on their differences. The paper is organized as follows. In Section 2, we review the procedures used for ﬁtting the HFLM in Malfait and Ramsay (2003). In Section 3, we describe our penalized ﬁtting procedures and in Section 4, we present simulation studies and their results. In Section 5, we analyze the data from the boilermaker study and we present conclusions and discuss possible future research directions in Section 6. 2. Historical functional linear model This section follows closely Malfait and Ramsay (2003). We assume without loss of generality that both responses and predictors are pointwise centered. The regression surface \u0003(s, t) is approximated by an expansion ̂\u0003(s, t) deﬁned as ̂\u0003(s, t) = K∑ k=1 bk\u0007k(s, t), where \u0007k(s, t), k = 1,...,K are known basis functions. Let \bik(t) = ∫ t s0(t) xi(s)\u0007k(s, t) ds, and consider the alternative formulation of model (2) yi(t) = K∑ k=1 bk ∫ t s0(t) xi(s)\u0007k(s, t) ds + ∫ t s0(t) xi(s)\u0004a(s, t) ds + \u0004i(t) = K∑ k=1 bk\bik(t) + \u0004′ i(t), (3) where \u0004a(s, t) = \u0003(s, t) − ̂\u0003(s, t), and \u0004′ i(t) combines the random error \u0004i(t) and the approximation error. In order to estimate the vector of coefﬁcients b =[b1,...,bK ]⊤, we minimize the squared error loss criterion, deﬁned as SSE = ∫ T 0 m∑ i=1{\u0004′ i(t)}2 dt, which is equivalent to solving an integral normal equation {∫ T 0 \u0002⊤(t)\u0002(t) dt} b = ∫ T 0 \u0002⊤(t)y(t) dt, (4) where y(t) =[y1(t),...,ym(t)]⊤ and \u0002(t) is an m × K matrix with \bik(t) entries. The estimation procedure still involves approximations of integrals. Simpliﬁcations proposed include representing the prediction process x(·) in terms of basis functions, and approximation of model (3) by a multivariate linear model based on evaluation of yi(t) at ﬁnite number of time points tq , q = 0,...,Q. Malfait and Ramsay (2003) used a computational technique called the ﬁnite element method (FEM) to estimate the vector of coefﬁcients b. In general, FEM divides the problem domain into small regions (“ﬁnite elements”), and then constructs separate low-order polynomials on the ﬁnite elements in such a way that the union of these pieces closely approximates the solution to the problem. Technical details on FEMs are provided in the books by Brenner and Scott (2002) and Braess (2001). 4914 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 s t Fig. 1. Plot of one ﬁnite element in the integration region. In our application, the region is a triangle (when \u0006 = T ), or a trapezoid (when 0 < \u0006 <T ). For such regions a natural “ﬁnite element” is a triangle, and the shape of a basis function is tent-like spanning six triangles adjacent to a node of interest (see Fig. 1). Basis functions deﬁned in such a way are an extension of linear B-spline basis functions deﬁned on a line. Each basis function \u0007k(s, t) is piecewise linear and continuous, with max \u0007k(s, t) = 1 at the node v that deﬁnes it and \u0007k(s, t) = 0 at the nodes of the triangles adjacent to v. The estimation of \u0003(s, t) involves approximation by ̂\u0003(s, t) linearly on each triangle deﬁning the region of integration. Each approximant can be written locally as a linear combination of three basis functions deﬁned on each of the triangle vertices using the system of area coordinates. The global interpolant is found by ﬁtting together the local approximations. 3. Penalized functional linear model Our proposal provides a more ﬂexible approach to estimation of the regression surface \u0003(s, t) in model (2) than that of least squares (LS). We work with the representation (3), but impose the penalties on the size of the coefﬁcients b. We extend the idea of discrete roughness penalties on the coefﬁcients of a basis function expansion (Eilers and Marx, 1996) to the HFLM setting by penalizing the interpretable directions of the regression surface. The penalty matrix for the coefﬁcients b contains the information about neighboring ﬁnite elements in a two-dimensional space. In our prediction problem, there are three important directions in which we wish to control the changes in the coefﬁcients b. The ﬁrst direction is along the s axis, which represents the cumulative effect of exposure from time s0(t) to t; the second direction is along the t axis, which reﬂects the point exposure effect at time s on the future times t> s, and the third direction is parallel to the diagonal s = t, which corresponds to the lagged effect of exposure at the constant time difference (t − s). Following the graphical representation of the regression surface \u0003(s, t), we call the penalty applied along the s axis a “horizontal” penalty (see Fig. 2), the penalty applied along the t axis a “vertical” penalty and the penalty applied in the direction parallel to the diagonal a “parallel” penalty. 3.1. Penalties We construct the horizontal penalty matrix DH (1) by stacking the difference matrices for every time point where the node of the basis function \u0007k(s, t) is deﬁned in the block-diagonal structure. Fig. 2 presents a simple example of the J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4915 0 1/3 2/3 1 0 1/3 2/3 1 b1 b2 b3 b4 b5 b6 b7 b8 b9 s t Fig. 2. Horizontal penalization. Arrows indicate the direction of penalization of the basis function coefﬁcients bk. Table 1 Horizontal difference operator—DH s0 1 0 1 2 1 2 3 t 1 12 223 33 -1 1 0 0 0 0 0 0 00 -1 100 00 D H 000-1 1 0 0 0 000 0 0 -110 000 000-1 1 horizontal direction of penalization with M = 3 subintervals of the time interval [0, 1] and B = 2 lag periods. Let b =[b1,...,b9] denote the basis function expansion of the regression surface \u0003(s, t). We can write the penalty term P(b) in the following form: P(b) =[b3 − b2] 2 +[b5 − b4]2 +[b6 − b5]2 +[b8 − b7]2 +[b9 − b8] 2. An alternative expression for P(b) is P(b) = b⊤(DH (1))⊤DH (1)b where DH (1) is presented in Table 1. In general, for a ﬁxed time t, we penalize the differences of the coefﬁcients corresponding to the neighboring basis functions. The vertical difference matrix (DV (1)) penalizes the differences of the coefﬁcients corresponding to the neighboring basis functions for a ﬁxed s, whereas the parallel difference matrix (DP (1)) penalizes the differences of the coefﬁcients corresponding to the neighboring basis functions for a ﬁxed difference (t −s). We drop the subscript (1) of the difference matrix D(1) in further discussion, since we work only with the ﬁrst-order difference matrices in our research. It is worth noticing that the null space or unpenalized part of the horizontal penalty DH consists of the constant functions for a 4916 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 given t, i.e. \u0003 H(s, t) = \u0003(t). Similarly, in the case of vertical penalty \u0003V(s, t) = \u0003(s), and in the case of parallel penalty \u0003 P(s, t) = \u0003(t − s). In the L2-norm approach, we write the minimization criterion for estimation as min b=(b1,...,bK ) m∑ i=1 J∑ j =1 [ yi(tj ) − K∑ k=1 \tik(tj )bk ]2 + b⊤Kb, (5) where m is the number of independent units, J is the number of observations per unit, tj , j =1,...,J are the observation times and \tik(tj ) = ∫ tj s0(tj ) \u0007k(s, tj )xi(s) ds. We create the penalty matrix K as a weighted sum of the penalty matrices with difference operators along the three directions, K = wHKH + wVKV + wPKP, (6) where KH =(DH) ⊤DH, KV =(DV) ⊤DV, KP =(DP)⊤DP, and wH,wV,wP are the adjustments to smoothing parameter in the horizontal, vertical, and parallel directions, respectively. Analogously, in the L1-norm approach, we work with the ﬁrst-order differences on the coefﬁcients b. We can write the minimization criterion as min b=(b1,...,bK ) m∑ i=1 J∑ j =1 [ yi(tj ) − K∑ k=1 \tik(tj )bk ]2 + ∥P(b)∥1, (7) where P(b) = wHDH(b) + wVDV(b) + wPDP(b) is a function of absolute values of differences of neighboring coefﬁcients in the respective directions: horizontal, vertical, and parallel. 3.2. Computational issues In the regression splines approach, the choice of the number and the position of the knots plays an important role in the ﬁtting procedure. Similarly the approach of Malfait and Ramsay (2003) depends in a crucial way on the choice of the parameters M and B controlling the resolution and the maximum lag, respectively. The total number of basis functions for the trapezoidal region is K = (B + 1)(M + 1 − B/2) and for a ﬁxed ratio B/M it grows quadratically in M. In our ﬁtting approach, we have the ability to estimate the approximate number of required basis functions via the notion of equivalent degrees of freedom (EDF) of the ﬁt. We start with a ﬁxed M and increase B from 2 to about M/2. We repeat the ﬁtting procedure with an increased M and select the model maximizing one of the criteria deﬁned in Section 3.3. Our proposed estimation procedure for the L2-norm approach is based on the equivalence of the penalized regression spline methods and linear mixed models (Brumback et al., 1999; Ruppert et al., 2003). In particular, parameter estimates found using the minimization criterion (5) are equivalent to those from a linear mixed model: yi = Xi\u0002 + Ziui + ϵ, where yi is the vector of responses for subject i, Xi contains covariates corresponding to unpenalized coefﬁcients, and Zi contains covariates corresponding to penalized coefﬁcients. Placing distributional assumptions on ui allows us to introduce random effects covariance structure into model (5). In our approach, the matrix Xi is either empty or it consists of the vector of ones, and Zi = \u0002i[(DA)⊤DA]−1(DA)⊤, where DA is one of the penalty matrices DH,DV,DP or their weighted combination as deﬁned in Section 3. The equivalence of the minimization criterion (5) and maximization of the mixed model loglikelihood enables us to use well-known mixed model algorithms and theoretical results in our estimation approach. Also, the choice of the smoothing parameter and the weights wH,wV and wP is equivalent to the estimation of the variance components associated with the random effects ui and the error terms, which we estimate using a restricted maximum likelihood (REML) approach. The L1-norm approach does not have a simple analytical solution. In our computations, we utilize the result of Grandvalet (1998), who proved that a modiﬁed LASSO scheme is equivalent to a weighted ridge regression. We choose the smoothing parameter based on the Akaike Information Criterion (AIC), and the weights wH,wV and wP using a grid search. J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4917 3.3. Model assessment Due to no generally agreed upon deﬁnition of R2 in the functional regression setting, we considered two proposals that are natural extensions of the deﬁnition of R2 in the linear regression setting: R2 int = (SSY − SSE)/SSY, and R2 ave = ∫ T 0 ∑ R2(t) dt T , where SSY = ∫ T 0 ∑m i=1{y(t)}2 dt and R2(t) = 1 − ∑i(yi(t) − ̂yi(t))2/ ∑i(yi(t))2. The norm of the estimated coefﬁcients ∥b∥ gave a good summary of “wiggliness” of surface \u0003(s, t) when the values of ̂R2 are very close to each other for all penalization techniques considered. In general, smaller values of ∥b∥ gave rise to estimates of the regression surface with less ﬂuctuation. EDF is deﬁned as a function of a smoothing parameter . It is a generalization of the number of parameters in parametric ﬁtting. In the L2-norm penalization framework, EDF is deﬁned as the trace of the smoother matrix S .In the L1-norm, EDF is deﬁned as the number of non-zero parameters (Zou et al., 2005). A proposal by Demidenko (2004) of hAIC (healthy Akaike Information Criterion), a modiﬁcation of AIC, combines the previously discussed measures of the ﬁt quality. Here, we give a formula for L2- and L1-norm hAIC which equals hAIC = Hq − 2lmax + 2p, (8) where lmax is the maximum value of log-likelihood, p stands for the EDF and H1=2p ln(∥b∥1/p), H2=p(ln(∥b∥2 2/p)− 1). We present the derivations of hAIC for both cases in the Appendix. 4. Simulation study In order to evaluate our penalized ﬁtting procedures, we conducted a Monte Carlo simulation study. We examined the performance of the various penalization techniques under several different scenarios of the data generating mechanism. We varied the combinations of the parameters deﬁning the regression surface \u0003(s, t) and lag \u0006. For each data generating scheme, we simulated 100 data sets and examined a number of possible estimation procedures. We varied the number of basis functions, applied penalties based on L1- and L2-norm and speciﬁed the direction of penalization. We also tested our ﬁtting procedures for sensitivity to misspeciﬁcation of the lag \u0006 and direction of penalization. 4.1. Data generation In our simulation study, we deﬁned the functions Xi(t) and Yi(t) on the closed interval t ∈[0, 1] and s ∈[max(0,t − \u0006), t]. We chose our predictor and response to resemble the data from the motivating boilermaker study. We deﬁned subject-speciﬁc exposures curves Xi(t) = \u000bi 0 + \u000bi 1t + \u000bi 2 sin (\u000bi 3t), (9) where \u000bi 0 ∼ N(5, 1.52), \u000bi 1 ∼ N(6, 22), \u000bi 2 ∼ N(6, 22), and \u000bi 3 ∼ N(18, 0.82). As depicted in Fig. 3, we speciﬁed the regression surface as \u0003(s, t) = a1 sin (a2t − a3s) + a4, (10) and the intercept function \u0002(t) = 1.5(4 − 7t + 6.5t 2). Speciﬁcation of values for the coefﬁcients a2 and a3 of the regression surface \u0003(s, t) enabled us to keep the regression surface varying slowly in the primary directions of interest: horizontal (|a3|>|a2|), vertical (|a2|>|a3|) and parallel (a2 ≈ a3). We selected the a2 = 20,a3 = 4 (horizontal), a2 = 4,a3 = 20 (vertical), and a2 = 30,a3 = 28 (parallel) for the maximum lag \u0006 = 0.2, while for the lag \u0006 = 0.4 we used the same coefﬁcients for the horizontal and vertical directions and speciﬁed a2 = 60,a3 = 56 for the parallel direction. In our Monte Carlo simulations, we speciﬁed \u0006 and calculated the noisy version of yi(t), i = 1,...,m, by applying the integral transform (2) with \u0004i(t) ∼ N(0, 0.22). We show a sample of generated curves for one simulation setting 4918 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 1.5 2 2.5 3 3.5 4 4.5 s t 2 2.5 3 3.5 4 Fig. 3. Function \u0003(s, t) used for a simulation study with a2 = 20,a3 = 4. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -5 0 5 10 15 20 25 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 Fig. 4. Plot of a 50 simulated Xi (t) (top panel) and Yi (t) (bottom panel) curves. with the lag \u0006 = 0.2in Fig. 4. It is worth noticing that the responses depend on the predictors for the maximum lag of \u0006 = 0.2. However, it is not immediately obvious from the plot in Fig. 4 that the relationship does not extend for times s< t − 0.2. In order to compare the estimation methods, we discretized the problem. We sampled the curves yi(·) and xi(·) at 101 equally spaced points on the interval [0, 1]. As described in Section 2, we approximated \u0003(·, ·) by a ﬁnite-dimensional expansion in terms of known basis functions \u0007k(·, ·), k = 1,...,K. J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4919 4.2. Model ﬁtting We divided the interval [0, 1] into M subintervals. In the set of simulations with known and ﬁxed value of \u0006,we speciﬁed the number of the lag subintervals B to match the lag \u0006 in the data generating step. The total number of basis functions \u0007k(·, ·) is K =(B +1)(M +1−B/2). Thus, for instance, when \u0006=0.2 and M =20, we speciﬁed B ≡ M\u0006=4 and K = 95. In addition, in order to test the sensitivity of our ﬁtting procedure to misspeciﬁcation of the lag B we conducted a smaller simulation study with B/M ̸= \u0006. We chose M = 20 and B = 2, 3, 5 and 6 corresponding to underestimating the lag \u0006 for the B = 2 and B = 3, and overestimating \u0006 for B = 5 and B = 6. We compared the ﬁts under the considered penalization methods to the LS approach of Malfait and Ramsay (2003) using the criteria described in Section 3.3. These criteria included the explained variation (R2), Lq norms of the estimated coefﬁcients ∥b∥, equivalent number of degrees of freedom of the estimated surface and the hAIC. 4.3. Simulation results For each regression surface \u0003(s, t), we found the estimates of the coefﬁcients b of basis functions \u0003 using the LS method and both L1- and L2-norm regularization techniques. For the Lq norms, we compared simpliﬁed penalization where either one or two directions of variation are penalized. For each simulation setup, one of the penalization directions corresponded to the primary mode of constancy of the regression surface. In total, we ﬁt 36 models with varying number of basis functions and different penalization techniques to each generated regression surface for a lag \u0006 = 0.2 and 27 models for a lag \u0006 = 0.4. We present only selected results of simulations for the considered setups with a lag \u0006 = 0.2. In Table 2, we summarize the hAIC with L1-norm for all the setups. In general, the best ﬁts were obtained with the correct penalization of the Table 2 Simulation results for all directions of constancy with lags \u0006 = 0.2, \u0006 = 0.4 and misspeciﬁed discretized lags for \u0006 = 0.2 M(B) D LS L2-norm L1-norm DH DV DHV DP DH DV DHV DP 10 (2) −8798 −8846 −8827 −8814 −8824 −8843 −8815 −8749 −8813 15 (3) H −8943 −9201 −9154 −9129 −9130 −9196 −9175 −8931 −9131 20 (4) −8523 −9277 −9201 −9180 −9174 −9286 −9217 −9211 −9185 25 (5) −6690 −9266 −9165 −9134 −9132 −9280 −9196 −9157 −9158 10 (2) −9189 −9214 −9249 −9219 −9213 −9207 −9247 −9037 −9211 15 (3) V −8985 −9211 −9278 −9208 −9202 −9209 −9278 −8809 −9209 20 (4) −8536 −9220 −9307 −9205 −9202 −9222 −9315 −8594 −9227 25 (5) −6701 −9177 −9287 −9152 −9151 −9185 −9304 −8334 −9185 10 (2) −8902 −9064 −9063 −9067 −9046 −9058 −9053 −8873 −9064 15 (3) P −8572 −9195 −9195 −9166 −9198 −9056 −9048 −9044 −9052 20 (4) −7792 −9230 −9226 −9172 −9224 −8864 −8839 −8928 −8870 25 (5) −6588 −9231 −9220 −9146 −9229 −8835 −8885 −8703 −8872 20 (2) −8124 −8100 −8103 NA −8398 −8093 −8096 −8230 −8298 20 (3) H −8665 −8909 −8915 −8830⋆ −9075 −8917 −8900 −8829 −9083 20 (5) −8055 −9185 −9147 −9079 −9173 −9200 −9168 −8966 −9191 20 (6) −7399 −9175 −9133 −9110 −9169 −9199 −9162 −8972 −9191 20 (2) −8282 −8269 −8263 NA −8585 −8257 −8262 −8372 −8489 20 (3) V −8672 −8929 −8934 −8834⋆ −9100 −8946 −8953 −8841 −9110 20 (5) −8071 −9167 −9202 −9101 −9192 −9179 −9223 −8750 −9215 20 (6) −7421 −9164 −9194 −9117 −9188 −9185 −9218 −8744 −9219 20 (2) −8553 −8525 −8528 NA −8812 −8529 −8531 −8657 −8900 20 (3) P −8387 −8795 −8795 −8701⋆ −9091 −8613 −8609 −8587 −9122 20 (5) −7278 −9060 −9063 −8977 −9212 −8205 −8240 −8057 −8927 20 (6) −5470 −9052 −9053 −8988 −9208 −7972 −7827 −7853 −8491 Summary of the hAIC with the L1-norm penalty on the basis functions coefﬁcients. Bold numbers indicate the smallest values of hAIC in each row and bold italic the second smallest (for ﬁxed M and B). NA denotes the entries with computationally non-estimable coefﬁcients and ⋆ denotes the entries based on a limited number of the convergent runs. 4920 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 Table 3 Simulation results for the horizontal direction of constancy with lag \u0006 = 0.2 based on 100 simulated data sets M(B) LS L2-norm L1-norm DH DV DHV DP DH DV DHV DP 10(2) 30 29.01 29.54 28.27 29.59 26.28 28.61 28.49 28.50 15(3) 58 44.80 50.35 41.11 52.74 36.02 37.53 46.82 43.98 20(4) 95 54.92 66.29 49.96 68.72 40.86 54.10 52.15 52.99 25(5) 141 63.25 78.59 58.08 80.89 46.17 57.16 60.51 59.32 10(2) −8818 −8861 −8850 −8837 −8846 −8857 −8836 −8760 −8835 15(3) −8977 −9238 −9197 −9179 −9175 −9215 −9201 −8934 −9161 20(4) −8576 −9331 −9268 −9249 −9242 −9305 −9251 −9228 −9218 25(5) −6754 −9341 −9259 −9227 −9231 −9299 −9228 −9171 −9194 Top four rows give the summary of the equivalent degrees of freedom (EDF) and the bottom four rows give the summary of the hAIC with L2-norm. Directions of penalization: DH—horizontal, DV—vertical, DHV—horizontal and vertical, DP—parallel. 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 -400 -300 -200 -100 0 100 200 300 400 s t -300 -200 -100 0 100 200 300 Fig. 5. Coefﬁcients b estimated using least squares method. primary direction of constancy in regression surface \u0003(s, t). L1 penalization techniques produced estimates with the smaller number of EDF and smaller norm ∥b∥1, while L2 penalization techniques were more robust to misspeciﬁcation of the direction of penalization, and outperformed L1 penalties especially in the case of parallel direction of changes in the regression surface \u0003(s, t). Also, in most simulation scenarios, the quality of the ﬁt could not be evaluated using the R2 criterion, since most estimates were approximately the same. For example, for all penalized techniques and the number of subdivisions M = 15 or greater, all estimates of R2 were within 1% of each other. The equivalent number of degrees of freedom indicated that for a ﬁxed M and B, L1 penalization required fewer basis functions than the corresponding L2 penalization to provide a good ﬁt. When EDF was compared to the total number of basis functions used in LS ﬁtting, the proportion of degrees of freedom, deﬁned as EDF/K, decreased from 87.6% (for M = 10,B = 2,K = 30) to 32.7% (for M = 25,B = 5,K = 141) for the correct penalty with the L1-norm penalty (see Table 3). Simulation results for the lag \u0006 = 0.4 were very similar to the results summarized above for the \u0006 = 0.2 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4921 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6 s t 1 1.5 2 2.5 3 3.5 4 4.5 5 Fig. 6. Coefﬁcients b estimated using L1 penalization technique with horizontal penalty. lag. The LS ﬁt was comparable to the penalized ﬁts only for the smallest number of basis functions with M = 10 and B = 4. However, according to the hAIC, penalized ﬁts with correct direction of penalization and greater number of basis functions outperformed the LS ﬁt. Figs. 5 and 6 show the plot of the estimated coefﬁcients b using LS and L1 penalization, respectively. We could assess visually that the LS ﬁt is very wiggly with the values of the estimated coefﬁcients ranging from −300 to 300, while the neighboring coefﬁcients obtained via L1 penalization were close to each other. We also tested the sensitivity of the ﬁtting methods to misspeciﬁcation of the lag \u0006. We present the summaries of the hAIC with L1-norm for misspeciﬁed lags in Table 2. In all cases, the hAIC criterion was the smallest for the correct lag used for ﬁtting. When the empirical lag used in ﬁtting (B/M) was smaller than the lag \u0006 used for data generation, the quality of the ﬁt was poor. Overspeciﬁcation of the lag introduced more non-zero parameters than should be used in the ﬁt. However, the values of the extraneous parameters were very close to zero, which resulted in the ﬁt being only marginally worse than that obtained using the correct lag. 5. Analysis of the boilermaker data set Here, we analyze a subset of the boilermaker data set consisting of observations during the workday starting at 8:30 am and ending at 5 pm for 14 subjects. For each boilermaker, it was the ﬁrst workday they participated in the study. The data set consisted of HRV and PM2.5 measurements obtained every 5 min for a total of 103 observations per subject. It is worth noticing that the composition and the toxicity of the PM2.5 particles changed during the workday. For instance, all the boilermakers were required to congregate in closed quarters during the work breaks where the main source of the particulate pollution was cigarette smoke. We present the plot of the raw data in Fig. 7 with the break times indicated by vertical lines. We analyze the data using the methods described in Section 4. The total observation period was 510 min. In order to explore possible long- and short-term effects of PM2.5 on HRV, we speciﬁed three values for the minimum considered resolution: 30, 15 and 10 min corresponding to the discretization of observation period into M =17, 34, 51 subintervals, respectively. Of course, we do not know the value of lag \u0006 in a given application.We perform the analysis using the values of B = 3, 5, 7, 9 for the number of subintervals M = 17. For the other values of M = 34 and 51, we doubled 4922 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 8 9 10 11 12 13 14 15 16 17 -6 -4 -2 0 2 4 log(PM2.5) log(HRV) 8 9 10 11 12 13 14 15 16 17 1 2 3 4 5 6 Time Fig. 7. Plot of HRV and PM2.5 values for 14 boilermakers. Vertical lines indicate the compulsory breaks. Table 4 The hAIC values from the analysis of the boilermaker data set MB LS L2-norm L1-norm DH DV DHV DP DH DV DHV DP 3 −2185 −1624 −1624 NA −1624 −2041 −1965 −1832 −2097 17 5 −2406 −1624 −1624 NA −1624 −2324 −2146 −1408 −2309 7 −2611 −1624 −1962 NA −1624 −2507 −2347 −1307 −2477 9 −2773 −1624 −2269 NA −1624 −2615 −2391 −779 −2618 6 −3090 −1624 −2140 NA −1624 −2465 −2787 −2062 −2264 34 10 −3776 −1624 −2908 −2470 −1624 −3565 −3544 907 −3593 14 −3869 −1624 −3603 −2911 −1624 −3450 −4640 1677 −3542 18 −4668 −1624 −4020 −3206 −1624 −4546 −4693 2119 −4558 9 −4006 −1940 −2859 −2338 −1624 −3733 −3393 −734 −3756 12 −4282 −1624 −3530 −2707 −1624 −3800 −5046 2355 −3716 15 −4580 −1624 −4406 −3156 −1624 −4014 −5785 −233 −4094 51 18 −5715 −1624 −5213 −3622 −1624 −5269 −6458 3010 −5270 21 −5484 −1624 −5257 −3654 −1624 −5702 −6680 4189 −5760 24 5379 −1624 −5225 −3628 −1624 −6006 −6873 3397 −6119 25 12010 −1624 −5240 −3638 −1624 −6110 −6930 3503 −6156 26 69371 −1624 −5271 NA −1624 −6151 −7013 NA NA 27 56509 −1624 −5214 NA −1624 −6236 −6648 NA NA Bold numbers indicate the smallest values of hAIC in each row and bold italic the second smallest (for ﬁxed M and B). Directions of penaliza- tion: DH—horizontal, DV—vertical, DHV—horizontal and vertical, DP—parallel. NA denotes the entries with computationally non-estimable coefﬁcients. and tripled the number of subintervals B, respectively. Note that the longest lags considered (e.g. M = 17,B = 9) corresponded to a maximum lag effect of 4.5 h, which enabled us to explore long-term effects of PM2.5 exposure on HRV. The summary of the results is presented in Table 4. J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4923 s t 9 10 111213 141516 9 10 11 12 13 14 15 16 -0.2 -0.1 0 0.1 0.2 0.3 Fig. 8. ̂\u0003 estimated using L1 penalization technique relating exposure to PM2.5 and HRV for the boilermaker data set with vertical penalty—DV. When the analysis was performed on the coarse grid of 30 min (M =17), the LS method outperformed the penalization techniques. However, the hAIC value using L1 penalization was close to that obtained using LS. When the resolution was increased to 15 min (M = 34), the L1 penalty with the vertical direction of penalization yielded the best ﬁt. Increasing the resolution further to 10 min (M = 51) indicated that the vertical direction of penalization corresponded to the primary mode of constancy in the relationship between the exposure to PM2.5 and HRV in the occupational exposure of boilermakers. The minimum value of hAIC was achieved for the lag of 260 min (B = 26) when the 10-min minimum resolution (M = 51) was used in the analysis. The plot of the estimated regression surface ̂\u0003(s, t) (Fig. 8) indicates that the environmental tobacco exposure during the breaks had the strongest association with the variation in the HRV responses. Magari et al. (2001) analyzed a larger sample of 33 boilermakers over a time span extending to the non-working hours. They assumed constant effects of the 4 h moving average exposure to PM2.5 on HRV and found a signiﬁcant decrease in HRV for every 1 \u0002g/m3 increase in PM2.5 levels. We applied the mixed models approach taken in Magari et al. (2001) to our sample of 14 boilermakers. We compared the results of this approach to our method via the explained variation measure R2 int which was estimated to be ̂R2 int = 0.61 in the mixed model approach and ̂R2 int = 0.76 in our model having the minimum hAIC. This big improvement can be attributed to the relaxation of the assumption of constant effects of the pollution on the HRV during the study period. Our approach allowed us to explore various modes of association between the exposure and the outcome. 6. Conclusions and discussion We have developed a procedure for ﬁtting penalized functional linear models under various penalties, taking into account spatial relationships between the coefﬁcients of the basis function representation of the regression sur- face. We have compared the performance of our method to the regression spline approach proposed by Malfait and Ramsay (2003). Expression of an L2-norm penalization in a mixed model formulation has enabled us to obtain smoothing parameter estimates using restricted maximum likelihood. Also, equivalence of L1-norm penalties to weighted ridge regression 4924 J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 penalization simpliﬁed the computations in the case of modiﬁed LASSO procedures. The methods were implemented in MATLAB, and sample programs are available from the ﬁrst author. Our simulation study indicated that among the penalization techniques considered, L1 penalization provided ﬁts with smaller number of degrees of freedom than L2 ﬁts, while the variation explained was the same up to 3 or 4 signiﬁcant digits. Least squares ﬁtting was comparable with the penalization techniques only when the number of basis functions was the smallest considered (M = 10). However, hAIC values obtained from the models using a larger number of basis functions were much smaller than those from models with few basis functions, indicating the need for ﬁner discretization of the region of integration for the functions considered. Finally, we applied our methods to assess the relationship between the exposure to pollutants and heart rate variability in the occupational exposure setting of boilermakers. The analysis indicated that primarily the point exposure to PM2.5 during the breaks showed the strongest association with changes in HRV in this sample of boilermakers. As in the simulation studies, the penalized techniques outperformed the least squares ﬁtting method in most cases. In addition, comparison with the mixed model methodology used in the Magari et al. (2001) favored our approach according to the explained variation criterion. It is worth noticing that model (2) is a generalization of the varying-coefﬁcient model (Hastie and Tibshirani, 1993) where the response Yi(t) depends on the predictor Xi(t) only concurrently at time t: Yi(t) = \u0002(t) + \u0003(t)Xi(t) + \u0004i(t). Another special case of model (2) is a distributed lag model (Almon, 1965; Zanobetti et al., 2000) deﬁned as Yi(t) = \u0002 + D∑ d=0 \u0003d Xi(t − d) + \u0004i(t), where the intercept, \u0002, is constant over time t, and the coefﬁcients \u0003d , 0 ⩽d ⩽D, depend on the lag time d = t − s between the observed Yi(t) and Xi(s) values. Application of the Zanobetti et al. (2000) approach to the boilermaker data set provided a minimal improvement over the mixed model approach of Magari et al. (2001), but still yielded a poorer ﬁt than that provided by our proposed historical model. Possible extensions of our methods include incorporation of dependence of response variables on additional covariates either scalar or time-varying. Also, an extension including higher-order ﬁnite elements and penalization of additional neighboring coefﬁcients might provide a better ﬁt to smooth surfaces than the piecewise linear basis functions. Acknowledgments This work was supported in part by NIH Grants ES07142, GM29745, ES012044, and ES00002. The authors thank the editor and the two reviewers for comments and suggestions for improvement. Appendix We present here the justiﬁcation beyond the healthy Akaike Information Criterion (hAIC) of Demidenko (2004), which is based on AIC—a popular method for model selection, especially for non-nested models. It is a well-known fact that in the linear regression models AIC =−2lmax + 2p, where lmax is the value of the maximized log-likelihood and p is the number of parameters. The best model according to AIC is the model with a minimum AIC. There are, however, weaknesses of AIC that have been pointed out by Demidenko (2004). A major weakness of AIC relevant to our research is a poor discrimination of models with multicollinear predictors. Let L(b; y) be conditional likelihood of data y on a p-dimensional random coefﬁcients b. If we assume bi ∼ G for i = 1,...,p, then the density of b is \f−pG(\f−1b), where \f is a scale parameter. Let g = ln G, then the marginal likelihood of \f takes the form l(\f) =−p ln \f + ln ∫ Rp el(b;y)+g(\f−1b) db, (11) and MLÊ\f maximizes Eq. (11). Using Laplace approximation for the integral in Eq. (11), we can obtain the following approximate expression for l(\f): l(\f) ≈−p ln \f + l(b; y) + g(\f−1b). (12) J. Harezlak et al. / Computational Statistics & Data Analysis 51 (2007) 4911 – 4925 4925 Assuming that the marginal distribution of b is Gaussian, we obtain a simpliﬁed expression for the likelihood of \f: l(\u00052) ≈− p 2 ln \u0005 2 + l(b; y) − 1 2\u00052 ∥b∥2. (13) Eq. (13) is maximized over the variance at ̂\u00052 =∥b∥2/p giving lmax =− p 2 ln(∥bML∥2/p) + l(bML; y) − p 2 . (14) Now, the healthy AIC takes the form hAIC = H − 2lmax + 2p = H + AIC, (15) where H ≡ H2 = p(ln(∥bML∥2 2/p) − 1). Another popular choice for the marginal distribution of b is double-exponential with a scale parameter \r. In this case, the H in the healthy AIC takes the form H ≡ H1 = 2p ln(∥bML∥1/p). References Almon, S., 1965. The distributed lag between capital appropriations and expenditures. Econometrica 33, 178–196. Braess, D., 2001. Finite Elements: Theory, Fast Solvers, and Applications in Solid Mechanics. Cambridge University Press, Cambridge. Brenner, S.C., Scott, R.L., 2002. The Mathematical Theory of Finite Element Methods. Springer, Berlin. Brumback, B.A., Ruppert, D., Wand, M.P., 1999. Comment on “Variable selection and function estimation in additive nonparametric regression using a data-based prior”. J. Amer. Statist. Assoc. 94, 794–797. Cuevas, A., Febrero, M., Fraiman, R., 2002. Linear functional regression: the case of ﬁxed design and functional response. Canad. J. Statist. 30, 285–300. Demidenko, E., 2004. Mixed Models. Wiley Interscience, New York. Dockery, D.W., Pope, C.A., Xu, X., Spengler, J.D., Ware, J.H., Fay, M.E., Ferris, B.G.J., Speizer, F.E., 1993. An association between air pollution and mortality in six U.S. cities. New England J. Med. 329, 1753–1759. Eilers, P.H.C., Marx, B.D., 1996. Flexible smoothing with B-splines and penalties (Disc: pp. 102–121). Statist. Sci. 11, 89–102. Grandvalet, Y., 1998. Least absolute shrinkage is equivalent to quadratic penalization. In: Perspectives in Neural Computing.Springer, Berlin, pp. 201–206. Hastie, T., Tibshirani, R., 1993. Varying-coefﬁcient models. J. Roy. Statist. Soc. Ser. B 55, 757–796. He, G., Müller, H.-G., Wang, J., 2003. Functional canonical analysis for square integrable stochastic processes. J. Multivariate Anal. 85, 54–77. Magari, S.R., Hauser, R., Schwartz, J., Williams, P.L., Smith, T.J., Christiani, D.C., 2001. Association of heart rate variability with occupational and environmental exposure to particulate air pollution. Circulation 104, 986–991. Malfait, N., Ramsay, J.O., 2003. The historical functional linear model. Canad. J. Statist. 31, 185–201. Pope, C.A., Hansen, M.L., Long, R.W., Nielsen, K.R., Eatough, N.L., Wilson, W.E., Eatough, D.J., 2004. Ambient particulate air pollution, heart rate variability, and blood markers of inﬂammation in a panel of elderly subjects. Environ. Health Perspect. 112, 339–345. Ramsay, J.O., Dalzell, C.J., 1991. Some tools for functional data analysis (Disc: pp. 561–572). J. Roy. Statist. Soc. Ser. B 53, 539–561. Ramsay, J.O., Silverman, B.W., 1997. Functional Data Analysis. Springer, Berlin. Ramsay, J.O., Silverman, B.W., 2002. Applied Functional Data Analysis: Methods and Case Studies. Springer, Berlin. Ruppert, D., Wand, M.P., Carroll, R., 2003. Semiparametric Regression. Cambridge University Press, Cambridge. Tikhonov, A., Arsenin, V., 1977. Solutions of Ill-Posed Problems. Wiley, New York. Zanobetti, A., Wand, M.P., Schwartz, J., Ryan, L., 2000. Generalized additive distributed lag models: quantifying mortality displacement. Biostatistics 1, 279–292. Zou, H., Hastie, T., Tibshirani, R., 2005. On the “degrees of freedom” of the lasso. Technical Report, Stanford University.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources.backup/Yang22elecPriceFrstGraphAttntn.pdf","text":"Received: 26 July 2021 Revised: 23 November 2021 Accepted: 22 January 2022 IET Renewable Power Generation DOI: 10.1049/rpg2.12413 ORIGINAL RESEARCH Short-term electricity price forecasting based on graph convolution network and attention mechanism Yuyun Yang1 Zhenfei Tan 1 Haitao Yang 1 Guangchun Ruan1 Haiwang Zhong1 Fengkui Liu2 1 State Key Laboratory of Power Systems, Department of Electrical Engineering, Tsinghua University, Beijing, China 2 ArtiÔ¨Åcial Intelligence Application Research Department, China Electric Power Research Institute, Beijing, China Correspondence Haiwang Zhong, State Key Laboratory of Power Systems, Department of Electrical Engineering, Tsinghua University, Beijing 100084, China. Email: zhonghw@mail.tsinghua.edu.cn Funding information National Key R&D Program of China, Grant/Award Number: No. 2020YFB0905900; supported by Ministry of Science and Technology of the People‚Äôs Republic of China Abstract In electricity markets, locational marginal price (LMP) forecasting is particularly important for market participants in making reasonable bidding strategies, managing potential trading risks, and supporting efÔ¨Åcient system planning and operation. Unlike existing methods that only consider LMPs‚Äô temporal features, this paper tailors a spectral graph convolu- tional network (GCN) to greatly improve the accuracy of short-term LMP forecasting. A three-branch network structure is then designed to match the structure of LMPs‚Äô compositions. Such kind of network can extract the spatial-temporal features of LMPs, and provide fast and high-quality predictions for all nodes simultaneously. The attention mechanism is also implemented to assign varying importance weights between different nodes and time slots. Case studies based on the IEEE-118 test system and real-world data from the PJM validate that the proposed model outperforms existing forecasting models in accuracy, and maintains a robust performance by avoiding extreme errors. 1 INTRODUCTION 1.1 Background With deregulation in global power industries over the last several decades, more than 30 countries/territories have established their electricity markets [1], while many developing countries also initiated electricity market reforms recently [2]. In most of these markets, the locational marginal price (LMP) mecha- nism is widely applied owing to its certiÔ¨Åed features of incen- tive compatibility, revenue adequacy, cost causation-awareness, and transparency [3]. Therefore, LMP forecasting helps market participants and system operators in various decision-making scenarios, including multi-area market coordination [4], energy sharing [5], bidding [6], microgrid dispatch [7], energy storage scheduling [8], and building energy management [9]. Among all prediction tasks in power systems, short-term LMP forecasting is more difÔ¨Åcult than its competitors, for example, load forecasting, renewable generation forecasting, and the reasons mainly lie in two aspects: First, LMPs are inÔ¨Çu- enced by many more factors and thus become more volatile [10]. This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non-commercial and no modiÔ¨Åcations or adaptations are made. ¬© 2022 The Authors. IET Renewable Power Generation published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology. The uncertainty poses signiÔ¨Åcant challenge to LMP forecast- ing. Second, LMPs rely heavily on commercial bidding strate- gies of market participants [11], which are private information and hard to collect. Third, LMPs at different locations are often spatially related due to the system network connection [12], so time-series analysis alone may not be enough. Most existing methods treat LMPs at different nodes as inde- pendent time series, and it remains challenging on how to inte- grate the spatial correlation in LMP forecasting. Therefore, we pay special attention to the third reason and point out that ignor- ing LMPs‚Äô spatial inter-dependency could restrict the forecast- ing accuracy. 1.2 Related works Time-series models are applicable for LMP forecasting and often exhibit understandable intuitions and physical interpre- tations. Reference [13] implemented a seasonal auto-regressive integrated moving average (ARIMA) model to generate the LMP scenario set which was informative to show the price IET Renew. Power Gener. 2022;16:2481‚Äì2492. wileyonlinelibrary.com/iet-rpg 2481 2482 YANG ET AL. uncertainty. The extended ARIMA approach in [14] was able to estimate the conÔ¨Ådence intervals of the predicted LMPs. In [15], the day-ahead LMP forecasting could reach an average 11% weekly mean absolute percentage error (MAPE). With the booming of various machine learning tech- niques [16], more and more researchers have shifted their focus to this new direction. In general, the machine learning mod- els are advantageous to handle complex and nonlinear correla- tions [17], making it become a powerful toolbox for tough fore- casting tasks. Reference [18] applied the deep neural network (DNN) and support vector regression to predict real-time mar- ket prices, and its mean square error reached 20.51 $‚àïMWh2. Reference [19] compared the performance of traditional AR models with deep learning approaches, for example, long-short term memory (LSTM) networks. Reference [20] took a step for- ward by formulating a hybrid DNN-LSTM network to simulta- neously predict day-ahead prices in several countries. This net- work achieved 13.06% of symmetric mean absolute percent- age error (sMAPE). Reference [21] stacked the decision tree regressor, random forest regressor, and extremely randomized tree regressor to predict different components of LMPs, and achieved the best mean absolute error (MAE) of 4.08 $‚àïMWh. In the above references, LMPs at different nodes are pre- dicted independently, failing to extract the inherent spatial cor- relations among different locations, especially in congestion sit- uations. It is generally believed that the spatial distribution of LMPs follows certain modes, but very limited attention is paid to this aspect, and simultaneous prediction of LMPs at multiple locations has not received much attention. The recent graph convolutional network (GCN) offers a way to incorporate the spatial correlation of LMPs in a neu- ral network. The GCN model was Ô¨Årst proposed in [22]to broaden the traditional convolution networks to incorporate graph-structured data. GCN is thus a promising candidate to capture the topological relationship of LMPs at different loca- tions. Previously, the GCN was successfully applied in trafÔ¨Åc Ô¨Çow forecasting [23]. GCNs were also utilized to forecast wind power generation incorporating spatial correlation in [24], and solved unit commitment and economic dispatch problems [25]. However, there was no paper that considered LMP forecasting with GCNs so far. It should also be pointed out that classical GCNs cannot capture the temporal correlations, which is a simple task for various recurrent neural networks, for example, [20, 26]. How- ever, simultaneous consideration of temporal and spatial corre- lation of LMPs has not been reported in the existing works of literature. To this end, we propose a novel model based on the GCN and temporal convolution (namely ST-Conv), and a useful tech- nique, attention mechanism, is also employed. The major con- tributions of this paper are summarized as follows: ‚àô Different from the existing works, the proposed model can capture the spatial and temporal features of LMPs simulta- neously. The spatial correlation is encoded with a spectral graph convolutional network by modeling the electric grid as an undirected graph, while the temporal correlation is cap- tured by a one-dimensional convolutional network. ‚àô The proposed model can handle all LMPs of different loca- tions simultaneously, and the spatial correlation is thus accu- rately and efÔ¨Åciently stored in this pattern. ‚àô The attention mechanism is implemented to guide the model to distinguish and focus more on the important input infor- mation. The remainder of this paper is organized as follows. Section 2 introduces the overall forecasting framework, Section 3 elabo- rates the key techniques, including the GCN implementation, temporal convolution, and attention mechanism. More details about how the forecasting model is trained are clariÔ¨Åed in Sec- tion 4. Several simulation results are discussed in Section 5. All boundary conditions, from which our dataset is generated, are from real load data of the 2016-2018 PJM market. At last, Sec- tion 6 draws the conclusions. 2 FRAMEWORK 2.1 LMP derivation and compositions The LMP for a network node is deÔ¨Åned as the incremental oper- ating cost to supply another megawatt of power at this node. Mathematically, the LMP is the optimal dual variable of the economic dispatch problem. The economic dispatch problem optimizes the total welfare of the whole system, which equals the total utility of consumers minus the total generation cost of generators. The LMP at each node consists of three parts: the energy component ùúÜ‚àà ‚Ñù, congestion component ùúá‚àà ‚ÑùN , and network loss component ùúà‚àà ‚ÑùN [12]. The network loss accounts for a minor percentage of LMP, so we ignore this component in the prediction. Then LMP can be represented as the sum of energy component ùúÜ and congestion component ùúá. When the congestion occurs, LMPs at different nodes will deviate from each other since congestion components become non-zero. The spatial distribution of the LMP follows certain modes. The congestion component of the LMP takes the form of a linear combination of power transmission distribution fac- tors (PTDF) of congested transmission lines and thus, LMPs at different nodes are restricted to an afÔ¨Åne subspace, which implies the necessity to put emphasis on the spatial correlation of LMPs. Take LMPatnode i for instance, LMPi =ùúÜ + ùúái =ùúÜ + m‚àë k=1 tW ‚à∂i,kùúák, (1) where tw‚à∂i,k is the constraint k‚Äôs power Ô¨Çow sensitivity to the injection at node i concerning the slack reference W ,and m is the number of constraints. From the view of power transfer distribution factor (PTDF), a certain congestion component ùúá represents a dual multi- plier by the restrictions of its corresponding power line, which 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License YANG ET AL. 2483 TABLE 1 Parameters of layer shape Input dimension Output dimension Ka Tb Attention layer T √ó NT √ó N ‚àï‚àï ST-Conv 1 T √ó NT √ó N √ó 128 2 3 √ó 1 ST-Conv 2 T √ó N √ó 128 T √ó N √ó 128 2 3 √ó 1 Full connect T √ó N √ó 128 1 √ó N √ó q ‚àï‚àï aGraph receptive Ô¨Åeld. bTemporal kernel‚Äôs shape. implies that the congestion components are restricted to the row space of PTDF. That means LMP would be affected by the topological connections between nodes. Traditional neural-network-based models directly give LMP predictions node by node according to its historical informa- tion. However, in this work, we try to forecast LMP at all nodes simultaneously. This leads to an additional challenge that models can hardly give strict zero-ùúá at all nodes when there is no con- gestion. Thus, we provide a novel LMP decomposition: LMP = ùúÜ+ sùúá, where congestion factor s ‚àà{0, 1}. The model addition- ally proposes a binary prediction of s. When s = 0, there is no congestion in the system, so LMP at all nodes equals ùúÜ; other- wise when s = 1, indicating congestion occurs and all non-zero dual multipliers should contribute to LMP,and LMP =ùúÜ + ùúá. In this way, we are able to give precise predictions at all nodes within one procedure. Unlike existing methods that take the temporal features of LMP into account only, the proposed method based on Graph Convolutional Network (GCN) considers a power system as an undirected graph and builds a spectral graph convolutional net- work to extract the spatial features of LMP. Furthermore, we propose an attention mechanism and temporal convolution to enhance its performance. The GCN-based method can forecast the LMP of all nodes simultaneously, making up for the defect that traditional neural networks can not accurately restore the spatial information of LMP. 2.2 Structure of the LMP forecasting model The proposed forecasting networks (GCN and its descendant: Attention-based Spatial-Temporal Graph Convolutional Net- work, ASTGCN) share the same structure of three similar branches (Figure 1), respectively, forecasting different com- ponents ùúÜ, s, and ùúá. For each branch, the input is always the historical loads of nodes in a topological structure. Then it goes through a temporal/spatial attention layer and the loads‚Äô value will be re-weighted according to their importance for prediction. After that, two identical consecutive layers of graph-temporal convolution would convolve nodes and their neighbours to get internal information. Finally, the full connect layer formulates the required shape of ùúÜ, s, and ùúá as output. Their composition LMP =ùúÜ + sùúá produces the predicted LMP. In different branches, we set corresponding parameters as Table 1 shows. The value of q depends on which branch the layer is in. For ùúÜ, q = 1, indicating an only value is determined for the energy component; for s, q = 2, indicating the possibil- ity of congestion or not; for ùúá, q = 16, using a 16-dimension tensor to represent characteristics of congestion. The detailed structure of ASTGCN will be introduced in the following sections. 3 MODEL The proposed forecasting model aims to solve this prob- lem: Given ùúí= (X1, X2,‚Ä¶, Xn) ‚àà‚Ñù‚Ñï, which denotes histor- ical power loads of all the nodes in the power system, it needs to forecast LMP at all nodes for the same moment. We assumed that the power system contains N nodes, and for each predic- tion, load data of previous T Hours of loads are available. A detailed zoom-in of one forecasting branch for instance is shown in Figure 2. Input load data comprising history loads at all nodes Ô¨Årst come through a pre-trained attention block and thus become a mapped input. The attention block contains two masks respectively designed for time and space pattern extrac- tion. After that, the mapped input will be processed by two consecutive ST-Conv blocks (made up of Graph and Tempo- ral Convolution). Finally, a Fully Connect layer would synthesize the patterns learnt from the input and give an LMP prediction of all nodes for the next future moment. 3.1 Input The initial input of the whole model contains the historical loads(MWh) at all nodes structured in table format. In the stud- ied case, the previous hourly loads of T hours are used as an input, whose shape is of T hour √ó N nodes. 3.2 Attention block The attention block consists of two masks corresponding to time and space dimension, namely spatial attention and tempo- ral attention, to help adaptively identify the correlations between different time-points/system-nodes to reduce the computing power requirements of the original spatial-temporal graph con- volution network. The masks are pre-trained with the train dataset, which is named ‚Äùtrainable mapping‚Äù in Figure 3.For example in node dimension, the history load input ùúír‚àí1 t will be processed according to: S = Vs √óùúé(ùúír‚àí1 t W ùúír‚àí1 t T + bs) , (2) where ùúír‚àí1 t = (X1, X2,‚Ä¶, XTr‚àí1 ) ‚àà‚ÑùN √óTr‚àí1, r is the layer index, N is node number, Tr‚àí1 is time period length of the r ‚àí 1 layer, W ‚àà‚ÑùN √óTr‚àí1, bs ‚àà‚ÑùN √óN . Vs, W , bs are trainable parameters, and we use sigmoid as an activation function. Atten- tion masks are dynamically computed for different inputs, Si, j represents the correlation strength of node i and node j . A nor- 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 2484 YANG ET AL. FIGURE 1 Framework of LMP-Forecasting ASTGCN FIGURE 2 One branch of LMP-Forecasting ASTGCN FIGURE 3 Training and deducing of attention mapping 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License YANG ET AL. 2485 malization is introduced to S rows (Equation 3). S i, j ‚Ä≤ = exp (S i, j ) ‚àëN j =1 exp (S i, j ) . (3) The internal procedure of the attention block is shown in Figure 3. The input historical nodal loads are consecutively re-weighted (can be regarded as a ‚Äômapping‚Äô) Ô¨Årst by tempo- ralAtt and then spatialAtt mask based on their co-importance to forecasting LMP. More speciÔ¨Åcally, the input of the Atten- tion Block will generate two attention masks (N nodes √ó N nodes for SpatialAtt Mask and T hours √ó T hours for TemporalAtt Mask), and then the input would do dot-product with the two masks one after another to obtain a mapped input. 3.3 ST-conv block To capture both patterns in time and space, a spatial-temporal convolutional (ST-Conv) block structure (ST-Conv block in Figure 2) is adopted in this model. It contains two modules: the Ô¨Årst is a spectral graph convolution layer to extract the spa- tial features of LMP, then the second is a traditional convolu- tion in the time dimension to exploit history dependencies for each node. 3.3.1 Graph convolution The spectral graph theory generalizes the convolution operation from grid-based data to graph structure data and accelerates it with spectral techniques. The power system is naturally one of such graphs. As for the graph, we applied ChebNet[27]tothe graph convolution layer, which uses Chebyshev polynomial to reduce its computational costs and to accelerate the convolution process. The GCN layer based on ChebNet can be represented as follows: y =ùúé(gùúÉ ‚àó x) =ùúé ( K ‚àí1‚àë k=0 ùúÉkTK (ÀúL) x ) , (4) where x denotes the layer input, y denotes the output. ÀúL is the convolution kernel whose parameters rest with training and ÀúL = 2L ùúÜmax ‚àí IN = U ÀúŒõxU T . K represents a K th truncation of Chebyshev polynomial, and this also leads to a K-step recep- tive Ô¨Åeld of graph convolution. TK is a Chebyshev polynomial Tk(x) = 2xTk‚àí1(x) ‚àí Tk‚àí2(x), where T0(x) = 1, T1(x) = x. ÓâÑ (r ) h = ReLU (Œ¶‚àó (ReLU ( gùúÉ ‚àóG ÃÇÓâÑ (r‚àí1) h ))) ‚àà‚ÑùN √óT , (5) where ÓâÑ (r ) h represents the output of the r th layer, ‚àó denotes a standard convolution operation, Œ¶ is the trainable parameters of the temporal dimension convolution kernel, and the acti- vation function is ReLU. Since the data along the temporal dimension are aligned (a.k.a. Euclidean), a standard convolu- tion is enough to extract the potential inÔ¨Çuence from previous data. 3.3.2 Convergence of graph convolution For a random graph G = (A, Z )with n nodes represented by a symmetric adjacency matrix A ‚àà{0, 1}n√ón and a matrix of input signals over the nodes A ‚àà‚Ñù n√ódz , ref. [28] pro- vided a proof of its convergence with a polynomial con- centration O(1‚àï ‚àö ùõºnn), where ùõºn is the sparsity level of the graph. 3.4 Fully connected layer The output of the ST-Conv block does not meet the shape that forecasting requires, so the output is altered to an appro- priate shape with dot-product. As illustrated in Table 1,the ST-Conv block output of shape T √ó N √ó 128 is transposed to 1 √ó N √ó 128 √ó T and then multiplied by a corresponding matrix to formulate the shape of 1 √ó N √ó q. Finally, the three branches are respectively sum-reduced along with q, which is of N √ó 1. The forecasting LMP of each node is a composition of all three branch outputs. LMP = ÓâÑùúÜ + ÓâÑsÓâÑùúá ‚àà‚ÑùN , (6) where ÓâÑùúÜ, ÓâÑs,and ÓâÑùúá denote the output of the three branches, which are ‚àà‚ÑùN . 4 TRAINING METHOD All neural networks need training, details of how the training is set in the proposed model will be discussed in this section. 4.1 Loss function Loss function plays an important role in model training. The proposed model contains three branches, so the necessary part is to decide how each branch weighs in the overall loss function. We set varying loss functions for different branches as below: lossenergy = ‚ÄñùúÜpred ‚àíùúÜGT ‚Äñ1 + ‚ÄñùúÜpred ‚àíùúÜGT ‚Äñ2, (7) losscongest = ‚Äñùúápred ‚àíùúáGT ‚Äñ1 + 2‚Äñùúápred ‚àíùúáGT ‚Äñ2, (8) lossstatus = cross_entropy( spred , sGT ) , (9) 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 2486 YANG ET AL. where ‚Äñ ‚ãÖ ‚Äñ1 is 1-norm, ‚Äñ ‚ãÖ ‚Äñ2 is 2-norm. We set a target loss function weighing the ones above for the training process: losstotal = lossenergy + 10 ‚ãÖ losscongest + 100 ‚ãÖ lossstatus. (10) 4.2 Parameter initialization The network contains a lot of parameters to be trained, or namely trainable parameters. They need to be pre-set before the training begins, which is called weight initialization. In deep learning networks, it could determine the layer outputs during the course of a forward pass through the network. If either the outputs‚Äô vanishing or exploding occurs, loss gradients will either be too large or too small to Ô¨Çow backwards beneÔ¨Åcially, and the network will take longer to converge, if it is even able to do so at all. In the proposed network, Xavier Initialization [29] is applied, which sets a layer‚Äôs weights to values chosen from a random uniform distribution that‚Äôs bounded between ¬± ‚àö 6 ‚àöni +ni+1 , where ni is the number of incoming network connections, or ‚Äúfan-in‚Äù to the layer, and ni+1 is the number of outgoing network con- nections from that layer, also known as the ‚Äúfan-out‚Äù. Accord- ing to Glorot and Bengio, Xavier Initialization can maintain the variance of activations and back-propagated gradients up or down the layers of a network and therefore brings substantially faster convergence. 4.3 Training settings The proposed LMP forecasting model is implemented with TensorFlow 1.14. The training process involves multiple hyper- parameters. We tested the number of the terms of Chebyshev polynomial K ‚àà{1, 3, 5}, and the accuracy is exalted with K ris- ing. However, the computing cost increased rapidly, so for a better trade-off in both forecasting performance and comput- ing efÔ¨Åciency, we set K = 3. Similarly, the kernel size of time- convolution is also set to 3 √ó 1. The model is optimized using Adam Optimizer and the initial learning rate is set to 1e-4 for 100 epochs. This work is examined by comparing MLP, GCN and AST- GCN. The following Figure 4 shows how the outputs are gener- ated with the corresponding inputs. The GCN only takes in the latest loads to give a forecast of LMP for the next period, while ASTGCN (adding temporal Conv and then attention) gives the forecast using previous T hours. 5 CASE STUDY 5.1 Dataset description The dataset involved in this work contains the topology of one IEEE-118 power system. The historical hourly loads and LMPs of all 118 nodes within 3 years are also included (Table 2). It is FIGURE 4 Input and Output of GCN/ASTGCN TABLE 2 Dataset details Topology Vertices Freq Time span IEEE-118 118 1 point/hour 3 year divided into a train set and a test set with a proportion of 2:1. To be more speciÔ¨Åc, the Ô¨Årst 2 years are set aside for training and the remaining 1 year is for testing. The dataset originates from the IEEE 118 case (of which the power line topology is given). We select real load data from the 2016-2018 PJM market [30], including 26 load areas. The other 92 nodes‚Äô loads are generated by linear-weighing the data above and adding noise as following. For moment t , denote the known load data by d 26√ó1 t , then the generated load for the other 92 areas Àúd 92√ó1 t are: Àúd 92√ó1 t = W 92√ó26d 26√ó1 t +ùõºN 92√ó1(0, 1), (11) where W 92√ó26 is a weight matrix obtained by sampling with Dirichlet distribution, which satisÔ¨Åes: ‚àë j Wi, j = 1,‚àÄi. (12) To induce system congestion, we add transmission capacity constraints to the highest mean-transmission-power lines. Also, we assume that bid curves are of quadratic function and are sub- ject to stochastic noises. For generator i at moment t with active power output gi,t , the bidding function is: Ci( gi,t ) = c2,i (t )g2 i,t + c1,i (t )gi,t , (13) where c2,i, c1,i are coefÔ¨Åcients depending on time and noises: c2,i (t ) = 1 1000 ‚àë j d j ,t c20,i + 0.001Óà∫ (0, 1), (14) c1,i (t ) = ( 0.5 + 1 50000 ‚àë j d j ,t ) c10,i + 0.5Óà∫ (0, 1), (15) 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License YANG ET AL. 2487 FIGURE 5 Predictions of GCN and ground truth at node 52 TABLE 3 GCN performance on test dataset MAE/($/MWh) RMSE/($/MWh) MAPE/(%) 0.6750 1.352 1.691 where c20,i, c10,i are bidding coefÔ¨Åcients of generator i in IEEE 118, Óà∫ (0, 1) is standard normal distribution. By solving the economic dispatch problem, we could acquire a dataset containing what is needed to train and evaluate the proposed model, which is separated into two parts: 2016-2017 as train dataset and 2018 as test dataset. In the case study, we suppose the model has acquired all history loads for each node as required in Figure 4 and tries to forecast the upcoming LMP at each node in the power system. With the generated dataset above, we compare the perfor- mance of load forecasting among traditional MLP (Multi-Layer Perceptron), GCN, and ASTGCN (GCN with temporal convo- lution and attention mechanism). 5.2 Contrasts with existing models The performance of GCN on the test dataset is shown in Table 3. To observe how GCN performs at a single node by time, we take the forecasted LMP curve at node 52 as an exam- ple in Figure 5. With focused sight into the red boxes on the LMP curve, we get detailed prediction performance visualizations in Figures 6 and 7. It can be induced from the Ô¨Ågures that GCN could make precise LMP forecasting, especially when the Ô¨Çuctuations are gentle like that in Figure 7. We then construct an ARIMA and a Multi-Layer Perceptron (MLP) based model to represent what is widely adopted in pre- vailing LMP forecasting works for comparison. The major dis- advantage of these two models is that for each node a sepa- rate model needs to be constructed to forecast its LMP, and this also hampers them to Ô¨Ånd potential relations between dif- TABLE 4 MAE comparison of GCN, ARIMA and MLP Node Index GCN ARIMA MLP 21 1.024 5.673 4.480 49 1.251 7.865 2.706 52 1.071 12.323 3.150 76 1.111 5.430 1.211 85 1.016 5.208 1.176 101 1.061 4.126 2.021 TABLE 5 RMSE comparison of GCN, ARIMA and MLP Node Index GCN ARIMA MLP 21 1.427 10.267 7.910 49 1.750 12.636 5.130 52 1.623 21.321 6.777 76 1.597 9.635 2.004 85 1.508 9.065 1.934 101 1.570 6.732 3.874 ferent nodes. As for the hyperparameters/structure, ARIMA model is calibrated with p = 15, q = 14, d = 1, and a structure of MLP network resembling that of GCN with three branches is proposed as in Figure 8. The MLP uses 10 hidden layers with 128 neurons for each. We randomly select node 21, 49, 52 ,76 ,85, and 101 and train one MLP model for each nodes‚Äô LMP forecasting. The comparison between ARIMA, MLP and GCN including MAE and RMSE is shown in Tables 4 and 5,which strongly demonstrates GCN‚Äôs effectiveness as it achieves much lower errors in terms of both metrics. According to the experiment, GCN shows a prominent advantage over the traditional regression or MLP methods in LMP forecasting, including its great improvement in precision and simpliÔ¨Åcation (as we need only one model for all nodes with GCN). 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 2488 YANG ET AL. FIGURE 6 Zoom-in 1 of ground truth and GCN‚Äôs predictions at node 52 FIGURE 7 Zoom-in 2 of ground truth and GCN‚Äôs predictions at node 52 FIGURE 8 MLP structure 5.3 GCN versus ASTGCN GCN could extract the topological correlations among nodes, but the history data of nodes‚Äô loads are utterly omitted. However, in reality, previous loads‚Äô trends usually have some impact on future LMP, on which basis traditional statistical models were built. Thus, an enhanced GCN with Temporal Convolution and Attention 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License YANG ET AL. 2489 FIGURE 9 Attention mask of nodes Mechanism taking historical data into account is proposed in this work. The introduced Graph Convolution takes effect in extracting topological information, while the added temporal convolution tries to discover the inÔ¨Çuence of time-continuity of power loads. In addition, the purpose of borrowing attention mechanism is to weigh historical node loads differently, meanwhile exalting the inter-weights of highly related nodes. Figure 9 shows a group of typical attention masks (illustrated in Section 3.2) among nodes generated with one input. The colored square of row i column j represents the inÔ¨Çuence of j th node on ith node (the redder means the higher importance). Thus, along the column axis, it is easy to discover some nodes are of higher inÔ¨Çuence on other nodes (like 5, 9, 10, 25, 26, 30, 37, 38, 61, 63, 64, 65, 68, 69, 71, 81, 87, 89, 111). From the topology of IEEE 118 (Figure 11), most of these nodes are located in crossroads of power lines or at the only neighbour of generators. Some of them are not that special, while their high attention scores indi- cate their signiÔ¨Åcance in LMP prediction. This feature shows an interpretable advantage of the proposed model highlighted hot nodes and connections‚Äô strength among them. Figure 10 shows the attention scores among different periods in the same way. ASTGCN brings the model reduced RMSE at most nodes (shown in Figure 12), showing a consistent superiority of AST- GCN in most cases. At nodes 70, 71 and 72, however, the AST- GCN fails to give a better prediction. This might comes from the accumulating errors when there occurs a highly Ô¨Çuctuating LMP curve. Since the ASTGCN additionally (compared with GCN) considers the historical loads, its tries to formulate a smooth temporal trend and this goes in the wrong direction for such cases and induces a bad RMSE at such nodes like node 71 (Figure 13) and node 70 (Figures 14 and 15) with strong tempo- ral Ô¨Çuctuations. A comparison of performance is shown in Table 6.Wecan see generally a progressive advance in both MAE and RMSE is FIGURE 10 Attention mask of time TABLE 6 Comparison of GCN and ASTGCN Model Accuracy of congestion factor s MAE RMSE Baseline GCN 93.8242% 0.987564 1.926941 ASTGCN 93.6758% 0.822848 1.538259 achieved via ASTGCN, while the accuracy of congestion factor s remains almost the same. In a nutshell, the attention could adaptively re-weigh the load according to their importance, and as well provide some inter- pretability of how we get some LMP prediction. Meanwhile, the temporal convolution puts enough emphasis on the previous load information which beneÔ¨Åts the model by fusing traditional time series models. 6 CONCLUSION To utilize both the system topology and time series of power loads in LMP forecasting, this paper proposes a novel LMP forecasting method based on the GCN. Several improvements are promoted, including the spatial-temporal convolution and attention mechanism. A three-branch network is introduced to predict the respective components of LMP. The case study shows that the proposed GCN-only model outperforms the existing MLP in both accuracy and simplicity by an average of 30‚Äì40% in prediction errors. With the ST-Conv blocks and attention blocks, ASTGCN succeeds to capture the dynamic spatial-temporal characteristics of LMPs. Further experiments on IEEE 118 dataset shows its capability to utilize more infor- mation and enhance precision. Price forecasting is important for setting rational offers in the short term, for signing bilateral contracts in the medium term, and for inÔ¨Çuencing long-term generation expansion planning. 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 2490 YANG ET AL. FIGURE 11 IEEE118 topology FIGURE 12 RMSE of Nodes on LMP by GCN and ASTGCN Our LMP forecasting model takes in previous historical loads in a power system as inputs and the topological structure, and then outputs the LMP at all nodes. The required system topology just includes a binary value 1/0 for each tentative connection between two nodes. This provides a possible way to make better LMP forecasting for market participants. For them, only limited public data can be acquired. Therefore, our model offers a way to mine more hidden features within the topological structure and make a better forecast. In this sce- nario, market participants‚Äô signing bilateral can be more sensible with extra information acquired and to reduce the risk of price volatility. Note that the GCN-based LMP forecasting method can also be extended to similar applications that involve other time series related to system topological structure. Still, the pro- posed method reaches its bottleneck when it comes to fre- quent LMP spikes. This might comes from the inherence of convolution operations, and more efforts in network design- ing are expected to tackle such defects. Price spike forecasting has two main aspects: prediction of price spike occurrence and value [31]. Current works apply such a spike detector before- hand like a Naive Bayesian classiÔ¨Åer (NBC) [32] or Support Vector Machine [33] to decide spike occurrence and whether to apply an extra spike-forecasting module. For a detected ten- tative spike, an extra module for spike forecasting is triggered. As for the spike value forecasting, the main direction is to for- mulate an exclusive predictor for spikes, for example, an extra 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License YANG ET AL. 2491 FIGURE 13 Bad case of ASTGCN FIGURE 14 Performance contrast 1 of GCN and ASTGCN FIGURE 15 Performance contrast of GCN and ASTGCN data mining module assessing a composite supply-demand bal- ance index (SDI) and relative demand index (RDI) [32]ora probabilistic neural network trained over historical spike hours [34]. However, how to apply such intuition to GCN-based LMP forecasting remains a question, and would be considered in future works. ACKNOWLEDGEMENTS This work was supported by the National Key R&D Program of China under Grant No. 2020YFB0905900. CONFLICT OF INTEREST The authors have declared no conÔ¨Çict of interest. 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 2492 YANG ET AL. DATA AVAILABILITY STATEMENT The data that support the Ô¨Åndings of this study are avail- able in Historical metered load data at https://dataminer2. pjm.com/feed/hrl_load_metered/deÔ¨Ånition, reference num- ber [44]. These data were derived from the following resources available in the public domain: https://dataminer2.pjm.com/ feed/hrl_load_metered/deÔ¨Ånition ORCID Yuyun Yang https://orcid.org/0000-0003-2752-8676 Zhenfei Tan https://orcid.org/0000-0002-0509-3790 Guangchun Ruan https://orcid.org/0000-0003-2660-9298 Haiwang Zhong https://orcid.org/0000-0001-9545-6243 REFERENCES 1. GrifÔ¨Ån, J.M., Puller, S.L.: Electricity Deregulation: Choices and Challenges, vol. 4. University of Chicago Press, Chicago (2009) 2. Tan, Z., Ruan, G., Zhong, H., Xia, Q.: Security pre-check method of bilateral trading adapted to independence of power exchange (in chinese). Autom. Electr. Power Syst. 42(10), 106‚Äì113 (2018) 3. Boucher, J., Smeers, Y.: Alternative models of restructured electricity sys- tems. Part 1: no market power. Oper. Res. 49(6), 821‚Äì838 (1999) 4. Wang, J., Zhong, H., Yang, Z., Lai, X., Xia, Q., Kang, C.: Incentive mecha- nism for clearing energy and reserve markets in multi-area power systems. IEEE Trans. Sustain. Energy 11(4), 2470‚Äì2482 (2019) 5. Chen, L., Wang, J., Wu, Z., Li, G., Zhou, M., Li, P., et al.: Communication reliability-restricted energy sharing strategy in active distribution networks. Appl. Energy 282, 116238 (2021) 6. Ruan, G., Zhong, H., Shan, B., Tan, X.: Constructing demand-side bidding curves based on a decoupled full-cycle process. IEEE Trans. Smart Grid 12(1), 502‚Äì511 (2020) 7. Zhou, M., Wu, Z., Wang, J., Li, G.: Forming dispatchable region of elec- tric vehicle aggregation in microgrid bidding. IEEE Trans. Ind. Inf. 17(7), 4755‚Äì4765 (2020) 8. Fang, X., Li, F., Wei, Y., Cui, H.: Strategic scheduling of energy storage for load serving entities in locational marginal pricing market. IET Gener. Transm. Distrib. 10(5), 1258‚Äì1267 (2016) 9. Zhang, T., Wang, J., Zhong, H., Li, G., Zhou, M., Zhao, D.: Soft open point planning in renewable-dominated distribution grids with building thermal storage. CSEE J. Power Energy Syst. (2020) 10. Bo, R., Li, F.: Impact of load forecast uncertainty on lmp. In: 2009 IEEE/PES Power Systems Conference and Exposition, pp. 1‚Äì6. IEEE, Piscataway (2009) 11. Saebi, J., Taheri, H., Mohammadi, J., Nayer, S.S.: Demand bidding/ buyback modeling and its impact on market clearing price. In: 2010 IEEE International Energy Conference, pp. 791‚Äì796. IEEE, Piscataway (2010) 12. Litvinov, E., Zheng, T., Rosenwald, G., Shamsollahi, P.: Marginal loss mod- eling in LMP calculation. IEEE Trans. Power Syst. 19(2), 880‚Äì888 (2004) 13. Ruan, G., Zhong, H., Wang, J., Xia, Q., Kang, C.: Neural-network-based Lagrange multiplier selection for distributed demand response in smart grid. Appl. Energy 264, 114636 (2020) 14. Zhou, M., Yan, Z., Ni, Y., Li, G., Nie, Y.: Electricity price forecasting with conÔ¨Ådence-interval estimation through an extended ARIMA approach. IEE Proc.-Gener. Transm. Distrib. 153(2), 187‚Äì195 (2006) 15. Contreras, J., Espinola, R., Nogales, F.J., Conejo, A.J.: ARIMA models to predict next-day electricity prices. IEEE Trans. Power Syst. 18(3), 1014‚Äì 1020 (2003) 16. Ruan, G., Zhong, H., Zhang, G., He, Y., Wang, X., Pu, T.: Review of learning-assisted power system optimization. CSEE J. Power Energy Syst 7(2), 221‚Äì231 (2020) 17. Weron, R.: Electricity price forecasting: A review of the state-of-the-art with a look into the future. Int. J. Forecast. 30(4), 1030‚Äì1081 (2014) 18. Luo, S., Weng, Y.: A two-stage supervised learning approach for electricity price forecasting by leveraging different data sources. Appl. Energy 242, 1497‚Äì1512 (2019) 19. Chang, Z., Zhang, Y., Chen, W.: Electricity price prediction based on hybrid model of Adam optimized LSTM neural network and wavelet transform. Energy 187, 115804 (2019) 20. Lago, J., De.Ridder, F., De.Schutter, B.: Forecasting spot electricity prices: Deep learning approaches and empirical comparison of traditional algo- rithms. Appl. Energy 221, 386‚Äì405 (2018) 21. Zheng, K., Wang, Y., Liu, K., Chen, Q.: Locational marginal price fore- casting: A componential and ensemble approach. IEEE Trans. Smart Grid 11(5), 4555‚Äì4564 (2020) 22. Kipf, T.N., Welling, M.: Semi-supervised classiÔ¨Åcation with graph convo- lutional networks. arXiv preprint, arXiv:160902907 (2016) 23. Guo, S., Lin, Y., Feng, N., Song, C., Wan, H.: Attention based spatial- temporal graph convolutional networks for trafÔ¨Åc Ô¨Çow forecasting. In: Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, vol. 33, pp. 922‚Äì929. AAAI Press, Palo Alto (2019) 24. Wang, J., Shahidehpour, M., Li, Z.: Security-constrained unit commitment with volatile wind power generation. IEEE Trans. Power Syst. 23(3), 1319‚Äì 1327 (2008) 25. Gaikwad, P.S.: Using graph convolutional network and message passing neural networks for solving unit commitment and economic dispatch in a day ahead energy trading market based on ercot nodal model. Professional, The University of Texas at Arlington (2020) 26. Zhang, C., Li, R., Shi, H., Li, F.: Deep learning for day-ahead electricity price forecasting. IET Smart Grid 3(4), 462‚Äì469 (2020) 27. Defferrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural net- works on graphs with fast localized spectral Ô¨Åltering. In: Advances in Neu- ral Information Processing Systems, pp. 3844‚Äì3852. MIT Press, Cam- bridge (2016) 28. Keriven, N., Bietti, A., Vaiter, S.: Convergence and stability of graph convolutional networks on large random graphs. arXiv preprint, arXiv:200601868 (2020) 29. Glorot, X., Bengio, Y.: Understanding the difÔ¨Åculty of training deep feedforward neural networks. In: Proceedings of the thirteenth interna- tional conference on artiÔ¨Åcial intelligence and statistics. JMLR Work- shop and Conference Proceedings, pp. 249‚Äì256. JMLR, Cambridge (2010) 30. PJM: Historical metered load data. https://dataminer2.pjm.com/feed/ hrl_load_metered/deÔ¨Ånition. Accessed November 2019 31. Amjady, N., Keynia, F.: Electricity market price spike analysis by a hybrid data model and feature selection technique. Electr. Power Syst. Res. 80(3), 318‚Äì327 (2010) 32. Lu, X., Dong, Z.Y., Li, X.: Electricity market price spike forecast with data mining techniques. Electr. Power Syst. Res. 73(1), 19‚Äì29 (2005) 33. Zhao, J., Dong, Z.Y., Li, X., Wong, K.P.: A general method for electricity market price spike analysis. In: IEEE Power Engineering Society General Meeting, 2005, pp. 286‚Äì293. IEEE, Piscataway (2005) 34. Ghadimi, N., Akbarimajd, A., Shayeghi, H., Abedinia, O.: Two stage forecast engine with feature selection technique and improved meta- heuristic algorithm for electricity load forecasting. Energy 161, 130‚Äì142 (2018) How to cite this article: Yang, Y., Tan, Z., Yang, H., Ruan, G., Zhong, H., Liu, F.: Short-term electricity price forecasting based on graph convolution network and attention mechanism. IET Renew. Power Gener. 16, 2481‚Äì2492 (2022). https://doi.org/10.1049/rpg2.12413 17521424, 2022, 12, Downloaded from https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/rpg2.12413, Wiley Online Library on [01/02/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources.backup/papers_to_add/Olivares22hierarchFrcstRefFm.pdf","text":"Journal of Machine Learning Research Working Paper HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Kin G. Olivares kdgutier@cs.cmu.edu Federico Garza federico@nixtla.io David Luo djluo@@cs.cmu.edu Cristian Chall´u cchallu@cs.cmu.edu Max Mergenthaler max@nixtla.io Artur Dubrawski awd@cs.cmu.edu Abstract Large collections of time series data are commonly organized into structures with diﬀerent levels of aggregation; examples include product and geographical groupings. A necessary condition for “coherent” decision-making and planning, with such datasets, is for the dis- aggregated series’ forecasts to add up exactly to the aggregated series forecasts, which motivates the creation of novel hierarchical forecasting algorithms. The growing interest of the Machine Learning community in hierarchical forecasting systems states that we are in a propitious moment to ensure that scientiﬁc endeavors are grounded on sound baselines. For this reason, we put forward the HierarchicalForecast library, which contains pre- processed publicly available datasets, evaluation metrics, and a compiled set of statistical baseline models. Our Python-based reference framework aims to bridge the gap between statistical, econometric modeling, and Machine Learning forecasting research. Keywords: Hierarchical Forecasting, Econometrics, Datasets, Evaluation, Benchmarks 1. Introduction Large collections of time series organized into structures at diﬀerent aggregation levels of- ten require their forecasts to follow their aggregation constraints, which poses the challenge of creating novel algorithms capable of coherent forecasts. A fundamental component in developing practical methods is extensive empirical evaluations and comparing newly pro- posed forecasting methods with state-of-the-art and well-established baselines; regarding this, Machine Learning research on hierarchical forecasting faces two obstacles: (i) Statistical Baseline’s Absence: Eventhough Python continues to grow in popu- larity among the Machine Learning community (Piatetsky, 2018), it lacks a lot of statistical and econometric model packages, for which its forecasting research strug- gles with access to these baselines. This problem exacerbates by the most recent advancements in the hierarchical forecasting literature. (ii) Statistical Baseline’s Computational Eﬃciency: The Python global interpreter lock limits its programs to use a single thread, which translates into a lost opportunity to speed up algorithms by leveraging the available CPU resources. When implemented naively, statistical baselines in Python take an excessively long execution, even sur- passing those of more complex methods, which discourages their use. ©2000 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python.arXiv:2207.03517v3 [stat.ML] 28 Jul 2022 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python yβ2,τ yβ3,τ yβ4,τ yTotal,τ yβ1,τ yβ1,τ + yβ2,τ yβ3,τ + yβ4,τ Figure 1: Three-level time series hierarchical structure example, with four bottom level vari- ables, marked with a gray background. In this description, each node represents non-overlapping series for a single point in time. We tackle these challenges, by putting forward HierarchicalForecast, an open-source benchmark for hierarchical forecasting tasks1. Our work builds upon the fastest (to the best of our knowledge) ETS/ARIMA open-source Python implementations to improve the availabil- ity of hierarchical forecasting baselines and provide utilities for evaluating and forecasting hierarchical time series systems. 1.1 Hierarchical Forecasting Notation We denote a cross-sectional hierarchical time series by the vector y[a,b],τ = [ y⊺ [a],τ | y⊺ [b],τ ]⊺ ∈ RNa+Nb, for the time step τ , where [a], [b] denote respectively the aggregate and bottom level indices. The total number of series in the hierarchy is |[a, b]| = (Na + Nb). We distinguish between the time indices [t] and the h steps ahead indices [t + 1 : t + h], and bottom and aggregate indexes β ∈ [b] , α ∈ [a]. At any time τ ∈ [t], a hierarchical time series is deﬁned by the following aggregation constraints in matrix representation: y[a,b],τ = Sy[b],τ ⇔ [ y[a],τ y[b],τ ] = [ A[a][b] I[b][b] ] y[b],τ (1) Under this notation the summing matrix S ∈ R(Na+Nb)×Nb controls the aggregation of the bottom series to the upper levels, and it is composed of an aggregate matrix A[a][b] ∈ RNa×Nb and the identity matrix I[b][b] ∈ RNb×Nb. In Figure 1 example Na = 3, Nb = 4, and yTotal,τ = yβ1,τ + yβ2,τ + yβ3,τ + yβ4,τ y[a],τ = [yTotal,τ , yβ1,τ + yβ2,τ , yβ3,τ + yβ4,τ ]⊺ y[b],τ = [yβ1,τ , yβ2,τ , yβ3,τ , yβ4,τ ] ⊺ (2) 1. License: CC-by 4.0, see https://creativecommons.org/licenses/by/4.0/. Code and documentation available in https://github.com/Nixtla/hierarchicalforecast. 2 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python The constraints matrix associated to Figure 1 and Equations (2) is: S =         A[a][b] I[b][b]         =           1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1           1.2 Hierarchical Forecasting Baselines The classic approach to hierarchical forecasting has been a two-stage process where base level forecasts are produced ˆy[a,b],τ and later reconciled into coherent forecasts ˜y[a,b],τ (Hyndman and Athanasopoulos, 2018). Reconciliation can be represented in the following notation: ˜y[a,b],τ = SPˆy[a,b],τ . (3) Where S ∈ R(Na+Nb)×Nb is the hierarchical constraints matrix, P ∈ RNb×(Na+Nb) is a matrix that maps base forecasts into bottom level, speciﬁed by the reconciliation strategies. 1. Bottom-Up: This method constrains the base-level predictions to the bottom-level series, which are usually treated as independent (Orcutt et al., 1968). The reconcili- ation of the method is a simple addition to the upper levels. Its P matrix is deﬁned PBU = [0[b][a] | I[b][b]], where the ﬁrst columns collapses the aggregate level predictions and the identity picks only the bottom-level forecasts. 2. Top-Down: The second method constrains the base-level predictions to the top-most aggregate-level serie and then distributes it to the disaggregate series through the use of proportions p[b]. Its P matrix is deﬁned PTD = [p[b] | 0[b][a+b−1]], the rest of the columns zero-out the base forecast below the highest aggregation. Several variants of the methods emerge depending on the strategy for the computation of p[b] (Gross and Sohl, 1990; Fliedner, 1999). 3. Alternative: Recent hierarchical reconciliation strategies, transcend the base fore- casts’ single level origin, and deﬁne the P optimally under reasonable assumptions. • Middle Out: This method is only available for strictly hierarchical structures. It anchors the base predictions in a middle level. The levels above the base predictions use the bottom-up approach, while the levels below use a top-down. • Minimum Trace: Wickramasuriya et al. (2019) computed the reconciliation matrix P to minimize the total forecast variance of the space of coherent fore- casts, with the Minimum Trace reconciliation. Under unbiasedness assump- tions for the base forecasts, in this method, PMinT = (S⊺W−1 τ S)−1S⊺W−1 τ and Wτ = Var[ (y[a,b],τ − ˆy[a,b],τ ) ] is the base predictions’ variance-covariance matrix. • Empirical Risk Minimization: The ERM approach relaxes the unbiasedness assumption and optimizes the reconciliation matrix minimizing an L1 regularized 3 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python objective (Ben Taieb and Koo, 2019). The method also has a closed-form solution considering only the reconciliation quadratic errors. PERM = argminP||y − SPˆy|| 2 2 + λ||P − PBU||1 The HierarchicalForecast package contains a curated collection of reference hierar- chical forecasting algorithms through the BottomUp, TopDown, MiddleOut, MinTrace, and ERM model classes. Currently, the collection is restricted to point and mean forecasting methods, but we are working on implementing coherent probabilistic algorithms. 2. HierarchicalForecast Package 2.1 Dependencies The HierarchicalForecast library is built with minimal dependencies using NumPy for linear algebra and array operations (Harris et al., 2020), Pandas for data manipulation (McKinney, 2010) and sklearn (Pedregosa et al., 2011) for data processing. Additionally, the auto-ARIMA and auto-ETS (Hyndman and Khandakar, 2008) base forecast models depend on NumBa and open-source just-in-time compiler that translates Python and NumPy code into C++ currently these are Python’s fastest implementations. 2.2 Evaluation Metrics For the evaluation of the forecasting algorithms we provide access to the following initial prediction accuracy metrics that have been used in past hierarchical forecasting literature. Mean/Point Forecasting Accuracy: To measure the point forecasts accuracy, we summarize the forecast errors into the mean absolute scaled error (MASE), and the mean squared scaled error (MSSE), using ˆy′ i,τ the Naive1’s in the denominator errors, their deﬁ- nition is inspired by Hyndman and Koehler (2006): MASE(yi, ˆyi, ˆy′ i) = ∑t+H τ =t+1 |yi,τ − ˆyi,τ | ∑t+H τ =t+1 |yi,τ − ˆy′ i,τ | MSSE(yi, ˆyi, ˆy′ i) = ∑t+H τ =t+1(yi,τ − ˆyi,τ )2 ∑t+H τ =t+1(yi,τ − ˆy′ i,τ )2 (4) Quantile Forecasting Accuracy: Transitioning from point predictions, towards prob- abilistic predictions we measure the accuracy of individual quantiles ˆy(q) i,τ = ˆF −1 i,τ ( q ) of a predictive distribution ˆFi,τ using the quantile loss (QL) deﬁned as: QL(ˆy(q) i,τ , yi,τ ) = QL( ˆF −1 i,τ (q), yi,τ ) = 2 (1 {yi,τ ≤ ˆy(q) i,τ } − q) (ˆy(q) i,τ − yi,τ ) (5) Probabilistic Forecasting Accuracy: For a fully probabilistic prediction, we use the continuous ranked probability score (CRPS) (Matheson and Winkler, 1976)2, that summa- rizes the accuracy of the entire predictive distribution ˆFi,τ , the CRPS deﬁnition is: CRPS( ˆFi,τ , yi,τ ) = ∫ 1 0 QL( ˆF −1 i,τ (q), yi,τ )dq (6) 2. The CRPS uses a left Riemann numeric approximation of the integral and averages a discrete set of uniformly distanced quantile losses. 4 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Table 1: Summary, of available hierarchical datasets. Dataset Total Aggregated Bottom Levels Observations Frequency Traffic 207 7 200 4 366 Daily Labour 57 25 32 4 514 Monthly Wiki2 199 49 150 5 366 Daily Tourism-S 89 33 56 4 36 Quarterly Tourism-L 555 175 76 / 304 4 / 5 228 Monthly 2.3 Datasets We are making the following preprocessed ﬁve publicly available hierarchical datasets easily accessible through the Pandas and NumPy libraries. Each dataset is accompanied by meta- data capturing its seasonality/frequency, the forecast horizon used in previous hierarchical forecast publications, its corresponding hierarchical aggregation constraints matrix, and the names of its levels. Hierarchical forecasting studies have used these datasets in the past. We brieﬂy describe the datasets in Table 1. In more detail, the available datasets are: 1. Traffic measures the occupancy of 963 traﬃc lanes in the Bay Area, the data is grouped into a year of daily observations and organized into a 207 hierarchical struc- ture (Dua and Graﬀ, 2017). 2. Tourism-S consists of 89 Australian location quarterly visits series; it covers from 1998 to 2006. Several hierarchical forecasting studies have used this dataset in the past (Tourism Australia, Canberra, 2005). 3. Tourism-L summarizes an Australian visitor survey managed by the Tourism Research Australia agency, the dataset contains 555 monthly series from 1998 to 2016, and it is organized into geographic and purpose of travel (Tourism Australia, Canberra, 2019). 4. Labour reports monthly Australian employment from February 1978 to December 2020. It contains a hierarchical structure built by the labour categories (Australian Bureau of Statistics, 2019). 5. Wiki2 contains the daily views of 145,000 Wikipedia articles from July 2015 to De- cember 2016. The dataset is ﬁltered and processed into 150 bottom series and 49 aggregate series (Anava et al., 2018; Ben Taieb and Koo, 2019). 2.4 Dataset Class Each dataset class contains load and preprocessing methods that output readily available hi- erarchical time series as well as its aggregation’s constraints matrix, and hierarchical indexes for each level Additionally, each dataset contains metadata that includes the frequency and seasonality of the data, hierarchical indexes for the convenient evaluation across the levels of the hierarchical or grouped structures, and the dates. 5 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python 3. HierarchicalForecast Example 3.1 Base Predictions and Reconciliation In this subsection, we demonstrate the use of the HierarchicalForecast library to predict twelve months of the 555 series of the Tourism-L dataset using auto-ARIMA base model and later reconcile using the BottomUp , MinTrace , and ERM classes. from datasetsforecast . hierarchical import HierarchicalData from hierarchicalforecast . core import H i e r a r c h i c a l R e c o n c i l i a t i o n from hierarchicalforecast . methods import BottomUp , MinTrace , ERM from statsforecast . core import StatsForecast from statsforecast . models import auto_arima # Load TourismL dataset Y_df , S , tags = HierarchicalData . load ( ’ ./ data ’ , ’ TourismLarge ’) Y_df = Y_df . set_index ( ’ unique_id ’) # Compute base auto - ARIMA predictions fcst = StatsForecast ( df = Y_df , models = [ ( auto_arima , 12 ) ] , freq = ’M ’ , n_jobs = - 1 ) Y_hat_df = fcst . forecast ( h = 12 ) # Reconcile the base predictions reconcilers = [ BottomUp () , MinTrace ( method = ’ ols ’) , MinTrace ( method = ’ wls_struct ’) , MinTrace ( method = ’ wls_var ’) , MinTrace ( method = ’ mint_shrink ’) , ERM ( method = ’ lasso ’) , ] hrec = H i e r a r c h i c a l R e c o n c i l i a t i o n ( reconcilers = reconcilers ) Y_rec_df = hrec . reconcile ( Y_hat_df , Y_df , S , tags ) 3.2 Hierarchical Forecast Evaluation Our library facilitates a complete evaluation across the levels of the hierarchical structure. We evaluate the reconciliation strategies’ predictions using the MASE metric in this ex- ample. In a complete hierarchical forecasting pipeline example is available in this jupyter notebook, we perform a thorough evaluation with a rolling window cross-validation hy- perparameter selection on the available hierarchical forecasting methods for the Traffic, Labour, Wiki2, Tourism-S, and Tourism-L datasets. from hierarchicalforecast . evaluation import HierarchicalEvaluation , mase evaluator = Hi er ar chi ca lE val ua ti on ( evaluators = [ mase ] ) evaluator . evaluate ( Y_h = Y_hat_df , Y_test = Y_test , tags = tags , benchmark = ’ naive ’) 6 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python 4. Conclusion We presented HierarchicalForecast, an open-source python library dedicated to hierar- chical time series forecasting. The library integrates publicly available processed datasets, evaluation metrics, and a curated set of highly eﬃcient statistical baselines. We provide usage examples and references to extensive experiments where we showcase the baseline’s use and evaluate the accuracy of their predictions. With this work, we hope to contribute to Machine Learning forecasting by bridging the gap to statistical and econometric modeling, as well as providing tools for the develop- ment of novel hierarchical forecasting algorithms rooted in a thorough comparison of these well-established models. We intend to continue maintaining and increasing the repository, promoting collaboration across the forecasting community. Acknowledments The authors want to thank Alejandro Alvarez, Shibo Zhou, and Jos´e Morales for their contributions. Additionally we thank Chirag Nagpal and for the insightful conversations. References Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C Maddix, Syama Sundar Rangapuram, David Salinas, Jasper Schulz, et al. GluonTS: Probabilistic and neural time series modeling in python. Journal of Machine Learning Research, 21(116):1–6, 2020. Oren Anava, Vitaly Kuznetsov, and (Google Inc. Sponsorship). Web traﬃc time series forecasting, forecast future traﬃc to wikipedia pages. Kaggle Competition, 2018. URL https://www.kaggle.com/c/web-traffic-time-series-forecasting/. Australian Bureau of Statistics. Labour force, australia. Accessed Online, 2019. URL https://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/6202.0Dec% 202019?OpenDocument. Souhaib Ben Taieb and Bonsoo Koo. Regularized regression for hierarchical forecasting without unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, page 1337–1347, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330976. URL https://doi.org/10.1145/3292500.3330976. David M. Burns and Cari M. Whyne. SegLearn: A python package for learning sequences and time series. Journal of Machine Learning Research, 19(83):1–7, 2018. URL http: //jmlr.org/papers/v19/18-160.html. Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. CoRR, abs/2201.12886, 2022. URL https://arxiv.org/abs/2201.12886. 7 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matthew D. Hoﬀman, and Rif A. Saurous. Tensorﬂow distributions. Computing Research Repository, abs/1711.10604, 2017. URL http:// arxiv.org/abs/1711.10604. Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017. URL http:// archive.ics.uci.edu/ml. Gene Fliedner. An investigation of aggregate variable time series forecast strategies with spe- ciﬁc subaggregate time series statistical correlation. Computers and Operations Research, 26(10–11):1133–1149, September 1999. ISSN 0305-0548. doi: 10.1016/S0305-0548(99) 00017-9. URL https://doi.org/10.1016/S0305-0548(99)00017-9. Isaac Godfried, Kriti Mahajan, Maggie Wang, Kevin Li, and Pranjalya Tiwari. Flowdb a large scale precipitation, river, and ﬂash ﬂood dataset, 2020. Charles W. Gross and Jeﬀrey E. Sohl. Disaggregation methods to expedite product line forecasting. Journal of Forecasting, 9(3):233–254, 1990. doi: 10.1002/for.3980090304. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304. Xing Han, Sambarta Dasgupta, and Joydeep Ghosh. Simultaneously reconciled quantile forecasting of hierarchically related time series. In Arindam Banerjee and Kenji Fuku- mizu, editors, Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 190–198. PMLR, 13–15 Apr 2021. URL http://proceedings.mlr.press/v130/han21a.html. Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Vir- tanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Na- ture, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https: //doi.org/10.1038/s41586-020-2649-2. Julien Herzen, Francesco Lassig, Samuele Giuliano Piazzetta, Thomas Neuer, Lao Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, Maxime Dumonal, Jan Koacisz, Dennis Bader, Frederick Gusset, Mounir Ben- heddi, Camila Williamson, Michal Kosinski, Matej Petrik, and Gael Grosch. DARTS: User-friendly modern machine learning for time series. Journal of Machine Learning Research, 23(124):1–6, 2022. URL http://jmlr.org/papers/v23/21-1177.html. Tao Hong, Pierre Pinson, and Shu Fan. Global Energy Forecasting Competition 2012. Inter- national Journal of Forecasting, 30(2):357–363, 2014. ISSN 0169-2070. doi: https://doi. org/10.1016/j.ijforecast.2013.07.001. URL https://www.sciencedirect.com/science/ article/pii/S0169207013000745. 8 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Rob Hyndman, Alan Lee, Earo Wang, Shanika Wickramasuriya, and Maintainer Earo Wang. hts: Hierarchical and Grouped Time Series, 2021. URL https://CRAN. R-project.org/package=fable. R package version 0.3.1. Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice. OTexts, Melbourne, Australia, 2018. available at https://otexts.com/fpp2/. Rob J. Hyndman and Yeasmin Khandakar. Automatic time series forecasting: The forecast package for r. Journal of Statistical Software, Articles, 27(3):1–22, 2008. ISSN 1548-7660. doi: 10.18637/jss.v027.i03. URL https://www.jstatsoft.org/v027/i03. Rob J. Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. International Journal of Forecasting, 22(4):679 – 688, 2006. ISSN 0169-2070. doi: https: //doi.org/10.1016/j.ijforecast.2006.03.001. Tim Januschowski, Jan Gasthaus, and Yuyang (Bernie) Wang. Open-source forecasting tools in python. Foresight Journal of Applied Forecasting, 2019. URL https://www. amazon.science/publications/open-source-forecasting-tools-in-python. Xiaodong Jiang, Sudeep Srivastava, Sourav Chatterjee, Jeﬀ Handler, Rohan Bopardikar, Dawei Li, Yanjun Lin, Yang Yu, Michael Brundage, Caner Komurlu, Rakshita Nagalla, Zhichao Wang, Hechao Sun, Peng Gao, Wei Cheung, Jun Gao, Qi Wang, Morteza Kazemi, Tiham´er Levendovszky, Jian Zhang, Ahmet Koylan, Kun Jiang, Aida Shoydokova, Ploy Temiyasathit, Sean Lee, Nikolay Pavlovich Laptev, Peiyi Zhang, Emre Yurtbay, Daniel Dequech, Rui Yan, William Luo, Marius Guerard, and Pietari Pulkkinen. Kats: a gen- eralizable framework to analyze time series data in python. Technical report, Facebook, 2021. URL https://github.com/facebookresearch/Kats. Markus L¨oning, Anthony J. Bagnall, Sajaysurya Ganesh, Viktor Kazakov, Jason Lines, and Franz J. Kir´aly. SKTIME: A uniﬁed interface for machine learning with time series. Computing Research Repository, abs/1909.07872, 2019. URL http://arxiv.org/abs/ 1909.07872. Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m5 competition: Background, organization, and implementation. International Journal of Forecasting, 2021. ISSN 0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2021.07.007. URL https: //www.sciencedirect.com/science/article/pii/S0169207021001187. James E. Matheson and Robert L. Winkler. Scoring rules for continuous probability distri- butions. Management Science, 22(10):1087–1096, 1976. ISSN 00251909, 15265501. URL http://www.jstor.org/stable/2629907. Carlo Mazzaferro. scikit-hts: Hierarchical Time Series Forecasting with a familiar API, 2022. URL https://scikit-hts.readthedocs.io/en/latest/. Python package. Wes McKinney. Data structures for statistical computing in python. In St´efan van der Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 51 – 56, 2010. 9 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Dave Moore, Jacob Burnim, and the TFP Team. Structural time series modeling in ten- sorﬂow probability, 3 2019. URL https://github.com/tensorflow/probability/. Edwin Ng, Zhishi Wang, Huigang Chen, Steve Yang, and Slawek Smyl. Orbit: Probabilistic forecast with exponential smoothing, 2020. Mitchell O’Hara-Wild, Rob Hyndman, Earo Wang, Gabriel Caceres, Tim-Gunnar Hensel, and Timothy Hyndman. fable: Forecasting Models for Tidy Time Series, 2021. URL https://CRAN.R-project.org/package=hts. R package version 6.0.2. Kin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafa l Weron, and Artur Dubrawski. Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with nbeatsx. International Journal of Forecasting, submitted, Working Paper version available at arXiv:2104.05522, 2021. URL https://arxiv.org/abs/2104.05522. Guy H. Orcutt, Harold W. Watts, and John B. Edwards. Data aggregation and information loss. The American Economic Review, 58(4):773–787, 1968. ISSN 00028282. URL http: //www.jstor.org/stable/1815532. Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: neural basis expansion analysis for interpretable time series forecasting. In 8th In- ternational Conference on Learning Representations, ICLR 2020, 2020. URL https: //openreview.net/forum?id=r1ecqn4YwB. Biswajit Paria, Rajat Sen, Amr Ahmed, and Abhimanyu Das. Hierarchically Regularized Deep Forecasting. In Submitted to Proceedings of the 39th International Conference on Machine Learning. PMLR. Working Paper version available at arXiv:2106.07630, 2021. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon- del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. Gregory Piatetsky. Python eats away at R: Top software for analytics, data science, machine learning in 2018: Trends and analysis. https://www.kdnuggets.com/2018/05/ poll-tools-analytics-data-science-machine-learning-results.html/2, 2018. Accessed: 2022-07-05. Syama Sundar Rangapuram, Lucien D. Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, and Tim Januschowski. End-to-end learning of coherent probabilistic forecasts for hierarchical time series. In Maria Florina Balcan and Marina Meila, editors, Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 06–11 Aug 2021. Skipper Seabold and Josef Perktold. StatsModels: Econometric and statistical modeling with python. In 9th Python in Science Conference, 2010. Julien Siebert, Janek Groß, and Christof Schroth. A systematic review of python packages for time series analysis. Engineering Proceedings, 5, 2021. doi: https://doi.org/10.3390/ engproc2021005022. URL https://arxiv.org/abs/2104.07406. 10 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Taylor G. Smith. pmdarima, 2 2022. URL https://github.com/alkaline-ml/pmdarima. Slawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting. International Journal of Forecasting, 07 2019. doi: 10.1016/j. ijforecast.2019.03.017. Romain Tavenard, Johann Faouzi, Gilles Vandewiele, Felix Divo, Guillaume Androz, Chester Holtz, Marie Payne, Roman Yurchak, Marc Rußwurm, Kushal Kolar, and Eli Woods. TSLEARN, a machine learning toolkit for time series data. Journal of Machine Learning Research, 21(118):1–6, 2020. URL http://jmlr.org/papers/v21/20-091. html. Ross Taylor. PyFlux: An open source time series library for python. PyData San Francisco 2016, 2016. URL https://github.com/RJT1990/pyflux. Sean J. Taylor and Benjamin Letham. Prophet: Forecasting at scale. The American Statistician, 72(1):37–45, 2018. doi: 10.1080/00031305.2017.1380080. URL https:// doi.org/10.1080/00031305.2017.1380080. Tourism Australia, Canberra. Tourism Research Australia (2005), Travel by Australians. https://www.kaggle.com/luisblanche/quarterly-tourism-in-australia/, Sep 2005. Tourism Australia, Canberra. Detailed tourism Australia (2005), Travel by Australians, Sep 2019. Accessed at https://robjhyndman.com/publications/hierarchical-tourism/. Shanika L. Wickramasuriya, George Athanasopoulos, and Rob J. Hyndman. Optimal fore- cast reconciliation for hierarchical and grouped time series through trace minimization. Journal of the American Statistical Association, 114(526):804–819, 2019. doi: 10.1080/ 01621459.2018.1448825. URL https://robjhyndman.com/publications/mint/. Bohan Zhang, Yanfei Kang, and Feng Li. pyths: A python package for hierarchical forecast- ing, 2022. URL https://angelpone.github.io/pyhts/tutorials/Tutorials.html. Python package. 11 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Appendix A. Appendix A.1 Open-source Python Forecasting Libraries The Python forecasting ecosystem continues to grow at a steady pace. Regarding classic statistical and econometric models and implementations, like ARIMA, ETS, GARCH among others, statsmodels, tf sts, kats, and pyflux have low-level implementations based on NumPy. While other libraries like gluonts, tf sts, flowforecast, and neuralforecast. The gluonts has Python API connections to R forecasting libraries that enable comparisons with these well-established methods, with the cost of R dependencies frictions. Finally, higher-level libraries like darts, sktime, tslearn, pmdarima, seglearn, have outstandingly curated, improved usability, and made available a large variety of models. We refer to Januschowski et al. (2019) and Siebert et al. (2021) for complete surveys. Table 2: Summary of some popular open-source forecasting libraries. Library Brief Description statsmodels Multi-purpose python framework for estimation, exploration and evaluation of general statistical models. Includes implementations of classic ARIMA and ETS models (Seabold and Perktold, 2010). statsforecast Specialized library. Eﬃcient and reliable econometric methods’ implementations. kats Toolkit for time series analysis, capable of forecasting, featurization, change and outlier detection (Jiang et al., 2021). pyflux Like kats, a library for forecasting and time series analysis, with an extensive pool of statistical models including ARIMA, GARCH, GAS, and VAR (Taylor, 2016). orbit Bayesian forecasting. It contains ETS, local-global and damped-local trend, and kernel regression, MCMC, VI and MAP optimization (Ng et al., 2020). fbprophet Hosts the prophet, additive forecasting method, that models nonlinear trends, seasonalities and holiday eﬀects (Taylor and Letham, 2018). gluonts Hosts probabilistic and neural forecasting innovations and experimentation tools, reference SOTA implementations, and an API to R (Alexandrov et al., 2020). tf sts Implementations of structural time series models for tensorﬂow, that makes sim- pliﬁes probabilistic neural forecasting (Dillon et al., 2017; Moore et al., 2019). neuralforecast A neural forecasting library hosting ESRNN, N-BEATS, N-BEATSx, N-HiTS (Smyl, 2019; Oreshkin et al., 2020; Olivares et al., 2021; Challu et al., 2022). flowforecast PyTorch forecasting framework, it started as ﬂash ﬂood and river benchmark. It has ten deep learning methods (Godfried et al., 2020). darts Enables user-friendly manipulation, point and probabilistic predictions of time series. It supports backtest and ensembling, and its models range from statistical to neural networks (Herzen et al., 2022). sktime Uniﬁed interface for multiple time series tasks, forecasting, classiﬁcation, cluster- ing, annotation, regression (L¨oning et al., 2019). tslearn A general-purpose time series library contains preprocessing clustering classiﬁca- tion and regression (Tavenard et al., 2020). pmdarima Another wrapper library for statsmodels-ARIMA, that mimics R project’s ARIMA. It additionally includes, statistical tests, decompositions (Smith, 2022). auto timeseries A wrapper for statsmodels, fbprophet and sklearn. It enables parallelization across time series with dask. seglearn A general-purpose library, with classiﬁcation, regression and forecasting capa- bilities for sequence and contextual data. Contains several data transformation utilities to enable ML inputs (Burns and Whyne, 2018). 12 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python A.2 Open-source Hierarchical Forecasting Libraries Hierarchical Forecasting continues to grow in popularity as shown by the international competitions (GEFCOM2012; Hong et al. 2014) and (M5; Makridakis et al. 2021), and the growing interest of the Machine Learning community on the topic (Rangapuram et al., 2021; Paria et al., 2021; Han et al., 2021). As mentioned in Section 1, for a long time, there has been an absence of reliable sta- tistical and econometric methods that exacerbates with forecasting-subﬁelds with recent innovations. One possible explanation is the poor computational eﬃciency of previous Python implementations of ARIMA and ETS methods that could not leverage the full multi- core capabilities of the machines. The forecasting community has made an eﬀort and started the adoption of recently published NumBa implementations of the methods available in the statsforecast package, like darts, and sktime two of the most popular open-source fore- casting frameworks in Python. The work of gluonts, darts, scikit-hts, sktime, and pyhts spearheaded the Python’s hierarchical forecasting availability. A summary is available in Table 3. With our work, we seize the opportunities to increase the available hierarchical methods beyond those valuable contributions and to perform a robust validation of the performance of said implementations to ensure that the Python community has access to eﬃcient and reliable baselines. Table 3: Hierarchical forecasting libraries. Library Brief Description fable One of R’s most used forecasting packages, it provides a collection of commonly used univariate and multivariate time series forecasting models, including auto- matically selected exponential smoothing (ETS) and autoregressive integrated moving average (ARIMA) models. These models work within the fable frame- work provided by the fabletools package, which provides the tools to evalu- ate, visualize, and combine models in a workﬂow consistent with the tidyverse (O’Hara-Wild et al., 2021). hts Provides methods for analyzing and forecasting hierarchical and grouped time series. It oﬀers a complete collection of hierarchical reconciliation methods, in- cluding bottom-up, top-down, and optimal reconciliation (Hyndman et al., 2021). gluonts/hts The gluonts package updated their library based on the novel contributions of Rangapuram et al. (2021). In their experiments, they prioritized their compar- ison with proven implementations of reconciliation methods. They built API connectors to R’s hts library; the connections are available here. darts/hts The darts package recently updated their repository with BottomUp, TopDown, and MinTrace, Reconciliator classes (Herzen et al., 2022), the data processing classes are available here. sktime/hts Similar to darts, the sktime library recently published reconciliation capabilities that include BottomUp, TopDown, OLS and WLS optimal combination strategies. The reconciliation classes code is available here. scikit-hts Python hierarchical forecasting package with pmdarima and statsmodels depen- dencies, with implementations of BottomUp, TopDown variants and MiddleOut rec- onciliation. The repository has low activity and has not been recently updated (Mazzaferro, 2022). pyhts Python hierarchical forecasting package with statsforecast dependencies, in- spired by the hts package in R. It has variants of MinTrace and OLS and WLS optimal combination reconciliation (Zhang et al., 2022). 13 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python A.3 Hierarchical Forecasting Evaluation To complement the examples from Section 3. Here we perform a thorough hierarchical fore- casting experiment on the Labour, Tourism-L, Tourism-S, Traffic, and Wiki2 datasets, comparing the predictions accuracy of BottomUp, MiddleOut, MinTrace, and ERM meth- ods using the mean absolute scaled error (MASE) using ˆy′ i,τ , the Naive1 forecast in the denominator. MASE(yi, ˆyi, ˆy′ i) = ∑t+H τ =t+1 |yi,τ − ˆyi,τ | ∑t+H τ =t+1 |yi,τ − ˆy′ i,τ | (7) We use two settings for the experiments; one uses a rolling window approach where we ﬁt and predict a horizon of length deﬁned in Table 1, using the last 25% of the dataset as the test set. For the second experiment, we restrict the evaluation to the last window of the forecast’s horizon length. Table 4 reports the rolling window evaluation, and Table 5 reports the single window evaluation. Table 4: Empirical evaluation of hierarchically coherent forecasts. Mean absolute scaled error (MASE). These experiments use a rolling window evaluation where we use the last 25% of the available observations to test. ARIMA BottomUp TopDown MiddleOut MiddleOut MiddleOut MinTrace MinTrace MinTrace MinTrace ERM Dataset Level (fcst prop) (level 2) (level 3) (level 4) (ols) (wls struct) (wls var) (shrink)LabourOverall 0.8763 0.9099 0.8817 0.9149 0.8838 - 0.8849 0.8616 0.8751 0.8796 0.9099 1 0.6042 0.8284 0.6042 0.7268 0.6953 - 0.5991 0.6641 0.7434 0.7550 0.8284 2 0.9303 0.9019 0.8914 0.9303 0.8744 - 0.9217 0.8621 0.8647 0.8697 0.9019 3 0.9422 0.9442 0.9619 0.9802 0.9422 - 0.9633 0.9094 0.9097 0.9136 0.9442 4 0.9304 0.9304 0.9539 0.9524 0.9391 - 0.9470 0.9222 0.9197 0.9209 0.9304Tourism-L Overall 0.5077 0.6026 - - - - 0.5036 0.5102 0.5335 0.5335 0.6026 1 (geo.) 0.3582 0.7028 - - - - 0.3425 0.4228 0.4980 0.4980 0.7028 2 (geo.) 0.3850 0.5992 - - - - 0.3788 0.4183 0.4613 0.4613 0.5992 3 (geo.) 0.5150 0.5926 - - - - 0.4827 0.4868 0.5105 0.5105 0.5926 4 (geo.) 0.5864 0.6045 - - - - 0.5659 0.5547 0.5577 0.5577 0.6045 5 (prp.) 0.3278 0.5730 - - - - 0.3596 0.4030 0.4585 0.4585 0.5730 6 (prp.) 0.4631 0.5506 - - - - 0.4508 0.4575 0.4851 0.4851 0.5506 7 (prp.) 0.5657 0.5905 - - - - 0.5584 0.5454 0.5607 0.5607 0.5905 8 (prp.) 0.6282 0.6282 - - - - 0.6471 0.6236 0.6207 0.6207 0.6282Tourism-SOverall 0.6026 0.5736 0.5868 0.6861 0.5671 - 0.6021 0.5831 0.5615 0.5615 0.5736 1 0.5183 0.5035 0.5183 0.7871 0.5323 - 0.5507 0.5591 0.5376 0.5376 0.5035 2 0.6888 0.5089 0.5654 0.6888 0.5158 - 0.5942 0.5505 0.5235 0.5235 0.5089 3 0.5449 0.5800 0.5659 0.6263 0.5449 - 0.5828 0.5650 0.5478 0.5478 0.5800 4 0.6358 0.6358 0.6464 0.6964 0.6321 - 0.6452 0.6287 0.6066 0.6066 0.6358TrafficOverall 0.6241 0.6676 0.6309 0.6290 0.6249 - 0.6273 0.6180 0.6529 0.6690 0.6676 1 0.4937 0.5694 0.4937 0.4907 0.4864 - 0.4911 0.4846 0.5473 0.5724 0.5694 2 0.5105 0.5735 0.5129 0.5105 0.5044 - 0.5100 0.5013 0.5529 0.5762 0.5735 3 0.5198 0.5768 0.5278 0.5258 0.5198 - 0.5247 0.5133 0.5581 0.5796 0.5768 4 0.8570 0.8570 0.8703 0.8696 0.8683 - 0.8652 0.8553 0.8538 0.8554 0.8570Wiki2 Overall 0.9827 0.9896 1.1546 1.1180 0.9436 0.9421 1.1251 0.9975 1.0394 1.0394 0.9896 1 0.9473 0.9910 0.9473 0.8998 0.8016 0.8015 0.9140 0.8376 0.8658 0.8658 0.9910 2 1.0715 0.9468 1.1069 1.0715 0.8754 0.8745 1.0054 0.8893 0.9614 0.9614 0.9468 3 0.9236 0.9801 1.1354 1.1065 0.9236 0.9238 1.0626 0.9390 0.9969 0.9969 0.9801 4 0.9286 0.9840 1.1371 1.1089 0.9286 0.9286 1.0872 0.9626 1.0013 1.0013 0.9840 5 1.0318 1.0318 1.3267 1.2815 1.0948 1.0897 1.4029 1.2348 1.2521 1.2521 1.0318 14 HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python Table 5: Empirical evaluation of hierarchically coherent forecasts. Mean absolute scaled error (MASE). These experiments use a single window evaluation where we use the last available observations to test. ARIMA BottomUp TopDown MiddleOut MiddleOut MiddleOut MinTrace MinTrace MinTrace MinTrace ERM Dataset Level (fcst prop) (level 2) (level 3) (level 4) (ols) (wls struct) (wls var) (shrink)LabourOverall 1.1519 1.1231 1.2132 1.1214 1.1512 - 1.1892 1.1470 1.1342 1.1254 1.1231 1 1.2309 1.1334 1.2309 1.1220 1.1654 - 1.2139 1.1629 1.1463 1.1357 1.1334 2 1.1217 1.1241 1.2175 1.1217 1.1533 - 1.1966 1.1497 1.1350 1.1256 1.1241 3 1.1490 1.1235 1.2174 1.1266 1.1490 - 1.1885 1.1461 1.1334 1.1245 1.1235 4 1.1132 1.1132 1.1905 1.1160 1.1392 - 1.1624 1.1319 1.1237 1.1173 1.1132Tourism-L Overall 0.6112 0.7333 - - - - 0.5961 0.6162 0.6550 0.6550 0.7333 1 (geo.) 0.3107 1.2293 - - - - 0.3040 0.6838 0.8996 0.8996 1.2293 2 (geo.) 0.5285 0.8485 - - - - 0.4430 0.5628 0.6706 0.6706 0.8485 3 (geo.) 0.7125 0.8232 - - - - 0.6462 0.6725 0.7242 0.7242 0.8232 4 (geo.) 0.7830 0.7563 - - - - 0.7331 0.7045 0.7078 0.7078 0.7563 5 (prp.) 0.4463 0.7030 - - - - 0.4241 0.4627 0.5362 0.5362 0.7030 6 (prp.) 0.5213 0.6152 - - - - 0.5099 0.5147 0.5462 0.5462 0.6152 7 (prp.) 0.6187 0.6396 - - - - 0.6139 0.5955 0.6122 0.6122 0.6396 8 (prp.) 0.6776 0.6776 - - - - 0.7223 0.6809 0.6761 0.6761 0.6776Tourism-SOverall 0.6350 0.6082 0.6390 0.7132 0.5909 - 0.6372 0.6115 0.5811 0.5811 0.6082 1 0.5828 0.5273 0.5828 0.7902 0.5405 - 0.6041 0.5786 0.5593 0.5593 0.5273 2 0.6528 0.5027 0.5666 0.6528 0.4799 - 0.5740 0.5252 0.4963 0.4963 0.5027 3 0.5865 0.6374 0.6279 0.6699 0.5865 - 0.6341 0.6166 0.5902 0.5902 0.6374 4 0.6868 0.6868 0.7208 0.7619 0.6909 - 0.6963 0.6790 0.6398 0.6398 0.6868TrafficOverall 0.2918 0.4109 0.2884 0.2903 0.2839 - 0.2860 0.3011 0.3868 0.4289 0.4109 1 0.1512 0.3225 0.1512 0.1548 0.1484 - 0.1520 0.1797 0.2950 0.3450 0.3225 2 0.1602 0.3257 0.1596 0.1602 0.1541 - 0.1586 0.1797 0.2958 0.3495 0.3257 3 0.1724 0.3317 0.1775 0.1787 0.1724 - 0.1767 0.1970 0.3024 0.3548 0.3317 4 0.6275 0.6275 0.6117 0.6137 0.6069 - 0.6038 0.5987 0.6161 0.6324 0.6275Wiki2 Overall 1.5259 1.7400 2.9061 2.5611 2.2417 2.2330 1.7593 1.4153 1.4389 1.4389 1.7400 1 16.4317 34.5394 16.4317 1.5588 0.8263 0.7546 13.0176 2.9936 3.7734 3.7734 34.5394 2 1.6932 2.1472 2.0661 1.6932 1.1936 1.1668 1.6256 0.9642 1.1518 1.1518 2.1472 3 1.3780 1.5570 1.7037 1.5396 1.3780 1.3719 1.4984 1.2730 1.3488 1.3488 1.5570 4 1.3271 1.3417 1.6015 1.4603 1.3321 1.3271 1.3815 1.1447 1.3047 1.3047 1.3417 5 1.4539 1.4539 4.6322 4.2796 3.8015 3.7961 2.0483 1.8275 1.6528 1.6528 1.4539 15","libVersion":"0.3.2","langs":""}
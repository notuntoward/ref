{"path":"lit/lit_sources/Bai23EstimatingTotalCorrelation.pdf","text":"Estimating Total Correlation with Mutual Information Estimators Ke Bai ∗ Pengyu Cheng ∗ Weituo Hao Ricardo Henao Lawrence Carin Duke University Tencent AI Lab ByteDance KAUST KAUST Abstract Total correlation (TC) is a fundamental concept in information theory which measures statistical dependency among multiple random variables. Recently, TC has shown noticeable effectiveness as a regularizer in many learning tasks, where the correlation among multiple latent embeddings requires to be jointly minimized or maximized. However, calculating precise TC values is chal- lenging, especially when the closed-form distribu- tions of embedding variables are unknown. In this paper, we introduce a uniﬁed framework to esti- mate total correlation values with sample-based mutual information (MI) estimators. More specif- ically, we discover a relation between TC and MI and propose two types of calculation paths (tree-like and line-like) to decompose TC into MI terms. With each MI term being bounded, the TC values can be successfully estimated. Further, we provide theoretical analyses concerning the statistical consistency of the proposed TC estima- tors. Experiments are presented on both synthetic and real-world scenarios, where our estimators demonstrate effectiveness in all TC estimation, minimization, and maximization tasks. The code is available at https://github.com/Linear95/TC- estimation. 1 INTRODUCTION Statistical dependency measures the association (correla- tion) of variables or factors in models and systems, and constitutes one of the key considerations in various scien- tiﬁc domains including statistics (Granger and Lin, 1994; Jiang et al., 2015), robotics (Julian et al., 2014; Charrow et al., 2015), bioinformatics (Lachmann et al., 2016; Zea et al., 2016), and machine learning (Chen et al., 2016; Alemi et al., 2016; Hjelm et al., 2018). In deep learning, statistical dependency has been applied as learning objective or reg- ularizer in many well-known training frameworks, such as ∗Equal contribution. Corresponding to Pengyu Cheng. Proceedings of the 26 th International Conference on Artiﬁcial Intel- ligence and Statistics (AISTATS) 2023, Valencia, Spain. PMLR: Volume 206. Copyright 2023 by the author(s). information bottleneck (Alemi et al., 2016), disentangled representation learning (Chen et al., 2018; Peng et al., 2019; Cheng and Li, 2022) and contrastive learning (Chen et al., 2020; Gao et al., 2021). Recent neural network studies have also demonstrated the beneﬁts of considering statistical de- pendency in terms of model robustness (Zhu et al., 2020), fairness (Creager et al., 2019), interpretability (Chen et al., 2016; Cheng et al., 2020b), etc. Among diverse measurement approaches for statistical de- pendency, the concept of mutual information (MI) is one of the most commonly used, especially in deep model train- ing (Alemi et al., 2016; Belghazi et al., 2018; Chen et al., 2020). Given two random variables x and y with joint and marginal distributions p(x, y), p(x) and p(y), respectively, their mutual information is deﬁned as: I(x; y) = Ep(x,y)[ log p(x, y) p(x)p(y) ] . (1) Recently, mutual information has been used as a training criterion to deliver noticeable performance improvement for deep models on various learning tasks such as conditional generation (Chen et al., 2016; Cheng et al., 2020b), domain adaptation (Gholami et al., 2020; Cheng et al., 2020a), repre- sentation learning (Chen et al., 2020; Gao et al., 2021; Yuan et al., 2021), and model debiasing (Song et al., 2019; Cheng et al., 2021). However, standard MI in equation 1 only handles the statistical dependency between two variables. When considering correlation among multiple variables, MI requires computation between each pair of variable, which leads to a quadratic scaling in computation cost. To address this problem, total correlation (TC) (Watanabe, 1960) or multi-information (Studen`y and Vejnarová, 1998) has been proposed for multi-variable scenarios: T C(X) =T C(x1, x2, . . . , xn) (2) =Ep(x1,x2,...,xn)[ log p(x1, x2, . . . , xn) p(x1)p(x2) . . . p(xn) ]. TC has been also proven effective to enhance machine learn- ing models in many tasks such as independent component analysis (Cardoso, 2003), structure discovery (Ver Steeg and Galstyan, 2014) and disentangled representation learn- ing (Chen et al., 2018; Locatello et al., 2019a; Kim and Mnih, 2018). However, TC suffers from the same practicalarXiv:2011.04794v2 [cs.IT] 22 Feb 2023 Estimating Total Correlation with Mutual Information Estimators problem as MI, namely, that the exact values of TC are difﬁcult to calculate without the availability of the closed- form distributions, or only relying on samples either via the kernel density estimation (KDE) (Kandasamy et al., 2015; Singh and Póczos, 2014) and k-Nearest Neighbor (k-NN) (Pál et al., 2010; Kraskov et al., 2004a; Gao et al., 2018), which may perform well on data with low dimen- sionality but failed on the high one. Furthermore, previous works on disentangled representation learning (Chen et al., 2018; Gao et al., 2019) strongly assume that both the joint distribution p(x1, x2, . . . , xn) and marginal distributions {p(xi)} n i=1 follow Gaussian distributions, so that the TC value can be calculated in closed-form. Poole et al. (2019) proposed an upper bound of TC by further introducing an auxiliary variable y. With a strong assumption that given y and xi|y, all variables are conditionally independent, p(X|y) = ∏n i=1 p(xi|y), Poole et al. (2019) showed that T C(X) = ∑n i=1 I(xi; y) − I(X; y), i.e., that the TC value can be bounded by MI estimators (Belghazi et al., 2018; Poole et al., 2019; Cheng et al., 2020a). All these TC estimation methods require additional strong assumptions on the distributions of data samples, which heavily limits their application in practice. In this paper, we propose a new Total Correlation Estimator via Linear Decomposition (TCeld), which importantly, does not require any assumptions about sample distributions. More speciﬁcally, we discover a linear decomposition rela- tion between TC and mutual information (MI). Based on this relationship, we linearly split the TC into several MI terms, then estimate each MI term aided with variational MI estimators. With two different TC decomposition paths (line-like and tree-like), we obtain two types of TC estima- tors, ̂T CTree and ̂T CLine, respectively. Moreover, we prove that the nice properties of MI estimators, such as consistency, are maintained in the corresponding TC estimators, thanks to the linearity of the proposed TC decomposition. In the experiments, we ﬁrst test the estimation quality of our TC estimators on simulation data sampled from multi-variate Gaussian distributions, then apply the TC estimators to two real-world TC optimization tasks. The numerical results demonstrate the effectiveness of the proposed TC estimators under both TC estimation and optimization scenarios. 2 BACKGROUND 2.1 Sample-based Mutual Information Estimators Although mutual information (MI) is a fundamental tool for measuring statistical dependency between two variables, calculating MI values with only samples provided is chal- lenging, especially when the closed-form distributions of variables are unknown. To estimate MI values with samples, several variational MI estimators have been introduced. Bar- ber and Agakov (2003) approximate the conditional distri- bution p(x|y) between x and y by a variational distribution q(x|y), and derive: IBA := H(x) + Ep(x,y)[log q(x|y)], (3) with H(x) as the entropy of x. Utilizing the Donsker- Varadhan representation (Donsker and Varadhan, 1983) of KL-divergence, Belghazi et al. (2018) obtain an MI Neural Estimator (MINE) with a score network φ(·, ·): IMINE :=Ep(x,y)[φ(x, y)] (4) − log(Ep(x)p(y)[exp(φ(x, y))]. Nguyen, Wainwright, and Jordan (NWJ) derive an- other lower bound considering MI as in a f-divergence form Nguyen et al. (2010), which also requires a score network φ(·, ·): INWJ := Ep(x,y)[φ(x, y)] − Ep(x)p(y)[e φ(x,y)−1]. (5) With Noise Contrastive Estimation (NCE) (Gutmann and Hyvärinen, 2010), Oord et al. (2018) propose a MI lower bound called InfoNCE, based on a group of samples {(xi, yi)} N i=1, to obtain a low-variance estimator: IInfoNCE := E[ 1 N N∑ i=1 log exp(φ(xi, yi)) 1 N ∑N j=1 exp(φ(xi, yj)) ]. (6) Different from the MI lower bounds introduced above, Cheng et al. (2020a) derive a Contrastive Log-ratio Up- per Bound (CLUB), which is also based on a variational approximation q(x|y) of p(x|y): ICLUB := Ep(x,y)[log q(x|y)] − Ep(x)p(y)[log q(x|y)] (7) 2.2 Statistical Properties of Estimators Given observed samples x 1, x2, . . . x m ∼ p(x), a statis- tic is deﬁned as T (x1, x2, . . . , xm), where T (·) is an ar- bitrary real-valued function taking samples x 1, x2, . . . x m as inputs (DeGroot and Schervish, 2012). Sample-based estimators to calculate statistical dependency, i.e. mutual information and total correlation, are also examples of statis- tics. To evaluate the performance of a statistic, statistical properties are introduced to describe behaviors of the statis- tic under different data situations (DeGroot and Schervish, 2012) (e.g., with large/small sample number, with/without outlier). For estimation of mutual information and total cor- relation, we follow prior works (Paninski, 2003; Belghazi et al., 2018) and will mainly consider the following three key properties: Deﬁnition 2.1 (Unbiasedness). An estimator ˆR = T (x1, x 2, . . . , xm) is unbiased of the ground true value Rp with respect to the distribution p(x), if Ep(x)[ ˆR] = Rp. Deﬁnition 2.2 (Consistency). An estimator ˆRm = T (x1, x 2, . . . , xm) is consistent to the ground true value Rp with respect to the distribution p(x), if ∀ε > 0, limm→∞ P{| ˆRm − Rp| ≥ ε} = 0. Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin Deﬁnition 2.3 (Strong Consistency). An estimator ˆRm = T (x1, x 2, . . . , xm) is strongly consistent to the ground true value Rp with respect to the distribution p(x), if ∀ε > 0, there exists an integer M > 0 such that ∀m > M , | ˆRm − Rp| ≤ ε almost surely. 3 TOTAL CORRELATION ESTIMATION Suppose m groups of data samples {X j}m j=1 = {(x j 1, x j 2, . . . , x j n)} m j=1 are observed from an unknown dis- tribution p(X) = p(x1, x2, . . . , xn). We seek to estimate the total correlation (TC) of these n variables as in equa- tion 2. Below we ﬁrst describe the proposed sample-based TC estimators, then analyze their statistical properties. 3.1 Sample-based TC Estimators With the deﬁnition of total correlation (TC) and mutual information (MI) in equation 2 and equation 1, we discover a connection between TC and MI and summarize it in the following Theorem 3.1. Theorem 3.1. Let X = (x1, x2, . . . , xn) be a group of random variables. Suppose set A = {i1, i2, . . . , ik} ⊆ {1, 2, . . . , n} is an index subgroup. ¯A = {i : i /∈ A} is the complementary set of A. Denote XA = (xi1, xi2, . . . , xik ) as the selected variables from X with the indexes A. Then we have T C(X) = T C(XA) + T C(X ¯A) + I(XA; X ¯A). Theorem 3.1 underscores the insight that the TC of a group of variables X can be decomposed into the TC of two sub- groups T C(XA) and T C(X ¯A) and the MI between the two subgroups I(XA; X ¯A). Therefore, we can recursively represent the TC of the subgroups in terms of MI terms for lower-level subgroups. With this decomposition strategy, we can effectively cast T C(X) into a summation of MI terms between variable subgroups. Note that the decomposition form depends on the separation of variables in each sub- group. In the following, we introduce two types of variable separation strategies: line-like and tree-like decomposition as shown in Figure 1. Line-like Decomposition In each variable subgroup sep- aration, our line-like decomposition strategy splits out a single variable. Let Xi:j = (xi, xi+1, . . . , xj) denote a subset of variables with indexes from i to j. Then we can extend Theorem 3.1 to the following Corollary 3.1.1 and Corollary 3.1.2. Based on Corollary 3.1.1, the line-like decomposition can be dynamically described as: T C(X1:i+1) = T C(X1:i) + I(X1:i; xi+1). (8) Iteratively applying equation 8 to the remaining TC term, we derive the representation of T C(X) as the summation of MI terms in Corollary 3.1.2. With a given MI estimation method ˆI applied to each MI term in Corollary 3.1.2, our line-like TC estimator can be calculated as: ̂T CLine[ˆI](X) = n−1∑ i=1 ˆI(X1:i; xi+1). (9) Corollary 3.1.1. Given the group X and another variable y, T C(X ∪ {y}) = T C(X) + I(X; y). Corollary 3.1.2. Given X = (x1, x2, . . . , xn), we have T C(X) = ∑n−1 i=1 I(X1:i; xi+1). Tree-like Decomposition In each variable subgroup separa- tion, the tree-like decomposition strategy separates variables Xi:j into balanced variable subgroups with similar sizes in the following way: T C(Xi:j) =T C(Xi:⌊(i+j)/2⌋) + T C(X⌊(i+j)/2⌋+1:j) + I(Xi:⌊(i+j)/2⌋; X⌊(i+j)/2⌋+1:j), (10) where ⌊t⌋ indicates the largest integer smaller than t. In ac- cordance with the line-like decomposition, iteratively apply- ing this dichotomous dynamic will ﬁnally convert T C(X) into the summation of MI terms. Since the closed-form of this tree-like TC estimator is hard to summarize in an equation, we describe it recursively in Algorithm 1. We call this novel TC estimator as Total Correlation Estimation with Linear Decomposition (TCeld). From the linearity of the above two decomposition strategies, one can easily derive: Theorem 3.2. If an MI estimator ˆI is an MI upper (or lower) bound, then the corresponding ̂T CLine[ˆI] and ̂T CTree[ˆI] are both the TC upper (or lower) bounds. Therefore, by selecting an MI lower bound ˆI0 and an MI upper bound ˆI1, we can limit the ground-truth TC value in a certain range, ̂T C∗[ ˆI0](X) ≤ T C(X) ≤ ̂T C∗[ ˆI1](X), where ̂T C∗ can be either line-like ̂T CLine or tree-like ̂T CTree. Both the line-like and tree-like TC estimators have no addi- tional requirement on the selection of MI estimators. How- ever, the statistical performance of the proposed TC esti- mators highly depends on the selected MI estimators. To further analyze the inﬂuence of MI bounds choice for TC estimators, we discuss the statistical properties between the TC estimators and MI estimators in the next subsection. 3.2 Statistical Properties of TC Estimators As introduced in Section 2.2, TC estimators can be also regarded as a type of statistics based on observed groups of sample data. Therefore, the aforementioned statistical prop- erties (unbiasedness, consistency, and strong consistency in Section 2.2) are applicable to TC estimators. Thanks to the form of linear combinations of MI terms in our TC estimators, we ﬁnd the following relations between TC and MI estimators in terms of statistical properties: Estimating Total Correlation with Mutual Information Estimators Figure 1: Two decomposition paths of total correlation T C(x1, x2, . . . , xn). Left (tree-like decomposition): Divide the variables in a group into two subgroups with similar sizes. Calculate the MI between the subgroups and recursively calculate the TC of both subgroups. ⌈n/2⌉ is the smallest number larger than n/2. Right (line-like decomposition): Calculate the MI between the current group of variables and the next variable, and then add the next variable into current group. Algorithm 1 Tree-like TC estimator ̂T CTree[ˆI] calculation. Prerequisite: MI estimation method ˆI, samples {X j 1:n}m j=1 = {(xj 1, x j 2, . . . , xj n)} m j=1 Function ̂T CTree[ˆI](Xi:j): if j − i < 1 then return 0 else m = ⌊(i + j)/2⌋ return ̂T CTree[ˆI](Xi:m) + ̂T CTree[ˆI](Xm+1:j) + ˆI(Xi:m; Xm+1:j) end if Theorem 3.3. If an MI estimator ˆI is unbiased, then the corresponding TC estimators ̂T CLine[ˆI] and ̂T CTree[ˆI] are both unbiased. Theorem 3.4. If an MI estimator ˆI is (strongly) consis- tent, then the corresponding TC estimators ̂T CLine[ˆI] and ̂T CTree[ˆI] are both (strongly) consistent. The deﬁnitions of unbiasedness and consistency are intro- duced in Section 2.2. The details of the proofs for both theorems are shown in the Supplementary Material. These two Theorems indicate that the unbiasedness and the con- sistency of the MI estimators is conveniently inherited by the corresponding TC estimators. Note that Belghazi et al. (2018) have shown that MINE (in equation 4) MI estimator is strongly consistent. Consequently, we have: n Corollary 3.4.1. There exists an score network function family {φ(·, ·)} (as described in equation 4), such that ̂T CLine[ˆIMINE] and ̂T CTree[ˆIMINE] are strongly consistent. Apart from the MINE MI estimator, we also analyze the asymptotic behaviors of other MI variational estimators (In- foNCE in equation 6, NWJ in equation 5, and CLUB in equation 7). We found that both InfoNCE and NWJ are strongly consistent while CLUB does not guarantee the con- sistency (supportive proofs are provided in Supplementary Material). Therefore, we summarize the consistency of the corresponding TC estimators in the corollaries below: Corollary 3.4.2. For any MI estimator ˆI ∈ {ˆIInfoNCE, ˆINWJ}, there exists a score network func- tion family {φ(·, ·)}, such that ̂T CLine[ˆI] and ̂T CTree[ˆI] are both strongly consistent. Moreover, strong consistency is a sufﬁcient condition for consistency (DeGroot and Schervish, 2012), hence all of TC-MINE, TC-NWJ and TC-InfoNCE estimators are also consistent statistics. For unbiasedness, though Theorem 3.3 indicates that unbiased MI estimators can lead to unbiased TC estimators, to the best of our knowledge, none of the previous variational MI estimators (Belghazi et al., 2018; Oord et al., 2018; Poole et al., 2019; Cheng et al., 2020a) are unbiased. Therefore, we leave the study of unbiased TC estimators for future work. Besides the above theoretic analysis, we empirically test our TC estimator in Section 5 with the application tasks introduced in Section 4. 4 RELATED WORK Disentangled Representation Learning Disentangled representation learning (DRL) seeks to map each data in- stance into independent latent subspaces, while different subspaces meaningfully represent different attributes of the instance (Locatello et al., 2019b). Recently, DRL meth- ods have attracted considerable interest on various learning tasks such as domain adaptation (Gholami et al., 2020), conditional generation (Burgess et al., 2018), and few-shot learning (Yuan et al., 2021). DRL methods are mainly rec- ognized into two categories as unsupervised and supervised DRL. Prior unsupervised DRL works (Burgess et al., 2018; Kim and Mnih, 2018) utilize different regularizers to make each dimension of latent space to be as independent as pos- sible, which has been challenged by Locatello et al. (2019b) in that each embedding dimension may not be related to a meaningful data attribute. Alternatively, supervised DRL methods (Hjelm et al., 2018; Kim and Mnih, 2018; Cheng et al., 2020b; Yuan et al., 2021) add different supervision terms on different embedding components, which effec- tively learn meaningful embedding while enabling disentan- glement. Both supervised and unsupervised DRL methods require correlation reduction technique to prevent the em- bedding information from leaking into other embedding components. To reduce embedding correlation, Hjelm et al. (2018); Kim and Mnih (2018) use adversarial training meth- ods, while Chen et al. (2018); Cheng et al. (2020b); Yuan et al. (2021) minimize statistical dependency (i.e., MI and Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin Figure 2: Simulation performance of TC line-like and tree-like estimators with different MI estimators. TC), between different embedding components. Contrastive Representation Learning Contrastive repre- sentation learning is a fundamental training methodology which maximizes the difference of positive and negative data pairs to obtain informative representations. In con- trastive learning, a pairwise distance/similarity score func- tion is always set to measure data pairs. Then, the learning objective is to maximize the margin between scores of posi- tive and negative data pairs. Prior contrastive learning has achieved satisfying performance in numerous tasks, such as metric learning (Weinberger et al., 2006; Davis et al., 2007), word embedding learning (Mikolov et al., 2013), and graph embedding learning (Tang et al., 2015; Grover and Leskovec, 2016). Recently, contrastive learning has been recognized as a powerful tool in unsupervised or semi- supervised learning scenarios (He et al., 2020; Chen et al., 2020; Gao et al., 2019), which signiﬁcantly narrows the gap of performance of supervised and unsupervised learn- ing methods. Among these unsupervised methods, Gao et al. (2021) proposed a contrastive text representation learn- ing framework (SimCSE). For each sentence, SimCSE use the dropout mechanism to generate sentence augmentation pairs, then maximize the mutual information within the aug- mented embedding pairs. The empirical results demonstrate contrastive learning obtains high-quality embeddings even with little supervision (Gao et al., 2021). 5 EXPERIMENTS In this section, we empirically evaluate the effectiveness of our TCeld on three tasks: TC estimation, minimization, and maximization. For TC estimation, we generate synthetic data from correlated multi-variate Gaussian distributions, then compare the predictions from our TC estimator with the ground-truth TC values. For TC minimization, we conduct a multi-component disentangled representation learning ex- periment on Colored-MNIST (Esser et al., 2020) dataset, to minimize the total correlation among the digit, style, and color embeddings of digit images. To test the TC maximiza- tion ability, we apply our TC estimator into a contrastive text learning framework to maximize the TC value among different sentence augmentations. Since our proposed TC es- timators can be ﬂexibly induced by different MI estimators, for convenience, we refer the TC estimator as TC-(MI esti- mator name), or TC-(Line/Tree)-(MI estimator name) if the decomposition strategy is speciﬁed. For example, TC-Line- MINE denotes the TC estimator by line-like decomposition with the MINE MI estimator. 5.1 TC Estimation on Simulation Data We ﬁrst test the estimation quality of our TC estimators under simulation scenarios. We selected four MI estima- tors, MINE (Belghazi et al., 2018), NWJ (Nguyen et al., 2010), InfoNCE (Oord et al., 2018), and CLUB (Cheng et al., 2020a), to induce our TC estimators. Then we test TCeld with both tree-like and line-like strategies. The de- tailed description and implementation of the four MI esti- mators are shown in the Supplementary Material, where the results of non-variational methods like KDE, k-NN and their variants are also reported. To evaluate TCeld’s estimation performance with differ- ent ground-truth values, we sample simulated data from a four-dimensional Gaussian distributions (x1, x2, x3, x4) ∼ N (0, Σ), where Σ is a covariance matrix with all diag- onal elements equal to 1, which means the variance of each xi is normalized to 1. With this Gaussian assump- tion, the true TC value can be calculated in a closed-form as T C(x1, x2, x3, x4) = − 1 2 log Det(Σ), where Det(Σ) is the determinant of Σ. Therefore, we can adjust the corre- lation coefﬁcients (non-diagonal elements) in Σ to set the ground-truth TC values in the set {2.0, 4.0, 6.0, 8.0, 10.0} (described in details in the Supplementary Material). The sample dimension is set to 20. The dimension of hidden states for variational estimators is 15. For each ﬁxed TC value, we sample data 4000 times, with batch size 64 and learning rate 0.001 to train the estimators. In Figures 2 we report the performance of TCeld with differ- ent MI bounds at each training step. In each ﬁgure, the true TC value is shown as a step function drawn as a black line. The line-like and tree-like estimation values are presented for different steps as shaded blue and orange curves respec- tively. The dark blue and orange curves illustrate the local averages of the estimated values, with a bandwidth equal to 200. With both tree-like and line-like decomposition, the estimation values for TC-MINE, TC-NWJ and TC-InfoNCE stay below the truth TC step functions, supporting the claims in Theorem 3.2. For the only MI upper bound CLUB, the Estimating Total Correlation with Mutual Information Estimators Figure 3: TC disentangling framework: x is an input image. zc, zd, zs are the color, digit, style embeddings respectively. xrec is the reconstruction. x † is a generated sample from the shufﬂed latent space for adversarial training. estimated values initially lie beneath the ground-truth TC, but ﬁnally converge above the step function. This is because at the beginning of the estimator training, the parameters are not well learned from the synthetic samples, and fail to support a valid MI upper bound. With the training progress going on, the estimator performs better and ﬁnally converges to the desired upper-bound estimation. Furthermore, we provided the bias, variance, and the mean squared error (MSE) of TC estimation values in the Supple- mentary Material. The tree-like and line-like strategies have insigniﬁcant effects on variance of our TC estimators, where TC-NWJ always keep the lowest variance. However, as for bias and MSE, TC-CLUB works uniformly better than other estimators under line-like step, while TC-InfoNCE outper- forms others mostly under tree-like decomposition. By further analyzing this phenomenon, we ﬁnd that when esti- mating I(v; u), CLUB requires a variational approximation qθ(v|u). When we use the line-like decomposition strat- egy, v = xi+1 is always a single variable, and u = X1:i is the concatenation of (x1, . . . , xi). The qθ(v|u) with a neural network implementation can have better performance with output v in a ﬁxed low dimension. However, all the other MI estimators need to learn a score function φ(v, u), where the imbalanced inputs v = xi+1 and u = X1:i can hinder the learning of function φ. In contrast, the tree-like TC estimators split variables equally into subgroups, which facilitate the learning of φ(u, v) with u = Xi:⌊(i+j)/2⌋ and u = X⌊(i+j)/2⌋+1:j for the lower-bound methods. For CLUB, the tree-like decomposition increases the output dimension of the variational net qθ(v|u) and makes the learning more challenging, explaining TC-CLUB’s lower performance than TC-InfoNCE in Supplementary Material. 5.2 TC Minimization for Disentangled Representation Learning For the TC minimization task, we conduct an experiment on a ColorMNIST hand-writing dataset (LeCun et al., 1998; Esser et al., 2020) to learn disentangled digit, color, and style representations of each hand-written number image. Each input data (xi, yi d, yi c) contains three components: a image xi, its digit label yi d ∈ {0, 1, . . . , 9}, and its color Figure 4: Generated examples with latent embedding swap- ping: The latent embeddings zcolor(left-side), zdigit(right- side) of the bottom row (source) are swapped to the corre- sponding embeddings of images in the right column (target). label vector yi c = (Ri, G i, Bi), where Ri, G i, Bi ∈ [0, 1] represents the intensity of colors (Red, Green, Blue). To learn the digit, color, and style representations from in- put images, we use the neural encoder E(·) to map each image xi to the corresponding latent representations zd, zc, zs, respectively. The digit representation zi d is sup- posed to include sufﬁcient digit information from x i, hence we use a digit classiﬁer Fd(·) to predict the digit label with loss Ldigit = Cross-Entropy(Fd(zi d), yi d). Similarly, we set a color regression function Fc(·) on the color em- bedding zi c to ensure representativeness by minimizing l- 2 norm Lcolor = ∥Fc(zi c) − yi c∥2. Excluding the digit and color information, the remaining part from the im- age should be the human hand-writing style information, which is assumed to be independent of the digit and color information. Therefore, we minimize the total correlation LTC = T C(zd, zc, zs) as a regularizer to make sure differ- ent representation components do not include information from the others. Finally, we introduce a decoder D(·) to reconstruct the original image x i from (zd, zc, zs) to in- duce sufﬁcient information into the latent representations with loss Lrecons = ∥D(zd, zc, zs) − x∥ 2. To further en- hance the generation quality of the decoder D(·) , we adapt an adversarial learning regularizer, where we randomize the combination of the latent representation components in each batch, then treat the corresponding decoder output as artiﬁcial (synthesized) data x†. With a discriminator H, we use adversarial training (Goodfellow et al., 2014) to en- sure the decoder to generate high-quality samples with loss Ladv = Ex[log H(x)] + Ex†[log(1 − H(x†))]. Figure 3 illustrates the whole framework. More details about the framework are shown in the Supplementary Material. Evaluations To evaluate the quality of our disentangled representations, we conduct a controllable generation test- ing task to check whether the learned embedding compo- nents (zd, zc, or zs) can control the corresponding attributes (digit, color, or style) of the generated sample D(zd, zc, zs). Hence, we consider three perspectives to evaluate the dis- entanglement: (i) digit transfer: Select another real sample Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin Table 1: Controllable generation results on ColorMNIST. “✓” means adversary training is applied. Accd and Accc are generated digit classiﬁcation accuracy. Res.l2 measures the residual l2-distance for color transferred samples. Adv. Methods Accd Accc Res. l2 AE 2.70 91.65 168.95 TC (Ours) 92.70 97.69 28.66 ✓ AE 0.14 98.82 389.73 ✓ VAE 20.24 92.21 254.83 ✓ DIIN 94.57 97.45 79.38 ✓ TC (Ours) 96.38 98.23 43.04 x′ from testing set and obtain its digit embedding z′ d. Next, feed the new latent embedding combination (z′ d, zc, zs) into the decoder to generate a sample x † = D(z′ d, zc, zs), which is supposed to share the same digit information with x′, so we predict the digit label on x † then report the clas- siﬁcation accuracy Accd. (ii) digit preservation: Select another testing sample x′′, and replace zc with z′′ c to gen- erate x‡ = D(zd, z′′ c , zs). Then predict the digit label of x‡ and report the digit classiﬁcation accuracy Accc. (iii) color transfer: For selected x′′ in (ii), we have its color label vector y′′ c . Hence, we can directly synthesize a ¯x ‡ by setting y′′ c on x. We report the l2-distance (Res.l2) between generated x‡ and synthetic ¯x‡ as color transfer quality. Results and Analysis We compared our method with vanilla auto-encoder (AE), variational auto-encoder (VAE) (Kingma and Welling, 2013), and DIIN (Esser et al., 2020), and report the aforementioned evaluation metrics in Table 1. Implementation and setup details are provided in the Supplementary Material. Vanilla AE fails on Accd because without any disentangled regularizer, zs can con- tain the information revealed from zd and zc VAE partially solves the embedding entanglement problem with its KL divergence term in the learning objective, which encour- ages the latent embedding being close to a dimension-wise- independent standard Gaussian. DIIN(Esser et al., 2020) achievers more signiﬁcant improvement, by projecting the latent space of an auto-encoder to a multi-variate Gaussian distribution using normalizing ﬂow (Kingma and Dhari- wal, 2018). Our TC-based method uniformly outperforms the vanilla AE without the adversarial training. Among all method with adversarial training, the vanilla AE reaches the highest color classiﬁcation accuracy, for which our TC method is also strongly competitive. Moreover, our TC is in the lead on the other two metrics and leave a signiﬁcant performance gap to the vanilla AE, which indicates our TC-based method can learn more balanced attribute embed- dings in terms of representativeness and disentanglement. In addition, we show the generated image examples of digit transfer (i) and color transfer (iii) in Figure 4, where both color and digit information can be successfully preserved in the transferred images. 5.3 TC Maximization for Contrastive Representation Learning To test the performance of TCeld on TC maximization, we conduct a unsupervised text representation learning experi- ment following the SimCSE (Gao et al., 2021) setups. More speciﬁcally, we aim to train a encoder E(·) to map each sentence x into representative embedding z = E(x). Ac- cording to SimCSE (introduced in Section 4), one can learn in a unsupervised way the encoder E(·), by ﬁrst generating several data augmentations ( ˜x1, ˜x2, . . . , ˜xn), then maximiz- ing the correlation of corresponding ( ˜z1, ˜z2, . . . , ˜zn). Most of the previous contrastive learning methods (Chen et al., 2020; Gao et al., 2021; Jiang et al., 2022) focus on two- augmentation cases, where the mutual information between the two augmentations I( ˜z1; ˜z2) is maximized for each input x. Moreover, none of current text contrastive learn- ing method handles multi-augmentation (n ≥ 3) situations. However, Tian et al. (2020) point out that contrastive learn- ing with more augmentations can further enhance the latent embedding quality. Therefore, for testing our TC estima- tors while attempting the ﬁrst multi-augmentation text con- trastive learning, we plan to maximize T C( ˜z1, ˜z2, . . . , ˜zn) to train the text encoder E. Model Frameworks We conduct our multi-augmentation text contrastive leaning by extending prior two- augmentation methods, SimCSE (Gao et al., 2021) and PromptBERT (Jiang et al., 2022) with four augmenta- tions for each text input. Both SimCSE and PromptBERT maximize the InfoNCE MI estimator between the two augmentation embeddings with a BERT(Devlin et al., 2018)-based pretrained text encoder. Correspondingly, our extended SimCSE-TC and PromptBERT-TC utilize the same encoder structure but maximize TC-InfoNCE of four augmentation embeddings for each sentence. Following SimCSE (Gao et al., 2021) and PromptBERT (Jiang et al., 2022), we ﬁnetine the text encoder E on pretrained BERTbase (Devlin et al., 2018) and RoBERTabase (Liu et al., 2019). Based on our observation in Section 5.1, tree-like decomposition empirically works better for InfoNCE estimator. Therefore, we select TC-Tree-InfoNCE for this text contrastive learning task. More setup details can be found in the Supplementary Material. Evaluation Following previous work (Gao et al., 2021; Jiang et al., 2022), we evaluate models on 7 semantic textual similarity (STS) datasets: STS12 (Agirre et al., 2012), STS13 (Agirre et al., 2013), STS14 (Agirre et al., 2014), STS15 (Agirre et al., 2015), STS16 (Agirre et al., 2016), STS Benchmark (Cer et al., 2017) and SICK- Relatedness (Marelli et al., 2014). The task is to predict the similarity (ranging from 0 to 5) between paired sentences with learned text representations. We report Spearman’s correlation (Myers et al., 2013) between model prediction and the ground truth. More details about the model design, hyperparameter settings and evaluation metrics are in the Estimating Total Correlation with Mutual Information Estimators (a) MI-ﬁx (b) MI-sample (c) Core-View (d) Full-Graph (e) TC-Tree Figure 5: Correlation maximization strategy for multiple augmentations. Nodes A1, A2, A3, A4 mean four input augmenta- tions. Each solid line indicates an MI maximization between the connected two augmentations. Each dashed line means the connected augmentation pair is randomly selected. Table 2: Text representation evaluation on STS tasks (Spearman’s correlation with “all” setting). Methods with “ -TC” means that total correlation is used as the loss function. Method STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg. IS-BERTbase (Zhang et al., 2020) 56.77 69.24 61.21 75.23 70.16 69.21 64.25 66.58 ConSERTbase (Yan et al., 2021) 64.64 78.49 69.07 79.72 75.95 73.97 67.31 72.74 SimCSE-BERTbase (Gao et al., 2021) 68.40 82.41 74.38 80.91 78.56 76.85 72.23 76.25 SimCSE-BERTbase-TC 68.66 81.45 74.34 81.81 79.24 78.85 72.46 76.69 PromptBERTbase (Jiang et al., 2022) 71.56 84.58 76.98 84.47 80.60 81.60 69.87 78.54±0.15 PromptBERTbase-TC 72.05 84.61 77.23 84.73 80.34 81.89 70.23 78.72±0.10 SimCSE-RoBERTabase (Gao et al., 2021) 70.16 81.77 73.24 81.36 80.65 80.22 68.56 76.57 SimCSE-RoBERTabase-TC 71.46 82.16 74.14 82.17 80.93 80.02 68.24 77.02 PromptRoBERTabase (Jiang et al., 2022) 73.94 84.74 77.28 84.99 81.74 81.88 69.50 79.15±0.25 PromptRoBERTabase-TC 72.58 85.06 78.24 85.82 81.95 82.94 70.47 79.58±0.25 Table 3: Ablation study of different correlation maximiza- tion strategies. Average Spearman coefﬁcient is reported over the seven STS tasks. Method Model Prompt BERT Prompt RoBERTa MI-ﬁx 78.54±0.15 79.28±0.23 MI-sample 78.41±0.13 79.27±0.34 Core-View (Tian et al., 2020) 78.64±0.22 79.41±0.25 Full-Graph (Tian et al., 2020) 78.52±0.13 79.49±0.26 TC 78.72±0.10 79.58±0.25 Supplementary Material. Results and Analysis We report the mean and standard de- viation over 10 runs with different random seeds in Table 2. On most of the evaluation datasets, our TC-based methods outperform their corresponding baselines, in which Prompt- BERT/RoBERTa are the state-of-the-art unsupervised sen- tence representation learning methods. These results also underline the claim that more augmentations lead to higher representation quality in contrastive learning. For the ablation study, we ﬁx the number of augmenta- tions to 4, and test the inﬂuence of different embedding correlation maximization strategies. Since there is no prior work on multi-augmentation text contrastive learning, we proposed several substitute strategies by ourselves in Fig- ure 5: (a) MI-ﬁx: only select the ﬁrst two augmentations and omit the others; (b) MI-sample: randomly select two aug- mentations and omit the others; (c) Core-View (Tian et al., 2020): calculate the MI between one ﬁxed augmentation and each augmentation in the rest. (d) Full-Graph (Tian et al., 2020): calculate the MI values between each augmen- tation pairs. From the results in Table 3, MI-ﬁx and MI- sample which only utilized two augmentations, have lower Spearman score than the other methods using all augmenta- tions’ information. Our TC-based correlation maximization strategy slightly outperforms Core-View and Full-Graph. Core-View does not consider the correlation among the other augmentations (A2, A3, A4 in Figure 5c). Full-Graph uses expensive MI estimators, which quadratically increases the computational complexity of augmentation correlation maximization, while being more prone to overﬁtting than TC with the same data size. 6 CONCUCLUSION We have derived a Total Correlation Estimation with Linear Decomposition (TCeld), which converts total correlation into summation of mutual information terms along two sep- aration paths (i.e., line-like and tree-like). By applying variational estimators to each MI term in TCeld, we have obtained TC-Line and TC-Tree estimators. Further, we have analyzed the statistical properties of the proposed TC esti- mators and claimed their strong consistency when induced by appropriate MI estimators such as MINE, NWJ, and In- foNCE. Moreover, we have empirically demonstrated the Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin effectiveness of the proposed TC estimators on both TC estimation and optimization tasks. The experimental results show that our TC estimators can only provide reliable esti- mation from samples, but also serve as an effective learning regularizer for model training. More properties of TCeld, such as sample complexity, unbiasedness, and low-variance, remain to be explored both theoretically and empirically. We hope this study can serve to promote TC, as a multi-variate information concept, to apply into cutting-edge deep learn- ing models, such as representation learning, controllable generation, model distillation and ensemble. References Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez- Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Computational Semantics–Volume 1: Pro- ceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Work- shop on Semantic Evaluation (SemEval 2012), pages 385– 393, 2012. Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. * sem 2013 shared task: Se- mantic textual similarity. In Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity, pages 32–43, 2013. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. Semeval- 2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th international workshop on seman- tic evaluation (SemEval 2014), pages 81–91, 2014. Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 252–263, 2015. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, German Rigau Clara- munt, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evalua- tion. In SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (Associ- ation for Computational Linguistics), 2016. Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. David Barber and Felix V Agakov. The im algorithm: a variational approach to information maximization. In Advances in neural information processing systems, page None, 2003. Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajesh- war, Sherjil Ozair, Yoshua Bengio, Devon Hjelm, and Aaron Courville. Mutual information neural estimation. In International Conference on Machine Learning, pages 530–539, 2018. Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in beta- vae. arXiv preprint arXiv:1804.03599, 2018. Jean-François Cardoso. Dependence, correlation and gaus- sianity in independent component analysis. Journal of Machine Learning Research, 4(Dec):1177–1203, 2003. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evalua- tion. arXiv preprint arXiv:1708.00055, 2017. Benjamin Charrow, Sikang Liu, Vijay Kumar, and Nathan Michael. Information-theoretic mapping using cauchy- schwarz quadratic mutual information. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 4791–4798. IEEE, 2015. Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in varia- tional autoencoders. In Advances in Neural Information Processing Systems, pages 2610–2620, 2018. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing gener- ative adversarial nets. In Advances in neural information processing systems, pages 2172–2180, 2016. Pengyu Cheng and Ruineng Li. Replacing language model for style transfer. arXiv preprint arXiv:2211.07343, 2022. Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. CLUB: A contrastive log- ratio upper bound of mutual information. arXiv preprint arXiv:2006.12013, 2020a. Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, and Lawrence Carin. Improving disentangled text representa- tion learning with information-theoretic guidance. arXiv preprint arXiv:2006.00693, 2020b. Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. Fairﬁl: Contrastive neural debiasing method for pretrained text encoders. In International Conference on Learning Representations, 2021. Estimating Total Correlation with Mutual Information Estimators Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. Flexibly fair representation learning by disentanglement. In Kamalika Chaudhuri and Rus- lan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, vol- ume 97 of Proceedings of Machine Learning Re- search, pages 1436–1445. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ creager19a.html. Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. Information-theoretic metric learning. In Proceedings of the 24th international conference on Machine learning, pages 209–216, 2007. Morris H DeGroot and Mark J Schervish. Probability and statistics. Pearson Education, 2012. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on pure and applied mathematics, 36(2):183–212, 1983. Patrick Esser, Robin Rombach, and Bjorn Ommer. A disen- tangling invertible interpretation network for explaining latent representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9223–9232, 2020. Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Auto-encoding total correlation explanation. In The 22nd International Conference on Artiﬁcial Intel- ligence and Statistics, pages 1157–1166, 2019. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing, pages 6894–6910, 2021. Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demys- tifying ﬁxed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8):5629– 5661, 2018. Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstanti- nos Bousmalis, and Vladimir Pavlovic. Unsupervised multi-target domain adaptation: An information theoretic approach. IEEE Transactions on Image Processing, 2020. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Ad- vances in neural information processing systems, 27, 2014. Clive Granger and Jin-Lung Lin. Using the mutual infor- mation coefﬁcient to identify lags in nonlinear models. Journal of time series analysis, 15(4):371–384, 1994. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016. Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormal- ized statistical models. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 297–304, 2010. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738, 2020. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018. Bo Jiang, Chao Ye, and Jun S Liu. Nonparametric k-sample tests via dynamic slicing. Journal of the American Statis- tical Association, 110(510):642–653, 2015. Ting Jiang, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Liangjie Zhang, and Qi Zhang. Promptbert: Improving BERT sen- tence embeddings with prompts. CoRR, abs/2201.04337, 2022. URL https://arxiv.org/abs/2201. 04337. Brian J Julian, Sertac Karaman, and Daniela Rus. On mu- tual information-based control of range sensing robots for mapping applications. The International Journal of Robotics Research, 33(10):1375–1392, 2014. Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, et al. Nonparametric von mises estimators for entropies, divergences and mutual informations. Advances in Neural Information Processing Systems, 28, 2015. Hyunjik Kim and Andriy Mnih. Disentangling by factoris- ing. In International Conference on Machine Learning, pages 2649–2658, 2018. Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. Alexander Kraskov, Harald Stögbauer, and Peter Grass- berger. Estimating mutual information. Physical review. E, Statistical, nonlinear, and soft matter physics, 69 6 Pt 2:066138, 2004a. Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin Alexander Kraskov, Harald Stögbauer, and Peter Grass- berger. Estimating mutual information. Physical review E, 69(6):066138, 2004b. Alexander Lachmann, Federico M Giorgi, Gonzalo Lopez, and Andrea Califano. Aracne-ap: gene network reverse engineering through adaptive partitioning inference of mutual information. Bioinformatics, 32(14):2233–2235, 2016. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Schölkopf, and Olivier Bachem. On the fairness of disentangled representations. In Ad- vances in Neural Information Processing Systems, pages 14611–14624, 2019a. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the un- supervised learning of disentangled representations. In International Conference on Machine Learning, pages 4114–4124, 2019b. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Ben- tivogli, Raffaella Bernardi, Roberto Zamparelli, et al. A sick cure for the evaluation of compositional distribu- tional semantic models. In Lrec, pages 216–223. Reyk- javik, 2014. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. Jerome L Myers, Arnold D Well, and Robert F Lorch Jr. Re- search design and statistical analysis. Routledge, 2013. XuanLong Nguyen, Martin J Wainwright, and Michael I Jor- dan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep- resentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Dávid Pál, Barnabás Póczos, and Csaba Szepesvári. Esti- mation of rényi entropy and mutual information based on generalized nearest-neighbor graphs. Advances in Neural Information Processing Systems, 23, 2010. Liam Paninski. Estimation of entropy and mutual informa- tion. Neural computation, 15(6):1191–1253, 2003. Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with disentangled representations. In Kamalika Chaudhuri and Rus- lan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, vol- ume 97 of Proceedings of Machine Learning Re- search, pages 5102–5112. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ peng19b.html. Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171–5180, 2019. Shashank Singh and Barnabás Póczos. Exponential con- centration of a density functional estimator. Advances in Neural Information Processing Systems, 27, 2014. Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning controllable fair representations. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 2164–2173, 2019. Milan Studen`y and Jirina Vejnarová. The multi-information function as a tool for measuring stochastic dependence. In Learning in graphical models, pages 261–297. Springer, 1998. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web, pages 1067–1077, 2015. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con- trastive multiview coding. In European conference on computer vision, pages 776–794. Springer, 2020. Greg Ver Steeg and Aram Galstyan. Discovering structure in high-dimensional data through correlation explanation. Advances in Neural Information Processing Systems, 27, 2014. Satosi Watanabe. Information theoretical analysis of multi- variate correlation. IBM Journal of research and develop- ment, 4(1):66–82, 1960. Kilian Q Weinberger, John Blitzer, and Lawrence K Saul. Distance metric learning for large margin nearest neigh- bor classiﬁcation. In Advances in neural information processing systems, pages 1473–1480, 2006. Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. Consert: A contrastive framework for self-supervised sentence representation transfer. arXiv preprint arXiv:2105.11741, 2021. Estimating Total Correlation with Mutual Information Estimators Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, and Lawrence Carin. Improving zero-shot voice style transfer via disentangled representation learning. In International Conference on Learning Representations, 2021. Diego J Zea, Diego Anfossi, Morten Nielsen, and Cristina Marino-Buslje. Mitos. jl: mutual information tools for protein sequence analysis in the julia language. Bioinfor- matics, 33(4):564–565, 2016. Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Li- dong Bing. An unsupervised sentence embedding method by mutual information maximization. arXiv preprint arXiv:2009.12061, 2020. Sicheng Zhu, Xiao Zhang, and David Evans. Learn- ing adversarially robust representations via worst-case mutual information maximization. arXiv preprint arXiv:2002.11798, 2020. Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin A PROOFS Proof of Theorem 3.1. Note that XA := (xi1, xi2 , . . . , xim ) and X ˆA = X/XA. Denote X ˆA = (xj1 , xj2, . . . , xjl ). Then T C(X) = Ep(X) [ log p(x1, x2, . . . , xn) p(x1)p(x2) . . . p(xn) ] =Ep(X) [ log ( p(XA) p(xi1)p(xi2) . . . p(xim ) · p(X ˆA) p(xj1)p(xj2) . . . p(xjl ) · p(X) p(XA)p(X ˆA) )] =T C(XA) + T C(X ˆA) + I(XA; X ˆA) Proof of Corollary 3.1.2. We denote Xi:j := (xi, xi+1, . . . , xj−1, xj). Note that T C(X1:n) =Ep(x1,x2,...,xn) [ log p(x1, x2, . . . , xn) p(x1)p(x2) . . . p(xn) ] =Ep(x1,x2,...,xn) [ log ( p(x1, x2, . . . , xn−1, xn) p(x1, x2, . . . , xn−1)p(xn) · p(x1, x2, . . . , xn−1) p(x1)p(x2) . . . p(xn−1) )] =I(x1, x2, . . . , xn−1; xn) + T C(X1:n−1) =I(X1:n−1; xn) + T C(X1:n−1) Similarly, T C(X1:n) = I(X1:n−1; xn) + I(X1:n−2; xn−1) + T C(X1:n−2) = n−1∑ i=1 I(X1:i; xi+1) (11) Proof of Theorem 3.3. First consider line-like TC estimator ̂T CLine[ˆI](X) = ∑n−1 i=1 ˆI(X1:i; xi+1). If the MI estimator ˆI is unbiased, by deﬁnition, E[ˆI(X1:i; xi+1)] = I(X1:i; xi+1). Taking expectation for the TC estimator, E[ ̂T CLine[ˆI](X)] = n−1∑ i=1 E[ˆI(X1:i; xi+1)] = n−1∑ i=1 I(X1:i; xi+1) = T C(X), (12) which means that ̂T CLine[ˆI](X) is unbiased. Similarly, we can show that ̂T CTree[ˆI](X) is unbiased. Proof of Theorem 3.4. For the convenience of notation, we show the proof with our ̂T CLine[ˆI] estimator. The proof can be easily applied to ˆT CTree[ˆI], since both ˆT CLine[ˆI] and ˆT CTree[ˆI] are linear combination of MI terms. We denote ˆIm(x; y) = ˆI({(xk, yk)}m k=1) and ̂T C[ˆI]m(X) = ̂T C[ˆI]({X k}m k=1) as the estimators with m samples {(xk, yk)}m k=1 ∼ p(x, y), and {X k} m k=1 ∼ p(X) respectively. Strong Consistency: If ˆI is a strong consistent estimator, by the Deﬁnition 2.3, ∀ε > 0, with a ﬁx variable dimension n ∈ N+, for each variable pair (X1:i, xi+1) (i = 1, 2, . . . , n − 1), ∃Mi > 0, such that ∀m > Mi, P { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ ≤ ε n } = 1. (13) Let ¯M = max{M1, M2, . . . , Mn−1}, then ∀m > ¯M , P { ∣ ∣ ∣ ̂T CLine[ˆI]m(X) − T C(X) ∣ ∣ ∣ ≤ ε } (14) =P { ∣ ∣ ∣ ∣ ∣ n−1∑ i=1 ˆIm(X1:i; xi+1) − n−1∑ i=1 I(X1:i; xi+1) ∣ ∣ ∣ ∣ ∣ ≤ ε} (15) ≥P [n−1⋂ i=1 { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ ≤ ε n } ] . (16) Estimating Total Correlation with Mutual Information Estimators The inequality between equation 15 and equation 16 is because the condition in equation 15 is sufﬁcient to deduce the condition in equation 15: ∣ ∣ ∣ ∣ ∣ n−1∑ i=1 ˆIm(X1:i; xi+1) − n−1∑ i=1 I(X1:i; xi+1) ∣ ∣ ∣ ∣ ∣ ≤ n−1∑ i=1 ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ ≤ n − 1 n ε < ε. Denote event Bi = { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ ≤ ε n } , by equation 13, P[Bi] = 1. Consider the union Bi ∪ Bj, we have: 1 = P[Bi] ≤ P[Bi ∪ Bj] = P[Bi] + P[Bj] − P[Bi ∩ Bj] = 2 − P[Bi ∩ Bj] ≤ 1, (17) which means P[Bi ∩ Bj] = 1. Iteratively applying this conclusion, we know P[∩ n−1 i=1 Bi] = 1. Therefore, P [⋂n−1 i=1 { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ ≤ ε n }] = P[∩ n−1 i=1 Bi] = 1. Combining with equation 14 and equation 16, we conclude that ∀ε, ∃ ˆM , such that ∀m > ˆM , ∣ ∣ ∣ ̂T CLine[ˆI]m(X) − T C(X)∣ ∣ ∣ ≤ ε almost surely, which supports ̂T CLine[ˆI] is strongly consistent. Consistency: If ˆI is a consistent estimator, by the Deﬁnition 2.2, ∀ε > 0 and σ > 0, with a ﬁxed variable dimension n ∈ N+, for each variable pair (X1:i, xi+1) (i = 1, 2, . . . , n − 1), ∃Mi, such that ∀m > Mi, P { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ < ε n } > 1 − σ n . (18) Let ˆM = max{M1, M2, . . . , Mn−1}, then ∀m > ˆM , similar to equation 14, equation 15, and equation 16, P { ∣ ∣ ∣ ̂T CLine[ˆI]m(X) − T C(X) ∣ ∣ ∣ < ε } (19) =P { ∣ ∣ ∣ ∣ ∣ n−1∑ i=1 ˆIm(X1:i; xi+1) − n−1∑ i=1 I(X1:i; xi+1) ∣ ∣ ∣ ∣ ∣ < ε } (20) ≥P [n−1⋂ i=1 { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ < ε n } ] . (21) Denote Bi = { ∣ ∣ ∣ˆIm(X1:i; xi+1) − I(X1:i; xi+1) ∣ ∣ ∣ < ε n } . Since 1 ≥ P[Bi ∪ Bj] = P[Bi] + P[Bj] − P[Bi ∩ Bj], we have P[Bi ∩ Bj] ≥ P[Bi] + P[Bj] − 1 > (1 − σ n ) + (1 − σ n ) − 1 = 1 − 2 n σ. Similarly, we can obtain P[Bi ∩ Bj ∩ Bk] ≥ P[Bi ∩ Bj] + P[Bk] − 1 ≥ (1 − 2 n σ) + (1 − σ n ) = 1 − 3 n σ and P{ ∣ ∣ ∣ ̂T CLine[ˆI]m(X) − T C(X) ∣ ∣ ∣ < ε} ≥ P [ ∩ n−1 i=1 Bi] ≥ 1 − σ. Therefore, ∀ε > 0, limm→∞ P{| ̂T CLine[ˆI]m(X) − T C(X)| ≥ ε} = 0. ̂T CLine[ˆI] is consistent. Proof of Corollary 3.4.2. By Theorem 3.4, only need to show both ˆIInfoNCE and ˆINWJ are strongly consistent. Inspired by the proof of Theorem 2 in Belghazi et al. (2018), we only need to proof the following two lemmas: Lemma A.1. For any η > 0, there exists a feedforward score network function ˆφ : Ω → R such that |I(x, y) − ˆI[ ˆφ]| ≤ η, where ˆI ∈ {ˆIInfoNCE, ˆINWJ}, ˆIInfoNCE[ ˆφ] = Ep(x,y)[ ˆφ(x, y)] − Ep(x)[log Ep(y)[exp ˆφ(x, y)]] and ˆINWJ[ ˆφ] = Ep(x,y)[ ˆφ(x, y)] − Ep(x)p(y)[exp( ˆφ(x, y) − 1)] Lemma A.2. For any η > 0, let H be the family of functions φ : Ω → R deﬁned by a give network architecture. Assume the parameter θ of network φ are restricted to some compact domain Θ ⊂ Rk. Then there exists N ∈ N+, such that, ∀m ≥ N , |ˆIm(x, y) − supφ∈H ˆI[φ]| ≤ η with probability one. Here ˆI ∈ {ˆIInfoNCE, ˆINWJ}. To proof Lemma A.1, for NWJ, we select function φ∗(x, y) = 1 + log p(x,y) p(x)p(y) . Then Ep(x,y)[ˆIInfoNCE[φ∗]] = I(x, y). The difference can be calculated as I(x, y) − ˆINWJ[φ] = Ep(x,y)[φ∗ − φ] − exp(−1)Ep(x)p(y)[exp(φ∗) − exp(φ)]. (22) Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin The right-hand side of equation 22 has the same form as equation (25) in Belghazi et al. (2018), except a coefﬁcient exp(−1) for the second term. Therefore, we can exactly follow the proof of Section 6.2.1 of Belghazi et al. (2018) to prove our Lemma A.1 for the NWJ estimator, with only adjusting the coefﬁcient weight of term |φ∗ − φ| and term | exp(φ∗) − exp(φ)|. For InfoNCE, we select φ∗(x, y) = log p(x|y), so that Ep(x,y)[ˆIInfoNCE[φ∗]] = I(x; y). The difference can be written as I(x; y) − ˆIInfoNCE[φ] =Ep(x,y)[φ∗ − φ] − Ep(x)[log Ep(y)[exp(φ∗)] − log Ep(y)[exp(φ)]]. (23) Similarly to Section 6.2.1 in (Belghazi et al., 2018), we can consider the cases whether φ is bounded, then apply the universal approximation theorem to show Lemma A.1 for InfoNCE. To proof Lemma A.2, we denote P = p(x, y), Q = p(x)p(y), and Pm, Qm for emprical distribution with m samples. For NWJ, we calculate the difference |ˆIm(x, y) − sup φ∈H ˆI[φ]| ≤ sup φ∈H |EP[φ] − EPm[φ]| + exp(−1) sup φ∈H |EQ[exp(φ)] − EQm[exp(φ)]|, (24) where the second term of right-hand side has the same form as equation (32) in Secion 6.2.2 in Belghazi et al. (2018). Therefore, we can follow the proof in Secion 6.2.2 of Belghazi et al. (2018) to prove Lemma 2 for NWJ. Similarly, the similar proving process can be applied to InfoNCE. B EXPERIMENTAL DETAILS All experiments are executed on a single NVIDIA Titan Xp GPU with 12,196M memory. B.1 TC estimation Experiment Design Mutual information between two multivariate Guassian distributions X1 ∼ N (0, Σ1), X2 ∼ N (0, Σ2) is 1 2 log Det(Σ1)Det(Σ2) Det(Σ) , where Σ is the covariance matrix of the joint distribution [X1, X2]. In our setting, our training samples are sampled from a joint distribution with n variables [Xi, i ∈ {0, 1, 2, 3, n − 1}], the marginal distribution of each variable have zero mean and identity covariance matrix with dimension d. Therefore, the determinant of covariance matrix of single variable Σi is 1 and the mutual information between two variables i, j is − 1 2 log Det(Σij). The total correlation among variables are − 1 2 log Det(Σ). To prove this, we use the idea of line-like structure. Assume that the total correlation of ﬁrst k variables are − 1 2 log Det(Σ[:k]), the total correlation of the ﬁrst k + 1 variables are − 1 2 log Det(Σ[:k+1]) = − 1 2 log Det(Σ[:k]) + 1 2 log Det(Σ[:k]) Det(Σ[:k+1]) , (25) where Σ:k is the covariance matrix of the ﬁrst k variables. In our proof-of-concept experiments, we set n = 4, d = 10. The covariance matrix is     Id σId 0 0 σId Id 0 0 0 0 Id σId 0 0 σId Id     The total correlation under such a design is −d log(1 − σ2). As we mentioned in the paper, we can adjust the correlation coefﬁcients (non-diagonal elements) σ to set the ground truth TC values in the set {2.0, 4.0, 6.0, 8.0, 10.0}. Hyper-parameters All MI lower bounds require the learning of a value function f (x, y); the CLUB upper bound requires the learning of a network approximation qθ(y|x). To make a fair comparison, we set the value function and the neural approximation with one hidden layer and the same hidden units. For the multivariate Gaussian setup, the number of hidden units is 20. On the top of the hidden layer output, we add the ReLU activation function. The learning rate for all estimators is set to 1 × 10 −4. Estimating Total Correlation with Mutual Information Estimators Figure 6: Bias, variance and MSE of line-like TC estimators. Figure 7: Bias, variance and MSE of tree-like TC estimators. Table 4: Comparison between ours(Tree-based CLUB) with other non-parametric methods under two circumstance with different data dimensionality (n = 2 and n = 10. The table shows the absolute error between predicted and ground truth. Bold means the minimal error and the best predication. n = 2 n = 10 Total Correlation 2 4 6 8 10 2 4 6 8 10 KNN ((Kraskov et al., 2004b; Pál et al., 2010) 0.08 0.29 0.66 1.29 2.27 1.34 2.66 4.05 5.47 6.98 Bias-improved KNN ((Gao et al., 2018)) 0.2 0.59 1.24 2.21 3.54 1.48 2.97 4.51 6.07 7.67 Kernel Density Estimation 1.44 1.40 1.40 1.41 1.37 9.02 8.89 8.98 9.00 8.97 Ours 0.03 0.04 0.55 0.88 1.63 0.23 0.37 0.22 1.03 1.96 Bias, Variance, Mean Squared Error Figure 6 7 show the bias, variance and mean squared error using different mutual information estimators. The explanation to this ﬁgure is shown in the main paper. Non-Parametric Methods Since the probability can be directly estimated using non-parametric methods, we can also use non-parametric methods to estimate the total correlation directly. Here we compare with KDE, k-NN based methods (Kraskov et al., 2004b; Pál et al., 2010) and its variant (Gao et al., 2018). As shown table Table 4, the non-parametric methods can only estimate the total correlation decently when the dimension of input n is low. Note that the experiment setting is exactly the same as the main paper except for dimension. Considering MINE (Belghazi et al., 2018) has already performs better than k-NN based methods (Kraskov et al., 2004b; Pál et al., 2010) and MINE is one of our baselines, our performance should perform better. This is also reﬂected in the table. Meanwhile, k-Nearest Neighbor estimation method focuses on total correlation estimation only with non-differentiable operation. The estimated value is devoted to variable independence and correlation analysis. While in our neural network- based methods, we calculate the total correlation in a derivative way and get meaningful gradient information for efﬁcient back-propagation. The results are shown in our disentangle and representation learning experiments. B.2 TC Maximization: Disentanglement Model Design The dimension of latent space z, zcolor, zdigit is 128, 32, 32. The structures of each model are shown in Table 5 6 7. We use TC-InfoNCE as our total correlation estimator. Note that when the number of variables is three. There is no difference between tree-based and line-based methods. Both of which require two mutual information estimators, one for mutual information estimation of any pair, the other is to capture the mutual information between the left one and the preselected pair. The parameters of these two estimators are listed in Table 7. Ke Bai ∗, Pengyu Cheng ∗, Weituo Hao, Ricardo Henao, Lawrence Carin Table 5: Structure of the encoder E(·), decoder D(·) described in Section B.2. The terms in brackets of Conv2d and DeConv2d are (input channel, output channel, ﬁlter size, stride size, zero padding size, bias included). IN means instance normalization. DeConv2d represents the deconvolution operator. Encoder Decoder 0 Conv2d(3, 64, 4, 2, 1, False), IN, LeakyReLU DeConv2d(128, 512, 2, 1, 0, True) 1 Conv2d(64, 128, 4, 2, 1, False), IN, LeakyReLU DeConv2d(512, 256, 4, 2, 1, False), IN, LeakyReLU 2 Conv2d(128, 256, 4, 2, 1, False), IN, LeakyReLU DeConv2d(256, 128, 4, 2, 1, False), IN, LeakyReLU 3 Conv2d(256, 512, 4, 2, 1, False), IN, LeakyReLU DeConv2d(128, 64, 4, 2, 1, False), IN, LeakyReLU 4 Conv2d(512, 128, 2, 1, 0, True), , DeConv2d(256, 128, 4, 2, 1, False), ,Tanh Table 6: Structure of classiﬁer Fd(·) and regressor Fc(·) described in Section B.2. The terms in the bracket of Linear are (input dimension, output dimension, bias included). Color Regressor Digit Classiﬁer 0 Linear(32, 16, True), ReLU Linear(32, 16, True), ReLU 1 Linear(16, 3, True), Sigmoid Linear(16, 10, True), We compare our method with autoencoder and variational autoencoder. Apart from the reconstruction loss Lrecons and KL loss used in AE and VAE, we also take regression loss Lcolor, classiﬁcation loss Ldigit into account. We illustrate the failure transfer case under the AE setup to show the difference without the total correlation term in Figure 8. Both the color and digit are not successfully transferred from the source (bottom row) to the target (rightmost column). Hyper-parameters The total training epoch and batch size are 300 and 512. Learning rate is 5e−3 for encoder and decoder, 1e−4 for the adversarial discriminator, 1e−3 for the digit classiﬁer and color regressor. All the optimizers are Adam. Since the ﬁnal loss is the summation of each term, the weight of each term is one except for the color regressor loss, which is ten. We also add spectral normalization (Miyato et al., 2018) to the adversarial discriminator to further stabilize the adversarial training. B.3 TC Minimization: Sentence Representation Training Setup We train the model with 106 randomly sampled sentences from the English Wikipedia (Foundation) dataset. Model Structure The model consists of two parts, f is the transformer-based large pretrained language model and g is a multiple layer perception with one hidden layer and ReLU activation function. The hidden neurons and output neurons are 256. The structure of the model follows Gao et al. (2021); Jiang et al. (2022). Hyper-parameters Considering that f and g are initialized in different ways, we scale up the learning rate of g. In detail, the learning rate and learning rate scale are 1e−5 and 10 for models with SimCSE-BERT, SimCSE-RoBERTa and PromptBERT as baselines, 5e−6 and 100 for the model with PromptRoBERTa as baselines. These hyper-parameters are selected according to the validation set. The batch size is 256. The maximum length of a sentence is limited to 32. The training epoch is one. The best model selected based on the performance of the validation set is applied to the testing set. The validation set is evaluated every 250 steps. Augmentations Following the simple but effective data augmentation techniques in SimCSE, we get four augmentations by feeding the same input to the network four times and sample four dropout samples independently. Prompt-BERT also applies this dropout strategy, while they modify the inputs as well as the output feature representation neuron. SimCSE and PromptBERT both apply the InfoNCE with cosine similarity as the score function and use the large masked language pretrained model like BERT(Devlin et al., 2018) as the initialization of the feature encoder f (·). The difference is how they construct the paired distribution p(x, y). SimCSE feeds the same sentence to the encoder twice and obtains different output embeddings by independently sampling two dropout masks of the encoder f (·) ([CLS] representation is taken as the representation of the sentence). PromptBERT adapts two ﬁxed cloze-style templates to augment the original Estimating Total Correlation with Mutual Information Estimators (a) color transfer (b) digit transfer Figure 8: Failure case of disentangle transfer, using autoencoder without the total correlation constraint. The bottom row is the source image and rightmost column is the target image. Table 7: Structure of the adversarial discriminator H(·) and two total correlation estimators described in Section B.2. The terms in brackets of Conv2d and DeConv2d are (input channel, output channel, ﬁlter size, stride size, zero padding size, bias included). The terms in the bracket of Linear are (input dimension, output dimension, bias included). BN means batch normalization. Discriminator TC estimator 1 TC estimator 2 0 Conv2d(3, 32, 5, 1, 2, False), BN, ReLU Linear(128, 90, True), ReLU Linear(64, 64, True), ReLU 1 Conv2d(32, 64, 5, 1, 2, False), BN, ReLU Linear(90, 90, True), ReLU Linear(64, 64, True), ReLU 2 Linear(4096, 1), , Linear(90, 1, True), Softplus Linear(64, 1, True), Softplus Table 8: Four templates used in PromptBERT-T C and PromptRoBERTa-T C. [CLS],[MASK], [SEP] are special tokens used in BERT. [X] is the placeholder of the input sentence. BERT RoBERTa Template 0 [CLS] This sentence of \"[X]\" means[MASK].[SEP] [CLS] This sentence of ’[X]’ means[MASK].[SEP] Template 1 [CLS] The sentence : \"[X]\" means [MASK].[SEP] [CLS] The sentence : ’[X]’ means [MASK].[SEP] Template 2 [CLS] The sentence ’ [X] ’ has the same meaning with [MASK].[SEP] [CLS] The sentence ’ [X] ’ has the same meaning with [MASK].[SEP] Template 3 [CLS] [MASK] has similar meaning with sentence : ’[X]’.[SEP] [CLS] [MASK] has similar meaning with sentence : ’[X]’.[SEP] sentence. For example, a template could be “this sentence : [X] means [MASK].”, where [X] is the placeholder of the origin input sentence and [MASK] is a special placeholder token. The representation of [MASK] token f (X)[MASK] is donated as the sentence embedding. In our total correlation estimation experiments, we augment the input sentence four times. More speciﬁcally, we sample four drop-outs for SimCSE and design four cloze-style templates for Prompt-BERT ( Table 8). Our design of the template partially follows Jiang et al. (2022).","libVersion":"0.3.2","langs":""}
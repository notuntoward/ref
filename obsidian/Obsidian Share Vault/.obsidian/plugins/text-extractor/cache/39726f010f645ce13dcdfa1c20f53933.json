{"path":"lit/lit_notes_OLD_PARTIAL/Hooshmand19EnergyPredictiveModels.pdf","text":"Energy Predictive Models with Limited Data using Transfer Learning Ali Hooshmand ahooshmand@nec-labs.com NEC Laboratories America, CA Ratnesh Sharma ratnesh@nec-labs.com NEC Laboratories America, CA ABSTRACT In this paper, we consider the problem of developing predictive models with limited data for energy assets such as electricity loads, PV power generations, etc. We specifically investigate the cases where the amount of historical data is not sufficient to effectively train the prediction model. We first develop an energy predictive model based on convolutional neural network (CNN) which is well suited to capture the interaday, daily, and weekly cyclostationary patterns, trends and seasonalities in energy assets time series. A transfer learning strategy is then proposed to address the challenge of limited training data. We demonstrate our approach on a usecase of daily electricity demand forecasting. we show practicing the transfer learning strategy on the CNN model results in significant improvement to existing forecasting methods. CCS CONCEPTS • Computing methodologies → Machine learning; Neural net- works. KEYWORDS Energy predictive models, time series, transfer learning, convolu- tional neural network (CNN), daily electricity load forecasting. ACM Reference Format: Ali Hooshmand and Ratnesh Sharma. 2019. Energy Predictive Models with Limited Data using Transfer Learning. In e-Energy ’19: Proceedings of the Tenth ACM International Conference on Future Energy Systems, June 25–28, 2019, Phoenix, AZ, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/ 10.1145/3307772.3328284 1 INTRODUCTION Various prediction models are utilized in power and energy systems such as electricity load forecasting, PV prediction, wind generation estimation, etc. Applying machine learning techniques to develop prediction models is a well-established and active research field at- tracting considerable researchers motivated by the new advances in computational technologies and machine learning techniques. Cur- rent algorithms require to have enough historical data of the energy assets and customers for training models. In [15], a recurrent neural network is designed to predict 24 hour ahead PV power generation. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. e-Energy ’19, June 25–28, 2019, Phoenix, AZ, USA © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6671-7/19/06. . . $15.00 https://doi.org/10.1145/3307772.3328284 Provided by Japan Meteorological Business Support Center, two years of meteorogical data was utilized in their research to train the deep learning model. Authors in [14] proposed a hybrid day-ahead energy price forecasting model based on time-series and adaptive wavelet neural networks. Their model is applied to forecast the electricity price for PJM electricty market and used one year of historical recorded PJM data to be trained. In [10], a deep learning model was proposed for wind power generation estimation where three years of field data of seven wind farms in India were used to develop the model. Hence, similar to other use-cases of modern machine learning techniques, having access to large amount of data is crucial in training accurate models. There are, however, many ap- plications in energy systems for which large historical data might not be available. For example, new customers might have been added or new facilities or assets might have been installed. In such cases, lack of sufficient data results in underperforming machine learning models. This constitutes the main challenge of this re- search which is establishing predictive platforms in settings where limited data is available for customers. We propose to compensate for small historical data by transferring features from models that are pre-trained with distinct data of similar nature. In doing so, we first develop a a deep learning model based on convolutional neural networks (CNN) for energy time series data with daily seasonality. It is able to consider the trends and seasonality of energy data in its prediction algorithm. We chose the CNN model over Recurrent Neural Networks (RNN), since it can efficiently extract correla- tions in electricity power data at different time-scales. Several other papers have also reported case-studies where CNNs outperform RNNs for time-series data [4, 8, 9]. We then propose a transfer learning procedure for pre-training the model on historical data and fine-tuning it with limited data of new customers. Our experi- mental results show that the proposed method outperforms current techniques for load forecasting. Authors in [12] has also applied transfer learning to forecast building energy consumption. They leverage the consumption data of similar buildings, and augment it to limited data of target building for which the model is trained. The augmented data is then passed through the steps of Hephaes- tus (their proposed transfer learning method), namely Time Series Adaptation and Non-Temporal Domain Adaptation. Their strategy is implemented on the input data and then the processed data is used to train a machine learning model. However, in our proposed method the transfer learning strategy is implemented on the way we train the CNN model. The rest of this paper is organized as follows. Section 2 presents our design choice for time series prediction model based on convo- lutional neural nets. In section 3, we propose our transfer learning approach using the CNN model. In section 4, we implement our designed transfer learning technique on load forecasting problemarXiv:1906.02646v1 [cs.LG] 6 Jun 2019 e-Energy ’19, June 25–28, 2019, Phoenix, AZ, USA Ali Hooshmand and Ratnesh Sharma as a case study and provide its experimental results, and Section 5 concludes the paper. 2 PROPOSED CNN-BASED TIME SERIES PREDICTIVE MODEL In this section, we describe the prediction model goal, our approach of preprocessing energy time series data as input for predictive model, and provide the structure of CNN-based predictive model. 2.1 Goal The objective in designing the prediction model is to use the his- torical recorded energy data to forecast the next 24 hours energy profile as the model output. The model could be applied for en- ergy time-series predictions such as electrical load, PV genera- tion, and wind power production. The prediction should be accom- plished so as to minimize prediction error at each time instance. We choose the mean absolute error (MAE) as the training criterion: MAE = (1/N ) ÍN i=1 |xi − ˆxi | where xi is the actual value, ˆxi is the predicted value, and N is number of samples. 2.2 Energy Time Series Data Energy data, e.g., electricity, PV, wind, etc., are usually in the form of time-series with daily cyclic patterns. It means for most of the applications, data profiles such as electricity load, PV generation, and wind turbine generation repeat their pattern every 24 hours. An example of daily load consumption of a school facility for 7 days of a week is shown in Figure 1. The two flat load profiles represent the electricity usage of weekend days. Figure 1: A sample of load consumption for 7 days of a week 2.3 Model Inputs The main input of the model is past daily data profiles. We use data of past four weeks to forecast energy profile of the next day. It is assumed the past four weeks data provides enough information about interaday trends, and cyclo-stationary seasonality patterns with daily and weekly periods. For each day, we reshape data of past four weeks into a three dimensional matrix of size 24 (it should be stated before that data is sampled once every 60 min) by 7 (num- ber of days in a week) by 4 (number of weeks), to align data of consecutive hours, days and weeks next to each other. Note that the number of samples per day might vary based on the sampling time of collected data and the requirements of application the prediction profile is used for. Table 1: Structure of convolutional network. Layer Output Shape Input (96, 7, 4) Convolutional with 32 filters of size (3, 3) (96, 7, 32) Relu, Batch Norm, Maxpool (4,1) (24, 7, 32) Convolutional with 64 filters of size (3, 3) (24, 7, 64) Relu, Batch Norm, Maxpool (2,1) (12, 7, 64) Convolutional with 128 filters of size (3, 3) (12, 7, 128) Relu, Batch Norm, Maxpool (2,1) (6, 7, 128) Convolutional with 256 filters of size (3, 3) (6, 7, 256) Relu, Batch Norm, Maxpool (2,2) (3, 3, 256) Convolutional with 512 filters of size (3, 3) (3, 3, 512) Relu, Batch Norm, Maxpool (2,2) (1, 1, 512) Dense, Relu, Dropout(0.5) (1, 512) Linear(output) (1, 96) Figure 2: Proposed predictive model development flowchart for energy assets with limited historical data. 2.4 Model Structure The three dimensional matrix of historical data is formed in such a way that the hourly, daily and weekly correlations are aligned across different axises and, thus, a convolutional neural network is a suitable tool to extract the spatial correlations of such a data. The designed CNN model is inspired by VGG network which was developed for classifying ImageNet datatset [13] (the winner of Image Net Large Scale Visual Recognition Challange, ILSVRC in 2014 [7]) and composed of five convolutional, one dense layer with relu activation and one linear output layer. The model output is the day-ahead energy time series profile. Hence, the model loss function is defined as follows: Loss = Ín i=1 ÍT t =0(Pi (t) − ˆPi (t))2 Table 1 provides the details of designed CNN model. 3 TRANSFER LEARNING METHOD With sufficient historical energy data, the CNN model could be effectively trained to yield desirable accuracy. However, in many cases, we do not have access to large amount of data. To address this problem, we propose a transfer learning procedure through the steps which are described in this section. Transfer learning is Energy Predictive Models with Limited Data using Transfer Learning e-Energy ’19, June 25–28, 2019, Phoenix, AZ, USA a general concept in machine learning in which the knowledge learned from a dataset or a problem is transferred to a different but related problem for which it is expensive or impossible to collect enough training data [11]. Step 1: Public Datasets At the first step, we collect publicly available energy data for dif- ferent types of customer categories and energy assets. This data could be obtained from open source datasets. As an example, Open PV Project [2] is a collaboration between Department of Energy (DOE), industry, and public community to create a comprehensive database for storing available PV generation data in the United States. This project is established by NREL (National Renewable Energy Laboratory) [1]. Using the public datasets, we can train the CNN model for the type of energy asset under study. Step 2: Normalization One problem with using public datasets is that the scale of data could be significantly different from the target application. In order to correct the scaling differences, we normalize the public dataset so as to enable the model to efficiently learn the trends and seasonality of time-series profiles which are useful for development of target customer predictive model. Step 3: Pre-Training The designed CNN model is trained using the pre-processed data of public repository datasets. The pre-training step helps us to learn from public data the specific features that are common across energy assets with the same type as the target asset. The gained knowledge is transfered to our target prediction model in the next step. Step 4: Training The trained model in Step 3 does not yet result in the best expected performance for the target energy asset since it has not been trained with its specific data and scale. To improve the accuracy, we fine- tune the model on the available limited data of the target user. however, in order to preserve the features that are learned using the large public datasets, we freeze the convolutional layers and only retrain the last fully-connected layers. This approach customizes the model to new data while keeping the original features. Notably, since the number of parameters in last two layers is significantly smaller than the convolutional layers, the small data of target asset suffices to achieve a desirable performance. To this end, the limited customer data is randomly split into training and test sets. The train set is normalized and the normalization reference (the maximum value of training data) is recorded for the prediction step. After fine-tuning, the model is evaluated on test data of the target asset. Step 5: Prediction To perform forecasting, the test dataset is first normalized using the normalization reference recorded in training step. Then, it is passed to the trained CNN model for prediction. The model output is then reversly normalized (using the normalization reference) at the final stage which results in the forecasted profile for the next 24-hour generation or consumption of target energy asset. By pretraining the CNN model based on public related-domain dataset, we could train the weights of convolutional layers as well as initialize the weights of last fully connected layers. It lets us to Figure 3: Input and output for load forecast CNN model. use the limited data of target customer to fine tune the weights of last layer and achieve better performance. The proposed strategy is summarized in the flowchart shown in Figure 2. 4 CASE STUDY: ELECTRICITY LOAD FORECASTING Day-ahead load forecasting in power and energy systems refers to the prediction of future electrical demand for the next 24 hours [6]. The load forecasting (LF) model input is chosen to be the load data for the past four weeks and the model output is forecasted profile for the next 24 hours of load consumption as shown in Figure 3. We aim to develop the LF model for customers who have limited historical recorded data insufficient for training the CNN model. To prepare the model, we will follow the steps presented in section 3. We start by utilizing the publicly available electrical load data for different types of customers. Here, this data is obtained from the open source dataset in UCI repository [3]. The dataset contains the four years electricity consumption of about 370 clients. 0 50 100 150 200 250 300 350 400 epoch 0.03 0.04 0.05 0.06 0.07 0.08 0.09MAE [p.u.] Training Error Validation Error Figure 4: Training and validation mean absolute errors (MAE) for load prediction during 400 epochs of training. To eliminate the local features, each customer load in dataset is normalized based on it maximum value that projects the scale of all loads into the range of 0 to 1. The dataset is randomly split into two sets of training data (70%) and validation data (30%). The model is trained (pre-training step described in section 3) to minimize the sum of MAE of load forecast error. Figure 4 shows the training and validation errors for the load forecast during 400 epochs of training. The model with the best validation error will be saved for the training step explained in section 3. By pre-training step, the model learned the electrical load common features from normalized version of UCI public data set. The model retraining is performed using the available load data of the target customer for which the forecasting model is prepared. The selected customers have recorded data for about five months from which four months worth of data is used for retraining the saved model in previous step. The rest of data is kept as the test set for assessing the prediction accuracy of the final model. e-Energy ’19, June 25–28, 2019, Phoenix, AZ, USA Ali Hooshmand and Ratnesh Sharma Figure 5: Normalized averaged MAE (N.A.MAE) comparison for proposed transfer learning strategy (TFL), Pre-trained, Fresh, and SARIMA models for 23 C&I customers in different segments (normalized based on Pre-trained model averaged MAE value). Table 2: Percentage improvement of proposed transfer learn- ing approach with respect to each model. Total Normalized Averaged MAE (TNA.MAE) Pre-trained Model (TNA.MAE: 1) Fresh Model (TNA.MAE: 0.84) SARIMA Model (TNA.MAE: 0.87) Transfer Learning Method (TNA.MAE: 0.7) 30% 17% 20% To evaluate the performance of proposed transfer learning strat- egy on designed CNN model, we compare our proposed method with the following medels: • SARIMA model as a classic time-series prediction technique [5] which does not require a lot data to be trained due to its linear and simple structure. • Fresh CNN model in which the designed CNN model in section 2 is trained only using the target load data without using the public dataset and pre-training step. • Pre-Trained model in which the best model saved during pre- training step in section 3 is directly used for load forecasting of target customer without using its limited historical data. The proposed transfer learning strategy was implemented for 23 randomly selected commercial and industrail (C&I) customers with limited data in different segments such as grocery, school, manufac- turing, mall, banking, hospital, theater, etc. A sample of daily load data of these customers can be seen in Figure 6 of Appendix section. Please note the difference in load scale of different customers. For each customer, the averaged MAE of test days is calculated for each method separately. Finally daily MAEs are averaged over all test days. Since the load scale of selected customers varies, the MAE results are normalized based on the averaged MAE value of Pre- trianed model to better observe and compare the performance of different models. We call this value NA.MAE (normalized averaged MAE). Figure 5 illustrates the performance of the developed method together with the performance of SARIMA, Fresh model, and Pre- trained model. As it can be seen, the proposed transfer learning strategy outperforms other models for all C&I loads as it always has the lowest NA.MAE value for all examined C&I loads. The absolute value of errors can also be found in Table 3 in Appendix section. Note that the performance of Fresh model is not satisfactory. However, the results are consistently improved when the limited training data is used to fine-tune the model that is pretrained with public datasets (TFL orange squares in Figure 5). We also calculate the total normalized averaged MAE (TNA.MAE) for each model which is the average of NA.MAE values over all 23 load categories. Table 2 provides the summary of TNA.MAE results. For example, the TNA.MAE of SARIMA model is 0.87, mean- ing that the total error of SARIMA model for all test days and all 23 customers is 87% of Pre-trained model’s error. This value for our proposed method is 0.70. The second row of the table also shows the percentage improvement of our transfer learning ap- proach with respect to each model. It performed 30%, 17%, and 20% ((0.87 − 0.70)/0.87) better than Pre-trained, Fresh, and SARIMA models respectively. Better performance of our proposed model in comparison with the Fresh model (17%) proves common features learned and transferred from public dataset has helped our approach to improve prediction results. Also, better prediction outcome of our approach compared to Pre-trained model (30%) demonstrates the necessity of retraining the last dense layers to learn the local features and behavior of target customer limited load data. 5 CONCLUSION In this paper, we proposed a procedure for designing energy pre- dictive models in small-data regimes. To this end, we designed a convolutional network for time-series forecasting and used ideas from transfer learning to improve the accuracy. The proposed frame- work was implemented on the usecase of short-term electricity load forecasting in which the model predicts the next 24 hours of elec- tricity demand based on the energy consumption of past four weeks. The simulation studies were conducted for 23 C&I customers and showed our approach consistently results in lower error compared to SARIMA, Fresh CNN model, and Pre-trained CNN model. The proposed approach could help energy customers with limited data to develop well-trained predictive tools. Energy Predictive Models with Limited Data using Transfer Learning e-Energy ’19, June 25–28, 2019, Phoenix, AZ, USA REFERENCES [1] [n. d.]. NREL. Available at https://openpv.nrel.gov/index. [2] [n. d.]. The Open PV Project. Available at https://openpv.nrel.gov/index. [3] Arthur Asuncion and David Newman. 2007. UCI machine learning repository. Available at https://archive.ics.uci.edu/ml/index.php. [4] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271 (2018). [5] Peter J Brockwell, Richard A Davis, and Matthew V Calder. 2002. Introduction to time series and forecasting. Vol. 2. Springer. [6] George Gross and Francisco D Galiana. 1987. Short-term load forecasting. Proc. IEEE 75, 12 (1987), 1558–1573. [7] ILSVRC2014. [n. d.]. At http://www.image-net.org/challenges/LSVRC/2014/. [8] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188 (2014). [9] Honglak Lee, Peter Pham, Yan Largman, and Andrew Y Ng. 2009. Unsupervised feature learning for audio classification using convolutional deep belief networks. In Advances in neural information processing systems. 1096–1104. [10] M Carolin Mabel and E Fernandez. 2008. Analysis of wind power generation and prediction using ANN: A case study. Renewable energy 33, 5 (2008), 986–992. [11] Sinno Jialin Pan, Qiang Yang, et al. 2010. A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22, 10 (2010), 1345–1359. [12] Mauro Ribeiro, Katarina Grolinger, Hany F ElYamany, Wilson A Higashino, and Miriam AM Capretz. 2018. Transfer learning with seasonal and trend adjustment for cross-building energy forecasting. Energy and Buildings 165 (2018), 352–363. [13] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014). [14] Lei Wu and Mohammad Shahidehpour. 2010. A hybrid model for day-ahead price forecasting. IEEE Transactions on Power Systems 25, 3 (2010), 1519–1530. [15] Atsushi Yona, Tomonobu Senjyu, Ahmed Yousuf Saber, Toshihisa Funabashi, Hideomi Sekine, and Chul-Hwan Kim. 2008. Application of neural network to 24-hour-ahead generating power forecasting for PV system. In 2008 IEEE Power and Energy Society General Meeting-Conversion and Delivery of Electrical Energy in the 21st Century. IEEE, 1–6. APPENDIX Figure 6 illustrates daily load samples of four randomly selected customers from school, hospital, light industry, and banking seg- ments. The unit is in kW and load is sampled every 15 minutes (96 samples per day). 0 20 40 60 80 100 100 200 300 School 0 20 40 60 80 100 3500 4000 4500 5000 Hospital 0 20 40 60 80 100 100 200 300 Bank 0 20 40 60 80 100 20 30 40 50 Light Industry Figure 6: Daily load samples of C&I customers. The load unit is in kW and data is sampled every 15 minutes (96 samples per day). Table 3 shows the day-ahead load forecasting mean absolute error, averaged over all test days, for selected 23 C&I customers. Table 3: Averaged MAE [kW] of test days for 23 C&I cus- tomers for evaluated predictive models Predictive Models Load Name TFL Pre-trained Fresh SARIMA grocery_1 6.1 8.3 9.5 7.0 school_1 17.9 26.2 19.6 20.6 manufacturing_1 37.5 40.8 42.4 51.0 mall_1 31.3 48.4 36.1 34.4 banking_1 32.4 56.4 41.4 51.1 hospital_1 161.8 244.9 207.9 191.5 realestate 38.1 47.4 46.9 45.6 school_2 41.6 63.4 50.3 56.4 manufacturing_2 232.3 234.4 238.5 252.1 light industry_1 4.4 6.4 4.6 5.1 grocery_2 8.6 13.1 10.1 10.3 mall_2 279.3 383.1 346.0 522.8 business_1 36.0 47.6 40.8 38.3 grocery_3 5.5 9.9 8.8 6.4 school_3 21.4 34.0 27.8 29.9 mall_3 29.5 47.4 37 30.5 school_4 28.1 41.3 33.1 28.0 theater 17.4 23.9 18.3 18.3 corpoffice_office 58.0 78.9 64.7 63.9 foodsales 6.7 9.5 8.7 7.8 hospital_2 165.0 258.1 219.6 197.4 light industry_2 22.4 34.1 23.9 24.6 grocery_4 7.1 10.0 11.8 7.8","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/Sevilla22ComputeTrendsThree.pdf","text":"4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 1/17 Arti cl es Paper Compute Trends Across Three Eras of Machine Learning We’ve compiled a dataset of the training compute for over 120 machine learning models, highlighting novel trends and insights into the development of AI since 1952, and what to expect going forward.Compute Trends Across Three Eras of Machine Learning 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 2/17 Published Feb 16, 2022 Last updated May 02, 2022 Authors Jaime S evilla Lennart Heim A nson Ho Tamay B esiroglu Marius Hobbhahn P ablo Villalobos Resources 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 3/17 Paper Visualization Dataset Cite Contents Summary: We have collected a dataset and analysed key trends in the training compute of machine learning models since 1950. We identify three major eras of training compute - the pre-Deep Learning Era, the Deep Learning Era, and the Large-Scale Era. Furthermore, we find that the training compute has grown by a factor of 10 billion since 2010, with a doubling rate of around 5-6 months. See our recent paper, Compute Trends Across Three Eras of Machine Learning, for more details. Introduction It is well known that progress in machine learning (ML) is driven by three primary factors - algorithms, data, and compute. This makes intuitive sense - the development of algorithms like backpropagation Introduction Methodology Results Compute trends are slower than previously reported Three eras of machine learning Implications and further work 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 4/17 transformed the way that machine learning models were trained, leading to significantly improved efficiency compared to previous optimisation techniques (Goodfellow et al., 2016; Rumelhart et al., 1986). Data has been becoming increasingly available, particularly with the advent of “big data” in recent years. At the same time, progress in computing hardware has been rapid, with increasingly powerful and specialised AI hardware (Heim, 2021; Khan and Mann, 2020). What is less obvious is the relative importance of these factors, and what this implies for the future of AI. Kaplan et al. (2020) studied these developments through the lens of scaling laws, identifying three key variables: Trying to understand the relative importance of these is challenging because our theoretical understanding of them is insufficient - instead, we need to take large quantities of data and analyse the resulting trends. Previously, we looked at trends in parameter counts of ML models - in this paper, we try to understand how training compute has evolved over time. Amodei and Hernandez (2018) laid the groundwork for this, finding a increase in training compute from 2012 to 2018, doubling every 3.4 months. However, this investigation had only around datapoints, and does not include some of the most impressive recent ML models like GPT-3 (Brown, 2020). Motivated by these problems, we have curated the largest ever dataset containing the training compute of machine learning models, with over 120 datapoints. Using this data, we have drawn several novel insights into the significance of compute as an input to ML models. Number of parameters of a machine learning model Training dataset size Compute required for the final training run of a machine learning model (henceforth referred to as training compute) • • • 300, 000× 15 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 5/17 These findings have implications for the future of AI development, and how governments should orient themselves to compute governance and a future with powerful AI systems. Methodology Following the approach taken by OpenAI (Amodei and Hernandez, 2018), we use two main approaches to determining the training compute of ML systems: \u0000. Counting the number of operations: The training compute can be determined from the number of arithmetic operations that is performed by the machine. By looking at the model architecture and closely monitoring the training process, we can directly calculate the total number of multiplications and additions, yielding the training compute. As ML models become significantly more complex (as continues to be the case), this approach becomes significantly more tedious and tricky. Doing this also requires knowledge of key details of the training process , which may not always be accessible. \u0000. GPU-time: A second approach, which is independent of the model architecture, uses the information about the total training time and hardware to estimate the training compute. This method typically requires making several assumptions about the training process, which leads to a greater uncertainty in the final value. Most of the time, we were able to use either of the above approaches to estimate the training compute for a particular ML model. In practice this process involved many difficulties, since authors would often not publish key information about the hardware used or training time. Of course, it would be infeasible for us to gather this data for all ML systems since 1950. Instead, we focus on milestone systems, based on the following criteria: 1 2 3 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 6/17 This selection process lets us focus on the most important systems, helping us understand the key drivers of the state-of-the-art. Results Using these techniques, we yielded a dataset with training compute for over 120 milestone ML systems, the largest such dataset yet. We have chosen to make this and our interactive data visualisation publicly available, in order to facilitate further research along the same lines. Clear importance: These are systems that have major historical influence, significantly improve on the state-of-the-art, or have over 1000 citations Relevance: We only include papers which include experimental results and a key machine learning component, and have a goal of pushing the existing state-of-the-art Uniqueness: If another paper describing the same system is more influential, then the paper is excluded from our dataset • 4 • • Training Compute of Notable machine learning Systems Over Time Training compute (FLOP) Large Scale Other 333 results Show options EPOCH Menu 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 7/17 NOTE: This visualization is dynamically updated as we collect further information on notable ML systems. As a result, the showcased trends differ from the time of our original publication. When analysing the gathered data, we draw two main conclusions. Compute trends are slower than previously reported Cite Dataset Documentation License: CC-BYDownloads Trends in training compute are slower than previously reported We identify three eras of training compute usage across machine learning • • 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 8/17 In the previous investigation by Amodei and Hernandez (2018), the authors found that the training compute used was growing extremely rapidly - doubling every 3.4 months. With approximately 10 times more data than the original study, we find a doubling time closer to 6 months. This is still extraordinarily fast! Since 2010, the amount of training compute for machine learning models has grown by a factor of 10 billion, significantly exceeding a naive extrapolation of Moore’s Law. This suggests that many previous analyses based on OpenAI’s paper were biased towards rapid developments, approximately by a factor of two. Three eras of machine learning One of the more speculative contributions of our paper is that we argue for the presence of three eras of machine learning. This is in contrast to prior work, which identifies two trends separated by the start of the Deep Learning revolution (Amodei and Hernandez, 2018). Instead, we split the history of ML compute into three eras: \u0000. The Pre-Deep Learning Era: Prior to Deep Learning, training compute approximately follows Moore’s Law, with a doubling time of approximately every 20 months. \u0000. The Deep Learning Era: This starts somewhere between 2010 and 2012 , and displays a doubling time of approximately 6 months. \u0000. The Large-Scale Era: Arguably, a separate trend of of models breaks off the main trend between 2015 and 2016. These systems are characteristic in that they are run by large corporations, and use training compute 2-3 orders of magnitude larger than systems that follow the Deep Learning Era trend in the same year. Interestingly, the growth of compute in these Large-Scale models seems slower, with a doubling time of about 10 months. A key benefit of this framing is that it helps make sense of developments over the last two decades of ML research. Deep Learning marked a major paradigm shift in ML, with an increased focus on training 5 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 9/17 larger models, using larger datasets, and using more compute. The bifurcation of the Deep Learning trend coincides with the shift in focus towards major projects at large corporations, such as DeepMind and OpenAI. However, there is a fair bit of ambiguity with this framing. For instance, how do we know exactly which models can be considered large-scale? How can we be sure that this “large-scale” trend isn’t just due to noise? To test these questions, we used different statistical thresholds for what counts as “large-scale”, and the resulting trend does not change very much, thus the findings are at least somewhat robust to different selection criteria. Of course, the exact threshold that we use is still debatable, and it is hard to be certain about the observed trends without more data. Implications and further work We expect that future work will build upon this research project. Using the aforementioned compute estimation techniques, more training compute data can be gathered, offering the potential for more conclusive analyses. We can also make the data gathering process easier, such as by: Taking these steps helps key actors obtain valuable information in the future. Naturally, we will also be looking at trends in dataset sizes, and comparing the relative importance of data and compute for increased performance. We can also look how factors like funding and talent influence the primary inputs of a ML system, like data and compute. Developing tools for automatically measuring training compute usage (as well as inference compute) Publishing key details about the training process, such as the GPU model used • • 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 10/17 Answering questions like these is crucial for understanding how the future of AI will look like. At Epoch, we’re particularly concerned about ensuring that AI is developed in a beneficial way, with appropriate governance intervention to ensure safety. Better understanding the progress of compute capabilities can help us better navigate a future with powerful AI systems. Read the full paper now on the arXiv. \u0000. Specifically, we focus on the final training run of a ML system. This is primarily due to measurability - researchers generally do not mention the total compute or training time that does not directly contribute to the final machine learning model. We simply do not have sufficient information to determine the total compute through the entire experimentation process. ↩ \u0000. i.e. the total number of iterations during training. ↩ \u0000. For instance, we often assumed that the GPU utilisation rate was 10%. We often also had to guess which GPU model was in use based on the year in which the paper was published, in the event that this information was not disclosed in the paper of interest and the authors were not able to provide an answer. ↩ \u0000. These criteria are ambiguous and can vary on a case-by-case basis. For instance, new papers (published within the last year or two) can be very influential without having had the time to gather many citations. In such cases we make relatively subjective decisions of the importance of these ML models. ↩ \u0000. We discuss this more in Appendix D of the paper. While AlexNet (Krizhevsky et al., 2012) is commonly associated with the start of Deep Learning, we argue that models before AlexNet have 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 11/17 key features commonly associated with Deep Learning, and that 2010 is most consistent with evidence. ↩ About the authors Jaime Sevilla is the director of Epoch. His research is focused on technological forecasting and the trajectory of AI. He has a background on Mathematics and Computer Science. Lennart Heim supports Epoch’s research and strategy. He extracts the strategy and policy implications of Epoch’s investigation for the AI governance community. In his primary role, Lennart is a researcher at the Centre for the Governance of AI in Oxford, focusing on Compute Governance. His research interests include the role of compute in the production of AI, the compute supply chain, forecasting emerging technologies, and the security of AI systems. His time is split into the strategic and technical analysis of the compute governance domain and active policy engagement with GovAI’s policy team. Anson Ho is a researcher at Epoch. He is interested in helping develop a more rigorous understanding of future developments in AI and its societal impacts. Prior to this, he completed his BSc in physics at the University of St Andrews. Tamay Besiroglu is the associate director at Epoch. His work focuses on the economics of computing and big-picture trends in machine learning. Previously, he was a researcher at the Future Tech Lab at MIT, led strategy for Metaculus, consulted for the UK Government, and worked at the Future of Humanity Institute. Former employee Marius Hobbhahn builds models for AI timelines and takeoff using historical trends and his best understanding of the future. 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 12/17 Pablo Villalobos has a background in Mathematics and Computer Science. After spending some time as a software engineer, he decided to pivot towards AI. His interests include the economic consequences of advanced AI systems and the role of algorithmic improvements in AI progress. Share Twitter LinkedIn Tags Trends Compute Related posts 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 13/17 REPORT · 27 MI N READ Trading Off Compute in Training and Inference Some techniques allow to increase the performance of machine learning models at the cost of more expensive inference, or reduce inference compute at the cost of lower performance. This possibility induces a tradeoff between spending more resources on training or on inference. We explore the characteristics of this tradeoff and outline some implications for AI governance. Jul 28, 2023 · By Pablo Villalobos and David Atkinson 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 14/17 PAPER · 3 MI N READ Who Is Leading in AI? An Analysis of Industry AI Research The private sector has emerged as a driving force in artificial intelligence, fueled by an explosion of investment in hardware and talent. But which companies are steering the field? Our new article compares leading AI companies by research publications, citations, size of training runs, and contributions to key algorithmic innovations. In this blog post, we summarize the key findings as well as some policy implications. Nov 27, 2023 · By Ben Cottier, Tamay Besiroglu and David Owen 4/15/24, 9:54 P M Compute Trends A cross Three Eras of Machine Learning – Epoch https://epochai.org/blog/compute-trends 15/17 Excited about our work? REPORT · 10 MI N READ The Limited Benefit of Recycling Foundation Models I investigate the benefits of recycling old foundation models to save training costs on large training runs, finding that it seems unlikely that model recycling will result in more than a modest increase in AI capabilities. Jul 07, 2023 · By Matthew Barnett","libVersion":"0.3.2","langs":""}
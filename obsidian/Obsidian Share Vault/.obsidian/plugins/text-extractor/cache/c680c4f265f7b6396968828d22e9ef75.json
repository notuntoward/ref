{"path":"lit/lit_sources.backup/Posocco21EstimatingExpectedCalibration.pdf","text":"Estimating Expected Calibration Errors Nicolas Posocco1 and Antoine Bonnefoy 1 EURA NOVA, Marseille, France firstname.lastname@euranova.eu Abstract. Uncertainty in probabilistic classiﬁers predictions is a key concern when models are used to support human decision making, in broader probabilistic pipelines or when sensitive automatic decisions have to be taken. Studies have shown that most models are not intrinsi- cally well calibrated, meaning that their decision scores are not consistent with posterior probabilities. Hence being able to calibrate these models, or enforce calibration while learning them, has regained interest in recent literature. In this context, properly assessing calibration is paramount to quantify new contributions tackling calibration. However, there is room for improvement for commonly used metrics and evaluation of calibra- tion could beneﬁt from deeper analyses. Thus this paper focuses on the empirical evaluation of calibration metrics in the context of classiﬁca- tion. More speciﬁcally it evaluates diﬀerent estimators of the Expected Calibration Error (ECE), amongst which legacy estimators and some novel ones, proposed in this paper. We build an empirical procedure to quantify the quality of these ECE estimators, and use it to decide which estimator should be used in practice for diﬀerent settings. Keywords: Uncertainty · Calibration · Reliability · Classiﬁcation 1 Introduction Almost all currently used classiﬁers are not intrinsically well-calibrated [11], which means their output scores can’t be interpreted as probabilities. This is an issue when the model is used for decision making, as a component in a more general probabilistic pipeline, or simply when one needs a quantiﬁcation of the uncertainty in model’s predictions, for example in high risk applications. To overcome this calibration issue, two main tracks have been explored by either correcting the calibration of the model via some post-training procedure [13,11,8,7] or by regularizing the model to enforce calibration during training [9]. Would it be for the quantitative comparison of the performances of calibra- tion methods or the evaluation of prediction’s uncertainty, one needs to precisely quantify calibration. The recent literature trend is to use estimators of the Ex- pected Calibration Error (ECE) [10], which we focus on in this work. We propose a few improvements on current ECE estimators as well as a novel approach for the estimation of this metric based on kernel density esti- mation. We also introduce via these new estimators a continuous equivalent of the reliability diagram constructed on the proposed notion of Local CalibrationarXiv:2109.03480v1 [cs.LG] 8 Sep 2021 2 N. Posocco et al. Error (LCE). This notion can be used in practice to evaluate the uncertainty of the predicted probabilities itself, with an optional uncertainty interval. Further- more we designed the ﬁrst experimental setup to enable the assessment of the calibration metrics, in order to identify which estimators are the most relevant. In this paper, we ﬁrst present the context of this study in Section 2 and set up the formal deﬁnition of calibration in Section 3. The theoretical calibration metric, namely the Expected Calibration Error, and its legacy and newly pro- posed estimators, are presented in Section 4, where we also introduce the concept of Local Calibration Error. We ﬁnally assess in Section 5 the relevance of legacy and proposed estimators empirically using a broad empirical setup.1 2 Context and Related Work The oldest attempt to quantify calibration has been the reliability diagram [3,11] for binary classiﬁcation. Although it has been useful for the evaluation of early calibration methods, it does not provide point estimates - a single value - re- quired to systematically compare calibration of diﬀerent models. The ﬁrst point estimate proposed in [16], which exploited a decision theory framework to use a proﬁt maximisation as a proxy for calibration quality, required a speciﬁc type of dataset to be usable in practice. Mirroring the procedure used to compute the reliability diagram, the empirical Expected Calibration Error (ECE) was designed [3], and later has been proven to be an estimator for the natural the- oretical notion of calibration error [4]. Meanwhile, some works have used the negative log-likelihood (NLL) or the Brier score [16], which both are weak proxis for the calibration of classiﬁers [6]. Using reliability diagrams has become even more diﬃcult in multiclass settings [17]. Recent works mostly rely on the binning based legacy estimator of the ECE to quantify calibration. Defects have been highlighted with this estimator, such as its reliance on a hyperparameter and its bias variance trade-oﬀ [12]. More recently [7] made clearer the notion of calibration for multiclass classiﬁers, and new estimators of the ECE with adaptive binning have been proposed in [12] along side with uncertainty aware reliability diagrams [1]. Although the notion of calibration was originally deﬁned for classiﬁers, this notion is currently being generalized to regression [5,15]. In this context we aim at improving the evaluation of calibration in the setting of classiﬁcation, and speciﬁcally focus on estimators of the ECE as the theoretical deﬁnition itself has been consistently adopted. Deﬁnition 1. Classiﬁer Let us consider the random variable (X, Y ), from which are drawn i.i.d sam- ples to build a training set, and a holdout set of size N : (x, y) ∈ X × [1..C]. A classiﬁer M : X → ∆C is a function learnt from the training set which outputs scores -ideally the probabilities P(Y |X)- of belonging to class c for c ∈ [1..C], 1 The code ensuring the reproducibility of the experiments presented in this work is available at https://github.com/euranova/estimating_eces Estimating Expected Calibration Errors 3 where ∆C ≜ {s ∈ [0, 1]C| ∑C j=1 sj = 1} is the probability simplex that ensures the scores sum up to one. In the rest of the paper the indexed notation sc repre- sents the cth element of any vector s and x i denotes the ith sample of the holdout set. For readability purpose we use the notation si for the output score M (x i). 3 Calibration In this section we present and formalize properly the 4 diﬀerent notions of cali- bration, and derive the corresponding Expected Calibration Errors (ECE). Calibration characterizes how much a model is able to output scores cor- responding to actual posterior probabilities. The ﬁrst and simplest calibration notion [13] is focused on a speciﬁc class and extends to the simultaneous calibra- tion of every classes considering their associated scores independent, namely the class-wise calibration [17]. This version considers a classiﬁer is well-calibrated if all one-vs-rest submodels are calibrated. The calibration concept for binary classiﬁcation is equivalent to class-speciﬁc calibration focusing on the positive class and to class-wise calibration, since the score for the negative class s0 is determined by the score for the positive class s1 = 1 − s0. The more recently introduced conﬁdence calibration [4] is only concerned about the model predict- ing relevant scores for the class it predicts for each sample. Throughout this paper, we only tackle the conﬁdence and class-wise settings. Finally the most rigorous evaluation of calibration should actually take into account all classes as non-independent, the corresponding deﬁnition, the multiclass-calibration [13] is almost never used in practice for computability reasons. All these notions are formalized in the following deﬁnition. Deﬁnition 2. Diﬀerent calibration notions of a probabilistic classiﬁer. A prob- abilistic classiﬁer M , is Calibrated for class c: ∀s ∈ [0, 1], P(Y = c|M (X)c = s) = s Class-wise calibrated: ∀c ∈ [1..C], ∀s ∈ [0, 1], P(Y = c|M (X)c = s) = s Conﬁdence-calibrated: ∀s ∈ [0, 1], P(Y = argmax c∈[1..C] (M(X)c)| max c∈[1..C] (M(X)c) = s) = s Multiclass-calibrated: ∀s ∈ ∆C, ∀c ∈ [1..C], P(Y = c|M (X) = s) = sc The Expected Calibration Error (ECE) of a given model M can be naturally derived from these theoretical formulations by computing the expected deviation from the perfect theoretical calibration. This concept is applied to the diﬀerent calibration settings and results in the following formulations: 4 N. Posocco et al. Deﬁnition 3. Expected calibration error (ECE) for the diﬀerent settings for a given model M on (X, Y ): ECEc(M ) ≜ Es∼M (X)c[∣ ∣P(Y = c | M (X)c = s) − s ∣ ∣] ECEcw(M ) ≜ 1 C ∑ c∈[1..C] ECEc(M ) ECEconf (M ) ≜ Es∼max(M (X))[∣ ∣P(Y = argmax(M (X)) | max(M (X)) = s) − s ∣ ∣] ECEmul(M ) ≜ E(s,c)∼(M (X),Y )[∣ ∣P(Y = c|M (X) = s) − sc∣ ∣] Where ECEc(M ) is the class-speciﬁc ECE associated to class c, ECEcw the class-wise ECE [17], ECEconf (M ) the conﬁdence ECE [4] and ECEmul(M ) the multiclass ECE. By replacing the expectation over the absolute values of the diﬀerences by a simple maximum over the absolute diﬀerences, we obtain the formulations of the Maximum Calibration Error (M CE) [10], which focus on the highest gap between posterior probabilities and the scores given by the model. 4 Estimation of calibration quantiﬁcation In this section we describe the challenges of calibration quantiﬁcation, then present the existing tools to handle these challenges namely the reliability di- agram and the legacy ECE estimator. We then introduce a new formalization of these estimators based on binning and sample mapping, which help us deﬁne new binning based estimators. Finally we present the new notion of Local Cali- bration Error on which we rely to build continuous estimators of the ECE based on Kernel Density Estimation. All estimators are written for the class-speciﬁc calibration setting, which can then be transposed to the other settings using Deﬁnition 3. 4.1 Challenges of such quantiﬁcation Quantifying calibration is challenging in practice for two main reasons: Calibra- tion is intrinsically a local notion. Miscalibration is deﬁned on the neighbourhood of a given output score. Thus any global quantiﬁcation of calibration depends on an aggregation procedure of local measures. This is what diﬀerentiates the ECE, which implicitly weights all parts of the score distribution according to its local density, from the M CE, which only cares about the worst case scenario. Since calibration depends on score distributions , any relevant estimator relies on these scores, which means that we are limited by the amount of available validation data to perform such quantiﬁcation. A good calibration metric should speciﬁcally quantify calibration: contrary to the Brier score and the NLL, which values only carry a partial information on Estimating Expected Calibration Errors 5 calibration, we expect a good metric to be independent of confusion factors. It should then be theoretically well-funded as well as tractable in practice. Finally, a good calibration metric should be able to take into account cost matrices for the classiﬁcation task, when available, risk management being intrinsically linked to such cost matrices. The ECE corresponds to the identiﬁed required properties for homogeneous cost matrices, since it directly derives from the theoretical notion of calibration and has an immediate interpretation. However, it doesn’t allow heterogeneous costs matrices, and as we will see in the next sections, current estimators provide poor estimations of the true value of the ECE. For these reasons we focus on the setting of homogeneous cost classiﬁcation, and try to provide better estima- tors for the ECE. Such estimators should be robust to hyperparameter choice, problem which can be solved by the use of a relevant heuristic. The estimator should be data-eﬃcient too, in order to provide good estimates with a low vari- ance even with few holdout labeled data points. Such estimation should provide low-bias estimates with a suﬃcient amount of available data and should ﬁnally be consistent and computable in a reasonable amount of time. 4.2 Reliability Diagram The reliability diagram introduces the classical way of calculating the ECE. To build the reliability diagram (in the binary setting), a uniform binning scheme (the [0, 1] interval is split into equal bins) is used, and each holdout sample is mapped into a bin based on the score given by the model for the positive class (procedure deﬁned below as 1-bin mapping). For each bin, the average score for the positive class and the proportion of samples belonging to the positive class are calculated. The ﬁrst is then plotted against the second. If the model is well calibrated, each point should fall on the line y = x. The local oﬀset of each point tells us if the model is locally over or under-conﬁdent on its scores for the positive class. Such diagram can be seen on Figure 1 (left). Originally designed for the binary classiﬁcation case, it can be easily extended to conﬁdence calibration in the multiclass setting. In that case, samples are sent into bins based on the score the model outputs for the class it predicts, and the ratio of correct predictions is plotted against the average over the scores given for the predicted class. 4.3 Binning based estimators In order to present diﬀerent binning-based estimators of the ECE, we formalize the binning and aﬀectation mapping objects. We note s i the score of the class of interest of the ith sample, which depends on if we consider the speciﬁc-class, class-wise (ﬁxed class) or conﬁdence (predicted class) calibration. Deﬁnition 4. Binning schemes The [0, 1] segment is split into B bins used to assign each data point to one (or more) bin. These bins are deﬁned by their respective thresholds. Hence to 6 N. Posocco et al. deﬁne a binning scheme one only needs to specify the increasing splitting function t : [1..B] → [0, 1] that computes the right threshold for each bin. Two main binning schemes have been used to compute the ECE in the literature: Uniform binning splits the segment into B bins of equal size : t(i) = i B and Adaptive binning splits the segment so that each split contains the same number of samples : t(i) = {s σ(j) | ∀j ∈ [1..N − 1], s σ(j) ≤ s σ(j+1)}⌊N/i⌋, σ being the permutation which sorts samples based on the score predicted for the class of interest. Deﬁnition 5. Aﬀectation mapping Given a binning of a domain [0, 1], an aﬀectation mapping of D in these bins is a matrix W composed of positive weights, so that Wij is the weight of the aﬀectation of the sample i in the bin bj. Rows of such matrix sum up to 1. Using this formalisation, we start from the 1-bin mapping W 1bin for which ev- ery sample is assigned to a single bin with unit weight, to go to the new proposed convex mapping W conv for which each sample may contribute to up to two bins for the computation of the binning based ECE estimators. This mechanism is the one referred to as linear binning in the kernel density estimation ﬁeld. These two mappings can be respectively mathematically written, as follows, where cj is the geometric centre of the jth bin tj−1−tj 2 : W 1bin ij = {1 if si ∈ [tj−1, tj] 0 otherwise ; W conv ij =    1 if si ∈ [0, c0] & j = 0 1 if si ∈ [cB, 1] & j = B si−cj cj+1−cj if si ∈ [cj, cj+1] 1 − si−cj−1 cj −cj−1 if si ∈ [cj−1, cj] The original estimator of the ECE is basically a weighted mean over the absolute diﬀerences calculated when the reliability diagram is computed (here expressed in the speciﬁc-class case). If W is a 1-bin mapping on a uniform binning and 1 is the indicator function, the legacy estimator is: ECEc l = 1 N B∑ j=1 ∣ ∣ ∣ N∑ i=1 Wij(1Y i=c − s i c) ∣ ∣ ∣ (1) Such estimator can be deﬁned in the same way for ECEconf and ECEcw. We unify binning-based estimators under equation (1) with diﬀerent bin- ning/mapping schemes. The ECEa uses an adaptive binning with 1-bin map- ping, while the ECEc uses a uniform binning and a convex mapping, and ﬁnally the ECEac uses both improvements on the legacy estimator - adaptive binning and convex mapping. In the case of class-wise calibration, the ACE deﬁned in [12] is equivalent to ECEa, when all bins contain the same amount of samples. Estimating Expected Calibration Errors 7 4.4 Local Calibration Error We deﬁne the notion of Local Calibration Error (LCE), and then use it to build the reliability curve, a continuous version of the reliability diagram. Let us ﬁrst begin with the formal deﬁnition of the LCE: Deﬁnition 6. Local calibration error (LCE) for the class-speciﬁc and the con- ﬁdence settings for a given model M on (X, Y ) LCEc M (s) ≜ P(Y = c | M (X)c = s) − s LCEconf M (s) ≜ P(Y = argmax(M (X)) | max(M (X)) = s) − s For the class-speciﬁc case, to estimate the LCE of a model for all scores s ∈ [0, 1], we have to estimate P(Y = c|M (X)c = s). We resort to the Bayes rule to tear down this estimation to estimating the densities of P(M (X)c = s|Y = c) and P(M (X)c = s), and the scalar P(Y = c). We can then rely on kernel density estimation (KDE) to estimate the two densities. Theoretically, this approach is continuous. In our implementation however, both KDEs are evaluated numerically in Fourier space (the ﬁrst one on all scores for the class c and the second one on all scores for the class c when the ground truth is the class c), which makes the computation eﬃcient with O(N + nlog(n)) complexity, if n is the number of numeric subdivisions of the domain [0, 1]. We use steps of 0.0003 for precision, and mirrored the data around s = 0 and s = 1, which are the limits of the domain. This mirroring implies a slight bias in estimations due to a leak of density mass. Once again the LCEconf can be estimated in the same way with the relevant scores and classes. A continuous equivalent of the reliability diagram can be derived from such object. The reliability curve associated with the classiﬁer M and the class c, for the class-speciﬁc calibration is: ∀ sc ∈ [0, 1], relM (sc) = LCEc M (sc) + sc (2) An example of such reliability curve is shown in Figure 1 (middle). The main beneﬁt this proposed notion of local calibration error oﬀers is its usability in practice to know the uncertainty of a model on a speciﬁc score, which cannot be evaluated with enough precision using previous tools (points in a reliability diagram can be used for an interpolation aiming at the same result, yet the precision of such procedure is very low, and interpolation at that scale is questionable). We propose to compute this curve on bootstrapped versions of the holdout set, in order to quantify the uncertainty on this LCE. In this context, the median curve is considered as the reliability curve and percentiles of interest are used for uncertainty quantiﬁcation. This idea, illustrated on Figure 1 (right), allows the prediction of conﬁdence intervals for the class probabilities instead of point estimates, by only looking at the uncertainty on the bootstrapped reliability curve at the score output by the model. 8 N. Posocco et al. Fig. 1. Reliability diagram with 15 bins (left), reliability curve with a bandwidth of 0.03 (middle) and the bootstrapped version with the same bandwidth (right). Each plot brings one more level of insight. 4.5 Density based estimator: ECEd Based on the deﬁnition of this Local Calibration Error we can derive a new ECE estimator, which is formalized as follows: ECEc d = ∫ 1 0 fM (X)c(s) |LCEM (s)| ds, where fM (X)c is the probability density function of the scores given by the model for class c. Heuristics for hyperparameter choices For all binning-based estimators, we investigate the use of a simple heuristic to select the number of bins used for the estimations: the bin amount is the square root of the number of samples. For the kde-based approach, we propose to use Silverman’s rule [14] to select the bandwidth (the bandwidth is estimated on P(M (X)c = s), and the same bandwidth is used to estimate the density of P(M (X)c = s|Y = c)). Other heuristics are often used for KDE computations, yet Silverman’s rule is to our knowledge the only one which provides satisfying results in small data contexts, for which legacy estimators struggle the most. From class-speciﬁc to the other settings To translate the class-speciﬁc esti- mators into the class-wise case, class-speciﬁc ECEs are estimated for all classes, and the class-wise ECE is the mean of these values. To get to the conﬁdence case, scores for the class of interest are replaced by the score for the predicted class, and the class of interest is the ground truth label. 5 Experimental setup We present the assessment of a few empirical properties of the diﬀerent ECE estimators. As pointed out in [12], the main diﬃculty with empirical evaluation Estimating Expected Calibration Errors 9 of calibration methods and calibration metrics is that we don’t have access to ground truths in general. This is why we worked on a setup which gives us access to arbitrarily precise estimates of the ECE considered as a the ground truth, in the class-wise and conﬁdence settings. 5.1 Procedure We aim at quantitatively compare the estimators in terms of approximation, data eﬃciency and variance. To do so we build curves which can indicate the expected performance of each estimator with its corresponding parameters, for diﬀerent sizes of holdout set. In order to observe statistically robust result we introduce various degrees of variability in our experiment at distribution level, in the algorithm used to train the models, and in terms of train/holdout sets splits. The results are thus produced based on numerous realistic output score distributions. The distribution variability is introduced by creating synthetic sample sets from Gaussian mixtures, where each class is composed of 4 modes of the mix- ture. For each mode we build the mean vector with elements uniformly drawn in [0, 1], and the covariance matrix is built as follows: we ﬁrst sample a matrix with elements uniformly drawn in [−0.3, 0.3] then multiply it with its transpo- sition to get the required positive deﬁnite matrix. This sample set generation is produced with various number of classes (2, 5, 7) and dimensions of the feature space (2, 5, 7) with 5 diﬀerent large datasets sampled from each combination, resulting into 45 synthetic distributions. In order to produce various relevant score distributions from these data dis- tributions, we trained 4 diﬀerent types of models (logistic regression, gaussian na¨ıve bayes classiﬁer, support vector classiﬁer and random forest) on 3 train sets of size 300 sampled out from the previously generated large datasets. For each of these trained models we compute the ”ground truth” ECE using the legacy estimator with high granularity (2000 bins) on the remaining holdout set (2.106 samples). Then, we build 200 evaluation sets which are bootstrapped versions of the holdout set of sizes taken between 30 and 500 on a logarithmic scale. The ”ground truth” ECE is used as reference to compute the approximation error (the absolute value of the diﬀerence between the estimated ECE and its true value normalized by the ground truth). Among those 200 values per evaluation set size, we keep the 95 th percentile of the approximation errors, below which 95% of such errors rely. For each evaluation set size and estimator, we ﬁnally plot the median over the 540 95 th percentiles obtained with each score distribu- tions. The resulting curves can be seen in Figure 2. The number of evaluations of the learning algorithms plus the ECE estimators makes this experiment long to run, but as all the estimators have limited computation complexity the overall computation remains feasible. 10 N. Posocco et al. Fig. 2. Median 95th percentile of the approximation error (absolute value of the nor- malized relative deviation with respect to the ground truth ECE) for the diﬀerent estimators of the ECEconf (top) and the ECEcw (bottom) (lower is better) for eval- uation sets of size between 30 and 500 samples. The scale is logarithmic for both axis 5.2 Results analysis For all settings, the error of all estimators is very high for small data regimes (the estimation error is around 10% of the true value), and thus one shouldn’t evaluate calibration on so little data, no matter what estimator is used. For the conﬁdence setting, the best performing estimator in almost all data regimes is the ECEconf d with Silverman’s rule. This is good news, since we now have a procedure to estimate the ECE which doesn’t rely on a sensitive hyper- parameter choice, but instead on a simple heuristic. As far as it is concerned, the convex mapping scheme empirically improves the performance of the legacy estimator and the one using the adaptive binning, which underperforms when alone, probably because of the increased variance induced by the adaptivity. It is worth noting that above 300 samples, a lot of the estimators show similar performances. As far as the square root heuristic is concerned for the automatic choice of number of bins used, the graphs suggest that the number of bins grows slightly too fast in average with an increasing amount of samples. Estimating Expected Calibration Errors 11 For the class-wise setting, there is no clear outperformer in all data regimes among the tested estimators. For less than 100 samples, the ECEcw ac with the square root heuristic seems to be the best choice. The same estimator, this time with a ﬁxed small number of bins, is then the most precise one. The observation made earlier about the square root heuristic still holds, and Silverman’s heuristic for the bandwidth seems to be a less relevant choice in the class-wise setting than in the conﬁdence one. We assume it is the case because of the sharpness of the score distributions for each classes in the class-wise setting (most of the density being very close to 0 and 1), which is a context in which Silverman’s bandwidth is known to underperform for kernel density estimation. 6 Conclusions We have introduced a few improvements on the legacy estimators, from the proposition of new binning schemes to the use of heuristics to automatically pick relevant values for hyperparameters of estimators of the ECE. On top of this, a novel approaches has been built to deﬁne properly the notion of local calibration error, which produces novel estimators for the ECEs. By testing all approaches on a synthetic experimental setup for which we had access to very precise estimates of the theoretical ECE, we have been able to compare all candidate estimators. This systematic evaluation, which had never been done until now, allowed us to formulate some recommendations on which estimator to use in what context. Our proposed solutions lead to natural potential future works. First, the in- troduced calibration curve suggests a natural post-training calibration method, since it can be seen as a calibration map. Such method would be interesting to evaluate, yet poses the problem that the associated calibration maps are not monotonous, which is considered as a prerequisite for post-hoc calibration pro- cedures in the literature. Then multiclass-calibration evaluation, which is still an open problem today, could potentially be evaluated in the scores space us- ing an adapted variant of our kde approach, which we think wouldn’t suﬀer as much as legacy estimators from the increase of dimensionality. Finally, even if this paper uses classical kernels and a mirroring approach to constrain density estimations on the domain [0, 1] which allows standard and fast KDE computa- tion, some preliminary investigations using a beta pseudo-kernel (the second one introduced in [2]) which is naturally constrained to this domain, show promising results. Because this kernel has a diﬀerent shape for all support points in [0, 1], it is computationally prohibitive for now, and needs further exploration. References 1. Br¨ocker, J., Smith, L.A.: Increasing the reliability of reliability diagrams. Weather and forecasting 22(3), 651–661 (2007) 2. Chen, S.X.: Beta kernel estimators for density functions. Computational Statistics & Data Analysis 31(2), 131–145 (1999) 12 N. Posocco et al. 3. DeGroot, M.H., Fienberg, S.E.: The comparison and evaluation of forecasters. Jour- nal of the Royal Statistical Society: Series D (The Statistician) 32(1-2), 12–22 (1983) 4. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural networks. In: 34th International Conference on Machine Learning, ICML 2017. vol. 3, pp. 2130–2143 (jun 2017) 5. Keren, G., Cummins, N., Schuller, B.: Calibrated prediction intervals for neural network regressors. IEEE Access 6, 54033–54041 (2018) 6. Kull, M., Flach, P.: Novel decompositions of proper scoring rules for classiﬁcation: Score adjustment as precursor to calibration. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. vol. 9284, pp. 68–85 (2015) 7. Kull, M., Perello-Nieto, M., K¨angsepp, M., Song, H., Flach, P., Others: Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirich- let calibration. In: Advances in Neural Information Processing System 32 (2019) 8. Kull, M., Silva Filho, T., Flach, P.: Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classiﬁers. In: Artiﬁcial Intelligence and Statistics. pp. 623–631. PMLR (2017) 9. Kumar, A., Sarawagi, S., Jain, U.: Trainable calibration measures for neural net- works from kernel mean embeddings. In: International Conference on Machine Learning. pp. 2805–2814. PMLR (2018) 10. Naeini, M.P., Cooper, G., Hauskrecht, M.: Obtaining well calibrated probabili- ties using bayesian binning. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol. 29 (2015) 11. Niculescu-Mizil, A., Caruana, R.: Predicting good probabilities with supervised learning. In: ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning. pp. 625–632 (2005) 12. Nixon, J.V., Dusenberry, M.W., Zhang, L., Jerfel, G., Tran, D.: Measuring Cali- bration in Deep Learning. In: CVPR Workshops. vol. 2 (apr 2019) 13. Platt, J., Others: Probabilistic outputs for support vector machines and compar- isons to regularized likelihood methods. Advances in large margin classiﬁers 10(3), 61–74 (1999) 14. Silverman, B.W.: Density estimation for statistics and data analysis, vol. 26. CRC press (1986) 15. Song, H., Diethe, T., Kull, M., Flach, P.: Distribution calibration for regression. In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the 36th International Con- ference on Machine Learning. Proceedings of Machine Learning Research, vol. 97, pp. 5897–5906. PMLR (2019) 16. Zadrozny, B., Elkan, C.: Obtaining calibrated probability estimates from deci- sion trees and naive Bayesian classiﬁers. In: International Conference on Machine Learning (ICML). pp. 1–8 (2001) 17. Zadrozny, Bianca and Elkan, C.: Transforming Classiﬁer Scores into Accurate Mul- ticlass Probability Estimates Bianca. In: Proceedings of the ACM SIGKDD In- ternational Conference on Knowledge Discovery and Data Mining. p. 704. ACM (2002)","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources.backup/papers_to_add/Papers I'm Reviewing Right Now/Mayr14evoBoost.pdf","text":"The evolution of boosting algorithms – from machine learning to statistical modelling ∗ Andreas Mayr†1, Harald Binder 2, Olaf Gefeller 1, Matthias Schmid 1,3 1 Institut f¨ur Medizininformatik, Biometrie und Epidemiologie, Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg, Germany 2 Institut f¨ur Medizinische Biometrie, Epidemiologie und Informatik, Johannes Gutenberg-Universit¨at Mainz, Germany 3 Institut f¨ur Medizinische Biometrie, Informatik und Epidemiologie, Rheinische Friedrich-Wilhelms-Universit¨at Bonn, Germany Abstract Background: The concept of boosting emerged from the ﬁeld of machine learning. The basic idea is to boost the accuracy of a weak classifying tool by combining various instances into a more accurate prediction. This general concept was later adapted to the ﬁeld of statistical modelling. Nowadays, boosting algorithms are often applied to estimate and select predictor eﬀects in statistical regression models. Objectives: This review article attempts to highlight the evolution of boosting algo- rithms from machine learning to statistical modelling. Methods: We describe the AdaBoost algorithm for classiﬁcation as well as the two most prominent statistical boosting approaches, gradient boosting and likelihood-based boosting for statistical modelling. We highlight the methodological background and present the most common software implementations. Results: Although gradient boosting and likelihood-based boosting are typically treated separately in the literature, they share the same methodological roots and follow the same fundamental concepts. Compared to the initial machine learning algo- rithms, which must be seen as black-box prediction schemes, they result in statistical models with a straight-forward interpretation. Conclusions: Statistical boosting algorithms have gained substantial interest during the last decade and oﬀer a variety of options to address important research questions in modern biomedicine. 1 Introduction Boosting algorithms represent one of the most promising methodological approaches for data analysis developed in the last two decades. The original algorithm [1] emerged from the ﬁeld of machine learning, where it gained much interest and was soon considered as a powerful instrument to predict binary outcomes. The basic idea is to iteratively apply simple ∗This article is not an exact copy of the original article submitted to Methods of Information in Medicine. The deﬁnitive publisher-authenticated version will be available online at: http://www.methods-online.com. If citing, please refer to the original article. †Address for correspondence: Andreas Mayr, Institut f¨ur Medizininformatik, Biometrie und Epidemiolo- gie, Friedrich-Alexander Universit¨at Erlangen-N¨urnberg, Waldstr. 6, 91054 Erlangen, Germany. 1arXiv:1403.1452v2 [stat.ME] 28 Apr 2014 classiﬁers and to combine their solutions to obtain a better prediction result. The concept of boosting was later adapted to the ﬁeld of statistical modelling, where it can be used to select and estimate the eﬀect of predictors on a univariate response variable in diﬀerent types of regression settings [2, 3]. Following a recent focus theme on boosting algorithms in Methods of Information in Medicine [4], the ﬁrst aim of this review is to highlight the evolution of boosting from a black-box machine learning algorithm to a ﬂexible tool to estimate and select interpretable statistical models. We will refer to this type of boosting algorithms as statistical boosting algorithms. The second aim is to bridge the methodological gap between two diﬀerent statistical boost- ing approaches which are typically treated separately in the literature, but share the same historical roots: gradient boosting [5] and likelihood-based boosting [6]. Both are increas- ingly applied in biomedical settings for diﬀerent kind of regression and prediction analysis [7, 8, 9]. The reasons for the success of statistical boosting algorithms are (i) their ability to incorpo- rate automated variable selection and model choice in the ﬁtting process, (ii) their ﬂexibility regarding the type of predictor eﬀects that can be included in the ﬁnal model and (iii) their stability in the case of high-dimensional data with possibly far more candidate variables than observations – a setting where most conventional estimation algorithms for regression settings collapse. The application of boosting algorithms thus oﬀers an attractive option for biomedical researchers: many modern biomedical settings like genome-wide association studies and research using other ’omics’ technologies are speciﬁcally challenging regarding all three points mentioned above [10, 11, 12]. This review is structured as follows: In Section 2, we introduce the machine-learning concept of boosting which led to the famous AdaBoost algorithm [1] for classiﬁcation. In Section 3 we present the statistical view on boosting which paved the way for the development of statistical boosting algorithms that are suitable for general regression settings. We describe the generic algorithms for gradient boosting and likelihood-based boosting and present the most common software packages. In the concluding Section 4, we summarize the main ﬁndings and highlight the diﬀerences between AdaBoost and statistical boosting. In a companion article [13], we additionally document the signiﬁcant progress in the method- ological research on statistical boosting algorithms over the last few years. 2 Boosting in machine learning The concept of boosting emerged from the ﬁeld of supervised learning, which is the au- tomated learning of an algorithm based on labelled data with observed outcome in order to make valid predictions for unlabelled future or unobserved data. Supervised learning is a subdiscipline of machine learning, which also comprises unsupervised learning based on unlabelled data and semi-supervised learning which is a combination of supervised and unsupervised learning [14]. A supervised learning machine typically yields a generalization function ˆh(·) that provides the solution to a classiﬁcation problem. The main goal of clas- siﬁcation is to categorize objects into a pre-deﬁned set of classes. For the remainder of this section we will consider the most common classiﬁcation problem, where the outcome variable Y has two classes, coded as {−1, 1}. Note that this coding diﬀers from the standard {0, 1} which is typically used in statistics for dichotomous outcomes. 2 The machine should learn from a training sample (y1, x1), ..., (yn, xn) with known class labels how to predict the class of a new observation xnew. The predictors x1, ..., xn are realizations of X, and n is the sample size. The task for the machine is to develop a prediction rule ˆh(·) to correctly classify a new observation: (y1, x1), ..., (yn, xn) supervised learning −−−−−−−−−−→ ˆh(xnew) = ˆynew 2.1 The concept of boosting The success story of boosting began with a question, not with an algorithm. The theoretical discussion was if any weak learning tool for classiﬁcation could be transformed to become also a strong learner [15]. In binary classiﬁcation, a weak learner is deﬁned to yield a correct classiﬁcation rate at least slightly better than random guessing (> 50%). A strong learner, on the other hand, should be able to be trained to a nearly perfect classiﬁcation (e.g., 99% accuracy). This theoretical question is of high practical relevance as it is typically easy to construct a weak learner, but diﬃcult to get a strong one [16]. The answer, which laid the ground for the concept of boosting, is that any weak base-learner can be potentially iteratively improved (boosted ) to become also a strong learner. To provide evidence for this concept, Schapire [17] and Freund [18] developed the ﬁrst boosting algorithms. Schapire and Freund later compared the general concept of boosting with “garnering wisdom from a council of fools” [19]. The “fools” in this case are the solutions of the simple base- learner: It classiﬁes only slightly better than the ﬂip of a coin. A simple base-learner is by no means a practical classiﬁcation rule, but even the simple base-learner must contain some valid information about the underlying structure of the problem. The task of a boosting algorithm is hence to learn from the iterative application of a weak learner and to use this information to combine it to an accurate classiﬁcation. However, just calling the weak learner multiple times on the same training sample would not change anything in its performance. The concept of boosting is not really to manipulate the base-learner itself to improve its performance but to manipulate the underlying training data by iteratively re-weighting the observations [19]. As a result, the base-learner in every iteration m will ﬁnd a new solution ˆh [m](·) from the data. Via repeated application of the weak base-learner on observations that are weighted based on the base-learner’s success in the previous rounds, the algorithm is forced to concentrate on objects that are hard to classify – as observations that were misclassiﬁed before get higher weights. Boosting the accuracy is achieved by increasing the importance of “diﬃcult” obser- vations. In each iteration m = 1, ..., mstop, the weight vector w[m] = (w[m] 1 , ..., w[m] n ) contains the individual weights of all observations depending on the success of their classiﬁcation in previous iterations. During the iteration cycle, the focus is shifted towards observations that were misclassiﬁed up to the current iteration m. In a ﬁnal step, all previous results of the base-learner are combined into a more accurate prediction: The weights of better performing solutions of the base-learner are increased via an iteration-speciﬁc coeﬃcient, which depends on the corresponding misclassiﬁcation rate. The resulting weighted majority vote [20] chooses the class most often selected by the base- learner while taking the error rate in each iteration into account (see point (5) in Box 1). This combination of forcing the algorithm to develop new strategies for problematic obser- vations and rewarding the base-learner in the ﬁnal aggregation for accurate solutions is the 3 Initialization (1) Set the iteration counter m = 0 and the individual weights wi for observations i = 1, ..., n to w[0] i = 1 n . Base-learner (2) Set m := m + 1 and compute the base-learner for the weighted data set: re-weight observations with w[m−1] 1 , ..., w[m−1] n base-learner −−−−−−→ ˆh [m](·) Update weights (3) Compute error rate and update the iteration-speciﬁc coeﬃcient αm → high values for small error rates. Update individual weights w[m] i → higher values if observa- tion was misclassiﬁed. Iterate (4) Iterate steps 2 and 3 until m = mstop. Final aggregation (5) Compute the ﬁnal classiﬁer for a new observation xnew: ˆfAdaboost(xnew) = sign ( mstop∑ m=1 αmˆh [m](xnew) ) Box 1: Schematic overview of the AdaBoost algorithm. main idea of boosting. Following this concept, it can be shown that all weak learners can potentially be boosted to become also strong learners [17, 18]. 2.2 AdaBoost The early boosting algorithms by Schapire [17] and Freund [18] were rather theoretical constructs for proving the idea of boosting than being suitable algorithms for practical usage. However, they paved the way for the ﬁrst concrete and – still today – most important boosting algorithm AdaBoost [1]. AdaBoost was the ﬁrst adaptive boosting algorithm as it automatically adjusts its parameters to the data based on the actual performance in the current iteration: both the weights wi for re-weighting the data as well as the weights αm for the ﬁnal aggregation are re-computed iteratively. For a schematic overview, see Box 1 – for worked out examples, we refer to [16, 19]. The introduction of AdaBoost gained much attention in the machine learning community. In practice, it is often used with simple classiﬁcation trees or stumps as base-learners and typically results in a dramatically improved performance compared to the classiﬁcation by 4 one tree or any other single base-learner [21, 22]. For example, Bauer and Kohavi [23] report an average 27% relative improvement in the misclassiﬁcation error for AdaBoost compared with a single decision tree. The authors additionally compared the accuracy of AdaBoost with the one of Bagging [24] in various settings. Bagging, in contrast to boosting, uses bootstrap generated samples to modify the training data and hence does not rely on the misclassiﬁcation rate of earlier iterations. After their large-scale comparison, Bauer and Kohavi concluded that boosting algorithms, in contrast to Bagging, are able to reduce not only the variation in the base-learner’s prediction error resulting from the use of diﬀerent training data sets (variance), but also the average diﬀerence between predicted and true classes (bias). This view is also essentially supported by an analysis of Breiman [25]. The success of AdaBoost allegedly led Breiman, who was a pioneer and leading expert in machine learning, to the statement [26]: Boosting is the best oﬀ-the-shelf classiﬁer in the world. 2.3 Overﬁtting A long-lasting discussion in the context of AdaBoost is its overﬁtting behavior. Overﬁtting describes the common phenomenon that when a prediction rule concentrates too much on peculiarities of the speciﬁc sample of training observations it was optimized on, it will often perform poorly on a new data set [27]. To avoid overﬁtting, the task for the algorithm therefore should not be to ﬁnd the best possible classiﬁer for the underlying training sample, but rather to ﬁnd the best prediction rule for a set of new observations. The main control instrument to avoid overﬁtting in boosting algorithms is the stopping iteration mstop. Very late stopping of AdaBoost may favor overﬁtting, as the complexity of the ﬁnal solution increases. On the other hand, stopping the algorithm too early does not only inevitably lead to higher error on the training data but could as well result in a poorer prediction on new data (underﬁtting). In the context of AdaBoost, it is nowadays consensus that although the algorithm may overﬁt [28, 29], it often is quite resistent to overﬁtting [5, 16, 19]. In their initial article, Freund and Schapire [1] showed that the generalization error on a test data set of AdaBoost’s ﬁnal solution is bounded by the training error plus a term which increases with the number of boosting iterations and the complexity of the base-learner. This ﬁnding was apparently supported by the widely acknowledged principle known as Occam’s Razor [30], which roughly states that for predictions, more complex classiﬁers should be outperformed by less complex ones if both carry the same amount of information. However, this theoretical result is not supported by the observation that AdaBoost, in practice, is often resistent to overﬁtting. As the complexity of the ﬁnal AdaBoost solution depends mainly on the stopping iteration mstop, following Occam’s Razor, later stopping of the algorithm should yield poorer predictions [16]. One way to explain AdaBoost’s overﬁtting behavior is based on the margin interpretation [22, 29, 31]: The margin of the ﬁnal boosting solution, in brief, can be interpreted as the conﬁdence in the prediction. With higher values of mstop, this margin may still increase and lead to better predictions on the test data even if the training error is already zero [32]. This theory was early questioned by results of Breiman [33], who developed the arc-gv algorithm which should yield a higher margin than AdaBoost, but clearly failed to outperform it in practice with respect to prediction accuracy. Later, Reyzin and Schapire [32] explained these ﬁndings with other factors like the complexity of the base-learner. For more on the margin 5 interpretation see the corresponding chapters in [16, 19]. Another explanation of the – seemingly contradictory – results on the overﬁtting behavior of boosting is the use of the wrong performance criteria for evaluation (e.g., [34]). The performance of AdaBoost has often been measured by evaluating the correct classiﬁcation rate, and the resistance to overﬁtting has usually been demonstrated by focusing on this speciﬁc criterion only. However, the criterion that is optimized by AdaBoost is in fact not the correct classiﬁcation rate but the so-called exponential loss function, and it can be shown that the two criteria are not necessarily optimized by the same predictions. For this reason some authors have argued that the overﬁtting behavior of AdaBoost should be analyzed by solely focusing on the exponential loss function [35]. For example, B¨uhlmann and Yu [36] have provided empirical evidence that too large mstop can lead to overﬁtting regarding the exponential loss without aﬀecting the misclassiﬁcation rate. 3 Statistical boosting Up to this point, we focused on the classical supervised learning problem where the task of boosting is to predict dichotomous outcomes. Nowadays, boosting algorithms are more often used to estimate the unknown quantities in general statistical models (statistical boosting). In the remainder of this section, we will therefore broaden the scope and consider general regression settings where the outcome variable Y can also be continuous or represent count data. The most important interpretation of boosting in this context is the statistical view of boosting by Friedman et al. [2]. It provided the basis for understanding the boosting concept in general and the success of AdaBoost in particular from a statistical point of view [21] by showing that AdaBoost in fact ﬁts an additive model. Most solutions of machine-learning algorithms, including AdaBoost, must be seen as black- box prediction schemes. They might yield very accurate predictions for future or unobserved data, but the way those results are produced and which role single predictors play are hardly interpretable. A statistical model, in contrast, aims at quantifying the relation between one or more observed predictor variables x and the expectation of the response E(Y ) via an interpretable function E(Y |X = x) = f (x). In cases of more than one predictor, the diﬀerent eﬀects of the single predictors are typically added, forming an additive model f (x) = β0 + h1(x1) + · · · + hp(xp) where β0 is an intercept and h1(·),...,hp(·) incorporate the eﬀects of predictors x1, ..., xp, which are components of X. The corresponding model class is called generalized additive models (’GAM’, [37]) and the aim is to model the expected value of the response variable, given the observed predictors via a link-function g(·): g(E(Y |X = x)) = β0 + p∑ j=1 hj(xj) GAMs are by deﬁnition no black boxes but contain interpretable additive predictors: The partial eﬀect of predictor x1, for example, is represented by h1(·). The direction, the size and 6 the shape of the eﬀect can be visualized and interpreted – this is a main diﬀerence towards many tree-based machine learning approaches. The core message delivered with the statistical view of boosting is that the original AdaBoost algorithm with regression-type base-learners (e.g., linear models, smoothing splines), in fact, ﬁts a GAM for dichotomous outcomes via the exponential loss in a stage-wise manner. The work by Friedman et al. [2] therefore provided the link between a successful machine-learning approach and the world of statistical modelling [21]. 3.1 Gradient boosting The concept of the statistical view of boosting was further elaborated by Friedman [3] who presented a boosting algorithm optimizing the empirical risk via steepest gradient descent in function space. Generally, the optimization problem for estimating the regression function f (·) of a statistical model, relating the predictor variables X with the outcome Y , can be expressed as ˆf (·) = argmin f (·) { EY,X[ρ(Y, f (X)) ] } , where ρ(·) denotes a loss function. The most common loss function is the L2 loss ρ(y, f (·)) = (y − f (·)) 2, leading to classical least squares regression of the mean: f (x) = E(Y |X = x). In practice, with a learning sample of observations (y1, x1), ..., (yn, xn) we minimize the empirical risk: ˆf (·) = argmin f (·) { 1 n n∑ i=1 ρ(yi, f (xi)) } The fundamental idea of gradient boosting is to ﬁt the base-learner not to re-weighted observations, as in AdaBoost, but to the negative gradient vector u[m] of the loss function ρ(y, ˆf (x)) evaluated at the previous iteration m − 1: u [m] = (u [m] i ) i=1,...,n = ( − ∂ ∂f ρ(yi, f ) ∣ ∣ ∣ ∣f = ˆf [m−1](·) ) i=1,...,n In case of the L2 loss, ρ(y, f (·)) = 1 2(y −f (·)) 2 leads simply to re-ﬁtting the residuals y −f (·). In every boosting iteration m, the base-learner is hence directly ﬁtting the errors made in the previous iteration y − f (·) [m−1]. Keeping this principle in mind, it becomes obvious that both AdaBoost and gradient boosting follow the same fundamental idea: Both algorithms boost the performance of a simple base-learner by iteratively shifting the focus towards problematic observations that are ‘diﬃcult’ to predict. With AdaBoost, this shift is done by up-weighting observations that were misclassiﬁed before. Gradient boosting identiﬁes diﬃcult observations by large residuals computed in the previous iterations. Generally, the underlying base-learner can be any regression technique; the most simple base- learner is a classical linear least-squares model with h(x) = x⊤β. If x is assumed to have a non-linear eﬀect on the response, smoothing splines could be used [3]. B¨uhlmann and Yu [38] 7 Initialization (1) Set the iteration counter m = 0. Initialize the additive predictor ˆf [0] with a start- ing value, e.g. ˆf [0] := (0)i=1,...,n. Specify a set of base-learners h1(x1), ..., hp(xp). Fit the negative gradient (2) Set m := m + 1. (3) Compute the negative gradient vector u of the loss function evaluated at the previous iteration: u [m] = (u [m] i ) i=1,...,n = ( − ∂ ∂f ρ(yi, f ) ∣ ∣ ∣ ∣f = ˆf [m−1](·) ) i=1,...,n (4) Fit the negative gradient vector u [m] separately to every base-learner: u [m] base−learner −−−−−−−→ ˆh [m] j (xj) for j = 1, ..., p. Update one component (5) Select the component j∗ that best ﬁts the negative gradient vector: j∗ = argmin 1≤j≤p n∑ i=1 (u [m] i − ˆh [m] j (xj)) 2 . (6) Update the additive predictor ˆf with this component ˆf [m](·) = ˆf [m−1](·) + sl · ˆh [m] j∗ (xj∗) , where sl is a small step length (0 < sl ≪ 1). A typical value in practice is 0.1. Iteration Iterate steps (2) to (6) until m = mstop. Box 2: Component-wise gradient boosting algorithm further developed the gradient boosting approach by applying component-wise smoothing splines as base-learners. The fundamental idea is that diﬀerent predictors are ﬁtted by separate base-learners hj(·), j = 1, ..., p. Typically, each base-learner hj(·) corresponds to one component xj of X and in every boosting iteration (as proposed in [3]) only a small amount of the ﬁt of the best-performing base-learner is added to the current additive predictor. The authors demonstrated that the resulting algorithm in combination with the L2 loss outperforms classical additive modelling in terms of prediction accuracy. This approach was further developed by B¨uhlmann [39] who specially focused on high-dimensional data settings. 8 B¨uhlmann and Hothorn [5] gave an overview of gradient boosting algorithms from a sta- tistical perspective presenting a generic functional gradient descent algorithm (see Box 2). As in [3], base-learners are used to ﬁt the negative gradient vector of the corresponding loss function. The algorithm descends the empirical risk via steepest gradient descent in function space, where the function space is provided by the base-learners. Each base-learner typically includes one predictor and in every boosting iteration only the best-performing base-learner and hence the best performing component of X is included in the ﬁnal model. This pro- cedure eﬀectively leads to data-driven variable selection during the model estimation. The base-learners h1(x1), ..., hp(xp) reﬂect the type of eﬀect the corresponding components will contribute to the ﬁnal additive model, which oﬀers the same interpretability as any other additive modelling approach. Examples for base-learners can be trees as in classical boosting algorithms, but commonly simple regression tools like linear models or splines are used to include linear as well as non-linear eﬀects on the response. Generally, it is consensus in the literature that base-learners should be weak in the sense that they do not oﬀer too complex solutions in a single iteration (e.g., penalized splines with small degrees of freedom [40]). In contrast to standard estimation methods, component-wise gradient boosting also works for high dimensional data where the number of predictors exceeds the number of observations (p > n). Furthermore, it is relatively robust in cases of multicollinearity. Due to the small step length in the update step (a typical value is 0.1 [41]) in combination with early stopping (Section 3.3), gradient boosting incorporates shrinkage of eﬀect estimates in the estimation process: The absolute size of the estimated coeﬃcients is intentionally reduced – this is a similarity to penalized regression approaches as the Lasso [42]. Shrinkage of eﬀect estimates leads to a reduced variance of estimates and should therefore increase the stability and accuracy of predictions [26]. The gradient boosting approach can be used to optimize any loss function that is at least convex and diﬀerentiable: The framework is speciﬁcally not restricted to statistical distri- butions that are members of the exponential family as in classical GAMs. For example, Ma and Huang [43] applied gradient boosting with an adapted ROC (receiver operating charac- teristics) approach, optimizing the area under the ROC curve for biomarker selection from high-dimensional microarray data. 3.2 Likelihood-based boosting When considering statistical models, estimation in low-dimensional settings typically is per- formed by maximizing a likelihood. While such a likelihood can also be used to deﬁne a loss function in gradient boosting, a boosting approach could also be built on base-learners that directly maximize an overall likelihood in each boosting step. This is the underlying idea of likelihood-based boosting, introduced by Tutz and Binder [6]. When the eﬀects of the predictors x1, . . . , xp can be speciﬁed by a joint parameter vector β, the task is to maximize the overall log-likelihood l(β). Given a starting value or estimate from a previous boosting step ˆβ, likelihood-based boosting approaches use base-learners for estimating parameters γ in a log-likelihood l(γ) that contains the eﬀect of ˆβ as a ﬁxed oﬀset. For obtaining small updates, similar to gradient boosting, a penalty term is attached to l(γ). The estimates ˆγ are subsequently used to update the overall estimate ˆβ. For continuous response regression models, including an oﬀset is the same as ﬁtting a model to the residuals from the previous boosting step, and maximization of l(γ) by a base-learner becomes standard least-squares 9 Initialization (1) Set the iteration counter m = 0. Initialize the additive predictor ˆf [0] with a starting value, e.g. ˆf [0] := (0)i=1,...,n or the maximum likelihood estimate ˆβ0 from an intercept model (if the overall regression model includes an intercept term). Candidate models (2) Set m := m + 1. (3) For each predictor xj, j = 1, ..., p estimate the corresponding functional term ˆhj(·), as determined by parameter γj, by attaching a penalty term to the log-likelihood l(γj), which includes ˆf [m−1](·) as an oﬀset. Update one component (4) Select the component j∗ that results in the candidate model with the largest log-likelihood l(ˆγj∗): j∗ = argmax 1≤j≤p l(ˆγj) (5) Update ˆf [m] to ˆf [m](·) = ˆf [m−1](·) + ˆh [m] j∗ (xj∗) , potentially adding an intercept term from maximum likelihood estimation. Iteration Iterate steps (2) to (5) until m = mstop. Box 3: Component-wise likelihood-based boosting algorithm estimation with respect to these residuals. In this special case, likelihood-based boosting thus coincides with gradient boosting for L2 loss [38]. Component-wise likelihood-based boosting performs variable selection in each step, i.e. there is a separate base-learner for ﬁtting a candidate model for each predictor xj by maximizing a log-likelihood l(γj). The overall parameter estimate ˆβ then only is updated for that predictor xj∗ which results in the candidate model with the largest log-likelihood l(ˆγj). In linear models, γj is a scalar value, and the penalized log-likelihood takes the form l(γj)−λγ2 j , where λ is a penalty parameter that determines the size of the updates. Component-wise likelihood-based boosting then generalizes stagewise regression [44]. For a schematic overview of component-wise likelihood-based boosting see Box 3. Tutz and Binder [6] applied this principle to generalized additive models with B-spline base-learners. Likelihood-based boosting for generalized linear models was introduced in another article by Tutz and Binder [45] and an approach for generalized additive mixed models was described 10 by Groll and Tutz [46]. In these approaches, the best component for an update is selected according to the deviance in each boosting step. To decrease the computational demand with a large number of covariates, the likelihood-based boosting approach for the Cox proportional hazards model [47] instead uses a score statistic. While component-wise likelihood-based boosting often provides results similar to gradient boosting (e.g., [47]), the use of standard regression models in the boosting steps allows for adaptation of techniques developed for the standard regression setting. For example, unpenalized covariates can be incorporated in a straightforward way by not incorporating these into the penalty term attached to l(γ) [47], but estimating their parameters together with a potential intercept term in steps (1) and (5). Approximate conﬁdence intervals for the estimated covariate eﬀects can be obtained by combining hat matrices from the individual boosting steps [6]. 3.3 Early stopping of statistical boosting algorithms Although there are diﬀerent inﬂuential factors for the performance of boosting algorithms, the stopping iteration mstop is considered to be the main tuning parameter [48]. Stopping the algorithm before its convergence (early stopping) prevents overﬁtting (Section 2.3) and typically improves prediction accuracy. In case of statistical boosting, mstop controls both shrinkage of eﬀect estimates and variable selection. The selection of mstop hence reﬂects the common bias-variance trade-oﬀ in statistical modelling: Large values of mstop lead to more complex models with higher variance and small bias. Smaller values of mstop lead to sparser models with less selected variables, more shrinkage and reduced variance [48]. To prevent overﬁtting, it is crucial not to consider the stopping iteration mstop that leads to the best model on the training data but to evaluate the eﬀect of mstop on separate test data. If no additional data are available, two general approaches are commonly applied: The ﬁrst is to use information criteria (AIC, BIC or gMDL [49]) which evaluate the likelihood on training data but additionally penalize too complex models by adding a multiple of their degrees of freedom. There are two problems with this approach: (i) for component-wise boosting algorithms these information criteria rely on an estimation of the degrees of freedom that is known to underestimate the true values [50]; (ii) they are only available for a limited number of loss functions. The second, more general approach is to apply resampling or cross-validation techniques to subsequently divide the data into test and training sets and choose mstop by evaluating the models on the test data. For the evaluation, it is crucial to use the same loss function the algorithm aims to optimize. If the algorithm in a binary classiﬁcation setting optimizes the exponential loss, one should use the exponential loss and not the misclassiﬁcation rate to select mstop. The optimal mstop is hence the one which leads to the smallest average empirical loss on the out-of-sample test data. 3.4 Implementation and computational complexity Most implementations of statistical boosting algorithms are included in freely available add- on packages for the open source programming environment R [51]. Worked out examples and R-code for applying the most important implementations are provided in the Appendix of this article. 11 Gradient boosting is implemented in the add-on package mboost (model-based boosting, [52]). The package provides a large variety of pre-implemented loss functions and base- learners yielding wide-ranging possibilities for almost any statistical setting where regression models can be applied. For an overview of how mboost can be used in biomedical practice, see Hofner et al. [41]. An alternative implementation of gradient boosting is provided with the gbm package [53] which focuses on trees as base-learners. Likelihood-based boosting for generalized linear and additive regression models is provided by the add-on package GAMBoost [54] and an implementation of the Cox model is contained in the package CoxBoost [55]. One of the main advantages of statistical boosting approaches compared to standard esti- mation schemes is that they are computationally feasible in p > n situations. The com- putational complexity of statistical boosting approaches depends mainly on the number of separate base-learners. In case of component-wise boosting, the complexity increases linearly with p [56]. The computationally most burdensome part of applying statistical boosting in practice is the selection of the stopping iteration mstop. In case of applying information criteria (as the AIC), this involves multiplication of n × n matrixes for each boosting it- eration, which becomes computationally problematic for data settings with large n. The computing-time to select mstop via resampling procedures depends mainly on mstop itself, the number of resamples B and p [48]. In practice, selecting mstop via resampling can be drastically fastened by applying parallel computing, which is implemented in all R packages for statistical boosting. 4 Conclusion One reason for the success of statistical boosting algorithms is their straight-forward inter- pretation. While competing machine learning approaches (including AdaBoost) may also yield accurate predictions in case of complex data settings, they must be seen as black boxes: The structure of the underlying data is considered irrelevant and the way diﬀerent predic- tors contribute to the ﬁnal solution remains unknown. Statistical boosting algorithms, in contrast, are typically applied with simple regression-type functions as base-learners and therefore yield classical statistical models, reﬂecting the contribution of diﬀerent predictors on an outcome variable of interest. As a result, their solution oﬀers the same interpretation as any other model in classical regression analysis – only that it was derived by applying one of the most powerful prediction frameworks available in the toolbox of a modern statistician. We presented two speciﬁc frameworks for statistical boosting: gradient boosting and likelihood- based boosting. Although both algorithms are typically treated separately in the literature, both follow the same structure and share the same historical roots. In some special cases like the L2 loss and Gaussian response they coincide. While gradient boosting is a more general approach and also allows for distribution-free regression settings like optimizing a ROC curve [43] or boosting quantile regression [57], likelihood-based boosting carries the advantage that it delivers the Hessian matrix, which can be used to compute approximate conﬁdence intervals for the estimated predictor eﬀects. It is by no means an exaggeration to forecast that the application of statistical boosting algorithms in biomedical research will increase in the years to come. One of the main reasons for this development is that the number of candidate variables and predictors for modern 12 biomedical research has continuously been increasing in recent years. In this type of settings, statistical boosting algorithms can demonstrate their full strengths via automated variable selection and model choice while still providing the same interpretability most biomedical research relies on. Acknowledgements The work on this article was supported by the Deutsche Forschungsgemeinschaft (DFG) (www.dfg.de), grant SCHM 2966/1-1. References [1] Freund Y, Schapire R. Experiments With a New Boosting Algorithm. In: Proceedings of the Thirteenth International Conference on Machine Learning Theory. San Francisco, CA: San Francisco: Morgan Kaufmann Publishers Inc.; 1996. p. 148–156. [2] Friedman JH, Hastie T, Tibshirani R. Additive Logistic Regression: A Statistical View of Boosting (with Discussion). The Annals of Statistics. 2000;28:337–407. [3] Friedman JH. Greedy Function Approximation: A Gradient Boosting Machine. The Annals of Statistics. 2001;29:1189–1232. [4] Schmid M, Gefeller O, Hothorn T. Boosting Into a New Terminological Era. Methods of Information in Medicine. 2012;51(2):150. [5] B¨uhlmann P, Hothorn T. Boosting Algorithms: Regularization, Prediction and Model Fitting (with Discussion). Statistical Science. 2007;22:477–522. [6] Tutz G, Binder H. Generalized Additive Modeling with Implicit Variable Selection by Likelihood-based Boosting. Biometrics. 2006;62:961–971. [7] Faschingbauer F, Beckmann M, Goecke T, Yazdi B, Siemer J, Schmid M, et al. A New Formula for Optimized Weight Estimation in Extreme Fetal Macrosomia (≥ 4500 g). European Journal of Ultrasound. 2012;33(05):480–488. [8] Lin K, Futschik A, Li H. A fast Estimate for the Population Recombination Rate Based on Regression. Genetics. 2013;194(2):473–484. [9] Saintigny P, Zhang L, Fan YHH, El-Naggar AK, Papadimitrakopoulou VA, Feng L, et al. Gene Expres- sion Proﬁling Predicts the Development of Oral Cancer. Cancer Prevention Research. 2011 2;4(2):218– 229. [10] Li H, Luan Y. Boosting Proportional Hazards Models Using Smoothing Splines, with Applications to High-Dimensional Microarray Data. Bioinformatics. 2005;21(10):2403–2409. [11] Binder H, Benner A, Bullinger L, Schumacher M. Tailoring Sparse Multivariable Regression Techniques for Prognostic Single-Nucleotide Polymorphism Signatures. Statistics in Medicine. 2013;32(10):1778– 1791. [12] Mayr A, Schmid M. Boosting the Concordance Index for Survival Data – A Uniﬁed Framework to Derive and Evaluate Biomarker Combinations. PloS ONE. 2014;9(1):e84483. [13] Mayr A, Binder H, Gefeller O, Schmid M. Extending Statistical Boosting – An Overview of Recent Methodological Developments. Methods of Information in Medicine. 2014;Available from: http:// arxiv.org/abs/1403.1692. [14] Bishop CM, et al. Pattern Recognition and Machine Learning. vol. 4. Springer New York; 2006. [15] Kearns MJ, Valiant LG. Cryptographic Limitations on Learning Boolean Formulae and Finite Au- tomata. In: Johnson DS, editor. Proceedings of the 21st Annual ACM Symposium on Theory of Computing, May 14-17, 1989, Seattle, Washigton, USA. ACM; 1989. p. 433–444. 13 [16] Zhou ZH. Ensemble Methods: Foundations and Algorithms. CRC Machine Learning & Pattern Recog- nition. Chapman & Hall; 2012. [17] Schapire RE. The Strength of Weak Learnability. Machine Learning. 1990;5(2):197–227. [18] Freund Y. Boosting a Weak Learning Algorithm by Majority. In: Fulk MA, Case J, editors. Proceed- ings of the Third Annual Workshop on Computational Learning Theory, COLT 1990, University of Rochester, Rochester, NY, USA, August 6-8, 1990; 1990. p. 202–216. [19] Schapire RE, Freund Y. Boosting: Foundations and Algorithms. MIT Press; 2012. [20] Littlestone N, Warmuth MK. The Weighted Majority Algorithm. In: Foundations of Computer Science, 1989., 30th Annual Symposium on. IEEE; 1989. p. 256–261. [21] Ridgeway G. The State of Boosting. Computing Science and Statistics. 1999;31:172–181. [22] Meir R, R¨atsch G. An Introduction to Boosting and Leveraging. Advanced Lectures on Machine Learning. 2003;p. 118–183. [23] Bauer E, Kohavi R. An Empirical Comparison of Voting Classiﬁcation Algorithms: Bagging, Boosting, and Variants. Journal of Machine Learning. 1999;36:105–139. [24] Breiman L. Bagging Predictors. Machine Learning. 1996;24:123–140. [25] Breiman L. Arcing Classiﬁers (with Discussion). The Annals of Statistics. 1998;26:801–849. [26] Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York: Springer; 2009. [27] Dietterich T. Overﬁtting and Undercomputing in Machine Learning. ACM Computing Surveys (CSUR). 1995;27(3):326–327. [28] Grove AJ, Schuurmans D. Boosting in the Limit: Maximizing the Margin of Learned Ensembles. In: Proceeding of the AAAI-98. John Wiley & Sons Ltd; 1998. p. 692–699. [29] R¨atsch G, Onoda T, M¨uller KR. Soft Margins for AdaBoost. Machine Learning. 2001;42(3):287–320. [30] Blumer A, Ehrenfeucht A, Haussler D, Warmuth MK. Occam’s Razor. Information Processing Letters. 1987;24:377–380. [31] Schapire RE, Freund Y, Bartlett P, Lee WS. Boosting The Margin: A new Explanation for the Eﬀectiveness of Voting Methods. The Annals of Statistics. 1998;26(5):1651–1686. [32] Reyzin L, Schapire RE. How Boosting the Margin can also Boost Classiﬁer Complexity. In: Proceeding of the 23rd International Conference on Machine Learning; 2006. p. 753–760. [33] Breiman L. Prediction Games and Arcing Algorithms. Neural Computation. 1999;11:1493–1517. [34] Mease D, Wyner A. Evidence Contrary to the Statistical View of Boosting. The Journal of Machine Learning Research. 2008;9:131–156. [35] B¨uhlmann P, Hothorn T. Rejoinder: Boosting Algorithms: Regularization, Prediction and Model Fitting. Statistical Science. 2007;22:516–522. [36] B¨uhlmann P, Yu B. Response to Mease and Wyner, Evidence Contrary to the Statistical View of Boosting. Journal of Machine Learning Research. 2008;9:187–194. [37] Hastie T, Tibshirani R. Generalized Additive Models. London: Chapman & Hall; 1990. [38] B¨uhlmann P, Yu B. Boosting with the L2 Loss: Regression and Classiﬁcation. Journal of the American Statistical Association. 2003;98:324–338. [39] B¨uhlmann P. Boosting for High-Dimensional Linear Models. The Annals of Statistics. 2006;34:559–583. [40] Schmid M, Hothorn T. Boosting Additive Models Using Component-Wise P-splines. Computational Statistics & Data Analysis. 2008;53:298–311. [41] Hofner B, Mayr A, Robinzonov N, Schmid M. Model-Based Boosting in R: A Hands-on Tutorial Using the R Package mboost. Computational Statistics. 2014;29:3–35. 14 [42] Tibshirani R. Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society - Series B. 1996;58(1):267–288. [43] Ma S, Huang J. Regularized ROC Method for Disease Classiﬁcation and Biomarker Selection with Microarray Data. Bioinformatics. 2005;21(24):4356–4362. [44] Efron B, Hastie T, Johnstone L, Tibshirani R. Least Angle Regression. Annals of Statistics. 2004;32:407– 499. [45] Tutz G, Binder H. Boosting Ridge Regression. Computational Statistics & Data Analysis. 2007;51(12):6044–6059. [46] Groll A, Tutz G. Regularization for Generalized Additive Mixed Models by Likelihood-based Boosting. Methods of Information in Medicine. 2012;51(2):168–177. [47] Binder H, Schumacher M. Allowing for Mandatory Covariates in Boosting Estimation of Sparse High- Dimensional Survival Models. BMC Bioinformatics. 2008;9(14). [48] Mayr A, Hofner B, Schmid M. The Importance of Knowing When to Stop – A Sequential Stopping Rule for Component-Wise Gradient Boosting. Methods of Information in Medicine. 2012;51(2):178–186. [49] Hansen MH, Yu B. Model Selection and the Principle of Minimum Description Length. Journal of the American Statistical Association. 2001;96(454):746–774. [50] Hastie T. Comment: Boosting Algorithms: Regularization, Prediction and Model Fitting. Statistical Science. 2007;22(4):513–515. [51] R Development Core Team. R: A Language and Environment for Statistical Computing. Vienna, Aus- tria; 2014. ISBN 3-900051-07-0. Available from: http://www.R-project.org. [52] Hothorn T, B¨uhlmann P, Kneib T, Schmid M, Hofner B. mboost: Model-Based Boosting; 2013. R package version 2.2-3. Available from: http://CRAN.R-project.org/package=mboost. [53] Ridgeway G. gbm: Generalized Boosted Regression Models; 2012. R package version 1.6-3.2. Available from: http://CRAN.R-project.org/package=gbm. [54] Binder H. GAMBoost: Generalized Linear and Additive Models by Likelihood Based Boosting.; 2011. R package version 1.2-2. Available from: http://CRAN.R-project.org/package=GAMBoost. [55] Binder H. CoxBoost: Cox Models by Likelihood-based Boosting for a Single Survival Endpoint or Com- peting Risks; 2013. R package version 1.4. Available from: http://CRAN.R-project.org/package= CoxBoost. [56] B¨uhlmann P, Yu B. Sparse Boosting. Journal of Machine Learning Research. 2007;7:1001–1024. [57] Fenske N, Kneib T, Hothorn T. Identifying Risk Factors for Severe Childhood Malnutrition by Boosting Additive Quantile Regression. Journal of the American Statistical Association. 2011;106(494):494–510. [58] Garcia AL, Wagner K, Hothorn T, Koebnick C, Zunft HJF, Tippo U. Improved Prediction of Body Fat by Measuring Skinfold Thickness, Circumferences, and Bone breadths. Obesity Research. 2005;13(3):626–634. [59] Hurvich CM, Simonoﬀ JS, Tsai CL. Regression and Time Series Model Selection in Small Samples. Journal of the Royal Statistical Society - Series B. 1998;60(2):271–293. [60] Rosenwald A, Wright G, Chan WC, Connors JM, Campo E, Fisher RI, et al. The Use of Molecular Proﬁling to Predict Survival After Chemotherapy for Diﬀuse Large-B-Cell Lymphoma. New England Journal of Medicine. 2002;346(25):1937–1947. [61] Li H, Gui J. Partial Cox Regression Analysis for High-Dimensional Microarray Gene Expression Data. Bioinformatics. 2004;20:208–215. 15 Appendix In the main article, we highlighted the concept and evolution of boosting, which is arguably one of the most important methodological contributions to the ﬁeld of machine learning in the last decades. The introduction of AdaBoost was a milestone for the development of purely data-driven prediction rules. It was, however, the statistical view of boosting [2] which paved the way for the success story of boosting algorithms in statistical modelling and their application in biomedical research. In contrast to AdaBoost, statistical boosting does not necessarily focus on classiﬁcation problems but can be applied to various type of regression settings. This Appendix provides examples on how to apply statistical boosting algorithms in practice. For the gradient boosting approach, we will concentrate on the implementation provided by the R [51] add-on package mboost (model-based boosting, [52]). For the likelihood-based boosting approach, we present examples and Code for the implementations provided by GAMBoost [54] as well as the CoxBoost [55] package. All packages and the underlying open source programming environment R are freely available at http://r-project.org. Gradient boosting The mboost package provides a large number of pre-implemented loss functions (families) and base-learners which can be combined by the user yielding wide-ranging possibilities for almost any statistical setting where regression models can be applied. For a detailed tutorial describing the application of mboost, including also tables presenting the various base-learners and families, see [41]. The bodyfat data The overall aim of this application is to compute accurate predictions for the body fat of women based on available anthropometric measurements. Observations of 71 German women are available with the data set provided by Garcia et al. [58] and included in mboost. This illustrative example has been already used for demonstration purposes in the context of boosting [5, 41]. The response variable is the body fat measured by DXA (DEXfat) which can be seen as the gold standard to measure body fat. However, DXA measurements are too expensive and complicated for a broad use. Anthropometric measurements as waist or hip circumferences are in comparison very easy to measure in a standard screening. A prediction formula only based on these measures could therefore be a valuable alternative with high clinical relevance for daily usage. In total, the data set contains 8 continuous variables as possible predictors. In the original publication [58], the presented prediction formula was based on a linear model with backward-elimination for variable selection. The resulting ﬁnal model utilized hip cir- cumference (hipcirc), knee breadth (kneebreadth) and a compound covariate (anthro3a) which is deﬁned as the sum of the logarithmic measurements of chin skinfold, triceps skinfold and subscapular skinfold: R> library(mboost) ## load package R> data(bodyfat) ## load data R> 16 R> ## Reproduce formula of Garcia et al., 2005 R> lm1 <- lm(DEXfat ~ hipcirc + kneebreadth + anthro3a, data = bodyfat) R> coef(lm1) (Intercept) hipcirc kneebreadth anthro3a -75.2347840 0.5115264 1.9019904 8.9096375 A very similar model can be easily ﬁtted by statistical boosting, applying glmboost() (which uses linear base-learners) with default settings: R> ## Estimate same model by glmboost R> glm1 <- glmboost(DEXfat ~ hipcirc + kneebreadth + anthro3a, + data = bodyfat) R> coef(glm1, off2int = TRUE) ## off2int adds the offset to the intercept (Intercept) hipcirc kneebreadth anthro3a -75.2073365 0.5114861 1.9005386 8.9071301 Note that in this case we used the default family Gaussian() leading to boosting with the L2 loss. Diﬀerent loss functions The loss function can be easily adapted via the family argument inside the ﬁtting functions of mboost. For example, we now ﬁt the median (with family = Laplace()), apply Gamma regression (GammaReg()) and Huber’s loss for robust regression (Huber()). To compare the prediction accuracy, we leave out the ﬁrst ten observations as test data: R> ## separate training and test data R> dat.train <- bodyfat[-(1:10),-1] ## removing age R> dat.test <- bodyfat[1:10,-1] R> R> ## original formula R> lm1 <- glm(DEXfat ~ hipcirc + kneebreadth + anthro3a, R> data =dat.train) R> ## boosting: R> ## \"response ~ .\" includes all remaining variables in the candidate model R> glm1 <- glmboost(DEXfat ~ . , data = dat.train) ## L_2 -> Gaussian R> glm2 <- glmboost(DEXfat ~ . , data = dat.train, family = Laplace()) R> glm3 <- glmboost(DEXfat ~ . , data = dat.train, family = GammaReg()) R> glm4 <- glmboost(DEXfat ~ . , data = dat.train, family = Huber()) R> R> ## predictions on test data: mean squared error of prediction R> mean((predict(lm1, dat.test) - dat.test$DEXfat)^2) ## orig. [1] 8.782721 R> mean((predict(glm1, dat.test) - dat.test$DEXfat)^2) ## boosting [1] 5.141709 R> mean((predict(glm2, dat.test) - dat.test$DEXfat)^2) ## median [1] 19.02454 R> mean((exp(predict(glm3, dat.test)) - dat.test$DEXfat)^2) ## gamma 17 [1] 8.748015 R> mean((predict(glm4, dat.test) - dat.test$DEXfat)^2) ## robust [1] 5.234016 Not surprisingly, the best performing loss function (regarding the mean squared error of prediction on the 10 test observations) is the L2 loss leading to classical regression of the mean. Note, that in this small illustrative example, we used the default of 100 boosting iterations with linear base-learners. Diﬀerent base-learners Up to now, we used the ﬁtting function glmboost() which automatically applies linear ordinary least squares base-learner. Via the function gamboost() one can use basically the same formula interface, however alternatively P-spline base-learners are ﬁtted for all variables. R> # now with P-splines R> gam1 <- gamboost(DEXfat ~ hipcirc + kneebreadth + anthro3a, + data = dat.train) R> mean((predict(gam1, dat.test) - dat.test$DEXfat)^2) [1] 9.274717 R> ## show partial effects R> par(mfrow=c(1,3)) ## three plots in a row R> plot(gam1) The plot() function for an object of the class gamboost automatically displays the partial eﬀects of the diﬀerent covariates on the response. ●● ● ● ●●● ●● ●● ● ●●● ● ● ●●●●● ●● ●●● ●●● ● ● ● ● ●●●● ●●●●● ● ● ● ● ●● ● ● ●●● ●●●●● ● ● 90 100 110 120 130−10−50510 hipcircfpartial ● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●● ● ●● ● ● ● ● ● 8 9 10 11−10−50510 kneebreadthfpartial ●● ● ●● ● ●● ●●● ● ● ●●●●● ● ●●●● ●● ● ●●●●●●●●●●●● ● ●●●●●●●● ● ●●● ● ●● ●● ●● ● ● ● 2.5 3.0 3.5 4.0 4.5−10−50510 anthro3afpartial R> # with all variables as candidates R> gam2 <- gamboost(DEXfat ~ . , data = dat.train) 18 R> mean((predict(gam2, dat.test) - dat.test$DEXfat)^2) [1] 4.931875 R> R> # Try the same with different family -> median R> gam3 <- gamboost(DEXfat ~ . , family = Laplace(), data = dat.train) R> mean((predict(gam3, dat.test) - dat.test$DEXfat)^2) [1] 10.58769 However, it is of course also possible to include some variables with a linear eﬀect and others with smooth eﬀects. This can be done by specifying the type of base-learner inside the formula interface. For example, bols(x1) applies an ordinary least squares base-learner for variable x1, while bbs(x2) incorporates a P-splines base-learner (cubic P-splines with second order diﬀerences, 20 inner knots and 4 degrees of freedom) for variable x2. For more on boosting with splines, see [40]. R> # linear effect for hipcirc, smooth effects for kneebreadth and anthro3a R> R> gam4 <- gamboost(DEXfat ~ bols(hipcirc) + bbs(kneebreadth) + bbs(anthro3a), + data = dat.train) R> R> mean((predict(gam4, dat.test) - dat.test$DEXfat)^2) [1] 9.725139 R> par(mfrow=c(1,3)) ## three plots in a row R> plot(gam4) ●● ● ● ●●● ●● ●●●●●● ● ● ●●●●●●● ●●●●●● ●● ● ● ●●●● ●●●●●● ● ● ● ●● ● ● ●●● ● ●● ● ● ● ● 90 100 110 120 130−10−50510 hipcircfpartial ● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●● ● ●● ● ● ● ● ● 8 9 10 11−10−50510 kneebreadthfpartial ●● ● ●● ● ●● ●●● ● ● ●●●●● ● ●●●● ●● ● ●●●●●●●●●●●● ● ●●●●●●●● ● ●●● ● ●● ●● ●●● ● ● 2.5 3.0 3.5 4.0 4.5−10−50510 anthro3afpartial Compared to the partial eﬀects of gam1, the model now included a linear eﬀect for hipcirc. Early stopping The main tuning parameter of boosting algorithms is the stopping iteration. In mboost, the number of boosting iterations can be speciﬁed inside the boost control() function which 19 can then be passed along to the ﬁtting function glmboost() and gamboost(). The default number of boosting iterations is mstop = 100. R> ## now mstop = 500 R> gam1 <- gamboost(DEXfat ~ hipcirc + kneebreadth + anthro3a, data = dat.train, + control = boost_control(mstop = 500, trace = TRUE)) [ 1] ...................................................... -- risk: 523.347 [ 126] ...................................................... -- risk: 476.3329 [ 251] ...................................................... -- risk: 456.9567 [ 376] ...................................................... Final risk: 445.9373 Once a model is ﬁtted, the number of boosting iterations can also be modiﬁed by simple indexing: R> ## go back to mstop = 450 R> gam1 <- gam1[450] R> mean((predict(gam1, dat.test) - dat.test$DEXfat)^2) [1] 15.68723 As described in the article, to select the optimal stopping iteration one can either apply information criteria as the AIC or resampling procedures. For both general approaches there exist pre-implemented functions in mboost: R> AIC(gam1) [1] 3.552281 Optimal number of boosting iterations: 149 Degrees of freedom (for mstop = 149): 9.593731 R> gam1 <- gam1[149] R> mean((predict(gam1, dat.test) - dat.test$DEXfat)^2) [1] 10.51821 Note that mboost per default applies a bias-corrected version of the AIC, which was pro- posed by Hurvich et al. [59]. However, the main problem with those criteria is that they (i) are not available for all loss functions, and more importantly, (ii) in case of boosting rely on estimations of the degrees of freedom which are severely biased. As a result, we generally think that resampling or cross-validation techniques in combination with the empirical loss are more appropriate to determine the stopping iteration [48]. In mboost, the function cvrisk() can be applied to automatically select the best-performing stopping iteration. It can be used to carry out cross-validation, subsampling or bootstrapping (the default). Note that in case of heavily unbalanced data sets, concerning a dichotomous outcome or an important predictor, it may be necessary to apply stratiﬁed resampling techniques via the argument strata. When the package parallel is available, cvrisk() applies parallel computing (if the machine contains more than one core). R> set.seed(123) R> cvr <- cvrisk(gam1) ## default: 25-fold Bootstrap (takes a few seconds) R> plot(cvr) 20 25-fold bootstrap Number of boosting iterationsSquared Error (Regression) 1 12 24 36 48 60 72 84 96 110 126 14250100150 R> mstop(cvr) ## distract the optimal mstop [1] 46 R> mean((predict(gam1[46], dat.test) - dat.test$DEXfat)^2) [1] 7.924422 Now we led the component-wise boosting algorithm select the most important predictors: R> ## include all variables, 100 iterations R> gam2 <- gamboost(DEXfat ~ . , data = dat.train) R> cvr <- cvrisk(gam2) ## 25-fold bootstrap R> mstop(cvr) [1] 38 R> gam2 <- gam2[mstop(cvr)] ## set to optimal iteration R> mean((predict(gam2, dat.test) - dat.test$DEXfat)^2) [1] 3.901965 Early stopping controls smoothness of splines The stopping iteration of boosting algorithms does not only control the variable selection properties, but also the amount of shrinkage and the smoothness of eﬀect estimates. The function gamboost() per default implements cubic P-splines with second order diﬀerences, 20 inner knots and 4 degrees of freedom. However, as the same spline base-learner can be chosen and updated in various iterations (and the ﬁnal solution is the sum of those base- learner eﬀects), boosting can adapt to an arbitrarily higher-order smoothness and complexity [40, 38]. This will be demonstrated in a small simulated example. R> set.seed(1234) R> x <- runif(150, -0.2, 0.2) R> y = (0.5 - 0.9* exp(-50*x^2))*x + 0.02 *rnorm(150) R> y <- y[order(x)] ## order obs by size of x R> x <- x[order(x)] ## just for easier plotting 21 We simulated a clearly non-linear eﬀect of the predictor x on the response y. We now ﬁt step-by-step a simple univariate model via a P-spline base-learner and the classical L2 loss while plotting the model ﬁt at the diﬀerent iterations. Furthermore, we control how boosting is re-ﬁtting the residual reducing the empirical loss (sum of squares of residuals). R> par(mfrow = c(1,2)) ## two plots in one device R> R> ## model fit R> plot(x, y, las = 1, main = \"model fit at m = 1\" ) ## observations R> curve((0.5 - 0.9* exp(-50*x^2))*x, add=TRUE, from = -.2, + to = .2, lty = 2, lwd = 2) ## true function R> ## now carry out one boosting iteration R> gam1 <- gamboost(y ~ x, control = boost_control(mstop = 1)) R> lines(x , fitted(gam1), col = 2, lwd = 2) ## plot fitted values R> R> ## residual plot R> plot(x, y - fitted(gam1[1]) , ylab = \"residuals\", main = \"residuals at m = 1\", + ylim = c(-.1, .1), las = 1) ## residuals R> lines(smooth.spline(x, y - fitted(gam1)), + col = 4, lwd = 2) ## show remaining structure ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 1 xy ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 residuals at m = 1 xresidualsNow we repeat the same after 5 iterations. Note that we make use of the possibility to modify the number of iterations of an existing model by simple indexing gam1[5]. R> ## model fit R> plot(x, y, las = 1, main = \"model fit at m = 5\" ) R> curve((0.5 - 0.9* exp(-50*x^2))*x, add=TRUE, from = -.2, to = 0.2, + lty =2, lwd = 2) R> lines(x , fitted(gam1[5]), col = 2, lwd = 2) R> 22 R> ## residual plot R> plot(x, y - fitted(gam1[5]) , ylab = \"residuals\", main = \"residuals at m = 5\", + las = 1, ylim = c(-.1, .1)) R> lines(smooth.spline(x, y - fitted(gam1[5])), col = 4, lwd = 2) ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 5 xy ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 residuals at m = 5 xresiduals We then repeat this procedure for iterations mstop = 30 and mstop = 50. One can clearly observe how the model ﬁt (red line) adapts nicely to the true function (dashed line) and how the structure in the residual (blue line) disappears. ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 30 xy ● ● ● ● ●●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 residuals at m = 30 xresiduals 23 ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 50 xy● ● ● ● ●●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 residuals at m = 50 xresiduals To determine the optimal stopping iteration, we again perform 25-fold bootstrapping and choose the iteration that optimizes the predictive risk on the out-of-bootstrap observations (as implemented in cvrisk()). R> set.seed(123) R> cvr <- cvrisk(gam1, grid = 1:200) ## max mstop = 200 R> mstop(cvr) [1] 110 ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 110 xy● ● ● ● ●●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 residuals at m = 110 xresiduals Slow overﬁtting behavior Although the selection of the stopping iteration mstop is crucial to control variable selection, shrinkage and the overall smoothness of spline base-learners, it has been stated that statis- 24 tical boosing algorithms show rather slow overﬁtting behavior [5]. This can also be shortly demonstrated based on the simple simulation from above. While the optimal number of boosting iterations was mstop = 110 one could also analyse what happens if the algorithm is stopped much later. R> ## model fit mstop = 1000 R> plot(x, y, las = 1, main = \"model fit at m = 1000\" ) R> curve((0.5 - 0.9* exp(-50*x^2))*x, add=TRUE, from = -.2, to = 0.2, + lty =2, lwd = 2) R> lines(x , fitted(gam1[1000]), col = 2, lwd = 2) R> R> ## model fit mstop = 50000 R> plot(x, y, las = 1, main = \"model fit at m = 50000\" ) R> curve((0.5 - 0.9* exp(-50*x^2))*x, add=TRUE, from = -.2, to = 0.2, + lty =2, lwd = 2) R> lines(x , fitted(gam1[50000]), col = 2, lwd = 2) We therefore compare the model ﬁt for mstop = 1000 iterations and the one resulting from mstop = 50000 with the optimal model: The plotted curve for mstop = 1000 looks basically the same as the optimal one – although we used about 10 times as much iterations as necessary. This is a nice indication of the rather slow overﬁtting properties of boosting. However, with mstop = 50000 it gets clear that also boosting will eventually overﬁt – the resulting curve is deﬁnitely much to rough. ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 1000 xy ● ● ● ● ● ●● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● −0.2 −0.1 0.0 0.1 0.2 −0.10 −0.05 0.00 0.05 0.10 model fit at m = 50000 xy Likelihood-based boosting The application of likelihood-based boosting, as implemented in the GAMBoost package is in fact not very diﬀerent from gradient boosting in mboost. 25 The bodyfat data First, we shortly describe how to apply likelihood-based boosting on the bodyfat data [58], included in mboost. The ﬁtting functions are now called GLMBoost() and GAMBoost(), the number of stopping iteration can be speciﬁed via the stepno argument (as in mboost, the default is 100). The distribution for the likelihood is deﬁned by the family argument. The predictors must be provided by a n × p matrix. R> library(GAMBoost) ## load package R> library(mboost) ## for the data R> data(bodyfat) ## load data R> R> glm1 <- GLMBoost(y = bodyfat$DEXfat, + x = as.matrix(bodyfat[,c(\"hipcirc\",\"kneebreadth\",\"anthro3a\")]), + family = gaussian(), stepno = 100) R> summary(glm1) family: gaussian (with canonical link) model components: 0 smooth, 3 linear (with penalty 71) model fit: at final boosting step (step 100): residual deviance 838.7505, df 3.99, AIC (corrected) 3.6227, BIC 860.0212 at minimum AIC (corrected) (step 8): residual deviance 842.4226, df 2.72, AIC (corrected) 3.5859, BIC 858.2864 at minimum BIC (step 10): residual deviance 839.1285, df 2.86, AIC (corrected) 3.5863, BIC 855.5622 fitted covariates: at final boosting step (step 100): intercept: 30.7828 3 non-zero estimates for linear terms: hipcirc (5.6476), kneebreadth (1.7377), anthro3a (4.2148) at minimum AIC (corrected) (step 8): intercept: 30.7828 3 non-zero estimates for linear terms: hipcirc (5.6187), kneebreadth (1.7058), anthro3a (4.1334) at minimum BIC (step 10): intercept: 30.7828 3 non-zero estimates for linear terms: hipcirc (5.6526), kneebreadth (1.7058), anthro3a (4.1933) The function summary() not only provides some information on the model ﬁt and the co- eﬃcients but already includes the optimal stopping iteration regarding the AIC (corrected version by Hurvich et al. [59]) and the BIC. The function cv.GLMBoost can be used to select the number of boosting steps by cross- validation: R> set.seed(123) R> cv.glm1 <- cv.GLMBoost(y = bodyfat$DEXfat, + x = as.matrix(bodyfat[,c(\"hipcirc\",\"kneebreadth\",\"anthro3a\")]), + family = gaussian(), maxstepno = 100) R> cv.glm1$selected 26 [1] 99 R> glm1 <- GLMBoost(y = bodyfat$DEXfat, + x = as.matrix(bodyfat[,c(\"hipcirc\",\"kneebreadth\",\"anthro3a\")]), + family = gaussian(), stepno = cv.glm1$selected) For ﬁtting generalized additive models, the equivalent GAMBoost functions can be used: R> set.seed(123) R> cv.gam1 <- cv.GAMBoost(y = bodyfat$DEXfat, + x = as.matrix(bodyfat[,c(\"hipcirc\",\"kneebreadth\",\"anthro3a\")]), + family = gaussian(), maxstepno = 100, just.criterion=TRUE) R> cv.gam1$selected [1] 25 R> gam1 <- GAMBoost(y = bodyfat$DEXfat, + x = as.matrix(bodyfat[,c(\"hipcirc\",\"kneebreadth\",\"anthro3a\")]), + family = gaussian(), stepno = cv.gam1$selected) The plot method provides the ﬁtted functions together with approximate 95% conﬁdence intervals: R> par(mfrow=c(1,3)) R> plot(gam1) 90 100 110 120 130−10−505 hipcirceta 8 9 10 11−10−505 kneebreadtheta 2.5 3.0 3.5 4.0 4.5−10−505 anthro3aetaLikelihood-based boosting for the Cox model: CoxBoost The CoxBoost package implements the likelihood-based boosting approach for the Cox proportional hazard model. We will apply CoxBoost for the prediction of survival after chemotherapy for diﬀuse large-B-cell lymphoma (DLBCL), based on a data set provided by Rosenwald et al. [60]. The main outcome is a potentially censored survival time from 240 patients with a median follow up of 2.8 years. In total, 57% of the patients died during that time. As possible molecular predictors, there are 7399 microarray features available. As clinical predictors, the International Prognostic Index (IPI) is available for 222 patients (compare to [47]). The data set can be downloaded from the Web: 27 R> ## The DLBCL data of Rosenwald et al. (2002) R> pat <- read.delim(\"http://llmpp.nih.gov/DLBCL/DLBCL_patient_data_NEW.txt\") R> intens <- read.delim(\"http://llmpp.nih.gov/DLBCL/NEJM_Web_Fig1data\") Pre-processing of the Rosenwald data set Before the data set can be applied, some pre-processing is necessary (including imputing missing values by mean of nearest neighbours, see [61]). These procedures are not boosting related and will not be explained in detail. R> ## Preprocessing of DLBCL data: R> #------------------------ LONG RUNTIME (ca 30 minutes) ------------------------ R> geneid <- intens$UNIQID R> patid <- names(intens)[3:ncol(intens)] R> exprmat <- t(intens[,3:ncol(intens)]) R> colnames(exprmat) <- paste(\"ID\",geneid,sep=\"\") R> exprmat <- exprmat[substr(patid, nchar(patid)-8, nchar(patid)) == \"untreated\",] R> patid <- rownames(exprmat) R> exprmat <- exprmat[match(pat$\"DLBCL.sample..LYM.number.\", + as.numeric(substr(patid, regexpr(\"LYM\",patid)+3, + regexpr(\"LYM\",patid)+5))),] R> train <- pat$\"Analysis.Set\" == \"Training\" R> R> # simplistic median imputation R> exprmat <- ifelse(is.na(exprmat),matrix(apply(exprmat,2,median,na.rm=TRUE), + nrow(exprmat),ncol(exprmat),byrow=TRUE),exprmat) R> R> obs.time <- pat$\"Follow.up..years.\" R> obs.status <- ifelse(pat$\"Status.at.follow.up\" == \"Dead\",1,0) R> R> ##Use only patients, where IPI is available: R> IPI <- pat$IPI.Group R> IPI.available <- ifelse(is.na(IPI),FALSE,ifelse(IPI == \"missing\", F, T)) R> IPI <- IPI[IPI.available] R> obs.time <- obs.time[IPI.available] R> obs.status <- obs.status[IPI.available] R> exprmat <- exprmat[IPI.available,] Model ﬁtting First, we prepare the predictor matrix xmat, which contains both the molecular data and clinical predictors, and a vector unpen.index that indicates the position of the covariates that should receive unpenalized estimates (in this case the clinical predictors). R> IPI.medhigh <- ifelse(IPI != \"Low\",1,0) R> IPI.high <- ifelse(IPI == \"High\",1,0) R> xmat <- cbind(IPI.medhigh,IPI.high,exprmat) R> colnames(xmat) <- c(\"IPImedhigh\",\"IPIhigh\",colnames(exprmat)) R> unpen.index <- c(1,2) 28 Afterwards, we specify the penalty such as to obtain a step size of 0.1, similar to gradient boosting, as well as the number of boosting iterations: R> nu <- 0.1 R> penalty <- sum(obs.status)*(1/nu-1) R> stepno <- 50 Finally we ﬁt a Cox model by the function CoxBoost() with and without adjusting for the IPI score. R> ## all predictors, including IPI: xmat R> cb1.IPI <- CoxBoost(time = obs.time, status = obs.status, xmat, + unpen.index = unpen.index, stepno = stepno, + penalty = penalty, standardize = TRUE, trace = FALSE) R> R> ## only molecular data: exprmat R> cb1 <- CoxBoost(time = obs.time, status = obs.status, exprmat, + stepno = stepno, standardize = TRUE, trace = FALSE) Inspecting the selected genes R> summary(cb1.IPI) 50 boosting steps resulting in 26 non-zero coefficients (with 2 being mandatory) partial log-likelihood: -558.12 Parameter estimates for mandatory covariates at boosting step 50: Estimate IPImedhigh 0.9849 IPIhigh 0.8273 Optional covariates with non-zero coefficients at boosting step 50: parameter estimate > 0: IPImedhigh, IPIhigh, ID31242, ID31981, ID34546, ID26474, ID34344, ID30931, ID24530, ID32302, ID32238, ID17385, ID33312 parameter estimate < 0: ID29871, ID27774, ID17154, ID29847, ID28325, ID24394, ID33157, ID19279, ID24376, ID16359, ID17726, ID20199, ID32679 R> summary(cb1) 50 boosting steps resulting in 28 non-zero coefficients partial log-likelihood: -577.2544 Optional covariates with non-zero coefficients at boosting step 50: parameter estimate > 0: ID24980, ID32424, ID28925, ID31242, ID34805, ID31981, ID31669, ID29176, ID17733, ID24400, ID34344, ID30634, ID29657, ID31254, ID33358, ID32238, ID34376 parameter estimate < 0: ID27774, ID24394, ID19279, ID24376, ID28641, ID27267, ID25977, ID29797, ID20199, ID28377, ID32679 29 Now we can analyse the overlap between the two Cox models: R> intersect(names(coef(cb1.IPI)[coef(cb1.IPI) != 0]), + names(coef(cb1)[coef(cb1) != 0])) [1] \"ID27774\" \"ID31242\" \"ID31981\" \"ID24394\" \"ID19279\" \"ID24376\" \"ID34344\" [8] \"ID32238\" \"ID20199\" \"ID32679\" The coeﬃcient paths, i.e. parameter estimates plotted against the boosting steps, show a marked diﬀerence due to adjusting for the clinical covariates: R> par(mfrow=c(1,2)) R> plot(cb1) R> plot(cb1.IPI) 0 1020304050−0.15−0.10−0.050.000.050.100.15 boosting stepestimated coefficients ID24980ID32424ID28925 ID27774 ID31242 ID34805 ID31981 ID31669ID29176ID17733 ID24394 ID19279 ID24376 ID24400 ID28641 ID27267 ID25977 ID34344 ID30634ID29657 ID29797 ID31254ID33358 ID32238 ID34376 ID20199 ID28377 ID32679 0 1020304050−0.2−0.10.00.10.2 boosting stepestimated coefficients ID29871 ID27774 ID31242 ID31981 ID34546 ID17154 ID29847 ID28325 ID24394 ID33157 ID19279 ID24376 ID16359 ID17726 ID26474 ID34344ID30931ID24530ID32302 ID32238 ID17385 ID33312 ID20199 ID32679 Early stopping Similar to the packages mboost and GAMBoost, the CoxBoost package provides auto- mated functionss to select the optimal stopping iteration. In our case, we will apply the function cv.CoxBoost() to determine the optimal value based on 10-fold cross-validation. R> set.seed(123) R> cv1 <- cv.CoxBoost(time = obs.time, status = obs.status, x = xmat, + unpen.index = unpen.index, maxstepno = 200, K = 10, + standardize = TRUE, trace = TRUE, penalty = penalty, + multicore = FALSE) 30 If the package parallel is available, one can specify multicore = TRUE leading to auto- mated parallel computing (if the machine contains more than one core). The optimal value is then the one optimizing the average likelihood on the 10 test folds: R> cv1$optimal.step [1] 35 Now we can compute our ﬁnal Cox model. The trace option provides a live view of the gene selection in the boosting steps: R> cb1.IPI.cv <- CoxBoost(time = obs.time, status = obs.status, xmat, + unpen.index = unpen.index, stepno = cv1$optimal.step, + penalty = penalty, standardize = TRUE, trace = TRUE) ID27774 ID27774 ID31981 ID31242 ID27774 ID31981 ID24376 ID32238 ID27774 ID31242 ID31981 ID24394 ID33312 ID32679 ID29871 ID31981 ID17154 ID33312 ID31242 ID24394 ID19279 ID31981 ID27774 ID32238 ID32679 ID24394 ID31981 ID32302 ID34344 ID31242 ID16359 ID32679 ID26474 ID28325 ID32238 31","libVersion":"0.3.2","langs":""}
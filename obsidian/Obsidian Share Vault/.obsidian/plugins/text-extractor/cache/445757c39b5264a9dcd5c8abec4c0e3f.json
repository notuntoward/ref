{"path":"Politics/Political Causality/Merged RAG Political Sources/merged_RAG_5.pdf","text":"Group 5 2025-01-23 19:37:19.998631 Misperceptions, Depression, and Voting for Election Deniers in the United States Author: Baum, Matthew A, Date: 2024-06-01 Collections: NeuroPsychoLinguisticPolitics Zotero Key: KDZVWH24 Cite Key: Baum24depressionElecDenial Zotero Item | Lit Note Forthcoming, International Journal of Public Opinion Research Misperceptions, Depression, and Voting for Election Deniers in the United States[1] Matthew A. Baum matthew_baum@hks.harvard.edu Harvard University John F. Kennedy School of Government 79 JFK St. Cambridge, MA 02138 617-495-1291 James N. Druckman (corresponding author) j.druckman@rochester.edu University of Rochester Department of Political Science 333 Harkness Hall Rochester, NY 14627-0146 585-275-4291 Katherine Ognyanova katya.ognyanova@rutgers.edu School of Communication & Information Rutgers University 4 Huntington Street New Brunswick, NJ 08901 848-932-8833 Jonathan Schulman JonathanSchulman2023@u.northwestern.edu Northwestern University Department of Political Science Scott Hall 601 University Place Evanston, IL 60208 847-491-7450 Matthew A. Baum is the Marvin Kalb Professor of Global Communications at the Harvard Kennedy School. James N. Druckman is a Professor of Political Science at the University of Rochester. Katherine Ognyanova is an Associate Professor of Communication at Rutgers University. Jonathan Schulman is a Ph.D. candidate in Political Science at Northwestern University. 1 We thank Jenny Mansbridge for insightful comments. This work was supported by the National Science Foundation under grants SES-2029292, SES-2029297, and SES-2116645, and the Peter G. Peterson Foundation. Misperceptions, Depression, and Voting for Election Deniers in the United States Abstract Two of the most significant concerns about the contemporary United States are the erosion of democratic institutions and the high rate of depression. We provide evidence connecting these phenomena. We use a survey (N=11,517) to show a relationship between misperceptions (about COVID-19 vaccines) and voting, in 2022, for gubernatorial candidates who denied or cast doubt on the legitimacy of the 2020 election results. We further predict and find that the presence of moderately-severe-to-severe depressive symptoms exacerbates the relationship between misperceptions and voting for election deniers or doubters. The results offer insight into the links between misperceptions, depression, and democratic backsliding (i.e., supporting candidates who challenge election results). We also contribute to a growing line of research on how mental health affects democratic functioning, potentially worldwide. Keywords: Misperceptions, Misinformation, Depression, Election Denial, Democratic Backsliding Democracy, at a minimum, requires the peaceful transfer of power based on election results. That did not occur in the U.S. in 2020, given the violent insurrection on January 6[th], 2021. Concerns remained in 2022 with gubernatorial candidates in 28 states (of the 36 states holding gubernatorial elections) denying outright or casting doubt on, without credible evidence, the legitimacy of the 2020 election (PBS, 2022). This is concerning given that states have, until recently, been seen as laboratories of democracy, but now may instead be domains of backsliding (Rocco, 2021). Much of the literature on democratic erosion in the U.S. focuses on partisan motivations (Finkel et al., 2020), often pointing to anti-democratic tendencies among Republican elites (Grumbach, 2022). Indeed, all the major party election deniers and doubters on the 2022 gubernatorial ballot were Republicans. The focus on partisanship, even asymmetrically, is sensible but incomplete. Other factors likely shape anti-democratic behaviors. Here, we explore the relationship between holding misperceptions in a related domain, COVID-19, and supporting an election denying or doubting (for convenience, henceforth “denying”) gubernatorial candidate in 2022. We also theorize and test whether exhibiting moderately-severe-to-severe depressive symptoms (for convenience, henceforth “severe”) moderates the correlation between COVID-19 misperceptions and supporting an election denying candidate. Our results provide evidence that misperceptions, depression, and their interaction can have negative democratic consequences. Misperceptions and Depression Misperceptions refer to “cases in which people’s beliefs about factual matters are not supported by clear evidence and expert opinion – a definition that includes both false and unsubstantiated beliefs about the world” (Nyhan and Reifler, 2010, p. 305). Substantial research shows that misperceptions relate to maladaptive attitudes and behaviors such as ignoring health advice (e.g., masking, vaccination) (Jolley and Douglas, 2014; Oliver and Wood, 2014b; Romer and Jamieson, 2020), dismissing environmental threats (van der Linden, 2015), displaying hostility toward out-groups including political adversaries (Kennedy and Pronin, 2008), and supporting political violence (Baum et al., 2023). In their synthesis of COVID-19 policy relevant research, Ruggeri et al. (2024) review 60 articles on misperceptions and report clear evidence of a relationship with vaccine hesitancy. A stream of work also reveals a relationship between misperceptions and vote choice (Gunther et al., 2019; Weeks and Garrett, 2014). Negative misperceptions about a candidate can straightforwardly lead one to vote against that candidate. For instance, in 2016, many past Obama voters who believed incorrect negative information about Clinton tended to defect to supporting Trump or refrain from voting (Gunther et al., 2019). Generally, politically relevant misperceptions vary as to whether they have partisan content (Enders and Uscinski, 2021, p. 586). When they do, misperceptions can straightforwardly influence support for a particular candidate from a given side, such as misperceptions about illegal Latino voters generating support for conservative candidates who embrace strict immigration policies (Smith, 2017). When misperceptions do not have partisan content, they can promote support for a candidate who shares the general predisposition invoked by the misperceptions. For example, some non partisan conspiratorial misperceptions, like the belief that the people who really “run” the country are not known to voters, correlate with voting for Trump, consistent with his embrace of the “deep state” (Enders and Uscinski, 2021; also see Oliver and Wood, 2014a; Sutton and Douglas, 2020).[1] These examples make clear that misperceptions not directly about candidates can still influence vote choice. Both the partisan and non-partisan logics suggest a potential connection between 1) COVID-19 vaccine misperceptions (e.g., vaccines alter people’s DNA) and 2) voting for candidates who challenged the legitimacy of the 2020 election. Republican elites endorsed misinformation about vaccines and all the 2022 denying candidates were Republican (e.g., Annenberg IOD Collaborative, 2023; Jones and McDermott, 2022). Additionally, both COVID 19 vaccine misperceptions and supporting a denying candidate invoke suspicions of elites, be they scientific or government elites (see Jennings et al., 2023). Voting for an election denying candidate enables one to act on their displeasure with the “system” (Chen et al., 2021; Enders et al., 2023). Hypothesis 1: Relative to those who do not, those holding misperceptions about COVID- 19 vaccines will be more likely to support a candidate who questioned the legitimacy of the 2020 election, all else constant. Just as there exists notable heterogeneity in who holds misperceptions based on psychological differences such as authoritarianism and narcissism (e.g., Cichocka et al., 2016; Douglas, 2021; Druckman et al., 2021), some people will be more likely than others to connect their (vaccine) misperceptions to their voting behavior (e.g., Ahmed and Tan, 2022; Rottweiler and Gill, 2020). Indeed, there is considerable work on heterogeneous reactions to misperceptions (Douglas, 2021). We focus here on variation stemming from depression, a common mood disorder where an individual experiences a persistent feeling of sadness and hopelessness and/or loses interest in most activities. The Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), states that a diagnosis of a severe depressive episode is appropriate when an individual experiences five or more of nine identified symptoms for at least two weeks. These include a depressed mood; diminished interest in daily activities; significant weight loss or gain; sleep difficulties; slowing of thought and reduction of physical movement; fatigue; feelings of worthlessness or excessive guilt; diminished ability to concentrate or make decisions; and recurrent suicidal ideation. Depression is the most prevalent mental health disorder in advanced societies (Lépine and Briley, 2011) and it increased nearly four-fold in the U.S. during the COVID-19 pandemic (Ettman et al., 2022). Depression is often accompanied by a sense of a loss of control (e.g., Cheng et al., 2013; Rehm, 1977). Lower control, in turn, increases the link between misperceptions and taking non normative political actions (Douglas, 2021, p. 2). Rottweiler and Gill (2022) show this is the case for conspiracy beliefs and violent intentions. We extend the idea to COVID-19 misperceptions and voting for an election denier. To be clear, depression on its own often quells action (Landwehr and Ojeda, 2021); however, when conjoined with misperceptions that offer a culprit – in this case, societal institutions – individuals will come to view actions against the threatening culprit (i.e., institutions) as a way to regain control (Moulding et al., 2016). Here, that means supporting those who challenge the institutions. Depression could exhibit this relationship via either of the aforementioned routes (e.g., among Republicans, given elite cues against science and electoral institutions, or among those with connected anti-system inclinations). Hypothesis 2: The relationship in Hypothesis 1 will manifest more strongly among those who suffer from severe depression, relative to those who do not, all else constant. Data We focus our analyses on gubernatorial elections. As mentioned, 36 states had such contests in 2022 and election deniers ran in 28 of those states. To obtain a suitable sample, we relied on the PureSpectrum survey recruitment platform, which aggregates and deduplicates paid panelists from multiple online survey sources. Though not a probability sample, the large scale of the sample and its demographic breadth provides the necessary flexibility for including quotas for gender, race, and age at the state level and reweighting of observations to match official U.S. Census figures. Emerging evidence suggests this methodology can perform as well as traditional probability sampling (Enns and Rothschild, 2021; Lazer et al., 2020; Lehdonvirta et al., 2021). We collected our data between October 6 and November 9, 2022. Our analyses include eligible voters who lived in a state with at least one election denier on the gubernatorial ballot (N=11,517) (see the appendix for sample demographics). As explained, we study misperceptions about COVID-19 vaccines. We selected five misperceptions based on Google searches for prevalent misinformation at the time and the Centers for Disease Control’s website area on common myths. They include assertions that the COVID-19 vaccines: alter people’s DNA; contain microchips that can track people; contain the lung tissue of aborted fetuses; can cause infertility, making it more difficult to get pregnant; and contain a bioluminescent marker used to trace people. We created a scale by counting the number of misperceptions endorsed (normalized on a 0-1 interval) (α = .79). We identified the stances of gubernatorial candidates from coding by the PBS Newshour (PBS, 2022) that identified the 28 (all Republican) gubernatorial candidates who publicly either claimed outright that the 2020 election was stolen or fueled doubts about the integrity of the 2020 election. Respondents were asked which candidate they supported in their particular state from a list of all candidates on the ballot in the given race. Our dependent variable is voting for an election denying or doubting candidate, coded 0 for no and 1 for yes (i.e., “supported” [prior to voting] or “voted for” [if they had already voted] the candidate who disputed the 2020 election result). As noted, we use the shorthand of “denying” for these candidates. We measured depressive symptoms with the Patient Health Questionnaire (PHQ-9), which is used to screen patients in primary care settings (Arroll et al., 2010) and has been validated for use in general population surveys (Kocalevent et al., 2013). Each of the nine items (listed above and in the appendix) asks about symptoms on a 0 (not at all) to 3 (nearly every day) scale, which is then summed for a total score ranging from 0 to 27. We employed the common threshold of 15 (/27) to indicate moderately-severe-to-severe depression, which, as noted, we refer to as “severe” (Kroenke and Spitzer, 2002). We use this as our operationalization of “depression.” In our sample, 13% of individuals exhibited severe depressive symptoms. The survey also measured various control variables that we include in all analyses: party identification, political interest, age, education, income, ideology, election confidence, race/ethnicity, COVID-19 vaccination status (e.g., whether the respondent was vaccinated), support for political violence ever or now, frequency of discussing COVID-19, following political news, voting turnout, and gender. The appendix provides details on all question wordings. Results We evaluate our hypotheses with logit models that regress supporting an election denier on the key variables and controls. Table A.1 in the appendix presents the regression results. Below, we present the results with figures depicting the relevant relationships. We use Clarify (King et al., 2000) to transform the logit coefficients into probabilities of voting for a denying candidate, while setting other variables to their mean values. Figure 1’s top panel (from Table A.1, Model 1) reveals a moderate relationship between misperceptions and the likelihood of supporting an election denying candidate. As COVID-19 vaccine misperceptions increase from the minimum to the maximum values, the probability of supporting/voting for a denier increases by 10 percentage points (.16 to .26; p<.01). This suggests, as predicted by Hypothesis 1, that holding misperceptions (about COVID-19 vaccines) is associated with a significantly higher likelihood of supporting a candidate who questions the legitimacy of the government. Figure 1’s bottom panel (from Table A.1, Model 2) offers evidence supporting Hypothesis 2. Among respondents who do not suffer from severe depression, variations in misperceptions have some association with the likelihood of voting for deniers (a maximum 7 point increase in likelihood of voting for a denier; p<.10). However, among severely depressed respondents, we find a far stronger relationship between misperceptions and vote choice: a more than threefold increase in the likelihood of supporting/voting for a denier (29 percentage points, from .13 to .42; p<.01). The difference in the magnitudes of the effects of misperceptions on the probability of voting for election deniers between those with and without severe depression is itself significant (p<.05). This vote model is robust to adding belief that Trump won in 2020 as an independent variable (see the Table A.2 in the appendix). In short, the presence of severe depression correlates with up to a 19-point increase in the likelihood of supporting a candidate for executive office (governor) who denies the legitimacy of what was, by all legal accounts, a sound election. This is certainly enough to swing a close election. [Insert Figure 1 Here] While we cannot make definitive causal statements about the relationships, we offer two re-assuring robustness checks, detailed in the appendix. First, a small sub-sample of respondents from states with deniers on the gubernatorial ballot were empaneled from a prior wave of the survey conducted in August/September 2022. Our results hold when we operationalize depression as the change from the prior wave to the present wave, suggesting depression preceded casting a vote. Second, we confirmed there is no evidence that the electoral realities of Republican candidates’ underperforming affected Republicans’ depression levels. Effects By Partisanship We next look for the same relationships for Republicans, Democrats, and independents separately (i.e., within each partisan group). This provides insight into the mechanism a la our prior discussion regarding partisan or anti-system perspectives. Among eligible voters in states with a denier on the ballot, the percentage voting for a denying candidate, by party, are: 4% for Democrats, 23% for independents, and 77% for Republicans.[2] In Figure 2, we present analyses equivalent to Figure 1, but separately for each partisan group. We find a significant relationship between misperceptions and voting for a denier among Democrats and independents (respectively, Figure 2a, from Table A.1, Model 3; and Figure 2c, Table A.1, Model 7). Consistent with the earlier percentages, independents exhibit a higher likelihood of voting for a denying candidate, relative to Democrats. Interestingly, we do not find a significant relationship between misperceptions and voting for a denier among Republicans (Figure 2e, from Table A.1, Model 5). These results suggest that the connection between the COVID-19 misperceptions and voting for a denier does not simply stem from Republicans with misperceptions supporting Republicans. As mentioned, there may be an underlying anti-system belief coherence driving the relationship. Yet, when we turn to Hypothesis 2, it is only statistically supported for Republicans (Figure 2f from Table A.1, Model 6). Republicans with neither misperceptions nor depression are surprisingly more likely to vote for a denier than their counterparts with depression (.64 versus .51). This relationship quickly flips as the number of misperceptions increases. By the maximum, those with severe depression exhibit a .85 probability of voting for a denier versus a .61 probability for those without depression: a statistically significant (p<.01) 24-percentage point difference. In contrast, the moderating role of depression among Democrats and independents is directionally as expected but not statistically significant. In both cases, there are notable increases in the likelihood of voting for a denying candidate among those severely depressed versus those not; at the maximum misperception point, there are respective 11 and 10 percentage point differences between those with and without severe depression (Figures 2b and 2d from Table A.1, Model 4 and Model 8). Yet, the confidence intervals for both groups are so large that the interaction falls short of significance and thus the evidence does not support Hypothesis 2.[3] This mix of results highlights the possible role of partisan belief systems such that Republicans seeking control (due to depression) connect and act in a partisan consistent way. Regardless, the strong Republican results along with the directionally consistent findings for Democrats and independents suggest that future work exploring the relationship between mental health and democracy is worthwhile. [Insert Figure 2 Here] Discussion Our results add to a robust literature on misperceptions and their consequences. We offer evidence that widely circulating misperceptions that appeal to common identities or dispositions link to one another. The findings raise as many, if not more, questions as they answer. First, we suggested the connecting mechanism could be either shared partisanship or an anti-system perspective. We lack the data to directly differentiate the two, and our analyses paint a mixed picture with Democrats and independents exhibiting clear evidence of the main misperception denier link and Republicans exhibiting the depression moderation. Our study was a difficult test given extreme polarization (Finkel et al., 2020) and the reality that the partisan and anti-system inclinations substantially overlapped (i.e., both constructs were Republican and anti-system). More work is needed, including that which incorporates measures of anti-establishment beliefs and related psychological constructs, particularly those that relate to misperceptions generally (Douglas, 2021). Second, we recognize our findings are limited to one case at one point in time using particular measures; as mentioned, we offer some data about causal sequence (in the appendix), but we cannot definitively offer a casual conclusion. Indeed, the beliefs we study are endogenous to contemporary politics and partisan rhetoric. The relative role of elite (partisan and/or anti system) signaling and psychological inclinations (for misperceptions) remains unclear. It also is important to extend analyses to other countries, as election denial is far from a U.S.-only phenomenon (e.g., Marie Le Pen in the 2022 French presidential election, Jair Bolsonaro in the 2022 Brazilian presidential election). Third, we opted for a clinically used measure of depressive symptoms (Kocalevent et al., 2013). Future work would benefit from more exploration of its use in survey settings as well as the role of related psychological variables (e.g., loneliness) that could serve as moderators. Our results also add to a recent stream of findings about how depression can have negative political consequences. This includes reducing the probability of voting by diminishing political motivation and physical energy (Landwehr and Ojeda, 2021), unfairly penalizing candidates who reveal depressive symptoms (Loewen and Rheault, 2021), leading directly to misperceptions (Green et al., 2022), and exacerbating (moderated) support for political violence (Baum et al., 2023). Insofar as low participation, voting based on tangential criteria, misperceptions, violence, and now – as we show – possibly supporting candidates who deny the legitimacy of elections, each contributes to backsliding, a country suffering from poor mental health may find itself confronting poor political health. References Ahmed, S., & Tan, H. W. (2022). Personality and perspicacity: Role of personality traits and cognitive ability in political misinformation discernment and sharing behavior. Personality and Individual Differences, 196, 111747. https://doi.org/10.1016/j.paid.2022.111747 Annenberg IOD Collaborative. (2023). Democracy amid crises. Oxford University Press. Arroll, B., Goodyear-Smith, F., Crengle, S., Gunn, J., Kerse, N., Fishman, T., Falloon, K., & Hatcher, S. (2010). Validation of PHQ-2 and PHQ-9 to screen for major depression in the primary care population. The Annals of Family Medicine, 8, 348–53. Bartels, L. M., & Carnes, N. (2023). House Republicans were rewarded for supporting Donald Trump’s ‘stop the steal’ efforts. Proceedings of the National Academy of Sciences, 120(34), e2309072120. Baum, M., Druckman, J.N., Simonson, M.D., Lin, J. & Perlis, R.H. (2023). The political consequences of depression. American Journal of Political Science, September. https://doi.org/10.1111/ajps.12827. Chen, E., Chang, H., Rao, A., Lerman, K., Cowan, G., & Ferrara, E. (2021). COVID-19 misinformation and the 2020 US presidential election. The Harvard Kennedy School Misinformation Review. https://doi.org/10.37016/mr-2020-57 Cheng, C., Cheung, S., Hin-man Chio, J., & Chan, M.S. (2013). Cultural meaning of perceived control. Psychological Bulletin 139(1), 152-88. Cichocka, A., Marchlewska, M., Golec de Zavala, A., & Olechowski, M. (2016). ‘They will not control us’: Ingroup positivity and belief in intergroup conspiracies. British Journal of Psychology, 107(3), 556–576. https://doi.org/10.1111/bjop.12158 Douglas, K. (2021). Are conspiracy theories harmless? The Spanish Journal of Psychology, 24, E13. doi:10.1017/SJP.2021.10 Druckman, J. N., Ognyanova, K., Baum, M. A., Lazer, D., Perlis, R. H., Volpe, J. D., Santillana, M., Chwe, H., Quintana, A., & Simonson, M. (2021). The role of race, religion, and partisanship in misperceptions about COVID-19. Group Processes & Intergroup Relations, 24(4), 638–657. https://doi.org/10.1177/1368430220985912 Enders, A., Klofstad, C., Stoler, J., & Uscinski, J. E. (2023). How anti-social personality traits and anti-establishment views promote beliefs in election fraud, QAnon, and COVID-19 conspiracy theories and misinformation. American Politics Research, 51(2), 247 59. https://doi.org/10.1177/1532673X221139434 Enders, A. & Uscinski, J. (2021). The role of anti-establishment orientations during the Trump presidency. The Forum, 19(1), 47-76. https://doi.org/10.1515/for-2021-0003 Enns, P.K., & Rothschild, J. (2021). Revisiting the “gold standard” of polling.” 3Streams, April 12. https://medium.com/3streams/revisiting-the-gold-standard-of-polling-new-methods outperformed-traditional-ones-in-2020-451650a9ba5b Ettman, C.K., Cohen, G.H., Abdalla, S.M., Sampson, L., Trinquart, L., Castrucci, B.C., Bork, R.H. et al. (2022). Persistent depressive symptoms during COVID-19. The Lancet Regional Health – Americas, 5(January), 100091. https://doi.org/10.1016/j.lana.2021.100091 Finkel, E.J., Bail, C.A., Cikara, M., Ditto, P.H., Iyengar, S., Klar, S., Mason, L. et al. (2020). Political sectarianism in America. Science 370(6516), 533–36. Gelman, A. (2018). You need 16 times the sample size to estimate an interaction than to estimate a main Eefect.” Statistical modeling, causal inference, and social science. Retrieved October 3, 2023 (https://statmodeling.stat.columbia.edu/2018/03/15/need16/). Green, J., Druckman, J.N., Baum, M., Lazer, D., Ognyanova, K., & Perlis, R.H. (2022). Depressive symptoms and conspiracy beliefs.” Applied Cognitive Psychology, 37(2), 332–59. Grumbach, J.M. (2022). Laboratories against democracy. Princeton University Press. Gunther, R., Beck P.A., & Nisbet Erik C. (2019). “Fake news” and the defection of 2012 Obama voters in the 2016 presidential election. Electoral Studies, 61, 102030. https://doi.org/10.1016/j.electstud.2019.03.006. Jacobson, G.C. (2023). The 2022 elections. Political Science Quarterly, 138(1), 1-22. Jennings, W., Valgarðsson, V., McKay, L., Stoker, G., Mello, E., & Baniamin, H. M. (2023). Trust and vaccine hesitancy during the COVID-19 pandemic: A cross-national analysis. Vaccine: X, 14, 100299. https://doi.org/10.1016/j.jvacx.2023.100299 Jolley, D., & Douglas, K. M. (2014). The Effects of Anti-Vaccine Conspiracy Theories on Vaccination Intentions. PLOS ONE, 9(2), e89177. https://doi.org/10.1371/journal.pone.0089177 Jones, D.R., & McDermott, M.L. (2022). Partisanship and the politics of COVID vaccine hesitancy. Polity, 54(3), 408–34. Kennedy, K. A., & Pronin, E. (2008). When Disagreement Gets Ugly: Perceptions of Bias and the Escalation of Conflict. Personality and Social Psychology Bulletin, 34(6), 833–848. https://doi.org/10.1177/0146167208315158 King, G., Tomz, M., & Wittenberg, J. (2000). Making the most of statistical analyses. American Journal of Political Science, 44, 347–61. Kocalevent, R., Hinzm A., Brähler, E., (2013). Standardization of the depression screener Patient Health Questionnaire (PHQ-9) in the general population. General Hospital Psychiatry, 35(5), 551-5. Kroenke, K., & Spitzer, R.L. (2002). The PHQ-9. Psychiatric Annals, 32, 509–15. Landwehr, C., & Ojeda, C. (2021). Democracy and depression. American Political Science Review, 115, 323–30. Lazer, D., Baum, M., Ognyanova, K., Volpe, J. D., Perlis, R., Druckman, J., Santillana, M., Green, J., Quintana, A., Simonson, M. D., Lin, J., Uslu, A., Gitomer, A., Trujillo, K. L., Safarpour, A., Qu, H., Pippert, C. H., & Radford, J. (2020). Validating the COVID States Method: A comparison of non-probability and probability-based survey methods. The COVID States Project. https://osf.io/qxez5 Lehdonvirta, V., Oksanen, A., Räsänen, P. & Blank. G. (2021). Social media, web, and panel surveys. Policy & Internet 13, 134–55. Lépine, Jean., & Briley, M. (2011). The increasing burden of depression. Neuropsychiatric Disease and Treatment, 7(Suppl 1), 3–7. Loewen, P. J., & Rheault, L. (2021). Voters Punish Politicians with Depression. British Journal of Political Science, 51(1), 427–436. https://doi.org/10.1017/S0007123419000127 Moulding, R., Nix-Carnell, S., Schnabel, A., Nedeljkovic, M., Burnside, E. E., Lentini, A. F., & Mehzabin, N. (2016). Better the devil you know than a world you don’t? Intolerance of uncertainty and worldview explanations for belief in conspiracy theories. Personality and Individual Differences, 98, 345–354. https://doi.org/10.1016/j.paid.2016.04.060 Nyhan, B., & Reifler, J. (2010). When Corrections Fail: The Persistence of Political Misperceptions. Political Behavior, 32(2), 303–330. https://doi.org/10.1007/s11109-010 9112-2 Oliver, J. E., & Wood, T. J. (2014a). Conspiracy Theories and the Paranoid Style(s) of Mass Opinion. American Journal of Political Science, 58(4), 952–966. https://doi.org/10.1111/ajps.12084 Oliver, J. E., & Wood, T. (2014b). Medical Conspiracy Theories and Health Behaviors in the United States. JAMA Internal Medicine, 174(5), 817–818. https://doi.org/10.1001/jamainternmed.2014.190 PBS. (2022). Live Results: Tracking GOP election deniers on the ballot. PBS NewsHour. https://www.pbs.org/newshour/elections-2022/gop-election-deniers Rehm, L. P. (1977). A self-control model of depression. Behavior Therapy, 8(5), 787–804. https://doi.org/10.1016/S0005-7894(77)80150-0 Rocco, P. (2021). Laboratories of What?. In Lieberman, R.C., Settler, S., & Roberts, K.M., (Eds.), Democratic resilience. Cambridge University Press. Romer, D., & Jamieson, K. H. (2020). Conspiracy theories as barriers to controlling the spread of COVID-19 in the U.S. Social Science & Medicine, 263, 113356. https://doi.org/10.1016/j.socscimed.2020.113356 Rottweiler B., & Gill, P. (2022). Conspiracy beliefs and violent extremist intentions, 34(7), 1485-1504. DOI: 10.1080/09546553.2020.1803288 Ruggeri, K., Stock, F., Haslam, S. A., Capraro, V., Boggio, P., Ellemers, N., Cichocka, A., Douglas, K. M., Rand, D. G., van der Linden, S., Cikara, M., Finkel, E. J., Druckman, J. N., Wohl, M. J. A., Petty, R. E., Tucker, J. A., Shariff, A., Gelfand, M., Packer, D., … Willer, R. (2024). A synthesis of evidence for policy from behavioural science during COVID-19. Nature, 625(7993), Article 7993. https://doi.org/10.1038/s41586-023-06840 9 Smith, R.C. (2017). ‘Don’t let the illegals vote!’ RSF: The Russell Sage Foundation Journal of the Social Sciences, 3(4), 148–75. DOI: 10.7758/RSF.2017.3.4.09. Sutton, R. M., & Douglas, K. M. (2020). Conspiracy theories and the conspiracy mindset: Implications for political ideology. Current Opinion in Behavioral Sciences, 34, 118– 122. https://doi.org/10.1016/j.cobeha.2020.02.015 van der Linden, S. (2015). The conspiracy-effect: Exposure to conspiracy theories (about global warming) decreases pro-social behavior and science acceptance. Personality and Individual Differences, 87, 171–173. https://doi.org/10.1016/j.paid.2015.07.045 Weeks, B. E., & Garrett, R. K. (2014). Electoral Consequences of Political Rumors: Motivated Reasoning, Candidate Rumors, and Vote Choice during the 2008 U.S. Presidential Election. International Journal of Public Opinion Research, 26(4), 401–422. https://doi.org/10.1093/ijpor/edu005 Appendix Table of Contents Sample Question Wording Figures 1-2 Models and Robustness Test Panel Data Analysis Depression and Partisanship Appendix References Sample Our weighted (full) sample included 51.7% women, and 48.3% men; 64.9% White, 12.4% Black or African American, 5.9% Asian American, and 15.7% Hispanic or Latino; 26.4% 20-34 years old, 32.9% 35 to 54 years old, 29.3% 55 to 74 years old, and 11.4% 75 years old or older; and 6.7% some high school or less, 27.9% high school graduate, 26.1% some college, 24.6% college degree, and 14.7% graduate degree. The respective percentages from the 2021 American Community Survey (ACS) are 50.5%, and 49.5%; 68.2%, 12.6%, 5.7%, and 18.4%; 27.1%, 34.1%, 30.2%, and 8.6%; 11.1%, 26.5%, 20.0%, 29.3%, and 13.1%. (The education data are for individuals 25 years old or older.) Across categories, the sample matches the ACS benchmarks fairly well. The largest discrepancies are that the sample includes more older people (and fewer middle-aged people) and fewer without a high school degree (and more with some college). These are well-known limitations of any survey sampling procedure, not just ours. Most notably, the least-educated are less likely to be online. There also are ostensibly fewer Hispanic or Latino people but that likely reflects the question we report here does not have a distinct item that asked about being Hispanic or Latino (i.e., it is one category for the overall question whereas for the ACS, it is a separate question). Question Wording Outcome Variable: Voting/Supporting a Denier/Doubter for Governor Do you plan to vote in the 2022 election for Governor of [State]? Yes, I already voted (1) Yes, I plan to vote (2) No, I do not plan to vote (3) No, I am voting in another state (4) No, I am not eligible to vote (5) Which candidate for Governor from [State] do you support? --Response options included every candidate in the respondent’s state (including the options “another candidate” and “I do not support any candidate.” Thus, there were 36 unique versions of this for each state with a gubernatorial election. The following is an example from Alabama. Which candidate for Governor from Alabama do you support? Kay Ivey (Republican) (1) Yolanda Flowers (Democrat) (2) James Blake (Libertarian) (3) Jared Budlong (Independent) (4) Another candidate (99) I do not support any candidate (100) --Among respondents who indicated that they supported or had voted for a gubernatorial candidate, in a state that had a denier/doubter on the ballot, per the PBS designations, and given that the respondent was eligible to vote, we recoded responses so that 0 = did not vote for an election denier/doubter for governor; 1 = voted for an election denier/doubter for governor. We excluded respondents who indicated that they were ineligible to vote in 2022 or who resided in a state without a denier/doubter on the ballot. COVID-19 Vaccine Misperceptions Below are some statements about the COVID-19 vaccines that are currently being distributed. To the best of your knowledge, are those statements accurate or inaccurate? 1. The COVID-19 vaccines will alter people’s DNA. 2. The COVID-19 vaccines contain microchips that could track people. 3. The COVID-19 vaccines contain the lung tissue of aborted fetuses. 4. The COVID-19 vaccines can cause infertility, making it more difficult to get pregnant. 5. The COVID-19 vaccines contain a bioluminescent marker used to trace people --Respondents were given one point for each false claim they classified as accurate. The answers were then summed and normalized to a 0-1 interval, where 0 indicates that the respondent did not believe any of the false claims and 1 indicates that a respondent believed all five false claims were accurate. Depressive Symptoms Over the last two weeks, how often have you been bothered by the following problems? • Little interest or pleasure in doing things • Feeling down, depressed, or hopeless • Trouble falling or staying asleep, or sleeping too much • Feeling tired or having little energy • Poor appetite or overeating • Feeling bad about yourself - or that you are a failure or have let yourself or your family down • Trouble concentrating on things, such as reading the newspaper or watching television • Moving or speaking so slowly that other people could have noticed -- or so fidgety or restless that you have been moving a lot more than usual • Thoughts that you would be better off dead, or thoughts of hurting yourself in some way ○ Not at all (0) ○ Several days (1) ○ More than half the days (2) ○ Nearly every day (3) --Each of nine items was summed for a total score ranging from 0 to 27. We employed the common threshold of 15 (/27) to indicate moderately-severe-to-severe depression (Kroenke and Spitzer 2002). Demographic Variables Raw household income as provided by vendor. --Numeric value of income. Gender as provided by vendor (M/F only) Female = Female Male = Male --Re-coded so that 0 = not Male and 1 = Male. Race as provided by vendor (select one, 5 categories) Black/African American Asian American Hispanic White Other --Re-coded so there are binary indicators (0/1) for Black, Asian American, and Hispanic. Education level Some High School or Less (1) High School Graduate (2) Some College (3) College Degree (4) Graduate Degree (5) --Numeric value as captured by scale values above. What is your current age? --Numeric value of age. Political Variables Generally speaking, do you think of yourself as a... Republican (1) Democrat (2) Independent (3) Other (4) --Re-coded so there are binary indicators (0/1) for Republican, Democrat, and Independent. In general, do you think of yourself as... Extremely liberal (1) Liberal (2) Slightly liberal (3) Moderate, middle of the road (4) Slightly conservative (5) Conservative (6) Extremely conservative (7) --Numeric value as captured by scale values above. In general, how interested are you in US politics and government? Extremely interested (5) Very interested (4) Somewhat interested (3) Not very interested (2) Not at all interested (1) --Numeric value as captured by scale values above. How closely do you follow news and information about politics and current affairs? Very closely (4) Somewhat closely (3) Not very closely (2) Not closely at all (1) --Numeric value as captured by scale values above. How confident are you in the fairness of the 2020 presidential election? Very confident (4) Mostly confident (3) Not very confident (2) Not at all confident (1) --Numeric value as captured by scale values above. Is it justifiable to engage in violent protest against the government right now? Definitely yes (1) Probably yes (2) Probably not (3) Definitely not (4) --Numeric value as captured by scale values above. Is it ever justifiable to engage in violent protest against the government? Definitely yes (1) Probably yes (2) Probably not (3) Definitely no (4) --Numeric value as captured by scale values above. The voting in 2022 in the state variable requires an affirmative answer (voted or plan to vote) to one of the following three questions. Do you plan to vote in the 2022 election for Senate in [State]? Yes, I already voted (1) Yes, I plan to vote (2) No, I do not plan to vote (3) No, I am voting in another state (4) No, I am not eligible to vote (5) Do you plan to vote in the 2022 election for Governor of [State]? Yes, I already voted (1) Yes, I plan to vote (2) No, I do not plan to vote (3) No, I am voting in another state (4) No, I am not eligible to vote (5) Do you plan to vote in the 2022 election for the House of Representatives? Yes, I already voted (1) Yes, I plan to vote (2) No, I do not plan to vote (3) No, I am not eligible to vote (4) Trump Won 2020 Election How much do you agree or disagree with the following statement: “If votes were fairly counted, Donald Trump would have won the 2020 election”? Strongly agree (5) Somewhat agree (4) Neither agree nor disagree (3) Somewhat disagree (2) Strongly disagree (1) --Numeric value as captured by scale values above. COVID-19 Variables Have you received a COVID-19 vaccine? No (1) Yes, one dose (2) Yes, two doses (3) Yes, three doses (4) Yes, four or more doses (5) --Numeric value as captured by scale values above. This is the COVID-19 Vaccinated (or vaccination status) variable. How often do you talk to people about COVID-19, either in person, over the phone, or electronically? A few times a day (6) Daily (5) A few times a week (4) Once a week (3) Less than once a week (2) Never (1) --Numeric value as captured by scale values above. |TABLE A.1. Probability of Voting for an Election Denier or Doubter among Eligible Voters in States with Deniers/Doubters on Gubernatorial Ballot*|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|* |---|---|---|---|---|---|---|---|---| ||All Respondents||Democrats||Republicans||Independents|| ||(1)|(2)|(3)|(4)|(5)|(6)|(7)|(8)| |VARIABLES||||||||| |Severe Depression|-0.144|-0.301*|0.00685|-0.182|-0.270|-0.541*|0.0319|-0.0167| ||(0.126)|(0.146)|(0.270)|(0.333)|(0.186)|(0.211)|(0.204)|(0.228)| |Misperceptions|0.600**|0.390^|1.080*|0.882|0.126|-0.156|0.820*|0.741^|** ||(0.213)|(0.232)|(0.511)|(0.584)|(0.271)|(0.283)|(0.351)|(0.379)| |Severe Depression x Misperceptions||1.226*||0.948||1.988**||0.453|** |||(0.477)||(0.894)||(0.737)||(0.838)| |Democrat|-1.009***|-1.018***||||||| ||(0.206)|(0.206)||||||| |Republican|2.322***|2.313***||||||| ||(0.181)|(0.181)||||||| |Independent|0.526**|0.519**||||||| ||(0.178)|(0.177)||||||| |Political Interest|0.213***|0.211***|0.112|0.111|0.176*|0.173*|0.282***|0.281***| ||(0.0502)|(0.0500)|(0.143)|(0.143)|(0.0789)|(0.0783)|(0.0796)|(0.0793)| |Age|0.0136***|0.0135***|0.0172*|0.0172*|0.00809*|0.00813*|0.0173***|0.0173***| ||(0.00266)|(0.00267)|(0.00779)|(0.00782)|(0.00399)|(0.00400)|(0.00420)|(0.00420)| |Education|-0.0154|-0.0173|-0.173|-0.180^|-0.0102|-0.0122|0.0373|0.0368| ||(0.0404)|(0.0404)|(0.106)|(0.106)|(0.0609)|(0.0609)|(0.0629)|(0.0629)| |Income|1.15e-06|1.15e-06|2.46e-06|2.50e-06|3.60e-07|3.49e-07|8.25e-07|8.34e-07| ||(7.19e-07)|(7.21e-07)|(1.76e-06)|(1.81e-06)|(9.09e-07)|(9.02e-07)|(9.16e-07)|(9.15e-07)| |Ideology|0.404***|0.407***|0.299***|0.302***|0.284***|0.290***|0.570***|0.572***| ||(0.0323)|(0.0324)|(0.0817)|(0.0823)|(0.0493)|(0.0495)|(0.0563)|(0.0564)| |Election Confidence|-0.500***|-0.504***|-0.667***|-0.675***|-0.335***|-0.338***|-0.589***|-0.591***| ||(0.0433)|(0.0433)|(0.135)|(0.137)|(0.0616)|(0.0613)|(0.0702)|(0.0701)| |Black|-0.465*|-0.470*|0.190|0.161|0.0238|-0.00901|-1.024**|-1.020**| ||(0.206)|(0.204)|(0.609)|(0.600)|(0.448)|(0.451)|(0.333)|(0.332)| |White|-0.0609|-0.0685|0.370|0.347|-0.00510|-0.0503|-0.0453|-0.0425| ||(0.175)|(0.173)|(0.595)|(0.588)|(0.383)|(0.387)|(0.256)|(0.254)| |Asian American|0.0148|0.0182|1.108^|1.082^|-0.306|-0.332|0.0154|0.0237| ||(0.238)|(0.236)|(0.656)|(0.651)|(0.458)|(0.461)|(0.347)|(0.345)| |Hispanic|-0.277|-0.274|0.357|0.336|-0.184|-0.216|-0.488|-0.480| ||(0.206)|(0.204)|(0.622)|(0.616)|(0.429)|(0.432)|(0.312)|(0.311)| |COVID-19 Vaccinated|-0.124***|-0.128***|-0.236**|-0.238**|-0.0983*|-0.105*|-0.104*|-0.105*| ||(0.0319)|(0.0319)|(0.0813)|(0.0813)|(0.0487)|(0.0489)|(0.0510)|(0.0509)| |Violence Ever Justified|0.0844|0.0827|-0.435|-0.449|0.00868|7.21e-05|0.295|0.297| ||(0.124)|(0.124)|(0.342)|(0.347)|(0.206)|(0.206)|(0.192)|(0.192)| |Violence Justified Now|-0.139|-0.152|0.586|0.594|-0.109|-0.130|-0.368|-0.372| ||(0.166)|(0.166)|(0.381)|(0.385)|(0.263)|(0.262)|(0.256)|(0.255)| |Discuss COVID-19|-0.111***|-0.114***|-0.0623|-0.0655|-0.116*|-0.118*|-0.118*|-0.119*| |Col1|(0.0330)|(0.0331)|(0.0862)|(0.0870)|(0.0499)|(0.0501)|(0.0563)|(0.0561)| |---|---|---|---|---|---|---|---|---| |Follow Political News|0.0669|0.0685|-0.199|-0.196|0.108|0.107|0.129|0.130| ||(0.0620)|(0.0618)|(0.158)|(0.158)|(0.0918)|(0.0912)|(0.100)|(0.0998)| |Voted in 2022|1.433***|1.434***|1.087**|1.097**|1.464***|1.464***|1.344***|1.346***| ||(0.125)|(0.124)|(0.408)|(0.406)|(0.165)|(0.165)|(0.203)|(0.203)| |Male|0.346***|0.346***|0.533*|0.532*|0.265*|0.274*|0.262*|0.259*| ||(0.0777)|(0.0777)|(0.213)|(0.213)|(0.118)|(0.119)|(0.125)|(0.126)| |Constant|-4.142***|-4.087***|-3.130***|-3.042***|-1.379**|-1.300*|-4.703***|-4.693***|** ||(0.306)|(0.306)|(0.783)|(0.780)|(0.522)|(0.527)|(0.449)|(0.448)| |Observations|11,517|11,517|4,584|4,584|3,150|3,150|3,243|3,243| |Robust standard errors in parentheses||||||||| |*** p<0.001, ** p<0.01, * p<0.05, ^ p<0.10||||||||| The quantities of interest are not the coefficients themselves, but the first differences in predicted probabilities or expected values on the outcome variables as the key causal variables (here, depression and COVID-19 misperceptions) vary in combination (see Tomz et al., 2003, p. 19). This is what is displayed in the Figures, revealing clear significance. Table A.2. Robustness Test adding belief that “Trump Won in 2020” as Control Variable Severe Depression -0.275^ |2020” as Control Variable|Col2| |---|---| |Severe Depression|-0.275^| ||(0.148)| |Misperceptions|0.0841| ||(0.242)| |Severe Depression x Misperceptions|1.095*|* ||(0.469)| |Democrat|-1.049***|** ||(0.207)| |Republican|2.155***|** ||(0.182)| |Independent|0.480**|** ||(0.178)| |Political Interest|0.190***|** ||(0.0503)| |Age|0.0133***|** ||(0.00269)| |Education|-0.0136| ||(0.0405)| |Income|1.32e-06^| ||(7.01e-07)| |Ideology|0.397***|** ||(0.0325)| |Election Confidence|-0.373***|** ||(0.0467)| |Black|-0.410*|* ||(0.202)| |White|-0.0564| ||(0.172)| |Asian American|0.0394| ||(0.236)| |Hispanic|-0.246| ||(0.205)| |COVID-19 Vaccinated|-0.114***|** ||(0.0321)| |Violence Ever Justified|0.0800| ||(0.126)| |Violence Justified Now|-0.230| Observations 11,510 Robust standard errors in parentheses *** p<0.001, ** p<0.01, * p<0.05, ^ p<0.10 |Col1|(0.169)| |---|---| |Discuss COVID-19|-0.119***|** ||(0.0333)| |Follow Political News|0.0536| ||(0.0620)| |Voted in 2022|1.396***|** ||(0.125)| |Male|0.349***|** ||(0.0785)| |Trump Won in 2020|0.713***|** ||(0.0983)| |Constant|-4.445***|** ||(0.305)| ||| |Observations|11,510| Panel Data Analysis One concern with any cross-sectional analysis is the difficulty of establishing the direction of causality between the dependent and independent variables, as well as the possible relationships between causal variables (e.g., partisan preferences and depression). Our data do not allow us to definitively address either problem. However, we are able to offer suggestive evidence concerning the direction of causality and the relationship between depression and other causal variables. A relatively small (N=1,848) subset of our respondents who were both eligible to vote and resided in a state with a denier or doubter on the ballot participated in both our August/September 2022 and October/November 2022 survey waves. For these respondents, we replicated our primary models, using, for our depression indicator, the change in the propensity to have moderately-severe-to-severe depression from August/September to October/November. This measure thus divides respondents into three categories: people who were not severely depressed in August/September but were severely depressed in October/November, people whose depression status did not change, and people who were severely depressed in August/September but were not severely depressed in October/November. For this analysis, all other causal variables, including COVID-19 vaccine misperceptions, are taken from the August/September wave. (One of the COVID-19 vaccine misperceptions questions, concerning bioluminescent markers in the vaccine, from the October/November survey was not available in the August/September survey. Consequently, our misperceptions scale for this wave includes only the four false vaccine claims that were available in both survey waves.) The outcome variable, of course, is drawn from the election wave (October/November). The much-reduced N in our panel precludes separate analyses of partisan subgroups (in effect a 3-way interaction, which greatly reduces the number of severely depressed respondents within subgroup models). Hence, we limit this analysis to the main effects of depression and COVID-19 vaccine misperceptions on the propensity to vote for a denier/doubter. Table A.3 presents our results, which we then graphically illustrate in Figure A.1. |misperceptions on the propensity to vote for a denier/doubter. Table A.3 presents our which we then graphically illustrate in Figure A.1.|Col2|Col3| |---|---|---| |TABLE A.3. Panel Analysis of Probability of Voting for an Election Denier or Doubter among Eligible Voters in States with Deniers/Doubters on Gubernatorial Ballot, Using Change in Moderately-Severe-to-Severe Depression from Aug/Sep ’22 to Oct/Nov ’22||| ||(1)|(2)| |||| |Change in Severe Depression (from Aug/Sep to Oct/Nov ’22)|0.327|0.131| ||(0.301)|(0.327)| |Misperceptions (Aug/Sep '22)|1.484**|1.353*|** ||(0.548)|(0.535)| |Change in Severe Depression x Misperceptions (Aug/Sep '22)||2.411**|** |||(0.910)| |Democrat (Aug/Sep '22)|-1.543**|-1.456*|** ||(0.594)|(0.608)| |Republican (Aug/Sep '22)|2.244***|2.345***| ||(0.530)|(0.548)| |Independent (Aug/Sep '22)|0.490|0.583| ||(0.530)|(0.549)| Observations 1,848 1,848 Robust standard errors in parentheses *** p<0.001, ** p<0.01, * p<0.05, ^ p<0.10 |Political Interest (Aug/Sep '22)|0.143|0.158| |---|---|---| ||(0.141)|(0.141)| |Age (Aug/Sep '22)|0.00761|0.00785| ||(0.00675)|(0.00674)| |Education (Aug/Sep '22)|-0.163^|-0.158^| ||(0.0918)|(0.0923)| |Income (Aug/Sep '22)|2.85e-06*|2.83e-06*| ||(1.30e-06)|(1.31e-06)| |Ideology (Aug/Sep '22)|0.512***|0.509***| ||(0.0818)|(0.0821)| |Black (Aug/Sep '22)|-1.485*|-1.548*| ||(0.678)|(0.684)| |White (Aug/Sep '22)|-0.764|-0.804| ||(0.598)|(0.606)| |Asian (Aug/Sep '22)|-0.663|-0.697| ||(0.724)|(0.734)| |Hispanic (Aug/Sep '22)|-0.708|-0.738| ||(0.497)|(0.496)| |COVID Vaccinated (Aug/Sep '22)|0.00234|0.0112| ||(0.192)|(0.193)| |Violence Ever Justified (Aug/Sep '22)|0.159|0.151| ||(0.130)|(0.131)| |Violence Justified Now (Aug/Sep '22)|-0.389*|-0.371*| ||(0.170)|(0.174)| |Discuss COVID (Aug/Sep '22)|-0.365***|-0.369***| ||(0.0835)|(0.0837)| |Follow Political News (Aug/Sep '22)|0.278|0.266| ||(0.176)|(0.176)| |Election Confidence (Aug/Sep '22)|0.518***|0.523***| ||(0.107)|(0.107)| |Voted in 2022 (Aug/Sep '22)|-0.388***|-0.388***| ||(0.101)|(0.102)| |Male (Aug/Sep '22)|0.506**|0.503**| ||(0.182)|(0.182)| |Constant|-2.948**|-3.082**| ||(1.119)|(1.131)| |||| |Observations|1,848|1,848| The results from this analysis replicate those from our primary investigations. As shown in the top graphic in Figure A.1, COVID-19 vaccine misperceptions have a strong positive relationship on the likelihood of voting for a denier or doubter when the data from the prior wave are used. As misperceptions increase from their minimum to maximum value, the probability of voting for a denier increases by 32 percentage points, from .19 to .51 (p<.01). This supports our first hypothesis. The bottom graphic in Figure A.1, in turn, tests our second hypothesis, this time interacting vaccine misperceptions with the change in the propensity to have severe depression from Aug/Sep 2022 to Oct/Nov 2022 – that is, the depression variable represents the change in severe depression from August/September to October/November, rather than the simple likelihood of having severe depression in the latter period. The results show that as misperceptions increase from their lowest to highest levels, given increased severe depression, the probability of voting for a denier increases by 67 points, from .22 to .89 (p<.01). The corresponding changes in the probabilities of voting for a denier or doubter, as misperceptions increase from their minimum to maximum values, are an increase of 29 points (from .19 to .48, p<.05) given no change in severe depression and a statistically insignificant decrease of 13 points (from .22 to .09) given a decline in severe depression. These results strongly support Hypothesis 2, and offer some suggestive evidence in support of our theorized direction of causality regarding the relationship between depression and voting. Depression and Partisanship To assess the possibility that our measure of depression is merely a capturing partisan responses to electoral outcomes (that is, the possibility that Republicans might have grown more depressed following the relatively poor showing of their party in the 2022 midterm elections, which underperformed prevailing expectations (Blake, 2022), we look at the correlations between depression and party, as well as levels of depression across parties and the changes in depression, by party, for a subset of panelists who participated in our survey and in a prior (August/September 2022) survey wave. Beginning with the correlations between party and depression, the results show small, mostly negative correlations between party affiliation and having moderately-severe-to-severe depression, based on the PHQ9 scale in our October/November wave: about -.05 for Republicans, -.02 for Democrats, and .03 for independents. Comparing overall levels of depression in this survey wave (again, using the PHQ9 scale), we see that Republicans are the least depressed group as a whole, with a (weighted) mean of 5.5. The corresponding PHQ9 means for Democrats and independents are 6.2 and 6.7, respectively. Next, looking at the change in depression from August/September to October/November by party, both overall and among our repeat panelists, we find similar patterns of very small correlations. The correlations with having moderately-severe-to-severe depression, based on the PHQ9 scale, are near-zero for Republicans and Democrats, and .02 for independents. The overall change in PHQ-9 scores, and in the propensity to be severely depressed, across our repeat respondents are, in both instances, small decreases in depression: -.4 (on the 0-27 PHQ-9 scale) and -.003 (for change in the moderately-severe-to-severe depression), respectively. Finally, the overall correlation between having moderately-severe-to-severe depression and the propensity to vote for a denier/doubter candidate is also small (-.06), as is the correlation between voting for a denier/doubter and the change in the propensity to have moderately-severeto-severe depression among our subset of repeat panelists (about -.01). In short, there is no evidence of a partisan-depression confound. Appendix References Blake, A. (2022, November 10). How bad the 2022 election was for the GOP, historically speaking. The Washington Post. https://www.washingtonpost.com/politics/2022/11/10/republican-losses-2022-midterms/ Kroenke, K., Spitzer, R. L., & Williams, J. B. W. (2001). The PHQ-9. Journal of General Internal Medicine, 16(9), 606–613. https://doi.org/10.1046/j.1525- 1497.2001.016009606.x Tomz, M., Wittenberg, J., & King, G. (2003). Clarify: Software for Interpreting and Presenting Statistical Results. Journal of Statistical Software, 8(1). https://doi.org/10.18637/jss.v008.i01 1 This work focuses on conspiracy beliefs, but it should generalize to misperceptions, generally. 2 Some suggestive evidence suggests that having a denier on the ballot may have somewhat suppressed Republican support for their party’s candidates. In the eight states without a denier on the ballot, 83% of Republicans voted for their party’s gubernatorial candidate, six percentage points higher than in the 28 states with denier on the ballot. This is consistent with the pattern described by Jacobson (2023) with respect to House candidates. That said, Bartels and Carnes (2023) report evidence that among House candidates, election denial was associated with neither better nor worse performance among Republicans in the general election. Since House districts are subject to partisan gerrymandering, which is unavailable in statewide gubernatorial candidates, it is difficult to draw direct comparisons between the electoral dynamics across these two sets of elections. 3 The insignificance could partially reflect the reality that interactive effects require substantial statistical power to detect (Gelman, 2018) and here we are in essence looking at three-way interactions. The effect size among Republicans is so large that it reaches significance. Also, we cannot replicate the aforementioned panel robustness test on partisan subgroups as the sample sizes are too small. Does ranked choice Voting Increase voter turnout and mobilization? Author: Dowling, E., Date: 2024-08-01 Collections: Voting Systems Zotero Key: U8HTZE5H Cite Key: Dowling24turnoutMobilizRCV Zotero Item | Lit Note Contents lists available at ScienceDirect Electoral Studies journal homepage: www.elsevier.com/locate/electstud Does ranked choice Voting Increase voter turnout and mobilization? E. Dowling [a], C. Tolbert [b], N. Micatka [b], T. Donovan [c][,] [* ]* a RCVRC, Univ. of CA, Davis, USA b Univ. of Iowa, USA c Western Washington University, USA A R T I C L E I N F O Keywords: Voter turnout Ranked-choice voting Electoral systems Mobilization 1. Introduction A B S T R A C T Many jurisdictions in the United States have recently adopted single-winner ranked choice voting (RCV) to replace first-past-the-post plurality elections. This study contributes to the literature examining the potential consequences of changing to RCV by modeling the relationship between electoral systems and voter turnout. We propose that RCV may increase turnout by incentivizing increased contacts with voters. Previous attempts at assessing the relationship between RCV and turnout in the US have been limited by a lack of individual-level turnout data measured across all cases where RCV is and is not used. The study utilizes large, unique data from administrative voter turnout records that overcomes this limitation. We find significant and substantially higher probabilities of turnout in places that use RCV, and find evidence that campaigns in RCV places have greater incidences of direct voter contacting than in similar places that do not use RCV. Despite the growing role that ranked choice voting (RCV) plays in US politics, there is a significant gap in the literature regarding the impact of RCV on voter turnout. This is important because the single most important political act for the average citizen is voting in elections. The sparse existing research on this topic generally focuses on a single jurisdiction (e.g. Neely and McDaniel, 2015; however, see McDaniel 2019), ballot roll-off, or relies on aggregate turnout data.[1 ]Kimball and Anthony (2016), for example, use a difference-in-differences design, matching cities using RCV with demographically similar cities using plurality voting on the same date. The results show that RCV helps reduce the substantial drop in voter participation that commonly occurs between primary and runoff elections by combining the two elections into a single instant run-off.[2 ]Yet, the use of RCV was not found to impact voter turnout or ballot completion. Other evidence, however, finds that voter turnout rates are higher in preferential than plurality local elec tions (Bowler et al., 2003), and youth voter turnout increases in places using RCV (Juelich and Coll 2021). The shortage of inquiry on this topic is notable given recent research shows potential voters are more likely to be contacted by multiple candidates in RCV elections (Wendland and Carman, 2023). Drutman and Stano (2022:25) state there is “mixed evidence,” and that it is “hard to assess” whether RCV increases political participation. If US elections are mostly uncompetitive with highly polarized plurality districts, citizens are exposed to less active campaigns and voters have limited choices among candidates (Donovan 2007). We propose that there is potential for ranked choice voting to increase turnout by expanding voter choice and/or affecting incentives candidates have to campaign in various ways. However, the limited empirical research on this topic leaves the question of whether RCV affects turnout mostly unanswered. The paper proceeds by first presenting a discussion of how RCV may affect candidate behavior by incentivizing voter contacting. This has us propose that turnout may be higher in RCV jurisdictions as a result. We then provide an overview using a national voter file to estimate individual-level models of voter turnout, and present results demon strating that the predicted probability of voting in local elections is 17 percent higher in RCV than in non-RCV jurisdictions, all else equal. Next, we test our causal assumption about voter contacting being greater • Corresponding author. E-mail addresses: emdowling@ucdavis.edu (E. Dowling), caroline-tolbert@uiowa.edu (C. Tolbert), nathan-micatka@uiowa.edu (N. Micatka), todd.donovan@ wwu.edu (T. Donovan). 1 Some research analyzing RCV adoption in San Francisco using aggregate data shows evidence of reduced voter turnout and demographic bias (McDaniel 2016). 2 Often RCV is implemented so that a primary and runoff are combined into one instant runoff election. The single election is held in November, “where it can share the ballot with higher-profile state and federal elections likely to attract more voters, thus boosting turnout overall.” (Drutman and Strano 2021, pg 26). https://doi.org/10.1016/j.electstud.2024.102816 Received 11 January 2024; Received in revised form 26 May 2024; Accepted 4 June 2024 Available online 8 June 2024 0261-3794/© 2024 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC license (http://creativecommons.org/licenses/by- nc/4.0/). in RCV cities than in comparable non-RCV cities and find significantly higher rates of in-person, print mail, and email contact in RCV cities. We conclude with a discussion of how our results present questions for future research. 1.1. RCV and changing candidate behavior Choosing elected officials using ranked choice voting (often called the Alternative Vote or instant run-off voting) has been shown to alter the electoral environment of political campaigns (Grofman and Lijphart, 1986). The Ranked Choice Voting Resource Center (RCVRC) provides a succinct definition of how instant run-off voting works3 . An electoral system that asks voters to rank candidates instead of choosing a single candidate for each office/seat may change candidate behavior. Because the electorate is offered the opportunity to evaluate multiple candidates, candidates may actively appeal to citizens for second and third-place votes, not just first-place votes as in plurality contests. Attacking candidates under RCV may be more costly given that a rival’s supporters could potentially provide votes to other candidates in later rounds of tallying. Research finds both voters and candidates alike perceive there is more civility and less negative campaigns compared to previous local elections under RCV (Donovan et al., 2016). Candidates in RCV contests are more likely to report groups telling voters to support multiple candidates and are more likely to perceive the tone of campaigns as positive compared to candidates in matched cities using plurality voting (Donovan and Tolbert, 2023). Text analysis of RCV city newspaper articles about the candidates and local elections had significantly more positive words and fewer negative words than plu rality cities (Kropf, 2021). This altered campaign decision-making cal culus from a simple change in ballot design and aggregation of ballots has far-reaching consequences for elections and democratic politics in the United States (Santucci, 2022). By extension, campaign mobilization efforts can also be expected to be modified under RCV. Candidates running for elected office often appeal for second and third-place votes and thus may make more campaign contacts, boosting participation rates. Direct campaign con tact – in the form of in-person, phone, mail, or social media/email recruitment – has been shown to increase turnout through information and social pressure mechanisms (Gerber and Green, 2008; Tolbert et al., 2008; Parry et al., 2008; Sinclair, 2012) and even tiny political actions online (Margetts et al., 2016) such as following candidates on X (formerly Twitter) (Gainous and Wagner, 2013) matter. When members of a social network indicate they have voted in an election or contributed to a candidate, for example, that can motivate others in their network to do the same (Rolfe, 2012). Under RCV, there is some evidence of increased candidate contact ing. With exit poll data from New York City’s 2021 mayoral primary using RCV, Wendland and Carman (2023) find that being asked to rank a candidate as a second or third is a statistically significant predictor of a voter’s propensity to rank several candidates, all else equal. Candidate appeals of this type are not uncommon; about a third of voters in RCV contests nationwide report candidates made appeals for second and third-place rankings (Donovan and Tolbert, 2023), but such appeals 3 The Ranked Choice Voting Resource Center defines RCV as an election method where voters rank candidates for an office based on their preference, from first choice to last. This method is different from traditional plurality voting, in which voters can only select one candidate. If a candidate receives more than half of the first-choice votes, they win the election just like in any other election. However, if no candidate receives a majority of the first-choice votes, a second round of counting takes place. In this instant runoff, the candidate with the fewest votes is eliminated, and voters who ranked that candidate as their first choice will have their votes count for their next choice. The process continues until a majority winner, a candidate with more than half of the vote, is declared. See https://www.rcvresources.org/what-is-rcv. often don’t extend to frontrunners. If candidates reach out to more people and elections are more competitive (i.e., more candidates), these mobilization efforts may increase turnout. Research finds that RCV elections attract on average more candidates than the plurality elections that preceded them (Drutman and Strano, 2022). In general, electoral competition has been shown to boost participation in elections (McDo nald and Samples, 2006; Donovan, 2007). The limited existing research on RCV and turnout in the US is in stark contrast to work done on other convenience voting reforms adopted by state and local governments (i.e., same-day registration, early voting, mail voting, etc.) for which there is substantial empirical analysis over time. This dearth of research may be rooted in limited data to measure voter turnout in off-year local elections. Most national surveys and research on electoral reforms focus on presidential or midterm (congressional) elections where 40–65 percent (on average) of the voting-eligible population participates. Outside of Maine and Alaska where RCV is used state-wide, RCV is predominantly used in lowturnout local elections - many of them nonpartisan and held in odd years separated from state and federal elections. Turnout in American municipal elections is generally very low and inconsistent (Anzi, 2014; Hajnal et al., 2021). One study of local election turnout in 50 US cities found that turnout in 10 of America’s 30 largest cities was less than 15 percent and averaged just over 20+ percent (Drutman and Strano, 2022; see also Hajnal and Trounstine, 2005). Most urban politics scholars study America’s 100 largest cities. Turnout in smaller population cities (100–200K) is even lower. Schaffner, Rhodes and La Raja (2022), in their book Hometown Inequality, examine participation and representation in local elections using voter file data. They examine electoral institutions such as election timing, districting, and partisan vs. nonpartisan contests, but they do not examine RCV. Insufficient data on observed or reported turnout helps explain why there is limited research on how ranked-choice voting (RCV) affects local election participation. This presents a challenge for researchers because they need to differentiate RCV’s impact from other factors that are known to cause changes in turnout between elections. For example, a significant ballot measure or a competitive race can also influence voter turnout, making it harder to isolate the effects of RCV. To address these concerns, this study proceeds in two parts. It first uses administrative panel data from the most recent (2023) national voter file for all US adults to test whether the implementation of RCV corresponds with higher turnout in off-year elections, controlling for other factors. Use of the administrative data overcomes sparse off-year election survey data. Because participation in local elections is habitual (Plutzer 2002), we control for voting in 2019 using panel data. Second, it uses a national survey conducted in 2013–2014 that matched RCV and non-RCV cities to model if registered voters residing in RCV jurisdictions were more likely to be contacted (i.e., mobilized) by political campaigns using varying methods. The results from both point to the previously un measured effects of RCV on voter turnout. 1.2. Hypotheses This study tests the following hypotheses. H1. Individuals living in jurisdictions that have implemented RCV will be more likely to vote in the 2021 off-year elections, controlling for their vote history in 2019, and other demographic and state contextual factors. H2. Registered voters living in cities that have implemented RCV will be more likely to be contacted by candidate campaigns through a variety of methods than similar people living in matched cities with plurality elections. 1.3. Measuring voter turnout in off-year elections using administrative data Most research on election laws and turnout rely on either selfreporting from election survey data (including the Census’s Current Population Survey, November supplement) or aggregate data from election returns for geographies (precincts, counties, states, etc). Making inferences about individual behavior from aggregate data is difficult because of the ecological fallacy; aggregate election returns do not allow an analysis of individual-level factors known to predict individual voting decisions, including whether the person is a habitual voter (i.e., vote history). On the other hand, representative national surveys in off-year elections, where most local elections occur, are rare and hard to come by. Studying a single city or even state where RCV has been adopted limits generalizability because of idiosyncratic local differences. While studies of election reform laws on turnout drawing on surveys or Census data have furthered the literature, issues with survey data stemming from self-reported turnout, smaller sample sizes, selection bias, representativeness, and a lack of panel data threaten the causal claims of previous works (Ansolabehere and Hersh, 2012; Erikson and Minnite, 2009). National surveys can fail to reflect state voting pop ulations since voters’ likelihood to respond can differ across de mographic groups (Ansolabehere et al., 2022; Ansolabehere and Hersh, 2012), with some groups more likely to overreport voting. While some national surveys, such as the large sample Cooperative Election Study (CES) and recent Pew Research Center surveys, use validation proced ures against the national voter files, the effectiveness of these methods can vary across states and over time (Grimmer et al., 2018). Even when large sample surveys such as the CES run smaller surveys with questions on whether people voted in off-year/local elections, they do not include reliable measures of individual voter history. Most places using RCV are local governments and thus turnout in off-year elections is important for our study. Even scholars studying turnout in off-year localonly elections focus on the largest U.S. population cities or cities in one state (Hajnal et al. 2021). Data on voter turnout for all US local juris dictions is difficult to obtain. To overcome these issues, this study measures voter turnout using administrative data with vote histories from the national voter file over time. Data for this study includes 1% random draws from the 2023 national voter files, allowing measurement of change in the probability of voting comparing 2019 and 2021 off-year elections, with over 2.5 million unique observations (as a robustness test we replicate identical models using the 2022 1% file. Combined there are 5 million observa tions). We leverage unique information from a national voter file (Catalist) of 265 million US adults to gain the largest possible sample of US adults who voted in the 2021 off-year and local elections.[4 ]These data are administrative data compiled from the 50-state voter rolls combined with commercial and USPS data. These data include all US adults, including people who are registered to vote and those who are not. The national voter file combines official voting records from all state voter files with additional data (e.g., U.S. Postal Service National Change of Address data), commercial data (cell phone records, credit bureau reports), and campaign canvass records from the near universe of the adult U.S. population. Catalist provides a random 1% sample to re searchers, with a growing number of studies showing these data provide a more accurate measure of voter turnout than self-reports of voting in election surveys or Census data (Ansolabehere et al., 2022; Cantoni and Pons 2021; Hersh, 2015; Fraga, 2016, 2018; Fraga and Holbein, 2020; Hersh and Nall, 2016; Rogers and Aida, 2014; Hersh and Ghitza, 2018; Nickerson and Rogers, 2014; Ritter and Tolbert, 2020; Ritter, Coll & Tolbert 2023). We test the effects of living in a jurisdiction that has implemented RCV on individual voting decisions in off-year [November] elections. 4 https://catalist.us/data/. Our data includes people who voted in 10,445 incorporated cities in 2021.[5 ] Mean voter age population (VAP) turnout turnout in 2021 offyear elections nationwide was 10.6 percent in our data. This is lower than the turnout in municipal elections for large-population cities. In dividuals who reported voting were coded 1 and non-voters coded 0. Besides providing a more accurate measure of voter turnout that comes from state governments—rather than reporting from individu als—these data also include a panel component. Individual-level vote histories can address endogeneity problems by providing a measure of within-person change in turnout over time. The use of a lagged variable to measure past voting decisions effectively measures the change in individual voting decisions across two sequential off-year elections. Using vote histories allows the use of recursive models designed to measure factors associated with change in individual-level voting de cisions. Like a within-group experiment, lagged panel data is widely used to improve causal inferences (Angrist and Pischke, 2009). In contrast, overreliance on cross-sectional data to make inferences can be misleading, as other factors (a competitive race, etc.) may be driving outcomes present in the data. These models make possible more precise measurements of how a person’s likelihood of voting changes from election to election. The primary explanatory variable is whether the respondent lives in a jurisdiction that has implemented ranked choice voting as of 2021 (i. e., the jurisdiction used RCV in an election in 2021 or earlier). Data on local government implementation of RCV is from the Ranked Choice Voting Resource Center (RCVRC).[6 ] Jurisdictions (i.e., county, state, or city) that have implemented RCV by 2021 are coded 1, while all other jurisdictions are coded 0 [in ME, RCV was used statewide]. See Ap pendix Table 3 for a list of cities with RCV elections in 2021 (the treatment group). We use the most recent off-year election for which voter roll data is available (2021) since there is the largest sample of RCV cities and states. The majority of American cities conduct elections in off-years.[7 ] A series of control variables are also included. As mentioned above, in creating the national voter file, Catalist draws on multiple sources. While age (measured in years) and gender (females coded 1 and males 0) are generally included in state voter files, these data also include modeled estimates of an individual’s race (in some states race is reported on the state voter files). The statistical models include binary variables for Black, Latino, Asian, and other race (coded 1, all others 0), marital 5 There are 19,502 incorporated cities, towns and villages in the U.S as of 2019. 16,410 had a population under 10,000 while only ten had populations above 1 million. 310 cities are considered medium cities with populations of 100,000 or more. Source: “Number of cities, towns and villages (incorporated places) in the United States in 2019, by population size.” Statistica. https: //www.statista.com/. 6 See https://www.rcvresources.org/where-is-rcv-used for more details on the jurisdictions using ranked-choice voting. 7 Of the 100 largest cities by population, 70 had elections in 2021. Ballot pedia covers elections in America’s 100 largest cities by population and elec tions in each state capital. The Catalist data analyzed here includes all municipal elections nationwide in 2021, including small-population cities not measured by Ballotpedia (10,445 cities held elections). In addition to the municipal elections, in 2021 gubernatorial elections were held in New Jersey and Virginia. Six special elections for US House seats occurred in November 2021 because of deaths or vacancies. This affected races in TX, LA, NM, and OH. The state fixed effects in the model control for these cases. Larger California cities, however, did not hold municipal elections in 2021, but the state-fixed effect controls for any unique CA effects. This analysis cannot be used to make inferences about voter turnout in large California cities using RCV. Source: United States Municipal Elections, 2021. Ballotpedia. https://ballotp edia.org. Table 1 Predicting 2021 Individual Voting Decisions, Lagging 2019 Turnout (Catalist 1% file, 2023 & 2022). 2023 Catalist File 2022 Catalist File Model 1 (State Fixed Effects & No COVI)< Model 2 (State< Model 3 (State Fixed Effects & No COVI)< Model 4 (State FE’s & COVI)< FE’s & COVI) Voted in 2019 2.352*** 2.351*** 2.329*** 2.328*** (0.006) (0.006) (0.006) (0.006) RCV Jurisdiction 0.457*** 0.457*** 0.528*** 0.528*** (0.019) (0.019) (0.019) (0.019) Age 0.043*** 0.043*** 0.045*** 0.045*** (0.001) (0.001) (0.001) (0.001) Age Squared − 0.0002*** − 0.0002*** − 0.0002*** − 0.0002*** (7.60–06) (7.60–06) (7.74–06) (7.75–06) Female 0.008 0.008 0.003 0.003 (0.005) (0.005) (0.005) (0.005) Asian − 0.811*** − 0.813*** − 0.785*** − 0.786*** (0.017) (0.017) (0.017) (0.017) Black − 0.259*** − 0.257*** − 0.240*** − 0.238*** (0.009) (0.009) (0.009) (0.009) Latino − 0.650*** − 0.650*** − 0.655*** − 0.654*** (0.011) (0.011) (0.011) (0.011) Race Other − 0.413*** − 0.414*** − 0.542*** − 0.545*** (0.019) (0.019) (0.061) (0.061) Married 0.037*** 0.036*** 0.046*** 0.045*** (0.007) (0.007) (0.007) (0.007) Bachelor’s Degree 0.215*** 0.215*** 0.231*** 0.231*** (0.007) (0.007) (0.007) (0.007) Income 0.251*** 0.252*** 0.237*** 0.238*** (0.003) (0.003) (0.003) (0.003) Catalist Ideology 0.007*** 0.007*** 0.007*** 0.007*** (0.0001) (0.0001) (0.0001) (0.0001) COVI 2020 − 24.243*** 14.218*** (4.562) (4.227) N 2,469,097 2,464,657 2,428,133 2,423,864 Log-likelihood − 536380.43 − 535858.4 − 525079.49 − 524657.07 AIC 1072889 1071843 1049440 1050287 State Fixed-Effects Yes Yes Yes Yes Note: Unstandardized logistic regression coefficients. Robust standard errors in parentheses. p < 0.05. p < 0.01. *** p < .001.** Source: Catalist 2023 & 2022 national voter file 1% random sample status (married coded 1, non-married 0), an ordinal scale for income (data from credit bureau reports), and education (coded as 1 if the probability of having a bachelor’s degree is 50 percent or higher, 0 otherwise).[8 ]The statistical models also include a control for modeled political ideology on a 0–100 scale with 100 indicating a high proba bility of being a strong liberal; this variable is provided to researchers by Catalist and is based on national survey data, canvassing data, campaign contributions, primary vote history, etc. Previous work has shown these demographic data are highly reliable (Fraga, 2016, 2018; Hersh, 2015). The Pew Research Center, for example, reports that national voter rolls are generally accurate when matched to respondents in their American Trends Panel. Comparing five separate commercial voter files, Pew found that unregistered individuals are generally included in the commercial files and that unregistered voters who did not appear were similar to all unregistered voters (Igielnik et al. 2018). When it is easy to register and cast a ballot, citizens are more likely to do so (Burden et al., 2014; Hanmer 2009; Leighley and Nagler 2013; Wolfinger and Rosenstone 1980). However, when the costs of partici pation are higher, it is less likely one completes the registration and voting process, especially young people (Juelich and Coll 2020). Some 8 Not all states require individuals to report race, or ethnicity and no state requires marital status, education, and income. As such, these latter variables are imputed by Catalist from commercial and canvassing data. Previous studies find that the imputed variables have high predictive validity (Hersh 2015; Fraga 2018). states have sought to expand access to the ballot through the adoption of mail voting or no-excuse absentee voting, in-person early voting, same-day registration, and automatic voter registration (AVR) (McDo nald, 2022; Ritter and Tolbert 2020). Others have passed reforms to make elections more secure but also more costly such as voter ID laws (Cantoni and Pons 2021). To account for these varying state election laws, we merge the individual data from the national voter file with the widely used Cost of Voting Index (COVI) that measures restrictions or barriers to voting by state (Li, Pomante II, & Schraufnagel 2018; Schraufnagel, Pomante II, & Li 2020; Pomante II et al., 2023). The COVI is measured on an interval scale with higher scores indicating more restrictive state laws.[9 ] 2. Results: RCV and voter turnout Table 1 reports logistic regression panel models predicting voting in the 2021 off-year election, lagging whether the individual voted in the prior off-year election (2019). We estimate models with and without controlling for the Cost of Voting Index (or the restrictiveness of state voting and registration laws in the respondent’s state). The two models on the left are drawn from 2023 1% national voter file; the two columns on the right are from the 2022 1% file. The back-to-back replication on two different samples of over 2.6 million people increases confidence in the results. We focus on the results using the 2023 data, as the results 9 Descriptive statistics for the 2020 COVI on the original scale range from − 2.91681 to 1.436939. Cost of Voting Index. https://costofvotingindex.com/. track each other closely. The models include demographic factors known to increase the probability of voting and state-fixed effects to account for any unmeasured factors. Given low turnout, it is not surprising that voting in the prior off-year election is a strong predictor. Habitual voters, or people who voted in 2019, are significantly more likely to vote in 2021, all else equal. Fe males are slightly less likely to vote in off-year elections than males, while Blacks, Asians, and Latinos are significantly less likely to vote in off-year elections, all else equal. Higher-income, age, and education significantly increase participation in these low-turnout elections. Peo ple who are likely to have a college degree are much more likely to vote than those without a degree. As expected, people living in states with more restrictive registration and voting laws (COVI) are less likely to participate. But controlling for all these factors, people living in jurisdictions with ranked choice voting are statistically more likely to vote in the 2021 election (using the 2023 or 2022 sample data). The parallel results across two different samples of the national files are strong. Converting the logistic regression coefficient from Model 2 using the 2023 Catalist file to predicted probabilities, holding all other variables constant, in dicates individuals living in RCV jurisdictions were on average 17 percent more likely to vote in the 2021 off-year elections than those in non-RCV jurisdictions, all else equal (1.8 percentage point increase in the probability of voting divided by the baseline 10.6 mean national turnout in 2021 = 17 percent). In contrast, the marginal effect of a oneunit increase in COVI (higher values more barriers to voting) is − 0.77 (standard error 0.15). To put this effect size in context, varying the COVI from low (− 2 standard deviations below the mean) to high (+2 standard deviations above the mean) results in a 2.58 percentage point reduced probability of voting in 2021 (or a 24 percent decrease probability of voting—2.58/10.6 mean national turnout).[10 ] Given the use of panel data with vote histories and administrative data built from the fifty-state voter files, this is a robust finding. A potential weakness is that educational efforts are often used, as they were in Alaska, to help people understand RCV. These educational efforts likely also increased voter turnout (Wendland and Carman, 2023). Educational efforts and outreach typically happen when a voting method is new to voters. For those jurisdictions who have used RCV for while, educational efforts aren’t as necessary. To address this we included a measure for how long the jurisdiction has used RCV. In Ap pendix Table 1, we replicated the models in Table 1 (2023 sample data), but instead of a variable for whether the jurisdiction uses RCV or not, we include an ordinal variable measuring the timing of adoption of RCV to control for any possible educational campaigns. The main explanation variable is coded (1) for 0–5 years since RCV adoption, (2), between 5 and 10 years prior (3) for more than 10 years ago. We used these in tervals to factor in presidential elections. So if RCV is adopted at some point during the cycle, educational groups will still educate even when it’s been around for two years, but in a presidential election, new voters will still need education. The coefficient for RCV remains statistically significant and positively related to higher off-year election turnout, meaning the effect we are measuring is not because the process is new and there are educational campaigns. 2.1. RCV and voter mobilization If turnout rates are higher in places that use RCV, are citizens more likely to be contacted by parties, candidates, and campaigns in RCV cities than in similar places using plurality voting? If candidates need more than first-place votes (i.e., they need second- and third-place votes), there may be more overall contact and personalized 10 Low COVI is set to − 1.92 as this is two standard deviations below the mean, whereas high COVI is set to 1.43 (the variable maximum) because the value that is two standard deviations above the mean is greater than the maximum value. Table 2 Logistic regression Predicting Contact Method comparing RCV and Plurality Cities, 2013–2014. In Person Print Mail Telephone E-mail Social Media RCV 0.38*** 0.19*** − 0.07 0.41*** 0.004** (0.066) (0.068) (0.065) (0.079) (0.106) Exposure 0.39*** 0.36*** 0.43*** 0.50*** 0.73***** (0.019) (0.019) (0.019) (0.025) (0.038) Democrat 0.01 0.08 0.09 0.18** 0.09** (0.074) (0.076) (0.073) (0.090) (0.120) Republican − 0.26** − 0.09 − 0.15 − 0.10 0.01** (0.112) (0.107) (0.105) (0.139) (0.184) Age 0.004* 0.01*** 0.02*** − 0.003 − 0.02***** (0.002) (0.002) (0.002) (0.002) (0.003) Education 0.13*** 0.21*** 0.15*** 0.26*** 0.19***** (0.035) (0.036) (0.034) (0.045) (0.060) Income 0.06*** 0.06*** 0.01 0.07*** 0.03** (0.019) (0.019) (0.018) (0.024) (0.030) Female 0.13** 0.15** 0.12* − 0.22*** − 0.10** (0.066) (0.068) (0.065) (0.079) (0.105) Black − 0.34*** − 0.56*** − 0.19* − 0.33** 0.08** (0.113) (0.111) (0.108) (0.135) (0.172) Asian − 0.09 − 0.43*** − 0.17 − 0.11 − 0.23** (0.171) (0.164) (0.158) (0.207) (0.297) Latino 0.09 − 0.09 0.12 − 0.29* − 0.04* (0.122) (0.124) (0.120) (0.159) (0.194) Other Race 0.15 − 0.33** − 0.15 0.01 0.47** (0.147) (0.153) (0.149) (0.179) (0.213) Married − 0.02 − 0.10 − 0.02 0.10 0.004 (0.073) (0.077) (0.073) (0.088) (0.116) 2013 − 0.06 − 0.40*** − 0.35*** − 0.26*** − 0.17** (0.069) (0.073) (0.069) (0.084) (0.112) Constant − 3.78*** − 2.38*** − 3.21*** − 5.38*** − 6.22***** (0.227) (0.219) (0.217) (0.289) (0.377) N 4888 4888 4888 4888 4888 Log − 2850.91 − 2673.25 − 2894.19 − 2112.55 − 1290.1 Likelihood AIC 5731.82 5376.49 5818.37 4355.11 2610.19 Note: Unstandardized logistic regression coefficient, with robust standard errors in parentheses. • p < .1. ** p < .05.** *** p < .01.** Source: 2013–2014 Rutgers-Eagleton Ranked Choice Voting Survey. canvassing. We hypothesize campaigns under RCV rules behave differ ently from campaigns in plurality elections. Campaigns in RCV places should be more likely to engage in personalized direct contact with potential voters. A long line of research in political science shows that contact and mobilization are linked to increased participation in politics (Rosen stone and Hansen, 1993; Schlozman, Verba and Brady, 2013; Verba et al., 1995). The more recent literature on campaign contact and turnout demonstrates that the impact of mobilization depends on the type of recruitment. Impersonal campaign mobilization techniques, such as robocalls, have been found to not affect turnout (Gerber et al., 2008); likewise, television and radio advertisements, as well as blanket modes of campaigning across large geographic areas have minimal effects (Parry et al., 2008). Direct campaign contact – in the form of an in-person, in-person phone call, mail, or electronic message voter recruitment – has been theorized to increase turnout through informa tion and social pressure mechanisms (Gerber and Green, 2008; Parry et al., 2008; Sinclair, 2012; Rolfe, 2012). For example, reminding citi zens that their neighbors voted in previous elections has been to increase turnout (Gerber et al., 2008). Individuals contacted directly by cam paigns become more informed when these agents tell them about pol icies, candidates, and elections. Additionally, candidate and party campaigns increasingly inform potential voters of state registration and election, prompting them to mail in absentee ballots. This helps citizens unfamiliar with the process of registering and voting to navigate a state’s election administration and cast a ballot more easily. 2.2. Matched national survey data Our question is centered on whether mobilization efforts of cam paigns differ in varying electoral systems. To measure whether campaign mobilization behavior is different in RCV and plurality cities, we turn to a unique national survey of registered voters. The Eagleton Institute at Rutgers University fielded two national random telephone surveys in 2013 and 2014 using a unique matched research design of people who had voted in previous local elections. Samples were drawn from cities using RCV, and each city was matched with cities using plurality elections with similar demographics and political contexts. The November 2013 survey included 2400 voters in three RCV cities (Min neapolis, St.Paul, Cambridge), and seven non-RCV cities with similar contests on the ballot and matched demographics (Boston, Seattle, Des Moines, Cedar Rapids, Tulsa, Lowell, MA, and Worcester, MA); the November 2014 survey was of 2400 voters in eleven Californian cities: the four Bay Area cities that use RCV (Berkeley, Oakland, San Francisco, and San Leandro) and seven control cities with similar contests and demographics (John and Douglas 2017). The matched city design sought to minimize as much as possible any potentially confounding effects. The survey was conducted in both English and Spanish. The two surveys are combined here. Respondents were asked if they were contacted by a campaign in a variety of ways. The results are robust both among only those reporting being mobilized as well as the entire sample (Table 2). Seventy-four percent of the overall sample remembered being contacted by a campaign in at least one way, and those reporting being contacted were asked follow-up questions about contact methods. Questions included if someone from a campaign knocked on their door, if they received print mail from a campaign, if they received a phone call from a campaign if they received an e-mail from a campaign, and if they were contacted by a campaign via social media. If the answer was yes, they were coded 1 and 0 for all others. These binary variables serve as the outcome vari ables in a series of logistic regression models. This research design al lows us to assess how campaigns performed at mobilization efforts, altering only the electoral system they are operating within. In addition to whether or not they were contacted directly, re spondents were also asked if they remembered seeing or hearing campaign activities in passing. The instances when they remembered seeing/hearing campaign activity that was not directed specifically at them were added together and included in our models to control for “level of exposure” to the campaign, broadly construed. We include this overall campaign exposure variable as an additional independent variable. The models include a standard set of control variables for de mographic factors including age measured in years, gender measured with a binary for female (coded 1), race (binary variables for Black, Latino Asian with white non-Hispanics as the reference group), ordinal variable for education from less than a high school degree to postgraduate, ordinal for income, binary for married (coded 1), binary variables for Republican and Democrat (each coded 1) with in dependents as the reference group, etc. To distinguish between the 2013 and 2014 surveys we include a binary variable for the year 2013. 2.3. Results RCV and voter contacting Table 2 reports unstandardized logit models estimating the proba bility of being contacted by a campaign in one of the five ways mentioned above. Living in a city with RCV has a statistically significant influence on the way campaigns attempt to mobilize voters; people in these cities are more likely to be contacted. Of the five contact methods analyzed, three have significant and positive coefficients: in-person, printed mail, and e-mail methods. The results provide additional evidence of the substantive impact of living in an RCV city on campaign mobilization. Respondents in RCV cities are estimated to be eight percentage points more likely to be contacted in person and about three percentage points more likely to be contacted through print mail. Those in an RCV city are also 5.4 per centage points more likely to be contacted via email compared to those not in an RCV city. The largest effect of RCV appears to be on campaign canvassing (in person) and e-mail marketing tactics. Voters in RCV electoral systems voters are much more likely to be contacted by can vassers or e-mail messaging than in plurality electoral systems. Social media behavior seems to remain unchanged across system types. As a robustness test, the Appendix estimates a model where people recalling hearing/seeing any type of campaign activity are coded 1 and all others 0. Individuals living in places using RCV are about 2.6 percentage points more likely to recall direct campaign contact, all else equal. This evidence suggests that campaigns are conducted significantly differently in RCV cities than in cities without RCV - with more campaign contacts in person, in print, and via email. These results pair with the administrative data finding of higher turnout in off-year local elections under RCV. While previous research had found evidence that campaings in RCV cities may be seen by voters as being less negative than other contests, the results here suggest campaigns also alter how they interact directly with voters. Campaigns may utilize more person alized contact methods, preferring to contact a voter one-on-one. This increased campaign contacting in RVC places may explain why voter turnout is found to be higher in places using RCV. 3. Conclusion As the use of RCV expands across the United States, it is important that we better understand how adoption of this electoral system may or may not change the status quo in terms of who votes and who is rep resented. Existing studies suggest there may be some trade-offs with RCV. Studies find that campaigns may be seen as less negative under RCV (Donovan, Tolbert & Gracey 2016), that voters’ reported under standing of RCV is generally high across race/ethnic groups (Donovan et al., 2019). Additional research has found that people of color may be less likely to rank multiple candidates (Donovan et al., 2022) which is concerning if this leads to ballot exhaustion that weakens the influence of these voters in later rounds of RCV counting (Burnett and Kogan 2015). The relatively complex ballots used in RCV contests may also lead to higher rates of ballots being voided due to overvoting, and that this may be more pronounced in areas with higher concentrations of His panic/Latino residents and residents with lower incomes (Cormack, 2023). We contribute to this literature by providing what is likely to be the largest scale assessment to date of how RCV may affect voter turnout. We find that campaings may engage in more direct voter contacting in places that use RCV, and that turnout is notably higher in places that use RCV (compared to standard plurality, first-past-the-post contests). Additional research is needed to assess the composition of electorates in order to determine who is potentially more likely to vote in RCV con tests. That is, are the potential turnout increases that we find associated with RCV altering the demographic composition of the participating electorates, or is this the case of more of the same types of voters being mobilized by RCV? Related to this, additional work is needed to assess if race/ethnic bias in participating electorates is mitigated or exacerbated if RCV causes turnout to increase. Finally, we acknowledge that even with the controls included in our models and state fixed effects, there may be something particular to the type of places that adopt RCV that correspond with increased turnout that remains unspecified here. Voter education efforts associated with RCV, or the activities of proponents of RCV reforms, for example, may somehow affect turnout in ways above and beyond candidate campaign effects we presume to be associated with RCV. Yet our models do ac count for a person’s previous voting history, and thus provide a rather rigorous test for differences in turnout between places using RCV and those that do not. All of this said, our study provides evidence that increased turnout and voter contating may be associated with use of RCV in the US. CRediT authorship contribution statement E. Dowling: Writing – original draft, Methodology, Formal analysis. C. Tolbert: Writing – original draft, Methodology, Data curation. N. Micatka: Software, Investigation. T. Donovan: Writing – review & editing, Writing – original draft. Table 1 Modeling Recent RCV Adoption vs Adoption 5 or 10 years Prior (2023 File) Declaration of competing interest None. Data availability Data will be made available on request. Appendix Model 1 (State Fixed Effects & No COVI) Model 2 (State FE’s & COVI) Voted in 2019 2.349*** 2.348*** (0.006) (0.006) RCV Jurisdiction Time Since Adoption 0.315*** 0.315*** (1 = 0–5 yrs, 2 = 5–10 yrs, 3 = 10+ yrs) (0.010) (0.010) Age 0.043*** 0.043*** (0.001) (0.001) Age Squared − 0.0002*** − 0.0002*** (7.59[−] [06]) (7.60[−] [06]) Female 0.009 0.009 (0.005) (0.005) Asian − 0.816*** − 0.817*** (0.017) (0.017) Black − 0.260*** − 0.258*** (0.009) (0.009) Latino − 0.648*** − 0.647*** (0.011) (0.011) Race Other − 0.413*** − 0.415*** (0.019) (0.019) Married 0.039*** 0.038*** (0.007) (0.007) Bachelor’s Degree 0.215*** 0.215*** (0.007) (0.007) Income 0.252*** 0.252*** (0.003) (0.003) Catalist Ideology 0.007*** 0.007*** (0.0001) (0.0001) COVI 2020 − 24.262***** (4.562) N 2,469,097 2,464,657 Log-likelihood − 536147.22 − 535625.05 AIC 1072422 1071376 State Fixed-Effects Yes Yes Note: Unstandardized logistic regression coefficient, robust standard errors in parentheses. *p < 0.05, p < 0.01. *** p < .001.** Source: Catalist 2023 national voter file 1% random sample Table 2 Logistic regression Predicting Recall Hearing/Seeing Campaign Activity comparing RCV and Plurality Cities, 2013–2014 RCV 0.14**** (0.070) Exposure 0.34***** (0.020) Democrat 0.12 (0.078) Republican − 0.08 (0.109) Age 0.01***** (0.002) Education 0.21***** (0.037) Income 0.05***** (0.020) Female 0.12** (0.070) Black − 0.52***** (0.114) Asian − 0.29** (0.168) Latino 0.08 (0.131) Other Race − 0.28** (0.158) Married − 0.13 (0.080) 2013 − 0.36***** (0.075) Constant − 2.02***** (0.223) N 4888 Note: Unstandardized logistic regression coefficients, robust standard errors in parentheses. • p < .1. ** p < .05.** *** p < .01.** Source: 2013–2014 Rutgers-Eagleton Ranked Choice Voting Survey. Table 3 Local Jurisdictions with RCV (i.e., places coded as 1 on our RCV variable) Jurisdiction Year Adopted RCV Year First Used RCV Jurisdiction Year Adopted RCV Year First Used RCV Maine (State) 2016 2018 Elk Ridge, UT 2021 2021 Berkeley, CA 2004 2010 Genola, UT 2021 2021 Oakland, CA 2006 2010 Goshen, UT 2021 2021 San Francisco, CA 2002 2004 Herber, UT 2021 2021 San Leandro, CA 2000 2010 Lehi, UT 2021 2021 Basalt, CO 2002 2020 Magna, UT 2021 2021 Takoma Park, MD 2006 2007 Midvale, UT 2021 2021 Cambridge, MA 1940 1941 Millcreek, UT 2021 2021 Easthampton, MA 2019 2021 Moab, UT 2021 2021 Bloomington, MN 2020 2021 Newton, UT 2021 2021 Minneapolis, MN 2006 2009 Nibley, UT 2021 2021 Minnetonka, MN 2020 2021 Payson, UT 2019 2019 St. Louis Park, MN 2018 2019 River Heights, UT 2021 2021 Saint Paul, MN 2009 2011 Riverton, UT 2021 2021 Las Cruces, NM 2018 2019 Salt Lake City, UT 2021 2021 Santa Fe, NM 2008 2018 Sandy, UT 2021 2021 New York City, NY 2019 2021 Springville, UT 2021 2021 Benton (County), OR 2016 2020 South Salt Lake, UT 2021 2021 Bluffdale, UT 2021 2021 Vineyard, UT 2019 2019 Cottonwood Heights, UT 2021 2021 Woodland Hills, UT 2021 2021 Draper, UT 2021 2021 All cities in ME 2020 2019 References Angrist, J., Pischke, J.S., 2009. Mostly Harmless Econometrics. Princeton University Press. Anzi, S., 2014. Timing and Turnout: How Off-Cycle Elections Favor Organized Groups. University of Chicago Press. Ansolabehere, S., Hersh, E., 2012. Validation: what big data reveal about survey misreporting and the real electorate. Polit. Anal. 20 (4), 437–459. Ansolabehere, S., Fraga, B., Schaffner, B.F.G., 2022. The CPS Voting and Registration Supplement overstates minority turnout. J. Polit. 84 (3), 1850–1855. Bowler, S., Donovan, T., Brockington, D., 2003. Electoral Reform and Minority Representation: Local Experiments with Alternative Elections. Ohio State University Press. Burnett, C.M., Kogan, V., 2015. Ballot (and voter)“exhaustion” under Instant Runoff Voting: an examination of four ranked-choice elections. Elect. Stud. 37, 41–49. Burden, B.C., Canon, D.T., Mayer, K.R., Moynihan, D.P., 2014. Election laws, mobilization, and turnout: the unanticipated consequences of election reform. Am. J. Polit. Sci. 58 (1), 95–109. Cantoni, E., Pons, V., 2021. Strict ID laws don’t stop voters: evidence from a U.S. nationwide panel, 2008–2018. Q. J. Econ. 136 (4), 2615–2660. Cormack, L., 2023. More choices, more problems? Ranked Choice Voting errors in New York City. Am. Polit. Res. 1532673X231220640. Donovan, T., Tolbert, C., 2023. Civility in ranked-choice voting elections: does evidence fit the normative narrative? Representation 1–18. Donovan, T., Tolbert, C., Harper, S., 2022. Demographic differences in understanding and utilization of ranked choice voting. Soc. Sci. Q. 103 (7), 1539–1550. Donovan, T., Tolbert, C., Gracey, K., 2019. Self-reported understanding of ranked-choice voting. Soc. Sci. Q. 100 (5), 1768–1776. Donovan, T., Tolbert, C., Gracey, K., 2016. Campaign civility under preferential and plurality voting. Elect. Stud. 42, 157–163. Donovan, T., 2007. A goal for reform: make elections worth stealing. PS Political Sci. Polit. 40 (4), 681–686. Drutman, L., Strano, M., 2021. What we know about ranked-choice voting. New America Report. https://www.newamerica.org/political-reform/reports/what-we-know-abo ut-ranked-choice-voting/. Drutman, L., Strano, M., 2022. Evaluating the effects of ranked-choice voting. New America Report. https://www.newamerica.org/political-reform/reports/evaluatin g-the-effects-of-ranked-choice-voting/introduction-lee-drutman-and-maresa-stran o/. Erikson, R.S., Minnite, L.C., 2009. Modeling problems in the voter identification—voter turnout debate. Election Law J. 8 (2), 85–101. Fraga, B., 2016. Candidates or districts? Reevaluating the role of race in voter turnout. Am. J. Polit. Sci. 60 (1), 97–122. Fraga, B., 2018. The Turnout Gap: Race, Ethnicity and Political Inequality in a Diversifying America. Cambridge University Press. Fraga, B., Holbein, J., 2020. Measuring youth and college student voter turnout. Elect. Stud. 65, 1020–1086. Gainous, J., Wagner, K.M., 2013. Tweeting to Power: the Social Media Revolution in American Politics. Oxford University Press, Oxford. Gerber, A.S., Green, D.P., 2008. Field Experiments and Natural Experiments: Design, Analysis, and Interpretation. W.W. Norton & Company Press. Gerber, A.S., Green, D.P., Larimer, C.W., 2008. Social pressure and voter turnout: evidence from a large-scale field experiment. Am. Polit. Sci. Rev. 102 (1), 33–48. Grimmer, J., Hersh, E., Meredith, M., Mummolo, J., Nall, C., 2018. Obstacles to estimating voter ID laws’ effect on turnout. J. Polit. 80 (3), 1045–1051. Grofman, B., Lijphart, A. (Eds.), 1986. Electoral Laws and Their Political Consequences, vol. 1. Algora Publishing. Hajnal, Z., Kogan, V., Markarian, A., 2021. Who votes: city election timing and voter composition. Am. Polit. Sci. Rev. 116 (1), 374–383. Hajnal, Z., Trounstine, J., 2005. Where turnout matters: the consequences of uneven turnout in city politics. J. Polit. 67, 515–535. Hanmer, M.J., 2009. Discount Voting: Voter Registration Reforms and Their Effects. Cambridge University Press. Hersh, E., Ghitza, Y., 2018. Mixed partisan households and electoral participation in the United States. PLoS One 13 (10), e0203997. Hersh, E.D., 2015. Hacking the Electorate: How Campaigns Perceive Voters. Cambridge University Press. Hersh, E.D., Nall, C., 2016. The primacy of race in the geography of income-based voting: new evidence from public voting records. Am. J. Polit. Sci. 60 (2), 289–303. Igielnik, R., Keeter, S., Kennedy, C., Spahn, B., 2018. Commercial Voter Files and the Study of US Politics. Pew Research Center Report. https://www.pewresearch.org/m ethods/2018/02/15/commercial-voter-files-and-the-study-of-u-s-politics/. John, S., Douglas, A., 2017. Candidate civility and voter engagement in seven cities with ranked choice voting. Natl. Civ. Rev. 106 (1), 25–29. Juelich, C.L., Coll, J.A., 2021. Ranked choice voting and youth voter turnout: the roles of campaign civility and candidate contact. Polit. Govern. 9 (2), 319–331. Kimball, D.C., Anthony, J., 2016. Voter participation with ranked choice voting in the United States. In: Annual Meeting of the. American Political Science Association. Kropf, M., 2021. Using campaign communications to analyze civility in ranked choice voting elections. Polit. Govern. 9, 280–292. Leighley, J.E., Nagler, J., 2013. Who Votes Now? Demographics, Issues, Inequality, and Turnout in the United States. Princeton University Press. Li, Q., Pomante II, M.J., Schraufnagel, S., 2018. Cost of voting in the American states. Election Law J. 17 (3), 234–247. Margetts, H., John, P., Hale, S., Yasseri, T., 2016. Political Turbulence: How Social Media Shape Collective Action. Princeton University Press. McDaniel, J.A., 2016. Writing the rules to rank the candidates: examining the impact of instant-runoff voting on racial group turnout in San Francisco mayoral elections. J. Urban Aff. 38 (3), 387–408. McDaniel, J.A., 2019. Electoral rules and voter turnout in mayoral elections: an analysis of ranked-choice voting. In: Paper Presented at the Election Systems, Reform and Administration Conference. Philadelphia, PA, July 11-12. McDonald, M.P., 2022. From Pandemic to Insurrection: Voting in the 2020 US Presidential Election. De Gruyter Press. McDonald, M.P., Samples, J. (Eds.), 2006. The Marketplace of Democracy: Electoral Competition and American Politics. Brookings Institution Press. Neely, F., McDaniel, J., 2015. Overvoting and the equality of voice under instant-runoff voting in San Francisco. Calif. J. Polit. Pol. 7 (4). Nickerson, D.W., Rogers, T., 2014. Political campaigns and big data. J. Econ. Perspect. 28 (2), 51–74. Parry, J., Barth, J., Kropf, M., Jones, E.T., 2008. Mobilizing the seldom voter: campaign contact and effects in high-profile elections. Polit. Behav. 30, 97–113. Pomante II, M., Schraufnagel, S., Li, Q., 2023. The Cost of Voting in the American States. University Press, of Kansas. Ritter, M., Tolbert, C.J., 2020. Accessible Elections: How the States Can Help Americans Vote. Oxford University Press. Rogers, T., Aida, M., 2014. Vote self-prediction hardly predicts who will vote, and is (misleadingly) unbiased. Am. Polit. Res. 42 (3), 503–528. Rolfe, M., 2012. Voter Turnout: A Social Theory of Political Participation. Cambridge University Press. Rosenstone, S., Hansen, J.M., 1993. Mobilization, Participation, and Democracy in America. Macmillan, New York. Santucci, J., 2022. More Parties or No Parties: the Politics of Electoral Reform in America. Oxford University Press, New York. Santucci, J., 2021. Variants of ranked-choice voting from a strategic perspective. Polit. Govern. 9 (2), 344–353. Schlozman, K.L., Verba, S., Brady, H.E., 2012. The Unheavenly Chorus: Unequal Political Voice and the Broken Promise of American Democracy. Princeton University Press. Schraufnagel, S., Pomante, M.J., Li, Q., 2020. Cost of voting in the American states: 2020. Election Law J. 19 (4), 503–509. Sinclair, B., 2012. The Social Citizen: Peer Networks and Political Behavior. University of Chicago Press. Tolbert, C.J., Mossberger, K., McNeal, R., 2008. Institutions, Policy Innovation, and E- Government in the American States. In: Public Administration Review, vol. 68. University Press of Kansas, pp. 549–563, 3. Verba, S., Schlozman, K.L., Brady, H.E., 1995. Voice and Equality: Civic Voluntarism in American Politics. Harvard University Press. Wendland, J., Carman, E., 2023. New or “normal” election? Understanding ranking activity in New York City’s first ranked-choice voting election. Soc. Sci. Q., vol. 104 (4), 591-604. Wolfinger, R.E., Rosenstone, S.J., 1980. Who Votes? Yale University Press. An Empirical Analysis of the Effect of Ballot Truncation on Ranked-Choice Electoral Outcomes Author: Dickerson, Mallory, Date: 2023-06-09 Collections: Voting Systems Zotero Key: BXZS9HA8 Cite Key: Dickerson2ballotTruncateRCVeffect Zotero Item | Lit Note AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION ON RANKED-CHOICE ELECTORAL OUTCOMES MALLORY DICKERSON, ERIN MARTIN, AND DAVID MCCUNE Abstract. In ranked-choice elections voters cast preference ballots which provide a voter’s ranking of the candidates. The method of ranked-choice voting (RCV) chooses a winner by using voter preferences to simulate a series of runoff elections. Some jurisdictions which use RCV limit the number of candidates that voters can rank on the ballot, imposing what we term a truncation level, which is the number of candidates that voters are allowed to rank. Given fixed voter preferences, the winner of the election can change if we impose different truncation levels. We use a database of 1171 real-world ranked-choice elections to empirically analyze the potential effects of imposing different truncation levels in ranked-choice elections. Our general finding is that if the truncation level is at least three then restricting the number of candidates which can be ranked on the ballot rarely affects the election winner. 1. Introduction The method of ranked-choice voting (also referred to as instant runoff voting, the Hare method, the plurality elimination rule, etc.) has been increasingly used in municipalities and states across the US since the turn of the century. The city of San Francisco began using the method for several municipal offices in 2004, followed by Minneapolis in 2009, Oakland in 2010, and several cities in Utah in 2021. New York City began using ranked-choice voting (RCV) for primary elections for city office in 2021; the Democratic primary election for mayor was the largest rankedchoice election ever held in the US, with almost one million votes cast. RCV has also been used for elections for federal office in Maine (since 2018) and Alaska (since 2022). In ranked-choice elections, voters cast a preference ballot which provides a preference ranking of the candidates. Often, voters do not provide a full ranking of the candidates, instead casting a partial ballot in which only a partial ranking of the candidates is provided. Partial ballots arise for two reasons. First, voters may choose not to provide a complete ranking, deciding to leave some candidates off their ballots. Second, some jurisdictions limit the number of candidates that voters can rank on their ballots. For example, Minneapolis (respectively New York City) allows a voter to rank only three (respectively five) candidates on their ballots, regardless of how many candidates are in the race. We say that an election’s truncation level is the number of candidates which can be ranked on a ballot, so that the truncation level of Minneapolis municipal elections is three, for example. The purpose of this article is to investigate empirically how the choice of truncation level can affect the outcome of a ranked-choice election. 2010 Mathematics Subject Classification. Primary 91B10; Secondary 91B14. Key words and phrases. ranked-choice voting, ballot truncation, empirical results. 1 AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 2 There are two main ways in which an electoral outcome can be affected when a jurisdiction imposes a truncation level: (1) In response to the truncation level, voters may choose to provide a different ranking of their top candidates than they otherwise would if the limit did not exist. For example, in Minneapolis a voter might not rank their three favorite candidates on their ballot because the voter might be worried that none of these candidates have a legitimate chance of winning, and thus the voter might choose to rank three “stronger” but less preferred candidates instead. (2) The RCV winner of an election without an imposed truncation level can be different from the winner when voters can provide a complete ranking, even if voters do not change how they rank the top candidates on their ballot in response to the limit. It is very difficult to investigate the effect of (1) on electoral outcomes because we cannot know how voters would have voted if the limit were removed. However, (2) can be investigated in a variety of ways. The most thorough examination can be found in [8], which primarily used Monte Carlo simulation to investigate how the RCV winner can change depending on the choice of truncation level, assuming voters do not change their preferences as we vary that limit. [8] also used real-world data in their analysis, but their empirical work included only 18 elections, and only 6 of those were from real-world elections for political office (which is our primary focus). We build on their work by using 1171 ranked-choice elections, 1148 of which are political elections from the US and Scotland. Thus, this article can be read as an empirical companion piece to [8], and our work is by far the largest empirical study to date of the potential effects of ballot truncation on electoral outcomes under RCV. The issue of partial ballots and the resulting potential effects on ranked-choice elections has been studied from many angles. [3] analyzes four ranked-choice elections in which the RCV winner did not obtain a majority of the vote because a large proportion of the ballots were partial. Several studies ([2], [4], [9]) study the problem of which candidates could become the RCV winner if partial ballots were filled in to create full rankings. Other studies ([2], [5], [7]) consider how partial ballots can be used for strategic voting or strategic campaigning, focusing on issues such as truncation or no-show paradoxes. Our work is most similar to [17], which contains an empirical component addressing the issue of how many different candidates could be the RCV winner as the truncation level varies. What sets our empirical work apart from theirs is that we use a much larger database of elections and we include an analysis of the effect of the truncation level on the likelihood of electing the Condorcet winner. 2. Preliminaries Let n be the number of candidates in an election and let TL denote the election’s truncation level, the number of candidates that voters are allowed to rank on their ballots. In a ranked-choice election, voters submit preference ballots, providing a ranking of up to TL candidates on their ballots. The ballots are then aggregated into a preference profile, which shows how many ballots of each type were cast. Table 1 provides an example of a preference profile for an election in which n = TL = 4, and the candidates are labeled A, B, C, and D. The number 57 denotes AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 3 Num. Voters 57 26 51 137 2 38 16 53 72 15 33 1st choice A A A B B C C C D D D 2nd choice B D D C C A B D A A B 3rd choice C A D D D A B C A 4th choice B B B C Table 1. A preference profile with four candidates. that 57 voters cast a ballot ranking A first, B second, and no candidate ranked third or fourth; the other numbers across the top row convey similar information about the number of ballots cast. For all of the elections in our database voters are not required to provide a complete ranking, and the preference profile in Table 1 has many voters who choose not to rank all four candidates. To choose a winner of an election given a preference profile, RCV proceeds in a series of rounds. In each round, a candidate’s first-place votes are counted; a candidate with a majority of the votes is declared the winner. If no candidate achieves a majority then the candidate with the fewest first-place votes is eliminated and their votes are transferred to the next candidate on their ballots who has not previously been eliminated. The method continues in this fashion until a candidate achieves a majority of the remaining votes. We note this method can be extended in a variety of ways to elect multiple candidates given a preference profile; in this article we focus only on the single-winner case. We illustrate the RCV algorithm using the preference profile from Table 1, where we assume TL = 4 so that none of the voters’ preferences are truncated. Example 2.1. Initially, the number of first-place votes for A, B, C, and D are 134, 139, 107, and 120, respectively. No candidate achieves a majority of first-place votes, and thus C is eliminated. Consequently 38 votes are transferred to A, 16 are transferred to B, and 53 are transferred to C, resulting in adjusted vote totals of 172, 155, and 173 for A, B, and D, respectively. There is still no candidate with a majority and thus B is eliminated. Since C has already been eliminated, 137 votes are transferred to A and 2 are transferred to D, and A wins the election with 309 votes to D’s 191. We now illustrate how the use of a truncation level can affect the winner of a ranked-choice election. Example 2.2. Table 2 shows preference profiles we obtain from the profile in Table 1 for truncation levels of 1, 2, and 3. When TL = 1, the RCV algorithm merely selects the candidate with the most first-place votes (referred to as the election’s plurality winner ), which is B with 139 first-place votes. When TL = 2, C is eliminated first, resulting in vote totals of 172, 155, and 173 for A, B, and D, respectively, just as in Example 2.1. However, in this case when B is eliminated D defeats A with 173 votes to A’s 172. In the original example with TL = 4, A was able to defeat D because A received 137 votes from the elimination of B; this is no longer the case as A was these 137 voters’ third choice, and when TL = 2 we do not register voters’ third choices. When TL = 3, the RCV algorithm proceeds exactly as the original example where TL = 4, and A wins the election. AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 4 TL = 1, RCV winner = B Num. Voters 57 26 51 137 2 38 16 53 72 15 33 1st choice A A A B B C C C D D D TL = 2, RCV winner = D Num. Voters 57 26 51 137 2 38 16 53 72 15 33 1st choice A A A B B C C C D D D 2nd choice B D D C C A B D A A B TL = 3, RCV winner = A Num. Voters 57 26 51 137 2 38 16 53 72 15 33 1st choice A A A B B C C C D D D 2nd choice B D D C C A B D A A B 3rd choice C A D D D A B C A Table 2. The preference profile from Table 1 with truncation levels 1, 2, and 3. The previous example leads to two remarks, which have been pointed out by many others (see [8] and [17], for example). Remark 2.3. In an n-candidate election, the RCV winner when TL = n is the RCV winner when TL = n − 1. Remark 2.4. In an n-candidate election, as we vary the truncation level from 1 to n the number of different RCV winners is at most n − 1. Example 2.2 demonstrates Remark 2.4: depending on the truncation level, the RCV winner can be A, B, or D. By Remark 2.3, it is not possible for the fourth candidate C to be an RCV winner as well under some truncation level. [17] show that the bound in Remark 2.4 is sharp, in that for any n there exists an n-candidate election with n − 1 different RCV winners as TL is varied from 1 to n − 1. In addition to analyzing the effect of ballot truncation on the RCV winner, [8] investigates how the truncation level affects the likelihood that RCV elects the Con_dorcet winner of the election. The concept of a Condorcet winner dates back to the_ social choice debates between Jean-Charles de Borda and the Marquis de Condorcet (see [1] for an interesting exposition of this history). A candidate is a Condorcet winner if this candidate beats all other candidates in head-to-head matchups; such candidates receive much attention in the social choice literature because they are seen as “strong” or “deserving” candidates. To provide an example, note that for the election in Table 1 candidate A defeats B in a head-to-head matchup because 57 + 26 + 51 + 38 + 53 + 72 + 15 = 312 voters prefer A to B, while only 188 voters prefer B to A. Similarly, 254 voters prefer A to C while 246 prefer C to A, and 309 voters prefer A to D while 191 prefer D to A. Thus, A is the Condorcet winner of this election. Example 2.2 shows that the choice of truncation level can effect whether the Condorcet winner wins the election under RCV. In our description of the Condorcet winner of the election in Table 1 we assumed that the candidates left off a voter’s ballot are all tied for last place in that voter’s preferences; that is, we choose to process partial rankings using the weak order model [14]. There are other ways to process partial ballots which would potentially |Num. Voters|57|26|51|137|2|38|16|53|72|15|33| |---|---|---|---|---|---|---|---|---|---|---|---| |1st choice|A|A|A|B|B|C|C|C|D|D|D| |Num. Voters|57|26|51|137|2|38|16|53|72|15|33| |---|---|---|---|---|---|---|---|---|---|---|---| |1st choice 2nd choice|A B|A D|A D|B C|B C|C A|C B|C D|D A|D A|D B| |Num. Voters|57|26|51|137|2|38|16|53|72|15|33| |---|---|---|---|---|---|---|---|---|---|---|---| |1st choice 2nd choice 3rd choice|A B|A D|A D C|B C A|B C D|C A D|C B D|C D A|D A B|D A C|D B A| AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 5 change our analysis, but we prefer the weak order model because it deals with exactly the information provided by the voters. Furthermore, election offices which run RCV elections process ballots under this model, although they do not use that language. To conclude this section, we list the questions in which we are interested, which we use real-world ranked-choice data to investigate. • Question 1: For a fixed truncation level TL, what percentage of elections satisfying TL < n − 1 have the property that the RCV winner when using TL is different from the RCV winner when preferences are not truncated? • Question 2: For 1 ≤ k ≤ n − 1, what percentage of elections have k different winners as we increase TL from 1 to n − 1? • Question 3: As we increase the truncation level, does the likelihood of electing the Condorcet winner increase? Before providing our results, we describe our data. 3. Data Sources Our database of 1171 elections comes from three sources. We briefly describe each source and the number of elections we collected from it. Note that if an election contains a candidate who achieves a majority in the first round then it is not interesting to investigate the effects of truncation; furthermore, to see the full effects of truncation, we are interested only in elections in which voters can express a complete ranking of the candidates if they so choose. Thus, our database consists of elections in which the RCV algorithm goes to at least a second round and the truncation level used in the actual election satisfies TL ≥ n − 1. American Psychological Association: The American Psychological Associa- tion (APA) uses RCV to elect its President and positions on its Board of Directors. Our database includes APA presidential elections from the years 1998-2009 and 2017-2021, as well as four elections for Board of Director positions from the years 2017-2021. These elections were collected by the third author for [12]. The elections from 1998-2009 are available at [10], and the 2017-2021 elections were provided directly by the APA. As far as we are aware, data from 2010-2016 is not available. Our database includes 23 elections from the APA, all of which are all single-winner. American Ranked-Choice Political Elections: As mentioned in the intro- duction, many American jurisdictions have started using RCV. Hundreds of RCV elections have occurred across the US since 2004, mostly for municipal office such as mayor or city council positions. Many of these elections are not useable for our purposes because they contain a majority candidate or they satisfy TL < n − 1. Our database includes 82 American political elections, 80 of which are single-winner. These elections were collected by the third author for [12], and most of them are now publicly available at the FairVote data repository [13]. Scottish Local Government Elections: For the purposes of local govern- ment, Scotland is divided into 32 council areas, each of which is overseen by a council. These council areas are roughly analogous to counties in the US, and the councils provide a range of services that Americans associate with services provided by county or municipal governments. Each council area is divided into wards, and every five years councilors from each ward are elected to represent the ward on the council using a multiwinner version of RCV called single-transferable vote. Scottish wards have used this version of RCV since 2007. Data from that year is essentially AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 6 n 3 4 5 6 7 8 9 10 11 12 13 14 Total APA 6 17 23 Amer. 24 31 13 8 4 1 1 82 Scot. 2 35 117 210 286 205 112 63 22 8 5 1 1066 Total 32 66 147 218 290 205 113 64 22 8 5 1 1171 Table 3. The number of elections with a given number of candidates from each data source. unavailable; most of the vote data for the 2012 and 2017 election cycles is available at [16]. The third author collected data for the 2022 elections from various council election offices for [11]. Sometimes by-elections are held in off-cycle years to fill a seat left vacant by a death or resignation; the third author collected the ballot data for as many of these by-elections as possible directly from council election offices (often, this data was not publicly posted). Of the Scottish elections 29 are single-winner and 1037 are multiwinner. We acknowledge it is not optimal that so many of our elections are multiwinner, since we study the effects of truncation only for RCV in the single-winner case. However, these multiwinner Scottish elections provide preference data for candidates in political elections, and thus are still of use for our purposes. In particular, these multiwinner elections tend to produce very close results from a single-winner perspective, and analyzing them could help provide an upper bound for the frequency with which truncation affects electoral outcomes (we expect truncation to matter infrequently if elections are not “close” in some sense; the most extreme example of an election which is not close is an election with a majority candidate, where the truncation level is irrelevant). In all, 132 of our elections are single-winner and 1039 are multiwinner. Table 3 summarizes the database by the number of candidates in each election, where the number of candidates does not include write-ins. Given a fixed truncation level, the question of whether the RCV winner would differ from the RCV winner in the truncated election is interesting only if TL < n − 1, and thus the table also shows how many elections are of interest for a fixed truncation level (we refer to such elections as useable). For example, if we use TL = 3 as Minneapolis does, then our database contains 1073 elections (corresponding to n ≥ 5) in which the RCV winner with truncation might differ from the RCV winner without. As mentioned in the introduction, the size of our database is partly what sets our work apart from previous studies. The largest prior empirical study of the effect of ballot truncation on the RCV winner is [17], which analyzes 168 ranked-choice elections (the ballot data for these elections is available from [10]). Our database includes some of these elections but, in our view, many of the elections analyzed in [17] are not ideal for this analysis. Some of their elections contain majority candidates and in some voters were not allowed to provide a complete ranking. In addition to the size of our database, we also include a detailed analysis of the effect of truncation on the Condorcet winner, which has only been done previously by [8] in 18 elections. We conclude this section by noting that real-world data is often messy and presents challenges beyond the choice of how to process partial ballots. For example, voters sometimes give two candidates the same ranking, resulting in a ballot error AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 7 known as an overvote. When this occurs, most election offices in the US choose to cut off the voter’s ballot at the ranking where the overvote occurs, ignoring the candidate choices at that ranking and any subsequent rankings. Also, sometimes voters skip multiple rankings on their ballots, perhaps casting a ballot in which A is ranked first and D is ranked fourth, but no candidates are ranked second or third. Most election offices in the US interpret this ballot as saying that A is the voter’s first choice and D is the second choice, just ignoring the skipped rankings, but a few election offices disregard any rankings on a ballot which occur after two skipped rankings. When we process ballots we choose to process them in line with how we think the given jurisdiction processes them (although sometimes there is ambiguity). Fortunately, most of our data avoids these issues, as the APA and Scottish elections are provided to us in a clean fashion; the APA and the Scottish election offices deal with the aforementioned issues before sharing the data, essentially providing their data in the form of Table 1. Thus, we have to deal with issues of voter error only in the 82 American political elections. 4. Results We now present our results, using our elections database to answer Questions 1-3 from Section 2. As mentioned in the introduction, our results are meant to be read as an empirical complement to the results of [8], who mainly use Monte Carlo simulations based on a spatial and random model of voter preferences to provide theoretical results. Thus, we contrast our findings with theirs as part of our discussion, although [8] study only elections with n ∈{4, 5, 6}. Question 1: For a fixed truncation level TL, what percentage of elections satisfying TL < n − 1 have the property that the RCV winner when using TL is different from the RCV winner when preferences are not truncated? Table 4 shows, for a fixed TL, the percentage of elections in which the RCV winner without truncation is not the RCV winner with the truncation level imposed. For TL = 1, we see that the percentage of elections in which the candidate who receives the most first-place votes (the election’s plurality winner) is also the RCV winner without truncation in 76.1% of the elections in the database. Thus, for more than three-quarters of our elections the RCV algorithm is in some sense unnecessary, as the algorithm simply chooses the plurality winner and any preference information past the first ranking does not make a difference. Note that when TL = 3, which is the case for elections in municipalities such as Minneapolis, the two winners with and without truncation agree in 96.8% of the elections, suggesting that truncating at depth 3 almost never makes a difference as long as voters do not adjust their stated preferences in response to the imposed truncation level. Furthermore, we did not include all possible truncation levels in order to keep our table a manageable size, but we note that there is one interesting Scottish election (which in reality was multiwinner) with 11 candidates in which the winner at TL = 9 is different from the winner with no truncation. In the 2012 Ward 3 council election of the North Ayrshire council area, candidate John Ferguson is the RCV winner for all truncation levels 1 ≤ TL ≤ 10 except TL = 9, where Joe Cullinane is the winner. This is the only election in which we observe a difference in winners for TL > 5. As often occurs when comparing empirical to theoretical findings, our percentages are much lower than what is reported by [8] under either of their models. AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 8 TL 1 2 3 4 5 6 7 Useable elections 1171 1139 1073 926 708 418 213 % Winners Agree 76.1% 89.7% 96.8% 98.9% 99.7% 100% 100% Table 4. For a given TL, the percentage of elections in which the RCV winner without truncation is the winner with the imposed truncation level. For example, under their random model the probabilities that the winner with no truncation is different from the winner with TL = 2 in 4, 5, and 6 candidate elections are approximately 20%, 30%, and 40%, respectively. The probabilities for TL = 2 are similar under their spatial model. Theoretical models are often used to establish upper bounds for the probabilities of various voting phenomena, and empirical work investigates how far the real-world data is from these theoretical upper bounds. In this case, what we see in practice is significantly different than what was produced by the theoretical models. Question 2: For 1 ≤ k ≤ n − 1, what percentage of elections have k different winners as we increase TL from 1 to n − 1? We do not find any elections which demonstrate more than three different winners as we vary the TL from 1 to n − 1. The database contains 14 elections which demonstrate three different winners; all of these elections are from the set of multiwinner Scottish elections. These elections account for only 1.5% of the elections satisfying n ≥ 4. The database contains 293 elections which demonstrate two different winners, accounting for 25.0% of the database. 73.8% of the elections produce the same RCV winner regardless of the imposed truncation level. The top image of Figure 1 shows these results broken down by the number of candidates. For example, the figure shows that of the 218 6-candidate elections in the database, three elections produce three different winners as TL varies from 1 to 5, 62 produce two different winners, and 153 produce only one winner. Complete details about these results are available at [6]. Recall from Remark 2.4 that the maximum number of different winners is n _−_ 1; we do not observe any such extreme outcomes in our data except for the uninteresting case when n = 3. The elections with 3 different winners come from elections satisfying n ≥ 6. Furthermore, when an election produces more than one winner, often this is because the plurality winner is different from the RCV winner at all other truncation levels. The bottom image of Figure 1 shows the number of different winners broken down by number of candidates, where we use only truncation levels satisfying 2 ≤ TL ≤ n − 1. We expect the plurality winner to differ from the RCV winner in a significant number of elections because the calculation of a plurality winner does not require preference data, and thus it is not particularly interesting if the plurality winner causes an increase in the total number of different winners. Figure 1 shows that the handful of elections which produced three different winners all achieved this outcome by having the plurality winner be different from the winners at all other truncation levels. If we use any preference data from the ballots past the first ranking, the maximum number of different winners we see is only two. Figure 2 is the version of the top image of Figure 1 where we use only the 132 single-winner elections in the database. For these elections we see at most two different winners. Furthermore, most of the elections which produce two winners AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 9 Figure 1. (Top) The number of different winners in each election as TL varies from 1 to n−1, separated by the number of candidates. (Bottom) The number of different winners in each election as TL varies from 2 to n − 1. are “uninteresting” in the sense that we get two winners only because the plurality winner is different from the winner at all other truncation levels. There are only three of these elections in which the RCV winner at a truncation level greater than 1 is different from the actual RCV winner. We list these elections below. AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 10 Figure 2. The number of different winners in each of the 132 single-winner elections as TL varies from 1 to n − 1. • The June 2021 Republican primary election for the District 50 city council seat in New York City. Mario Kepi wins for TL ∈{1, 2} and David Carr wins for TL ∈{3, 4}. • The 2021 Scottish by-election in the Isle of Bute ward of the Argyll and Bute council area. Kim Findlay wins for TL ∈{1, 2, 3} and Liz McCabe wins for TL = 4. • The 2021 Scottish by-election 2021 in the Patrick East/Kelvindale ward of the Glasgow City council area. Abdul Bostani wins for TL ∈{1, 2} and Jill Brown wins for TL ∈{3, 4, 5}. As was the case with Question 1, for this question we find much lower levels of winner disagreement than do [8]. For example, under their spatial model, in 6candidate elections their simulations return estimated approximate probabilities of 45%, 55%, and 5% that an election produces 1, 2, or 3 different winners, respectively. Their simulations also returned a handful of 6-candidate elections with 4 different winners. From Figure 1 we see that these relatively large disagreement probabilities are not found in our data for any number of candidates. Question 3: As we increase the truncation level, does the likelihood of electing the Condorcet winner increase? To investigate the effects of truncation on the likelihood of electing the Condorcet winner, only some of the elections in the database are relevant for our analysis. We say that an election is Condorcet-useable if the election contains a Condorcet winner and the Condorcet winner is the RCV winner for some truncation level. If an election is not Condorcet-useable then it is not useful for investigating the effect of truncation on whether the Condorcet winner is the RCV winner. Of the 1171 elections, 13 do not contain a Condorcet winner and 52 have a Condorcet winner AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 11 TL 1 2 3 4 5 6 7 C-useable elections 1106 1074 1011 870 669 397 207 % CW wins 77.2% 90.2% 96.9% 98.4% 98.8% 98.5% 98.6% Table 5. The percentage of Condorcet-useable elections in which the Condorcet winner is the RCV winner under the given TL. but this candidate is not the RCV winner for any truncation level, resulting in 1106 Condorcet-useable elections. The Condorcet results are given in Table 5. The table shows that, in general, a higher truncation level leads to an increased likelihood that the Condorcet winner will win the election. Furthermore, it seems that the increased likelihood tends to level off at approximately 98.5%, and this convergence occurs by TL = 4. However, in an individual election it is not always the case that a higher TL is better for the Condorcet winner. There are seven elections where the Condorcet winner isn’t the RCV winner for TL = n − 1 but is the RCV winner for a smaller truncation level. Also, there are 21 elections with the property that there exist i < j < k where the Condorcet winner is the RCV winner for TL values of i and k but is not the RCV winner for TL = j. Thus, there is some Condorcet winner “non-monotonicity” in the data. We note that the single-winner elections tend to produce less interesting Condorcet results than the multiwinner elections. Of the 132 single-winner elections only four (respectively one) elections satisfy that the Condorcet winner is not the RCV winner for TL = 2 (respectively TL = 3), and if TL > 3 then the Condorcet winner and RCV winner always agree (although our data pool shrinks considerably for larger TL values). Furthermore, we do not observe any of the non-monotonicity mentioned above in any of the single-winner elections. There are two potential reasons that we see more tame behavior in these elections. First, we simply have far fewer single-winner elections our database; second, the Condorcet dynamics of multiwinner elections might be different and more complicated. As with the previous questions, our Condorcet results produce much less disagreement than the results of [8]. For example, under their random model the probability that the Condorcet winner (assuming one exists) is the RCV winner with TL = 2 is less than 80% for each n ∈{4, 5, 6}. 5. Ballot Sampling The results of the previous section suggest that the choice of truncation level generally has little effect on the RCV winner when using vote data from real-world elections, especially when TL > 1. In particular, except for the uninteresting case of n = 3, we found no elections which produce the theoretical maximum of n − 1 different winners as TL varies from 1 to n − 1. Even though our database is large, to investigate the possible effects of ballot truncation in data like ours we use our elections to run simulations where we generate new elections by sampling ballots with replacement. This technique is common in empirical voting analyses; see [15] and [17], for example. We use this technique to investigate the worst-case scenario for the effects of ballot truncation when using elections from our database. AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 12 Before presenting our simulation results we describe the methodology. For each election in the database, we repeated the following steps 1000 times in order to generate 1000 new elections (we refer to such a new election as a pseudoprofile). (1) Randomly sample min{1001, number of voters} ballots from the election with replacement. (2) Calculate the number of different RCV winners in the generated pseudoprofile as we vary TL from 1 to n − 1. We choose 1001 ballots as the size of our sample (except in the few elections where the number of voters is less than 1001) due to limits of computation time, and because we are interested in the worst-case scenario. While 1001 is not a small sample size, its relative smallness compared to the electorate size in some of our larger elections might result in ballot data which produces interesting outcomes. Once the 1000 runs are complete, we record the maximum number of different winners from the 1000 generated pseudoprofiles and assign this number to the original election. We then use these numbers to build a “worst-case” version of Figure 1, based on the maximum number of different winners observed for each election across the 1000 generated pseudoprofiles. To see how this process plays out for a given election, we analyze a city council election in Berkeley, CA. Example 5.1. The 2014 ranked-choice election for the City Council seat in Dis- trict 8 of Berkeley, CA, contained the four candidates George Beier, Mike Cohen, Jacquelyn McCormick, and Lori Droste. In the actual election, the first round vote totals for the four candidates (in alphabetical order) were 1198, 1165, 837, and 1318. McCormick was eliminated, resulting in adjusted vote totals of 1473, 1300, and 1614 for Beier, Cohen, and Droste, repectively. After Cohen was eliminated, Droste narrowly won with 2072 votes to Cohen’s 2056. In the actual election, Droste is the RCV winner for all truncation levels. However, as the narrow vote margins suggest, if we use ballot sampling to generate a pseudoprofile then it would not be surprising to obtain an election with different winners for different truncation levels. As described above, we generated 1000 pseudoprofiles each with 1001 ballots, where the ballots are chosen at random with replacement. Of these 1000 generated elections, 614 produced only one winner, 383 produced two, and three pseudoprofiles produced three different winners. Thus, it is possible (although not likely) that vote data similar to this election could produce three different winners for the three different truncation levels. Since three is the maximum number of different winners observed, we assign a value of three to this election when constructing a new figure. We note this election shows that using a sample of size 1001 helps create a larger maximum number of observed different winners. If we use the original electorate size then all generated pseudoprofiles produce only two different winners (Beier or Droste) for any truncation level. The results of our ballot sampling are summarized in Figure 3. The complete results are available at [6]. 308 elections produce only one winner across all truncation levels in every generated pseudoprofile, 546 produce a maximum of two different winners for some pseudoprofile, 297 produce a maximum of three different winners for some pseudoprofile, and 20 produce four different winners. Even with using a maximum of different winners across 1000 generated pseudoprofiles, the expected AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 13 Figure 3. For each election, the figure shows the maximum number of different winners as TL varies from 1 to n − 1 across the 1000 generated pseudoprofiles. maximum number of winners across all truncation levels is only 2.02, suggesting that seeing an average of two different winners is the worst-case scenario in realworld elections. Even when using our ballot sampling methodology, we only see the theoretical maximum of n _−_ 1 different winners generated from a single election (Example 5.1) with n > 3. In general, real-world data is not likely to produce a number of different winners anywhere close to that theoretical maximum. We again emphasize that Figure 3 represents a worst-case outcome in a number of ways. First, we use a relatively small sample of size 1001 when generating a new pseudoprofile. Second, we record only the maximum number of different winners from the 1000 runs and that maximum generally occurs infrequently, as was the case with the Berkeley election in Example 5.1. We generated a total of 1,073,000 pseudoprofiles with five or more candidates, and only 62 pseudoprofiles (coming from 20 different elections) produced four different winners. Figure 4 shows the sampling results when we disregard TL = 1 (top image) and when we consider only the 132 single-winner elections in the database (bottom image). As can be seen from the figure, we only obtain four different winners in our sampling data because the plurality winner differs from the winner at all other truncation levels in the pseudoprofiles that returned four winners. For the singlewinner case, only 10 elections generated a pseudoprofile with three different winners, and this outcome was very improbable. Of the 10,000 generated pseduoprofiles from these 10 elections, 141 produced three different winners. The single-winner election which generated the most pseudprofiles with three different winners is a 2009 city council election in Aspen, CO, which generated 59 pseudoprofiles with three different winners. AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 14 Figure 4. (Top) Our sampling results, disregarding TL = 1. (Bottom) Our sampling results for only the single-winner elections in the database. 6. Conclusion To analyze the potential effects of truncating ballots on the winner of a rankedchoice election, we empirically investigated ballot truncation in a large database of elections. The elections we used are ideal for this analysis because they do not contain a majority candidate and voters were allowed to provide a complete preference ranking of the candidates. Assuming voters do not change their stated preferences for the top candidates on their ballot if a truncation level were imposed, our data suggests that the imposition of a truncation level greater than one rarely changes the RCV winner. Therefore, when jurisdictions such as Minneapolis or AN EMPIRICAL ANALYSIS OF THE EFFECT OF BALLOT TRUNCATION 15 New York City impose truncation levels of 3 and 5, respectively, the effect on the RCV winner likely manifests only insofar as voters choose to provide an insincere ranking of their top candidates in response to the truncation level. The extent to which voters behave in this way, and the question of whether such voter adaptation is undesirable, are beyond the scope of this article and represent fertile ground for future research. References [1] J. Barnett, The French Connection: Borda, Condorcet and the Mathematics of Voting Theory, Convergence, 17 (2020). [2] D. Baumeister, P. Faliszewski, J. Lang, J. Rothe, Campaigns for lazy voters: truncated ballots. In AAMAS, 577–584, (2012). [3] C.M. Burnett, V. Kogan, . Ballot (and voter) “exhaustion” under Instant Runoff Voting: An examination of four ranked-choice elections, Electoral Studies, 37 (2015): 41-49. [4] Y. Chevaleyre, J. Lang, N. Maudet, J. Monnot, Possible winners when new candidates are added: The case of scoring rules. In AAAI, (2010). [5] P.C. Fishburn, S.J. Brams, Manipulability of voting by sincere truncation of preferences. Public Choice, 44 (1984), 397–410. [6] Github site Truncated Voting. https://github.com/martinerin/TruncatedVoting, (2023). [7] E. Kamwa, Scoring rules, ballot truncation, and the truncation paradox, Public Choice, 197 (2022), 79–97. [8] D.M. Kilgour, J.C. Gregoire, A.M. Foley, The prevalence and consequences of ballot truncation in ranked-choice elections, Public Choice, 184 (2020): 197–218. [9] K. Konczak, J. Lang, Voting procedures with incomplete preferences. In IJCAI Multidisci_plinary Workshop in Advances in Preference Handling, volume 20, (2005)._ [10] N. Mattei, T. Walsh, Preflib: A library for preferences. In ADT (2013), 259–270. Springer. [11] D. McCune, A. Graham-Squire, Monotonicity Anomalies in Scottish Local Government Elections, (2022), Preprint. [12] D. McCune, L. McCune Does the Choice of Preferential Voting Method Matter? An Empirical Study Using Ranked Choice Elections in the United States. Representation, (2022). https: //doi.org/10.1080/00344893.2022.2133003 [13] D. Otis, Single winner ranked choice voting CVRs. https://doi.org10.7910/DVN/AMK8PJ, Harvard Dataverse, V5 (2022). [14] S. Popov, A. Popova, M. Regenwetter, Consensus in Organizations: Hunting for the Social Choice Conundrum in APA Elections, Decision 1 (2014), 123–146. [15] M. Regenwetter, A. Kim, A. Kantor, M-H Ho, The unexpected empirical consensus among consensus methods, Psychological Science 18 (2007), 559–656. [16] A. Teale. Local Elections Archive Project. Accessed April 16, 2022. https://www. andrewteale.me.uk/leap/. [17] Tomlinson, K., Ugander, J., and Kleinberg, J. Ballot Length in Instant Runoff Voting. (2022), Preprint: https://arxiv.org/pdf/2207.08958.pdf. Mallory Dickerson, William Jewell College, 500 College Hill, Liberty, MO, 640681896 Email address: dickersonm.19@william.jewell.edu Erin Martin, Department of Mathematics and Data Science, William Jewell College, 500 College Hill, Liberty, MO, 64068-1896 Email address: martine@william.jewell.edu David McCune, Department of Mathematics and Data Science, William Jewell College, 500 College Hill, Liberty, MO, 64068-1896 Email address: mccuned@william.jewell.edu Parochial Altruism and Political Ideology Author: Brewer, Marilynn, Date: 2022-07-19 Collections: NeuroPsychoLinguisticPolitics Zotero Key: KMR2V9HZ Cite Key: Brewer22parochPolitAltruism Zotero Item | Lit Note Political Psychology, Vol. 44, No. 2, 2023 doi: 10.1111/pops.12852 Parochial Altruism and Political Ideology Marilynn B. Brewer Ohio State University Nancy R. Buchan University of South Carolina Orgul D. Ozturk University of South Carolina Gianluca Grimalda Kiel Institute for the World Economy Parochial altruism refers to the propensity to direct prosocial behavior toward members of one’s own ingroup to a greater extent than toward those outside one’s group. Both theory and empirical research suggest that parochialism may be linked to political ideology, with conservatives more likely than liberals to exhibit ingroup bias in altruistic behavior. The present study, conducted in the United States and Italy, tested this relationship in the context of the COVID- 19 pandemic, assessing willingness to contribute money to charities at different levels of inclusiveness— local versus national versus international. Results indicated that conservatives contributed less money overall and were more likely to limit their contribution to the local charity while liberals were significantly more likely to contribute to national and international charities, exhibiting less parochialism. Conservatives and liberals also differed in social identification and trust, with conservatives higher in social identity and trust at the local and national levels and liberals higher in global social identity and trust in global others. Differences in global social identity partially accounted for the effects of political ideology on donations. KEY WORDS: parochialism, altruism, political ideology, moral foundations, social identity Like climate change, the COVID- 19 pandemic has challenged our ability as humans to override individual, local, and national self- interest and cooperate at a global level (Muldoon et al., 2021). However, existing accounts of cooperation and human altruism suggest a parochial character for prosociality (Choi & Bowles, 2007; Yamagishi & Kiyonari, 2000), that is, a higher propensity to extend benefits to members of one’s own ingroup rather than to member of outgroups (Bernhard et al., 2006; DeDreu et al., 2010; Romano et al., 2017; Romano, Sutter, Liu, Yamagishi, & Balliet, 2021). 383 0162-895X © 2022 The Authors. Political Psychology published by Wiley Periodicals LLC on behalf of International Society of Political Psychology. Published by Wiley Periodicals, LLC., 350 Main Street, Malden, MA 02148, USA, 9600 Garsington Road, Oxford, OX4 2DQ, 384 M.B. Brewer et al. Most research on parochialism poses the question whether prosociality is greater when directed toward ingroup members than toward outgroup members, where the two groups are mutually exclusive. Ingroup bias in this form has been demonstrated across a wide variety of group identities, including religion, ethnicity, and political groups (Balliet et al., 2014). We propose, however, that another way to define parochialism is in terms of the inclusiveness of ingroup identities. For example, empirical studies of parochial altruism often define the ingroup in terms of national identity (e.g., Dorrough & Glöckner, 2016; Romano, Sutter, Liu, Yamagishi, & Balliet, 2021), asking whether individuals are more likely to act cooperatively or prosocially toward others who are members of their own nation compared to others from outgroup nations. Yet, in contemporary societies, ingroups come at different levels of inclusiveness— small, local groups are frequently nested within larger collectives (such as villages within states within nations) (Turner et al., 1987), and nations themselves are subgroups of a global community which can also be a social identity (McFarland et al., 2012, 2019). In contrast to mutually exclusive ingroup- outgroup divisions, with nested social identities an ingroup at the subgroup level is included within superordinate groups. Thus, benefiting the superordinate also benefits members of the ingroup as well as members of other subgroups. In such a system of nested group identities, parochialism can be defined as limiting one’s prosocial behavior to ingroups at lower levels of inclusiveness when more inclusive group identities are available (Gallier et al., 2019; Grimalda et al., 2021). The question then becomes whether individuals privilege more local ingroups rather than benefiting more broadly inclusive or universal collectives (Aaldering & Böhm, 2020; Blackwell & McKee, 2003; Enke et al., 2021; Fellner & Lünser, 2014; Grimalda et al., 2021; Wit & Kerr, 2002). Research on social identities at different levels of inclusiveness generally shows that, on average, community identity is stronger than national identity, which in turn is stronger than global human identification (McFarland et al., 2012). Results from field experiments on charitable giving also suggest that giving is largely parochial. In an experiment on the effectiveness of normative appeals, Agerström et al. (2016) found that local norms were more effective than global norms for increasing donations. Further, when individuals are given a choice between charities at different levels of inclusiveness, most choose to give to more local charities rather than to international ones (Knowles & Sullivan, 2017). However, individuals vary in where they invest their primary social identities and the strength of identification at different levels of inclusiveness can vary under different circumstances. Individuals high in “moral universalism” are more likely to donate money globally rather than locally (Enke et al., 2021; McFarland et al., 2012), and identification with the global community is associated with contributing to international causes (Buchan et al., 2011; McFarland et al., 2019). Further, identities are often fluid and highly susceptible to being shaped by globalization (Buchan et al., 2011; Held et al., 2000; Rosenmann et al., 2016). Thus, that prosociality during a global crisis such as the COVID- 19 pandemic is parochial is not a foregone conclusion. This is particularly the case for a pandemic, where averting the pandemic in one country will also benefit other countries within the larger global community, although to a lesser degree (Barragan et al., 2021; Vignoles et al., 2021). It is precisely global shocks like COVID- 19 that can trigger a stronger sense of “humanity as a whole” (Giddens, 1991) that may supersede more parochial identities. One factor that may moderate the relationship between group identity and altruism is political ideology (Romano, Sutter, Liu, & Balliet, 2021). Research on endorsement of personal values indicates that conservatives place significantly greater emphasis than do liberals on conformity, loyalty, and group cohesion (Jost, 2017). Similarly, liberals and conservatives have been Parochial Altruism 385 found to rely on different psychological foundations to construct their moral systems, with liberals consistently showing greatest endorsement of harm/care and fairness foundations (individualizing foundations) whereas conservatives give relatively more importance to authority/respect, ingroup loyalty, and purity/sanctity (communal- binding foundations) (Graham et al., 2009; Sinn & Hayes, 2017). Relatedly, conservatives have been found to be lower than liberals in empathic concern in general (Ruisch et al., 2021) and to have a tighter moral circle, showing greater compassion toward smaller and well- defined groups, while liberals tend to express compassion toward broader groups (Waytz et al., 2019). Finally, cross- national studies of social- value orientation have shown that those with proself orientations are more likely to endorse conservative political preferences than do those with prosocial orientation (Van Lange et al., 2012). All of this research suggests that conservatives may be less altruistic than liberals in general, and more importantly, that their altruistic behavior may be more limited to local ingroup members (i.e., more parochial) than is the case for liberals. Little research has been done to directly test the relationship between political ideology and parochialism in cooperation. In one experimental games study, Aaldering and Böhm (2020, Experiment 1) found that participants who identified as Democrats were more likely to exhibit universal cooperation (contributing to ingroup and outgroup equally) than did those who identified as Republicans. Balliet et al. (2018) tested whether Republicans and Democrats differed in the extent of ingroup bias in cooperation with members of their own versus the other party but did not find a significant effect of ideology. Both Republicans and Democrats expressed greater trust (expectation of cooperation) in members of their own party, and this accounted for their willingness to extend more cooperation to their ingroup relative to the outgroup. Similarly, Fowler and Kam (2007) found that both Republicans and Democrats showed partisan bias in allocations to a partner in a dictator game, and bias was related to strength of party identification. In field research, a large cross- national study by Romano, Sutter, Liu, and Balliet (2021) was conducted to compare liberals and conservatives on cooperation and national parochialism. In this study, participants from 42 different nations played a series of cooperation games with different partners who were either from the participant’s own nation, an outgroup nation, or unidentified. In the game, each player is given an endowment and then decides how much of the endowment to keep for themselves and how much (if any) to give to the other player. Any amount given to the other is then doubled by the researcher so that the more each gives, the greater the total payoff, but the payoff for each individual depends on what the other has decided to give. Cooperation is measured by the amount of endowment sent to the partner by each participant in each game, and ingroup bias (national parochialism) is measured by the difference in each player’s donations to a partner from the same nation compared to one from an outgroup nation or stranger. Combining data from all 42 nations, Romano et al. (2021) found that (1) liberals, compared to conservatives, cooperated more, independent of the other’s group membership, (2) liberals showed less national parochialism (ingroup bias) in cooperation than conservatives, (3) conservatives had higher trust (i.e., expectation of cooperation) in ingroup members relative to outgroup members to a greater extent than liberals, and (4) liberals, compared to conservatives, identified less with their nationality and more with the world as a whole. These results provided strong support for the hypothesis that political ideology moderates the extent to which cooperation is limited by parochial identity and that these differences are associated with differences in social identification and trust. The present study addresses the same questions about the role of political ideology on parochialism as Romano, Sutter, Liu, and Balliet (2021). However, the present study does not 386 M.B. Brewer et al. define parochialism in terms of the difference between ingroup and outgroup cooperation. Instead, we compare altruism directed toward groups at different levels of inclusiveness— local, national, and international. We keep these three levels as nested with one another, which is consistent with the interdependent nature of collective action during a pandemic. This design allows us to assess whether parochialism is more evident at the local or national level and how it compares to nonparochial (global) altruism in the context of a global crisis. Furthermore, our study occurs in the context of the coronavirus pandemic, measuring prosociality in the form of unilateral contributions to either a local, national, or global charity, rather than cooperation in an experimental game where direct interdependence between specific players was more evident. The present study was conducted in the United States and Italy. We ran parallel surveys in two countries in order to assess whether any effects of political ideology generalized outside of the prevailing political context in the United States. In the context of an online survey, participants were given an unexpected monetary bonus and then asked whether they wished to donate some or all of the bonus and, if so, to which one of three charitable organizations providing aid to those affected by the COVID- 19 pandemic. Any amount donated was doubled by the researchers. The three charities participants were given to choose from varied in level of inclusiveness— one at the international (global) level, one at the national (United States or Italy) level, and one at the local (state or region) level. Thus, our decision task provided a measure of both how much participants were willing to give (from 0% to 100%) and to whom they preferred to direct their donation, if any. The online survey also asked participants to rate their political orientation on a liberal- conservative dimension and assessed their degree of social identification and trust with their local (state or region) ingroup, their national ingroup, and the world as a whole. This provided the opportunity to determine whether the findings by Romano, Sutter, Liu, and Balliet (2021) with respect to differences between conservatives and liberals in parochial cooperation, social identity, and trust would extend to a different measure of prosociality (altruistic charitable giving rather than cooperation) and adding local identity as another level of parochialism. Method Recruitment and Selection of Participants In the United States, the survey was conducted during the week between May 13 and May 20, 2020. Respondents (N = 932) were recruited from the Prolific worker pool, screened to include only U.S. citizens or permanent residents and using quota sampling to achieve equal participation across the four Center for Disease Control (CDC) regions of the United States and two age groups (18– 30; over 30). The replication of the survey was conducted in Italy between 11 and June 23, 2020. Respondents in Italy (N = 723) were recruited from the same worker pool used in the United States, thus ensuring roughly comparable socioeconomic characteristics for participants from the two countries and their exposure to identical survey procedures. Although the Prolific worker pool is not a representative random sample of the countries as a whole, we applied quota sampling for region of residence, gender, and age to achieve sufficient variability and to ensure equivalent frequencies in the two countries on these dimensions. The survey questionnaire was translated from the original English version into Italian by bilingual members of the research team and cross- checked with a third party. Parochial Altruism 387 Along with other demographic questions, survey respondents were asked to identify their political orientation on a scale ranging from 1 (very liberal) to 5 (very conservative). For purposes of the present study, those who rated themselves as 1 or 2 were classified as liberals, and those who self- rated as 4 or 5 were classified as conservative. We decided to treat ideology as a categorical variable for a number of reasons. First, it is not reasonable to assume that the 5- point scale is a continuous equal- interval measure (as multiple regression requires). Respondents who chose the midpoint of the scale are a heterogeneous set, some of whom (in the U.S. data) identify as Republicans and some as Democrats. We preferred to make comparisons only between those who were clearly self- identified liberals and conservatives.[1]As partial validation of this categorization, data on self- reported political affiliation in the U.S. sample indicated that of respondents who self- identified as liberals, 75% also identified as Democrats and only 1% as Republicans; of self- identified conservatives, 73% were Republican and 5% Democrats. In addition, mean comparisons are more meaningful and interpretable than regression weights (b coefficients) that assume equal intervals. Across both samples, 889 respondents identified as liberals (483 in the United States; 406 in Italy) and 363 identified as conservative (268 in the United States; 95 in Italy). Although the subsample of conservatives is proportionally smaller in Italy than in the United States, the pattern of donation data and intercorrelations among the measures were the same for both countries (see the online supporting information, Tables S1a– c and S2), so our analyses used the combined samples. These respondents (total N = 1252) constituted the sample for the present study. Procedure After participants had responded to the demographic questions (including the respondent’s state or region of residence), the critical decision task was introduced as part of the survey questionnaire. The decision was preceded by a short paragraph reminding participants of the seriousness of the COVID- 19 pandemic as a medical and economic crisis. Participants then received instructions containing the following information: As a participant in this study being conducted at the height of the coronavirus pandemic crisis, you will be given a bonus payment of $5 (adjusted to 4€ in Italy) in addition to the $3 (€2.5) base pay for completing this survey. You may keep the bonus payment for yourself or you can choose to donate some, all or none of it to one of three charitable organizations that are providing food, medical, and other assistance to individuals and families that have been seriously impacted by the pandemic. The three options for donations are (A) an aid organization in [participant’s state or region] to provide for those most affected by the pandemic across the state (region); (B) a national aid organization to provide for those most affected by the pandemic across the United States (Italy); (C) an international aid organization to provide for those most affected by the pandemic across the world.[2]No specific charities were named so the three options varied only in scope of operations (level of inclusiveness). If you choose to make a donation, you will first have to select one among those three options. Then you will be asked to indicate how much money you want to contribute toward that organization. For any amount of money you contribute, we will double that amount by a matching donation from our funds. (Note that because of doubling, the donation decision had one basic property of a public goods dilemma: Contributed funds resulted in increased benefits at the collective level but 388 M.B. Brewer et al. a loss at the personal level for the individual donor. However, because the decision is unilateral, it most accurately reflects a measure of altruism rather than cooperation or trust.) Respondents were given a comprehension test to be sure they understood the nature of the decision. (A participant would be rejected from the study in the event of failure after three test trials, with no further collection of data.) They were then asked whether they wanted to make a donation to one of the three listed charities or preferred not to donate. If they chose to donate to one of the options they then specified how much, in any amount up to $5 (€4). Measures The amount that participants chose to donate to charity and the decision about which charity to designate constituted the dependent variables in this study. Following the decision task, the online questionnaire contained items assessing respondents’ perceptions of the pandemic and their degree of social identification with and trust of others in their local region, nation, and the world as a whole. Social Identity Scales We used answers to a three- item scale inquiring about the participant’s attachment, closeness, and perception of being a typical member of the local, national, and international community to construct a measure of social identity for each level considered in our study. The items were taken from previous research by Buchan et al. (2009, 2011). Ratings for each item were made on a 4- point scale and then averaged to create an index of strength of social identity at each level of collectivity. Trust The questionnaire included single- item measures of trust in other people in their local community, people in their own country, and people in other countries (the world as a whole). Each item was rated on a 5- point scale ranging from 1 (do not trust them at all) to 5 (trust them completely). Both the social identity and trust measures were adopted from previous research, using the original scale values. Since the trust ratings were made on a single- item scale, 5 points provide for more variance, whereas the social identity measures were three- item scales with 4 points for each item. Ethical Approval Our research plan was approved by the Institutional Review Board for Human Subjects at the University of South Carolina (Pro00099715) for the United States and by the Reggio Emilia Behavioral and Experimental Laboratory for Italy. Participation in the research was voluntary and informed consent was obtained from all participants. There was no deception involved in the study. Any bonus money designated by participants for contributions was doubled and distributed to relevant charities by the researchers. Results Descriptive statistics for our primary measures are reported in Table 1 for the sample as a whole and for conservative and liberal subsamples. (Intercorrelations among the measures are reported in Table S1 in the online supporting information). Parochial Altruism 389 Table 1. Descriptive Statistics Variable Overall (N = 1,250) Liberals (N = 889) Conservatives (N = 363) Total Donation M = .42 M = .48 M = .30 (.00– 1.0) SD = .37 SD = .37 SD = .34 Local Donation M = .21 M = .21 M = .22 (.00– 1.0) SD = .32 SD = .33 SD = .31 Nation Donation M = .12 M = .14 M = .06 (.00– 1.0) SD = .27 SD = .30 SD = .20 World Donation M = .10 M = .13 M = .02 (.00– 1.0) SD = .27 SD = .31 SD = .13 Local Social Id M = 2.50 M = 2.44 M = 2.63 (1– 4) SD = .75 SD = .72 SD = .79 Nation Social Id M = 2.70 M = 2.57 M = 3.02 (1– 4) SD = .77 SD = .75 SD = .71 Global Social Id M = 2.63 M = 2.79 M = 2.23 (1– 4) SD = .86 SD = .84 SD = .80 Local Trust M = 3.07 M = 2.98 M = 3.28 (1– 5) SD = .88 SD = .88 SD = .86 Own Nation Trust M = 2.69 M = 2.58 M = 2.96 (1– 5) SD = .85 SD = .84 SD = .84 Other Nation Trust (1– 5) M = 2.82 M = 2.93 M = 2.56 SD = .80 SD = .74 SD = .88 Donations With respect to the decision whether to donate any bonus money to a charity, liberals were significantly higher than conservatives in percentage of donors (70% vs. 56%; z = 4.67; p < .001) and amount donated. Mean donations (as a proportion of bonus money, including zeros) to each of the charities by liberals and conservatives are presented in Figure 1. In total (across all three charities), liberals donated more of their bonus money (M = .48) than did conservatives (M = .30) (t = 7.57, df = 1250, p < .001, Cohen’s d = .472). However, both liberals and conservatives donated the most to the local (state or region) level and did not differ significantly in mean donations at that level (mean difference = −.014; t = −0.677; df = 1250; p = .50). The difference between the two groups showed in donations to the national level charity (mean difference = .08; t = 4.43; df = 1250, p < .001, Cohen’s d = .276) and the international charity (mean difference = .11; t = 6.61; df = 1250, p < .001, Cohen’s d = .410). Compared to liberals, conservatives showed a significantly bigger drop in donations between local and national levels (t = −4.92; p < .001) and between local and the world charities (t = −3.26; p < .001). Thus, although both groups show a preference to donate to the local charity, conservatives show a greater tendency to limit their charitable donations to the most local level, whereas liberals donate more evenly across the levels and give significantly more than conservatives to the more inclusive charities. Social Identity Figure 2 depicts the mean level of social identification with each of the three collective identities (local, national, world) for liberals and conservatives. Replicating the findings of Romano, Sutter, Liu, and Balliet (2021), compared to liberals, conservatives were higher on local (mean 390 M.B. Brewer et al. |Mean Social Identity Score by Locality and Conservatism Status|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| |||Liber1als||||||Conserv3atives|||| Figure 2. Mean social identity score by locality and conservatism status. Parochial Altruism 391 difference = −.19; t = 4.09, df = 1250, p < .001, Cohen’s d = −.255) and national identification (mean difference = −.45; t = 9.96, df = 1250, p < .001, Cohen’s d = −.602), whereas liberals scored higher on global social identity (mean difference = .55; t = 10.74; df = 1250, p < .001, Cohen’s d = .669). What is of interest in comparing the results for donations (Figure 1) and social identity (Figure 2) is that, although conservatives are higher than liberals on identification with the nation, they are significantly less likely than liberals to give their donations to that level. Instead, conservatives show a greater pull toward donating at the most parochial level despite greater identification at the national rather than local level. Trust In addition to differences between liberals and conservatives in nationalism, Romano, Sutter, Liu, and Balliet (2021) found that liberals reported higher levels of trust (i.e., expectations that others would reciprocate contributions) regardless of nationality, whereas conservatives reported higher trust for ingroup members than for members of outgroups. Consistent with that finding, the results from our measures of trust in other people showed a pattern that parallels that for social identification (see Figure 3). Conservatives were higher than liberals in trust in people from local community (mean difference = −.30; t = 5.44, df = 1250, p < .001, Cohen’s d = −.339) and own country (mean difference = −.38; t = 7.19, df = 1250, p < .001, Cohen’s d = −.448), but significantly lower in trust in people from other countries (mean difference = .37; t = 7.59, df = 1250, p < .001, Cohen’s d = .474). Thus, conservatives exhibit significantly more differential trust in ingroup members than outgroup members compared to liberals who make less distinction between ingroups and outgroups in |Mean Trust Score by Locality and Conservatism Status|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| |||Libera1ls||||||Conser3vatives|||| Figure 3. Mean trust score by locality and conservatism status. 392 M.B. Brewer et al. their assessments of trustworthiness. Conservative parochialism is reflected in trust ratings as well as behavioral altruism. Relationship Between Trust, Social Identity, and Donation Behavior The parallels among differences between liberals and conservatives in donations to national and global charities and the differences on the dimensions of social identification and trust lead to the question of whether trust and social identity account for the observed differences in donation behavior. In order to explore this relationship, we combined the data on donations to national and global charities[3]We were not able to run regression analyses separately for each level of donations because the distributions of values for national and world donations were highly skewed owing to the large number of 0s, particularly for global donations (where variance for Conservatives in Italy was 0). for purposes of conducting regression analyses. We then created two new variables for social identification and trust that reflected the extent to which individuals scored higher in identity and trust for their local ingroup compared to that for other people in the world. Differential identification was defined as the difference between strength of identification with local community and identification with the global community. Similarly, differential trust was computed as the difference between trust ratings for members of the local community and trust ratings for people in other nations. Political ideology, differential identity, and differential trust, along with country and demographic control variables, were entered into regression models to predict national/ global donations. Results of these regression analyses are reported in Table 2. Results from Model 1 demonstrate that the contrast variable of political ideology (Conservative— Liberal) was highly significant even after controlling for other relevant demographic characteristics. When differential identity and trust were entered into the prediction (Model 2), two results of interest emerged. First, differential identity had a significant main effect on donations, but differential trust added no significant contribution to prediction of donations once social identity was included.[4]Social identification and trust were moderately positively correlated at each level of identity (rs = .36– .44) (see Table S1 in the online supporting information). Second, political ideology still made a significant contribution to prediction even after social identification and trust had been added to the equation. The coefficient for ideology was reduced somewhat (.058 compared to .076; with z value dropping from 8.33 to 6.11) but remained significant (p < .0001). Thus, the effect of political ideology on donations to more inclusive groups was not fully accounted for by differences between conservatives and liberals in global social identity or trust. Table 2. Multiple Regressions Predicting National/Global Donation Model 1 Model 2 Coef. (b) SE z p Coef. (b) SE z p Country .173 .022 7.71 .000 .150 .022 6.69 .000 Age .001 .001 .45 .650 .001 .001 .10 .921 Sex .023 .019 1.20 .229 .017 .019 .92 .360 Income group .005 .005 1.13 .259 .008 .005 1.67 .094 Conservatism −.076 .009 −8.33 .000 −.058 .009 −6.11 .000 Diff SocId −.058 . 010 −6.01 .000 Diff Trust −.001 .010 −.02 .984 Parochial Altruism 393 Discussion The primary take- home point from these analyses is that parochial altruism is even more parochial (exclusive) than demonstrated in previous research, particularly among conservatives. When given a choice of contributing to charities at different levels of inclusiveness, conservatives almost exclusively chose the least inclusive (most local) ingroup. Despite a relatively high level of identification and trust in the national ingroup, conservatives rarely directed their donations to charities that would benefit others in the country as a whole, beyond their immediate state or region.[5]Note that, based on the nature of the decision in the present study, we are unable to say whether conservatives would have made donations to the national charity if that had been the only ingroup option. However, the preference for more local benefit is clear from our data. Although liberals also privileged the local charity in their donation choices, they were significantly more likely than conservatives to direct donations to the more broadly inclusive charities at the national or global level and showed significant identification and trust at the global level relative to local and national ingroups. Overall, the present findings are consistent with previous research on the relationship between political ideology and the moral intuitions (virtues) underlying moral judgments (Graham et al., 2009; Haidt & Graham, 2007). Across multiple measures of moral foundations, liberals consistently show highest endorsement and use of moral appeals to protect and care for others (harm/care foundation) and appeals to fairness and justice (fairness/reciprocity foundation). In their development of moral foundations theory, Haidt and Joseph (2004) labeled these two virtues as the individualizing foundations because of their relation to the liberal philosophy tradition and its emphasis on rights and welfare of individuals. Although conservatives also endorse those moral virtues, they give equivalent importance to a cluster of three other moral foundations – ingroup loyalty, authority/respect, and purity/sanctity. Haidt and Joseph labeled this cluster the binding foundations because of their emphasis on group- binding loyalty and duty. In a more recent treatment of the relationship between moral judgments and political ideology, Sinn and Hayes (2017) argue that the individuating foundations associated with liberal ideology are better construed as universalism, as they reflect a broader set of moral commitments and broader sociality than the egocentric individualism implied by moral foundations theory. The present finding that, compared to conservatives, liberals express greater identification with the world as a whole and higher levels of contribution to global charities provides some support for Sinn and Hayes’ reconstrual of the key difference between liberal and conservative ideologies. This reasoning is also supported by results from research on the Identity With All Humanity (IWAH) scale, which is structurally the same as our measure of global social identity. McFarland et al. (2019) report that IWAH correlates positively with the care and justice foundations from the Graham et al. (2011) measure of moral foundations and negatively with loyalty, authority, and purity. In general, the results from the present study parallel results from the cross- national study by Romano, Sutter, Liu, and Balliet (2021) with respect to the relationship between ideology and cooperation, trust, and social identity. As in the earlier research, a clear difference emerged in our survey between liberals and conservatives in global social identity, and this accounted in part for the difference in donation to national and world charities. However, differences in locus of trust did not appear to mediate donation differences in our data. This contrasts with findings from previous research (Balliet et al., 2018; Romano, Sutter, Liu, & Balliet, 2021) where ingroup trust did account for differences in cooperation with ingroups versus outgroups. 394 M.B. Brewer et al. However, our measure of trust was substantially different from that used in these studies, where trust was defined as the expectation that partners (or other group members) would cooperate. It is possible that expectations have a more proximal relationship to cooperative decisions than rating scales like ours that measure generalized trust in others. In addition, the decision task used in the present study (unilateral donations) is most likely not one that requires trust in the recipient, in contrast to cooperation dilemmas where interdependence is more salient. The present findings also complement other recent studies exploring the role of political ideology and other sociopolitical factors in responses to the coronavirus pandemic (Muldoon et al., 2021). In one study comparing China and the United States in adoption of disease- preventative health behaviors, Chan et al. (2021) found that among conservatives in the United States, strength of national identification was (paradoxically) associated with low compliance with recommended health measures, similar to findings in Australia where liberals reported more compliance with physical distancing and handwashing recommendations than conservatives (Cardenas et al., 2021). A related study in Britain (Vignoles et al., 2021) also found that British national identity was not mobilized effectively to promote compliance with health directives to combat the spread of COVID- 19, and these authors concluded that only national identity that is inclusive across subgroups facilitates collective action. This reasoning may help account for why high levels of national identity among conservatives in the present study did not translate to high contributions to national charities compared to contributions at the local level. Finally, in combination with the studies cited previously, the results of the present study attest to the generalizability of the relationship between ideology and parochial altruism, across different manifestations of prosociality (experimental games, distribution decisions, charitable giving, compliance with appeals to collective welfare), and across national contexts. It is imperative to consider the implications of this sociopolitical factor for global cooperation, particularly in a context in which political parties around the world appealing to nationalist, xenophobic, and isolationist discourse appear to be gaining popularity (De Matas, 2017). Appeals to collective needs at the local level, more tightly linked and close to home, are likely to be effective for both liberals and conservatives. But appeals to global interdependence will fail among those low on global social identity, trust, and universalistic values, particularly if contributing to global efforts is perceived to be at the expense of local welfare (e.g., stocking vaccine supplies for local use vs. worldwide distribution). Rather than trying to mobilize global social cohesion directly, it may be necessary to create messages that effectively link global cooperation to local welfare in order to engage communal- binding motives and appeal to conservative values. ACKNOWLEDGMENT Correspondence concerning this article should be addressed to Marilynn B. Brewer, Department of Psychology 1835 Neil Avenue Columbus, OH 43210, USA. E- mail: brewer.64@osu.edu REFERENCES Aaldering, H., & Böhm, R. (2020). Parochial versus universal cooperation: Introducing a novel economic game of within- and between- group interaction. Social Psychological and Personality Science, 11, 36– 45. Agerström, J., Carlsson, R., Nicklasson, L., & Guntell, L. (2016). Using descriptive social norms to increase charitable giving: The power of local norms. Journal of Economic Psychology, 52, 147– 153. Parochial Altruism 395 Balliet, D., Tybur, J. M., Wu, J., Antonellis, C., & Van Lange, P. A. (2018). Political ideology, trust, and cooperation: In- group favoritism among Republicans and Democrats during a US national election. Journal of Conflict Resolution, 62, 797– 818. Balliet, D., Wu, J., & DeDreu, C. K. W. (2014). Ingroup favoritism in cooperation: A meta- analysis. Psychological Bulletin, 140, 1556– 1581. Barragan, R. C., Oliveira, N., Khalvati, K., Brooks, R., Reinecke, K., Rao, R. P., & Meltzoff, A. N. (2021). Identifying with all humanity predicts cooperative health behaviors and helpful responding during COVID- 19. PLoS One, 16(3), e0248234. Bernhard, H., Fischbacher, U., & Fehr, E. (2006). Parochial altruism in humans. Nature, 442, 912– 915. Blackwell, C., & McKee, M. (2003). Only for my own neighborhood? Preferences and voluntary provision of local and global public goods. Journal of Economic Behavior and Organization, 52, 115– 131. Buchan, N., Grimalda, G., Wilson, R., Brewer, M., Fatas, E., & Foddy, M. (2009). Globalization and human cooperation. Proceedings of the National Academy of Sciences, 106, 4138– 4142. Buchan, N. R., Brewer, M. B., Grimalda, G., Wilson, R. K., Fatas, E., & Foddy, M. (2011). Global social identity and global cooperation. Psychological Science, 22, 821– 828. Cardenas, D., Orazani, N., Stevens, M., Cruwys, T., Platow, M., Zekulin, M., & Reynolds, K. (2021). United we stand, divided we fall: Sociopolitical predictors of physical distancing and hand hygiene during the COVID- 19 pandemic. Political Psychology, 42, 845– 861. Chan, H., Wang, X., Zuo, S.- J., Chiu, C., Liu, L., Yiu, D., & Hong, Y. (2021). War against COVID- 19: How is national identification linked with the adoption of disease- preventative behaviors in China and the United States? Political Psychology, 42, 767– 793. Choi, J.- K., & Bowles, S. (2007). The coevolution of parochial altruism and war. Science, 318, 636– 640. De Matas, J. (2017). Making the nation great again: Trumpism, Euro- scepticism and the surge of populist nationalism. Journal of Comparative Politics, 10, 19– 36. DeDreu, C. K. W., et al. (2010). The neuropeptide oxytocin regulates parochial altruism in intergroup conflict among humans. Science, 328, 1408– 1411. Dorrough, A. R., & Glöckner, A. (2016). Multinational investigation of cross- societal cooperation. Proceedings of the National Academy of Sciences, 113, 10836– 10841. Enke, B., Rodríguez- Padilla, R., & Zimmermann, F. (2021). Moral universalism: Measurement and economic relevance. Management Science, 68, 3590– 3603. https://doi.org/10.1287/mnsc.2021.4086 Fellner, G., & Lünser, G. K. (2014). Cooperation in local and global groups. Journal of Economic Behavior and Organization., 108, 364– 373. Fowler, J. H., & Kam, C. D. (2007). Beyond the self: Social identity, altruism, and political participation. The Journal of Politics, 69, 813– 827. Gallier, C., Goeschl, T., Kesternich, M., Lohse, J., Reif, C., & Römer, D. (2019). Leveling up? An inter- neighborhood experiment on parochialism and the efficiency of multi- level public goods provision. Journal of Economic Behavior & Organization, 164, 500– 517. Giddens, A. (1991). Modernity and self- identity: Self and society in the late modern age. Stanford University Press. Graham, J., Haidt, J., & Nosek, B. A. (2009). Liberals and conservatives rely on different sets of moral foundations. Journal of Personality and Social Psychology, 96, 1029– 1046. Graham, J., Nosek, B. A., Haidt, J., Iyer, R., Koleva, S., & Ditto, P. (2011). Mapping the moral domain. Journal of Personality and Social Psychology, 101, 366– 385. Grimalda, G., Buchan, N. R., Ozturk, O. D., Pinate, A. C., Urso, G., & Brewer, M. B. (2021). Exposure to COVID- 19 is associated with increased altruism, particularly at the local level. Scientific Reports, 11, 18950. https://doi. org/10.1038/s4159 8- 021- 97234 - 2 Haidt, J., & Graham, J. (2007). When morality opposes justice: Conservatives have moral intuitions that liberals may not recognize. Social Justice Research, 20, 98– 116. Haidt, J., & Joseph, C. (2004). Intuitive ethics: How innately prepared intuitions generate culturally variable virtues. Daedalus: Special Issue on Human Nature, 133, 55– 66. Held, D., McGrew, A., Goldblatt, D., & Perraton, J. (2000). Global transformations: Politics, economics and culture. In C. Pierson & S. Tormey (Eds.), Politics at the edge. Political studies association yearbook series (pp. 14– 28). Palgrave Macmillan. Jost, J. T. (2017). Ideological asymmetries in the essence of political psychology. Political Psychology, 38, 167– 208. Knowles, S., & Sullivan, T. (2017). Does charity begin at home or overseas? Nonprofit and Voluntary Sector Quarterly, 46, 944– 962. 396 M.B. Brewer et al. McFarland, S., Hackett, J., Hamer, K., Katzarska- Miller, I., Malsch, A., Reese, G., & Reysen, S. (2019). Global human identification and citizenship: A review of psychological studies. Political Psychology, 40, 141– 171. McFarland, S., Webb, M., & Brown, D. (2012). All humanity is my ingroup: A measure and studies of identification with all humanity. Journal of Personality and Social Psychology, 103, 830– 853. Muldoon, O. T., Liu, J. H., & McHugh, C. (2021). Editorial: The political psychology of COVID- 19. Political Psychology, 42, 715– 728. Romano, A., Balliet, D., Yamagishi, T., & Liu, J. (2017). Parochial trust and cooperation across 17 societies. Proceedings of the National Academy of Sciences, 114, 12702– 12707. Romano, A., Sutter, M., Liu, J., & Balliet, D. (2021). Political ideology, cooperation and national parochialism across 42 nations. Philosophical Transactions of the Royal Society B Biological Sciences, 376, 2020146. https://doi. org/10.1098/rstb.2020.0146 Romano, A., Sutter, M., Liu, J., Yamagishi, T., & Balliet, D. (2021). National parochialism is ubiquitous across 42 nations around the world. Nature Communications, 12, 4456. Rosenmann, A., Reese, G., & Cameron, J. E. (2016). Social identities in a globalized world: Challenges and opportunities for collective action. Perspectives on Psychological Science, 11, 202– 221. Ruisch, B., Moore, C., Granados Samayoa, J., Boggs, S., Ladanyi, J., & Fazio, R. (2021). Examining the left- right divide through the lens of a global crisis: Ideological differences and their implications for responses to the COVID- 19 pandemic. Political Psychology, 42, 795– 816. Sinn, J. S., & Hayes, M. W. (2017). Replacing the moral foundations: An evolutionary- coalitional theory of Liberal- Conservative differences. Political Psychology, 38, 1043– 1064. Turner, J. C., Hogg, M. A., Oakes, P. J., Reicher, S. D., & Wetherell, M. S. (1987). Rediscovering the social group: A self- categorization theory. Basil Blackwell. Van Lange, P. A. M., Bekkers, R., Chirumbolo, A., & Leone, L. (2012). Are conservatives less likely to be prosocial than liberals? From games to ideology, political preferences and voting. European Journal of Personality, 26, 461– 473. Vignoles, V., Jaser, Z., Taylor, F., & Ntontis, E. (2021). Harnessing shared identities to mobilize resilient responses to the COVDI- 19 pandemic. Political Psychology, 42, 817– 826. Waytz, A., Iyer, R., Young, L., Haidt, J., & Graham, J. (2019). Ideological differences in the expanse of the moral circle. Nature Communications, 10, 4389– 4401. Wit, A. P., & Kerr, N. L. (2002). ‘Me versus just us versus us all:’ Categorization and cooperation in nested social dilemmas. Journal of Personality and Social Psychology, 83, 616– 637. Yamagishi, T., & Kiyonari, T. (2000). The group as the container of generalized reciprocity. Social Psychology Quarterly, 63, 116– 132. Supporting Information Additional supporting information may be found in the online version of this article at the publisher’s web site: Table S1. (a) Intercorrelations: Total Sample (N = 1252). (b) Intercorrelations: U.S. Sample (N = 751). (c) Intercorrelations: Italy Sample (N = 501) Figure S1. (a) Mean donations by liberals and conservatives: U.S. (b) Mean donations by liber- als and conservatives: Italy. Figure S2. (a) Mean social identity for liberals and conservatives: U.S. (b) Mean social identity for liberals and conservatives: Italy. Figure S3. (a) Mean trust for liberals and conservatives: U.S. (b) Mean trust for liberals and conservatives: Italy. Table S2. Alternative Regression Model (Component Scores) Research and data on RCV in practice Author: FairVote, Date: 11/2024 Collections: Voting Systems Zotero Key: XX4E62IC Cite Key: FairVote24researchDataRCV Zotero Item | Lit Note Current snapshot of RCV use in the United States Since 2004, dozens of cities, counties, and states have held over 800 RCV elections, with more than 30 million ranked choice ballots cast. The video below shows how fast RCV has grown in the last two decades. A full list of jurisdictions using RCV, including year of adoption, is available here. Voter turnout and participation Evidence shows that RCV elections often generate relatively high turnout. For example, when New York City used RCV in its 2021 primaries, that election had its highest turnout in over 30 years. Other evidence finds that RCV increases turnout in municipal elections and RCV boosts youth voter turnout. However, RCV’s full impact on turnout is still not yet known. Most places that have adopted RCV have switched from a two-round system to a single RCV election. Primary and runoff elections often draw low turnout; RCV substantially improves turnout by consolidating primary and runoff elections into a single higher-turnout general election. In general elections, turnout is most strongly driven by competitive campaigns and whether the election takes place in an even-numbered year, according to researchers at the University of Missouri-St.Louis. Other characteristics that are independent of the election method, such as media attention, also make it difficult to control for the impact of RCV when studying turnout. Voter turnout • Voters in RCV jurisdictions are 17% more likely to turn out for municipal elections than those in non-RCV jurisdictions, according to a 2024 study. The same study found that voters in RCV jurisdictions are more likely to be contacted by campaigns, an important measure of voter engagement. (See Does ranked choice Voting Increase voter turnout and mobilization? Dowling, Tolbert, Micatka, and Donovan. August 2024.) • Youth turnout in RCV cities was higher than youth turnout in non-RCV cities, according to a 2021 study by researchers in Iowa and Wisconsin. They attribute the cause to greater campaign civility and mobilization and increased contact in RCV elections. (See Ranked Choice Voting and Youth Voter Turnout: The Roles of Campaign Civility and Candidate Contact. Juelich, C & Coll, J. June 2021.) • RCV caused a 10% increase in turnout in the Minneapolis-St. Paul area when it was implemented in 2009 for Minneapolis and 2013 for St. Paul, according to a 2020 study by researchers in Australia. The effect on turnout was higher for precincts with higher poverty rates. (See Effect of Instant Run-off Voting on Participation and Civility. McGinn, E. July 2020.) • When compared to the primary and runoff elections they replace, RCV general elections are associated with a 10 point increase in voter turnout, according to a 2016 study by researchers at the University of Missouri in St. Louis. They found RCV did not affect inequities in turnout. (Read our one-page summary here, or see Voter Participation with Ranked Choice Voting in the United States. Kimball, D & Anthony, J. October 2016.) • In 2018, San Francisco held a highly competitive special mayoral election at the same time as statewide primaries for governor and senator. More San Franciscans participated in the city’s RCV mayoral election than in non-RCV primaries at the top of the ballot, demonstrating that a competitive RCV election can drive turnout, according to FairVote’s analysis. • Our analysis of RCV races in the six largest U.S. cities using RCV found stronger turnout in RCV races than those held before RCV implementation and compared to concurrent races in non-RCV cities. We did not control for other factors, such as competitiveness of races on the ballot, which could drive turnout. Voter engagement • Candidates in RCV cities are more likely to reach out to voters in person than those in cities that do not use RCV. Additionally, voters in RCV cities were more likely to discuss politics with their families, friends or co-workers than voters in cities that do not use RCV. (See Ranked Choice Voting and Participation: Impacts on Deliberative Engagement. Smith, Haley. June 2016.) Additional reading: RCV ballot use This section explores how voters use the ranked ballot, including number of candidates ranked and ballot error. Number of rankings used In ranked choice voting elections, voters have the option to rank as many or as few candidates as they choose. In practice, most voters choose to rank multiple candidates. The number of voters who choose to rank multiple candidates can indicate public understanding and enthusiasm for ranked choice voting. At the same time, voters may vote for only one candidate if they so wish. This can be an active choice, meaning voters who don’t rank multiple candidates aren’t necessarily lacking understanding. • Our research regularly tracks how many voters choose to rank multiple candidates across all RCV elections in the U.S.* ○ A median of 68% of voters rank multiple candidates. ○ In highly competitive elections (those with 5+ candidates), even more votes rank multiple candidates (74%). ○ 72% of RCV voters in the 2020 Democratic presidential primaries ranked multiple candidates, even though Joe Biden was already the presumptive nominee by the time voting began in the four RCV states. • A 2021 study by FairVote found that voters of color tended to use more rankings in 2020 elections than White voters. In all elections in the study, voters of all racial and ethnic groups ranked at least half of the candidates on the ballot. (See Ranked choice voting elections benefit candidates and voters of color. FairVote. 2021.) • Black, Latino, and Asian American voters were more likely to fully rank their ballots in San Francisco, according to California researchers. (See Whose votes count?: A ballot in which the voter did not select any candidates for that office. Under Ranked Choice Voting (RCV), this term ordinarily means the voter has skipped voting for the office entirely, but it is sometimes erroneously also used to refer to an unused ranking on a ballot (more commonly called a skipped ranking).Undervotes, overvotes, and ranking in San Francisco’s instant-runoff elections by Francis Neely and Corey Cook. 2008.) • Australian voters in states without compulsory ranking tend to follow party recommendations when choosing how many candidates to rank, according to a 2021 study in Australia. ○ The proportion of “single rankings” by Labor party voters, for example, reached 72% in elections for which the Labor Party’s campaign material recommended single rankings. That rate fell sharply when the Labor Party’s materials instead recommended additional rankings. ○ A similar effect occurred in 2018 in Maine’s 2nd district RCV election, in which incumbent Bruce Poliquin signaled anti-RCV sentiment and subsequently did not earn as many second- and third-choice rankings as his more RCV-friendly rivals. ○ (See Ranked choice voting in Australia and America: Do voters follow party cues? by Benjamin Reilly. 2021.) • Voter education materials are effective for both informed and uninformed voters in RCV elections and can impact ballot use, according to a 2021 experimental study. The study finds that participants who received a voter guide detailed the candidates’ stances on various issues used more rankings and voted for candidates better aligned with their own political views. The study also found that voters’ top choices tend to be a good reflection of the voter’s policy views, but disparities exist between voters classified as informed and uninformed (based on individuals’ knowledge of local issues). Additionally, the voter guide closed the gap between informed and uninformed voters. (See Ranked-Choice Voting and Political Expression: How Voting Aids Narrow the Gap between Informed and Uninformed Citizens by Cheryl Boudreau, Jonathan Colner, and Scott Mackenzie. March 2021.) • Voters use their rankings, whether they vote in person or by mail. According to FairVote’s own research, the number of rankings used differed by less than ten percent between mail-in and in-person voting across Alaska, New York City, and four California cities (Albany, Berkeley, Oakland, and San Francisco). On average, mail-in voters used slightly more rankings, demonstrating that voters take advantage of their RCV ballots even without polling-site instruction.* Ballot error This section examines research into voter error as a measure of voter participation. All ballot types result in some voter errors. In single-choice elections, ordinarily only overvotes — invalidated ballots because voters attempted to vote for more candidates than are allowed — count as ballot errors. In RCV elections, voters may make other types of deviant marks, including skipping rankings or including overvotes at later ranking orders. However, most of these ballots are counted as the voter intended. For example, if a voter leaves their second ranking blank but provides a third ranking, the third ranking will be counted as the voter’s second ranking. Only first-round overvotes can be compared with errors in single-choice elections, since those are the only errors that, in both systems, invalidate the ballot entirely. In other words, if a ballot is invalidated in a later round of RCV, the ballot is no less valuable in determining the outcome than it would be in our current system of plurality voting. Overall, research indicates that ballot error in RCV elections follows the same pattern as errors in non-RCV elections. In all RCV elections in the U.S. with 3+ candidates, the median first-round overvote rate is 0.15%. According to professors at Utah Valley University, “relatively few ballots in RCV elections contain an error, and even fewer ballots are rejected,” but “if RCV and single-choice voting differ in terms of ballot error, that difference should be weighed against the fact that RCV makes more ballots count meaningfully. Recent research shows that RCV causes an average of 17% more votes to directly affect the outcome between top candidates.” • Errors on ranked choice ballots reveal no significant differences when comparing racial and ethnic groups, according to a 2020 study. The author concludes that the evidence suggests blank rankings on a ballot may be a choice rather than evidence of difficulty casting a ballot. (See Demographic Disparities Using Ranked Choice Voting? Ranking Difficulty, Under-Voting, and the 2020 Democratic Primary, by Joseph Coll. June 2021.) • Ranked ballots and score ballots produced more valid votes than traditional choose-one ballots according to a 2020 survey experiment. Additionally, ranked ballots were associated with smaller discrepancies in error-proneness according to race and gender. Find a research brief by the author here. (See Voting Error Across Multiple Ballot Types: Results from Super Tuesday (2020) Experiments in Four American States, by Jason Maloy. October 2020.) • Ranked ballots do not raise the probability that a voter would cast a void (uncountable) vote, despite raising the probability of at least one violation of voting instructions. (See The Impact of Input Rules and Ballot Options on Voting Error: An Experimental Analysis by J.S. Maloy and Matthew Ward. June 2021.) • The adoption of RCV was not associated with any change in the number of residual votes, including overvotes and undervotes, in a 2016 study of 26 cities. (See FairVote’s one-page summary here or see Voter Participation with Ranked Choice Voting in the United States, by David Kimball and Joseph Anthony. October 2016.) • Patterns of overvoting are similar in both RCV and non-RCV contests, according to a 2015 study. With both voting methods, overvotes are more common in precincts with more African-American, Latino, elderly, foreign-born, and less wealthy citizens. (See Overvoting and the Equality of Voice under Instant-Runoff Voting in San Francisco, by Francis Neely and Jason McDaniel. 2015.)* Inactive ballots Inactive ballots — also known as exhausted ballots — occur when ballots can’t be counted for a candidate in a given round of vote tabulation. The more active ballots that are in play in the final round, the more utility those ballots have in deciding the outcome. Ballots can become inactive in three ways: | Reason for inactive ballot | Description | | --- | --- | | Voluntary abstention | The voter does not use all allowed rankings, and all ranked candidates are eliminated during the round-by-round tabulation. | | Ranking limit | The voter uses as many rankings as allowed on the ballot, but all ranked candidates are eliminated during tabulation. This occurs in jurisdictions that limit voters to fewer rankings than the number of candidates, such as allowing only three rankings. | | Ballot error | The voter makes an error that prevents their ballot from being counted. | Voters are permitted to rank as many choices as they want, but they have the right to not rank candidates beyond those they support. Thus, we could also consider ballots exhausted by voluntary abstention as ballots exhausted “by choice.” Therefore, these ballots are not problematic for RCV, but rather an indication of voter choice – the choice to express preferences for multiple candidates (a choice option that does not exist under plurality voting). We analyzed all single-winner RCV races in the U.S. between 2004 and 2022 and found that few votes become inactive due to either ranking limits or ballot error. Voluntary abstention is by far the most common source of inactive votes. • Total impact of inactive ballots in RCV races: When combining single-round and multi-round races, the data set includes 300 elections that released full ballot data, including over 14 million ballots. The total impact of inactive ballots is as follows, on average: ○ 4.8% inactive by voluntary abstention ○ 1.6% inactive by ranking limit ○ 0.06% inactive by error • In races with multiple rounds of tabulation: We have complete data for over 180 single-winner races that used multiple rounds to determine a winner, including 10 million ballots. For those races, we found the following average rates of inactive ballots: ○ 7.8% inactive by voluntary abstention ○ 2.6% inactive by ranking limit ○ 0.1% inactive by error • Compared to two-round runoff elections, RCV makes many more votes count. In runoff elections, participation typically declines by nearly 40% because not all voters turn out to vote a second time. In RCV, also known as “instant runoff voting,” every voter has the option to rank the candidates, and the decline in participation is less than 10% (as measured by “voluntary abstention” above.) Other research on inactive ballots: Strategic voting • “RCV is not easily manipulable in practice,” according to a 2024 paper. (See Optimal Strategies in Ranked Choice Voting by Sanyukta Deshpande, Nikhil Garg, and Sheldon H. Jacobson)* • With RCV, voters have no incentive to vote strategically (i.e. submit a ballot that is different from their actual preferences) if their goal is to elect candidate(s) who match their preferences, according to a 2024 study. (See Voting when Rankings Matter: Truthful Equilibria, Efficiency, and Abstention by Roland Pongou and Ghislain Junior Side).* • “Manipulability is rarely a concern in practice” and “affect a relatively small enough number of voters that attempting to manipulate an election . . . seems like an absurd strategy,” according to a 2023 empirical study that examines American RCV elections. (See: An Examination of Ranked Choice Voting in the United States, 2004-2022by Adam Graham-Squire and David Mccune).* • “(RCV) is less susceptible to strategic voting on average than plurality, in the sense that it creates smaller average incentives to vote strategically”, according to a 2023 study. (See Susceptibility to strategic voting: a comparison of Plurality and instant-runoff elections by Andrew C. Eggers and Tobias Nowacki).* Voter support and understanding This section examines how well voters understand RCV and their level of satisfaction with it. The data is summarized in downloadable format in Exit Surveys: Voters Love Ranked Choice Voting. Voter support • In 2022, 62% of survey respondents support Alaska’s new primary system that is open to all voters and allow the top-four vote getters to advance to the general election.* • In 2022, a majority (56%) of Virginia Republican primary voters who used RCV in congressional primaries reported that they prefer RCV to single-choice elections.* • In 2021, 77% of voters surveyed by Rank the Vote NYC in the New York City primaries supported using RCV for future local elections.* • In 2021, 62% of Utah voters in cities that used RCV liked voting with an RCV ballot. 88% were satisfied.* • In 2018, 61% of voters in Maine’s general election expressed support for keeping or expanding RCV after using it for the first time.* • In 2018, 94% of Santa Fe voters reported feeling “very satisfied” or “somewhat satisfied” with their first use of RCV.* • In 2017, 66% of voters in Minneapolis’ municipal elections expressed continued support for the use of RCV.* • In a multi-lingual, multi-ethnic, New York-based focus group, over two-thirds of participants felt confident about voting in an upcoming RCV election after watching or listening to a basic explanation of RCV.* Other research on voter support for ranked choice voting: • RCV may be an acquired taste, according to two 2021 survey experiments (here and here). Researchers found that most survey respondents said they prefer single-choice voting to RCV, but one study also found that those who have used RCV are more positive towards it, suggesting the presence of a status quo bias. The findings underscore the benefits of a voter education initiative with RCV, because voters in real-world RCV elections report that they like RCV and prefer it over their prior voting method, even after using RCV just one time. (See Public Perceptions of Alternative Voting Systems: Results from a National Survey Experiment, by David Kimball and Joseph Anthony, April 2021, and Choosing to Vote as Usual by Andre Blais, Carolina Plescia, and Semra Sevi, February 2021.) • Multiparty proportional or ranked-choice systems can offer the benefits of a small winner-loser gap and an absence of interparty animosity, according to a 2021 study based on a large-scale behavioral game. The study finds that certain institutions, namely those based on proportional representation and RCV, as well as multiparty arrangements, decrease “perceived legitimacy gap”, meaning election winners and losers are in closer agreement that the election is legitimate. (See Electoral Systems and Political Attitudes: Experimental Evidence by Sean Fischer, Amber Lee, and Yphtach Lelkes. May 2021.) • After their first regular RCV election in 2022, a majority of poll respondents in Alaska said their vote mattered more than in previous years. This sentiment was consistent across region, race, gender, and age, though Alaska Native voters were most likely to say their vote mattered more. • When Utahans across 23 cities used RCV in 2021, 60% said they were more likely to vote for their favorite candidate. Voter understanding Understanding how to vote, as well as how individual votes are used to determine the winner, is an important component of voter empowerment. This section focuses on self-reported understanding through polling. For research on how voters interact with the ranked ballot in practice, such as data on undervotes, overvotes, and number of rankings used, see Data on RCV: Ballot Use. • In 2022, 85% of Alaska voters reported that RCV is “simple” in their first RCV election.* • Maine voters embraced RCV and understood the ballot in their first two RCV election cycles, according to a 2021 analysis.* ○ Most voters took advantage of the opportunity to rank candidates. ○ The proportions of blank ballots were the same as in Maine’s prior non-RCV election. ○ Many voters used multiple rankings to cross party lines. ○ See An analysis of ranked choice voting in Maine, by Matt Germer. 2021. • Nearly all voters (95%) — across every ethnic group — in the New York City’s 2021 primary elections found the ballot “simple to complete.” Most said they understood RCV “extremely” or “very” well.* • Nearly all (90%) Maine voters said their experience with RCV in the state’s June 2018 primary elections was “excellent” or “good.” Most respondents hadn’t used RCV before.* • Voters in Santa Fe, New Mexico, overwhelmingly (more than 67% of respondents) reported they were not confused by their ballot after their first RCV election.* • In 2021, 88% of Minneapolis voters found RCV simple.* • The vast majority of voters in two Utah cities — Payson and Vineyard — ranked candidates for city council races in 2019, even though it was their first use of RCV.* • After 23 Utah cities used RCV in 2021, 81% of survey respondents said RCV was somewhat or very easy, and 90% said the instructions on the ballot were clear.* • In two surveys in 2013 and 2014 of likely voters in cities using RCV, 90% and 89% of respondents said they found the RCV ballot easy to understand. The level of understanding was high across demographic and socioeconomic groups. Additionally, self-reported understanding of RCV compares favorably to understanding of other election methods. • There are no differences in RCV cities in how White, Black, and Latino respondents reported understanding RCV in a 2019 study. Find highlights from the study here. Women and Asian Americans tended to report lower levels of understanding of election systems in general, including single-choice plurality and the top-two system. (See Self-reported understanding of ranked‐choice voting, by Todd Donovan, Caroline Tolbert, and Kellen Gracey. 2019.) • The percentage of voters in RCV cities who understood RCV at least “somewhat well” (84%) was roughly equivalent to the share of voters in plurality cities who understood plurality (83%).* • More respondents (49%) in RCV cities reported understanding RCV extremely or very well than reported understanding the top-two primary extremely or very well (40%). * See Socioeconomic and demographic perspectives on Ranked Choice Voting in the Bay Area. Sarah John and Caroline Tolbert. 2015.) • Older voters were less likely to leave blank rankings on their ballots, but were slightly more likely to report difficulty with the ranked ballot according to a 2021 study. No significant relationships were found across racial and ethnic groups, and only weak evidence linked socioeconomic status to blank rankings. The findings support previous evidence of older voters reporting difficulty but challenge research that assumes difficulty leads to undervoting and that racial and ethnic groups are disadvantaged by RCV. (See Demographic disparities using ranked choice voting? Ranking difficulty, under-voting, and the 2020 Democratic Primary, by Joseph Coll. 2021.) Additional reading: Representation Research shows that RCV improves representation for women, people of color, and other groups. Representation for women and people of color • Candidates of color gain more votes in the round-by-round counting process than White candidates, indicating consensus-building, according to a 2021 FairVote study. Additionally, candidates of color in RCV elections pay no penalty when competing against other candidates of the same racial or ethnic group. (See Ranked choice voting elections benefit candidates and voters of color, FairVote. 2021.)* • The proportional form of RCV increased women’s representation in cities that used it in the early 20th century, according to a 2021 study. The study also shows that the single-winner version of RCV has been effective at increasing women’s representation in the 21st century. (See Election Reform and Women’s Representation: Ranked Choice Voting in the U.S, by Cynthia Terrell, Courtney Lamendola, and Maura Reilly. 2021.) • Cities with RCV have better overall electoral outcomes for women and people of color, according to a 2020 study by RepresentWomen. Over the last decade, women have won 48% of all municipal ranked choice elections. As of April 2020, nearly half of all mayors (46%) and city council seats (49%) decided by RCV are held by women. By comparison, women comprise only 23% of mayors in non-RCV jurisdictions. (See In Ranked Choice Elections, Women WIN, by RepresentWomen. 2020.) • California cities that adopted RCV saw increases in the percentage of candidates of color running for office and in the probability of female candidates and female candidates of color winning office, according to a 2018 study. (See The alternative vote: Do changes in single-member voting systems affect descriptive representation of women and minorities?, by Sarah John, Haley Smith, and Elizabeth Zack. 2018.) • People of color hold office at a higher rate under RCV, according to a 2019 FairVote report on RCV in California cities. (See Ranked Choice Voting and Racial Minority Voting Rights, FairVote. 2019.) • RCV voters in presidential primaries used rankings to identify the strongest presidential nominee while ensuring diverse representation at the national convention, according to a study on the 2020 Democratic presidential primaries. (See Ranked-Choice Voting Delivers Representation and Consensus in Presidential Primaries, by Baodong Liu, Nadia Mahallati, and Charles Turner. 2021.) • Candidates of color appear to earn lower support than White candidates in both plurality elections and RCV elections. A survey experiment from 2021 finds that RCV does not ameliorate that penalty, penalties were significantly lower for respondents who displayed a high level of understanding of RCV. Adding partisan labels to the candidates also significantly reduces the penalty. (See Ranking Candidates in Local Elections: Neither Panacea nor Catastrophe by Melody Crowder-Meyer, Shana Kushner-Gadarian, and Jessica Trounstine. 2021.) • Proportional RCV benefited candidates from ethnic and political minority groups, according to a 2014 FairVote report. The reason: proportional RCV’s low electoral threshold allows people of color to have representation based roughly on their share of the population. (See The Effect of Fair Representation Voting on 2013 Cambridge Municipal Elections, by Andrew Douglas. 2014.) • RCV increases descriptive representation for women, people of color, and women of color, according to a 2016 FairVote report. This is because RCV is often used to replace unrepresentative, low-turnout elections and that it allows multiple candidates appealing to the same community to run without splitting the vote. (See Ranked Choice Voting and Representation of Underrepresented Groups, by Sarah John, Haley Smith, and Elizabeth Zack. 2016.) • RCV does not decrease racially polarized voting, according to two studies (here and here). Racially polarized voting occurs when voters of different racial or ethnic groups vote distinctly from one another. These results are unsurprising because RCV tends to attract more diverse candidates, giving voters more opportunity to cast a vote for someone who represents their community. (See Does More Choice Lead to Reduced Racially Polarized Voting? Assessing the Impact of Ranked-Choice Voting in Mayoral Elections, by Jason McDaniel. (October 2018) and Does Ranked-Choice Voting Reduce Racial Polarization? A Clustering Approach to Ranked Ballot Data, by Yuki Atsusaka and Theodore Landsman. (April 2021).* • Proportional RCV, also known as single transferable vote or STV, tends to elect candidates of choice for people of color in proportion to their share of the population, according to a 2021 study. The authors note that proportional RCV will stably and reliably secure representation for people of color, whereas single-winner districts can have a wide range of performance and it is difficult to produce district maps that hold up over time with respect to voter turnout and residential shifts. These concerns are not present with proportional RCV, because proportionality is a structural property. (See Ranked Choice Voting and Minority Representation, by Gerdus Benade, Ruth Buck, Moon Duchin, Dara Gold, and Thomas Weighill. 2021.) Representation for political viewpoints More than one-third (35%) of voters are Independents, yet they have little influence in government because it is difficult for Independent candidates to get elected under plurality voting rules. RCV can improve their representation at the state and federal levels of government, allowing supporters of Independent and third-party candidates to rank their preferred candidate first without “wasting” their votes or “spoiling” the election outcome. The theory and scholarship behind how RCV (particularly the single-winner version) can otherwise impact ideological representation is mixed. At worst, the effect is neutral. Related research: • Independent and third-party candidates fare better under RCV elections, according to a 2021 study. However, respondents in a survey experiment reacted negatively to the idea of a come-from-behind victory in an RCV election while feeling no dissatisfaction with come-from-behind victories in two-round runoffs or non-majority winners in plurality elections. In actuality, non-majority winners in plurality elections can be a key driving force behind implementation of RCV, indicating voters are in fact dissatisfied with the status quo. (See Ranked-Choice Voting, Runoff, and Democracy: Insights from Maine and Other U.S. States by Joseph Cerrone and Cynthia McClintock. 2021.) • RCV does not lead to more support for extreme candidates, according to a 2021 study. Ideologically extreme candidates are not viewed as more electable in RCV elections than in plurality elections, among both liberals and conservatives. (See Voters Evaluate Ideologically Extreme Candidates as Similarly Electable under Ranked Choice Voting and Plurality Voting, by Melissa Baker. 2021.) • A study of municipal RCV use in nine cities found that RCV had no apparent impact on ideological composition of city councils in those cities, and does not appear to change councilors’ voting behavior. The study questions whether RCV will in fact improve ideological representation, but notes that it only considers progressive cities, and further research on other cities and statewide implementation will be informative. (See Electoral Institutions and Substantive Representation in Local Politics: The Effects of Ranked Choice Voting by Arjun Vishwanath. 2021.) • RCV may also reduce legislative polarization by allowing space for moderate, conservative, liberal and other voters to elect candidates in proportion to their overall numbers in the electorate, according to a 2016 FairVote report. Evidence from Cambridge, Massachusetts, which uses proportional RCV, indicates that candidates and city councilors there are not highly polarized. (See Polarization and Multi-winner Ranked Choice Voting in Cambridge, Massachusetts, City Council Elections, by Sarah John and Brandon Leinz. 2016.) Evaluating RCV election outcomes This section explores how RCV works in practice in the United States, particularly the types of outcomes it produces. The section assesses RCV’s success at electing majority winners, evaluates whether incumbent candidates succeed in RCV elections, and explores come-from-behind wins, as well as RCV’s impact on technical grounds, namely monotonicity and its tendency to elect Condorcet winners. Majority winners Majority rule is a fundamental principle of our democracy. However, our current system often elects winners with less than majority support or even less than half of the votes. Between 1992 and 2019, 49 senators from 27 states were elected with less than 50% support. Primary elections in particular can lead to nominees with small pluralities. Because votes are often split between many candidates, it often takes a relatively small proportion of votes to win. In some highly-contested races, winners earn as little as 22% of the vote, meaning 78% of people voted against them, but this small plurality is enough to win the nomination in an election with many candidates. This chart shows plurality wins in primary elections for the U.S. House of Representatives in 2024. Ranked choice voting solves this problem by requiring that the winner receives over 50% of votes active in the final round, therefore electing nominees and candidates with the broadest support. Majority winners in RCV elections • There have been roughly 300 single-winner ranked choice elections in the United States that included at least three candidates (meaning no candidate can win a majority by default. When there are two candidates, one candidate must mathematically win over 50% of votes, except in the event of a tie). A majority winner was identified in the first round in about 40% of these races. The remaining 60% races were decided by instant runoff before declaring a winner.* • Sometimes, winners of RCV elections don’t earn a majority of total votes cast. A winner is declared when a candidate has a majority of votes active in a given round of counting (excluding ballots that have become inactive). This has occurred in roughly one-third of single-winner RCV elections in the United States. In these races, the winner was preferred by a majority of voters who expressed a preference between the finalists. * Inactive ballots occur more often when jurisdictions limit the allowed number of rankings. Half of the elections in which the winner earned less than 50% of total ballots cast in the first round occurred in elections where voters were limited to only 3 choices. Learn more about the majority criterion with RCV in our FAQ. Consensus value “Consensus value” is the portion of voters who rank the winner as their first, second, or third choice. We use this value to measure how much support winning candidates garnered from the community as a whole. This measure tells us how many voters find winning candidates acceptable. • In single-winner RCV races, nearly two in three RCV winners has the consensus of two-thirds of voters.* • In single-winner races for which we have enough data to determine consensus value, 75% of ballots ranked a winning candidate in their top three.* • In proportional RCV races, 93% of voters ranked at least one winner in their top 3 choices. This rate is higher for elections that elect more seats (95% of voters in races electing six or more seats) than in races with fewer winners (89% in races electing 2-3 winners). Source: FairVote’s RCV Elections Database, proportional RCV tab.* Incumbency Come-from-behind winners A “come-from-behind” winner is a candidate who did not have the most votes in the first round, but secured enough second, third, or other choice preferences to win in a later round. A “come-from-behind” winner is a natural feature of RCV that means it is working how it is supposed to, that is, rewarding candidates with broad support over those who can only win by small pluralities. Nonetheless, the first-round winner is most often the overall winner. • Since 2004, 29 RCV races were won by a candidate other than the first-round leader. That amounts to 7% of all single-winner RCV races with 3+ candidates (444 races), and 11% of all races that used multiple rounds of counting (260 races). ○ Compare this with the rate of come-from-behind wins in two-round runoff elections, estimated at 32% in congressional primary runoffs. • Of the 29 come-from-behind wins in RCV elections, 27 were won by the candidate who began in second place.* • Two races were won by the candidate who began in third place: San Francisco’s 2010 election for the 10th District which elected Malia Cohen, and San Francisco’s 2020 election for the 7th District which elected Myrna Melgar. Analysis of ballot data reveals that both would have won head-to-head match-ups against any other candidate in those elections, making them the “The candidate who would defeat every other candidate in a head-to-head or pairwise election. Election methods which always elect the Condorcet Winner if one exits are known as Condorcet Voting methods.Condorcet Winner” or “beats-all” winner. These two cases are examples of RCV electing a Condorcet winner when a two-round runoff election would not have done so.* • Come-from-behind wins happen more often in Alaska where candidates of the same party are likely to face each other in general elections. Alaska uses “top-four RCV,” where four candidates regardless of party compete in the general election. In all of Alaska’s come-from-behind wins to date, the eventual winner is from the party that earned the most first-choice preferences.* • This chart shows all come-from-behind victories in the United States since 2004:* “Condorcet” winners The Condorcet criterion states that the candidate who would win a one-on-one matchup against every other candidate should win the election. RCV does not guarantee that the “Condorcet winner” will win, but it does make it extremely likely, and certainly out-performs traditional elections. Nonetheless, the Condorcet criterion is only one of many criteria we could use to evaluate an electoral system. • It’s unclear how often single-choice plurality (our current voting method) or two-round runoff elections elect Condorcet winners because voters’ back-choices are not publicly disclosed.* • Of the nearly 500 single-winner RCV elections in the United States since 2004 in which we have sufficient ballot data to assess whether the Condorcet winner won the election, all but two were won by the Condorcet winner. The two elections that did not elect the Condorcet candidate are the 2009 mayoral election in Burlington, Vermont and the 2022 special election for U.S. House in Alaska.* • In the rare instances where RCV does not elect the Condorcet candidate, that candidate must have attracted too little core support to place first or second in the final round. These discrepancies are rare, but when they occur it is for a good reason. RCV prioritizes candidates with both broad support and deep support, while Condorcet winners only need to have broad support. When the two methods differ, it is necessarily because of a lack of deep support for the Condorcet candidate.* • In the Burlington election, the three strongest candidates (after all others were eliminated) had 37%, 34%, and 29% support. The third-place candidate, Andy Montroll, was eliminated. After the election, ballot image data showed that supporters of the top two candidates overwhelmingly ranked Montroll ahead of the other finalist, making Montroll the Condorcet candidate. It may be disputed whether it would have been better for Montroll to win the election despite attracting so little core support. However, it is certain that Montroll would have also lost under a two-round runoff election or a single-choice plurality election. • In the 2022 special election for U.S. House in Alaska, the last-place candidate was Republican Nick Begich, eliminated in the first round. He trailed the leader, Democrat Mary Peltola, by 12 points. Ballot data later revealed that Begich was a popular second choice for Peltola voters, and voters for the other Republican, Sarah Palin. 52.5% of voters (who had a preference) ranked Begich higher than Peltola, and over 60% of voters ranked Begich higher than Palin. That makes Begich the Condorcet winner, but nonetheless a candidate with low support as a top choice. RCV did not elect Begich because he was in last place – the last-place candidate will never win in RCV. Begich almost certainly would have also lost under Alaska’s previous voting method, where he would have lost to Sarah Palin in the Republican primary. • Two RCV races elected the Condorcet winner when a two-round runoff would not have done so. In these races, both winners began in third place and finished in first place after the RCV tally. They are Connie Chan from San Francisco’s 10th district in 2010 and Myrna Melgar from San Francisco’s 7th district in 2020. In each case, a two-round runoff election would not have allowed the Condorcet winner to advance to the final round.* The “monotonicity criterion” Monotonicity means that ranking candidates lower doesn’t help them and ranking them higher doesn’t hurt them. Any voting method in which votes are counted in rounds — including RCV and two-round runoff elections — can have nonmonotonic outcomes but the realities of RCV in practice make it extremely unlikely. To our knowledge, no group of voters in an RCV election has ever attempted to exploit the possibility of nonmonotonicity for strategic purposes. Doing so successfully requires highly unusual circumstances and a detailed and accurate prediction of how the electorate will rank the candidates. Such a degree of hindsight or voter control does not exist. As such, monotonicity under RCV is a largely academic question: it has never impacted an RCV campaign and is unlikely to impact a future one. Learn more in our FAQ. Of some 500 RCV elections in the United States, there are claims thatone had a possibly non-monotonic outcome: the 2009 mayoral race in Burlington, Vermont. Below are results from that election. Whether this election constitutes a non-monotonic outcome depends on how strictly the criterion is defined. No candidate could have won if voters merely ranked them lower. However, if some Wright voters (between 367 and 589 voters) had instead ranked Kiss first (and changed no other ballots), Wright would have been eliminated instead of Montroll, and Montroll would have beaten Kiss. In other words, a group of Wright voters could have caused Kiss to lose by ranking him higher. However, this would be Wright voters helping to elect Montroll. No group of voters could have elected their own preferred candidate by ranking them lower. A similar situation arose in the 2022 special election for U.S. House of Representatives in Alaska. No group of voters could have helped their preferred candidate win by ranking them lower, but a group of voters could have helped some other candidate win (Nick Begich) by ranking them lower. Monotonicity is “rarely a concern in practice” and “affect a relatively enough small enough number of voters that attempting to manipulate an election in this fashion seems like an absurd strategy,” according to a 2023 empirical study that examines American RCV elections. (See: An Examination of Ranked Choice Voting in the United States, 2004-2022 by Adam Graham-Squire and David Mccune). Evaluating electoral systems on criteria like Condorcet and monotonicity are certainly interesting (though often hypothetical) exercises. However, as per Arrow’s Impossibility Theorem, no method of group decision-making can meet every criteria in every circumstance. Therefore, we caution not to get too “stuck” on niche scenarios like the above when overall, RCV has proven success at electing broadly popular consensus candidates (as shown on this page). Increased campaign civility Ranked choice voting (RCV) encourages civil discourse because candidates campaign not only for first- but also second-choice support. Consequently, candidates are incentivized to appeal to a broader range of voters and to avoid negative statements about opponents to reduce the risk of alienating their supporters. • RCV resulted in a more positive congressional primary in Virginia, according to a survey of Virginia Republican primary voters who used RCV in 2022. (See Measuring The Effects Of Ranked Choice Voting In Republican Primaries from the Center for Campaign Innovation. 2022.) • In debates in RCV races, candidates referred to their opponents in more positive terms rather than negative or neutral words, according to a 2020 study by Australian researchers. (See Effect of Instant Run-off Voting on Participation and Civility by Eamon McGinn. 2020.) • First-time RCV users in Santa Fe in 2018 reported more positive campaigning. 67% of poll respondents believed the tone of the mayoral election was more positive than prior mayoral elections while only 3% responded that the tone was more negative. (See Santa Fe Voters Support Ranked Choice Voting and Have High Confidence in City Elections by FairVote. 2018.) • Voters in RCV cities were more satisfied with the conduct of campaigns and perceived less candidate criticism and negative campaigning compared to voters in non-RCV cities, according to a 2013 and 2014 survey. Virtually every demographic group studied reported less negativity in RCV cities. (See Socioeconomic and Demographic Perspectives on Ranked Choice Voting in the Bay Area by Sarah John and Caroline Tolbert. 2015. The survey methodology document for the Eagleton Poll is also available.) • Media coverage in RCV cities was 85% more positive than negative, according to a 2013 analysis. By contrast, 77% of news coverage in non-RCV control cities was positive. (See Content Analysis of Campaign Tone in Newspapers and Twitter Feeds in 2013 RCV Elections, by FairVote. 2015.) • A 2021 analysis found that candidates were more likely to engage with each other in RCV cities than in plurality cities. Articles about campaigns in RCV cities had far more positive than negative words. This effect was not present, however, on social media. (See Using Campaign Communications to Analyze Civility in Ranked Choice Voting Elections, by Martha Kropf. 2021). • A 2016 study found that “people in cities with (RCV) were significantly more satisfied with the conduct of local campaigns than people in similar cities with plurality elections” and “less likely to view campaigns as negative, and less likely to respond that candidates were frequently criticizing each other.” (See Campaign civility under preferential and plurality voting, by Todd Donovan, Caroline Tolbert, and Kellen Gracey. 2016). • To learn more, read our series on research on civility in RCV:* Research on proportional representation Proportional representation awards legislative seats in proportion to the votes earned by candidates or parties. Many countries around the world use proportional representation for national elections and local elections, but it is relatively uncommon in the United States. The Fair Representation Act is a form of proportional representation. Research shows that proportional representation is effective at delivering fair outcomes. • Proportional RCV, also known as single transferable vote or STV (and also what the Fair Representation Act uses), tends to elect candidates of choice for people of color in proportion to their share of the population, according to a 2021 study. The authors note that proportional RCV will stably and reliably secure representation for people of color, whereas single-winner districts can have a wide range of performance and it is difficult to produce district maps that hold up over time with respect to voter turnout and residential shifts. These concerns are not present with proportional RCV, because proportionality is a structural property. (See Ranked Choice Voting and Minority Representation, by Gerdus Benade, Ruth Buck, Moon Duchin, Dara Gold, and Thomas Weighill. 2021). • Proportional RCV for the U.S. House of Representatives would lead to fair representation for each political party, according to a 2021 study. Additionally, advantage-seeking partisans would have their power to gerrymander significantly curtailed. (See Combatting Gerrymandering with Social Choice: the Design of Multi-member Districts, by Nikhil Garg, Wes Gurnee, David Rothschild, and David Shmoys. 2021.) • Multi-member districts produce more diverse racial representation, according to a 2021 study of election methods around the world. (See Multi-seat Districts and Larger Assemblies Produce More Diverse Racial Representation, by Michael Latner, Jack Santucci, and Matthew Shugart. 2021.) • A 2022 report finds that The Fair Representation Act (multi-member Congressional districts with proportional RCV) would improve representation for communities of color in nearly every state. Even more strikingly, it finds that these positive impacts are essentially immune to gerrymandering – the positive findings remain regardless of how the lines are drawn. (See Modeling the Fair Representation Act and the complementary 50-state supplement by the MGGG Redistricting Lab. 2022.) • Multiparty proportional or ranked-choice systems can offer the benefits of a small winner-loser gap and an absence of interparty animosity, according to a 2021 study based on a large-scale behavioral game. The study finds that certain institutions, namely those based on proportional representation and RCV, as well as multiparty arrangements, decrease “perceived legitimacy gap”, meaning election winners and losers are in closer agreement that the election is legitimate. (See Electoral Systems and Political Attitudes: Experimental Evidence, by Sean Fischer, Amber Lee, and Yphtach Lelkes. 2021.) • A 2022 report on redistricting in the U.S. finds that there are limits to “fair” maps when using single-winner districts, and makes a strong case for proportional representation in multi-winner legislative districts. (See What We Know About Redistricting and Redistricting Reform from the New America Foundation, 2022.) • International research examines vote transfer in areas with ethnic conflict and determines that proportional RCV (also known as single transferable vote) is appropriate for divided societies. (See The single transferable vote and ethnic conflict: The evidence from Northern Ireland.) • Scottish voters make the most of their preferences when voting for local councils with proportional RCV. 86% of ballots ranked multiple preferences and 61% contained three or more – a steady growth since the first proportional RCV election in Scotland. (See The Power of Preferences: STV in Scottish Local Elections.) • Globally, countries using proportional representation voting typically see higher turnout, even when controlling for factors such as GDP, level of democracy, and level of competition (see studies here, here, here, and here). Most estimates put this turnout boost in national elections at 5 to 7 percentage points compared to majority and plurality elections, respectively. See also a study attributing Malta’s near-universal turnout to maximization of the impact of a single ballot through proportional RCV. • Women are more likely to be elected in multi-member districts, with district magnitude (number of people elected per district) as a key predictor. This is true internationally and in U.S. state legislatures. (See The number of candidates to be elected in each District. Single-Winner Districts elect one member of the legislature whereas Multi-Winner Districts elect two or more.District Magnitude’s Effect on Female Representation in U. S. State Legislatures by Richard Matland & Deborah Dwight Brown, 1992, and Electoral Systems, Contextual Factors and Women’s Opportunity for Election to Parliament in Twenty-Three Democracies by Wilma Rule, 1987.) • Societies with proportional representation are less prone to gerrymandering, according to a 2016 paper. (See Gerrymandering in comparative perspective by Ferran Martínez i Coma and Ignacio Lago. 2016.) Thematic Analysis: A Corpus-Based Method for Understanding Themes/Topics of a Corpus through a Classification Process Using Long Short-Term Memory (LSTM) Author: Altameemi, Yaser, Date: 2023/1 Collections: PoliticalML Zotero Key: FSR4X8GW Cite Key: Altameemi23topicShiftLSTM Zotero Item | Lit Note applied sciences Article Thematic Analysis: A Corpus-Based Method for Understanding Themes/Topics of a Corpus through a Classification Process Using Long Short-Term Memory (LSTM) Yaser Altameemi [1,]* and Mohammed Altamimi [2] 1 Department of English Language, College of Arts and Literature, University of Ha’il, Ha’il 81481, Saudi Arabia 2 Department of Information and Computer Science, College of Computer Science and Engineering, University of Ha’il, Ha’il 81481, Saudi Arabia ***** Correspondence: y.albakry@uoh.edu.sa Abstract: Using advanced algorithms to conduct a thematic analysis reduces the time taken and increases the efficiency of the analysis. Long short-term memory (LSTM) is effective in the field of text classification and natural language processing (NLP). In this study, we adopt LSTM for text classification in order to perform a thematic analysis using concordance lines that are taken from a corpora of news articles. However, the statistical and quantitative analyses of corpus linguistics are not enough to fully identify the semantic shift of terms and concepts. Therefore, we suggest that a corpus should be classified from a linguistic theoretical perspective, as this would help to determine the level of the linguistic patterns that should be applied in the experiment of the classification process. We suggest investigating the concordance lines of the articles rather than only the relationship between collocates, as this has been a limitation for many studies. The findings of this research work highlight the effectiveness of the proposed methodology for the thematic analysis of media coverage, reaching 84% accuracy. This method provides a deeper thematic analysis than only applying the classification process through the collocational analysis. Citation: Altameemi, Y.; Altamimi, M. Thematic Analysis: A Corpus Based Method for Understanding Themes/Topics of a Corpus through a Classification Process Using Long Short-Term Memory (LSTM). Appl. Sci. 2023, 13, 3308. https://doi.org/ 10.3390/app13053308 Academic Editor: Christos Bouras Received: 25 January 2023 Revised: 27 February 2023 Accepted: 27 February 2023 Published: 5 March 2023 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). Keywords: thematic analysis; Long Short-Term Memory (LSTM); corpus linguistics; concordance lines; collocations 1. Introduction There are diverse areas in which corpus linguistics (CL) has contributed specifically to the notion of language use, which is the norm of critical discourse analysis (CDA). This embedded relationship between CL and CDA has contributed to significant findings, specifically with the focus on building representative data and producing a representative result in order to generalize a social phenomenon. Recent studies have discussed the thematic analysis and what can be called the aboutness of a corpus, concentrating on collocational analysis. According to Baker ([1], p. 96), collocation is a “way of understanding meanings and associations between words which are otherwise difficult to ascertain from a small-scale analysis of a single text”. Previous studies highlight the importance of analyzing keywords and the collocations in a corpus to provide a full understanding of the themes and topics of a corpus. These methodological perspectives are important not only for providing themes and topics, but also for providing the semantic structure of a text. These studies examine deep analyses by investigating the changes in meaning through the collocational network, as argued by Brezina et al. [2]. However, statistical and quantitative analyses are not enough to fully identify the semantic shift of terms and concepts in a corpus, specifically for specialized corpora. Taking this into account, Altameemi [3] manually analyzed a sample of the concordance lines Appl. Sci. 2023, 13, 3308 2 of 12 for strong collocates, as the number of lines is considerable. However, he suggests the importance of applying an automatic thematic analysis to all of the concordance lines. Due to this limitation, the authors suggest the importance of developing a methodology that is based on the proposed algorithm to help linguists in analyzing all of the concordance lines of the strong collocates, in order to increase the representativeness when analyzing a corpus. In this research, we test a proposed methodology that has not been applied in any previous studies, according to the knowledge of the researchers. This article aims to develop a methodology of applying automatic thematic analysis using deep learning models. In other words, we suggest that experts in computer science should consider the level of linguistic elements in discourse/text, as well as linguistic theories, to improve the proficiency of the algorithm’s classification and categorization. Corpus linguists use tools such as LancsBox to apply a thematic analysis to collocations (relations between words) rather than a text’s concordance lines. Therefore, this article will contribute to the field of corpus linguistics by annotating/classifying the extracted lines from keywords in context (KWIC). Previous studies have failed to apply thematic analysis for the concordance lines of the collocates. They only focus on thematic analysis based on the relationship between collocates. In order to provide a robust, thematic analysis of a corpus, we argue that corpus linguists need to analyze the lines of the strong collocates by taking into consideration their collocational relationships. The proposed methodology suggests applying the proposed algorithms to perform an automatic thematic analysis of the concordance lines in addition to the collocational analysis. In the current paper, we try to provide a comprehensive methodology for thematic analysis by combining theoretical aspects in the linguistics field with the practical steps for conducting thematic analysis by applying deep learning models using LSTM. We did not simply classify the whole corpus, but, generally, we performed the following steps: 1. Pre-process and clean the dataset. 2. Identify strong collocates using the DeltaP measurement. 3. Extract lines from the concordance lines of the strong collocates. 4. Apply the algorithm for classifying the extracted KWIC. 5. Compare the automatic findings with the manual analysis of Altameemi’s findings [3]. These steps are discussed in detail in Section four. This paper starts with a discussion of the previous research that mainly focuses on the thematic analysis and the aboutness of a corpus. As this study is a combination of both linguistic and computer science perspectives, the literature review is presented from this perspective. Then we discuss the methodological aspects, including long short-term memory (LSTM) and the steps for automatic thematic analysis. In the analysis section, we discuss the findings based on the results of the experiment and the accuracy of the automatic thematic analysis. The results support the effectiveness and the high accuracy of the proposed methodology and the automatic analysis of the concordance lines. 2. Literature Review 2.1. Corpus Linguistics and Thematic Analysis Corpus linguistics (CL) has contributed to various areas. In particular, it is typical of critical discourse analysis (CDA) as an ideal of language. This interrelation between CL and CDA has contributed to significant findings, especially with a focus on building representative data and making a prototypical result in order to allow for generalizations to social phenomena. For example, several studies have dealt with political discourse, language learning research, or self-representation discourse. In addition, the synchronic development of combining CL and CDA has become crucial from the perspective of understanding the context of the discourse regarding a specific issue. This point has been raised by Altameemi, who suggests the importance of following Wodak’s [4] approach regarding the need for several levels of analysis that may help an analyst to understand the connection between discourse and context. Appl. Sci. 2023, 13, 3308 3 of 12 Recent studies have discussed the thematic analysis, and what can be called the aboutness of a corpus, by concentrating on collocational analysis. According to Baker ([1], p. 96), collocation is a “way of understanding meanings and associations between words which are otherwise difficult to ascertain from a small-scale analysis of a single text.” Taking this into account, the collocation can be used for two central purposes: the lexical-syntactic analysis and the expanded consideration of the semantic field. In this section, we will discuss these two points. Many studies have considered collocational analysis in the language production of first-language and second-language speakers [5], e.g., [6,7]. Other studies, e.g., [7], focus on analyzing the semantic relationship between words through their relationship and cooccurrence in a corpus. According to Evert [8], the frequency of occurrences, the exclusivity of collocates, and their directionality are important dimensions for the semantic relationship between words’ co-occurrence (see Evert [8] for more details on the differences between the dimensions). Brezina et al. [2] assert the importance of analyzing collocation; they suggest that collocational analysis reveals the aboutness of a corpus to determine the topics and themes. Gablasova et al. [9] go further to measure and interpret the use of collocation in L1 and L2 production. They follow this approach as they argue that three dimensions have an impact on learners’ knowledge, so these dimensions should be considered before the selection of the association measures for the collocation. The analysis of collocation is also applied by various researchers to analyze how a corpus represents a topic with various ideas and themes. Neg and Tan [10] investigated the diverse representations of COVID-19 and link them to cultural values. They applied the corpus approach by using the mutual information score to measure the word associations in the corpora. Then they look at the media coverage, as well as the prevalence rate of COVID-19. Neg and Tan (2021) not only consider the collocation relationship, but they also connect their findings with the cultural values that were stated in advance before the collocation analysis. Another study conducted by Huang et al. [11] highlights the importance of analyzing online reviews using the polymerization topic sentiment model (PTSM) for sentiment thematic analysis. They argue that PTSM can reveal the impact of reviews on shaping products, as well as on the sales promotion. However, this study did not fully consider the analysis of keywords and the shifts in their meanings within the discourse. Therefore, analysts of discourse may need to go deeper into the analysis and consider how the meanings of concepts relate to various representations. Analyzing the collocation is also used as a key tool for analyzing lexical–thematic analyses. Bondarchuk et al. [12] and Biber and Reppen [13] argue that analyzing the keywords quantitatively is effective for identifying the lexical structure of a corpus. Huang et al. [14] used the dependency SCOR-topic sentiment (DSTS) to predict online sales performance. Bondarchuk et al. [12] investigated the dominant themes in British weather news. By taking advantage of the capacity of WordSmith Tools 7.0 for lexical–thematic analysis, they highlight the frequently occurring words and categorize them into themes according to their lexical–thematic groups. In agreement with Sinclair [15], they suggest that lexical units and keywords are important elements that contribute to constructing the semantic framework. The analysis of collocation is expanded not only for categorizing themes and topics of a corpus, but also for linking these topics to the actions that relate to self-presentation in social media. Thelwall et al. [16] applied a thematic analysis to identify the influence of gender identities on self-presentation on Twitter. They argue that many studies have not fully covered the personal disclosers that discuss non-binary users in addition to male and female users. In comparison with other studies, they deal with deep thematic analyses as they consider other possible themes and topics related to the identification of the self. They use thematic analysis by identifying word association [16], a method to uncover differences between sets of texts by identifying words that occur together with high frequency in a subset. Their study discussed the themes and gender differences in classified users. They found that each type of male, female, and nonbinary user had 33 themes based on the analysis of the collocations. Appl. Sci. 2023, 13, 3308 4 of 12 The studies of thematic analysis using collocation networks did not fully consider how collocates are used in various contexts by looking at the concordance lines. The previous studies considered the strong quantitative measurement of the relationship between words and lexis as a central approach for thematic analysis. By doing this, they discussed the general topics of a corpus depending on the mathematical relationship. This means that the previous studies did not investigate the semantic shifts of keywords and their meanings according to their situational context in a text. This limitation suggests the need for a more detailed methodology for providing deep classification of the same topics of a corpus. 2.2. Long Short-Term Memory Networks Using human resources to handle text classification is a costly process. This problem can be solved by using advanced algorithms to handle the text-classification process. However, using advanced algorithms would require several processes, including data gathering, pre-processing/cleaning, annotation/tagging, and, finally, building a predictive model to facilitate classifying new instances. With the recent increase in the availability of data, this would reduce the time taken by humans to classify data, as well as increase the efficiency of the analysis. The machine learning approach with neural networks, specifically long short-term memory (LSTM), is effective in the field of text classification and natural language processing (NLP). In this research, we adopt LSTM for text classification to perform a thematic analysis using concordance lines that are taken from news articles. We suggest the importance of investigating the concordance lines of the articles rather than only the topics of the article because multiple themes may occur within an article. For example, an article may start with a main idea, but various subthemes may appear clearly in other parts of the article. Thus, we focus on the semantic shift of concepts and ideas in the whole corpus of news articles. Long short-term memory (LSTM) has excelled recently in the field of artificial intelligence and deep learning [17]. It is an advanced method of using the recurrent neural network (RNN). Unlike RNN, which is unable to learn long-term dependencies [18], LSTM keeps track of the input stream for a long period of time. LSTM efficiently improves performance by memorizing the relevant information, and it uses this information for prediction. This process allows for the retention of information passed to neurons, specifically with regard to long sentences [19]. LSTM has been used to perform various classification tasks in NLP. Ranjan et al. [20] presented document classification tasks using LSTM. The experiment was performed on 20 categories using 20 newsgroup datasets. The accuracy obtained from the classification was approximately 92% [20]. Furthermore, Andrade et al. [21] used LSTM for different purposes, such as detecting different types of malware. They looked at software hexadecimal codes instead of dealing with textual data. They classified malware to five different types and obtained an accuracy that reached 67.60% In addition, LSTM was used to identify sentence subjectivity using political and ideological datasets. In Al Hamoud et al. [22], an experiment was performed to identify each sentence that referenced a specific issue as either as a pro or con. They achieved an accuracy rate approaching 96%. However, these studies did not provide a methodology for a thematic analysis of a specific topic, in contrast to the current study, which investigates themes and topics of the media coverage for the Syrian chemical attack. Other studies applied a general thematic analysis, and the classification was only on a surface level; it was not a deep thematic classification, as is the case in the current study. For example, the dataset of Jelodar et al. [23] was classified into two classes, either positive or negative, and their model achieved 81.15% accuracy. Further, LSTM has been used in sentiment analysis, reaching 85% accuracy [24]. These studies do not contribute to providing detailed and complex thematic analyses for a corpus around a particular topic. This literature review has shown that previous studies highlight the importance of analyzing keywords and the collocations in a corpus to provide a full understanding of Appl. Sci. 2023, 13, 3308 5 of 12 the themes and topics of a corpus. These methodological perspectives are important for providing themes and topics and outlining the semantic structure of a text. The literature suggests that LSTM is an effective tool for classifying the data into groups or themes. However, these studies should consider deep analysis by investigating the changes in the meaning through the collocational network, such as with a deeper method as proposed by Brezina et al. [2]. Altameemi [3] suggests that considering the themes of a corpus should not be limited to the collocation network; concordance lines should be analyzed to consider how collocates are used in their specific contexts. Thus, the statistical and quantitative analyses are not enough to fully present the semantic shift of terms and concepts in a corpus. Considering this, Altameemi [3] manually analyzed a sample of the concordance lines for the strong collocates. However, he suggests it might be good to apply an automatic thematic analysis of all the concordance lines. Due to these limitations, the current research suggests the importance of developing a methodology for using algorithms for the automatic analysis of all the concordance lines of the strong collocates, which will increase the representativeness of analyzing a corpus. 3. Methodology In this study, we test a proposed methodology that has not been adopted by any previous studies, according to the best knowledge of the researchers. The proposed methodology suggests applying a combination of algorithms to carry out the automatic thematic analysis of the concordance lines. In this section, we highlight the steps that the researchers applied to conduct the automatic thematic analysis. Then we present the data that was used in this research. Next, we demonstrate the experiment conducted in this research, along with the obtained results. 3.1. Architectures of LSTM Units There are several architectures of LSTM units. We can observe the basic architecture of an LSTM memory block in Figure 1. A typical architecture of an LSTM unit consists of three gates: a forget gate, an input gate, and an output gate, in addition to the memory cell, which is part of the LSTM unit. Information is added or removed to the cell state through those gates. The forget gates determines what relevant information needs to be kept or removed during training. Values are determined according to the sigmoid function, typically 0 or 1. When the value is close to zero, then the information is ignored. Otherwise, when the value is close to one, then the information is kept. The input gate controls which information needs to be saved in the memory. The output gate determines which information should be exposed to the memory cell. Similarly, both gates are controlled by the sigmoid function, which determines whether the information needs to be processed or not [23]. 3.2. Datasets and Pre-Processing The data of the research are adopted from Altameemi’s thesis [3], but for a different purpose: testing a proposed methodology. The data is taken from four UK newspapers: The Telegraph, The Guardian, The Sun, and The Mirror. The dataset consists of articles related to covering the use of chemical weapons in Syria. The concordance lines of the corpus are taken from the occurrences of the strong collocates that are pre-processed with Altameemi’s thesis. These lines are manually analyzed by Altameemi according to the themes (for more details about the thematic analysis of the concordance lines, see Altameemi [3]). The themes are extracted according to their original classification in Altameemi’s project. Four themes are generated and represent most lines in the dataset. The themes used in the experiment are “Evaluation of the Syrian situation”, “International stance”, “Representation of the UK and USA together”, and “Local context of the UK”. A total of 1062 lines are extracted that represent the themes, as shown below in Table 1. The total number of lines and tokens for each theme are listed below. Appl. Sci. 2023, 13, 3308 6 of 12 Figure 1. Architecture of the long short-term memory (LSTM) neural network (cited from [25]). Table 1. Total number of concordance lines and tokens for each theme. Themes Number of Lines Number of Tokens Evaluation of the Syrian situation 250 8396.0 Representation of the UK and USA together 264 9230.0 Local context of the UK 250 9780.0 International stance 298 10,635.0 In the first theme, the lines focus on evaluating the Syrian situation with either the representation of the chemical attacks or the tragedy in Syria. The second theme focuses on the representation of the UK and USA together, specifically from the perspectives of supporting allies and their political orientations. The third theme represents the UK’s political stance in relation to the Syrian chemical attack and the possibility of British military action. The last theme, “International stance”, highlights the role of international agents such as the United Nations. Table 2 below shows a sample of these lines. Table 2. Sample of thematic analysis from extracted KWIC. Themes Lines Evaluation of the It portrays ISIS as the main threat to Syrians, despite Assad Syrian situation killing at least six times more civilians. Representation of the UK and USA together Saturday August 24: David Cameron and USA President Barack Obama pledge a “serious response” to the Syria gas attack after a critical 30 min phone call between the pair at 4.30pm n which they decide to consider “all options”. Downing Street says, after a weekend of briefing that Local context of the UK military intervention may be imminent. A former UN appeal judge and leading international lawyer, International stance said yesterday that Russia was wrong to insist that military intervention would hinge on UN consent. In the current study, we suggest that analysts may take a random selection of concordance lines that might be representative of the results. Then, the rest of the concordance lines should be classified electronically according to the manual categorization of the se Appl. Sci. 2023, 13, 3308 7 of 12 lected analyzed concordance lines. To test this method’s validity, we used the same data that were used in Altameemi’s thesis. The lines are pre-processed with some processing steps. For example, numbers, special characters, stop words, and non-English letters are removed. Then, each line is converted into a sequence of tokens which serves as the input to the neural network. Each token represents a single word converted to a digit, and each digit is used for the training process. We chose the most frequent 5000 words. The chosen words represent the maximum features considered in the input layer of the neural network. 4. Experiments and Results 4.1. Experiments The first layer in our model is an embedding layer. This layer receives the previously tokenized words using a dense vector representation. The order of the words processed is taken into consideration. The next layer is an LSTM layer with 100 neurons. A dropout layer is used for regulating the network and keeping it away, as much as possible, from any bias. The final dense layer is the output layer, which has four classes representing the four distinct themes in this experiment. All of the data processed for training are balanced among the four themes. The models are compiled using the Adam optimizer and sparse_categorical_crossentropy. The Adam optimizer is an ideal deep learning optimizer for handling classification problems [26,27]. For training and testing, we split our dataset into 80% training and 20% testing (849 lines for training and 213 lines for testing). The model was implemented in Keras, an open-source library that provides a high-level neural network API [28]. Table 3 shows the experimental configuration setting of our LSTM models. Table 3. Details of long short-term memory (LSTM) model. Layer Output Shape Number of Parameters Embedding (None, 520, 100) 5,000,000 Dropout layer (None, 520, 100) 0 LSTM (None, 100) 80,400 Dense (None, 4) 404 Total parameters 5,080,804 Trainable parameters: 5,080,804 Non-trainable parameters 0 Furthermore, in order to verify the effectiveness of choosing the LSTM model, we compared the results obtained with another deep learning model using a recurrent neural network (RNN). The experiment was performed using the same dataset and settings mentioned in Table 3. However, instead of using LSTM layers, we used the RNN layer (SimpleRNN), which handles information over time using a memory cell. 4.2. Evaluation Measures After performing the experiment, we used several measures to evaluate our results. We calculated the confusion matrix for all the themes in the experiment. Each entry in the confusion matrix represents the following: the number of true positive (TP), which is the number of lines where the classification matches the correct theme; the false positive (FP) which represents the number of lines where the classification matches the incorrect theme as positive; true negative (TN), which is the number of cases where the classification matches the correct negative theme; and false negative (FN), which refers to the number of lines where the classification matches the incorrect negative theme. We also evaluated the model to assess the performance using accuracy, precision, recall, and F1 score (Davis, 2006), based on the result of the confusion matrix. Appl. Sci. 2023, 13, 3308 8 of 12 Accuracy is the measurement of all lines that have been classified correctly. It consists of all the true positive and negative lines divided by all the lines being classified. It is defined as: TP + # TN Accuracy = TP + # FP + # FN + # TN [.] Precision implies the number of selected lines that are correctly classified as positive. It consists of all the true positive lines divided by all the lines that are classified as positive. It is defined as: TP Precision = TP + # FP [.] Recall indicates the proportion of actual classes that are correctly categorized as positive. It consists of all the true positive lines divided by all the lines that are actually positive. It is defined as: TP Recall = TP + # FN [.] F1 score is the combination of both precision and recall. It is defined as: F1 = 2 × [Recall][ ×][ Precision] Recall + Precision [.] 4.3. Results In this section, we first present our classification results in all the measures mentioned above. Then we analyze the misclassification lines that were incorrectly classified by the modes. Lastly, we discuss our findings and limitations. Table 4, below, presents the results of the thematic analysis of concordance lines using both long short-term memory (LSTM) and recurrent neural network (RNN). The result shows the high accuracy of using LSTM, reaching 84%, in comparison to the 79% accuracy achieved using RNN Table 4. Results of using deep learning modes LSTM and RNN in thematic analysis of concor- dance lines. Measures LSTM RNN Precision 0.84 0.79 Recall 0.84 0.79 F1 score 0.83 0.78 Accuracy 0.84 0.79 Table 5, below, shows the results of the automatic thematic analysis of concordance lines. The results show a comparison between both LSTM and RNN for the four identified themes. We can clearly see that LSTM outperforms RNN within each theme, with regard to the accuracy of classifying the concordance lines. The lines are categorized into four themes: (1) Evaluation of the Syrian situation, (2) International stance, (3) Representation of the UK and USA together, and (4) Local context of the UK. The obtained accuracy was tested on unseen concordance lines. Using the LSTM, the “Evaluation of the Syrian situation” theme achieved 100% recall without misclassifying any lines. This is the same result achieved by RNN. Next, the “Representation of the UK and USA together” theme achieved the second-highest recall of 94%, with only three lines having been misclassified. RNN achieved a very close result, reaching a recall of 93%. Furthermore, the “International stance” theme achieved a low recall of 69%, with 16 lines having been misclassified. RNN achieved the worst result, reaching a recall of 44%. Appl. Sci. 2023, 13, 3308 9 of 12 Table 5. Results of the text automatic thematic analysis over four themes using LSTM and RNN. Evaluation of the Syrian Representation of the UK Themes International Stance Local Context of the UK Situation and USA Together Measures LSTM RNN LSTM RNN LSTM RNN LSTM RNN Precision 0.85 0.74 0.72 0.77 0.88 0.94 0.89 0.72 Recall 1.00 1.00 0.69 0.44 0.94 0.93 0.71 0.79 F1 score 0.92 0.85 0.71 0.56 0.91 0.93 0.79 0.75 In terms of precision, the “Local context of the UK”, “Representation of the UK and USA together”, and “Evaluation of the Syrian situation” themes achieved high precision using LSTM, with results of 89%, 88%, and 85%, respectively. The “International stance” theme achieved a low precision of 72%, and 14 lines were classified as belonging to the “Local context of the UK” theme. Generally, RNN achieved a lower precision score than LSTM. For the themes of “International stance” and “Representation of the UK and USA together”, the RNN obtained slightly better results (77% and 94%, respectively) than LSTM. However, LSTM obtained better results for the themes of “Evaluation of the Syrian situation” (74% in RNN) and “Local context of the UK” (72% in RNN). Considering the overall results in Tables 4 and 5, we can see how LSTM is more accurate, in general, for the thematic analysis than the RNN model. The confusion matrix in Table 6 shows how many items match the correct categorization and how many lines match incorrect themes. Thus, we consider the percentage of the accuracy for each theme rather than only the total amount of accuracy. Table 4, below, shows that the categorization of the lines that relate to the first and third theme have the highest correct categorization among the themes. The fourth theme, “Local context of the UK”, comes after the first and third themes in the accuracy of categorizing the lines. However, the “International stance” them has the lowest accuracy of categorization. In this section, we discuss how the lines are categorized in each theme and the instances of incorrect categorization, which highlights the effectiveness and issues of the proposed methodology. Table 6. Confusion matrix for thematic analyses using LSTM model. Predicted Label Evaluation of the Representation of the Local Context of International Stance Syrian Situation UK and USA Together the UK Evaluation of the 51 0 0 0 Syrian situation International stance 6 36 5 5 Representation of the 0 3 51 0 UK and USA together Local context of the UK 3 11 2 40 As shown in the table above, the theme of evaluating the Syrian attack was the most accurate theme, which matches the exact categorization. All of the categorized lines meet the original classification. Thus, for this theme, there is no discussion of the incorrect instances. The second most accurate theme was the “Representation of the UK and USA together” theme, which had three incorrect classifications. For example: 1. John Kerry administered a diplomatic slap in the face to Britain following David Cameron’s withdrawal of military support for intervention in Syria. In the experiment, Example 1 was categorized under the theme of “International stance”, while it originally related to “Representation of the UK and USA together”. Categorizing such examples is difficult, as this theme seems to be in the blurred area between Appl. Sci. 2023, 13, 3308 10 of 12 more than one theme (this point will be expanded below, with the consideration of the other themes). This example and the other lines in the corpus show the high accuracy of classifying the lines of this theme, even though the algorithm missed three instances. A less-accurately categorized theme was “Local context of the UK”. The algorithm missed the correct categorization of 16 lines. From the lexical perspective, regarding their semantic use, words such as “UK” and “Britain” were frequently used both for the representation of the local context and also from the perspective of the international stance. For example: 2. International community needs to ask: would Assad utilize chemical weapons stage bring potential western military intervention international community to investigate the reasons behind the use of chemical weapons. 3. Later came comments from the USA House speaker John Boehner backing a Syria war resolution, adding to the likelihood of Congress voting for USA action. Examples 2 and 3 were classified as a part of the UK stance, while they were originally categorized in the theme of “International stance”. Lexically, words such as “international”, “community”, “internationally”, and “prohibited” were frequently used with the “International stance” theme. In Example 4, the algorithm considered terms such as “voting” and “resolution” as strongly connected to the local context, while the semantic function of the whole line suggested the connection of the example to the “International stance” theme. The least accurate theme in the categorization was the “International stance” theme. Table 5, above, shows that the “International stance” theme had various incorrect classifications for the different themes. One reason might be that the “International stance” theme had strong terms reoccurring both in this theme and, at the same time, in other themes. The incorrect lines in the “International stance” theme have almost the same issues, for example: 4. He (General Sir Nick Houghton) revealed no decisions have been made on military involvement in Syria. 5. We have seen the unwinnable nature of the Afghan conflict. The terrible sores of the Balkan civil wars are still raw enough to remind us of what little effect our intervention had there. Examples 4 and 5 were classified under the “International stance” theme, while they were originally categorized under the “Local context of the UK” theme. Terms such as “Syria”, “Afghan”, “Balkan”, and “civil” strongly occurred in both themes. In the local context, they were used to refer to the social imagination of the country regarding the UK’s experience in international interventions, such as in Iraq and Afghanistan. At the same time, these terms were also used to represent the role of the United Nations and other actors, such as the US. Thus, these findings are consistent with those of Altameemi (2020), in that some lines are in the blurred area between more than one theme. 5. Discussion and Conclusions The main contribution of the article is in applying an automatic thematic analysis of concordance lines in a corpus using LSTM. The proposed methodology in this study is useful in various ways. First, the automatic categorization considers the aboutness of the corpus by the relationship between collocates and keywords, as well as their occurrences within the context of the whole text. The contribution this article provides is a deeper automatic thematic analysis than the previous studies, as discussed in the literature review. Linguists such as Brezina et al. [2] and Neg and Tan [10] focus only on the analysis of the aboutness of the corpus by quantitively analyzing the collocational network. On the other hand, researchers in the field of computer science, such as Gao et al. [19] and Ranjan et al. [20], apply classification and sentiment analysis of the whole text without looking at the semantic shifts of keywords in their concordance lines. The proposed method deals with how the collocates are used in their specific context through a thematic Appl. Sci. 2023, 13, 3308 11 of 12 analysis of the concordance lines. The categorization of the concordance lines determines which themes are given more concern than the others in the whole corpus. Second, the findings of this paper help to reassert the original result of the manual analysis of the randomly selected lines by looking at some other lines in the automatic categorization. In this study, when we checked some of the automatically categorized lines, we found them in different categories from the ones they had been place in during the original categorization. However, in semantic use, the automatic categorization seems to be more accurate than the original, manual categorization. This automatic thematic analysis method fills the gap proposed by Altameemi [3] that the collocation analysis does not provide a deep interpretation of the aboutness of a corpus. Thus, this algorithm may help analysts to revisit their manual categorization of some lines. Regarding the specific use of LSTM in thematic analyses, we have seen in the literature that this methodology is used for various purposes, such as sentiment analysis and document classification. Applying the method proposed in the current study will increase the efficiency of the result through a full analysis of the concordance lines, specifically for studies that investigate complex types of themes. The findings of the research suggest the effectiveness of the proposed methodology for the thematic analysis of media coverage. This method is applied to provide a deep thematic analysis. In other words, the current proposed methodology divides the concordance lines into themes in specific orders of discourse and topics (e.g., themes in the UK press coverage for the Syrian chemical attack). The proposed method of this research should take into account the following considerations. First, in applying this method, analysts need to look at the lines of the collocates and categorize a random selection of lines manually to train the algorithm before the automatic categorization. Further, analysts should apply the confusion matrix to test the algorithm before conducting the whole analysis. Then, the incorrect classifications should be reconsidered to improve the algorithm and validate the automatic analysis. For future research, it is recommended that the automatic categorizing should name the themes according to the most salient relationship between collocates/words rather than using the manual naming of themes in the manual analysis of the concordance lines. Author Contributions: Conceptualization, Y.A. and M.A.; methodology, Y.A. and M.A.; software, M.A.; validation, Y.A. and M.A.; formal analysis, Y.A.; investigation, M.A.; resources, Y.A. and M.A.; data curation, M.A.; writing—original draft preparation, Y.A. and M.A.; writing—review and editing, Y.A.; visualization, M.A.; supervision, Y.A.; project administration, Y.A.; funding acquisition, Y.A. All authors have read and agreed to the published version of the manuscript. Funding: This research has been funded by the Deputy for Research and Innovation, Ministry of Education through the Initiative of Institutional Funding at University of Ha’il-Saudi Arabia through project number IFP-22 050. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Data Availability Statement: Not applicable. Acknowledgments: This research has been funded by the Deputy for Research and Innovation, Ministry of Education through the Initiative of Institutional Funding at University of Ha’il-Saudi Arabia through project number IFP-22 050. Conflicts of Interest: The authors declare no conflict of interest. References 1. Baker, P. Using Corpora in Discourse Analysis; Bloomsbury Academic: London, UK; New York, NY, USA, 2006. 2. Brezina, V.; Mcenery, T.; Wattam, S. Collocations in context A new perspective on collocation networks. Int. J. Corpus Linguist. 2015, 202, 139–173. [CrossRef] 3. Altameemi, Y. Defining ‘Intervention’: A Comparative Study of UK Parliamentary Responses to the Syrian Crisis-ORCA; University of Cardiff: Cardiff, UK, 2020. Appl. Sci. 2023, 13, 3308 12 of 12 4. Wodak, R. The semiotics of racism: A Critical Discourse-Historical Analysis. In Discourse, of Course: An Overview of Research in Discourse Studies; Renkema, J., Ed.; Benjamins: Amsterdam, The Netherlands, 2009; pp. 311–326. 5. Fernández, B.G.; Schmitt, N. How much collocation knowledge do L2 learners have?: The effects of frequency and amount of exposure. Int. J. Appl. Linguist. 2015, 166, 94–126. 6. Nesselhauf, N. Collocations in native and non-native speaker language. In Collocations in a Learner Corpus; John Benjamins Publishing Company: Amsterdam, The Netherlands, 2005; Volume 14, pp. 1–10. 7. Paquot, M.; Granger, S. Formulaic Language in Learner Corpora. Annu. Rev. Appl. Linguist. 2012, 32, 130–149. [CrossRef] 8. Evert, S. Corpora and collocations. In Corpus Linguistics: An International Handbook; Lüdeling, A., Kytö, M., Eds.; Mouton de Gruyter: Berlin, Germany, 2008; pp. 1212–1248. 9. Gablasova, D.; Brezina, V.; McEnery, T. Collocations in Corpus-Based Language Learning Research: Identifying, Comparing, and Interpreting the Evidence. Lang. Learn. 2017, 67, 155–179. [CrossRef] 10. Ng, R.; Tan, Y.W. Diversity of COVID-19 news media coverage across 17 countries: The influence of cultural values, government stringency and pandemic severity. Int. J. Environ. Res. Public Health 2021, 18, 11768. [CrossRef] [PubMed] 11. Huang, L.; Dou, Z.; Hu, Y.; Huang, R. Textual analysis for online reviews: A polymerization topic sentiment model. IEEE Access 2019, 7, 91940–91945. [CrossRef] 12. Bondarchuk, N.; Bekhta, I.; Melnychuk, O.; Matviienkiv, O. Keyword-based Study of Thematic Vocabulary in British Weather News. CEUR Workshop Proc. 2022, 3171, 451–460. 13. Biber, D.; Reppen, R. The Cambridge Handbook of English Corpus Linguistics; Cambridge University Press: Cambridge, UK, 2020. 14. Huang, L.; Dou, Z.; Hu, Y.; Huang, R. Online Sales Prediction: An Analysis with Dependency SCOR-Topic Sentiment Model. IEEE Access 2019, 7, 79791–79797. [CrossRef] 15. Sinclair, J. Corpus, Concordance, Collocation; Oxford University Press: Oxford, UK, 1991. 16. Thelwall, M.; Thelwall, S.; Fairclough, R. Male, Female, and Nonbinary Differences in UK Twitter Self-descriptions: A Fine-grained Systematic Exploration. J. Data Inf. Sci. 2021, 6, 1–27. [CrossRef] 17. Dou, Z.; Sun, Y.; Zhang, Y.; Wang, T.; Wu, C.; Fan, S. Regional Manufacturing Industry Demand Forecasting: A Deep Learning Approach. Appl. Sci. 2021, 11, 6199. [CrossRef] 18. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997, 9, 1735–1780. [CrossRef] [PubMed] 19. Gao, M.; Shi, G.; Li, S. Online prediction of ship behavior with automatic identification system sensor data using bidirectional long short-term memory recurrent neural network. Sensors 2018, 18, 4211. [CrossRef] [PubMed] 20. Ranjan, M.N.M.; Ghorpade, Y.R.; Kanthale, G.R.; Ghorpade, A.R.; Dubey, A.S. Document classification using lstm neural network. J. Data Min. Manag. 2017, 2, 1–9. 21. Andrade, E.D.; Viterbo, J.; Vasconcelos, C.N.; Guérin, J.; Bernardini, F.C. A model based on LSTM neural networks to identify five different types of malware. Procedia Comput. Sci. 2019, 159, 182–191. [CrossRef] 22. Al Hamoud, A.; Hoenig, A.; Roy, K. Sentence subjectivity analysis of a political and ideological debate dataset using LSTM and BiLSTM with attention and GRU models. J. King Saud Univ. Inf. Sci. 2022, 34, 7974–7987. [CrossRef] 23. Jelodar, H.; Wang, Y.; Orji, R.; Huang, S. Deep Sentiment Classification and Topic Discovery on Novel Coronavirus or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network Approach. IEEE J. Biomed. Health Inform. 2020, 24, 2733–2742. [CrossRef] [PubMed] 24. Murthy, G.S.N.; Allu, S.R.; Andhavarapu, B.; Bagadi, M.; Belusonti, M. Text based sentiment analysis using LSTM. Int. J. Eng. Res. Tech. Res. 2020, 9, 299–303. 25. Zhang, A.; Lipton, Z.C.; Li, M.; Smola, A.J. Dive into deep learning. arXiv 2021, arXiv:2106.11342. 26. Rao, A.; Spasojevic, N. Actionable and political text classification using word embeddings and LSTM. arXiv 2016, arXiv:1607.02501. 27. Fatima, N. Enhancing Performance of a Deep Neural Network: A Comparative Analysis of Optimization Algorithms. ADCAIJ Adv. Distrib. Comput. Artif. Intell. J. 2020, 9, 79–90. [CrossRef] 28. Géron, A. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow; O’Reilly Media, Inc.: Sebastopol, CA, USA, 2022. Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Testing increases suggestibility for narrative-based misinformation but reduces suggestibility for question-based misinformation Author: LaPaglia, Jessica A., Date: 2013 Collections: NeuroPsychoLinguisticPolitics, PoliticalML Zotero Key: LDTF7M3L Cite Key: LaPaglia13TestIncrsSuggestibility Zotero Item | Lit Note Behavioral Sciences and the Law Behav. Sci. Law 31: 593–606 (2013) Published online 16 September 2013 in Wiley Online Library (wileyonlinelibrary.com) DOI: 10.1002/bsl.2090 Testing Increases Suggestibility for Narrative-based Misinformation but Reduces Suggestibility for Question-based Misinformation Jessica A. LaPaglia* and Jason C. K. Chan[‡]* A number of recent studies have found that recalling details of an event following its occurrence can increase people’s suggestibility to later presented misinformation. However, several other studies have reported the opposite result, whereby earlier retrieval can reduce subsequent eyewitness suggestibility. In the present study, we investigated whether differences in the way misinformation is presented can modulate the effects of testing on suggestibility. Participants watched a video of a robbery and some were questioned about the event immediately afterwards. Later, participants were exposed to misinformation in a narrative (Experiment 1) or in questions (Experiment 2). Consistent with previous studies, we found that testing increased suggestibility when misinformation was presented via a narrative. Remarkably, when misinformation was presented in questions, testing decreased suggestibility. Copyright # 2013 John Wiley & Sons, Ltd. In a recent New Jersey ruling, there was a reformulation of juror instructions aimed at clarifying how eyewitness testimony should be evaluated (Weiser, 2012). The new instructions educate jurors on findings from decades of research demonstrating the fallibility of eyewitness memory. Indeed, memory for an event or perpetrator can be drastically altered when witnesses are exposed to misleading post-event information (the misinformation effect; for a review, see Loftus, 2005). More relevant to the present research is a counterintuitive finding that has been reported in a series of studies; namely, that recalling the details of an event can increase eyewitness suggestibility to later presented misinformation (e.g., Chan, Thomas, & Bulevich, 2009; Chan & LaPaglia, 2011). This finding, termed retrieval-enhanced suggestibility (RES), is surprising because retrieval often enhances retention of an event (i.e., the testing effect; Roediger & Karpicke, 2006). Given that retrieval practice is such a powerful memory enhancer, the logical prediction is that completing a recall test for a witnessed event soon after its occurrence should enhance retention of that event and thus protect witnesses from later suggestions. Consistent with this idea, having a better memory for a witnessed event is often associated with lower susceptibility to misinformation. For example, improved encoding of a witnessed event can reduce the misinformation effect (Lane, 2006; Pezdek & Roe, 1995). Similarly, the impact of misinformation is weaker when the witnessed event is still “fresh in the mind” than if forgetting has set in (e.g., Chan & Langley, 2011; Loftus, Miller, & Burns, 1978). However, contrary to the prediction that testing would reduce Correspondence to: Jessica A. LaPaglia, Department of Psychology, Morningside College, 1501 Morningside Ave, Sioux City, IA 51106, U.S.A. E-mail: lapagliaj@morningside.edu †Department of Psychology, Morningside College, 1501 Morningside Ave, Sioux City, IA 51106, U.S.A. ‡Iowa State University 594 J. A. LaPaglia and J. C. K. Chan suggestibility, Chan and colleagues have repeatedly found that testing increased the misinformation effect (e.g., Chan & Langley, 2011; Chan et al., 2009; Chan, Wilford, & Hughes, 2012). A two-factor account has been proposed to explain the RES phenomenon. The first factor suggests that initial testing may paradoxically enhance learning of the subsequent misinformation (Gordon & Thomas, 2013), thus increasing its likelihood of being recalled later (e.g., Zaragoza & Mitchell, 1996). For example, after watching a bank robbery video, participants may be asked about the amount of time the robber gives the police to gather the ransom. Later, when participants heard misinformation regarding this detail, the initial test question may inadvertently draw attention to, and thus enhance the encoding of, this misinformation. The second factor suggests that performing retrieval may trigger reconsolidation of the original memory, thus allowing the misinformation to update the original memory trace and making later recall of the original memory more difficult. A growing body of evidence has suggested that previously consolidated memories re-enter a labile state upon retrieval and must be reconsolidated, and the reactivated memories are particularly susceptible to disruption during this reconsolidation window (Chan & LaPaglia, 2013). Indeed, reconsolidation-based memory updating has been suggested to play a central role in false memories in humans (Hardt, Einarsson, & Nader, 2010). Although the RES effect shows that testing can be detrimental to the accuracy of eyewitness memory, several studies have reported the contrary, such that initial testing can protect eyewitnesses from misleading suggestions (e.g., Gabbert, Hope, Fisher, & Jamieson, 2012; LaPaglia & Chan, 2012; Memon, Zaragoza, Clifford, & Kidd, 2010; Pansky & Tenenboim, 2011; Saunders & MacLeod, 2002). Numerous methodological differences between these sets of studies likely contribute to the disparity in the effects of testing on suggestibility. For example, compared with studies demonstrating RES, the studies that showed a testing effect used different to-be-remembered materials (faces instead of events; LaPaglia & Chan, 2012), types of misinformation suggested [central details instead of peripheral details (Gabbert et al., 2012); self-fabricated misinformation instead of experimenter-provided misinformation (Memon et al., 2010)], and type of initial test administered (the Self-Administered Interview instead of a standard recall test; Gabbert et al., 2012). It is beyond the scope of the present study to address the potential contribution of all of these variables, so we focus on one factor that might determine whether initial testing enhances or exacerbates eyewitness suggestibility. Specifically, we examined whether variations in the way misinformation is presented (i.e., in a narrative or misleading questions) modifies the influence of prior testing on suggestibility. This variable is of particular interest because studies that have found initial testing to reduce suggestibility often presented misinformation in questions (Pansky & Tenenboim, 2011; Saunders & MacLeod, 2002), whereas studies that have demonstrated RES always presented misinformation in a narrative. In an experiment conducted by Saunders and MacLeod (2002), participants read two written narratives describing separate burglaries. After the encoding phase, participants were tested on half of the items from one of the burglaries, while all remaining items were not tested. Participants then completed a free recall test for all the studied items before misinformation was introduced via misleading questions. For instance, one narrative mentioned that the burglars stole a necklace that was next to the sink. During the misinformation phase, participants responded to the question, “When the burglars stole the earrings that were next to the sink in the kitchen, they knocked some items on the floor breaking them. What did they break?” Performance was then measured in a forced-choice Testing and suggestibility 595 recognition test. Unlike studies reporting RES, Saunders and MacLeod found that participants were less likely to select the misinformation for the tested items than for the non-tested items. Three key differences between Saunders and MacLeod’s (2002) study and studies that found RES might have contributed to these contrasting findings. First, initial retrieval accuracy was far higher (~89% of questions were recalled correctly) in Saunders and MacLeod’s study than was typically observed in studies that reported RES (~55% initial test accuracy). This is important because highly recallable items (i.e., easy items, central items) are more resistant to the RES effect than items that are more difficult to remember (e.g., peripheral items; Chan & Langley, 2011; Wilford, Chan, & Tuhn, 2013). So it is possible that Saunders and MacLeod’s critical items were easily remembered and thus not susceptible to the RES effect. Secondly, because all participants had completed a free recall test before they were exposed to misinformation, there was no true no-test control condition in this study. Thirdly, and of particular relevance to the current study, the misinformation was presented within misleading questions (as opposed to a narrative). Retrieving information while being exposed to misinformation is different from passively listening to or reading misinformation presented in a narrative, and this may play a key role in determining whether testing would increase or decrease subsequent suggestibility. In Pansky and Tenenboim’s (2011) study, participants viewed a slideshow as their critical event and then took a test over some of the information before they were exposed to misinformation. A final test was given 48 hours later. Pansky and Tenenboim found that participants were less suggestible for the tested items than for the non-tested items. This study is of particular interest because its methodology is highly similar to the studies reporting RES. In an attempt to explain the inconsistencies between their results and those reported by Chan et al. (2009), Pansky and Tenenboim (2011) pointed to four methodological differences: the length of the event video; initial testing manipulated within versus between subjects; a 48-hour versus 30-minute retention interval; and misinformation introduced via questions versus narrative. Based on current knowledge, the first three differences are likely irrelevant, because the RES effect has been demonstrated across different witnessed event lengths (Chan et al., 2009, 2012), retention intervals (Chan & Langley, 2011), and in both within- and between-subjects designs (Chan & LaPaglia, 2011). We are thus left with the fourth methodological difference – that Pansky and Tenenboim presented their misinformation via written questions, whereas studies showing RES presented misinformation in a narrative. As is apparent from this brief review, initial testing can reduce or increase suggestibility, although the processes underlying the opposite patterns of results are unknown. We suspect that these discrepant findings are related to the way in which misinformation is presented. Here we examine whether variations in how misinformation is delivered (namely, via questions or via a narrative) alter the influence of testing on eyewitness suggestibility. Several studies have investigated the impact of the misinformation presentation method on suggestibility, and the results are mixed. For example, Zaragoza and Lane (1994) found that presenting misinformation in questions increased suggestibility relative to presenting misinformation in a narrative. Similarly, Saunders (2009) reported that misleading questions are more effective at inducing false memories than is a narrative, although this effect was only observed for misinformation regarding central details and not peripheral details. Gobbo (2000) showed that presenting misinformation in questions alone produced a smaller misinformation effect than presenting misinformation in both 596 J. A. LaPaglia and J. C. K. Chan questions and a narrative (such that the misinformation was repeated). Unfortunately, it is impossible to disentangle the effects of misinformation repetition and presentation method in this study. Moreover, presenting misinformation in questions that do not presuppose the truth of the misinformation, but rather questions its existence (e.g., “I used the Dove soap, was it Dove?”) can eliminate the misinformation effect altogether (Lee & Chen, 2012). In sum, if we were to draw a preliminary conclusion based on extant findings, it appears that question-based misinformation might be more effective at inducing false recall than narrative-based misinformation, though this effect is far from universal. More important for present purposes, however, is that we know virtually nothing about whether testing would alter this tentative relationship. The Current Study In two experiments, participants watched a video of a bank robbery. Afterwards, participants in the test condition took an initial recall test over the contents of the video. For instance, one critical detail in the present experiments is the amount of time the bank robber gives the police to meet his demands. In the initial test, participants were asked, “How long does the robber give the police to get him the money?” The correct answer to this question is 2 hours. After a short delay, they either listened to a narrative containing misinformation (Experiment 1) or responded to new questions containing misinformation (Experiment 2). If the misinformation was presented in a narrative, participants heard: “The robber demands that the police get him his money and a car in 1 hour.” If the misinformation was presented in a question, participants were asked: “In addition to the money, the robber gives the police 1 hour to also get him what?” Later, all participants completed a final recall test over the video. The question here was whether an RES effect would remain when misinformation was presented in questions. We hypothesized that, compared with presenting misinformation in a narrative, presenting misinformation in questions may result in an elimination of the RES effect. Attempting to answer a question, as opposed to passively listening to a narrative, could encourage participants to engage in more effortful processing (Carpenter, 2009; Pyc & Rawson, 2009). More concretely, research has shown that taking a recall test can sometimes trigger spontaneous retrieval of related information in memory (Chan, 2009). When placed into the present context, answering the misleading question about what the robber demanded (a getaway car) may lead participants to covertly retrieve the related detail (i.e., he needed the car in 2 hours). This would increase the likelihood that one would detect a discrepancy between one’s memory (from either the video event itself or from the response produced during the initial test, or both) and the misinformation (e.g., the robber needed the car in 1 hour), thereby reducing the influence of the misinformation (Tousignant, Hall, & Loftus, 1986). In fact, based on this logic, it is possible that presenting misinformation in questions might not only eliminate the RES effect, it may reverse it, such that testing would reduce the misinformation effect. Notably, we believe that question-based misinformation would reduce suggestibility relative to narrativebased misinformation only for participants who have received an initial test. Specifically, we believe that participants would be far more likely to notice contradictions between their memory and the misinformation in the question if they have produced an overt response of that very detail during the initial test. A further prediction from this logic is that testing would reduce question-based misinformation only if participants can recall the critical detail correctly during the initial test. Testing and suggestibility 597 EXPERIMENT 1 In the first experiment, we attempted to extend the RES effect with narrative presentation to a new set of witnessed event materials. Although a number of studies have demonstrated RES, most have used the same 40-minute-long video of a terrorist attack as the witnessed event (e.g., Chan et al., 2009; Chan & LaPaglia, 2011). One other published study has demonstrated the effect with a 10-minute video event of a museum burglary (Chan et al., 2012). Here we developed a new video featuring a bank robbery, and the length of the video was somewhere between the longer (40 minutes) video and the shorter (10 minutes) video. Therefore, finding RES with this new set of materials would further ascertain its generalizability. Method Participants and Design Experiment 1 used a 2 (test vs. no test)× 2 (post-event information: neutral vs. misled) mixed design. Test vs. no test was manipulated between subjects. Post-event information was manipulated within subjects. There were 20 participants in each between-subjects condition for a total of 40 participants (23 males). Their mean age was 19.50 years (SD= 1.18). Participants were undergraduate students from a Midwestern university who took part in this experiment for partial course credit. The experiment consisted of four main phases. First, participants completed the encoding phase in which they viewed the witnessed event video. Secondly, participants completed the no-test/test phase. Here, they either performed a distractor task or a recall test. Thirdly, participants were exposed to misinformation via an audio narrative. Finally, participants were administered the final recall test. Materials and Procedure Participants began by watching a 25-minute video from an episode of the television show Flashpoint. The video depicted a bank robbery by a disgruntled former employee. Participants were given intentional encoding instructions. Specifically, they were told to pay close attention to the video, including the actions and surrounding environment, because their memory would be tested later. Following the video, participants in the no-test condition played the videogame Tetris for 7 minutes as a distractor activity, whereas participants in the test condition took a cued recall test over their memory for the video. This test consisted of 14 open-ended, non-leading questions (e.g., “How long does the robber give the police to get him the money?”). Participants were given 30 seconds to answer each question and the initial test phase lasted 7 minutes. They were told to be as accurate as possible and not to guess. No corrective feedback was given. Once participants completed the initial test or Tetris, they were shown another video to fill a retention interval. The video was a clip from the BBC show Spooks. It depicted a terrorist plot against abortion doctors and lasted approximately 20 minutes. Participants were told that they should pay close attention to the video, but they were not told whether their memory for the video would be tested later. Following the distractor video, participants were presented with misinformation embedded within an audio narrative via headphones. The audio narrative lasted roughly 598 J. A. LaPaglia and J. C. K. Chan 6 minutes. All 14 critical details queried in the initial test were included in the narrative. The critical details were either presented as a misled item or a neutral item. For example, one critical detail was the amount of time the robber gave police to get him the money. If this detail was presented as misinformation, the participants heard that the robber gave the police 1 hour (when he had in fact given them 2 hours). If this detail was presented as a neutral item, participants heard that the robber asked for money, but the amount of time was not specified. Half of the items were misled and the other half were neutral. Whether a detail was misled or not was counterbalanced across participants. Following a 25-minute filled retention interval in which participants completed the Reading Span working memory task (Unsworth, Heitz, Schrock, & Engle, 2005) and played Tetris, they took the final test (which was identical to the initial test). Participants were told to answer the questions based only on their memory for the video. Immediately after each question, participants were asked to rate their confidence in their response from 1 (“I guessed”) to 5 (“I am very sure”). After the final test, participants completed a short demographic questionnaire. Results and Discussion Initial Test Responses in the initial and final tests were classified as correct, matching the misinformation presented later (i.e., spontaneous misinformation recall), no response (i.e., a blank response or “I don’t know”), or an other response (i.e., any response that was incorrect but did not match the misinformation). Correct recall probability was similar to previous RES studies (M = 0.64) and spontaneous misinformation recall probability was, as expected, very low (M = 0.07) (see Table 1 for the complete data on the initial test). Final Test We focus our data analysis on the correct and misinformation recall probabilities (see Figure 1). For the sake of completeness, data regarding the “other” and “no response” types are given in Table 2. Correct Recall. A 2 (no test, test) × 2 (post-event information: misled, neutral) ANOVA revealed a significant interaction [F(1, 38) = 4.14, p = 0.05, η2p = 0.10]. Specifically, the tested participants recalled fewer correct details (M = 0.35) than the non-tested participants (M = 0.52) for the misled items [t(38) = 3.35, p = 0.002, d = 0.80] but not for neutral items (t < 1, p = 0.73). That is, initial testing reduced accurate recall when participants were misled – a reversed testing effect. There was also a significant main effect of postevent information, such that participants recalled more correct details for neutral items (M = 0.59) than for misled items (M = 0.44) [F(1, 38) = 10.51, p = 0.002, η2p = 0.22]. The main effect of initial testing was not significant [F(1, 38)= 2.83, p = 0.10, η2p = 0.07]. Table 1. Mean probabilities (and standard deviations) of correct, misinformation, other and no responses on the initial test in Experiments 1 and 2 Correct Misinformation No response Other Experiment 1 0.64 (0.13) 0.07 (0.07) 0.05 (0.06) 0.24 (0.11) Experiment 2 0.58 (0.14) 0.03 (0.04) 0.07 (0.08) 0.32 (0.13) Testing and suggestibility 599 .70 .60 .50 .40 .30 .20 .10 .00 Neutral Misled Neutral Misled Correct Recall Misinformation Recall Figure 1. Experiment 1 final test correct and misinformation recall probabilities. Error bars indicate 95% confidence intervals. |No-Test Test|Col2| |---|---| |Neutral Misled Correct Recall|Neutral Misled Misinformation Recall| Table 2. Mean probabilities (and standard deviations) of no response and other responses on the final test in Experiments 1 and 2 Experiment 1 (narrative) Experiment 2 (questions) No test Test No test Test No response Neutral 0.04 (0.06) 0.05 (0.08) 0.03 (0.08) 0.06 (0.09) Misled 0.01 (0.03) 0.01 (0.04) 0.04 (0.08) 0.04 (0.10) Other responses Neutral 0.31 (0.17) 0.29 (0.18) 0.42 (0.22) 0.35 (0.20) Misled 0.24 (0.14) 0.17 (0.13) 0.24 (0.20) 0.26 (0.22) Misinformation Recall. A 2 (no test, test) × 2 (post-event information) ANOVA revealed a significant interaction [F(1, 38) = 13.72, p = 0.001, η2p = 0.27]. Testing nearly doubled the misinformation recall probability for the misled items [t(38) = 3.35, p = 0.002, d = 1.05], which was an RES effect (M = 0.47 for test and M = 0.24 for no test), but (as expected) not for the neutral items (t < 1, p = 0.44). Not surprisingly, there was a significant misinformation effect overall, with greater misinformation recall probabilities for the misled items (M = 0.35) than for the neutral items (M = 0.07) [F(1, 38) = 70.99, p < 0.001, η2p = 0.65]. The main effect of testing was also significant [F(1, 38) = 6.93, p = 0.01, η2p = 0.15]. During testimony, a witness may be prompted to respond only when they are highly confident. Using the confidence data collected in the final test phase, we examined misinformation recall probability only for responses in which participants were highly confident (a confidence rating of 4 or 5, corresponding to “I am sure” or “I am very sure”, respectively). Remarkably, even among these highly confident responses, the RES effect remained intact (M = 0.48 for tested and M = 0.25 for non-tested) [t(38) = 2.37, p = 0.02, d = 0.77]. To further investigate the effect of initial testing on later suggestibility, we performed a conditional analysis examining final test accuracy depending on whether 600 J. A. LaPaglia and J. C. K. Chan the participant successfully recalled an item on the initial test. Not surprisingly, tested participants who were initially incorrect for a given item were more likely to recall the misinformation on the final test (M = 0.56) than otherwise (M = 0.43 for initially correct items), although the difference was not statistically significant [t(19)= 1.29, p = 0.21, d = 0.37]. However, more surprising is the fact that being able to recall an item correctly during the initial test by no means protected one from the influence of RES, as the misinformation recall probability for these initially correct items was still far greater than that in the no-test condition (M = 0.24) [t(38) = 3.76, p < 0.001, d = 1.19]. Moreover, misinformation had a powerful negative effect on retention of these initially correct items, dropping correct recall probability on the final test from 0.88 (neutral items) to 0.54 (misled items) [t(19) = 3.29, p = 0.003]. EXPERIMENT 2 In Experiment 1, we generalized the RES effect to a new set of materials. Experiment 2 included the same four phases as Experiment 1 (i.e., encoding, no test/test, misinformation introduction, final test), except that the misinformation was embedded in questions instead of a narrative. Method Participants A total of 40 participants (25 male) were tested, with 20 each in the no-test and test conditions, respectively. Their mean age was 19.65 (SD = 2.09). Materials and Procedure The materials and procedure of Experiment 2 were identical to Experiment 1 with the exception of the misinformation phase. During the misinformation phase, participants were presented with 14 questions, each querying a non-critical detail about the video. Seven of the questions included a piece of misinformation (misled items), whereas the remaining questions presented no misleading information (neutral items). Participants were given 30 seconds to answer each question. One question that included misinformation was “In addition to the money, the robber gives the police 1 hour to also get him what?” In this case, like Experiment 1, the misinformation was the time (1 hour) the robber gave the police. Note that participants were always asked about the non-critical details during the misinformation phase. Results and Discussion Initial Test Initial and final test data were coded in the same manner as described in Experiment 1 (see Table 1 for initial test recall probabilities). Not surprisingly, the correct recall probability was similar to Experiment 1 (M = 0.58) and spontaneous misinformation recall probability was again low (M = 0.03). Testing and suggestibility 601 Misleading Questions Phase In Experiment 1, the misinformation was presented in a narrative that did not require any responses from participants. In Experiment 2, the misinformation was introduced in a memory test of the non-critical details. Responses from the misleading questions were coded as either correct, no response, or other responses. Recall probabilities are presented in Table 3. Correct recall probability was marginally lower for the tested participants (M = 0.76) than the non-tested participants (M = 0.82) [t(38) = 1.78, p = 0.08, d = 0.55}. Tested participants had more “I don’t know” and blank responses (M = 0.08) than non-tested participants (M = 0.04); this difference was again only marginally significant [t(38) = 1.88, p = 0.07, d = 0.55]. These data suggest that completing the initial test might have disrupted recall of related information, a finding known as retrieval-induced forgetting (Anderson, 2003). However, we caution against any strong conclusions here because the difference was small and only marginally significant. We consider the possible cause for this pattern more thoroughly in the general discussion. Final Test Correct Recall. Correct and misinformation recall probabilities are presented in Figure 2 (see Table 2 for data in the “other” and “no response” categories). A 2 (test, no test) × 2 (post-event information) ANOVA showed no significant interaction or main effects of testing and post-event information (all F < 2.52, p > 0.12). Table 3. Mean probabilities (and standard deviations) of correct, no response, and other responses on the misleading questions test in Experiment 2 Correct No response Other No test 0.82 (0.07) 0.04 (0.05) 0.14 (0.09) Test 0.76 (0.15) 0.08 (0.09) 0.17 (0.10) .70 .60 .50 .40 .30 .20 .10 .00 Correct Recall Misinformation Recall Figure 2. Experiment 2 final test correct and misinformation recall probabilities. Error bars indicate 95% confidence intervals. 602 J. A. LaPaglia and J. C. K. Chan Misinformation Recall. As expected, a 2 (test, no test) × 2 (post-event information) ANOVA revealed a significant misinformation effect, with participants reporting more misinformation for the misled items (M = 0.20) than for the neutral items (M = 0.02) [F(1, 38) = 27.34, p < 0.001, η2p = 0.42]. Far more interesting, however, is the fact that a testing effect, instead of RES, was observed [F(1, 38) = 12.34, p = 0.001, η2p = 0.25] such that overall misinformation recall probability was higher for the non-tested participants (M = 0.14) than for the tested participants (M = 0.08). Moreover, these main effects were qualified by a marginally significant interaction [F(1, 38) = 3.00, p = 0.09, η2p = 0.07], which is driven by the fact that testing substantially reduced misinformation recall probability for the misled items (M = 0.14 for the test condition and M = 0.26 for the no-test condition) [t(38)= 2.40, p = 0.02, d = 0.78], but (unsurprisingly) not for the neutral items [t(38) = 1.44, p = 0.16]. Like Experiment 1, we examined misinformation recall probabilities for the highly confident responses and again found a protective effect of testing (M = 0.08 for tested participants and M = 0.26 for non-tested participants) [t(37) = 2.41, p = 0.02, d = 0.78]. We also performed a conditional analysis examining final test accuracy depending on whether the participant successfully recalled an item on the initial test. Misinformation recall probability was very low (M = 0.10) for the initially correct items but remained quite high for the initially incorrect items (M = 0.31) [t(18) = 2.36, p = 0.03, d = 0.77]. We again examined the effects of encountering misinformation on retention of the initially correct items. Unlike the result in Experiment 1, presenting misinformation had no effect on final test correct recall probability here (M = 0.93 for neutral and M = 0.87 for misled) [t(19)= 1.39, p = 0.18]. GENERAL DISCUSSION In the present experiments, we sought to discover factors that govern whether retrieval exacerbates or reduces eyewitness suggestibility. We found that testing increased suggestibility when the misinformation was presented in a narrative; however, it protected against suggestibility when the misinformation was embedded in questions. These results help to clarify why, despite the apparently similar methodologies used in prior studies, testing has produced conflicting findings regarding its effects on eyewitness suggestibility. More broadly, the present data provide an important empirical advancement to our knowledge regarding the complex interplay between retrieval practice and suggestibility. In particular, the conditions under which witnesses encounter misinformation can have a profound impact on their suggestibility. Along these lines, prior research has shown that people are far less susceptible to misinformation delivered by a peer than by an authority figure (Ceci, Ross, & Toglia, 1987; Dodd & Bradshaw, 1980; Lampinen & Smith, 1995; Smith & Ellsworth, 1987; Underwood & Pezdek, 1998), and that repeatedly encountering misinformation can increase its likelihood of being reported later (Foster et al., 2012; Mitchell & Zaragoza, 1996; Zaragoza & Mitchell, 1996). As indicated in the introduction, some research has also shown that encountering misinformation from a narrative or a question can influence its effectiveness at altering memory reports, but the results are somewhat mixed. In the present study, a quick comparison between data from the two experiments reveals that presenting misinformation in a narrative or a question had virtually no impact on eyewitness suggestibility when no initial test had taken place. However, far more interestingly, initial testing dramatically altered the relationship between Testing and suggestibility 603 misinformation presentation method and its effectiveness in inducing false recall. Whereas testing exacerbated suggestibility when misinformation was presented in a narrative, it reduced suggestibility when misinformation was presented in a question. How can we account for these results? We hypothesized that testing might reduce suggestibility if misinformation is presented in a question because attempting to answer a question may cause participants to retrieve other related information in memory, and the tested participants would be more likely (than the non-tested subjects) to notice the inconsistency between the misinformation and what they remembered from the witnessed event. Consistent with this hypothesis, the testing effect in Experiment 2 was found only for items that were correctly recalled during the initial test. This finding suggests that conflict detection is driven primarily by people comparing their memory of what was recalled on the initial test (and not just what was seen in the video) with the misinformation. This remains a viable explanation for our findings. In an attempt to keep our experimental design as consistent as possible with former studies, the misinformation was presented in a story-like audio narrative that recapped much of the witnessed event in Experiment 1. By contrast, the misleading questions were presented in a similar fashion to previous studies that demonstrated a testing effect (e.g., Pansky & Tenenboim, 2011). Here, the questions were presented visually, participants were given 30 seconds to answer each question, and the 14 questions appeared without any background or contextual information about the story of the witnessed event. Owing to these differences, we cannot ascertain that the retrieval requirement in the questions was the sole contributor to the reversal of the RES effect. We are currently conducting follow-up experiments to further examine the mechanisms underlying this interesting reversal.[1] An intriguing finding in Experiment 2 was that the tested participants displayed poorer performance on the misleading questions (for which they needed to recall the non-critical details) than the non-tested participants. One interpretation of this finding is that retrieving the critical details during the initial test might have impaired recall of the related, noncritical information (i.e., retrieval-induced forgetting). However, we cautioned against such an interpretation for three reasons. First, as described earlier, the impairment in recall performance for the tested participants relative to the non-tested participants was only marginally significant. Second, our materials were highly coherent and interconnected in nature, and materials like these are highly resistant to retrieval-induced forgetting (Chan, 2009; Chan & LaPaglia, 2011; Migueles & Garcia-Bajos, 2007). Third, and perhaps most intriguing, the decrement in performance for the tested participants might be the result of these participants noticing the inconsistencies between their memory of the video and the misinformation. Specifically, this conflict detection might have drawn attention away from the task at hand – retrieving the non-critical detail that was requested by the question. Such diversion of attention would thus lower performance during the misleading question phase. 1 A potential limitation is that we are making comparisons between studies. That is, the participants in Experiment 1 could differ from those in Experiment 2 because data for the two experiments were not collected simultaneously. However, we believe this is unlikely to have caused the different effects of testing in Experiments 1 and 2, because data for both experiments were collected within a 4-week period. Moreover, all participants were from the same university, participating for course credit, and the mean age of the participants did not differ between the experiments (t < 1, p = 0.69). 604 J. A. LaPaglia and J. C. K. Chan CONCLUDING REMARKS Perhaps due to the popularity and excitement surrounding the idea that testing is a powerful retention enhancer (e.g., Rawson & Dunlosky, 2011; Roediger & Karpicke, 2006), there has been a recent surge of interest in applying retrieval practice to reduce eyewitness suggestibility and to improve eyewitness memory performance in general. In line with this notion, testing has been shown in some studies to be an effective means to reduce suggestibility, and the common explanation here is that testing boosts retention of the witnessed event, which in turn allows witnesses to reject subsequently presented misinformation (e.g., Pansky & Tenenboim, 2011). More surprisingly, however, is the fact that testing can sometimes produce the opposite effect – it can make witnesses far more susceptible to misinformation (e.g., Chan et al., 2009). The stark contrast of these findings raises the question of how to account for this empirical puzzle. In the present experiments, we found that a different misinformation presentation method can be a powerful driving force behind whether retrieval enhances or reduces suggestibility. Although the exact mechanisms that underlie these disparate results remain to be elucidated, we believe the present findings represent an important step toward a fuller understanding of the relationship between retrieval and eyewitness suggestibility. ACKNOWLEDGMENTS These data were collected as part of the dissertation of the first author while she was at Iowa State University. We thank the following research assistants for their help with data collection: Jennifer Dillon, Kelsi Dodd, and Jennifer Piatak. REFERENCES Anderson, M. C. (2003). Rethinking interference theory: Executive control and mechanisms of forgetting. Journal of Memory & Language, 49, 415–445. doi: 10.1016/j.jml.2003.08.006 Carpenter, S. K. (2009). Cue strength as a moderator of the testing effect: The benefits of elaborative retrieval. Journal of Experimental Psychology. Learning, Memory, and Cognition, 35, 1563–1569. doi: 10.1037/a0017021 Ceci, S. J., Ross, D. F., & Toglia, M. P. (1987). Suggestibility of children’s memory: Psycholegal implications. Journal of Experimental Psychology, 116, 38–49. doi: 10.1037/0096-3445.116.1.38 Chan, J. C. K. (2009). When does retrieval induce forgetting and when does it induce facilitation? Implications for retrieval inhibition, testing effect, and text processing. Journal of Memory and Language, 61, 153–170. doi: 10.1016/j.jml2009.04.004 Chan, J. C. K., & Langley, M. (2011). Paradoxical effects of testing: Retrieval enhances both accurate recall and suggestibility in eyewitnesses. Journal of Experimental Psychology. Learning, Memory, and Cognition, 37, 248–255.doi: 10.1037/a0021204 Chan, J. C. K., & LaPaglia, J. A. (2011). The dark side of testing: Repeated retrieval can enhance eyewitness suggestibility. Journal of Experimental Psychology. Applied, 17, 418–432. doi: 10.1037/a0025147 Chan, J. C. K., & LaPaglia, J. A. (2013). Impairing existing declarative memory in humans by disrupting reconsolidation. Proceedings of the National Academy of Sciences of the United States of America, 110, 9309–9313. doi:10.1073/pnas.1218472110. Chan, J. C. K., Thomas, A. K., & Bulevich, J. B. (2009). Recalling a witnessed event increases eyewitness suggestibility: The reversed testing effect. Psychological Science, 20, 66–73. doi: 10.1111/j.14679280.2008.02245.x Chan, J. C. K., Wilford, M. M., & Hughes, K. L. (2012). Testing can increase or reduce suggestibility depending on how memory is tested: The importance of source complexity. Journal of Memory and Language, 67, 78–85. doi: 10.1016/j.jml.2012.02.006 Testing and suggestibility 605 Dodd, D. H., & Bradshaw, J. M. (1980). Leading questions and memory: Pragmatic constraints. Journal of Verbal Learning and Verbal Behavior, 19, 695–704. doi: 10.1016/S0022-5371(80)903795 Foster, J. L., Huthwaite, T., Yesberg, J. A., Garry, M., & Loftus, E. F. (2012). Repetition, not number of sources, increases both susceptibility to misinformation and confidence in the accuracy of eyewitnesses. Acta Psychologia, 139, 320–326. doi: 10.1016/j.actpsy.2011.12.004 Gabbert, F., Hope, L., Fisher, R. P., & Jamieson, K. (2012). Protecting against misleading post-event information with a Self-Administered Cognitive Interview. Applied Cognitive Psychology, 26, 568-575. doi: 10.1002/acp.2828 Gordon, L. T., & Thomas, A. K. (2013). Testing potentiates new learning in the misinformation paradigm. Manuscript submitted for publication. Gobbo, C. (2000). Assessing the effects of misinformation on children’s recall: How and when makes a difference. Applied Cognitive Psychology, 14, 163–182. doi: 10.1002/(SICI)1099-1720(200003/04)14:2<163:: AID-ACP630>3.0.CO;2-H Hardt, O., Einarsson, E. O., & Nader, K. (2010). A bridge over troubled water: Reconsolidation as a link between cognitive and neuroscientific memory research traditions. Annual Review of Psychology, 61, 141–167. doi:10.1146/annurev.psych.093008.100455 Lampinen, J. M., & Smith, V. L. (1995). The incredible (and sometimes incredulous) child witness: Child eyewitnesses’ sensitivity to source credibility cues. Journal of Applied Psychology, 80, 621–627. doi:10.1037/0021-9010.80.5.621 Lane, S. M. (2006). Divided attention during a witnessed event increases eyewitness suggestibility. Applied Cognitive Psychology, 20, 199–212. doi: 10.1002/acp.1177 LaPaglia, J. A., & Chan, J. C. K. (2012). Retrieval does not always enhance suggestibility: Testing can improve witness identification performance. Law and Human Behavior 36, 478-487. doi: 10.1037/h0093931 Lee, Y., & Chen, K. (2012). Post-event information presented in a question form eliminates the misinformation effect. British Journal of Psychology, 104, 119-129. doi: 10.1111/j.2044-8295.2012.02109.x Loftus, E. F. (2005). Planting misinformation in the human mind: A 30-year investigation of the malleability of memory. Learning & Memory, 12, 361–366. doi: 10.1101/lm.94705 Loftus, E. F., Miller, D. G., & Burns, H. J. (1978). Semantic integration of verbal information into a visual memory. Journal of Experimental Psychology: Human Learning and Memory, 4, 19–31. doi: 10.1037/02787393.4.1.19 Memon, A., Zaragoza, M., Clifford, B. R., & Kidd, L. (2010). Inoculation or antidote? the effects of cognitive interview timing on false memory for forcibly fabricated events. Law and Human Behavior, 34, 105–117. doi: 10.1007/s10979-008-9172-6 Migueles, M., & Garcia-Bajos, E. (2007). Selective retrieval and induced forgetting in eyewitness memory. Applied Cognitive Psychology, 21, 1157–1172. doi: 10.1002/acp.1323 Mitchell, K. J., & Zaragoza, M. (1996). Repeated exposure to suggestion and false memory: The role of contextual variability. Journal of Memory and Language, 35, 246–260. doi: 10.1006/jmla.1996.0014 Pansky, A., & Tenenboim, E. (2011). Inoculating against eyewitness suggestibility via interpolated verbatim vs. gist testing. Memory & Cognition, 39, 155–170. doi: 10.3758/s13421-010-0005-8 Pezdek, K., & Roe, C. (1995). The effect of memory strength on suggestibility. Journal of Experimental Child Psychology, 60, 116–128. doi: 10.1006/jecp.1995.1034 Pyc, M. A., & Rawson, K. A. (2009). Testing the retrieval effort hypothesis: Does greater difficulty correctly recalling information lead to higher levels of memory? Journal of Memory and Language, 60, 437–447. doi: 10.1016/j.jml.2009.01.004 Rawson, K. A., & Dunlosky, J. (2011). Optimizing schedules of retrieval practice for durable and efficient learning: How much is enough? Journal of Experimental Psychology. General, 140, 283–302 doi: 10.1037/a0023956. Roediger, H. L., & Karpicke, J. D. (2006). Test-enhanced learning: Taking memory tests improves long term-retention. Psychological Science, 17, 249–255. doi: 10.1111/j.1467-9280.2006.01693.x Saunders, J. (2009). Memory impairment in the weapon focus effect. Memory and Cognition, 37, 326–335. doi: 10.3758/MC.37.3.326 Saunders, J., & MacLeod, M. D. (2002). New evidence on the suggestibility of memory: The role of retrievalinduced forgetting in misinformation effects. Journal of Experimental Psychology. Applied, 8, 127–142. doi: 10.1037//1076-898X.8.2.127 Smith, V. L., & Ellsworth, P. C. (1987). The social psychology of eyewitness accuracy: Misleading questions and communicator expertise. Journal of Applied Psychology, 72(2), 294–300. doi:10.1037/ 0021-9010.72.2.294 Tousignant, J. P., Hall, D., & Loftus, E. F. (1986). Discrepancy detection and vulnerability to misleading postevent information. Memory & Cognition, 14, 329–338. doi: 10.3758/BF03202511 Underwood, J., & Pezdek, K. (1998). Memory suggestibility as an example of the sleeper effect. Psychonomic Bulletin Review, 5(3), 449–453. doi:10.3758/BF03208820 Unsworth, N., Heitz, R. P., Schrock, J. C., & Engle, R. W. (2005). An automated version of the operation span task. Behavior Research Methods, 37, 498–505. doi: 10.3758/BF03192720 Weiser, B. (2012, July). New Jersey Court issues guidance for juries about reliability of eyewitnesses. The New York Times. Retrieved from www.nytimes.com 606 J. A. LaPaglia and J. C. K. Chan Wilford, M. M., Chan, J. C. K., & Tuhn, S. J. (2013). Retrieval enhances eyewitness suggestibility to misinformation in free and cued recall. Journal of Experimental Psychology: Applied, in press. Zaragoza, M. S., & Lane, S. M. (1994). Source misattributions and the suggestibility of eyewitness memory. Journal of Experimental Psychology. Learning, Memory, and Cognition, 20, 934–945. doi: 10.1037/0278-7393.20.4.934 Zaragoza, M. S., Mitchell, K. J. (1996). Repeated exposure to suggestion and the creation of false memories. Psychological Science, 7, 294–300. doi: 10.1111/j.1467-9280.1996.tb00377.x The New, Highly Touted Study On Hormones For Transgender Teens Doesn’t Really Tell Us Much Of Anything Author: Singal, Jesse, Date: 2023-02-07 Collections: IdentityPolitics Zotero Key: DRJ2AHYZ Cite Key: Singal23transTeenHormoneStudy Zotero Item | Lit Note I’m choosing not to put a paywall between readers and my critique of this paper, because I think it’s important that critical perspectives on it from non-hyperpartisan sources be aired far and wide. This is very labor-intensive work, though, and I believe I’m one of the only journalists covering the subject in this manner, so if you find what follows useful, please consider becoming a paid subscriber or giving a paid subscription as a gift. Give a gift subscription For those of you who already pay for my newsletter: Thank you! You’re the reason I’m able to write what I write. If you haven’t read Part 1 yet, you should do that before you read this. As I noted in that post, earlier this month The New England Journal of Medicine published a highly anticipated study called “Psychosocial Functioning in Transgender Youth after 2 Years of Hormones.” The research team has spent years following a cohort of kids who have been administered puberty blockers or hormones at four participating clinics. In this study, they reported on how the kids who went on hormones did over the two-year span following the start of that process. The participants filled out surveys every six months on issues pertaining to their mental health, gender dysphoria, and so on. According to the authors, the kids showed key improvements two years later. “Our results provide a strong scientific basis that gender-affirming care is crucial for the psychological well-being of our patients,” said Robert Garofalo,one of the principal investigators for the study, as well as co-director of the youth gender clinic at Lurie Children’s Hospital in a Chicago, in a press release accompanying the study. A number of media outlets echoed this narrative. But that’s a questionable interpretation of the results for a number of reasons. In my last post, I pointed out something arguably suspicious about the study: In their study protocol, including a version that they submitted into a preregistration database, the researchers hypothesized that members of this cohort would experience improvement on eight measures, including ones that are just about universally recognized by youth gender researchers as important outcomes, such as gender dysphoria, suicidality, and self-harm. Then, in the published NEJM paper, the researchers changed their hypothesis and six of those variables were nowhere to be found. The two remaining — anxiety and depression — moved in a positive direction for trans boys (natal females) but not trans girls (natal males). The researchers reported on three other variables, too, without explaining how they picked them (two improved for trans girls and boys, and one just for trans boys). I won’t rehash the whole post here, but in my view this missing variables issue does call the entire effort into question, simply because if many of the variables the researchers tracked didn’t improve, or even worsened, the fact that they were able to cherry-pick five that did show some improvement might not mean anything at all. We may well be looking at nothing but statistical noise — we just can’t say for sure since the researchers are obscuring so many of their results. For this post, though, let’s temporarily set aside this potentially crippling issue. Let’s imagine, instead, that the authors had preregistered an interest in the five variables they did report all along, and let’s proceed accordingly in our evaluation of their study. For the sake of this post, I’m also not going to quibble with, or even deeply evaluate, the specific statistical techniques the authored employed: As I’ll show, even if we grant that they made the correct decisions here (which may or may not be the case) and take their findings at face value, the results are still ambiguous at best. A tiny bit of political throat-clearing before we start: If you write critically about youth gender medicine, you will hear from a lot of people who are aghast that you could do so given the threats trans people in the United States (adults and children alike) face. And as Dave Weigel noted in a Friday article in Semafor, Donald Trump just publicized a very crazy new proposal to severely curtail both transgender rights for adults and access to youth gender medicine for kids and teens. At the risk of repeating myself, I am opposed to the sorts of policies Trump is proposing — both outright restrictions on youth gender medicine and his even more radical proposal to codify into federal policy a ban on even adults changing their legal sex. That latter part, in particular, is downright cruel and pointless, other than being red meat for the evangelical voters he is hoping to court in 2024. (Trump’s stances on these issues shift, sometimes jarringly, with the political winds. Weigel writes that “he was best known in 2016 for publicly moderating the party’s faith-infused stance on LGBT issues, including opposing North Carolina’s ‘bathroom bill’ and inviting Caitlyn Jenner to use whichever facilities she preferred when on Trump properties.”) But as Weigel noted in his write-up, there’s a pretty noteworthy difference between what Trump is proposing and the more substantive, mainstream debates currently raging over youth medical transition. At the end of the day, given the rapidly increasing popularity of these treatments and the sometimes overconfidentproclamations made about them by their advocates, the questions surrounding youth gender medicine are in urgent need of answers regardless of who the president is or what threats the LGBT community faces. Now is not the time to discuss this is not an argument — it’s a derailing tactic. And it’s one I encountered long before Republicans latched on to this issue, to be honest. If we wait until there are no longer reactionaries trying to profit off of fear of transgender people before we figure out exactly whether and to what extent youth gender medicine works — issues which remain, by all preexisting and widely agreed-upon standards of medical evidence, unresolved — we’ll continue flying blind. And that’s all I’m going to say about that — I am a science writer more than a pundit, and if I pepper every paragraph with parentheticals reiterating that I have the “right” beliefs about Trumpist policies, this will quickly become unreadable. Plus, it doesn’t really matter: What follows is correct or incorrect on its own merits, regardless of the beliefs of the author. **** To be clear, this New England Journal of Medicine study is a significant improvement over what passes for research in the area of youth gender medicine (though that’s a low bar to clear). It’s excellent that researchers are closely following cohorts of kids going on blockers and hormones, and collecting rich data on their mental and physical health trajectories. It’s also useful that this team preregistered its protocol. But the fact is that this particular study really does not provide substantive evidence that hormones improve the mental health of trans kids. This post will be organized around the following main points: 1. The kids in this study had an alarmingly high suicide rate. 2. Most of the improvements the cohort experienced were small. 3. It’s impossible to attribute the improvements observed in this study to hormones rather than other forms of treatment that took place at these clinics. 4. The one bigger improvement was in a variable that might not mean all that much. 5. The researchers don’t even consider the possibility these treatments don’t work — their only answer is “more hormones.” While the authors had other issues with transparency in their study, they do note, right in the abstract, that two participants died by suicide. In the body, they write that “one [suicide occurred] after 6 months of follow-up and the other after 12 months of follow-up.” So within about a year of starting hormones, two of this study’s 315 kids were dead. They also note that there were 11 instances of “suicidal ideation during study visit.” Let’s set aside the ideation issue, because the researchers have not provided us with the information we need to evaluate it. “Suicidal ideation” can mean very different things, ranging from occasional, fleeting thoughts of suicide to, much more seriously, the presence of a plan and the possession of whatever tools are required to carry it out. “I do think it’s fair to say that their use of ‘suicidal ideation’ is ambiguous,” said a suicide researcher whose brain I have sometimes picked on this issue, but to whom I always offer anonymity because they are totally uninvolved in the youth gender medicine fight. The NEJM researchers did administer a suicidal ideation scale to capture this in richer detail, but as we know from the last post, they simply didn’t report that data. So there’s just no way to know whether the 11 incidents of suicidal ideation being reported during visits with the researchers is high or low or somewhere in the middle. As for the rate of completed suicides, a common way to measure and compare suicide rates and other such outcomes is per 100,000 individuals, per year. In the US that figure is 13.9-ish, though of course it can be recalculated every year and varies significantly by subgroup. The closest we can get for an annual estimate for the general population within the age range of the study (12–20) is 14.2 suicides per 100,000 members of the 15–24 age band. As Michael Biggs, a sociology professor at Oxford University and a frequent critic of youth gender research, pointed out to me, this figure was about 317 suicide deaths per 100,000 patient-years in the NEJM study. That’s quite high. We should be cautious here, because per-100,000 rates are weird when the raw number of events is this low: One fewer suicide would have halved the rate, and one more would have increased it by 50%. For what it’s worth, when I asked the suicide researcher, they responded: “I’d say yes. I agree two suicide deaths in that age group for that sample size is high compared to the general population for sure.” But it would be unfair to say “Aha, the kids in your group had a high rate of suicide — your treatment doesn’t work.” These were kids who already had some mental health concerns; gender dysphoria itself can be quite distressing and the LGBT population is known, more broadly, to have elevated rates of mental health issues. So we probably shouldn’t expect kids in this study to have the same suicide rate as age-matched peers from the general population. When I raised this point in an email with the suicide researcher, they said that they agreed that “a comparison to suicide rates in another group with mental health struggles would probably be more appropriate than a general population rate.” Unfortunately, we don’t have much data here. In a letter published in Archives of Sexual Behavior, Biggs had previously calculated that the rate of completed suicide at the Tavistock clinic in England was 13 per 100,000 patient-years, much lower than what was observed in the NEJM study. The only other decently apples-to-apples comparison we have at hand here is also mentioned in his letter: Only one published study has reported suicide fatalities among transgender adolescents. Belgium’s pediatric gender clinic provided counseling to 177 youth aged from 12 to 18 years, who had been referred between 2007 and 2016: five of them (2.8%) committed suicide (Van Cauwenberg et al., 2021). The mean age of referral was 15, implying a mean duration of 3 years before transition to an adult clinic, which translates to an annual suicide rate of 942 per 100,000. This is the highest suicide mortality recorded for any transgender population. So the NEJM sample didn’t have the sky-high suicide rate of that Belgian cohort, but it’s undeniably high. That doesn’t mean the suicides in the NEJM cohort were caused by the hormones. “Of course we can’t attribute those suicides to cross-sex hormones, because we lack a control group,” said Biggs in an email. “Likewise, we can’t attribute the improvement to the cross-sex hormones!” We’ll return to that latter point, but one needn’t make a causal argument here to be concerned. One of the most common justifications for why youth gender medicine is worth it, despite the myriad remaining unknowns, is that kids will kill themselves if they don’t go on it. Well, here was a sample of kids who had access to it in supposedly high-quality settings, with a lot of support and monitoring, and they still had a very high rate of suicide. How does that not raise questions? The researchers have nothing to say about it, other than noting the number of completed suicides and instances of “ideation.” The issue becomes only more worrisome when you look at the study protocol we spent so much time poring over in the last post and see that kids who had severe psychiatric problems, including suicidality, were excluded from the study at the outset: “Presence of serious psychiatric symptoms (e.g., active hallucinations, thought disorder) that would impair the individual’s ability to provide true informed consent or participate in the baseline ACASI [audio computer-assisted self-interviewing]” was an exclusion criterion, as was being “Visibly distraught (e.g., suicidal, homicidal, exhibiting violent behavior) at the time of consent or the baseline ACASI[.]” So kids could have some degree of suicidality and still participate in the study — researchers don’t view suicidality as an on-off binary — but kids who were very suicidal, or otherwise very unwell, were excluded, meaning we actually wouldn’t expect this to be a particularly suicidal cohort going in. And yet, there were still two suicides. That’s not good, and should be seen as a red flag that deserves explanation. We have none, because the researchers fail to report on the cohort’s overall level of suicidality over time, despite it being part of their core hypothesis. And The New England Journal of Medicine, publishing what it knew would be a highly attention-getting study on a hotly controversial issue that is constantly paired with suicide in the public conversation, didn’t ask them to. These were the improvements observed over time, all statistically significant, in the variables the researchers reported, according to their statistical model. Remember that the study covers a two-year span: Appearance congruence: Increase of 0.96 out of 5 points Positive affect: Increase of 1.6 out of 100 points Life satisfaction: Increase of 4.64 out of 100 points Depression: Decrease of 2.54 out of 63 points Anxiety: Decrease of 2.92 out of 100 points These numbers are the average changes for the whole group. For the bolded ones, there were statistically significant changes for both sexes. For the non-bolded ones, trans boys but not girls saw benefits. Appearance congruence and positive affect are the only two variables where the researchers were able to report salutary increases in both sexes over the two years of the study. (Appearance congruence is going to get its own section soon, so I’m going to ignore it here.) Given these differences, it would have been useful for the researchers to lay out plainly what the average changes were for the trans boys versus the trans girls, who were, after all, administered totally different hormones. They don’t do that (nor do they offer any speculation as to why two-thirds of their sample were natally female). If you know how to read Table 3 you can sort of reverse engineer some of this information, but it really should be clearer. The first question you should ask in a situation like this, where you have statistically significant improvements that look small in magnitude, is whether they matter. You can have a statistically significant effect that isn’t clinically significant, meaning it wouldn’t represent noticeable improvement or worsening in the condition being measured. Statisticians argue about effect sizes all the time, but there often isn’t an easy answer as to whether a small one is too small. I do think sometimes, for super-small effects, it’s okay to turn to common sense. Positive affect, as measured by an instrument in the NIH Toolbox, increased by 1.6 points over two years on a 100-point scale. If a researcher touting a treatment for your kid says “This will improve their score on this self-reported scale 1.6% over two years,” you have every right to be skeptical. I really don’t buy the idea that we should care about this or view it as evidence supporting the idea that hormones help kids. (I did check to see if these documents about the NIH Toolbox had any information about interpreting changes to scores in positive affect or life satisfaction, but they don’t appear to.) What about some of the other results that look small, but not quite so tiny? Should we care about those? All we can really do is try to look around in the literature and find other comparisons. One of the meatier papers I found on the subject of clinical versus statistical significance was a meta-analysis from the Cochrane Database of Systematic Reviews on “New generation antidepressants for depression in children and adolescents” (Cochrane is considered one of the best games in town for this sort of careful research evaluation). There, the authors sum up studies that compared these antidepressants to a placebo, and that used something called the Children’s Depression Rating Scale-Revised to evaluate symptoms. The CDRS-R has a range of 17 to 113, meaning it’s a 97-point scale. The authors describe differences as high as 3.51 as “small and unimportant.” So again, a difference that doesn’t appear tiny, per se, might not matter clinically. More directly applicable to the present discussion, here’s a 2015 paper by researchers examining the concept of minimally important clinical difference, or “the smallest difference in score considered clinically worthwhile by the patient,” as it pertains to the Beck Depression Inventory 2 (BDI-II), which is the item the NEJM researchers used to measure depression in their study. The 2015 authors “estimated a MCID of a 17.5% reduction in scores from baseline…. The corresponding estimate for individuals with longer duration depression who had not responded to antidepressants was higher at 32%.” In the NEJM researcher’s statistical model, the patients had a mean baseline score of 15.46 and an average reduction of 2.54. Since 17.5% of 15.46 is about 2.51, if we trust these estimates, the average kid in the NEJM study just ever so barely noticed an improvement in their depression symptoms in their two years on hormones. (I couldn’t find any research on what constitutes a clinically significant improvement on the anxiety instrument the researchers used, the Revised Children's Manifest Anxiety Scale, where the researchers observed an average improvement of 2.92 out of 100 points.) Of course, the average kid had only mild depression symptoms to begin with, which makes things more complicated to analyze. Other patients had higher BDI-II scores at baseline, which at least arguably means they’d only view reductions as worthwhile if they were significantly larger. This is a useful chart from the Supplementary Appendix: I don’t know what to make of this. The researchers tout the fact that a lot of kids moved to lower levels of depression and anxiety over the course of the study, but also acknowledge that a fair number of them remained in the clinical range at follow-up. That’s clearly true. This is a genuinely complicated situation to interpret, partially because the baseline numbers are such a mixed bag: It’s not fair to harp on the lack of improvement in a kid who wasn’t doing that poorly to begin with. This is an issue in some youth gender medicine clinical research, which typically involves cohorts who have been screened for serious mental health problems beforehand: They don’t have much room to improve, so the deck is somewhat statistically stacked against researchers seeking to demonstrate mental health improvements. (I guess on the other hand, it’s also useful information that the kids in this cohort didn’t seem to get significantly worse, for the most part.) It would have been helpful if the researchers provided more fine-grained information about, say, the average numerical improvement among kids in the moderate or severe ranges on the depression measure. As we’ve seen previously, reporting instead on the percentage of participants in different clinical categories can obscure a lot of useful information: In the most extreme cases, a one-point drop the patient doesn’t even notice can bring them from (say) “moderate” to “mild.” It would also be useful to know if the relatively large proportion of kids who didn’t provide data at 24 months, which is about a third of them, differed at other time points from the rest of the group, because if the kids with missing 24-month data were on average doing better or worse than their peers in the study, that could seriously skew the results. (I could be missing something, but I think the researchers make this comparison only for the very small number of true dropouts, rather than for kids who technically stayed in the study but didn’t provide data on some items at the final observation.) At the end of the day, it seems hard to deny that a lot of kids stayed unwell despite two years of regular access to a gender clinic and to a medication designed to improve their mental health outcomes. At baseline, 18.6% had moderate depression, and two full years later 10% still did. For severe depression, there was even less improvement: 15.6% to 13.7%. Same general story with anxiety, where 58.8% of the kids scored in the clinical range at baseline and 47.7% did at follow-up. On the other hand, it’s hard to ignore that clearly some kids did experience meaningful reductions in depression and/or anxiety and/or other symptoms. Doesn’t that, at least, constitute some evidence for the efficacy of hormones? Unfortunately… This is a longitudinal study without a comparison group. As any graduate of an AP Statistics course will tell you, this makes it much harder to claim that any particular influence is responsible for any changes that are observed over time. If you track two otherwise similar groups over two years, and you give one of them medicine and the other a placebo, and you observe differences between the groups, you might then be able to begin to make some reasonably confident causal inferences about the effects of the medicine, though how confident depends on a host of factors. But with only one group, this is a notorious statistical problem, even in relatively simple situations. If I give you flu medication, and your flu symptoms have improved significantly five days later, does that mean the meds worked? Maybe. But people also tend to get better over time. Scores on certain scales tend to revert to the mean if the first observation is quite high or very low. And so on. Again, these are really basic statistical principles — this isn’t nitpicking. This NEJM study is far more complicated than a two-week flu study, though. And we actually have some pretty good reasons to suspect other factors may have contributed to the (mostly small) improvements observed by the researchers. As they note early in their paper, “All participating clinics employ a multidisciplinary team that includes medical and mental health providers and that collaboratively determines whether gender dysphoria is present and whether gender-affirming medical care is appropriate. For minors, parental consent is required to initiate medical treatment. Publications by individual study teams provide details on site-specific approaches to care.” That last sentence cites four papers, and if you read those papers, you’ll see some mentions of therapy and medication for patients experiencing mental health duress above and beyond their gender issues. To take one example, the team at the Gender Identity & Sex Development Program at Lurie Children’s Hospital in Chicago writes that “In cases when both evidence-based therapy and psychopharmacotherapy are indicated, a psychologist and the psychiatrist may comprise a patient’s treatment team — the psychologist serving as a primary therapist and the psychiatrist offering psychotropic medication management.” Every kid in this study was seen at one of these multidisciplinary clinics. So it stands to reason that the ones with serious anxiety and depression issues were likely provided with access to psychotherapy, medication, or both. It also stands to reason that the worse a kid’s mental health symptoms were at baseline, the more likely they were to have been provided with one of these interventions, and the more room they had to improve. This means that even when it comes to the subset of kids whose improvements were sizable, we have to ask: Did their symptoms abate because of the hormones, the medication, or the therapy? Or was it some combination of all three? There’s no way to know. This makes it effectively impossible to interpret these results fully. And it’s unfortunate, because I believe the researchers could have potentially accounted for this in their statistical models since surely they had access to patient records. One of the questions I sent them was on this very subject, but as I mentioned in my last post, they aren’t doing any interviews or responding to queries. Despite all this, the researchers confidently proclaim in their abstract that “[Gender-affirming hormones] improved appearance congruence and psychosocial functioning.” This is straightforwardly causal language, but their methodology doesn’t come close to warranting it. Moreover, they don’t even mention this potential confound, which, again, really is the sort of thing you would learn during the first year of a college-level introductory stats course. Imagine encountering this question on the midterm of such a course: A group of kids with mental health problems gets access to psychological counseling, psychiatric medication, and Treatment X for two years. At the end of this period they are doing significantly better. Is this because of a) the counseling b) the medication c) Treatment X d) without more information, it’s impossible to know One hundred out of one hundred stats professors would tell you the correct answer is (d). (An accompanying NEJM commentary on the paper by Annelou de Vries and Sabine Hannema mentions the therapy issue but not the medication issue.) There’s one other potential problem with pinning the improvements noticed in this study on the powers of gender-affirming hormones, per se. It was pointed out to me by a depression researcher with whom I exchanged some emails about this paper. He initially said I could quote him by name but changed his tune, saying the subject was too fraught. When I asked if it would be okay to describe him as “a depression researcher,” he responded, “Sure, I’d even be ok with ‘a depression researcher who’s too chickenshit to be named…’😅.” (I include this because I thought it was funny, not because I think he’s chickenshit! I don’t blame him at all.) He pointed out that the researchers “utterly ignore the obvious point that testosterone has large mood-elevating, anti-anxiety, and antidepressant effects!” I’d heard others raise this point in the past, and it poses another genuine challenge for youth gender medicine researchers. There is indeed some evidence that testosterone has mood-improving effects, and this raises the possibility that it can cause improvements in mental-health symptoms that don’t have to do with treating gender dysphoria per se. (I should be clear that the studies I’m referencing and linking to generally involve natal males rather than natal females, adults rather than juveniles, and sometimes cover periods of time significantly shorter than two years. So we can’t state definitively that T has similar effects on young natal females than it does on older natal males, but it’s definitely a possibility, and there’s some anecdotal evidence of this among trans boys and men.) Let’s again be maximally generous and set aside this possibility. Summing up the rest of this section, even if we put a thumb on the scale and zoom in only on the kids who got a lot better in this study, ignoring the small average effect sizes, the suicides, the truly disappointing results for trans girls, and what appears to be a veritable carnival of variable cherry-picking, we simply have no way of knowing — full stop — whether they improved because of hormones, therapy, medication, or a combination of the three. I am not good at stats, but I don’t need to be to make this point. It is not esoteric. This is a very basic issue, well known in the social sciences. It seriously undermines our ability to determine whether the kids in this study benefited from going on hormones, and it reflects poorly on The New England Journal of Medicine’s decision to allow the researchers to use such strong, straightforward causal language to describe their results. So far we’ve been arguably talking about peanuts, as far as average effect sizes are concerned. The authors do highlight one more impressive-seeming finding, though: Increasing appearance congruence is a primary goal of GAH, and we observed appearance congruence improve over 2 years of treatment. This was a moderate effect, and the strongest effect observed across our outcomes, consistent with the effect seen in research involving other samples, which has noted large effects of GAH on body image and small-to-moderate effects on mental health. Appearance congruence was also associated with each psychosocial outcome assessed at baseline and during the follow-up period, such that increases in appearance congruence were associated with decreases in depression and anxiety symptoms and increases in positive affect and life satisfaction. These findings suggest that appearance congruence is a candidate mechanism by which [gender-affirming hormones]influences psychosocial functioning. [footnote omitted] Specifically, over two years the kids in the study experienced about a one-point improvement on the five-point appearance congruence scale of the Transgender Congruence Scale. The TCS is a 10- or 12-item instrument with the following items, as summarized in a table from a 2021 study in Sexuality Research and Social Policy, which was the most recent one I could find that had been conducted on its psychometric properties: This past fall I wrote a piece about the problem of researchers in this field touting impressive-seeming findings by relying on instruments that might not mean much. In that case, I discussed adolescent top surgery research where the headline finding was that kids’ scores “improved,” after receiving double mastectomies, on scales that seemed to consist mostly of items asking them whether they presently had breasts. That is, if you ask someone with gender dysphoria to rate their agreement with “I worry that people are looking at my chest” — anactualitem from the scale in question — before and after they have surgery, it would be shocking if their score didn’t improve. But that obviously doesn’t tell us much about whether double mastectomies “work” in the longer-term sense we would want a major surgery to work. The paper in question didn’t include more substantive items on the kids’ mental health. I think there’s a version of that going on here. To be fair, the authors of the NEJM paper also report on some more common and better-validated measures, including ones addressing anxiety and depression, but as we’ve seen, those changes were small, of questionable clinical importance, and didn’t apply to the male-to-female transitioners. Improvements in appearance congruence were experienced by both sexes, and it is the only decent-sized effect the researchers uncovered, as they themselves note. But there’s a case to be made that the game is somewhat rigged here. Take items like “My outward appearance represented my gender identity” and “My physical appearance adequately expressed my gender identity.” For one thing, these items are so similar that I’m surprised they’re both on the scale — they seem truly redundant, and it seems almost impossible that if one goes up or down, the other isn’t going to follow right along with it, which might artificially inflate observed changes in respondents’ scores. (In theory, when a scale is first validated, someone checks for these sorts of issues, and I’m not going to claim to have looked deeply into that process here. Though it’s worth noting that in the 2021 paper, the authors note one item on the appearance congruence subscale was removed due to its high covariance with others.) But more importantly, it seems almost impossible to imagine how someone’s scores on an item like this wouldn’t “improve” as the physical changes of hormones took hold and brought their body in line with who they felt they were on the inside. I mean, I don’t want to discount this finding entirely — it would certainly be bad news if it didn’t improve, because it might suggest their gender identity or transition goals had changed mid-treatment (which wouldn’t be ideal) — but I’m just not sure how impressed we should be by this. That’s doubly true when you notice that the appearance congruence scale doesn’t appear to really correlate with other, more robust measures of well-being, anyway. Or at least that’s what the authors of the 2021 paper found: Don’t worry if you can’t interpret this. The point is that this measure only inconsistently correlates with other, better-established ones. Without getting too into the weeds, the authors do note that this conflicts with previous research into the TCS and its subscales that found it did correlate, pretty strongly, with other items. But it seems like an open question whether changes in the TCS or its subscales matter all that much, clinically — a question worth investigating further, but an unanswered one. If this is the biggest improvement you note in your big NEJM study, you should be asking yourself some questions. The researchers argue that they did some other statistical work supporting the idea that appearance congruence might be particularly important. They include this fancy-looking visual: One of my go-to folks for stats stuff that is above my head is Stuart Ritchie, a research psychologist and the author of the great book Science Fictions: How Fraud, Bias, Negligence, and Hype Undermine the Search for Truth, as well as his own Substack newsletter of the same name that was recently acquired by the UK’s i newspaper. I sent him this chart and the authors’ claims about their latent growth curve modeling and what it showed about the importance of appearance congruence. As luck would have it, Ritchie had published research employing similar statistical techniques. “To me it makes sense that the appearance congruence would change alongside the other mental health variables - but that doesn’t say anything about whether changing appearance congruence would change mental health in a causal sense,” Ritchie wrote in an email. “The causality could easily have just gone the other way (people who felt better, mentally speaking, tended to worry less about their appearance).” Ritchie summed up this part of the NEJM paper as “a fancy way of messing around with correlations, nothing causal, kind of interesting but not a clincher of any argument.” The anonymous depression researcher made the exact same point, independently, in an email. “The latent growth curve models are not terribly persuasive,” he wrote. “Sure, they suggest that changes in appearance congruence are correlated with changes in some other psych variables, but there’s no proof of causality there, and we certainly know that self-reported appearance measures are heavily mood-state-dependent – so an increase in depression, say, would almost certainly cause a decrease in appearance congruence.” Ritchie and the depression researcher also both independently noted the high number of statistically insignificant links (the dashed lines) in the above diagram. There doesn’t appear to be a lot there. There’s a subtly revealing part of this paper where the researchers attempt to grapple with the fact that the natal males in the study hardly showed any improvements during their first two years on hormones: Given that some key estrogen-mediated phenotypic changes can take between 2 and 5 years to reach their maximum effect (e.g., breast growth), we speculate that a longer follow-up period may be necessary to see an effect on depression, anxiety, and life satisfaction. Furthermore, changes that are associated with an endogenous testosterone-mediated puberty (e.g., deeper voice) may be more pronounced and observable than those associated with an endogenous estrogen-mediated puberty. Thus, we hypothesize that observed differences in depression, anxiety, and life satisfaction among youth designated female at birth as compared with those designated male at birth may be related to differential experiences of gender minority stress, which could arise from differences in societal acceptance of transfeminine (i.e., persons designated male at birth who identity [sic] along the feminine spectrum) as compared with transmasculine persons. Indeed, gender minority stress is consistently associated with more negative mental health outcomes, and research suggests that transfeminine youth may experience more minority stress than transmasculine youth. [citations omitted] Two things on this: First, I don’t think this really jibes with the fact that the improvements in appearance congruence were statistically equal between natal males and females. Of course appearance congruence is only one aspect of a successful transition, but you would think that if all these other obstacles were getting in the way of trans girls feeling better after two years on hormones, it would show up in the variable that most closely tracks the physical effects of those hormones. More importantly, scienceis supposed to be open-minded. If you’re evaluating a new treatment, you’re supposed to attend to the possibility that it doesn’t work as intended. It could be everything the authors are saying in this excerpt is true, but it’s impossible to ignore the chain of events here: They put a cohort of kids on hormones, two of them died from suicide, and the natal males appear to have experienced no measurable improvements other than a truly tiny one on a positive affect scale and a questionably important one on a rather tautological appearance congruence scale. Their response is to say… maybe the kids just need to be on hormones longer. There isn’t even a moment of pause or reflection or uncertainty. It’s full steam ahead. The fact is that some members of this team aren’t just clinicians and researchers — they’re also steadfast advocates for these treatments. They strongly believe that these treatments help trans kids, and they benefit materially from administering them and from participating in the public debate about them. This situation, in which strong advocates with clearly stated preexisting views are producing what is supposed to be top-tier evidence (it’s The New England Journal of Medicine, after all), shouldn’t concern us a little? It’s probably inevitable that in some cases advocates for a treatment are also going to research that treatment. But we should at least acknowledge the potential pitfalls here. When Cochrane or the UK’s National Institute for Health and Care Excellence (NICE) publish scientific evidence, there’s the expectation that the bureaucrat-nerds responsible for producing it are entering the process without strong priors, and are reasonably invested in examining the evidence in a fair and evenhanded manner. In fact, part of the reason NICE’s damning reviews of the evidence for giving puberty blockers to kids and hormones to adolescents marked a major turning point in this discussion is because it is such a trusted institution. If Cochrane published an evidence review showing that one particular antidepressant outshined others by a wide margin, and it was subsequently revealed that a coauthor on that review had a husband who worked for the pharmaceutical company that produced the antidepressant in question — and that this relationship hadn’t been disclosed — that would immediately cast a dark shadow across the article. It would be considered a conflict of interest. Why should none of that logic apply here? A single NEJM article isn’t the same as a comprehensive evidence review, sure, but why should we completely ignore the reality, which is that since humans are humans, a team of researchers deeply invested in a treatment might be less capable of carefully, dispassionately evaluating the evidence for it? Maybe there’s a reason why none of that logic should apply here at all. But if there is, I can’t come up with it. Especially in the continuing absence of any sorts of systematic reviews of youth gender medicine in the US — an important point Moti Gorin made here — each study on this subject is going to take on an outsized importance. Which is all the more reason to demand that the researchers behind these studies adhere to the highest standards of rigor and transparency. I don’t think that happened here, and it isn’t the first time strong advocates for youth gender transition have produced questionable research about it. As I noted in Part 1, in this case it might not be fair to lay all the question marks at the feet of the researchers themselves; some of them may be the result of the NEJM’s editorial input. But still: We’re now many years and many hundreds of thousands of dollars into this research effort. The researchers have published what was supposed to be one of their blockbuster studies, and it is missing absolutely crucial evidence we need in order to evaluate these treatments. All these years later, we don’t even know whether the cohort experienced reduced suicidality or gender dysphoria. What we do know is not encouraging: a high suicide rate, tiny-to-small improvements except on a questionable measure, no improvements on most measures for the trans girls. Again: It could be that if we properly adjusted for all the missing, unreported variables, the researchers didn’t really find anything at all. It’s frustrating that we’re still in a position where that’s a live possibility — more of these questions should have been answered by now. Questions? Comments? Unfounded claims? I’m at singalminded@gmail.com or on Twitter at @jessesingal. A Strategy for Factory Towns Author: Lux, Mike, Date: 2023-02-22T08:00:00.000Z Collections: Hot Takes US Elect 2024 Zotero Key: QY6WNSTZ Cite Key: Lux23strategyFactoryTowns Zotero Item | Lit Note EXECUTIVE SUMMARY Hard times, effective right-wing messaging, the demise of local news, and sometimes the Democratic Party itself have led to big changes in the voting and opinions of people living in small and midsized towns that have been most impacted by deindustrialization and increased Big Business power in the economy. But these Factory Towns voters are not lost causes to the Democratic Party, and we cannot afford to write them off. They comprise 48% of the voters in Pennsylvania and the Midwest, and if we continue to lose ground with them, the entire region will become more and more like Iowa and Missouri – tough states for the foreseeable future. However, if these counties start to move back toward the Democrats, that kind of progress could be the linchpin to building sustained Democratic majorities that can usher our country into a more progressive future. This report is part of a continuing effort by American Family Voices to do on-the-ground research and data analysis to understand the thinking and motivation of working-class voters, and to recommend strategies that can begin to rebuild the Democratic Party’s and progressive movement’s historic connection to America’s working class. The project focuses on voters in “Factory Town” counties in six key states: Iowa, Michigan, Minnesota, Ohio, Pennsylvania, and Wisconsin. These states were Ground Zero in 2016, breaking down the “Blue Wall” critical to Democratic victories. Joe Biden did just enough better in 2020 to help win back Michigan, Pennsylvania, and Wisconsin, but these communities in all six states remain very tough for Democrats and will be among the most highly competitive counties for 2024. Despite the challenges, this is a moment where Democrats have an opportunity to make more gains. Biden and the Democratic Congress have passed substantial legislation that can bring progressive change, all the way down to the community level, over the next two years. The president’s policies, background, and genuine affinity for these working-class communities make him an ideal leader for this effort. This report combines data from our most recent polling, Facebook and digital analytics, and comparisons of county-by-county elections results in 2022 to the past decade of state election results. The report closes with recommendations on how Democrats and progressive issue advocates should move forward with Factory Towns voters and counties. Here is the bottom line in our findings: 1. The presidential horse race numbers are very competitive in these counties, but Republicans are stronger in terms of the economic frame. 2. Voters have negative opinions of both parties: this presents both challenges and opportunities for Democrats. Voters in these counties tend to think Democrats lack an economic plan, but they see the GOP as the party of wealthy corporations and CEOs. 3. Populist economics and the Democratic economic policy agenda play very well in these counties. These voters respond best to an agenda focused on kitchen-table economic issues. 4. Contrary to conventional wisdom, populist economic messaging works much better than cultural war messaging. Our strongest Democratic message on the economy beats the Republican culture war message easily. The Republican economic message is a bigger threat to us. 5. Community building needs to be at the heart of our organizing strategy. 6. I recommend that Democrats and progressives make major investments in local field organizing and door-to-door, special events that build community, online community building, existing local media and progressive media targeted to these counties, and progressive organizations that make sure voters know how to benefit directly from the Biden policy initiatives of the last two years. FactoryTowns.POLL.2023.01.27 .pdf Download PDF • 2.85MB INTRODUCTION American Family Voices (AFV) started the Factory Towns Project in 2021. We identify “Factory Towns” as the working-class manufacturing counties hardest hit by deindustrialization outside of the biggest metro areas. Democrats saw the bottom drop out of our margins and percentage in Factory Towns between 2012 and 2016. Biden performed a little bit better in these counties and that helped him win in Pennsylvania, Michigan, and Wisconsin. And Democrats made enough progress in these counties to help us win key statewide races in 2022 and many of the contested House races in Pennsylvania and the Midwest. But these counties remain very tough for us, and are among the most highly competitive counties for 2024. The midsized Factory Towns in our research went for Obama by a net vote margin of over 100,000 votes. Four years later we lost them by about 700,000 votes. (The smaller manufacturing-oriented counties in our research fell off worse than that – our net margin in 2016 was about 1.9 million worse than 2021.) But after Biden made a modest improvement in 2020, they are about even right now in the polling, which is good news for us. And there are lots of swing voters here: according to our TargetSmart analysis last year, probably about 60% of that change in vote between 2012 and 2016 was due to people switching their votes from D to R, the rest split between Trump surge voters and Democrats who did not turn out. Since 2021, AFV has done demographic economic and voter file analysis and generated conversations with more than 500 local leaders. We conducted a baseline poll early in 2022 followed by focus groups in the spring and another poll in December to see whether voters’ perspectives were changing in these communities. Last fall, we experimented with message testing on digital platforms and community building on Facebook, learning a lot about what worked with these voters, and we organized a Factory Towns tour of these communities with the Rick Smith Show, a radio program focused on labor issues. FAVORABILITY RATINGS Trump is now 39-56 in these Factory Town counties where he previously swamped us twice. Most of those unfavorables are strongly unfavorable, so he is in bad shape at the moment. Kevin McCarthy is the least well-known of the politicians we asked about, with almost a third of voters having no impression of him yet (either no opinion or never heard of him at all), but for those who know him, he is already deeply disliked: down 29-41, with 29% of voters strongly unfavorable towards him. The more voters hear about him, the better for Democrats. DeSantis is surprisingly well known, with only a fifth of voters having no impression of him (21% no opinion/never heard). And he is a threat, as he is the only one of the four politicians we asked about who has a net-positive rating, 42-37 – although his margin of support is entirely soft. His strongly favorable vs strongly unfavorable ratings are even at 28-28. Given that these are counties we lost pretty badly in 2020, Biden is in decent shape, only underwater in terms of favorability by six points, 46-52. The bad news is that intensity is much higher on the negative side. Among voters with strong opinions, we are down a lot: 24-45 (24% of voters are strongly favorable, while 45% are strongly unfavorable). One other thing that is important to note here: in spite of Biden’s relatively decent approval numbers, and Trump’s abysmal ones, the horse race numbers are still basically a dead heat. A considerable number of voters here will have at least an initial tendency to vote for Trump even if they don’t like him all that well. STATE BY STATE Congressional Generic Ballot Combined Biden/generic congressional polling state by state. The old Factory Towns in Michigan tend to have remained more Democratic than in other parts of the Midwest, and we are strongest there in terms of horse race numbers (the generic Congressional ballot generic is at +8 for Democrats), favorability ratings (Biden is +6), and how people are responding to the strong economic messaging from Michigan Democrats (message ratings in Michigan are consistently higher than in other states, in several by 20+ points). In spite of the barrage of attack ads on Biden and inflation last year, our Factory Town numbers in Wisconsin have held up pretty well: Biden is only two points down (47-49) in his favorability rating. Interestingly, Trump’s worst state on favorability in these counties is Wisconsin, where he is down by 27 points. And Democrats are at +4 in Wisconsin on the generic Congressional ballot. In the other states, Biden is more underwater in these Factory Towns: -11 in Pennsylvania, -12 in Ohio, and -20 in the combined Iowa/Minnesota numbers. (We did have a small sample size for Iowa and Minnesota, so some of the variation may be due to that.) The generic Congressional ballot is closer in PA (-2) and OH (-5), but awful in IA/MN: -17. There are, however, solid gains on the generic Congressional ballot in IA/MN after voters hear our economic messaging, so in the closely competitive House and state legislative races in those states, our economic message could help us win a couple of seats. 2022 Election Results Compared with Past Elections Michigan. Obama routed Romney in the midsized manufacturing counties, taking over 55% of the vote, while Clinton was beaten in these counties, getting only 48%. Biden was able to add a little more than a point to Clinton’s total, but Whitmer in both of her elections reached 54%, close to the Obama numbers. In the small manufacturing counties, Clinton suffered an even steeper drop: Obama won almost 44% of the vote, while she won only about 35%. Biden got about 37%, while Whitmer had nearly 42% both times. Pennsylvania. The midsized Factory Town counties hovered around 50% D/R in 2012 and 2014 statewide races, but took a deep dive in the 2016 presidential race – Clinton got about 43% of the vote. Biden added two points in 2020, getting to 45%, and Fetterman added two more, getting to 47%. Shapiro went over 52%, which was even better (by a point) than Casey’s numbers in 2018. In the small Factory Town counties, Obama 2012 was at 38.5%, and Hillary did more than seven points worse, slightly over 31%. Biden was able to add a point and a half to that, ending at almost 33%, while Fetterman got almost 36%. Wisconsin. Obama 2012 won about 52% of the vote in the midsized counties, while Clinton won just over 46%. Biden gained only a point in 2020, getting to 47%. Governor Tony Evers was at about 48% both times, but Senate candidate Mandela Barnes performed slightly worse than Clinton, just below 46%. In the small manufacturing counties in Wisconsin, Obama won almost 50%, but Clinton crashed here, barely getting above 40%. Biden was almost exactly the same (40.2 vs 40.3), the one state where he didn’t do better than Hillary by at least a point or two. Evers got a little above 43% in 2018, and a little below 42% in 2022. Barnes only won 30.5% in these counties. Tammy Baldwin was an outlier: close to 50% here both in 2012 and 2018, showing her strength as a candidate. General observations in the Factory Town counties in other states in 2022. • Iowa should be a warning sign for us all about the Factory Towns: if we don’t have a serious organizing strategy for these counties, they are going to keep getting worse and worse. Iowa has been generally moving away from us for a decade now, but even so, in 2018 there were solid candidates who targeted small and midsized counties. We won three of four congressional races, and almost won the most rural district in the state and the governor's race. The last four years have seen very little organizing work outside of the Des Moines metro area, and the results are no Democratic congressional seats, and even popular, longtime statewide officials who held their offices for four decades (Tom Miller and Mike Fitzgerald) lost.* • The Tim Ryan race in Ohio is a reminder that targeting only the swing, working-class voters in the Factory Towns is not a path to victory, unless it is combined with a strategy for energizing and turning out the Democratic base vote. For one thing, there are still quite a few base-vote Democrats who live in Factory Towns, and you can’t improve the margins there to the maximum extent without also engaging and turning out those voters. More importantly, if you look at 2022 turnout for Democrats in big cities and among young voters, big city voters, and people of color, it was pretty weak. Ryan did relatively well in the Factory Town counties, holding his own or doing better compared to recent Democratic performances, but his margins in the big Democratic counties were far smaller than they needed to be. The formula for victory has to be to do both. • It is no surprise that Gov. Walz in his re-election campaign did pretty well in the Factory Town counties given where he is from and his positioning. Strong progressive AG Keith Ellison survived a weaker performance than in 2018 in the Twin Cities’ suburbs by paying a lot of attention to Factory Town counties in the Iron Range and in Southern Minnesota. His populist message, willingness to take on big corporate interests including Big Ag and Big Oil, and focus on voter registration and GOTV of immigrant communities not only in the Twin Cities, but in the Factory Towns paid off in a big way. His victory is another model for winning through targeting Factory Towns.* • Besides the aforementioned disappointments in Iowa, we did pretty well in the congressional races with a Factory Town demographic. We won all three of the competitive Factory Town dominant races in Pennsylvania, three competitive seats in Ohio (two of which were pure Factory Town districts and one of which was a combination of Cincinnati and some Factory Town-style turf outside the city), three of four races in Michigan, and two Mississippi River districts in Illinois. The other loss in the region was Ron Kind’s open seat in Wisconsin.* • Another huge area where we had some exciting wins in this region were state legislative chambers. We flipped the Minnesota Senate, both the House and Senate in Michigan, and the Pennsylvania House. These were the Democrats’ only chamber flips in the country. The fact that the Factory Towns region had four of them is a sign that we are making progress in these states.* SPECIFIC ISSUES Economically populist issues dominate what’s most important to these voters to address with policy, and in my later section on practical populism, I will discuss them more. Some other issues worth noting: • The number one economic policy proposal both in terms of overall support (93%) and strong support (81%) is protecting Social Security and Medicare. Expanding those programs is also very popular, but less so than just protecting them.* • Investing in apprenticeships and job training was the second highest rated economic policy proposal, with 77% strong support and 92% support overall.* • Investing in state and local infrastructure is also incredibly popular, with 92% overall support and 66% strong support.* • Investing in home-based care options for the elderly and disabled has 88% overall support and 64% strong support.* • Raising the salary threshold on overtime has 81% overall and 61% strong support.* • Most of the Democratic policy agenda has fairly strong support, even items that do not top the list, including more financial aid for college, free community college, guaranteed paid family and medical leave, and making the Child Tax credit permanent. All are in the 60s in terms of overall support, and in the 40s in terms of strong support.* When we tested Facebook ads, the messages that got the strongest positive responses were ads attacking oil company price gouging, affirming the value of unions, and talking about the importance of helping small businesses. These tested well among swing voters and even some moderately conservative audiences. The union ad, for example, tested very well among the working-class conservative audiences. The good news is that we haven’t lost much ground in the horse race numbers over the last year, and a populist Democratic economic message is pretty close to even with a Republican economic message. The bad news is that right now, our poll shows Democrats are slipping in the battle over the economic narrative. The voters in these counties have a natural proclivity to economic populism, and that is still definitely true, but there have been recent shifts that raise warning signs for 2024. Compared to the original baseline poll we did in these counties a year ago, the horse race numbers are about the same, very close to even. Given how badly we lost these counties in 2016 and 2020, even would actually be quite good. The problem is that voters in these counties, including swing voters, independents, and even some Democrats, are viewing the economy along a much more Republican narrative than they were a year ago. Given how dominant economic issues are for these voters, this is going to become a much bigger problem if we don’t do something soon to turn it around. A year ago, these voters were indicating that corporate greed and corporate CEO decision making was the dominant reason for economic hardship in their communities. Now they are saying that big spending and government waste is the number one problem. It’s not that these folks can’t be moved by populist messaging, but the progressive populist narrative is no longer top of mind for them. All those TV ads blaming Biden for inflation have clearly had an impact, and we can’t let that GOP narrative stand. DEMOCRATIC BRAND CHALLENGES There is no doubt that we have some big challenges to overcome with Factory Town voters. I am a firm believer that with the strategies we have been talking about, we can continue to make steady progress in these counties, but we will need a concerted, strategic effort to achieve that progress. All of the problems outlined in this section are related – and they reinforce each other in a negative feedback loop. That is why it is going to take a more comprehensive approach to solving the Democrats’ poor performance in Factory Towns. In a regression analysis isolating the effect of voters’ perceptions of negative traits of the parties on their Congressional vote, holding demographic variables and party identification constant, negative traits that most affected the vote in isolation were things like “don’t understand my life,” “don’t share my values,” “work for the elite, not the people,” “have no real economic plan,” and “too extreme.” Both parties run into some trouble on these topics. Troublingly, Factory Towns voters think Democrats don’t have an economic plan and don’t understand their lives. Republicans also have significant economic weaknesses, including that voters, particularly independents, perceive them, more so than Democrats, as favoring the elite and wealthy corporations over people. The biggest barrier to Democrats and progressive groups making gains in the Factory Towns is the intense cynicism of these voters. As I noted above, they don’t like or trust either party, or the media, or the government. Political campaign speeches and TV ads by themselves aren’t going to create much movement. The second problem is many decades of Republican messaging about tax-and-spend Democrats and lazy people getting handouts. This old right-wing line still packs a punch. The third big problem is the “extremism” thing. Swing voters do have the impression that Democrats (as well as Republicans) are too extreme, though they don’t necessarily identify any particular issue, or even a set of issues, as problematic. It seems more impressionistic than anything else. A fourth challenge is that most people in these counties believe their neighbors are more Republican than they are. Peer pressure can be a problem, so we need to figure out on-the-ground strategies to lessen this dynamic. Finally, one big negative that comes up in polling and focus groups in these working-class non-metro counties is that they see Democrats as weak and ineffective, especially when it comes to economics. And they don’t believe Democrats have an economic plan, at least not one focused on their lives. Democrats across the region, though, are proving that these challenges can be overcome. Candidates such as Sherrod Brown, Gretchen Whitmer, John Fetterman, Bob Casey, Josh Shapiro, Tony Evers, and Tammy Baldwin have all proven you can do well enough in these counties to score solid statewide wins in purple or even lean-red states. Joe Biden showed enough improvement in these counties over Hillary Clinton’s 2016 to win back Michigan, Pennsylvania, and Wisconsin. Even a candidate strongly identified as a progressive like Keith Ellison survived a relatively weak showing in the Minneapolis suburbs (due to intense police union attacks) by doing relatively well in Minnesota’s Factory Town counties in the Iron Range and the South. Culture War vs Economics? There is little doubt that the cultural differences between metro America and non-metro America play a role in the political divide between the two sectors, and that working-class folks find urban and intellectual “wokeism” annoying. But the evidence in our research (as well as other polling we have seen) is that, contrary to many pundits’ assumptions, economic issues are driving the problems of Democrats in non-metro working-class counties far more than the culture war. Based on the evidence I have seen, these voters wouldn’t care all that much about the cultural difference and the woke thing if they thought Democrats gave more of a damn about the economic challenges they face deeply and daily. One important thing to note: both Democrats and swing voters in Factory Town counties are pro-abortion rights, pro-gay rights, anti-book burning, in favor of teaching children about the real history of slavery and the civil rights movement, and in favor of a path to citizenship for most immigrants. While cultural issues framed a certain way by Republicans can pack a punch, the voters we need to win in these counties are not inherently right wing on social issues. It is also important to point out that while the swing voters in these counties say they are against defunding the police and in favor of a secure border, they are open to Democratic messaging on both crime and immigration. And those issues are not listed as high priority issues by many swing voters, let alone Democrats. Immigration is listed as a top tier issue by 16% of voters, and crime by 12%, far below the top tier issues (which are economic). Furthermore, these numbers are driven more by Republican voters, as opposed to swing voters or Democrats. Most importantly, when we tested a Republican culture war message vs a populist economic Democratic message, we won handily: 50-41. However, we are in a hole on the economy in general. Our strongest economic message is competitive with their economic message, but loses slightly to it, 43-46, because people are inclined to accept the Republican economic framing. A three-point margin isn’t bad given that we were losing to the Republicans nationwide on economics by 20 points in 2022, but we have the potential to do better. It is also true that the reason the numbers are that close is that the populist message wins strongly in Michigan; that message loses by bigger margins in Pennsylvania and Wisconsin. In Ohio, we are down four points on the head-to-head message comparison, which isn’t bad given the Republican lean in the state. KITCHEN TABLE POPULISM We tested a number of different approaches to economic messaging in the poll, and found that our most populist message performed the best. We also discovered in our broader issue messaging that some of the most populist language on other issues also tests very well. Language about the top 1% gaming the system to get richer tests very well. A range of populist economic policies test extraordinarily well with these voters, including cracking down on wage theft, cracking down on corporate price gouging, closing tax loopholes that allow wealthy corporations to pay no taxes, increasing overtime pay, breaking up corporate monopolies, and lowering interest rates on payday loans. All of these issues test in the 70s or 80s in overall support, and 55% or higher in terms of strong support. So Democratic policy proposals are popular, and populist language about them works. But populism all by itself does not work to move votes as well unless you tie it to other, more practical kitchen table concerns. Overcoming cynicism with results. Working-class voters in non-metro America are deeply, profoundly cynical about politics and politicians. Mouthing general populist campaign rhetoric or running a few TV ads with that tilt are not going to move people very far, because they feel like they have heard it all before, and they think no one – Republican nor Democrat – ever truly fights for them or delivers on their behalf. Our poll shows that even messaging that people strongly agree with doesn’t by itself move horse race numbers very much. Democrats must show up and fight for working people. They have to combine populist rhetoric with delivering practical, tangible results. The good news is that Democrats actually did do exactly that in the last two years. But these voters’ lives are too busy and too stressful to follow national political news much, and a lot of them live in news deserts where local newspapers are closing or being dramatically cut back. Local TV news stations cover mostly crime and traffic accidents -- and/or have a rightwing agenda themselves, as 50% of local TV stations are owned by three media corporations headed by far-right owners. Things are also complicated by the fact that many of these voters live in states where the Republicans control the legislature and counties where Republicans control the county board. This means that a lot of the benefits passed by Democrats at the federal level have not yet been delivered to local communities, and if they have, local Republicans are undoubtedly taking credit for them. As more and more infrastructure projects get started, and more and more factories open because of the CHIPS Act and the Inflation Reduction Act, Democrats will have plenty of chances to take the credit they deserve. But implementation of how the money gets rolled out and aggressive promotion of its benefits are going to be absolutely critical. Democrats and their progressive allies at all levels need to spend some serious time thinking through the strategy on how, when, and where money gets moved, and on how to communicate our successes as the resources get out the door. Combining populism with support of small business. The second big thing Democrats need to understand is that while voters don’t like greedy corporations and the top 1% taking advantage of working people, they also remain focused on their own jobs and economic well-being. The best way to translate that reality into something practical is to focus on Democratic support for small business. For these cynical voters, the institution of small business is the closest thing to the gold standard there is today. They are cynical about politicians, both political parties, government at every level, the “corporate media” (a phrase that kept coming up in focus groups with high disdain), wealthy corporations and corporate CEOs, the internet, and many other things in modern life. But they have a high regard for most small business people. Most working-class folks very much think of small business owners as part of the working class, as people who work hard, have the same challenges they do, and who are essential to rebuilding their communities. Democrats and progressive issue advocates should always talk about how much they care about small businesses doing well, and should be specific about the ways they want to help the small business community, whether it is taking on corporate monopolies, giving procurement and infrastructure contracts to small business first, cutting red tape, or other things that lower the challenges facing small business owners. Combining populism with the support of labor unions. Factory Town voters like labor unions almost as much as they like small business, and they in no way see liking both things as a contradiction. The favorability ratings of unions in these counties is in the high 60s, and more than that, people really trust what unions say on economic issues and candidates. Candidates and issue advocates would be well served to talk about how much they are supported by the labor movement, and to use labor leaders as spokespeople and endorsers in their ads and mail. Furthermore, voters see unions as practical and focused on what will work in getting more jobs and higher wages and benefits, so combining populist rhetoric with the stamp of approval from union leaders makes the populist messaging seem more real. And by the way, combining a small business owner and union worker in your messaging is likely to be extremely powerful. One other thing to note: in our Facebook issue testing as to what would generate the most positive engagement, the best-performing content was our “What would unions do?” ad, while another top performer was about small business. These tested well with men and more conservative audiences, as well as with more Democratic-leaning audiences. THE GENERATIONAL OPPORTUNITY PRESIDENT BIDEN PROVIDES DEMOCRATS President Biden’s powerful State of the Union address showed once more that he is the president more focused on reviving America’s working class than any president since the days of FDR and Truman. The Biden administration, along with Democrats in Congress, has delivered more tangible results for working-class families in two years than any other president has for their entire tenure in office in at least 60 years. The American Recovery Program, the infrastructure bill, the CHIPS Act, and the Inflation Reduction Act combine for the biggest investment in the American people in history. The Biden administration has been the most pro-labor administration since FDR, and has revitalized antitrust enforcement after 40+ years of everyone in both parties following Robert Bork’s pro-big business philosophy. Executive orders like ending junk fees and no-compete clauses for workers, a Buy America policy in federal procurement, student loan debt relief, and his sweeping Executive Order on Antitrust are changing the game for working and middle-class Americans. These policies lifted the economy out of the crisis it was in after COVID hit, and have sustained it through all the headwinds of the war in Ukraine, the supply chain crisis, and the Fed raising interest rates. They have given workers more bargaining power and delivered more than 800,000 new manufacturing jobs. They have meant that more people have health insurance than ever before, and have given seniors $35 insulin with far more drug price savings to come. For cynical voters in the Factory Towns who feel like politicians don’t care about them and never deliver for them, this is our opportunity. We have an incredible story to tell about what we have delivered and what we want to do next to make their lives better. We have to do it in the right way, though. Most voters are not following national news or the details of the legislation, and many haven’t yet seen the impact on their lives. And working-class voters outside of the big metro areas are still leading pretty tough lives, so we have to balance the story of our success on policy with the recognition of those hard times. Joe Biden gets it. He is focused on a policy agenda that will lift these communities up. As he said in the State of the Union: My economic plan is about investing in places and people that have been forgotten. Amid the economic upheaval of the past four decades, too many people have been left behind or treated like they’re invisible. Maybe that’s you, watching at home. You remember the jobs that went away. And you wonder whether a path even exists anymore for you and your children to get ahead without moving away. That’s why we’re building an economy where no one is left behind. Jobs are coming back, pride is coming back, because of the choices we made in the last two years. This is a blue-collar blueprint to rebuild America and make a real difference in your lives. We have to make sure we are telling our success in the right kind of way. If we succeed, we could see real gains for Biden and other Democrats in the Factory Towns. And if we pull that off, Democrats will win in 2024 decisively and set the stage for a long-term governing majority. IMPLICATIONS FOR INVESTMENTS Based on this research and analysis, we think there are important implications for what the progressive movement and Democratic Party should invest in over the next two years and beyond. Here are our recommendations: 1. Democrats and their progressive allies should be organizing in these communities right away. Some TV ads the last couple of months before the election will not cut it – voters are too cynical about ads and political rhetoric. We need to invest in early organizing in these counties. Our recommendation is that both the national Democratic Party and progressive non-profits invest in hiring regional organizers based in targeted Factory Town counties. They should be assigned to build local committees and volunteer structures in the counties. One of the key activities would be to organize fun and helpful events where people can come together to share information, get targeted help, and do that community building thing. Staff should also be assigned to organize local media events that get them on the local news, and to coordinate with all the groups working on economic issues. 2. Facebook community building. The people in Factory Towns spend a lot of time on Facebook, and like the community building and information sharing aspects of it. Based on some pilot projects AFV launched in 2022, it is clear that progressive groups and the Democratic Party should make a major investment in building local and regional networks of Facebook pages. In late October AFV ran a campaign to reach the target audiences for the Factory Towns project across both Facebook and via programmatic paid advertising. The overall campaign had 9.3 million impressions at an average cost-per-thousand of $9.45, a remarkable cost effective rate. We used local Factory Town social media pages to deliver both policy/political and non-political issue content via organic and paid promotion. We reached 1.2 million people on social media for a modest $15,636. The average CPL (cost per like) was 16 cents across our network. For the social media ads, we promoted a combination of original content based on our positive engagement testing and curated news articles that fit within the relevant themes. The engagement rate was high, with curated news and the union, small business, oil company price gouging, and prescription drug costs content performing the best. In the course of two weeks, we added 24,420 fans to our social media pages through paid and organic growth. We also delivered 4.3 million impressions and 2.7 million video views through targeted online programmatic advertising. This effort featured four 31-second cut downs of the Factory Towns’ closing argument video. All four had over a 60% completion rate. Ads targeting Pennsylvania and Wisconsin had the highest number of views, especially among a target group of moderate conservatives. The union and infrastructure messages had completion rates of over 70% in Iowa and Ohio. You can view all five videos here. We intend to use 2023 as an opportunity to continue messaging to the key audiences of the Factory Towns project from established social media pages. This will allow us to have an ongoing conversation in these states with a blend of political and non-political content that hopefully builds trust between our Factory Town pages and these voters. We will use the social media effort to recruit local influencers and messengers to support organizing online and in the community by ourselves and our allies. I believe that this kind of investment in Facebook community building is essential for communicating in the news deserts of the Factory Towns. 3. Investment in local media. Progressives need to be investing in both (a) reaching out to the local media outlets that still exist, and (b) investing in our own sympathetic media entities that focus on these communities. On the first point, there needs to be a concerted, consistent, and systematic effort toward moving articles, op-eds, videos, and other content to existing local newspapers and radio/TV stations that serve these communities. There are some groups that do this right now, but there is no broad systematic project to make this happen on a consistent basis. It’s not rocket science, but would need resources to make such a project happen. Just as importantly, media projects and programs like Courier Newsroom and Rick Smith’s Working Class Heroes show should get major investments from donors. In 2022, AFV worked with the Rick Smith show to organize a tour of Factory Towns, and it was a big success. 4. Creating a coalition of local, regional, and national organizations to support progressive movement infrastructure building in Factory Towns. When you are a cynical, alienated person living in a forgotten, ignored community, you need more than rhetoric to move you. Organizations that provide direct services and benefits to people are an important component for reaching out to Factory Towns voters. There are lots of local service providers and community organizations around the country that do this, and we will need to work to recruit as many as possible into our network. Well-resourced national organizations like WorkMoney also are playing a major role.WorkMoney provides direct benefits and services to its members while engaging in political education and activism, all crucial components of winning over working-class voters. WorkMoney is focused on going into these communities and making sure that the folks there know what benefits and potential jobs are available to people because of the legislative investments of the past two years. And when they connect people with those benefits, they can make sure that people know which politicians are to thank for getting those bills passed. We must amplify this effort exponentially across the network. 5. Don’t forget the basics: door knocking and phone calls still matter. Old-fashioned field organizing, especially door knocking, is still the single most effective way of reaching voters. Organizations like Working America, In Union (which targets the many pro-union voters in target states), Center for Popular Democracy, and People’s Action are continuing the door-knocking tradition and are targeting some of the Factory Town counties. And Democratic campaigns that have big field programs are much more likely to win. CONCLUSION The Democratic Party and its progressive allies dug themselves into a deep hole in terms of Factory Towns and the people who live in them. Our brand is pretty damaged in these places, and voters are both cynical about what we are saying now, and unaware of all that Democrats have accomplished that will directly benefit them. The good news is that the Republican brand is in no better shape, as voters know well that Republicans tend to be in the pockets of the same wealthy corporations that have been screwing them over. A lot of the people who voted for Trump because they thought he would fight for them are sorely disappointed. There are a lot of swing voters in these counties, and a fair number of base Democratic voters who have mostly stopped showing up for elections. But there are plenty of issues that appeal to these voters, and messages that resonate with them. The Biden administration and congressional Democrats have made policy gains in the past two years that could provide a major boost to the people living in these communities. If we communicate effectively about it, and make sure the benefits actually get into folks’ hands, Democrats will reap major rewards. The problems the Democratic Party brand has here will not be changed overnight. We will need a comprehensive long-term strategy for moving these counties back toward us again. It will take a lot of time and a lot of work. However, the results from the last couple of elections show that concerted efforts in these counties do reap short-term results. Fortunately, there are a lot of great organizations at the national, state, and local level who are eager to do the work, and more and more Democrats who want to join them. These counties cannot be ignored. Factory Town counties are 48% of the voters in Pennsylvania and the Midwest, and the numbers don’t add up for Democrats if we don’t start moving them back home to our side. With Joe Biden of Scranton as our president, with the policy successes for working-class Americans that Democrats and the progressive movement have delivered, Democrats have their best opportunity to make progress in these counties in a generation. The next two years is the time to build this strategy in a big way. Populism, Media Revolutions, and Our Terrible Moment Author: Green, Hank, Date: 2024-11-22 Collections: Hot Takes US Elect 2024, Polarization Zotero Key: Q753PM3I Cite Key: Green24PopulismMediaRevTerrible Zotero Item | Lit Note 00:00 good morning John in this video I would 00:02 like to try to explain what the hell is 00:04 going on which I know is a tall order 00:07 and it's going to be imprecise but we're 00:09 going to do our best everyone lives 00:10 through unprecedented times but a thing 00:12 I keep thinking especially when I'm 00:13 looking down at an 8-year-old boy who's 00:15 probably going to be alive in the year 00:16 2100 is how weird is this really how 00:20 weird is his life going to be and John 00:23 I'd like to make the case that vary like 00:25 his life is going to be historically 00:28 weird my son is going to grow up in a 00:29 world that has been fractured by a set 00:31 of circumstances that the world has not 00:33 seen for more than 500 years now media 00:36 Technologies absolutely do evolve and 00:38 change and there are new ones all the 00:40 time but I'd like to start if you will 00:42 forgive me talking a little bit about 00:44 radio which I think we think of radio as 00:47 one of the most boring Communications 00:48 Technologies there is just a way to get 00:50 music that we didn't pick interspersed 00:52 by ads that are at least locally 00:54 relevant but let's put ourselves into 00:56 the minds of people back in the early 00:58 1900s it's hard to overe exaggerate how 01:02 powerful radio was when it first hit 01:04 which was not 500 years ago we're going 01:06 to get there we're starting with radio 01:08 radio was invented in the 1890s but it 01:10 wasn't until the 1920s that lots of 01:12 people started having radios in their 01:14 homes and by the 1930s they were cheap 01:17 and easy to get and I need you to 01:19 imagine for a moment how extremely weird 01:22 this technology must have been for most 01:24 people the radio was the first time in 01:27 their lives that they heard music 01:29 without the musician being physically 01:31 near to them that was the world less 01:33 than a hundred years ago but more 01:36 relevant here it was the first time that 01:39 one person's voice could reach millions 01:42 of people and it could do that in real 01:44 time in fact at first it could only do 01:47 that in real time this was all happening 01:48 at the same time as the Great Depression 01:50 people had good reasons to be pretty 01:52 dissatisfied with their situations and 01:54 this wasn't just an American thing 01:57 Europe was also struggling especially 02:00 Germany which had been hit hard by 02:01 having to pay reparations to the winners 02:03 of World War I though how much that 02:05 contributed to things is apparently 02:07 debated by historians of which I am not 02:09 one people were not in great places 02:11 there was demand for people wanted 02:14 things to blame their circumstances on 02:17 and for a few reasons radio was much 02:19 better at satisfying that demand than 02:21 newspapers newspapers had been around 02:24 for a long time they were more expensive 02:26 you had to pay for them every time you 02:27 wanted a new one there was also more 02:29 regulation and structure around them 02:30 people had built up an understanding 02:32 that newspapers were sometimes trying to 02:34 manipulate them so they had some 02:36 built-in weariness radio on the other 02:37 hand was brand new it was a one-time 02:40 investment you didn't need to be 02:41 literate which many people weren't you 02:43 didn't need to go into this city to buy 02:45 a new one every day and more than 02:47 anything it was magical like I have a 02:50 hard time imagining how new and weird 02:52 and wonderful it must have been to like 02:54 listen to a charismatic person talk to 02:56 me from thousand miles away and what a 02:59 spell that must have cast and also to 03:00 know that people all over my country 03:02 were hearing the same speech at the same 03:05 time and that we were all there together 03:07 experiencing it millions of us there 03:08 were also in the beginning very few 03:10 legal regulations on radio and I think 03:12 just as powerfully if not more 03:14 powerfully there were no mental 03:16 regulations there were no Norms people's 03:19 minds were just open to this to have a 03:21 person speaking to them you might go and 03:24 like watch a talk or a debate or 03:26 something but otherwise the only people 03:28 people talked to were their friends and 03:31 this was a new way that strangers 03:33 powerful charismatic strangers could 03:35 speak to you for example father Cogan 03:38 who was one of the first radio preachers 03:40 and he tapped into specifically this 03:43 dissatisfaction he connected with and 03:46 empathized with normal everyday ordinary 03:49 Americans he spoke about elitist power 03:51 structures the banks the inequality and 03:53 not all but certainly some of the Jews 03:57 atheistic Jews communist Jews millions 04:00 of people listened to these broadcasts 04:02 he was an extraordinarily powerful 04:04 figure in the US and as Hitler Rose to 04:06 power in Germany in part because he was 04:08 also super Savvy at using the radio to 04:10 communicate with the common people of 04:12 Germany father Cogan spoke about him 04:14 with great appreciation and interest I'm 04:18 simplifying a ton here but there's 04:20 something happening here it is a post 04:22 media Revolution rise in populism and it 04:25 happens pretty much every time there's a 04:28 media Revolution but but populism begins 04:31 not as the ideology but the marketing 04:34 I'm seeing people get this wrong all 04:35 over the place right now populism is uh 04:38 not an ideology it's a marketing 04:39 strategy I see your real human problems 04:43 and I also see who is causing them all 04:45 the institutions that are impenetrable 04:47 and imperfect but also the people those 04:50 institutions are secretly supporting at 04:52 your expense or controlled by to your 04:55 detriment and if you listen to me we 04:57 will solve all the problems with a 04:59 number number of new policies but 05:01 sometimes populism actually becomes the 05:04 policy it goes from being the message to 05:07 being what we're going to do here are 05:09 the bad guys the policy is get rid of 05:12 the bad guys that's what happened with 05:14 Father Cogan it's what happened with 05:15 Nazi Germany father kogan actually 05:17 started to advocate for the US to work 05:20 with Nazi Germany he was an extremely 05:22 harsh critic of FDR he began to say very 05:24 nazil likee things about what Americans 05:26 should do to Jews but as the tensions 05:29 between US and Germany rose that became 05:32 untenable and the public and the 05:34 government persuaded radio stations to 05:37 stop carrying his broadcasts even though 05:39 cogin remained popular FDR basically 05:41 said no this guy cannot talk to the 05:44 American people anymore this isn't print 05:46 it's radio it's new it's a limited 05:48 public resource it's a spectrum that we 05:51 can't fit everything on and this is one 05:52 of the things we can't fit on it and I 05:54 think this was a fairly clear violation 05:57 of the freedom of speech father cogin 05:59 drifted Into Obscurity he became a 06:01 normal priest again he died the year 06:03 before I was born so people are thinking 06:06 probably right now okay so Trump is like 06:09 the cogin of the internet age or like 06:11 the Hitler of the internet age and like 06:13 a little bit like you can see it in 06:15 there like the alignment with people's 06:17 legitimate grievances the railing 06:19 against every institution from public 06:21 schools to doctors the use of a new 06:23 somewhat magical unregulated 06:25 Communications technology to name those 06:27 threats and label who is responsible for 06:30 them as the enemy Trump is the product 06:32 of a post media Revolution rise in 06:35 populism I don't think anyone would 06:36 argue with that that does not mean that 06:38 he is the same guy as cogin or Hitler 06:41 but it is interesting that he is a 06:43 product of a phenomenon that we have 06:45 seen before but also he's not like these 06:48 other guys in another way because I 06:49 don't think that radio is the best 06:51 analogy for the internet radio and TV 06:54 are centralizing whereas the internet is 06:57 decentralizing what do I mean well to 07:00 make a radio station you need advanced 07:02 technology lots of money you need 07:04 permission it's very easy to regulate 07:07 because there are only so many radio 07:08 frequencies and governments actually had 07:10 to quite quickly move in and regulate 07:12 who would have access to them or 07:13 stations would constantly be overlapping 07:15 with each other or the most powerful 07:17 wealthiest person could just take up the 07:19 whole radio spectrum television has been 07:21 the same there were only so many 07:22 channels each tightly regulated they're 07:24 very expensive to run and in the 07:26 beginning there was only space for three 07:28 or four of them this media Revolution 07:30 because it was so centralizing was a 07:32 force that actually ended up being quite 07:33 useful for bringing America and other 07:35 countries together reducing the number 07:38 of voices that could be heard thus 07:41 silencing many marginal perspectives but 07:43 eventually TV did start to fracture 07:45 until we got multiple news channels from 07:47 different perspectives first Fox News 07:49 with its more right-wing nationalist 07:51 perspectives then MSNBC with a more 07:53 Progressive leftwing perspective but 07:55 even then TV was a relatively 07:56 centralizing Force because so much of it 07:58 was available and there were lots of 07:59 rules and regulations that had built up 08:01 over the years the Internet isn't like 08:04 this it is the opposite of this but 08:06 there has been another media Revolution 08:08 very much like this the first one the 08:11 printing press the movable type printing 08:12 press at least the version used in 08:13 Europe was invented by a guy named 08:15 johanes Gutenberg to solve the problem 08:17 of every book in Europe being written by 08:19 hand which does seem like a difficult 08:21 way to do things for a long time after 08:22 Gutenberg started making Bibles no one 08:25 really knew what to do with print 08:27 Technologies books were big and 08:29 expensive and there just wasn't that 08:31 much demand for them that demand was 08:33 mostly soaked up by the most powerful 08:35 institutions out there which is largely 08:37 the Catholic Church the only church of 08:39 note in Europe but then a guy had an 08:43 idea he like lots of other people did 08:46 not like how the dominant system was 08:47 operating and there had been many 08:49 arguments about that over the years but 08:51 instead of just talking about it he went 08:53 to a printer and he said hey print me 08:57 105 page pamphlets and that was way 09:00 easier for the printer than making a 09:02 whole book it was cheap the printing 09:04 press was already there all they needed 09:05 was the paper and the ink this guy's 09:06 name was Martin Luther and you might 09:08 know him from one of the things he made 09:10 uh Lutheranism people could pay just a 09:12 little bit for a pamphlet like that and 09:14 you'd actually make quite a lot of money 09:16 as a printer doing it you could create 09:18 an economy around that printers in their 09:21 off time could just print up a bunch of 09:22 Martin Luther's pamplets and sell them 09:24 themselves Martin Luther interestingly 09:26 just like everyone on Twitter didn't 09:27 actually make any money selling these 09:29 pamphlets it was the people with the 09:31 presses who got paid and Luther was just 09:33 doing it because he wanted to get his 09:35 message out familiar and again I have to 09:37 try and get in touch with how weird this 09:40 must have felt to the people at the time 09:42 books were big and expensive and for 09:45 hundreds of years the ideas of a normal 09:47 person owning one would have been 09:48 ludicrous and so almost all of the 09:50 people who were printing anything were 09:52 really big powerful institutions but now 09:56 there was this thing that was like a 09:57 book and it had ideas ideas that weren't 10:02 the ideas of the powerful people who 10:03 controlled the books they were new ideas 10:05 from a guy who wasn't particularly 10:07 powerful and the powerful people really 10:09 didn't like these ideas and it seems 10:12 that Martin Luther realized that the 10:14 more inflammatory his writing got the 10:16 more people wanted to buy the pamphlets 10:18 and the more printers wanted to print 10:20 them and to be fair the institutions of 10:21 the time were not undeserving of 10:23 critique now Martin Luther was not the 10:25 only person who printed pamphlets like 10:27 these but he was very good at it I 10:29 didn't really understand this for the 10:30 first couple of decades after knowing 10:32 that Martin Luther existed and that the 10:34 Reformation was a thing but this guy was 10:37 a social media influencer and he was 10:40 also not chill he was a reckless guy you 10:43 might not be super surprised to hear 10:44 this but he hated Jews during a peasant 10:47 Rebellion which was in part actually 10:49 inspired by his work he called on land 10:51 owners to just kill the peasants 10:53 likening them to dogs that needed to be 10:56 put down somewhere between 100,000 and 10:58 300,000 of them were killed he called 11:00 the Pope the Antichrist he he was not a 11:03 chill guy with the rhetoric is all I'm 11:06 saying though it should be said several 11:07 times the Catholic church in the 16th 11:09 century was worse just absolutely very 11:12 bad I I don't I I don't don't feel like 11:13 I necessarily need to get into it but 11:16 pretty evil and also in addition to 11:18 being inflammatory Martin Luther was an 11:20 exceptional Theologian his arguments 11:22 were good that's part of the reason why 11:24 he was so popular back then and indeed 11:26 remains fairly popular now and again 11:28 there were at first uh because it was 11:30 also so new no regulations on this new 11:33 form of media in law and just as 11:36 importantly none in the minds of people 11:38 and there were regulations on uh calling 11:40 the Pope the Antichrist and Martin 11:42 Luther was excommunicated but he did 11:44 manage to survive which is amazing to me 11:47 like he he lived I need I would love 11:50 someone to explain to me how how he was 11:52 not just killed but when you like look 11:54 at the details of this it is all much 11:56 more Twitter screaming match than I had 11:58 first imagined it would be it was very 12:00 conversational people were mad at each 12:03 other and they wrote pamphlets back and 12:05 forth and they hated each other they 12:07 were just mean arasmus was a friend and 12:10 also eventual rival of Martin Luther and 12:12 in his last ever letter to Martin Luther 12:14 he wrote what distresses me as it 12:17 distresses all decent people is the fact 12:19 that because of your arrogant insolent 12:22 and turbulent personality you cause a 12:24 fatal dissension that unsettles the 12:26 whole world you expose good men and 12:29 lovers of the humanities to the fury of 12:31 the Pharisees and you arm Wicked and 12:33 rebellious men for revolution there is a 12:36 similarity between Martin Luther and 12:37 father cogin and Donald Trump and many 12:40 other anti-establishment populists Who 12:43 Rose to power after media revolutions 12:45 and it is that they all were if nothing 12:48 else Reckless as a person who has a 12:51 fairly significant platform and is often 12:53 terrified that I will say something that 12:54 is incorrect or will have an overall 12:56 negative impact on the world this is 12:58 like kind of hard for me to understand 12:59 but Martin Luther did not like that his 13:02 words had resulted in armed conflict and 13:05 death but he simply could not stop 13:07 talking he felt that he was right and of 13:09 course about many things he was and that 13:11 was far more important to him than any 13:13 potential unintended consequences 13:15 tearing down the corrupt institutions of 13:17 the Catholic Church resulted in a 13:19 complete fracturing of their hegemony 13:21 and there is basically nothing that has 13:22 happened since in European history that 13:24 was not influenced by that was the 13:26 Reformation entirely caughted by the 13:28 printing press of course not there were 13:29 many factors that happened all at the 13:31 same time enough people for example 13:33 needed to be able to read for it to 13:34 happen and that took a long time to 13:36 occur but I don't think anybody's out 13:38 there arguing that it wasn't an 13:39 important piece of the puzzle so there 13:41 is something happening over and over 13:43 again here and and like weirdly the same 13:45 way which is that there are big 13:47 institutions that kind of make Society 13:49 work and they're always some level of 13:52 imperfect and when there is a media 13:54 Revolution really anytime whether it's 13:56 radio or newspapers or pamphlets there 13:58 is a rise in populism it helps when 14:00 there are other problems going on at the 14:02 same time like for example a bunch of 14:04 post-pandemic inflation or the Catholic 14:07 church is maybe overextending its reach 14:10 a little bit by selling the ability to 14:12 get into heaven or there's a great 14:14 depression but the media Revolution 14:17 plays an important role we got to get it 14:19 out of our heads and I have this problem 14:20 too that this somehow aligns with 14:22 current political ideologies of the left 14:25 and right in the US but in general there 14:27 are times when it is more from the left 14:31 and by which I mean that the enemy that 14:33 has created all the problems are the 14:34 wealthy who gathered the power around 14:36 themselves and use it to control 14:37 everyone else and then there are times 14:39 when the populism is more right-wing 14:41 when the enemy is the group with like an 14:43 unaligned cultural identity whether 14:45 that's immigrants or Jews or another 14:47 race or something like that 14:49 nationalistic populism or socialistic 14:51 populism but honestly populism is about 14:54 marketing so these things blend together 14:57 populism in America right now is clearly 14:59 from the right it's from the right-wing 15:00 political party but it has a sting of 15:02 socialistic populism where people 15:05 deserve support as long as they are the 15:06 right people the people who have built 15:08 those systems are the cultural Elites 15:10 not necessarily the wealthy precisely 15:13 but certainly some of them are wealthy 15:14 and certainly they are like the high 15:16 status cast of soft skinned overeducated 15:19 wine drinkers but it's not like there's 15:21 nothing to attack there's not like 15:23 there's no problems anti-establishment 15:24 populism at this moment was inevitable 15:27 both because our systems are perfect and 15:29 people's situations are not great and 15:32 because of this intense decentralized 15:34 media Revolution like we need to 15:36 remember that this shift to 15:38 authoritarians is not a postco 15:40 phenomenon this is an internet 15:43 phenomenon imagine Donald Trump without 15:46 Twitter you can't I think I mean I know 15:49 that the bureaucracies of today are 15:51 better and more transparent than those 15:52 of the 1930s or especially 15:56 1470 but they are imperfect 15:59 obviously there are real problems in our 16:01 society whether that's how easy it is to 16:04 get unhealthy food or how hard it is to 16:06 get good medical treatment or child care 16:08 there are lots of real reasons to be 16:10 pissed off communication is what we do 16:12 it is the thing that makes us human and 16:15 when a technology like the social 16:16 internet hits it's a big deal back when 16:19 print happened it was the first time any 16:21 person who did not have a huge amount of 16:24 power could use a nonverbal form of 16:27 communication on a lot of other people 16:29 and then when radio happened it was the 16:31 first time one person could reach 16:33 Millions at the same time now we've 16:35 lowered that print barrier so far that 16:38 it isn't just like some overeducated 16:40 priest like Martin Luther who has access 16:43 it's everybody and it's not just one 16:45 person who can access Millions it's 16:47 Millions who can access Millions it's 16:49 everyone who can access everyone it is 16:51 entirely unprecedented in the human 16:53 story and we don't have the tools to 16:56 deal with it we have this many to many 16:57 communication and this really intense 16:59 collaboration between ingenious humans 17:02 who want to capture attention and 17:04 recommendation algorithms that reward 17:06 whoever is doing it the best so things 17:09 are going to get flipped they get 17:10 flipped because first we have very few 17:13 legal regulations and second we have no 17:16 regulations in our minds we do not have 17:18 the techniques to deal with the powerful 17:20 spell that these systems are casting on 17:23 our brains I don't think the mechanisms 17:24 here are super opaque but maybe it's 17:26 worth outlining that if you ask a 17:28 newspaper reporter to go and tell you 17:30 some problems with the FDA you're going 17:31 to need to wait 6 months while they put 17:33 together a clear cogent story about 17:35 something that could be better in the 17:37 world if you ask a traditional 17:38 bureaucrat to try and fix something at 17:40 the FDA they're going to come back in 17:41 four years with a list of things that 17:43 need to be done and a much longer list 17:45 of why you can't do that but if you ask 17:47 RFK Jr to tell you what's wrong with the 17:48 FDA he's going to do it on YouTube 17:50 Instagram Facebook Tik Tok and four 17:52 different podcasts by the end of the day 17:54 will he have prioritized accuracy or 17:57 actionable implementation no he will 17:59 have prioritized getting views on 18:01 content and he will have achieved that 18:03 goal because RFK Jr is working in 18:05 collaboration with recommendation 18:07 algorithms and he is very good at that 18:09 and a really good way to do it is to 18:11 tell individual ordinary people that 18:13 they are being prayed upon by large 18:15 powerful institutions there's this 18:16 period of time after a communication 18:18 Revolution where our minds are just like 18:21 open and ready to have these magic 18:24 spells cast upon them I it's so hard to 18:27 remember how new this is and how magical 18:31 it is this ability for everyone to 18:33 communicate with everyone this is the 18:34 worst possible example but it's the one 18:36 coming to mind right now we live in a 18:38 world where Haw Tu a girl is possible 18:40 and that that's new and people who see 18:43 massive problems with the world or like 18:45 a clear path to power and status or 18:47 let's be honest probably both are going 18:49 to use that I know two brothers who have 18:51 used that just I hope not as recklessly 18:54 as some and when the system is 18:56 decentralized and automated and 18:58 operating at the speed of light and 19:00 optimized to grab and hold attention 19:02 like no communication system ever has 19:04 been in the history of humanity the 19:06 system is going to select and share the 19:09 stuff that sticks and when the story 19:11 doesn't actually need to be true cuz why 19:14 would it need to be true what who's 19:16 who's got the time to fact check all 19:18 this stuff one one very compelling story 19:21 is this the systems are bad and they are 19:24 designed by people who are bad to help 19:27 people who don't deserve it while 19:28 ignoring the people who do deserve it 19:31 that is as far as I can tell what's 19:33 going on here and it is the populist 19:36 message generally there is demand a lot 19:38 of demand for that kind of content and 19:41 as I have often said on today's internet 19:43 any content that will be consumed will 19:45 be created before the internet and also 19:47 right now like a a lot of society is 19:50 built on trust like we can't this is a 19:52 complicated world we can't keep all of 19:54 it in our heads at the same time nobody 19:55 is going to know everything we have to 19:57 trust that there are people who 19:59 understand the way that the roads work 20:01 people who understand how the voting 20:02 Works people who understand how clinical 20:04 trials work and that all those people 20:06 are worthy of trust because one person 20:08 can't know all of those things that is 20:10 the whole thing that has allowed humans 20:12 to get to where we are is to share that 20:15 labor John I'm editing this video and I 20:17 have realized that there are two things 20:19 that I need to add here the first is 20:20 that we cannot just trust these things 20:21 there needs to be some checks there 20:23 needs to be somebody who's investigating 20:25 and and pushing back because if you just 20:27 trust then that trust will be abused and 20:29 the way that we did that before the 20:30 internet and we still do is with the 20:32 Press an institution that has incentives 20:35 to find problems to discover ways that 20:38 trust is being abused by systems and the 20:40 great thing about the Press is that they 20:43 have that same pressure against them so 20:45 if they make up a story to sound really 20:49 inflammatory the whole institution of 20:52 that organization whether it's like at 20:54 the newspaper or the TV station or 20:56 whatever that reputation can get harmed 20:58 and thus 20:59 a lot of value can be destroyed by them 21:01 making big mistakes and so the Press had 21:03 an incentive to build systems that 21:05 prevented them from betraying their 21:08 audience's trust and getting things 21:09 wrong because otherwise it would be 21:11 damaging not just to society but to the 21:14 institution for which they work all of 21:17 the stuff that you learn in journalism 21:18 school about ethics and objectivity were 21:21 are all there because one the Press is 21:25 attempting to actually provide a true 21:28 value for society and two because there 21:31 is a lot of harm that can be done to an 21:33 institution by a member of that 21:35 institution getting something wrong but 21:36 now we are in a world where it is so 21:38 easy to destroy trust often times that 21:41 can happen without anything that big 21:44 getting done incorrectly just like it is 21:46 easy to destroy trust in anything at 21:48 this moment it is very easy to destroy 21:49 trust in the Press which has a 21:51 compounding effect uh especially when 21:53 you add that on to the fact that the 21:54 internet completely destroyed the 21:56 business model of the press that we have 21:57 lost a large portion of one of the main 21:59 institutions that was actually good at 22:02 helping people understand their world 22:04 and at uh identifying things when they 22:07 actually go wrong this is less the case 22:10 when it's just a bunch of people because 22:12 if one person loses their credibility 22:14 that isn't that much value being 22:15 destroyed and there's always another 22:17 person ready to take their space very 22:18 quickly and also it's hard in an 22:20 environment with just so 22:22 much information to actually you know 22:26 you can make a video and I have about 22:28 how many things Eon musk got wrong in a 22:31 single day and that's not going to do 22:33 very much harm to his reputation for 22:35 whatever reason it just feels like it's 22:36 a lot easier to destroy people's faith 22:38 in an institution and like an 22:40 organization of lots of people than in 22:41 any individual person that you just 22:43 might like really Vibe with even if you 22:44 see them making a lot of mistakes it's 22:46 like well they of course they make 22:47 mistakes they're just a person he's just 22:48 some guy with a podcast of course he's 22:49 wrong about stuff all the time that's 22:51 what I like about him he's being a human 22:52 anyway that was the first thing I felt 22:53 like I had to include here the second 22:55 thing is that as I was editing this 22:57 video I realized that the is named after 22:59 the Press I don't know if that's just me 23:01 but that was I was like w and now we can 23:04 go back to the video as a refresher cuz 23:06 this was longer than I expected it to be 23:07 we were talking about trust and how a 23:08 society is built on trust and how we 23:10 cannot do things without trust okay 23:13 continue stories that tell people why 23:15 their trust is misplaced are extremely 23:17 compelling they feel as if they could be 23:20 life-saving even and certainly feeling 23:22 like you might have been duped or that 23:24 everyone else is being duped but not you 23:27 are powerful tools for storytelling and 23:30 Powerful antitrust forces not like 23:33 antitrust in terms of monopolies just 23:35 like degrading of trust but it's really 23:37 important to realize that that force is 23:39 not necessarily pointed toward the worst 23:41 offenders Martin Luther was correct that 23:44 the Catholic church in the 1500s was 23:46 corrupt and evil and had lost its way 23:48 the movement he kicked off is called the 23:50 Reformation because it caused very 23:52 needed reforms he had a good Target but 23:55 at the moment it feels like everything 23:58 is a Target and the internet is less a 23:59 machine that devours corruption and more 24:02 a machine that devours trust trust in 24:04 each other trust in society in 24:06 institutions in expertise in Tradition 24:09 in our ability to do democracy I don't 24:12 think that's going to stop getting worse 24:14 soon though please I would love to be 24:16 proved wrong maybe it's not as bad as I 24:18 think it is because of course I'm 24:20 exposed to the worst of it and maybe we 24:22 can turn it around quicker than I think 24:26 but in a society we're so much trust has 24:28 been destroyed people are going to 24:30 gravitate toward individuals they feel 24:32 they can trust rather than institutions 24:34 that actually have the knowledge and 24:36 ability and those individuals will thus 24:38 have a great deal of power without a 24:40 great deal of the ability to use it well 24:42 and that doesn't tend to be a great 24:44 recipe for progress I don't however want 24:46 to learn the wrong lesson here I don't 24:48 actually know what the lesson is the 24:50 lesson I hear getting tossed around 24:51 including from myself is okay well 24:53 everybody's going to have to do populism 24:54 now and like yeah but also I don't know 24:57 like here's the thing I know that I am 25:00 one of the Elites in question but like 25:02 sometimes popular things suck right-wing 25:04 folks hear me out here if you're 25:06 listening let's start here sometimes 25:09 when the internet starts piling on to a 25:11 person we go too far too fast we cancel 25:14 people when maybe we didn't need to do 25:16 that and in my mind the word cancel 25:18 makes so much more sense if we just call 25:20 it reputational harm because cancel 25:22 means so many different things so in 25:25 this case someone's reputation gets 25:27 badly damaged and maybe that was the 25:29 right thing to do but maybe it wasn't in 25:31 the same way I think there are two 25:33 factors here when it comes to New Media 25:35 induced populist attacks on expertise 25:37 and institutions first there is how much 25:40 does the expert or the institution 25:43 deserve the harm to its reputation which 25:45 is not always zero and not always 100 25:49 and second how easy is it to do damage 25:52 to the reputation and those things have 25:54 a relationship with each other but they 25:56 are not always tightly connected just 25:58 like with a person sometimes there's 25:59 going to be an institution like say 26:01 whatever is making it so much harder to 26:03 build new homes in blue States than red 26:05 states that I think deserves to be named 26:08 in shame but then there's also going to 26:09 be some things that are I think are very 26:10 easy to attack that yet have all 26:13 available evidence suggesting they don't 26:15 deserve to be canceled like the measles 26:17 vaccine or FDA regulations on raw milk 26:19 and I worry that our systems might be 26:21 getting cancelled like experiencing 26:23 tremendous reputational harm in a way is 26:25 going to be bad for the individual 26:27 people of the country despite the fact 26:29 that they didn't do much wrong there's a 26:31 reason why tons of Institutions are 26:34 suddenly almost equally under 26:36 reputational attack the CDC the FDA the 26:38 EPA Public Schools universities the 26:41 Press social media companies governments 26:43 that want to regulate certain social 26:44 media companies as long as they're owned 26:46 by Elon Musk Ukraine Europe NATO the UN 26:49 cities voting machines chemotherapy 26:51 water fluoridation there's a feedback 26:53 cycle between creators who make stuff 26:55 the systems that recommend it and the 26:57 audiences who watch and share it and I 26:59 know this because that's my world I 27:02 spend all of my time in it as a Creator 27:05 you know what's working and what's not 27:07 you know what's going to get attention 27:09 and what's paying your bills the system 27:10 tells creators what is working and 27:13 creators do that like it's very hard to 27:16 not just do the things that succeed just 27:19 like Martin Luther and his printers but 27:21 at the speed of light if we aren't 27:22 better at grabbing and holding attention 27:24 than the next guy we instantly Fade Into 27:27 irrelevance and The View ERS accompany 27:29 the Creator on the trip down the hole 27:32 starting a little suspicious of powerful 27:34 and impenetrable systems and ending up 27:36 certain that every election that doesn't 27:38 go their way is fraudulent it's 27:40 important to note here that the group 27:41 with less alignment to the existing 27:43 systems is going to be the one that does 27:45 best in that situation in my group who 27:48 are more likely to be the ones who 27:49 literally staff the bureaucracy they're 27:51 going to be the ones who don't do as 27:53 well and yeah I am super suspicious of 27:56 charismatic powerful people who convince 27:58 others that every system is bad and 28:01 broken and that they're the only thing 28:04 that you can trust that often goes bad I 28:07 think we need to be very wary of that so 28:10 that's where we're at I think and it's 28:12 bad but it is not that weird actually 28:15 and that's why it's happening all over 28:16 the place at the same time and look I do 28:18 not know what a political party is 28:20 supposed to do thank God I am not a 28:22 politician and do not aspire to be but I 28:25 do know what I feel like I should do so 28:28 my job is that I make things to help 28:30 people understand things and one of the 28:32 problems is that it's a lot easier to 28:35 attack systems that are impenetrably 28:37 complex so I'd like to suggest to folks 28:40 out there who are who might be able to 28:42 do this make them less complex and also 28:45 work to make them better but barring 28:48 that which is your work not mine we need 28:50 to continue to develop technologies that 28:53 explain things well and that help people 28:56 understand the mechanism 28:58 at work in society and ideally we need 29:01 to do that in ways that are not 29:02 perceived as 29:12 hyperparadise that the system is quite 29:15 bad and we need to be honest about that 29:17 remember there are two parts there's how 29:19 easy it is to cancel a system and 29:21 there's how much a system deserves it 29:24 and there is interplay between those two 29:25 things sometimes a system is ripe for 29:27 can ation for bad reasons like that 29:30 people don't like needles and have a 29:32 subconscious inclination to believe that 29:33 having things injected into them is 29:35 probably bad and sometimes they are 29:37 susceptible for good reasons like if a 29:39 college education has been increasing in 29:41 price for the last 50 years to the point 29:43 where it's now saddling students with a 29:45 lifetime of debt and that more than 50% 29:47 of students with debt never end up with 29:49 a degree that's bad and we should point 29:52 at it and we should say this is bad and 29:55 we let it happen this is our fault and 29:57 we can't just sit there and not do 30:00 anything about it I don't know if this 30:01 all means that adopting populism is the 30:03 right call for people who want to win 30:04 elections I'm actually not thinking 30:06 about elections though I'm thinking can 30:08 we speedrun this because after the print 30:11 Revolution there were 200 years of 30:13 religious wars and for a hundred of them 30:15 the bestselling book after The Bible was 30:17 about how to identify and burn witches 30:19 and I would just love to avoid that yes 30:22 just like how some populist forces 30:25 identify Jews and some identify 30:26 immigrants and identify the wealthy 30:29 Elites the for a long time it was 30:32 witches and like hundreds of thousands 30:34 of them were murdered and now we have 30:35 forgotten it so much that witches are 30:37 like an adorable little folksy thing in 30:40 our culture we move forward in the ways 30:43 that we do I guess I want to leave you 30:44 with one last thing about Martin Luther 30:46 and the Reformation that I find 30:47 fascinating and important as he 30:49 critiqued the Catholic church and his 30:50 pamphlets made money for printers his 30:52 words were translated from the German 30:54 that he spoke and wrote in into other 30:56 European languages meanwhile the church 31:00 stuck doggedly to its policy of only 31:02 ever publishing content in the properly 31:05 authoritative Latin meaning that most 31:08 people were unable to read them because 31:10 not only could they not read Latin they 31:12 couldn't speak Latin it was a language 31:14 purely of the elites and I mean like 31:17 good because the Catholic church had 31:19 become so corrupted deserve what was 31:20 coming to it but I feel like there's a 31:22 parallel here to leaders today feeling 31:25 like they have to stick to the properly 31:27 authorit of news sources but not the 31:30 actually accessible Joe Rogan 31:32 experiences and that's not just a media 31:35 thing like it's not just a where you go 31:36 and who you talk to thing it's a you 31:38 need to be a human thing you need to act 31:41 like a human and I need to see you less 31:44 on the stage with Beyonce and more 31:46 talking to people who people actually 31:48 think are relevant thinkers to their 31:51 situations we need people who are good 31:53 at communicating and also have an 31:56 allegiance to the truth and that's 31:57 harder 31:58 it's harder because there's only one 32:00 truth while there are infinite lies as 32:02 you're doing the work to try and get 32:04 everything right a grifter on reals has 32:05 run a couple of laps around you so they 32:07 have that Advantage but they also have a 32:09 disadvantage which is that the things 32:11 that they are saying aren't true and as 32:13 we get better at not letting this New 32:15 Media cast spells on our brains as we 32:17 always eventually do the folks with the 32:20 better information do tend to win out 32:24 eventually the question is uh how long 32:26 the space between the ref and the 32:28 enlightenment was not quick it was more 32:30 than 150 years it seems important to me 32:33 that our path here be a lot shorter than 32:35 that so let's focus on that John I'll 32:39 see you on Tuesday a bibliography for 32:42 the end of this video um this was video 32:44 is heavily inspired by and informed by 32:47 Jeff Jarvis's Gutenberg parenthesis very 32:50 interesting and I would suggest it to 32:52 anyone I also pulled a lot from Wrong by 32:54 Don gold young uh and invisible r rulers 32:58 by Renee Desta I would also suggest 33:00 peripherally this was informed as is 33:03 everything I do these days by high 33:05 conflict by Amanda Ripley I think it's 33:07 probably the most important book that 33:10 anyone can read if I could make everyone 33:12 read one book it would be this book 33:15 similarly how Minds change by David 33:17 mcran and also going back to the speech 33:18 stuff cheap speech by Richard Hassen 33:21 also I I read that a while back uh like 33:23 two years ago but I feel like a lot of 33:26 that stayed in the brain certainly there 33:28 are things in all those books I disagree 33:30 with they don't Vibe with or think is 33:32 sort of an overextension of the thesis 33:35 but um all good works to help understand 33:38 a moment um if a 30 minute long video 33:42 isn't enough for you there's uh like 50 33:45 more hours of content just there for you 33:47 if you'd like to listen to audiobooks 33:49 about stuff like this doesn't make sense 33:51 to do a bibliography at the end of a 33:52 video I feel like one like that it does 33:55 all right goodbye Voting Intention and Choices: Are Voters Always Rational and Deliberative? Author: Lee, I.-Ching, Date: 2016 Feb 17 Collections: PollMethods, NeuroPsychoLinguisticPolitics Zotero Key: DAHHY8V2 Cite Key: Lee16votersRationalDelib Zotero Item | Lit Note OPEN ACCESS Citation: Lee I-C, Chen EE, Tsai C-H, Yen N-S, Chen ALP, Lin W-C (2016) Voting Intention and Choices: Are Voters Always Rational and Deliberative? PLoS ONE 11(2): e0148643. doi:10.1371/journal. pone.0148643 Editor: Koustuv Dalal, Örebro University, SWEDEN Received: July 3, 2015 Accepted: January 10, 2016 Published: February 17, 2016 Copyright: © 2016 Lee et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability Statement: All relevant data are within the paper and in https://drive.google.com/ open?id=0Bxjyvx9UN3aOZWhBQVY3RkJFX3c. Funding: This work was supported by MOST 1032221-E-004 -007 -MY3 to Dr. Arbee C. P. Chen in hiring WCL to assist with the project. It was also supported by MOST 102-2420-H-004 -014 -MY3 to Dr. Lee in hiring research assistants to do literature search and for the PLOS ONE publication fee. Financial assistance (no grant numbers were given) from the the Research Center for Mind, Brain, and Learning at National Chengchi University covered expanses for IRB approval and participants' incentives. RESEARCH ARTICLE Voting Intention and Choices: Are Voters Always Rational and Deliberative? I-Ching Lee[1,2], Eva E. Chen[3], Chia-Hung Tsai[4], Nai-Shing Yen[1,2], Arbee L. P. Chen[5], WeiChieh Lin[6] 1 Department of Psychology, National Chengchi University, Taipei, Taiwan, 2 Research Center for Mind, Brain, and Learning, National Chengchi University, Taipei, Taiwan, 3 Division of Social Science, The Hong Kong University of Science and Technology, Hong Kong S.A.R., China, 4 Election Study Center, National Chengchi University, Taipei, Taiwan, 5 Department of Computer Science and Information Engineering, Asia University, Taichung, Taiwan, 6 Department of Computer Science, National Chengchi University, Taipei, Taiwan • iclee@nccu.edu.tw Abstract Human rationality–the ability to behave in order to maximize the achievement of their presumed goals (i.e., their optimal choices)–is the foundation for democracy. Research evidence has suggested that voters may not make decisions after exhaustively processing relevant information; instead, our decision-making capacity may be restricted by our own biases and the environment. In this paper, we investigate the extent to which humans in a democratic society can be rational when making decisions in a serious, complex situation– voting in a local political election. We believe examining human rationality in a political election is important, because a well-functioning democracy rests largely upon the rational choices of individual voters. Previous research has shown that explicit political attitudes predict voting intention and choices (i.e., actual votes) in democratic societies, indicating that people are able to reason comprehensively when making voting decisions. Other work, though, has demonstrated that the attitudes of which we may not be aware, such as our implicit (e.g., subconscious) preferences, can predict voting choices, which may question the well-functioning democracy. In this study, we systematically examined predictors on voting intention and choices in the 2014 mayoral election in Taipei, Taiwan. Results indicate that explicit political party preferences had the largest impact on voting intention and choices. Moreover, implicit political party preferences interacted with explicit political party preferences in accounting for voting intention, and in turn predicted voting choices. Ethnic identity and perceived voting intention of significant others were found to predict voting choices, but not voting intention. In sum, to the comfort of democracy, voters appeared to engage mainly explicit, controlled processes in making their decisions; but findings on ethnic identity and perceived voting intention of significant others may suggest otherwise. Competing Interests: The authors have declared that no competing interests exist. Introduction Throughout history, human rationality–the ability to behave in order to maximize the achievement of their presumed goals (i.e., their optimal choices [1])–has fascinated scholars studying human cognition in myriad fields (e.g., economics, sociology, psychology, political sciences). It is also a foundation of the democratic systems. That is, voting is largely considered to be a “deliberate act” [2]: The ability and capacity of individuals to vote with their preferences after deliberation, without having their voting choices (i.e., actual votes) forcibly restricted by external forces, are crucial to the well-being of any democracy [2–3]. In our research, we investigate human rationality in decision-making processes for situations as complicated and consequential as political elections. Understanding how voting decisions in elections may be impacted allows us to evaluate human rationality and the potency of democratic systems. The degree to which we can make decisions rationally has long been debated. On the one hand, many economists have adopted a utilitarian concept of rationality and have argued for comprehensive rationality stance: Human decisions are made in order to maximize the net benefits of the decisions [4]. On the other hand, researchers studying human cognition have found evidence for a bounded rationality stance: The ability to make decisions is constrained by environmental restrictions and human capacities [4–6]. However, carefully examining the evidence for the rationality debate has revealed that there are different operational definitions for rationality. For instance, researchers may define optimization differently, depending on the fields in which they investigate rationality. Researchers may rely on mathematical rules (e.g., expected values [7]) or individuals’ subjective views (e.g., preferences in rational choices theory [8], purposes [9], consistency [10]). Although mathematical rules are a common operationalization of rationality, not every choice could be or should be defined in numerical terms. Therefore, in this study, we define rationality as the ability to behave so that the likelihood of achieving one’s goals is maximized [6, 8]; that is, one behaves in accordance with one’s intentions to achieve a particular goal. If environmental cues and human capacities affect one’s choices outside of one’s intentions, the evidence suggests that one’s rationality is bounded. For example, a person may consider environmental sustainability the most important issue and national security a non-issue when evaluating candidates. As a result, he intends to vote for Candidate A, whose platform includes increased protections for the environment (i.e., voting intention). However, on voting day, he votes for Candidate B instead (i.e., voting behavior), because the situational cues (e.g., a recent terrorist attack) have prompted his fears regarding national security; he was therefore persuaded by Candidate B’s promises to prioritize national security above all. His vote is not rational, but is bounded by his fears about national security. Thus, we consider an inconsistency between voting intention and voting behavior to be an indication of bounded rationality; consistency between voting intention and voting behavior, on the other hand, would be evidence for comprehensive rationality (e.g., if the person in the above example follows through with his intentions and votes for Candidate A). Independently, factors previously found to affect one’s voting choices (e.g., political party preference) are not necessarily indicators of comprehensive rationality or bounded rationality. If these factors successfully impact both people’s intentions and behavior, they could be considered as support for the comprehensive rationality stance on human cognition. If not, these factors could be considered as evidence for the bounded rationality stance. We aim not only to provide insight into the comprehensive rationality versus bounded rationality debate, but also to evaluate the relevancy of the two types of human rationality in an actual election. Previous research has found two primary factors in accounting for voting behavior in Taiwan: political party preference and ethnic identity (see [11] for a review). The Taiwanese political parties can be classified into two categories. The pan-Blue political parties, dominated by the Kuomintang Party (KMT), are largely considered to be more supportive of a closer relationship with the People’s Republic of China [12]. The pan-Green parties, dominated by the Democratic Progressive Party (DPP), are generally thought to be more supportive of Taiwanese independence [12]. Historically, Taipei–the capital of Taiwan–has been a KMT stronghold. However, social events and controversies in the recent years have led to rising public sentiment against the KMT-led government (culminating in the 2014 Sunflower Movement, which saw mass protests in Taipei [13–14]). As a result, during the 2014 Taipei mayoral elections period, support for Wen-Je Ko, an independent candidate perceived to be representing pan-Green interests, grew rapidly; Ko proceeded to successfully challenge the KMT candidate, Sean Lien. Because elections are often driven by political parties in Taiwan, we separately examined explicit and implicit political party preferences. Explicit political party preferences for either the pan-Blue or pan-Green camps have been found to be strongly related to voting intention [15] and choices [16], consistent with the comprehensive rationality stance. Explicit political party preferences have also been found to predict voters’ choices elsewhere [17–19]. Accordingly, our first hypothesis was that explicit political party preference should significantly predict voting intention and choices. That is, when respondents favor the DPP over the KMT, they should be likely to express an intention and actually vote for the candidate Ko, and vice versa if they favored the KMT over DPP (expressing an intention and subsequently voting for Lien). We focused on the more general explicit political party preferences, rather than evaluations of the specific candidates, partisanship, or party identification for three reasons. First, we did not target evaluations of the specific candidates to avoid conceptual conflation with voting intention. Second, we did not target partisanship or party identification because a good proportion of Taiwanese voters (e.g., about 40%) often do not reveal their partisanship [20], especially when their parties become unpopular [21]. Third, less educated Taiwanese people tend not to consider themselves partisans [21]. If the general explicit political party preferences of voters predict both their voting intention and choices, these results support the comprehensive rationality stance. In addition to explicit political party preference, we examined implicit political party preference. The impact of implicit political party preference on voting intention and choices is more difficult to predict. Galdi and colleagues [22] examined voting intention during an election to determine whether a U.S. military base located in Vicenza, Italy should be expanded. The explicit attitudes regarding the base expansion enlargement best predicted decided respondents’ voting intentions. However, the implicit attitudes regarding the base expansion enlargement best predicted the voting intentions of respondents who stated that they were undecided on the issue. Conversely, when examining voting intention and choices in the 2006 Italian national elections, Roccato and Zogmaister [23] found that although explicit voting intention was the most important predictor for voting choices, respondents’ explicit and implicit political party preferences had separate influences. Specifically, the more respondents favored their preferred political party, explicitly or implicitly, the more likely they intended to vote for the candidate belonging to that party. The researchers also found that when the explicit and implicit political party preferences of the respondents were inconsistent with each other, respondents took more time to make a decision. Thus, based on the available literature, it is difficult to determine the precise role of implicit political party preference (and the degree of rationality involved) in voting intention and choices. Therefore, we explore the relationship of implicit political party preference with both voting intention and choices. If implicit political party preference can predict both voting intention and voting choices, this evidence would lend support to the comprehensive rationality stance; if implicit political party preference predicted voting choices but not voting intention, then the evidence would provide support to the bounded rationality stance. In addition to political party preferences, the shift in public opinion before and during the election campaign period in Taipei meant that other factors were also likely to have a sizable impact on voting intention and choices. Thus, we examined two other factors: ethnic identity and the perceived voting intention of significant others. Ethnic identity (i.e., whether respondents identify as Taiwanese or Chinese) has been found to predict voting choices in Taiwanese presidential elections, in that voters who identify as Taiwanese are more supportive of a pan-Green candidate while those who identify as Chinese are more supportive of a pan-Blue candidate [24]. Because cross-strait issues should be less prominent in the Taipei mayoral election (compared to a nation-wide presidential election), we expect that if ethnic identity should have an effect, its impact would be in line with the bounded rationality stance. That is, although individuals may not deliberately consider their ethnic identity when voting, they may be more likely to vote for the candidate Ko (the DPP-leaning candidate) if they identify more strongly as Taiwanese. The fourth and final factor we examined is the impact of people who are significant to an individual (i.e., one’s significant others). Taiwan is a society that emphasizes social relationships [25–26]; thus, significant others may impact respondents’ voting intention and subsequent behavior. There is evidence that discussion with other people can lead to shifts in one’s voting choices (e.g., in the U.S. presidential elections [3, 27]). Therefore, we examined the impact of the perceived voting intention of significant others on voters’ intention and choices. If the perceived voting intention of significant others predicted both voters’ intention and subsequent choices, the results would support the comprehensive rationality stance. Conversely, if the perceived voting intention of significant others predicted only voters’ choices but not intention, the bounded rationality stance would be supported. To summarize, we examined four predictors that are likely to be key in predicting voting intention and voting choices, focusing on the 2014 Taipei mayoral elections: (a) explicit political party preference, (b) implicit political party preference, (c) ethnic identity, and (d) the perceived voting intention of significant others. To our knowledge, our study is the first to systematically examine human rationality by investigating the impact of these four predictors on both voting intention and choices. The inclusion of all four predictors is crucial because doing so allows us to assess whether voters incorporate different types of information into their decisions. The inclusion of both voting intention and choices allows us to assess whether or not voters’ behavior reflect their intentions, which in turn allows us to evaluate the rationality of these individuals’ cognitive processes. If all four predictors were associated with voting intention and choices, the results would suggest that voters are capable of marshalling all relevant information to make their decisions before and during the voting period. However, if the predictors were associated with voter choices but not intention, it is possible that voters may experience some cognitive limitations when considering the information presented to them before they vote, thus casting doubt on our capability for comprehensive rationality. Materials and Method Ethic Approval This research was supported by a grant (MOS 103-2221-E-004-007-MY3) to one of the authors (A. L. P. C.) and by financial assistance from the Research Center for Mind, Brain, and Learning, National Chengchi University. The research was approved by the Research Ethics Committee, National Taiwan University (NTU-REC No. 201402EM023). Participants provided written informed consent before they began the study. Participants We targeted residents from all twelve districts in Taipei, Taiwan. We advertised our study using various social media platforms (i.e., Facebook and the Bulletin Board System, platforms that are popular in Taiwan) and through personal connections. In total, 124 respondents (64 males) were recruited. Respondents were eligible voters, and 86.3% were young adults (i.e., adults younger than 40 years of age). Most of the respondents (40.3%) did not indicate any party identification; 38.7% identified with the DPP party; and 19.4% identified with the KMT party. The majority of the respondents (71.0%) had voted in the 2012 presidential election, the last major election in Taiwan. Procedure and materials Approximately one month before the 2014 mayoral elections in Taipei, respondents were invited by phone to complete a survey and an implicit association test (IAT) at a public university in Taipei. The survey (see items in Table 1) asked participants to answer questions on their party identification, explicit political party preference, ethnic identity, and voting intention. Participants also had to report the perceived voting intention for their significant others. To provide a parallel comparison with implicit political party preference (as measured by the IAT), explicit political party preference was calculated by contrasting the respondents’ preference for the DPP over the KMT; that is, the higher the survey scores, the more the respondents preferred the DPP over the KMT. Voting intention and perceived voting intention of Table 1. Measurements and reliabilities in the survey (translated from Chinese). Constructs Example items Political party ID (2 items) 1. Currently, there are the following major parties in our country: the Kuomintang, the Democratic People’s Party, the People First Party, the New Party, and the Taiwan Solidarity Union Party. Which party are you inclined to support? 2. To what degree are you inclined to support your chosen party? Explicit political party preference (2 If 0 represents “strongly dislike” and 10 represents “strongly like,” items)[a,b] how would you score the two main national parties? 1. KMT: ______ 2. DPP: _____ Ethnic ID (one item)[c] In our society, some people identify themselves as Chinese, and some people identify themselves as Taiwanese. How would you identify yourself? Voting intention (2 items)[a] 1. If you will vote in the upcoming elections, which mayoral candidate are you more likely to vote for? 2. How sure are you about your voting intent? Perceived voting intention of 1. Please think about a close family member or friend who has significant others (2 items) [a] the most impact on you in terms of politics. Which mayoral candidate is the person likely to vote for? 2. How sure is your family member or friend about their voting intent? aEach item taps different aspects of the construct; thus, no reliabilities were calculated. bExplicit political party preference was calculated by contrasting the respondents’ preference for the DPP over the KMT. cCoded 3 for Taiwanese only, 2.5 for Taiwanese priority, 2 for equally half, 1.5 for Chinese priority, and 1 for Chinese only. doi:10.1371/journal.pone.0148643.t001 significant others were calculated using the same rationale. Voting intention was measured by two items, one item for their intended candidate and one item for the certainty of such a decision. Intended candidate was coded as follows: 1 for the candidate Ko, -1 for the candidate Lien, and 0 for all other candidates. The strength of voting intention was calculated by multiplying the intended choice with the degree of certainty. Similarly, the perceived voting intention of significant others was estimated by multiplying the perceived significant other’s intended candidate choice (coded the same way as for the participant’s intended candidate) with the perceived certainty of the significant other. Higher scores indicate a preference for Ko over Lien, as well as stronger certainty. Following the survey, respondents took a political party preferences IAT (see S1 Appendix for a detailed description of how the test was developed). The IAT tapped respondents’ implicit associations of valence with the main political parties in Taiwan, the KMT and DPP, by measuring how positive and negative words may reduce or prolong the reaction time of stimuli representing either party. We calculated the resulting D-scores so that higher D-scores indicated a stronger preference for the DPP (i.e., DPP = good). The order of the blocks (i.e., Blocks 3 and 5; see S1 Appendix) within the IAT did not affect respondents’ implicit political party preference scores (p = .88). One week after the conclusion of the Taipei mayoral elections, respondents were contacted again by phone to report their actual voting choice. Responses were coded the same way as in intended candidate: 1 for the candidate Ko, -1 for the candidate Lien, and 0 for all other candidates. Results Descriptive analysis On average, respondents explicitly stated that they favored the DPP over the KMT (MD = 1.28, SD = 4.02), t(122) = 3.52, p = .001, but they showed no implicit preference for either party (M = 0.01, p = .82). The explicit political party preferences were consistent with respondents’ party identity. Participants who identified with the DPP showed the most explicit preference toward the DPP over KMT (M = 4.38, on a scale of -10 to 10). By contrast, participants who identified with the KMT and participants who did not identify with a party showed significantly less explicit preference for the DPP (M = -4.08 and M = 0.88, respectively), all pairwise contrasts at ps < .01. Furthermore, implicit political party preferences were also consistent with respondents’ party identity, with the DPP identifiers showing the strongest implicit preference toward the DPP (M = 0.29), no-party identifiers showing little preference for either party (M = 0.02), and KMT identifiers showing a preference for the KMT (M = -0.58), all pairwise contrasts at ps < .01. The majority of the respondents identified as Taiwanese only (72.7%). On a scale of 1 (Chinese only) to 3 (Taiwanese only), respondents scored M = 2.60, SD = 0.67. Most of the respondents (83.6%) indicated that they would vote in the mayoral election, and 69.4% indicated that they would vote for the candidate Ko. Participants were very certain of their decision before the election (on a scale from 0 to10, with 0 indicating strong uncertainty and 10 indicating strong certainty; M = 8.22, SD = 2.69). Over half (56.4%) also reported that their significant other would vote for the candidate Ko, and that these significant others were also very certain of their choices (M = 8.65, SD = 2.25). To examine the degree to which the four predictors–explicit political party preference, implicit political party preference, ethnic identity, and perceived voting intention of significant others–can predict voting intention and choices, we first ran hierarchical regression analyses on voting intention, as well as on choices, respectively. In the first step, we entered respondent gender and education level (with a STEPWISE method). In the second step, we entered the explicit and implicit political party preference scores (with an ENTER method). Finally, we entered ethnic identity, perceived voting intention of significant others, as well as the interaction of the explicit and implicit political party preferences (with a STEPWISE method). Analyses for voting intention Respondents’ explicit political party preferences predicted their voting intention, standardized B = 0.49, p < .001, suggesting that the more respondents favored the DPP over the KMT, the more likely they expressed an intention to vote for the candidate Ko. There was also a significant interaction between explicit and the implicit political party preferences, B = -0.22, p = .001. As seen in Fig 1, there was a steeper slope of respondents with low implicit DPP preference compared to those with high implicit DPP preference. Respondents who had consistently both low explicit and implicit DPP preferences were least likely to express an intention to vote for the candidate Ko. Respondents who had high explicit DPP preference were most likely to express an intention to vote for the candidate Ko, regardless of their implicit DPP preferences. The perceived voting intention of significant others, B = 0.15, p = .047, and implicit DPP preference, B = 0.13, p = .07, also predicted respondents’ intention to vote. The more respondents perceived significant others’ intent to vote for the candidate Ko, as well as the more Fig 1. Respondents’ explicit and implicit political party preferences (DPP over KMT) in predicting voting intention. Solid diamond: low implicit DPP preference. Solid square: high implicit DPP preference. doi:10.1371/journal.pone.0148643.g001 Fig 2. Respondents’ explicit and implicit political party preferences (DPP over KMT) in predicting voting behavior. Solid diamond: low implicit DPP preference. Solid square: high implicit DPP preference. doi:10.1371/journal.pone.0148643.g002 respondents preferred DPP implicitly, the more likely (marginally for implicit preference) they expressed an intention to vote for candidate Ko. No other variables were significant, and the model accounted for 54.8% of the variance. Analyses for voting choice The explicit political party preference of respondents, B = 0.22, p = .028, and an interaction between explicit and implicit political party preferences, B = -0.21, p = .01, significantly predicted their voting choices (see Fig 2), replicating the findings in the analyses for voting intention. Ethnic identity, B = 0.23, p = .007, and the perceived voting intention of significant others, B = 0.19, p = .031, were also found to be robust predictors. The interpretations of the effects of explicit political party preferences, the interaction between explicit and implicit political party preferences, and the perceived voting intention of significant others on voting choices were identical to those on voting intention. Respondents who identified themselves as Taiwanese were more likely to vote for the candidate Ko compared to respondents who identified themselves as Chinese. No other variables were significant, and the model accounted for 39.0% of the variance. Path model analysis: putting pieces together We conducted a path model analysis with a bootstrapping method (see Fig 3 and the correlation matrix in S2 Appendix). Based on the findings in the regression analyses, explicit political party preferences, implicit political party preferences, the interactions between the two types of party preferences, and the perceived voting intention of significant others were proposed to Fig 3. A path model in voting intention and choice. doi:10.1371/journal.pone.0148643.g003 predict voting intention and choices, respectively. Ethnic identity was proposed to predict voting choices. Because ethnic identity and different levels of political party preferences were often intertwined in political discourse, the error terms of ethnic identity, explicit political party preferences, implicit political party preferences and the interaction term of the two kinds of political party preferences were covaried. Lastly, respondents’ explicit political party preference may affect with whom they would like to be associated. Thus, a path from explicit political party preference to the perceived voting intention of significant others was drawn. When fitting the model with the participants’ responses, the model has a good fit, χ2 = 8.82, p = .27; CFI = 0.99, RMSEA = 0.046. The paths are listed in Table 2. Consistent with the findings from hierarchical regression modeling, the explicit political party preferences of the respondents and the interaction between explicit and implicit political party preferences predicted voting intention. Once voting intention was taken into account, respondents’ explicit political party preference and the interaction term were no longer significant predictors for voting choices. Thus, voting intention served as the complete mediator Table 2. Paths in the model predicting voting intention and choice: Standardized coefficients. Paths Estimate Predicting voting intention Explicit political party preference ! Voting intention .49***** Implicit political party preference ! Voting intention .10 Interaction b/w explicit & implicit preferences ! Voting intention -.18**** Perceived voting intention of significant others! Voting intention .11 Predicting vote choice Voting intention ! vote choice .37***** Ethnic (Taiwanese) identity ! Vote choice .24**** Perceived voting intention of significant others! Vote choice .18** Other paths Ethnic (Taiwanese) identity $ Explicit political party preference .48***** Ethnic (Taiwanese) identity $ Implicit political party preference .37***** Ethnic (Taiwanese) identity $ Interaction b/w explicit & implicit preferences -.31***** Explicit political party preference $ Implicit political party preference .47***** Explicit political party preference $ B/W explicit & implicit preferences -.32***** Implicit political party preference $ B/W explicit & implicit preferences -.17+ Explicit political party preference ! Perceived voting intention of significant others .50***** *: p < .001, : p < .01, : p < .05, +: p = .06. doi:10.1371/journal.pone.0148643.t002 between (a) the explicit political party preferences to voting choices, Sobel Z = 3.67, p = .0002, and (b) between the interaction of implicit and explicit political party preferences to voting choices, Sobel Z = -2.43, p = .015. Lastly, the associations of ethnic identity and perceived voting intention of significant others on voting choices remained significant after controlling for voting intention. Supplementary analysis We tested whether an interaction between explicit and implicit political party preferences may account for reticent or undecided voters, as was reported in Roccato and Zogmaister [23]. Following Roccato and Zogmaister’s conceptualization of reticent and undecided voters, participants who refused or were unable to indicate their intended candidate (coded 1 as reticent or undecided) were separated from those who indicated their intended candidate (coded 0). Indeed, a logistic regression model showed that when participants had lower explicit political party preference for the DPP, B = -1.23, p = .001, lower implicit political party preference for the DPP, B = -0.70, p = .037, and when their explicit and implicit political party preferences were inconsistent with one another, B = -1.15, p = .007, they were less likely to have made a voting decision when questioned prior to the elections (see Fig 4). General Discussion The present study weighs evidence for the comprehensive rationality stance and the bounded rationality stance on human cognition through an examination of voting intention and choices in the 2014 Taipei mayoral elections. In this regional election, we found evidence for Fig 4. Respondents’ explicit and implicit party preference (DPP over KMT) in predicting undecided/reticent voters before election (coded 1 for undecided/reticent voters; and 0 for decided voters). Solid diamond: low implicit DPP preference. Solid square: high implicit DPP preference. doi:10.1371/journal.pone.0148643.g004 comprehensive rationality: Taiwanese voters were largely rational and deliberative, with explicit political party preferences serving as the best predictor of their voting intentions, which in turn served as the best predictor of their voting choices. In addition, voting intention mediated the effects of explicit and implicit political party preferences on voting choices. Our evidence suggests that although implicit attitudes are often treated as a challenge to human rationality [5], this may not be the case [19]. We replicated Roccato and Zogmaister’s findings [23], demonstrating that when participants’ explicit and implicit political preferences did not ally, participants were less likely to express their intended candidates. In other words, taken together with the results from Roccato and Zogmaister [23], the impact of implicit preferences on voting appears to be present across different cultures, which may signal to individuals that they need to process information more cautiously when deciding for whom to vote. However, we also found evidence to support bounded rationality. The voting choices of our participants were affected by their ethnic identity, even though it was not directly associated with the voting intentions of the participants. There are several plausible explanations for these findings. First, voters may not be aware of the impact of ethnic identity on their decisions. Alternatively, political candidates (e.g., Ko, Lien) may have exerted their efforts in the final days prior to the day of the election to sway the decisions of participants, appealing indirectly to voters’ ethnic identity through their campaigns. Finally, participants may have actively denied the possibility of ethnic identity exerting an influence when reporting on their voting intention, because they found it irrelevant to the mayoral election; but when they actually voted, they were unable to resist the impact of their ethnic identity. Given that an individual’s social network is often composed of social ingroup members (e.g., those who belong to the same ethnicity), it is perhaps unsurprising that ethnic identity has been found to predict political activities, such as voting preference among Latino populations in the U.S. [28–29] and political involvement among immigrants in Germany [30]. Ethnic identity has often been treated as a group marker in which voters opt for candidates who are members of the same ethnic group. Graves and Lee [28] delineated a theory of ethnic voting, stating that ethnicity may affect voters’ partisanship and candidate evaluation, which in turn affects their voting preferences. That is, voters may favor a political party and candidates endorsed by their ethnic members (e.g., the Latino population typically supports the Democratic Party and its candidates in the U.S.). Voters may evaluate candidates from their ethnic group more favorably than those from different ethnic groups, perceiving these candidates to be more supportive of issues related to their ethnic group (e.g., Latino political candidates may have a more lenient view on immigration). Additionally, ethnic identity may be viewed as a politicized collective identity [31]. That is, in a society where social power is divided along ethnic lines, members of subordinate ethnic groups may collectively blame an outgroup (or outgroups) for their groups’ predicament. As a result, these group members may engage in political activities to enact change. Our findings suggest that the impact of ethnic identity may be more in line with bounded rationality, as its effects were not predicted by one’s voting intention prior to the election. Thus, it is plausible that individuals vote for a candidate who shares the same ethnic (e.g., Taiwanese) identity without consciously deciding to do so. If individuals vote in blind support of their ethnic ingroup or in opposition of ethnic outgroups, their decisions will not involve substantial systematic processing, potentially undermining the principles and effectiveness of the democratic system. Lastly, according to the path model, the voting choices of our participants were affected by the perceived voting intention of their significant others, but it was not associated with voting intention after all other variables were simultaneously controlled. The finding suggests that the association between voting intention and perceived voting intention of significant others may be pseudo. Previous research has shown that significant others may affect one’s political behavior, such as voter turnout [32–36], voting consistency [37], and voting choices [35, 38]. The effects of significant others are often conceptualized as a rational act. Significant others may enforce a participation norm so people are more likely to vote [32, 36]. Through discussion, people may gain political information and knowledge from their significant others and may be more likely to vote as a result [34, 36]. Previous research has also shown that when people are in a social network that favors their preferred candidates, they are more likely to vote for them [35–36]. Schmitt-Beck [38] also found that significant others may serve as a filter to reinforce or block media information on the candidates. Thus, it appears that voting is not an individual choice but is embedded in one’s social network. However, in our research, the evidence of perceived significant other’s voting intention may be more in line with bounded rationality, as its effects on voting choices were not mediated by one’s voting intention prior to the election. Our findings suggest that there are ways that significant others may affect us that have not been recognized before. More research investigating the impact of significant others beyond individuals’ awareness will be needed to further understand voters’ behavior. In summary, although the explicit and implicit political preferences of individuals do exert a powerful influence on their voting intention and behavior, other factors–the ethnic identity and the perceived voting intention of their significant others–play important roles as well. There is a need to explore the influence of social groups and significant others, especially in different types of political systems. Future studies should further examine the degree to which relatively sensitive issues (e.g., ethnic identity) and mainstream cultural values (e.g., the extent of other people’s influence in collectivistic societies and in individualistic societies, such as Taiwan and the U.S. respectively) affect the results of an election. Overall, our findings indicate that although the effectiveness of a democracy should rely on the rational and careful decisions of individual voters, actual democratic societies are imperfect systems, shaped by the issues faced by various social groups within the system and by the mutual impact their citizens have on one another. When looking towards the future of democracy, both in Chinese societies and beyond, we must acknowledge the impact of these factors and incorporate our understanding into the way we educate and motivate the citizens who form the fabric of democratic societies across the world. Supporting Information S1 Appendix. The Development of the Political Party Preferences Implicit Association Test (IAT). (DOCX) S2 Appendix. The correlation matrix of the variables in the path model (n = 124)[a]. [a]missing data were estimated by interpolation method. [b]standardized scores in the correlations and raw scores in the means and standard deviations. [��]: p < .001, [+]: p < .08. (DOCX) Author Contributions Conceived and designed the experiments: ICL EEC CHT NSY ALPC. Performed the experiments: WCL. Analyzed the data: ICL WCL. Contributed reagents/materials/analysis tools: ICL CHT NSY ALPC. Wrote the paper: ICL EEC CHT NSY. References 1. Tsebelis G. Nested games: Rational choice in comparative politics. Berkeley, CA: University of California Press; 1990 2. Friese M, Bluemke M, Wänke M. Predicting voting behavior with implicit attitude measures: The 2002 German Parliamentary Election. Exp Psychol 2007; 54: 247–255. doi: 10.1027/1618-3169.54.4.247 PMID: 17953144 3. Beck PA, Dalton RJ, Greene S, Huckfeldt R. The social calculus of voting: Interpersonal, media, and organizational influences on presidential choices. Am Political Sci Rev 2002; 96, 57–73. doi: 10.1017/ S0003055402004239 4. Jones BD. Bounded rationality. Annual Rev Pol Sci 1999; 2: 297–321. doi: 10.1146/annurev.polisci.2. 1.297 5. Kahneman D. A perspective on judgment and choice: Mapping bounded rationality. Am Psychol 2003; 58: 697–720. doi: 10.1037/0003-066X.58.9.697 PMID: 14584987 6. Simon H. Administrative behav. 4th ed. New York: Free Press; 1997. 7. Quattrone GA, Tversky A. Contrasting rational and psychological analyses of political choice. Am Political Sci Rev 1988; 82: 719–736. doi: 10.2307/1962487 8. Aldrich JH. Rational choice and turnout. Am J of Political Sci 1993: 37, 246–278. doi: 10.2307/2111531 9. Lindenberg S. The method of decreasing abstraction. In: Coleman J. S., & Fararo T. J., editors. Rational choice theory: Advocacy and critique. London: SAGE; 1992. p. 3–20. 10. Zafirovski M. What is really rational choice? Beyond the utilitarian concept of rationality. Current Soc 1999; 47: 47–113. doi: 10.1177/0011392199047001005 11. Achen CH, Wang TY. (in press). The Taiwan voter: An introduction. In: Achen CH, & Wang TY, editors. The Taiwan voter. Taipei, Taiwan: The ECP. 12. Hsieh JFS, Niou EMS. Issue voting in the Republic of China on Taiwan’s 1992 legislative yuan election. Int Political Sci Rev 1996; 17: 13–27. doi: 10.1177/019251296017001002 13. Li L. Activists, gov't must cut through sound and fury in protest. China Post. 20 Mar 2014, Available: http://www.chinapost.com.tw/taiwan/analysis/2014/03/20/403276/Activists-gov't.htm. Accessed 4 June 2015 14. Wang C. (2014). Opposition, groups protest trade pact. Taipei Times. 19 Mar 2014, Available: http:// www.taipeitimes.com/. Accessed 3 June 2015 15. Tsai CH. Electoral accountability: a case study of Taiwan’s 2009 county and city mayoral election. Rev of Soc Sci 2012; 6: 35–67. 16. Tsai CH. Party voting in comparative perspective: The United States, Taiwan, and Japan. Ph.D. dissertation, Ohio State University, Columbus, Ohio. 2003. 17. Bartel LM. The study of electoral behavior. In: Leighley J-E, editor. The Oxford handbook of American elections and political behavior. New York: Oxford University Press; 2010. p. 239–261. doi: 10.1093/ oxfordhb/9780199235476.003.0014 18. Campbell A, Converse PE, Miller W, Stokes D. The American voter. Chicago: The University of Chicago Press; 1960. 19. Miller WE. (1991). Party identification, realignment, and party voting: Back to the basics. Am Political Sci Rev 1991; 85: 557–568. doi: 10.2307/1963175 20. The Election Study Center of National Chengchi University. Changes in the party identification of Taiwanese as tracked in surveys by the Election Study Center, NCCU; 2014. Database: figshare. Available: http://esc.nccu.edu.tw/course/news.php?Sn=165 21. Tsai CH, Chao SC. Nonpartisans and party system of Taiwan: Evidence from 1996, 2000, and 2004 presidential elections. J of Asian and Afr Stud 2008; 43: 615–641. doi: 10.1177/0021909608096657 22. Galdi S, Arcuri L, Gawronski B. Automatic mental associations predict future choices of undecided deci sion-makers. Sci 2008; 321: 1100–1102. doi: 10.1126/science.1160769 23. Roccato M, Zogmaister C. Predicting the vote through implicit and explicit attitudes: A field research. Political Psychol 2010; 31: 249–274. doi: 10.1111/j.1467-9221.2009.00751.x 24. Wu CL, Tsui HC. Ethnicity, Empowerment, and Electoral Evaluation: Taiwanese-Mainlander Differences in the 2004 and 2008 Presidential Elections. Taiwan Democracy Q 2010; 7: 137–182. 25. Hofstede G. Masculinity and femininity: The taboo dimension of national cultures. London, United Kingdom: Sage Publications; 1998. 26. Hofstede G. Geert HofstedeTM cultural dimensions. Available: http://www.geert-hofstede.com/. Accessed 18 July 2008. 27. Huckheldt R, Sprague J. Network in context: The social flow of political information. Am Political Sci Rev 1987; 81: 1197–1216. 28. Graves S, Lee J. Ethnic underpinnings of voting preference: Latinos and the 1996 U.S. senate election in Texas. Soc Sci Q 2000; 81: 226–236. 29. Jackson M. Priming the sleeping giant: The dynamics of Latino political identity and vote choice. Political Psychol 2011; 32: 691–716. doi: 10.1111/j.1467-9221.2011.00823.x 30. Fischer-Neumann M. Immigrants’ ethnic identification and political involvement in the face of discrimination: A longitudinal study of the German case. J of Ethn and Migration Stud 2014; 40: 339–362. 31. Simon B, Klandermans B. Politicized collective identity: A social psychological analysis. Am Psychol 2001; 56: 319–331. doi: 10.1037/0003-066X.56.4.319 PMID: 11330229 32. Glasford D. Predicting voting behavior of young adults: The importance of information, motivation, and behavioral skills. J of Appl Soc Psychol 2008; 38: 2648–2672. doi: 10.1111/j.1559-1816.2008.00408.x 33. Jang SJ. Are diverse political networks always bad for participatory democracy? Indifference, alienation, and political disagreements. Am Political Res 2009; 37: 879–898. doi: 10.1177/ 1532673X09332790 34. Kenny C. The microenvironment of political participation. Am Politics Res 1993; 21: 223–238. doi: 10. 1177/1532673X9302100204 35. Knoke D. Networks of political action: Toward theory construction. Soc Forces 1990; 68: 1041–1063. doi: 10.1093/sf/68.4.1041 36. Schmitt-Beck R, Mackenrodt C. Social networks and mass media as mobilizers and demobilizers: A study of turnout at a German local election. Electoral Stud 2010; 29: 392–404. doi: 10.1016/j.electstud. 2010.03.011 37. Ikeda K, Liu JH, Aida M, Wilson M. Dynamics of interpersonal political environment and party identification: Longitudinal studies of voting in Japan and New Zealand. Political Psychol 2005; 26: 517–542. doi: 10.1111/j.1467-9221.2005.00429.x 38. Schmitt-Beck R. Mass communication, personal communication and vote choice: The filter hypothesis of media influence in comparative perspective. Br J Political Sci 2003; 33: 233–259. doi: 10.1017/ S0007123403000103 Election polls are 95% confident but only 60% accurate Author: Kotak, Aditya, Date: 2022-10-01 Collections: PollMethods Zotero Key: L7MS6Y9U Cite Key: Kotak22ElectionPollsAre Zotero Item | Lit Note finding Election polls are 95% confident but only 60% accurate Aditya Kotak & Don A. Moore [] abstract Election polls in the United States are more confident than accurate— meaning the reported margins of error often do not encompass the actual election outcomes in spite of pollsters claiming a 95% confidence level (that is, a 95% chance that their predictions will fall within the margin of error). In an analysis of polls for more than 6,000 contests, we have found that the actual vote total for a given candidate fell within the 95% confidence interval for just 60% of the polls. This degree of accuracy was reached only when the polls were conducted in the week before an election; accuracy was worse for polls conducted earlier. Polls would, in fact, need margins of error at least twice their current standard reported width to achieve 95% accuracy. We have also found that when laypeople read about poll results, they tend to overestimate the poll’s accuracy, even when they have historical data demonstrating that the predictions made by polls are often inaccurate. These results illustrate polls’ vulnerability to overconfidence and the limitations of the lay public’s understanding of these shortcomings. We conclude by suggesting ways that pollsters and reporters could enable the public to interpret poll data more realistically. Kotak, A., & Moore, D. A. (2022). Election polls are 95% confident but only 60% accurate. Behavioral Science & Policy, 8(2), 1–12. a publication of the behavioral science & policy association 1 n 2016, the world was stunned by a couple of surprising election outcomes. On June 23, 52% of voters in the United Kingdom elected I to leave the European Union. Many eligible voters who supported staying in the European Union did not bother to vote that day, possibly because most of the credible polls forecast a likely victory for the “remain” side.[1] Voter turnout was lowest in areas that most strongly favored remaining in the European Union, such as London, Scotland, and Northern Ireland.[2] Then, in November, Donald Trump beat Hillary Clinton in a closely contested U.S. presidential election. On the eve of the election, poll aggregator Nate Silver’s website, FiveThirtyEight, gave Clinton a 71% chance of winning. [3] Many Democrats who did not vote reportedly believed their votes did not matter.[4] Polls influence more than voter turnout. The Commission on Presidential Debates allows candidates to participate only if polls indicate that they have the support of at least 15% of the electorate.[5] Candidates leading in the polls attract more support, including financial contributions.[6] And polls are powerful drivers of press attention and its notorious “horse race” coverage that focuses on who will win.[7] Polls garner attention not only from the press and the public but also from politicians eager to divine the will of the electorate.[8] Preelection polls have increased in number and frequency since their introduction by George Gallup in 1936.[9,10] Given the power of polls in democracies, all citizens ought to care about their accuracy. In this article, we compare pollsters’ confidence with their accuracy. That is, we report on a project that assessed the extent to which polls’ confidence intervals encompassed the actual outcomes of races. In polling, as in statistical analyses in general, the term confi- dence has a specific meaning. Pollsters report a particular margin of error that quantifies their degree of uncertainty about their prediction. If a pollster reports that 50% of 800 likely voters favor a particular candidate in an upcoming election and claims to have 95% confidence that the poll has a margin of error of ±3.5%, the pollster is also claiming there is a 95% likelihood that the candidate should receive between 46.5% and 53.5% of the vote from the broader electorate. We assessed the match between confidence and accuracy by measuring how often the 95% confidence intervals encompassed the actual election outcomes. We conclude that common reporting formats put too much faith in poll results, and we discuss ways to report election outcomes that could help the public interpret polling data more accurately. Sources of Error Before we describe our studies, we review some factors that can cause polls to be inaccurate. Statistical procedures help to quantify one potential source of error known as sampling error. Sampling error occurs when, despite a researcher’s best efforts, a group of people chosen by random sampling methods may not actually be representative of the population of interest after all. For instance, if pollsters seeking a representative sample of the voting population just happened to reach a preponderance of Clinton supporters ahead of the 2016 election, the finding that 60% of the poll’s respondents favored Clinton probably would not mean that 60% of votes actually cast in the election would be for Clinton. When someone is calculating the confidence interval for an outcome, statistical methods helpfully take into account the possibility that an erroneous prediction may result from a chance failure of random sampling methodology. However, researchers who study survey errors have documented at least five additional sources of error that are more difficult to quantify.[11,12] Specification error is the result of a mismatch between the survey question and the answer it produces. For example, the question “Which candidate is better qualified?” may not predict votes, because not everyone votes for the candidate they believe to be best qualified. Frame error describes the discrepancy between the population sampled and the larger population. For example, respondents who were attending a Clinton rally, even if sampled randomly, would poorly predict the opinions of the broader electorate. Nonresponse error 2 behavioral science & policy | volume 8 issue 2 2022 arises from nonrandom nonresponse. For example, Trump supporters’ suspicion of the mainstream media might have made many of them reluctant to participate in polls and thus could have led pollsters to undercount the likely Trump vote. Measurement error describes bias introduced by the method of measurement. For example, the race of an interviewer might affect respondents’ reported attitudes toward candidates of that race. Finally, data-processing errors can occur in the cleaning or analysis of poll results. Even pollsters who acknowledge the existence of these error sources may have difficulty using that knowledge to provide more realistic confidence intervals.[13] Data-processing errors illustrate the dilemma. Every researcher knows data-processing errors occur and tries to minimize them. To determine the full extent to which data-processing errors have influenced the results of a particular poll, pollsters would have to know which errors they have made; however, if they knew what mistakes they had made, they would have corrected them. The usual response to the difficulty of identifying and quantifying errors is to ignore them. That is, pollsters do their best to minimize these different sources of error and then pretend they have succeeded. The consequence is that reported confidence intervals are likely to be too small:[14] The stated interval will give the impression that the election outcome will be closer to the poll’s result than is actually likely. From a psychological perspective, it is not entirely surprising that pollsters tend to be overly confident that their results will predict the actual election outcome. Overprecision, or being overly certain that one’s judgments are correct, is one of the most pervasive biases; it affects most human judgments, including forecasts. Poll results are not technically forecasts because they capture attitudes at a moment in time, but they are typically interpreted as being forecasts of election outcomes. Research suggests that forecasters routinely act too sure that they know what will happen.[15,16] They confidently underestimate their vulnerability to error and fail to consider all the ways they could be wrong.[17] Philip E. Tetlock conceptualizes approaches to forecasting with the ancient Greek aphorism “The fox knows many things, but the hedgehog knows one big thing.”[18,19] Experts who bring a strong ideological orientation to their work and neatly fit the messy details of reality’s complexity into their organizing narratives are hedgehogs. Foxes, by contrast, are generalists who are less likely to see universal laws and coherent ideologies; they are open to revising their views and accept the possibility that they might be wrong. Foxes consistently make more accurate forecasts than do hedgehogs, suggesting that one useful strategy people can apply to counteract their overconfidence is to consider the possibility that they could be mistaken.[20] Research examining poll accuracy has primarily assessed how closely poll results correspond to the shares of votes that candidates receive in their elections.[8,21] In the first of two studies described below, we sought to add to prior research by comparing poll accuracy against the degree of confidence claimed, thereby more finely assessing how well poll designs are calibrated to reflect reality. In the second study, we examined how much faith the public has in the accuracy of polls and tested whether informing the public of historical inaccuracies alters that faith. Study 1: Polls & Election Forecasts Method We preregistered a plan to investigate whether the confidence intervals stated for election polls reflected the polls’ accuracy in predicting outcomes. By preregistering, we sought to assure readers that we would not selectively report analyses and results. We were able to obtain data from RealClearPolitics (https://www.realclearpolitics.com/) on four election cycles: 2008, 2012, 2016, and 2020. We used data for Democratic presidential primaries in Iowa and New Hampshire during 2008, 2016, and 2020 and data for Republican presidential primaries in the same states during 2012 and 2016. We also used data from polls conducted ahead of the general presidential elections of 2008, 2012, 2016, and 2020. Prior to 2008, the a publication of the behavioral science & policy association 3 data did not consistently include sample sizes. Our preregistration, data, and code are available at https://osf.io/65za7/. We analyzed primary data only for races in Iowa and New Hampshire, which are the earliest in the election cycle, because primaries get more complicated after the ones held in those states. For instance, candidates may drop out of the race between the poll and the primary, making it difficult to assess the accuracy of forecasted vote shares for the absent candidates and changing the competitive landscape for the remaining candidates. The polling data we used can be accessed at the links below. (Print readers, find the links in the online text of this article.) • 2020 Iowa Democratic Presidential Caucus • 2016 Iowa Republican Presidential Caucus • 2016 Iowa Democratic Presidential Nomination • 2020 New Hampshire Democratic Presiden- tial Primary • 2016 New Hampshire Republican Presidential Primary • 2016 New Hampshire Democratic Primary • 2012 Iowa Republican Presidential Caucus • 2012 New Hampshire Republican Presidential Primary • 2008 Iowa Democratic Caucus • 2008 New Hampshire Democratic Primary • 2020 General Election: Trump vs. Biden • 2016 General Election: Trump vs. Clinton • 2012 General Election: Romney vs. Obama • 2008 General Election: McCain vs. Obama In total, we analyzed data for 14 sets of polls— 1,931 polls for election cycles from 2008 to 2020. Because some polls asked about several candidates, the 1,931 polls produced 6,654 vote-share estimates. We recorded the margins of error from all polls that reported them and calculated the others as described in note A. Results The analyses we present in this article are consistent with those we preregistered but proved more informative than the set we initially put forward. See the Supplemental Material for details of the preregistered analyses and visit https://osf.io/keswd/ for the results file. Our primary analysis examined hits—instances when a poll’s 95% confidence interval included the election’s actual result—as a function of time between the poll and the election. We were particularly interested in accuracy over time because we wanted to see whether polls conducted far in advance of an election were generally less accurate than those conducted closer to the election. We grouped the polls into seven-day intervals and averaged the hit rate in each interval to estimate the accuracy of the polls as a function of time to the election. Figure 1 shows that hit rates averaged around 60% in the week prior to the election. A year prior to the election, average hit rates were lower: around 40%. Because most polls ask participants to indicate how they would vote “if the election were held today,” it might be no surprise that accuracy decreases as the distance in time between the poll and the election increases. It is worrisome, however, that even just a few days before the election, the 95% confidence intervals captured the actual vote share only 60% of the time. We calculated how much wider the confidence intervals should have been to achieve 95% accuracy. First, we identified all the misses in a weekly group—the instances in which the true election outcomes fell outside the stated confidence interval. Then, for each week, we calculated how much wider the interval should have been so that 95% of the true election outcomes would have fallen within the confidence intervals. Figure 2 visualizes our findings. A week before the election, reported margins of error would have to have been 2 times wider, on average, to be 95% accurate. A year before the election, margins of error would have to have expanded by more than a factor of 3 to be 95% accurate. 4 behavioral science & policy | volume 8 issue 2 2022 Figure 1. Poll accuracy, by weeks before the election Note. Each dot represents the percentage of polls reported during a given week that proved accurate—that is, that the actual election outcome fell within the stated margins of error for a 95% confidence level. The orange line shows the best fit for the data. In general, poll accuracy increased as the elections drew near; however, at best, only about 60% of the polls proved accurate. The confidence interval (CI) shown on the graph refers to our data. Figure 2. Adjustment to confidence intervals for polls to achieve a 95% hit rate Note. For the polls in Study 1 to have hit their targets—that is, to have encompassed the actual election outcomes within their 95% confidence intervals (CIs)—their confidence intervals would have needed to expand by a factor of 3.5 when the polls were conducted 70 weeks ahead of the election and by a factor of 2 in the week before the election. a publication of the behavioral science & policy association 5 Finally, we also compared election cycles to look for trends over many years. We conducted the same analysis as shown in Figure 1 but segmented the results by year. As Figure 3 shows, we found little evidence that polls have become less accurate over the years. Study 2: What Do Laypeople Think About Polling Accuracy? Given the low rates of poll accuracy, members of the public should be skeptical when reading reports about political polls. Are they? Do they understand that polls are often poor predictors of election outcomes? We delved into these questions in Study 2 and examined the extent to which the amount of time before the election affected faith in poll results. Find our preregistration plan to assess responses to seven different poll reporting styles at https://osf.io/9qhmf. Method We conducted an online survey using participants recruited from Amazon Mechanical Turk.[22] We restricted our sample to residents of the United States, seeking a population that roughly matched the country’s demographics.[23] We opened our survey to 230 people and wound up with 217 complete responses. Data, materials, and code are available at https://osf.io/5wmqe/. We randomly assigned participants to groups that read about a poll result that had been obtained one day, three months, or one year before an election. The survey then presented the poll finding to each participant using seven different reporting styles for the same poll result, in the following order (see note C): • Style 1 consisted of a point estimate: “The poll’s results give one of the candidates 49% of the vote.” • Style 2 consisted of a point estimate paired with a margin of error: “The poll’s results give one of the candidates 49% of the vote with a margin of error of ±3 percentage points.” Figure 3. Poll accuracy, by election year 70% 60% • Style 3 consisted of a point estimate with a margin of error and a 95% confidence interval: “The poll’s results give one of the candidates 49% of the vote with a margin of error of ±3 percentage points for a 95% confidence interval.” 2020 95% CI 2016 2020 2012 2008 50% 40% 30% 20% 70 60 50 40 30 20 10 1 Number of Weeks Until Election Note. Analyses of poll accuracy across four election cycles indicate that despite the concerns of some observers, accuracy— as measured by actual election results falling within a poll’s reported margin of error for a 95% confidence level (CI)—has not declined in recent years. The shaded region shows the 95% confidence interval around the 2020 results. See the Supplemental Material for additional data displays relating to the accuracy of elections by year. 6 behavioral science & policy | volume 8 issue 2 2022 • Style 4 consisted of an interval: The poll s results give one of the candidates between 46% and 52% of the vote.” • Style 5 consisted of an interval with a 95% confidence interval: “The poll’s results give one of the candidates between 46% and 52% of the vote with a 95% confidence interval.” • Style 6 consisted of a point estimate with a margin of error and a 95% confidence interval, as in Style 3, but with the addition of information about the historical accuracy of polls conducted in the time frame specific to the survey group. That is, the data for historical accuracy varied according to the survey group’s time frame. For example, the one-year group’s survey said, “Historically, a year before the election, polls capture the true outcome 35% of the time.” For polls three months before and one day before the election, the figures given were 55% and 60%, respectively. • Style 7 consisted of an interval with a 95% confidence interval, as in Style 5, that was paired with information about historical accuracy that varied with the survey group s time frame, as in Style 6. For each of the seven reporting styles, we asked participants to indicate, on a 0%–100% scale, how sure they were that the election outcome would be consistent with the poll’s forecast. We predicted that participants would have more faith in polls than was justified by the historical accuracy of polls and, in particular, than was justified by the most commonly used approach of reporting a point estimate with a margin of error. Results As predicted, the average level of faith in the polls’ accuracy (M = 59.9%) exceeded the average reported historical accuracy of polls (M = 49.7%), p < 10[−11]. Overall, we saw excessive faith with all reporting styles. (See note B for a discussion of the statistical terms used in this article.) The time horizon and the reporting style each had an effect on the extent of the belief in the polls’ accuracy (p < 10[−13] and p < 10[−7], respectively); however, that faith varied by time horizon and reporting style. (See Figure 4 Figure 4. Faith in poll results, by time horizon & reporting style 70 60 50 40 Point estimate (PE) PE + MoE PE + Interval 95% CI PE + 95% MoE + 95% CI + 95% MoE Accuracy Accuracy Style One Day Historical Accuracy Three Month Historical Accuracy One Year Historical Accuracy One Day Confidence Rates Three Months Confidence Rates One Year Confidence Rates Note. The reporting style refers to how poll results were reported to participants; the x-axis labels describe the distinguishing features of each reporting style, which are defined as follows: Style 1, point estimate, was a poll result reported as a single percentage. Style 2 had the point estimate and a margin of error (MoE) specified. Style 3 included a point estimate, a margin of error, and a 95% confidence interval (CI). Style 4, interval, had the poll result reported as a range of values. Style 5 was an interval with a 95% confidence interval also stated. Style 6 consisted of the point estimate, a margin of error, and a 95% confidence interval, with historical accuracy—the percentage of polls that were accurate in the past at one day, three months, or one year before the election—specified. Style 7 consisted of a range of values with a 95% confidence interval and historical accuracy specified. Confidence rates were the average rating of participants’ faith that the poll will be accurate. Error bars show standard errors for our results. The data indicate that for the most part, participants’ faith in the accuracy of polls conducted one day, three months, or one year before an election exceeded the historical accuracy of polls conducted in the corresponding time frames. Even when participants were told of the historical accuracy of polls, they still overestimated the current poll’s accuracy. a publication of the behavioral science & policy association 7 “Even when informed of the inaccuracy of past polls, participants continued to place excessive faith in the current poll’s ability to predict an election outcome” and the Supplemental Material for more details of the data analyses.) With respect to the time horizon, for instance, participants generally placed their greatest faith in the polls conducted a day before the election. With respect to reporting style, consider the results relating to Style 2 and Style 7. Style 2, one of the most common reporting approaches, uses a point estimate along with a margin of error. For this style, faith in the poll’s result exceeded the historical accuracy of polls to a statistically significant extent only when participants were told that the poll came out a year before the election (p < 10[−11]) but not when they were told the results came out three months, (p = .29) or one day (p = .02) before the election. By contrast, Style 7 came with an explicit warning specifying polls’ historical accuracy. Although we had expected this disclosure to reduce participants’ confidence in the reported poll result, it had surprisingly little effect. The results displayed in Figure 4 underscore both people’s excessive faith in polls’ predictive accuracy and the challenge of correcting their misperception. Providing more information about polls’ poor record of accuracy in Styles 6 and 7 failed to bring the participants’ faith in the polls in line with the polls’ historical accuracy. General Discussion Overview Our analyses showed that the 95% confidence intervals reported for the polls we studied included the actual election result substantially less often than 95% of the time. For 95% confidence intervals to include the true results—in technical terms, to be “calibrated with their hit rates”—they would have to at least double in size. Moreover, variations in how the findings of polls are reported make little difference to people’s perceptions of their accuracy: Even when informed of the inaccuracy of past polls, participants in Study 2 continued to place excessive faith in the current poll’s ability to predict an election outcome—faith that is not justified by the past successes of polls. The only comfort provided by the data—and it is small comfort— is that participants were not totally unaware of the potential flaws in poll predictions. They were not highly confident in the accuracy of the polls; on average, they reported being only 60% confident. One hopes this skepticism might help voters take future poll results with a grain of salt. As we have explained, poll results might deviate from election results for many reasons—such as errors in sampling, specification, frame, nonresponse, measurement, and data processing.[24] The statistics used to generate confidence intervals are easier to adjust for sampling error than for other sources of error, although all result from systematic but difficult-to-assess differences between the people who participate in the polls and the larger population of voters. Because statistical models have trouble quantifying these differences, the confidence intervals they produce are likely to be inaccurate and will contribute to pollsters’ excessive confidence in the accuracy of their polls. Are New Social Trends Affecting Poll Accuracy? FiveThirtyEight was not alone in underestimating the turnout for Donald Trump in the 2016 race against Hillary Clinton. His win prompted speculation that the accuracy of polling was declining.[25,26] It is possible that the 2016 presidential election was affected by a new phenomenon: right-wing voters being unusually reluctant to respond to polls because of a general suspicion of media organizations.[27] If that speculation were accurate, polling errors should be increasing. However, as Figure 3 shows, we did not find that hit rates of the polls conducted in the 2020 election cycle were 8 behavioral science & policy | volume 8 issue 2 2022 lower than in previous years. In a 2018 report, Will Jennings and Christopher Wlezien also found no evidence of a decline in poll accuracy over time.[10] A simpler explanation for the failure of polls to predict Donald Trump’s win is that their unusually poor showing represented an aberration; polls and elections will always include noise. Although many observers criticized Nate Silver and his FiveThirtyEight election forecasting website for giving Hillary Clinton a 71% chance of prevailing on the eve of the 2016 election, Silver sensibly defended himself by noting that events with a 29% probability of occurring do happen. [28] Nevertheless, Trump’s surprisingly strong showing four years later in the 2020 election has underscored concerns that recent polling misses may stem in part from some broad social trend that should be taken into account when polls are designed and interpreted in the future. This concern has been heightened by the flaws of the presidential polls in 2020. Going into the 2020 presidential election, national polls favored Joe Biden by 8 percentage points.[29] In fact, he won by only 4.5%.[27] Poll watchers are now wondering whether the failure to predict Trump’s strong performances in 2016 and 2020 were due to chance or whether they might they be attributable to a “Trump effect,” some mysterious factor related to Trump that systematically disrupts the accuracy of polls that involve him.[30] Also unclear is the answer to the important question of whether the failures reflect longterm trends with implications for future polls. Ongoing Challenges for Pollsters Such questions highlight the challenge of adjusting poll methodologies to account for their shortcomings. In principle, it ought to be possible to adjust polls’ confidence intervals on the basis of their known limitations so that they become more accurate. And, in fact, sophisticated pollsters and poll aggregators have long attempted precisely this.[31] However, these adjustments can take into account only the known discrepancies between the populations sampled by polls and the larger population of people who vote. The list of discrepancies is long and changeable and omits unknown “Voters are making a mistake when they decide not to vote because they believe polling tells them how the election is going to turn out.” discrepancies, making it incomplete. How incomplete? Estimating that incompleteness requires specifying unknown unknowns, an epistemological impossibility. For instance, it is impossible to know what news stories might emerge between a poll and Election Day and how they will affect different voters. The approach we used in the studies reported in this article looks backward, measuring the historical discrepancies between poll results and election outcomes. This approach can provide at least some guidance for estimating the size of such discrepancies. However, unless the past is perfectly representative of the future, it will be an imperfect guide. And, of course, the world is ever changing, with a future course that is never perfectly predictable. The reasons that poll results may differ from election outcomes in the future are not limited to the reasons that explained differences in the past. Action Implications We see several action implications of our findings. First, pollsters should either lower the level of confidence they claim (to a percentage lower than 95%) or expand the size of the confidence intervals they report. If they wish to report confidence intervals that are well calibrated with reality, they ought to widen their confidence intervals substantially. Relatedly, journalists who report on poll data should point out that the true proportion of voters selecting a given candidate is likely to be smaller or larger than the polls predict. In addition, consumers of polling data ought to put less faith in the accuracy of poll results. Voters are making a mistake when they decide not to vote because they believe polling tells a publication of the behavioral science & policy association 9 them how the election is going to turn out. Unfortunately, data from our second study do not provide much reason for optimism with regard to voters’ corrigibility. Even when we provided information about the poor historical performance of polls, participants’ confidence in their accuracy was not dampened sufficiently. Finally, we are skeptical of rules, such as those governing the inclusion of candidates in political debates, that rely exclusively on polling data. Reliance on polls to decide which candidates deserve attention must be tempered with knowledge of the imperfections in polls’ predictive accuracy. We believe, therefore, that decisions relating to debate participation ought to err on the side of including more candidates. Without being able to anticipate why poll results will differ from the actual vote, pollsters will continue to overestimate their accuracy and underestimate their propensity for error. Even pollsters who are willing to admit, in the abstract, that polls are vulnerable to various sources of inaccuracy beyond simple sampling error may be reluctant to discount the accuracy of their own polls. After all, they have done all they can to minimize error and correct for known biases. Voters’ trust that pollsters are doing all they can to correct for sources of error, along with voters’ failure to understand that pollsters do not know all the sources of error, may explain why the participants in our second study were not sufficiently influenced by past inaccuracies in poll results and expressed optimism that the poll results reported to them in the study would be more accurate than polls have proven to be in the past. Beyond Polls An inability to compensate well for uncertainty is, of course, a problem that affects many statistical tools. Economists and sociologists have long been concerned about the issue.[32,33] Sometimes a scientific theory to explain some phenomenon is wrong and applies the wrong statistical model to evaluate data related to the phenomenon. But because scientists cannot test all possible models, they cannot know whether their model is the best one and will come away overconfident in the model’s value. The overconfidence we have identified in pollsters is not a problem unique to them or even to people who test scientific theories. As we explained earlier, overprecision—a form of overconfidence in which people are overly sure that they know the truth—is one of the biases in judgment most resistant to eradication.[16] People can be overconfident in their correctness for many reasons. For instance, they may base beliefs on information that is biased or otherwise imperfect in ways they did not anticipate. And because it is difficult for us humans to consider information we do not know, it is easy for us to overestimate the accuracy of our beliefs. The results we report in this article suggest that overconfidence in one’s judgments routinely afflicts not only pollsters but also the people who interpret those reports. We fear that the public and candidates will continue to be ill served if polls continue to be conducted and reported in the same old ways. endnotes A. The size of a reported margin of error in a poll or study is determined by the confidence level, the sample variance (that is, the spread of responses from participants), and the sample size. We found that the most common margin of error reported was 1.96 standard deviations around the poll result, assuming a normal distribution in sampling error. (A normal distribution is symmetric about the mean.) That is, the most common 95% confidence interval reported was the range going from 1.96 standard deviations below the poll result to 1.96 standard deviations above it, which translates to 3.5 percentage points above and below the poll result for a poll with 800 respondents. For polls that did not report a margin of error for a 95% confidence level, we used the poll’s result and sample size to compute a confidence interval of ±1.96 standard deviations around the result. (See note B for further discussion of the statistical terms used in this article.) B. Editors’ note to nonscientists: For any given data set, the statistical test used—such as the chi-square (χ[2]) test, the t test, or the F test—depends on the number of data points and the kinds of variables being considered, such as proportions or means. F tests and t tests are parametric: they make some 10 behavioral science & policy | volume 8 issue 2 2022 assumptions about the characteristics of a popu lation, such as that the compared groups have an equal variance on a compared factor. In cases violating these assumptions, researchers make some adjustments in their calculations to take into account dissimilar variances across groups. The p value of a statistical test is the probability of obtaining a result equal to or more extreme than would be observed merely by chance, assuming there are no true differences between the groups under study (this assumption is referred to as the null hypothesis). We preregistered p < .005 as the threshold of statistical significance, with lower values indicating a stronger basis for rejecting the null hypothesis. Standard deviation is a measure of the amount of variation in a set of values. Approximately two-thirds of the observations fall between one standard deviation below the mean and one standard deviation above the mean. Stan_dard error uses standard deviation to determine_ how precisely one has estimated a true population value from a sample. For instance, if one took enough samples from a population, the sample mean ±1 standard error would contain the true population mean around two-thirds of the time. C. The fixed order in which the survey presented the seven styles represents a deviation from our preregistered plan, which specified that every participant would see all of the styles “in a different randomly determined order.” After completing the preregistration but before launching the study, we decided that a more conservative test of our hypothesis would be to present the different reporting styles in order of increasing caveats. author affiliation Kotak and Moore: University of California, Berkeley. Corresponding author’s e-mail: dm@ berkeley.edu. author note Thanks to Amelia Dev for her support for this project and to Gabriel Lenz and David Broockman for valuable feedback on a draft of this article. supplemental material • http://behavioralpolicy.org/journal • Method & Analysis In Brief: Key Action Implications for Policymakers • Organizations that conduct polls should report larger margins of error for 95% confidence levels or admit that their margins of error have a lower likelihood of encompassing true election outcomes. The margins of error tend to be too narrow, particularly when polls are conducted long before an election. • News media should require reporters to specify margins of error and to indicate that election outcomes often fall outside the reported margins—so that voters can take that information into account in deciding whether to vote and for whom. a publication of the behavioral science & policy association 11 1. YouGov. (2016, June 23). YouGov on the day poll: Remain 52%, leave 48%. https://yougov.co.uk/topics/ politics/articles-reports/2016/06/23/ yougov-day-poll 2. EU referendum results. (2016). BBC News. https://www.bbc.com/news/ politics/eu_referendum/results 3. Silver, N. (2016, November 8). 2016 election forecast. FiveThirtyEight. https://projects.fivethirtyeight. com/2016-election-forecast/ 4. Harvey, H. M. (2016, December 19). Skeptical 70,000 black voters abstained from presidential vote. The Hill. https:// thehill.com/blogs/pundits-blog/ national-party-news/311099-skeptical- 70000-black-voters-abstained-from 5. The Commission on Presidential Debates. (2020). The Commission on Presidential Debates: An overview. https://www.debates.org/about-cpd/ overview/ 6. McMinn, S., & Hurt, A. (2020, April 21). Tracking the money race behind the presidential primary campaign. NPR. https://www. npr.org/2019/04/16/711812314/ tracking-the-money-race-behind-the- presidential-campaign 7. Westwood, S. J., Messing, S., & Lelkes, Y. (2020). Projecting confidence: How the probabilistic horse race confuses and demobilizes the public. The Journal of Politics, 82(4), 1530–1544. https://doi. org/10.1086/708682 8. Ford, R., Wlezien, C., Pickup, M., & Jennings, W. (2017). Polls and votes. In K. Arzheimer, J. Evans, & M. S. LewisBeck (Eds.), The SAGE handbook of electoral behaviour (pp. 787–812). SAGE Publications. https://doi. org/10.4135/9781473957978.n34 9. Hillygus, D. S. (2011). The evolution of election polling in the United States. Public Opinion Quarterly, 75(5), 962–981. https://doi.org/10.1093/poq/ nfr054 10. Jennings, W., & Wlezien, C. (2018). Election polling errors across time and space. Nature Human Behaviour, 2(4), 276–283. https://doi.org/10.1038/ s41562-018-0315-6 11. Biemer, P. P. (2010). Total survey error: Design, implementation, and evaluation. Public Opinion Quarterly, 74(5), 817–848. https://doi.org/10.1093/poq/ nfq058 12. Groves, R. M., & Lyberg, L. (2010). Total survey error: Past, present, and future. Public Opinion Quarterly, 74(5), 849–879. https://doi.org/10.1093/poq/ nfq065 13. Voss, S., Gelman, A., & King, G. (1995). The polls—A review: Preelection survey methodology: Details from eight polling organizations, 1988 and 1992. Public Opinion Quarterly, 59(1), 98–132. https://doi.org/10.1086/269461 14. Goot, M. (2021). How good are the polls? Australian election predictions, 1993–2019. Australian Journal of Political Science, 56(1), 35–55. https:// doi.org/10.1080/10361146.2020.1825 616 15. Moore, D. A., Swift, S. A., Minster, A., Mellers, B., Ungar, L., Tetlock, P., Yang, H. H. J., & Tenney, E. R. (2017). Confidence calibration in a multiyear geopolitical forecasting competition. Management Science, 63(11), 3552–3565. https://doi.org/10.1287/ mnsc.2016.2525 16. Moore, D. A., Tenney, E. R., & Haran, U. (2015). Overprecision in judgment. In G. Wu & G. Keren (Eds.), Handbook of judgment and decision making (pp. 182–212). Wiley. https://doi. org/10.1002/9781118468333.ch6 17. Fischhoff, B., Slovic, P., & Lichtenstein, S. (1977). Knowing with certainty: The appropriateness of extreme confidence. Journal of Experimental Psychology: Human Perception and Performance, 3(4), 552–564. https://doi. org/10.1037/0096-1523.3.4.552 18. Tetlock, P. E. (2005). Expert political judgment: How good is it? How can we know? Princeton University Press. 19. Berlin, I. (1953). The hedgehog and the fox: An essay on Tolstoy’s view of history. Simon & Schuster. 20. Tetlock, P. E., & Gardner, D. (2015). Superforecasting: The art and science of prediction. Crown Publishers. 21. Jones, R. J., Jr. (2008). The state of presidential election forecasting: The 2004 experience. International Journal of Forecasting, 24(2), 310–321. https:// doi.org/10.1016/j.ijforecast.2008.03.002 22. Litman, L., Robinson, J., & Abberbock, T. (2017). TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences. Behavior Research Methods, 49(2), 433–442. https://doi.org/10.3758/ s13428-016-0727-z 23. Moss, A., & Litman, L. (2020, June 12). Demographics of people on Amazon Mechanical Turk. CloudResearch. https://www.cloudresearch.com/ resources/blog/who-uses-amazon- mturk-2020-demographics/ 24. Shirani-Mehr, H., Rothschild, D., Goel, S., & Gelman, A. (2018). Disentangling bias and variance in election polls. Journal of the American Statistical Association, 113(522), 607–614. https:// doi.org/10.1080/01621459.2018.1448 823 25. Cohn, N. (2017, May 31). A 2016 review: Why key state polls were wrong about Trump. The New York Times. https:// www.nytimes.com/2017/05/31/ upshot/a-2016-review-why-key-state- polls-were-wrong-about-trump.html 26. Mercer, A., Deane, C., & McGeeney, K. (2016). Why 2016 election polls missed their mark. Pew Research Center. https://www.pewresearch. org/fact-tank/2016/11/09/why-2016- election-polls-missed-their-mark/ 27. Russonello, G. (2020, November 5). The polls underestimated Trump— again. Nobody agrees on why. The New York Times. https://www.nytimes. com/2020/11/04/us/politics/poll- results.html 28. Silver, N. (2016, November 11). Why FiveThirtyEight gave Trump a better chance than almost anyone else. FiveThirtyEight. https://fivethirtyeight. com/features/why-fivethirtyeight-gave- trump-a-better-chance-than-almost- anyone-else/ 29. Silver, N. (2020, November 3). Biden’s favored in our final presidential forecast, but it’s a fine line between a landslide and a nail-biter. FiveThirtyEight. https:// fivethirtyeight.com/features/final-2020- presidential-election-forecast/ 30. Isakov, M., & Kuriwaki, S. (2020). Towards principled unskewing: Viewing 2020 election polls through a corrective lens from 2016. Harvard Data Science Review, 2(4). https://doi. org/10.1162/99608f92.86a46f38 31. Silver, N. (2012). The signal and the noise: Why so many predictions fail— But some don’t. Penguin Press. 32. Hansen, L., & Sargent, T. J. (2001). Robust control and model uncertainty. American Economic Review, 91(2), 60–66. https://doi.org/10.1257/ aer.91.2.60 33. Young, C., & Holsteen, K. (2017). Model uncertainty and robustness: A computational framework for multimodel analysis. Sociological Methods & Research, 46(1), 3–40. https://doi. org/10.1177/0049124115610347 12 behavioral science & policy | volume 8 issue 2 2022 DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning Author: Popat, Kashyap, Date: 2018-09-17 Collections: PoliticalML Zotero Key: FLFTA5VA Cite Key: Popat18DebunkFalseDeClarE Zotero Item | Lit Note DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning Kashyap Popat[1], Subhabrata Mukherjee[2], Andrew Yates[1], Gerhard Weikum[1] 1Max Planck Institute for Informatics, Saarbr¨ucken, Germany 2Amazon Inc., Seattle, USA {kpopat,ayates,weikum}@mpi-inf.mpg.de, subhomj@amazon.com Abstract Misinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering external sources related to a claim. However, these methods require substantial feature modeling and rich lexicons. This paper overcomes these limitations of prior work with an end-toend model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our method. 1 Introduction Motivation: Modern media (e.g., news feeds, mi- croblogs, etc.) exhibit an increasing fraction of misleading and manipulative content, from questionable claims and “alternative facts” to completely faked news. The media landscape is becoming a twilight zone and battleground. This societal challenge has led to the rise of fact-checking and debunking websites, such as Snopes.com and PolitiFact.com, where people research claims, manually assess their credibility, and present their verdict along with evidence (e.g., background articles, quotations, etc.). However, this manual verification is time-consuming. To keep up with the scale and speed at which misinformation spreads, we need tools to automate this debunking process. State of the Art and Limitations: Prior work on “truth discovery” (see Li et al. (2016) for survey)[1] largely focused on structured facts, typically in the form of subject-predicate-object triples, or on social media platforms like Twitter, Sina Weibo, etc. Recently, methods have been proposed to assess the credibility of claims in natural language form (Popat et al., 2017; Rashkin et al., 2017; Wang, 2017), such as news headlines, quotes from speeches, blog posts, etc. The methods geared for general text input address the problem in different ways. On the one hand, methods like Rashkin et al. (2017); Wang (2017) train neural networks on labeled claims from sites like PolitiFact.com, providing credibility assessments without any explicit feature modeling. However, they use only the text of questionable claims and no external evidence or interactions that provide limited context for credibility analysis. These approaches also do not offer any explanation of their verdicts. On the other hand, Popat et al. (2017) considers external evidence in the form of other articles (retrieved from the Web) that confirm or refute a claim, and jointly assesses the language style (using subjectivity lexicons), the trustworthiness of the sources, and the credibility of the claim. This is achieved via a pipeline of supervised classifiers. On the upside, this method generates user-interpretable explanations by pointing to informative snippets of evidence articles. On the downside, it requires substantial feature modeling and rich lexicons to detect bias and subjectivity in the language style. Approach and Contribution: To overcome the limitations of the prior works, we present De_ClarE[2], an end-to-end neural network model for_ assessing and explaining the credibility of arbi 1As fully objective and unarguable truth is often elusive or ill-defined, we use the term credibility rather than “truth”. 2Debunking Claims with Interpretable Evidence trary claims in natural-language text form. Our approach combines the best of both families of prior methods. Similar to Popat et al. (2017), DeClarE incorporates external evidence or counterevidence from the Web as well as signals from the language style and the trustworthiness of the underlying sources. However, our method does not require any feature engineering, lexicons, or other manual intervention. Rashkin et al. (2017); Wang (2017) also develop an end-to-end model, but DeClarE goes far beyond in terms of considering external evidence and joint interactions between several factors, and also in its ability to generate userinterpretable explanations in addition to highly accurate assessments. For example, given the natural-language input claim “the gun epidemic is the leading cause of death of young African- American men, more than the next nine causes put together” by Hillary Clinton, DeClarE draws on evidence from the Web to arrive at its verdict cred_ible, and returns annotated snippets like the one_ in Table 6 as explanation. These snippets, which contain evidence in the form of statistics and assertions, are automatically extracted from web articles from sources of varying credibility. Given an input claim, DeClarE searches for web articles related to the claim. It considers the con_text of the claim via word embeddings and the_ (language of) web articles captured via a bidirectional LSTM (biLSTM), while using an attention mechanism to focus on parts of the articles according to their relevance to the claim. DeClarE then aggregates all the information about claim source, web article contexts, attention weights, and trustworthiness of the underlying sources to assess the claim. It also derives informative features for interpretability, like source embeddings that capture trustworthiness and salient words captured via attention. Key contributions of this paper are: • Model: An end-to-end neural network model which automatically assesses the credibility of natural-language claims, without any handcrafted features or lexicons. • Interpretability: An attention mechanism in our model that generates user-comprehensible explanations, making credibility verdicts transparent and interpretable. • Experiments: Extensive experiments on four datasets and ablation studies, demonstrating effectiveness of our method over state-of-theart baselines. 2 End-to-end Framework for Credibility Analysis Consider a set of N claims ⟨Cn⟩ from the respective origins/sources ⟨CSn⟩, where n ∈ [1, N ]. Each claim Cn is reported by a set of M articles ⟨Am,n⟩ along with their respective sources ⟨ASm,n⟩, where m ∈[1, M ]. Each corresponding tuple of claim and its origin, reporting articles and article sources – ⟨Cn, CSn, Am,n, ASm,n⟩ forms a training instance in our setting, along with the credibility label of the claim used as ground-truth during network training. Figure 1 gives a pictorial overview of our model. In the following sections, we provide a detailed description of our approach. 2.1 Input Representations The input claim Cn of length l is represented as [c1, c2, ..., cl] where cl ∈ℜ[d] is the d-dimensional word embedding of the l-th word in the input claim. The source/origin of the claim CSn is represented by a ds-dimensional embedding vector csn ∈ℜ[d][s]. A reporting article Am,n consisting of k tokens is represented by [am,n,1, am,n,2, ..., am,n,k], where am,n,k ∈ℜ[d] is the d-dimensional word embedding vector for the k-th word in the reporting article Am,n. The claim and article word embeddings have shared parameters. The source of the reporting article ASm,n is represented as a dsdimensional vector, asm,n ∈ℜ[d][s]. For the sake of brevity, we drop the notation subscripts n and m in the following sections by considering only a single training instance – the input claim Cn from source CSn, the corresponding article Am,n and its sources ASm,n given by: ⟨C, CS, A, AS⟩. 2.2 Article Representation To create a representation of an article, which may capture task-specific features such as whether it contains objective language, we use a bidirectional Long Short-Term Memory (LSTM) network as proposed by Graves et al. (2005). A basic LSTM cell consists of various gates to control the flow of information through timesteps in a sequence, making LSTMs suitable for capturing long and short range dependencies in text that may be difficult to capture with standard recurrent neural networks (RNNs). Given an input word embedding of tokens ⟨ak⟩, an LSTM cell performs various nonlinear transformations to generate a hidden vector state hk for each token at each timestep k. Claim Word Embeddings Claim Source Figure 1: Framework for credibility assessment. Upper part of the pipeline combines the article and claim embeddings to get the claim specific attention weights. Lower part of the pipeline captures the article representation through biLSTM. Attention focused article representation along with the source embeddings are passed through dense layers to predict the credibility score of the claim. We use bidirectional LSTMs in place of standard LSTMs. Bidirectional LSTMs capture both the previous timesteps (past features) and the future timesteps (future features) via forward and backward states respectively. Correspondingly, there are two hidden states that capture past and future information that are concatenated to form the final output as: hk = [h[−→]k, [←−]hk]. 2.3 Claim Specific Attention As we previously discussed, it is important to consider the relevance of an article with respect to the claim; specifically, focusing or attending to parts of the article that discuss the claim. This is in contrast to prior works (Popat et al., 2017; Rashkin et al., 2017; Wang, 2017) that ignore either the article or the claim, and therefore miss out on this important interaction. We propose an attention mechanism to help our model focus on salient words in the article with respect to the claim. To this end, we compute the importance of each term in an article with respect to an overall representation of the corresponding claim. Additionally, incorporating attention helps in making our model transparent and interpretable, because it provides a way to generate the most salient words in an article as evidence of our model’s verdict. Following Wieting et al. (2015), the overall representation of an input claim is generated by taking an average of the word embeddings of all the words therein: c¯ = [1] l � cl l We combine this overall representation of the claim with each article term: aˆk = ak ⊕c¯ where, ˆak ∈ℜ[d][+][d] and ⊕ denotes the concatenate operation. We then perform a transformation to obtain claim-specific representations of each article term: a[′]k [=][ f] [(][W][a][a][ˆ][k] [+][ b][a][)] where Wa and ba are the corresponding weight matrix and bias terms, and f is an activation function[3], such as ReLU, tanh, or the identity function. Following this, we use a softmax activation to calculate an attention score αk for each word in the article capturing its relevance to the claim context: exp(a[′]k[)] αk = (1) �k [exp(][a]k[′] [)] 2.4 Per-Article Credibility Score of Claim Now that we have article term representations given by ⟨hk⟩ and their relevance to the claim given by ⟨αk⟩, we need to combine them to predict the claim’s credibility. In order to create an 3In our model, the tanh activation function gives best results. attention-focused representation of the article considering both the claim and the article’s language, we calculate a weighted average of the hidden state representations for all article tokens based on their corresponding attention scores: g = [1] k � αk · hk (2) k We then combine all the different feature representations: the claim source embedding (cs), the attention-focused article representation (g), and the article source embedding (as). In order to merge the different representations and capture their joint interactions, we process them with two fully connected layers with non-linear activations. d1 = relu(Wc(g ⊕cs ⊕as) + bc) d2 = relu(Wdd1 + bd) where, W and b are the corresponding weight matrix and bias terms. Finally, to generate the overall credibility label of the article for classification tasks, or credibility score for regression tasks, we process the final representation with a final fully connected layer: Classification: s = sigmoid(d2) (3) Regression: s = linear(d2) (4) 2.5 Credibility Aggregation The credibility score in the above step is obtained considering a single reporting article. As previously discussed, we have M reporting articles per claim. Therefore, once we have the per-article credibility scores from our model, we take an average of these scores to generate the overall credibility score for the claim: cred(C) = [1] M � sm (5) m 3.1 Snopes Snopes (www.snopes.com) is a general factchecking website where editors manually investigate various kinds of rumors reported on the Internet. We used the Snopes dataset provided by Popat et al. (2017). This dataset consists of rumors analyzed on the Snopes website along with their credibility labels (true or false), sets of reporting articles, and their respective web sources. 3.2 PolitiFact PolitiFact is a political fact-checking website (www.politifact.com) in which editors rate the credibility of claims made by various political figures in US politics. We extract all articles from PolitiFact published before December 2017. Each article includes a claim, the speaker (political figure) who made the claim, and the claim’s credibility rating provided by the editors. PolitiFact assigns each claim to one of six possible ratings: true, mostly true, half true, mostly false, false and pants-on-fire. Following Rashkin et al. (2017), we combine true, mostly true and half true ratings into the class label true and the rest as false – hence considering only binary credibility labels. To retrieve the reporting articles for each claim (similar to Popat et al. (2017)), we issue each claim as a query to a search engine[4] and retrieve the top 30 search results with their respective web sources. 3.3 NewsTrust NewsTrust is a news review community in which members review the credibility of news articles. We use the NewsTrust dataset made available by Mukherjee and Weikum (2015). This dataset contains NewsTrust stories from May 2006 to May 2014. Each story consists of a news article along with its source, and a set of reviews and ratings by community members. NewsTrust aggregates these ratings and assigns an overall credibility score (on a scale of 1 to 5) to the posted article. We map the attributes in this data to the inputs expected by DeClarE as follows: the title and the web source of the posted (news) article are mapped to the input claim and claim source, respectively. Reviews and their corresponding user identities are mapped to reporting articles and article sources, respectively. We use this dataset for the regression task of predicting the credibility score of the posted article. 4We use the Bing search API. This aggregation is done after the model is trained. 3 Datasets We evaluate our approach and demonstrate its generality by performing experiments on four different datasets: a general fact-checking website, a political fact-checking website, a news review community, and a SemEval Twitter rumour dataset. Dataset SN PF NT SE Total claims 4341 3568 5344 272 True claims 1164 1867 - 127 False claims 3177 1701 - 50 Unverified claims - - - 95 Claim sources - 95 161 10 Articles 29242 29556 25128 3717 Article sources 336 336 251 89 Table 1: Data statistics (SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE: SemEval). 3.4 SemEval-2017 Task 8 As the fourth dataset, we consider the benchmark dataset released by SemEval-2017 for the task of determining credibility and stance of social media content (Twitter) (Derczynski et al., 2017). The objective of this task is to predict the credibility of a questionable tweet (true, false or unverified) along with a confidence score from the model. It has two sub-tasks: (i) a closed variant in which models only consider the questionable tweet, and (ii) an open variant in which models consider both the questionable tweet and additional context consisting of snapshots of relevant sources retrieved immediately before the rumor was reported, a snapshot of an associated Wikipedia article, news articles from digital news outlets, and preceding tweets about the same event. Testing and development datasets provided by organizers have 28 tweets (1021 reply tweets) and 25 tweets (256 reply tweets), respectively. 3.5 Data Processing In order to have a minimum support for training, claim sources with less than 5 claims in the dataset are grouped into a single dummy claim source, and article sources with less than 10 articles are grouped similarly (5 articles for SemEval as it is a smaller dataset). For Snopes and PolitiFact, we need to extract relevant snippets from the reporting articles for a claim. Therefore, we extract snippets of 100 words from each reporting article having the maximum relevance score: sim = simbow×simsemantic where simbow is the fraction of claim words that are present in the snippet, and simsemantic represents the cosine similarity between the average of claim word embeddings and snippet word embeddings. We also enforce a constraint that the sim score is at least δ. We varied δ from 0.2 to 0.8 and found 0.5 to give the optimal perfor Parameter SN PF NT SE Word embedding length 100 100 300 100 Claim source embedding length - 4 8 4 Article source embedding length 8 4 8 4 LSTM size (for each pass) 64 64 64 16 Size of fully connected layers 32 32 64 8 Dropout 0.5 0.5 0.3 0.3 Table 2: Model parameters used for each dataset (SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE: SemEval). mance on a withheld dataset. We discard all articles related to Snopes and PolitiFact websites from our datasets to have an unbiased model. Statistics of the datasets after pre-processing is provided in Table 1. All the datasets are made pub licly available at https://www.mpi-inf. mpg.de/dl-cred-analysis/. 4 Experiments We evaluate our approach by conducting experiments on four datasets, as described in the previous section. We describe our experimental setup and report our results in the following sections. 4.1 Experimental Setup When using the Snopes, PolitiFact and NewsTrust datasets, we reserve 10% of the data as validation data for parameter tuning. We report 10-fold cross validation results on the remaining 90% of the data; the model is trained on 9-folds and the remaining fold is used as test data. When using the SemEval dataset, we use the data splits provided by the task’s organizers. The objective for Snopes, PolitiFact and SemEval experiments is binary (credibility) classification, while for NewsTrust the objective is to predict the credibility score of the input claim on a scale of 1 to 5 (i.e., credibility regression). We represent terms using pre-trained GloVe Wikipedia 6B word embeddings (Pennington et al., 2014). Since our training datasets are not very large, we do not tune the word embeddings during training. The remaining model parameters are tuned on the validation data; the parameters chosen are reported in Table 2. We use Keras with a Tensorflow backend to implement our system. All the models are trained using Adam optimizer (Kingma and Ba, 2014) (learning rate: 0.002) with categorical cross-entropy loss for classification and mean squared error loss for regression task. We use L2-regularizers with the True Claims False Claims Macro Dataset Configuration AUC Accuracy (%) Accuracy (%) F1-Score LSTM-text 64.65 64.21 0.66 0.70 CNN-text 67.15 63.14 0.66 0.72 Distant Supervision 83.21 80.78 0.82 0.88 Snopes DeClarE (Plain) 74.37 78.57 0.78 0.83 DeClarE (Plain+Attn) 78.34 78.91 0.79 0.85 DeClarE (Plain+SrEmb) 77.43 79.80 0.79 0.85 DeClarE (Full) 78.96 78.32 0.79 0.86 LSTM-text 63.19 61.96 0.63 0.66 CNN-text 63.67 63.31 0.64 0.67 Distant Supervision 62.53 62.08 0.62 0.68 PolitiFact DeClarE (Plain) 62.67 69.05 0.66 0.70 DeClarE (Plain+Attn) 65.53 68.49 0.66 0.72 DeClarE (Plain+SrEmb) 66.71 69.28 0.67 0.74 DeClarE (Full) 67.32 69.62 0.68 0.75 Table 3: Comparison of various approaches for credibility classification on Snopes and PolitiFact datasets. fully connected layers as well as dropout. For all the datasets, the model is trained using each claimarticle pair as a separate training instance. To evaluate and compare the performance of DeClarE with other state-of-the-art methods, we report the following measures: • Credibility Classification (Snopes, PolitiFact and SemEval): accuracy of the models in classifying true and false claims separately, macro F1-score and Area-Under-Curve (AUC) for the ROC (Receiver Operating Characteristic) curve. • Credibility Regression (NewsTrust): Mean Square Error (MSE) between the predicted and true credibility scores. 4.2 Results: Snopes and Politifact We compare our approach with the following state-of-the-art models: (i) LSTM-text, a recent approach proposed by Rashkin et al. (2017). (ii) CNN-text: a CNN based approach proposed by Wang (2017). (iii) Distant Supervision: stateof-the-art distant supervision based approach proposed by Popat et al. (2017). (iv) DeClare (Plain): our approach with only biLSTM (no attention and source embeddings). (v) DeClarE (Plain+Attn): our approach with only biLSTM and attention (no source embeddings). (vi) DeClarE (Plain+SrEmb): our approach with only biLSTM and source embeddings (no attention). (vii) DeClarE (Full): end-to-end system with biLSTM, attention and source embeddings. The results when performing credibility classification on the Snopes and PolitiFact datasets are shown in Table 3. DeClarE outperforms LSTMtext and CNN-text models by a large margin on both datasets. On the other hand, for the Snopes dataset, performance of DeClarE (Full) is slightly lower than the Distant Supervision configuration (p-value of 0.04 with a pairwise t-test). However, the advantage of DeClarE over Distant Supervision approach is that it does not rely on hand crafted features and lexicons, and can generalize well to arbitrary domains without requiring any seed vocabulary. It is also to be noted that both of these approaches use external evidence in the form of reporting articles discussing the claim, which are not available to the LSTM-text and CNN-text baselines. This demonstrates the value of external evidence for credibility assessment. On the PolitiFact dataset, DeClarE outperforms all the baseline models by a margin of 7-9% AUC (p-value of 9.12e−05 with a pairwise t-test) with similar improvements in terms of Macro F1. A performance comparison of DeClarE’s various configurations indicates the contribution of each component of our model, i.e, biLSTM capturing article representations, attention mechanism and source embeddings. The additions of both the attention mechanism and source embeddings improve performance over the plain configuration in all cases when measured by Macro F1 or AUC. 4.3 Results: NewsTrust When performing credibility regression on the NewsTrust dataset, we evaluate the models in terms of mean squared error (MSE; lower is better) for credibility rating prediction. We use the Configuration MSE CNN-text 0.53 CCRF+SVR 0.36 LSTM-text 0.35 DistantSup 0.35 DeClarE (Plain) 0.34 DeClarE (Full) 0.29 Table 4: Comparison of various approaches for credibility regression on NewsTrust dataset. first three models described in Section 4.2 as baselines. For CNN-text and LSTM-text, we add a linear fully connected layer as the final layer of the model to support regression. Additionally, we also consider the state-of-the-art CCRF+SVR model based on Continuous Conditional Random Field (CCRF) and Support Vector Regression (SVR) proposed by Mukherjee and Weikum (2015). The results are shown in Table 4. We observe that DeClarE (Full) outperforms all four baselines, with a 17% decrease in MSE compared to the bestperforming baselines (i.e., LSTM-text and Distant Supervision). The DeClarE (Plain) model performs substantially worse than the full model, illustrating the value of including attention and source embeddings. CNN-text performs substantially worse than the other baselines. 4.4 Results: SemEval On the SemEval dataset, the objective is to perform credibility classification of a tweet while also producing a classification confidence score. We compare the following approaches and consider both variants of the SemEval task: (i) NileTMRG (Enayet and El-Beltagy, 2017): the best performing approach for the close variant of the task, (ii) IITP (Singh et al., 2017): the best performing ap- proach for the open variant of the task, (iii) DeClare (Plain): our approach with only biLSTM (no attention and source embeddings), and (iv) DeClarE (Full): our end-to-end system with biLSTM, attention and source embeddings. We use the evaluation measure proposed by the task’s organizers: macro F1-score for overall classification and Root-Mean-Square Error (RMSE) over confidence scores. Results are shown in Table 5. We observe that DeClarE (Full) outperforms all the other approaches — thereby, re-affirming its power in harnessing external evidence. Macro Configuration RMSE Accuracy IITP (Open) 0.39 0.746 NileTMRG (Close) 0.54 0.673 DeClarE (Plain) 0.46 0.687 DeClarE (Full) 0.57 0.604 Table 5: Comparison of various approaches for credibility classification on SemEval dataset. 5 Discussion 5.1 Analyzing Article Representations In order to assess how our model separates articles reporting false claims from those reporting true ones, we employ dimensionality reduction using Principal Component Analysis (PCA) to project the article representations (g in Equation 2) from a high dimensional space to a 2d plane. The projections are shown in Figure 2a. We observe that DeClarE obtains clear separability between credible versus non-credible articles in Snopes dataset. 5.2 Analyzing Source Embeddings Similar to the treatment of article representations, we perform an analysis with the claim and article source embeddings by employing PCA and plotting the projections. We sample a few popular news sources from Snopes and claim sources from PolitiFact. These news sources and claim sources are displayed in Figure 2b and Figure 2c, respectively. From Figure 2b we observe that DeClarE clearly separates fake news sources like nationalreport, empirenews, huzlers, etc. from mainstream news sources like nytimes, cnn, wsj, foxnews, washingtonpost, etc. Similarly, from Fig- ure 2c we observe that DeClarE locates politicians with similar ideologies and opinions close to each other in the embedding space. 5.3 Analyzing Attention Weights Attention weights help understand what DeClarE focuses on during learning and how it affects its decisions – thereby, making our model transparent to the end-users. Table 6 illustrates some interesting claims and salient words (highlighted) that DeClarE focused on during learning. Darker shades indicate higher weights given to the corresponding words. As illustrated in the table, DeClarE gives more attention to important words in the reporting article that are relevant to the claim and also (a) Projections of article representations using PCA; DeClarE obtains clear separation between representations of noncredible articles (red) vs. true ones (green). (b) Projections of article source representations using PCA; DeClarE clearly separates fake news sources from authentic ones. (c) Projections of claim source representations using PCA; DeClarE clusters politicians of similar ideologies close to each other in the embedding space. Figure 2: Dissecting the article, article source and claim source representations learned by DeClarE. Table 6: Interpretation via attention (weights) ([True]/[False] indicates the verdict from DeClarE). play a major role in deciding the corresponding claim’s credibility. In the first example on Table 6, highlighted words such as “..barely true...” and “..sketchy evidence...” help our system to identify the claim as not credible. On the other hand, highlighted words in the last example, like, “..reveal...” and “..documenting reports...” help our system to assess the claim as credible. 6 Related Work Our work is closely related to the following areas: Credibility analysis of Web claims: Our work builds upon approaches for performing credibility analysis of natural language claims in an opendomain Web setting. The approach proposed in Popat et al. (2016, 2017) employs stylistic lan guage features and the stance of articles to assess the credibility of the natural language claims. However, their model heavily relies on handcrafted language features. Rashkin et al. (2017); Wang (2017) propose neural network based approaches for determining the credibility of a textual claim, but it does not consider external sources like web evidence and claim sources. These can be important evidence sources for credibility analysis. The method proposed by Samadi et al. (2016) uses the Probabilistic Soft Logic (PSL) framework to estimate source reliability and claim correctness. Vydiswaran et al. (2011) proposes an iterative algorithm which jointly learns the veracity of textual claims and trustworthiness of the sources. These approaches do not consider the deeper semantic aspects of language, however. Wiebe and Riloff (2005); Lin et al. (2011); Recasens et al. (2013) study the problem of detecting bias in language, but do not consider credibility. Truth discovery: Prior approaches for truth dis- covery (Yin et al., 2008; Dong et al., 2009, 2015; Li et al., 2011, 2014, 2015; Pasternack and Roth, 2011, 2013; Ma et al., 2015; Zhi et al., 2015; Gao et al., 2015; Lyu et al., 2017) have focused on structured data with the goal of addressing the problem of conflict resolution amongst multisource data. Nakashole and Mitchell (2014) proposed a method to extract conflicting values from the Web in the form of Subject-Predicate-Object (SPO) triplets and uses language objectivity analysis to determine the true value. Like the other truth discovery approaches, however, this approach is mainly suitable for use with structured data. Credibility analysis in social media: Mukher- jee et al. (2014); Mukherjee and Weikum (2015) propose PGM based approaches to jointly infer a statement’s credibility and the reliability of sources using language specific features. Approaches like (Castillo et al., 2011; Qazvinian et al., 2011; Yang et al., 2012; Xu and Zhao, 2012; Gupta et al., 2013; Zhao et al., 2015; Volkova et al., 2017) propose supervised methods for detecting deceptive content in social media platforms like Twitter, Sina Weibo, etc. Similarly, approaches like Ma et al. (2016); Ruchansky et al. (2017) use neural network methods to identify fake news and rumors on social media. Kumar et al. (2016) studies the problem of detecting hoax articles on Wikipedia. All these rely on domain-specific and community-specific features like retweets, likes, upvotes, etc. 7 Conclusion In this work, we propose a completely automated end-to-end neural network model, DeClarE, for evidence-aware credibility assessment of natural language claims without requiring hand-crafted features or lexicons. DeClarE captures signals from external evidence articles and models joint interactions between various factors like the context of a claim, the language of reporting articles, and trustworthiness of their sources. Extensive experiments on real world datasets demonstrate our effectiveness over state-of-the-art baselines. References Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on twitter. In Proceedings of the 20th International Conference on World Wide Web, WWW ’11, pages 675–684, New York, NY, USA. ACM. Leon Derczynski, Kalina Bontcheva, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz Zubiaga. 2017. Semeval-2017 task 8: Rumoureval: Determining rumour veracity and support for rumours. In Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017, pages 69–76. Xin Luna Dong, Laure Berti-Equille, and Divesh Srivastava. 2009. Integrating conflicting data: The role of source dependence. Proc. VLDB Endow., 2(1):550–561. Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy, Van Dang, Wilko Horn, Camillo Lugaresi, Shaohua Sun, and Wei Zhang. 2015. Knowledge-based trust: Estimating the trustworthiness of web sources. Proc. VLDB Endow., 8(9):938–949. Omar Enayet and Samhaa R. El-Beltagy. 2017. Niletmrg at semeval-2017 task 8: Determining rumour and veracity support for rumours on twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Van- couver, Canada, August 3-4, 2017, pages 470–474. Jing Gao, Qi Li, Bo Zhao, Wei Fan, and Jiawei Han. 2015. Truth discovery and crowdsourcing aggregation: A unified perspective. PVLDB, 8(12):2048– 2049. Alex Graves, Santiago Fern´andez, and J¨urgen Schmidhuber. 2005. Bidirectional lstm networks for improved phoneme classification and recognition. In Proceedings of the 15th International Con_ference on Artificial Neural Networks:_ Formal Models and Their Applications - Volume Part II, ICANN’05, pages 799–804, Berlin, Heidelberg. Springer-Verlag. Aditi Gupta, Hemank Lamba, Ponnurangam Kumaraguru, and Anupam Joshi. 2013. Faking sandy: Characterizing and identifying fake images on twitter during hurricane sandy. In Proceedings of the 22Nd International Conference on World Wide Web, WWW ’13 Companion, pages 729–736, New York, NY, USA. ACM. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Srijan Kumar, Robert West, and Jure Leskovec. 2016. Disinformation on the web: Impact, characteristics, and detection of wikipedia hoaxes. In Proceed_ings of the 25th International Conference on World_ Wide Web, WWW ’16, pages 591–602, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee. Qi Li, Yaliang Li, Jing Gao, Lu Su, Bo Zhao, Murat Demirbas, Wei Fan, and Jiawei Han. 2014. A confidence-aware approach for truth discovery on long-tail data. Proc. VLDB Endow., 8(4):425–436. Xian Li, Weiyi Meng, and Clement Yu. 2011. Tverifier: Verifying truthfulness of fact statements. In Proceedings of the 2011 IEEE 27th International Conference on Data Engineering, ICDE ’11, pages 63–74, Washington, DC, USA. IEEE Computer Society. Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su, Bo Zhao, Wei Fan, and Jiawei Han. 2016. A survey on truth discovery. SIGKDD Explor. Newsl., 17(2):1–16. Yaliang Li, Qi Li, Jing Gao, Lu Su, Bo Zhao, Wei Fan, and Jiawei Han. 2015. On the discovery of evolving truth. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages 675–684, New York, NY, USA. ACM. Chenghua Lin, Yulan He, and Richard Everson. 2011. Sentence subjectivity detection with weaklysupervised learning. In Proceedings of 5th Interna_tional Joint Conference on Natural Language Pro-_ cessing, pages 1153–1161. Asian Federation of Nat- ural Language Processing. Shanshan Lyu, Wentao Ouyang, Huawei Shen, and Xueqi Cheng. 2017. Truth discovery by claim and source embedding. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Man- agement, CIKM ’17, pages 2183–2186, New York, NY, USA. ACM. Fenglong Ma, Yaliang Li, Qi Li, Minghui Qiu, Jing Gao, Shi Zhi, Lu Su, Bo Zhao, Heng Ji, and Jiawei Han. 2015. Faitcrowd: Fine grained truth discovery for crowdsourced data aggregation. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages 745–754, New York, NY, USA. ACM. Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon, Bernard J. Jansen, Kam-Fai Wong, and Meeyoung Cha. 2016. Detecting rumors from microblogs with recurrent neural networks. In Proceedings of the Twenty-Fifth International Joint Conference on Ar- tificial Intelligence, IJCAI’16, pages 3818–3824. AAAI Press. Subhabrata Mukherjee and Gerhard Weikum. 2015. Leveraging joint interactions for credibility analysis in news communities. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM ’15. Subhabrata Mukherjee, Gerhard Weikum, and Cristian Danescu-Niculescu-Mizil. 2014. People on drugs: Credibility of user statements in health communities. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, pages 65–74, New York, NY, USA. ACM. Ndapandula Nakashole and Tom M. Mitchell. 2014. Language-aware truth assessment of fact candidates. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Vol- ume 1: Long Papers, pages 1009–1019. Jeff Pasternack and Dan Roth. 2011. Making better informed trust decisions with generalized factfinding. In IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intel- ligence, Barcelona, Catalonia, Spain, July 16-22, 2011, pages 2324–2329. Jeff Pasternack and Dan Roth. 2013. Latent credibility analysis. In Proceedings of the 22Nd International Conference on World Wide Web, WWW ’13, pages 1009–1020, New York, NY, USA. ACM. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natu_ral Language Processing, EMNLP ’14._ Kashyap Popat, Subhabrata Mukherjee, Jannik Str¨otgen, and Gerhard Weikum. 2016. Credibility assessment of textual claims on the web. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Man- agement, CIKM ’16, pages 2173–2178, New York, NY, USA. ACM. Kashyap Popat, Subhabrata Mukherjee, Jannik Str¨otgen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the web and social media. In Proceedings of the 26th International Conference on World Wide Web Companion, WWW ’17 Companion. Vahed Qazvinian, Emily Rosengren, Dragomir R. Radev, and Qiaozhu Mei. 2011. Rumor has it: Identifying misinformation in microblogs. In Proceed_ings of the Conference on Empirical Methods in_ Natural Language Processing, EMNLP ’11, pages 1589–1599, Stroudsburg, PA, USA. Association for Computational Linguistics. Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Con_ference on Empirical Methods in Natural Language_ Processing, EMNLP ’17. Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic models for analyzing and detecting biased language. In Proceed_ings of the 51st Annual Meeting of the Association_ for Computational Linguistics (Volume 1: Long Pa- pers), pages 1650–1659. Association for Computa- tional Linguistics. Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017. Csi: A hybrid deep model for fake news detection. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM ’17, pages 797–806, New York, NY, USA. ACM. Mehdi Samadi, Partha Talukdar, Manuela Veloso, and Manuel Blum. 2016. Claimeval: Integrated and flexible framework for claim evaluation using credibility of sources. In Proceedings of the Thir_tieth AAAI Conference on Artificial Intelligence,_ AAAI’16, pages 222–228. AAAI Press. Vikram Singh, Sunny Narayan, Md. Shad Akhtar, Asif Ekbal, and Pushpak Bhattacharyya. 2017. IITP at semeval-2017 task 8 : A supervised approach for rumour evaluation. In Proceedings of the 11th In_ternational Workshop on Semantic Evaluation, Se-_ mEval@ACL 2017, Vancouver, Canada, August 3-4, 2017, pages 497–501. Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and Nathan Hodas. 2017. Separating facts from fiction: Linguistic models to classify suspicious and trusted news posts on twitter. In Proceedings of the 55th An_nual Meeting of the Association for Computational_ Linguistics (Volume 2: Short Papers), pages 647– 653. Association for Computational Linguistics. V.G. Vinod Vydiswaran, ChengXiang Zhai, and Dan Roth. 2011. Content-driven trust propagation framework. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 974–982, New York, NY, USA. ACM. William Yang Wang. 2017. ”liar, liar pants on fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2017, _Vancouver, Canada, July 30 - August 4, Volume 2:_ Short Papers, pages 422–426. Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of the 6th International Conference on Computational Linguistics and Intel- ligent Text Processing, CICLing’05, pages 486–497, Berlin, Heidelberg. Springer-Verlag. John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal paraphrastic sentence embeddings. In Proceedings of the Inter_national Conference on Learning Representations_ (ICLR). Qiongkai Xu and Hai Zhao. 2012. Using deep linguistic features for finding deceptive opinion spam. In Proceedings of COLING 2012: Posters, pages 1341–1350. The COLING 2012 Organizing Committee. Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012. Automatic detection of rumor on sina weibo. In Pro_ceedings of the ACM SIGKDD Workshop on Mining_ Data Semantics, MDS ’12, pages 13:1–13:7, New York, NY, USA. ACM. Xiaoxin Yin, Jiawei Han, and Philip S. Yu. 2008. Truth discovery with multiple conflicting information providers on the web. IEEE Trans. on Knowl. and Data Eng., 20(6):796–808. Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. Enquiring minds: Early detection of rumors in social media from enquiry posts. In Proceedings of the 24th International Conference on World Wide Web, WWW ’15, pages 1395–1405, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee. Shi Zhi, Bo Zhao, Wenzhu Tong, Jing Gao, Dian Yu, Heng Ji, and Jiawei Han. 2015. Modeling truth existence in truth discovery. In Proceedings of the 21th ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, KDD ’15, pages 1543–1552, New York, NY, USA. ACM. Social media’s dark secrets: A propagation, lexical and psycholinguistic oriented deep learning approach for fake news proliferation Author: Ahmed, Kanwal, Date: 2024-12-01 Collections: NeuroPsychoLinguisticPolitics, PoliticalML Zotero Key: U52DC98Q Cite Key: Ahmed24propLexPsychFakeNewsProlif Zotero Item | Lit Note 1. Introduction Recent years have seen a dramatic shift in the way information is consumed, with social media emerging as a principal news provider ( Karami, Nazer, & Liu, 2021). One of the greatest strengths of online social networks is the ease with which knowledge can be shared widely. However, the uncontrolled character of social media platforms, along with the possibility of automation and rapid transmission, makes it simple for users to post false or deliberately deceptive information, endangering access to credible and dependable information. False, incorrect, or misleading information can have devastating effects on society, posing serious dangers to political stability and the economy. This in turn may have an impact on how readers interpret the text ( Karami et al., 2021 ). To begin, an individual’s tendency to believe false information increases with repeated exposure, especially if the false information is consistent with the individual’s pre-existing beliefs ( Jiang, Karami, Cheng, Black, & Liu, 2021 ). With the proliferation of fabricated content, the boundary between reality and deception becomes less distinct, causing users to be skeptical about the authenticity of all content and to assume bias in all content, making it more difficult to distinguish between fake and authentic content ( Karami et al., 2021). The credibility of the whole media network could be jeopardized in the future if this trend continues. Therefore, stopping the spread of misinformation is essential.In this context, computational linguistics, as a branch of scientific inquiry, delves into the study of language from a computational perspective (Rashkin, Choi, Jang, Volkova, & Choi, 2017). This field provides a suite of tools and methodologies that are indispensable in the progressive development of automated systems for detecting patterns in fake news. By employing advanced computational linguistics techniques, researchers and technologists are able to extract meaningful insights from textual content (Volkova, Shaffer, Jang, & Hodas, 2017). These techniques not only facilitate a deeper understanding of the language used in various contexts but also enhance our ability to discern and interpret the complex structure and dynamics prevalent within information and social networks (Pérez-Rosas, Kleinberg, Lefevre, & Mihalcea, 2017 ). Consequently, computational linguistics stands as a critical component in the broader effort to understand and mitigate the spread of misinformation in the digital age Ruffo, Semeraro, Giachanou, and Rosso (2023) . Chamberlain, Spezzano, Kettler, and Dit (2021) proposed that apocryphal narratives are particularly potentin eliciting emotions such as fear, disgust, and surprise among recipients. This emotional response is likely due to the sensational and often alarming content of such stories, which are crafted to capture attention and provoke reaction. Conversely, authentic content is found to induce a spectrum of emotions that include anticipation, sadness, joy, and trust ( Vosoughi, Roy, & Aral, 2018c ). The distinct emotional profiles elicited by false and true stories highlight the significant impact of the veracity of information on human emotion and cognition. Although there are mutual benefits, numerous techniques have frequently overlooked network topological elements, prioritizing content-based aspects instead ( Luvembe et al., 2023 , Zhang et al., 2022). Fake content may attempt to deceive readers by imitating the language patterns of authentic material, thereby distorting the handcrafted components of the content (Shu, Sliva, Wang, Tang, & Liu, 2017). Similarly, an exclusive focus on network topology can result in unreliable and unstable systems (Tang, Chang, & Liu, 2014 ). Thus, a thorough examination of network features and a nuanced analysis of user interactions are essential. Furthermore, integrating these features into a unified and informative feature vector representation poses significant challenges ( Farhangian, Cruz, & Cavalcanti, 2024 ). The formation and distribution of false information are highly dependent on user engagement. In this research, the individuals primarily engaged in disseminating disinformation are referred to as disinformers. While those involved in generating and sharing factually accurate content are referred to as informers. Consequently, identifying both deceptive content and the disinformers is imperative, as the latter offers substantial data essential for formulating mitigation or fusion strategies aimed at rapidly curtailing proliferation ( Sharma et al., 2019). In this context, the imperative to pinpoint disinformers on social media platforms is unprecedentedly critical. This study offers a foundational demonstration for the application of a deep learning model to trace the origins of disinformation on social media platforms. This research investigates the differentiation between disinformers and informers across social media platforms. A novel deep learning methodology based on GCNN, enhanced with multi-head attention mechanisms is introduced. The model leverages a comprehensive array of meticulously selected psycholinguistic attributes extracted from users’ posts, including sentiment analysis, emotional content evaluation, linguistic characteristics, personality analysis, readability metrics, and communication style assessment. These attributes are further enriched through BERT embeddings to enhance the quality of textual representation. Moreover, our proposed framework encompasses the development of two distinct graph structures: the user interaction graph and the Semantic propagation graph. These graphical representations play a crucial role in organizing the dynamic interactions and dissemination patterns of information among users. For each individual user, a customized set of features is meticulously designed, capturing their unique characteristics and behavioral tendencies. These features are then integrated into the User Interaction Graph, facilitating a more nuanced comprehension and representation of user interactions within the model. The major contributions of this research are summarized as follows: • 1.* A novel deep learning methodology based on Graph Convolutional Neural Networks, augmented with multi-head attention mechanisms, is introduced for analyzing and combating disinformation, particularly in graph-structured data settings. • 2.* The proposed model leverages a comprehensive array of precisely selected psycholinguistic attributes extracted from users’ posts. These attributes enrich the understanding of the complex interplay between language use and misinformation dissemination. • 3.* The proposed model encompasses the development of two distinct graph structures: the user interaction graph and the Semantic propagation graph. These graphical representations provide a visual and structured framework for analyzing user behavior and information flow in social media networks. • 4.* For each individual user, a customized set of features is meticulously designed, capturing their unique characteristics and behavioral tendencies. This allows for a personalized and nuanced analysis of user interactions and content dissemination. • 5.The integration of user-specific features into the User Interaction Graph facilitates a more nuanced comprehension and representation of user interactions within the model. This deepens the understanding of user behavior dynamics and their role in information dissemination* . These scientific contributions collectively advance the state-of-the-art in disinformation detection and mitigation strategies, providing valuable insights and methodologies in the field of computational linguistics, deep learning, and social media analytics. The remaining parts of the paper are structured as Section 2 addresses related work on detecting fake news and the role that users play in the situation. Following that, the methodology of the proposed framework is provided in Section 3 . The experimental setting is described in Section 4 , and our findings and interpretations are discussed in Section 5 . In Section 6 , we go over the limits of our research as well as some ethical considerations, and then in Section 7 , we draw conclusions and address future work. 5. Results and analysis In this section, we initially provide the performance outcomes derived from our proposed methodology, followed by a comprehensive analysis of the variances and the significance of various psycholinguistic and hand-crafted features incorporated within the study. The comparative analysis clearly substantiates the superiority of the proposed model’s methodology and attributes, demonstrating its dominance over substitute models in critical evaluative criteria such as accuracy, precision, recall, and f1-sore for FakeNewsNet and FibVid datasets as shown in Table 4 , Table 5 , respectively. Our model surpasses by outperforming its counterparts, particularly in recall metrics, while maintaining competitive precision levels. This performance underscores the critical importance of integrating various sources of information for the effective detection of informers and disinformers. Notably, the model presented by Sansonetti, Gasparetti, D’aniello, and Micarelli (2020) achieved a significant equilibrium between precision and recall, reinforcing the value of tweet and user statistics as pertinent indicators for disinformer detection. In contrast, the Ghanem, Ponzetto and Rosso (2020) approach, characterized by its segmentation of users’ timelines into brief tweet clusters, yielded the least favorable outcomes. This suggests that such segmentation may lead to an inadequate representation of user characteristics, undermining the effectiveness of the model. Furthermore, while Giachanou et al. (2022) and Shrestha and Spezzano (2022) did not exceed the performance of our proposed model, they demonstrated substantial improvements in recall, indicating a possible sensitivity to class distribution. This observation suggests that while these models may not yet rival the proposed model in overall performance, their approach to handling class distribution is noteworthy and may contribute valuable insights for future model enhancements. Additionally, an ablation study was rigorously executed to validate the efficacy of each component within the proposed model for FakeNewsNet and FibVid datasets as shown in Table 6 , Table 7 , respectively. The results indicated that the proposed Semantic Propagation Graph notably outperformed alternative content-based methods, suggesting that the statistical analysis of tweets may provide more robust indicators for the identification of disinformers than the analysis of the content itself. Furthermore, the performance of embedding content representations was found to be on par with that of hand-crafted features, which could imply that deceptive and genuine content share similar stylistic and characteristic features ( Giachanou et al., 2020b ). Table 4. Comparison of the proposed models with state-of-the-art models using FakeNewsNet dataset. The symbol (w) denotes the weighted parameters. Table 5. Comparison of the proposed models with state-of-the-art models using FibVid dataset. The symbol (w) denotes the weighted parameters. The submissive performance of embedding representations may be attributable to the topically concentrated nature of the dataset, where both deceptive and authentic content are closely related, making the task of differentiating them more challenging for embeddings to effectively accomplish. Moreover, the variation in binary and weighted (w) metrics was found to be more pronounced in comparison models than in our proposed model. This observation suggests that the informer class has a more pronounced effect on the evaluation metrics for baseline models than for the proposed model, highlighting the importance of a balanced and nuanced approach to feature selection and model design in the detection of disinformation. In order to evaluate the scalability of our proposed framework and assess the performance of our model in a real-world setting, we utilized the publicly available Twitter15 dataset ( Ma, Gao, & Wong, 2017 ), which includes news articles with associated labels, along with data detailing the directed network connections among users, followers, and followees. This dataset comprises 1490 news articles categorized as true, unverified, non-rumor, or false. We specifically focused on 300 articles identified as false, constructing their propagation traces and calculating scores for those involved. Performance is measured by the overall processing time. Table 6. Ablation study of the proposed models with state-of-the-art models using FakeNewsNet dataset. | Model | Accuracy | Precision | Recall | f1-score | Accuracy (W) | Precision (W) | Recall (W) | f1-score (W) | | --- | --- | --- | --- | --- | --- | --- | --- | --- | | Contextualized word embeddings | 51.46 | 49.83 | 33.59 | 40.13 | 42.59 | 40.86 | 37.29 | 38.99 | | User handcrafted features | 58.69 | 54.62 | 42.36 | 47.72 | 46.21 | 45.77 | 71.63 | 55.85 | | User interaction graph | 76.27 | 74.28 | 68.61 | 71.33 | 54.73 | 53.49 | 64.29 | 58.39 | | Semantic propagation graph | 82.43 | 81.64 | 75.22 | 78.30 | 67.59 | 66.49 | 65.43 | 65.96 | | Complete model | 91.07 | 91.24 | 89.27 | 90.24 | 77.36 | 76.57 | 75.28 | 75.92 | The symbol (w) denotes the weighted parameters. Table 7. Ablation study of the proposed models with state-of-the-art models using FibVid dataset. | Model | Accuracy | Precision | Recall | f1-score | Accuracy (W) | Precision (W) | Recall (W) | f1-score (W) | | --- | --- | --- | --- | --- | --- | --- | --- | --- | | Contextualized word embeddings | 58.91 | 54.21 | 46.92 | 50.30 | 46.52 | 45.86 | 44.74 | 45.29 | | User handcrafted features | 64.21 | 62.44 | 51.52 | 56.46 | 57.60 | 57.1 | 49.28 | 52.90 | | User interaction graph | 82.69 | 81.25 | 60.41 | 69.30 | 69.21 | 68.72 | 64.30 | 66.44 | | Semantic propagation graph | 86.24 | 84.28 | 74.88 | 79.30 | 77.09 | 75.48 | 74.26 | 74.87 | | Complete model | 94.20 | 94.92 | 90.48 | 92.65 | 84.76 | 83.94 | 79.25 | 81.53 | The symbol (w) denotes the weighted parameters. The proposed model uses a GCNN with multi-head attention and BERT embeddings to enhance psycholinguistic attributes from user posts. Graphical representations organize user interactions and information dissemination. Customized features for each user capture their unique traits and are integrated into the User Interaction Graph for a deeper understanding of user interactions. Table 8 illustrates that our proposed model outperforms other models in terms of accuracy and F1 score. Additionally, the execution time of the proposed model is shorter compared to existing methods. Therefore, it can be concluded that the proposed model exhibits superior performance and considerable robustness. Furthermore, it has been observed that for smaller graphs, the detection process occurs almost instantaneously, which is advantageous for identifying and targeting disinformers. For larger networks, the entire process is completed in near real-time, taking less than eight minutes. Consequently, the most effective approach involves immunizing the network by focusing on the most detrimental nodes. Table 8. Evaluation of performance using Twitter15 dataset. (s) denotes Execution time in seconds. An in-depth examination of the distinct psycholinguistic attributes and custom-crafted features provides insight into their individual and collective impact on the model’s performance. This includes a discussion on how specific features contribute to the interpretability and precision of the model, as well as their roles in enhancing or constraining the overall effectiveness. The analysis aims to not only validate the methodological choices made but also to underscore the nuanced interplay between different types of features and their bearing on the robustness and reliability of predictive analytics. This dual approach ensures a holistic understanding of the methodological outcomes and the elemental features that underpin the model’s predictive prowess. 5.1. Age and gender Demographic factors have been identified as significant predictors in the dissemination of fake news, as indicated in previous studies ( Zuckerman et al., 1981 ). Fig. 3 , Fig. 4 illustrate the demographic distribution, specifically age, and gender, within the FibVid and FakeNewsNet datasets. It has been observed that individuals predicted to be below 20 or above 40 years of age are predominantly inclined to disseminate more fake news compared to genuine information. Conversely, the propensity to share fake news diminishes among users aged between 20 and 40 years. While extant literature has consistently reported an increased likelihood of older individuals, particularly those over 50, to share fake news compared to younger cohorts (20–50 years), the sharing patterns of individuals under 20 have remained relatively unexplored. Our observations suggest that these younger users, along with the older demographic above 40, may exhibit heightened susceptibility to misinformation, aligning somewhat with prior findings. The prevalent inability among teenagers to critically evaluate the credibility of information, coupled with the diminished digital literacy observed in older adults, may contribute to their vulnerability. Gender disparities in sharing behaviors are also notable; our analysis of Fig. 4 indicates a tendency for female users to share more fake news than their male counterparts within the dataset. This disparity may be attributed to differential engagement with political news, potentially rendering female users less informed and thereby more susceptible to fake news on such topics ( Buda and Bolonyai, 2020 , Jiang et al., 2021). It is crucial to acknowledge the significant imbalance within the age groups and the expansive heterogeneity of the last age category, which may limit the generalizability of our findings to the specific datasets analyzed. 5.2. Political orientation Fig. 5 delineates the distribution of individuals disseminating fake and genuine news, stratified by their political orientations, within the FakeNewsNet dataset. The distribution indicates a higher tendency for left-leaning individuals to distribute fake news compared to their right-leaning counterparts. This observation does not contradict the findings of Zuckerman et al. (1981) , who noted in 2016 that conservatives were more inclined to share content from pro-Trump fake news domains, reflecting an alignment with their ideological beliefs and the predominance of right-leaning misinformation during that period. In our analysis of the FakeNewsNet dataset, we investigated the ideological bias of the news sources, utilizing data from the MediaBias/FactCheck website. The results revealed a preponderance of left-leaning sources within the dataset, with such content being shared more frequently than that from right-leaning sources. Consequently, this suggests that the political leaning of individuals sharing fake news in the FakeNewsNet dataset tends to correspond with the dominant ideological slant of the misinformation prevalent within the dataset. This alignment underscores the nuanced relationship between the political orientation of news disinformers and the ideological bias of the sources, reflecting the complex dynamics of misinformation dissemination in different political contexts. 5.3. Popularity index Fig. 6 presents the average Twitter follower-to-following (F2F) ratio among users within the FibVid and FakeNewsNet datasets. The distribution vividly demonstrates that individuals who do not spread fake news (informers) generally command significantly higher popularity, evidenced by a F2F ratio where the number of followers is approximately 400 times that of the number they follow. This stark discrepancy suggests that users with a lower F2F ratio may be more inclined to disseminate fake news, possibly as a strategy to augment their visibility and engagement on the platform. Several motivations can underlie this phenomenon. For instance, some users, despite recognizing the dubious veracity of a news item, might still choose to share it, driven by the content’s humorous or provocative nature, or to resonate with and stimulate interaction among their followers. Alternatively, users with a lower F2F ratio might be relatively new to Twitter and thus less adept at navigating the platform’s intricacies. This lack of familiarity could inadvertently lead to the sharing of misinformation. Furthermore, the quest for social validation or the desire to align with a particular community or ideology on Twitter might prompt users to share content that amplifies their perceived digital identity or group affiliation, even if it means propagating fake news. This behavior underscores the multifaceted motivations behind news sharing on social media and highlights the need for nuanced strategies in misinformation mitigation, considering both the sociological aspects of user behavior and the technical attributes of social media platforms. 5.4. Profile lifetime The incorporation of timestamps within the FakeNewsNet dataset facilitates an exploration of the tweeting behaviors characteristic of individuals propagating fake news (disinformers). Fig. 7 illustrates the behavioral attributes under consideration within both the FibVid and FakeNewsNet datasets. Observations reveal that disinformers possess newer accounts, exhibit shorter intervals between successive tweets, and demonstrate a propensity for tweeting during nocturnal hours, compared to their counterparts disseminating genuine news (informers). These characteristics suggest that disinformers are generally less tenured users of the platform, potentially lacking comprehensive knowledge of its functionalities and norms. Their more frequent tweeting could be a tactic aimed at strengthening their presence and influence within the digital community. The behavioral tendencies of disinformers, as illuminated by these observations, underscore the importance of considering temporal dynamics, account longevity, and engagement patterns in understanding and combating the spread of misinformation. Recognizing these patterns not only contributes to the profiling of typical disinformers but also aids in designing targeted interventions and educational campaigns to mitigate the proliferation of fake news on social media platforms. 5.5. Writing style Similarly, Table 9 elucidates the distinctive writing styles between disinformers and informers of news. The Table 9 employs a notation system where “ Informer ¿ Disinformer ” signifies that, on average, a feature is more prevalent among real news spreaders than fake news spreaders, with the converse denoted by “Informer ¡ Disinformer”. It has been observed that disinformers characteristically employ a greater number of uppercase characters and fewer hashtags in their tweets, alongside a tendency to disseminate more content deemed as breaking news. This pattern is consistent across both datasets under study. In the FakeNewsNet dataset, disinformers are noted to utilize an increased frequency of uppercase words and URLs, yet their tweets are generally shorter and contain fewer words, lowercased characters, punctuation marks , ellipses (‘...’), stop words, and mentions. Conversely, in the FibVID dataset, disinformers are distinguished by higher use of lowercase characters and reduced employment of emojis and retweets. These variations suggest a deliberate strategy by disinformers to capture attention and disseminate their content more effectively. The heightened use of uppercase words and URLs can be interpreted as a method to draw attention and convey a sense of urgency or importance, aligning with the nature of breaking news. Furthermore, the reduced use of conventional tweet features such as punctuation, lowercase characters, and mentions may reflect a more direct, assertive communication style typically employed in sensational or misleading content. The relative scarcity of emojis and retweets among disinformers in the FibVID dataset might indicate a focus on original content creation over engagement with existing material. Collectively, these stylistic choices may be aimed at increasing visibility, inciting reactions, and fostering a sense of credibility or immediacy, ultimately aiding in the spread of fake news. Understanding these writing style patterns offers valuable insights into the behavioral and tactical nuances of fake news dissemination, aiding in the development of more sophisticated detection tools and countermeasures. It also underscores the complexity and adaptability of strategies used by those aiming to influence or deceive via social media platforms. All differences are statistically significant (p<0.001 for FakeNewsNet and p<0.03 for FibVid). Table 9. Writing style features that differ in user feed. | Features | FakeNewsNet | FibVid | | --- | --- | --- | | Words | Informer > Disinformer | Informer > Disinformer | | Characters | Informer > Disinformer | Informer > Disinformer | | Lowercase words | Informer > Disinformer | Informer < Disinformer | | Uppercase words | Informer < Disinformer | Informer > Disinformer | | Lowercase characters | Informer < Disinformer | Informer < Disinformer | | Uppercase characters | Informer < Disinformer | Informer > Disinformer | | Stop words | Informer > Disinformer | Informer > Disinformer | | Punctuation symbols | Informer > Disinformer | Informer > Disinformer | | Hashtags | Informer > Disinformer | Informer > Disinformer | | URLs | Informer < Disinformer | Informer > Disinformer | | Mentions | Informer > Disinformer | Informer < Disinformer | | Emoji and smileys | Informer > Disinformer | Informer > Disinformer | | Breaking or breaking news | Informer < Disinformer | Informer > Disinformer | 5.6. Readability Contrasting with the consistency observed in emotional and personality attributes, readability metrics exhibit a lack of generalization across the datasets under consideration. Specifically, within the FakeNewsNet dataset, disinformers tend to produce content with lower readability levels compared to those of informers, as shown in Fig. 8 . This suggests a strategic use of language that might be more simplistic, less structured, or perhaps intentionally disguised to appeal to or mislead a broader audience. On the other hand, the trend is inversed in the FibVID dataset, where disinformers seem to adopt a higher readability level in their communications. This could indicate a more sophisticated or tailored approach to creating and sharing content, potentially aimed at a different audience or seeking to establish a semblance of credibility. The divergent trends in readability between the two datasets highlight the variable nature of fake news strategies and underscore the importance of context in analyzing and understanding misinformation. The disparity may reflect differences in the topics covered, the intended audiences, or the evolving tactics of disinformers as they adapt to various platforms and audience reactions. It also suggests that readability, as a standalone feature, might not be a reliable indicator of misinformation across different contexts, necessitating a more nuanced, multidimensional approach to fake news detection that considers a range of linguistic, psychological, and behavioral factors. Understanding these complexities is crucial for developing more effective strategies to identify and combat fake news, as well as for tailoring educational and intervention efforts to diverse audiences and evolving misinformation landscapes. 5.7. Communication style Fig. 9 shows a noteworthy observation from the linguistic analysis that informers utilize a significantly higher frequency of words denoting insight (informers = 107.14 and disinformers = 86.89) and (informers = 104.21 and disinformers = 78.84) for FakeNewsNet and FibVID dataset, respectively. Additionally, informers also result in higher causation scores (informers = 104.74 and disinformers = 74.12) and (informers = 105.61 and disinformers = 81.41) for FakeNewsNet and FibVID datasets, respectively, which may be attributed to a tendency to elaborate upon the information they disseminate. This propensity aligns with findings from other studies, such as ( Giachanou et al., 2023 ), which indicated that disinformers exhibit a reduced use of action-oriented language in contrast to informers. Further analysis reveals that informers also engage more in information-seeking and rational discourse (informers = 0.063 and informers = 0.626, respectively) compared to disinformers (disinformers = 0.046 and disinformers = 0.587, respectively) for FakeNewsNet dataset. Moreover, statistical analysis employing t-statistics to compare psycholinguistic features between the two groups demonstrates that informers significantly employ more positive, causative, and insightful language, as well as expressions of trust, anticipation, surprise, anger, and fear. Conversely, disinformers are characterized by greater use of informal netspeak, swear words, and self-disclosing language, indicating a propensity to share personal experiences and opinions. However, no statistically significant differences were noted in the personality features between informers and disinformers, and a strong correlation was observed among different personality measures. This may be attributable to the nature of the texts analyzed, which are more informal and less rich in personality-indicative language than other forms of written communication, such as essays. This suggests a need for cautious application and interpretation of widely used analytical tools in new contexts, such as social media, and underscores the importance of post-analysis for validating the reliability of these tools and the conclusions drawn from them. 5.8. Linguistic feature The methodologies encompass textual analysis, particularly focusing on Word Count (WC) as an indicator of user engagement and conversational dominance. It is imperative to maintain a balanced WC in deceptive contexts to ensure descriptiveness does not compromise accuracy, as excessive verbosity may inadvertently expose falsehoods. Empirical evidence consistently underscores the statistical significance of word count in both source and reactive tweets pertaining to fake news. The average word count (AWC) for rumor-originating tweets was 25, while the response tweets averaged 17.28 words. Notably, the AWC for source tweets of both real and fake news was comparable; however, real response tweets exhibited a higher AWC, potentially reflecting the necessity for more elaborate refutations or engagement in non-rumored discourse. Linguistic Inquiry and Word Count (LIWC) function words—including various pronouns, articles, prepositions, auxiliary verbs, adverbs, conjunctions, and negations—further elucidate user personality traits and communication intent. For instance, personal pronouns suggest considerable insights into users’ communicative approach and personal stance. Notably, the usage frequency of various pronouns differed between disinformer and informer source tweets, with informer tweets displaying a higher average across all pronoun categories. This trend persisted in response tweets, where informer content similarly employed more personal pronouns. While the overall significance of function words varied across reaction tweets, a specific analysis of events revealed discernible differences in the linguistic patterns between disinformer and informer responses, as shown in Fig. 10 . Additionally, the examination of prepositions indicated a higher frequency in disinformer tweets, reflecting a concern for precision in deceptive narratives. Conversely, negation words, often psychologically linked to inhibition, were more prevalent in disinformer reaction tweets, suggesting a defensive or corrective stance in discrediting the fake news. A comparative analysis of linguistic features such as conjunctions, adverbs, auxiliary verbs, and impersonal pronouns further demonstrated a generally lower mean in disinformer source tweets compared to informer ones, with reaction tweets exhibiting increased usage in informer contexts. This nuanced linguistic landscape underscores the complexity and variability in communication patterns across disinformer and informer discourses, highlighting the importance of rigorous linguistic analysis in understanding the dynamics of fake news propagation and response on social media platforms. 5.9. Personality traits Fig. 11 delineates the Big Five personality traits for two distinct user groups. Notably, extroversion and neuroticism emerge as statistically significant traits across both datasets (with all p-values < 0.003), exhibiting consistent trends wherein disinformers are inferred to be more extroverted and less neurotic compared to informers. The trait of extroversion is linked to an individual’s social connectivity, as evidenced by the number of friends, whereas neuroticism correlates with the frequency of one’s postings. Consequently, those who propagate false news might do so to engage or entertain their social circles or perhaps to broaden their social network. Conversely, the infrequency of fake news distribution, in comparison to genuine news sharing, suggests a lower degree of neuroticism among the former group due to their less frequent sharing activities.The remaining three personality dimensions—agreeableness, conscientiousness, and openness—were found to be statistically significant exclusively within the FakeNewsNet dataset (with all p-values < 0.003). Here, individuals promulgating false news are posited to exhibit higher levels of agreeableness, conscientiousness, and openness than informers. Agreeableness reflects the emotional tone (positive or negative) of social media interactions, conscientiousness pertains to the tendency to post about political matters, and openness involves the propensity to share diverse types of media. Therefore, it is postulated that disinformers may have their posting behaviors influenced by a range of emotions and possess a pronounced interest in political affairs, as indicated in Fig. 12 . This suggests a complex interplay of personality traits influencing the dissemination of news, authentic or otherwise, on social platforms. 1. Download: Download high-res image (343KB) 2. Download: Download full-size image Fig. 11. Parameter analysis based on user’s personality traits. 5.10. Insomnia index In our examination, we segmented the 24-hour cycle into daytime and nighttime periods, defining the latter as 10 PM to 5 AM and the former as 5:01 AM to 9:59 PM, utilizing the local time of the user. We then analyzed the normalized discrepancy in the volume of tweets disseminated during these intervals for each individual, as per methodologies outlined in studies ( Rubin, 2010 , Shu, Zhou et al., 2019 ). This analysis revealed that disinformers typically possess newer Twitter accounts compared to informers. Additionally, they tend to have a shorter interval between consecutive tweets, indicating a more frequent engagement with the platform, as shown in Fig. 13. Notably, our findings suggest a heightened activity from these users during nighttime hours, leading to a higher insomnia index for disinformers than informers. This nocturnal tweeting behavior might reflect various underlying factors such as the pursuit of international audiences in different time zones, lifestyle patterns, or strategic timing to influence discourse, necessitating further exploration into the motives and implications of such online activity patterns. The delineation of day and night tweeting behavior provides an intriguing lens through which to understand and differentiate the digital footprints of disinformers versus informers, potentially offering insights into their habits, motivations, and the temporal dynamics of misinformation dissemination. 5.11. Activity index Disinformers are often relatively recent adopters of the platform and may exhibit a lesser degree of proficiency with its functionalities and nuances. Their propensity to tweet more frequently could be interpreted as an effort to augment their social presence and influence, a phenomenon possibly aimed at amplifying their social capital. Additionally, the observed increased nocturnal online activity among these users could be indicative of heightened stress conditions associated with the propagation of fake news. This heightened nighttime engagement might reflect a strategic or compulsive use of the platform, possibly driven by the need to navigate or manipulate the 24-h news cycle, reach different time zones, or exploit times of lower information competition. The correlation between the dissemination of misinformation and increased stress levels warrants a deeper exploration into the psychological and environmental factors influencing these behaviors, as shown in Fig. 14 . It is plausible that the urgency to maintain credibility or propagate certain narratives necessitates a more constant and reactive engagement with the platform, thereby contributing to irregular or intensive tweeting patterns. Furthermore, the lesser familiarity with the platform might lead to inefficient navigation and increased activity as users attempt to establish their footing and influence within the digital community. Understanding these dynamics is critical for developing more effective strategies to identify, monitor, and mitigate the spread of misinformation across social media platforms. 5.12. Emotions index Our analysis indicates that disinformers consistently exhibit a higher expression of negative emotions, including fear, sadness, disgust, and anger, as well as stress, in their tweets compared to those disseminating factual news (all p-values < 0.003). This tendency might be attributed to the inherent attention-grabbing nature of negatively charged information, as the general populace demonstrates a psychological predisposition towards negative news. Consequently, fake news purveyors strategically infuse their tweets with negative sentiments, aiming to enhance virality and engagement within the digital ecosystem, as shown in Fig. 15 . In contrast, informers typically express more positive emotions, such as happiness and inspiration, yet they also exhibit signs of fear, possibly reflecting concerns over the misinformation landscape or the societal implications of news (all p-values < 0.003). The motivational spectrum for these users often extends beyond mere information dissemination; they engage with social media to foster connections, share personal achievements, offer advice, and provide support. This demographic demonstrates a pronounced skepticism towards unverified content, aligning their social media behavior with broader objectives of constructive community engagement and information integrity. The differential emotional landscapes of these two user groups suggest distinct psychological profiles and behavioral patterns, influencing how they interact with news and information on social platforms. Understanding these divergent approaches and their underpinning motivations offers valuable insights into the mechanics of information spread and the emotional strategies employed in the propagation or containment of news, fake or otherwise. It underscores the need for nuanced approaches in digital literacy and misinformation mitigation efforts, tailored to address the specific emotional and cognitive mechanisms at play. 6. Theoretical and practical implications Our investigation offers potentially significant insights into the characteristics of disinformers and the possibilities for their automated identification, yet it is not without its limitations and ethical implications. A primary limitation stems from the reliance on automated tools to deduce user personality traits from their tweets. While these tools have demonstrated commendable predictive accuracy in other datasets, they are inherently susceptible to errors, a commonality among automated systems. Additionally, these tools are typically calibrated and validated on conventional text, not specifically on the idiosyncratic and often abbreviated language found in social media texts, which may lead to less accurate personality inferences from tweets. The absence of concrete ground truth data regarding users’ personality traits further compounds the challenge of validating these inferred characteristics. Ethically, our research, which pivots on user-level classification, raises important considerations. It is crucial to underscore that any system capable of distinguishing between informers and disinformers must not be employed to stigmatize individuals who have previously circulated misinformation. Rather, such systems should be utilized to augment research in fake news detection and prevention. For instance, they could serve as assistive tools for alerting users who may unknowingly disseminate false information, thereby promoting a more informed and critical approach to news sharing. Additionally, recognizing and amplifying the visibility of informers’ contributions could incentivize accurate information dissemination. While our approach shows promise in the realm of social media, its applicability to other domains, such as politics, sports, and entertainment, might be limited. Customizing the textual information encoder to specific domains, especially those influenced by ethical considerations or political censorship, could enhance adaptability and performance. Overcoming challenges related to domain shift and labeling, as indicated by recent studies, could further expand the model’s versatility. It is essential that any proposed system distinguishes between user categories, integrating ethical considerations at each stage. This includes ensuring that the system respects privacy, avoids reinforcing biases or stereotypes, and contributes constructively to the digital information ecosystem. In doing so, it can support a more nuanced and ethical approach to understanding and mitigating the spread of misinformation online. The intersection of technology, user behavior, and ethics presents a complex landscape that requires careful navigation to harness the potential benefits while mitigating the risks and ethical dilemmas inherent in such endeavors. 7. Conclusions A novel deep learning approach , utilizing Graph Convolutional Neural Networks enhanced with multi-head attention mechanisms, has been introduced to differentiate between users primarily engaged in disseminating false information (disinformers) and those dedicated to sharing factually accurate content (informers) across social media platforms. Our model capitalizes on an extensive range of meticulously curated psycholinguistic attributes extracted from users’ posts, encompassing factors such as sentiment analysis, emotional content assessment, linguistic traits, personality profiling, readability metrics, and communication style analysis. These attributes undergo further enrichment through BERT embeddings to bolster textual representation quality. Additionally, our framework incorporates the creation of two distinct graph structures: the user interaction graph and the Semantic propagation graph. These graphical representations serve as pivotal instruments for organizing the dynamic interactions and dissemination patterns of information among users. For each individual user, a hand-crafted set of features is crafted, encapsulating their unique characteristics and behavioral patterns. These features are subsequently integrated into the User Interaction Graph, facilitating a more nuanced understanding and representation of user interactions within the model. The efficacy of the proposed framework has been demonstrated through extensive evaluations on established datasets. Notably, the model achieved impressive performance metrics, with accuracy and precision scores of 91.07% and 91.24% on the FakeNewsNet dataset, and 94.20% and 94.92% on the FibVid dataset, respectively. These results underscore the robustness and effectiveness of our approach in discerning between disinformers and informers, showcasing its potential utility in combating misinformation on social media platforms. 7.1. Limitations The model’s demonstration of the effectiveness of psycholinguistic attributes in discerning between informers and disinformers sheds light on the intricate nuances of language use in the context of information dissemination. Our comprehensive comparative analysis delved into specific linguistic markers, revealing distinct preferences and tendencies among these groups. Disinformers, for instance, were found to gravitate towards informal language styles, incorporating netspeak and swear words into their discourse. In contrast, informers exhibited a preference for positive language, often articulating causal relationships and employing work-related vocabulary. This contrast extended to the emotional tone as well, with informers displaying higher levels of trust, anticipation, and occasional anger in their communication. These findings underscore the importance of linguistic features such as information-seeking language and trust as key indicators not only of content accuracy but also of emotional and relational aspects within communication. However, alongside these insights, our analysis also uncovered challenges related to the estimation of personality traits, particularly when dealing with texts from social media platforms like Twitter. The observed high correlation among estimated traits suggests potential limitations in the methodology employed for analyzing tweets compared to more conventional text sources. This highlights the critical need for cautious consideration and thorough validation of automated feature extraction tools, especially in the realm of research where nuanced interpretations of linguistic data are essential. 7.2. Future work Future research directions entail a more comprehensive exploration of the interrelationships among personality traits, with a particular focus on understanding how post-vocabulary influences these correlations. Additionally, there is a growing interest in incorporating psycholinguistic attributes into advanced fake news detection systems, thereby advancing both our comprehension of misinformation dynamics and the technological tools essential for combating it effectively. Furthermore, upcoming endeavors will explore the potential of quantum computing-based neural networks for extracting highly relevant features that can be instrumental in addressing the challenges posed by the dissemination of disinformation. These initiatives collectively aim to foster a deeper understanding of the complexities surrounding misinformation while harnessing cutting-edge technologies to develop more robust and efficient solutions. The Youth Vote in 2024 Author: CIRCLE, Date: 2024-11-06 11/6/24 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: DCCSFI7B Cite Key: CIRCLE24youthVote2024 Zotero Item | Lit Note The 2024 general election was a major opportunity for youth to exercise their democratic rights, use their political voices, and shape the future of the country. As part of our work to understand young people’s civic learning and participation, we have been tracking major trends about the youth vote and sharing key data on election week about young people’s participation, priorities, and choices at the ballot box. Here’s some of what we know about young voters immediately following the election. Data + Analysis: Young Voters in 2024 Overall Youth Turnout Down From 2020 But Strong in Battleground States Updated with data as of November 6, 5:00pm ET We estimate that 42% of young voters (+/- 1%), ages 18-29, cast ballots in the 2024 presidential election, a lower youth turnout than in 2020—when our early estimate put youth turnout above 50%—and approximately on par with the 2016 presidential election. Georgia, Michigan, Nevada, North Carolina, Pennsylvania, and Wisconsin. Young voters cast 14% of all ballots in the 2024 election, according to the National Election Pool exit poll conducted by Edison Research. While this number may be adjusted in the coming days, and other data sources may show different numbers, this 2024 youth share of the vote was also lower than in 2020 (17%) and to 2016 (19%) based on the sane data source. Our full analysis of youth voter turnout and highlights from the youth vote choice and issues data on this page are in our news release here (https://circle.tufts.edu/latest-research/overall-youth-turnout-down-2020-strong-battleground-states) . Youth Vote +6 for Harris, But Young Men +14 for Trump Updated with data as of November 6, 11am ET Young voters favored Kamala Harris over Donald Trump in the 2024 election by 6 points: 52% to 46%. That was a much smaller margin than young voters gave President Biden over Trump in 2020 (+25), but still by far the strongest support for Harris of any age group in this election. Five percent of young voters backed a third-party or independent candidate, slightly higher than the 3% who did so in 2020. Young Voters Give Harris Strongest Support in 2024, but Less than for Biden in 2020 The percentage of voters, by age group, who supported each candidate in the 2024 presidential election. Kamala Harris Donald Trump Other 46% 52% 47% 51% 46% 51% 48% 52% f in Ages 18-29 Ages 30-44 Ages 45-64 Ages 65+ Note: Updated with data as of November 6, 11am ET Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press [Get the data](javascript:void(0)) There was a notable difference by age within the 18-29 age group. Voters ages 18-24 supported Harris by 10 points, while their slightly older peers (25-29) supported Trump by 1 point. These voters ages 25-29, which includes the oldest Gen Z voters and the youngest Millennials, were ages 21-25 in the 2020 election. That year, 18- to 24-year-olds supported Biden by 29 points, which suggests either a major shift from those same young people or very different segments of the youth electorate turning out in 2020 and 2024. The Gender Gap: Young Women +18 for Harris, Young Men +14 for Trump As many observers expected before the election, there was a significant gender gap among young voters. Young women preferred Harris to Trump by an 18-point margin: 58% to 40%. But young men preferred Trump by an 14-point margin: 42% to 56%. Notably, that gender gap is larger than the gender gap for the electorate overall. Among voters of all ages, 53% of all female voters backed Harris, compared to 46% for Trump (+7). While among all male voters, Trump won 54% to 44% (+10). Youth who identify as nonbinary or in a way other than male or female favored Harris by a 69-point margin (82% vs. 13% for Trump) and were also more likely than other youth to vote for a third-party candidate. Major Youth Gender Gap: Young Women +18 for Harris, Young Men +14 for Trump The national vote choice of youth ages 18-29 in the 2024 and 2020 presidential elections. Harris (2024) / Biden (2020) Trump Other Young Women in 2024 58% 40% Young Women in 2020 65% 33% Young Men In 2024 42% 56% Young Men in 2020 56% 41% Nonbinary/Other Youth in 2024 82% 13% 6% f in Notes: Data not available for nonbinary youth in 2020. Data may not add up to 100% due to rounding. Updated with data as of November 6, 11am ET. Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image In past elections, the youth gender gap has been driven largely by young white men’s stronger support for the Republican candidate. In an upcoming analysis we will examine vote choice by race/ethnicity and gender together. Favor Trump Black youth and Asian youth voted for Harris over Trump by the highest margin: 54% to 23% among Black youth, and 72% to 23% among Asian youth. Young Hispanic/Latino voters also preferred Harris, but by a smaller margin: 58% to 38%, and white youth voted for Trump over Harris 54% to 44%. Youth of Color Favor Harris; White Youth Favor Trump in the 2024 Presidential Election The percentage of voters of different racial and ethnic groups (ages 18-29) that supported each candidate in the 2024 presidential election. Donald Trump Kamala Harris Other candidate f in White African American or Black Latino or Hispanic Asian Notes: Updated with data as of November 6, 11am ET. Survey data does not allow us to reliable share vote choice for other racial/ethnic groups. Tufts University Tisch College · CIRCLE Source: CIRCLE Analaysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press [Get the data](javascript:void(0)) Non-College and Rural Youth Favor Trump There was also a notable gap by educational attainment. Young people whose highest level of education is a high school diploma or less preferred Trump over Harris by a 10-point margin: 54% to 44%. By contrast, youth with at least some college experience backed Harris by at least 12 points. The percentage of young voters (ages 18-29), by level of education, who supported each candidate in the 2024 presidential election. Donald Trump Kamala Harris Other in High school or less Some college/assoc. degree College graduate Postgraduate study Note: Updated with data of November 6 at 11am ET Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Breaking down the data by education alongside race and gender also provides some insights. Young white men preferred Trump regardless of whether they had a college degree. However, young white women with a college degree preferred Harris, and those without favored Trump. Youth of color preferred Harris by similar margins whether they have a college degree or not. When comparing to the slightly older 30-44 age groups, we find that the youngest white men with a college degree were more likely to vote for Trump than the older voters. On the other hand, the youngest white and nonwhite women without college degrees were more likely to support Harris than their 30- to 44-year-old counterparts. The percentage of youth by age, race, gender, and education who voted for each presidential candidate in the 2024 presidential election. Kamala Harris Donald Trump Other Ages 18-29 White men, college 42% 56% 2% degree White men, no college 32% 67% 1% degree Non-white men, 58% 38% 4% college degree Non-white men, no 54% 43% 3% college degree White women, college 60% 39% 1% degree White women. no 43% 55% 2% college degree Non-white women, 75% 22% 3% college degree Non-white women, no 74% 24% 3% college degree Ages 30-44 White men, college 55% 42% 2% degree White men, no college 31% 67% 2% degree Non-white men, 61% 37% 3% college degree Non-white men, no 53% 43% 3% college degree White women, college 58% 40% 2% degree White women. no 34% 65% 2% college degree Non-white women, 73% 24% 2% college degree Non-white women, no 64% 33% 3% college degree Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image in (56% to 42%) and young urban voters (61% to 37%) strongly preferred Harris. The vote choice of rural youth usually varies widely by race, and we will break down that data in an upcoming analysis. Youth in Rural Areas Supported Trump, Urban and Suburban Youth Backed Harris The percentage of voters, by rurality, who supported each candidate in the 2024 presidential election. Kamala Harris Donald Trump Another Candidate f in Urban Suburban Small town/Rural Note: Updated with data as of on November 6 at 11am ET Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Source: Unless otherwise noted, data comes from CIRCLE analyses of the AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Gender Gap Driven by Young White Men, Issue Differences Updated with data as of November 6, 11am ET While young voters overall supported Kamala Harris by a 6-point margin in this election, that was a much smaller margin than the 25 point support young voters gave President Biden in 2020. Much of that shift appears to be driven by young men, who voted for Biden in 2020 but Trump in 2024, and especially from young white men. Early data suggests that there was an extraordinary 32-point gap in youth vote choice by gender, with young women favoring Harris by 18 percentage points, and young men favoring Trump by 14 points. Nonbinary/other youth supported Harris by a 69-point margin. The data by gender and race combined shows that this shift was largely driven by young white men, who voted for Trump by a 28 point margin. They were the only race/gender group to prefer Trump, though young white women and young Latino men appear to have split their vote equally between Trump and Harris. Black and Asian women had the highest level of support for Harris. Split their Vote The presidential choice of young voters (18-29), by race and gender in the 2024 election. Kamala Harris Donald Trump Other in White men White women Black men Black women Latino men Latino women AAPI men AAPI women 35% 63% 49% 49% 64% 34% 85% 13% 48% 47% 66% 31% 64% 31% 79% 15% 6% Notes: Sample size does not allow reporting on groups beyond male and female. Due to rounding, data may not add up to 100%. Updated with data as of November 6 at 11am ET. Tufts University Tisch College · CIRCLE Source: SOURCE: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Some of these numbers represent major shifts from 2020. In that election, young white men supported Biden over Trump by 6 points, young white women by 15 points, and young Latino men by 40 points. There were also differences by education and gender. Notably, both young men with a college degree and those without a college degree preferred Donald Trump—though the latter by a much larger margin. Young women with a college degree were the most likely to support Harris, voting for her by 30 points. 2024 The presidential choice of young voters (ages 18-29), by education and gender in the 2024 election. Kamala Harris Donald Trump Other Men without a college degree 40% 58% Men with a college degree 47% 51% Women without a college degree 55% 43% Women with a college degree 64% 34% Note: Sample size does not allow for identities beyond male and female. Due to rounding, data may not add up to 100%. Updated with data as of November 6 at 11am ET. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Issue Priorities by Gender May Have Driven Vote Choice Our previous analysis of youth issue priorities (https://circle.tufts.edu/2024-election#economy-was-the-top-youth-issue,-drove-youth-vote-for-trump) and vote choice found that young people who named immigration, crime, and the economy were much more likely to vote for President Trump. And young men were more likely than young women to select each of those issues as their top concern this election—especially immigration. Youth who chose immigration as their top issue in 2024 voted for Trump by a nearly 70-point margin. in Meanwhile, youth who selected climate change and abortion were much more likely to vote for Harris, and young women were more than twice as likely than young men to choose abortion as their number one priority. Focused on Immigration The presidential choice of young voters (ages 18-29), divided by gender, who listed each issue as their top prioritiy in the 2024 election. Young Men Young Women in The Economy And Jobs Health Care Immigration Abortion Crime Climate Change Gun Policy 42% 39% 5% 6% 7% 10% 15% 9% 8% 17% 6% 4% 8% 7% Note: Data does now allow us to reliably share data on gender identities other than male and female. Updated with data as of November 6 at 11am ET. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image These differing issue priorities, combined with the extraordinary differences in which candidate young voters preferred based on their #1 issue, partially explains the wide gap between the vote choices of young men and women. Source: Unless otherwise noted, data comes from CIRCLE analyses of the AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press. Economy Was the Top Youth Issue, Drove Youth Vote for Trump Updated with data as of November 6, 11am ET and immigration. And while young voters who prioritized abortion were overwhelmingly likely to vote for Kamala Harris, youth who considered the economy or immigration their top issue favored Donald Trump by wide margins. The AP VoteCast survey allowed voters to choose one of nine issues as the “most important facing the country.” Forty percent of youth selected “the economy and jobs,” 13% chose abortion, and 11% selected immigration as the most important. Young voters were slightly more likely than other age groups to select abortion as their top priority, and they were the only group for whom abortion was in 2nd place as a top issue. Other age groups were more likely to choose immigration than abortion as their top priority. Latino Youth and Overall Youth Priorities Were Very Similar: Economy and Jobs Was the Top Issue The percentage of young Latinos and of all youth (ages 18-29) who said each was their top issue in the 2024 election. Latino or Hispanic All Youth f in The Economy And Jobs Immigration Abortion Health Care Climate Change Gun Policy Crime Racism Foreign Policy 41% 40% 11% 11% 11% 13% 9% 9% 7% 8% 7% 5% 5% 5% 5% 5% 3% 4% Notes: Updated with data as of November 6, 11am ET. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Young people’s issue priorities shaped their vote choice in the presidential election. While youth overall were more likely to vote for Harris, support was highest among youth who prioritized abortion, climate change, health care, racism, and gun policy. However, ( ) Young Voters Who Prioritized the Economy, Immigration, and Crime Favored Donald Trump The presidential choice of young voters (ages 18-29) who listed each issue as their top prioritiy in the 2024 election. Kamala Harris Donald Trump Other Immigration 15% 84% The economy and jobs 38% 60% Crime 42% 55% Foreign policy 49% 43% 9% Gun policy 74% 24% Health care 74% 23% Racism 73% 23% Abortion 79% 20% Climate change 82% 16% Note: Updated with data as of November 6 at 11am ET. Due to rounding, data may not add up to 100% Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press [Get the data](javascript:void(0)) f in Issue priorities can vary by race, gender and other factors. In subsequent analyses we’ll explore these differences among groups of youth. One major question about the youth vote in this election was the potential impact of the Israel-Palestine conflict. While only 4% of youth chose “foreign policy” as their top issue, being able to select only one issue in the poll may fail to provide an accurate picture of the extent to which youth were concerned about the issue. And among youth who did choose foreign policy as their top priority, 9% voted for a third-party candidate, much higher than youth overall (2%). Notably, when asked which candidate they trusted more to handle “the situation in the Middle East”, 32% said Harris, 45% Trump, 6% both equally, and 16% of youth said neither. Some ambivalence about the candidates they cast ballots for may also be evident in young people’s answers about whether their vote was motivated more by support for their chosen candidate or opposition to their opponent. When asked whether their vote was more reflective of support for their chosen candidate or opposition to the opponent, 81% of young Trump voters said their vote was for Trump and 19% that it was against Harris. Among youth who voted for Harris, 66% said their vote was for her and 34% that it was against Trump. Source: Unless otherwise noted, data comes from CIRCLE analyses of the AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Youth Vote Choice Varied Widely by State; Some Major Shifts Toward Trump Updated with data as of November 6, 11:00am ET While nationally the youth vote favored Kamala Harris over Donald Trump by 6 points: 52% to 46%, there were major differences by state, and there appears to be an overall trend of increased youth support for Donald Trump compared to the 2020 presidential race. According to AP VoteCast data, Kamala Harris saw gains over Joe Biden’s numbers with young voters in only three states: Maine, Wisconsin, and Indiana. Meanwhile, in states like Louisiana, Missouri, Florida, Iowa, Ohio, Nebraska, South Carolina, and Texas, the data suggests that Trump lost the youth vote in 2020 but won it in 2024. In key battleground states like Arizona, Michigan, Georgia and Pennsylvania, where Biden had strong double-digit youth support in 2020, Harris’ still won young voters but by a much smaller single-digit margin. The youth vote choice (ages 18-29) for Kamala Harris and Donald Trump, the margin between them, and the margin between President Biden and Donald Trump in 2020. Search in table Page 1 of 3 State Kamala Harris Donald Trump Other 2024 Margin 2020 margin Alabama* 41% 56% 3% R+15 R+7* Alaska* 47% 48% 5% R+1 N/A* Arizona 52% 46% 3% D+6 D+22 Arkansas 38% 59% 3% R+21 N/A California 60% 36% 4% D+24 D+49 Colorado 61% 36% 3% D+25 D+31 Connecticut 51% 46% 2% D+5 D+36 Florida 44% 54% 1% R+10 D+30 Georgia 53% 46% 1% D+7 D+19 Illinois 52% 47% 1% D+5 D+27 Indiana 48% 49% 2% R+1 R+6 Iowa* 41% 55% 3% R+14 D+1* Kansas* 39% 58% 3% R+19 R+5* Kentucky 37% 61% 2% R+24 R+4 Louisiana 42% 55% 4% R+13 D+11 Notes: State-level data has a margin of error of at least +/-5 percentage points, and states with an asterisk have a margin of error above +/- 8 percentage points. Data not available for all states. Updated with data as of November 6 at 11am ET. Tufts University Tisch College · CIRCLE Download image Source: Unless otherwise noted, data comes from CIRCLE analyses of the AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press The Youth Electorate Was More Republican and Conservative in 2024 Updated with data as of November 6, 11am ET in The significant shift in youth vote choice (+25 for President Biden over President Trump in 2020, +6 for Harris over Trump in 2024) can be examined and explained in different ways. In other analyses, we have focused on demographic shifts; for example, young men and white youth (https://circle.tufts.edu/2024-election#youth-vote-+6-for-harris,-but-young-men-+14-for-trump) (https://circle.tufts.edu/2024 election#economy was the top youth issue, drove youth vote for trump) , which was young people’s top priority and drove the youth vote for Trump. One other way to examine and explain this shift is the partisan and ideological makeup of the electorate. Compared to 2020, young voters in 2024 were more likely to identify as conservative and as Republican. And the young moderates who cast ballots were more likely to vote for Trump A Different Partisan and Ideological Make-Up While young voters in 2024 were still more likely to identify as Democrats than as Republicans or with neither party, this year’s youth electorate was more Republican than it was in 2020 by 9 percentage points. Democratic-identifying youth dropped by 5 points and those who identify as “neither” decreased by 4 points. Note that this may not indicate ideological shifts in all young people, but in those who turned out to vote in each election. The 2024 Electorate Had More Youth Who Identify as Republicans than 2020 The 2020 and 2024 youth electorates broken down by identification with Democrats, Republicans, or neither. Democrat Republican Neither 2020 2024 47% 30% 23% 42% 39% 19% Notes: Updated with data as of November 6, 11am ET. Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press [Get the data](javascript:void(0)) In addition to the youth electorate’s party identification shifting, their preference for candidates also changed. After supporting Biden by a margin of 33 percentage points In 2020, young people who don’t consider themselves a part of either major party supported Harris by 17 points in 2024. There were also slight shifts in ideological identification. The 2024 youth electorate had 3-point fewer “liberal” youth and 4-points more “conservative” youth. And while the share of young voters who identify as “moderate” was the same in 2020 and 2024 (30%), moderate voters broke for Trump by a 3-point margin this year after supporting Biden by a 20-point margin in the previous election. The ideological breakdown of young voters ages 18-29 in the 2024 and 2020 presidential elections. Liberal Moderate Conservative in 2024 2020 42% 30% 29% 45% 30% 25% Notes: Data may not add up to 100% due to rounding. Updated with data as of November 6, 11am ET. Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image In fact, youth ages 18-29 were the only age group in the 2024 election whose moderate-identifying voters preferred Trump over Harris. More Young Male Voters of Color Identified as Republicans As expected from the major gender gap in youth vote choice (https://circle.tufts.edu/2024-election#gender-gap-driven-by-young-white-men,-issue-differences) , there were gender differences in party identification of the youth electorate. Young women were 14 points more likely than young men to identify as Democrats, and young men 11 points more likely than young women to identify as Republicans. There were also big shifts in party identification by race/ethnicity. Those shifts were strongest among young men, especially young Black men (+20 identifying as Republican compared to 2020) and young Latino men (+13). Even with those shifts, young Black and Latino men still were more likely to identify as Democrats, while young white were 25 points more likely to identify as Republicans—an 11 point increase over 2020. than in 2020 Among voters 18-29, there were large shifts in party identification between 2020 and 2024, especially by race/ethnicity. Those shifts were strongest among young men. 2024 Swing Toward Democrat Republican Neither Republicans White men 27… 52% 2… +11 White women 40% 43% 1… +6 Black men 56% 31% 13% +20 Black women 70% 13% 1… +6 Latino men 43% 38% 2… +13 Latino women 58% 26… 16% +9 All others 50% 1… 31% +1 in 2020 White men 32% 41% 27… White women 45% 37% 1… Black men 69% 11% 2… Black women 72% 7% 2… Latino men 53% 2… 2… Latino women 64% 1… 1… All others 54% 1… 28% Notes: Data as of November 6 at 11am EST. Tufts University Tisch College · CIRCLE Download image There were also more Republican young women in the 2024 youth electorate: 6 percentage points more white women and Black women compared to 2020, and 9 points more Latina women. Young people who identify as neither major party also made up a smaller part of the 2024 youth electorate. That is especially notable because young people overall are less likely than other age groups to identify with any of the major political parties (https://circle.tufts.edu/latest-research/young-peoples-ambivalent-relationship-political-parties) . Other Signs of a New and Different Electorate A significant number of young voters in 2024 were voting in a general election for the first time (30%), with some major differences by race. Latino/Hispanic youth had an especially high share of new voters (40%), followed by Asian youth (33%), Black youth (32%), Young Latinos and youth without college experience have historically participated at lower rates, so it may be a positive sign that there were more new voters among these groups in 2024. Forty percent of the voting-eligible youth population are young people without college experience, and 38% of all youth votes were cast by this group in 2024, which puts these non-college youth closer to full representation in the electorate Young Latinos Prioritized the Economy, Shifted Toward Trump Updated with data as of November 6, 11am ET Young Latinos favored Vice President Harris by a substantial 20-point margin over former President Trump in the 2024 presidential election. However, that’s a significant shift from their 2020 vote choice, when they preferred President Biden by a 49-pont margin over Trump. That is the largest shift for any racial/ethnic group of youth between the 2020 and 2024 elections. Among all Latinos, youth were still the age group with the strongest support for Harris in an election in which the entire Latino electorate shifted toward Trump. Young Latinos Backed Harris by 20 Points, The Highest Margin of Any Latino Age Group The percentage of Latinos, by age group, who supported each candidate in the 2024 presidential election. Kamala Harris Donald Trump Another Candidate f in Ages 18-29 Ages 30-44 Ages 45-64 Ages 65+ Note: Data may not add up to 100% due to rounding. Updated with data as of November 6, 11am ET Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image This data from the AP VoteCast survey shows Latinos overall with double-digit support for Harris. Other sources, like the National Election pool exit poll, showed a smaller, single-digit margin for Harris among all Latinos. One of the other highly notable things about the young Latino electorate in 2024 is that it was relatively “new”—meaning, it was full of young people who had never voted before in a general election. While every youth electorate has a sizable percentage of new voters who have aged into voting forty percent of young Latinos who cast a ballot did so for the first time compared to about a third previously were not engaged or mobilized in this cycle. In subsequent work, we hope to explore that question and what may have made these new Latino voters different from those who had voted before. 2 in 5 Young Latino Voters in 2024 Were Voting in their First General Election The percentage of young voters (ages 18-29), by race, who had or had not voted before in a general election First Time Voting Had Voted Before Latino or Hispanic 40% 60% AAPI 33% 67% African American or Black 32% 68% White 26% 74% Note: Updated with data of November 6 at 11am ET Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press [Get the data](javascript:void(0)) Key Differences by Gender and Education f in The Latino vote shift toward Donald Trump appears to have been driven largely by young men. While young Latinas backed Harris by 35 points, young Latino men split their vote fairly evenly between Trump and Harris. Vote Evenly The presidential choice of young voters (18-29), by race and gender in the 2024 election. Kamala Harris Donald Trump Other in White men White women Black men Black women Latino men Latino women AAPI men AAPI women 35% 63% 49% 49% 64% 34% 85% 13% 48% 47% 66% 31% 64% 31% 79% 15% 6% Notes: Sample size does not allow reporting on groups beyond male and female. Due to rounding, data may not add up to 100%. Updated with data as of November 6 at 11am ET. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image There were also differences by education—both within Latino youth and differences between young Latinos and other young people. Young Latinos without any college experience were less likely than those with some college experience or a degree to favor Harris, but they still favored her by 13 points. That’s a notable difference from the youth electorate overall: among all young voters, those without any college experience preferred Donald Trump. Youth with postgraduate studies also favored Harris by a similar margin as non-college Latino youth, but those youth with advanced studies are a relatively small group in the sample. Back Harris The percentage of young Latino voters (ages 18-29), by level of education, who supported each candidate in the 2024 presidential election. Kamala Harris Donald Trump Other in High school or less Some college/assoc. degree College graduate Postgraduate study Note: Data may not add up to 100% due to rounding. Updated with data of November 6 at 11am ET Tufts University Tisch College · CIRCLE Source: CIRCLE analysis of AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Young Cubans Preferred Trump, Other Latinos Backed Harris Looking at Latino youth by country of origin, nearly every group preferred Harris to Trump in the 2024 election—with the exception of young Cuban voters, who backed Trump by 22 points and, notably, 8 percent voted for a third-party candidate. Salvadoran, Mexican, and Puerto Rican youth had the highest support for Kamala Harris. It’s important to note that the sample size for most of these groups, except for Mexican and Puerto Rican youth, are fairly small and thus the data has a higher margin of error (above +/-12 points). For the groups with faded bars below, the margin between Trump and Harris is within the margin of error. Latino Youth The presidential choice of young voters (18-29), by race and gender in the 2024 election. Kamala Harris Donald Trump Other in Salvadoran Mexican Puerto Rican Other Central American South American Dominican Cuban 66% 33% 62% 35% 62% 36% 59% 36% 58% 41% 53% 44% 35% 57% 8% Notes: Due to smaller samples, the margin of error for groups other than Mexican and Puerto Rican is higher than +/- 12 points. For groups with faded bars, the difference in vote choice for Harris or Trump is within the margin of error. Due to rounding, data may not add up to 100%. Updated with data as of November 6 at 11am ET. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Young Latinos Share Economic Concern, Issue Priorities with their Peers Like all young voters in 2024, young Latinos overwhelmingly selected the economy and jobs as their top issue priority in this presidential election. More than 40% of young Latinos selected it as their #1 issue, about the same as all voters. No other issue was chosen by more than 12% of young Latinos—abortion and immigration followed the economy with 11% each. The striking similarities in young Latinos’ issue priorities and those of all young voters may further speak to the fact that the economy was the dominant issue in this election. Because this survey only allowed youth to select one issue, it may obscure other differences in concerns between Latinos and other youth, which we will explore in our subsequent work. the Top Issue The percentage of young Latinos and of all youth (ages 18-29) who said each was their top issue in the 2024 election. Latino or Hispanic All Youth in The Economy And Jobs Immigration Abortion Health Care Climate Change Gun Policy Crime Racism Foreign Policy 41% 40% 11% 11% 11% 13% 9% 9% 7% 8% 7% 5% 5% 5% 5% 5% 3% 4% Notes: Updated with data as of November 6, 11am ET. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Source: Unless otherwise noted, data comes from CIRCLE analyses of the AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press White Youth Favored Trump Overall, Key Differences by Education Updated with data as of November 6, 11am ET White youth continue to make up a disproportionate amount of the youth electorate. Despite making up only 56% of the eligible voting population ages 18-29, they made up 66% of young voters in the 2024 election, similar to their 65% share in 2020. This reflects and continues a long-time trend of inequity in voter turnout by race and ethnicity y pp p g g White youth favored President Trump by a 10-point margin over Vice President Harris in the 2024 presidential election. In 2020, despite showing the strongest support of any racial/ethnic group for Trump, white youth still ultimately supported President Biden by 6 points. Notably, that 16-point swing toward Trump is still the smallest shift for any racial/ethnic group of youth between the 2020 and 2024 elections. All age groups of white voters favored Trump over Harris, but only 30-to-44-year-olds supported Harris more than the younger 18-29 age group. White Youth Favored Trump by a 10-Point Margin; White Voters of All Ages Backed Trump The percentage of white voters in each age group who voted for each presidential candidate in 2024 Kamala Harris Donald Trump Other f in White Voters 18-29 White Voters 30-44 White Voters 65+ White Voters 45-64 44% 54% 2% 46% 52% 2% 40% 59% 2% 43% 56% 1% Note: Updated with data as of November 6, 11am ET Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image Key Differences by Gender and Education White youth had the smallest gender gap in vote choice of any racial group. Young white men provided President Trump with the highest support of any race/gender group of youth, overwhelmingly preferring him to Harris (63% vs 35%). Young white women split their vote, with 49% supporting each candidate. White youth underwent a substantial shift toward Trump from 2020, when young white men favored Biden over Trump by 6 points and young white women chose Biden by 15 points. Harris: +14 and +4, respectively. Meanwhile, young people with some college experience (which includes current college students) narrowly supported Trump (+2), and those with only high school experience overwhelmingly favored Trump by 33 points. Notably, white youth without college experience supported Trump by 14 percentage points more than the entire (all ages) electorate with that level of educational attainment. Youth Without College Experience Gave Trump Major Advantage; Youth With College Degrees Favored Harris The percentage of young white voters (18-29), divided by education, who voted for each presidential candidate in 2024 Kamala Harris Donald Trump Other f in High school or less Some college/assoc. degree College graduate Postgraduate study 33% 66% 2% 48% 50% 2% 51% 47% 2% 56% 42% 2% Note: Updated with data as of November 6, 2024, 11am EST. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image White Youth Share Economic Concerns, Other Priorities Differ by Gender Like all other racial/ethnic groups, white youth ranked the economy and jobs as their top issue in this election (41%). Abortion (14%) and immigration (13%) were the next two most frequently chosen priorities. And when it comes to those other issues beyond the economy there were significant divides by gender, likely causing some of the difference in their vote choice. For example, 17% of young white men selected immigration as their top issue concern, compared to 10% of young white women. On the other hand, 18% of young women picked abortion as their top issue, compared to 8% of young white men. Gender Young white voters (ages 18-29), divided by gender, who listed each issue as their top priority in the 2024 election. Young white men Young white women in The Economy And Jobs Immigration Abortion Climate Change Health Care Gun Policy 43% 41% 17% 10% 4% 5% 8% 18% 8% 8% 6% 10% Note: Data does now allow us to reliably share data on gender identities other than male and female. Updated with data as of November 6, 11am EST. Tufts University Tisch College · CIRCLE Source: AP VoteCast Survey conducted by the AP-NORC Center for Public Affairs Research for Fox News and The Associated Press Download image (https://circle.tufts.edu/2024-youth-poll) CIRCLE Pre-2024 Election Youth Survey (https://circle.tufts.edu/2024-youth-poll) CIRCLE's early poll of youth (ages 18-34) ahead of the 2024 presidential election highlights major trends in young people's political views and participation. (/latest-research/youth-voter-registration-major-challenge-2024-election) State by State: Youth Voter Registration (/latest-research/youth-voter-registration-major-challenge-2024-election) In 36 states where data is available, we are tracking changes in youth voter registration compared to 2020 (/latest-research/changes-election-laws-may-affect-youth-voting-2024) Election Laws and Policies (/latest-research/changes-election-laws-may-affect-youth-voting-2024) CIRCLE research examines how changes in voting and registration policies can shape youth voting in the 2024 election. Media Inquiries CIRCLE experts are available to discuss our data, analyses, and insights about youth participation in the 2024 presidential election. To ask a question or request an interview, email us at circle@tufts.edu. © Tufts University 2024 (https://www.tufts.edu) Morning consult political risk ratings: Ratings definitions & methodology primer Author: McMann, Jason I., Date: 2024 Collections: ElectionPredFeats Zotero Key: JKBYBJH2 Cite Key: McMann24mornCnsltRateDefMeth Zotero Item | Lit Note Ratings Definitions & Methodology Primer H2 2024 Morning Consult Political Risk Ratings Ratings Definitions & Methodology Primer |Authors|Overview| |---|---| |Jason I. McMann, PhD Head of Political Intelligence jmcmann@morningconsult.com Contributors Sonnet Frisbie Deputy Head of Political Intelligence sfrisbie@morningconsult.com|Morning Consult's Global Political Risk Ratings provide high-frequency, survey-derived risk ratings for 36 global markets on a monthly basis. Ratings are narrowly defni ed to refel ct the degree of public approval of the incumbent chief executive and of their country’s trajectory in each rated market. For corporates, the ratings are intended to facilitate scenario planning and risk management efofrts. For market actors and ratings agencies, the ratings are intended to serve as a quantitative political input into portfolio allocation strategies and investment decision-making, including by serving as an input into sovereign credit ratings models. This document provides detailed information on the ratings’ design, the underlying survey data, key terminology, and related methodological considerations.| ||Commercial Access & Related Information:| |Data Inquiries politics@morningconsult.com Press Inquiries press@morningconsult.com|Latest Ratings & Market Commentary: The latest Global Political Risk Ratings, companion monthly Market Commentary & Risk Outlook (MACRO) Political Briefni gs, and summary charts by market and market group are available here for Morning Consult Pro subscribers. Contact pro@morningconsult.com with subscription inquiries. Ratings Data: Machine-readable ratings data is available here for Morning Consult Pro Plus subscribers. Contact pro@morningconsult.com with subscription inquiries. High-Frequency Component Data: The underlying component data used to produce Morning Consult’s Political Risk Ratings — measured on a higher-frequency, daily basis for all rated markets — is available via API for enterprise use. Contact politics@morningconsult.com with inquiries. Global Political Intelligence Data Methodology Primer: Additional methodological information on our Global Political Intelligence Data — from which the underlying ratings data derives — is available here.| 1. Overview: High-frequency, survey-derived political risk ratings 1A. Ratings Overview Morning Consult's Global Political Risk Ratings provide high-frequency, survey-derived risk ratings for 36 global markets on a monthly basis. The underlying survey data used to produce the ratings is collected daily. Ratings are narrowly defined to reflect the average degree of public approval of the incumbent chief executive and of their country’s trajectory in each rated market. For corporates, the ratings are intended to facilitate scenario planning and risk management efforts. For market actors and ratings agencies, the ratings are intended to serve as a quantitative political input into portfolio allocation strategies and investment decision-making, including by serving as an input into sovereign credit ratings models. To the best of our knowledge, Morning Consult’s ratings are the only high-frequency survey-derived sovereign political risk metrics available for commercial use at the time of writing. The table below provides a summary of ratings coverage. Table 1. Summary of Ratings Coverage Total rated markets 36 (see Table A5 in Appendix for details) 2017-present; variable by market (see Table A5 in Temporal coverage Appendix for details) Ratings frequency Monthly[1] Component data frequency Daily Variable by market; see here for the latest sample Target sample size per market sizes) Publication cadence Monthly (during the first half of each month) 1 All ratings and data generally publish no later than the seventh day of each month. In cases where the seventh day falls on a weekend or holiday, ratings and data generally publish within the following two business days. Consult our annual publication schedule here. |Table 1. Summary of Ratings Coverage|Col2| |---|---| |Total rated markets|36 (see Table A5 in Appendix for details)| |Temporal coverage|2017-present; variable by market (see Table A5 in Appendix for details)| |Ratings frequency|Monthly1| |Component data frequency|Daily| |Target sample size per market|Variable by market; see here for the latest sample sizes)| |Publication cadence|Monthly (during the frist half of each month)| 1B. Availability of Ratings, Underlying Data, and Market Commentary Current and historical risk ratings and the corresponding data used to produce them publish monthly during the first half of each month, along with a companion Market Commentary & Risk Outlook (MACRO) Political Briefing, covering risk ratings, ratings action (upgrades and downgrades) ratings watches (intra-risk band movement), and related volatility and confidence metrics. Market commentary is produced and maintained by Morning Consult’s global political analysts. 1C. Ratings Construction, Terminology, and Methodology The remainder of this document provides detailed information on the construction of Morning Consult’s Global Political Risk Ratings, key terminology, and related methodological considerations. 2. Ratings Design 2A. Underlying Concepts Morning Consult's Global Political Risk Ratings provide high-frequency, survey-derived risk ratings for 36 global markets on a monthly basis. The underlying survey data used to produce the ratings is collected daily. Ratings are narrowly defined to reflect the average degree of public approval of the incumbent chief executive and of their country’s trajectory in each rated market. 2B. Ratings Construction For each rated market, Morning Consult's Global Political Risk Ratings are defined as the simple intra-month average of two component data series: (1) the net share of adults who approve of the incumbent chief executive (“net leader approval”) and (2) the net share of adults who say their country is moving in the right direction (“net right direction” or alternatively, “net country trajectory”), both scaled on the -100 to 100 interval. To compute the risk ratings, we first take the daily average of the two component series, and subsequently compute the intra-month mean of the resulting values, yielding a monthly “risk reading” ranging from -100 to 100. Readings are subsequently rounded to the nearest whole number and mapped onto an ordinal ratings band scale, yielding monthly “risk ratings” ranging from AAA to U (unstable). Readings are not normalized or rescaled to preserve range alignment with the underlying component data series, which similarly range from -100 to 100. Ratings of BBB- or higher, commonly used to denote the cutoff for investment grade markets and/or assets,[2] corresponds to risk readings above zero in Morning Consult’s ratings scale, reflecting net-positive public sentiment. Table A2 (see Appendix) provides complete survey question wording and scales for each component data series, and outlines the transformations used to produce the composite risk readings. Table A3 (see Appendix) provides a complete mapping from risk readings to ratings bands. 2 See S&P Global Ratings. 2022. Guide to Credit Rating Essentials. p.9.. See also Fitch Ratings. June 11, 2024. Ratings Defnitions: Special Reporti . p.6. 2C. Additional Information on Component Data Series The component data used to provide Morning Consult’s Political Risk Ratings derives from a standard battery of daily surveys fielded in each rated market using non-probability sampling methods. All survey interviews are conducted online. Data is collected using quota sampling and weighted daily to represent the target population within each market. The target population reflects the 18+ general population, literate population, or internet-using population depending on market factors. Weighting parameters vary by market. Sample sizes similarly vary by market, ranging from approximately several hundred to several thousand respondents per day. The component data used to produce the ratings reflects survey responses among adults (ages 18+) in each surveyed market. Consult our Global Political Intelligence Data Methodology Primer for additional details on fielding mechanisms, weighting procedures/parameters and sample sizes. 3. Key Ratings Terminology Morning Consult issues monthly “ratings action,” “ratings watches,” and “risk trajectories” based on variation in underlying risk readings in each rated market. Morning Consult also issues monthly “forward outlooks” for markets subject to ratings action (i.e. upgrades/downgrades) and for select markets subject to ratings watches. This section describes each concept in detail, along with related volatility and confidence metrics. Table A1 (see Appendix) provides a summary of all key terminology discussed herein. Table A3 (see Appendix) provides a complete listing of all bands and their corresponding notches. 3A. Ratings Risk “ratings” reflect markets’ assignment into ordinal ratings ranging from AAA (the highest rating, reflecting the lowest risk level) to U (the lowest rating, reflecting the highest risk level) based on their underlying risk readings. All letter ratings are notched with plus and minus sub-ratings (e.g. A+, A-) with the exception of the lowest rating (U, corresponding to “unstable”), referred to hereafter as “notches.” Table A3 (see Appendix) provides a complete listing of all bands and their corresponding notches. “Ratings transitions” refer to a transition from one rating to another. 3B. Ratings Action Morning Consult issues “ratings action” for rated markets on a monthly basis indicating whether a market has experienced a ratings “upgrade” or “downgrade,” defined as a month-on-month transition from a lower ratings band to a higher band (in the event of an upgrade), or from a higher ratings band to a lower band (in the event of a downgrade).[3] Ratings bands include AAA, AA, A, BBB, BB, B, CCC, and U. Table A3 (see Appendix) provides a complete listing of all bands and their corresponding notches. Ratings action represents our most aggregate-level metric of whether a rated market’s risk profile has changed. 3 Ratings action assessments are made based on underlying risk readings rounded to the nearest whole number prior to taking their month-on-month difference. See additional discussion of risk readings below. Markets that do not experience a month-on-month ratings transition are assessed as “stable,” reflecting a lack of ratings action. Ratings actions always reflect either increasing month-on-month risk readings (in the event of an upgrade) or decreasing risk readings (in the event of a downgrade). By contrast, risk readings (see discussion below) may increase or decrease in the absence of ratings action, reflecting changes in risk readings that fall short of a ratings transition. 3C. Ratings Watches Morning Consult issues “ratings watches” for rated markets on a monthly basis indicating whether a market has experienced a transition from one ratings “notch” to another within the same letter band (e.g. from AA+ to AA, from BBB+ to BBB, from CCC to CCC-). Ratings watches represent our second most aggregate-level metric of whether a rated market’s risk profile has changed.[4] “Downside ratings watches” indicate a transition from a higher notch to a lower notch within the same band, indicating an increase in risk. “Upside ratings watches” indicate a transition from a lower notch to a higher notch within the same band, indicating a decrease in risk. Each ratings band aside from U has three notches. Table A3 (see Appendix) provides a complete listing of all bands and their corresponding notches. Ratings watches always reflect either increasing or decreasing month-on-month risk readings. By contrast, risk readings may increase or decrease in the absence of ratings watch issuance, reflecting changes in risk readings that fall short of a ratings transition. Markets for which Morning Consult has issued ratings action in a given month are not assigned ratings watches and are marked as “not applicable.” Additionally, Morning Consult’s political analysts may periodically issue a “discretionary ratings watch” in cases where they assess that a particular market’s political dynamics are worth monitoring absent the issuance of a formal ratings watch. The factors informing such watches are described in detail in Morning Consult’s monthly Market Commentary & Risk Outlook (MACRO) Political Briefings. 3D. Risk Trajectories Morning Consult issues “risk trajectories” for rated markets on a monthly basis indicating whether a market has experienced increasing or decreasing “risk readings” month-on-month, corresponding to positive and negative trajectories, respectively. “Risk trajectories” represent our most micro-level metric of whether a rated market’s risk profile has changed. Markets that do not experience a month-on-month change in risk readings are assessed as “stable.” As distinct from ratings action and ratings watches, which are issued based on ratings transitions, risk trajectories reflect movement in the underlying numeric risk readings scaled on the -100 to 100 interval. As 4 Ratings action assessments are made based on underlying risk readings rounded to the nearest whole number prior to taking their month-on-month difference. See additional discussion of risk readings below. such, risk trajectories are assessed for every rated market on a monthly basis regardless of whether they have experienced ratings action or a ratings watch. Table A3 (see Appendix) provides a complete mapping of risk readings onto their respective bands and sub-bands. 3E. Companion Risk Trajectory Metrics: Volatility and Confidence To facilitate more nuanced risk profile assessments, Morning Consult leverages intra-month variability in market-specific risk readings to produce volatility and confidence metrics for all rated markets. The sections below describe their use and measurement. 3E.1. Volatility Morning Consult defines volatility as the intra-month standard deviation in the risk reading for a given rated market, rounded to the nearest whole number, resulting in volatility assessments of “increasing,” “stable,” or “decreasing.” Higher volatility indicates greater dispersion of a country’s intra-month risk readings around the mean. In the context of Morning Consult’s Market Commentary & Risk Outlook (MACRO) Political Briefings, increasing volatility is interpreted as a potential signal of growing risk profile instability, while decreasing volatility is interpreted as a potential signal of growing risk profile stability. Volatility assessments are not performed for newly rated markets for which less than a single month of data exists and therefore precludes them. Volatility readings are dependent on daily sample sizes in each rated market, which are variable across markets. Volatility readings should therefore only be compared within markets across time, and not across markets. 3E.2 Baseline Confidence Confidence reflects a statistical assessment of the risk trajectory for a rated market, reflecting the likelihood that a month-on-month change in a market’s risk reading has occurred. Morning Consult derives baseline confidence metrics from difference-in-means tests of the intra-month difference in risk readings for each rated market. To do so, we employ two-sample t-tests with unequal variances, where the tested mean is the intra-month risk reading and the tested variance is derived from daily risk reading observations within the indicated month. Tests providing statistical evidence of month-on-month differences are assigned a “medium” or “high” confidence, corresponding to statistically significant test results at the 10% and 5% levels or higher, respectively. Rated markets whose monthly tests fail to meet either of the above significance thresholds are assigned “low” confidence. Confidence assessments are not performed for newly rated markets for which a single month of data exists and therefore precludes them. Table A4 (see Appendix) provides a summary mapping of confidence assessments onto statistical significance. Because confidence assessments rely on volatility metrics as inputs, confidence readings should only be compared within markets across time, and not across markets. In addition, markets commonly exhibit marked risk adjustments in the window just after elections or similar political transitions. Intra-country comparisons of confidence should therefore be attentive to sharp movements in confidence readings during such time periods. 3F. Forward Outlooks Morning Consult issues monthly “forward outlooks” for rated markets experiencing upgrades/downgrades and for select markets experiencing ratings watches as part of its Market Commentary & Risk Outlook (MACRO) Political Briefings. Outlooks reflect the six-month forward-looking risk assessment of Morning Consult’s analysts, and are assessed as positive, negative or stable with high, medium, or low certainty. 4. Additional Risk Ratings Considerations This section outlines additional risk ratings considerations, including temporal continuity, retroactive data stability, handling of political transitions, tracking dates, and weighting updates. 4A. Temporal Continuity All component data used to produce Morning Consult’s Global Political Risk Ratings is measured continuously (i.e. no temporal gaps exist in the data series). All survey questions used to produce the ratings are similarly temporally stable (i.e. question wording remains unchanged over the full ratings history for each rated market). Combined, these features yield ratings series that are fully temporally continuous for all rated markets.[5] 4B. Retroactive Data Stability Morning Consult occasionally reprocesses ratings component data in the event of survey fraud (such as that arising from bot activity), resulting in limited retroactive shifts in the data due to the elimination of fraudulent responses and higher quality ratings. All component data is otherwise fully retroactively stable. Weighting adjustments, when implemented, are applied strictly on a forward-looking basis, facilitating systematic historical analysis by end-users of the ratings and component data. Please contact us with any questions regarding prior episodes of data reprocessing. Morning Consult clients are apprised of data reprocessing incidents on an ongoing basis. 5 The two sole exceptions for rated markets are Spain (for which leader approval data for Pedro Sánchez is unavailable from June 18-20, 2022, and the Czech Republic (for which leader approval data for Petr Fiala is unavailable from March 11-16, 2023). 4C. Updates to Leader Approval Tracking Following Political Transitions In the event of a political transition, Morning Consult generally begins tracking public approval of the new chief executive within one week of his/her inauguration, at which time tracking of the prior chief executive concludes. In the event of overlapping tracking, risk ratings incorporate approval of the new incumbent leader from the first complete day of available tracking data; all overlapping days of data resulting from ongoing tracking of the prior leader following the seating of the new one are removed from the data prior to constructing the ratings. Please contact us with questions regarding exact tracking start and end dates for chief chief executives in rated markets. 4D. Handling of Intra-Month Political Transitions In cases of intra-month political transitions (i.e. transitions that do not occur on the first date of a given month), Morning Consult’s “net leader approval” component data prices in public approval of all incumbent leaders in a given month, such that their approval is treated interchangeably. 4E. Start Dates for Rated Markets For all rated markets for which component data readings are available prior to the 16th day of the first month of tracking, risk ratings are available beginning in that same month. For rated markets for which component data readings are available from the 16th day of the first month of tracking onwards, risk ratings are available beginning in the subsequent month. This criteria applies solely to the first month in which a rated market has been incorporated in Morning Consult’s Global Political Risk Ratings. See Table A5 for a complete listing of rating start dates by market. 4F. Market-Specific Weighting Adjustments Morning Consult periodically updates its survey weights and weighting variables for rated markets to better reflect the demographic composition of the target population. Weighting targets are reviewed annually, with changes to markets with new population data occurring in the first week of each calendar year, alongside occasional intra-year weighting updates. Please contact us for additional information on the history of weighting adjustments for rated markets. 5. Market Commentary & Risk Outlook (MACRO) Political Briefing Morning Consult’s monthly Market Commentary & Risk Outlook (MACRO) Political Briefings provide a complete set of ratings metrics for all rated markets — covering ratings action and watches, risk trajectories, risk readings, and related volatility and confidence metrics — along with analyst commentary for all markets for which Morning Consult has issued ratings action, and selective commentary for markets for which Morning Consult has issued ratings watches. MACROs publish monthly alongside the latest ratings. The latest MACRO and six months of prior briefings are made available here. 6. Ratings Validation Morning Consult relies on externally available data to validate its political risk metrics. At present, Morning Consult has focused on validation on two fronts: (1) relationships between risk readings and economic policy uncertainty, and (2) relationships between risk readings and the likelihood of social unrest. The findings below provide a summary of validation findings to date. Please contact us with additional questions on validation methodology. 6A. Economic Policy Uncertainty Morning Consult’s risk readings are inversely correlated with prominent global metrics of economic policy uncertainty — specifically Blake et al.’s Economic Policy Uncertainty indexes — across rated markets for which external validation data is available, reflecting higher economic policy uncertainty (more positive values on y-axis) in markets with higher political risk (more negative values on x-axis), and lower economic policy uncertainty in markets with lower political risk, where indexes are measured via normalized counts of newspaper articles discussing economic policy uncertainty. Figure 1. Morning Consult Risk Readings vs. Economic Policy Uncertainty Indexes with Simple Linear Model Best Fit Line Notes: Dots represent country-month risk reading observations (x-axis) plotted against country-month Economic Policy Uncertainty Indices (y-axis) reflecting normalized counts of newspaper articles discussing economic policy uncertainty. Higher x-axis values indicate lower political risk. Higher y-axis values indicate higher economic policy uncertainty. Blue line represents best-fit line from a simple linear model with 95% confidence interval (gray shaded region). Morning Consult data covers extended markets for which corresponding Economic Policy Uncertainty data is available from January 2022 onwards. Source: Morning Consult data comes from Morning Consult Intelligence. Economic Policy Uncertainty Index data comes from https://www.policyuncertainty.com. For additional details, see Baker et al. “Measuring Economic Policy Uncertainty.” The Quarterly Journal of Economics. July 11, 2016. Vol. 131. Issue 4. Pages 1593-1636. For country-specific data sources, see the following: Australia, Brazil, Canada, France, Germany, India, Italy, Mexico, South Korea, Russia, United Kingdom, United States: Baker, Bloom and Davis (2016); Chile: Cerda, Silva and Valente (2016); China: Baker, Bloom, Davis and Wang (2013); Colombia: Gil and Silva (2018); Global: Davis (2016); Greece: Hardouvelis, Karalas, Karanastasis and Samartzis (2018); Ireland: Zalla (2016); Japan: Arbatli, Davis, Ito and Miake (2019); The Netherlands: Kroese, Kok and Parlevliet (2015); Singapore: Davis (2016); Spain: Ghirelli, Perez, and Urtasun (2019); and Sweden: Armelius, Hull, and Köhler (2017). 6B. Social Unrest As described in earlier sections, Morning Consult’s Political Risk Ratings are derived from two measures of public satisfaction with the political status quo: (1) approval of the incumbent chief executive and (2) perceptions that the country in general is headed in the right direction (or not). As such, one way to validate our risk metric is to evaluate its relationship to a physical manifestation of political risk outside of the regular election cycle: protests. To that end, Morning Consult’s global political analysts have used the underlying risk readings as an input into “Protest Predictor” – a predictive model for protest activity maintained by Morning Consult. The direct relationship between Morning Consult’s risk readings and protest prevalence — which we assess using the Armed Conflict Location and Event Data (ACLED) — is complex and non-linear, in keeping with the literature and as seen in Figure 2 below. Figure 2. Morning Consult Risk Readings vs. ACLED Protest Counts with LOESS Curve Sources: Morning Consult data comes from Morning Consult Political Intelligence. Social unrest data comes from the Armed Conflict Location and Event Data. See www.acleddata.com for additional details. Nevertheless, the relationship is systematic. Including our risk readings as an input to the model improved predictions of protest likelihood over a baseline estimate of average past levels of unrest. Adding controls for the level of democracy, urbanization, and food inflation improved the model further. Figure 3 below reports model results showing actual versus predicted protests levels by rated market. Figure 3. Actual vs. Predicted Protest Levels Over Time Source: Morning Consult data comes from Morning Consult Intelligence. Social unrest data comes from the Armed Conflict Location and Event Data (ACLED). See www.acleddata.com for additional details. Control variable data on food inflation comes from the Food and Agriculture Organization, FAOSTAT. Control variable data on urbanization comes from the World Bank Group, World Bank DataBank. Control variable data on democracy comes from Figure 3. Actual vs. Predicted Protest Levels Over Time Source: Morning Consult data comes from Morning Consult Intelligence. Social unrest data the Varieties of Democracy (V-Dem) Project’s Liberal Democracy Index, as processed by Our World in Data. More information on Morning Consult’s Protest Predictor model is forthcoming. Contact us for additional details. 7. Commercial Availability of Higher-Frequency Ratings Data The underlying component data used to produce Morning Consult’s monthly Global Political Risk Ratings is collected on a daily basis for all rated markets, and can be used to produce daily risk ratings and readings for clients who envision higher-frequency use-cases. All data is available via API for enterprise use. Contact us with inquiries. Appendix Table A1. Key Terminology The following table defines key terminology used in the context of Morning Consult’s Global Political Risk ratings. Table A1.1 Risk Ratings & Ratings Action Terminology |Term|Defni ition|Possible Values| |---|---|---| |Risk Ratings|Risk “ratings” refel ct countries’ assignment into ordinal ratings ranging from AAA (the highest rating, refel cting the lowest risk level) to U (the lowest rating, refel cting the highest risk level) based on their underlying risk readings, rounded to the nearest whole number. Ratings are notched with plus and minus sub-ratings (e.g. A+, A-), referred to as “notches.”|1. AAA (unnotched) 2. AA (with +/- notches) 3. A (with +/- notches) 4. BBB (with +/- notches) 5. BB (with +/- notches) 6. B (with +/- notches) 7. CCC (with +/- notches) 8. U (unstable)| |Rating Transition|A transition from one rating to another.|| |Ratings Band|Aggregate letter ratings bands, which group all ratings sharing the same letter combination into a single band. Ratings action (see below) is determined on the basis of movement across ratings bands.|1. AAA (unnotched) 2. AA (unnotched) 3. A (unnotched) 4. BBB (unnotched) 5. BB (unnotched) 6. B (unnotched) 7. CCC (unnotched) 8. U (unnotched)| |Ratings Band Transition|A transition from one ratings band to another.|| |Ratings Action|Indicates whether a market has experienced a ratings “upgrade” or “downgrade.” Markets that do not experience either are assessed as “stable,” refel cting a lack of ratings action.|1. Upgrade 2. Stable 3. Downgrade| |Upgrade|A month-on-month transition from a lower ratings band to a higher ratings band, indicating decreasing risk.|| |Downgrade|A month-on-month transition from a higher ratings band to a lower ratings band, indicating increasing risk.|| Table A1.2 Ratings Watch Terminology |Term|Defni ition|Possible Values| |---|---|---| |Ratings Watch|Indicates whether a market has experienced an intra-band ratings transition, refel cting movement from one ratings “notch” to another within the same letter band (e.g. from AA+ to AA, from BBB+ to BBB, from CCC to CCC-). Markets for which Morning Consult has issued ratings action in a given month are not assigned ratings watches and are assessed as “not applicable.”|1. Upside 2. Downside 3. Not Applicable (n/a)| |Upside Ratings Watch|“Upside ratings watches” indicate a transition from a lower notch to a higher notch within the same band, indicating decreasing risk.|| |Downside Ratings Watch|“Downside ratings watches” indicate a transition from a higher notch to a lower notch within the same band, indicating increasing risk.|| |Discretionary Ratings Watch|A ratings watch issued at the discretion of Morning Consult’s political analysts in cases where they assess that a particular market’s political dynamics are worth monitoring absent the issuance of a formal ratings watch.|1. Upside 2. Downside| Table A1.3 Risk Readings Terminology |Term|Defni ition|Possible Values| |---|---|---| |Risk Reading|The simple intra-month average of (1) net approval of the incumbent chief executive and (2) net country trajectory, defni ed as the net share of adults in each market who say their country is headed in the right direction, each measured on a daily basis. Specifcially, to compute the risk ratings, we frist take the daily average of the two component series, and subsequently compute the intra-month mean of the resulting values, yielding a monthly “risk reading” ranging from -100 to 100. Higher values correspond to lower risk. Ratings assignments (see preceding rubric) are made based on underlying risk readings rounded to the nearest whole number.|● -100 to 100 (rounded to the nearest whole number)| |Risk Reading Volatility|The intra-month standard deviation of daily risk readings for a given country.|| |Risk Reading Volatility, Monthly Change|The month-on-month difefrence in volatility, corresponding to “increasing,” “stable” (refel cting no change), or “decreasing.” Assessments are made based on volatility readings rounded to the nearest whole number prior to taking their difefrence.|1. Increasing 2. Stable 3. Decreasing| Table A1.4 Risk Trajectory & Outlook Terminology |Term|Defni ition|Possible Values| |---|---|---| |Risk Trajectory|Indicates whether a country’s underlying risk readings are increasing, stable, or decreasing month-on-month, corresponding to a “positive,” “stable” (refel cting no change), or “negative” trajectory. A value of “pending” indicates that a newly rated market does not have sufcfiient temporal history to assess its risk trajectory. Trajectory assessments are made based on risk readings rounded to the nearest whole number prior to taking their difefrence.|1. Positive 2. Stable 3. Negative 4. Pending (trajectory cannot be assessed due to insufcfiient temporal history)| |Confdi ence (“Baseline Confdi ence”)|Refel cts a statistical confdi ence assessment for a rated market’s monthly risk trajectory, derived from an independent two-sample difefrence-in-means test with unequal variances of risk readings assessed month-on-month. Reported confdi ence metrics refel ct the statistical signifciance of the test results.|1. High (p-value <= .05) 2. Medium (p-value > .05 and <= .10) 3. Low (p-value > .10) (Assessment thresholds are inverted for markets with stable risk readings, refel cting the statistical likelihood that no month-on-month change in risk has occurred.)| |Forward Risk Outlook|Issued for rated markets experiencing upgrades/downgrades and for select markets experiencing ratings watches. Refel cts the six-month forward-looking risk assessment of Morning Consult’s analysts, assessed as “positive,” “stable,” or “negative.”|1. Positive 2. Stable 3. Negative| |Outlook Certainty|The degree of certainty assigned to the forward risk outlook by Morning Consult’s analysts, assessed as “high,” “medium,” or “low.”|1. High 2. Medium 3. Low| Table A2. Component Data Series: Survey Question Wording, Response Scales, and Transformation into Net Metrics Component Series Survey Wording/Net Metric Response Scales 1. Leader Approval Do you approve or disapprove Strongly approve of the job [chief executive Somewhat approve name] is doing as [chief Somewhat disapprove executive type, e.g. prime Strongly disapprove minister]? Don’t know/No opinion Resulting Net Net leader approval Approval – Disapproval Metric (monthly mean of daily net metric) 2. Country Now, generally speaking, Right direction Trajectory would you say that things in Wrong track your country are going in the right direction, or have they pretty seriously gotten off on the wrong track? Resulting Net Net right direction Right Direction — Wrong Track Metric (monthly mean of daily net metric) Note: Risk readings for each rated market reflect the simple intra-month average of net leader approval and net right direction. |Component Series|Survey Wording/Net Metric|Response Scales| |---|---|---| |1. Leader Approval|Do you approve or disapprove of the job [chief executive name] is doing as [chief executive type, e.g. prime minister]?|Strongly approve Somewhat approve Somewhat disapprove Strongly disapprove Don’t know/No opinion| |Resulting Net Metric|Net leader approval|Approval – Disapproval (monthly mean of daily net metric)| |2. Country Trajectory|Now, generally speaking, would you say that things in your country are going in the right direction, or have they pretty seriously gotten off on the wrong track?|Right direction Wrong track| |Resulting Net Metric|Net right direction|Right Direction — Wrong Track (monthly mean of daily net metric)| Table A3. Ratings Scale The following table provides a complete mapping of risk readings onto ratings bands and ratings.[6] Risk Reading: Risk Reading: Rating Band Rating Lower Bound Upper Bound AAA AAA 90 100 AA AA+ 80 90 AA 70 79 AA- 60 69 A A+ 50 59 A 40 49 A- 30 39 BBB BBB+ 20 29 BBB 10 19 BBB- 9 0 BB BB+ -10 -1 BB -20 -11 BB- -30 -21 B B+ -40 -31 B -50 -41 B- -60 -51 CCC CCC+ -70 -61 CCC -80 -71 CCC- -90 -81 U U -100 -91 6 Ratings of BBB- or higher, commonly used to denote the cutoff for investment grade markets and/or assets, corresponds to risk readings above zero in Morning Consult’s ratings scale, reflecting net-positive public sentiment. |Rating Band|Rating|Risk Reading: Lower Bound|Risk Reading: Upper Bound| |---|---|---|---| |AAA|AAA|90|100| |AA|AA+|80|90| ||AA|70|79| ||AA-|60|69| |A|A+|50|59| ||A|40|49| ||A-|30|39| |BBB|BBB+|20|29| ||BBB|10|19| ||BBB-|9|0| |BB|BB+|-10|-1| ||BB|-20|-11| ||BB-|-30|-21| |B|B+|-40|-31| ||B|-50|-41| ||B-|-60|-51| |CCC|CCC+|-70|-61| ||CCC|-80|-71| ||CCC-|-90|-81| |U|U|-100|-91| Table A4. Confidence Mapping The following table provides a complete mapping of difference-in-means test results onto risk trajectory confidence bands. Difference-In-Means Baseline Statistical Significance Confidence <= .05 High > .05 and <= .10 Medium > .10 Low Note: Assessment thresholds are inverted for markets with stable risk readings, reflecting the statistical likelihood that no month-on-month change in risk has occurred. |Difefrence-In-Means Statistical Signifciance|Baseline Confdi ence| |---|---| |<= .05|High| |> .05 and <= .10|Medium| |> .10|Low| Table A5. Rated Markets |Market Argentina|Ratings Available From: November 2021| |---|---| |Austria|September 2021| |Australia|January 2019| |Belgium|December 2021| |Brazil|January 2019| |Canada|January 2019| |Chile|November 2021| |Colombia|November 2021| |Czech Republic|December 2021| |France|December 2018| |Germany|December 2018| |India|December 2018| |Indonesia|August 2021| |Ireland|April 2022| |Israel|March 2023| |Italy|May 2020| |Japan|January 2019| |Malaysia|December 2021| |Mexico|December 2018| |Nigeria|November 2021| |Netherlands|September 2021| |Norway|September 2021| |---|---| |Peru|December 2022| |Pakistan|January 2022| |Philippines|September 2021| |Poland|June 2022| |South Africa|November 2021| |Romania|May 2024| |South Korea|June 2020| |Spain|May 2020| |Sweden|September 2021| |Singapore|June 2022| |Thailand|September 2021| |Turkey|November 2021| |United Kingdom|October 2018| |United States|January 2017| Table A6. Market Classifications The following table provides Morning Consult’s default classification of rated markets by region and bloc. “Core markets” and “extended markets” are designations internal to Morning Consult which reflect markets where shared historical coverage is continuously available across all such markets. Morning Consult’s MACRO Political Briefings routinely report average risk readings among the other groups of markets indicated in this table; only those markets identified as Core and Extended within each group are included when constructing the averages to facilitate temporal comparisons. Assignments to developed and emerging/frontier market categories were made based on available MSCI market classifications[7] at the time of writing, such that Morning Consult’s developed markets classification is aligned with the MSCI World Index,[8] and Morning Consult’s emerging and frontier markets composite classification is aligned with membership across the MSCI Emerging[9] and MSCI Frontier[10] Indexes. The exceptions are Argentina and Nigeria, which Morning Consult classifies as emerging and frontier markets and which MSCI classifies as stand-alone markets. Classification Definition Markets Core Markets for which ratings are Australia, Brazil, Canada, France, Markets available continuously since at Germany, India, Japan, Mexico, United least January 2019; average Kingdom, United States values reported from that date Extended Markets for which ratings are Argentina, Austria, Belgium, Chile, Markets available continuously since at Colombia, Czech Republic, Indonesia, least January 2022; average Italy, Malaysia, Nigeria, Netherlands, values reported from that date Norway, Philippines, Pakistan, South Korea, Spain, Sweden, Thailand, Turkey, South Africa Developed Markets Morning Consult defines Austria, Australia, Belgium, Canada, Markets as “developed” for ratings France, Germany, Ireland, Israel, Italy, purposes; average values Japan, Netherlands, Norway, Singapore, reported from Jan. 2022 Spain, Sweden, United Kingdom, United States 7 MSCI. Market Classifcationi . 20 June, 2024. 8 MSCI. Developed Markets Indexes. As of 31 May, 2023. 9 MSCI. Emerging Markets Indexes. As of March 2023. 10 MSCI. Frontier Markets Indexes. As of July 2024. Of those markets covered by Morning Consult’s Political Risk Ratings, Pakistan and Romania are classified by MSCI as frontier markets. |Classifciation|Defni ition|Markets| |---|---|---| |Core Markets|Markets for which ratings are available continuously since at least January 2019; average values reported from that date|Australia, Brazil, Canada, France, Germany, India, Japan, Mexico, United Kingdom, United States| |Extended Markets|Markets for which ratings are available continuously since at least January 2022; average values reported from that date|Argentina, Austria, Belgium, Chile, Colombia, Czech Republic, Indonesia, Italy, Malaysia, Nigeria, Netherlands, Norway, Philippines, Pakistan, South Korea, Spain, Sweden, Thailand, Turkey, South Africa| |Developed Markets|Markets Morning Consult defni es as “developed” for ratings purposes; average values reported from Jan. 2022|Austria, Australia, Belgium, Canada, France, Germany, Ireland, Israel, Italy, Japan, Netherlands, Norway, Singapore, Spain, Sweden, United Kingdom, United States| |Emerging & Frontier Markets|Markets Morning Consult defni es as “emerging” for ratings purposes; average values reported from Jan. 2022|Argentina, Brazil, Chile, Colombia, Czech Republic, Indonesia, India, Mexico, Malaysia, Nigeria, Pakistan, Peru, Philippines, Poland, Romania, South Africa, South Korea, Thailand, Turkey| |---|---|---| |G7|Rated Morning Consult markets among the G7 member-countries; average values reported from Jan. 2022|Canada, France, Germany, Italy, Japan, United Kingdom, United States| |G20|Rated Morning Consult markets among the G20 member-countries; average values reported from Jan. 2022|Argentina, Australia, Brazil, Canada, France, Germany, India, Indonesia, Italy, Japan, Mexico, South Africa, South Korea, Turkey, United Kingdom, United States| |Americas|Rated Morning Consult markets in the Americas; average values reported from Jan. 2022|Argentina, Brazil, Canada, Chile, Colombia, Mexico, Peru, United States| |Asia-Pacifci|Rated Morning Consult markets in Asia-Pacifci (APAC); average values reported from Jan. 2022|Australia, India, Indonesia, Japan, Philippines, Malaysia, Singapore, South Korea, Thailand| |Europe|Rated Morning Consulted markets in Europe; average values are reported from Jan. 2022|Austria, Belgium, Czech Republic, France, Germany, Ireland, Italy, Netherlands, Norway, Poland, Romania, Spain, Sweden, United Kingdom| |MENA|Rated Morning Consult markets in the Middle East and North Africa (MENA); average values reported from Jan. 2022|Israel, Nigeria, Pakistan, South Africa, Turkey| LEGAL DISCLAIMER THE INFORMATION PROVIDED IN THIS REPORT IS PROVIDED \"AS IS\" WITHOUT ANY REPRESENTATION OR WARRANTY OF ANY KIND, AND MORNING CONSULT DOES NOT REPRESENT OR WARRANT THAT THE REPORT OR ANY OF ITS CONTENTS WILL MEET ANY OF THE REQUIREMENTS OF A RECIPIENT OF THE REPORT (\"YOU\"). MORNING CONSULT IS NOT PROVIDING FINANCIAL, TAX AND ACCOUNTING, ECONOMIC, POLITICAL, LEGAL, OR ANY OTHER PROFESSIONAL ADVICE BY ALLOWING YOU TO ACCESS AND USE THIS REPORT AND ANY DATA CONTAINED HEREIN. ANY DECISIONS YOU MAKE IN RELIANCE ON THE REPORT AND ANY INTERPRETATION BY YOU OF ANY DATA OR CONTENT HEREIN ARE YOURS ALONE FOR WHICH YOU SHALL HAVE FULL RESPONSIBILITY. MORNING CONSULT IS NOT RESPONSIBLE FOR ANY DAMAGES RESULTING FROM ANY DECISIONS OR INTERPRETATIONS BY YOU, OR BY ANYONE ACCESSING THE REPORT THROUGH YOU, MADE IN RELIANCE ON THE REPORT OR ANY DATA CONTAINED HEREIN, INCLUDING FINANCIAL, TAX AND ACCOUNTING, ECONOMIC, POLITICAL, LEGAL, COMPLIANCE, ADVERTISING, MARKETING, AND/OR RISK MANAGEMENT DECISIONS AND INTERPRETATIONS. THE DATA AND ANALYSIS PROVIDED TO YOU IN THE REPORT IS NOT INTENDED TO CONSTITUTE ANY TYPE OF RECOMMENDATION AS TO THE VALUE OF ANY SECURITIES OR AS TO THE ADVISABILITY OF INVESTING IN, PURCHASING, OR SELLING SECURITIES, AND YOU ARE SOLELY RESPONSIBLE FOR ANY DECISIONS YOU MAY MAKE REGARDING INVESTMENTS IN SECURITIES. From GenAI to Political Profiling Avatars: A Data-Driven Approach to Crafting Virtual Experts for Voting Advice Applications Author: Andrade, Mancera, Date: 2024-06-11 Collections: MediaAdsPolit, PoliticalML Zotero Key: WRWTQ4R5 Cite Key: Mancera24genAIpolitiProfileAvatar Zotero Item | Lit Note From GenAI to Political Profiling Avatars: A Data-Driven Approach to Crafting Virtual Experts for Voting Advice Applications José Mancera Luis Terán jose.mancera@hslu.ch luis.teran@hslu.ch Lucerne University of Applied Sciences and Arts Lucerne, Switzerland University of Fribourg Fribourg, Switzerland ABSTRACT Voting advice applications (VAAs) are pivotal web-based tools that guide citizens to align with political parties and candidates that match their preferences. Traditional methods for creating candidate profiles predominantly rely on questionnaire responses, a time-intensive and costly process. To address these challenges, we introduce a data-centric methodology utilizing generative artificial intelligence (GenAI), culminating in creating political avatars. These political avatars are engineered using cutting-edge large language models (LLMs), including GPT-4 and Bard. They are adept at processing and interpreting data primarily sourced from Twitter and leveraging bespoke, self-trained datasets. Integrating advanced AI technology with diverse data sources equips political avatars with unprecedented analytical and predictive capabilities, setting a new standard in political analysis. Unlike traditional methods, political avatars are adept at emulating the responses of real politicians or experts, showcasing a remarkable capacity to interact with VAA surveys. This novel approach presents the potential to either compete with or enhance the insights traditionally obtained from human experts. Another critical aspect of our study is comparing political avatars and previous research employing question-answering (QA) models based on advanced natural language processing (NLP) techniques for political profiling. This comparative analysis reveals that Political Avatars offer a significantly more robust solution for profile construction. While QA models provide structured responses based on specific queries, political avatars bring an element of dynamism and depth, capable of generating nuanced, context-aware responses. This shift from static, questionnaire-based profiling to dynamic, AI-driven avatars marks a substantial leap in political analysis. Generative AI in crafting Political Avatars introduces a transformative element to data analysis. This approach facilitates a layered and more sophisticated interpretation of political stances, moving beyond the limitations of traditional profiling methods. By This work is licensed under a Creative Commons Attribution-NoDerivs International 4.0 License. DGO 2024, June 11–14, 2024, Taipei, Taiwan © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0988-3/24/06 https://doi.org/10.1145/3657054.3657092 employing political avatars, our methodology not only streamlines the profiling process but also enriches the quality of insights derived, paving the way for a more nuanced understanding of the political landscape. CCS CONCEPTS • Computing methodologies → Natural language processing; Discourse, dialogue and pragmatics; Machine learning algo- rithms; • Applied computing → Computers in other domains; • Information systems → Question answering; • Social and pro- fessional topics → Political speech; KEYWORDS Voting Advice Applications, Question Answering, Natural Language Processing, Social Media, GPT-4, Bard, Large Language Models, Generative AI, Political Avatars ACM Reference Format: José Mancera and Luis Terán. 2024. From GenAI to Political Profiling Avatars: A Data-Driven Approach to Crafting Virtual Experts for Voting Advice Applications. In 25th Annual International Conference on Digital Government Research (DGO 2024), June 11–14, 2024, Taipei, Taiwan. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3657054.3657092 1 INTRODUCTION Voting advice applications (VAAs), such as the prominent Swiss application smartvote, play a crucial role in guiding citizens towards political parties and candidates that align with their personal political beliefs, particularly during elections [20]. These applications are designed to enhance the user’s understanding of political parties in a straightforward and user-friendly manner, thereby boosting their interest in politics. Switzerland, with its unique political landscape characterized by frequent voting due to its direct democracy system, serves as the focal point of this study. Swiss citizens often rely on VAAs to inform their opinions before voting. Established in 2003, smartvote is a leading VAA in Switzerland, managed by Politools, a politically impartial and non-profit organization. It has significantly influenced over 200 local, cantonal, and national elections. Smartvote operates through an election-specific questionnaire covering a wide range of political topics, allowing voters to assess and compare their political positions with those of DGO 2024, June 11–14, 2024, Taipei, Taiwan Mancera and Terán various candidates or parties. Consequently, it provides tailored voting recommendations. Political scientists meticulously crafted this questionnaire. When politicians have not personally constructed their profile, political scientists develop it on their behalf. This work aims to facilitate the role of political scientists by proposing the development of Political Avatars (PA) that can also respond to Voter Advice Applications (VAAs) surveys. We can summarize our contributions as follows: (1) We introduce the political avatars approach within the domain of generative AI (GenAI) for developing political avatars and measure the efficacy of these virtual agents in responding to VAA questions. The evaluation is based solely on data gathered from Twitter and further involves a comparative analysis with the comprehensive data that the large language model (LLM) possesses regarding a politician. (2) A performance quantitative analysis between political avatars and QA Models developed by Nigon, Mancera, and Terán (2023) [36]. This work is structured as follows: Section 2 related works and impacts of VAAs. presents the theoretical background of this research effort. The problem definition, framework context, and GenAI method developed and applied in the execution of this work are presented in Section 3. The experiments and results for a QA Model as a baseline are presented in Section 4. Section 5 evaluates the new strategy using GenAI to create political avatars. Later, Section 6 discusses limitations, recommendations, and applicability for the models. Finally, Section 7 summarizes the work conducted and presents the lessons learned. 2 RELATED WORK The field spectrum involving recommender systems constantly extends into education and learning support, healthcare delivery, and tourism [32]. In addition, it is increasingly used in politics to promote democratic decision-making, improve citizens’ participation, and revitalize civic engagement using cutting-edge technology. Well-known examples of recommender systems in politics are VAAs like the Swiss smartvote platform[1]. VAAs based on political profiling and matching algorithms provide suggestions to citizens regarding the best-matched vote intention [20]. Lately, the use and impacts of VAAs have drawn growing research interest in many countries [17, 27, 40]. In Switzerland, for instance, a significant and growing ratio of the electorate uses these applications in federal, cantonal, and local elections has drawn researchers’ interest [15, 33]. Concerning citizen participation, in several countries, including the Netherlands, Finland, Germany, and Switzerland, VAAs have already evolved institutionalized tools. Although the investigation of VAAs applies primarily to a social and political perspective, the development and implementation of this type of application are also catching the attention of technical-oriented networks within the extent of their research areas. Communities connected to recommender systems, social computing, data mining, eDemocracy, and others are also attracting researchers with contributions relevant to VAA development [2–5, 13, 16, 24, 46, 47]. In recent years, research on the design, implementation, and application of VAAs has led many political and social scientists 1https://www.smartvote.ch/ to study such tools as a significant feature of the election campaign in different countries before national or regional elections [7, 18]. In [19], the authors describe the state-of-the-art VAA research conducted until 2014. Early studies show that VAAs research focused mainly on such platforms’ users [6, 8, 12, 45, 50]. The literature shows the need for trans-disciplinary approaches to improve traditional VAA implementations to implement groundbreaking methods. 2.1 Impact of VAAs The impact of VAAs and the growing number of users require enhancing transparency in developing questionnaires and policy statements towards establishing party positions on high-dimensional political landscapes [37, 48]. Moreover, it is essential to consider selecting a method for matching the profiles of parties/candidates with users that could impact user recommendations [30, 31]. In [19], the authors mention three types of effects that VAAs can have on users—individuals’ information-seeking behavior, cognitive effects, and vote choice—measured by quantitative (turnout) and qualitative (vote intention) methods. Research conducted on the impact of VAAs shows a correlation between the users of VAAs and electoral participation [9, 14, 22, 29, 34]. Moreover, there is proof that recommendations persuaded the users of different voting advice platforms they received in their voting intentions [8, 11, 26, 28, 35, 49]. Later on, [20] showed that the abundance of data generated by VAAs potentiates comparative studies of public opinion and party systems across countries, thereby bridging research on VAAs to general inquiries of political science research [43, 44]. Even though VAAs research applies more from a political and social point of view, the development and implementation of such applications are gaining the attention of technical-oriented networks within the scope of their research fields. Communities related to recommender systems, data mining, social computing, argument mining, and eDemocracy, among others, are also attracting researchers with contributions related to VAAs developments [2– 5, 13, 16, 24, 46, 47]. Based on the impact and growing number of users of VAAs, higher levels of transparency were required to develop questionnaires and policy statements to establish party positions on highdimensional political maps [37, 48]. Additionally, it is essential to consider selecting a method for matching the profiles of parties/candidates with users that could impact user recommendations [30, 31]. In their work, [19] mention three types of effects that VAAs can have on users: individuals’ information-seeking behavior, cognitive effects, and vote choice, measured with quantitative (turnout) and qualitative (vote intention) methods. The conducted research regarding the impact of VAAs shows a correlation between the uses of VAAs and electoral participation [9, 14, 22, 29, 34, 41]. Additionally, evidence was found that VAA users declared that the recommendations that the different platforms provided influenced their voting intentions [1, 8, 11, 26, 28, 35, 39, 49]. Later on, [20] show the abundance of data generated by VAAs potentiates comparative studies of public opinion and party systems across countries and thereby bridges research on VAAs to general questions of political science research. From GenAI to Political Profiling Avatars DGO 2024, June 11–14, 2024, Taipei, Taiwan 3 PROBLEM DEFINITION AND METHODOLOGY One of the most challenging steps towards the design of VAAs is the generation of candidates’ profiles, which are constructed either by using answers reported by the candidates themselves or by their political parties or by using the answers reported by experts (e.g., academics and journalists, among others). Depending on how the profiles are constructed, they can be biased from the candidates’ real political position and can thus mislead the citizens’ vote decision. This work uses politicians’ Twitter data to answer the VAA questionnaire. Usually, when candidates do not have a profile, political experts will answer the questions for the candidate. However, this is tedious, and also, those experts are expensive. Additionally, politicians can be biased while answering the VAA questionnaire. This work can support political experts in analyzing the politician’s answers. In most VAA projects, the profile generation includes only candidate answers and expert opinions. In [43], profile generation for VAAs includes static and dynamic components. Static elements are included to adjust policy statements (𝐼1, ..., 𝐼𝑛). Dynamic elements are composed of different types of parts, including context awareness (CA), user interaction (UI), privacy settings (PS), and sentiment analysis (SA). These elements are presented in Figure 1. Figure 1: Dynamic Profiles for VAAs, adapted from [43] The central assumption in creating dynamic candidate profiles is that most candidates are unwilling to answer the VAA questionnaires [23] and that using expert opinions demands extensive preparation. The problem increases when the list of candidates that will be analyzed is considerably extensive, as well as the campaign’s duration. In certain elections, the list of candidates is completed a few days before the political campaigns start, and the campaigns’ duration varies from days to several months. For elections with a big list of candidates and short periods for the campaign, getting the answers from all candidates or using expert opinions to generate political profiles are highly challenging and sometimes impossible to implement. In [44], authors introduced so-called dynamic profiles using sentiment analysis, and Twitter feeds from candidates. This approach uses candidates’ Twitter as an additional source and dynamic element. Furthermore, the work of [36] uses additional data sources (i.e., Twitter and YouTube) and applies QA models to generate dynamic profiles. This work extends traditional static profile generation that comes from either the answers of candidates or expert opinions using a Figure 2: Dynamic Profiles for VAAs using Political Profile Avatars Delphi method. Additionally, it extends the works of [44] and [36] that use sentiment analysis and QA methods by adding a so-called political avatar as shown grey in Figure 2. In Figure 2, the four elements of a dynamic profile described above are presented. Each element of the dynamic profile is weighted (𝑤𝑖 ). Depending on the quality of the information provided by each element, it can be included and averaged to avoid biased results and capture political candidates’ opinions dynamically. It is important to mention that political opinion is not static and can vary and evolve [10]. This approach can capture when such changes are presented and how they impact voters’ opinions [42]. 3.1 Generative AI Models LLMs have been significantly enhanced through the integration of GenAI. These models have been trained with vast amounts of online information, which could help support the creation of sophisticated virtual experts. In addition, LLMs can fit specific needs via finetuning or prompting with additional data, thus tailoring them to have a certain expertise on specific topics or domains. For instance, an LLM can be enriched with diverse personal inputs, such as opinions on a subject, interviews, books, or documents authored by an individual. This process facilitates the creation of a specialized knowledge base reflective of the content generated by a person. These virtual experts can handle complex tasks such as responding to surveys, guiding a topic, or offering informed recommendations. Different studies have worked on this idea to create human virtual experts based on advanced techniques such as reinforcement learning with generative adversarial feedback (RLGAF) [51]. This innovative approach aligns the outputs of LLMs with human expert demonstrations, enabling them to learn from valuable human input without direct exposure to specific training examples. This methodology indicates the potential for automating AI alignment, making LLMs more adaptable and effective in diverse applications, including VAAs [51]. For a more specific study on evaluating political information, we could find a study by Kuznetsova et al. [25] that assesses the proficiency of ChatGPT and Bing Chat in evaluating political information. This study found ChatGPT to be more accurate, with a 72 % accuracy rate compared to Bing Chat’s 67 %. It highlighted the variance in performance based on factors such as language, political communication concepts, and the context of the statements [25]. DGO 2024, June 11–14, 2024, Taipei, Taiwan Mancera and Terán Nevertheless, this paper advances beyond merely analyzing general political information. It proposes the creation of a customized Virtual Political Expert capable of emulating the role of a politician or providing insights on specific topics. On Twitter and YouTube Data. In the context of our research, we utilize datasets from Twitter and YouTube related to Carlo Sommaruga[2] to develop a specialized variant of an LLM. The adapted LLM integrates this additional information into its knowledge base. LLMs and Prompting. Typically, LLMs are stateless, meaning they do not retain memory of previous interactions. However, for this research, choosing LLMs capable of concatenating text between questions and answers via an API was important. The models selected for this study include: (1) GPT-4, developed by OpenAI. The LLM is known for its human-like text generation and understanding. It was trained on a diverse internet dataset; the last training month is April 2023 [38]. (2) Bard is Google’s LLM. It is based on Google’s pathways language model (PaLM). Bard was officially launched on March 21, 2023, [21]. The construction of this specialized knowledge base primarily utilizes prompting techniques to steer the model’s responses within the scope of its pre-existing training parameters. This method was chosen over fine-tuning, which requires significantly more computational resources, especially given the moderate size of the Twitter and YouTube datasets involved. The data remains manageable within the constraints of the LLMs’ API capabilities for text concatenation, which typically ranges between 50 and 60 prompts. Political Avatars with GenAI. Contrasting with QA models, eval- uating VAA surveys using GenAI considers fewer steps. The quality of input data requires careful consideration, including removing elements like emojis or URLs. However, widely used English, German, French, and Spanish translations are considered within the LLMs. This process is illustrated in Figure 3. Twitter Data Cleaning Avatar (GenAI Model) Answers Figure 3: GenAI process The code and datasets used are available in the following GitHub repository[3]. 4 QA MODELS EVALUATION For the first experiment, we select a Swiss politician, Carlo Sommaruga. He represents the canton of Geneva in the Council of States and is a member of the Socialist Party. This politician is chosen due to his active presence on Twitter, YouTube, and smartvote. As a first step, the questions and answers are extracted from Carlo Sommaruga’s smartvote profile. The code and dataset is available in the following GitHub repository[4] as presented in [36]. 2Carlo Sommaruga is a Swiss lawyer and politician of the Social Democratic Party of Switzerland. He has been a National Councilor for over a decade, and since 2019, he has been a member of the Council of States. Source: https://fr.wikipedia.org/wiki/ Carlo_Sommaruga 3https://github.com/jose0628/Generative-AI-to-Political-Profiling-Avatars- 4https://github.com/camille-sophiie/vaa-qa-models 4.1 Twitter Data The Twitter data from Carlo Sommaruga’s profile is fetched using the Tweepy Python library API. The data is cleaned and translated into English. As the QA model needs a question and a context to predict an answer, context vectors have been created using keywords. Tweets are then matched to different political topics [36]. The 60 questions of the smartvote questionnaire are extracted and simplified. The respective context vectors are then applied to the simplified questions. Table 1: Table defining Twitter results, adapted from [36] Actual Answers Negative Positive Negative 2 1 Predicted Answers Positive 2 16 Not Conclusive 17 22 The confusion matrix in Table 1 shows many unclassified tweets. However, the classified tweets are mainly rightly predicted. Additionally, the output of the QA model also returns where the model extracts the text used to answer the question. 5 GENERATIVE AI MODELS EVALUATION The political avatar evaluation consists of two cases to answer the 60 smartvote questions. (1) Unknown Politician: The LLM focuses only on Tweets and YouTube information. (2) Know Politician: Besides the Tweets and YouTube information, the LLM knows the politician’s name. Additionally, we take advantage of the translation capabilities of LLMs and utilize them to receive the original questions from the smartvote survey as input. In this case, unlike with the QA models, we did not employ the simplified versions of the questions. 5.1 Prompting Questions After the model was supplied with the dataset information, the following prompting questions were systematically posed as a template for each case. (1) Unknown Politician: Utilizing only the information from transcripts and tweets, can you determine whether the fol- lowing statements are true (yes), false (no), or uncertain (not conclusive)? Please provide a brief explanation for each answer. (2) Know Politician: Considering the provided transcripts and tweets related to Carlo Sommaruga, in conjunction with your pre-existing knowledge base and current internet resources, and acknowledging that your task is to answer questions, can you accurately categorize the following statements as “true” (yes), “false” (no), or “uncertain” (not conclusive)? For each answer, please include a concise explanation to support your conclusion. 5.2 Prompting Answers The following answer examples are based on the 60 VAA survey used by Nigon, Mancera, and Terán (2023) [36] |Col1|Negative|Positive| |---|---|---| |Negative|2|1| |Positive|2|16| |Not Conclusive|17|22| |Twitter Data|Col2| |---|---| ||| |Cleaning|Col2| |---|---| ||| |Avatar (GenAI Model)|Col2| |---|---| ||| From GenAI to Political Profiling Avatars DGO 2024, June 11–14, 2024, Taipei, Taiwan BARD Answers. Bard’s answers are more binary, with “Yes” or “No” as the answer choice. It also rephrases the answer using elements of the question. (1) Question: Should vaccines be compulsory? Answer: Compulsory vaccines: Yes (2) Question: Should undocumented migrants receive a status? Answer: Undocumented migrants: No (3) Question: Do you support stricter control of equal pay for men and women? Answer: Stricter control of equal pay for men and women: Yes (4) Question: Should we support renewable energies? Answer: Support renewable energies: Yes GPT-4 Answers. The responses provided by the GPT-4 model exhibit a broader range of nuances. They avoid binary answers and offer responses along a spectrum, such as “Yes,” “Likely Yes,” and so forth. Additionally, the model incorporates essential insights into the underlying reasons for its answers, particularly concerning political topics and the context. (1) Question: Should vaccines be compulsory? Answer: Compulsory vaccines: likely yes (2) Question: Should undocumented migrants receive a status? Answer: Likely yes or yes, based on humanitarian concerns. (3) Question: Do you support stricter control of equal pay for men and women? Answer: Likely yes, based on gender equality principles. (4) Question: Should we support renewable energies? Answer: Likely yes, based on environmental sustainability. 5.3 Bard - Case: Unknown Politician The Table 2 indicates a marginal enhancement in positive outcomes compared to both QA models. However, the limitation of basing predictions solely on the Twitter dataset persists, leading to a consistent trend of high numbers in non-conclusive answers. Table 2: Table Bard - Unknown Politician results Actual Answers Negative Positive Negative 1 2 Predicted Answers Positive 4 1 Not Conclusive 18 34 5.4 GPT-4 - Case: Unknown Politician In the Table 3, GPT-4 demonstrates an increase in accurate predictions for the Positive category, likely due to its specialized language comprehension capabilities, especially regarding the more complex original questions, compared to Bard. This observation highlights GPT-4’s ability to handle higher complexity, although it still produces many non-conclusive answers. 5.5 Bard - Case: Know Politician The Table 4 illustrates that when Bard is aware of the politician’s name (Carlo Sommaruga), it responds to every smartvote question, reducing the count of Not Conclusive responses to zero. This results in a significant increase in both true positives and negatives. Table 3: Table GPT-4 - Unknown Politician results Actual Answers Negative Positive Negative 1 6 Predicted Answers Positive 24 2 Not Conclusive 14 13 Notably, the answers to the prompts suggest that Bard’s responses begin to incorporate information about this politician beyond the information of the Twitter dataset. It also appears to consider additional sources like articles, news, and interviews, which are likely part of the data Bard was trained on. Table 4: Table Bard - Know Politician results Actual Answers Negative Positive Negative 9 12 Predicted Answers Positive 33 6 Not Conclusive 0 0 5.6 GPT-4 - Case: Know Politician The Table 5 demonstrates that GPT-4 has better prediction capabilities than Bard. Remarkably, GPT-4 outperforms Bard by correctly answering more questions. It suggests that GPT-4’s knowledge base is more comprehensive than Bard’s, and it may possess enhanced linguistic skills, potentially enabling it to forge connections across various pieces of information more effectively. Table 5: Table Bard - Know Politician results Actual Answers Negative Positive Negative 11 10 Predicted Answers Positive 37 2 Not Conclusive 0 0 6 DISCUSSION 6.1 GenAI Models vs QA Models Based on a survey of 60 questions and an analysis of a Twitter dataset, it was observed that GenAI models demonstrated slight improvements in their responses when comparing the QA modelbased case on Twitter (refer to Table 1) with scenarios involving Bard and GPT-4, particularly when the politician’s identity was not previously known to the LLMs. This indicates that QA Models can be an effective initial approach for generating insights when creating a profile based solely on social media data from static datasets. However, when Bard and GPT-4 are familiar with the politician’s name, their performance significantly surpasses expectations. In these cases, the models leverage the specific datasets provided (e.g., the politician’s tweets) and draw upon a broader knowledge base that includes news, articles, books, and more. These models benefit |Col1|Negative|Positive| |---|---|---| |Negative|1|6| |Positive|24|2| |Not Conclusive|14|13| |Col1|Negative|Positive| |---|---|---| |Negative|9|12| |Positive|33|6| |Not Conclusive|0|0| |Col1|Negative|Positive| |---|---|---| |Negative|11|10| |Positive|37|2| |Not Conclusive|0|0| |Col1|Negative|Positive| |---|---|---| |Negative|1|2| |Positive|4|1| |Not Conclusive|18|34| DGO 2024, June 11–14, 2024, Taipei, Taiwan Mancera and Terán from extensive training sets, enabling them to identify information relationships and contextualize their responses. Yet, at this stage, LLMs still function as “black boxes,” making it challenging to verify the accuracy and unbiased nature of the information they have been trained on. Consequently, we position these models as an additional dimension rather than a primary element for profiling politicians within our dynamic profiles framework (as shown in Figure 2). 6.2 GenAI Models Limitations. LLMs such as Bard and GPT-4 encounter contextual understanding and memory limitations, notably capped at around 60 historical prompts at the API level. This constraint limits their ability to acquire profound insights in complex fields like politics, where information is considerable. Moreover, these models are significantly limited by their training data, and fine-tuning them demands high computational resources. Additionally, there are notable concerns regarding bias and ethics, particularly in politically charged contexts where cultural, gender, and social biases may be more pronounced. As for accuracy, these models are not infallible and are prone to errors, a critical factor to consider in their application and reliance. Recommendations. LLMs serve as powerful instruments, ideally empowering users seeking to explore political viewpoints or opinions using available internet data about specific political topics or candidates. However, it is crucial to inform users about the potential for errors in this technology. From our observations, providing high-quality data is essential to enhance the accuracy of predictions. These models are better suited as tools to assist experts rather than replace them. Politics is a complex domain, and such tools are valuable for navigating the vast array of correct and incorrect information about candidates and political propositions. While relying on the outputs of these systems is beneficial, it is always prudent to conduct thorough research before making voting decisions. The use of LLMs should be approached as part of a broader, more comprehensive decisionmaking process. Applicability. Utilizing LLMs as political avatars represents one of many potential use cases or applications. These models can be adapted across various domains that rely on expert knowledge. It has been demonstrated that, with the appropriate input, LLMs can yield meaningful results and recommendations. LLMs extend beyond simple question-answering about specific topics. They are also valuable for data transformation, providing rapid access to learning new subjects and serving as experts in niche areas. This versatility could be particularly transformative in fields like education or where expert advice is not readily available, offering guidance and insights. Such applications highlight the broad utility of LLMs in augmenting human expertise and facilitating knowledge acquisition. 7 CONCLUSION This study introduces an innovative method to improve dynamic profiles for VAAs by employing Generative AI and creating political avatars. We evaluate the effectiveness of these avatars in comparison to previously developed QA models that incorporate advanced NLP technologies within the political domain. Our approach utilizes LLMs to develop political avatars capable of participating in VAA surveys. This method shows promise in enhancing or even surpassing the insights provided by human experts. Our findings indicate that with precise information on politicians, LLMs can notably increase response accuracy and decrease the rate of inconclusive answers. However, the effectiveness of Political Avatars is limited by the scope of their training data and their ability to interpret complex political contexts accurately. Future endeavors should aim to broaden and diversify the training datasets for these avatars, enhance their grasp of intricate political discussions, and tackle ethical issues such as bias and the spread of misinformation. In summary, this research significantly advances the integration of GenAI and LLM technologies into politics. Investigating GenAI models paves the way for innovations in political analysis and voter guidance, laying the groundwork for continued exploration and development in this cross-disciplinary area. REFERENCES [1] Kees Aarts and Henk Van der Kolk. 2007. The parliamentary election in the Netherlands, 22 November 2006. Electoral Studies 26, 4 (2007), 832–837. [2] Marilena Agathokleous and Nicolas Tsapatsoulis. 2013. Voting Advice Applications: Missing value estimation using matrix factorization and collaborative filtering. In IFIP International Conference on Artificial Intelligence Applications and Innovations. Springer, 20–29. [3] Marilena Agathokleous and Nicolas Tsapatsoulis. 2016. Applying hidden Markov models to voting advice applications. EPJ Data Science 5, 1 (2016), 34. [4] Marilena Agathokleous, Nicolas Tsapatsoulis, and Ioannis Katakis. 2013. On the quantification of missing value impact on Voting Advice Applications. In International Conference on Engineering Applications of Neural Networks. Springer, 496–505. [5] Ioannis Andreadis. 2013. Voting Advice Applications: a successful nexus between informatics and political science. In Proceedings of the 6th Balkan Conference in Informatics. ACM, 251–258. [6] Marcel Boogers and Gerrit Voerman. 2003. Surfing citizens and floating voters: Results of an online survey of visitors to political web sites during the Dutch 2002 General Elections. Information Polity 8, 1, 2 (2003), 17–27. [7] Lorella Cedroni and Diego Garzia (Eds.). 2010. Voting Advice Applications in Europe. The State of the Art. Scripta Web. [8] Roberto De Rosa. 2010. cabina-elettorale. it (Provides advice to Italian voters since 2009). Voting Advice Applications in Europe: The State of the Art. Naples: CIVIS/Scriptaweb (2010), 187–98. [9] Elias Dinas, Alexander H Trechsel, and Kristjan Vassil. 2014. A look into the mirror: preferences, representation and electoral participation. Electoral studies 36 (2014), 290–297. [10] David Doherty, Conor M Dowling, and Michael G Miller. 2016. When is changing policy positions costly for politicians? Experimental evidence. Political Behavior 38 (2016), 455–484. [11] Patrick Dumont and Raphaël Kies. 2012. Smartvote. lu: usage and impact of the first VAA in Luxembourg. International Journal of Electronic Governance 5, 3-4 (2012), 388–410. [12] A Dziewulska. 2010. The Use of Voter Advice Application in Poland – Glosuje.com.pl, Chapter 11. In Cedroni and Garzia [7]. [13] Vincent Etter, Julien Herzen, Matthias Grossglauser, and Patrick Thiran. 2014. Mining democracy. In Proceedings of the second ACM conference on Online social networks. ACM, 1–12. [14] Jan Fivaz and Giorgio Nadig. 2010. Impact of voting advice applications (VAAs) on voter turnout and their potential use for civic education. Policy & Internet 2, 4 (2010), 167–200. [15] Jan Fivaz and Daniel Schwarz. 2017. Data-Driven Democracy–Chancen und Risiken datenbasierter Demokratien. In Smart City – Strategie, Governance und Projekte. Wiesbaden, Andreas Meier and Edy Portmann (Eds.). Springer Vieweg, 103–129. [16] Esther Galbrun and Pauli Miettinen. 2016. Analysing political opinions using redescription mining. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW). IEEE, 422–427. From GenAI to Political Profiling Avatars DGO 2024, June 11–14, 2024, Taipei, Taiwan [17] Diego Garzia and Stefan Marschall (Eds.). 2014. Matching Voters with Parties and Candidates: Voting Advice Applications in Comparative Perspective. ECPR Press. [18] Diego Garzia and Stefan Marschall (Eds.). 2014. Matching Voters with Parties and Candidates: Voting Advice Applications in Comparative Perspective. ECPR Press. [19] Diego Garzia and Stefan Marschall (Eds.). 2014. Voting Advice Applications in a Comparative Perspective: An Introduction, Chapter 1. In Garzia and Marschall [18]. [20] Diego Garzia and Stefan Marschall. 2019. Voting Advice Applications. In Oxford Research Encyclopedia of Politics. [21] Google. 2023. Bard. https://bard.google.com/. [22] Fadi Hirzalla, Liesbet Van Zoonen, and Jan de Ridder. 2010. Internet use and political participation: Reflections on the mobilization/normalization controversy. The Information Society 27, 1 (2010), 1–15. [23] Ville-Juhani Ilmarinen, Veikko Isotalo, Jan-Erik Lönnqvist, and Åsa von Schoultz. 2022. Do politicians’ answers to voting advice applications reflect their sincere beliefs? comparing publicly and confidentially stated ideological positions in a candidate-centred electoral context. Electoral Studies 79 (2022), 102504. [24] Ioannis Katakis, Nicolas Tsapatsoulis, Vasiliki Triga, Constantinos Tziouvas, and Fernando Mendez. 2012. Clustering online poll data: Towards a voting assistance system. In Semantic and Social Media Adaptation and Personalization (SMAP), 2012 Seventh International Workshop on. IEEE, 54–59. [25] Elizaveta Kuznetsova, Mykola Makhortykh, Victoria Vziatysheva, Martha Stolze, Ani Baghumyan, and Aleksandra Urman. 2023. In Generative AI we Trust: Can Chatbots Effectively Verify Political Information? arXiv:2312.13096 [cs.CL] [26] Andreas Ladner, Gabriela Felder, and Jan Fivaz. 2010. More than toys? A first assessment of voting advice applications in Switzerland, 91–123. In Cedroni and Garzia [7]. [27] Andreas Ladner and Jan Fivaz. 2012. Voting Advice Applications – State of the Art and Future Development. In Electronic Democracy, Norbert Kersting (Ed.). Barbara Budrich Publishers, Chapter 7, 177–198. [28] Andreas Ladner, Jan Fivaz, and Joëlle Pianzola. 2012. Voting advice applications and party choice: evidence from smartvote users in Switzerland. International Journal of Electronic Governance 5, 3-4 (2012), 367–387. [29] Andreas Ladner and Joëlle Pianzola. 2010. Do voting advice applications have an effect on electoral participation and voter turnout? Evidence from the 2007 Swiss Federal Elections. Electronic participation (2010), 211–224. [30] Tom Louwerse and Simon Otjes. 2012. Design challenges in cross–national VAAs: the case of the EU Profiler. International Journal of Electronic Governance 5, 3-4 (2012), 279–297. [31] Tom Louwerse and Martin Rosema. 2014. The design effects of voting advice applications: Comparing methods of calculating matches. Acta politica 49, 3 (2014), 286–312. [32] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015. Recommender system application developments: a survey. Decision Support Systems 74 (2015), 12–32. [33] Stefan Marschall. 2014. Profiling users. In Matching Voters with Parties and Candidates: Voting Advice Applications in Comparative Perspective, Diego Garzia and Stefan Marschall (Eds.). ECPR Press, 93–104. [34] Stefan Marschall and Martin Schultze. 2012. Voting Advice Applications and their effect on voter turnout: the case of the German Wahl–O–Mat. International Journal of Electronic Governance 5, 3-4 (2012), 349–366. [35] Juri Mykkänen and Tom Moring. 2006. Dealigned politics comes of age? The effects of online candidate selectors on Finnish voters. In Conference of Politics on the Internet: New Forms of Media for Political Action, Vol. 25. [36] Camille Nigon, José Mancera, and Luis Terán. 2023. Enhancing Dynamic Profiles on Voting Advice Applications Using Social Media Analysis. In 2023 Ninth International Conference on eDemocracy & eGovernment (ICEDEG). IEEE, 1–7. [37] Michiel Nuytemans, Stefaan Walgrave, and Kris Deschouwer. 2010. Do the vote test: the Belgian voting aid application, 125–142. In Cedroni and Garzia [7]. [38] OpenAI. 2023. GPT-4. https://www.openai.com/. [39] Joëlle Pianzola, Alexander H Trechsel, Guido Schwerdt, Kristjan Vassil, and R Michael Alvarez. 2012. The Effect of Voting Advice Applications (VAAs) on Political Preferences-Evidence from a Randomized Field Experiment. (2012). paper presented at Annual Meeting of the American Political Science Association, New Orleans, LA, USA, 30 August–2 September. [40] Lorena Recalde, Gabriela Baquerizo, and Esteban Zunino. 2019. Women in Politics and Their Presence in Twitter: Argentina as a Case Study. In 2019 Sixth International Conference on eDemocracy & eGovernment (ICEDEG). 236–241. https: //doi.org/10.1109/ICEDEG.2019.8734343 [41] Outi Ruusuvirta and Martin Rosema. 2009. Do online vote selectors influence electoral participation and the direction of the vote. In ECPR general conference. 13–12. [42] Sebastian Stier, Arnim Bleier, Haiko Lietz, and Markus Strohmaier. 2020. Election campaigning on social media: Politicians, audiences, and the mediation of political communication on Facebook and Twitter. In Studying Politics Across Media. Routledge, 50–74. [43] Luis Terán and Aigul Kaskina. 2016. Enhancing voting advice applications with dynamic profiles. In Proceedings of the 9th international conference on theory and practice of electronic governance. 254–257. [44] Luis Terán and José Mancera. 2019. Dynamic profiles using sentiment analysis and twitter data for voting advice applications. Government Information Quarterly 36, 3 (2019), 520–535. [45] Alexander H Trechsel. 2007. Inclusiveness of old and new forms of citizens’ electoral participation. Representation 43, 2 (2007), 111–121. [46] Nicolas Tsapatsoulis, Marilena Agathokleous, Constantinos Djouvas, and Fernando Mendez. 2015. On the design of social voting recommendation applications. International Journal on Artificial Intelligence Tools 24, 03 (2015), 1550009. [47] Yannis Tzitzikas and Eleftherios Dimitrakis. 2016. Preference-enriched Faceted Search for Voting Aid Applications. IEEE Transactions on Emerging Topics in Computing PP, 99 (2016). [48] Stefaan Walgrave, Michiel Nuytemans, and Koen Pepermans. 2009. Voting aid applications and the effect of statement selection. West European Politics 32, 6 (2009), 1161–1180. [49] Stefaan Walgrave, Peter Van Aelst, and Michiel Nuytemans. 2008. ‘Do the vote test’: The electoral effects of a popular vote advice application at the 2004 Belgian elections. Acta Politica 43, 1 (2008), 50–70. [50] Matthew Wall, Maria Laura Sudulich, Rory Costello, and Enrique Leon. 2009. Picking your party online–An investigation of Ireland’s first online voting advice application. Information Polity 14, 3 (2009), 203–218. [51] Zhang Ze Yu, Lau Jia Jaw, Wong Qin Jiang, Zhang Hui, and Bryan Kian Hsiang Low. 2023. Fine-tuning Language Models with Generative Adversarial Feedback. arXiv:2305.06176 [cs.CL] Received 20 February 2024; revised 1 March 2024; accepted 8 March 2024 Masculinity, Podcasts and Trump: 12 Men Discuss Author: Healy, Patrick, Date: 2025-01-16 Collections: Hot Takes US Elect 2024, MediaAdsPolit, Polarization, MisDisinformation Zotero Key: XRNQP24A Cite Key: Healy25masculinPodTrump Zotero Item | Lit Note What’s a word that What’s a word that describes how you feel describes how you feel things things are going in America? are going in America? ‘Where’s Our Place in Society?’: 12 Men Who Backed Trump Grapple With America ‘Where’s Our Place in Society?’: 12 Men Who Backed Trump Grapple With America America has lost its way when it comes to men: the lessons they are taught as boys, the behavior expected of them at work and socially, the expectations of what being a man are. The return of Donald Trump as president might start setting things straight — that’s the hope, at least, among the 12 men in our latest Times Opinion focus group. They all voted for Mr. Trump in November not only because they liked his views on the economy, crime and immigration but government leadership and approach to American culture will help many men who feel devalued, overlooked, lost or isolated. “One of the reasons so many young men probably voted for Donald Trump is they're having trouble finding their way in our current society,” argued one of our participants, Matthew, a 43-year-old from Tennessee. With Mr. Trump preparing to take office on Monday, we wanted to dig into why one of his core constituencies — men who prefer Joe Rogan, Tucker Carlson, YouTube and X over the mainstream media — put so much trust in these sources of information and believe that Mr. Trump, in particular, has their backs. We spoke with a mix of Republicans, Democrats and one independent of various ages, races and ethnicities and professions. Why do these men trust what they trust? Time and again, they said they were drawn to qualities like common sense, curiosity and freewheeling debate and to people with strong beliefs who are willing to go toe-to-toe with anyone. Listening to the men, you can see why Mark Zuckerberg, who recently appeared on Mr. Rogan’s podcast, shelved the fact-checking operation at Meta: They said their views were shaped in part by conversation and debates on podcasts that could last hours and not from news and social media th t id d f t h ki th t th did ’t h f ith i “man’s man” — a theme that kept coming up — and why some men feel isolated or depressed in society today. For the most part, though, they were optimistic about the future under Mr. Trump and his allies. “I’ve been an Elon Musk fan since before probably most of you knew who Elon was,” said Jon, a 45-year-old from Pennsylvania. “And the fact that I have a chance to live in an America where Elon and Vivek Ramaswamy are charged with ridding the government of fraud, waste and abuse — that’s the America I want to live in.” Patrick Healy, Kristen Soltis Anderson and Adrian J. Rivera Mr. Healy is the deputy Opinion editor. Ms. Soltis Anderson is a pollster. Mr. Rivera is an editorial assistant in Opinion. PARTICIPANTS Matthew 43, Tennessee, white, Republican, attorney Ron W. 39, California, Black, independent, construction Ron G. 55, Kentucky, white, Republican, nurse Rich 54, New York, Latino, Democrat, construction Michael 52, Illinois, white, Republican, minister Walter 65, New York, white, Republican, consultant Glen 59, Florida, white, Republican, community manager Brett 62, California, Black, Democrat, retired Ken 39, western U.S., Pacific Islander, Democrat, teacher Lui 39, bira Dem une Moderator, Kristen Soltis Anderson When you wake up, what does the first hour of your day look like? Matthew, 43, Tennessee, white, Republican, attorney Helping get my three kids ready. Then work email. Michael, 52, Illinois, white, Republican, minister I am a minister. I usually wake up at 4 a.m. My first half-hour is usually personal time, prayer, meditation, and then I start to engage with email and texts. Ron W., 39, California, Black, independent, construction I wake up at 5 a.m., and I’ll work out for an hour. Moderator, Kristen Soltis Anderson Now I want to hear about the last hour of your day. Rich, 54, New York, Latino, Democrat, construction I get in bed, get on my laptop, answer some emails. Have the TV on in the background, watch some news and try to decompress from the day. Brett, 62, California, Black, Democrat, retired Almost every night, I have some ice cream, watch a I want to hear more about how you get news and information about important issues affecting the country. Ken, 39, western U.S., Pacific Islander, Democrat, teacher Online. I use MSN to filter and then look at other news outlets. I’ll listen to Joe Rogan’s podcast, but at the same time, I’ll listen to something else from another side. I try to be nonbiased, because it’s so easy to look at something in just one way, but that’s just half the story. Jon, 45, Pennsylvania, white, Republican, business owner I get most of my info from X. That’s a constant stream of intel throughout the day. Moderator, Kristen Soltis Anderson Are there particular accounts that you follow on X? Jon, 45, Pennsylvania, white, Republican, business owner Folks on the right, folks on the left, podcasters I enjoy. Obviously we all get bombarded with Elon Musk posts. If I am spending that last hour of the day with my wife, we’ll watch some of our favorite YouTube news feeds on our TV. The one we watch I’ll usually go online between CNN and Fox News and see what I think is right, what I think is wrong and what I agree or don’t agree with. Luis, 39, Florida, biracial, Democrat, unemployed I go to Reddit. Usually you can see both sides because somebody will point out the opposite side of whatever the issue is. Michael, 52, Illinois, white, Republican, minister I use Newsmax a lot, but I also use Turning Point USA, CBN and i24news, which is Israel’s 24-hour I watch very little TV news. It drives me insane. It just feels like there’s not a lot of positive to it. Moderator, Patrick Healy I wanted to learn more about what podcasts and hosts you guys trust the most — trust that keeps you coming back to listen more. Matthew, 43, Tennessee, white, Republican, attorney Ben Shapiro. He’s quick on his feet. He’s an attorney by training. He goes to debate forums and colleges. He goes into the belly of the beast. He’s very analytical, has the facts, well researched. It just gives you an aura of credibility. Moderator, Patrick Healy What do you mean by going “into the belly of the beast”? Matthew, 43, Tennessee, white, Republican, attorney I think a lot of left-wing groups will often protest when he goes to campuses to try to prevent him from speaking. He’s been on Jubilee, where it’s, like, 20 folks from the left debate one person on the right or vice versa. He’s not afraid to debate really Trust is very difficult because you get fed all kinds of information from all different sides, and unfortunately we’ve been fed a lot of wrong truths. I listened to Charlie Kirk a lot. My Christian beliefs are in tune with his, and I believe what he says. Rich, 54, New York, Latino, Democrat, construction I like Tucker Carlson. And I like Joe Rogan. I just feel they’re genuine, authentic, and they come across like they’re saying what they really feel and not putting their finger in the air and deciding where the political winds are going. I like who they interview. Moderator, Patrick Healy Is there an interview you liked that comes to mind? Rich, 54, New York, Latino, Democrat, construction Recently Tucker interviewed the Russian right- hand man of Putin, and I found it interesting. I’m open to his point of view. What builds trust in the people you listen to regularly? Michael, 52, Illinois, white, Republican, minister I think a lot of it has to do with common sense. A lot of what’s in newspapers and the drive-by media, it’s just not common-sense facts there. It’s common sense what a lot of podcasters say. My top three would definitely be Ben Shapiro, Charlie Kirk and Bill O’Reilly. Jon, 45, Pennsylvania, white, Republican, business owner I’m looking for hosts that approach a topic out of curiosity. I love the “All In” podcast for that reason. There’s four hosts, Silicon Valley entrepreneurs. They approach topics from a point of curiosity. I also really enjoy Lex Fridman. He just interviewed Volodymyr Zelensky. Those can be difficult because Lex’s interviews are often three hours, but the legacy media is so anti-intellectual that it drives me to long-form content. I want to get to really know how people got to their beliefs. Ken, 39, western U.S., Pacific Islander, Democrat, teacher Yeah, Joe Rogan usually does interviews that are three hours long. It’s very difficult for a guest to k f th t l th f ti Y ’ i questions where you might have to admit you’re wrong. I will be working out, jogging, free weights, listening to the podcast. Moderator, Patrick Healy What kinds of things are you learning from these podcasts? Do you feel it’s more valuable than what you learned in school? Ron G., 55, Kentucky, white, Republican, nurse Back in my college days, I probably didn’t have as much interest in politics or learning any more than what I needed to learn. Today, I’m a family man. I have more interest in what’s going on in the country and how it affects me and my family and my growth for the future. Church and faith have probably caused me to lean towards trying to find out what’s going on about current events. Moderator, Patrick Healy Is there a difference between your male friends and your female friends, in terms of who each group follows on podcasts or YouTube or where they get their information from? Yes. That’s probably why I like listening to Ben Shapiro and Charlie Kirk. I have friends who are ultrafar left or ultrafar right. I was the first conservative Republican in my family. There’s a lot of butting of heads. Listening to those podcasts sometimes gives me an insight into how I can approach a conversation with somebody. Jon, 45, Pennsylvania, white, Republican, business owner My hunch is that men are more prone to listening to long-form content, which I think was indisputable on the right this election cycle. Men have more solitary jobs where they can listen to long-form content; we’re on the road. Females just have more family responsibilities. It’s hard for a female to sit down for three hours and listen to Joe Rogan or Lex Fridman. It’s easier for dudes. Matthew, 43, Tennessee, white, Republican, attorney My female friends probably have less overall interest in political podcasts. You probably see less of the Joe Rogans and testosterone-fueled podcasts and more traditional, intellectual, Ben Shapiro types. Not exactly a testosterone-producing podcast. Years ago, I would listen to a different left-leaning podcast. And then I felt lied to. Moderator, Patrick Healy Is there something they lied about, do you think, Rich? Rich, 54, New York, Latino, Democrat, construction Russia. And I felt like they lied to me about the lacrosse kids with Duke. They just were so emphatic that they were guilty. I just checked out. They lost me. Years ago, I was a big Ed Koch fan, and I remember going to see him. And he said to me and my group: If you agree with me 80 percent, vote for me. If you agree with me 100 percent, go see a therapist. And I kind of live my life that way when I listen to news. Moderator, Kristen Soltis Anderson Let’s switch gears a little. What’s a word that describes how you feel things are going in America? Walter, 65, New York, white, Republican, consultant Stagnant. Brett, 62, California, Black, Democrat, retired In need of a change. Financial insecurity. Ron W., 39, California, Black, independent, construction Uncertain. Luis, 39, Florida, biracial, Democrat, unemployed Surviving. Ken, 39, western U.S., Pacific Islander, Democrat, teacher Surviving an uncertain economy. Jon, 45, Pennsylvania, white, Republican, business owner Optimistic. Rich, 54, New York, Latino, Democrat, construction S-show. Glen, 59, Florida, white, Republican, community manager Confused. Andy, 60, New York, white, Republican, service manager Challenging. Michael, 52, Illinois, white, Republican, minister Chaotic. Matthew, 43, Tennessee, white, Republican, attorney Optimistic. Matthew, 43, Tennessee, white, Republican, attorney It seems like woke culture is being pushed back. And I don’t just mean politically with the election. It just seems like common sense is starting to make a comeback in the psyche of Americans, corporations and others. At least that’s my hope. Jon, 45, Pennsylvania, white, Republican, business owner I couldn’t say it much better than Matthew. I think a lot will depend on the next 90 days — how confirmations to cabinet positions go. But for the first time in my politically aware lifetime, which probably starts in the 2000 Bush-Gore election, I feel like the incoming administration is more than a few degrees different than the outgoing one. Moderator, Kristen Soltis Anderson Michael, you said “chaotic.” Michael, 52, Illinois, white, Republican, minister Well, I live in the heart of Chicago, and crime is rampant. We are a huge sanctuary city. The amount of illegal gangs that have infiltrated this city and the woke, liberal D.A.s that have allowed this criminality to exist are why Illinois along with the country. So many people voted for this incoming administration because they know what they’re getting and there’s some common sense to “Hey, stop trying to force other ideas on us.” Moderator, Kristen Soltis Anderson Rich, I have to ask you about your PG-13 response, “S- show.” Rich, 54, New York, Latino, Democrat, construction Well, I’m with Matthew, Michael, and Jon, as far as hope for the new administration. But I live in Manhattan, on the Upper West Side near Columbia University. I see the naïveté of the students. I see how New York is now charging us $9 to come into the city under congestion pricing. I see that poor woman who was lit on fire on the subway. The whole city is a mess. Life is about wrapping your head around situations. I cannot wrap my head around what they’re doing. Inflation was out of control. Rent goes up every year. Bills go up. Electricity goes up. Gas goes up. Part of the reason why I decided to vote for Donald Trump is because I actually believe that he’ll be able to help change a lot of that. Andy, 60, New York, white, Republican, service manager Although I’m happy Trump was elected, I think he’s going to have a very tough time getting the economy squared away. I work for a manufacturing company, and we have been decimated by foreign imports, to the point where I’m probably going to be without a job within the next year because these imports have taken away our ability to sell our product when overseas companies are selling them for less. I’m hoping Trump’s going to be able to get some tariffs put in place. I know there’s some people here that probably disagree with tariffs, but he’s got to do something. I’m in Staten Island, and I’ve also seen it drastically change for the worse. The cops’ hands are tied. These very loose liberal judges are letting them out with no bail back to being repeat offenders. Moderator, Kristen Soltis Anderson crime. Are there any other issues that were really top of your mind, a big driver in why you voted the way you did in this past election? Walter, 65, New York, white, Republican, consultant Illegal immigration. I’m in upstate New York, and I know New York City was busing a lot of these refugees, whatever the hell you want to call them, up to the Albany area, putting them in hotels and spending millions of dollars on them. I mean, why can’t we spend that money on our American people? Why do we have to spend that money on foreigners? I lived in Dallas, and it was sickening, because they would come to this country and want us to adapt to their customs and their language. When you go into McDonald’s, menus are in Spanish and not English. Learn the damn language. I worked with a lady. She’d been here for 24 years and still couldn’t speak English. My descendants are Polish. They came to this country through Ellis Island with maybe nothing in their pocket. But they worked hard. They got a job. They formed communities. But they bettered themselves. They didn’t look for government Give me a word that describes Donald Trump. Rich, 54, New York, Latino, Democrat, construction Leader. Matthew, 43, Tennessee, white, Republican, attorney Fighter. Michael, 52, Illinois, white, Republican, minister He’s a fighter. Glen, 59, Florida, white, Republican, community manager Erratic. Andy, 60, New York, white, Republican, service manager Leadership. Jon, 45, Pennsylvania, white, Republican, business owner Change. Ken, 39, western U.S., Pacific Islander, Democrat, teacher Here we go. Ron W., 39, California, Black, independent, construction Proven. Luis, 39, Florida, biracial, Democrat, unemployed All over the place. Change. Ron G., 55, Kentucky, white, Republican, nurse Change. Walter, 65, New York, white, Republican, consultant Rocks the boat. Moderator, Kristen Soltis Anderson Ken, tell me more about “Here we go.” Ken, 39, western U.S., Pacific Islander, Democrat, teacher You’ve got DOGE, and what is that actually going to do? Sometimes we don’t like bureaucracy or certain things that happen when it comes to dealing with certain agencies, but sometimes you don’t want to take an ax to everything. You’ve got the people around him like Elon Musk who look at the economy from a business standpoint. But your dinner table economics is very different from a government standpoint. So it’s going to be very interesting to see what programs get cut, what programs get increased and the framework of how they view economics helping. Moderator, Kristen Soltis Anderson I think he’s got the ability to reach across the aisle and motivate the other side into agreeing with him on some key issues. One of the biggest is immigration. I’m all for immigration. If you’re bringing an education or a trade and you come to this country legally and you’ve got something to contribute, by all means, you are welcome. My grandparents were immigrants. They came here legally, they worked hard, and they built a better life for themselves and their families. All immigrants are welcome. Just come here legally. So he’s got to start there. Moderator, Kristen Soltis Anderson Some of you said Trump is a fighter. Who is Donald Trump fighting, or who is he fighting for? Michael, 52, Illinois, white, Republican, minister When they did the first assassination attempt on Trump, the way he bounced back — that’s just not something that you train to do. That’s instinct. It was an instinct to say what he said and do what he did. He cared about the people and was willing to jump up and fight for it. And as a person who’s been in the service and who’s been shot in the service takes to bounce back from something like that. That’s about integrity and less about politics. That’s why I know for sure he’s a fighter. If you keep overtaxing companies, keep putting these Orwellian policies on them or trying to stranglehold them, they’re not going to survive, and you’re going to lose the economy, and people aren’t going to work. I think that’s what Trump is fighting. Moderator, Kristen Soltis Anderson Were you surprised there were assassination attempts against Trump? Michael, 52, Illinois, white, Republican, minister I wasn’t. Ron W., 39, California, Black, independent, construction I was very surprised. I mean, I know it’s happened before in history. But the timing of it all, it just caught me off guard. Ron G., 55, Kentucky, white, Republican, nurse There’d been a lot of pushing to get him as a criminal, put him in as a criminal. And then that didn’t seem to be working. That’s where I feel like the assassination took place I’m not saying I think promoting that. But I’m not surprised there was an attempt on his life. And I think him bouncing back is not a surprise, either. He’s an American that loves his country and that he’s ready to fight for it and take it and lay his life down, like a serviceman would. Moderator, Patrick Healy Trump is coming back into office next week. Is there anything you’re excited to see him do as president? Brett, 62, California, Black, Democrat, retired I’m very excited that he’s going to close the border. I’m in California where the border is, and I see busloads of illegal aliens being dropped off in my neighborhood. Glen, 59, Florida, white, Republican, community manager I used the word “erratic,” but I think it’s a positive. From a military defense standpoint, I think people don’t know what he’s going to do, but they know he’s going to do something and he takes a stand and doesn’t back down. I think other leaders respect that. Jon 45 Pennsylvania white Republican business owner most of you knew who Elon was. And the fact that I have a chance to live in an America where Elon and Vivek Ramaswamy are charged with ridding the government of fraud, waste and abuse — that’s the America I want to live in. Michael, 52, Illinois, white, Republican, minister Trump is going to make us energy independent again and open up our oil reserves. This way, we’re not codependent upon foreign adversaries. And I think Trump has a lot more respect for the military. I have no respect for the ultraleft wing of the Democratic Party — especially after the Afghanistan debacle — because they made a joke of the military. Benghazi was bad enough, but they made a joke of it. Luis, 39, Florida, biracial, Democrat, unemployed Address Social Security. Maybe something with people with disabilities, because I’m disabled myself. Moderator, Patrick Healy Is there anything you’re worried Trump might do as president? Or anything that you don’t want him to do? I hope he doesn’t get caught up in retribution and replaying old games. There was part of me that hopes Joe Biden gives a blanket pardon to all these crackpots so Trump doesn’t even think about trying to prosecute any of them. Ken, 39, western U.S., Pacific Islander, Democrat, teacher One of the things on the chopping block is the Consumer Financial Protection Bureau. Love them or hate them, it brought back about $12 billion to your average person because of things like bank drafts. Also, I don’t want to see troops leave Korea. Matthew, 43, Tennessee, white, Republican, attorney Stick to the things we’ve talked about here. Ron W., 39, California, Black, independent, construction Save all the business for when you’re out of the office. Moderator, Patrick Healy Donald Trump did well in this election among men, including younger men. What do you think made Donald Trump appeal to men like you? Michael, 52, Illinois, white, Republican, minister directly as if we’re at a coffee table. He doesn’t talk down to us. He doesn’t look down to us like most Democrats do. He doesn’t talk down to us as some low-life minion. Andy, 60, New York, white, Republican, service manager He’s a man’s man, right? I actually had the honor of meeting him long before he was president in some business dealings, and he’s an absolute true gentleman. He doesn’t beat around the bush. Moderator, Patrick Healy What does being “a man’s man” mean to you guys? Rich, 54, New York, Latino, Democrat, construction Responsible. A leader. Strong. When Trump walks into the U.F.C., that place explodes. Those are a bunch of testosterone men. Even when he went to the Army-Navy game, they love him. I think, not to sound morbid, but when Trump was shot, men just felt, “That’s a dude.” Ken, 39, western U.S., Pacific Islander, Democrat, teacher Mr. Testosterone. I work out, but I’ve dealt with a lot of people who are like that. What matters is: Can that person think? What’s their ability to projected onto Trump because he's a man's man! different way? Ask the right questions? I’ve dealt with a lot of people who perceive leadership as the ability to yell or think their physical prowess and ability to yell equal leadership. And that’s definitely not the case. Glen, 59, Florida, white, Republican, community manager Maybe I’m the minority here. Trump’s not a man’s man to me. I voted for him as the lesser of two evils. I believe in the defense of our country, I think he’s a businessman, and I think people are looking at it and going, “You know what? I can buy a home. I can make more money.” I have a 34-year-old son, and I have grandsons. Do I want them to talk to and treat people the way Trump does at times? Absolutely not. I think you can have strong opinions and get things accomplished without being demeaning and divisive. If he could have just dialed it back, he wouldn’t have lost the 2020 election to Biden. Because of the way he treated people, people didn’t want to hear it anymore. Brett, 62, California, Black, Democrat, retired I only believe in two genders: boys and girls. That’s it. No transgender. You got male genitalia, you’re a l Y t f l it li ’ f l I Incredible! This guy thinks Trump knows how to change a tire! Change your own tire. Change your own oil. Put your own gas in your car. Guys nowadays can’t do that. Guys who are 22, they can’t change their own tire. You have to call somebody to help you. And it’s the parents’ fault. But a man’s man should be able to change his own tire. Should be able to not tweet out “covfefe” all the time. Be able to keep your tweets to yourself, especially the crazy ones. I agree that Trump is the lesser of the two evils because Kamala Harris would have been horrible. Moderator, Kristen Soltis Anderson Who comes to mind when you think of a man’s man? Jon, 45, Pennsylvania, white, Republican, business owner My grandfather. Matthew, 43, Tennessee, white, Republican, attorney John Wayne. Rich, 54, New York, Latino, Democrat, construction Yeah, my dad. Ron G., 55, Kentucky, white, Republican, nurse My father. Glen, 59, Florida, white, Republican, community manager Michael, 52, Illinois, white, Republican, minister I’m going to say Jesus Christ. Andy, 60, New York, white, Republican, service manager An uncle. Luis, 39, Florida, biracial, Democrat, unemployed I’ll go with Jesus. Ken, 39, western U.S., Pacific Islander, Democrat, teacher Eddie Murphy. Ron W., 39, California, Black, independent, construction My grandfather. Brett, 62, California, Black, Democrat, retired Martin Luther King Jr. Walter, 65, New York, white, Republican, consultant My grandfather. Moderator, Kristen Soltis Anderson Do you think there are generational differences between how older men and younger men think about the concept of being a man or masculinity? Ron G., 55, Kentucky, white, Republican, nurse we’ve lost chivalry a little bit. I think that people are not as kind to each other. I think the way Trump talks sometimes is not appropriate to women or to people. But I also like his sternness in trying to make a point and get it across. Today’s generation of kids, they’re not as mindful. They don’t listen. They’re entitled, and I think that that’s the difference in the way I grew up and where I am today and to the kids today. I didn’t raise my kids that way. They’re hard workers, and they work hard to get what they get. Michael, 52, Illinois, white, Republican, minister I do a lot of mentoring. A lot of the younger generation simply do not know how to have respect. It has been in the education system, the media, the entertainment industry. A lot of young people have no clue what it is to have respect, what it is to stand up and fight for what’s right, what it is to open up a door for a lady, to show some respect. I’m not talking male chauvinism, but I’m just saying there’s some common respect for women. Most of us are working men of integrity who are punches. A lot of this younger generation won’t do that. Ken, 39, western U.S., Pacific Islander, Democrat, teacher The dating scene is much different than before. Something that would be perceived as chivalrous, you could get into a lot of trouble for. We’re litigious to begin with, but everything’s about liability or just, if we’re going to go out, we have to sign this consent form. Jon, 45, Pennsylvania, white, Republican, business owner website of any social organization, especially one that’s associated with the government. And you’ll usually find a section called “who we serve.” If you look down through that list, they serve everybody but young white men. Other groups needed a lot of help at the time, for good reason. Maybe the pendulum swung a little too far. Matthew, 43, Tennessee, white, Republican, attorney I don’t think young women have it easy, either, just to be clear. But one of the reasons so many young men probably voted for Donald Trump is they’re having trouble finding their way in our current society. The things ranging from the MeToo movement, “believe all women, no matter what.” I think that intimidated a lot of young men. You look at academic scores. Boys, men are falling behind. You look at the feminization of men, where traditional male characteristics like courage, strength — things that used to be viewed as positive — are now viewed as toxic masculinity. Things like that have increasingly made young men feel like, “Where’s our place in society?” Ron G., 55, Kentucky, white, Republican, nurse men more so. But I think the whole issue of transgender people in women’s sports and issues of bathrooms and locker rooms and so forth — I think it’s harder on women with that. Moderator, Kristen Soltis Anderson We’ve been talking about challenges that men in America face these days. I want to dig in a little bit more on the challenge around loneliness and social isolation. How hard is it to connect, to make friends, date and so on these days? What would be your theory for why more men report feeling lonely or more isolated? Michael, 52, Illinois, white, Republican, minister Some men just simply don’t know how to respond anymore in social circles. What used to be acceptable as chivalry, what used to be acceptable as being a man of integrity, everything you do and say now is under the ground. You’re scrutinized for everything. Maybe it’s just me, but most men will just go back into the safe zone and say, “You know what? Why bother? I’m going to stick with what I know what works and what I know I need to do with my life and forget about everything else.” Men I’m 65 now. Going back to when I was younger, girls could bond over anything. When guys went out, if it wasn’t sports or something masculine, then guys couldn’t bond at all. And a lot of times if you tried to bond with somebody or just be friends with the guy, the guy would look at you like you’re just strange. And I think as I grew up and aged, that kind of stayed with our generation. I work at a senior center, and every day they have a luncheon there for seniors. And out of about 30 or 40 people there, I’d say 80 percent are women. And it’s because the women want socialization. We’ve got things for the men to participate in, but I don’t think that’s something men would do. Andy, 60, New York, white, Republican, service manager Men are afraid to say anything anymore for the fear of offending somebody. I’ve got one young lady on my team who was the first female in our field service organization, and she has done wonderfully. But it didn’t come without a lot of challenges for her and for the men that are working with her to try to treat her equally while not offending her. It’s taught me to be a different kind of manager, having a girl on a team. Which is really unusual in our trade. But right? No matter where they go. If you open a door for a woman now, sometimes they’ll look at you like, “Well, I don’t need you to hold the door for me.” If you’re on a subway and there’s a woman standing next to you, you stand up, and you give that lady the seat. If you’re walking down the street with a woman, you stand on the curbside. She walks on the inside, right? If you do that now, you’re looked at there’s something wrong with you. Matthew, 43, Tennessee, white, Republican, attorney I think I saw something about how the top 10 percent of men — over six feet tall, six-pack abs, six-figure income — 80 percent or 85 percent of women compete for those men. The large group of men that are left are often isolated and lonely because there are fewer young women their age to have relationships with. I think it’s largely related to hookup culture. That’s created isolation, less socialization, more awkwardness and an inability for them to have meaningful connections and relationships with the opposite sex. Glen, 59, Florida, white, Republican, community manager and “intentional.” In the last year, I got divorced. I’m living on my own. I don’t have kid stuff to deal with anymore. So I had to be intentional. I’ve gotten involved in men’s groups through my church. There’s areas where you can volunteer. If I sit at home and go, “Well, I’m worried about what do people think,” then I’m going to be lonely. Technology can be great, but I think our young people have not learned how to communicate and socialize because they hide behind a keyboard or a gaming station and don’t know how to have conversations like we’re having here. I think we’re doing a disservice to our young people by not helping them be in more social settings where they’re learning how to treat each other with dignity, manners, respect, even in our differences. You don’t have to be lonely. But you’re going to have to be vulnerable. And you’re going to have to be intentional about having things going on in your life, because they’re not going to just fall in your lap or knock on your door. America in Focus seeks to hear and understand the views of cross-sections of Americans whose voices are often not heard in opinion journalism. years for Republican candidates and partisan groups. She chose the participants. (Times Opinion paid her for the work.) This transcript has been edited for length and clarity; an audio recording of the session is also included. Participants provided their biographical details. As is customary in focus groups, our role as moderators was not to argue with or fact-check the speakers, and some participants expressed opinions not rooted in facts. Illustrations by Lucinda Rogers. Political marketing & advertising with the use of traditional and new audio-visual media services Author: Gkounas, Athanasios, Date: 2023 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: FUXCYXC7 Cite Key: Gkounas23politAdsTradAudioViz Zotero Item | Lit Note Political Marketing & Advertising with the Use of Traditional and New Audio-Visual Media Services Athanasios Gkounas International Hellenic University, Thessaloniki, Greece Политически маркетинг и реклама с използването на традиционни и нови аудио-визуални медийни услуги Атанасиос Гкунас Международен гръцки университет, Солун, Гърция Author Note Gkounas Athanasios https://orcid.org/0000-0001-9534-5519 The authors have no known conflict of interest to disclose. Correspondence concerning this article should be addressed to Dr. Athanasios Gkounas, International Hellenic University – Thessaloniki campus, Department of Organisation Management, Marketing and Tourism, PO Box 141 GR, 17 Km Thessaloniki-Sindos, 57400 Thessaloniki, Greece. Email: espa@gounas.gr Бележки за автора Гкунас Атанасиос https://orcid.org/0000-0001-9534-5519 Авторът няма известен конфликт на интереси за разкриване. Кореспонденцията относно тази статия трябва да бъде адресирана до д-р Атанасиос Гкунас, Международен гръцки университет – кампус в Солун, Департамент по управление на организацията, маркетинг и туризъм, пощенска кутия 141 GR, 17 км Солун-Синдос, 57400 Солун, Гърция. Имейл: espa@gounas.gr Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Abstract The current research focuses on the use of audiovisual media services in the political scene and tries to explore the political marketing and advertising concepts on which these media/services are based. As a holistic approach, this research explores the citizen-voter as well as the politician concerning political advertising and audiovisual media/services. It separates the audiovisual media into traditional and new ones. So, primarily, this research empirically investigates the politicians’ profiles (high-ranked politicians, parliament, and regional ones) to determine the type and use of audiovisual media in terms of political information, statements, communication, and advertising. Moreover, it establishes the citizen/voter’s profile in terms of political information, political advertising, and communication received from politicians and political parties. Finally, it is a study with the same conceptual dimensions between the politician and the citizen/voter to establish the basis for creating a conceptual framework-tool of their typology that will contribute to a holistic view of political marketing and political advertising through the use of traditional and new audiovisual media. Findings about politicians show relationships between audiovisual media reliability and political statement actions. The success of political advertising has multiple positive effects, as well as political communication via audiovisual services also plays an important role. Furthermore, models were created concerning the use of audiovisual media services for political statements and the impact of audiovisual media services on the intention to vote. In addition, the results from the politicians indicate several differences regarding the characteristics of the politicians and their profile as a user of audiovisual media services in political advertising. Findings about voters show that the reliability of audiovisual media services is a very important factor for political information. Keywords: new media, traditional media, political marketing, political advertising, political audience typology Резюме Настоящото изследване се фокусира върху използването на аудиовизуални медийни услуги на политическата сцена и се опитва да изследва концепциите за политически маркетинг и реклама, на които се основават тези медийни услуги. Като холистичен подход това изследване изследва гражданина-гласоподавател, както и политиката във връзка с политическата реклама и аудиовизуалните медиийни услуги. Той разделя аудиовизуалните медии на традиционни и нови. Това изследване изследва емпирично профилите на политиците (високопоставени политици, парламентарни и регионални), за Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org да определи вида и употребата на аудиовизуалните медии по отношение на политическа информация, изявления, комуникация и реклама. Освен това той установява профила на гражданина/избирателя по отношение на политическа информация, политическа реклама и комуникация, получена от политици и политически партии. И накрая, това е изследване със същите концептуални измерения между политик и гражданин/избирател, за да установи основата за създаване на концептуална рамка-инструмент на тяхната типология, която ще допринесе за холистичен поглед върху политическия маркетинг и политическата реклама чрез използването на традиционни и нови аудиовизуални медии. Констатациите за политиците показват връзки между надеждността на аудиовизуалните медии и политическите изявления. Успехът на политическата реклама има множество положителни ефекти, както и политическата комуникация чрез аудиовизуални услуги също играе важна роля. Освен това бяха създадени модели относно използването на аудиовизуални медийни услуги за политически изявления и въздействието на аудиовизуалните медийни услуги върху намерението за гласуване. В допълнение, резултатите от политиците показват няколко разлики по отношение на характеристиките на политиците и техния профили като потребители на аудиовизуални медийни услуги за политическа реклама. Констатациите относно избирателите показват, че надеждността на аудиовизуалните медийни услуги е много важен фактор за политическата информация. Ключови думи: нови медии, традиционни медии, политически маркетинг, политическа реклама, типология на политическата аудитория ARTICLE INFO: Original Article Received: 12, 10.2023 Revised: 30, 10.2023 Accepted: 27, 11.2023 Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Political Marketing and Advertising with the Use of Traditional and New Audio-Visual Media Services Introduction Citizens/voters and elected politicians (parliament and regional ones) within Greek political parties are the means to understand the issue of audiovisual services and political marketing. The use of audiovisual services in political marketing and advertising as well as the current evolution of social media is a phenomenon that was born, attempted, developed, and researched firstly in the USA and later on in other areas of the world, such as Europe. Thus, it is interesting to research how all these developments have affected European countries like Greece, which have not yet been studied, taking into account their particular political history. Greece is a country that only after the political changeover[1] that began in 1974, with the fall of the dictatorship (Junta), freely allowed the use of political advertisements by all political parties. It is also a country that has faced a devastating economic crisis since 2008, which has led to political upheaval and a major shift in the political dipole that has been the norm for the past 35 years. Such peculiar political characteristics were the criterion for Greece to be selected for this study concerning political marketing and advertising through the use of audiovisual services (traditional and new ones) by the two interrelated players, the citizen/voter, and the politician. The study begins with the analysis of useful terms and theoretical context as auxiliary conceptual tools in the subject matter. Then, it continues by analyzing the political scene in Greece. More specifically, it discusses the political parties that constitute the parliament[2] and the course of political communication and marketing with the citizen/voter. To achieve this, the analysis is based on the official web pages of the political parties. In this way, there is an understanding of each political party’s political marketing strategies and all the combinations of audiovisual media used (traditional and new) on the part of each political party. The citizen voter is then examined, as well as the politician and the empirical results of this research, as far as political marketing and political advertising are concerned, with the use of audiovisual media. 1 The transition from the military dictatorship or Regime of the Colonels to a multi-participatory democracy (Kassimeris, 2005). 2Up to July 2019 Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Literature Review What is audio-visual media/services? Audiovisual media[3] produce images and sound. It is «media formats that contain sound and/or images (still and/or moving) that require an internet third-party device to be able to play/display their content … there are seven recognized forms of mass media: these are print, audio-recordings, cinema, radio, television, the internet and, now mobile phones (Henderson, 2010 p. 112). Audiovisual media are therefore means of mass communication that transfer messages from the transmitter to the receiver. The difference between traditional and new audiovisual media is the combination of the senses appears to affect the receiver of the information. In the past, there were visual media (e.g. posters, printed newspapers) and later on audio media (e.g. radio), which were perceived by a single sense such as the ear (radio-audio media), or the eye (newspaper-visual media). Television and even more the internet have brought about changes in the transmission of information, since there is now a simultaneous combination of both senses (hearing and sight), being thus referred to as audio-visual media. Political campaigns with the use of the internet The association of the political campaign with the Internet started in America. Barack Obama and his team were the initiators of the integration of social media into political marketing and communication, and they became renowned in political history for their innovative and effective election campaign methods (Bimber, 2014; Harfoush, 2009; Kiss, 2008; Miller, 2008; Raoof et al., 2013; Writer, 2016). Although Obama has gone down in history for his online strategy, since 1996, the candidates of the American elections have started to develop more technologically sophisticated websites and to organize administration groups in charge of internet information (for example with the use of e-mail lists, newsgroups, interest groups, blogs, online fund-raising, net organized house parties and the social platforms that existed in those years, such as YouTube, MyParty) for their campaigns (Gueorguieva, 2008; Rice, 2004). In this way, they could, as with television broadcasts, reach out and inform a new audience that was connected to the platforms and acquired political information from there. The first innovative steps of the digital campaign were made by Howard Dean's team, who had thoroughly analyzed Jessie Ventura's campaign 3For an extensive and excellent presentation of the chronological evolution of audio-visual services, from the 19th century to the 21st, please see the analysis of Henderson (2010) Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org in Minnesota in 1998 (Davy, 2010). In his successful campaign[4], Jessie Ventura (gaining the vote of 50% of the citizens/voters under 30) did not have offices or physical organizational structures, but a list of e-mail receivers that grew rapidly and through which he could present his political positions (Rice, 2004). From this platform, Howard Dean presented his positions after the instigation of his campaign manager, Joe Trippi. A simple incorporation of the meetup.com link on the official Dean's Campaign website brought great results managing to increase the initial 432 Dean's Campaign meet-up members to the impressive number of 200,000 (Nelson, 2011). Also, through this page, he was able to acquire e-mail addresses to expand the list of his citizens/voters and sponsors. Election campaign with the use of social media The link between politics and the use of social media as a tool for election campaigns was strengthened in the elections of 2008 in America, by Barack Obama who was then a candidate (Raoof et al., 2013). He put political communication on a strong orbit beyond the use of the traditional tools of audio-visual services (television, radio, newspapers) and specifically within the framework of social media (Baumgartner & Morris, 2010; Raoof et al., 2013). Howard Dean (The Web's candidate for President[5]) might have been one of the pioneering politicians who used a social media platform but Obama (The First Internet President[6]) used multiple internet applications as a political tool (Gibson, 2015). His team created web pages (barrackobama.com), used social media (MyParty, Facebook, and Twitter), and uploaded online videos (YouTube) in combination with the use of standardized phone text messages and e-mails. Obama himself considered the Internet to be the key to his success, stating «change comes from the bottom up» and «There is no more powerful tool for grass-roots organizing than the internet» (Stelter, 2008). Surely, without that, Barack Obama would hardly have achieved his goal. Research method A quantitative research method was used for the current study, to measure the specific variables through appropriate questions and collect the necessary information to answer the research questions. The quantitative data collection tool was a questionnaire related to the research topic. The selection of quantitative research methods was based on the nature of the research topic and the need to explore, discuss, and generalize the findings for the population 4won 50% of citizen-voters under the age of 30 5(Dodson & Hammersley, 2003) 6(Greengard, 2009) Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org (in our case, voters and politicians) (Δαυτόπουλος, 2004; Τηλικίδου, 2004; Φράγκος, 2004). Additionally, the specific characteristics of the quantitative research methods such as the ability to collect a great amount of data at the limited possible time were those who support the selection of the specific method (Rahman, 2017). This research attempt will analyze the opinions of both politicians and voters to follow a more holistic approach, make the necessary comparisons, and end up with specific suggestions that can be a guide for the use of traditional and new media for political purposes. Quantitative research based on a questionnaire, investigates a topic using codified numerical data while obtaining information not easily observed. Then the use of statistical analysis proceeds for description, research question testing, and findings discussion (Remenyi et al, 2002). The questionnaire is the main data collection tool in quantitative research, and it allows the researchers to quantify information in a precise manner. Moreover, quantitative research allows the faster collection of data compared to other methods (Rahman, 2017). The current research empirically investigates the citizen/voter’s opinions on the use of traditional and new media for political purposes, the politician and the political party. It establishes the citizen/voter’s profile using traditional (such as newspapers, radio, and television) and new (such as web and social platforms) audiovisual media, in terms of the political information, advertising, and communication received from politicians and political parties. Research Models Development Models Based on Politicians - Correlations - Politicians This section investigates the relationships between the variables concerning politicians and their use of audiovisual media services, to reveal correlations between them. Specifically, the relationship between the factors was studied within each of the three groups of variables (Figure 1, Group 1: variables that refer to the profile of the political statement of the politician. Group 2: variables that refer to the greater impact of the political advertising on the political party and the politician, the impact of the slogan on the voters, the perceived success of the political advertising, and the impact on voting intention,[7] and Group 3 with variables referring to the political party communication (dialogue) with voters through the use of media, the informative adequacy of media for communication purposes, and the effect of media on the 7 meaning the political leaning of the voter Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org recognition[8] of the politician). The current model was based on the following research questions: Research question 1: Is there a statistically significant relationship between the choice of media use for political statement and their reliability? This research question examines the association between media reliability and the utilization of specific media forms by politicians; Research question 2: Is there a statistically significant relationship between the success of political advertising, the impact on voting intention, the impact of slogans on voters, and the impact of media on the political advertising of the political party and the politician? The second research question investigates the connection between the perceived success of political advertising, the impact of media on the actions of the voters, and the impact of the slogan on the voters as the greater impact of media on political advertising both for the political party and the politician. Research question 3: Is there a statistically significant relationship between the politician’s recognition via media, political communication between the political party and the voter, and the adequate use of media by political parties for communication? The third research question checks the correlation between political communication of the political party with the voters, the impact of exposure to media on the politicians’ recognition and how effective is the use of those media for communication by the political parties. Moreover, regarding Group 1 of variables the next research question was examined. Research question 4: Is there a statistically significant relationship between the use of media for political statements, in the case that politicians follow the political campaigns of others? The question is based on the comparison of media usage with how the other politicians use the aforementioned media. 8 In terms of the recognizability of the politician to the general public Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Figure 1 Correlations between the variables for politicians Regression models - Politicians Two regression models (Models 1 and 2) were created for politicians. The models were an attempt to statistically predict the use of media for political statements (politicians). Specifically, the following regression model has ten (10) independent variables which are tested in terms of their impact on the dependent variable. Those independent variables were divided into sub-groups according to their content. The first group of variables refers to the profile of the political statement of the politician and, more specifically, to the way the reliability of the media and how the media were used by others (political campaign of other politicians) affect their attitude towards the use of media for a political statement. The next group of variables refers to the greater impact of political advertising on the political party and the politician, the impact of the slogan on the voters, and the perceived success of the political advertising. Finally, the third group refers to the communication–dialogue with the voters through the use of media, the informative adequacy of the use of media by political parties, and the effectiveness of media regarding the recognition of the politician. Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Research question 5: Which factors can predict the use of audiovisual media services for political statements by politicians? Furthermore, the second prediction model (Regression Model 2-politicians) also has ten (10) independent variables which are tested regarding their impact on the dependent variable (impact on voting intention) (Figure 2). This model tries to predict the impact of media on the political action of the voters (impact on voting intention) according to the politicians. This model was based on the sixth research question. Research question 6: Which factors can predict the impact of audiovisual media services on the intention to vote? This research question will show the determinants for the politicians regarding the factors that affect voting intention. Figure 2 Prediction of the impact on voting intention according to the politicians Typology Politicians - Voters Finally, the existence of a media user classification of politicians and voters regarding their dynamic (or not) use of media was investigated. This attempt was based on the need for a new categorization in both groups which can be used for a strategic approach in political marketing as well as for market segmentation and the exposure of political actions via media. Particularly, this typology simultaneously checks all media (newspapers, radio, TV, the web, Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org and social media) about politicians and then to voters and how they perceived the aforementioned media's impact on political marketing and advertising. Data collection tools The questionnaire was selected as the data collection method. This selection was based on the positive aspects of the questionnaire against other methods of data collection such as semi-structured interviews and focus groups. More explicitly, the ability to collect a large amount of information and the quantification of the findings which can lead to specific propositions, were the main reasons for the selection of the questionnaire. Two questionnaires were used to collect data from two specific populations (voters-politicians). Research sample The research sample was part of the research population (Bryman & Bell, 2015) and it was selected differently for politicians and voters. The research population consists of all units that have the necessary characteristics to be selected for a research attempt. Simple random sampling was used for the selection of politicians. Specifically, politicians were in top political positions such as representatives in the Greek Parliament, mayors or regional governors, and municipal and regional counselors. Their selection was made randomly from a name list. The questionnaire was distributed either by e-mail or in person. The response rate was 68.2%. This method of sample selection allows the researcher to make easier and more valid generalizations. On the contrary, the sample of the voters was suitably selected from voters residing in the large Greek cities (Athens, Thessaloniki) of the country and from other peripheral cities as well. The inclusion criterion was the participation of the voters in the last election process. The final sample of voters was 457. The questionnaire was distributed hand by hand. This method of sample selection allows the researcher to select a great amount of data in a limited time and with limited cost and it is based on those voters who answer the research questions. Research process and data analysis A pilot study was conducted before the actual research. The research questionnaires were completed by politicians and voters to detect potential issues and make necessary changes. The outcome of the pilot study showed that some changes were necessary to be made, especially in the wording part of the questionnaire. After the data collection, the answers from all questionnaires were coded and the data were transferred in an Excel file. Potential outliers were checked and those questionnaires were excluded from the final sample. The analysis of the data Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org selected through the research process was made with SPSS. The findings were depicted in tables and figures, while inferential statistics, scale reliability, Pearson correlation, regression analysis, and comparisons with t-test and One-way ANOVA were used to answer the research questions. In all cases, the level of significance was a = 0.05 (5%). Findings The first research question explores the relationship between the choice of media for political statements by politicians and the reliability of those media. Specifically, the findings indicate that the higher the perceived reliability of audiovisual media services, the higher their use for political statements by the politician. As a result, it can be stated that politicians are intent on ensuring the credibility of audiovisual media services to use them and make a political statement. The second research question investigates the relationship between the success of political advertising, the impact on voting intention, the impact of slogans on voters, and the impact of media on the political advertising of the political party and the politician, based on the opinion of the politicians. Specifically, the findings show that the success of political advertising presents medium and strong positive statistically significant relationships with \"the impact on voting intention\" (r = .564, p < 0.01), \"the impact of the slogan on voters\" (r = .760, p < 0.01) and “the impact of media on the political advertising of the political party” (r = .541, p < 0.01). The third research question focuses on the relationship between the politician's recognition via media, political communication between the political party and the voter, and the adequate use of media by political parties for communication. The findings from the analysis show that a politician's recognition via media presents a weak positive statistically significant relationship with \"the adequate use of media by political parties for communication\" (r = .222, p < 0.05), while political communication between the political party and the voter also show a positive relationship with “the adequate use of media by political parties for communication” (r = .372, p < 0.01). Findings from the fourth research question are related to the relationship between the use of media for political statements in the case that politicians follow the political campaigns of others. They indicate that the use of media for political statements presents a medium positive statistically significant relationship in the case that \"politicians are following the political campaign of other candidates\" (r = .390, p < 0.01). Specifically, the more politicians follow Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org the political campaigns of others, the more they use media for political statements. Therefore, it can be stated that this action is associated with their attempts to stay competitive or it is a sign of \"me too\" behavior. Regression analysis has been used to create a prediction model for the use of media for political statements by politicians. The model predicts 53.9% of the variance and the variables that participate in the regression model are \"media reliability for the political statement\" (b = .426, p < 0.05) and “the impact of media on the political advertising of the political party” (b = -.268, p < 0.05). Specifically, the use of media for political statements by politicians is maximized when the media are considered to be reliable and their impact on the political advertising of the political party is lower. This finding could be explained by the attempt of the politician to cover up for the absence of the political party's advertising on media or by the attempt of the politician to promote his ideas locally with minimum association with the party and its political advertising which is nationwide. Research question six focuses on the prediction of the impact of audiovisual media services on the intention to vote. The findings suggest that the regression model predicts 56.1% of the variance and the variables included are “the political communication between the political party and the voter” (b = .204, p < 0.05) and “the impact on the political advertising of the politician” (b = .254, p < 0.05). Specifically, the impact of media on voting intention is maximized when the political communication between the political party and the voter is high, and their impact on the political advertising of the politician is also higher. Conclusion The current thesis is an empirical investigation of the topic with references to the current situation. The main goal of this study is to create a conceptual framework using traditional and new audiovisual media in political marketing and their impact on voters and politicians. The analysis reveals the practices of Greek politicians and their political parties in the use of audiovisual media by their official political parties, the politicians’ websites, and social media. Additionally, the perceptions of the voters, which are analyzed based on the same variables and findings, reveal their point of view as well as their behavior regarding the use of media. The thesis is an in-depth empirical study on the use of traditional and new media by voters and their attitudes towards these media as a source of political information. The profiles of voters and politicians are analyzed based on the use of traditional and new audiovisual media as sources of political information, means of political advertising, and Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org tools for communication by politicians and political parties. The media profile characteristics of politicians are also described to illustrate the use of media for political purposes as well as their behavior choices during a political campaign. Those profiles are established based on data regarding the use of media for political purposes, including political advertising and communication, either on traditional or new media. The study ultimately reveals the connection between politicians and voters, the use and the differences between traditional and new media, and the typology created based on their responses, a fact that leads to the creation of a conceptual framework for political marketing that illustrates the connection between the different variables in the use of traditional and new audiovisual media. Generally, the findings for politicians indicate that the higher the perceived reliability of the audiovisual media services, the higher their use for political statements by the politician. As a result, it can be stated that politicians attach great value to the credibility of audiovisual media services to use them and make a political statement. Moreover, the findings suggest that the higher the success of political advertising, the higher the impact on voting intention, the impact of slogans on voters, and the impact of audiovisual media services on the political advertising of the political party and the politician. Additionally, the higher the impact of slogans on voters and the impact of audiovisual media services on the political advertising of the political party, the higher their intention to vote. Data analysis shows that the more politicians follow the political campaigns of others, the more they use media for political statements. Therefore, it can be stated that this action is associated with their attempts to remain competitive or that it is a sign of \"me too\" behavior, which means, as mentioned above, that the politician tends to emulate other politicians' activity. In the attempt to predict the use of audiovisual media in political marketing and advertising by politicians and the intention to vote based on politicians, two models were created concerning the use of audiovisual media services for political statements (Model 1-politicians) and the impact of audiovisual media services on the intention to vote (Model 2- politicians). The first model predicts 53.9% of the variance and the variables that participate in the model are \"media reliability for the political statement\" and \"the impact of media on the political advertising of the political party\". Finally, the findings from the politicians indicate several differences regarding the demographic characteristics of the politicians and their profiles as users of audiovisual media services in political advertising, while at the same time, new media were found to have a Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org significantly higher impact on political advertising than traditional media. Explaining these findings, it can be stated that new media, as a new marketing tool, can affect the user in multiple different ways as well as that the user-politician has control of his political statement and political information in contrast with traditional media, whereas other groups (journalists and owners of the media) control the distribution of information. Research limitations Despite the important findings from the primary research, the current study has also encountered some limitations that should be taken into consideration. The first and most important limitation of the research process is related to the research sample. Specifically, the sample of politicians includes male respondents in the majority. This fact makes any comparison between male and female politicians-users of media difficult, despite their obvious differences in terms of media usage in previous studies. Therefore, a better representation of the research sample could bring more generalized findings. Similarly, the sample of the voters presents a concentration in terms of age and educational level, with most respondents belonging to the 18-35 age group, and holding a degree from Higher Education Institutions. Future research developments Regarding future research developments, the current study could be extended to countries with low usage of media for political purposes. This fact will show how different media adoption levels have an impact on the use of media for political purposes, as well as how other factors, such as the cultural background of the respondents, could affect their decisions. A similar approach could be adopted for the voters with the addition of other factors that affect their voting intention to clearly describe the whole decision-making process. References Baumgartner, J. C., & Morris, J. S. (2010). Who wants to be my friend? Obama, youth, and social networks in the 2008 campaign. In J. A. Hendricks & R. E. Denton (Eds.), Communicator-in-Chief: How Barack Obama Used New Media Technology to Win the White House (pp.51-66). Lexington Books. Bimber, B. (2014). Digital media in the Obama campaigns of 2008 and 2012: Adaptation to the personalized political communication environment. Journal of Information Technology & Politics, 11(2), 130-150. https://doi.org/10.1080/19331681.2014.895691 Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Davy, S. (2010, April 06 ). How technology changed American politics in the internet age. http://mediashift.org/2010/04/how-technology-changed-american-politics-in-the- internet-age096/ Dodson, S., & Hammersley, B. (2003, December 18 ). The web's candidate for President, The Guardian. https://www.theguardian.com/media/2003/dec/18/newmedia.uselections2004 Gibson, R. K. (2015). Party change, social media, and the rise of ‘citizen-initiated campaigning. Party politics, 21(2), 183-197. Greengard, S. (2009). The first internet president. Communications of the ACM, 52(2), 16-18. https://doi.org/10.1145/1461928.1461935 Gueorguieva, V. (2008). Citizen-voters, myparty, and Youtube: The impact of alternative communication channels on the 2006 election cycle and beyond. Social Science Computer Review, 26(3), 288-300. https://doi.org/10.1177/0894439307305636 Harfoush, R. (2009). Yes, we did! An inside look at how social media built the Obama brand. New Riders - Pearson Education. Henderson, S. (2010). Audiovisual media. In S. Moss (Ed.), The Entertainment Industry: An Introduction (pp. 112-132). Oxfordshire UK CABI. Kassimeris, G. (2005). Junta by another name? The 1974 Metapolitefsi and the Greek extra parliamentary left. Journal of Contemporary History, 40 (4), 745-762. https://doi.org/10.1177/0022009405056128 Miller, C. C. (2008, November 07). How Obama’s internet campaign changed politics, The New York Times. https://bits.blogs.nytimes.com/2008/11/07/how-obamas-internet- campaign-changed-politics/ Nelson, C. J. (2011). Grant Park: the democratization of presidential elections, 1968-2008. Brookings Institution Press. https://catalog.swanlibraries.net/Record/a1168741 Raoof, J. K., Zaman, H. B., Ahmad, A., & Al-Qaraghuli, A. (2013). Using social network systems as a tool for political change. International Journal of Physical Sciences, 8 (21), 1143-1148. Rahman, S. (2017). The advantages and disadvantages of using qualitative and quantitative approaches and methods in language “testing and assessment” research: A literature review. Journal of Education and Learning, 6(1), 102-112. DOI:10.5539/jel.v6n1p102 Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Remenyi, D., Williams, B., Money, A., & Swartz, E. (2002). Doing research in business and management: An introduction to process and method. Sage Publications Rice, A. (2004). The power of the internet. Campaigns Online. Org A Project of the Centrists for the Study of American Government at Johns Hopkins University Retrieved February 20, 2018, from http://www.campaignsonline.org/reports/1104.html Stelter, B. (2008, July 7 ). The Facebooker who friended Obama. The New York Times Retrieved May 28, 2018, from https://www.nytimes.com/2008/07/07/technology/07hughes.html Writer, S. (2016). 2008: Barack Obama embraces the power of social media in the election. Retrieved January 10, 2018, from http://www.thedrum.com/news/2016/06/23/marketing-moment-84-barack-obama- embraces-power-social-media-2008-election Δαυτόπουλος Γ.Α. (2004). Μεθοδολογία κοινωνικών ερευνών. [Datopoulos, G.A. (2004). Social research methodology. Zygos Publications]. Τηλικίδου Ε. Ι. (2004) Η έρευνα του Μάρκετιγκ, θεωρητικές προσεγγίσεις και εφαρμογές. Ελληνικά Γράμματα [Telikidou E.I. (2004). Marketing research, theoretical approaches and applications]. Greek letters. Φράγκος X.(2004). Μεθοδολογία έρευνας αγοράς και ανάλυση δεδομένων με την χρήση του στατιστικού πακέτου SPSS for windows. Interbooks [Frankos, X. (2004). Market research methodology and data analysis using the statistical package SPSS for Windows. Interbooks]. https://www.politeianet.gr/books/9789603901389-fragkos-k christos-interbooks-methodologia-ereunas-agoras-kai-analusi-dedomenon-me-chrisi tou-statistikou-paketou-spss-for-windows-14308 Postmodernism Problems / Проблеми на постмодерността Vol. 13, No. 3, 2023, ISSN: 1314-3700, https://pmpjournal.org Asymmetries abound: Ideological differences in emotion, partisanship, motivated reasoning, social network structure, and political trust Author: Jost, John T., Date: 2017-10-01 Collections: ElectionPredFeats, MediaAdsPolit, PoliticalML, Polarization Zotero Key: UB4G5I87 Cite Key: Jost17psychDiffsIdeaology Zotero Item | Lit Note �������� ���������� Asymmetries abound: Ideological differences in emotion, partisanship, motivated reasoning, social network structure, and political trust John T. Jost PII: S1057-7408(17)30055-4 DOI: doi:10.1016/j.jcps.2017.08.004 Reference: JCPS 595 To appear in: Journal of Consumer Psychology Received date: 20 August 2017 Accepted date: 20 August 2017 Please cite this article as: Jost, J.T., Asymmetries abound: Ideological differences in emotion, partisanship, motivated reasoning, social network structure, and political trust, Journal of Consumer Psychology (2017), doi:10.1016/j.jcps.2017.08.004 This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. Asymmetries abound: Ideological differences in emotion, partisanship, motivated reasoning, social network structure, and political trust John T. Jost Department of Psychology New York University Corresponding author details: Department of Psychology, New York University, 6 Washington Place, New York, NY 10003, Tel +1 212 998 7900, Email john.jost@nyu.edu Acknowledgements: The writing of this article was supported in part by the National Science Foundation (Award # BCS-1627691). I thank György Hunyady and Sharon Shavitt for helpful advice in preparing this response and several of my collaborators—Ruthie Pliskin, Eran Halperin, Pablo Barberá, Davide Morisi, and Vishal Singh—for their invaluable contributions to the research I have summarized. Abstract This article is a response to Rao (2017), Krishna and Sokolova (2017), and Oyserman and Schwarz (2017), all of whom provided extremely thoughtful commentaries on a target article in which I summarized several lines of research in political psychology on liberal-conservative differences in personality, cognition, motivation, values, and neurological structures and functions (Jost, 2017a). I begin by correcting a possible misconception, namely that the theory of political ideology as motivated social cognition cannot explain dynamic shifts in ideological affinities; on the contrary, we have demonstrated that “top-down” situational—as well as “bottom-up” dispositional—processes work in conjunction to produce ideological outcomes, and this is why tailored forms of political persuasion can be highly effective in producing change. Next I describe additional evidence (including previously unpublished evidence) bearing on ideological symmetries and asymmetries with respect to emotion, partisanship, social identification, motivated reasoning, social network structure, and political trust. I end by asking consumer psychologists for their continued collaboration in addressing profound challenges associated with understanding and reconciling sources of ideological divergence— not only for the sake of research in behavioral science but also for the smooth functioning of democratic society. Keywords: political psychology, ideology, liberalism, conservatism, motivation “Ideologies separate us. Dreams and anguish bring us together.” (Eugène Ionesco, 1958) Through this most collegial and constructive interdisciplinary exchange I am delighted to discover that scholarly interest in political psychology is even deeper and broader than I had anticipated among theorists and practitioners of consumer behavior. I want to first thank the editorial leadership of the Journal of Consumer Psychology—and especially Sharon Shavitt—for providing me with such a marvelous opportunity to interact in this way with a new and welcoming academic community. I am also extremely grateful to Akshay Rao, Aradhna Krishna, Tatiana Sokolova, Daphna Oyserman, and Norbert Schwarz for their thoughtful entreaties to expand the discussion of political ideology into neighboring territories, such as emotion, partisan identification, motivated reasoning, social network structure, and political trust. Before I jump into the empirical evidence bearing on these and related phenomena, please allow me to correct a possible misconception. Theoretical Clarification: The Integration of “Top-Down” and “Bottom-Up” Processes Although I agree with most of what Oyserman and Schwarz (2017) have written, they are mistaken to associate my theoretical perspective with the “essentialist belief” that people are “the same across time and space” (p. 4)—a perspective that, they argue, needs to be “counterbalanced by a consideration of immediate contextual variables influencing motivation, identities and attitudes” (p. 5). Rao (2017) arrived at a similar misreading, so I must not have been clear enough about my theoretical foundations in social psychology. He imagines that it is central to my perspective that “dyed-in-the wool-conservatives and progressives” are very much alike and, above all, “brand loyal.” (An earlier draft of suggested that I assumed ideological differences to be “immutable,” which is not at all the case). With all due respect to my esteemed commentators, these are caricatures of the position I sought to articulate, namely that—in contrast to the assumption that ordinary citizens are simply “non-ideological” (e.g., Kinder & Kalmoe, 2017)—there are a number of meaningful social, cognitive, and motivational differences associated with people who drawn to the left vs. right poles in the ideological sphere (Jost, 2017a, 2017b). From the start, my colleagues and I have emphasized that there are dynamic situational (or environmental) as well as fairly stable dispositional (or personality) determinants of political orientation (Jost, Glaser, Kruglanski, & Sulloway, 2003). Inspired by the work of Silvan Tomkins (1963), we propose that ideology is a “love affair”—an “elective affinity”—between a person (who possesses a wide range of chronic and temporary needs, interests, and motivations) and a socially constructed, historically situated belief system that offers some opportunity for seduction. Thus, “top-down” processes of institutional socialization and communication, including exposure to messages from political elites (what Krishna and Sokolova [2017] refer to as “macro-level factors”) interact with “bottom-up” psychological (or “micro-level”) factors to produce matching (or resonant) ideological outcomes (Jost, Federico, & Napier, 2009). There is nothing “immutable” (or “invariant,” as Oyserman and Schwarz [2017) suggest) about these outcomes, and there is no non-tautological sense in which they may be said to spring from “essential differences” between liberal and conservative types of people. We have often pointed out, for instance, that education and travel experiences—which are powerful forces that can lead people to perceive that which is uncertain, unfamiliar, complex, and ambiguous not as threatening but as exciting or enjoyable—tend to make people more open-minded, liberal, egalitarian, and interested in diversity (see Jost, Nosek, & Gosling, 2008). Conversely, exposure to highly threatening circumstances such as terrorist attacks, governmental warnings, and drastic shifts in racial demography often precipitate conservative shifts in political attitudes (e.g., Bonanno & Jost, 2006; Craig & Richeson, 2014; Schüller, 2015), presumably because feeling threatened elicits “motivated closed-mindedness” (Thόrisdottir & Jost, 2011). In a recent meta-analytic review we observed that in 22 of 34 studies exposure to objectively threatening stimuli significantly increased the attractiveness of conservative, right wing ideas and opinions (Jost, Stern, Rule, & Sterling, 2017; see Figure 1). To paraphrase Ionesco, then, anguish may indeed supersede ideological differences—at least temporarily, for better and for worse. That is, terrorist attacks and other intense threats seem to increase consensus around conservative, patriotic, and authoritarian ways of thinking (e.g., Van de Vyver, Houston, Abrams, & Vasiljevic, 2016; van der Toorn, Nail, Liviatan, & Jost, 2014; Vasilopoulos, Marcus, & Foucault, 2017). These converging lines of research show that external situations or contexts can contribute to dynamic shifts in ideological affinities for internal, psychological reasons—such as increased activation of epistemic, existential, or relational motives. In this way, situation-specific processes are built into the model of political ideology of social cognition. Thus, I agree that political orientations are, among other things, “situated identities” (Oyserman & Schwarz, 2017) and that “segments are dynamic” and many people “transition from one state to the other because of changes in life-stage, the environment . . . and the like” (Rao, 2017, p. 16). Likewise, my perspective should not be taken to suggest that political persuasion of “micro-targets” through rational (or emotional) means is futile. I certainly agree with Rao (2017) that ideological preferences may be “nudged,” and I applaud his own efforts in this regard (e.g., Kim, Rao, & Lee, 2009). And I concur with Oyserman and Schwarz (2017) that shifting the boundaries of social identification may be one especially fruitful way of promoting ideological change. Along these lines, my students and I have demonstrated that it is possible to increase support for environmental initiatives aimed at addressing anthropogenic climate change (Feygina, Jost, & Goldsmith, 2010) as well as the rights and liberties of Muslim Americans (Nam & Jost, 2014) by emphasizing ways in which these causes are congruent (rather than incongruent) with the preservation of American values and ideals. Interventions such as these seek to harness system justification motivation on behalf of social change rather than eliciting system-defensiveness. It may be that dreams, in other words, are capable of bringing us together as well. I alluded to some examples of political persuasion and situationally induced ideological shifts in the target article, but I probably should have made more explicit the fact that we regard ideological outcomes as shaped by environmental and contextual (“top-down”) as well personal and dispositional (“bottom-up”) factors. I am grateful for the opportunity to try to clear up misconceptions concerning the theoretical (and meta-theoretical) underpinnings of our research program on political ideology as motivated social cognition. More Evidence of Ideological Symmetries and Asymmetries Emotional Experiences Rao (2017) is on solid ground when he points out that there are important emotional differences between liberals and conservatives and that I said fairly little about these differences in my target article. I am pleased to have the opportunity to add a bit more now about the connection between ideology and emotion. It was Tomkins (1963) who first proposed that leftists are preoccupied with hope, excitement, empathy, sadness, and guilt, whereas rightists are fixated on fear, anger, contempt, pessimism, disgust, and shame (i.e., self-disgust). Ideological differences in fear and disgust have been investigated rather thoroughly at this point (e.g., Jost et al., 2017; Terrizzi, Shook, & McDaniel, 2013), but research on most of the others has been patchy thus far. Nevertheless, the evidence that does exist is promising. Recent work suggests that anger, for instance, promotes social and economic forms of conservatism (Milburn & Conrad, 2016). A series of studies by Kettle and Salerno (2017) revealed that making people angrier had the effect of increasing their competition for material resources, and this led them to show stronger support for political candidates who were espousing conservative economic values. With respect to social conservatism, terrorist activity in France appears to have increased authoritarianism among right-wingers, and the effect was mediated by increased anger (Vasilopoulos et al., 2017). Finally, as noted by Krishna and Sokolova (2017), experiments conducted by Huber, Van Boven, Park, and Pizzi (2015) showed that inducing feelings of anger (vs. sadness) led Republicans—but not Democrats—to strengthen their perceptions of political conflict and polarization. Emotional Self-Regulation My own research with Ruthie Pliskin and Eran Halperin in Israel and the U.S. suggests that leftists and rightists differ not only in terms of emotional experiences but also in terms of their self-regulation goals. More specifically, we have found that rightists are more motivated than leftists to increase their own feelings of anger and fear when it comes to issues of immigration and electoral competition (Pliskin, Halperin, & Jost, 2017). Leftists, by contrast, are more motivated to increase feelings of hope in the context of the longstanding Arab-Israeli conflict (Pliskin, Nabet, Jost, Tamir, & Halperin, 2017). These findings are broadly consistent with the notion that down-regulating negative emotions such as anger and disgust undermines support for conservative ideas and policies (Feinberg, Willer, Antonenko, & John, 2012; Lee, Sohn, & Fowler, 2013). Rao (2017) wonders whether emotional appeals based on nostalgia would be more successful on the left or right and mentions some preliminary support for the latter possibility (Lasaleta, Rao, & Kondaveeti, 2017).[1] This question has also been addressed by Lammers and Baldwin (2017), who—based on the theory of political ideology as motivated social cognition (Jost et al., 2003)—reasoned that conservatives, by virtue of their stronger system-justifying tendencies and attachment to traditions arising from the status quo, would be more likely than liberals to be persuaded by nostalgic messages. With respect to a number of important issues (including gun control, climate change, and support for racial and ethnic diversity), Lammers and Baldwin have demonstrated that framing progressive messages in terms of past-oriented (as opposed to future-oriented) themes leads conservative (but not liberal) recipients to be less resistant to social change. Partisan Identification Krishna and Sokolova (2017) are absolutely right that—in addition to ideological differences between the left and right (Jost, 2017a, 2017b)—the social and political world is rife with partisan commitments arising from affiliation with political parties, such as the Democratic 1 Rao (2017) notes that—in research by Lasaleta, Rao and Kondaveeti (2017)—conservatives were found to be highly motivated to “belong to like-minded conservative-oriented groups and organizations” and that this desire to affiliate with like-minded others was greater among conservatives than liberals. It is worth pointing out that my colleagues and I have observed a very similar phenomenon, namely a stronger relational motivation to “share reality” with like-minded others among conservatives than liberals (Stern, West, Jost, & Rule, 2014). and Republican parties. Oyserman and Schwarz (2017), too, underscore the role of identity based motivations in political psychology and fact that these motivations may be highly sensitive to local, contextual factors. Thus, as Krishna and Sokolova note, it is possible (at least in principle, in some situations) to be a liberal Republican who advocates free public transportation for the elderly or a conservative Democrat who wants to lower property taxes. At the same time, it is clear that, as an empirical matter, the connection between partisan identification and liberal-conservative ideology has risen sharply over the past few decades. As a result, there are many fewer liberal-minded Republicans and conservative-minded Democrats than in earlier eras of American politics. In the mid-1970’s, the correlation between ideology and partisan identification ranged from r = .15 to .30 in adolescent and adult samples; it now hovers around r = .50 to .60 (Twenge, Honeycutt, Prislin, & Sherman, 2016, p. 1380). The strength of the correlation between partisanship and ideology is taken by political scientists to be an important sign that we are living through an especially polarized historical period (e.g., Abramowitz & Saunders, 2008). I readily acknowledge that political partisanship (or social identification with a political party) is not the same thing as ideology—although political parties are much more often than not ideological in key respects. I also agree that some of the effects of partisanship are symmetrical, in the sense that Democrats and Republicans are capable of acting in self-serving and group-serving ways that are more similar than dissimilar (e.g., Iyengar & Westwood, 2015; Jost, Hennes, & Lavine, 2013).[2] At the same time, there are also a number of asymmetrical effects, 2 I share Rao’s (2017) concerns that drawing superficial parallels between conservatives and progressives may foster a sense of false equivalence when it comes to rates of embracing self-, group-, and systemserving “conclusions . . . of questionable veracity” (see also Jost, Hennes, & Lavine, 2013). I would, for that is, ways in which the behavior of Democrats and Republicans is qualitatively different— and these are highly consistent with the account of ideological differences that I provided in the target article (Jost, 2017a; see also Jost, 2017b). As Krishna and Sokolova (2017) point out, registered Democrats have long outnumbered Republicans in the U.S., and yet Republicans do as well (if not better) than Democrats at election time. This implies that Democrats are more willing to vote for Republicans than vice versa or—as Rao (2017) would say—Democrats are more likely to be “brand switchers” than Republicans. Indeed, this is part of a much broader pattern of asymmetry. Evidence is accumulating rapidly in political science that Republicans are more intensely ideological—and cohesive, in terms of unity, strength, and single-mindedness of purpose—in comparison with Democrats (Abramowitz, 2015; Grossmann & Hopkins, 2016; Hacker & Pierson, 2006; Mann & Ornstein, 2012). Thus, the increasing political polarization in American society is largely attributable to the fact that Republicans have grown more extreme, while Democrats have largely stayed the same (Barber & McCarty, 2013; McCarty, Poole, Rosenthal, & Hare, 2012). Mann and Ornstein (2012) conclude that: “one of the two major parties, the Republican Party, has become an insurgent outlier—ideologically extreme; contemptuous of the inherited social and economic policy regime; scornful of compromise; unpersuaded by conventional understanding of facts, evidence, and science; and dismissive of the legitimacy of its political opposition” (p. xiv). I submit that political psychology has something to say about why this is the case: there are, in other words, “bottom-up” as well as that very reason, question his emphasis on the concept of “tribalism” and his choice of metaphor of Capulets and Montagues from Romeo and Juliet. In this response I summarize even more evidence that the extent of partisan bias (or “tribal” behavior) is highly asymmetrical, at least when it comes to the left and right in American politics—and in some other countries as well. “top-down” factors at play. Motivated Reasoning On the topic of motivated reasoning, Krishna and Sokolova (2017) recount Cohen’s (2003) experiments involving college students, which suggested that Democrats and Republicans were equally susceptible to partisan biases with respect to policy preferences. In these studies, participants were generally supportive of policies that they believed were proposed by members of their own political party and rejecting of policies that they believed were proposed by members of the opposing party. Cohen concluded on the basis of these results that partisanship outweighed philosophical (or ideological) concerns and that partisan bias was a symmetrical phenomenon exhibited by Democrats and Republicans alike. Before accepting these conclusions, however, one should ponder the results of a parallel set of experiments conducted by Bullock (2011), which were based on larger and more representative samples of adult partisans. These studies revealed that Republicans were significantly more influenced by partisan cues, in comparison with Democrats. Conversely, Democrats were more attentive than Republicans to substantive information about policy content. Krishna and Sokolova (2017) also cite research by Sen (2017), which found that Republicans were more sensitive than Democrats to partisan cues when asked to evaluate judicial candidates. Taken in conjunction, these results are consistent with Krishna and Sokolova’s (2017) argument that systematic (vs. heuristic) processing of information attenuates the influence of partisan cues—as well as the observation that liberals are more likely than conservatives to engage in systematic processing (Jost, 2017a, 2017b). Social Network Structure Krishna and Sokolova (2017) argue that homogenous social networks may exacerbate partisan biases by limiting exposure to novel and contradictory information and encouraging heuristic (as opposed to systematic) forms of reasoning. It should be of interest, then, that research on social media suggests that conservatives may be more likely than liberals to favor an “echo-chamber” type of informational environment. Boutyline and Willer (2017), for instance, observed in a study of over 260,000 Twitter users that more conservative users, such as followers of the Cato Institute, tended to have more homogenous online networks than liberal users, such as followers of Amnesty International. Findings such as these call into question the common assumption that those on the left and right are equally tempted to avoid contradictory points of view by engaging in selective information exposure (e.g., Frimer, Skitka, & Motyl, 2017). Barberá, Jost, Nagler, Tucker, and Bonneau (2015) estimated the ideological positions of 3.8 million Twitter users and—drawing on a dataset of 150 million tweets pertaining to 12 political and non-political issues—compared rates of cross-ideological retweeting, that is, the tendency for liberals to forward messages written by conservatives (and vice versa). For 11 of the 12 issues, we observed that liberals were in fact more likely than conservatives to engage in cross-ideological dissemination of information (see Figure 2). Although one cannot infer on the basis of retweeting behavior that liberals approved of messages written by conservatives, the evidence suggests that liberals were more likely than conservatives to expose themselves to messages from the other “side” and, indeed, to pass them on to others members of their social networks. A number of other investigations lead to the virtually inescapable conclusion that, at least during the present historical period, liberals are less enamored than conservatives of informational environments that are biased, polarized, conspiratorial, and otherwise riddled with false or misleading statements (Benkler, Faris, Roberts, & Zuckerman, 2017; Cherry, 2017; Hjorth & Adler-Nissen, 2017; Marwick & Lewis, 2017; Miller, Saunders, & Farhart, 2015). Political Trust and its Implications for Democratic Functioning The legitimacy and stability of democratic systems depend, in crucial respects, upon some degree of political trust and a certain readiness on the part of citizens to accept the will of the majority and, if necessary, to place the public good above partisan interests. Recently, however, political scientists have worried that the erosion of democratic norms has led to a situation in which “partisans whose party is out of power have almost no trust at all in a government run by the other side” (Hetherington & Rudolph, 2015, p. 1). Consistent with all of the other ideological asymmetries that I have been describing, my colleagues and I have found that conservatives and Republicans are more likely than liberals and Democrats to trust the government more when it is led by a chief executive who shares their own ideological orientation than when it is not (see Figure 3) More specifically, Morisi, Singh, and Jost (2017) analyzed several decades’ worth of data from the American National Election Studies (ANES), General Social Survey (GSS), and European Social Survey (ESS) and discovered that citizens on the right trust the government and national parliament more when the leader of the government shares their own ideology, and they do so more than citizens on the left. Conversely, leftists are more willing to grant legitimacy to governments led by rightists than vice versa. This ideological asymmetry also applies to otherwise ostensibly principled stances on “small” or “limited government.” We find that conservatives are, in fact, much more approving of governmental intervention when there is a like-minded executive in office than when there is not, but liberals do not exhibit the same double-standard. These findings suggest a number of worrying implications about democratic functioning, including the possibility that conservative intransigence provides a systematic advantage in electoral politics, because liberal governments, even after they are elected, may find it more difficult than conservative governments to maintain popular consent. As Mann and Ornstein (2012) point out, the stage is thereby set for “dysfunctional politics,” insofar as significant ideological asymmetries such as this one make it “extremely difficult to enact policies responsive to the country’s most pressing challenges” (p. xiv.). Concluding Remarks In a target article I sought to provide an integrative summary of several independent lines of empirical research revealing that liberals and conservatives differ in terms of personality characteristics, cognitive processing styles, motivational concerns, personal values, and neurological structures and physiological functions (Jost, 2017a). I am delighted to learn from Rao (2017), Krishna and Sokolova (2017), and Oyserman and Schwarz (2017) that this work is of genuine interest—as I had hoped—to scholars in consumer psychology. In this reply, I have sought to take seriously my commentators’ myriad interests in emotion, partisanship, social identification, motivated reasoning, social network structure, and political trust and to provide additional evidence (including previously unpublished evidence) of left-right symmetries and asymmetries pertaining to each of these phenomena. Once again, I wish to thank my esteemed commentators for sharing their creative, constructive, and insightful perspectives with me. As I hope to have conveyed here, the challenges of understanding and reconciling the sources of ideological divergence are profound indeed—not only for research in the social and behavioral sciences but also for the smooth functioning of democratic society. My sense is that we need to work together in both of these areas, and I would very much like to count on your continued collaboration. References Abramowitz, A.I. (2015). The new American electorate: Partisan, sorted, and polarized. In J.A. Thurber & A. Yoshinaka (Eds.), American gridlock: The sources, character, and impact of political polarization (pp. 19-44). New York: Cambridge University Press. Abramowitz, A.I., & Saunders, K.L. (1998). Ideological realignment in the U.S. electorate. Journal of Politics, 60, 634–652. Barber, M., & McCarty, N. (2013). Causes and consequences of polarization. In J. Mansbridge & C. J. Martin (Eds.), Report of the task force on negotiating agreement in politics (pp. 19-53). Washington, DC: American Political Science Association. Barberá, P., Jost, J.T., Nagler, J., Tucker, J.A., & Bonneau, R. (2015). Tweeting from left to right: Is online political communication more than an echo chamber? Psychological Science, 26, 15311542. Benkler, Y., Faris, R., Roberts, H., & Zuckerman, E. (2017). Breitbart-led right-wing media ecosystem altered broader media agenda. Columbia Journalism Review. https://www.cjr.org/analysis/breitbart-media-trump-harvard-study.php Bonanno, G.A., & Jost, J.T. (2006). Conservative shift among high-exposure survivors of the September 11th terrorist attacks. Basic and Applied Social Psychology, 28, 311-323. Boutyline, A., & Willer, R. (2017). The social structure of political echo chambers: Variation in ideological homophily in online networks. Political Psychology, 38, 551–569. Bullock, J.G. (2011). Elite influence on public opinion in an informed electorate. American Political Science Review, 105, 496-515. Cherry, T. (2017, February 9). Yes, fake news exists on the left—but it’s being overblown. Media Matters for America. https://www.mediamatters.org/blog/2017/02/09/yes-fake-news-exists- left-its-being-overblown/215273 Cohen, G.L. (2003). Party over policy: The dominating impact of group influences on political beliefs. Journal of Personality and Social Psychology, 58, 808-822. Craig, M.A., & Richeson, J.A. (2014). On the precipice of a “majority-minority” America: Perceived status threat from the racial demographic shift affects white Americans’ political ideology. Psychological Science, 25, 1189-1197. Feinberg, M., Willer, R., Antonenko, O., & John, O.P. (2012). Liberating reason from the passions. Psychological Science, 23, 788-795. Feygina I., Jost J.T., Goldsmith R.E. (2010). System justification, the denial of global warming, and the possibility of “system-sanctioned change.” Personality and Social Psychology Bulletin, 36, 326–338. Frimer, J.A., Skitka, L., & Motyl, M. (2017). Liberals and conservatives are similarly motivated to avoid exposure to one another’s opinions. Journal of Experimental Social Psychology, 72, 1-12. Grossmann, M., & Hopkins, D.A. (2016). Asymmetric politics: Ideological Republicans and group interest Democrats. New York: Oxford University Press. Hacker, J.S., & Pierson, P. (2006). Off center: The Republican revolution and the erosion of American democracy. New Haven, CT: Yale University Press. Hetherington, M.J., & Rudolph, T.J. (2015). Why Washington won’t work: Political trust and the governing crisis. Chicago: University of Chicago Press. Hjorth, F., & Adler-Nissen, R. (2017). The asymmetric reach of online Pro-Kremlin disinformation. Paper presented at the CVAP Research Seminar, University of Copenhagen, Denmark. Huber, M., Van Boven, L., Park, B., & Pizzi, W.T. (2015). Seeing red: Anger increases how much Republican identification predicts partisan attitudes and perceived polarization. PLoS ONE 10(9): e0139193. https://doi.org/10.1371/journal.pone.0139193 Iyengar, S., & Westwood, S.J. (2015). Fear and loathing across party lines: New evidence on group polarization. American Journal of Political Science, 59, 690–707. Jost, J.T. (2017a). The marketplace of ideology: “Elective affinities” in political psychology and their implications for consumer behavior. Journal of Consumer Psychology, this issue. Jost, J.T. (2017b). Ideological asymmetries and the essence of political psychology. Political Psychology, 38, 167-208. Jost, J. T., Federico, C. M., & Napier, J. L. (2009). Political ideology: Its structure, functions, and elective affinities. Annual Review of Psychology, 60, 307-337. Jost, J. T., Glaser, J., Kruglanski, A. W., & Sulloway, F. J. (2003). Political conservatism as motivated social cognition. Psychological Bulletin, 129, 339-375. Jost, J.T., Hennes, E.P., & Lavine, H. (2013). “Hot” political cognition: Its self-, group, and system-serving purposes. In D. Carlston (Ed.), Oxford handbook of social cognition (pp. 851875). New York: Oxford University Press. Jost, J. T., Nosek, B. A., & Gosling, S. D. (2008). Ideology: Its resurgence in social, personality, and political psychology. Perspectives on Psychological Science, 3, 126-136. Jost, J.T., Stern, C., Rule, N.O., & Sterling, J. (2017). The politics of fear: Is there an ideological asymmetry in existential motivation? Social Cognition, 35, 324–353. Kettle, K.L., & Salerno, A. (2017). Anger promotes economic conservatism. Personality and Social Psychology Bulletin. doi: 10.1177/0146167217718169 Kim, H., Rao, A.R., & Lee, A.Y. (2009). It’s time to vote: The effect of matching message orientation and temporal frame on political persuasion. Journal of Consumer Research, 35, 877-888. Kinder, D. R., & Kalmoe, N. P. (2017). Neither liberal nor conservative: Ideological innocence in the American public. Chicago, IL: University of Chicago Press. Krishna, A., & Sokolova, T. (2017). A focus on partisanship: How it impacts voting behaviors and political attitudes. Journal of Consumer Psychology, this issue. Lammers, J., & Baldwin, M. (2017, July). Nostalgia as the core of political conservatism. Presented at the Annual Meeting of the International Society of Political Psychology (ISPP), Edinburgh, Scotland. Lasaleta, J., Rao, A. R. & Kondaveeti, V. (2017). Back to the future: Preferences for future-oriented versus nostalgic ads among conservatives and liberals. Working Paper, Carlson School of Management, University of Minnesota. Lee J. J., Sohn, Y., & Fowler, J. H. (2013). Emotion regulation as the foundation of political attitudes: Does reappraisal decrease support for conservative policies? PLoS ONE, 8(12): e83143. https://doi.org/10.1371/journal.pone.0083143 Mann, T.E., & Ornstein, N.J. (2012). It’s even worse than it looks: How the American constitutional system collided with the new politics of extremism. New York: Basic Books. Marwick, A., & Lewis, R. (2017). Media manipulation and disinformation online. Data & Society. https://datasociety.net/pubs/oh/DataAndSociety_MediaManipulationAndDisinformation Online.pdf McCarty, N., Poole, K., Rosenthal, H., & Hare, C. (2012). Polarization is real (and asymmetric). The Monkey Cage. http://themonkeycage.org/2012/05/polarization-is-real-and-asymmetric/ Milburn, M.A., & Conrad, S. (2016). Raised to rage: The politics of anger and the roots of authoritarianism. Cambridge, MA: MIT Press. Miller, J.M., Saunders, K.L., & Farhart, C.E. (2015). Conspiracy endorsement as motivated reasoning: The moderating roles of political knowledge and trust. American Journal of Political Science, 60, 824-844. Morisi, D., Singh, V., & Jost, J.T. (2017). The “President-in-power” effect: Is there an ideological asymmetry in political trust? Manuscript submitted for publication. Nam, H.H., & Jost, J.T. (2014, July). Which American way? Overcoming resistance to change through system-sanctioned appeals. Presented at the Annual Meeting of the International Society of Political Psychology (ISPP), Rome, Italy. Oyserman, D., & Schwarz, N. (2017). Conservatism as a situated identity: Implications for consumer behavior. Journal of Consumer Psychology, this issue. Pliskin, R., Halperin, E., & Jost, J.T (2017, July). Mapping ideological differences in anger regulation. Presented at the Annual Meeting of the International Society of Political Psychology (ISPP), Edinburgh, Scotland. Pliskin, R., Nabet, E., Jost, J.T., Tamir, M., & Halperin, E. (2017). Holding on to hope (or fear): Emotional change in the service of ideological self-justification. Presented at “Understanding the Winds of Change” Conference, Appingedom, the Netherlands. Rao, A.R. (2017). Red, blue and purple States of mind: Segmenting the political marketplace. Journal of Consumer Psychology, this issue. Schüller, S. (2015). The 9/11 conservative shift. Economics Letters, 135, 80-84. Stern, C., West, T.V., Jost, J.T., & Rule, N.O. (2014). “Ditto heads”: Do conservatives perceive greater consensus within their ranks than liberals? Personality and Social Psychology Bulletin, 40, 1162-1177. Terrizzi, J.A., Shook, N.J., & McDaniel, M.A. (2013). The behavioral immune system and social conservatism: A meta-analysis. Evolution & Human Behavior, 34, 99–108. Thórisdóttir, H., & Jost, J.T. (2011). Motivated closed‐mindedness mediates the effect of threat on political conservatism. Political Psychology, 32, 785-811. Tomkins, S. (1963). Left and right: A basic dimension of ideology and personality. In R. W. White (Ed.), The study of lives: Essays on personality in honor of Henry A. Murray (pp. 388– 411). New York: Atherton Press. Twenge, J.M., Honeycutt, N., Prislin, R., & Sherman, R.A. (2016). More polarized but more independent: Political party identification and ideological self-categorization among U.S. adults, college students, and late adolescents, 1970-2015. Personality and Social Psychology Bulletin, 42, 1364–1383. Van de Vyver, J., Houston, D. M., Abrams, D., & Vasiljevic, M. (2016). Boosting belligerence: How the July 7, 2005, London Bombings affected liberals’ moral foundations and prejudice. Psychological Science, 27, 169-177. van der Toorn, J., Nail, P., Liviatan, I., & Jost, J.T. (2014). My country, right or wrong: Does activating system justification motivation eliminate the liberal-conservative gap in patriotism? Journal of Experimental Social Psychology, 54, 50-60. Vasilopoulos, P., Marcus, G.E., & Foucault, M. (2017). Emotional responses to the Charlie Hebdo attacks: Addressing the authoritarianism puzzle. Political Psychology. doi:10.1111/pops.12439 Figure 1: Distribution of Average Effect Sizes for Studies Investigating the Hypothesis that Exposure to Objectively Threatening Circumstances Would Be Associated with Political Conservatism Source: This figure, which was prepared by Joanna Sterling and based on a meta-analytic review by Jost, Sterling, Stern, and Rule (2007), is adapted from Jost (2017b, Figure 14, p. 188). Aggregating across these studies, the unweighted (r = .14) and weighted (r = .07) average effect sizes indicated that exposure to objectively threatening circumstances was associated with conservative preferences, although the effect size was modest. Figure 2: Cross-Ideological Retweeting Rates: Liberals Were More Likely to Forward Twitter Messages Written by Conservatives than Vice Versa for 11 out of 12 Political and Nonpolitical Topics Note: This figure, which was prepared by Pablo Barberá, is adapted from Barberá, Jost, Nagler, Tucker, and Bonneau (2015). Each point represents an exponentiated coefficient of a Poisson regression for each topic and ideological group. The lines indicate confidence intervals at the 99.9% level, some of which are invisible because of the extremely large sample sizes in terms of tweets. The dashed vertical line corresponds to a value of 1, which would indicate identical retweeting rates for individuals of the same vs. different ideological orientations. Figure 3: An Ideological Asymmetry in Political Trust: Conservatives and Rightists Exhibit Stronger Tendencies than Liberals and Leftists in the U.S. and Europe to Trust the Government More When It is Led by a Chief Executive Who Shares Their Ideology (vs. Not) ESS Note: This figure, which was prepared by Davide Morisi, is adapted from Morisi, Singh, and Jost (2017, Figures 2 and 5). It illustrates average marginal effects of a president/prime minister with a similar ideology on trust in the government, relative to a president with a different ideology (value 0 on Y-axis) with 95% confidence intervals. Estimates are based on OLS regressions. ANES = American National Election Studies; GSS = General Social Survey; ESS = European Social Survey. |Col1|A C C E P T E D M A| |---|---| Voting behavior is reflected in amygdala response across cultures Author: Rule, Nicholas O., Date: 2009 Dec 5 Collections: NeuroPsychoLinguisticPolitics Zotero Key: RSDZ8JKG Cite Key: Rule09voteAmygdalaCulture Zotero Item | Lit Note doi:10.1093/scan/nsp046 SCAN (2010) 5, 349^355 Voting behavior is reflected in amygdala response across cultures Nicholas O. Rule,[1] Jonathan B. Freeman,[1] Joseph M. Moran,[2] John D. E. Gabrieli,[2] Reginald B. Adams Jr,[3] and Nalini Ambady[1] 1Psychology Department, Tufts University, Medford, Massachusetts, USA, 02155, 2Brain and Cognitive Sciences Department, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, 02139 and [3]Psychology Department, The Pennsylvania State University, University Park, Pennsylvania, USA, 16802 Voting to determine one’s leaders is among the most important decisions we make, yet little is known about the brain’s role in how we come to these decisions. Behavioral studies have indicated that snap judgments of political candidates’ faces can predict election outcomes but that the traits that lead to these judgments differ across cultures. Here we sought to investigate the neural basis for these judgments. American and Japanese natives performed simulated voting judgments of actual American and Japanese political candidates while neural activity was measured using functional magnetic resonance imaging (fMRI). Candidates for whom participants chose to vote elicited stronger responses in the bilateral amygdala than candidates for whom participants chose not to vote. This was true regardless of either the participant’s culture or the target’s culture, suggesting that these voting decisions provoked the same neural response cross-culturally. In addition, we observed a participant culture by target culture interaction in the bilateral amygdala. American and Japanese participants both showed a stronger response to cultural outgroup faces than they did to cultural ingroup faces, however this was unrelated to their voting decisions. These data provide insight to the mechanisms that underlie our snap judgments of others when making voting decisions and provide a neural correlate to cross-cultural consensus in social inferences. Keywords: culture; nonverbal behavior; face perception; politics; amygdala INTRODUCTION Voting is one of the most important decisions that individuals can make in a democratic society. Despite the weighty consequences that result from an electoral choice, research has shown that electoral outcomes can be predicted based on individuals’ snap judgments of politicians’ faces (Todorov et al., 2005; Chiao et al., 2008a). One study found that judgments of competence based on seeing American political candidates’ faces for only 100 milliseconds accurately predicted 59.6% of the actual electoral outcomes (Ballew & Todorov, 2007). Culture may play an important role in determining for whom individuals choose to vote. Although much previous work has shown evidence for cultural universality in the perceptions of individuals for some personality and physiognomic traits (Zebrowitz et al., 1993; Albright et al., 1997), there is also research showing that culture has a substantial impact on the traits that people value in their leaders (Misumi & Peterson, 1985). For instance, leaders in Japan versus the USA are believed to vary markedly in Received 22 March 2009; Accepted 19 October 2009 Advance Access publication 5 December 2009 The authors would like to thank Mariko Shimada, Ryutaro Kawabata and Kana Okano for their assistance with this study. This research was supported in part by National Science Foundation grant (BCS-0435547) to Nalini Ambady, a National Science Foundation graduate research fellowship to Nicholas O. Rule, and a Social Sciences Research Institute grant from the Pennsylvania State University to Reginald B. Adams, Jr. Correspondence should be addressed to Nicholas O. Rule, Psychology Department, Tufts University, 490 Boston Avenue, Medford, MA 02155, USA. E-mail: nicholas.rule@tufts.edu character (e.g., Ayman, 1993; S. N. Kaplan, 1994; Yamagishi et al., 1998). Yet the majority of studies examining the relationship between trait inferences and electoral outcomes have examined Western cultures. Todorov and colleagues (Todorov et al., 2005; Ballew & Todorov, 2007) investigated Americans’ judgments of American candidates, Martin (1978) examined Australians’ judgments of Australian candidates, Poutvaara et al. (2009) conducted an online survey of mostly Western respondents’ judgments of Finnish candidates, and Antonakis and Dalgas (2009) studied Swiss children’s and adults’ judgments of French candidates. One recent study (Rule et al., in press), however, examined American and Japanese participants’ judgments of American and Japanese political candidates. This study showed that although both American and Japanese perceivers agreed across cultures in their judgments of individual candidates’ faces, they chose to elect different candidates. Specifically, traits related to power (dominance and facial maturity) were related to American participants’ electoral choices whereas traits related to warmth (likeability and trustworthiness) were related to Japanese participants’ electoral choices. The current study was therefore interested in exploring the basis for this cross-cultural convergence in perception but divergence in electoral choice. To do so, we measured American and Japanese perceivers’ brain activity while making electoral choices using fMRI. � The Author (2009) Published by Oxford University Press For Permissions please email: journals permissions@oxfordjournals org 350 SCAN (2010) N.O.Rule et al. One brain area believed to be critically involved in the processing of social information about others is the amygdala. Lesion studies have shown that patients with amygdala damage are severely impaired in their ability to evaluate others. For instance, individuals with bilateral amygdala damage are significantly more likely to perceive untrustworthy faces as trustworthy and to believe that these individuals are approachable. Additionally, patients with amygdala damage are impaired in reading others’ emotions and inferring their thoughts from nonverbal cues (Adolphs et al., 2002). Functional magnetic resonance imaging (fMRI) studies of healthy participants have shown that the amygdala is involved not only in evaluations of trustworthiness from faces (Winston et al., 2002) but also in the evaluation of facial attractiveness (Winston et al., 2007) and a host of other social traits (e.g. responsibility, intelligence, and competence, among others; Todorov & Engell, 2008). In addition, the amygdala has been implicated in the evaluative processing of social information (see Adolphs, 2003; Phelps, 2006; Schiller et al., 2009) and is sensitive to a stimulus’s motivational importance (Phelps & LeDoux, 2005; Cunnigham et al., 2008) or the degree to which it is arousing (Anderson et al., 2003; Sander et al., 2003). The amygdala therefore seemed like a probable structure to be involved in voting judgments and was the focus of the current investigation. Moreover, previous work in cultural neuroscience has implicated the amygdala in social judgments from faces. Chiao et al. (2008b) found that American and Japanese perceivers showed significant amygdala responses compared to baseline when evaluating emotional fear faces from both cultures. Critically, however, the participants showed greater amygdala responses to same-culture versus other-culture faces that were expressing fear but statistically equivalent responses to both same-culture and other-culture faces that were angry, happy, or neutral. These data show evidence of cultural tuning, whereby the evolutionary importance of accurately detecting fear leads to a greater response to these expressions among similar versus dissimilar others, perhaps because threats to the ingroup may be more relevant to threats to the self than to the outgroup. These findings differ slightly, however, from previous intergroup work which has reported that the amygdala often responds more strongly to outgroup versus ingroup members. One early study showed that both Black and White Americans exhibited a greater bilateral amygdala response to faces of the other race than to faces of their own race (Hart et al., 2000). A more recent study extended this work by showing that the faces of learned outgroup members elicited a stronger amygdala response regardless of the targets’ races (Van Bavel et al., 2008). Interestingly, Chiao et al. (2008b) did not observe an overall outgroup effect for culture but showed an opposite ingroup effect (though only during perceptions of fear faces). Thus, rather than finding an increase in amygdala response, potentially because of a negative bias towards outgroup members (e.g. Phelps et al., 2001), Chiao et al. (2008b) found an increase related to a positive bias that may have evolved to help ingroup members. Given that these studies differ not only in terms of the type of group studied but also in the nature of their tasks, we were curious to see whether we might also observe a group-based difference in amygdala response to the Japanese and American faces during voting judgments. Several recent studies have examined neural responses to political stimuli, yet each has explored a different question and not all are relevant to the context of electoral choice (Mitchell et al., 2005; Westen et al., 2006; Kaplan et al., 2007; Amodio et al., 2008; Spezio et al., 2008). In one very relevant study, however, Knutson et al. (2006) observed greater activation in the amygdala in response to the faces of politicians with whom the participants held congruent political beliefs (e.g. Democrats responding to Democrat politicians’ faces) as opposed to those politicians for whom the participants’ political beliefs were incongruent (e.g. Democrats responding to Republican politicians’ faces). In essence, the amygdala responded more to the faces of individuals who the perceivers were likely to politically support versus those who they were likely to not support. Thus, we would expect to see a greater amygdala response to American participants’ choice of winners (consistent with Knutson et al., 2006). It is not clear whether the same pattern will hold for Japanese participants or for judgments of outgroup political candidates, however. Thus, to examine both (i) the processes involved in choosing candidates and (ii) the role of culture in voting behavior, the current work investigated the neural substrates relating to American and Japanese individuals’ electoral choices employing targets from both cultures. Participants from both cultures were shown a series of American and Japanese politicians’ faces and asked to indicate whether they would vote for each person while being scanned with fMRI. The study therefore had a 2 (participant culture) � 2 (target culture) � 2 (voted for/not voted for) design. Given previous work that has implicated the amygdala in impressions of others’ social traits from the face (e.g. Winston et al., 2002) and its role in processing faces of politicians (Knutson et al., 2006), we selected the left and right amygdalae as a priori regions of interest (ROIs) in voting decisions. METHODS Participants Thirty-two American (n ¼ 16; eight females) and Japanese (n ¼ 16; nine females) university students provided informed consent in a manner approved by the Massachusetts Institute of Technology’s Committee On the Use of Humans as Experimental Subjects and the Tufts University Institutional Review Board, and participated in exchange for monetary compensation. One American participant was Voting and amygdale across cultures SCAN (2010) 351 removed from analysis because of excessive movement and three participants (two Japanese, one American) were removed because they identified one or more same-culture targets. The final sample therefore consisted of 28 participants, half of whom were Japanese. All participants were screened for neurological history, MRI compatibility, handedness, and use of psychoactive medications. Participants were recruited via online message-board postings or paper advertisements. All of the Japanese participants were students at a Japanese university who were in the USA participating in a language studyabroad program at Tufts University. These participants were screened for their exposure to US culture, including any previous time spent visiting the USA. None had spent more than a month in the USA, including the time between their arrival for the foreign study program and the date of scanning. Stimuli Photos of 58 (29 election winners, 29 election losers) candidates from the 2004 and 2006 American Senate elections and photos of 58 (29 winners, 29 losers) candidates from the 2000 election of the Japanese Diet were downloaded from databases on internet websites; all targets were male. Each image was cropped to show only the candidate’s face, converted to grayscale, and standardized for size and image resolution. Pre-testing showed that the targets did not differ in affect or attractiveness according to culture [F ’s < 0.86, p’s > 0.52] or electoral success [F ’s < 1.11, p’s > 0.48]; nor were there interactive effects between culture and electoral success [F ’s < 2.16, p’s > 0.14]. Procedure Participants were instructed in their native language. Native Japanese research assistants translated and back-translated the instructions and other materials for the experiment. Participants were instructed that they would be seeing a series of faces for 2 s and that they should indicate via button-press whether they would or would not vote for each person (participants’ average number of vote decisions ¼ 50.1%, SD ¼ 17.6%; Cronbach’s � ¼ 0.79). Participants were informed that the face would remain on the screen for the entire 2 s regardless of when they made their response (thereby controlling for stimulus duration and looking time) but that they must make their judgment while each face was present, as responses made after the 2 s presentation had elapsed would not be recorded (2% of all trials). Participants were given several practice trials to ensure that they understood the task and to acquaint them with the timing of the stimulus presentation before the scan began. Response times occurred on an average of 1,076 (SD ¼ 267) milliseconds after stimulus onset and did not differ by participant culture, target culture, electoral success, or participant response: F ’s < 1.79, p’s > 0.19. The faces were presented in random order and the order of targets versus null trials of fixation (i.e. the presentation of a fixation cross) was pseudo-randomized into 3 orders using the Counter script designed for Matlab, which spaces each condition’s trials so as to optimize the efficiency of blood-oxygenation-level-dependent (BOLD) signal estimation (Dale and Buckner, 1997). Participants were presented with equal numbers of American faces, Japanese faces, and fixation trials. Null (fixation) trials were not modeled so as to serve as a baseline. fMRI acquisition Data were acquired using a Siemens Magnetom Tim Trio 3-T scanner located at the Athinoula A. Martinos Brain Imaging Center at the Massachusetts Institute of Technology in Cambridge, MA. Anatomical images were acquired using a T1-weighted protocol (256 � 256 matrix, 192 1.33-mm sagittal slices). Functional images consisted of 32 (5 mm thick with 1 mm slice gap) oblique-axial slices, parallel to the AC-PC line and acquired in interleaved order (voxel dimensions: 3.125 mm[2]). Single-shot gradient echo EPI imaging was used with a TR of 2 seconds, TE of 30 ms, and flip angle of 908. Data were collected in a single run consisting of 174 volumes (58 American faces, 58 Japanese faces, and 58 null fixation trials). Functional data were analyzed in an event-related design using BrainVoyagerQX. Preprocessing of the data consisted of 3D motion correction, slice scan time correction (sinc interpolation), spatial smoothing using a 3D Gaussian filter (7-mm FWHM), and voxelwise linear detrending and high-pass filtering of frequencies (above three cycles per time course). Structural and functional data for each participant were transformed to standard Talairach stereotaxic space (Talairach and Tournoux, 1988) and coregistered both mathematically and manually to assure precise matching between the functional and structural images. fMRI analysis Each participant’s BOLD signals were modeled in an event-related design using two separate design matrices, where individual predictors were modeled as boxcar functions convolved with a two-gamma hemodynamic response function. First-level general linear model (GLM) analyses conducted on individual participants’ fMRI signal were submitted to a second-level random effects analysis, treating participants as a random factor. The first design matrix contained the predictors based on participants’ actual subjective voting behavior: Japanesevoted, Japanese-not voted, American-voted and Americannot voted. The second design matrix contained predictors based on objective election outcomes of target faces: Japanese winner, Japanese loser, American winner, and American loser. Given our a priori hypothesis of amygdala involvement, we extracted parameter estimates (beta values) of BOLD 352 SCAN (2010) N.O.Rule et al. signal from anatomical ROIs of the left and right amygdalae, based on a hypothesis-blind research assistant’s drawings of each participant’s individual anatomy; tracing criteria were based on standard neuroanatomical definitions of the amygdala’s boundaries. Once extracted, these mean signal values were subjected to statistical analysis using SPSS. RESULTS A 2 (participant culture: American, Japanese) � 2 (target culture: American, Japanese) � 2 (outcome: voted, not voted) � 2 (amygdala: left, right) ANOVA (repeated measures on all but the first factor) was conducted on the beta values extracted from each participant’s anatomically defined bilateral amygdalae. This analysis showed a main effect of outcome and the direction of the means showed significantly more amygdala response for the targets for whom participants voted as compared to the targets for whom the participants did not vote: F(1, 26) ¼ 4.81, p ¼ 0.037, r ¼ 0.40; see Figure 1. In addition, the analysis showed a significant participant culture � target culture interaction: F(1, 26) ¼ 5.86, p ¼ 0.025, r ¼ 0.43; see Figure 2. Examination of the means indicated that individuals showed greater bilateral amygdala response to cultural outgroup faces than they did to cultural ingroup faces. No other effects were significant, all F’s < 1.63, all p’s > 0.21. We conducted a second ANOVA to compare targets’ actual electoral success, as opposed to participants’ choices about for whom to vote. The results of this 2 (participant culture: American, Japanese) � 2 (target culture: American, Japanese) � 2 (outcome: elected, not elected) � 2 (amygdala: left, right) ANOVA with repeated measures on all but the first factor showed no effects for outcome but repeated the participant culture � target culture interaction [F(1, 26) ¼ 4.72, p ¼ 0.039, r ¼ .39]. No other effects were significant, all F ’s < 2.74, all p’s > 0.11. Participants’ voting choices were uncorrelated with the outcomes of the actual elections, regardless of participant or target culture: all r’s < |0.24|, all p’s > 0.07. However, there was high crosscultural agreement between American and Japanese participants’ average voting decisions (r ¼ 0.58, p < 0.001) for both American (r ¼ 0.40, p < 0.001) and Japanese (r ¼ 0.79, p < 0.001) targets. We also examined these effects at the whole-brain level (voxelwise p < 0.001; minimum cluster size > five functional voxels) for all of the 2 (participant culture: American, Japanese) � 2 (target culture: American, Japanese) � 2 (outcome: voted, not voted; or outcome: elected, not elected) main effects and interactions. Only two contrasts revealed significant clusters of activation at this threshold. Comparisons of election winners and election losers showed clusters of significantly greater activation to losers over winners in the right (Talairach coordinates: 17, �52, �38; 1019 mm[3]) and left (Talairach coordinates: �8, �54, �38; 148 mm[3]) tonsils of the cerebellum and comparisons of Japanese over American targets revealed a significant cluster of activation in the right inferior parietal lobule (Talairach coordinates: 43, �38, 41; BA 40; 224 mm[3]) and right superior frontal gyrus (Talairach coordinates: 26, 23, 49; BA 8; 225 mm[3]). However, it should be acknowledged that signal loss due to imaging artifacts might have prevented the observation of additional effects, particularly in areas such as the orbitofrontal cortex. We also conducted an exploratory whole-brain analysis of our primary contrasts of interest (voted versus not-voted targets and the target � participant interaction) at a more liberal threshold: voxelwise p < 0.005, minimum cluster size > five functional voxels. These contrasts showed Fig. 1 Contrast of activation of targets for whom participants chose to vote over targets for whom participants chose not to vote in the left and right amygdala. Panel A shows activations in individual participants, representative of both cultural groups (displayed in neurological convention). Panel B shows mean extracted beta values from participant-specific ROIs of the left and right amgydala; error bars represent the standard errors of the means. Voting and amygdale across cultures SCAN (2010) 353 Fig. 2 Mean beta values showing the participant culture by target culture interaction, plotted separately for the left and right amygdalae. Participants showed a greater response to opposite-culture, outgroup targets. significant clusters of activation for voted over not-voted targets in the right (Talairach Coordinates: 22, �2, �23; 192 mm[3]) and left (Talairach Coordinates: �22, �6, �20; 275 mm[3]) amygdalae, as well as in the left superior temporal gyrus (Talairach Coordinates: �41, 2, �15; BA 38; 449 mm[3]) and right inferior frontal gyrus (Talairach Coordinates: �29, 10, �15; BA 47; 409 mm[3]). The target culture � participant culture interaction showed two clusters of significant activation: one in the left superior frontal gyrus (Talairach Coordinates: �19, 39, 36; BA 9; 175 mm[3]) and a second in the left culmen of the cerebellum (Talairach Coordinates: �9, �34, �17; 293 mm[3]). Mean extracted beta values from these clusters showed an outgroup effect, similar to what we observed in the amygdala ROI analyses above. Specifically, Japanese participants showed a greater response to outgroup, American targets and American participants showed a greater response to outgroup, Japanese targets in both the cerebellar culmen and the superior frontal gyrus. DISCUSSION The bilateral amygdala was significantly more responsive to the faces of politicians for whom participants chose to vote versus those for whom they chose not to vote. Importantly, this effect occurred independent of both the participants’ and the targets’ cultural group membership, showing evidence of cross-cultural universality. Yet we also found evidence for cultural specificity that was unrelated to voting choice in the form of a target culture by participant culture interaction in which participants showed a greater response to other-culture targets than they did to sameculture targets. Previous work has shown that individuals agree in their perceptions of traits from faces across cultures (Zebrowitz et al., 1993; Albright et al., 1997). The current data parallel those findings. American and Japanese participants’ behavioral responses showed significant cross-cultural agreement. Moreover, no main effect was observed between American and Japanese participants’ neural responses, neither in the amygdala ROI nor in the whole-brain analysis. Thus, participants from both cultures showed high agreement in their voting judgments and in their neural responses to the candidates from both cultures. Importantly, this does not mean that perceivers from both cultures were considering the same traits when making their judgments. Previous work has observed that American and Japanese voters value different traits in their leaders (e.g. Misumi and Peterson, 1985), the former favoring traits related to power and the latter favoring traits related to warmth (Rule et al., in press). One goal of the current study was to seek evidence for this dissociation. For instance, previous work has suggested that the amygdala may not encode specific social traits from faces but, instead, may be evaluating the overall valence of the face, with lower valence faces being associated with a greater amygdala response (Todorov and Engell, 2008). In such a case, we might have expected that Japanese perceivers would show a lower amygdala response for the candidates for whom they chose to vote and that American perceivers would show a greater amygdala response for the candidates for whom they chose 354 SCAN (2010) N.O.Rule et al. to vote. Rather, we observed that both groups of perceivers showed the latter effect. This could be because the amygdala simply serves as the perceptual or evaluative tool by which the perceivers form their impressions, not a reflection of the outcome of their electoral decision. Thus, although the perceivers might have been valuing different traits, this evaluation may be reflected in the amygdala response, rather than its outcome. Given that the amygdala is important for social evaluation (e.g. Winston et al., 2002; Schiller et al., 2009), it is not surprising that it was found to be involved in participants’ voting choices. Other studies have suggested that the amygdala is involved in processing the relevance of stimuli (see Sander et al., 2003 for a review). Similarly, the amygdala is known to respond more to stimuli that are more arousing (Anderson et al., 2003) and to social information that is subjectively valued (Schiller et al., 2009). Thus, the amygdala response observed in the current work might well reflect participants’ inclination to vote for candidates whom they found to be more salient or arousing. Indeed, related work examining amygdala response to politicians found that participants expressed a greater amygdala response to the faces of candidates who agreed with their political ideology over the faces of candidates who did not agree with their political ideology (Knutson et al., 2006). Those authors interpreted their effects as possibly reflecting a relevancebased response whereby salient stimuli provoked a greater response in the amygdala. Indeed, this may serve to explain the basis for individuals’ preferences for candidates based on perceptions of their faces in the current study, as well. Although participants from both cultures showed a similar amygdala response to the candidates for whom they chose to vote versus those for whom they chose not to vote, they showed a different overall response according to the target’s culture, independent of their electoral choice. The results of the participant culture by target culture interaction presented evidence for an outgroup effect such that participants had a stronger amygdala response to oppositeculture faces than they did to same-culture faces. This effect is consistent with a host of previous studies that have found increased amygdala activation to the faces of outgroup versus ingroup members. Hart et al. (2000), for example, found that Black and White individuals showed a stronger bilateral amygdala response to racial outgroup (White and Black, respectively) faces (see also Cunningham et al., 2004; Richeson et al., 2008). Similarly, Fischer et al. (2004) found that the left amygdala responded more when men viewed outgroup (female) faces than ingroup (male) faces. One explanation offered for these outgroup effects is that the amygdala is highly responsive to novel stimuli, such as the faces of outgroup members (Dubois et al., 1999; Phelps et al., 2001; Van Bavel et al., 2008). This may be particularly applicable to the current study, in which the outgroup faces differed not only in race but also in nationality and culture (Chiao and Ambady, 2007). The current findings may therefore contribute towards reconciling the previously observed difference between the intergroup amygdala differences found in some work and the intercultural amygdala differences reported by Chiao et al. (2008b). Specifically, although Chiao et al. (2008b) observed a greater amygdala response to ingrounp faces, this was specific to the context of fearful emotional expressions. The authors interpreted this effect as an evolutionary adaptation whereby signals of threat elicit stronger responses from similar (ingroup) individuals than from dissimilar (outgroup) individuals. The outgroup effect observed here during a voting task, then, may reflect a more general intergroup response that was not captured in Chiao et al.’s (2008b) study, which showed no intercultural differences for neutral, angry, or happy faces. Methodological differences might also contribute to these differences, as Chiao et al.’s (2008b) participants were scanned at two different locations, at different times, and in different countries; whereas the participants in the intergroup (e.g. Van Bavel et al., 2008), interracial (e.g. Hart et al., 2000), and current studies employed participants at one scanning site, in one country, at the same time. Thus, the current findings raise some new conceptual and methodological questions worth exploring with additional work. Another area worthy of future study is to examine or improve the ecological validity of the current voting task. In particular, in the present study (like many of the hypothetical voting studies conducted to date; e.g. Todorov et al., 2005) participants were unfamiliar with all of the targets and were therefore basing their impressions on appearances. This is typically not how voting decisions are believed to be made in actual elections and it is therefore uncertain how much these results may generalize to realworld voting behavior. Although it is nevertheless interesting that some of these judgments have been found to predict the actual outcomes of elections, future efforts may wish to extend this work using more realistic methodological contexts. In sum, the current study presents two novel findings of interest to cultural neuroscience. First, participants’ voting decisions about candidates from two different cultures were reflected in an amygdala response to preferred political candidates’ faces. These findings suggest that the amygdala may be important in evaluating candidates for political office. Moreover, these effects were independent of either the perceiver’s culture or the candidate’s culture, suggesting that the neural basis for electoral decisions may extend across cultures. Second, participants showed a greater overall amygdala response to the faces of outgroup candidates than they did to the faces of ingroup candidates. This extends the work that has been previously done examining amygdala response across cultures and helps to reconcile what is known about the neural response to group members in intercultural contexts with what is known about the neural response to group members in intracultural contexts. Voting and amygdale across cultures SCAN (2010) 355 REFERENCES Adolphs, R. (2003). Cognitive neuroscience of human social behaviour. Nature Reviews Neuroscience, 4, 165–78. Adolphs, R., Baron-Cohen, S., Tranel, D. (2002). Impaired recognition of social emotions following amygdala damage. Journal of Cognitive Neuroscience, 14, 1264–74. Albright, L., Malloy, T.E., Dong, Q., et al. (1997). Cross-cultural consensus in personality judgments. Journal of Personality and Social Psychology, 72, 558–69. Amodio, D.M., Jost, J.T., Master, S.L., Yee, C.M. (2008). Neurocognitive correlates of liberalism and conservatism. Nature Neuroscience, 10, 1246–7. Anderson, A.K., Christoff, K., Stapin, I., et al. (2003). Dissociated neural representations of intensity and valence in human olfaction. Nature Neuroscience, 6, 196–202. Antonakis, J., Dalgas, O. (2009). Predicting elections: Child’s play! Science, 323, 1183. Ayman, R. (1993). Leadership perception: The role of gender and culture. In: Chemers, R.M., Ayman, R., editors. Leadership Theory and Research. San Diego, CA: Academic Press, pp. 137–165. Ballew, C.C.II, Todorov, A. (2007). Predicting political elections from rapid and unreflective face judgments. Proceedings of the National Academy of Sciences, 104, 17948–53. Chiao, J.Y., Ambady, N. (2007). Cultural neuroscience: parsing universality and diversity across levels of analysis. In: Kitayama, S., Cohen, D., editors. Handbook of Cultural Psychology. NY: Guilford Press, pp. 237–54. Chiao, J.Y., Bowman, N.E., Gill, H. (2008a). The political gender gap: gender bias in facial inferences that predict voting behavior. PLoS ONE, 3, 3666. Chiao, J.Y., Iidaka, T., Gordon, H.L., et al. (2008b). Cultural specificity in amygdala response to fear faces. Journal of Cognitive Neuroscience, 20, 2167–74. Cunnigham, W.A., Raye, C.L., Johnson, M.K. (2004). Implicit and explicit evaluation: fMRI correlates of valence, emotional intensity, and control in the processing of attitudes. Journal of Cognitive Neuroscience, 16, 1717–29. Cunningham, W.A., Van Bavel, J.J., Johnsen, I.R. (2008). Affective flexibility: evaluative processing goals shape amygdala activity. Psychological Science, 19, 152–60. Dale, A.M., Buckner, R.L. (1997). Selective averaging of rapidly presented individual trials using fMRI. Human Brain Mapping, 5, 329–40. Dubois, S., Rossion, B., Schiltz, C., et al. (1999). Effect of familiarity on the processing of human faces. NeuroImage, 9, 278–89. Fischer, H., Sandblom, J., Herlitz, A., Fransson, P., Wright, C., Backman, L. (2004). Sex-differential brain activation during exposure to female and male faces. NeuroReport, 15, 235–8. Hart, A.J., Whalen, P.J., Shin, L.M., McInerney, S.C., Fischer, H., Rauch, S.L. (2000). Differential response in the human amygdala to racial outgroup vs ingroup face stimuli. NeuroReport, 11, 2351–5. Kaplan, J.T., Freedman, J., Iacoboni, M. (2007). Us versus them: political attitudes and party affiliation influence neural response to faces of presidential candidates. Neuropsychologia, 45, 55–64. Kaplan, S.N. (1994). Top executive rewards and firm performance: a comparison of Japan and the United States. Journal of Political Economy, 102, 510–46. Knutson, K.M., Wood, J.N., Spampinato, M.V., Grafman, J. (2006). Politics on the brain: an fMRI investigation. Social Neuroscience, 1, 25–40. Martin, D.S. (1978). Person perception and real-life electoral behaviour. Australian Journal of Psychology, 30, 255–62. Misumi, J., Peterson, M.F. (1985). The performance-maintenance (PM) theory of leadership: review of a Japanese research program. Administrative Science Quarterly, 30, 198–223. Mitchell, J.P., Macrae, C.N., Banaji, M. (2006). Dissociable medial prefrontal contributions to judgments of similar and dissimilar others. Neuron, 50, 655–63. Phelps, E.A. (2006). Emotion and cognition: insights from studies of the human amygdala. Annual Review Psychology, 57, 27–53. Phelps, E.A., LeDoux, J.E. (2005). Contributions of the amygdala to emotion processing: from animal models to human behavior. Neuron, 48, 175–87. Phelps, E.A., O’Conner, K.J., Cunningham, W.A., et al. (2000). Performance on indirect measures of race evaluation predicts amygdala activation. Journal of Cognitive Neuroscience, 12, 729–38. Poutvaara, P., Jordahl, H., Berggren, N. (2009). Faces of politicians: babyfacedness predicts inferred competence but not electoral success. Journal of Experimental Social Psychology, 45, 1132–5. Richeson, J.A., Todd, A.R., Trawalter, S., Baird, A.A. (2008). Eye-gaze direction modulates race-related amygdala activity. Group Processes & Intergroup Relations, 11, 233–46. Rule, N.O., Ambady, N., Adams, R.B. Jr, et al. (in press). Polling the face: prediction and consensus across cultures. Journal of Personality and Social Psychology, DOI: 10.1037/a0017673. Sander, D., Grafman, J., Zalla, T. (2003). The human amygdala: an evolved system for relevance detection. Reviews in the Neurosciences, 14, 303–16. Schiller, D., Freeman, J.B., Mitchell, J.P., Uleman, J.S., Phelps, E.A. (2009). A neural mechanism of first impressions. Nature Neuroscience, 12, 508–14. Spezio, M.L., Rangel, A., Alvarez, R.M., et al. (2008). A neural basis for the effect of candidate appearance on election outcomes. Social Cognitive and Affective Neuroscience, 3, 344–52. Talairach, J., Tournoux, P. (1988). Co-planar stereotaxic atlas of the human brain. New York: Thieme Medical Publishers, Inc. Todorov, A., Engell, A.D. (2008). The role of the amygdala in implicit evaluation of emotionally neutral faces. Social Cognitive and Affective Neuroscience, 3, 303–312. Todorov, A., Mandisodza, A N., Goren, A., Hall, C.C. (2005). Inferences of competence from faces predict election outcomes. Science, 308, 1623–6. Van Bavel, J.J., Packer, D.J., Cunningham, W.A. (2008). The neural substrates of in-group bias: a functional magnetic resonance imaging investigation. Psychological Science, 19, 1131–9. Westen, D., Blagov, P.S., Harenski, K., Kilts, C., Hamann, S. (2006). Neural bases of motivated reasoning: an fMRI study of emotional constraints on partisan political judgment in the 2004 U.S. presidential election. Journal of Cognitive Neuroscience, 18, 1947–58. Winston, J.S., O’Doherty, J., Kilner, J.M., Perrett, D.I., Dolan, R.J. (2007). Brain systems for assessing facial attractiveness. Neuropsychologia, 45, 195–206. Winston, J.S., Strange, B.A., O’Doherty, J., Dolan, R.J. (2002). Automatic and intentional responses during evaluation of trustworthiness of faces. Nature Neuroscience, 5, 277–83. Yamagishi, T., Cook, K.S., Watabe, M. (1998). Uncertainty, trust, and commitment formation in the United States and Japan. American Journal of Sociology, 104, 165–94. Zebrowitz, L.A., Montepare, J.M, Lee, H.K. (1993). They don’t all look alike: individuated impressions of other racial groups. Journal of Personality and Social Psychology, 65, 85–101. On Scientific Transparency, Researcher Degrees Of Freedom, And That NEJM Study On Youth Gender Medicine (Updated) Author: Singal, Jesse, Date: 2023-01-31 Collections: IdentityPolitics Zotero Key: C9Z2DWUB Cite Key: Singal23TranspDOFyouthGendMed Zotero Item | Lit Note (Update: Part 2 is published here.) There’s a concept called “researcher degrees of freedom” that is quite important to understanding forms of shoddy science that are less salacious but (far) more common than, say, outright data fraud. As Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn wrote in 2011: In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both? It is rare, and sometimes impractical, for researchers to make all these decisions beforehand. Rather, it is common (and accepted practice) for researchers to explore various analytic alternatives, to search for a combination that yields “statistical significance,” and to then report only what “worked.” The problem, of course, is that the likelihood of at least one (of many) analyses producing a falsely positive finding at the 5% level [of confidence that is a common benchmark for statistical significance in the social sciences] is necessarily greater than 5%. As I put it in my book: “If I sell you a pill on the basis of data showing it reduces blood pressure relative to a control group given a placebo, but fail to tell you that I also tested its efficacy in improving twenty-five other health outcomes and came up empty on all of them, that’s a very weak finding. Statistically, if you have enough data and run enough tests, you can always find something that is, by the standards of the statistical tests psychologists use, ‘significant.’ ” (Update, 6/20/2023: Back in April, I received an email from an astute observer who pointed out that the example I used here isn’t exactly right in light of what follows. I told him I’d look into it within a couple days and then promptly forgot, and here we are, in June. I’ll simply put the bulk of our email exchange in a footnote so that interested readers can see. My correspondent himself describes the difference between what I’m describing in my book and what I lay out in this post as “an incredibly subtle distinction.” After reading his emails and Part 2 of my coverage of this study, you’ll see that nowhere do I quibble with the statistical significance of the variables that did show improvement, so I think this is a fairly technical nitpick, but I don’t want to sweep it under the rug.) Give a gift subscription Or, phrased differently, “Torture data long enough and it will confess to anything.” Then, once you get the confession you want, you can HARK, or hypothesize after the results are known — Ah, yes, this is what we expected to find all along. I knew the pill would help fight high blood pressure. In a situation like this, though, there’s a serious risk that what you’re looking at is not a pill that lowers high blood pressure, but statistical noise. The good news is that there’s a growing awareness among researchers in psychology and other fields affected by replication crises of how these practices can generate weak and non-replicable results.Science reformers have begun constructing guardrails that effectively constrict researchers and reduce their degrees of freedom: For instance, you can incentivize or require researchers to “preregister” their hypotheses and lay out exactly which statistical tests they plan to run, so that if they change their analysis plan or hypothesis midstream, this will be visible for all the world to see. You can also incentivize or require them to share their data, which makes it easier for other researchers to check and see whether they engaged in statistical chicanery. I think there’s a reasonably strong case to be made that the positive results reported in “Psychosocial Functioning in Transgender Youth after 2 Years of Hormones,” a highly anticipated research paper just published in The New England Journal of Medicine, can be at least partially explained by the sort of statistical cherry-picking that tends to generate wobbly findings. The NEJM paper is part of the Trans Youth Care–United States (TYCUS) Study, which the researchers describe as “a prospective, observational study evaluating the physical and psychosocial outcomes of medical treatment for gender dysphoria in two distinct cohorts of transgender and nonbinary youth,” one receiving puberty blockers (the team hasn’t reported on those results yet), and another — this one — receiving hormones. The TYCUS study is taking place atfour major youth gender clinics: The Center for Transyouth Health and Development at Children’s Hospital Los Angeles; the Gender Multispecialty Service at Boston Children’s Hospital; the Child & Adolescent Gender Center Clinic at Benioff Children’s Hospital in San Francisco; and the Gender Identity & Sex Development Program at Lurie Children’s Hospital of Chicago. The researchers listed as coauthors on this study include some of the bigger names in youth gender medicine and psychology: Diane Chen, Johnny Berona, Yee-Ming Chan, Diane Ehrensaft, Robert Garofalo, Marco A. Hidalgo, Stephen M. Rosenthal, Amy C. Tishelman, and Johanna Olson-Kennedy. A number of them have been outspoken advocates for youth gender medicine treatments. This team has received significant sums of grant money to study this population, and for good reason: American gender clinics are very behind in producing useful data that can help us better understand whether and under what circumstances youth gender medicine benefits kids with gender dysphoria. As the authors write in their study protocol, their goal is to “collect critical data on the existing models of care for transgender youth that have been commonly used in clinical settings for close to a decade, although with very limited empirical research to support them.” They wrote that in 2016, but the situation hasn’t really changed: There is almost no good — or even decent — data on these vital questions. On the same page of the protocol they write: “This research is highly significant in scope as it is the first longitudinal study collecting data — assessing both physiologic and mental health outcomes — to evaluate commonly used clinical guidelines for transgender youth in the U.S.” The researchers’ NEJM study marks the first time they have published data tracking, over time, the mental health of the youth cohort that went on hormones. And the news seems to be good: The team reports that two years following the administration of hormones, the trans kids in their study experienced increases in their appearance congruence, or the sense that their external appearance matches their gender identity, and positive affect. Trans boys (natal females) also experienced reductions in depression and anxiety, and increases in life satisfaction, though trans girls (natal males) didn’t. On the basis of these findings, the study is being widely touted, both by most mainstream media outlets that have covered it and by the authors themselves, as solid evidence that hormones improve the well-being of trans youth. “Our results provide a strong scientific basis that gender-affirming care is crucial for the psychological well-being of our patients,” said Garofalo,one of the principal investigator for the study, as well as co-director of the youth gender clinic at Lurie Children’s Hospital, in a press release published out by the hospital. “The critical results we report demonstrate the positive psychological impact of gender-affirming hormones for treatment of youth with gender dysphoria,” added Olson-Kennedy, Medical Director of the Children's Hospital Los Angeles clinic. I disagree with these assessments, but I’ll take that up in a separate Part 2 of my critique of this study, which will focus on the results the researchers reported. This post is mostly about the results theydidn’t report, which is an important story in its own right. My case that something questionable is going on here is simple, and rests largely on the study protocol the authors wrote as part of their Institutional Review Board (IRB) approval process. This document is listed and linked to right there in the supplementary section of their article’s NEJM page: You can read or download the protocol, the paper, and a supplementary appendix we’ll get to later here if you want. The protocol is a long, rich document, and among other information it lays out the study procedures for both the hormones and the blockers cohorts. There’s a note in the supplementary section of the document explaining that it includes both the “original” (2016) and “final” versions of the protocol. I’ll quote from the final and therefore more applicable version, which was submitted May 11, 2021, though when it comes to what I’m about to discuss, there aren’t substantive differences between the two versions (with one exception I’ll get to). The protocol document acts as a de facto preregistration for Chen and her team (they also published a shorter version of it as a Registered Report, a form of more official preregistration), and it shows that in the NEJM study, the researchers simply excluded most of the key variables they had hypothesized would improve as a result of hormones, and that they changed their hypothesis significantly, in a manner that shunts some of those variables off to the side. Let’s get specific about what the authors did and why it raises questions. They list a number of hypotheses in their protocol document. One of them fits the current study: “Hypothesis 2a: Patients treated with gender-affirming hormones will exhibit decreased symptoms of anxiety and depression, gender dysphoria, self-injury, trauma symptoms, and suicidality and increase [sic] body esteem and quality of life over time.” The first subsection of their “Statistical Analysis” section is headed “Primary Objective: Effects of Hormonal Interventions on Mental Health and Psychological Well-Being,” and there the authors explain that their analysis “will investigate the changes over time in gender dysphoria, depression, anxiety, trauma symptoms, self-injury, suicidality, body esteem, and quality of life.” So it’s pretty clear, between the hypothesis and this primary objective bit, what they were most interested in investigating. In Appendix II of the protocol document the researchers include a helpful chart of the instruments they plan on using to track these and other variables. It’s a bit out of date, though, as it still uses “cross-sex hormones” rather than “gender-affirming hormones” and includes a couple of variables that the researchers subsequently dropped. A more up-to-date version is downloadable here, and here’s the part of it listing all the variables for which data was collected at every six-monthwave of the research effort (that’s the data the researchers would be most likely to report on in a big study): Okay, now let’s jump to the “Measures” section of the present study in NEJM, where the researchers list their variables: “With respect to longitudinal outcomes, participants completed the Transgender Congruence Scale, the Beck Depression Inventory–II, the Revised Children’s Manifest Anxiety Scale (Second Edition), and the Positive Affect and Life Satisfaction measures from the NIH (National Institutes of Health) Toolbox Emotion Battery at each study visit.” If you compare that to the protocol document, you’ll notice that of the eight key variables the researchers were most interested in — “gender dysphoria, depression, anxiety, trauma symptoms, self-injury, suicidality, body esteem, and quality of life” — the ones I bolded are not reported in the NEJM paper. That’s six out of eight, or 75% of the variables covered by the researchers’ hypothesis in their protocol document (including the “officially” preregistered shorter version). In fact, most of these variables aren’t mentioned at all in the NEJM paper or its supplementary appendix. “Gender dysphoria” comes up early, because how could it not in a paper about, well, gender dysphoria, but there’s no mention at all of any gender dysphoria scale (this is the one missing variable they have a partial but non-exculpatory explanation for, which I’ll get to). Neither the phrase “quality of life” nor any mention of the Health-Related Quality of Life Scale appear in the paper. The authors do report on the number of completed suicides and instances of suicidal ideation in the sample (more on that in Part 2), but there’s zero mention of the suicidality scale — the one they reported on in the baseline characteristics paper— that would allow them to statistically analyze the sample’s level of suicidality over time the way they analyze other variables’ longitudinal trajectories. (They definitely have data on body esteem and suicidality, at least, because they report the baseline numbers for these variables in a 2021 paper.) I checked the NEJM article’s supplementary appendix, too, to see if there’s any explanation for the disappearances there. I came across a very promising short section subheadlined “Rationale for Selecting Primary Mental Health Outcome Measures,” but alas, it concerns a relatively minor and unrelated issue — it doesn’t actually explain where their key variables went. Those variables go unmentioned in the rest of this appendix as well. (If the authors wanted to explain the absence of certain variables without taking up potentially limited space in the paper itself, this would have been a good place to do so.) The researchers’ hypothesis changes in the NEJM study, too. To be fair, when they reference their hypothesis here it’s in a less formal and more colloquial way — there’s no official Hypotheses section like there is in the protocol document. But still, look at this shift: Hypothesis from the latest version of protocol, published in 2021: “Patients treated with gender-affirming hormones will exhibit decreased symptoms of anxiety and depression, gender dysphoria, self-injury, trauma symptoms, and suicidality and increase [sic] body esteem and quality of life over time.” Hypothesis from the NEJM study, published in 2023: “We hypothesized that [after kids were administered hormones], appearance congruence, positive affect, and life satisfaction would increase and that depression and anxiety symptoms would decrease. We also hypothesized that improvements would be secondary to treatment for gender dysphoria, such that increasing appearance congruence would be associated with concurrent improvements in psychosocial outcomes.” There’s some similarity, in that depression and anxiety are mentioned in both instances, but the changes are rather striking. Out are a number of the variables they originally hypothesized would be most important, including several — GD, suicidality, and self-harm — universally recognized by youth gender researchers as vitally important. In are some other variables, like “appearance congruence, positive affect, and life satisfaction,” that, yes, were included in the original protocol document, but which weren’t treated as particularly important, failing to garner mentions in the hypothesis or primary objective sections. (And no, quality of life and life satisfaction are not the same construct — they’re listed under two different variables in the study protocol, and there’s at least one study out there attempting to evaluate the strength of the correlation between the two.) In the NEJM paper, the researchers appear to be much more interested in the concept of “appearance congruence” than they were previously. While the vital terms suicide (and its variants) and dysphoriaare mentioned eight and nine times, respectively, in the paper… …“appearance congruence” gets mentioned 52 times. It even comes up in the very first paragraph: “An important goal of such treatment is to attenuate gender dysphoria by increasing appearance congruence — that is, the degree to which youth experience alignment between their gender and their physical appearance.” I mean, maybe? But again, the shift in emphasis is noteworthy. The phrase appearance congruence doesn’t get mentioned once in the protocol document, and the only time the word appearance turns up in this context, it’s in this description of the Transgender Congruence Scale (TCS), one of the variables the researchers collected data on: “A construct of congruence to conceptualize the degree to which transgender individuals feel genuine, authentic, and comfortable with their gender identity and external appearance.” Even here, there’s apparent cherry-picking. In both the protocol document and the NEJM paper, the authors mention administering the TCS. But they don’t report the full results anywhere in the NEJM paper — instead, they report on only one of the scale’s two subscales, Appearance Congruence (again, we know they have the full data because they provided some of it in their baseline measures paper). This means the researchers had three bites at the apple: They could analyze the changes over time on the full scale, and then each of the two subscales. They report on only one of these three results, andthisnets them what they describe in the paper as their strongest finding: Over the course of their two years on hormones, the average kid in the study improved about one point on this five-point subscale. The researchers then build a significant chunk of their paper around this finding, going so far as to say they hypothesized that appearance congruence would be important — which, to me, reads as though they hypothesized it all along, when I’m not seeing any evidence they did. Rather, they hypothesized something pretty different in their protocol document, and then they changed that hypothesis without explaining why. (I also think this finding about appearance congruence is far less impressive than the researchers are making it out to be, but I’ll leave that for Part 2.) There’s something a bit similar going on in the researchers’ approach to the NIH Toolbox, “a multidimensional set of brief measures assessing cognitive, emotional, motor and sensory function from ages 3 to 85.” It’s basically a buffet of different survey items (more info and scoring scales here) — there are a lot of them. If I pull that big table back up and indicate the different items the researchers included from this battery… …you’ll see that the researchers had participants in the study fill out NIH Toolbox items on Emotional Support, Friendship, Loneliness, Perceived Hostility, Perceived Rejection, Anger, Fear, Sadness, General Life Satisfaction, Positive Affect, and Self-Efficacy. None of these items were emphasized by the researchers in their original hypothesis or primary objective section, so we probably shouldn’t have any prior belief about which ones we should expect them to report in a major study, but still: Nine of the 11 are nowhere to be found, leaving us with only Positive Affect and Life Satisfaction measures. Why? And why was it more important to report on “Life Satisfaction,” not listed in the hypothesis or primary outcomes sections, than it was to report on “Quality of Life,” which was? For that matter, why report on the Positive Affect but not the Negative Affect items? If hormones help kids feel better, shouldn’t they experience less anger, less fear, and less sadness over time? We have a lot of useful information about the researchers’ protocol thanks to what they’ve posted online. They definitely needed to submit their protocol document for IRB approval and have it on file somewhere, but I’m a bit of an ignoramus about all this bureaucratic stuff, so I don’t know if they were obligated to post it publicly as a grant requirement or an NEJM requirement, or if they did so outof a spirit of openness. Either way, the existence of the protocol document nicely demonstrates why this sort of scientific transparency is helpful: In this case, it allows us to go beyond the officially published results, to compare those results to the broader process that produced them, and to ask questions. What’s less helpful is the lack of any sort of information about why the authors made the choices they made for the NEJM paper. It should go without saying that I’m not arguing that if hormones have a beneficial effect on trans youth, every variable in this study should exhibit a large and salutary change. My point is that because there’s no explanation, we canonly speculate as to how the researchers made all these subtle decisions — decisions that allowed them to write, in their final published article, that “there were significant within-participant changes over time for all psychosocial outcomes in hypothesized directions.” This is borderline misleading. By “all” psychosocial outcomes, they don’t mean all the ones they measured and evaluated for change over time — they mean the ones they chose to report results on. Which might render their findings totally trivial. To take a more extreme example, I can’t flip a coin over and over and over, until I finally get 10 heads in a row hours later, and then write “The presence of 10 heads in a row suggests the coin is not fair.” So why did so many of the variables disappear? There are a few possibilities. One is that the researchers plan on reporting on these outcomes in an upcoming study, though I’m not sure why they would hold back from the NEJM, and this still wouldn’t explain how they chose which variables to report. Another possibility is that the journal itself asked the researchers to focus on specific variables. The path to getting a paper published in a journal as prestigious as NEJM can be a bruising one, and you might submit a first draft that you’re very excited about, because of what it shows about A and B and C, only to have the peer reviewers butcher your beautiful baby until, many months or even years (not to mention gray hairs) later, you resignedly sigh and agree to publish a paper with much less exciting, severely hedged findings pertaining to the far less sexy variables X, Y, and Z. I guess that could have happened here, but it would just kick that can of “Why did you go with these particular variables?” over to the NEJM, meaning the serious methodological questions would remain unanswered. Also, as we’ll see in Part 2, the NEJM didn’t exactly crack the whip when it came to this paper’s methodological tightness, so I’m not sure how much I buy this hypothetical version of events. Overall, if I had to guess, I think the most likely explanation here is that the researchers did a lot of “exploratory analysis” until they found reasonably impressive-seeming results, and then chose to refocus their efforts — and to rejigger their hypothesis — around those results, tossing some disappointing results in a file drawer. If I’m right, this wasn’t necessarily an intentional process on their part. When you have a lot of people poking around in a lot of data without certain guardrails in place, it’s easy to lose track of all those unsuccessful statistical tests you ran while remembering the positive results that do support your preferred hypothesis. But regardless of whether I’m correct and whether any potential cherrypicking here was intentional, the researchers should have at least realized what was missing from their NEJM paper and explained what happened somewhere. But all I can do is speculate, because they won’t answer any questions about their process, or about the possibility of sharing their data so others can investigate these issues. I sent specific questions to the NEJM, to press contacts at two of the universities, and to four members of the team (Chen, Hidalgo, Rosenthal, and Tishelman), and other than a response from NEJM saying I should reach out to the researchers' institutions directly, I only heard back from a Lurie Children’s Hospital press person confirming the researchers weren't doing any interviews. To be fair, the no-interviews stance appears to be consistent, regardless of the journalist who asks. My last email asked that press person if the team could share their data — she said she'd check but I haven't heard back. (I’m actually not sure how data-sharing works here — their protocol document notes that the team will eventually share it with other National Institutes of Health researchers, but it could be that they face restrictions when it comes to random journalists or independent academics. So an unwillingness to share data isn’t necessarily grounds for suspicion in this case.) This refusal to talk to journalists is an unfortunate decision on the researchers’ part, especially when paired with their glowing quotes about the importance of their findings — quotes that obscure a lot of nuance and missing results. At the end of the day, this team publicly predicted that eight variables would move in a particular direction. Then, when it was time to report their data, they only told us what happened to two of those variables, and the two they did report weren’t even direct hits, given that trans girls didn’t experience reductions in depression and anxiety. If these findings are so impressive, where are all those other variables?** **** I believe the seminal article on HARKing, or hypothesizing after the results are known, is this 1998 one published by Norbert L. Kerr in Personality and Social Psychology Review. At the time, this wasn’t a well-known phenomenon, and some people actually argued for it! The idea was that if in poking around your data you uncovered a new explanation, why not update your hypothesis to account for it? Many otherwise talented and good-hearted researchers didn’t really understand the statistical and other downsides to this back then, so Kerr had to actually, affirmatively argue that the cons of HARKing outweigh the pros. In that sense it’s a strange read by present standards — these days most researchers understand why these practices lead to shaky science**.** Kerr writes: Probably everyone would agree that, all other things being equal, “good” (i.e., clear, coherent, engaging, exciting) scientific writing is better than “bad” (i.e., incoherent, unclear, turgid, unengaging) scientific writing. But everyone would also probably acknowledge that an author of a scientific report has constraints on what he or she can write under the guise of telling a good story. Scientific reports are not fiction, and a scientist operates under different constraints than the fiction writer. No matter how much the addition might improve the story, the scientist cannot fabricate or distort empirical results. The ultimate question is whether any such constraints should apply to the fictional aspects of HARKing (e.g., inaccurately representing certain hypotheses as those hypotheses that guided the design of the study). Did the NEJM article’s authors “inaccurately represent[] certain hypotheses as those hypotheses that guided the design of the study”? Maybe this is too strong a claim, but I’m not sure. The researchers are crystal clear about the variables they are most interested in in the protocol document that supposedly underpins this study — they hypothesize that “Patients treated with cross-sex hormones will exhibit decreased symptoms of anxiety and depression, gender dysphoria, self-injury, trauma symptoms, and suicidality and increase [sic] body esteem and quality of life over time.” Then, in the study that is one of the main reasons they were collecting all this data in the first place — a study that includes the line “The authors vouch for the accuracy and completeness of the data and for the fidelity of the study to the protocol” — their hypothesis is substantially different, and they present their interest in appearance congruence as a hypothesis they had all along, when there’s no evidence that was the case. This change, and the disappearance of all these variables, go almost entirely unexplained. As I alluded to earlier, the authors do offer a partial explanation for the absence of gender dysphoria variables. Originally, they collected GD data using two instruments, the Utrecht Gender Dysphoria Scale (UGDS) and the Gender Identity/Gender Dysphoria Questionnaire for Adolescents and Adults (GIDYQ-AA). In a 2019 article in Transgender Health, they write about some of the ostensible flaws with these measures and explain that they stopped collecting data on them: The necessity for an improved measure to capture the nuanced elements of gender dysphoria and its potential for intensification or mitigation over time has been highlighted by our transgender team members who have been on the front line with youth participating in the study. After significant deliberation, we chose to include the UGDS in this study, in the hopes that we might demonstrate its limitations in capturing the dynamic experience of youth with gender dysphoria. For similar reasons, we also included The Gender Identity/Gender Dysphoria Questionnaire for Adolescents and Adults (GIGDQAA) [GIDYQ-AA]. Over the course of the past 2 years, there were concerns articulated by participants regarding distress they experience when confronted with some of the items from both of these instruments. A decision was made to remove the UGDS and GIGDQAA [GIDYQ-AA] from the participant assessments, except for those participants completing a substudy to obtain participant feedback regarding items included in those two scales. Our team felt that the Transgender Congruence Scale and the Gender Minority Stress and Resilience Scale are likely the best existing measures to collect information about both distal and proximal drivers of gender dysphoria. [footnotes omitted] I found this a little strange — they used these two GD scales not to measure GD, as their protocol document explains, but because they thought they were bad and wanted to demonstrate that? Setting that aside, this does check out, as the protocol document includes a 2019 “Letter of Amendment” removing those two instruments. (They’re still administered at the 24-month visit.) But if the Transgender Congruence Scale and the Gender Minority Stress and Resilience Scale are, in fact, “likely the best existing measures” to evaluate gender dysphoria, why are they both missing from the NEJM article, other than that one TCS subscale? And according to the protocol, the kids in this cohort were also asked about their DSM-5 gender dysphoria symptoms until a separate 2021 Letter of Amendment halted that question. But that data, too, is absent from the NEJM paper. Why? So, long story short, whether the abandonment of the UGDS and GIDYQ-AA was justified, the researchers did collect data on three other measures that they believe serve a similar purpose, but then didn’t publish it. It’s disappointing that this study offers no data on GD, given that alleviating GD is the ostensible medical justification for putting kids on blockers and/or hormones in the first place despite the lack of solid published evidence about these treatments. It would be interesting to know whether any peer reviewers or NEJM editors asked the authors why their paper lacked longitudinal data on gender dysphoria, suicidality, and self-harm given the importance of these variables, and given the research team’s demonstrated prior interest in monitoring these outcomes. Nothing I’m saying about researcher degrees of freedom or HARKing is new or controversial. Again, researchers have known for years that you really can’t do this — if you don’t account for the fact that you made a bunch of other statistical comparisons you didn’t report on, it might call your entire analysis into question, because results that appear to be statistically significant can be pushed under that threshold once you make the appropriate corrections. Here’s a handy table from the supplementary appendix running down the average scores on the five variables the researchers reported at baseline and 24 months (the final wave of data collection for this study): Setting aside various issues that I will get to in Part 2, such as the fact that the researchers are touting a two-year increase of 0.82 points on a 100-point Positive Affect scale as evidence that hormones work, there’s so much missing from this. Really — if you’ll excuse my truly dismal MS Paint skills — the chart should look something like this: And that’s just the variables the researchers mentioned in their hypothesis; they saw fit to pick and choose from all those other variables, too, so really we could make a much, much longer version of this chart with many more question marks. If you fill in those question marks, are the researchers still able to hail their study as an impressive finding? The whole reason you’re supposed to communicate in a clear, transparent manner about your methodological choices is to prevent critics from being able to ask such questions in the first place. The fact that all of this is up in the air should be seen as a shortcoming on the part of the authors, the journal, or both. Or, to put it another way: Let’s do a visualization of the outcomes they did report, out of all the outcomes they could have reported: The point is, however you phrase it or visualize it, the researchers explain so little about their process, about the path from their original study protocol to this finished product — an article in one of the top research journals in the world — that until they fill in some of these gaps, I can’t help but be skeptical. And I think you should be too. Questions? Comments? Evaluations of this newsletter that suffer from myriad methodological shortcomings? I’m at singalminded@gmail.com or on Twitter at @jessesingal. *correction: In this paragraph, I originally said that the researchers revealed the results of six of their variables. The correct figure here is two, so I fixed it. Thanks to the attentive commenters who noticed this error.* These 14 Voters Think Trump Has One Mandate Above All, and It’s Not About the Economy Author: Healy, Patrick, Date: 2024-12-10 Collections: Hot Takes US Elect 2024 Zotero Key: KAGG642Q Cite Key: Healy24focusTrumpMandateNotEcon Zotero Item | Lit Note These 14 Voters Think Trump Has One Mandate Above All, and It’s Not About the Economy These 14 Voters Think Trump Has One Mandate Above All, and It’s Not About the Economy After the November election, we were curious about a particular slice of American voters: those who described Donald Trump as “extreme” and differed with him on some key issues, including abortion rights, and decided to vote for him anyway. “Extreme” used to be an ugly word in politics; was it anymore? Abortion used to be a voting issue for Americans who favored reproductive rights and ones who opposed abortion; had that changed? In our latest Times Opinion focus group, we explored these questions with 14 Trump voters — mostly Republicans and independents who shared these views — and also asked them to look forward: What do they want Mr. Trump to do in office? What do they want him to avoid? What kind of mandate does he have in the eyes of these supporters, who aren’t MAGA die-hards? Many of these 14 voters saw his victory as a sign that America is “a little more unified than we thought we were,” in the words of Jason, 38 ld i d d t f Fl id A t i fl ti d the participants argued. Most felt he had a mandate to deport undocumented immigrants who were violent or had committed crimes — though they were more divided over deporting law- abiding immigrants. When it came to Mr. Trump’s being extreme, the participants generally used that word about his language and leadership style, and most of them liked that he threw around threats against foreign adversaries or took a hard line on issues like illegal immigration. As for abortion, it simply was not a vote driver: Our group thought Mr. Trump was probably more conservative on the issue than they were, but they also saw him as not having a role now that it’s up to the states. The idea of America moving on from politics and being more united than it seems was a core idea for many in the group. They said they were tired of being criticized for holding conservative or contrary opinions and thought a lot of other Americans were, too. What may unite us in the end? “Eventually, everybody’s going to get tired of nitpicking,” said Julie, a 65-year-old Republican from Maryland. Patrick Healy, Margie Omero and Adrian J. Rivera Mr. Healy is the deputy Opinion editor. Ms. Omero is a pollster. Mr. Rivera is an editorial assistant in Opinion. PARTICIPANTS Ashley 31, white, Missouri, customer service Brandon 48, white, North Carolina, food industry Chris 46, white, Pennsylvania, line technician Direnda 66, white, Kentucky, house cleaner Erich 23, white, New Jersey, lacrosse coach Jason 38, white, Florida, realtor Joseph 55, white, Minnesota, handyman Moderator, Margie Omero In one word, what’s going wrong in the country today? Brandon, 48, white, North Carolina, food industry Economy. Kathi, 57, white, Ohio, property management Violence. Ashley, 31, white, Missouri, customer service Prices. Kenneth, 62, Black, California, truck driver Crime. Nicholas, 20, white, New York, driver The political divide. Joseph, 55, white, Minnesota, handyman A possible war with Russia over Ukraine. Moderator, Margie Omero And what’s going well in the country today? Chris, 46, white, Pennsylvania, line technician That’s a good question. Come back to me. Noah, 62, Latino, Texas, retired Employment. Seneca, 28, white, Arizona, charge specialist A lot of the power is going back to the states. The laws on abortion now being up to the states, for example. Mary, 50, Asian, Wyoming, assistant looks like the economy is doing pretty well right now. Erich, 23, white, New Jersey, lacrosse coach Cryptocurrency is skyrocketing. Kenneth, 62, Black, California, truck driver The election went well, but even prior to the new administration coming into office, things were going well. The economy is better. And other countries — we’re negotiating back to common sense. Jason, 38, white, Florida, realtor Someone just mentioned that we were divided, but I feel like this last election showed that we’re a little more unified than we thought we were, than maybe mainstream media thought we were. Mainstream media is another problem with our country. They tell us a lot of things that aren’t true. Moderator, Margie Omero Direnda, why are America’s best days happening right now? Direnda, 66, white, Kentucky, house cleaner We don’t know what’s going to happen in the future. Everybody assumes this war with Russia is going to stop, and I don’t see that happening. Noah, 62, Latino, Texas, retired the election did bare some truths. People decided the right thing to do is to actually vote for what was needed and not what people were being told. Moderator, Margie Omero Joe, I want to know why you think America’s best days are behind us. Joseph, 55, white, Minnesota, handyman The political crap has caused problems more than anything else. These days, it seems you share an opinion and two or three people in the crowd want to go crazy. A lot of us have lost our patriotism. In the days of the baby boomers, there was a lot of fun and excitement, even though we had the wars and everything we had to go through. We had good jobs. Incomes were increasing quicker than inflation was increasing. We were ahead of the curve. It just seemed like just a much better world to live in. Moderator, Margie Omero Chris, tell me why you think America’s best days are ahead of us. Chris, 46, white, Pennsylvania, line technician I think it more comes from a hope. Right now, people have a hard time having normal, civil conversations about things. Everybody gets all wrapped up in feelings, and it just gets hard to have good conversations when you don’t really listen to the other person’s side. You just want to scream your view louder. Hopefully, at some point, it will get better Eventually, everybody’s going to get tired of nitpicking. Everybody expects you just to be politically correct when they speak, like Chris was saying. Moderator, Margie Omero What’s one word that describes how you see Donald Trump today? Ashley, 31, white, Missouri, customer service Optimistic. Brandon, 48, white, North Carolina, food industry Change. Mary, 50, Asian, Wyoming, assistant Common sense. Kenneth, 62, Black, California, truck driver Compassion. Direnda, 66, white, Kentucky, house cleaner I feel optimistic about him. Erich, 23, white, New Jersey, lacrosse coach Patriotism. Nicholas, 20, white, New York, driver I was about to say the same thing. Moderator, Margie Omero Everyone here voted for Donald Trump. Have people’s views toward him changed over three elections? Mary, 50, Asian, Wyoming, assistant with my husband. I said, “Murray, why would you vote for this person?” He said, “He’s kind of a joke.” But seeing what he did in his term and seeing what he’s saying now, he’s definitely more qualified than I thought he was. Kathi, 57, white, Ohio, property management His mouth gets a little bit too much, but I thought he was a little bit more polished in this election, which I appreciated. Direnda, 66, white, Kentucky, house cleaner I thought he was so arrogant, but now, I think we need someone a little arrogant. We need somebody like that to straighten up the world. I just hope he doesn’t tweet anymore. Moderator, Margie Omero Tell me why you voted for Trump. Nicholas, 20, white, New York, driver I voted for Donald Trump because I like the way that he was with other leaders. He wasn’t just like, “Oh, yeah, let’s be friends.” He threatened them, and he had to because they were killing our economy. Kenneth, 62, Black, California, truck driver I served in the military. I’ve been a police officer. We spend lots of money in countries that most people couldn’t find on a map, and we have people that are starving in the streets of L.A. Trump believes in this country. He believes in Christ. He loves this country. This is only my third election I’m voting in. You see the difference in what he was able to accomplish in his four years, as opposed to what happened these past four. He runs this country like a business. And to some extent, that’s what you have to do, because the more money that we make, the more it’s going to benefit us. I think it’s tough for some people to see that, because of the brash side of him. Seneca, 28, white, Arizona, charge specialist I voted for him because I thought he was really smart and really good with money. And then also health care. I think it’s really cool that he’s going to take on fighting the big health care corporations that are charging insane amounts and hopefully get that under control. Moderator, Patrick Healy I want to talk a little bit about how you would describe Trump’s vision for America’s future. If he had his way, what would things be like in America four years from now? Jason, 38, white, Florida, realtor He had a booming economy in his first term, and I can only assume he’s going to do the same thing the second time around, especially with trying to bring a lot of our foreign jobs back over here. The tariff idea is already working, as we’ve seen him meet with Justin Trudeau. So yeah, I think we’re going to be booming in the next four years. Brandon, 48, white, North Carolina, food industry down the government. I work in hospitality. It’s cheaper for me to go get a banquet chair from China than it is to get an American one. And you have these lifelong politicians — as long as they get their donation money, they just keep promising whatever. So I really like the idea of the Department of Government Efficiency. It’s too much government. Joseph, 55, white, Minnesota, handyman Hopefully, the economy will be good, and he can resolve some of these issues in the foreign countries with the wars that are going on. I hope there’s more “Made in America,” bringing more jobs back here. From Covid, we learned you still have to have enough things being made here so if something goes sideways real quick, we can actually do something and not fall captive to a foreign market. Ashley, 31, white, Missouri, customer service At the end of these four years, we will have hopefully avoided World War III. Our food’s going to be healthier, not so many chemicals, because they’ll all be gone. Housing prices have hopefully gone down by then, more jobs here, corporate jobs here, manufacturing jobs here. Moderator, Patrick Healy What do you think Trump has a mandate to do as president? Direnda, 66, white, Kentucky, house cleaner Get rid of all these immigrants that have come into And do you support that, Direnda? Direnda, 66, white, Kentucky, house cleaner I really do, because it does throw the United States in a whirlwind. Moderator, Patrick Healy Does it affect you personally, Direnda? Direnda, 66, white, Kentucky, house cleaner It does. My father is from Iceland. He came here. He was in the military. He spent four years in the American military. He became a citizen, and he worked for it. He didn’t sneak in to do it. I have seen a lot of theft and crime, and it revolves around people that are not documented. Moderator, Patrick Healy Nicholas, what do you think Trump has a mandate to do as president? Nicholas, 20, white, New York, driver To defend the people of our country. Illegal immigration, now, that is something that does affect me personally. I live in South Brooklyn, where we have the infamous Floyd Bennett Field migrant shelter. And during a snowstorm or a storm last year, the migrants were brought to my former alma mater to sleep there for the night, and people could not go to school the next day because we had to house them. Mary, 50, Asian, Wyoming, assistant here, but she did it the right way. It is a problem. You have illegal immigrants who have contributed nothing who are getting thousands of dollars. Kenneth, 62, Black, California, truck driver Immigration is a really big deal, but I believe that people voted because of the economy. Inflation and the cost of living — you can’t sustain it. Kathi, 57, white, Ohio, property management Everybody talks about illegals, right? I’ve got a problem with that, to a certain extent. But we don’t talk about the people that are allowed to walk across, legally, into the United States. They come here. They get so many benefits. I think we need to move a little bit away from keep talking about the illegals and talking about immigration in general. They buy houses. They put 15, 20 people in those houses. They live right around the corner from me. Some take care of them. Some don’t. It brings your property values down, brings your car values down. It brings everything down. I live in farm country. They take all farm jobs. They do all the little housing jobs, the little tinkering jobs, is what they call them down here. Moderator, Patrick Healy Julie, are there Americans who would take those jobs? Julie, 65, white, Maryland, machinist There are. I’m retired. I can’t do it, but there are people down here that do those types of things. Chris, 46, white, Pennsylvania, line technician I think that there’s probably a very good majority of people that are undocumented that are normal, law-abiding people and could be contributing members of society. So I don’t think that getting rid of all of those is necessarily the answer. There are some that obviously don’t deserve to be here, but to just get rid of all of them — there’s people that are born here every day that — would they have to go start a new life in some other country, through no fault of their own? There could be some common- sense ways to go about doing it, other than just picking up and shipping them all out. Mary, 50, Asian, Wyoming, assistant I think I agree with Chris. There needs to be some type of metric or something. You can’t just drive a bus down the road and say, “Hey, show me your papers or get on the bus and we’re taking you to hard-working immigrants here who deserve a shot. If they’ve been here for 10, 15 years, contributing, I see no problem with that. Moderator, Patrick Healy Is there anything that concerns you about a Trump presidency? Is there anything you don’t want him to do as president? Chris, 46, white, Pennsylvania, line technician I just hope that it doesn’t go to his head and he starts to get reckless. I just don’t want him to get some of the wrong people in his corner, that are in his ear, that have their own agenda, maybe. Not that he would necessarily do it on purpose, but I feel like he might — Direnda, 66, white, Kentucky, house cleaner Be influenced. Chris, 46, white, Pennsylvania, line technician — go with the flow and want to please everybody and it just gets out of hand. Kathi, 57, white, Ohio, property management I’m a little bit with Chris on that, but it does seem like this so-called election, he’s been much better about containing himself. Moderator, Patrick Healy Is there anything that Trump says or does or wants to do that strikes you as kind of extreme? Joseph, 55, white, Minnesota, handyman with the stuff going on with Russia and Ukraine. Hopefully arrogance and whatever cockiness doesn’t get in the way of calming this little situation down so we don’t take it to the next level. Brandon, 48, white, North Carolina, food industry With Trump and with everybody in the political game, it’s just the bad-mouthing of other people. Just run a country. Have the best people. That’s the biggest thing. Jason, 38, white, Florida, realtor Is he perfect? No, but when people say he’s brash or he’s too forward, I’m like, “Well, we kind of need that.” I don’t want someone in war rooms who’s going to be polite and courteous. Be professional, but I don’t want you to be a pushover. Ashley, 31, white, Missouri, customer service I think an extreme thing that he could potentially do is banning gender-affirming care for adults. Leave the children out of it, but there’s no reason to prevent adult Americans from living their lives how they want to live their life. Seneca, 28, white, Arizona, charge specialist I think he needs to be a little extreme to be a leader, and a lot of times, there’s going to be people who don’t agree with him, and that’s just how things are. Moderator, Margie Omero Let’s talk a bit about abortion. Moderator, Margie Omero Kenneth, you didn’t raise your hand. Kenneth, 62, Black, California, truck driver My opinion on abortion is that it’s a woman’s body. I don’t think politicians should be in a position to tell a person what they should do with their body. I just don’t agree with everything that Democrats say, but Donald is a little extreme. Mary, 50, Asian, Wyoming, assistant I know what Kamala Harris has said on that topic, but I didn’t know what Donald had said. I couldn’t form an opinion one way or the other. What was your sense of Harris’s position? Mary, 50, Asian, Wyoming, assistant I feel like she ran a lot of her campaign on that, on the fear of abortion being banned completely. Moderator, Margie Omero What do you see as Trump’s position on abortion? Mary, 50, Asian, Wyoming, assistant I think that he took it off the federal government and put it on the states. He doesn’t have a say anymore. Erich, 23, white, New Jersey, lacrosse coach On abortion, I fell in the middle, leaning toward Trump, where I do agree on leaving it up to the states. But it is a woman’s body. It is her choice as to what happens with that, but there should be some regulation to late-term abortions and stuff. Direnda, 66, white, Kentucky, house cleaner Yeah, I’m kind of in the middle. People are trying to make laws on what people do with their bodies, but it really depends upon the situation. You’ve got rape victims, people who are traumatized, people who are deathly ill. You need a law that covers that. Moderator, Margie Omero How would you describe Trump’s position on abortion, Direnda? Direnda, 66, white, Kentucky, house cleaner his hands of the whole situation and said: OK, you guys decide. Seneca, 28, white, Arizona, charge specialist I think Trump wanted to give the abortion rights to the state, and in my state, we just had a vote on it, where an abortion would be legal up to 24 weeks, six months of being pregnant. I think there’s certain situations when a woman should get an abortion and they should have an option to do it, but I think 12 weeks, unless it’s a certain situation — rape, incest, medically necessary for the mother or the baby — should be the limit. I like that it’s back on the states. Moderator, Margie Omero During recruitment for this group, everyone here said that they support abortion rights being legal. Trump has taken credit for overturning Roe v. Wade, and since then, abortion has been banned in some states around the country. Trump has also said that he doesn’t support a national abortion ban. So tell me a little bit about how the issue of abortion factored in your thinking about how you would vote. Ashley, 31, white, Missouri, customer service I didn’t really vote for a president based on the topic of abortion. Kathi, 57, white, Ohio, property management Personally, I don’t think the government should have any decision. They don’t tell us if we need a heart transplant, we need our leg amputated. I don’t really feel that overturning Roe v. Wade and putting it back on the states was a big win for the country. It’s actually probably going to do a little bit more harm than probably intended. I feel that he probably did that in order to placate his base a little bit, but I don’t agree with it. Seneca, 28, white, Arizona, charge specialist It didn’t really affect my voting for him, because I Same as Seneca. I’m in Minneapolis, so we’re a Democratic state. You can get abortions up here. My daughters live out of the state, so that might become issues where they’re living. But that’s not why I voted for him. I voted for him for trying to get the economy back, bring jobs back to America and other reasons, too. Jason, 38, white, Florida, realtor It doesn’t affect my voting on him, either. I was concerned about staying out of war and being more harsh on illegal immigration. Moderator, Margie Omero Speaking of war: Should Trump push for an end to the war in Ukraine or continue supporting it in its defense against Russia? [All 14 focus group members raised a hand in supporting a push to end the war in Ukraine.] Nicholas, 20, white, New York, driver I feel like Putin isn’t really that much of a threat, considering the fact that he really hasn’t done anything to the United States. In fact, the United States has done a lot more to him. He’s trying to take over another country, and he’s failing. Kenneth, 62, Black, California, truck driver I don’t think we should be focusing on a country that has nuclear warheads, as Russia does, in the name of defending Ukraine. Moderator, Margie Omero heard about his picks for attorney general, F.B.I. director, secretary of defense and more? Brandon, 48, white, North Carolina, food industry I have not followed it too much, other than that whole aspect of just stripping the government down. There’s a lot of unnecessary spending that we can do better with. Mary, 50, Asian, Wyoming, assistant I heard about Elon Musk taking some things over. Moderator, Margie Omero What do you think about Elon Musk’s role? Mary, 50, Asian, Wyoming, assistant He could do a good job. Who knows? Nicholas, 20, white, New York, driver I saw that Trump picked Thomas Homan as his border czar. He would do a great job, considering the fact he was in the first administration as head of ICE. Kathi, 57, white, Ohio, property management I can’t remember the guy’s name, but for lack of a better description, the one that was accused of sexual misconduct. But I do like the fact that Trump’s trying to align his forces and set up his team. So it’s give and take. Moderator, Margie Omero How do you think the Senate should go about considering Trump’s nominees? I think they should check on the people, run a background check and make sure that they’re qualified to be in the position, but unless there’s something huge, I think they should be going with Trump’s decision. Joseph, 55, white, Minnesota, handyman If there’s somebody that even the Republicans don’t want, then there’s probably a problem. Brandon, 48, white, North Carolina, food industry They need to do their job of background check but not go so far that they really dive into that one little mistake somebody made 10 years ago. People change. Moderator, Patrick Healy I want to focus on a few of Trump’s potential nominees. One is Robert F. Kennedy Jr. to lead the Department of Health and Human Services. That department oversees a lot of the science agencies — the F.D.A., the C.D.C. Robert F. Kennedy Jr. has talked quite a bit about focusing on food and water safety. He’s also raised questions about scientific studies and about vaccines. Do you have strong feelings about Kennedy? Jason, 38, white, Florida, realtor I’m glad he got picked. His stance on everything during Covid — I very much agreed with it. Obviously, if you wanted to get jabbed, by all means, get jabbed. Again, your body, your choice. We’re back to that. But as far as mandating people, I wasn’t for it. And yeah, he’s a Democrat, a lifelong see again that Trump is trying to pull people from a diverse group of people that are available to him. Joseph, 55, white, Minnesota, handyman I like the fact that he’s not going to go with the status quo on food. We put so much crap in our food that’s not legal in most countries. That’s why obesity is such a big problem here. Noah, 62, Latino, Texas, retired I don’t know that I have enough insight on him, but I have a mild historical optimism. I hope he gets almost Uncle Bobby-like and takes responsibility for all of the fraud that is committed throughout those departments, because that is one of the biggest sources of our expenditures in the country. Moderator, Patrick Healy I’m curious how you think the F.B.I. and the Justice Department should operate in America. How much should they be doing what the president wants those agencies to do? Or should they be independent of the president? Joseph, 55, white, Minnesota, handyman You have to have people that are able to follow the laws of America, the Constitution, and not have somebody that wants to bend the rules and enforce whatever their mind-set might be at the time. You’ve got to also remember his age. But no, he can’t control all the legal processes in America. You’ve still got to have other people that are in control. As long as they’re taking the good of the country into consideration, that’s really my main concern. Nicholas, 20, white, New York, driver I actually do go to school for criminal law, so I just want to say that the F.B.I. director and the U.S. attorney general should be holding accountable those who need to be accounted for in the government. Political witch hunts should end on both sides. Moderator, Patrick Healy When you say “both sides,” Nicholas, who’s an example of someone on the right, and who’s an example of someone on the left that’s been a witch hunt target? Nicholas, 20, white, New York, driver Donald Trump and then let’s say Hunter Biden. Moderator, Patrick Healy How do people feel about President Biden’s decision to pardon Hunter Biden, who was convicted on federal tax and gun charges? Kathi, 57, white, Ohio, property management Illegal. Joseph, 55, white, Minnesota, handyman Foolish. Joseph, 55, white, Minnesota, handyman No skin off my teeth. Erich, 23, white, New Jersey, lacrosse coach going to pardon his son, he was going to leave the decision up to the courts and then to do it — he should have just pardoned him earlier and saved the tax money of going through that process if he was going to do this the whole time. Jason, 38, white, Florida, realtor It’s on brand for the Bidens. Seneca, 28, white, Arizona, charge specialist It’s illegal. It’s a conflict of interest. I’m a notary in my state, and I can’t even notarize for my own family members. So for someone to pardon someone of their crimes, being a family member, it’s just insane. Mary, 50, Asian, Wyoming, assistant It’s nepotism at its finest, right there. Kathi, 57, white, Ohio, property management I need more details on each case. Ashley, 31, white, Missouri, customer service They destroyed property. They got caught. They should probably suffer the consequences of what they did. Direnda, 66, white, Kentucky, house cleaner Ditto. Moderator, Patrick Healy Kenneth, you’re a veteran. How do you see the Jan. 6- ers and the idea of pardoning them? Kenneth, 62, Black, California, truck driver I’m kind of in the middle. The ones that were actually lawbreakers shouldn’t be pardoned. When they went into the Capitol building, some were just there, peaceful protesters. Maybe those people can get some type of pardon or commute their sentences. They shouldn’t be serving 15 years for just walking into the Capitol building. The ones that were violent and actually went in and took over in the Capitol building, they shouldn’t be pardoned. Julie, 65, white, Maryland, machinist The ones that actually did violence should be punished accordingly, but the ones that were If there’s one thing you could change about America today, to improve our future, what would it be? Kathi, 57, white, Ohio, property management Stop putting labels on things or people. Brandon, 48, white, North Carolina, food industry Yeah, same thing. Stop looking at this side, that side. Kenneth, 62, Black, California, truck driver Politicians, they divide us. Not labels. We watch football, and everyone gets along, and we’re happy. When we listen to Fox and CNN, they divide us. Mary, 50, Asian, Wyoming, assistant I’d like to see more unity and a lot less hate. There’s so much hate-speak. It would be really nice if we could all talk to each other, finally. Ashley, 31, white, Missouri, customer service Being able to afford houses now is my biggest concern. Big corporations buying up all the housing and renting them out is, I think, the worst thing that’s happening right now. Nicholas, 20, white, New York, driver Stop criticizing people for their political opinions. Chris, 46, white, Pennsylvania, line technician Term limits on every public servant. Nothing’s going to change if we have the same people in office for 30 years. Julie, 65, white, Maryland, machinist I want to see the space program take off again. You want to go to Mars or the moon or somewhere else? Julie, 65, white, Maryland, machinist All of it. Direnda, 66, white, Kentucky, house cleaner The youth needs to learn how to get back to the basics. They need to learn how to put in a garden. They need to learn how to repair their cars and their vehicles. They need to learn how to sew. The whole bit — just start over. Seneca, 28, white, Arizona, charge specialist I think we need to change how we manage our money. Noah, 62, Latino, Texas, retired I think our country would be better off having a more informed citizenry and where people argue their own ideas, not just their party’s statement, on everything from immigration to the economy. Erich, 23, white, New Jersey, lacrosse coach I want to see a healthy country again. I want to see kids playing outside instead of only being on screens. I want to see people, as they get older, continue to stay active. I want to see people care about what we’re putting in our bodies just a little bit more than what we see now. Jason, 38, white, Florida, realtor We need a better political system than two parties, because I think having parties makes things so divided. I don’t know how it would work, but having through everyone arguing all the time. We need something better. America in Focus seeks to hear and understand the views of cross-sections of Americans whose voices are often not heard in opinion journalism. This discussion was moderated by a focus group veteran, Margie Omero, and the New York Times deputy Opinion editor, Patrick Healy. Ms. Omero has done similar work over the years for Democratic candidates and partisan groups. She chose the participants. (Times Opinion paid her for the work.) This transcript has been edited for length and clarity; an audio recording of the session is also included. Participants provided their biographical details. As is customary in focus groups, our role as moderators was not to argue with or fact-check the speakers, and some participants expressed opinions not rooted in facts. Illustrations by Lucinda Rogers. The small effects of political advertising are small regardless of context, message, sender, or receiver: Evidence from 59 real-time randomized experiments Author: Coppock, Alexander, Date: 2020-09-04 Collections: MediaAdsPolit Zotero Key: Q7RY7LHN Cite Key: Coppock20SmallEffectsPolitical Zotero Item | Lit Note SO C I AL SCIENCES The small effects of political advertising are small regardless of context, message, sender, or receiver: Evidence from 59 real-time randomized experiments Alexander Coppock[1]*, Seth J. Hill[2], Lynn Vavreck[3] Evidence across social science indicates that average effects of persuasive messages are small. One commonly offered explanation for these small effects is heterogeneity: Persuasion may only work well in specific circum- stances. To evaluate heterogeneity, we repeated an experiment weekly in real time using 2016 U.S. presidential election campaign advertisements. We tested 49 political advertisements in 59 unique experiments on 34,000 people. We investigate heterogeneous effects by sender (candidates or groups), receiver (subject partisanship), content (attack or promotional), and context (battleground versus non-battleground, primary versus general election, and early versus late). We find small average effects on candidate favorability and vote. These small effects, however, do not mask substantial heterogeneity even where theory from political science suggests that we should find it. During the primary and general election, in battleground states, for Democrats, Republicans, and Independents, effects are similarly small. Heterogeneity with large offsetting effects is not the source of small average effects. Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC). INTRODUCTION Efforts by one actor to influence the choices of others pervade the social world. Political campaigns aim to persuade voters, firms aim to persuade consumers, and public service groups and governments aim to persuade citizens. Because persuasion is attempted in so many settings, the conditions for effective persuasion have been studied by scholars across many fields. The resulting set of theories and evidence points in the same direction: Study by study, social scientists have reported that persuasive influence tends to be small on average but have theorized that those small average effects mask large differ ences in responsiveness. While the details vary across disciplines, persuasion is thought to require a specific mix of message content and environmental con text, along with a special match of features of the sender with features of the receiver. In other words, persuasion is presumed to be condi tional on who says what to whom and when, and getting this recipe right is thought to be critical for changing minds. Across fields, scholars have elaborated different elements of this mixture. Psychologists laid out an initial model of attitude change (1) and demonstrated how characteristics of people (2) and pathways of thought (3) affect acceptance of messages and, therefore, persua sion. Contemporary work in psychology has taken context seriously, suggesting that culture, habit, social networks, and the framing of messages affect the magnitude of persuasion (4, 5). In marketing and consumer research, persuasive success has been shown to increase with shared social or ethnic identities among senders and receivers (6) and to depend on receiver experience with promotional appeals (7) and sender level of expertise (8). Work in management science shows similar heterogeneity: Successful transfers of information within firms depend on the capacity of the recipient to absorb inform ation, the ambiguity of the causal process at issue, and the personal relations between the speaker and the audience (9). In economics, 1Department of Political Science, Yale University, New Haven, CT, USA. 2Department of Political Science, University of California, San Diego, San Diego, CA, USA. 3Department of Political Science, University of California, Los Angeles, Los Angeles, CA, USA. Corresponding author. Email: alex.coppock@yale.edu the magnitude and effectiveness of persuasion are argued to vary with the preferences of the speaker and the prior beliefs of the audience (10). In our discipline of political science, a large body of work focuses on how much campaigns affect voter preferences (11–21). Recent work has focused on the relatively small size of the persuasive effects of campaign advertisements and the rapid decay of these effects (22, 23). Persuasion is generally believed to vary by characteristics of the messages such as advertising content (17, 24, 25), identity of the sender (26, 27), differences across receivers like partisanship or knowledge about the topic at hand (28), and contextual factors, in cluding whether competing information is present (28, 29). Synthesizing results across fields into a coherent theory of hetero geneity in persuasive effects is frustrated by difficult-to-overcome challenges of research design. For example, to understand the rela tive importance of message content, context, sender, and receiver, we need a design that allows each of the features to vary inde pendently while holding others constant at the same levels each time. Typically, experiments vary one feature and hold all others constant at an idiosyncratic level in a single setting. When subsequent exper iments turn to investigate a different attribute, the other features of the experiment become fixed at their own idiosyncratic levels— most likely different levels than in previous experiments. Further more, owing to the decades that have been spent researching per suasion, not only do studies investigate different targets and types of persuasion while holding other factors constant at varying levels, they do so with different designs, instruments, and sampling methods. Aggregating results from this set of studies—executed in largely uncoordinated ways—is challenging. Adding to the difficulty of forming a coherent theory of heterogeneity is the fact that publication and career incentives reward evidence of difference more highly than evidence of similarity (30, 31). As a result, the empirical record may overemphasize findings of treatment effect heterogeneity. We have designed a series of unique tests spanning 8 months in which the sample, design, instrument, and analysis are all held constant. We measure the effects of 49 unique presidential advertisements made by professional ad makers during the 2016 presidential election among large, nationally representative samples using randomized experiments (we tested some of the 49 unique advertisements in multiple weeks). This design allows us to examine possible differences in persuasion related to message, context, sender, and receiver. The summary finding from our study is that, at least in hardfought campaigns for the presidency, substantial heterogeneities in the size of treatment effects are not hiding behind small average effects. Attack and promotional advertisements appear to work similarly well. Effects are not substantially different depending on which campaign produced the advertisements or in what electoral context they were presented. Subjects living in different states or who hold different partisan attachments appear to respond to the advertisements by similar degrees. While we do not claim absolute homogeneity in treatment effects, our estimates of heterogeneity are substantively small. We estimate an average treatment effect of presidential advertising on candidate favorability of 0.05 scale points on a five-point scale, with an SD across experiments of 0.07 scale points. On vote choice, the average effect is 0.7 percentage points with an SD of 2 points. Advertising effects do vary around small average effects, but the distribution of advertising effects in our experiments excludes large persuasive effects. These results suggest that scholars may want to revisit previous findings and re-evaluate whether current beliefs about heterogeneity in persuasion rest on heterogeneity of studies and designs rather than an essential conditionality of persuasion. MATERIALS AND METHODS Our experiments were fielded from March to November (Election Day) in 2016, covering the primary elections of both major American political parties and the general election. Each week for 29 (not always consecutive) weeks, a representative sample of Americans was divided at random into groups and assigned to watch campaign advertisements or a placebo advertisement before answering a short survey. This sample period strengthened our design because it covered both the primary election, when voters lacked information about candidates or strong partisan cues, and the general election, when information about the two major party candidates was more easily available. Our subjects were recruited by YouGov, which furnished samples of exactly 1000 (or 2000, depending on the week) complete responses. Participants were part of YouGov’s ongoing survey research panel and were invited to this particular survey after agreeing to take surveys for YouGov generally. This process was approved by the University of California, Los Angeles Institutional Review Board (IRB#16-000691). Some subjects (42%, on average) started the survey but did not finish, which can cause bias away from our inferential target, the U.S. popu lation average treatment effect. We rely on YouGov’s poststratifica tion weights to address the problem that some kinds of people are more likely to finish the survey than others. A second source of bias is the possibility that our treatments caused subjects to stop taking the survey. Using information on the full set of respondents who began each survey, we find no evidence of differential attrition by treatment condition. Specifically, we conduct separate [2] tests of the dependence between response and treatment assignment within each week of the study. Of the 29 tests, three return unadjusted P values that are statistically significant. When we adjust for multi ple comparisons using the Holm or Benjamini-Hochberg corrections (32, 33), none of the tests remain significant. Although this analysis does not prove that the missingness is independent of assignment, we proceed under that assumption. We chose treatment advertisements from the set of advertisements released each week during the 2016 campaign by candidates, parties, or groups. We picked advertisements to test on the basis of real-time ad-buy data from Kantar Media and news coverage of each week’s most important advertisements. See the Supplemental Materials for more information about the treatments, including transcripts, date of testing, and the advertisements themselves. In total, we tested 30 advertisements attacking Republican candidate Donald Trump, 11 attacking Democratic candidate Hillary Clinton, 8 promoting Clinton, and 3 promoting Trump. The remainder were promotional advertise ments for other primary candidates fielded before the general election. The design of each weekly experiment was consistent. During the primaries (14 March to 6 June), 1000 respondents were randomly assigned to one of four conditions: watch either one of two campaign advertisements, both, or neither. Subjects who saw neither treatment advertisement were shown a placebo advertisement for car insurance. Subjects who saw two advertisements sometimes saw pairs of advertisements with competing messages and sometimes saw advertisements with reinforcing messages. We also showed some advertisements in multiple weeks to estimate whether the effect of the same advertisement varies with changing context. In analyses reported in the Supplementary Materials, we find no significant dif ferences in effectiveness over time: The same advertisement works approximately equally as well, regardless of when during the campaign we test it. During the period surrounding the conventions before the tradi tional start of the general election campaign (20 June to 15 August), 2000 respondents were recruited every other week. These subjects could be assigned to control or one of the two video conditions, but we removed the “both” condition during this period. In the fall, we returned to 1000 respondents a week and re-introduced the “both” condition on 26 September. We used the Bernoulli random assignment to allocate subjects to treatment conditions with equal probabilities. We are confident that treatments were delivered as intended: Tiny fractions of the control groups claimed to have seen campaign advertisements (2 to 4%) compared with large majorities of the treatment groups (92 to 95%). After watching the videos, subjects took a brief survey, the full text of which is available in the Supplementary Materials. Here, we focus on two main outcome variables: subjects’ favorability rating of the target candidates on a five-point scale and their general elec tion vote intention. Favorability and vote intention can be thought of as two points along a spectrum of candidate evaluation running from more to less pliable. If advertisements are effective at all, they should move candidate favorability before choice (34). Vote choice (as measured by vote intention) is the more consequential political outcome but may be more difficult to change via persuasive appeal. Our design allows us to measure whether advertisements move one, the other, both, or neither of these outcomes. We estimate treatment effects separately for each week’s experi ment using ordinary least squares (OLS) and robust SEs. OLS is a consistent estimator of the average treatmen effect (ATE) under our design (35). To increase precision (36), we control for the following covariates measured before treatment: seven-point party identifica tion, five-point ideological self-placement, voter registration status, gender, age, race, income, education, region, and a pretreatment question about whether the country was on the right track. The treatment effects in any single 1000-person study are esti mated with a fair amount of sampling variability, but pooling across weeks via random-effects meta-analysis allows us to sharpen the estimates considerably. Random-effects meta-analysis allows us to directly estimate the extent of heterogeneity with respect to features of the persuasive environment. RESULTS In Figure 1, we plot average treatment effect estimates (and 95% confidence intervals) for each of our 59 experimental comparisons for our two dependent variables. The top facet presents ATE esti mates on candidate favorability, and the bottom facet presents ATE estimates on vote intention. The x axis represents calendar time to show when each experiment happened over the course of the cam paign. Each point is one ATE estimated by OLS. To the right of each time series, we plot the meta-analytic estimate. On average, the ad vertisements moved target candidate favorability 0.049 scale points (1 to 5 scale) in the “correct” direction—the analysis is scaled such that the treatment effect of promotional advertisements is in the direction of favorability and that of attack advertisements is in the opposite direction. This estimate, though small, is statistically sig nificant owing to the large size of our study. The effect on target candidate vote choice is also small at 0.7 percentage points, but is not statistically significant. Figure 1 also provides a first indication of our main finding of low treatment effect heterogeneity. While ATE estimates vary from advertisement to advertisement, this variability is no greater than what would be expected from sampling variability. Using formal tests, we fail to reject the null of homogeneity in both cases (favora bility, P = 0.09; vote choice, P = 0.07). As we discuss below, other statistical tests for large (nonsampling) variability generate similarly weak evidence. We pause here to reflect on what Fig. 1 would mean if experi mental results were subject to a statistical significance publication filter. Imagine that Fig. 1 represents the sampling distribution of persuasive treatment effects. If only statistically significant results were published, then we would be left with one large negative result (nearly −0.5 points on favorability in early May), one large positive result toward the end of the campaign in October (more than +0.5 points), and two other negative results in October. If the remaining experiments were not published, then one could imagine theorists hypothesizing that the effects depend on specific features of the content of these advertisements, the context of the campaign on these dates, or something about the senders and receivers in these experiments. Our research design of repeated experiments provides context for these estimates. The main story of the graph is that treatment effects are similarly small over time, but sample sizes of 1000 or 2000 generate week-to-week sampling variability. Without multiple experiments, or much larger experiments, publication bias could lead to a distorted view of the heterogeneity in persuasive effectiveness, which, in turn, could lead to overfit theories of persuasion. Conditionality of persuasive effects We now more directly evaluate whether treatment effects are condi tional on theoretically posited drivers of difference across message, context, sender, and receiver. In Table 1, we regress conditional average treatment estimates (conditioning on subject partisanship and battleground state residence) on a set of predictors of hetero geneity common to political science studies of campaigns. Alto gether, we consider seven sources of heterogeneity, one feature of the receiver (respondent partisanship), three features of context (time to election day, whether the advertisement was aired during the primary or the general, and battleground state residency), and three features of message or sender [whether the advertisement was attack or promotional, if sponsored by a campaign versus an outside group Political Action Committee (PAC), and the target candidate]. Turning first to features of receiver, we see that the differences in treatment response across Democrats, Independents, and Republicans Weekly estimates Meta−analysis 0.5 0.0 −0.5 0.1 0.0 −0.1 April May June July August September October November Fig. 1. Average treatment effect of advertising on candidate vote choice and favorability. are small, with SEs greater than coefficients. Independents do not appear to be more malleable than partisans, and neither partisan group is more responsive than the other. We consider below if par tisanship of respondent interacts with partisanship of sender. Of the three features of context, the slope with respect to the timing of the advertisement is negative in three of four specifications and statistically distinct from zero in one. The estimate in the third column suggests that effects do decline as the campaign progresses. Difference in effectiveness for subjects who do and do not live in battleground states is 0.008 scale points. On average, general election advertisements move candidate favorability by 0.121 scale points more than primary advertisements, although the estimate is not precise enough to achieve statistical significance. Even though this point estimate is large compared with the others in the table, it remains substantively small. The upper bound of the 95% confidence interval for the average difference between primary and general election advertise ments is about 0.25 scale points on a five-point favorability scale. If there is heterogeneity by election phase, it is not of large political importance. Characteristics of the advertisements themselves do not cor relate strongly with estimated effects. We find that attack advertise ments are about as effective in achieving their goals as promotional advertisements and that PAC- or SuperPAC-sponsored advertisements are no more effective than those sponsored by candidates. While pro-Clinton advertisements tended to be more effective than adver tisements in support of or in opposition to other candidates, these differences cannot be distinguished from zero. Overall, there is little evidence that the magnitude of persuasion is conditional on the moderators that we evaluate motivated by political science theory. Table 1 estimates average differences in response by features of the subjects and the advertisements separately, but it is possible that effectiveness depends instead on the interaction of receiver and message, as argued by some existing theories of persuasion. For example, some theories posit that partisan respondents will view in-party messages more favorably than out-party messages, or more generally, that treatments need to “match” subjects on important dimensions to be effective. Figure 2 provides partial support for the “partisan match” theory: Democratic subjects respond more strongly to pro-Democratic advertisements than to pro-Republican advertisements. However, we do not observe a corresponding pattern among Republican respondents: Both pro-Democratic and pro- Republican adver tisements have approximately the same small, positive, nonsignificant effect. Table S2 presents formal tests of homogeneity of treatment effects across experiments and across subgroups. For the sample average treatment effects (SATEs) across experiments, P values from tests against the null of treatment effect homogeneity are 0.09 for favor ability and 0.07 for vote choice. For the set of conditional average treatment effects (CATEs), P values are 0.0002 and 0.96. While we fail to reject the null hypothesis of treatment effect ho mogeneity in three of four opportunities, we do not affirm that null. Instead, we rely on the direct measure of treatment effect hetero geneity provided by the random-effects estimator. The square root of the [2] statistic is an estimate of the true SD of the treatment ef fects. We estimate this value to be 0.07 (SATEs) and 0.15 (CATEs) for favorability on a five-point scale and 0.02 and 0.02 for the vote choice. Since most estimates can be expected to fall within 2 SDs of the average, this analysis suggests small substantive effects even for the largest of CATEs. Across our many experiments that vary content, context, sender, and receiver, we find very little evidence that large persuasive effects occur even under a specific mix of features. DISCUSSION Across social science fields, a common pattern has emerged: Per suasive attempts tend to produce small average effects. Our 59 experiments demonstrate this. A persistent worry is that these small average effects mask large and offsetting conditional effects. Scholars from many traditions have forwarded theories to predict the cir cumstances under which such conditionality will obtain. Theories of heterogeneity have tended to outpace empirical demonstrations and confirmations of such heterogeneity due to basic constraints of research design. We need fine control over the many features pre sumed to cause heterogeneity, large sample sizes to measure small variations in response, and repeated experiments to confirm generality. The present study is unusual in its size (34,000 nationally repre sentative subjects) and breadth of treatments (a purposive sample of 49 of the highest-profile presidential advertisements fielded in the midst of the 2016 presidential election), allowing us to system atically investigate how variations in message, context, sender, or receiver condition persuasive effects—while holding all other tools of the research design constant. We do not find strong evidence of heterogeneity. The magnitude of the effects of campaign advertisements on candidate favorability and vote choice does not appear to depend greatly on characteristics of the advertisement like tone, sponsor, or target; characteristics of the information environment such as timing throughout the elec tion year or battleground state residency; or characteristics of actors such as partisanship. Of course, we have not tested all potential sources of heterogeneity. There may well be a mix of message features and subject character istics that generates politically important persuasion. We have not considered here some hypothesized moderators such as need for cognition, need for evaluation, need for cognitive closure, moral foundations, personality type, or interest in politics for the main reason that we allocated our budget to many shorter surveys rather than fewer longer surveys that could have measured these possible sources of variation in treatment response. However, even if we had measured these and found that they did not predict heterogeneity, Pro−Democratic advertisement Pro−Republican advertisement 0.048 (0.039) Republican respondents 0.039 (0.036) Independent respondents Democratic respondents |Col1|Col2|Col3|0.07|0 (0.0|68)|Col7| |---|---|---|---|---|---|---| ||||0|.106|(0.02|7)| |Col1|Col2|0.012|(0.0|87)|Col6|Col7| |---|---|---|---|---|---|---| ||−0.0|26 (0.|049)|||| −0.002 (0.009) Republican respondents 0.007 (0.014) Independent respondents Democratic respondents |Col1|0|.005|(0.02|4)|Col6|Col7| |---|---|---|---|---|---|---| |||0.02|7 (0.|008)||| |Col1|Col2|0.02|7 (0.|029)|Col6|Col7| |---|---|---|---|---|---|---| ||−0.0|23 (0|.015)|||| −0.1 0.0 0.1 0.2 −0.1 0.0 0.1 0.2 Meta−analytic conditional average treatment effects Fig. 2. Average effects of advertisements on favorability and vote choice, conditional on subject partisanship and advertisement target. we still would not affirm complete homogeneity because future scholarship could always discover as-yet unknown and unmeasured sources of variation. All that said, we have tested many of the key theoretical ideas from political science in the context of a presidential campaign and found little evidence of large differences. Despite these small effects, campaign advertising may still play a large role in election outcomes. Our intervention delivers one addi tional ad in the heart of a marked presidential campaign that aired hundreds of thousands of such advertisements. This promotes external validity, but we are measuring the marginal effect of one additional advertisement. We do not measure the impact of an en tire advertising campaign. If effectiveness were to increase linearly in advertisements viewed (or if the marginal returns diminished slowly enough), then these small effects could be highly consequential, consistent with the observed level of spending by candidates on ad vertising. Our data cannot speak to this question of scale, although the result in Table 1 that effects do not vary by battleground status (where people see many more advertisements than those who live in non-battleground states) suggests that marginal effectiveness may not depend on ambient levels of advertising. How should scholars add our evidence to the science of politi cal persuasion and to persuasion more generally? We suggest two conclusions. First, the marginal effect of advertising is small but detectable; thus, candidates and campaigns may not be wrong to allocate scarce resources to television advertising because, in a close election, these small effects could be the difference between winning and losing. Second, the expensive efforts to target or tailor adver tisements to specific audiences require careful consideration. The evidence from our study shows that the effectiveness of advertise ments does not vary greatly from person to person or from adver tisement to advertisement. SUPPLEMENTARY MATERIALS Supplementary material for this article is available at http://advances.sciencemag.org/cgi/ content/full/6/36/eabc4046/DC1 17. L. Vavreck, The Message Matters: The Economy and Presidential Campaigns (Princeton Univ. Press, 2009). 18. M. M. Franz, T. N. Ridout, Political advertising and persuasion in the 2004 and 2008 presidential elections. Am. Politics Res. 38, 303–329 (2010). 19. J. Sides, L. Vavreck, The Gamble: Choice and Chance in the 2012 Presidential Election (Princeton Univ. Press, 2013). 20. D. E. Broockman, D. P. Green, Do online advertisements increase political candidates’ name recognition or favorability? Evidence from randomized field experiments. Polit. Behav. 36, 263–289 (2014). 21. J. Sides, M. Tesler, L. Vavreck, Identity Crisis: The 2016 Presidential Election and the Battle for the Meaning of America (Princeton Univ. Press, 2018). 22. A. S. Gerber, J. S. Gimpel, D. P. Green, D. R. Shaw, How large and long-lasting are the persuasive effects of televised campaign ads? Results from a randomized field experiment. Am. Polit. Sci. Rev. 105, 135–150 (2011). 23. S. J. Hill, J. Lo, L. Vavreck, J. Zaller, How quickly we forget: The duration of persuasion effects from mass communication. Polit. Commun. 30, 521–547 (2013). 24. S. Ansolabehere, S. Iyengar, Going Negative: How Attack Ads Shrink and Polarize the Electorate (Free Press, 1995). 25. E. F. Fowler, M. M. Franz, T. N. Ridout, Political Advertising in the United States (Westview Press, 2016). 26. W. M. Rahn, The role of partisan stereotypes in information processing about political candidates. Am. J. Polit. Sci. 37, 472–496 (1993). 27. D. Broockman, J. Kalla, Durably reducing transphobia: A field experiment on door-todoor canvassing. Science 352, 220–224 (2016). 28. J. Zaller, The Nature and Origins of Mass Opinion (Cambridge Univ. Press, 1992). 29. J. N. Druckman, On the limits of framing: Who can frame? J. Polit. 63, 1041–1066 (2001). 30. Open Science Collaboration, Estimating the reproducibility of psychological science. Science 349, aac4716 (2015). 31. A. Gelman, E. Loken, The statistical crisis in science: Data-dependent analysis—a “garden of forking paths”—explains why many statistically significant comparisons don’t hold up. Am. Sci. 102, 460–465 (2014). 32. S. Holm, A simple sequentially rejective multiple test procedure. Scand. J. Stat. 6, 65–70 (1979). 33. Y. Benjamini, Y. Hochberg, Controlling the false discovery rate: A practical and powerful approach to multiple testing. J. R. Stat. Soc. B. Methodol. 57, 289–300 (1995). 34. D. R. Shaw, The Race to 270: The Electoral College and the Campaign Strategies of 2000 and 2004 (University of Chicago Press, Chicago, 2006). 35. W. Lin, Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique. Ann. Appl. Stat. 7, 295–318 (2013). 36. A. S. Gerber, D. P. Green, Field Experiments: Design, Analysis, and Interpretation (W.W. Norton, 2012). Acknowledgments: We thank seminar audiences at Princeton University, NYU, the University of Michigan, the University of Texas, Vanderbilt University, Stanford University, and Columbia University—and particularly D. Green—for helpful feedback and engagement. Funding: We are grateful for financial support from the Andrew F. Carnegie Foundation and from J. G. Geer of Vanderbilt University. Shawn Patterson provided excellent research assistance throughout. J. Williams at YouGov helped make this project a success. A.C. acknowledges the support of the UCLA Marvin Hoffenberg Fellowship. The data collection and research presented in this manuscript was funded, in part, by The Andrew F. Carnegie Corporation as part of an Andrew F. Carnegie Fellowship in the Humanities and Social Sciences awarded to L.V. in 2015, and by UCLA’s Marvin Hoffenberg Chair in American Politics and Public Policy. Portions of the data collection were supported by J. G. Geer, Dean of the College of Arts and Sciences, Vanderbilt University. Author contributions: Funding for the project was secured by L.V. L.V. and S.J.H. designed the project. L.V. contracted, fielded, and executed the data collection and weekly experiments. A.C. performed data analyses and produced visualizations. All authors participated in writing and editing the manuscript. Competing interests: The authors declare that they have no competing interests. Data and materials availability: All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. Additional data related to this paper may be requested from the authors. The replication archive for this study is available from the Harvard Dataverse (DOI https:// dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TN7KWR). REFERENCES AND NOTES 1. C. I. Hovland, I. L. Janis, H. H. Kelley, Communication and Persuasion (Yale Univ. Press, 1953). 2. M. Fishbein, I. Ajzen, Attitudes and opinions. Annu. Rev. Psychol. 23, 487–544 (1972). 3. R. E. Petty, J. T. Cacioppo, Communication and Persuasion: The Elaboration Likelihood Model of Persuasion (Springer, 1986). 4. R. B. Cialdini, Influence: Science and Practice (Pearson, ed. 5, 2009). 5. A. Tversky, D. Kahneman, Loss aversion in riskless choice: A reference-dependent model. Q. J. Econ. 106, 1039–1061 (1991). 6. R. Deshpandé, D. M. Stayman, A tale of two cities: Distinctiveness theory and advertising effectiveness. J. Mark. Res. 31, 57–64 (1994). 7. M. Friestad, P. Wright, The persuasion knowledge model: How people cope with persuasion attempts. J. Consum. Res. 21, 1–31 (1994). 8. E. J. Wilson, D. L. Sherrell, Source effects in communication and persuasion research: A meta-analysis of effect size. J. Acad. Mark. Sci. 21, 101–112 (1993). 9. G. Szulanski, Exploring internal stickiness: Impediments to the transfer of best practice within the firm. Strateg. Manag. J. 17, 27–43 (1996). 10. E. Kamenica, M. Gentzkow, Bayesian persuasion. Am. Econ. Rev. 101, 2590–2615 (2011). 11. K. F. Kahn, J. Geer, Creating impressions: An experimental investigation of political advertising on television. Polit. Behav. 16, 93–116 (1994). 12. T. M. Holbrook, Do Campaigns Matter? (Sage Publications, Inc., 1996). 13. S. E. Finkel, J. G. Geer, A spot check: Casting doubt on the demobilizing effect of attack advertising. Am. J. Polit. Sci. 42, 573–595 (1998). 14. D. R. Shaw, The effect of tv ads and candidate appearances on statewide presidential votes, 1988-96. Am. Polit. Sci. Rev. 93, 345–361 (1999). 15. R. Johnston, M. G. Hagen, K. H. Jamieson, The 2000 Presidential Election and the Foundations of Party Politics (Cambridge Univ. Press, 2004). 16. D. R. Shaw, The Race to 270: The Electoral College and the Campagin Strategies of 2000 and 2004 (University of Chicago Press, 2008). Submitted 22 April 2020 Accepted 21 July 2020 Published 2 September 2020 10.1126/sciadv.abc4046 Citation: A. Coppock, S. J. Hill, L. Vavreck, The small effects of political advertising are small regardless of context, message, sender, or receiver: Evidence from 59 real-time randomized experiments. Sci. Adv. 6, eabc4046 (2020). Parsimonious data: How a single Facebook like predicts voting behavior in multiparty systems Author: Kristensen, Jakob Bæk, Date: 2017-9-20 Collections: PoliticalML Zotero Key: ZW4MHLMQ Cite Key: Kristensen17facebookPostPredVote Zotero Item | Lit Note a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 OPEN ACCESS Citation: Kristensen JB, Albrechtsen T, Dahl- Nielsen E, Jensen M, Skovrind M, Bornakke T (2017) Parsimonious data: How a single Facebook like predicts voting behavior in multiparty systems. PLoS ONE 12(9): e0184562. https://doi.org/ 10.1371/journal.pone.0184562 Editor: Lidia Adriana Braunstein, Universidad Nacional de Mar del Plata, ARGENTINA Received: April 25, 2017 Accepted: August 25, 2017 Published: September 20, 2017 Copyright: © 2017 Kristensen et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability Statement: All relevant data are within the paper and its Supporting Information files. Funding: T.B was supported by a grant from the KU16 funding of Copenhagen University, J.B.K from University of Canterbury, UC Doctoral Scholarship. T.A., E.D, M.J. and M.S did not receive specific funding for this work but were allowed to participate in the process during work hour. RESEARCH ARTICLE Parsimonious data: How a single Facebook like predicts voting behavior in multiparty systems Jakob Bæk Kristensen[1]*, Thomas Albrechtsen[2], Emil Dahl-Nielsen[2], Michael Jensen[2], Magnus Skovrind [3], Tobias Bornakke[4] 1 School of Social and Political Sciences, University of Canterbury, Christchurch, New Zealand, 2 Department of Research, Nextwork A/S, Copenhagen, Denmark, 3 Department of Research, Analyse & Tal F.M.B.A, Copenhagen, Denmark, 4 Department of Sociology, University of Copenhagen, Copenhagen, Denmark • jakob.kristensen@pg.canterbury.ac.nz Abstract This study shows how liking politicians’ public Facebook posts can be used as an accurate measure for predicting present-day voter intention in a multiparty system. We highlight that a few, but selective digital traces produce prediction accuracies that are on par or even greater than most current approaches based upon bigger and broader datasets. Combining the online and offline, we connect a subsample of surveyed respondents to their public Facebook activity and apply machine learning classifiers to explore the link between their political liking behaviour and actual voting intention. Through this work, we show that even a single selective Facebook like can reveal as much about political voter intention as hundreds of heterogeneous likes. Further, by including the entire political like history of the respondents, our model reaches prediction accuracies above previous multiparty studies (60–70%). The main contribution of this paper is to show how public like-activity on Facebook allows political profiling of individual users in a multiparty system with accuracies above previous studies. Beside increased accuracies, the paper shows how such parsimonious measures allows us to generalize our findings to the entire population of a country and even across national borders, to other political multiparty systems. The approach in this study relies on data that are publicly available, and the simple setup we propose can with some limitations, be generalized to millions of users in other multiparty systems. Introduction The representative opinion survey has long been the pinnacle of empirical research in political science [1,2]. The recent immense growth in digital platforms has provided researchers with the possibility of studying human behaviour on a whole new scale from traces left behind by our digital interactions [3]. From being limited to surveys with a couple of thousand respondents, political studies covering millions of people have emerged within the field of |a1111111111 a1111111111 a1111111111|Col2| |---|---| Competing interests: As one data source among many, T.A., E.D, M.J. and M.S. make use of political likes in their work as strategic consultants. T.B and J.B.K have no competing interest. computational social science, generating important new knowledge about our digital and analogue lives. Within the subfield of election forecasting, scholars have shown the potential for predicting election outcomes based on digital data from a diverse range of platforms including YouTube [4], Google [5], Twitter [6,7], Facebook [8,9], and even Wikipedia [10]. Studies based on the big social media platforms, that is, Facebook and Twitter, have largely been the most successful with prediction rates that in accuracy and scale often have outperformed traditional pooling 4 . While this emerging field has mainly focused on predicting aggregated electoral results [12], a smaller group of studies has focused on the challenge of predicting individual political orientation [9,13–19]. Notably, Ceron et al. [11] were able to reach very high accuracies in their political profiling using only Twitter data, just as David et al. [18] displayed how political orientation can be determined by comparing individuals’ writing style with the writings on politicians’ public Facebook profiles. While many of these studies attain high prediction accuracies, this accuracy is often reached by limiting the study to the most active users [17]. Further, few studies validate their results against offline data such as surveys. These limitations have, however, been tackled in the work of Kosinski and colleagues who, in two papers ranked among the top 10 most influential papers in the history of PNAS (As calculated by Altmetric: https://pnas.altmetric.com/details/3058702 and https://pnas.altmetric.com/ details/1294474), have shown how our personality and political attitudes can be predicted with great accuracy based solely on Facebook likes [13,14]. Applying machine learning algorithms to search for patterns in hundreds of diverse Facebook likes, these already famous experiments have thus disclosed how people’s preferences for Hallo Kitty and Harley Davidson can reveal details about their personality and political attitudes—often with better precision than their friends or family. Thus far, the majority of studies predicting individual voting behaviour based on digital traces have focused on two-party systems or applied a left/right-wing scale, thereby avoiding the more challenging task of making all-inclusive predictions in multiparty settings, the principal political system of our time [18]. In this paper, we fill this gap by studying how individual party choice in a multiparty system is linked to liking posts made by political actors on Facebook. We base our prediction on likes for posts on public pages of Danish parties and politicians collected between January 2015 and 2017 through the Facebook Graph API. Through machine learning–based prediction models, we test how ‘political likes’, consisting of likes on posts created by politicians and parties, are able to predict present-day voter intention in a multiparty system for a subsample of surveyed respondents. The main contribution of this paper is to show how public Facebook activity, even within a challenging multiparty system, can be effectively used to predict an individual’s present-day voter intention. Based on the simple measure of political likes, our models reach a prediction accuracy of between 60% and 70%, which are above any previous multiparty studies. Also, we show how even a single selective Facebook post-like can reveal as much about our presentday voter intention as hundreds of diverse likes drawn from our profile. In doing this we wish to challenge the current trend toward broader and bigger data running through the majority of studies within computational social science. By exploring the parsimonious measure of political likes, we make the point that though likes pertaining to everything from reality stars to soil types can accurately predict personal traits, more accurate and generalizable results in the case of present-day voter intention will tend to follow from a more parsimonious data strategy. Materials and methods Data We base our prediction on likes entered in page posts by Danish parties and politicians collected from January 2015 to 2017 through the public Facebook Graph API. Likes are a generic mechanism used by Facebook users to express their support of content that has already shown to be a good proxy for predicting both electoral results and personal traits [8,13,14], making it an immediate choice for exploring the possibility for predicting political orientation. For use in the final analysis, we only include respondents who were able, and willing, to share their public Facebook ID (N = 1216). We also limit our analysis to respondents who had liked political actors during the period and would vote for any of the nine parties currently in parliament (N = 659). The final sample is slightly smaller (~23%) than what we theoretically would expect given our database of political likes containing data from 1.3 million Danes. We ascribe most of this dropout to privacy concerns (S1 Table). As a result of this dropout, representativeness of the data sample becomes slightly distorted (see non-response analysis in S2 Appendix). However, for the most part, the distortion simply reproduces Facebook’s already skewed user groups with the only large bias being an underrepresentation of older users; a skew that was recently shown to have limited effect on how often a person would like political actors [20]. The data process is illustrated in Fig 1. Method We test post-likes against a baseline model developed from survey data of sociodemographics, political values, and opinions on political issues developed based on current “best practice” within political science (see survey features in S5 Table). Moving on from the baseline model, we gradually compare a selection of multinomial logistic regression models, all predicting which party a person would vote for but modeled on different selections of Facebook data as well as combinations of Facebook and survey data. Using L1 regularization LASSO [21–23], Fig 1. Data process. The process in three parts (1) A representative survey was completed by 3050 randomly selected people living in Denmark, providing information on standard sociodemographic qualities, political values, and present-day voter intention toward parties eligible in the general election. As shown in S3 Table, the sample is somewhat demographically representative of the country’s entire population. Respondents were subsequently asked to log in with their Facebook account, and if willing to accept the same, respondents’ public Facebook ID was stored. (2) Post-likes were independently collected from all public profiles of Danish parties and politicians on Facebook. (3) After completion of steps 1 and 2, we linked each respondent to the collected Facebook data and applied a LASSO-based multinomial logistic regression model to predict voter intention based on Facebook data. https://doi.org/10.1371/journal.pone.0184562.g001 only features that contribute significantly to the overall prediction are included in the models. In each model an L1 penalty was selected using 10-fold cross validation to avoid overfitting and account for variance in the prediction accuracy. Ethics statement The survey was conducted by Userneeds, which is a professional European Marketing and Social Research company. Participants’ personal data is protected by Userneed’s privacy policy, which is in accordance with ESOMAR guidelines. Written consent was obtained specif- ically for this project and every participant was informed of the purpose of the study. Authors on this project only received fully anonymised data from Userneeds. The Human Ethics Committee at the University of Canterbury, New Zealand stated that formal committee approval was not necessary since researchers only had access to the anonymised data. Data from Facebook used in this study was collected only from fully public repositories available through Facebook’s Public Graph API and in accordance with their Terms and Policies. Results The results depict how different uses of “political likes” are able to predict which of the nine parties in the Danish parliament a given person would vote for. The significance of the results is held against a null hypothesis that denotes no relationship between present-day voter intention and explanatory variables (H0: P = 1/9). The results are found in Fig 2. Establishing a baseline from sociodemographics, political values, and opinions We initiate our analysis establishing a baseline model based on sociodemographic variables, political values, and opinions toward current issues collected through survey questions. The questions were selected to mirror the most typical variables for explaining voter alignment within the discipline of political science [24]. The baseline model (model 0) includes 19 different features. Note however, that several coefficients are neutralized by L1 regularization, which is implemented in LASSO regression to prevent model overfitting. The optimal model makes predictions with 35.8% accuracy (confidence interval [CI] of 2.9%) including 101 out of 668 coefficients. This echoes the accuracies of similar survey studies within political science, on average reaching an accuracy of approximately 35% (e.g. [24–26]). For comparison reasons, we calculate the same model’s accuracy for predicting present-day voting intention on a right versus left scale. Not surprisingly the accuracy is much higher when using this binary classification (80.3% accuracy). The power of a single political like With an established baseline model, we turn toward our collected Facebook data. As an initial experiment, we create a model that uses just a single feature, the latest like that the respondent has entered to a post by a party or politician. This very simple setup (model 1) is more accurate and, on average, marginally better than our baseline model. With an accuracy of 43.9% (CI ±3.8%) and a right/left accuracy of 81.3%, model 1 indicates that a person’s single latest political post-like tends to say more about party choice than a prediction model trained on a sample with 19 different features on each person, including questions on core political values. Fig 2. Multinominal logistic regression models predicting present-day voting intention. https://doi.org/10.1371/journal.pone.0184562.g002 Raising accuracy by including individuals’ entire political-like history We now include all political likes for each person collected during the two-year period (model 2). The features in this model consist of the number of posts that a person has liked for each of the nine parties in parliament. For example, if a respondent has liked a post made by a party or a politician from that party on their public Facebook page, then that counts as one like to that party for that respondent. To compare respondents who are extremely active on public pages with those who are less active, all values are normalized across each respondent’s likes toward each of the nine parties. Applying these features, we predict which party a person would vote for with an accuracy of 60.9% (CI ± 3.7%). This result is notably better than both the baseline model and model 1. Interestingly, the best L1 penalty in model 2 was 0.0, meaning that excluding coefficients would not increase the cross-validated accuracy. With a right/left average accuracy climbing to 90.1%, the model suggests political likes as an efficient predictor for voter intention. Combining survey and political likes only minutely increases prediction rate We now consider the possibility of a positive complementary effect by combining the best from two worlds. We add in the features of the baseline model to model 2 in order to explore whether the survey questions drawn from political science literature encapsulate other dimensions than the political likes: Do the two approaches overlap or complement each other? The new model, model 3, hereby includes all the sociodemographic background information, core political values from the baseline model, and the entire political-like history from model 2. The prediction accuracy is now 62.0% with (CI ± 3.7%). This is higher than model 2, but still within the margin of error. The increase in area under the (receiver operating characteristic, or ROC) curve (AUC) and in right/left accuracy, however, suggests that the model is still only slightly better than model 2. The sample size in model 3 is lower than the number of coefficients, which is one probable explanation for why the added data does not deliver a significant increase in accuracy. Even though L1 regularization filters out most of the unnecessary noise, it is conceivable that the regression algorithm would perform much better with this selection of features if the sample size could correspondingly be raised. Optimizing political-likes prediction rates with minimum-like criteria The previous models propose political post-likes as the single strongest variable for predicting individual party choice. It is therefore reasonable to consider whether we can further optimize the use of this variable. Since we normalize the values for number of posts liked across each of the nine parties for each respondent, our models might make overconfident predictions based on respondents who have only liked a single political post. Similarly, a person for whom 90% of her likes go to the same party should yield better predictions than a person whose likes have been evenly distributed across four parties. We explore the relationship between these two criteria, namely (1) minimum likes, excluding respondents with less total likes than the threshold, and (2) party like cap, excluding respondents with a lower percentage of likes directed toward a single party than the threshold. The results can be seen in Fig 3 and Table 1 provides the values corresponding to the figure. Fig 3 shows how the accuracy increases with both min likes and party like cap indicating that respondents with many likes distributed to one or few parties yield the most accurate predictions. With, for example, min likes = 7 and party like cap = 0.8, prediction accuracy goes above 90%; however, sample size is down to 153, which also considerably increases the error rate (see Table 1). Thresholding total likes greatly increases accuracy Most importantly, Fig 3 shows that overall prediction rates, when thresholding individuals on their total likes, begin to converge significantly with a total minimum of 7 political likes. Setting the minimum likes criterion higher than 7 results in only a little gain in total accuracy, but considerably reduces sample size. We therefore interpret a threshold of political likes at 7 as the best choice for a near optimal prediction rate. Based on the optimization exploration, we deploy a fourth and final model that has the same features as model 2, but only includes respondents with a total of 7 or more likes for posts from parties or politicians. The effective sample size is now 468 while prediction accuracy has increased to 70.8% (CI = ±4.2%). It is indicative of better prediction rates by imposing a criterion for how many total political likes a user should have. Accuracy for right/left is now 96%. Discussion The main implication of our results is the potential for studying political behavior in multiparty systems on social media on a large scale and in near real time. The profiling of individual users through their political-like history thus lends itself as a tool to study political Fig 3. Accuracy at min likes levels. The x-axis shows party-like cap (PLC), which denotes how many likes in percentages that at the least go toward only a single party, meaning that at PLC = 0.8, only users who have at least 80% likes toward a single party are included. The y-axis shows the percentage of users who are accurately labeled. Each colored line shows accuracy for samples where all respondents have a minimum of total likes. Because the two criteria, party-like cap and minimum likes, involve filtering out respondents and thus effectively cutting down the sample size, it is unfeasible to rely on the training of machine learning algorithms for classification. Consequently, we made a simple algorithm that derives predictions based on the party a respondent has liked the most at different intersections of the two criteria. https://doi.org/10.1371/journal.pone.0184562.g003 participation on social media. Through collecting political likes, we become able to profile approximately 1.3 million Danes—23% of the entire population—with a least one political like, and 1 million with at least seven. By filtering posts based on political segments defined by millions of likes, the approach offers scholars, practitioners, and politicians a view into democratic voters’ political dreams and the issues they engage in—all at a scale hitherto unknown to the discipline of political science. The approach could potentially add political significance to studies of trends and cluster formation in news consumption on Facebook as recently brought forth by Schmidt et al. [27]. Fig 4 constitutes a preeliminary experiment of studying a specific segment’s interest for particular political topics over time. Table 1. Prediction rates and sample sizes at different party-like caps with min likes = 7 (p < 0.001). Party-like cap 0.0 0.5 0.7 0.8 0.9 Sample size 468 328 197 153 97 95% Confidence interval (CI) 0.046 0.05 0.058 0.059 0.062 Accuracy 0.64 0.777 0.861 0.912 0.93 https://doi.org/10.1371/journal.pone.0184562.t001 |Table 1. Prediction rates and sample sizes|s at different party-lik|ke caps with min likes|= 7 (p < 0.001).|Col5|Col6| |---|---|---|---|---|---| |Party-like cap|0.0|0.5|0.7|0.8|0.9| |Sample size|468|328|197|153|97| |95% Confidence interval (CI)|0.046|0.05|0.058|0.059|0.062| |Accuracy|0.64|0.777|0.861|0.912|0.93| Fig 4. Combining trending news topics with users’ voter intention. Example showing trending topics for a group of users aligned with a specific party through their likes. https://doi.org/10.1371/journal.pone.0184562.g004 The technique is limited to the 23% of Danes who have liked political actors within the 2-year timeframe of the study. Due to the non-random subsampling in this study, which only considers respondents who are active Facebook users, the national population is not perfectly represented. Women, younger people and people with higher education are overrepresented in the samples used for the regression models (see S1 Appendix). However, we do not feel that any one group is totally left out or overrepresented to a degree that calls the overall results into question. Our own future studies, that is, predicting the aggregate electoral outcome based solely on political likes, however, are comparable to most opinion polls suggesting that our results can potentially be generalized to the entire Danish population (details in S5 Appendix). Further, tentative explorations of neighboring multiparty countries (Sweden, Germany, and Norway) show no indications that either the amount, or the usage of political likes should vary radically across national borders. Generalizing our findings outside the group of political likers, one should, however, be attentive of the bias inherent to Facebook as mentioned earlier. There are clear similarities between the our sample demographics and those found by Facebook’s own Audience Insights (https://www.facebook.com/ads/audience-insights/people?act= 41292822&age=18-&country=DK). Further, one should also expect political likers to be slightly more political active than the rest of the population [20]. With these limitations in mind, and in accordance with studies of other social media platforms [11], we estimate that the general mechanism of political likes would be reproducible in most open Western multiparty democracies where Facebook has become a central political arena. Toward parsimonious data Searching for patterns within big datasets of diverse traces is not limited to Kosinski and colleagues’ work. Rather, the trend toward bigger and broader datasets seems to have become the standard for data experiments in the field of computational social science (e.g., [28]). Increasingly, this ideal has also appeared in commercial data analysis as illustrated by Cambridge Analytica, a data analytics firm drawing on Kosinsky et al.’s [13] work, when it proclaimed to have secured Donald Trump’s victory through the collection of “4–5,000 data points on every American” [29]. We share our colleagues’ fascination with the many stories about human behavior that these broad datasets can tell us. However, we argue that the field of computational social science by now has reached a level of maturity that makes it timely to replace the ideal of broad data with a parsimonious ideal of selective data. While the studies by Kosinski and colleagues should not be compared 1:1 (due to differences in goals and context), it seems fair to note how using only the respondents’ single latest political like delivers performance comparable to their best prediction of political attitude built on hundreds of likes 13,14 . [30] Also reach a result of AUC = 0.8 using the same dataset as [13]. Here we are reporting the left-right AUC of our study in order to make our results comparable. Future studies will reveal other areas to which parsimonious data strategy can be applied. Still, if our preference for “Hello Kitty” and “Harley Davidson” can accurately reveal our personal traits, then what greater expectations should we have of future predictions built on selective data linked to specific traits of interest? Why liking predicts voting: Contours of a theory Accuracy with generalizability is the main advantage of our parsimonious data strategy. Based solely on this limited data scope, consisting of the single latest like per respondent, we were able to predict multiparty choice with an accuracy of 0.439. The accuracy was lifted above 0.6 by including all likes, and then above 0.7 by imposing a minimum like criteria of 7 likes. Our results thus indicate that even a single political like is comparable in accuracy to most multiparty studies in political science, commonly reaching accuracy of around 35%, by combining survey questions on sociodemographics, political values, and opinions toward current issues (e.g., [24–26]). While this line of research is not entirely comparable, with political scientists typically searching for explanation rather than prediction, the predictive power of political likes becomes striking when contemplating the approximately 30 survey questions involved in reaching 35% accuracy. Given this background, it seems reasonable to consider why likes predict voting behavior dramatically better than survey questions: what makes a like predictive of our political behavior in the first place? Referencing major theories in studies of voting behavior, one could suggest that a like is predictive because it reveals alignment with the ideology of the liked party [1], the issue taken up [31,32], or the personal traits of the party candidate [33,34]. Our response to this question is to re-articulate an often-used designation: that likes comprise a generic mechanism for users to show their support. Political likes should be seen as a measure that captures a multitude of the abovementioned—and probably also other—theories for why we vote (i.e., ideology, shared issue, or personal identification). This response is in line with both the overall high accuracies reached, which makes it difficult to imagine a single theoretical driver, and with the lack of complementary effects seen in model 3, which suggest that we should view likes as encapsulating a number of different motives and preferences. The high accuracies and lack of complementary effects also indicate that most people are highly selective with their political likes. We should thus not think of political likes as a costfree interaction that we carelessly direct toward any post that catches our attention, but rather as an interaction form that we apply when we are clearly aligned across one or even multiple axes of preferences. As such, political likes should be seen as a parsimonious measure that condenses a heterogeneous mixture of different motives and individuals’ inscription into politics. Supporting information S1 Appendix. Data collection. (PDF) S2 Appendix. Non-response analysis. (PDF) S3 Appendix. Post-likes normalization. (PDF) S4 Appendix. Regression models. (PDF) S5 Appendix. Election prediction. (PDF) S1 Table. Data filtering and sample sizes. (PDF) S2 Table. Population and sample distributions for base demographics. (PDF) S3 Table. Population and sample distributions compared. (PDF) S4 Table. Results of non-response chi-squared permutation tests. (PDF) S5 Table. Baseline model: Predictive strength of individual features. (PDF) S1 Fig. Log of post-likes per user in main sample used in models (N = 659). (PDF) S2 Fig. Post-likes normalization procedure. (PDF) S3 Fig. Results of election prediction tests. (TIF) S1 Dataset. Anonymised survey responses and corresponding public Facebook actions. (ZIP) Acknowledgments We thank Sune Lehmann, Samuel Roberts, Anders Blok, Michael Bossetta, Vedran Sekera, Piotr Sapiezynski, Enys Mones, and Snorre Ralund for their critical reading of the manuscript. We also wish to acknowledge Nextwork A/S and Analyse & Tal F.M.B.A., for allowing T.A., E. D, M.J., and M.S. to participate in the process during work hours and for lending us access to their database. Author Contributions Conceptualization: Jakob Bæk Kristensen, Thomas Albrechtsen, Tobias Bornakke. Data curation: Jakob Bæk Kristensen, Michael Jensen, Tobias Bornakke. Formal analysis: Jakob Bæk Kristensen. Funding acquisition: Thomas Albrechtsen. Investigation: Jakob Bæk Kristensen, Thomas Albrechtsen, Emil Dahl-Nielsen, Michael Jen- sen, Magnus Skovrind, Tobias Bornakke. Methodology: Jakob Bæk Kristensen, Tobias Bornakke. Project administration: Jakob Bæk Kristensen, Thomas Albrechtsen, Tobias Bornakke. Software: Jakob Bæk Kristensen. Supervision: Thomas Albrechtsen, Tobias Bornakke. Validation: Jakob Bæk Kristensen. Visualization: Jakob Bæk Kristensen. Writing – original draft: Jakob Bæk Kristensen, Tobias Bornakke. Writing – review & editing: Jakob Bæk Kristensen, Michael Jensen, Magnus Skovrind, Tobias Bornakke. References 1. Campbell A. The american voter. University of Chicago Press; 1960. 2. Verba S, Nie NH. Participation in America: political democracy and social equality. University of Chicago Press ed. Chicago: University of Chicago Press; 1987. 3. Dalton RJ. The potential of big data for the cross-national study of political behavior. Int J Sociol. 2016; 46: 8–20. 4. Franch F. (Wisdom of the Crowds)2: 2010 UK Election Prediction with Social Media. J Inf Technol Polit. 2013; 10: 57–71. https://doi.org/10.1080/19331681.2012.705080 5. Mavragani A, Tsagarakis KP. YES or NO: Predicting the 2015 GReferendum results using Google Trends. Technol Forecast Soc Change. 2016; 109: 1–5. 6. Ceron A, Curini L, Iacus SM. iSA: A fast, scalable and accurate algorithm for sentiment analysis of social media content. Inf Sci. 2016; 367–368: 105–124. https://doi.org/10.1016/j.ins.2016.05.052 7. Jungherr A. Analyzing political communication with digital trace data the role of Twitter messages in social science sesearch. Cham: Springer Internat. Publishing; 2015. 8. Giglietto F. If likes were votes: An empirical study on the 2011 italian administrative elections. 2012; https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1982736 9. Barclay FP, Pichandy C, Venkat A, Sudhakaran S. India 2014: Facebook “Like” as a Predictor of Election Outcomes. Asian J Polit Sci. 2015; 23: 134–160. https://doi.org/10.1080/02185377.2015.1020319 10. Yasseri T, Bright J. Wikipedia traffic data and electoral prediction: towards theoretically informed models. EPJ Data Sci. 2016; 5: 1–15. 11. Ceron A, Curini L, Iacus S. Using social media to fore-cast electoral results: A review of state-of-the-art. Ital J Appl Stat. 2015; 25: 237–259. 12. Makazhanov A, Rafiei D, Waqar M. Predicting political preference of Twitter users. Soc Netw Anal Min. 2014; 4: 1–15. 13. Kosinski M, Stillwell D, Graepel T. Private traits and attributes are predictable from digital records of human behavior. Proc Natl Acad Sci. 2013; 110: 5802–5805. https://doi.org/10.1073/pnas.1218772110 PMID: 23479631 14. Youyou W, Kosinski M, Stillwell D. Computer-based personality judgments are more accurate than those made by humans. Proc Natl Acad Sci. 2015; 112: 1036–1040. https://doi.org/10.1073/pnas. 1418680112 PMID: 25583507 15. Quercia D, Kosinski M, Stillwell D, Crowcroft J. Our twitter profiles, our selves: Predicting personality with twitter. Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on. IEEE; 2011. pp. 180– 185. http://ieeexplore.ieee.org/abstract/document/6113111/ 16. Adalı S, Golbeck J. Predicting personality with social behavior: a comparative study. Soc Netw Anal Min. 2014; 4: 1–20. 17. Volkova S, Bachrach Y, Armstrong M, Sharma V. Inferring Latent User Properties from Texts Published in Social Media. AAAI. 2015. pp. 4296–4297. https://www.microsoft.com/en-us/research/wp-content/ uploads/2016/02/InferringUserTraitsFromSocialMedia.pdf 18. David E, Zhitomirsky-Geffet M, Koppel M, Uzan H. Utilizing Facebook pages of the political parties to automatically predict the political orientation of Facebook users. Online Inf Rev. 2016; 40: 610–623. https://doi.org/10.1108/OIR-09-2015-0308 19. Sudhahar S, Cristianini N, Veltri G. Automated analysis of the US presidential elections using Big Data and network analysis. Big Data Soc 21 2015 2053951715572916. 2015; 20. Kalsnes B, Larsson AO, Enli GS. The social media logic of political interaction: Exploring citizens’ and politicians’ relationship on Facebook and Twitter. First Monday. 2017;22. http://ojs-prod-lib.cc.uic.edu/ ojs/index.php/fm/article/view/6348 21. Tibshirani R. Regression shrinkage and selection via the lasso. J R Stat Soc Ser B Methodol. 1996; 267–288. 22. Friedman Jerome, Hastie Trevor, Tibshirani Rob. Regularization paths for generalized linear models via coordinate descent. J Stat Softw. 2010; 33. PMID: 20808728 23. Zou H. The Adaptive Lasso and Its Oracle Properties. J Am Stat Assoc. 2006; 101: 1418–1429. https:// doi.org/10.1198/016214506000000735 24. Stubager R, Hansen KM, Goul Andersen J. Krisevalg: økonomien og folketingsvalget 2011. København: Jurist- og Økonomforbundets Forlag; 2013. 25. Clarke HD, Sanders D, Stewart MC, Whiteley PF. Performance Politics and the British Voter. Cambridge: Cambridge University Press; 2009. 26. Merrill S, Grofman B. A unified theory of voting: directional and proximity spatial models. Cambridge, UK; New York: Cambridge University Press; 1999. 27. Schmidt AL, Zollo F, Del Vicario M, Bessi A, Scala A, Caldarelli G, et al. Anatomy of news consumption on Facebook. Proc Natl Acad Sci. 2017; 114: 3035–3039. https://doi.org/10.1073/pnas.1617052114 PMID: 28265082 28. Watts DJ. Computational Social Science: Exciting Progress and Future Directions. The Bridge. 201343. https://www.nae.edu/Publications/Bridge/106112/106118.aspx?utm_content=buffer9a463&utm_ medium=social&utm_source=twitter.com&utm_campaign=buffer 29. Tett G. Donald Trump’s campaign shifted odds by making big data personal. The Financial Times. 26 Jan 2017. https://www.ft.com/content/bee3298c-e304-11e6-9645-c9357a75844a. Accessed 17 Mar 2017. 30. Theodoridis, Thomas, Symeon Papadopoulos, Yiannis Kompatsiaris. Assessing the reliability of facebook user profiling. Proc 24th Int Conf World Wide Web ACM 2015. 2015; 31. Nie NH, Verba S, Petrocik JR. The changing American voter. Bridgewater, NJ: Replica Books; 1999. 32. Petrocik JR. Issue Ownership in Presidential Elections, with a 1980 Case Study. Am J Polit Sci. 1996; 40: 825–850. https://doi.org/10.2307/2111797 33. Kinder DR, Peters MD, Abelson RP, Fiske ST. Presidential Prototypes. Polit Behav. 1980; 2: 315–337. 34. Ohr D, Oscarsson H. Leader Traits, Leader Image, and Vote Choice. In: Aarts K, Blais A, Schmitt H, editors. Political Leaders and Democratic Elections. Oxford University Press; 2013. pp. 187–214. https:// doi.org/10.1093/acprof:osobl/9780199259007.003.0011 Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors: implications for Governance Author: Palmer, Alexis, Date: 2023-09-02 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: T56CPQY3 Cite Key: Palmer23llmsArgueConvincing Zotero Item | Lit Note Large Language Models Can Argue in Convincing Ways About Politics, But Humans Dislike AI Authors: Implications for Governance Alexis Palmer[] Arthur Spirling[α] March 25, 2024 Abstract All politics relies on rhetorical appeals, and the ability to make arguments is considered perhaps uniquely human. But as recent times have seen successful large language model (LLM) applications to similar endeavors, we explore whether these approaches can out-compete humans in making appeals for/against various positions in US politics. We curate responses from crowdsourced workers and an LLM and place them in competition with one another. Human (crowd) judges make decisions about the relative strength of their (human v machine) efforts. We have several empirical “possibility” results. First, LLMs can produce novel arguments that convince independent judges at least on a par with human efforts. Yet when informed about an orator’s true identity, judges show a preference for human over LLM arguments. This may suggest voters view such models as potentially dangerous; we think politicians should be aware of related “liar’s dividend” concerns. Introduction  Department of Politics, New York University, New York, NY 10012. α Department of Politics, Princeton University, Princeton, NJ 08544. 1 What persuades an audience to accept a particular argument may be the oldest and most studied political science question of all. And Aristotle’s Rhetoric arguably remains the standard for understanding this process. In that account, speakers have three resources to convince their listeners: the speaker’s own personal character (ethos), the emotional feelings of their audience (pathos) and the quality of the logic in the argument itself (logos). Perhaps the most obvious example of these concepts is when politicians compete for votes by debating in front of the electorate, but we often see leaders convincing citizens to do other things. These include living healthier lives or signing up to new policy schemes. A natural assumption historically is that the entity making the argument is human; however, recent technical advances means that this need not be the case. That is, we now have access to generative “large language models” (LLMs) that allow computers to produce human-like text in response to user prompts (see e.g. Dai and Radford, 2023; Halterman, 2022 for recent political science applications). For social scientists interested in persuasion, a fundamental question is whether these machines can out-perform Aristotle’s “political animal” (i.e. mankind) in their rhetorical interactions with other humans. This matters for several reasons. First, because it teaches us something inherently interesting about arguments—what works and what doesn’t. And because these machines may then be a useful tool in making the public case for a position. Indeed, scholars in the discipline are already applying related ‘chat’ technologies to potentially alter citizen perceptions of interventions (e.g. Rosenzweig and Offer-Westort, 2022). Second, it matters because LLMs used in this way may pose a threat to democracy per se. Social scientists have written extensively on the possibility that new generative technologies could lead to a “liar’s dividend” (Chesney and Citron, 2019), by which voters cannot tell whether the messages they receive are true data about the world or misleading misinformation. This potentially helps those actively seeking to destabilize polities and may be bad for accountability more generally (Schiff, Schiff and Bueno, 2022). The historical 2 focus in that literature has been on “deep fake” images, audio and video, but obviously it could apply to the text product of an LLM too. In this scenario, LLMs may be a way to flood the discourse with “fake rhetoric” (rather than “fake news” in the sense of Lazer et al, 2018), thereby confusing citizens as to the genuine empirical or ideological arguments for policy. This could conceivably lead to bad actors convincing voters to do things against their own interests, or to do things that harm more vulnerable members of society. This is all the more true if citizens show no inherent preference for human generated reasoning, assuming they ever discover the genesis of the arguments they read. For these reasons, we ask not merely whether LLMs can construct an appealing argument in terms of content (logos), but also how an audience responds to their ethos—that is, the knowledge that the orator is a machine rather than a human. In this way we connect longstanding questions of political philosophy to those of political science, via the methods of computer science. We use an open source LLM—the Meta OPT-30B model (Zhang et al., 2022)—and prompt it to make arguments for and against common positions in contemporary US politics and society. [1 ]These same argument prompts are then given to humans, specifically large numbers of crowd workers. The pairs of responses (one human, one LLM) are then shown to a set of independent human judges. Those judges must decide whether the machine or human argument for a position is the more convincing. To be clear, we curate both (human and LLM) sets of responses to ensure the contests are between the “best” quality outputs. In that sense, our headline findings are “possibility” results. Importantly for assessing causal claims about ethos, we randomize whether crowd respondents are informed about the identity—machine or human—of the argument producer. That is, in some cases respondents are aware which position statement was produced by the LLM, and in some cases they are not. 1 See e.g. Spirling (2023) for discussion as to why open source LLMs may be generally preferable to proprietary efforts. 3 Our findings are first, that LLMs are capable of producing human-style arguments for different positions on subjects as varied as abortion, guns, immigration, and organ donation. In terms of convincing human judges, they can out-perform human authors, though this varies by topic. Second, we show that the structure and style of LLM arguments differs from those offered by humans. Specifically, LLM arguments tend to have lower levels of reading difficulty and differ somewhat in substance from human positions. Finally, when informed of orator identify, human judges show a small but statistically significant preference for human producers for arguments—though this is partly driven by certain issues. In terms of our concerns about democratic health, the results are mixed. On the one hand, we anticipate that producing compelling arguments for propositions---whatever their welfare effects---will be straightforward for LLMs. That may be good or bad depending on the use-case. That is, it may yield healthier, happier citizens who make better choices after being convinced on the merits, or disengaged cynics who trust little of what they read. On the other hand, informed citizens are somewhat wary of machine output per se, which may encourage politicians to steer clear of using LLM generated arguments for their own personal purposes, or to regulate their use in politics more generally. Results Our first goal is to assess whether and to what extent LLMs can make arguments—and how well they can do this relative to humans. We begin by demarcating the five issue positions for which the arguments should be made. Three of these issues are known to be some of the most polarizing matters in contemporary US politics (see, e.g., Grumbach, 2018), namely abortion laws, gun rights and immigration. These “polarized” prompts are, respectively: 1. Recently, there has been a lot of discussion in the US about gun rights and gun control. Some people favor more gun control, and others do not want to add restrictions. From your perspective, what is the best argument for [against] more gun control? 2. Abortion is a heavily debated topic in the US. Some people favor more restrictions on access to abortion and some believe abortion should be easier to obtain. From your perspective, what is the best argument for easier access to [more restrictions on] abortion? 4 3. There are many diverse opinions on immigration to the US. From your perspective, what is the best argument for increasing [restricting] immigration to the US? A respondent—either a human or the LLM—randomly receives either the prompt as is, or with the relevant position (underlined above) substituted with the contents of the square brackets. Note that, slightly differently in each case, the prompts make reference to current debates or discussion about these matters. They are written in language similar, but not identical, to that used by public opinion researchers. [2 ]The fourth issue was deliberately chosen to be low salience and of no particular partisan association—namely, the use of an “opt-in” versus an “opt-out” scheme of organ donation after death: 4. In some countries, organ donation after death is the default: people must explicitly ‘opt out’ of the scheme while alive. In the United States it is not the default, and people must explicitly ‘opt in’ for their organs to be donated after death. What is the best argument for an ‘opt in’[opt out] system? The fifth prompt is about policy priority (rather than preference) and asks “What social, economic or political problems do you think will be most important in the United States over the next 10 years?” In every instance, respondents have a total of 300 characters (maximum, counting down on-screen as respondents typed) to make their case. So: there are a total of nine possible prompts: the pro/anti for issues (1)–(4), and then the fifth prompt which has only one mode. Our setup is that respondents to the prompts are of one of two types: they are the machine (LLM) or human crowdworkers. The outcome was a set of 30 LLM provided arguments[3 ]and 25 crowdworker arguments for each of the nine prompts. We curated these response sets 2 For example, Gallup has historically asked “In your view, should immigration be kept at its present level, increased or decreased?” 3 For 4 prompts, a large run of 300 arguments was generated. We did this to get a general sense of performance, in terms of how many ‘usable’ arguments we should expect: see Supporting Information (SI) C. 5 such that the resulting contests would be between the highest quality outputs from both types of producers.[4] In the second stage of the analysis, an independent set of 760 crowd workers were asked to adjudicate between two arguments for a particular position (say, both in favor of expanded gun rights, or both in opposition to more immigration)—one of which was produced by a machine, and one by a human. These arguments were randomized in terms of their order on the page. We designed a lightweight web application for this purpose. These judges were asked to “click the one that you find more convincing”. In some (randomized at respondent level) cases, the judges were told (truthfully) which argument was produced by a human, and which was produced by “an Artificial Intelligence algorithm”. In all cases, workers were shown a total of 10 sets of arguments (one LLM, one human written for each prompt) for prompts randomly chosen from our set of nine. Therefore, each prompt with a pair of arguments was shown 840 times and each individual argument approximately 280 times. LLMs can make convincing arguments We say an argument is “convincing” to the extent that independent human judges prefer it to another. The structure of the tasks above means that the relevant comparison is statistically simple, and in Figure 1 we show the probability that the human-generated (as opposed to LLM) argument was chosen by crowdworkers. This was calculated from a linear probability model, both overall (All) and for each prompt. We provide a 95% confidence interval on each value. In two cases—arguing for opt-in organ donation and for more restrictions on abortion—the human written arguments were consistently preferred to the LLM written ones. Put differently, for every other argument, there was no statistically significant difference between the LLM and the human writers, in terms of their ability to convince a judge (everything overlaps with a 0.5 probability). The actual data and p-values are included 4 SI D gives more information on the curation process. 6 in SI F. In terms of individual arguments, based on a qualitative reading, the most preferred arguments were those that were more moderate and displayed some amount of nuance. This is true across implied partisan stances (i.e. whether for or against a given position). This suggests that crowdworkers are indeed considering the persuasiveness of the response, rather than (only) judging quality as a function of their personal stance on an issue. Overall, the quantitative “no rhetorical edge” result is interesting, but it does not mean there are no substantive differences between human and LLM arguments. We now turn to these. LLMs can make novel arguments We say a set of arguments is “novel” to the extent that it differs in some well-defined qualitative or quantitative way from another set. Here, our interest is how arguments produced by the LLM—irrespective of their ultimate popularity—have properties in common with each other, and different to those of the humans. In SI E we give full details of that analysis, but our summary is as follows. First, LLM arguments are easier to read (in terms of traditional Flesch Reading Ease measures). Second, 7 Figure 1: LLMs and humans are generally equally able to convince independent judges as to the merits of arguments for positions. LLMs produce arguments that are consistently more positive in tone than human ones—as measured by sentiment dictionaries. In addition, LLM arguments exhibit lower variance on both characteristics (i.e. generally more similar to each other) than human efforts. The LLM produced more coherent arguments on topics where (we believe) it has access to copious training data—e.g. social media posts for abortion discussion. For instance, tracking terms on Reddit—one of the sources of training data for the Meta OPT model— shows the phrase “gun control” appears at least ten times more often than “organ donation”. Consequently, for the more obscure topic of organ donation, the model produces fewer unique arguments and less ‘human-like’ text as discussed below. Though admittedly a judgement call, we note that the LLM tends to argue in more simplistic, direct, less nuanced ways than humans do. For example, an argument against (more liberal) abortion (laws) written by the LLM was “I think the best argument for more restrictions on abortion is that it’s murder. I think that’s pretty clear.” Crowdworkers do not view such claims as favorably as more subtle human-produced cases. These differences help explain the aggregate performance contrast between the LLMs and human writers. While the latter had a higher mean performance, they also exhibited lower variance in the appeal of their arguments. More specifically, there were two arguments written by humans that crowdworkers picked at least of the time in the control condition, and the worst human written argument was picked 38% of the time. Conversely, the most preferred LLM argument was picked 59% of the time and the least only 26% of the time. In terms of the “partisan” nature of judge preferences, our results are mixed. Earlier research (e.g. Motoki et al, 2023) finds that LLMs have a consistent liberal/left political bias. Given this and given that crowdworkers might also be disproportionately liberal/left, we could imagine that this alignment of politics influences the judges’ decisions. That is, we might be concerned that, even within topic, workers are responding to the political tone of the arguments rather than their rhetorical appeal. This does not seem to be the case. In 8 particular, crowdworkers seem to like arguments from both the (traditional) left and right of the spectrum. For example, the second most popular LLM argument in the control condition (in terms of how often it was picked) was an obviously conservative position: A lot of immigrants take jobs away from Americans and that we should focus on the Americans doing those jobs, before immigration. Also that they take up valuable resources such as healthcare and government services. For a final and more general comparison we created document embeddings for all of (i.e. the superset of) the arguments from both groups after some filtering. Specifically, we dropped responses that made no sense given the prompt (e.g. actually made the opposite argument) and/or were unintelligible. About 15-25% of text produced by the LLM was both coherent and unique (not a direct repeat of a previous argument). We focus on the most complete sets of responses; in practice: the responses to “most important problem”, more restrictions on abortion, more gun control, opt-in organ donation. In Figure 2, we display the results of reducing these document embeddings to two dimensions and plotting each argument in that space. In addition, we clustered all (embedded) points using k-means, where k=2. In each cluster, the majority class is either LLM or human. Where the particular point, i.e. argument, is actually from an LLM (human) and is in the LLM (human) majority class cluster, we say it is “correctly” classified. The points labelled “misclass. LLM” were human written arguments that were placed by the k-means algorithm with the (majority) LLM written ones based on these embeddings; vice versa for “misclass. Human”. Where there are many misclassifications, we have evidence that humans and machines make very similar arguments; specifically, they are sufficiently similar that we cannot easily tell them apart in the embedding space clustering. Where there are few misclassifications, we have evidence that humans and machines make different types of arguments and can be separated via text alone. 9 The clearest differences—i.e. the topics for which the LLM and human arguments are most different—are for anti-abortion prompts and on opt-in organ donation. These are much larger than the differences on “most important problem” and gun control. From qualitative inspection, another observation is that misclassification typically goes one way. That is, while it is relatively often the case that LLM arguments mimic exactly the ones our human crowdworkers make, the LLM is also prone to unusual phrasing (e.g. repetition) that humans are not. For instance, this argument generated by the LLM in favor of increasing immigration was rarely picked by judges: I think the best argument is that we need more people to keep our economy going. We need more people to work, pay taxes, and buy things. We need more people to pay for our social security and medicare. We need more people to pay for our schools and roads. We need more people to pay for 10 Figure 2: LLMs make more distinct (from humans) arguments on some topics than others. Specifically, the LLM anti-abortion and opt-in organ donation responses read differently in general to human prompt responses. our military. We need more people to pay for our police and fire departments. We need more people to pay for our parks and libraries. We need more people to pay for our courts and jails. Importantly, the types of LLM arguments that are misclassified as human tend to be more popular with judges in the control condition. Put crudely, judges like the machine to “sound human”. Man v Machine: Humans prefer Human Orators Finally, we ask whether knowing the identity (LLM or human) of the author of a particular argument had a causal effect on how convincing an audience found it. We did not have strong a priori beliefs: on the one hand, an LLM may be viewed as less biased or having access to a greater amount of information and therefore preferred. On the other, given the sensitive and nuanced nature of some prompts, a human perspective could be seen as more valuable and perhaps less “dangerous” or more trustworthy. To address this, we assigned crowdworkers to either a control condition where they saw only the arguments (377 people) or a treatment condition where workers were told who (LLM or human) wrote each argument, with the order they were presented randomized (388 people). [5 ]Figure 3 shows the treatment effect of knowing the author on the relative probability workers preferred the human written argument, with fixed effects for each unique argument in all regressions. That is, we are estimating the author effect holding the actual text shown constant. Standard errors were clustered by prompt. The relevant data is included in the Supporting Information. The total treatment effect is positive and significant but small, resulting in an additional 5 percentage point probability that crowdworkers would 5 The task was fielded through MTurk with a random treatment assignment. Through random chance, the control group was slightly larger than the treatment. We also had several more people in the treatment group vs. the control fail to complete the task adequately to be included in the analysis. 11 pick the human written argument. That is, overall, the causal effect of being told whether an argument was produced by an LLM or human is to prefer the human effort—but not by much. The aggregate effect represents a consistent positive effect of knowing the author for all prompts, however it is only significant for three: the argument for reducing immigration, increasing gun control, and restricting abortion. For the first two of these, there was no preference for either author in the control condition. This implies the treatment effect is not, for instance, learning poor text is written by the LLM and this somehow increasing the dislike for the machine-produced content. Nor is it that judges are learning well-written arguments are written by an LLM and thus feeling ‘’betrayed”. Further, though the abortion arguments written by the LLM were textually distinct per Figure 2, the arguments made in favor of gun control were often misclassified as human. Indeed, in the treatment condition, the arguments which were misclassified as human in the previous section are still picked less than half the Figure 3: Causal effect of knowing author identity on judge preferences for a humanproduced argument (relative to an LLM-produced one) is positive and statistically significant overall. There is considerable heterogeneity by topic, however. 12 time when the crowdworkers know it was written by an LLM. This suggests that argument quality, in the sense of being distinguishable, is not driving our results. In SI G we give an alternative (but equivalent), regression-based assessment of this difference. To summarize that analysis: in the control group, judges are about 2 percentage points more likely to pick the human-written arguments (than LLM arguments) on average, though this is not significant. When informed about the author, they are about 10 percentage points less likely to pick the LLM written argument—a substantial difference over the control condition. Discussion For Aristotle, the purpose of rhetoric is to assist the orator in persuading their listeners (Rapp, 2009). This need not help with the communication of knowledge or finding of fact: a “good” argument by these standards is one that convinces a public, non-expert audience of the correctness of a position. This idea informed our experiments above, and we found that humans are not unique in terms of rhetorical abilities. On the matter of logos—i.e. the content of arguments—we showed that LLMs perform equivalently to humans in suggesting the phrasing for particular issue positions. This was true on both controversial and more banal matters, albeit for a curated “best of” set of arguments. On ethos—that is, the appeal arising from the nature of the speaker themselves—our findings suggest that machines have less appeal than humans as orators. This may give elected (and unelected) officials pause as they contemplate open use of such technology to help them in the democratic marketplace. And, given implied citizen preferences, they may seek to regulate the use of such models for argumentation more generally. We did not explore the use of pathos—that is, the manipulation and exploitation of the emotions of the audience. Or rather, it was bundled with the content of the arguments. Future 13 studies might try to separate this out more than we have done, though we sound two cautionary notes. First, there are ethical concerns with (re)training and instructing LLMs to psychologically manipulate humans, not least because humans may not be able to detect machine-generated language (Jakesch, Hancock and Naaman, 2023). Second, and an issue that affects our work here too, is our “audience” was one of convenience—meaning lessons about pathos may be hard to generalize. While we know that our crowdworker judges are based in the United States, we have no reason to believe they are representative of, say, the American voting population (though see, e.g., Coppock, Leeper and Mullinix, 2018, for discussion of why this may present fewer problems than initially supposed). The same is true of our prompt writers. Presumably neither group meets the highest levels of human rhetorical creativity or analysis. So the next steps in such work might be to compare the LLM’s abilities to those of true domain experts, like elected politicians. The broader implications of our work apply to both politics and policy. As we noted, LLMs are not always popular with audiences per se. But in any case, while the LLM was able to suggest texts that human coders did not, we did not observe wholly new ideas to justify particular positions. This does not mean models will never be capable of such things: this is a fast-moving area, and there are already products available that outperform the model we used here (e.g. Touvron et al., 2023). Where the problem is to convince the public of the merits of some extant policy, the use of LLMs is more immediate: our experiments on opt in/opt-out organ donation are in-line with this claim. But as we discussed in the Introduction, such abilities also bring dystopic visions of voters who do not know what or whom to trust. One potential result is that citizens become skeptically inured to all argumentation, and never update their personal beliefs about the merits of a particular position. Much worse, they may be convinced to act in a way that harms themselves and others, though previous work on fake news suggests that the causal effects of such messages are muted in the aggregate (e.g. Allen et al, 2020). 14 To reiterate, humans seem wary of machines as authors—even when they would otherwise like the content of the output. Future work might helpfully investigate how general this human distrust of machine composition is, and what its genesis might be. One natural extension of our work would be to use deception; that is, to lie to (some) respondents about whether a human or LLM produced the (same) statement, and to see if that author treatment affects the respondents’ perceptions of the merits of the argument, holding the content identical. Indeed, one might be interested in whether respondents are more bothered by misrepresentation of human messages as being created by machines, or the false portrayal of machine output as human. Given our study, our belief is that they will find the latter considerably more concerning. In addition, future work might give broader, more open ended prompts and use topic or contest models. These could be used to assess what factors (what content) characterize LLM versus human argumentation and makes those statements more or less successful when judged (see e.g. Loewen et al, 2012 for a related approach). Perhaps as LLMs become more familiar, humans will relax regarding their efforts. We anticipate ethical challenges in the work ahead, for example over whom voters can hold responsible for machine-generated rhetorical appeals that lead to normatively undesirable outcomes. Put more simply, this new technology is political, and requires ongoing study of political philosophy. References Allen, J., Howland, B., Mobius, M., Rothschild, D. and Watts, D.J., 2020. Evaluating the fake news problem at the scale of the information ecosystem. Science advances, 6(14), p.eaay3539. Bisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel and Jennifer Larson. 2023. “Artificially Precise Extremism: How Internet-trained LLMs Exaggerate Our Differences.”. 15 Chesney, Bobby, and Danielle Citron. \"Deep fakes: A looming challenge for privacy, democracy, and national security.\" Calif. L. Rev. 107 (2019): 1753. Coppock, Alexander, Thomas J Leeper and Kevin J Mullinix. 2018. “Generalizability of heterogeneous treatment effect estimates across samples.” Proceedings of the National Academy of Sciences 115(49):12441–12446 Dai, Yaoyao and Benjamin J Radford. 2023. “Large Language Models for Measuring Contested and Multi-dimensional Concepts” Paper presented at Summer Political Methodology Meeting, 2023. Grumbach, Jacob M. 2018. “From backwaters to major policymakers: Policy polarization in the states, 1970–2014.” Perspectives on Politics 16(2):416–435. Halterman, Andrew. 2023 \"Synthetically generated text for supervised text analysis.\" arXiv preprint arXiv:2303.16028 Hu, Minqing and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 168–177. Jakesch, Maurice, Jeffrey T Hancock and Mor Naaman. 2023. “Human heuristics for AIgenerated language are flawed.” Proceedings of the National Academy of Sciences 120(11):e2208839120. Lazer, David MJ, et al. \"The science of fake news.\" Science 359.6380 (2018): 1094-1096. Loewen, Peter John, Daniel Rubenson, and Arthur Spirling. \"Testing the power of arguments in referendums: A Bradley–Terry approach.\" Electoral Studies 31.1 (2012): 212-221. Motoki, Fabio, Valdemar Pinho Neto, and Victor Rodrigues. \"More human than human: Measuring chatgpt political bias.\" Available at SSRN 4372349 (2023). Martindale, Colin. 1990. The clockwork muse: The predictability of artistic change. Basic Books. Rapp, Christof. 2009. The Nature and Goals of Rhetoric. In A Companion to Aristotle, ed. Georgios Anagnostopoulos. Wiley Online Library pp. 577–596. Rosenzweig, Leah R and Molly Offer-Westort. 2022. “Testing interventions to address vaccine hesitancy on Facebook in East and West Africa.” Open Science Framework . Schiff, Kaylyn Jackson, Daniel S. Schiff, and Natalia Bueno. \"The Liar’s Dividend: Can Politicians Use Deepfakes and Fake News to Evade Accountability?.\" (2022). 16 Spirling, Arthur. 2023. “Why open-source generative AI models are an ethical way forward for science.” Nature 616(7957):413–413. Stone, Philip J, Dexter C Dunphy and Marshall S Smith. 1966. The General Inquirer: A computer approach to content analysis. MIT press. Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave and Guillaume Lample. 2023. “LLaMA: Open and Efficient Foundation Language Models.”. Zhang, Susan, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang and Luke Zettlemoyer. 2022. “OPT: Open Pre-trained Transformer Language Models.” 17 Contrast vs Attack Ads: How Different Forms of Negative Campaign Advertisements Affect Senate Candidate’s Vote Share Author: Perea, Luke, Date: 2023 Collections: MediaAdsPolit Zotero Key: 8LDXH549 Cite Key: Perea23contrastVsAttackAdvertSenate Zotero Item | Lit Note Governance: The Political Science Governance: The Political Science Journal at UNLV Journal at UNLV Volume 7 Article 5 2023 Contrast vs Attack Ads: How Different Forms of Negative Contrast vs Attack Ads: How Different Forms of Negative Campaign Advertisements Affect Senate Candidate’s Vote Share Campaign Advertisements Affect Senate Candidate’s Vote Share Luke Perea University of Nevada, Las Vegas Follow this and additional works at: https://digitalscholarship.unlv.edu/governance-unlv Part of the American Politics Commons Recommended Citation Recommended Citation Perea, Luke (2023) \"Contrast vs Attack Ads: How Different Forms of Negative Campaign Advertisements Affect Senate Candidate’s Vote Share,\" Governance: The Political Science Journal at UNLV: Vol. 7, Article 5. Available at: https://digitalscholarship.unlv.edu/governance-unlv/vol7/iss1/5 This Article is protected by copyright and/or related rights. It has been brought to you by Digital Scholarship@UNLV with permission from the rights-holder(s). You are free to use this Article in any way that is permitted by the copyright and related rights legislation that applies to your use. For other uses you need to obtain permission from the rights-holder(s) directly, unless additional rights are indicated by a Creative Commons license in the record and/ or on the work itself. This Article has been accepted for inclusion in Governance: The Political Science Journal at UNLV by an authorized administrator of Digital Scholarship@UNLV. For more information, please contact digitalscholarship@unlv.edu. Contrast vs Attack Ads: How Different Forms of Negative Campaign Advertisements Affect Senate Candidate’s Vote Share Luke Perea PSC 499-01 Department of Political Science University of Nevada, Las Vegas Abstract: This research is intended to find out how Senate candidates’ vote shares are affected by advertisements with a negative tone. Prior literature is largely inconsistent in determining the impact of advertisement tone, but separating negative advertisements into two subcategories, advertisements that attack an opponent and advertisements that contrast between oneself and one’s opponent, clearer correlations become evident. Literature suggests that attack advertisements are more effective at generating voter responsiveness and mobilizations compared to contrast advertisements, they also come with a backlash effect that often damages the attacker more than the target. Thus, candidates with a higher percentage of attack advertisements should have a lower vote share, and contrast advertisements should generate mixed results. Using data from the Wesleyan Media Project and the MIT Election Lab, it was clear based on a linear regression test that contrast advertisements instead of attack advertisements showed a more significant relationship between percentage and vote share. However, while the relationship between contrast percentage and vote share was statistically significant, the relationship between attack percentage and vote share was not. Introduction Election season is right around the corner when campaign advertisements start increasing in frequency. Every two years, when elected officials at the local, state, and federal levels are up for re-election, it is nearly impossible to escape the repetitive number of messages when sitting down to watch television. The large amounts of televised advertisements are a big part of the mobilization strategies of campaigns and are intended to help a candidate win an election. Given that there are so many advertisements run by these campaigns over a relatively short time span, it is clear that not all of these advertisements are alike, despite some of the cliché and satire attached to their uniform layout. A notable categorization that campaign advertisements can be separated by is their tone. On the surface, it is easy to watch an advertisement and immediately tell whether it is meant to highlight the achievements and positive attributes of a candidate or unleash a scathing critique of an opponent. Yet while scholars have generally used this separation of positive or negative advertisements to analyze the effect that tone has on a candidate’s success, this simple categorization has produced wildly inconsistent and sometimes contradictory results. The purpose of my research is to expand the categories of tone, specifically looking at negative advertisements, in order to find more consistent findings and relationships between negative advertisements and vote share. Just as not all advertisements are alike, not all negative advertisements are alike either. While some advertisements do nothing but attack the opposition, others include a semblance of positivity by contrasting the undesirable attributes of an opponent with the good qualities of the attacker. By separating negative advertisements into attack and contrast advertisements, new facts about the effects that these specific tones emerge. Two major findings find that attack advertisements have a greater strength of mobilization and create more responsiveness amongst voters as compared to contrast advertisements, but attacks can quickly lead to backlash which can do more damage than good for an attacker (Fridkin and Kenny 2004). These two facts have led me to hypothesize that candidates who run a high percentage of attack advertisements will experience a lower vote share, but candidates who run a high percentage of contrast advertisements can experience both a high and low vote share based on external factors,not on the tone of the advertisement. Using data accrued by the Wesleyan Media Project, I obtained variables showing the percentage of advertisement tone for each Senate candidate in 2016. I crossed these percentages into a dataset containing vote share acquired from the MIT Election Lab. Upon creating scatterplots showing each candidate showing their attack and contrast advertisement percentages compared with their vote share, I used a linear regression test to see how both independent variables affect vote share. These results for these tests were unanticipated, as although both attack and contrast advertisements had a negative coefficient, the rate of decline for vote share was greater for contrast advertisements. Additionally, the relationship between contrast advertisements and vote share was statistically significant, but the same could not be said about the relationship between attack advertisements and vote share. As a result, it could be concluded that either the results were correct, meaning that candidates are more aware of the backlash theory and only run attack advertisements in certain situations, or more research could be done with a greater sample size. Despite the initial inaccuracy of the hypothesis, the research shows that more work can be done to further categorize campaign advertisements to find clearer relationships between advertisement tone and candidate success. Literature Review The Problem With “Negative” Advertisements There is an extensive amount of prior research regarding the impact of campaign advertisements and their tone. Generally, researchers in the past who investigated political advertisements categorized the tone of an advertisement as either positive or negative, which are designed to promote oneself or attack an opponent, respectively. However, the results shown by various experiments specifically pertaining to the political effects of negative advertisements are inconsistent, to put it lightly. Kahn and Kenny (1999, 887) suggested that “negative information is helpful and motivates participation as long as it addresses relevant topics and is presented in an appropriate manner, based on their results. They argue that negative ads can potentially increase turnout, as they not only present a higher sense of urgency to potential voters, but they also convey a more stimulating or exciting message than a positive ad (Kahn and Kenny 1999). On the other hand, Malloy and Pearson-Merkowitz (2016) examined the effects of positive and negative advertising and found that negative advertisements “do not appear to be effective, and they (negative advertisements) even seem to have the opposite effect to what the campaign intends” (Malloy and Pearson-Merkowitz 2016, 9). Then there is the evidence presented by Lau et. al. (2007), who argued that negative advertisements don’t help or hurt candidates any more than positive ones do. Simply put, comparing advertisements based on whether their tone is negative or positive has yet to produce consistent results, or at least results that are agreed upon by scholars as presented thus far. Contrast or Attack During the 2020 Colorado Senate election, Democratic challenger and former governor John Hickenlooper addressed allegations of running mainly negative advertisements by stating that they were not purely negative but were contrast advertisements. Hickenlooper stated, “Here are what I call contrast ads where you’re just saying, ‘Here are the facts, here’s what this person stands for, here’s what I stand for, here’s what this person said they were going to do, and here’s what they did’” (Ashby 2020). Contrast advertisements, generally those that compare the negative aspects of an opponent with the positives of the broadcaster, separate negative advertisements by distinguishing between advertisements containing both a positive and negative tone and those with only a negative tone. This allows for advertisements previously classified as just negative to be split up into contrast and attack advertisements, respectively. Contrast advertisements, when practically applied, change the way an election is portrayed through the advertisements candidates run. When reviewing advertisement data from the 2000 Presidential election, Goldstein and Freedman (2002) saw that when advertisements were categorized purely into positive or negative categories, the percentage of negative advertisements exceed the percentage of positive advertisements, but when incorporating contrast advertisements as a third category, the number of positive advertisements was essentially split in half, with one-quarter of all advertisements run being classified as contrast advertisements. The incorporation of contrast advertisements in Goldstein and Freedman’s research (2002) shifted the perception of the election from being overly negative to fairly positive. Goldstein and Freedman (2002) continued to use the contrast advertisements as an additional category to test several variables such as competitiveness and timing, and also found that the results from these tests were vastly different than the results from tests simply comparing positive and negative tone. This shows that the separation of negative advertisements into the categories of contrast and attack can drastically shift the way an election is perceived through its advertising and is a valid distinction to make when conducting campaign advertisement research. Since scholars have been unable to create a common understanding regarding the impact of negative advertisements on measures of campaign success, then narrowing down negative advertisements by separating them into attack and contrast advertisements should yield stronger correlations between advertisement tone and measures of candidate success. Fridkin and Kenny (2004) examined negative advertising’s effects on voters’ responsiveness. For one of their experiments, they compared contrast advertisements with attack advertisements, to see which one would generate more of a response and found that attack advertisements, whether employed by an incumbent or challenger in Senate elections, were more effective at changing a voters’ opinion (Fridkin and Kenny 2004). While attack advertisements were more effective at generating attentiveness to the issues presented by an advertisement, researchers also found that changes in opinion caused by attack advertisements were not always to the benefit of the candidate running the advertisement. Fridkin and Kenny (2004) found that candidates who issue attack advertisements were more likely to receive backlash than those who didn’t issue attacks at all. The backlash caused by attack advertisements creates adverse effects for candidates who issue these advertisements, as shown in additional studies. A survey divided up participants and gave each group different transcripts of created by the researchers, with the identity of the candidate who issued the attack being different for each group (Roese and Sande 1993). The results showed that the candidate who was the target of the attacks received more votes than the candidate who issued them (Roese and Sande 1993). Further, when controlling for the candidate who was initially favored by the participants, the researchers found that if that candidate was the recipient of an attack, their vote share did not decrease, but when they were the issuer, their vote share declined (Roese and Sande 1993). In another study, Banda and Windett (2016) found that when a candidate issued more attack advertisements relative to their opponent over time, the support shown for them by the public from polling results also declined. The idea of backlash due to attack advertisements makes sense when considering survey data which reveals that 82% of Americans believe that negative campaigning is undermining and damaging our democracy (Lipsitz et. al. 2006, 338). While attack advertisements have been shown to attract more responsiveness than contrast advertisements, there is also a much larger risk of a backlash effect causing negative responsiveness to the attacking candidate. Vote Share Vote share, or the percentage of votes received, is one of the two most used measures of candidate success alongside voter turnout, but for the purposes of measuring the effect of campaign advertising, it is the better variable. This is shown by research conducted by Spenkuch and Toniatti (2018), who found differences between the effects of advertising on turnout and vote share. The authors conclude that voter turnout, when controlling partisan factors, is not significantly impacted by political advertisements. This is because the mobilizing and demobilizing characteristics of the tone and other factors of the advertisements cancel each other out in most situations, and as the authors note, is the reason why many studies found inconclusive and contradictory results (Spenkuch and Toniatti 2018). However, the authors assert that advertisements can affect vote share by influencing the “right” constituency to the polls, such as strong partisans and the politically informed (Spenkuch and Toniatti 2018). Theory and Hypothesis Based on these findings, I want to look at how political advertisements that contain contrasts and attacks affect vote share. While advertisements categorized as being positive or negative have seemingly impacted vote returns as well as other variables measuring candidate success in varying ways, dividing negative advertisements between contrast and attack advertisements generates more accurate implications of advertisement tone in the real world. This method combines the above findings by showing that the advertisements containing only attacks consistently draw more attention from voters but also have a higher backlash effect that declines both voter support and vote share. I believe using vote share as a dependent variable will provide clearer correlations than voter turnout and is thus the ideal measure of candidate success when discussing campaign advertisements. After calculating the percentage of advertisements containing attacks for each candidate and comparing the resulting data with the candidate’s vote share, I argue that the backlash effects of the attack advertisements will create a negative correlation between the two variables. I hypothesize that Senate candidates with a higher percentage of attack advertisements will have a subsequently lower vote share, and that the relationship between contrast advertisements and vote share will be insignificant and sporadic. Data Methods There are a multitude of political offices and election years to choose from as a case, but I will be using the US Senate elections of 2016 to test my hypothesis. The reason I chose the Senate instead of another political office is because of the media market from which Senate advertisements are broadcasted. Ansolabehere, Snowberg, and Snyder (2006) provide a good explanation for this, stating, “Senators and governors receive more free media coverage than do U.S. House members, and they spend more on paid television advertisements during election campaigns. It is also much easier to identify in-state and out-of-state media markets than in-district and out-of-district markets” (Ansolabehere, Snowberg, and Snyder 2006, 484). I chose Senate elections over gubernatorial ones as there was simply a larger sample size of Senate candidates up for election during the year 2016. To test my hypothesis, I will use data from the Wesleyan Media Project as well as the Massachusetts Institute of Technology Election Data and Science Lab. The Wesleyan Media Project (WMP) is a database housing information on campaign advertisements aired from all levels of government (Fowler et. al. 2019). The WMP collects advertisements filed and tracked by Kantar Media’s Campaign Media Analysis Group (CMAG) through a market-based tracking system that provides accurate information on how many times an advertisement was aired, where it was aired, and what date, time, channel, and show it was aired on (Fowler et. al. 2019). The system also stores the advertisements in a video file and presents all the aforementioned data to the WMP (Fowler et. al. 2019). The Project staff then conduct further research and carefully code the advertisements’ video files based on a web-based analysis platform (Fowler et. al. 2019). I use the WMP’s data to gain information on my independent variable, advertisement tone. To combine the WMP’s data with the dataset for vote shares, I find the percentage of attack ads run by each candidate. To do this, I utilize the WMP’s civility variable listed under “ad_tone”, which reads, “In your judgment, is the primary purpose of the ad to promote a specific candidate, attack a candidate, or contrast the candidates?” This data is coded by the WMP staff, with “1” representing contrast advertisements, “2” representing promote advertisements, and “3” representing attack advertisements. When crossing this data with the candidate’s name, the total number of each advertisement in each of the above categories is shown for every candidate in the dataset for which data is present, along with the total amount of all advertisements run for each candidate. This allows for the percentage of attack and contrast advertisements to be calculated and operationalized under the terms “attperc” and “contperc” respectively, and subsequently listed in the dataset alongside the dependent variable. For my dependent variable, vote share, I utilize data provided by the MIT Election Data and Science Lab. The Lab (2017) provides information on vote returns on elections from multiple levels of government. For my project, I will use the data set providing state-level returns for Senate candidates from 1976 to 2020. To find the vote share, I use the variables titled “candidate votes”, showing the amount of votes the Senate candidate received, and divide them by “total votes” showing the total number of votes available for Senate candidates in each state. This calculation will be operationalized as vote share, represented in the data set as vote percentage, or “votperc”. Below, I provide summary statistics tables for both the independent variables as well as the dependent variable, followed by a linear regression to find whether the relationship between the two independent variables and the dependent variable follows the hypothesis, as well as comparing the statistical significance between the two. Statistics Below is the descriptive statistics table (Table 1) for the two independent variables, contrast and attack percentage, and the dependent variable, vote percentage. Note that mean, minimum, and maximum are all percentage values and not totals. While initially, only 63 candidates had available data from both data sets, 6 candidates whose vote share was under 10 percent were removed, as they greatly skewed the data. There remain several outliers whose vote share was under 20 percent, but in order to preserve a meaningful sample size and not ruin the integrity of the data by removing certain major party candidates, they are still included in this experiment. Table 1. Descriptive Statistics Variable Observations Mean Standard Deviation Minimum Maximum |Contrast % Attack % Vote Share|57 26.3442 27.0422 0 100 57 9.8008 15.3491 0 68.7434 57 48.1387 14.3686 10.2155 78.4780| |---|---| On average, 26 percent of candidate advertisements were contrast advertisements, and 9 percent were attack advertisements. This would seem to fit with both theories, as while the average amount of contrast advertisements is relatively low, fitting with the lack of mobilizing power and vague rhetoric, yet it is still larger than the average amount of attack advertisements ran at 9 percent, fitting the fear of backlash theory. Despite this, the range of contrast and attack percentages is large, as several candidates chose not to run contrast advertisements and others chose not to run any attacks. Yet, some candidates ran nothing but contrast advertisements. For example, 70 percent of one candidate’s advertisements, Patty Judge (D) of Iowa, were attacks. This discrepancy in range could be due to several outside factors such as race competitiveness but should still result in the same impact on vote share. Regarding vote share, the average vote share for Senate candidates in 2016 was around 48 percent. The range of vote shares is almost as high as the independent variables, with David Fleming (R) of Louisiana receiving the lowest vote share at just over 10 percent, and John Hoeven (R) of North Dakota receiving the highest vote share at 78.5 percent. Analysis When examining Figures 1 and 2 below, some noticeable observations can be made that immediately call the hypothesis into question. For Figure 1, a scatterplot showing each candidate’s contrast advertisement percentage crossed with their vote share, there is a noticeable linear decline in vote share with an increase in contrast advertisements. While there are some outliers in certain clusters, every candidate who ran contrast advertisements 20 percent of the time or less had over 60 percent of the vote share of their respective states. On the scatterplot with attack advertisements (Figure 2), there is a much less clear correlation between the independent and dependent variables. One takeaway that was made early on was that I severely overestimated the number of candidates who chose not to run any attack advertisements whatsoever. Whether this is an outlier event for just the 2016 election or whether this is consistent for all Senate elections is unclear, but one potential explanation could be that candidates have become more aware of the backlash effect and are using contrast advertisements as a safer way to carry out attacks. Regardless, only 11 candidates had over 20 percent of their total advertisements take an attacking tone, with various and inconsistent results as some candidates had a significantly higher vote share than others albeit while running more attack advertisements. Table 2 shows the results of the linear regression testing the effects of attack and contrast advertisements on vote share, and they confirm the suspicions raised by Figures 1 and 2 regarding the accuracy of the hypothesis. While I predicted that attack advertisements would cause a steeper decline in vote share than contrast advertisements would, the opposite was true in this case. The coefficient between attack advertisement percentage and vote share was negative, which did follow the hypothesis. For every 1 percent increase in attack advertisements, a Figure 1. Contrast Ad. Percentage and Vote Shar/e Figure 2. Attack Ad. Percentage and Vote Share Table 2. Regression Analysis 𝞫 (std. error) Attack % -.1613 (.1118) Contrast % -.2352***** (.0634) Constant 55.9177***** (2.6063) Observations 57 Vote Share estimated using regression. Standard Error in parentheses *** p<.01 **p<.05 *p<.10 candidate would likely see their vote share decrease by .1613 percent. However, the relationship between attack percentage and vote share was statistically insignificant at the 0.05 p level. These results depict a possibility that the relationship between attack advertisement percentage and vote share is much more erratic than I assumed. At the same time, contrast advertisements had a negative coefficient with vote share, which was unanticipated by the hypothesis. For every 1 percent increase in contrast advertisements, a candidate would likely see their vote share decrease by .2352 percent. This was also unanticipated as an increase, in contrast, advertisement percentage resulted in an even larger decrease in vote share than an increase in attack advertisement percentage created. Unlike attack advertisements, the relationship between contrast advertisement percentage and vote share was statistically significant. These results show a much stronger correlation between contrast percentage and vote share as compared to attack percentage and vote share, and the hypothesis is shown to be not only incorrect, but flipped on its head entirely. The possibility that these findings are correct certainly exists, despite the subversion of expectations based on prior literature. One observation to make specifically when comparing Figures 1 and 2 is to note that there was a much higher number of candidates who ran a significant percentage of contrast advertisements, while negative advertisements were only run over 20 percent of the time by 11 candidates. this could be due to an outlier year from the normal percentages of advertisements ran, or his could also be due to more of an understanding of the backlash effect. It could be possible that candidates only issue a high percentage of attack advertisements when they feel that it is a necessary course of action, such as when a heavy underdog feels it is worth the risk to try and generate greater responsiveness, or when a heavy favorite feels that there is no risk as their voter base is essentially secured. However, this is not entirely supported by Figure 2, as 8 of the 11 candidates who ran over 20 percent of their advertisements with an attacking tone ended up with a vote share between 40 and 50 percent, suggesting a narrow election. If attack advertisements are becoming more infrequent, contrast advertisements could potentially replace attack advertisements as the main source of negativity in political advertisements, resulting in the effect initially attributed to attack advertisements I believe it is more likely that further research could show a strong correlation between attack advertisements and vote share. One change could be to include a greater sample size. The number of candidates sampled did not include every major party candidate from the year 2016, as the two datasets used did not have data on the independent and dependent variables available for each candidate. Unless the backlash awareness theory noted above holds, it is likely that 2016 could have been an outlier year for the low average percentages of attack advertisements ran, and that a larger sample of Senate candidates spanning multiple elections could show a much stronger correlation. The same can be said for contrast advertisements, as there is a decent amount of variety in vote shares depicted on Table 2, as some candidates with a higher vote share also have a higher percentage of contrast advertisements, and a greater sample size could potentially fit the hypothesis. It could also be possible that the correlation exists due to the attacking tone inherent in contrast advertisements, albeit less severe than the correlation between attack advertisements and vote share. Regardless, the data gathered from the sample size was anticipated to be enough to strongly prove my hypothesis correct, but that did not end up taking place. Conclusion I set out to find clearer correlations between advertisement tone and vote share, and while my hypothesis has been initially disproved, I did end up finding some interesting correlations. The higher the percentage of a candidate’s advertisements were contrast advertisements, their vote share declined at a statistically significant level, and a higher substantive level than attack advertisements. While this did ultimately not follow my hypothesis, this was an interesting turnout as it proved that the relationship between contrast advertisements and vote share is most likely more significant than originally anticipated overall. While this doesn’t necessarily mean that contrast advertisements don’t generate less responsiveness and don’t mobilize voters well as stated in the literature, it can be concluded that voters make their decisions empathetically less neutral to a contrast tone than what was assumed. However, while attack advertisements did not show a large substantive or statistical level of significance, it can be less definitively concluded that this data is representative of the overall trend, as only a small number of candidates ran a significant amount of attack advertisements. In the future, researchers could focus on establishing further categorizations of advertisements when testing for measures of candidate success. The results from this study showed a correlation between contrast advertisements and vote share that both did not meet previous expectations and represented new findings for a more specific categorization of advertisement tone. Some researchers have focused on other characteristics similar to tone such as civility or issue area, and combining these variables with tone could also yield different correlations. Campaign advertisements, as the primary medium of distributing political information by candidates during elections, are likely to continue to be a heavily researched topic and given the expansion of advertisements into mediums such as social media, advertisement tone should continue to be at the forefront of research. Research on advertisement tone and how it affects voters and election results could influence the future of political advertisements, as we continue to find which tones and other factors are the most successful at generating a high voter share, turnout, or any other measure of candidate success. Bibliography Ansolabehere, Stephen, Snowberg, Erik C., Snyder Jr., James M. 2006. “Television and the Incumbency Advantage in U.S. Elections.” Legislative Studies Quarterly 31(4). http://dx.doi.org/10.3162/036298006X201896. Ashby, Charles. 2020. “Hickenlooper: Contrast Ads Are Not Negative.” The Grand Junction Daily Sentinel. https://www.gjsentinel.com/news/western_colorado/hickenlooper-contrast-ads-are-not-ne gative/article_a69a66d8-01be-11eb-aa0b-074b4df19f1e.html (April 19, 2023). Banda, Kevin K., and Jason H. Windett. 2016. “Negative Advertising and the Dynamics of Candidate Support.” Political behavior 38(3): 747–66. http://dx.doi.org/10.1007/s11109-016-9336-x. Fridkin, Kim Leslie, and Patrick J. Kenney. 2004. “Do Negative Messages Work?: The Impact of Negativity on Citizens’ Evaluations of Candidates.” American politics research 32(5): 570–605. http://dx.doi.org/10.1177/1532673x03260834. Fowler, Erika Franklin, Michael M. Franz, Travis N. Ridout, and Laura M. Baum. 2019. “Political Advertising in 2016.” Version 1.0 [dataset]. Middletown, CT: The Wesleyan Media Project, Department of Government at Wesleyan University. “Home Page.” Mit.edu. https://electionlab.mit.edu/ (March 8, 2023). Kahn, Kim Fridkin, and Patrick J. Kenney. 1999. “Do Negative Campaigns Mobilize or Suppress Turnout? Clarifying the Relationship between Negativity and Participation.” The American political science review 93(4): 877–89. http://dx.doi.org/10.2307/2586118. Lau, Richard R., Lee Sigelman, and Ivy Brown Rovner. 2007. “The Effects of Negative Political Campaigns: A Meta-Analytic Reassessment.” The journal of politics 69(4): 1176–1209. http://dx.doi.org/10.1111/j.1468-2508.2007.00618.x. Lipsitz, Keena, Christine Trost, Matthew Grossmann, and John Sides. 2005. “What Voters Want from Political Campaign Communication.” Political communication 22(3): 337–54. http://dx.doi.org/10.1080/10584600591006609. Malloy, Liam C., and Shanna Pearson-Merkowitz. 2016. “Going Positive: The Effects of Negative and Positive Advertising on Candidate Success and Voter Turnout.” Research & politics 3(1): 205316801562507. http://dx.doi.org/10.1177/2053168015625078. MIT Election Data and Science Lab. 2017. “U.S. Senate 1976–2020.” “Project Background.” Wesleyan.edu. https://mediaproject.wesleyan.edu/about/project-background/ (March 8, 2023). Roese, Neal J., and Gerald N. Sande. 1993. “Backlash Effects in Attack Politics1.” Journal of applied social psychology 23(8): 632–53. http://dx.doi.org/10.1111/j.1559-1816.1993.tb01106.x. Spenkuch, Jörg L., and David Toniatti. 2018. “Political Advertising and Election Results.” The Quarterly Journal of Economics 133(4): 1981–2036. http://dx.doi.org/10.1093/qje/qjy010. Visualizing Attention, a Transformer's Heart | Chapter 6, Deep Learning Author: 3Blue1Brown, Date: 2024-04-07 Collections: Generative AI, Polarization Zotero Key: 6B76TMFW Cite Key: 3Blue1Brown24visAttenTransfrmr Zotero Item | Lit Note 00:00 In the last chapter, you and I started to step 00:02 through the internal workings of a transformer. 00:04 This is one of the key pieces of technology inside large language models, 00:07 and a lot of other tools in the modern wave of AI. 00:10 It first hit the scene in a now-famous 2017 paper called Attention is All You Need, 00:15 and in this chapter you and I will dig into what this attention mechanism is, 00:19 visualizing how it processes data. 00:26 As a quick recap, here's the important context I want you to have in mind. 00:30 The goal of the model that you and I are studying is to 00:33 take in a piece of text and predict what word comes next. 00:36 The input text is broken up into little pieces that we call tokens, 00:40 and these are very often words or pieces of words, 00:43 but just to make the examples in this video easier for you and me to think about, 00:47 let's simplify by pretending that tokens are always just words. 00:51 The first step in a transformer is to associate each token 00:54 with a high-dimensional vector, what we call its embedding. 00:57 The most important idea I want you to have in mind is how directions in this 01:02 high-dimensional space of all possible embeddings can correspond with semantic meaning. 01:07 In the last chapter we saw an example for how direction can correspond to gender, 01:11 in the sense that adding a certain step in this space can take you from the 01:15 embedding of a masculine noun to the embedding of the corresponding feminine noun. 01:20 That's just one example you could imagine how many other directions in this 01:23 high-dimensional space could correspond to numerous other aspects of a word's meaning. 01:28 The aim of a transformer is to progressively adjust these 01:31 embeddings so that they don't merely encode an individual word, 01:35 but instead they bake in some much, much richer contextual meaning. 01:40 I should say up front that a lot of people find the attention mechanism, 01:43 this key piece in a transformer, very confusing, 01:46 so don't worry if it takes some time for things to sink in. 01:49 I think that before we dive into the computational details and 01:52 all the matrix multiplications, it's worth thinking about a couple 01:55 examples for the kind of behavior that we want attention to enable. 02:00 Consider the phrases American true mole, one mole of carbon dioxide, 02:04 and take a biopsy of the mole. 02:06 You and I know that the word mole has different meanings in each one of these, 02:10 based on the context. 02:11 But after the first step of a transformer, the one that breaks up the text 02:15 and associates each token with a vector, the vector that's associated with 02:18 mole would be the same in all of these cases, because this initial token 02:22 embedding is effectively a lookup table with no reference to the context. 02:26 It's only in the next step of the transformer that the surrounding 02:30 embeddings have the chance to pass information into this one. 02:33 The picture you might have in mind is that there are multiple distinct directions in 02:38 this embedding space encoding the multiple distinct meanings of the word mole, 02:42 and that a well-trained attention block calculates what you need to add to the generic 02:47 embedding to move it to one of these specific directions, as a function of the context. 02:53 To take another example, consider the embedding of the word tower. 02:57 This is presumably some very generic, non-specific direction in the space, 03:01 associated with lots of other large, tall nouns. 03:04 If this word was immediately preceded by Eiffel, 03:06 you could imagine wanting the mechanism to update this vector so that 03:10 it points in a direction that more specifically encodes the Eiffel tower, 03:14 maybe correlated with vectors associated with Paris and France and things made of steel. 03:19 If it was also preceded by the word miniature, 03:22 then the vector should be updated even further, 03:24 so that it no longer correlates with large, tall things. 03:29 More generally than just refining the meaning of a word, 03:32 the attention block allows the model to move information encoded in 03:35 one embedding to that of another, potentially ones that are quite far away, 03:39 and potentially with information that's much richer than just a single word. 03:43 What we saw in the last chapter was how after all of the vectors flow through the 03:47 network, including many different attention blocks, 03:50 the computation you perform to produce a prediction of the next token is entirely a 03:55 function of the last vector in the sequence. 03:59 Imagine, for example, that the text you input is most of an entire mystery novel, 04:03 all the way up to a point near the end, which reads, therefore the murderer was. 04:08 If the model is going to accurately predict the next word, 04:11 that final vector in the sequence, which began its life simply embedding the word was, 04:16 will have to have been updated by all of the attention blocks to represent much, 04:20 much more than any individual word, somehow encoding all of the information 04:24 from the full context window that's relevant to predicting the next word. 04:29 To step through the computations, though, let's take a much simpler example. 04:32 Imagine that the input includes the phrase, a 04:35 fluffy blue creature roamed the verdant forest. 04:38 And for the moment, suppose that the only type of update that we care about 04:42 is having the adjectives adjust the meanings of their corresponding nouns. 04:47 What I'm about to describe is what we would call a single head of attention, 04:50 and later we will see how the attention block consists of many different heads run in 04:54 parallel. 04:56 Again, the initial embedding for each word is some high dimensional vector 04:59 that only encodes the meaning of that particular word with no context. 05:04 Actually, that's not quite true. 05:05 They also encode the position of the word. 05:07 There's a lot more to say way that positions are encoded, but right now, 05:11 all you need to know is that the entries of this vector are enough to 05:15 tell you both what the word is and where it exists in the context. 05:19 Let's go ahead and denote these embeddings with the letter e. 05:22 The goal is to have a series of computations produce a new refined 05:26 set of embeddings where, for example, those corresponding to the 05:29 nouns have ingested the meaning from their corresponding adjectives. 05:33 And playing the deep learning game, we want most of the computations 05:37 involved to look like matrix-vector products, where the matrices are 05:40 full of tunable weights, things that the model will learn based on data. 05:44 To be clear, I'm making up this example of adjectives updating nouns just to 05:48 illustrate the type of behavior that you could imagine an attention head doing. 05:52 As with so much deep learning, the true behavior is much harder to parse because it's 05:57 based on tweaking and tuning a huge number of parameters to minimize some cost function. 06:01 It's just that as we step through all of different matrices filled with parameters 06:05 that are involved in this process, I think it's really helpful to have an imagined 06:09 example of something that it could be doing to help keep it all more concrete. 06:14 For the first step of this process, you might imagine each noun, like creature, 06:18 asking the question, hey, are there any adjectives sitting in front of me? 06:22 And for the words fluffy and blue, to each be able to answer, 06:25 yeah, I'm an adjective and I'm in that position. 06:28 That question is somehow encoded as yet another vector, 06:32 another list of numbers, which we call the query for this word. 06:36 This query vector though has a much smaller dimension than the embedding vector, say 128. 06:42 Computing this query looks like taking a certain matrix, 06:46 which I'll label wq, and multiplying it by the embedding. 06:50 Compressing things a bit, let's write that query vector as q, 06:54 and then anytime you see me put a matrix next to an arrow like this one, 06:58 it's meant to represent that multiplying this matrix by the vector at the arrow's start 07:02 gives you the vector at the arrow's end. 07:05 In this case, you multiply this matrix by all of the embeddings in the context, 07:10 producing one query vector for each token. 07:13 The entries of this matrix are parameters of the model, 07:16 which means the true behavior is learned from data, and in practice, 07:19 what this matrix does in a particular attention head is challenging to parse. 07:23 But for our sake, imagining an example that we might hope that it would learn, 07:27 we'll suppose that this query matrix maps the embeddings of nouns to 07:31 certain directions in this smaller query space that somehow encodes 07:34 the notion of looking for adjectives in preceding positions. 07:38 As to what it does to other embeddings, who knows? 07:41 Maybe it simultaneously tries to accomplish some other goal with those. 07:44 Right now, we're laser focused on the nouns. 07:47 At the same time, associated with this is a second matrix called the key matrix, 07:51 which you also multiply by every one of the embeddings. 07:55 This produces a second sequence of vectors that we call the keys. 07:59 Conceptually, you want to think of the keys as potentially answering the queries. 08:03 This key matrix is also full of tunable parameters, and just like the query matrix, 08:07 it maps the embedding vectors to that same smaller dimensional space. 08:12 You think of the keys as matching the queries whenever they closely align with each other. 08:17 In our example, you would imagine that the key matrix maps the adjectives like fluffy 08:21 and blue to vectors that are closely aligned with the query produced by the word creature. 08:27 To measure how well each key matches each query, 08:30 you compute a dot product between each possible key-query pair. 08:34 I like to visualize a grid full of a bunch of dots, 08:37 where the bigger dots correspond to the larger dot products, 08:40 the places where the keys and queries align. 08:43 For our adjective noun example, that would look a little more like this, 08:47 where if the keys produced by fluffy and blue really do align closely with the query 08:52 produced by creature, then the dot products in these two spots would be some large 08:57 positive numbers. 08:59 In the lingo, machine learning people would say that this means the 09:02 embeddings of fluffy and blue attend to the embedding of creature. 09:06 By contrast to the dot product between the key for some other 09:09 word like the and the query for creature would be some small 09:12 or negative value that reflects that are unrelated to each other. 09:17 So we have this grid of values that can be any real number from 09:21 negative infinity to infinity, giving us a score for how relevant 09:25 each word is to updating the meaning of every other word. 09:29 The way we're about to use these scores is to take a certain 09:32 weighted sum along each column, weighted by the relevance. 09:36 So instead of having values range from negative infinity to infinity, 09:40 what we want is for the numbers in these columns to be between 0 and 1, 09:44 and for each column to add up to 1, as if they were a probability distribution. 09:49 If you're coming in from the last chapter, you know what we need to do then. 09:52 We compute a softmax along each one of these columns to normalize the values. 10:00 In our picture, after you apply softmax to all of the columns, 10:03 we'll fill in the grid with these normalized values. 10:06 At this point you're safe to think about each column as giving weights according 10:10 to how relevant the word on the left is to the corresponding value at the top. 10:15 We call this grid an attention pattern. 10:18 Now if you look at the original transformer paper, 10:20 there's a really compact way that they write this all down. 10:23 Here the variables q and k represent the full arrays of query 10:27 and key vectors respectively, those little vectors you get by 10:31 multiplying the embeddings by the query and the key matrices. 10:35 This expression up in the numerator is a really compact way to represent 10:39 the grid of all possible dot products between pairs of keys and queries. 10:44 A small technical detail that I didn't mention is that for numerical stability, 10:48 it happens to be helpful to divide all of these values by the 10:51 square root of the dimension in that key query space. 10:54 Then this softmax that's wrapped around the full expression 10:57 is meant to be understood to apply column by column. 11:01 As to that v term, we'll talk about it in just a second. 11:05 Before that, there's one other technical detail that so far I've skipped. 11:09 During the training process, when you run this model on a given text example, 11:13 and all of the weights are slightly adjusted and tuned to either reward or punish it 11:17 based on how high a probability it assigns to the true next word in the passage, 11:21 it turns out to make the whole training process a lot more efficient if you 11:25 simultaneously have it predict every possible next token following each initial 11:29 subsequence of tokens in this passage. 11:31 For example, with the phrase that we've been focusing on, 11:34 it might also be predicting what words follow creature and what words follow the. 11:39 This is really nice, because it means what would otherwise 11:42 be a single training example effectively acts as many. 11:46 For the purposes of our attention pattern, it means that you never 11:49 want to allow later words to influence earlier words, 11:52 since otherwise they could kind of give away the answer for what comes next. 11:56 What this means is that we want all of these spots here, 11:59 the ones representing later tokens influencing earlier ones, 12:02 to somehow be forced to be zero. 12:05 The simplest thing you might think to do is to set them equal to zero, 12:08 but if you did that the columns wouldn't add up to one anymore, 12:11 they wouldn't be normalized. 12:13 So instead, a common way to do this is that before applying softmax, 12:16 you set all of those entries to be negative infinity. 12:19 If you do that, then after applying softmax, all of those get turned into zero, 12:23 but the columns stay normalized. 12:26 This process is called masking. 12:27 There are versions of attention where you don't apply it, but in our GPT example, 12:31 even though this is more relevant during the training phase than it would be, 12:34 say, running it as a chatbot or something like that, 12:37 you do always apply this masking to prevent later tokens from influencing earlier ones. 12:42 Another fact that's worth reflecting on about this attention 12:45 pattern is how its size is equal to the square of the context size. 12:49 So this is why context size can be a really huge bottleneck for large language models, 12:54 and scaling it up is non-trivial. 12:56 As you imagine, motivated by a desire for bigger and bigger context windows, 13:00 recent years have seen some variations to the attention mechanism aimed at making 13:04 context more scalable, but right here, you and I are staying focused on the basics. 13:10 Okay, great, computing this pattern lets the model 13:12 deduce which words are relevant to which other words. 13:16 Now you need to actually update the embeddings, 13:18 allowing words to pass information to whichever other words they're relevant to. 13:22 For example, you want the embedding of Fluffy to somehow cause a change 13:26 to Creature that moves it to a different part of this 12,000-dimensional 13:30 embedding space that more specifically encodes a Fluffy creature. 13:35 What I'm going to do here is first show you the most straightforward 13:38 way that you could do this, though there's a slight way that 13:40 this gets modified in the context of multi-headed attention. 13:44 This most straightforward way would be to use a third matrix, 13:47 what we call the value matrix, which you multiply by the embedding of that first word, 13:51 for example Fluffy. 13:53 The result of this is what you would call a value vector, 13:55 and this is something that you add to the embedding of the second word, 13:59 in this case something you add to the embedding of Creature. 14:02 So this value vector lives in the same very high-dimensional space as the embeddings. 14:07 When you multiply this value matrix by the embedding of a word, 14:10 you might think of it as saying, if this word is relevant to adjusting the meaning of 14:15 something else, what exactly should be added to the embedding of that something else 14:19 in order to reflect this? 14:22 Looking back in our diagram, let's set aside all of the keys and the queries, 14:26 since after you compute the attention pattern you're done with those, 14:29 then you're going to take this value matrix and multiply it by every 14:32 one of those embeddings to produce a sequence of value vectors. 14:37 You might think of these value vectors as being 14:39 kind of associated with the corresponding keys. 14:42 For each column in this diagram, you multiply each of the 14:45 value vectors by the corresponding weight in that column. 14:50 For example here, under the embedding of Creature, 14:52 you would be adding large proportions of the value vectors for Fluffy and Blue, 14:57 while all of the other value vectors get zeroed out, or at least nearly zeroed out. 15:02 And then finally, the way to actually update the embedding associated with this column, 15:06 previously encoding some context-free meaning of Creature, 15:09 you add together all of these rescaled values in the column, 15:13 producing a change that you want to add, that I'll label delta-e, 15:16 and then you add that to the original embedding. 15:19 Hopefully what results is a more refined vector encoding the more 15:23 contextually rich meaning, like that of a fluffy blue creature. 15:27 And of course you don't just do this to one embedding, 15:30 you apply the same weighted sum across all of the columns in this picture, 15:34 producing a sequence of changes, adding all of those changes to the corresponding 15:38 embeddings, produces a full sequence of more refined embeddings popping out 15:42 of the attention block. 15:44 Zooming out, this whole process is what you would describe as a single head of attention. 15:49 As I've described things so far, this process is parameterized by three distinct 15:54 matrices, all filled with tunable parameters, the key, the query, and the value. 15:59 I want to take a moment to continue what we started in the last chapter, 16:02 with the scorekeeping where we count up the total number of model parameters using the 16:07 numbers from GPT-3. 16:09 These key and query matrices each have 12,288 columns, matching the embedding dimension, 16:15 and 128 rows, matching the dimension of that smaller key query space. 16:20 This gives us an additional 1.5 million or so parameters for each one. 16:24 If you look at that value matrix by contrast, the way I've described things so 16:30 far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, 16:35 since both its inputs and outputs live in this very large embedding space. 16:41 If true, that would mean about 150 million added parameters. 16:45 And to be clear, you could do that. 16:47 You could devote orders of magnitude more parameters 16:49 to the value map than to the key and query. 16:52 But in practice, it is much more efficient if instead you make 16:55 it so that the number of parameters devoted to this value map 16:57 is the same as the number devoted to the key and the query. 17:01 This is especially relevant in the setting of 17:03 running multiple attention heads in parallel. 17:06 The way this looks is that the value map is factored as a product of two smaller matrices. 17:11 Conceptually, I would still encourage you to think about the overall linear map, 17:15 one with inputs and outputs, both in this larger embedding space, 17:18 for example taking the embedding of blue to this blueness direction that you would 17:23 add to nouns. 17:27 It's just that it's a smaller number of rows, 17:29 typically the same size as the key query space. 17:33 What this means is you can think of it as mapping the 17:35 large embedding vectors down to a much smaller space. 17:39 This is not the conventional naming, but I'm going to call this the value down matrix. 17:43 The second matrix maps from this smaller space back up to the embedding space, 17:47 producing the vectors that you use to make the actual updates. 17:51 I'm going to call this one the value up matrix, which again is not conventional. 17:55 The way that you would see this written in most papers looks a little different. 17:58 I'll talk about it in a minute. 17:59 In my opinion, it tends to make things a little more conceptually confusing. 18:03 To throw in linear algebra jargon here, what we're basically doing 18:06 is constraining the overall value map to be a low rank transformation. 18:11 Turning back to the parameter count, all four of these matrices have the same size, 18:16 and adding them all up we get about 6.3 million parameters for one attention head. 18:22 As a quick side note, to be a little more accurate, 18:24 everything described so far is what people would call a self-attention head, 18:27 to distinguish it from a variation that comes up in other models that's 18:30 called cross-attention. 18:32 This isn't relevant to our GPT example, but if you're curious, 18:35 cross-attention involves models that process two distinct types of data, 18:39 like text in one language and text in another language that's part of an 18:43 ongoing generation of a translation, or maybe audio input of speech and an 18:48 ongoing transcription. 18:50 A cross-attention head looks almost identical. 18:52 The only difference is that the key and query maps act on different data sets. 18:57 In a model doing translation, for example, the keys might come from one language, 19:02 while the queries come from another, and the attention pattern could describe 19:06 which words from one language correspond to which words in another. 19:10 And in this setting there would typically be no masking, 19:12 since there's not really any notion of later tokens affecting earlier ones. 19:17 Staying focused on self-attention though, if you understood everything so far, 19:20 and if you were to stop here, you would come away with the essence of what attention 19:24 really is. 19:25 All that's really left to us is to lay out the 19:28 sense in which you do this many many different times. 19:32 In our central example we focused on adjectives updating nouns, 19:35 but of course there are lots of different ways that context can influence the 19:38 meaning of a word. 19:40 If the words they crashed the preceded the word car, 19:43 it has implications for the shape and structure of that car. 19:47 And a lot of associations might be less grammatical. 19:49 If the word wizard is anywhere in the same passage as Harry, 19:52 it suggests that this might be referring to Harry Potter, 19:55 whereas if instead the words Queen, Sussex, and William were in that passage, 20:00 then perhaps the embedding of Harry should instead be updated to refer to the prince. 20:05 For every different type of contextual updating that you might imagine, 20:08 the parameters of these key and query matrices would be different to 20:11 capture the different attention patterns, and the parameters of our 20:15 value map would be different based on what should be added to the embeddings. 20:19 And again, in practice the true behavior of these maps is much more 20:23 difficult to interpret, where the weights are set to do whatever the 20:26 model needs them to do to best accomplish its goal of predicting the next token. 20:31 As I said before, everything we described is a single head of attention, 20:35 and a full attention block inside a transformer consists of what's 20:38 called multi-headed attention, where you run a lot of these operations in parallel, 20:43 each with its own distinct key query and value maps. 20:47 GPT-3 for example uses 96 attention heads inside each block. 20:52 Considering that each one is already a bit confusing, 20:54 it's certainly a lot to hold in your head. 20:56 Just to spell it all out very explicitly, this means you have 96 21:00 distinct key and query matrices producing 96 distinct attention patterns. 21:05 Then each head has its own distinct value matrices 21:08 used to produce 96 sequences of value vectors. 21:12 These are all added together using the corresponding attention patterns as weights. 21:17 What this means is that for each position in the context, each token, 21:21 every one of these heads produces a proposed change to be added to the embedding in 21:26 that position. 21:27 So what you do is you sum together all of those proposed changes, 21:31 one for each head, and you add the result to the original embedding of that position. 21:36 This entire sum here would be one slice of what's outputted from this multi-headed 21:41 attention block, a single one of those refined embeddings that pops out the other end 21:47 of it. 21:48 Again, this is a lot to think about, so don't 21:50 worry at all if it takes some time to sink in. 21:52 The overall idea is that by running many distinct heads in parallel, 21:56 you're giving the model the capacity to learn many distinct ways that context 22:00 changes meaning. 22:03 Pulling up our running tally for parameter count with 96 heads, 22:07 each including its own variation of these four matrices, 22:10 each block of multi-headed attention ends up with around 600 million parameters. 22:16 There's one added slightly annoying thing that I should really 22:19 mention for any of you who go on to read more about transformers. 22:22 You remember how I said that the value map is factored out into these two 22:25 distinct matrices, which I labeled as the value down and the value up matrices. 22:29 The way that I framed things would suggest that you see this pair of matrices 22:34 inside each attention head, and you could absolutely implement it this way. 22:38 That would be a valid design. 22:40 But the way that you see this written in papers and the way 22:42 that it's implemented in practice looks a little different. 22:45 All of these value up matrices for each head appear stapled together in one giant matrix 22:50 that we call the output matrix, associated with the entire multi-headed attention block. 22:56 And when you see people refer to the value matrix for a given attention head, 23:00 they're typically only referring to this first step, 23:03 the one that I was labeling as the value down projection into the smaller space. 23:08 For the curious among you, I've left an on-screen note about it. 23:11 It's one of those details that runs the risk of distracting 23:13 from the main conceptual points, but I do want to call it out 23:16 just so that you know if you read about this in other sources. 23:19 Setting aside all the technical nuances, in the preview from the last chapter we saw 23:23 how data flowing through a transformer doesn't just flow through a single attention block. 23:28 For one thing, it also goes through these other operations called multi-layer perceptrons. 23:33 We'll talk more about those in the next chapter. 23:35 And then it repeatedly goes through many many copies of both of these operations. 23:39 What this means is that after a given word imbibes some of its context, 23:43 there are many more chances for this more nuanced embedding 23:47 to be influenced by its more nuanced surroundings. 23:50 The further down the network you go, with each embedding taking in more and more 23:54 meaning from all the other embeddings, which themselves are getting more and more 23:59 nuanced, the hope is that there's the capacity to encode higher level and more 24:03 abstract ideas about a given input beyond just descriptors and grammatical structure. 24:07 Things like sentiment and tone and whether it's a poem and what underlying 24:11 scientific truths are relevant to the piece and things like that. 24:16 Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, 24:22 so the total number of key query and value parameters is multiplied by another 96, 24:27 which brings the total sum to just under 58 billion distinct parameters 24:32 devoted to all of the attention heads. 24:34 That is a lot to be sure, but it's only about a third 24:38 of the 175 billion that are in the network in total. 24:41 So even though attention gets all of the attention, 24:44 the majority of parameters come from the blocks sitting in between these steps. 24:48 In the next chapter, you and I will talk more about those 24:51 other blocks and also a lot more about the training process. 24:54 A big part of the story for the success of the attention mechanism is not so much any 24:58 specific kind of behavior that it enables, but the fact that it's extremely 25:03 parallelizable, meaning that you can run a huge number of computations in a short time 25:07 using GPUs. 25:09 Given that one of the big lessons about deep learning in the last decade or two has 25:13 been that scale alone seems to give huge qualitative improvements in model performance, 25:17 there's a huge advantage to parallelizable architectures that let you do this. 25:22 If you want to learn more about this stuff, I've left lots of links in the description. 25:25 In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold. 25:30 In this video, I wanted to just jump into attention in its current form, 25:33 but if you're curious about more of the history for how we got here 25:36 and how you might reinvent this idea for yourself, 25:38 my friend Vivek just put up a couple videos giving a lot more of that motivation. 25:43 Also, Britt Cruz from the channel The Art of the Problem has 25:45 a really nice video about the history of large language models. 26:04 Thank you. The Influence of Social Media Marketing on Voting Intention in Indonesia Author: Massoud Moslehpour, Date: 08 Feb 2024 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: UFVI3BDY Cite Key: Moslehpour24socMediaInflIntent Zotero Item | Lit Note The Influence of Social Media Marketing on Voter's Intention in Indonesia Massoud Moslehpour Department of Business Administration, Asia University, Taiwan, writetodrm@gmail.com Stephen Lewi Department of Business Administration, Asia University, Taiwan, stephenlewi97@gmail.com Dessy Kurniawati Department of Business Administration, Asia University, Taiwan; dessyk1406@gmail.com Taufiq Ismail** Department of Business Administration, Asia University, Taiwan; taufiq.ismail@ub.ac.id Yeneneh T. Negash** ** Department of Business Administration, Asia University, Taiwan; yenuta@gmail.com *Presenting Author; Corresponding Author This study aims to describe the interaction of social media marketing, candidate image, religious beliefs, and voter's intention in Indonesia. This study applies the quantitative method to examine the proposed hypotheses. The questionnaires were distributed to Indonesian social media users who have eligible electoral rights. This study uses primary data collection and gathers 396 respondents via an online questionnaire. Further, SEM and path analysis methods were conducted to examine the data in this study. Social media marketing, candidate image, and religious beliefs positively affect the voter's intentions. The two dimensions of the social media marketing that directly influence a voter's intention are interaction and customization. This research limits the respondents to Indonesian media social users whom eligible have electoral rights. The findings of this research suggest several practical directions for candidate or political party and their team using social media marketing.     CCS CONCEPTS • Information systems World Wide Web Online advertising Social advertising Content match advertising Additional Keywords and Phrases: Social Media, Social Media Marketing, Voter's Intention, Election, Indonesia 1 INTRODUCTION Social media has become an essential part of society and turned into a useful asset for communicating suppositions, perspectives, and thoughts by considering the recent growth of internet users. Social media also becomes an influential instrument for creating an opinion, including in political context lately. Political parties use social media as an instrument to communicate with their competitors, potential voters, and other related parties. Moreover, political parties or politicians consider using social media as their advertising purpose or marketing tool [1]. Social media has been significantly developed as a discussion medium for political activities in its various structures [2]. There are several candidates or electoral parties that have used social media as an efficient campaigning tool. A successful campaign will also help to build a stronger democracy. Further, social networking sites have been progressively assuming a fundamental role in the 2019 Indonesian Presidential Election Campaign. In Indonesia, there are 268 million population in which 150 million of them are active internet users and social networking sites. In addition, they are mostly recognized as eligible voters with ages ranging between 18 to 34 years old. Typically, they use numerous virtual newsrooms or social networking sites and spend over 50% of their time over the Internet [3]. Indonesian people share their aspirations about the political candidate and presidential election. Also, they are involved in some conversations and discussions about the issues related to the candidates after a week of the live presidential debate on the television. Furthermore, some non-political accounts are also associated with political conversation to elevate the popularity of the candidates [3]. Currently, social media influences every aspect of human life, including in the elections. Social media has become one of the most important electoral campaign platforms, not merely in Indonesia but also worldwide. Social media has been expected as fair and competitive electoral strategies. Thus, politicians will take advantage of the incentives and start using websites for the campaign. Therefore, this research discusses social media marketing's influence on voters' intention with religious beliefs and candidate image as the mediating variables. Social media marketing consists of entertainment, interaction, trendiness, and customization. 2 LITERATURE REVIEW 2.1 Election and 2019 Indonesia General Election An election is an official procedure of choosing an individual for public office or the process to accept or reject political proposition in democracy system through voting [4]. Indonesia held a general election on April 17th, 2019. This election was the first moment in Indonesia's political history that simultaneously elect house or representatives both at national and local levels as well as the president at same day. More than 190 million of Indonesia's citizens participated in the general election. There are two candidates for the presidential election and sixteen political parties participated in the 2019 Indonesian general election. In September 2018, candidates' nominations for the general election, including the presidential election, were finalized. The period for campaigning was started from October 13rd, 2018 to April, 13rd, 2019. 2.2 Social Media Marketing Nowadays, social media is not only utilized to post activities and opinions among people, but also as e means of digital media marketing [5]. Social media marketing grows fast because of high market demand and plays a crucial role in people's daily routines. Costumers usually use their social networking for their decisionmaking process about certain products or services [6]. There are several advantages to use social media marketing. First, collecting more information in a relatively speedy way. Second, it can be used by companies as a media to communicate with consumers while they are in their purchasing process. Third, it can speed up word-of-mouth advertising, including the consumers' comments about their buying experience that may impact another prospective consumer [5]. This research discusses social media marketing adapted from Kim and Ko's proposed framework [7] in the Indonesian Election context. The social media marketing components from Kim and Ko's framework [7] include entertainment, interaction, trendiness, and customization that will be further explained below: 2.2.1. Entertainment Entertainment can drive users of social media to some point [8]. Entertainment experience is important to boost the values of the brand or organization perceived by the individuals [9]. Entertainment in social media marketing points out that the information and substance formed by a specific brand give amusement and pleasure [10]. 2.2.2. Interaction Social interaction is a significant motivator for making users-created content [11]. Social networking sites can help users bringing more space to exchange opinions and discussions. Social networking website's intensive interactions can motivate individuals to change their intention to purchase [12]. Social media also can increase an individual's trust in the organization[13]. Feedback and interaction are the essential aspects of social media [14]. 2.2.3. Trendiness Social media delivers the newest information and hottest topics for discussions [15]. Social media helps the eligible voters to learn more about the candidate's latest information and understand better about the electoral agendas in a more advanced and modern way. Trendiness in social media marketing spreads the latest and the most common evidence about a specific political figure’s brand image for this research context [10]. The important thing for the brand on social media is to spread trending information and increase engagement among consumers and brands [16]. 2.2.4. Customization Individuals tend to seek information that fits their desires and preferences [17]. Customization in social media marketing points out that social media networking offers convenience in seeking customized services and information [10]. 2.3. Candidate Image The political candidate's key traits are intelligence, morality, power, credibility, leadership, and competence [18]. The candidate image can be measured based on personality attributes that characterize one or more aspects of the candidate's image [19]. The candidate image represents an overall evaluation of the candidates based on their personality traits [20]. Researchers found that when they studied voter behavior, they discovered that the candidate's image also played an important role in political elections [20]. 2.4. Religious Beliefs Religious beliefs are a group's practices, values, and beliefs taught by a spiritual leader [21]. There is a significant impact of religion on political behavior [22]. Religious beliefs in a political candidate are perceived as how the candidate/party's religious beliefs can influence voters' decisions on a candidate or political party [20]. Some researchers have explored religious beliefs in their perspectives [22]. 2.5. Voter's intention Voter's intention becomes a global issue regarding success and effective political elections [23]. The intention is termed as a person's premeditated or anticipated future behavior [24]. The intention is a person's willingness to vote in the upcoming election. Ben-Ur & Newman developed the model of voter behavior that the aim is to analyze the beliefs, attitudes, motives, intentions of the voter. This study used the key variable \"situational contingency\" that influences the potential voters' thinking of the election result [25]. 2.6. The Interrelationship between Social Media, Candidate Image, Religious Beliefs, and Voter's Intention Customer relationship has played a significant role in affecting consumers' luxury fashion brands [37, 38]. The finding revealed that the entertaining and interactive marketing via Facebook and mostly shared by followers could make a closer relationship between students and educational institutions and increase enrollment intention. For political context, media had a significant role in delivering the political brand to the voters' intention, and there was an extensive impact using social media by the political candidates or parties [20]. Therefore, it is supposed that: H1: SMM has a positively significant influence on VI Social media marketing had a positive impact on the company's brand image [26, 27]. Social media marketing had a significant effect on the organization's brand image in the sector of hospitality. This study proposes a positive relationship between social media marketing and a person's image [28] but associated in the political field, or electoral in specific. Other studies indicate that media had a significant role in delivering the political brand to the voters and there was an extensive impact using social media by the political candidates or parties [20]. Therefore, it is supposed that: H2: SMM has a positively significant influence on CI Media become the triggers of religious substance, aesthetic, the institutional, and technological qualities of the media impact the confining of religion and how users and audiences should interact with religion. Media may make virtual worlds and narratives that welcome individuals to have encounters of a religious-like character [29]. There is a complex relationship between digital media and the religiosity of Web 2.0 [30]. Media can be the space to transform the content of specific religions, rites, and beliefs. Social media provides the benefit to allow believers and non-believers by giving them space for easy interactions with each other [31]. Therefore, it is supposed that: H3: SMM has a positively significant influence on RB Seven factors affect the voter's intention to vote [19]. Furthermore, one of the most important factors that influenced a voter's intention is a candidate image. Another study confirmed that the candidate image also has significant domains determining the intention to vote [20, 32]. The finding revealed that voters' intentions also focused on the candidate image in political elections. The result mentioned that there is a positive relationship between the candidate image and the voter's intention. Therefore, it is supposed that: H4: CI has a positively significant influence on VI Religious beliefs affect individuals' behaviors and attitudes [22, 33]. The impact of religion, including the ideologies of several religious groups, has changed over time on social issues. Individual's religious beliefs can affect their behavior and attitudes [34]. Religious beliefs had an impact on, particularly controversial products and social or political groups [35]. Another study discovered that the candidate and political party won the citizens' sympathy to vote for them during the elections by using the injections of their religious beliefs and ideologies [36]. Therefore, it is supposed that: H5: RB has a positively significant influence on VI 3. RESEARCH METHODOLOGY 3.2. Research Design and Variables The independent variable of this study is Social Media Marketing. There are four aspects of Social Media Marketing, namely Entertainment, Interaction, Trendiness, and Customization, that influence voters' intention, as presented in the figure 1 below. Figure 1. Research framework 3.3. Measurement and Scale This study gathers information based on the previous literature review with some modifications to fit the study's context. There are thirteen (13) items to measure social media marketing derived from two research [7, 10], five (5) items to measure candidate image derived from previous research [19], four (4) items to measure religious beliefs obtained from a previous study [21], and four (4) items to measure voter's intention derived from two previous studies [37, 38]. This survey uses the five-point Likert Scale, ranging from strongly disagree to strongly agree. 3.4. Data Collection and Analysis A total of 396 valid questionnaires were collected from Indonesian social media users eligible for the election through an online questionnaire. Statistical Package for the Social Science (SPSS) and Analysis of Moment Structures (AMOS) is used to analyze the data. Several statistical methods will be used in this study, such as Descriptive Statistical Analysis, Exploratory and Confirmatory Factor Analysis, Reliability Analysis or Cronbach's alpha, Structural Equation Modeling (SEM). 4. RESULTS AND DISCUSSION Out of 396 questionnaires, 43.7% of them are male, and 56.3% of the rest are females. The respondents' ages are between 17 and 60 years old, and most of them range from 22–26 years (43.9%). Meanwhile, most of the respondents had the highest education level at the bachelor's degree (64.9%). Over half of the total respondents (53.6%) are employees. In terms of religion, more than half of the respondents (70.2%) dominate by Muslims. In terms of using social media, most respondents use social media (100%), and most respondents (58.3%) use Instagram more often than other social media. 4.2. Construct Validity Exploratory factor analysis (EFA) was conducted to measure the variables' loading and assess the validity of the measurement items. The latent components analysis with Varimax rotation was applied to discover the latent components of the measurement model item and maximize the squared loadings' total variances. Furthermore, the Kaiser-Meyer-Olkin (KMO) and Barlett's Test of Sphericity were performed to verify the construct's validity and data collection feasibility. After undergoing several testing steps, five questionnaire items (ENT4, CUS1, CI5, RB2, and VI3) showed a factor loading value less than 0.50. After eliminating the five questionnaire items above, the questionnaire used in this research became more reliable. The KMO showed a better result = 0.80, with p-value = 0.00 and total variance extracted = 74.72%. CFA model analysis indicates overall fit. It shows the proposed model framework has a good level of fit: X2/df = 1.82, p value = 0.00, Goodness of Fit Index (GFI) = 0.93, Comparative Fit Index (CFI) = 0.96, TuckerLewis Index (TLI) = 0.95, and Root Mean Square Error of Approximation (RMSEA) = 0.05 as presented in the Table 1 below. Table 1. Reliability, Discriminant validity, convergent validity, and composite reliability (n=396) Items α CR AVE MSV Max r CUS TRD CI INT RB VI ENT CUS 2 0.73 0.737 0.586 0.278 0.527 0.766 TRD 3 0.88 0.887 0.726 0.107 0.305 0.327 0.852 CI 4 0.85 0.851 0.588 0.169 0.403 0.248 0.079 0.767 INT 3 0.75 0.752 0.508 0.278 0.527 0.527 0.217 0.234 0.713 RB 3 0.81 0.812 0.591 0.162 0.403 0.190 0.122 0.403 0.174 0.769 VI 3 0.86 0.883 0.723 0.169 0.327 0.327 0.054 0.305 0.331 0.326 0.850 ENT 3 0.75 0.751 0.502 0.154 0.393 0.393 0.305 0.275 0.311 0.260 0.193 0.709 The results of discriminant and convergent validity tests, and composite reliability test show the value standards for each factor as follows: CR > 0.7, AVE > 0.5, MSV < AVE, and √AVE > Max Correlation [39]. The overall results show that the validity and reliability of all components used in this study are considered good. 4.3. Path Analysis Result Structural Equations Modeling is used to analyze and assess the relations and hypotheses between research variables. SEM is used to determine the direction and significance of those relationships [40]. The detailed outcome of the model fit indices of the proposed framework model. X2/df = 1.98 with p-value = 0.00. Further, Goodness of Fit Index (GFI) = 0.92, Comparative Fit Index (CFI) = 0.95, Adjusted Goodness of Fit Index (AGFI) = 0.90, and Root Mean Square Error of Approximation (RMSEA) = 0.05. The standardized coefficients are used because the value of the coefficients is in the normalized units. As shown in Table 2 below, all hypotheses are supported. SMM positively influences VI with β= 0.285***, with the** p-value = 0.000. SMM influence CI, proven by β= 0.411***, with the p-value = 0.000. SMM also affects RB, it** was proved by β= 0.367***, with the p-value = 0.000. CI to VI was supported with β = 0.249**, and RB to VI was also shown supported with β= 0.134*, with the p-value = 0.021. Figure 2 shows the path analysis results.* Further, the hypothesis testing results are preseted in Table 2. Figure 2. The result of the Structural Equation Model (SEM) path analysis Table 2. Result of hypotheses testing Hypothesis Path β S E P-value Result H1 SMM → VI 0.285*** 0.122 0.000 Supported** H2 SMM → CI 0.411*** 0.103 0.000 Supported** H3 SMM → RB 0.367*** 0.101 0.000 Supported** H4 CI → VI 0.249*** 0.073 0.000 Supported** H5 RB → VI 0.134* 0.070 0.021 Supported* 4.4. Discussion As several previous studies revealed that social media marketing has a positive influence on purchase intention [7, 41, 42], this study's results also get a consistent outcome with the former research. However, this study contributes some new knowledge and information as this study explores voter's intention that is not considered in the previous study. In line with that, this study indicated that social media marketing significantly impacts voter's intention (β = 0.285***). Therefore, it can be concluded that better management of social** media marketing handled by a candidate or political party and their team will result in greater voters' intention to give a vote to them. From the statistical analysis results, it was also found that there were two dimensions of the social media marketing that significantly influence voter's intention in this study, namely interaction and customization. Former studies reveal that social media marketing positively affects the brand image [10, 26-28]. The results of this study also get a consistent outcome with the former studies. However, this study brings a person's image as its contribution since it is not considered in the previous study. In line with that, this study indicated that social media marketing significantly impacts the candidate image (β = 0.411***). It means that** candidate's image will become more positive if they can appropriately manage their social media marketing. From the statistical analysis results, it was also found that three dimensions of social media marketing significantly affect candidate image in this study, namely entertainment, interaction, and customization. The presented results reveal that social media marketing directly impacts religious beliefs (β = 0.367***).** This study also reported similar results conducted by previous studies [29, 31]. However, this study contributes to some new knowledge and information because social media marketing significantly influence religious beliefs is not considered in previous research. So, it can be concluded that if the candidate can control their marketing on social media well, they will get the positively religious beliefs' impact. The statistical analysis results demonstrated that one dimension of social media marketing has a significant positive influence on religious beliefs in this research, namely entertainment. It has been confirmed that a candidate image directly and significantly impacts the voter's intention [19, 20, 32]. The result emphasized in this study indicated a significant positive relationship between the candidate's image and the voter's intention (β = 0.249***). It implies a better perception of the candidate image among** voters' minds will lead to a higher voter's intention to vote. Voters may have a good or poor perception of the candidate that will directly influence the voting decisions. Farrag and Shamma revealed in a previous study about factors influencing voter's intention in Egypt that religious beliefs are one of the essential variables to Newman and Sheth's model [19, 20]. Moreover, another study also showed the same consistent results [36]. The statistical analysis outcomes show that religious beliefs significantly and positively influence a voter's intention (β = 0.134*). It means that if the candidate or* political party can use religious beliefs as the political marketing tool appropriately, it will increase the chance for them to win the voters' sympathy. Religious beliefs are essential because they can affect every aspect of people's lives, including behaviors, attitudes, and preferences, especially in social and political ideas and groups. 5. CONCLUSION AND RECOMMENDATION Social media marketing has a significant influence on a voter's intention. Social media marketing has become one of the essential aspects of marketing. It is also can be considered as a political marketing tool and campaign. Social media provides easy to use and low-cost to share information about the election, the candidate or political party. If they can use social media appropriately, it will give many benefits and conveniences. This study's findings provide some practical aspects for the candidate of a political party and their team success who use social media as a political marketing tool. This study delivers an explanation of the aspects that become the basic considerations for voter's intention to vote for the candidate with social media marketing. Based on the results, social media marketing, candidate image, and religious beliefs significantly affected the voter's intention. Further, for managerial implications, there are two pivotal contributions. The aspects of social media marketing consist of entertainment, interaction, trendiness, and customization, and the whole of these dimensions impacts voter's intention to vote. First, two dimensions are more relevant and directly affect a voter's intention to vote in the election, namely interaction and customization. According to this result, the candidate and their team should pay attention and give more effort to develop those two dimensions to increase the potential voters. Secondly, the candidate or political party and their team should not use social media marketing as only a tool to get the potential voters and as a useful tool for creating candidate image and religious beliefs since this study presents that candidate image and religious beliefs have a deeper meaning to voters. If they can manage to make the voters have a good perception of the candidate's image and religious beliefs will not be unwilling to vote for the candidate or political party. 6. REFERENCES [1] Safiullah, Md, Pathak, Pramod, Singh, Saumya and Anshul, Ankita, 2017. Social media as an upcoming tool for political marketing effectiveness. Asia Pacific Management Review (02/01). DOI: 10.1016/j.apmrv.2016.10.007. [2] Tenhunen, Susanna and Karvelyte, Vilma, 2015. The Role Played By Social Media In Political Participation And Electoral Campaigns. [3] Rizal, Yose, 2019. Social Media and Indonesia's 2019 Elections. [4] Eulau, Heinz, Gibbins, Roger and Webb, Paul David, 2015. Election Political Science. [5] Zhao, Fang and Collier, Alan, 2016. Digital Entrepreneurship: Research and Practice. [6] Filo, Kevin, Lock, Daniel and Karg, Adam, 2015. Sport and social media research: A review. Sport Management Review 18 (05/01). DOI: 10.1016/j.smr.2014.11.001. [7] Kim, Angella and Ko, Eunju, 2010. Impacts of Luxury Fashion Brand's Social Media Marketing on Customer Relationship and Purchase Intention. Journal of Global Fashion Marketing 1 (08/01), 164171. DOI: 10.1080/20932685.2010.10593068. [8] Park, Namsu, Kee, Kerk and Valenzuela, Sebastián, 2009. Being Immersed in Social Networking Environment: Facebook Groups, Uses and Gratifications, and Social Outcomes. Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society 12 (08/01), 729-733. DOI: 10.1089/cpb.2009.0003. [9] Song, Hak, Lee, Choong-Ki, Park, Jin, Hwang, Yoo and Reisinger, Yvette, 2014. The Influence of Tourist Experience on Perceived Value and Satisfaction with Temple Stays: The Experience Economy Theory. Journal of Travel & Tourism Marketing 32 (11/07), 401-415. DOI: 10.1080/10548408.2014.898606. [10] Godey, Bruno, Manthiou, Aikaterini, Pederzoli, Daniele, Rokka, Joonas, Aiello, Gaetano, Donvito, Raffaele and Singh, Rahul, 2016. Social media marketing efforts of luxury brands: Influence on brand equity and consumer behavior. Journal of Business Research 69 12 (2016/12/01/), 5833- 5841. DOI: https://doi.org/10.1016/j.jbusres.2016.04.181. [11] Daugherty, Terry, Li, Hairong and Biocca, Frank, 2008. Consumer Learning and the Effects of Virtual Experience Relative to Indirect and Direct Product Experience. Psychology and Marketing 25 (07/01), 568-586. DOI: 10.1002/mar.20225. [12] Clark, Melissa and Phillips Melancon, Joanna, 2013. The Influence of Social Media on Relational Outcomes: A Relationship Marketing Perspective. Academy of Marketing Studies Journal 5 (08/01), 132-142. DOI: 10.5539/ijms.v5n4p132. [13] Jakic, Ana, Wagner, Maximilian and Meyer, Anton, 2017. The impact of language style accommodation during social media interactions on brand trust. Journal of Service Management 28 (06/16), 00-00. DOI: 10.1108/JOSM-12-2016-0325. [14] Kaplan, Andreas and Haenlein, Michael, 2010. Users of the World, Unite! The Challenges and Opportunities of Social Media. Business Horizons 53 (02/28), 59-68. DOI: 10.1016/j.bushor.2009.09.003. [15] Naaman, Mor, Becker, Hila and Gravano, Luis, 2011. Hip and Trendy: Characterizing Emerging Trends on Twitter. JASIST 62 (05/01), 902-918. DOI: 10.1002/asi.21489. [16] Song, So Young, Cho, Erin and Kim, Youn-Kyung, 2017. Personality factors and flow affecting opinion leadership in social media. Personality and Individual Differences 114 (08/01), 16-23. DOI: 10.1016/j.paid.2017.03.058. [17] Yahia, Imene Ben, Al-Neama, Nasser and Kerbache, Laoucine, 2018. Investigating the drivers for social commerce in social media platforms: Importance of trust, social support and the platform perceived usage. Journal of Retailing and Consumer Services 41 (2018/03/01/), 11-19. DOI: https://doi.org/10.1016/j.jretconser.2017.10.021. [18] Kinder, Donald R., 1994. Beliefs, Reasoning, and Decision Making. [19] Newman, Bruce and Sheth, Jagdish, 1985. A Model of Primary Voter Behavior. Journal of Consumer Research 12 (02/01), 178-187. DOI: 10.1086/208506. [20] Farrag, Dalia and Shamma, Hamed, 2014. Factors influencing voting intentions for Egyptian parliament elections 2011. Journal of Islamic Marketing 5 (03/04). DOI: 10.1108/JIMA-01-2013-0003. [21] Plante, Thomas and Boccaccini, Marcus, 1997. The Santa Clara Strength of Religious Faith Questionnaire. Pastoral Psychology 45 (05/01), 375-387. DOI: 10.1007/BF02230993. [22] Fastnow, Chris, Grant, J. Tobin and Rudolph, Thomas J., 1999. Holy Roll Calls: Religious Tradition and Voting Behavior in the US House. Social Science Quarterly 80 4, 687-701. [23] Chuchu, Tinashe, 2015. To Vote or Not To Vote: Marketing Factors Influencing the Voting Intention of University Students in Johannesburg. Journal of Economics and Behavioral Studies Vol. 7 (12/16), pp. 81-93. [24] Lam, Terry and Hsu, Cathy H. C., 2006. Predicting behavioral intention of choosing a travel destination. Tourism Management 27 4 (2006/08/01/), 589-599. DOI: https://doi.org/10.1016/j.tourman.2005.02.003. [25] Ben-Ur, Joseph and Newman, Bruce, 2002. Motives, Perceptions and Voting Intention of Voters in the 2000 US Presidential Election. Psychology and Marketing 19 (12/01), 1047-1065. DOI: 10.1002/mar.10052. [26] Seo, Eun-Ju and Park, Jin-Woo, 2018. A study on the effects of social media marketing activities on brand equity and customer response in the airline industry. Journal of Air Transport Management 66 (2018/01/01/), 36-41. DOI: https://doi.org/10.1016/j.jairtraman.2017.09.014. [27] Wang, Hsing-Wen, Wu, Yenchun and Dong, Tse-Ping, 2015. Exploring the Impacts of Social Networking on Brand Image and Purchase Intention in Cyberspace. JOURNAL OF UNIVERSAL COMPUTER SCIENCE 21 (12/01), 1425-1438. [28] Perera, Graham and Perera, Irosha, 2016. Influence of Social Media Marketing on the Brand Image of Organizations in the Hospitality Industry of Sri Lanka. International Journal of Asian Business and Information Management 7 (01/01), 30-41. DOI: 10.4018/IJABIM.2016010103. [29] Hjarvard, Stig, 2011. The Mediatisation of Religion: Theorising Religion, Media and Social Change. Culture and Religion 12 (06/01), 119-135. DOI: 10.1080/14755610.2011.579719. [30] Cheong, Pauline Hope, Peter, Fischer-Nielsen, Stefan, Gelfgren and Charles, Ess, 2012. Digital Religion, Social Media and Culture. Peter Lang, Bern, Switzerland. [31] Coman, Ioana and Coman, Mihai, 2017. Religion, popular culture and social media: The construction of a religious leader image on facebook. ESSACHESS - Journal for Communication Studies 10 (12/01), 129-143. [32] Cwalina, Wojciech, Falkowski, Andrzej and Newman, Bruce, 2010. Towards the development of a cross-cultural model of voter behavior: Comparative analysis of Poland and the US. European Journal of Marketing 44 (04/06), 351-368. DOI: 10.1108/03090561011020462. [33] Abu-Alhaija, Ahmad, Nerina, Raja, Hashim, Haslinda and Jaharuddin, Nor Siah, 2018. Religion in Consumer Behaviour Research: the Significance of Religious Commitment and Religious Affiliation (02/07). [34] Kuzma, Ann and Kuzma, J., 2009. How religion has Embraced Marketing and the Implications for Business. Journal of Management and Marketing Research 2 (01/01), 1-10. [35] Fam, Kim, Waller, David and Erdogan, B. Zafer, 2004. The influence of religion on attitudes towards the advertising of controversial products. European Journal of Marketing 38 (05/01), 537-555. DOI: 10.1108/03090560410529204. [36] Wald, Kenneth, Silverman, Adam and Fridy, Kevin, 2005. Making Sense of Religion in Political Life. Annu. Rev. Polit. Sci 8 (06/15), 121-143. DOI: 10.1146/annurev.polisci.8.083104.163853. [37] Duffett, Rodney, 2017. Influence of social media marketing communications on young consumers' attitudes. Young Consumers 18 (04/01). DOI: 10.1108/YC-07-2016-00622. [38] Glynn, Carroll, Huge, Michael and Lunney, Carole, 2009. The Influence of Perceived Social Norms on College Students' Intention to Vote. Political Communication 1 (02/09), 48-64. DOI: 10.1080/10584600802622860. [39] Moslehpour, Massoud, Wong, Wing-Keung, Lin, Yi and Nguyen, Thi Le, 2017. Top purchase intention priorities of Vietnamese low cost carrier passengers: expectations and satisfaction. Eurasian Business Review 8 (10/20). DOI: 10.1007/s40821-017-0093-5. [40] Baumgartner, Hans and Homburg, Christian, 1996. Applications of structural equation modeling in marketing and consumer research: A review. International Journal of Research in Marketing 13 2 (1996/04/01/), 139-161. DOI: https://doi.org/10.1016/0167-8116(95)00038-0. [41] Gautam, Dr Vikas and Sharma, Vikram, 2017. The Mediating Role of Customer Relationship on the Social Media Marketing and Purchase Intention Relationship with Special Reference to Luxury Fashion Brands. Journal of Promotion Management (06/09), 1-17. DOI: 10.1080/10496491.2017.1323262. [42] Spackman, Jonathan and Larsen, Ross, 2017. Evaluating the Impact of Social Media Marketing on Online Course Registration. The Journal of Continuing Higher Education 65 (09/02), 151-165. DOI: 10.1080/07377363.2017.1368774. How Hitler Dismantled a Democracy in 53 Days Author: Ryback, Timothy W., Date: 2025-01-08 Collections: PollMethods, PartyOrganization Zotero Key: 86Z75V53 Cite Key: Ryback25hitlerDismantledDemoc53days Zotero Item | Lit Note Ninety-two years ago this month, on Monday morning, January 30, 1933, Adolf Hitler was appointed the 15th chancellor of the Weimar Republic. In one of the most astonishing political transformations in the history of democracy, Hitler set about destroying a constitutional republic through constitutional means. What follows is a step-by-step account of how Hitler systematically disabled and then dismantled his country’s democratic structures and processes in less than two months’ time—specifically, one month, three weeks, two days, eight hours, and 40 minutes. The minutes, as we will see, mattered. Hans Frank served as Hitler’s private attorney and chief legal strategist in the early years of the Nazi movement. While later awaiting execution at Nuremberg for his complicity in Nazi atrocities, Frank commented on his client’s uncanny capacity for sensing “the potential weakness inherent in every formal form of law” and then ruthlessly exploiting that weakness. Following his failed Beer Hall Putsch of November 1923, Hitler had renounced trying to overthrow the Weimar Republic by violent means but not his commitment to destroying the country’s democratic system, a determination he reiterated in a Legalitätseid—“legality oath”—before the Constitutional Court in September 1930. Invoking Article 1 of the Weimar constitution, which stated that the government was an expression of the will of the people, Hitler informed the court that once he had achieved power through legal means, he intended to mold the government as he saw fit. It was an astonishingly brazen statement. “So, through constitutional means?” the presiding judge asked. “Jawohl!” Hitler replied. By January 1933, the fallibilities of the Weimar Republic—whose 181-article constitution framed the structures and processes for its 18 federated states—were as obvious as they were abundant. Having spent a decade in opposition politics, Hitler knew firsthand how easily an ambitious political agenda could be scuttled. He had been co-opting or crushing right-wing competitors and paralyzing legislative processes for years, and for the previous eight months, he had played obstructionist politics, helping to bring down three chancellors and twice forcing the president to dissolve the Reichstag and call for new elections. When he became chancellor himself, Hitler wanted to prevent others from doing unto him what he had done unto them. Though the vote share of his National Socialist party had been rising—in the election of September 1930, following the 1929 market crash, they had increased their representation in the Reichstag almost ninefold, from 12 delegates to 107, and in the July 1932 elections, they had more than doubled their mandate to 230 seats—they were still far from a majority. Their seats amounted to only 37 percent of the legislative body, and the larger right-wing coalition that the Nazi Party was a part of controlled barely 51 percent of the Reichstag, but Hitler believed that he should exercise absolute power: “37 percent represents 75 percent of 51 percent,” he argued to one American reporter, by which he meant that possessing the relative majority of a simple majority was enough to grant him absolute authority. But he knew that in a multiparty political system, with shifting coalitions, his political calculus was not so simple. He believed that an Ermächtigungsgesetz (“empowering law”) was crucial to his political survival. But passing such a law—which would dismantle the separation of powers, grant Hitler’s executive branch the authority to make laws without parliamentary approval, and allow Hitler to rule by decree, bypassing democratic institutions and the constitution—required the support of a two-thirds majority in the fractious Reichstag. The process proved to be even more challenging than anticipated. Hitler found his dictatorial intentions getting thwarted within his first six hours as chancellor. At 11:30 that Monday morning, he swore an oath to uphold the constitution, then went across the street to the Hotel Kaiserhof for lunch, then returned to the Reich Chancellery for a group photo of the “Hitler Cabinet,” which was followed by his first formal meeting with his nine ministers at precisely 5 o’clock. Hitler opened the meeting by boasting that millions of Germans had welcomed his chancellorship with “jubilation,” then outlined his plans for expunging key government officials and filling their positions with loyalists. At this point he turned to his main agenda item: the empowering law that, he argued, would give him the time (four years, according to the stipulations laid out in the draft of the law) and the authority necessary to make good on his campaign promises to revive the economy, reduce unemployment, increase military spending, withdraw from international treaty obligations, purge the country of foreigners he claimed were “poisoning” the blood of the nation, and exact revenge on political opponents. “Heads will roll in the sand,” Hitler had vowed at one rally. From the March 1932 issue: Hitler and Hitlerism: a man of destiny But given that Social Democrats and Communists collectively commanded 221 seats, or roughly 38 percent, of the 584-seat Reichstag, the two-thirds vote Hitler needed was a mathematical impossibility. “Now if one were to ban the Communist Party and annul their votes,” Hitler proposed, “it would be possible to reach a Reichstag majority.” The problem, Hitler continued, was that this would almost certainly precipitate a national strike by the 6 million German Communists, which could, in turn, lead to a collapse of the country’s economy. Alternatively, Reichstag percentages could be rebalanced by holding new elections. “What represents a greater danger to the economy?” Hitler asked. “The uncertainties and concerns associated with new elections or a general strike?” Calling for new elections, he concluded, was the safer path. Economic Minister Alfred Hugenberg disagreed. Ultimately, Hugenberg argued, if one wanted to achieve a two-thirds Reichstag majority, there was no way of getting around banning the Communist Party. Of course, Hugenberg had his own self-interested reasons for opposing new Reichstag elections: In the previous election, Hugenberg had siphoned 14 seats from Hitler’s National Socialists to his own party, the German Nationalists, making Hugenberg an indispensable partner in Hitler’s current coalition government. New elections threatened to lose his party seats and diminish his power. When Hitler wondered whether the army could be used to crush any public unrest, Defense Minister Werner von Blomberg dismissed the idea out of hand, observing “that a soldier was trained to see an external enemy as his only potential opponent.” As a career officer, Blomberg could not imagine German soldiers being ordered to shoot German citizens on German streets in defense of Hitler’s (or any other German) government. Hitler had campaigned on the promise of draining the “parliamentarian swamp”—den parlamentarischen Sumpf—only to find himself now foundering in a quagmire of partisan politics and banging up against constitutional guardrails. He responded as he invariably did when confronted with dissenting opinions or inconvenient truths: He ignored them and doubled down. The next day, Hitler announced new Reichstag elections, to be held in early March, and issued a memorandum to his party leaders. “After a thirteen-year struggle the National Socialist movement has succeeded in breaking through into the government, but the struggle to win the German nation is only beginning,” Hitler proclaimed, and then added venomously: “The National Socialist party knows that the new government is not a National Socialist government, even though it is conscious that it bears the name of its leader, Adolf Hitler.” He was declaring war on his own government. We have come to perceive Hitler’s appointment as chancellor as part of an inexorable rise to power, an impression resting on generations of postwar scholarship, much of which has necessarily marginalized or disregarded alternatives to the standard narrative of the Nazi seizure of power (Machtergreifung) with its political and social persecutions, its assertion of totalitarian rule (Gleichschaltung) and subsequent aggressions that led to the Second World War and the nightmare of the Holocaust. In researching and writing this piece, I intentionally ignored these ultimate outcomes and instead traced events as they unfolded in real time with their attendant uncertainties and misguided assessments. A case in point: The January 31, 1933, New York Times story on Hitler’s appointment as chancellor was headlined “Hitler Puts Aside Aim to Be Dictator.” In the late 1980s, as a graduate student at Harvard, where I served as a teaching fellow in a course on Weimar and Nazi Germany, I used to cite a postwar observation, made by Hans Frank in Nuremberg, that underscored the tenuous nature of Hitler’s political career. “The Führer was a man who was possible in Germany only at that very moment,” the Nazi legal strategist recalled. “He came at exactly this terrible transitory period when the monarchy had gone and the republic was not yet secure.” Had Hitler’s predecessor in the chancellery, Kurt von Schleicher, remained in office another six months, or had German President Paul von Hindenburg exercised his constitutional powers more judiciously, or had a faction of moderate conservative Reichstag delegates cast their votes differently, then history may well have taken a very different turn. My most recent book, Takeover: Hitler’s Final Rise to Power, ends at the moment the story this essay tells begins. Both Hitler’s ascendancy to chancellor and his smashing of the constitutional guardrails once he got there, I have come to realize, are stories of political contingency rather than historical inevitability. Hitler’s appointment as chancellor of the country’s first democratic republic came almost as much as a surprise to Hitler as it did to the rest of the country. After a vertiginous three-year political ascent, Hitler had taken a shellacking in the November 1932 elections, shedding 2 million votes and 34 Reichstag seats, almost half of them to Hugenberg’s German Nationalists. By December 1932, Hitler’s movement was bankrupt financially, politically, ideologically. Hitler told several close associates that he was contemplating suicide. But a series of backroom deals that included the shock weekend dismissal of Chancellor Schleicher in late January 1933 hurtled Hitler into the chancellery. Schleicher would later remember Hitler telling him that “it was astonishing in his life that he was always rescued just when he himself had given up all hope.” Thomas Weber: Hitler would have been astonished The eleventh-hour appointment came at a steep political price. Hitler had left several of his most loyal lieutenants as political roadkill on this unexpected fast lane to power. Worse, he found himself with a cabinet handpicked by a political enemy, former Chancellor Franz von Papen, whose government Hitler had helped topple and who now served as Hitler’s vice chancellor. Worst of all, Hitler was hostage to Hugenberg, who commanded 51 Reichstag votes along with the power to make or break Hitler’s chancellorship. He nearly broke it. As President Hindenburg waited to receive Hitler on that Monday morning in January 1933, Hugenberg clashed with Hitler over the issue of new Reichstag elections. Hugenberg’s position: “Nein! Nein! Nein!” While Hitler and Hugenberg argued in the foyer outside the president’s office, Hindenburg, a military hero of World War I who had served as the German president since 1925, grew impatient. According to Otto Meissner, the president’s chief of staff, had the Hitler-Hugenberg squabble lasted another few minutes, Hindenburg would have left. Had this occurred, the awkward coalition cobbled together by Papen in the previous 48 hours would have collapsed. There would have been no Hitler chancellorship, no Third Reich. In the event, Hitler was given a paltry two cabinet posts to fill—and none of the most important ones pertaining to the economy, foreign policy, or the military. Hitler chose Wilhelm Frick as minister of the interior and Hermann Göring as minister without portfolio. But with his unerring instinct for detecting the weaknesses in structures and processes, Hitler put his two ministers to work targeting the Weimar Republic’s key democratic pillars: free speech, due process, public referendum, and states’ rights. Frick had responsibility over the republic’s federated system, as well as over the country’s electoral system and over the press. Frick was the first minister to reveal the plans of Hitler’s government: “We will present an enabling law to the Reichstag that in accordance with the constitution will dissolve the Reich government,” Frick told the press, explaining that Hitler’s ambitious plans for the country required extreme measures, a position Hitler underscored in his first national radio address on February 1. “The national government will therefore regard it as its first and supreme task to restore to the German people unity of mind and will,” Hitler said. “It will preserve and defend the foundations on which the strength of our nation rests.” Frick was also charged with suppressing the opposition press and centralizing power in Berlin. While Frick was undermining states’ rights and imposing bans on left-wing newspapers—including the Communist daily The Red Banner and the Social Democratic Forward—Hitler also appointed Göring as acting state interior minister of Prussia, the federated state that represented two-thirds of German territory. Göring was tasked with purging the Prussian state police, the largest security force in the country after the army, and a bastion of Social Democratic sentiment. Rudolf Diels was the head of Prussia’s political police. One day in early February, Diels was sitting in his office, at 76 Unter den Linden, when Göring knocked at his door and told him in no uncertain terms that it was time to clear house. “I want nothing to do with these scoundrels who are sitting around here in this place,” Göring said. A Schiesserlass, or “shooting decree,” followed. This permitted the state police to shoot on sight without fearing consequences. “I cannot rely on police to go after the red mob if they have to worry about facing disciplinary action when they are simply doing their job,” Göring explained. He accorded them his personal backing to shoot with impunity. “When they shoot, it is me shooting,” Göring said. “When someone is lying there dead, it is I who shot them.” Göring also designated the Nazi storm troopers as Hilfspolizei, or “deputy police,” compelling the state to provide the brownshirt thugs with sidearms and empowering them with police authority in their street battles. Diels later noted that this—manipulating the law to serve his ends and legitimizing the violence and excesses of tens of thousands of brownshirts—was a “well-tested Hitler tactic.” As Hitler scrambled to secure power and crush the opposition, rumors circulated of his government’s imminent demise. One rumor held that Schleicher, the most recently deposed chancellor, was planning a military coup. Another said that Hitler was a puppet of Papen and a backwoods Austrian boy in the unwitting service of German aristocrats. Still others alleged that Hitler was merely a brownshirt strawman for Hugenberg and a conspiracy of industrialists who intended to dismantle worker protections for the sake of higher profits. (The industrialist Otto Wolff was said to have “cashed in” on his financing of Hitler’s movement.) Yet another rumor had it that Hitler was merely managing a placeholder government while President Hindenburg, a monarchist at heart, prepared for the return of the Kaiser. There was little truth to any of this, but Hitler did have to confront the political reality of making good on his campaign promises to frustrated German voters in advance of the March Reichstag elections. The Red Banner published a list of Hitler’s campaign promises to workers, and the Center Party publicly demanded assurances that Hitler would support the agricultural sector, fight inflation, avoid “financial-political experiments,” and adhere to the Weimar constitution. At the same time, the dismay among right-wing supporters who had applauded Hitler’s earlier demand for dictatorial power and refusal to enter into a coalition was distilled in the pithy observation “No Third Reich, not even 2½.” On February 18, the center-left newspaper Vossische Zeitung wrote that despite Hitler’s campaign promises and political posturing, nothing had changed for the average German. If anything, things had gotten worse. Hitler’s promise of doubling tariffs on grain imports had gotten tangled in complexities and contractual obligations. Hugenberg informed Hitler during a cabinet meeting that the “catastrophic economic conditions” were threatening the very “existence of the country.” “In the end,” Vossische Zeitung predicted, “the survival of the new government will rely not on words but on the economic conditions.” For all Hitler’s talk of a thousand-year Reich, there was no certainty his government would last the month. Over the eight months before appointing Hitler as chancellor, Hindenberg had dispatched three others—Heinrich Brüning, Papen, and Schleicher—from the role, exercising his constitutional authority embedded in Article 53. And his disdain for Hitler was common knowledge. The previous August, he had declared publicly that, “for the sake of God, my conscience, and the country,” he would never appoint Hitler as chancellor. Privately, Hindenburg had quipped that if he were to appoint Hitler to any position, it would be as postmaster general, “so he can lick me from behind on my stamps.” In January, Hindenburg finally agreed to appoint Hitler, but with great reluctance—and on the condition that he never be left alone in a room with his new chancellor. By late February, the question on everyone’s mind was, as Forward put it, how much longer would the aging field marshal put up with his Bohemian corporal? That Forward article appeared on Saturday morning, February 25, under the headline “How Long?” Two days later, on Monday evening, shortly before 9 p.m., the Reichstag erupted in flames, sheafs of fire collapsing the glass dome of the plenary hall and illuminating the night sky over Berlin. Witnesses recall seeing the fire from villages 40 miles away. The image of the seat of German parliamentary democracy going up in flames sent a collective shock across the country. The Communists blamed the National Socialists. The National Socialists blamed the Communists. A 23-year-old Dutch Communist, Marinus van der Lubbe, was caught in flagrante, but the Berlin fire chief, Walter Gempp, who supervised the firefighting operation, saw evidence of potential Nazi involvement. From the May 1944 issue: What is German? When Hitler convened his cabinet to discuss the crisis the next morning, he declared that the fire was clearly part of a Communist coup attempt. Göring detailed Communist plans for further arson attacks on public buildings, as well as for the poisoning of public kitchens and the kidnapping of the children and wives of prominent officials. Interior Minister Frick presented a draft decree suspending civil liberties, permitting searches and seizures, and curbing states’ rights during a national emergency. Papen expressed concern that the proposed draft “could meet with resistance,” especially from “southern states,” by which he meant Bavaria, which was second only to Prussia in size and power. Perhaps, Papen suggested, the proposed measures should be discussed with state governments to assure “an amicable agreement,” otherwise the measures could be seen as the usurpation of states’ rights. Ultimately, only one word was added to suggest contingencies for suspending a state’s rights. Hindenburg signed the decree into law that afternoon. Put into effect just a week before the March elections, the emergency decree gave Hitler tremendous power to intimidate—and imprison—the political opposition. The Communist Party was banned (as Hitler had wanted since his first cabinet meeting), and members of the opposition press were arrested, their newspapers shut down. Göring had already been doing this for the past month, but the courts had invariably ordered the release of detained people. With the decree in effect, the courts could not intervene. Thousands of Communists and Social Democrats were rounded up. On Sunday morning, March 5, one week after the Reichstag fire, German voters went to the polls. “No stranger election has perhaps ever been held in a civilized country,” Frederick Birchall wrote that day in The New York Times. Birchall expressed his dismay at the apparent willingness of Germans to submit to authoritarian rule when they had the opportunity for a democratic alternative. “In any American or Anglo-Saxon community the response would be immediate and overwhelming,” he wrote. More than 40 million Germans went to the polls, which was more than 2 million more than in any previous election, representing nearly 89 percent of the registered voters—a stunning demonstration of democratic engagement. “Not since the German Reichstag was founded in 1871 has there been such a high voter turnout,” Vossische Zeitung reported. Most of those 2 million new votes went to the Nazis. “The enormous voting reserves almost entirely benefited the National Socialists,” Vossische Zeitung reported. Although the National Socialists fell short of Hitler’s promised 51 percent, managing only 44 percent of the electorate—despite massive suppression, the Social Democrats lost just a single Reichstag seat—the banning of the Communist Party positioned Hitler to form a coalition with the two-thirds Reichstag majority necessary to pass the empowering law. The next day, the National Socialists stormed state-government offices across the country. Swastika banners were hung from public buildings. Opposition politicians fled for their lives. Otto Wels, the Social Democratic leader, departed for Switzerland. So did Heinrich Held, the minister-president of Bavaria. Tens of thousands of political opponents were taken into Schutzhaft (“protective custody”), a form of detention in which an individual could be held without cause indefinitely. Hindenburg remained silent. He did not call his new chancellor to account for the violent public excesses against Communists, Social Democrats, and Jews. He did not exercise his Article 53 powers. Instead, he signed a decree permitting the National Socialists’ swastika banner to be flown beside the national colors. He acceded to Hitler’s request to create a new cabinet position, minister of public enlightenment and propaganda, a role promptly filled by Joseph Goebbels. “What good fortune for all of us to know that this towering old man is with us,” Goebbels wrote of Hindenburg in his diary, “and what a change of fate that we are now moving on the same path together.” A week later, Hindenburg’s embrace of Hitler was on full public display. He appeared in military regalia in the company of his chancellor, who was wearing a dark suit and long overcoat, at a ceremony in Potsdam. The former field marshal and the Bohemian corporal shook hands. Hitler bowed in putative deference. The “Day of Potsdam” signaled the end of any hope for an Article 53 solution to the Hitler chancellorship. That same Tuesday, March 21, an Article 48 decree was issued amnestying National Socialists convicted of crimes, including murder, perpetrated “in the battle for national renewal.” Men convicted of treason were now national heroes. The first concentration camp was opened that afternoon, in an old brewery near the town center of Oranienburg, just north of Berlin. The following day, the first group of detainees arrived at another concentration camp, in an abandoned munition plant outside the Bavarian town of Dachau. Plans for legislation excluding Jews from the legal and medical professions, as well as from government offices, were under way, though Hitler’s promise for the mass deportation of the country’s 100,000 Ostjuden, Jewish immigrants from Eastern Europe, was proving to be more complicated. Many had acquired German citizenship and were gainfully employed. As fear of deportation rose, a run on local banks caused other banks and businesses to panic. Accounts of Jewish depositors were frozen until, as one official explained, “they had settled their obligations with German business men.” Hermann Göring, now president of the newly elected Reichstag, sought to calm matters, assuring Germany’s Jewish citizens that they retained the same “protection of law for person and property” as every other German citizen. He then berated the international community: Foreigners were not to interfere with the domestic affairs of the country. Germany would do with its citizens whatever it deemed appropriate. Adolf Hitler's address to the Reichstag on March 23, 1933, at the Kroll Opera House. On this day, a majority of the delegates voted to eliminate almost all constitutional restraints on Hitler’s government. (Ullstein Bild / Getty) On Thursday, March 23, the Reichstag delegates assembled in the Kroll Opera House, just opposite the charred ruins of the Reichstag. The following Monday, the traditional Reich eagle had been removed and replaced with an enormous Nazi eagle, dramatically backlit with wings spread wide and a swastika in its talons. Hitler, dressed now in a brown stormtrooper uniform with a swastika armband, arrived to pitch his proposed enabling law, now formally titled the “Law to Remedy the Distress of the People and the Reich.” At 4:20 p.m., he stepped up to the podium. Appearing uncharacteristically ill at ease, he shuffled a sheaf of pages before beginning to read haltingly from a prepared text. Only gradually did he assume his usual animated rhetorical style. He enumerated the failings of the Weimar Republic, then outlined his plans for the four-year tenure of his proposed enabling law, which included restoring German dignity and military parity abroad as well as economic and social stability at home. “Treason toward our nation and our people shall in the future be stamped out with ruthless barbarity,” Hitler vowed. Read: Trump: ‘I need the kind of generals that Hitler had’ The Reichstag recessed to deliberate on the act. When the delegates reconvened at 6:15 that evening, the floor was given to Otto Wels, the Social Democratic leader, who had returned from his Swiss exile, despite fears for his personal safety, to challenge Hitler in person. As Wels began to speak, Hitler made a move to rise. Papen touched Hitler’s wrist to keep him in check. “In this historic hour, we German Social Democrats solemnly pledge ourselves to the principles of humanity and justice, of freedom and socialism,” Wels said. He chided Hitler for seeking to undermine the Weimar Republic, and for the hatred and divisiveness he had sowed. Regardless of the evils Hitler intended to visit on the country, Wels declared, the republic’s founding democratic values would endure. “No enabling act gives you the power to destroy ideas that are eternal and indestructible,” he said. Hitler rose. “The nice theories that you, Herr Delegate, just proclaimed are words that have come a bit too late for world history,” he began. He dismissed allegations that he posed any kind of threat to the German people. He reminded Wels that the Social Democrats had had 13 years to address the issues that really mattered to the German people—employment, stability, dignity. “Where was this battle during the time you had the power in your hand?” Hitler asked. The National Socialist delegates, along with observers in the galleries, cheered. The rest of the delegates remained still. A series of them rose to state both their concerns and positions on the proposed enabling law. The Centrists, as well as the representatives of the Bavarian People’s Party, said they were willing to vote yes despite reservations “that in normal times could scarcely have been overcome.” Similarly, Reinhold Maier, the leader of the German State Party, expressed concern about what would happen to judicial independence, due process, freedom of the press, and equal rights for all citizens under the law, and stated that he had “serious reservations” about according Hitler dictatorial powers. But then he announced that his party, too, was voting in favor of the law, eliciting laughter from the floor. Shortly before 8 o’clock that evening, the voting was completed. The 94 Social Democrat delegates who were in attendance cast their votes against the law. (Among the Social Democrats was the former interior minister of Prussia, Carl Severing, who had been arrested earlier in the day as he was about to enter the Reichstag but was released temporarily in order to cast his vote.) The remaining Reichstag delegates, 441 in all, voted in favor of the new law, delivering Hitler a four-fifths majority, more than enough to put the enabling law into effect without amendment or restriction. The next morning, U.S. Ambassador Frederic Sackett sent a telegram to the State Department: “On the basis of this law the Hitler Cabinet can reconstruct the entire system of government as it eliminates practically all constitutional restraints.” Joseph Goebbels, who was present that day as a National Socialist Reichstag delegate, would later marvel that the National Socialists had succeeded in dismantling a federated constitutional republic entirely through constitutional means. Seven years earlier, in 1926, after being elected to the Reichstag as one of the first 12 National Socialist delegates, Goebbels had been similarly struck: He was surprised to discover that he and these 11 other men (including Hermann Göring and Hans Frank), seated in a single row on the periphery of a plenary hall in their brown uniforms with swastika armbands, had—even as self-declared enemies of the Weimar Republic—been accorded free first-class train travel and subsidized meals, along with the capacity to disrupt, obstruct, and paralyze democratic structures and processes at will. “The big joke on democracy,” he observed, “is that it gives its mortal enemies the means to its own destruction.” Topic analysis in news via sparse learning: a case study on the 2016 US presidential elections Author: Calafiore, Giuseppe C., Date: 2017 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: KBL5IGVD Cite Key: Calafiore17newsTopicSparseLrnPresid Zotero Item | Lit Note ScienceDirectScienceDirect IFAC PapersOnLine 50-1 (2017) 13593–13598 Topic analysis in news via sparse learning: a case study on the 2016 US presidential elections Giuseppe C. Calafiore [Ď] Laurent El Ghaoui [ĎĎ] Alessandro Preziosi [ĎĎĎ] Luigi Russo [ĎĎĎĎ] _Ď_ giuseppe.calafiore@polito.it _ĎĎ_ elghaoui@berkeley.edu _ĎĎĎ_ alessandro.preziosi@polito.it _ĎĎĎĎ_ luigi russo@me.com Abstract: Textual data such as tweets and news is abundant on the web. However, extracting useful information from such a deluge of data is hardly possible for a human. In this paper, we discuss automated text analysis methods based on sparse optimization. In particular, we use sparse PCA and Elastic Net regression for extracting intelligible topics from a big textual corpus and for obtaining time-based signals quantifying the strength of each topic in time. These signals can then be used as regressors for modeling or predicting other related numerical indices. We applied this setup to the analysis of the topics that arose during the 2016 US presidential elections, and we used the topic strength signals in order to model their influence on the election polls. © 2017, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved. Keywords: Text analytics, news analysis, big data, sparse optimization. 1. INTRODUCTION A huge amount of textual data is being produced every minute; for instance, bloggers on Wordpress publish 350 blog posts per minute, while Twitter users send over 100,000 tweets per minute. This goldmine of textual data can provide a lot of information on global events and opinions, but the vast majority of it is unstructured, hence it is not easy to feed this information to mathematical models. Textual data is high-dimensional (a dataset can contain hundreds of thousands of different words), and not readily measurable or interpretable by a machine. In this paper, we describe a method to treat this high dimensional unstructured data, and produce time-based signals allowing us to quantify and possibly predict phenomena over time. As case study, we apply this method to news articles about the 2016 US presidential elections. The initial phase in our approach is constituted by suitable technologies for retrieving large amounts of textual data and representing them in numerical format, as discussed in Section 3. Then, we applied a sparse automated topic extraction method to the data. A first step is thus a dimensionality reduction obtained via sparse PCA, as described in Section 4. This allows us to transform the data from a high-dimensional space (where each dimension corresponds to a word) to a more interpretable space, where each dimension represents the concept of a “topic.” Such a sparse optimization approach has been used in the context of text analytics in, e.g., El Ghaoui et al. (2013). After computing the principal components (or topics), we project the topics onto a sliding windowed portion of the news in a given time interval, and thus synthesize a timesignal representing the strength of each topic over time, h i S ti 5 2 C bi i th i l ith the public polls, we describe a method for understanding which topics had the most positive and negative impact for each candidate. To this purpose, we compute a sparse Elastic Net regression of the poll results, using the topic strengths as inputs, as discussed in Section 6. In Section 3, we discuss scraping the source data, we apply a data cleaning step, and finally we code data into a suitable numerical representation. In Section 4 we describe Sparse Principal Component Analysis. In Section 5 we extract a list of principal components from the news articles, and we present some examples of the extracted topics, then we include the time component in order to compute the strength of each topic over time. In Section 6 we cross topic signals with polls data, presenting both a descriptive and a predictive model. Section 7 sums up the results of our study. 2. STATE OF THE ART Latent Dirichlet Allocation (LDA, see Blei et al. (2003)) is the most common algorithm used for topic extraction, but it does not incorporate sparsity directly into the model, so applying it for text processing tasks requires a final ‘thresholding’ step to make the result interpretable. Instead of plain LDA, we used sparse Principal Component Analysis (SPCA), since sparse solutions are not only faster to compute, but also more interpretable and provide better generalization to new data. An example of application of LDA to political data can be found in Jahanbakhsh and Moon (2014), who analyzed the tweets preceding the 2012 election. In our contribution, we use sparse PCA instead of LDA, and news articles instead of tweets; From a qualitative i t f i b th t th lti t i easily interpretable, due to the sparsity constraint. Additionally, instead of using a classical supervised sentiment analysis, we show that the raw topic data can be directly used to compute a sparse regression of the poll results, in a completely unsupervised fashion. 3. SOURCE DATA The textual corpus we used as a case study in this paper pertains to news articles dealing with the 2016 United States presidential elections. The data covers the year 2016, thus focusing on the last part of the presidential campaign, and on the race between Hillary Clinton and Donald Trump. 3.1 Scraping and data cleaning We used news articles obtained from the EventRegistry python API (see Leban et al. (2014)). For each day of 2016 we downloaded news articles in English mentioning the main Democratic and Republican candidates. We obtained a total of 144608 articles, about 500 news articles per day, from about 5185 different RSS feeds of major newspapers and blogs (Washington Post, Outside the Beltway, USA Today, Daily Mail, The Hill, Huffington Post, LA Times, CNN, NY Times, Reuters, etc.). In the experiments described in this paper, we only selected articles mentioning either the keyword ”Clinton” or ”Trump”. Each news article is saved in an SQLite database, containing information about the title, content, source and date of publication. The documents span from January 1st 2016 to November 3rd 2016. The full dataset can be downloaded at: https://goo.gl/FJ8lzm Before analysing our dataset, we applied some simple data cleaning. We removed common unfocused expressions using a list of stop words, and we used regular expressions to detect and remove urls and other short words (1 or 2 characters). We did not apply any stemming, so singular and plural words, for example, are treated as different entities. 3.2 Textual data coding Once textual data is pre-processed as discussed above, a key step is to code it into a suitable numerical representation. In this paper, we used a standard “vector space model,” in which each document is represented as a high-dimensional vector where each entry corresponds to a “term” in the document collection, see, e.g. Chapter 6 in Manning et al. (2008). By term we mean either a single word, or a short sentence (an m-gram). In the machine learning terminology terms are usually referred to as fea_tures, and the collection of all features appearing in the_ document corpus constitutes the dictionary. For each term i and document j, we define the term frequency tfij as the number of occurrences of term i in document j; we define the document frequency dfi as the number of documents in which term i occurs, and the collection frequency cfi as the total number of occurrences of term i in the collection. A crude encoding of the document collection is given by the term-frequency (TF) matrix [M ]ij = tfij, i = 1, . . ., n, j = 1, . . ., N, where n is the total number of terms and N is the total number of documents in the considered collec vector, given by the j-th column of M ; the i-th row of M is instead an N -dimensional row vector that contains the counts of term i in each of the N documents. The information that is captured by the term frequency is how important a given word is within a document (the higher the count, the more likely is that the word is a good description of that document). Term frequency is often dampened by a slowly monotonic increasing function (such as 1 + log tfij), in order to avoid giving too much importance to highly occurring words. Document frequency can instead be interpreted as an indicator of informativeness of a term: a term occurring in all documents (such as stop words, or an unfocused term such as “do”, “be”, “like”, etc.) is not likely to have a semantic relevance in a specific document. On the contrary, focused terms appearing only in few documents (such as, say, “nuclear”) are likely to identify documents referring to a narrowly defined topic. Term i appearing in document j is thus more relevant if the overall document frequency of that term is low: this idea leads to a commonly employed encoding of the document corpus, in which a term’s frequency in a document is scaled proportionally to the (log) inverse of the document frequency of that term. Namely, the so-called TF-IDF (term frequency, inverse document frequency) matrix of the document collection is defined as � [M ]ij = (1 + log tfij) log df[N]i , if tfij ≥ 1 0, if tfij = 0. Further, the columns of M are usually normalized by dividing them by their Euclidean norm. This so-called cosine normalization allows for measuring the similarity between two documents (or between a document and a query) by simply computing the inner product of their corresponding vector encodings. 4. SPARSE PCA Principal Component Analysis (PCA) is a classical method for dimensionality reduction that has been used in text analytics for topic modeling and latent semantic indexing (LSI), see, e.g., Papadimitriou et al. (2000); Hofmann (2001). A key step of standard PCA applied to the documents encoding matrix M amounts to finding vectors p ∈ R[n] and q ∈ R[N] such that ∥M − pq[⊤]∥F[2] [is mini-] mized (here the suffix F stands for the Frobenius matrix norm). The interpretation is that pq[⊤] is the best rank-one approximation of M, where vector p contains the terms that best summarize the entire document set, hence gives a description of the main topic discussed in the corpus. Vector q contains values qi, i = 1, . . ., N, representing the relevance of the main topic p in the i-th document. Once the first topic p and corresponding topic distribution q are extracted from M, the coding matrix is suitably deflated to a matrix M1, and PCA can be applied again to M1 in order to extract the second topic, and so on; see Mackey (2009). Observe that M is typically a very sparse matrix, whereas p and q, as obtained from standard PCA, are generally non-sparse. It would indeed be important for the purpose of semantic interpretation of the topic to have a sparse topic model p, containing only few really important terms defining the topic. Similarly, a sparse q vector would point us only to the few most relevant documents in which the topic is discussed. Sparse PCA (SPCA) addresses these requirements, by posing the problem as where ∥· ∥0 denotes the cardinality (i.e., the number of nonzero entries) of its argument, and k, h are given upper bounds on the desired cardinalities, see, e.g., Zhang et al. (2012); Zou et al. (2006). Empirically, we find that low values of p are desirable, we obtain the best results with p = 4 (rarely a more than 4 keywords are necessary to define a topic). The value of q doesn’t change the resulting topics much. Depending on the kind of dataset, it should be roughly proportional to how many topics per document we expect to find. Unfortunately, SPCA is not solvable exactly and efficiently for realistic highdimensional instances. There exist, however, heuristics that proved to be very effective and extremely scalable in practice. One such algorithm, which we employed in the present study, is a thresholded version of the power iteration method, see Shen and Huang (2008): we alternate over the two variables p, q a thresholding and projection operation: [Tk(Mq)]B → p, [Th(M [⊤]p)]B → q, (1) where Tt(v) is the hard thresholding operator on vector v, which returns a vector of the same size as v obtained by zeroing out all but the t largest components of v, and [v]B denotes projection of v onto the unit Euclidean ball, that B is [v]B = v/∥v∥2. Clearly, for k = n, h = N, the recursion in (1) reduces to the standard power iteration method for plain PCA. In practice, the thresholded power iteration method for SPCA is much faster that its plain counterpart, since p and q are maintained sparse at each iteration and thus, being M also sparse, only sparse matrix/vector products are performed at each iteration. 5. TOPIC EXTRACTION The matrix M can be quite big, in our case it has dimension 144608 by 191482 (the first dimension being the documents, the second being the features), but using sparse PCA, without any knowledge about the articles in our dataset, we can quickly extract a list of principal components that describe all the main topics that were discussed in this election cycle. Then, for each topic, we can extract the most relevant articles, or analyze the evolution of the topic over time. In the next section we list some interesting topics obtained by running SPCA on our textual corpus. 5.1 Example topics We ran SPCA on a subset of our dataset, removing from M the rows that do not mention “Clinton” or “Trump”. We set the sparsity requirement to a total of 4 keywords per topic, and we compute the first 100 principal components. For each topic, we provide the list of keywords, ordered by decreasing weight. Many of the automatically generated topics clearly define a human-understandable subject, and can offer a good overview of the main themes discussed in this election cycle: [1] [black, african, american, americans] [2] [women, men, woman, female] [3] [court, supreme, justice, scalia] [4] [rally, protesters, supporters, event] [5] [emails, departement, fbi, email] [6] [nuclear, policy, korea, foreign] [8] [big, banks, money, polls] [9] [tax, returns, taxes, release] [10] [foundation, donors, charity, work] [11] [superdelegates, won, pledged, primaries] [12] [gun, guns, control, background] [13] [abortion, life, pro, abortions] [14] [immigration, immigrants, illegal, issue] [15] [climate, change, energy, global] [16] [trade, china, deal, tpp] [17] [health, care, plan, insurance] [18] [war, iraq, military, isis] [19] [north, korean, kim, test] [20] [star, tweet, anti, image] [21] [income, class, middle, pay] We can see that news have talked a lot about african americans [1] and women [2]. Another big topic of the elections has been the death supreme court judge Scalia, and the need for his replacement [3]. Trump’s rallies have brought a great amount of supporters and protestors and received a lot of news coverage [4]. Another big subject was the investigation regarding state department emails that were erased from Clinton’s server [5]. Many other campaign subjects also appear among the main topics. Nuclear policy [6], the wall with Mexico [7], banks [8], gun regulation [12], abortion [13], immigration [14], climate change [15], international trade [16], health care [17], middle class income [21]. Other topics involve controversies during the campaign, about Donald Trump’s tax returns [9], the Clinton Foundation [10] or Trump tweeting the Star of David [20]. In the next section, we discuss how to analyze the dynamics of topic evolution over time. 5.2 Topic dynamics Once we computed the principal components using SPCA, we can analyze the data in a much simpler space, where each dimension corresponds to a human-interpretable topic. If we include the time component into our analysis, we can extract useful insights from the dataset, by computing the evolution of the relative strength of each topic over time. The strength St of topic t in a certain time window w, is obtained by computing the cosine similarity of the topic vector of interest with respect to all the documents in that time window: � St = (divt), (2) i where di is the normalized i-th row of Mw (Mw is a portion of the coding matrix M, containing the columns corresponding to the documents in the considered time window) representing the i-th document published in the time window w, and vt is a normalized column vector representing a topic. Figure 1 shows an example of topic strength over time, computed over a daily window. Marco Rubio was competing in the Republican primaries against Trump and lost, so, as expected, news mentioning him taper off after he loses to Trump. In Figure 2 we can see instead that Mike Pence appears Fig. 1. Strength over time, for topic “Marco Rubio”. Fig. 2. Strength over time, for topic “Mike Pence”. Fig. 3. Strength over time, for topic “emails”. Fig. 4. Strength over time, for topic “Comey” (director of the FBI). vice president, corresponding to a peak in news mentioning him. From Figure 3 we can see how the controversy about Clinton’s State Department emails has been a hot topic in the last part of the campaign. We have a big peak on May 25th, corresponding to the release of an 83-page report about the State Department’s email practices. A second noticeable peak is around July 7th, when the State Department reopened its probe into the email controversy. Still related to the email scandal figure 4 shows a topic July 5th, when the director recommended no charges against Hillary, and one at the end of october, when Comey reopened the case. We have shown how to produce, from an initial dataset of unstructured textual data, a series of time signals representing how much different subjects have been discussed over time. This kind of temporal signals can then be used to instantiate a predictive or descriptive model, as illustrated in the next section. 6. CROSSING TOPIC SIGNALS WITH POLLS DATA 6.1 Descriptive modelling Fig. 5. National polling average. Blue for Clinton, red for Trump. In this section we discuss how to relate the variation in the polling average to the main topics discussed in the news a few days earlier. The objective of such a model is not necessarily predicting the election’s result, but understanding which topics had the most positive and negative impact for each candidate. To this purpose, we model the variation in the polls as a linear combination of the strengths of each topic. The topics with a strong positive coefficient will be the ones who were associated with a rise in the polls for the selected candidate. Given the high number of topics, computing an ordinary least squares regression would result in overfitting and would not produce a good model because there are more topics than samples (days). To solve this problem, we looked for sparse solutions once again, penalizing solutions that use too many variables. The use of an ℓ1-norm regularization term in the least squares regression leads to the well known Lasso criterion, see, e.g., Tibshirani (1996). The Lasso, however, has some downsides: if there is a group of variables among which the pairwise correlations are very high, then the Lasso tends to select only one variable from the group, and does not care which one is selected. For problems where the number of variables n is larger than the number of observations N, the Lasso is also not the ideal method, because it can only select at most N variables out of n candidates. Better results can be obtained by using an Elastic Net (EN) criterion, which includes both ℓ1 and ℓ2 regularization terms. Simulation studies and real data examples show that the EN in accuracy, see, e.g., Zou and Hastie (2005), and, if there are highly correlated variables, EN will tend to keep all in the model (grouping effect), while Lasso only keeps one. The elastic net problem can be expressed, in its basic formulation, as βˆ = argmin(∥y − Xβ∥[2] + λ2 ∥β∥2[2] [+][ λ][1][ ∥][β][∥]1) Where β is the coefficients vector while λ1 and λ2 are, respectively, the ℓ1 and ℓ2 penalties. In our case, each column of the matrix X represents a topic, each row represents a day. Element Xij represents the strength of topic j at day i, as computed in (2). The observation vector y is the variation in the poll results after w days: y(t) = polls(t + w) polls(t). _−_ The function polls(t) returns the average polling data, obtained from the web at time t. A script to download this poll data is provided at https://goo.gl/FJ8lzm. As shown in Figure 5, the poll signals for the two candidates are not specular due to third parties and variable voter turnout, therefore when modelling the news influence on a specific candidate we will use the polling signal relative to that candidate, obtaining a different β for each candidate. Thanks to the high interpretability of the automatically generated topics it is easy to understand and evaluate the results. As an example, in the next paragraphs we show the results of a descriptive model built using 4-word topics, and computing their impact on Trump’s polls on a 3-day window (w = 3). The results show that the topics most positively correlated to a raise in the polls of Trump are: emails, department, fbi, email • gun, guns, control, backgorund • When the news have been talking a lot about guns and immigration or about the FBI investigation on the departement emails erased from Clinton’s servers, Trump has seen a rise in polls. Conversely, the most negative topics for Trump, identified by the method, were: women, men, woman, female • sanders, bernie, democratic, hillary • palin, sarah, endorsement, alaska • Trump’s comments on women have had a negative impact on polls, but also two major events seem correlated with his fall in the polls. The endorsement by Sarah Palin was also correlated with a fall in the polls. Other minor topics, relative to a very punctual event, were also associated with a fall in polls, for example, the talk given by Khizr Khan, the father of a Muslim U.S. soldier killed in combat, at the DNC convention. In Figure 6 we can see Trump’s fall in the polls after the Democratic convention (July 25-28) and, during the same period, a lot of mentions in the news of Khan’s talk. Running the same analysis on Clinton, the two most negative topics were sanders, bernie, democratic, hillary • court, supreme, scalia, justice • As seen in Figure 7, the news of the death of Supreme Court judge Scalia was associated with a fall in Clinton’s polls. It also appears that when Bernie Sanders was mentioned a lot in the news, this had a negative correlation with Clinton’s results as many people preferred Sanders Fig. 6. Above: Donald Trump national polling average. Below: Strength of topic “Khizr Khan”. Fig. 7. Above: Hillary Clinton national polling average. Below: Strength of topic “Scalia”. 6.2 Predictive modelling In addition to analyzing past data, we can use EN regression for building a predictive model. In order to do this, and find suitable parameters for the Elastic Net criterion, we use cross-validation. 80% of the dataset is used for training and 20% for testing, the testing days are always contiguous, starting at a random day of the year. For each combination of parameters, we computed the mean absolute error over 100 different splits of training and test data. For finding suitable ℓ1 and ℓ2 penalties, we rewrite the penalties in eq. (3) as α(ρ ∥β∥1 + [(1][ −] [ρ][)] _∥β∥2[2][)]_ 2 From Figure 8 it is possible to observe that values of α close to zero lead to very poor out-of-sample results. This corresponds to an ordinary least-squares regression without regularization, which results in overfitting and in bad predictive performance on validation data. As shown in the chart, increasing the regularization leads to better results in the testing. The optimal ratio between the ℓ1 and ℓ2 penalty is dependent on the overall amount of regularization. For high levels of α, using ℓ2 penalty is preferred, as ℓ1 can be too aggressive. In our case, we found a suitable ρ value of 0.7, with an α value of 0.06. We tested models on different windows, from 1 to 30 days. Smaller windows are good for a descriptive model but it is harder to build a predictive model on those smaller scales, due to the noisiness of the polls data. The models with the best predictive power are those computed on a window of ten Fig. 8. Mean absolute error in the prediction of the poll percentage change. Results of the cross-validations over different α and ℓ1 ratio (ρ.) Fig. 9. Donald Trump’s poll signal reconstructed using only the news as input. Original signal in red, reconstructed signal in blue. Models computed with these parameters can predict the movement in polls with a mean absolute error of 0.48 percentage points, on a ten-day window. Figure 9 shows the reconstructed poll signal, using only the topics as an input. The initial state of the system corresponds to the real, initial, value of the polls. Then, each subsequent state is based only on the previous state, and 18 input topics selected by the sparse regression. We can see that the drift is moderate, even if each prediction is based on previous outputs. 7. CONCLUSIONS We described a methodology for acquiring large and unstructured textual data and transforming it into understandable topics and related time-signals of topic strengths. These signals can then be used as inputs in a sparse regression setting in order to model or predict other related quantitative signals. Using sparse PCA allow us to identify the main topics covered by the corpus, even with no prior knowledge. Forcing sparsity has several advantages such as speeding up computations while making Our key contribution was to introduce time information, by splitting our dataset into time slices and projecting the data in each slice onto our principal components (topics), in order to analyze the relative importance of each topic over time. Once textual data has been converted to real-valued time signals, the information can be used to evaluate a topic’s effect on a certain numerical outcome (the election polls in our case). A similar approach, relating a topic strength signal to a numerical outcome, could be used to predict other variables, such as macroeconomic indicators, or market movements. REFERENCES Blei, D.M., Ng, A.Y., and Jordan, M.I. (2003). Latent dirichlet allocation. Journal of machine Learning re- search, 3(Jan), 993–1022. El Ghaoui, L., Pham, V., Li, G.C., Duong, V.A., Srivastava, A., and Bhaduri, K. (2013). Understanding large text corpora via sparse machine learning. Statisti_cal Analysis and Data Mining: The ASA Data Science_ Journal, 6(3), 221–242. Hofmann, T. (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42, 177– 196. Jahanbakhsh, K. and Moon, Y. (2014). The predictive power of social media: On the predictability of us presidential elections using twitter. arXiv preprint arXiv:1407.0622. Leban, G., Fortuna, B., Brank, J., and Grobelnik, M. (2014). Event registry: learning about world events from news. In Proceedings of the 23rd International Conference on World Wide Web, 107–110. ACM. Mackey, L. (2009). Deflation methods for sparse pca. Advances in neural information processing systems, 21, 1017–1024. Manning, C., Raghavan, P., and Sch¨utze, H. (2008). Intro_duction to Information Retrieval. Cambridge University_ Press. Papadimitriou, C., Raghavan, P., Tamaki, H., and Vempala, S. (2000). Latent semantic indexing: A probabilistic analysis. Journal of Computer and System Sciences, 61(2), 217–235. Shen, H. and Huang, J. (2008). Sparse principal component analysis via regularized low rank matrix approximation. J. Multivar. Analysis, 99, 1015–1034. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 267–288. Zhang, Y., d’Aspremont, A., and Ghaoui, L.E. (2012). Sparse PCA: convex relaxations, algorithms and applications. In M. Anjos and J. Lasserre (eds.), Handbook on Semidefinite, Cone and Polynomial Optimization, International Series in Operational Research and Management Science. Springer. Zou, H., Hastie, T., and Tibshirani, R. (2006). Sparse principal component analysis. J. Computational and Graphical Statistics, 15(2), 265–286. Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67, 301–320. Women Voters Revisited: Inflation, Abortion, and Increased Motivation in the 2024 Election Countdown Author: Kearney, Audrey, Date: 2024-10-11 Collections: Hot Takes US Elect 2024 Zotero Key: THUK5JNM Cite Key: Kearney24womenInflAbrtnMotiv Zotero Item | Lit Note Key Takeaways In the three months since the initial KFF Survey of Women Voters, several major unexpected political events have taken place, impacting voters’ motivations and voting decisions for Election Day. This includes President Biden’s announcement that he would no longer seek reelection, with Vice President Harris quickly garnering the Democratic nomination as his replacement. To better understand how these and other events have impacted campaign dynamics and how voters feel leading into the November election, KFF resurveyed women voters and finds a very different female electorate one month before Election Day. Some key takeaways from the KFF Survey of Women Voters Revisited include: • Since Vice President (VP) Harris entered the 2024 presidential race, abortion has become the most important issue for women under age 30, rising in the ranks above inflation, this group’s top issue from earlier this summer. Four in ten (39%) women voters under 30 now say abortion is the most important issue to their vote, doubling the share who said the same back in June. Harris’ increased campaign rhetoric on reproductive rights also seems to be resonating with the Democratic base, notably younger Democratic women voters. Democratic women of reproductive age (under age 50) are now nearly twice as likely say they trust her “a lot” to discuss issues related to abortion (66%, up from 35% in June). Among women voters overall, Harris holds a two to one advantage over former President Trump on which candidate they trust to do a better job deciding policy related to abortion access, birth control access, and IVF.* • Inflation continues to be the number one issue for women voters heading into this election and remains high among priorities for Black (51%) and Hispanic (41%) women. In a flip from earlier this summer, voters of color are now giving VP Harris an advantage on handling the rising costs of household expenses. Black women are seven times more likely to say they trust VP Harris (70% trust Harris, 10% trust Trump) and most Hispanic women also trust the VP (53% trust Harris, 35% trust Trump). This is a change from June, when many Black and Hispanic women said they did not trust either political party to do a better job addressing household costs and many disapproved of President Biden’s handling of inflation.* • Women voters, especially Democratic and Black women voters, are now more satisfied with their choices for president and more motivated to vote than they were back in June. The share of Democratic women voters who say they are satisfied with their choices for president has increased thirty-nine percentage points, as well as a thirty-point increase among Black women. In addition, majorities of both Democratic women and Black women now say they are “more motivated” to vote in the upcoming election than in previous presidential elections, and many attribute this increased motivation directly to VP Harris replacing President Biden as the Democratic nominee. Republican women voters, on the other hand, who were more positive back in early summer than their Democratic counterparts, now trail behind in both candidate satisfaction and motivation to vote.* • In addition to feeling more enthusiastic, a larger share of Democratic women voters now reports feeling “hopeful” about the upcoming election, but they are also more likely to say they feel “anxious.” Democratic women voters may be feeling increased anxiety as they are also now more likely to see the November election as having major implications on abortion access in the country.* • Women voters are now 11-percentage points more likely to say that this presidential election will have a “major impact” on access to abortion in the U.S. (65%, up from 54%). Democratic women of reproductive age are now 13-percentage points more likely to say this election matters in a major way for abortion access. Republican women, on the other hand, don’t see the election as a major tipping point on abortion access with a majority saying the presidential election will have either a “minor impact” or “no impact.”* Inflation Remains Women Voters’ Top Issue, Harris Is Seen as Better Candidate to Handle Rising Household Expenses Inflation, including the rising cost of household expenses, continues to be the most important issue for women voters overall, with more than one in three (36%) citing it as the “most important” issue in their vote for president. This is followed by threats to democracy (24%), and immigration and border security (13%). A slightly larger share of women voters now say abortion is the most important issue to them (13%) than earlier this summer (10%), perhaps a reflection of the increased emphasis placed on reproductive rights by the Harris-Walz ticket. With Harris as the Democratic nominee heading into the election, about one in five (18%) Democratic women voters, rising to one in four (26%) Democratic women voters of reproductive age, say that abortion is the most important issue to them. Even still, inflation remains the most important issue for Democratic and Republican women voters, unchanged from earlier in this campaign season. The rising cost of household expenses remains the top issue among key groups of women voters, most notably Black and Hispanic women. Large majorities of Black women voters (68%) and Hispanic women voters (70%) report worrying “a lot” about affording basic household expenses, and this may be why inflation remains their top voting issue with about half (51%) of Black women and four in ten Hispanic women (41%) saying inflation is their top issue, compared to about one-third of White women (32%). Similar shares of White women prioritize inflation and threats to democracy (27%) this election cycle. Vice President Harris holds the edge among women voters when it comes to handling the rising cost of household expenses, with about half (46%) trusting her over former President Trump (39%). One in six women voters say they trust “neither” candidate to address costs. Harris’ overall advantage on this issue is especially notable because back in June, voters were split evenly on which party they trusted more to deal with this issue, giving neither party the advantage. Harris’s advantage is particularly strong among Black and Hispanic women voters, while White women are split between the two candidates on the issue. Black women voters are seven times as likely to trust Harris on this issue over Trump (70% trust Harris more vs. 10% trust Trump). A slight majority of Hispanic women voters trust Harris (53%) to address costs, and about one in three trust Trump (35%) on the issue. In June, the Democratic Party fared better than the Republican Party among Black and Hispanic women, though to a lesser extent. At the time, four in ten Black (41%) and Hispanic (43%) women voters said they trusted neither political party to best address costs. About half of White women now say they trust Trump to better handle costs (47%), while four in ten trust Harris more (40%). Partisans are split, with majorities trusting their party’s nominee to better address the cost of household expenses. One in seven Republican (14%) and one in eight Democratic (12%) women voters say they do not trust either candidate. Harris also has a strong advantage as the candidate women voters trust to do a better job addressing health care costs, with half saying they trust her and one in three (34%) saying they trust Trump. One in six women voters say they trust “neither” candidate to address health care costs. Among Black women, Harris holds the strongest advantage, with three in four saying they trust the VP more on this issue, while just 5% say they trust Trump more. Hispanic women trust Harris more, at a 2 to 1 ratio (55% for Harris vs. 27% for Trump). White women are split evenly between the two candidates on who they would trust to lower the cost of health care for people like them (43% trust Harris vs. 42% trust Trump). Again, partisans are most likely to trust their party’s candidate on this issue, though nearly one in five Republican women (18%) and one in ten Democratic women say they trust neither candidate. Women Voters View the Stakes on Abortion Access as High Women voters are more likely to say that this election will have a “major” impact on abortion access than they were June, when President Biden was still the Democratic nominee. Now, two-thirds (65%) of women voters think this election will have a “major” impact, up from just over half (54%) in June. Large majorities of Black women (72%), Hispanic women (75%), White women (61%), women of reproductive age (65%), women over age 50 (64%), and Democratic women (82%) now expect this election to have a major impact on abortion access. Democratic women of reproductive age are now 13 percentage points more likely to say this election will majorly impact abortion access (79% now, 66% in June). For Republican women, the perceived stakes of this election on abortion have remained low throughout this campaign cycle with about four in ten Republican women continuing to say they expect a major impact. Three in four women voters say that if elected, they think former President Trump would sign a federal law banning abortion in the U.S. after 15 weeks of pregnancy, should Congress pass such a law. This includes nearly half (47%) who say this is “very likely.” Majorities across partisans as well as women of reproductive age say they think it is likely that former President Trump will sign a federal law banning abortions after 15 weeks in the U.S., if such a law is passed by Congress. Trump has repeatedly said that he will not sign a federal abortion ban. A federal ban on abortion is unpopular among women voters, with two-thirds overall saying they would oppose a nationwide ban on abortion at 15 weeks of pregnancy. Six in ten Republican women, however, support a nationwide ban. In contrast, most women across party lines, including about nine in ten (94%) Democratic women and three in four Republican women, say VP Harris is likely to sign a law restoring Roe v. Wade, protecting nationwide access to abortion, if she is elected and if such a law is passed by Congress. Seven in ten women voters support a nationwide right to abortion, including majorities of Democratic and independent women voters. Notably, these women voters were less certain of President Biden’s intentions for abortion policy in his second term when he was the Democratic nominee; in June, about seven in ten (72%) women voters said it was likely Biden would sign a law guaranteeing a federal right to abortion until fetal viability, including about one-third (36%) who said this was “very likely.” At the time, Republican women were more likely to say this was “very likely” than Democratic women (46% vs. 32%), suggesting that the messaging from the Harris campaign has been clearer to Democratic voters. VP Harris Holds Advantage on Reproductive Health Issues Nearly twice the share of women voters say they trust Vice President Harris to do a better job than former President Trump deciding policy related to abortion access in the U.S. (58% v. 29%), birth control access (60% vs. 25%) and IVF (55% vs. 29%). Democratic women overwhelmingly say they trust Harris over Trump to decide policy on each of the issues: abortion access (90% vs. 4%), birth control access (92% vs. 4%), and IVF access (88% vs. 5%). In addition, Democratic women are ten percentage points more likely to say they trust Harris to address each of these reproductive health issues than they were to trust Biden in June. About six in ten Republican women say they trust Trump over Harris on the issue of abortion access (65% vs. 17%), birth control access (57% vs. 21%), and access to IVF (64% vs. 14%). However, about one in five or more Republican women say they do not trust either candidate on each of these areas. Among women voters overall, Harris fares better on each of these issues than Biden did in June. At the time, President Biden had the edge over Trump on each of these issues, but a substantial share of women voters said they trust “neither” candidate. Democratic women voters are also becoming more comfortable with Harris speaking on abortion policy. Back in June, about half (49%) of Democratic women voters said they trusted her “a lot” to speak about abortion policy. That share has increased to three in four in the latest Women Voters resurvey. The most notable increase is among Democratic women of reproductive age, who were skeptical of the VP on this issue earlier this summer. The share of Democratic women ages 18 to 49 who say they trust her “a lot” to speak about abortion has nearly doubled in four months, from about one-third (35%) to about two-thirds (66%). Surveyed before the first and only vice-presidential debate of this election cycle, Democratic women are 23-percentage points more likely to say they have “a lot” of trust in their party’s VP candidate, Tim Walz, to speak on issues related to reproductive health than Republican women are of Republican vice-presidential nominee JD Vance (56% vs. 33%). Abortion Rises as a Key Issue for Young Women Beyond inflation, larger shares of women voters rank abortion as their most important issue heading into this election than in June (13%, up slightly from 10%). This increase is largely driven by young women. Abortion is now the single most important issue to women under age 30, with about four in ten (39%) naming it as their top issue, followed by inflation (28%). This is a dramatic shift from earlier this summer, when half (48%) of women under age 30 cited inflation, and one in five (20%) cited abortion as their top issue. About one in ten women ages 30-54 (12%) or ages 55 and older (7%) say that abortion is their top issue, unchanged from earlier this summer. The stakes for this group are high: about seven in ten (72%) women voters ages 18-29 say this year’s election will have a major impact on access to abortion and one in five say it will have a “minor impact.” Few (8%) say this election won’t impact abortion access. VP Harris fares better among this group on the issue of abortion than Trump (69% say they trust her on the issue, 19% trust the former president), and she fares better than Biden did among this group in June. Earlier this summer, about half (49%) said they trusted Biden to do a better job deciding abortion access policy, and about one in five (18%) trusted Trump. An additional third (33%), however, said they trusted neither candidate at the time. Mood of the Election In early summer 2024, the initial KFF Survey of Women Voters reported that women voters were not largely enthusiastic about the upcoming 2024 presidential election. Resurveyed three months later with a change at the top of the Democratic Party ticket, women voters are not only more satisfied about their options for president, but two-thirds (64%) say they are more motivated to vote in this election compared to previous presidential elections. And while many still feel “anxious” and “frustrated,\" a majority now say they are “hopeful” and half say they are “enthusiastic” – marking a massive shift in how women voters see the 2024 election. Two-thirds of women voters now say they are either “very satisfied” or “somewhat satisfied” with their options for president, up from 40% back in June 2024. Three months ago, the only groups of women voters who said they were satisfied with their options were slight majorities of Republican women (54%) and women voters ages 65 and older (52%). In the latest poll, a majority of women voters across race and ethnicity, party identification, and age say they are satisfied with their choices. This includes a nearly forty percentage point increase in satisfaction among Democratic women voters. Republican women, who were more satisfied than Democrats with their choices back in June, are now less likely to report being satisfied than their Democratic counterparts. About half (52%) of Republican women voters say they are satisfied with their choices for president, similar to their responses from June. In addition, the share of women voters who now say they are “more motivated” to vote in this presidential election compared to previous elections has increased 19 percentage points, up to 64% in September. This includes a 26-percentage point increase among Democratic women voters, while the share of Republican women voters who say they are more motivated to vote has increased slightly from 53% to 61%. Black women are another group that has seen a considerable increase in the share who say they are now more motivated to vote, especially since so many reported being unsatisfied and not as motivated back in June. Nearly six in ten Black women voters now say they are more motivated to vote compared to the previous election, up from 37% in June. Women voters, across age groups, are all more likely to say they are “more motivated” now compared to four months ago. For many, the increase in motivation isn’t just because it is closer to Election Day; half of voters (51%) say Vice President Harris becoming the Democratic nominee for president has made them “more motivated” to vote in the upcoming election. The share of voters who say Harris’ candidacy has made them more motivated increases to nearly six in ten Black women voters and Democratic women voters. The share of women voters who say they are more motivated this election cycle because of Harris’ candidacy also includes 55% of women voters who say abortion is the most important issue in determining their vote choice. When asked explicitly why they are more motivated now that President Biden has decided not to run for reelection and VP Harris is the nominee, Democratic women offer responses saying she is a better candidate or that she has a better chance of beating former President Trump. Democratic Women in Their Own Words: “Why are you now more motivated to vote in the upcoming election now that Vice President Harris is the Democratic nominee?” “She seems like a competent candidate closer to my age group and will understand issues that affect my generation more than Biden.” – Hispanic Democratic woman from California, age 37 “I feel the Democrats have a better chance with Harris as the candidate.” – White Democratic woman from New Hampshire, age 64 “She is younger, female and a person of color. I was concerned that President Biden could not win reelection. I now think we have a chance.” – White Democratic woman from Michigan, age 50 “I think she is educated I think she is on top of it smart and intelligent and knows some of the ropes. If we need a change we might as well start with her.” – Black Democratic woman from Oklahoma, age 70 But it isn’t just key constituents of the Democratic base who are now more motivated by Harris’ nomination – nearly half (46%) of Republican women voters also say they are more motivated to vote, mostly to ensure Harris isn’t elected. When asked directly about why they are now more motivated, Republican women say it because they want to make sure Trump wins because they don’t want her to win. Republican Women in Their Own Words: “Why are you now more motivated to vote in the upcoming election now that Vice President Harris is the Democratic nominee?” “She hasn't done anything to help the American people in four years, why would we want her to be commander in chief!” – White Republican woman from Ohio, age 75 “So President Trump will be reelected.” – Multi-racial Republican woman from Illinois, age 43 “She is not qualified to be president.” – White Republican woman from Pennsylvania, age 64 “Hoping she doesn’t become the first female president.” – White Republican woman from Texas, age 38 “Because Kamala Harris is a threat to our country.” – White Republican woman from Nebraska, age 77 Women Voters Feel Both Anxious and Hopeful About Upcoming Election Back in June, the most common feelings women voters had about the upcoming presidential election were frustration and anxiety. Three months later, a majority of women voters still say they feel “frustrated” (65%) or “anxious” (77%) about the upcoming election, but seven in ten (69%) now say they feel “hopeful” (up from 53% in June). In addition, nearly half (47%) of women voters say they feel “enthusiastic,” up from a third back in June. The share of women voters who say they feel “uninterested” in the election has dropped from 21% to 12%. While anxiety and frustration are high for women across partisanship, Democrats now hold the edge on hope and enthusiasm. Eight in ten Democratic women report being “hopeful” (up from 51% in June). Earlier this summer, Republican women were more hopeful than Democratic women, but Democratic women now report feeling more hope. Democratic women also hold the edge on “enthusiasm,” whereas Republican women were more enthusiastic than Democratic women back in June. In Their Own Words: “What emotion best describes how you feel about the upcoming presidential election?” When women voters are asked to offer an emotion that best describes how they are feeling about the upcoming presidential election, “hopeful” and “scared” are among the most common feelings – illustrating an electorate that is experiencing both positive and negative emotions from the upcoming election with two candidates who they view very differently. Among Democratic women voters: “Very concerned, but cautiously optimistic.” – White Democratic woman from Texas, age 60 “Nervous, scared, hopeful, angry, and anxious.” – Hispanic Democratic woman from Texas, age 66 “We need a good, strong and firm presidential leader for all the people in the United States of America.” – Black Democratic woman from Florida, age 68 Among Republican women voters: “Worried that fraud will take place again like it did in 2020.” – White Republican woman from Texas, age 73 “Fear of outcome. Uneasiness.” – White Republican woman from Nebraska, age 77 “Ready to MAGA.” – White Republican woman from New Jersey, age 39 Vice President Harris has a strong advantage among women voters on which candidate they trust do a better job “looking out for the interests of women,” with nearly three times as many women voters saying they trust Vice President Harris (60%) than say the same about former President Trump (23%). Nearly all of Democratic women voters (92%) say they trust VP Harris more to look out for the interests of women, while about half of Republican women voters (52%) say they trust former President Trump. One in five (18%) Republican women voters say they trust VP Harris more to do a better job looking out for the interests of women and three in ten say they don’t trust either candidate. For Some, the First Female President Is a Motivating Factor When asked why they are now more motivated to vote, seven percent of women voters say it is because they want a woman candidate to win and are motivated by the thought of having the first woman president. Half (47%) of women voters say that being a woman doesn’t hurt or help Vice President Harris as a candidate while one in three (34%) say it helps her as a candidate, and one in five say it hurts her as a candidate. Republican women voters are much more likely to say Harris’ gender doesn’t make a difference (60% v. 35%), while three in ten (28%) Democratic women voters say it hurts her as a candidate compared to one in ten Republican women. Most women voters say their vote choice hasn’t changed since VP Harris became the Democratic nominee, with the vast majority (98%) of women voters who said back in June that they planned on voting for President Biden now saying they plan on voting for VP Harris. Additionally, 85% of women voters who said they planned on voting for former President Trump back in June continue to say they plan on voting him in the most recent poll. However, a small share (20%) of women voters who either planned on voting for Trump back in June or said they were not going to vote now say they plan on voting for VP Harris (11% of all women voters). When voters are asked why they now plan on voting for VP Harris, about a quarter offer reasons related to seeing Harris as a better candidate and for many her personal characteristics including her age, experience, and ability to care for people resonates most. In Their Own Words: “What about Harris makes you want to vote for her?” “I guess because I’m a woman and it makes me hopeful that she can do a better job running compared to past presidents. Plus we've never had a female president so I think it’s about time.” – White independent woman from California, age 36 “She seems like she will make changes for everyone and will actually do things for the country and do things for our future, especially with me being part of the younger generation to vote.” – Black Democratic woman from North Carolina, age 26 “She genuinely cares about people and I feel she will make the USA a better place to live and afford.” – White independent woman from Pennsylvania, age 56 “Her leadership skills and stances on abortion and economy and experience in politics.” – Hispanic Democratic woman from California, age 33 “She's pleasant. She treats everyone fairly. I've never heard her disrespect anyone or treat them unfairly and I think just having morals is very important.” – Black Democratic woman from Georgia, age 38 “She wants to raise the minimum wage, she's an advocate for reproductive rights, and stricter gun control.” – Asian Democratic woman from Texas, age 23 “She is sharp, energetic, not intimidated by Trump, has values, will stand up for women.” – White Democratic woman from Washington, age 61 “Plans to improve health, boost the economy and mitigate climate change.” – Hispanic Democratic woman from Minnesota, age 27 “Her age, her stance on issues, how she conducts herself, her experience. She was my top choice in 2020 and I have been a supporter of hers for a long time. I think her experience in every branch of government, her specific stances, and her personal traits are all assets in this moment in American history.” – White Democratic woman from New York, age 35 On the other hand, women voters who plan on voting for former President Trump in the 2024 presidential election offer responses related to his previous experience or his policy decisions when asked about their vote choice. Four in ten women voters who plan on voting for Trump say they think he is a better choice and that he did a good job during his presidency. About a quarter of Trump supporters say his economic policies are why they plan on voting for him. In Their Own Words: “Why do you plan to vote for former President Donald Trump in the 2024 presidential election? “He is a business man who understands relationships, the economy and has integrity. I can see why people can't stand his arrogance, but beyond that his word you can take to the bank.” – White Republican woman from Oregon, age 33 “Kamala Harris's position on many issues is way too liberal.” – White Republican woman from Florida, age 61 “The cost to live has gone up and I can barely afford things anymore. My mortgage has gone up $300/ month.” – White Republican woman from Illinois, age 37 “Because he's strong and I believe he's got the nation's best interests at heart. Even though I don't like his personality but that doesn't matter.” – White Republican woman from Kentucky, age 75 “He is better for the hard working middle class Americans and he has many grandkids to look out for by making good policies for America and he has experience and a good record as president.” – White Republican woman from Colorado, age 29 “His policies align more with my values of small government, lower taxes, and less socialism.” – White Republican woman from Missouri, age 45 The Worst (And BEST!) Election Takes I've Seen So Far Author: Green, Hank, Date: 2024-11-09 Collections: Hot Takes US Elect 2024 Zotero Key: RA4S6JTL Cite Key: Green24WorstBestElectTakes Zotero Item | Lit Note 00:00 I probably shouldn't be making this 00:01 video but I want to make a video that's 00:02 called the best and worst election takes 00:04 I saw or whatever the title of this 00:06 video is cuz I saw some real stinkers 00:08 but I also saw what I thought were good 00:10 ones and I felt like that might be 00:12 helpful for people to hear my thoughts 00:15 about those thoughts and also just to be 00:17 exposed maybe to some of the the better 00:19 pieces of analysis but though I will 00:21 admit everything I'm going to show you 00:22 in this video is a tweet so don't expect 00:25 it to be high level I'm me I don't I'm 00:30 not a political scientist but I'm going 00:31 to start with just terrible takes and 00:34 then we're going to get through to some 00:36 pretty good ones and then really good 00:38 ones that's how the the structure of the 00:39 video is going to be and we're just 00:40 going to start with terrible and I have 00:42 a folder on my desktop it's called 00:43 terrible the first one is from Mike Lee 00:45 who's an actual US senator which is the 00:48 reason why this is such a terrible take 00:50 if it was just some person I'd be like 00:52 okay 70,000 people liked this tweet is 00:54 there any credible non-suspicious 00:56 explanation for Arizona and Nevada 00:58 taking this long to count votes yes the 01:01 answer to that question is yes but if I 01:02 would add to that answer I would add 01:04 you're a 01:05 senator just ask somebody you have 01:08 resources at your disposal Mike you can 01:11 find out how the election systems in 01:14 Nevada and Arizona work you've been a 01:16 senator since 01:18 2011 you know how elections work this is 01:21 not in good faith but 70,000 people 01:23 liked it next tweet how do you call any 01:25 election when you have us intelligence 01:27 concluding a foreign adversary has 01:30 interfered this is true the US 01:32 intelligence did conclude that a foreign 01:33 adversary has interfered a little bit 01:36 one way or another I'm not going to get 01:38 too deep into it but like some somebody 01:41 who is probably based in Russia called 01:43 in some bomb threats to some polling 01:44 places at the very least but Jack and 01:46 also the 63,000 others of you that's not 01:49 how it works we can say for sure in this 01:52 case that it didn't matter like if that 01:54 makes you feel any better but even if 01:56 the Russian government somehow 01:58 influenced the the vote of the us 02:02 through bomb threats or whatever you do 02:05 have to accept the results that came in 02:08 if it wasn't the ideal situation it 02:09 still happened and we have to continue 02:11 to do America you don't get to not 02:12 certify the election because Russia did 02:16 some stuff you don't get to not certify 02:18 the election because social media 02:20 companies did some stuff you have to 02:22 just have it go how it goes and then you 02:24 can try and fix those problems later but 02:25 Russia isn't going to stop meddling in 02:27 US elections and yet we still have to do 02:29 democ 02:30 even with the world being imperfect 02:32 around that Karen says okay remember 02:34 when Donald Trump said don't vote it's 02:35 okay we have all the votes we need 02:37 something's not right about this 113,000 02:39 likes like I understand trying to make 02:41 sense of a of of a world that you didn't 02:42 want to live inside of but 113,000 02:44 people shouldn't be liking this tweet 02:46 that's that's a lot of likes for a tweet 02:48 I looked this up and I looked at the the 02:50 speech where he said this and it was 02:51 clear to me that Donald Trump is not 02:54 always the best at talking and he was 02:56 trying to cover up a flub and I just 02:58 want to say to all the people who are 02:59 going to use those two tweets that I 03:01 just showed and didn't say the Democrats 03:03 also deny elections this is going to be 03:05 important listen this is some person 03:07 named Karen Watson and this was some guy 03:10 named Jack and none of the people who 03:12 are actually in charge and have power in 03:15 this situation are taking that and 03:17 running with it kamla Harris conceited 03:20 graciously and democracy is working the 03:22 way that it should right what happened 03:24 in the other case a different thing 03:25 happened and Donald Trump has still 03:27 never said that he lost and when you ask 03:28 him why he thinks he one he gives any 03:31 number of responses that have all been 03:33 disproven in one way or another or are 03:35 not legitimate like saying that a media 03:37 company making a decision is a reason 03:39 why we shouldn't certify an election the 03:40 first one that's not from Twitter I'm 03:42 pretty sure this is from threads we are 03:43 officially in the movie Idiocracy again 03:45 for the 80th time idiocracy is about 03:48 Eugenics the whole point of the movie is 03:50 that rich people don't breed and poor 03:53 people do and I'll note I did just say 03:54 rich and poor not smart and stupid but 03:58 then what happens because the rich 04:00 people didn't breed and the poor people 04:01 did the whole world gets stupid I 04:04 understand it's a comedy movie and I'm 04:05 fine with it as a comedy movie but when 04:07 we start to apply to the real world 04:09 that's got some bad vibes but this is a 04:11 map showing that more Highly Educated 04:13 states are more likely to vote for 04:15 Democrats and I think that's true and I 04:17 don't think it necessarily has anything 04:19 to do with intelligence I think it has a 04:21 lot to do with a backlash against 04:24 existing systems and I think that has a 04:26 lot to do with a lot of things that I'm 04:28 not going to get into right now but this 04:29 this is a terrible take to say it's 04:31 because people are stupid we're going to 04:32 see that take again in a slightly better 04:35 form later on 70,000 likes on this one I 04:37 really only picked ones that had a lot 04:39 of likes when it came to the bad takes 04:41 because there's lots of bad takes that 04:43 don't get a lot of likes but I feel like 04:44 the ones that get a lot of likes are 04:45 worth paying attention to Matt Walsh 04:47 says now that the election is over I 04:48 think we can finally say that yeah 04:50 project 2025 is the agenda l two things 04:53 about that first Matt Walsh's job is not 04:56 to do government policies to get 04:58 attention on the internet he will say 05:00 anything to get attention on the 05:01 internet and that's what he's doing 05:02 right now but I do think that it is the 05:03 case that many things in Project 2025 05:05 which is a very long document will be 05:07 things that the Trump Administration 05:08 tries to do the fact that Trump was like 05:10 I have no idea what that is and I've 05:11 never read it and there are some good 05:12 things in it and some bad things in it 05:13 and I'm like how did you know that if 05:14 you've never read it and you don't know 05:15 what it is I think that was a lie and I 05:16 think it was bad an onion person here 05:18 says I think that the left and 05:19 especially Democrat liberals need to 05:21 internalize that intentionally lying is 05:22 an incredibly useful and good political 05:24 strategy I think that Donald Trump is 05:26 exceptionally good at lying not just 05:28 because he's like good at lying but 05:30 because he like says everything he just 05:32 like constantly talks and so people are 05:36 kind of like well you can't really take 05:37 him at his word because he's just like 05:38 he's just he's like being a person he's 05:40 just off the cuff you know so there's a 05:42 reason why this is a pretty bad take 05:44 which is that um it this can be a good 05:47 or bad strategy depending on you the 05:49 electorate you're trying to win and the 05:51 candidate that you have doing the line I 05:54 think that those things matter a great 05:55 deal with regards to whether Ling is a 05:56 strategy I think this a terrible take 05:59 because we shouldn't do that I don't 06:01 know call me oldfashioned so we're going 06:03 to move on to the pretty good takes 06:06 these were like okay pretty good takes 06:09 let's check out what we got here the 06:11 first one this one's from Blue Sky and 06:14 he says one of the things I keep 06:15 circling back to is all of the terrible 06:17 people actions and ideas that people 06:19 will now claim were 06:20 validated like the tweet that we just 06:22 saw Trump teal and all the worst 06:25 impulses of the worst parts of the tech 06:27 wealth class Vance and all this will now 06:29 be seen as good successful strategy like 06:32 and I think there's a bit of this 06:34 because I experienced this too despite 06:35 the fact that I think this is a pretty 06:36 good take I think there's a bit of this 06:38 this that is probably 06:40 about um like the feeling of losing 06:44 there's all these things that were like 06:46 exceptionally annoying to me in 06:48 particular you know netanyahu's strategy 06:51 bezos's cowardice Etc that like those 06:54 things in particular that annoyed me I'm 06:56 like experiencing the loss in an 06:59 addition weighted way that makes me 07:01 think this is probably not that big of a 07:02 deal because it's more about an 07:04 emotional reaction that I have and I 07:07 can't speak for Jacob but it's more 07:08 about an emotional reaction that I have 07:10 when I'm like I we we lost and we didn't 07:12 just lose the election we also lost the 07:14 story and now there's going to now like 07:16 I have to like live in the world where 07:17 these people who I think are acting very 07:20 badly are feeling like they they're bad 07:23 action is actually successful and good 07:26 and they're having their champagne 07:27 parties and living it up and I also 07:29 experienced that it just feels very 07:31 emotional to me which makes me think 07:32 it's probably not that big of a deal 07:34 Brian stelter this is an interesting one 07:36 this is a two-parter from Twitter it's a 07:37 mistake to dismiss real dissatisfaction 07:39 with the economy and the government is 07:40 merely a product of right-wing 07:42 propaganda but it is also a mistake to 07:44 ignore the Miss and disinformation that 07:46 permeates prot Trump media from top to 07:48 bottom for example and then he shows 07:50 this ipsos poll from last month people 07:53 who answered factual questions about 07:55 inflation crime and immigration 07:56 incorrectly were more likely to opt for 07:58 Trump and those who Ed correctly we're 08:00 more likely to be Harris voters and you 08:01 can see this graphic here and it's a 08:04 pretty wide swing so like a 08:06 75 85 point swing when it comes to Crime 08:10 a a 30% swing when it comes to the stock 08:13 market and a and like a 70% swing when 08:16 it comes to us Mexico border stuff now I 08:19 am very much I think this a pretty good 08:22 take because I'm very much on the team 08:25 of the information environment is 08:27 terrible people there isn't a way to get 08:30 good information to people anymore and 08:32 most information is is tuned to one of 08:35 two purposes if it is like on Fox News 08:37 it is tuned for a political purpose Fox 08:40 News is just an arm of the Republican 08:41 Party MSNBC is just an arm of the the 08:43 Democratic party those are tuned for a 08:45 political purpose it could additionally 08:46 be tuned for just Gathering and holding 08:49 on to attention in which case telling 08:52 people that like the stock market's 08:54 pretty good like that's not going to be 08:55 good interesting content like there's 08:57 not an interesting story to be told 08:58 there and the greatest bias in all media 09:00 is toward interesting stories rather 09:02 than any individual political candidate 09:04 or ideology so I so I agree with it in 09:08 that way where I do think the 09:10 information environment is very bad and 09:12 I do also think that if you have a more 09:14 accurate picture of the situation you 09:16 would be more likely to be sympathetic 09:19 to vice president Harris and if you have 09:21 a less accurate picture you would be 09:23 more likely to vote for Donald Trump but 09:25 I don't think that this graph 09:27 necessarily tells us that because every 09:30 one of these questions is something that 09:33 if you want it to be true then you are a 09:36 Harris voter this would only be like a 09:39 useful tool to actually make brian's 09:40 case here if there was something on this 09:43 list that it would be better for a 09:45 Harris voter to believe falsely and that 09:48 could very well be the case but to me 09:50 what this this could also easily just 09:52 mean here are a bunch of partisan 09:54 questions and people of both parties are 09:56 likely to believe the thing that is 09:58 better for their candidate and in this 10:00 case all of them are better for kamla 10:01 Harris and they are also in addition to 10:03 that true so we need something that is 10:06 worse for Kam Harris but it's also true 10:08 to actually be able to make the case 10:11 that Brian is making here sorry that 10:12 took me so long to say next pretty good 10:14 take as someone who doesn't live in a 10:16 swing state I'm going to say this with 10:17 my full chest I'm tired of seven [ __ ] 10:20 States deciding the fate of 50 states 10:22 623,000 votes this is a pretty good take 10:25 because I think that the Electoral 10:27 College is outdated and we don't need it 10:28 anymore and we should go by 10:29 popular vote and I think that that is 10:32 becoming a more popular perspective even 10:34 I am surprised to find when I looked it 10:36 up among Republicans and independents 10:38 but it's only a pretty good take because 10:39 in this case it didn't matter if it was 10:41 popular vote the outcome would have been 10:43 the same cat says since 20120 I've 10:45 increasingly felt like we're in a 10:47 digital Dark Ages where illiteracy is 10:49 skyrocketing people believe in magic 10:51 disease and rampant oligarchs Rule and 10:53 it's all facilitated by ultramodern 10:55 technology that has evolved to reinforce 10:57 it I I think that it is ident 10:59 identifying a the more of a symptom than 11:02 a disease and I also think that it is a 11:05 little bit over the top I think 11:07 certainly people believe in magic like 11:08 people believe in astrology and crystals 11:10 and always have but like I think that's 11:12 actually like fewer people right now 11:14 believe in magic than at most times in 11:16 history disease is rampant not compared 11:18 to the dark agees it's like we we are we 11:20 have taken it feels like a little bit of 11:21 a step back there are more oligarchs it 11:23 is weird to watch Elon Musk very 11:25 intentionally become an oligarch and I 11:27 think this is a pretty good take because 11:29 it like all of these things seem of the 11:32 past but they are enabled by this 11:34 ultramodern technology and that's really 11:36 interesting I just think it's a little 11:38 over the top and and I think that the 11:41 connection between these things between 11:43 the technology and what has happened is 11:46 really strong and needs to be like 11:48 articulated clearly somehow in a thing 11:51 that isn't the length of a book read 11:53 books about this but did I people aren't 11:56 going to read the books and they're all 11:58 by well I guess the You' all know a 11:59 Harari book is pretty whatever that new 12:02 the new one is pretty accessible and 12:04 meant for a broad audience the other 12:06 books I've read that are like by 12:07 professors and are not don't really to 12:09 me feel like they are for a broad 12:10 audience but they're great for me I'm 12:12 thinking of the Gutenberg parenthesis by 12:13 Jeff Jarvis if you want to check that 12:15 one out and also wrong there's a book 12:17 called wrong that is also interesting in 12:19 this fa in this and I can't remember who 12:21 wrote it I'll put the cover on the 12:22 screen and then our last pretty good 12:24 take is coming from Naval who I don't 12:27 usually agree with on anything but says 12:28 El made being Republican cool again I 12:30 think it's only pretty good because I 12:32 don't think that being a member of 12:34 either party is cool and I think that 12:36 Elon would not himself call himself a 12:38 republican most people who are in this 12:41 group of of people who are younger 12:44 cooler people voting for Donald Trump 12:48 are would call themselves Independents I 12:50 would guess but there's something to 12:51 this I think that is by far more a 12:55 bigger piece of Trump's growth than Elon 12:57 spending money is El being a person who 13:01 is seen as an exceptional person 13:05 normalizing his support for Trump and 13:07 and an aspirational person for many 13:09 people I think it's a pretty good take I 13:11 don't think I'm not all the way there on 13:12 it but like I I I think that it's worth 13:14 admitting that many people see Elon Musk 13:17 as remarkable and aspirational and that 13:20 his full-throated support for Donald 13:22 Trump and full body dive into conspiracy 13:26 theory specifically around voting stuff 13:29 and and immigration it has really 13:31 normalized that stuff for a lot of 13:33 people and is a strong force for the 13:37 specific kind of populism that Trump and 13:39 musk currently are embodying and and 13:42 representing I think it's worth 13:44 interfacing with everybody and then 13:45 we're going to move on to our actually I 13:48 think very good takes but before we get 13:50 to the good takes this video is brought 13:52 to you by good. store and specifically 13:54 it's brought to you by good. store's 13:56 overp purchase of socks we have had 10 13:59 of thousands of subscribers to the 14:00 awesome socks club for a long time now 14:02 over three years and we always order a 14:06 you know a few more socks than we need 14:08 just in case we have to do returns and 14:10 stuff and that overage adds up over the 14:12 years and so right now we have some 14:14 tremendously good deals on those excess 14:17 socks because we need to get them out of 14:18 the warehouse I can't believe I'm saying 14:20 our warehouse we need we're Overstock we 14:22 need to get it out of the warehouse but 14:24 like Josh who runs the warehouse is like 14:26 Hank we need to get these socks out of 14:27 the warehouse so we got good socks deals 14:29 right now I'll put them in the 14:30 description and maybe you can buy just 14:32 buy a bunch of socks and give them all 14:34 away to a bunch of your friends and also 14:36 because we've already paid for these 14:38 socks basically all of it for us is 14:40 profit and that's good for you and the 14:42 world because we donate all of our 14:44 profit to charity while you're there you 14:45 might check out our coffee or our tea or 14:47 our soap they're all very good and worth 14:49 a try and excellent for gifting all 14:51 orders over $59 have free shipping 14:53 domestically right now and if you use 14:55 the code save 10 you can get 10% off any 14:57 order over $50 and with save 20 you can 15:00 get 20% off of any order over $100 15:03 thousands of I'm looking at the map here 15:04 thousands of people have used those 15:07 coupon codes so don't miss out we might 15:09 turn them off soon who knows all right 15:12 good takes we going start with Jamal 15:13 Buie on Blue Sky who says so far the 15:16 only explanation I've seen that doesn't 15:18 smack of if only she'd listened to me 15:20 she would have won the first half of 15:21 that is a great take I saw so many 15:23 people who were like see I told you you 15:26 wouldn't win if you didn't do what I 15:27 told you to do it's like well yeah I 15:29 mean like of course I feel that way like 15:32 I feel like if she'd more been more 15:35 about the things that I care about the 15:37 most that she would have done better 15:38 because I think I'm like most people but 15:39 I'm not and I I've saw so many of those 15:41 takes and I think that they are useless 15:43 and I did not include any of them here 15:44 because I did not want to deride any 15:47 specific perspective and they all came 15:49 from specific perspectives so the only 15:51 explanation that doesn't smack of that 15:52 is global discontent with incumbents 15:54 because of inflation and post-pandemic 15:56 delays plus toxic information 15:58 environments maybe we'll do toxic 16:00 information environment in a second um 16:02 but we're going to move on first to this 16:04 a similar perspective from Christopher 16:07 Frederico who says people need to 16:09 discipline themselves against 16:10 willy-nilly arguments about who or what 16:11 is to blame for the pro-democracy side 16:14 folks are going to be tempted to a light 16:17 on whatever's been bothering them about 16:19 their own side for years I think that 16:21 this is wise the fact of the matter is 16:23 whoever is holding the political ball 16:24 when The Vibes turn bad is screwed and 16:27 whoever the alternative is will benefit 16:29 even if they happen to be cranks and and 16:31 included this from the financial times 16:33 which shows that every governing party 16:35 facing election in a developed country 16:37 this year lost vote share the first time 16:39 this has ever happened and that seem it 16:42 feels like it matters to me and so if 16:44 you look at that you might think what is 16:45 it about the us where the incumbent 16:46 party was so strong instead of the way 16:49 that I think most people are looking at 16:50 it which is how on Earth did this happen 16:53 now I think that there are unique 16:55 circumstances in the US that would make 16:56 the incumbent party more strong 16:58 particularly that Donald Trump is in a 17:02 way un incumbent and that he has been 17:05 president before but he's not the 17:06 incumbent and like the reality is that 17:09 inflation is really hard and and The 17:11 Vibes around it are very bad and I don't 17:13 think that people understood the extent 17:15 to which The Vibes around it would be 17:16 very bad because you know you look at 17:17 other areas on this graph where 17:19 inflation was very high back in the 17:21 early 1980s and you know you see like 17:24 that that Clump there in the early 1980s 17:26 is below average with one outlier but 17:29 not like this you know not like that and 17:32 I think that that comes back to what 17:33 Jamal is saying that it is more it like 17:35 it is important to recognize that that 17:38 who's holding the ball when The Vibes 17:39 turn bad is a huge part of this but in 17:42 addition there is the toxic information 17:44 environment which I just think is a 17:48 really real Factor right now we would do 17:50 well to try and understand it better 17:53 which I am doing right now in a script 17:56 that will hopefully be my Vlog brother's 17:58 video next week Kate says on Blue Sky 18:01 most of my good takes are from Blue Sky 18:02 which is interesting the right built a 18:04 powerful partisan and participatory 18:06 media environment to support its 18:07 messaging which offers a compelling deep 18:10 story for its participants the left 18:12 relied on Rigid self-preserving 18:14 institutional media and its story is 18:16 little more than a defense of imperfect 18:18 institutions and I just think that's 18:20 really spot-on I I think it's really 18:22 hard we have a toxic information 18:24 environment that makes it very unpopular 18:26 to defend institutions I think we're 18:28 underestimating the significance of the 18:30 information environment shift that has 18:32 occurred over the last 20 years I think 18:35 it's very significant and I this makes 18:37 me think even deeper thoughts about it 18:40 especially with respect to how the left 18:43 kind of can't win nationally at least 18:47 while the story is just the defense of 18:49 imperfect institutions I think the word 18:51 imperfect there is very important and I 18:53 I think that is kind of the story right 18:54 now and if the left wants to win 18:56 elections I feel like it's going to have 18:57 to shift I saw a couple I didn't include 18:59 this in here but I saw a couple of of 19:00 tweets that were basically just like 19:02 look we're going to have to do populism 19:03 now that's the choice we have and and 19:05 like I don't necessarily like it as a 19:07 person who is a kind of defender of 19:09 imperfect institutions kind of guy but I 19:11 think that's right and then of last 19:13 we're gonna look at my friend technology 19:15 connections who says for [ __ ] sake 19:16 don't fight over why this happened do 19:18 not make your pet issue your identity 19:21 [ __ ] work together the wildest thing 19:23 is lusting for a coalitional form of 19:25 governance when you can't even form a 19:27 coalition of friends just fantastic 19:29 string of words there and I know that 19:31 Alec doesn't like to be super political 19:33 on Main so I hope that he's okay with me 19:36 sharing this I'll actually check and see 19:38 if he left it up he did and then he said 19:40 he did reply to it and he said I 19:42 genuinely hate bringing this tone out 19:44 especially on an Neil's post but the 19:47 time for bickering is over I'm sorry it 19:49 it may be over but it is not going to be 19:51 over so I apologize for that generally I 19:55 I don't know how much I should have made 19:58 this video video I sense making is 20:00 important and one of the ways that we 20:02 make sense is that we look to the people 20:04 around us and how they are making sense 20:05 of the world and we take that as how we 20:09 are making sense of the world but I'm 20:10 trying to approach information in a way 20:12 that really is about wanting it to be 20:15 true like wanting to believe true things 20:18 and that's hard on the internet and I 20:19 also want to try and model how I do that 20:21 when I make content which is why I'm 20:23 doing this this is uncomfortable for me 20:25 especially because I'm sure that I'm 20:26 going to get shattered about about a lot 20:27 of stuff because there's nothing quite 20:28 hotter than an election especially one 20:31 like this but I I hope that it's 20:34 valuable if it doesn't feel valuable I 20:37 will take the video down but that's what 20:39 we'll do we'll see Electoral Volatility in Latin America Author: Cohen, Mollie J., Date: 07/2018 Collections: ElectionPredFeats Zotero Key: 5V9YVNGK Cite Key: Cohen18ElectoralVolatilityLatin Zotero Item | Lit Note S H O R T A R T I C L E Electoral Volatility in Latin America Mollie J. Cohen, Vanderbilt University Facundo E. Salles Kobilanski, Vanderbilt University Elizabeth J. Zechmeister, Vanderbilt University Using an original database of legislative and presidential electoral results from the democratic transitions of the 1970s and 1980s to the present day, we provide a new assessment of electoral volatility in Latin America. Following a model established in studies of other regions, we decompose volatility into two subtypes: party replacement and stable party volatility. We demonstrate the relevance of this approach to Latin America and, further, document that volatility has persisted rather than waned in Latin America’s posttransition period largely because of party replacement. We then examine the contested link between economic performance and volatility and document temporal instability in this relationship: our analysis affirms previous conclusions regarding a connection in the 1980s in Latin America but uncovers scant evidence of a relationship between the economy and volatility for the 1990s to present day. our decades into democracy’s third wave, many Latin vance to Latin America of a framework that assesses volatility American political systems continue to experience in- in terms of party replacement and established party subtypes stitutional conflicts and the weakening or collapse of (Birch 2003; Powell and Tucker 2014). Further, and contrary Fparties (Helmke 2010, 2017; Lupu 2014; Morgan 2011; Rob- to views of consolidation in Eastern Europe and Africa (Powell erts 2003; Seawright 2012). Instability matters because it and Tucker 2014; Weghorst and Bernhard 2014), we show that undermines citizens’ ability to navigate programmatic poli- electoral volatility still is not diminishing in Latin America, tics (Carlin, Singer, and Zechmeister 2015) and motivates and we identify that this is largely due to persistent party reelites to secure positions and experiment with unanticipated placement. appeals (Helmke 2017; Lupu and Riedl 2013; Stokes 2001). Third, we provide a new perspective on the debate about We use new data to assess of one form of instability: electoral the relevance of the economy to volatility. Remmer’s (1991) volatility. classic work shows that, in the 1980s, poor economic perFirst, we present an original, comprehensive database of formance increased electoral volatility in Latin America. Revote shares for Latin America: the Latin American Presi- cently, Powell and Tucker (2014) sparked fresh debate on the dential and Legislative Elections (LAPALE) database (http:// relevance of economic indicators to electoral volatility (Crabwww.lapaledata.com). LAPALE establishes source documen- tree and Golder 2016, 2017; Powell and Tucker 2014, 2017; tation, draws from official sources, runs from democratic tran- see also Dassonneville and Hooghe 2016). We document a sitions in the 1970s/80s to 2016, and facilitates tests of ro- relationship between economic performance and volatility bustness to older data sets (e.g., Nohlen 2005) and to varying for the 1980s, as per Remmer (1991); however, we find no party inclusion thresholds. Second, we demonstrate the rele- evidence that this link persists. In short, we show that the Mollie J. Cohen (mollie.cohen@gmail.com) is a postdoctoral research fellow, Facundo E. Salles Kobilanski (facundo.e.salles.kobilanski@vanderbilt.edu) is a graduate student, and Elizabeth J. Zechmeister (liz.zechmeister@vanderbilt.edu) is Cornelius Vanderbilt Professor of Political Science at Vanderbilt University, Nashville, TN 37240. In the first stage of this project, the second author was supported by a Fulbright doctoral fellowship awarded by the Fulbright Commission in Argentina and the city of Buenos Aires, Argentina. The LAPALE data and supporting materials necessary to reproduce the numerical results in the article are available in the JOP Dataverse (http://thedata.harvard.edu/dvn/dv/jop). An online appendix with supplementary material is available at http://dx.doi.org/10.1086 /697464. The Journal of Politics, volume 80, number 3. Published online May 10, 2018. http://dx.doi.org/10.1086/697464 q 2018 by the Southern Political Science Association. All rights reserved. 0022-3816/2018/8003-00XX$10.00 000 000 / Electoral Volatility in Latin America Mollie J. Cohen, Facundo E. Salles Kobilanski, and Elizabeth J. Zechmeister economy can affect volatility in regions where democratic party politics has recently (re)emerged, but such links are transient across epochs. ELECTORAL VOLATILITY AND ITS (CONTESTED) ECONOMIC UNDERPINNINGS Since Przeworski (1975) and Pedersen (1979), scholars have been assessing, reflecting on, and debating measures, causes, and consequences of electoral volatility. Volatility in Western Europe has increased over time (Dalton and Wattenberg 2000; Dassonneville and Hooghe 2016), while African and Eastern European systems have been stabilizing (Powell and Tucker 2014; Weghorst and Bernhard 2014). One perspective is that volatility decreases over time in posttransition regions. Latin America appears to be an exception (Concha Olivares 2014; Lupu and Stokes 2010): high volatility (Roberts 2014) seems to persist amid parties and systems splintering or collapsing (Lupu 2014; Morgan 2011). Understanding volatility is important because where instability reigns, political elites are prone to make unanticipated moves (Lupu and Riedl 2013; Stokes 2001; Tavits 2008). Such outcomes reflect and feed a vicious dynamic in which instability creates conditions that are ripe for out-ofprogram maneuvers by elites who diminish party brands and thus undercut the public’s capacity to use programmatic shortcuts and dampen party loyalties, all of which fuels further electoral instability (Carlin et al. 2015; Lupu 2016; Morgan 2011; Zechmeister and Corral 2013). While some vote fluctuation is healthy, too much suggests de-alignment or underinstitutionalization (Dalton and Wattenberg 2000; Mainwaring and Pérez-Liñan 2015). Electoral volatility theoretically is influenced by retrospective calculations. Economic downturns lead voters toward anti-incumbent preferences (Benton 2005; Fiorina 1981; Murillo and Visconti 2016; Roberts 2014; Roberts and Wibbels 1999). Yet, the effect of the economy on volatility is subject to debate. Powell and Tucker (2014) argue that Eastern European economic downturns increase volatility, but Crabtree and Golder (2017) challenge the robustness of this finding to the exclusion of an outlying case (in response, see Powell and Tucker 2017). Weghorst and Bernhard (2014) identify a connection between economic performance and electoral volatility in Africa. Remmer’s (1991) oft-cited work documents a relationship between economic downturns and volatility in Latin American in the 1980s (see also Roberts and Wibbels 1999). Yet, Roberts (2014) suggests that this could be contingent on parties’ capacity to weather backlash against poor output. In sum, the link between the economy and volatility is contested. DATA AND MEASURES While electoral volatility has been examined in Latin America, the field has lacked a comprehensive data set for the years since the transitions of the 1970s and 1980s. Furthermore, a significant number of analyses use the classic Pedersen Index measure of total electoral volatility, which lumps together all types of interparty volatility in the system. [1] We collected original vote-share data from official electoral management bodies (EMBs) across 18 Latin American countries, from democratic transitions to current times. Much existing scholarship on electoral volatility in the region draws heavily on Nohlen’s (2005) data set (see the appendix, available online). With a commitment to replication, we worked to confirm (or refine, when needed) his database and extend it through 2016. When possible, we collected official data from countries’ EMBs. Otherwise, we supplemented our data set with alternative data (see table A1; tables A1–A6 are available in the online appendix). We find substantial congruence between official vote share data and Nohlen’s (2005) data set. Correlations for key measures between Nohlen’s database and our own for the time period in which there is overlap (1978–2005) range from .80 to .95, which verifies the general accuracy of Nohlen’s data (see the appendix). At the same time, the LAPALE database extends beyond previous data sets on electoral outcomes in the source documentation it contains, in transparency, in flexibility regarding decisions on thresholds for party inclusion, and in terms of the time period it covers. Following recent work that distinguishes volatility attributable to the entry of new parties or exit of established options from that caused by shifts in vote shares among established parties (Birch 2003; Powell and Tucker 2014; Sikk 2005; Weghorst and Bernhard 2014), we used the LAPALE database to deconstruct total volatility into these constituent parts. The approach keeps total volatility in the picture, while isolating flux in the menu of political options due to new parties forming and competing with varying degrees of success and duration and old parties splintering or exiting. We produced three measures of volatility for legislative and presidential elections. First, Party Replacement Volatility measures shifts in vote shares among parties that newly en 1. Exceptions include Coppedge (1998), who measures average (ideological) block volatility but does not address differences across new and established parties; Mainwaring, España-Nájera, and Gervasoni (2016), who break the Pedersen Index into within-system and extra-system volatility, which requires considerable conceptual clarity and copious case knowledge (Hug 2001, 79–80); Su (2014), who focuses on volatility within political parties (vs. systems); and Carreras, Morgenstern, and Su (2015), who account for transfers from old to new parties but focus on the two largest parties. tered competition at time t, as well as parties that exited competition between time t 2 1 and t (type A, per Powell and Tucker 2014):[2] Type A p o pexit(t21) 1 o penter(t) ��� ��� : 2 Second, Stable Party Volatility calculates volatility in vote shares among established parties (those that have not changed in name since the most recent election; Powell and Tucker’s type B): pit21 2 pit ��� ��� Type B p [o] ; among stable parties: 2 Third, we measure Total Volatility by adding these subtypes together. Each measure theoretically ranges from 0 to 1, with 0 indicating no change in vote shares over time, and 1 meaning that all votes were distributed across different parties at time t versus time t 2 1. In considering whether to calculate values using vote shares for all parties or only those electoral options that reach a particular threshold, we assessed four decisions: no threshold, and thresholds of 2%, 5%, and 7% of all valid votes cast; we present results for analyses with no threshold and a 7% threshold.[3] Following studies that identify economic performance as a predictor of volatility (e.g., Remmer 1991; Roberts and Wib 2. A key challenge is how to identify stable vs. new parties. One approach considers new parties resulting from a split or merger as successors of preexisting parties. This can be problematic, as parties formed by splits do not always adhere to the older party’s program (e.g., Mexico’s PRD split from the PRI, Costa Rica’s PAC split from the PLN; see Roberts 2014). Further, parties often run under coalition labels that disband postelection (e.g., Guatemala’s 2011 short-lived UNE-GANA coalition). Mainwaring and Zoco (2007) offer a more nuanced approach, by taking the largest resulting party from a split as the continuation of the prior and coding as the predecessor of a merger the party that received the most votes during the last election (see also Powell and Tucker 2014). We define each party name change as a new observation, even if the new party is the direct successor of an extant party or the product of a merger or split (we exclude minor changes, e.g., the inclusion or exclusion of articles [a, an, the]). This most closely approximates voters’ experiences: changes in names imply information costs. Thus, our measures of party replacement volatility are ceiling measures that capture the maximal volatility caused by party entry/exit. 3. Some argue that only electorally relevant parties (those that won a given percentage of the total vote [usually 2%, 5%, or 7%]) should be included in analyses of system dynamics because including smaller parties may bias estimates upward (Moraes 2015). Yet, excluding small parties simplifies the political space in ways that do not reflect citizens’ decisionmaking environments. Variation in threshold use and levels has been haphazard, although evidence from Eastern Europe suggests similarity in results across thresholds (see Powell and Tucker 2014). We affirm this conclusion for Latin America: measures of Party Replacement and Stable Party Volatility for the lowest (0) vs. the highest (7%) thresholds are highly correlated, at .97 and .84, respectively. Volume 80 Number 3 July 2018 / 000 bels 1999), we also assess three measures of economic development: DGDP, Inflation, and Exchange Rate data for the year before each election in the database. For the latter two, we calculated the natural log of inflation and the exchange rate to account for very large values.[4] ELECTORAL VOLATILITY PERSISTS IN LATIN AMERICA, DRIVEN BY PARTY REPLACEMENT Total electoral volatility in Latin America increased sharply between the 1980s and 1990s, at which point presidential volatility leveled off, while legislative volatility continued to increase but at a lower rate. This affirms the persistence of a trend noted by Roberts (2014) and runs counter to those concluding that third-wave democracies trend toward lower volatility and more consolidation. We add further insight by demonstrating that party replacement is the principal driver of this persistent electoral volatility. Figure 1 displays summary statistics for the two subtypes of volatility. Dark-gray boxes represent observed values for Party Replacement Volatility, and light-gray boxes represent Stable Party Volatility. Each box shows the 25th–75th percentile observations, and the horizontal line within each box represents median volatility for each decade. Whiskers represent the highest and lowest observations in an approximately normal distribution; dots signify outlying observations. As shown, Party Replacement Volatility has increased over time in Latin America, while Stable Party Volatility has remained more level, especially from the 1990s to the present day. Party Replacement Volatility across legislative elections trended upward from the 1980s to the 2000s, and variation in this subtype of volatility increased substantially, especially in the post-2000 period. Volatility among stably competing parties, in contrast, has remained relatively constant since the 1990s. For presidential elections, volatility caused by both party replacement and vote shifts across extant parties declined slightly in the recent period. Whereas volatility among established parties accounted for a greater portion of total volatility in the 1980s, average party replacement volatility increased in the 1990s and accounts for a substantially greater portion of all volatility observed than does stable party volatility. We further note that, while volatility has tended to increase over this period, there are some countries (e.g., Mexico) where the trend does not hold. The region-average patterns do not change across different thresholds, as illustrated by the left versus right panes of figure 1. 4. DGDP and Inflation data come from the World Bank Development Indicators database; Exchange Rate data, from the International Monetary Fund. We use election-year economic indicators when elections were held in October or later. 000 / Electoral Volatility in Latin America Mollie J. Cohen, Facundo E. Salles Kobilanski, and Elizabeth J. Zechmeister Figure 1. Trends in subtypes of volatility across time, Latin America. Left, no threshold applied; right, 7% threshold applied In sum, upward trends in Latin American volatility are driven largely by new party entry and party exit.[5] These dynamics have been facilitated by reforms that, in several cases, have made it easier for new parties to form and compete; by low party loyalty among much of the public (see Cohen 2017); and by frequent experimentation in party branding by populists. In contrast to findings from Eastern Europe and Africa, in posttransition Latin America, a state of disequilibrium persists as parties and the voting public repeatedly churn through new options, frequently tossing out the status quo in favor of new alternatives that offer hope for better outcomes. THE CONTINGENT RELATIONSHIP BETWEEN ECONOMIC PERFORMANCE AND VOLATILITY While previous scholarship has found that economic conditions shape electoral volatility in Latin America (Mainwaring and Zoco 2007; Remmer 1991; Roberts and Wibbels 1999), this relationship has not been explored for more recent times and for volatility subtypes in the region. We operationalize economic performance as inflation, exchange rate, and change in GDP (gross domestic product) in the year before the election. We control for the effective number of parties and candidates competing and analyze legislative and presidential elections separately. We present results using the no 5. As stated in n. 2, the measure counts rebranding as a new party entry; this liberal coding strategy allows us to define the upper bound on volatility levels, avoids arbitrary decisions in distinguishing a new party from a rebranded party, and reflects changes that complicate politics and make programmatic voting more difficult. threshold measure; we find no substantive difference using a 7% threshold.[6] Data columns 1–4 of table 1 replicate models (e.g., Remmer 1991) linking economic performance to volatility in the decade after transitions to democracy across the region (1980– 90). For the 1980–90 period, considering the party replacement measure, inflation and contractions in GDP predict more volatility in legislative elections, and inflation is significant for presidential elections. Growth negatively predicts stable party volatility in legislative elections in the 1980s, while we find no relationship between economic indicators and stable party volatility in presidential elections. The 1980s were a volatile time, and some elements of the economy predict electoral volatility. Yet, that result is epoch contingent. Table 1 columns 5–8 show results from a similar set of models for the whole time period. The difference is stark: across a broader range of cases, economic performance variables do not reach statistical significance. The only possible exceptions come from adopting a more generous threshold for statistical significance and, even then, only for inflation and exchange rate in one model (col. 8).[7] Results estimated for each decade reveal the same thing: after the economic 6. We include country fixed effects for the 1978–2016 models but, because of the small number of cases from 1980 to 1990, do not include fixed effects in those models. 7. The bivariate correlation among the three variables ranges from 0.03 to 20.13 for the full data set. Findings are robust to the stepwise inclusion of economic variables. See tables A4–A6. While we control for some political measures, we acknowledge that the decision to focus primarily on the economy could leave the analyses vulnerable to bias, if there are omitted relevant variables that are collinear to these measures. Volume 80 Number 3 July 2018 / 000 Table 1. Economic Predictors of Volatility (Ordinary Least Squares) 1980–90 1978–2016 Party Replacement Volatility Stable Party Volatility Party Replacement Volatility Stable Party Volatility Legislative Presidential Legislative Presidential Legislative Presidential Legislative Presidential (1) (2) (3) (4) (5) (6) (7) (8) Inflation (ln) .08** .07** 2.00 .02 .01 .01 .00 .03[1] (.02) (.02) (.01) (.02) (.02) (.02) (.01) (.02) DGDP 2.02* 2.01 2.01* 2.01 2.00 2.01 .00 2.00 (.01) (.01) (.00) (.01) (.00) (.00) (.00) (.00) Exchange rate (ln) .02 2.01 2.02 .00 2.02 2.03 .01 .01[1] (.01) (.01) (.00) (.01) (.01) (.03) (.01) (.01) N 26 23 26 23 134 116 134 116 R[2] .38 .39 .48 .24 .57 .63 .33 .36 Note. Robust standard errors, clustered by country, in parentheses. Year control included. GDP p gross domestic product. 1 p ! .10. • p ! .05. ** p ! .01, two-tailed.** and political crises of the 1980s, there is no robust link between the economy and electoral volatility in Latin America. CONCLUSION Understanding electoral volatility is critical to assessments of democratic party politics. We contribute to this effort with a new database of Latin American electoral outcomes that is more comprehensive in scope and source documentation than databases that are currently in use. We affirm the relevance and traveling capacity of a recent framework for electoral volatility—party replacement and established party subtypes—to the Latin American region. We document continued persistence in volatility (Roberts 2014) that runs counter to conclusions reached by scholars examining other thirdwave democracies. Finally, while we affirm a classic finding in scholarship on volatility in Latin America—that economic factors are relevant to political turnover in the 1980s (e.g., Remmer 1991; Roberts and Wibbels 1999)—we show that this link is ephemeral. Overall, economic factors are not robust predictors of electoral volatility in Latin America. Scholars seeking to explain the frequency by which Latin American parties enter and exit, and win and lose, will need to broaden their scope to consider other factors as they engage the LAPALE database to move scholarship forward. ACKNOWLEDGMENTS We thank Jon Hiskey, Vicky Murillo, Luis Schiumerini, Giancarlo Visconti, Hye Young You, and participants in the Vanderbilt Political Science Department’s Graham Symposium for feedback on early versions of the project. We also thank the journal’s anonymous reviewers for their feedback. We are grateful to Oscar Castorena, Alyssa Chvasta, Roberto Colón, and Christine Huang for their assistance in finalizing the LAPALE data set and supplemental material. We thank Dieter Nohlen and his coauthors for creating the excellent electoral database that sparked this project. A previous version of this manuscript was presented at the 2016 meeting of the Midwest Political Science Association. REFERENCES Benton, Allyson L. 2005. “Dissatisfied Democrats or Retrospective Voters? Economic Hardship, Political Institutions, and Voting Behavior in Latin America.” Comparative Political Studies 38 (4): 417–42. Birch, Sarah. 2003. Electoral Systems and Political Transformation in PostCommunist Europe. New York: Palgrave Macmillan. Carlin, Ryan E., Matthew M. Singer, and Elizabeth J. Zechmeister, eds. 2015. The Latin American Voter: Pursuing Representation and Accountability in Challenging Contexts. Ann Arbor: University of Michigan Press. Carreras, Miguel, Scott Morgenstern, and Yen-Pin Su. 2015. “Refining the Theory of Partisan Alignments: Evidence from Latin America.” Party Politics 21 (5): 671–85. Cohen, Mollie J. 2017. “Support for Electoral Democracy.” In Mollie J. Cohen, Noam Lupu, and Elizabeth J. Zechmeister, eds., The Political Culture of Democracy in the Americas, 2016/17: A Comparative Study of Democracy and Governance. Nashville: Vanderbilt University Press. Concha Olivares, Eduardo. 2014. “Institutionalization of Party Systems: A Cross-Regional Approach Using the Weighted Volatility Index.” Presented at the Political Studies Association 64th Annual International Conference, Manchester. 000 / Electoral Volatility in Latin America Mollie J. Cohen, Facundo E. Salles Kobilanski, and Elizabeth J. Zechmeister Coppedge, Michael. 1998. “The Dynamic Diversity of Latin American Party Systems.” Party Politics 4 (4): 547–68. Crabtree, Charles, and Matt Golder. 2016. “Response to Powell and Tucker’s ‘Little Is Known about Party System Volatility in PostCommunist Europe, but We Have Interesting New Methods and Data for Studying It.’” Working paper. Crabtree, Charles, and Matt Golder. 2017. “Party System Volatility in PostCommunist Europe.” British Journal of Political Science 47 (1): 229–34. Dalton, Russell J., and Martin P. Wattenberg. 2000. “Partisan Change and the Democratic Process.” In Russel J. Dalton and Martin P. Wattenberg, eds., Parties without Partisans: Political Change in Advanced Industrial Democracies. Oxford: Oxford University Press. Dassonneville, Ruth, and Marc Hooghe. 2016. “Economic Indicators and Electoral Volatility: Economic Effects on Electoral Volatility in Western Europe, 1950–2013.” Comparative European Politics 15 (6): 919–43. Fiorina, Morris P. 1981. Retrospective Voting in American Elections. New Haven, CT: Yale University Press. Helmke, Gretchen. 2010. “The Origins of Institutional Crises in Latin America.” American Journal of Political Science 54 (3): 737–50. Helmke, Gretchen. 2017. Institutions on the Edge: The Origins and Consequences of Inter-branch Crises in Latin America. New York: Cambridge University Press. Hug, Simon. 2001. Altering Party Systems: Strategic Behavior and the Emergence of New Political Parties in Western Democracies. Ann Arbor: University of Michigan Press. Lupu, Noam. 2014. “Brand Dilution and the Breakdown of Political Parties in Latin America.” World Politics 66 (4): 561–602. Lupu, Noam. 2016. Party Brands in Crisis: Partisanship, Brand Dilution, and the Breakdown of Political Parties in Latin America. New York: Cambridge University Press. Lupu, Noam, and Rachel Beatty Riedl. 2013. “Political Parties and Uncertainty in Developing Democracies.” Comparative Political Studies 46 (11): 1339–65. Lupu, Noam, and Susan C. Stokes. 2010. “Democracy, Interrupted: Regime Change and Partisanship in Twentieth-Century Argentina.” Electoral Studies 29 (1): 91–104. Mainwaring, Scott, Annabella España-Nájera, and Carlos Gervasoni. 2016. “Extra- and Within-System Electoral Volatility.” Party Politics 23 (6): 623–35. Mainwaring, Scott, and Aníbal Pérez-Liñan. 2015. “Cross-Currents in Latin America.” Journal of Democracy 26 (1): 45–58. Mainwaring, Scott, and Edurne Zoco. 2007. “Political Sequences and the Stabilization of Interparty Competition: Electoral Volatility in Old and New Democracies.” Party Politics 13 (2): 155–78. Moraes, Juan Andrés. 2015. “The Electoral Basis of Ideological Polarization in Latin America.” Working paper no. 403, Helen Kellogg Institute for International Studies, University of Notre Dame. Morgan, Jana. 2011. Bankrupt Representation and Party System Collapse. University Park: Pennsylvania State University Press. Murillo, Maria Victoria, and Giancarlo Visconti. 2016. “Economic Performance and Incumbents’ Support in Latin America.” Electoral Studies 45 (February): 180–90. Nohlen, Dieter, ed. 2005. Elections in the Americas: A Data Handbook. New York: Oxford University Press. Pedersen, Mogens N. 1979. “The Dynamics of European Party Systems: Changing Patterns of Electoral Volatility.” European Journal of Political Research 1 (7): 1–26. Powell, Eleanor N., and Joshua A. Tucker. 2014. “Revisiting Electoral Volatility in Post-Communist Countries: New Data, New Results and New Approaches.” British Journal of Political Science 44 (1): 123–47. Powell, Eleanor N., and Joshua A. Tucker. 2017. “Little Is Known about Party System Volatility in Post-Communist Europe, but We Have Interesting New Methods and Data for Studying It.” British Journal of Political Science 47 (1): 235–39. Przeworski, Adam. 1975. “Institutionalization of Voting Patterns; or, Is Mobilization the Source of Decay?” American Political Science Review 69 (1): 49–67. Remmer, Karen L. 1991. “The Political Impact of Economic Crisis in Latin America in the 1980s.” American Political Science Review 85 (3): 777– 800. Roberts, Kenneth M. 2003. “Social Correlates of Party System Demise and Populist Resurgence in Venezuela.” Latin American Politics and Society 45 (3): 35–57. Roberts, Kenneth M. 2014. Changing Course in Latin America: Party Systems in the Neoliberal Era. New York: Cambridge University Press. Roberts, Kenneth M., and Erik Wibbels. 1999. “Party Systems and Electoral Volatility in Latin America: A Test of Economic, Institutional, and Structural Explanations.” American Political Science Review 93 (3): 575–90. Seawright, Jason. 2012. Party-System Collapse: The Roots of Crisis in Peru and Venezuela. Stanford, CA: Stanford University Press. Sikk, Allan. 2005. “How Unstable? Volatility and the Genuinely New Parties in Eastern Europe.” European Journal of Political Research 44 (3): 391–412. Stokes, Susan C. 2001. Mandates and Democracy: Neoliberalism by Surprise in Latin America. New York: Cambridge University Press. Su, Yen-Pin. 2014. “Explaining Electoral Volatility in Latin America: Evidence at the Party Level.” Latin American Politics and Society 56 (2): 46–69. Tavits, Margit. 2008. “On the Linkage between Electoral Volatility and Party System Instability in Central and Eastern Europe.” Journal of European Political Research 47 (5): 537–55. Weghorst, Keith R., and Michael Bernhard. 2014. “From Formlessness to Structure? The Institutionalization of Competitive Party Systems in Africa.” Comparative Political Studies 47 (12): 1707–37. Zechmeister, Elizabeth J., and Margarita Corral. 2013. “Individual and Contextual Constraints on Ideological Labels in Latin America.” Comparative Political Studies 46 (6): 675–701. We Need to Learn How to Win Again (w/ James Carville) Author: The Bulwark, Date: 2024-11-08 Collections: Hot Takes US Elect 2024 Zotero Key: 8XJJT42P Cite Key: TheBulwark24learnToWin Zotero Item | Lit Note 00:00 Winning is Everything winning if when 00:02 you have to win it's more important than 00:04 loyalty it is everything if you don't 00:08 win you have done 00:10 nothing and we're unfortunately we we 00:13 finding that out the hard 00:15 [Music] 00:22 way hello and welcome to a bonus edition 00:24 of the bullwark podcast I'm your host 00:25 Tim Miller I'm here with the rag and cun 00:28 James Carville he's got a docu mentary 00:31 uh that one of our last podcasts uh Fe 00:33 was featured in uh and it is live on Max 00:37 November 14th Winning is Everything 00:40 stupid James um the last time you were 00:43 here we were talking about a kamla 00:44 Harris parade uh I guess we had they 00:47 rained on our parade brother yeah well 00:50 they did and so let's just say let's 00:53 reviewed a bidding so you and I been in 00:56 politics for a long time all right and 01:00 if the country wants something something 01:03 different you try to give the country 01:04 something different and I think that 01:07 this election really bow down to two 01:11 massive mistakes first nothing we can do 01:14 about it Biden gets out July 01:16 21st understand that this's this massive 01:19 amount of talent in the Democratic party 01:21 as I say playing there 350 hitter USS 01:23 all of a AAA ball we just don't know and 01:26 know she gets out he gets out on the 01:28 21st 01:30 I told Adam durny we didn't even get a 01:32 chance to kick the tires we just said 01:34 you're buying the car and you have no 01:36 choice and of course we bought the car 01:37 and I think if this campaign is 01:39 reducible to one moment we're in a 65% 01:42 wrong trck country the country wants 01:45 something different and she's asked is 01:48 so often the case in a friendly audience 01:50 on The View how would you be different 01:53 than Biden that's the one question that 01:55 you exist to answer all right that is it 02:00 that's the money question that's the one 02:01 you want that's the one that everybody 02:04 wants to know to answer 02:05 to and you 02:08 freeze it literally freeze and say well 02:11 I can't think of 02:13 anything and once you and it so we said 02:17 65% want something different we are just 02:20 not going to give into to them but maybe 02:22 the odiousness of trump combined with 02:25 the do decision we can overcome it well 02:27 we didn't overcome it but when if we go 02:30 back and history unearth this it's going 02:32 to be right there on The View and I 02:34 think her name was sunny hon Houston or 02:36 whatever ask the question and that 02:40 that's the most devastating answer you 02:41 could imagine yeah and I mean look I I I 02:45 think that there are some limits to the 02:47 value of various particular 02:49 recriminations given the broad scope of 02:51 the movement towards Trump so I want to 02:53 talk about that a little bit in a second 02:54 but but on this minor point of Biden I I 02:57 do 02:58 think I I think she was also worried 03:01 about him right I I think that he got 03:03 out but he got out late and it also to 03:05 me felt like that there was still an 03:07 error of sensitivity around all of it 03:10 that like you know people were walking 03:12 around eggshells they didn't want to 03:13 hurt his feelings like rather than what 03:15 should have happened was just him saying 03:17 I'm passing the torch you do what you 03:19 need to do to win I didn't feel like 03:20 there was a sense of you need to do you 03:22 do what you need to do to win and I 03:24 think she was torn a little bit by kind 03:26 of loyalty and worrying about that 03:30 I bet you when the answer is going to be 03:32 and the true answer is going to be 03:35 president people like him he has a lot 03:38 of friend a lot of people really like 03:39 him all right she being one and look he 03:42 gave me my big shot in politics and I'm 03:46 a loyal person and I just I just 03:48 couldn't I just couldn't do it I was 03:50 told to do it and I think she was and I 03:53 just couldn't bring myself and you know 03:56 that's why Winning is Everything winning 03:59 if when you have to win is more 04:00 important than loyalty it is everything 04:04 if you don't win you have done 04:06 nothing and we're unfortunately we we 04:10 finding out out the hard way but I am 04:13 sure that when we told what happen on 04:16 the view is going to be well they had 04:17 given me the answer but I just couldn't 04:20 get the words out of my 04:22 mouth I it that's a very human thing but 04:25 when you're running for 04:27 president you're not allowed to have 04:31 human reactions to the most fundamental 04:34 question in the election and that is how 04:36 are you going to be different than what 04:38 you 04:39 got and we FL the hardest thing about 04:42 this for me James is that like I don't 04:45 know maybe there's some elections where 04:47 winning is not everything maybe there 04:48 are times in life where you know doing 04:51 the moral thing is important or 04:52 maintaining your integrity or saving 04:54 yourself for down the line but if there 04:56 was ever a time when winning was 04:58 everything was this 05:00 time you're right you know we know we've 05:03 been elections and you B Romney if it's 05:05 Obama if it's Bob no 05:07 Clinton H you know you you you you want 05:11 to win you know you think you have 05:13 better ideas here is it's just 05:17 fundamentally the Director of National 05:19 Intelligence has it's his View and 05:21 people in the whole intelligence that 05:23 he's being blackmail by Putin Jesus how 05:26 bad can you get right but but I don't 05:29 like the price of 05:30 eggs okay but we're going to pay dearly 05:35 for this I mean we're really going to 05:36 pay yeah oh oh I do want to get into the 05:39 that the price of exil and of this and 05:41 the inflation but and one other thing 05:42 just kind of about the campaign itself 05:44 looking back that I wanted to talk to 05:45 you about because the last time we were 05:47 on I said to you that one of my worries 05:50 was I just I just watched the 05:51 documentary for the for the first time 05:54 and you know it's showing the old clip 05:56 of the Whiteboard you know it's the 05:57 economy is stupid what about health 06:00 you know you your three points on the 06:02 first thing was change versus more the 06:04 same change vers more we were the more 06:06 the same candidate right we St out a 06:10 position to be the more the same 06:12 candidate yeah and I said to you in that 06:15 podcast I was like what are her three 06:16 things right if yours was Changers more 06:18 the same it's the economy stupid and 06:19 don't forget Healthcare what were her 06:21 three and I just she ran like like the 06:25 big moments were good the set pieces 06:27 were good the speeches were good the 06:29 debates were good so I don't want to 06:30 come off as like overly nitpicky but 06:32 like at the narrative 06:34 level what were the three things that 06:37 was something that was sticking in my 06:39 brain the whole time as a worry that I 06:41 don't know that regular people 06:43 knew well okay we did not we did not 06:47 make we doubl down on more of the same 06:50 yeah all right and then the second thing 06:52 was it actually said the economy stupid 06:55 but want say it's economy stupid that's 06:57 fine and we tried to convinced people 07:01 that it was and it probably was better 07:03 than it was given credit for but that's 07:05 not a case you're going to be able to 07:06 make in 07:07 a in a 07:08 month so we we we in healthcare didn't 07:13 figure into it but on the the two basic 07:16 fundamentals we flued it yeah and and 07:20 people go to all and by the way she had 07:24 every Advantage with a United party 07:27 from dick sheny to AOC everybody was 07:31 whatever you want to do is fine we had 07:34 more people on the ground we had more 07:37 volunteers we had more money all right 07:40 we had more surrogates but we didn't 07:43 have a reason in since the dawn of time 07:47 since the first Greek stood in the 07:50 middle of Athens and said this election 07:52 is a choice all right and we have had 07:56 every piece of technology and you can 07:58 imagine 08:00 presses you know radio TV 08:04 computer AI algorithms it's still a 08:09 reason is the most powerful motivating 08:12 thing in all of 08:14 politics and you cannot not have a 08:17 reason and beat it with technology or 08:19 beat it with 08:20 volunteers or or do that and and that 08:23 that's the the overall message here Tim 08:27 a a reason and she didn't give us a 08:30 reason I know I know I mean she I guess 08:33 the In fairness the reason I I think 08:35 what they would say is you know Freedom 08:38 right like we we're turning the page um 08:41 but it was it was a little surface level 08:45 it was yes I mean I'm sure to go to say 08:47 well we had a we had a housing plan and 08:50 that you did great okay the housing plan 08:52 is number 08:54 one I I'm just saying she 08:57 never and if we would have 08:59 had this process would have had gone 09:02 through it and we'd have had this this 09:04 Mega level of talent that 09:07 exists and all of these people would 09:10 have been different they would have been 09:12 energetic it it would have been it would 09:15 have created a sense of real excitement 09:18 yeah 09:19 Biden he just blocked all that from 09:22 happening I'm sorry then he goes on to 09:24 after the election he says you know 09:27 we're going to be fine I mean this is 09:28 Kevin Bacon all is well calm down all is 09:31 well just stay in place okay don't 09:35 worry yeah it's all 09:39 control don't you love that scene I do 09:42 animal house and that's that's right I 09:45 that's what we're we we have the animal 09:47 White House coming and yeah I know I 09:49 didn't like it either I I mean I know 09:51 you feel like you want to be the 09:52 president and you want to follow the 09:54 rules but like there are legitimate 09:56 things to be worried about you know and 09:58 like I just think sanding down the the 10:01 danger um is not really is not really 10:04 helpful um the other thing I really want 10:07 to talk to you about I mean thinking 10:08 about both the movie and your life you 10:12 know look you come from regular folk you 10:14 know and um and the 10:17 Democrats like the main takeaway from 10:19 the election if you get out of the 10:20 political strategist stuff and 10:22 nitpicking messaging and speeches and 10:25 whatever tactics like the Thematic thing 10:28 is like the Democrats just did worse 10:31 across every demographic group uh that 10:36 is that is not is working class does 10:39 non- college black Hispanic Asian Native 10:42 American white everybody like if you did 10:46 not if you did not have a college degree 10:47 and are upper middle class that you the 10:51 Democrats you know lost ground with you 10:55 that is that seems to me like a like 10:56 just a massive problem and I just 10:59 a kind of open-ended question like is it 11:01 fixable what do you do to fix it like 11:03 what do you what do you how how how did 11:05 the party get here so one thing I'm no 11:07 longer criticized for and people have 11:09 said wa you know what in the end of the 11:11 day you're right James when I Ed the 11:13 term quote preachy females unquote oh 11:16 boy that meant Coastal overeducated 11:20 lecturing uh but the the other thing 11:24 that 11:25 happened in my new name for it I call it 11:28 identitarianism but most people refer to 11:31 it as wokeness yeah we couldn't wash the 11:34 stench off of 11:35 us we we I mean she didn't embrace it to 11:40 be fair to her she never used any of 11:42 that language wals didn't use it but 11:45 people were still remembering that in on 11:49 Election Day there was a prosecutor in 11:51 Alam County which you're a resident of I 11:54 think or used to for sure yeah used to 11:57 used to be was which is they Rec called 12:00 her by 30 12:01 points yeah because she was an advocate 12:04 of 12:04 identitarianism which is not even 12:06 popular in Berkeley yeah right okay I 12:10 mean that's how that's how unpopular no 12:12 San Francisco San Francisco got rid of 12:14 their mayor got rid of the most 12:16 obnoxious city council member Oakland 12:19 got rid of their mayor and still 12:23 dealing although she didn't no one talks 12:26 like that anymore it's it the stench of 12:31 it it's like I tell people you know you 12:33 clothes you get fireplace and you get 12:36 clothes smell Smoky somebody smoking a 12:38 cigar in the room or you smoke a cigar 12:40 and then you put your shirt on the next 12:41 morning God damn thing stinks and you 12:43 got to wash it twice that that's what 12:46 that's what they did to us they they 12:48 they were cigar smok in our clothes and 12:50 we couldn't get the smell off it it was 12:53 bad and that the it and people keep 12:56 telling me today well I don't like the f 12:59 I don't like the left what left to the 13:01 Democratic or I don't like all that you 13:03 know oh [ __ ] but no one we did it was a 13:07 mistake and you know I go back and I did 13:10 that box interview in the spring of 13:13 2021 it was just evident that this was a 13:17 really dumb backwards looking NPR 13:22 idiotic 13:23 move and of course we all came to 13:26 realize that but it it was too late what 13:28 did they do do I mean how do you get 13:30 working people back assuming what we do 13:34 so Crystal's peace is is the rallying 13:36 point all right we're all in an 13:39 opposition party understand that 13:42 understand what an opposition party is 13:45 opposition Party by Nature has no 13:48 power so hakeim oruma you know maybe 13:51 they can get a couple of things that 13:53 that they need but they're not going to 13:55 get anything so we have to start acting 13:57 like an opposition party this is the 14:00 most radical and don't laugh when I tell 14:02 you this idea okay no promises so you 14:04 know how private Equity looks to some 14:07 existing place that has a how a 14:10 distribution network but is a kind of 14:12 fail company they're going to buy we 14:14 ought to take over the 14:16 DNC they have a charter they have a 14:19 building they have tax exempt status 14:22 they can put people and and put 14:25 communicators in there and put people 14:28 there and that know how to be part of 14:30 the 14:31 opposition and have a research side and 14:34 crank out the distribution tables on 14:37 what and settle on a narrative about 14:40 Trump and the narrative that I think 14:43 should be about Trump is 14:45 betrayal that he betrayed you look you 14:49 off for change he said he was for for 14:51 the middle class he said he was for the 14:53 working people he said and then and as 14:55 soon as they do this you know what 14:57 they're going to do you already know 14:59 they got to write to tax code for laon 15:01 musk and pet a theal all right get ready 15:04 for it you know it's coming it's just 15:06 you know it and they're gonna put every 15:09 it's not coming James it's not coming 15:11 James it's already here I'm just pulling 15:13 this up here axus this morning we're 15:15 taping this on Friday axus this morning 15:17 big Winners to watch oil and gas crypto 15:21 and Tech firms musks companies and banks 15:24 are all poised to benefit from Trump's 15:27 agenda that's those the Forgotten men 15:31 yeah I I would like to ask the the kind 15:33 of uh Latino male that voted for Trump 15:38 was this what you were voting for do do 15:41 you think that the banks and all 15:42 companies and and musk and do you think 15:45 they lack 15:46 power well yeah I didn't like you know I 15:50 I I like to okay fine and we have to 15:53 tell them that 15:55 in you know people say I'm going my kid 15:58 is going I don't know UCLA and they want 16:00 to B in politics what should they study 16:02 they should study the history of the 16:04 Bible because every message in politics 16:07 comes right out of the Bible and the 16:10 whole Trump people he's King Cyrus he's 16:12 the strong God be no he's he betrays you 16:15 and betrayal is a big is a big part of 16:17 the Bible and as opposed to telling 16:21 people that they were wrong which is a 16:23 hard thing to do you well you thought he 16:25 was going to do this and look what he 16:26 did now you know and that's that's the 16:29 only way that's the way we come back do 16:31 the Democrats have I know you said they 16:33 got a bunch of 350 hitters across and I 16:36 think that that's true especially for 16:38 the midterm electorate which is which is 16:40 much more college educated you know uh 16:44 uh the type of elector of people that 16:45 show up in offe elections are are much 16:47 more democratic friendly and so I think 16:49 that they've got a lot of strong bench 16:52 for midterms do they have a good do they 16:54 have a strong bench for talking to the 16:56 Working Class People that the part's B 16:59 so absolutely they do absolutely I I I 17:03 mean you never see Basher Spiro or West 17:06 Moore or warau or I don't I I hate just 17:10 giving names or what yes sure but do 17:13 that you say somebody so what what they 17:15 should do and this is kind of weird we 17:17 should have we did it back in the 80s we 17:20 should have a mini convention in the 17:22 summer of 17:24 2026 and we should invite top tier 17:27 candidates to speak to convention and 17:29 maybe have like a straw pole you have to 17:32 create you have to show people you have 17:33 all of this Talent okay we got to play 17:35 an exhibition game oh we have to have 17:37 Home Run Derby and let these guys come 17:39 up and just 17:41 Crush you know batting practice pitches 17:43 and hit them 400 feet and let people see 17:46 that I mean let people see that there 17:49 there's help out there there's hope out 17:50 there that that we kept all of these 17:53 people Mitch you know I if they would 17:57 never do this 17:59 I he probably wouldn't take it because 18:00 it's thought to be a dead-end job 18:02 although in the right set of 18:04 circumstances it doesn't have to be I 18:06 would make Mitch the chairman of the DNC 18:08 I'd have him as the you know going on TV 18:12 and and putting Paul Bala out there on 18:14 Sunday morning shows as a communications 18:17 director or something I mean somebody 18:19 that knows how to deliver a message 18:20 knows how to frame an argument knows how 18:22 to do all the stuff that we we weren't 18:26 able to do in this cycle but there 18:28 Talent out that dude we just we got to 18:30 get we got to we got to have a home run 18:32 derby let them all speak to a mini fake 18:35 mini convention and the sum of 2026 18:39 there's just so much Mary in the film I 18:41 have to ask what she think about all 18:44 this well I you know she's coming up 18:48 tomorrow but I suspect she's pretty 18:50 happy but the the really interesting 18:53 backstory of the movie is is the 18:55 director said there's two people I have 18:58 to have J 18:59 I have to have Bill Clinton and I have 19:01 to have Mary to make this work and I 19:03 said I can get you Bill Clinton but I 19:06 can't get you Mary okay I that so he 19:10 takes her to dinner at uh Cafe Milano 19:15 and he says he's like on a and he's 19:18 trying to persuade her and she's pretty 19:21 and the actual the the the hero of the 19:24 whole movie is go of a 19:26 doll because Matt the director used to 19:30 work with gval at V and Mary had read 19:33 all of gal's books and 19:35 gordal read a really good review of mar 19:38 and I wrote a good review of mar I's 19:40 book in uh the New York Review of Books 19:43 oh which kind of set the T you know like 19:45 G all in yor VI book that who who's G to 19:50 so that was the common thing so he gets 19:52 her to say yes instead out in the shanor 19:55 valley and that just he he gets a talk 19:58 for 6 hours but the cameraman didn't 20:01 have a tripod and the guy is sitting 20:05 there dying you know holding the camera 20:08 up and he's like you know we we we got 20:10 all of this and she was spectacular in 20:13 it I mean just spectacular she looked 20:16 great I mean she was very honest 20:18 sometimes to to my 20:20 detriment but yeah you know and I think 20:23 it was good for her to get you know some 20:25 of the stuff out but she she she she the 20:28 movie would not beat a movie without her 20:30 without her it would have been a pretty 20:31 good movie with her it's way better than 20:34 a a very good movie and I mean I know 20:37 it's a good movie and not being arrogant 20:40 about it it's just it's just it's a good 20:42 director it's well put together it's 20:43 well lit it's well edited it's carries a 20:46 narrative I mean the whole thing but but 20:48 without Mary it doesn't it doesn't work 20:51 like this at all I agree all right what 20:55 um what are your fears what are you 20:57 worried about what are your feelings 21:01 everything I don't know about you I mean 21:03 but this thought with 21:05 Ukraine I start with like I don't know 21:09 somebody I'm GNA have to sit down with a 21:12 a person to explain to me to Palestinian 21:15 mind that they that that in effect 21:19 turned over us Middle East policy to 21:21 Donald Trump and BB net Yahoo I don't 21:23 know how they could possibly think this 21:25 was a good idea but they do uh that that 21:28 would be the beginning so that the three 21:31 this is November the 8th I guess today 21:35 the yeah the eth there are three systems 21:39 in the Atlantic Basin right now there's 21:42 a category three in the Gulf of Mexico 21:45 fortunately it's going to go west it'll 21:47 do it'll be by the time it gets to 21:49 Mexico it it will not but you have a 21:52 category three on nove and maybe this is 21:55 real but that's just one thing to think 21:57 about change 21:59 yeah right and of course the the 22:02 corruption that we're getting ready to 22:04 go through and the consequences of it 22:06 are going to be just Draconian for the 22:08 country and I know this and the only 22:11 thing that we can do is we have to be 22:14 all part of the 22:16 opposition and we have to oppose oppose 22:20 oppose they got all the power in the 22:22 world and they're going to run right 22:25 over all of the people that voted for 22:27 them they're going to run right over up 22:28 all of the other people and all we can 22:31 do is like rally rally to opposition 22:35 hold them off get to 2028 so yeah James 22:38 uh well I didn't want to be in the 22:39 opposition with you but I'm happy to be 22:41 in the opposition with you 22:43 no we have no 22:45 choice so Bello and he said if there was 22:49 some way for me to run I would have run 22:51 I didn't but I didn't know where to go 22:53 so I just had to shoot 22:57 everybody well we're going to do the 22:59 best we can and we'll keep the combo 23:01 flowing thanks for thanks for always 23:02 hanging with us and I appreciate you 23:04 know your efforts go talk go Tigers 23:08 thanks to James Carville go check out on 23:10 Max November 14th his movie Winning is 23:12 Everything stupid we'll see y all soon 23:14 peace 23:15 [Music] Birds of a Feather Flock Conjointly (?): Rhyme as Reason in Aphorisms Author: McGlone, Matthew S., Date: 2000-09-01 Collections: MediaAdsPolit Zotero Key: XZA5FMUP Cite Key: McGlone00BirdsFeatherFlock Zotero Item | Lit Note Research Report BIRDS OF A FEATHER FLOCK CONJOINTLY (?): Rhyme as Reason in Aphorisms Matthew S. McGlone and Jessica Tofighbakhsh Lafayette College Abstract—We explored the role that poetic form can play in people’s perceptions of the accuracy of aphorisms as descriptions of human behavior. Participants judged the ostensible accuracy of unfamiliar aphorisms presented in their textually surviving form or a semanti- cally equivalent modified form. Extant rhyming aphorisms in their original form (e.g., “What sobriety conceals, alcohol reveals”) were judged to be more accurate than modified versions that did not pre- serve rhyme (“What sobriety conceals, alcohol unmasks”). However, the perceived truth advantage of rhyming aphorisms over their modi- fied forms was attenuated when people were cautioned to distinguish aphorisms’ poetic qualities from their semantic content. Our results suggest that rhyme, like repetition, affords statements an enhancement in processing fluency that can be misattributed to heightened convic- tion about their truthfulness. Aphorisms are succinct statements that offer observations and advice about universal human concerns such as happiness (e.g., “Better to be happy than wise”), health (“An apple a day keeps the doctor away”), and friendship (“Birds of a feather flock together”). Although they enjoy a reputation among laypeople as distillations of age-old psychological wisdom, aphorisms are commonly characterized by psychologists as dubious generalizations to be contrasted with more precise scientific descriptions of human behavior (Gibbs & Beitel, 1995; Teigen, 1986). To this end, many introductory psychology textbooks draw attention to the existence of apparently contradictory pairs of aphorisms such as “Birds of a feather flock together” versus “Opposites attract” and “Out of sight, out of mind” versus “Absence makes the heart grow fonder” (Baron, Byrne, & Kantowitz, 1980; Taylor & Manning, 1975). Presumably, a set of statements about behavior can be considered scientifically “accurate” only if each statement’s truth conditions can be operationalized and the statements do not contradict each other (Dowty, Wall, & Peters, 1981). However, the notorious vagueness of aphorisms makes specification of their truth conditions especially difficult. For example, what conditions must be satisfied for the statement “Haste makes waste” to be true? Which forms of urgent action constitute “haste”? How does one distinguish a priori between a situation in which “Haste makes waste” is good advice and one in which “He who hesitates is lost” would be more appropriate? If the persuasive force of an aphorism depended critically on the clarity of its truth conditions, we should find it surprising that people invest any belief in such statements. Given the murkiness of aphorisms’ truth conditions, why might people believe that such statements describe human behavior accurately? One important factor is an aphorism’s familiarity (Higbee & Millard, 1983). For example, consider the well-worn observation that “opposites attract.” American college students not only are highly Address correspondence to Matthew S. McGlone, Department of Psychology, Lafayette College, Easton, PA 18042-1781; e-mail: mcglonem@lafayette. edu. familiar with this aphorism, but also judged it to be a more accurate description of companion selection than novel statements that entail the same claim (e.g., “People with divergent interests and personalities tend to be drawn to one another”; McGlone & Necker, 1998). The conflation of familiarity and accuracy in aphorisms is consistent with experimental demonstrations of the influence of repetition on people’s judgments of statements with uncertain truth value (Bacon, 1979; Begg, Anas, & Farinacci, 1992; Begg & Armour, 1991; Hasher, Goldstein, & Toppino, 1977). For example, Hasher et al. (1977) found that repetition of unsubstantiated trivia statements (e.g., “Divorce is found only in technically advanced societies”) produced a systematic shift in their rated truth value: Repeated statements were judged as more likely to be true than nonrepeated statements. Begg et al. (1992) characterized this and other demonstrations of the “illusory truth” effect as evidence that a fluency heuristic operates in people’s judgment of a statement’s truth; repetition increases a statement’s familiarity, and the processing fluency that familiarity affords the statement is misattributed to belief in its propositional truth (cf. Whittlesea, 1993). Although fluency of this sort might contribute to people’s belief in conventional aphorisms such as “Haste makes waste,” it does not encourage belief in unfamiliar aphorisms such as “Variety prevents satiety.” Yet unfamiliar aphorisms often seem to have a “ring of truth” as well, their dubious truth conditions notwithstanding. What characteristics of unfamiliar aphorisms might contribute to this perception? The present study focuses on the role that aesthetic properties of an aphorism can play in people’s (specifically, readers’) perceptions of its truthfulness. Although these statements’ reputation as kernels of psychological wisdom may be dubious, their reputation as verbal art forms is well deserved. Aphorisms employ many of the aesthetic devices exalted in poetry, including metaphor (e.g., “Oppression is the mother of liberty”), alliteration (“Fortune favors the fool”), assonance (“A rolling stone gathers no moss”), and rhyme (“Haste makes waste”; Gibbs & Beitel, 1995; Odlin, 1986). Traditionally, literary scholars have classified these devices as aspects of aphoristic form that are separate from propositional content (e.g., Goodwin & Wenzel, 1979). In his seminal work on structuralist poetics, Culler (1975) acknowledged this distinction by suggesting that the rhetorical effectiveness of an aphorism depends on the “observable accuracy of its meaning” (i.e., content) and “the aesthetic pleasure afforded by its form” (p. 143). Although the distinction between content and form clearly has analytic value, it has not been established that readers routinely separate the contributions that these components make to their overall appreciation of an aphorism (McGlone & Tofighbakhsh, 1999). For example, consider how readers might respond differently to “Variety prevents satiety” and a slightly modified version of this statement, “Variation prevents satiety.” The two statements do not differ appreciably in propositional content, but the former has an aesthetic element (i.e., repetition of the stressed vowel and subsequent speech sounds in two or more words, or rhyme; Brogan, 1994) that the latter does not. If readers distinguish between the propositional content of the aphorism and its poetic form, then there should be no difference in the perceived accuracy of the rhyming and nonrhyming versions. However, rhyme, like familiarity, can increase the fluency with which words forming a statement are recognized and understood (Meyer, Schvaneveldt, & Ruddy, 1975; Rubin, 1995). For example, Meyer et al. (1975) found that people are faster to judge that a string of letters presented visually is a word if it is preceded by a rhyming prime word than if it is preceded by a nonrhyming prime. This effect is observed even when the prime word is presented auditorily, suggesting that lexical activation (as opposed to simple graphemic correspondence) is the locus of facilitation (Hillinger, 1980). If, as Begg et al. (1992) have argued, people base their judgments of a statement’s accuracy in part on the fluency with which the statement is processed, then the fluency that rhyme affords an aphorism may confer upon the statement a perceived truth advantage over a semantically equivalent nonrhyming version. Such an advantage would indicate that the traditional analytic distinction between a statement’s “rhyme and reason” (i.e., form and content) is not always appreciated by readers; in some circumstances, rhyme may be treated as reason. In this article, we report an experiment exploring the influence of poetic form on people’s perceptions of aphorisms’ accuracy as descriptions of human behavior. This experiment tested two distinct hypotheses. First, we hypothesized that people would misattribute the processing fluency produced by an aphorism’s rhyming form to heightened conviction about the statement’s accuracy (following Begg et al., 1992), relative to a semantically equivalent nonrhyming version of the aphorism. Second, we predicted that this misattribution would be attenuated when people were prompted to attribute processing fluency to its actual source (cf. Schwarz & Clore, 1996; Whittlesea, Jacoby, & Girard, 1990). Specifically, we expected that people advised to distinguish aphorisms’ poetic qualities from their propositional content would be less prone to exhibit the “rhyme as reason” effect. Matthew S. McGlone and Jessica Tofighbakhsh most difficult weapon to overcome”).[1] The extant nonrhyming aphorisms and their modified counterparts were included in the stimulus materials to control for the possibility that a perceived truth advantage for an extant rhyming aphorism over a nonrhyming version might be attributable not to rhyme per se, but rather to modification of the statement’s textually surviving form. We expected that if the latter were true, we would observe a truth advantage for the extant nonrhyming aphorisms over their modified counterparts as well. On the basis of a pilot experiment (n � 20), we chose 30 pairs (original plus modified version) from each of the rhyming and nonrhyming aphorism sets. A pair was selected only if all participants indicated that (a) they could not recall having read or heard the original aphorism in the past and (b) they did not perceive a difference in meaning between the original and modified versions. Examples of the selected pairs are presented in Table 1. Two lists were created from these materials. Each list contained 60 aphorisms: 15 extant rhyming aphorisms in their original form and 15 in modified (i.e., nonrhyming) form, and 15 extant nonrhyming aphorisms in their original form and 15 in modified form. Only one version of each extant aphorism appeared in each list. Although the order in which the aphorisms appeared was randomized, a given aphorism and its modified form were in the same position in their respective lists. METHOD Participants For participating in this experiment, 120 Lafayette undergraduates received extra credit in a course they were taking. Twenty participated in the materials check and 100 in the experiment proper. All were native English speakers. Materials Initially, 50 rhyming and 50 nonrhyming aphorisms were selected from published aphorism collections using the following criteria: (a) The aphorism was an advisory or descriptive statement about human behavior (as opposed to a value judgment or opinion, which people might be hesitant to judge as accurate or inaccurate); (b) it was not similar in meaning to another selected aphorism; and (c) it was unfamiliar to the authors. For each extant (i.e., textually surviving) rhyming aphorism (e.g., “What sobriety conceals, alcohol reveals”), a modified version was created by replacing one of the rhyming words with a close synonym that did not rhyme with any of the other words in the statement (e.g., “What sobriety conceals, alcohol unmasks”). For each extant nonrhyming aphorism (e.g., “Benefaction is the most difficult weapon to conquer”), a modified version was created by replacing a content word with a close synonym that did not rhyme with any of the other words in the statement (e.g., “Benefaction is the Design and Procedure This experiment employed a 2 × 2 × 2 design with aphorism type (extant rhyming or nonrhyming) and version (original or modified) as within-subjects factors and instruction condition (control or warning) as a between-subjects factor. Upon arrival in the laboratory, participants were randomly assigned to one of the aphorism lists and an instruction condition. The first page of each questionnaire indicated that the experiment was part of a larger study exploring the psychological theories implied by English aphorisms and provided instructions for the accuracy ratings. Participants were instructed to read each aphorism carefully and then to rate the degree to which they perceived the aphorism as “an accurate description of human behavior,” on a scale from 1 (not at all accurate) to 9 (very accurate). In the controlinstructions condition, the instructions did not include any mention of or admonition concerning the distinction between aphorisms’ poetic qualities and their ostensible accuracy. In contrast, instructions in the warning condition specifically cautioned participants to base their accuracy judgments “only on the claim that the statement makes about behavior, not the poetic quality of the statement’s wording.” This caveat was presented in boldface type in the instructions, and was further emphasized by the observation that “a statement might strike you as quite poetic, but not particularly accurate; on the other hand, a statement might strike you as quite accurate, but not particularly poetic.” After participants in both conditions completed the accuracy ratings, they were asked the following yes/no question: “In your opinion, do aphorisms that rhyme describe human behavior more accurately than those that do not rhyme?” After responses to this question were 1. Although it would have been ideal to construct modified rhyming versions of the extant nonrhyming aphorisms (thus putting assignment of statements to the rhyming and nonrhyming conditions under the experimenter’s control), it was impossible in most cases for us to create such versions that adequately preserved the meaning of the originals. Rhyme as Reason Table 1. Examples of the aphorism pairs Original version Modified version Extant rhyming aphorisms Woes unite foes. Woes unite enemies. What sobriety conceals, alcohol reveals. What sobriety conceals, alcohol unmasks. Life is mostly strife. Life is mostly struggle. Caution and measure will win you treasure. Caution and measure will win you riches. Variety prevents satiety. Variation prevents satiety. Extant nonrhyming aphorisms Fools live poor to die rich. Fools live poor to die wealthy. Power grows mightier with each trial. Power grows mightier with each challenge. Short pleasure, long repentance. Short pleasure, long regret. He who rides a tiger is afraid to dismount. He who rides a tiger is afraid to get off. Good intentions excuse ill deeds. Good intentions excuse ill acts. recorded, participants were debriefed regarding the true purpose of the experiment. On average, the experimental sessions lasted 25 min. RESULTS Initial analyses did not reveal main effects or interactions involving stimulus list, so subsequent analyses collapsed across this factor. Separate analyses of variance were conducted on the ratings data treating participants (Fp) and items (Fi) as random factors. Analyses of the accuracy ratings indicated that, overall, there were no reliable differences in mean ratings between extant rhyming and nonrhyming aphorisms (5.51 and 5.45, respectively) or original and modified versions (5.63 and 5.35), p > .10 in both cases. However, participants in the control-instructions condition generated slightly higher ratings overall than those in the warning condition (5.68 and 5.26), Fp(1, 98) � 2.79, p < .08, and Fi(1, 58) � 2.63, p < .12. This marginal effect was moderated by a reliable Aphorism Type × Version × Instruction Condition interaction, Fp(1, 98) � 7.84, p < .01, and Fi(1, 58) � 5.58, p < .02. The relevant means are presented in Figure 1. Planned analytical comparisons (Keppel, Saufley, & Tokunaga, 1992) were used to investigate differences among the means. As we predicted, participants who were not cautioned to distinguish aphorisms’ semantic content from their poetic qualities (the control-instructions condition) assigned higher accuracy ratings to the original rhyming aphorisms than their modified counterparts (6.17 and 5.26), Fp(1, 98) � 12.77, p < .01, and Fi(1, 58) � 8.62, p < .03; however, they assigned comparable ratings to the original and modified nonrhyming aphorisms (5.79 and 5.51), Fp(1, 98) � 1.21, p > .10, and Fi(1, 58) � 0.82. The fact that the difference in ratings between aphorism versions was reliable for the extant rhyming aphorisms, but not the nonrhyming aphorisms, suggests that the difference is attributable specifically to manipulation of rhyme in the former and not simply to the modification of their textually surviving form.[2] Participants in the warning condition exhibited a markedly different pattern of accuracy ratings. The original rhyming aphorisms were assigned reliably lower accuracy ratings in this condition than in the control condition (5.42 and 6.17), Fp(1, 98) � 4.79, p < .05, and Fi(1, 58) � 4.26, p < .05. In addition, in the warning condition, there were no reliable differences in participants’ ratings for original and modified versions of the extant rhyming aphorisms (5.42 and 5.17), Fp(1, 98) � 0.96, and Fi(1, 58) � 0.65, or of the extant nonrhyming aphorisms (5.14 and 5.36), Fp(1, 98) � 0.75, and Fi(1, 58) � 0.52. Thus, bringing the distinction between an aphorism’s poetic qualities and semantic content to participants’ attention had the desired effect of thwarting their tendency to conflate fluency with perceived accuracy. However, there is no evidence that this tendency in the control condition stemmed from an explicit belief that rhyming aphorisms are more accurate than nonrhyming ones. When asked if they held such a belief, all participants in both conditions responded “no” (and many gave us quizzical looks). DISCUSSION In The Gay Science, Nietzsche (1878/1986) attributed the origin of poetry to a primitive belief that rhythm and rhyme could confer magical powers to the words of prayers, carrying them “closer to the ears of the gods.” Although this superstition was dismissed long ago in most cultures, Nietzsche observed that “even now . . . the wisest among us are still occasionally fooled by rhythm—if only insofar as we sometimes consider an idea truer because it has a metrical form and presents itself with a divine spark and jump” (pp. 139–140, em- phasis added). Our results offer some support for Nietzsche’s claim: Participants conflated the rhyme and perceived accuracy of aphorisms unless they were explicitly instructed to distinguish the statements’ semantic content from their poetic qualities. This occurred despite the fact that participants did not read the aphorisms aloud, which would 2. A second possibility is that the perceived truth advantage was attributable in part to modification of the rhyming aphorisms’ meter. It proved diffi cult in many cases for us to create modified versions of these aphorisms in which rhyming words were replaced with synonyms that had the same number of syllables and stress pattern as the replaced words, thereby preserving both the meanings and the meter of the statements; however, we were able to do this for 14 of the 30 rhyming aphorisms used (see examples in Table 1). We found no differences in the pattern of accuracy ratings for these aphorism pairs and those in which the modified version preserved only the meaning of the original. Thus, we tentatively conclude that the meter confound did not contribute appreciably to the effect reported. Matthew S. McGlone and Jessica Tofighbakhsh Fig. 1. Mean accuracy ratings by aphorism type, version, and instruction condition. have made those that rhymed especially salient. Among the prosodic poetic devices (e.g., alliteration, assonance), rhyme is the first that children learn to appreciate and one that adults routinely notice even during silent reading (Hayes, Chemelski, & Palmer, 1982). Thus, it is not surprising that participants discriminated between the rhyming and nonrhyming aphorisms in the stimulus lists. What is surprising is that they discriminated between these forms in terms of accuracy, even though none of them reported believing that rhyme confers a truth advantage on such statements. Psychologists have documented that people are often unaware of factors that influence their judgments (e.g., Nisbett & Wilson, 1977). However, barring an unconscious belief in the magical power of rhyme on the part of our participants, what accounts for the rhyme-as-reason effect? We suggest that this effect is a product of the enhanced processing fluency that rhyme affords an aphorism such as “What sobriety conceals, alcohol reveals” relative to a semantically equivalent nonrhyming version. Although enhanced processing fluency is often the consequence of repeated exposure to a stimulus (Begg et al., 1992; Jacoby & Kelley, 1987), it can also be produced by factors in the present stimulus environment. For example, manipulations of fluency such as adjusting the figure-ground contrast or presentation duration of a stimulus produce misattributions akin to those generated by repetition manipulations (Reber, Winkielman, & Schwarz, 1998; Whittlesea, 1993). When these manipulated factors are brought to people’s attention, misattribution of processing fluency to other psychological dimensions (e.g., liking, familiarity) is attenuated (Whittlesea et al., 1990). In the same fashion, our participants misattributed processing fluency to a perceived truth advantage of rhyming aphorisms over nonrhyming versions; however, when they were cautioned to distinguish an aphorism’s poetic form from its semantic content, the advantage was significantly reduced. Although we have explored the rhyme-as-reason effect within the narrow domain of antiquated sayings, it clearly can occur in contemporary communications as well. Consider defense attorney Johnnie Cochran’s celebrated plea to the jury during O.J. Simpson’s criminal trial: “If the gloves don’t fit, you must acquit!” Journalists have focused almost exclusively on the mnemonic value of rhyme in this statement: Rhyme increased the likelihood that jurors would rehearse, remember, and thus apply Cochran’s directive (Buckley, 1997). However, the fluent quality of the statement undeniably overshadows its dubious proposition—after all, the jury was obligated to consider all of the presented evidence, not just the tight gloves! We wonder how persuasive the jury might have found this proposition had Cochran proclaimed, “If the gloves don’t fit, you must find him not guilty!” Acknowledgments—A preliminary report of this research was presented at the annual convention of the American Psychological Society in Washington, D.C., June 1998. We would like to thank Dana Morris for her assistance in data collection. Thanks also go to Dale Barr, Richard Dickson, Sam Glucksberg, Rachel Herz, Tory Higgins, Sid Horton, and Phil Johnson-Laird for their valuable suggestions and comments. Rhyme as Reason REFERENCES Bacon, F.T. (1979). Credibility of repeated statements: Memory for trivia. Journal of Experimental Psychology: Human Learning and Memory, 5, 241–252. Baron, R.A., Byrne, D., & Kantowitz, B.H. (1980). Psychology: Understanding behavior. New York: Holt, Rinehart, & Winston. Begg, I.M., Anas, A., & Farinacci, S. (1992). Dissociation of processes in belief: Source recollection, statement familiarity, and the illusion of truth. Journal of Experimental Psychology: General, 121, 446–458. Begg, I.M., & Armour, V. (1991). Repetition and the ring of truth. Journal of Experi_mental Psychology: General, 121, 446–458._ Brogan, T.V.F. (1994). The new Princeton handbook of poetic terms. Princeton, NJ: Princeton University Press. Buckley, C. (1997). Wry martinis. New York: Harper Perennial. Culler, J. (1975). Structuralist poetics. Ithaca, NY: Cornell University Press. Dowty, D.R., Wall, R.E., & Peters, S. (1981). Introduction to Montague semantics. Boston: Reidel. Gibbs, R.W., & Beitel, D. (1995). What proverb understanding reveals about how people think. Psychological Bulletin, 118, 133–154. Goodwin, P.D., & Wenzel, J.W. (1979). Proverbs and practical reasoning: A study in socio-logic. Quarterly Journal of Speech, 65, 289–302. Hasher, L., Goldstein, D., & Toppino, T. (1977). Frequency and the conference of referential validity. Journal of Verbal Learning and Verbal Behavior, 16, 107–112. Hayes, D.S., Chemelski, B.E., & Palmer, M. (1982). Nursery rhymes and prose passages: Preschoolers’ liking and short-term retention of story events. Developmental Psy_chology, 96, 211–222._ Higbee, K.L., & Millard, R.J. (1983). Visual imagery and familiarity ratings for 203 sayings. American Journal of Psychology, 96, 211–222. Hillinger, M.L. (1980). Priming effects with phonemically similar words: The encodingbias hypothesis reconsidered. Memory & Cognition, 8, 115–123. Jacoby, L.L., & Kelley, C.M. (1987). Unconscious influences of memory of a prior event. Personality and Social Psychology Bulletin, 13, 314–336. Keppel, G., Saufley, W.H., & Tokunaga, H. (1992). Introduction to design and analysis (2nd ed.). New York: W.H. Freeman. McGlone, M.S., & Necker, R. (1998). The perils of paraphrase. Unpublished manuscript, Lafayette College, Easton, PA. McGlone, M.S., & Tofighbakhsh, J. (1999). The Keats heuristic: Rhyme as reason in aphorism interpretation. Poetics, 26, 235–244. Meyer, D.E., Schvaneveldt, R.W., & Ruddy, M.G. (1975). Loci of contextual effects on visual word recognition. In P.M. Rabbit & S. Dornic (Eds.), Attention and perfor_mance, Volume V (pp. 98–118). London: Academic Press._ Nietzsche, F. (1986). The gay science. New York: Mentor. (Original work published 1878) Nisbett, R.E., & Wilson, T.D. (1977). Telling more than we can know: Verbal reports of mental processes. Psychological Review, 84, 231–259. Odlin, T. (1986). Language universals and constraints on proverbial form. Proverbium, 3, 125–151. Reber, R., Winkielman, P., & Schwarz, N. (1998). Effects of perceptual fluency on affective judgments. Psychological Science, 9, 45–48. Rubin, D.C. (1995). Memory in oral traditions: The cognitive psychology of epic, ballads, and counting-out rhymes. Oxford, England: Oxford University Press. Schwarz, N., & Clore, G.L. (1996). Feelings and phenomenal experiences. In E.T. Higgins & A. Kruglanski (Eds.), Social psychology: Handbook of basic principles (pp. 433–465). New York: Guilford. Taylor, D.A., & Manning, S.A. (1975). Psychology: A new perspective. London: Van Nostrand. Teigen, K.H. (1986). Old truths or fresh insights? A study of students’ evaluations of proverbs. British Journal of Social Psychology, 25, 43–49. Whittlesea, B.W.A. (1993). Illusions of familiarity. Journal of Experimental Psychology: Learning, Memory, and Cognition, 6, 1235–1253. Whittlesea, B.W.A., Jacoby, L.L., & Girard, K. (1990). Illusions of immediate memory: Evidence of an attributional basis for feelings of familiarity and perceptual quality. Journal of Memory and Language, 29, 716–732. (RECEIVED 2/3/99; REVISION ACCEPTED 2/4/00) The Future of Political Discourse: AI and Media Literacy Education Author: Shalevska, Elena, Date: 2024 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: PJYHEY7M Cite Key: Shalevska24FuturePoliticalDiscourse Zotero Item | Lit Note Published online by the Institute for Research and European Studies at www.iies.mk/jlpe Copyright © 2024 The author/s This work is licensed under the CC BY 4.0 license () Corresponding author Peer review method: Double-blind Original scientific article DOI: https://www.doi.org/10.47305/JLPE2411050sh Received: 28.05.2024 • Accepted after revision: 06.07.2024 • Published: 12.07.2024 Pages: 50-61 The Future of Political Discourse: AI and Media Literacy Education Elena Shalevska[1][] 1  Faculty of Education - Bitola, University “St. Kliment Ohridski” - Bitola, North Macedonia elena.shalevska@uklo.edu.mk Abstract: The rise of Artificial Intelligence (AI) has significantly impacted the political landscape, introducing novel challenges alongside opportunities for all involved. However, one of the most significant impacts can be found in its ability to manipulate information and shape public opinion, which has led to numerous concerns about the integrity of political discourse. Recognizing this issue, this paper explores the challenges posed by AI-powered disinformation and misinformation in political discourse, focusing on deepfakes, microtargeting, and weaponized bots and how they manipulate public opinion. Using a qualitative approach, the paper analyzes existing media literacy handbooks to develop a comprehensive framework for enhancing media literacy education. Aligned with the study‟s objectives, this framework aims to equip students with the critical thinking skills necessary to navigate this „tangled web‟ and engage more effectively in democratic processes. The study argues that robust media literacy education is essential in mitigating the negative impacts of AI-powered disinformation and misinformation in political contexts. Keywords: Media Literacy; Artificial Intelligence; Disinformation; Misinformation; Political Discourse; Education INTRODUCTION A healthy democracy thrives on an informed citizenry engaged in constructive political discourse. This discourse is the foundation for policy decisions and a strong sense of civic duty (Ahmadov 2022). However, the rise of Artificial Intelligence (AI) has cast a long shadow over the integrity of political discourse. Malicious actors can now misuse generative AI models to generate deepfakes and other forms of synthetic media, deliberately manipulating information and swaying public opinion (Tiernan et al. 2023). As a sub-field of AI, generative AI - easily produces novel text, images, music, and software by analyzing enormous collections of digitized material (Kaplan 2024). Thus, generative AI can produce different types of content within seconds. Depending on the person prompting the generative AI model, this content can vary from harmless to malicious. This troubling trend may mean that we must radically reevaluate our approach to media literacy education, as AI has significantly simplified the process of creating misleading content. Recognizing this issue, this paper proposes a framework for media literacy education within the broader context of political education, specifically designed for the age of AI. This framework equips students with the critical thinking skills necessary to distinguish fact from fiction in an increasingly complex information landscape. By empowering students to be made more media literate, the future can be safeguarded, ensuring democracy based on informed political decisions. 50 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe LITERATURE REVIEW AI-Powered Deception: A New Era of Political Disinformation The digital age has produced a potent new weapon for political manipulation: AI. Generative AI models are on the rise, fundamentally altering the landscape of political discourse and creating a breeding ground for both disinformation and voter manipulation. Some of the most troubling instances include deepfakes and synthetic media, microtargeting, and weaponized bots. Deepfakes and Synthetic Media One of the most concerning advancements in the field of generative AI is the rise of deepfakes and synthetic media. According to OED (2023), deepfakes are “any of various media, esp. a video, that has been digitally manipulated to replace one person‟s likeness convincingly with that of another, often used maliciously to show someone doing something that he or she did not do.” Deepfakes are a subset of synthetic media which “is an all-encompassing term to describe any type of content whether video, image, text or voice that has been partially or fully generated using AI or machine learning” (Askari 2023). These AI-generated videos and audio recordings can realistically portray individuals saying or doing things they never did. This technology can be used to fabricate political scandals, which are proven to dissuade voters (Maier 2011), sow discord amongst opposing factions, and erode trust in legitimate media sources, thus manipulating voters and significantly impacting the political landscape. If left unchecked, the deepfakes could have serious consequences for how journalists report the news, people‟s ability to make informed decisions, and the overall health of democracy (Flynn et al. 2017; Bennett and Livingston 2018; Chadwick et al. 2018). (Political) Microtargeting Beyond fabricating content, AI excels at personalizing it. Microtargeting, defined as “creating customized winning messages, proof points and offers, accurately predicting their impact, and delivering them directly to individuals” (Agan 2007, 2), is widely used in advertising and politics. It is a technique that nowadays leverages technology for best results. Technology is particularly useful in advanced political microtargeting to single out the potential voters, donors, and supporters (Aagard and Marthedal 2023; Cacciotto 2017; Bennett 2015; Bimber 2014), and AI is just the next iteration of tech-based microtargeting. Political microtargeting (PMT) “is a way to capture the attention of citizens who are on the one hand very reachable, because they carry their phones with them at all times, because their whereabouts are tracked, and because their personal data such as home addresses are collected on a large scale” (Dobber 2020, 9). The year 2016 marked a significant shift in how people viewed the use of digital tools in PMT political campaigns, in general. The unexpected results of the Brexit vote and the US presidential election of Donald Trump, both of which heavily utilized PMT tactics, drew widespread attention and proved that PMT is a powerful persuasion tool capable of swaying voters‟ opinions (Mahboob 2019). 51 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe AI can simplify the PMT process as it uses vast datasets to build detailed profiles of voters. This allows political campaigners to tailor messages that resonate with individual preferences, biases, and anxieties. Understandably, this can be a powerful tool for voter suppression (Mie Kim 2018), discouraging individuals from voting or swaying their opinions with emotionally charged content that bypasses critical thinking. Weaponized Bots and Algorithmic Amplification Social media platforms are rife with automated accounts, often called bots. A bot, “a computer program that performs automatic repetitive tasks” (Merriam-Webster 2024), can be easily programmed to spread misinformation and propaganda. Such a bot can create the illusion of widespread support for a particular political candidate or viewpoint, drowning out legitimate voices in a sea of automated noise (Howard et al. 2018). Recent developments in AI technology have made bots-creation significantly easier. As a result, somewhat capable, custom-made AI-powered bots have been on the rise. However, it is important to note that even ready-made AI algorithms that curate social media feeds can amplify misinformation by prioritizing content likely to generate clicks and engagement, regardless of its truthfulness. This confluence of all the aforementioned AI-powered manipulation techniques creates a “perfect storm” for political manipulation. For instance, a recent picture collage, shared by Republican presidential candidate Ron DeSantis, portrayed President Donald Trump hugging Anthony Fauci, the former Covid czar. Three of the six images in the collage were found to be AI-generated. And while the line between fact and fiction was indeed blurred, the goal of the collage itself was clear: to spread misinformation and sway voters. In April, the Republican National Committee also released an ad featuring AI-generated content. Their goal was to illustrate hypothetical crises that could occur if President Joe Biden were to secure a second term (Bond 2023). These examples best show how easily generative AI models can be misused during political campaigns to produce realistic-looking content that appears reliable and credible. Media Literacy in the Age of AI Digital literacy is “the ability to search for, evaluate, understand, and integrate information found online” (Ng 2012), and by extension, media literacy has never been so critical. People are bombarded with all kinds of content every waking moment, and their ability to critically assess this information is becoming more crucial in this digital era. In the face of AI-powered manipulation, media literacy, and by extension, media literacy education have become the only reliable weapon against potential disinformation, political or otherwise. AI is everywhere, and thus, the impact of AI on media literacy has become an even more complex issue that continues to evolve, especially in post-ChatGPT frenzy times (Tiernan et al. 2023; Valtonen et al. 2019). Voice-clonning, deepfakes, and AI-generated images are posted and shared online daily. And they are becoming increasingly difficult to identify. A cornerstone of media literacy is the ability to assess information sources. This involves checking websites for credibility and asking basic questions to determine reliability: Who 52 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe publishes the content? Are there clear author attributions and affiliations? Does the site exhibit transparency about its funding and potential biases? Similarly, faced with increasing AI-generated content, students must be adept at detecting manipulated content. Deepfakes and synthetic media require a critical eye for inconsistencies, unnatural movements, or audio glitches - skills that numerous students lack. Additionally, AI systems are trained on vast datasets, and these datasets can reflect the biases present in the real world. This means that AI-powered recommendations and search results may unconsciously favor certain political viewpoints or perspectives - which students, who are also voters, need to understand. Bykov and Medvedeva (2024) advocate for heightened media literacy efforts in the age of AI, suggesting that improved media literacy can even help people understand and use AI more effectively. As stated, AI poses significant challenges to media literacy, and some educational initiatives are already underway to meet these challenges. Programs like MIT‟s “Media Literacy in the Age of Deepfakes” aim to equip students and the broader public with the tools necessary to navigate content in the era of highly believable deepfakes. MIT‟s course incorporates case studies, such as the deepfake of President Nixon‟s speech in “In Event of Moon Disaster”, to illustrate the perils of AI in media. These educational efforts further emphasize the need for an up-to-date media literacy curriculum that will include AI aspects, ensuring students are prepared to critically assess AI-generated content in whichever scenario (Harvard Graduate School of Education 2024). METHODOLOGY For the purposes of this research, a qualitative analysis of the existing media literacy handbooks and curricula was conducted with a specific focus on the three most popular options in North Macedonia. The analysis followed the qualitative content analysis (QCA) approach by Schreier (2012) and the following steps: handbook selection, definition of a research question, data collection, coding frame development, and data analysis and interpretation. The three handbooks chosen were: 1. Handbook for Teachers for Studying Media Literacy in Mother Language Teaching: for Primary and Secondary Education (2010). 2. Media Literacy: Trainers‟ Handbook (USAID n.d.). 3. Media Literacy Handbook for Youth, Media, and Youth Organizations (2018). The research question was then defined as: “How do media literacy handbooks in North Macedonia introduce/discuss the concept of artificial intelligence?”. Initial categories, or codes, were then set as part of the coding framework: • Code 1: AI Definitions and Explanations • Code 2: Ethical Considerations • Code 3: Fake News and AI The handbooks were then fragmented into units (sections) and analyzed according to the coding framework. Drawing on the insights from the analysis of the chosen handbooks, a 5module framework for enhancing media literacy skills was developed by the author, focusing on 53 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe AI. This framework aims to provide a practical and adaptable approach that educators can utilize to equip students with the critical thinking skills necessary to navigate the complexities of AIpowered political (dis)information. RESULTS Analysis of Existing Media Literacy Handbooks For the purposes of this study, a comprehensive analysis of three media literacy handbooks was conducted. The chosen handbooks are all highly regarded, easily accessible, and written in Macedonian. The analysis revealed a significant gap in addressing the challenges posed by AIpowered manipulation in media, in general, but also in the political sphere: • Code 1: AI Definitions and Explanations None of the handbooks provided comprehensive definitions or explanations of AI and its applications in media and/or political news. • Code 2: Ethical Considerations The ethical implications of AI in media were notably absent in all three handbooks. As mentioned above, AI technologies raise several ethical concerns, so the lack of such content seems noteworthy. • Code 3: Fake News and AI While the proliferation of fake news is a well-documented issue in media literacy, the role of AI in generating and spreading misinformation was not addressed in any of the handbooks. Although all three handbooks provide precious insights and excellent, reliable information for critical thinking and source evaluation, they do not explicitly address the emerging threats posed by deepfakes, algorithmic bias, and weaponized bots. This has further highlighted the urgent need for a framework specifically tailored to equip students with the skills necessary to navigate the evolving landscape of AI-powered (political) discourse. Proposed Framework for Media Literacy in the Age of AI The following five-module framework has been developed to address the abovementioned critical gap. This framework focuses specifically on AI and its challenges regarding modern media literacy. The framework designed and proposed by the author is as follows: Module 1: Demystifying AI • Define AI and its role in shaping online information environments. • Explore the concept of algorithmic bias and its potential impact on search results and social media feeds. • Discuss the importance of recognizing diverse perspectives and mitigating echo chambers. 54 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe Module Objectives: After completing this module, students will: • Gain a basic understanding of AI and how it works. • Identify common uses of AI in everyday life. • Understand how AI systems can exhibit biases based on the data they are trained on. • Learn about the potential impacts of algorithmic bias on search results. • Recognize the importance of consuming information from a variety of sources. • Develop strategies to mitigate the effects of echo chambers in online environments in a political context. Module 2: Deepfakes and the Art of Deception • Analyze the technology behind deepfakes and synthetic media in general. • Develop techniques for identifying potential manipulation in videos and audio recordings. • Discuss the ethical implications of deepfakes and their potential to erode trust in media sources. Module Objectives: After completing this module, students will: • Comprehend the basic technology behind deepfakes and other forms of synthetic media. • Learn how AI creates realistic but fake videos, images, and audio recordings. • Develop skills to detect potential manipulation in videos and audio recordings. • Utilize tools and techniques to verify the authenticity of media content. • Understand the potential consequences of deepfakes on trust in media and society. Module 3: Evaluating Websites and Information Sources • Refine website evaluation skills by identifying credible news organizations and academic sources. • Explore tools for fact-checking and verifying information online. • Discuss techniques for lateral reading, consulting multiple sources to gain a comprehensive understanding of an issue, political or otherwise. Module Objectives: After completing this module, students will: • Learn how to distinguish between credible news outlets and less reliable sources. • Understand the characteristics of reputable academic and peer-reviewed sources. • Know about various online tools and resources for fact-checking and verifying information. • Practice using these tools to evaluate the accuracy of online content. • Adopt lateral reading techniques to cross-check information from multiple sources. • Apply these skills to comprehensively and accurately understand political issues in their countries. 55 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe Module 4: Understanding Social Media Manipulation • Analyze the tactics employed in social media manipulation, including fake political accounts and bots. • Develop strategies for critically evaluating social media content and identifying potential misinformation. • Explore concepts like political microtargeting and understanding how online behavior can be exploited. Module Objectives: After completing this module, students will: • Identify common tactics used in social media manipulation, such as fake accounts and bots. • Analyze real-life examples of manipulation campaigns. • Develop skills to assess the authenticity and credibility of social media content critically. • Learn to identify red flags that may indicate misinformation. • Comprehend how political campaigns use data to target specific demographics with tailored messages. • Discuss the ethical implications and potential impact of microtargeting on democratic processes. Module 5: Building a Culture of Verification • Equip students with skills for utilizing reliable fact-checking resources. • Discuss the importance of open discourse and respectful debate in a healthy democracy. • Encourage students to actively participate in the information ecosystem, sharing credible sources and fostering critical thinking amongst peers. Module Objectives: After completing this module, students will: • Learn to utilize reliable fact-checking resources effectively. • Understand the importance of open and respectful debate in a healthy democracy. • Develop skills for engaging in productive and evidence-based discussions. • Understand the importance of sharing credible information on their networks. • Improve their critical thinking skills. It is important to note that this framework is rather general, so it can be adapted to fit the specific needs of different educational settings. The ultimate goal is to help educators develop students‟ media literacy skills in an age where AI-generated content, in both political and non-political contexts, is becoming increasingly realistic and challenging to identify. 56 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe CONCLUSION AI is now part of the media sphere. The analysis conducted for this study revealed a concerning gap in existing media literacy curricula: a lack of explicit focus on the challenges posed by AI in the political sphere. This study addresses the identified gap by proposing a comprehensive framework for media literacy that focuses on AI specifically. This 5-module framework aims to help students become discerning information consumers: individuals who can assess content critically and engage responsibly with political discourse. As AI technologies evolve, so must our approach to media literacy education. The race between technological innovation and media literacy education is continuous. While this framework provides a solid foundation, ongoing research, and adaptation will ensure that future generations are equipped to participate meaningfully in a healthy democracy. All in all, the emergence of AI has undoubtedly altered the landscape of political discourse. AI-powered tools now produce deepfakes, facilitate political microtargeting, and spread false information. Consequently, they pose a significant threat to informed participation in democracy. AI has created a breeding ground for misinformation and manipulation, eroding trust in media sources and politicians. Herein lies the critical importance of media literacy education within the broader context of political education. Equipping students with the skills necessary to critically analyze information, identify potential manipulation, and verify sources is truly paramount. 57 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe COMPLIANCE WITH ETHICAL STANDARDS Acknowledgments: Not applicable. Funding: Not applicable. Statement of Human Rights: This article does not contain any studies with human participants performed by any authors. Statement on the Welfare of Animals: This article does not contain any studies with animals performed by any authors. Informed Consent: Informed consent was obtained from all individual participants included in the study. Disclosure statement: No potential conflict of interest was reported by the author/s. 58 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe PUBLISHER‟S NOTE The Institute for Research and European Studies remains neutral concerning jurisdictional claims in published maps and institutional affiliations. 59 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe REFERENCES 1. Aagaard, Peter, and Selma Marthedal. 2023. “Political Microtargeting: Towards a Pragmatic Approach.” Internet Policy Review 12, no. 1. https://doi.org/10.14763/2023.1.1690 2. Ahmadov, Anar, and Floris Holstege. 2023. “Does Schooling Promote Democracy? A Meta-Analysis.” Democratization 30, no. 1: 57-77. doi:10.1080/13510347.2022.2109016. 3. Askari, Javahir. “Deepfakes and Synthetic Media: What Are They and How Are TechUK Members Taking Steps to Tackle Misinformation and Fraud.” TechUK. August 18, 2023. https://www.techuk.org/resource/synthetic-media-what-are-they-and-how-are-techuk- members-taking-steps-to-tackle-misinformation-and-fraud.html 4. Bennett, Colin J. 2016. “Voter Databases, Micro-targeting, and Data Protection Law: Can Political Parties Campaign in Europe as They Do in North America?” International Data Privacy Law 6(4): 261-275. https://doi.org/10.1093/idpl/ipw021 5. Bennett, W. Lance, and Steven Livingston. 2018. “The Disinformation Order: Disruptive Communication and the Decline of Democratic Institutions.” European Journal of Communication 33, no. 2: 122-139. 6. Bimber, Bruce. 2014. “Digital Media in the Obama Campaigns of 2008 and 2012: Adaptation to the Personalized Political Communication Environment.” Journal of Information Technology & Politics 11(2): 130-150. https://doi.org/10.1080/19331681.2014.895691 7. Bond, S. 2023. “DeSantis Campaign Shares Apparent AI-Generated Fake Images of Trump and Fauci.” NPR. https://www.npr.org/2023/06/08/1181097435/desantis-campaign- shares-apparent-ai-generated-fake-images-of-trump-and-fauci 8. Bykov, Ilya A., and Mariia V. Medvedeva. 2024. “Media Literacy and AI-technologies in Digital Communication: Opportunities and Risks.” In 2024, Communication Strategies in Digital Society Seminar (ComSDS). DOI: 10.1109/ComSDS61892.2024.10502053 9. Cacciotto, Marco M. 2017. “Is Political Consulting Going Digital?” Journal of Political Marketing 16(1): 50-69. https://doi.org/10.1080/15377857.2016.1262224 10. Chadwick, Andrew, Cristian Vaccari, and Ben O‟Loughlin. 2018. “Do Tabloids Poison the Well of Social Media? Explaining Democratically Dysfunctional News Sharing.” New Media & Society 20, no. 11: 4255-4274. 11. Chu-Ke, C., & Dong, Y. 2024. “Misinformation and Literacies in the Era of Generative Artificial Intelligence: A Brief Overview and a Call for Future Research.” Emerging Media 0(0). https://doi.org/10.1177/27523543241240285 12. Flynn, D. J., Brendan Nyhan, and Jason Reifler. 2017. “The Nature and Origins of Misperceptions: Understanding False and Unsupported Beliefs about Politics.” Political Psychology 38: 127-150. 13. Handbook for Teachers for Studying Media Literacy in Mother Language Teaching: for Primary and Secondary Education. 2010. Macedonian Institute for Media and Institute for Democracy “Societas Civilis“ Македонски институт за медиуми и Институт за [демократија “Societas Civilis“]. Available at: https://idscs.org.mk/wp- content/uploads/2009/08/priracnik_za_nastavnici_za_izucuvanje_na_mediumskata_pisme nost_vo_nastavata_po_majcin_jazik.pdf 60 Published online by the Institute for Research and European Studies at www.iies.mk/jlpe 14. Howard, Philip N., Samuel Woolley, and Ryan Calo. 2018. “Algorithms, Bots, and Political Communication in the US 2016 Election: The Challenge of Automated Political Communication for Election Law and Administration.” Journal of Information Technology & Politics 15, no. 2: 81-93. doi:10.1080/19331681.2018.1448735. 15. Kaplan, J. 2024. Generative Artificial Intelligence: What Everyone Needs to Know. Oxford University Press. 16. Kim, Young Mie. 2018. “Voter Suppression Has Gone Digital.” Brennan Center for Justice, November 20, 2018. https://www.brennancenter.org/our-work/analysis-opinion/voter- suppression-has-gone-digital. 17. Mahboob, Tehseen. 2019. “How Facebook Wins Elections.” CBC, October 19, 2019. https://www.cbc.ca/passionateeye/m_features/how-facebook-was-harnessed-to-microtarget-voters-and-promote-donald-trump. 18. Maier, Jürgen. 2011. “The Impact of Political Scandals on Political Support: An Experimental Test of Two Theories.” International Political Science Review 32, no. 3: 283302. https://doi.org/10.1177/0192512110378056. 19. Media Literacy Education and AI. Harvard Graduate School of Education. 2024. Accessed May 21, 2024. https://www.gse.harvard.edu/ideas/education-now/24/04/media-literacy- education-and-ai 20. Media Literacy Handbook for Youth, Media, and Youth Organizations. 2018. Higher School of Journalism and Public Relations Висока школа за новинарство и за односи со [јавноста]. https://www.medium.edu.mk/attach/Priracnik-za-mediumska-pismenost- MK.pdf?5c1a6b78 21. Media Literacy in the Age of Deepfakes. 2021, October 18. Media Literacy in the Age of Deepfakes. MIT. https://deepfakes.virtuality.mit.edu/ 22. Media Literacy: Trainers‟ Handbook. n.d. USAID. Available at: https://www.irex.org/files/l2d-trainers-manual-n-macedonia.pdf 23. Merriam-Webster.com Dictionary, s.v. “bot”, accessed May 21, 2024, https://www.merriam-webster.com/dictionary/bot. 24. Ng, Wan. 2012. “Can we teach digital natives digital literacy?” Computers & Education 59(3): 1065-1078. 25. Oxford English Dictionary, s.v. “deepfake (n.),” September 2023, https://doi.org/10.1093/OED/9547101155. 26. Schreier, Margrit. 2012. Qualitative Content Analysis in Practice. London: Sage. 27. Tiernan, Peter, Eamon Costello, Enda Donlon, Maria Parysz, and Michael Scriney. 2023. “Information and Media Literacy in the Age of AI: Options for the Future.” Education Sciences 13, no. 9: 906. https://doi.org/10.3390/educsci13090906 28. Tiernan, Peter, Eamon Costello, Enda Donlon, Maria Parysz, and Michael Scriney. 2023. “Information and Media Literacy in the Age of AI: Options for the Future.” Education Sciences. 29. Valtonen, Teemu, Matti Tedre, Kati Mäkitalo, and Henriikka Vartiainen. 2019. “Media Literacy Education in the Age of Machine Learning.” Journal of Media Literacy Education 11, no. 2: 20-36. 61 Kamala Harris DOMINATES in Final Polls (w/ J. Ann Selzer) Author: Miller, Tim, Date: 2024-11-03 Collections: PollMethods Zotero Key: EAW78K6A Cite Key: Miller24harrisDominate Zotero Item | Lit Note 00:00 people looked at my methodology and it's 00:02 published there's no secret sauce people 00:04 sometimes think that it's published in 00:07 every article in the De Moine register 00:09 uh right there how we do it but you look 00:11 at it on paper and you go it's too 00:13 simple this this can't possibly work and 00:17 so far it has but I'm prepared that one 00:21 day it will not work and that I'll I'll 00:24 blow up into tiny little pieces and be 00:27 scattered across the city of De Moine 00:31 [Music] 00:36 hello and welcome to a bonus Sunday 00:38 edition of the bullar podcast I'm your 00:39 host Tim Miller we got the shock Seltzer 00:43 Iowa poll last night 47 to 44 among 00:48 likely voters in favor of Vice President 00:50 kamla Harris in Iowa so obviously we had 00:55 to have an emergency pod with the queen 00:57 herself Jay an Seltzer president of the 00:59 polling firm s in company she's best 01:00 known for conducting the Iowa Poll for 01:02 the de Mo register how you doing I'm 01:04 doing great thank you Tim good to be 01:06 with you no Tiara for the for the queen 01:09 today there're I leave my Tiaras and my 01:12 crystal balls at home okay uh well let's 01:15 just start first with what happened to 01:16 the poll I got to tell you I looked at 01:18 it three times before I realized that 01:20 kamalo was winning I just saw 4744 and I 01:23 was like that's pretty good for the vice 01:24 president down three and then it took me 01:27 about 2 minutes to for to like register 01:29 that she was up three so anyway I'm I'm 01:31 wondering uh at the just at the top 01:33 level uh what you saw as happening in 01:35 the poll and then we'll kind of go 01:36 through the particulars um I I don't 01:38 think anybody would look at the numbers 01:41 and not be you know in a somewhat State 01:43 of Shock we started interviewing Monday 01:46 night so Tuesday morning I walked in my 01:49 assistant was already uh crunching 01:51 numbers and she said did you see the 01:53 unweighted data I go yeah I'd like to 01:55 see the weighted data um so we're we're 01:58 always very cautious about ring things 02:00 too much I'm always telling my clients 02:02 don't get married to these numbers 02:04 because they can change but all they did 02:08 was confirm what we were seeing in the 02:11 the unweighted data uh after Monday 02:13 night that Harris was in the lead in 02:16 Iowa and that 02:18 was nobody in the right mind would 02:22 predict it our methodology is to make no 02:24 assumptions and we made no assumptions 02:26 and here we are yeah I mean quite a 02:31 remarkable change from your poll over 02:34 this early summer that had President 02:36 Biden down 18 you know to to a poll in 02:39 September that had it had it 02:41 significantly closer to this one like 02:43 what what are you seeing as having been 02:47 the impetus for such a dramatic 02:49 turnaround you know I can't give you a 02:51 data driven answer which would be my 02:54 preference sure what it seems like um 02:57 given just this morning so the we Rel 03:00 the findings from the Congressional 03:02 races yeah and we are now showing two 03:06 Iowa districts tilting toward the 03:09 Democratic candidate and that includes a 03:11 big lead in the first district which is 03:14 53 to 03:16 37 and that District in particular has 03:20 the the driving issue from what I read 03:23 has been the abortion issue and it was 03:26 over the summer that Iowa's six- we ban 03:29 on abortion went into effect after all 03:32 the court challenges were taken care of 03:34 so that the the decision being made the 03:37 law going into force and then people 03:39 living with it for a while uh and it 03:42 being a strong uh sort of compelling 03:46 thing from Iowa's in the congressional 03:48 districts Iowa right now all four 03:50 congressional districts are held by 03:52 Republicans so yeah to me it seems like 03:55 to a certain extent there's something 03:57 organic going on because the 03:59 presidential candidates certainly aren't 04:01 spending any money or time here um and 04:05 it may be in fact that these 04:06 Congressional races are driving the the 04:09 presidential race toward the Democrat um 04:12 I want to get into the Congressional 04:13 races a little deeper but just a couple 04:14 other things on the presidential level 04:16 first um the uh the gender gap is is is 04:20 particularly noteworthy and you look at 04:22 Independent Women older women in 04:24 particular uh talk about what you found 04:26 on that and which obviously could also 04:28 speak to the abortion question right 04:30 well the pollsters who work with uh 04:33 contests that are going to be decided by 04:35 a gender gap uh typically say you need 04:37 to win with women more than you lose 04:40 with men and we're seeing that in these 04:43 data uh right now so men are 04:47 5238 for 04:49 Trump and but the margin is wider 04:52 5636 a 20po gap with women for Harris 04:57 the gap for women age 65 and over and 05:01 this you know the age 65 and over group 05:03 I've been watching this whole cycle 05:05 because they in past Cycles have tilted 05:07 more Republican uh but now they are 05:11 strongly tilted toward Harris and the 05:14 the margin among women 65 and over is 05:17 more than two to one it's 05:20 6328 wow again that's sort of a 05:23 jawdropping number yeah I I wonder um 05:27 did you I I didn't actually look at the 05:29 at the is there do you do a religion 05:31 cross stamp like is do we know like our 05:33 church going I I do wonder how that uh 05:36 Cuts uh particularly among women I I 05:39 can't give you among women um but I can 05:42 give you evangelicals which is the group 05:44 that has been consistently and staunchly 05:46 supportive of President Trump and that's 05:49 more than three to one so they haven't 05:52 shaken off of their affection for the 05:55 former president as people are looking 05:56 at this you know you're everyone's 05:58 trying to explain it they're like I mean 06:00 it's just such a dramatic uh uh result 06:03 uh you've I guess I should have noted at 06:04 the top you've been uh Gone contrary to 06:07 the conventional wisdom several times 06:09 the Obama caucuses in 2008 were right 06:12 about that I remember the feeling in the 06:14 pit of my stomach still seeing your poll 06:16 in 2016 um you know showing Donald Trump 06:19 winning by a wider margin than expected 06:21 significantly in Iowa which turned out 06:23 correct I would say I just insert here I 06:26 have a nickname as I have a couple of 06:28 nicknames one is the outlier Queen yeah 06:31 um because people keep seeing my numbers 06:32 not match other so I've been in this 06:34 seat before uh the second is starting 06:37 with uh the the Congressional or sorry 06:40 Senatorial race between Bruce brilley 06:42 and Joanie Ernst our poll was the only 06:45 one showing Joanie Ernst with a 06:47 comfortable lead and I became known 06:49 among those campaign workers as the 06:53 harbinger of 06:55 Doom was the one time that was not on my 06:57 side that back that was that was my 06:59 final cycle as a republican in good 07:00 standing so I was uh I was actually 07:02 Consulting on on the the harbinger of 07:04 good news uh side of that uh of that of 07:07 that sener race but um yeah so so you've 07:10 been here before in the but anytime 07:11 there's an outlier look sometimes 07:12 they're just outliers right like 07:14 sometimes they're just like that's just 07:15 statistical modeling sometimes there are 07:17 just outliers that don't turn out to to 07:19 be harbingers of Doom Or Glory uh but 07:22 your outliers have tended to to d uh 07:25 direct towards what the end result was 07:27 going to be but when there is an outlier 07:29 people like okay what could it be and 07:31 one thing that there was a series of New 07:32 York Times Cenna polls today that Nate 07:35 con observed was that among older voters 07:39 like there's Democrats are more likely 07:41 to like answer right like that there 07:43 potentially is a like do you worry at 07:45 all about kind of response bias where 07:48 like older democrats for whatever reason 07:51 feel are more interested in in taking 07:53 the 07:54 poll well our method controls for that 07:58 so our method is we're looking to end up 08:00 with 800 likely voters but for people we 08:03 talk to who don't qualify as a likely 08:06 voter that is they've already voted or 08:08 they say they will definitely vote 08:10 that's the two ways you get into my pool 08:12 then we we go ahead and collect your age 08:15 your sex and what county you live in I 08:17 think race as well so we have 08:20 demographics for the full Iowa Adult 18 08:24 and over population and we wait that to 08:27 the known population parameters so now 08:30 we have a cross-section of all Ians and 08:32 we can extract from that the 800 and I 08:35 think it was 08:37 808 uh people who qualified so if older 08:40 people are more likely to vote and they 08:43 are they show up in my voter sample in 08:46 more plentiful numbers so for example we 08:49 talked to had had contacts with 08:53 1,38 all Ians in order to get enough 08:58 likely voters at at 808 so we've 09:01 controlled for that idea of response 09:03 bias by making sure they're not over 09:06 represented at the state level and then 09:09 they are what they are at the likely 09:11 voter level sure I guess that that that 09:13 would control for the fact that the you 09:15 know whatever that the likelihood of 09:16 older voters the correct amount the 09:19 likelihood is in the sample but does it 09:20 control for the fact that a specific 09:23 older voters who are spe or are more 09:24 likely to vote for Donald Trump might 09:26 not be as likely to to answer I mean I 09:28 guess the nonr response bias is the 09:30 question yeah well you can't control for 09:32 that right um we we do know I mean we do 09:35 we wait if if we have too many 65 and 09:38 overs we wait them down so that they're 09:41 not over represented at the state level 09:44 they might look like a larger population 09:46 than they are among the likely voter 09:48 sample but that's because they're more 09:51 likely to vote one of the other things 09:53 um as you mentioned you're the outlier 09:55 Queen so you're comfortable with this 09:56 but Nate Sila wrote in his uh Silver B 09:59 bu last night um uh talked about the 10:03 coones that you had to to drop this poll 10:06 you know um and he said basically that 10:09 what we've seen out there in the 10:11 industry is that there's a lot of what 10:14 he calls pole hurting right whereas 10:16 people are wait you know waiting things 10:18 to 2020 recall Vote or waiting things to 10:20 this or that in or in order to not have 10:24 a result that ends up as this this much 10:26 of an outlier uh I'm curious uh uh 10:30 clearly that's not something that ever 10:31 crosses your mind but I'm curious what 10:32 you think about the idea that other 10:33 people are are hurting um and and that 10:37 might explain why why um you're you're 10:39 so unique right now well and and please 10:42 I am I am grateful to have clients who 10:46 are we're in we work intimately together 10:49 they know my method they know my history 10:51 and so they the de Mo register had every 10:54 right to say this is too far off nobody 10:56 will believe it we're going to can it 10:58 but they don't and I think they think 11:01 like I think is we've taken our very 11:04 best shot at what we think is what's 11:07 true out there I don't we we've we've 11:10 tweaked a little bit here and there 11:12 we've there things that we decided 11:14 that's a corner cut that we don't want 11:16 to do and so I don't lie in bed thinking 11:20 I oh I wish we' done something different 11:23 so that you know it doesn't mean I'm not 11:25 nervous about whether my poll will be 11:26 good or not good as for other people 11:29 methodology to be honest that's the 11:32 first thing I go looking for when a poll 11:34 is published by anybody is to see what 11:36 it is that they're doing I've kept my 11:38 mouth quiet about this idea of waiting 11:41 to the previous recalled vote so how did 11:44 people vote in 2020 I I have a lot of 11:47 questions about how that would work out 11:49 actually 11:50 arithmetically but I have talked about 11:53 my method as pulling 11:55 forward and so what I want is my data to 11:58 reveal to me without favor what this 12:02 future electorate looks like so when I 12:05 hear people saying well you know we're 12:06 trying to figure out what this future 12:08 electorate looks like and I kind of 12:09 think to myself why bother it's how how 12:12 could you and if you're looking at the 12:15 past to say that's what the future's 12:17 going to look like to me that's pulling 12:19 backward so you're doing something that 12:21 will give you results that look like 12:24 2020 perhaps um I want results that look 12:27 like 2024 I I was listening to a um a 12:31 interview you did back after 2020 when 12:34 once again you were you weren't quite as 12:35 much of an outlier in 2020 um but it was 12:38 a little bit of an outlier again it was 12:40 less favorable to Biden than what the 12:42 rest of the national plls were show 12:43 particularly in the region and it turned 12:45 out to look more like the electorate 12:47 that that you had prognosticated but in 12:49 a post kind of one of these posttop 12:51 interviews you said you did worry about 12:53 like how long this method will last 12:55 right like that there that there are 12:57 potential issues still with polling I 12:59 have a pollster friend of mine who 13:00 called me a couple weeks ago and he's 13:01 like you're being too you know you're 13:04 taking this these polls too much at 13:05 Phase value we really it's so hard to 13:07 pull 18 to 30 year olds right now and 13:09 like we don't know so so I'm just 13:11 wondering like do you what what are the 13:13 anxieties that you have kind of looking 13:15 at at the current polling methodology 13:18 well I'm guessing the anxiety level will 13:20 tick up on Tuesday um that that's sort 13:24 of par for the course um I have had I've 13:27 had a good track record in like to say 13:29 may it always be so yeah um right but I 13:34 I do think and I have said if people 13:36 looked at my methodology and it's 13:38 published there's no secret sauce people 13:41 sometimes think that it's published in 13:43 every article in the De Moine register 13:45 uh right there how we do it but you look 13:47 at it on paper and you go it's too 13:49 simple this this can't possibly work and 13:53 so far it has but I'm prepared that one 13:57 day it will not work and that I'll I'll 14:00 blow up into tiny little pieces be 14:03 scattered across the city of De Moine 14:05 well may that not be today um the the uh 14:08 the other note uh you mentioned the 14:10 Congressional numbers at the top there 14:11 are four uh Republican Republicans hold 14:14 all four districts in Iowa right now um 14:17 that Iowa Third District which is around 14:19 De Moine uh you showed the Democrat 14:21 winning but by a margin that feels 14:23 potentially possible it's a swing 14:24 District de Mo has a lot of college 14:26 educated Republicans that are trending 14:27 left um then you as you mentioned the 14:30 first district which is Iowa City which 14:33 has college but then also some of rural 14:35 kind of Eastern Iowa and and some some 14:37 other of the smaller cities out there 14:39 and the Quad Cities Quad Cities 14:40 Davenport um that had a big lead for the 14:43 Democrats and so I do wonder you do you 14:45 look at that and think I don't know like 14:47 this uh uh I mean obvious obviously 14:50 you're not going to change your results 14:51 but I just wonder if that that first 14:53 district Congressional result gave you 14:54 any kind of pause about the overall 14:56 makeup of the state well keep in mind 14:58 that we have been seeing something going 15:00 on in the first district in terms of 15:03 approval ratings even for Joe Biden uh 15:06 for more than a year and so when we 15:09 would do briefings with the reporters 15:11 and editors I kept saying what's going 15:14 on in the first district there's 15:15 something going on in the first district 15:17 so this wasn't new but it was something 15:21 that caught our attention to have a lead 15:23 that wide for someone from the opposing 15:25 party so we gave that extra attention 15:28 and looked at it several different ways 15:31 and in the end we had no reason to doubt 15:35 and keep in mind we we wait by geography 15:39 as well so there there wasn't a way it 15:43 for that us to be over representing that 15:46 District unless they're more in that 15:47 District intending to vote so you said 15:49 Monday night the unweighted was showing 15:51 good stuff for Harris when you saw the 15:53 final numbers which I guess would have 15:54 been what Friday morning I'm I'm just 15:56 curious what was your what was your 15:58 mental date there did you have any were 16:00 you starting to sweat at all looking at 16:02 it did you look at it two times like I 16:05 did and say wait she's really winning 16:06 right three uh you know we watch it day 16:08 by day so so it was a it was an 16:11 incremental difference on Friday morning 16:14 uh on I will say that my nephew is a 16:17 senior at the University of Nebraska and 16:20 currently interning at Gallup 16:22 interestingly enough and I he wanted to 16:25 come and I wanted to have him here for 16:26 this and he walked in Thursday night and 16:28 I had a folder I had to swear him to 16:31 secrecy has to take the oath and he 16:33 opened the folder and and he he couldn't 16:37 believe what he was looking at and his 16:39 his mouth sort of fell down to the 16:41 kitchen 16:42 counter and it took him a while to wrap 16:44 his head around it so um you know you're 16:48 not alone in first of all not 16:51 recognizing what the poll numbers meant 16:53 because your expectation was for a 16:55 completely different Difference by we do 16:58 a preliminary briefing with the 16:59 reporters and editors on Thursday by the 17:02 time we got there Friday there was um 17:05 you know we're we're we're settled on a 17:07 plan of action for how to get the 17:08 stories out and uh I think I'd answered 17:12 all of the questions people had about 17:13 how could this be to their satisfaction 17:16 and now we'll the voters will tell us 17:20 yeah you said it was changing 17:21 incrementally through the week was it 17:23 moving one way or the other through the 17:24 week you know if you look at the a 17:27 rolling two-day average or even a 17:29 rolling 3-day average it's pretty flat 17:32 there there's daily variation as there 17:34 is with any pole sure um the the last 17:38 one of these that you had a Miss on was 17:41 04 John Cary does that one still haunt 17:43 your dreams or do you uh have all the 17:46 successes since then uh washed that away 17:49 I was just starting to get over it it 17:53 interesting um I was told by someone who 17:56 had access to uh exit polls that not 18:00 only would carry win Iowa but by a wider 18:02 margin than we thought I had a an 18:07 incident where I ran into former 18:09 Governor Terry Branstad this is before 18:12 his uh second Reign and he saw me in a 18:16 conference room where he was the 18:18 president of the organization and he 18:20 walked in and said to me a long story 18:24 but the short of it is that because we 18:26 had to stop polling then on Friday night 18:29 we couldn't have gotten the late um Bush 18:32 surge there was a rally in Sous City 18:35 that he'd never seen anything like it in 18:38 terms of the work that they were doing 18:40 with the rally participants to have them 18:43 go out and get more votes and he said 18:46 you couldn't have caught that in your 18:47 data and the fourth district which is 18:50 where suity is um came in with a big 18:53 margin for Bush and governor Branstad 18:56 claimed that rally was the reason Tom 18:59 dashel was evicted from his senate seat 19:02 in next door South Dakota South Dakota 19:04 so it it that was a a very uh that was a 19:07 gift to me for someone who understands 19:10 where every vote is in this state to say 19:13 your poll couldn't have caught it it 19:16 takes a few years for for that pit in 19:19 your stomach to resolve I have a lot 19:22 more L's in that so you can you work you 19:24 work your way through it all right last 19:25 thing is there any I mean obviously 19:28 Wisconsin is not a mirror image of Iowa 19:30 there there there differences in in 19:32 demographics but just is there anything 19:34 you think can be learned about the other 19:36 uh swing States um from the data that 19:38 you that you saw you know I said before 19:41 when people wanted to say well if this 19:43 is going on in Iowa this is what's going 19:44 on in Wisconsin that our state parties 19:48 function very differently um Iowa is so 19:51 much more red than Wisconsin is both 19:55 houses of our state house are Republican 19:58 by large majorities every Statewide 20:02 executive that's elected by the entire 20:05 state are Republican except for the 20:08 auditor and the entire Washington 20:10 delegation is Republican so it's hard to 20:13 get more Republican than Iowa and 20:15 Wisconsin is more purple than that I'm 20:18 not going to say they're blue they've 20:20 got they've moved back and forth more 20:21 than Iowa has so I don't think I can 20:25 make any Claim about Wisconsin unless we 20:28 did our method in 20:31 Wisconsin all right well the whites seem 20:34 kind of similar though you know the 20:36 types of people the Culver the kind type 20:38 of food that they're taking in I don't 20:40 know I I'm I'm I'm hoping to take some s 20:43 of kind of cultural similarity uh 20:46 between the Badgers and the Hawkeyes but 20:48 uh we will I guess we'll have to wait 20:50 and see on Tuesday thank thank you so 20:52 much they do have a professional 20:54 football team which we do not have that 20:56 is true um thank you so much an for uh 20:59 coming on to the BG podcast morning 21:01 after the big poll uh 47 for KLA Harris 21:04 44 for Donald Trump we will see how 21:06 things shake out on Tuesday and hope to 21:08 be in touch I don't know sometime before 21:10 the 2027 Iowa 2028 Iowa cus this all 21:15 right good to talk to you Tim thanks Dan 21:18 Selter we'll be back tomorrow for our 21:20 normal Monday edition of the Bor podcast 21:21 peace 21:23 [Music] Let Them Fight! Bannon Vs. Musk Brawl Is Exploding! Will It Tear MAGA Apart?! Author: Lance, JV, Date: 2025-01-13 Collections: NeuroPsychoLinguisticPolitics, Polarization, MisDisinformation Zotero Key: RUJNNBW2 Cite Key: Vance25fightBannonMusk Zotero Item | Lit Note 00:00 hey there it's JVL here with the 00:01 bulwarks Andrew Edgar Andrew over the 00:05 weekend we uh we had the the eruption of 00:08 a fight between Elon Musk and Steve 00:11 Bannon and it was glorious to behold I I 00:14 don't need like we need all of the gifts 00:16 we need the Let Them Fight gift we need 00:19 the worst person you know just his right 00:22 gift all of the gifts that exist in the 00:24 meme verse but I'm team 00:27 Bannon how about you 00:30 so it's a weird thing um it's a weird 00:32 thing like like you didn't necessarily 00:34 expect the coalition to break down quite 00:35 this quickly even though obviously uh 00:37 Steve Bannon has had an ex to grind with 00:39 Elon Musk in particular for a few years 00:41 um but but but so I feel like we're all 00:43 kind of like caught a little bit napping 00:44 it's like oh wait uh the magga Coalition 00:46 that was so seemed so ascendent is 00:49 already kind of splintering a little bit 00:50 and I guess take sides about it yeah I 00:53 mean it's very well it's interesting 00:54 because like the the the main thing this 00:56 Coalition exists to do is to beat up on 00:59 Democrats when Democrats are in power 01:00 right I mean that's that's not Trump's 01:02 big political aim but that's the that's 01:03 the political strength of the movement 01:04 because it's all reverse polarization 01:06 against the Democrats um and uh and it 01:09 just turns out it it falls apart more 01:11 quickly even than I think we would have 01:12 imagined but but yeah like you you you 01:14 said your your team ban and I'm here to 01:16 kind of argue the side of of of Elon 01:18 Musk in this whole thing but you wrote 01:19 you wrote your newsletter about it today 01:21 so can you just like lay out a little 01:22 bit of in this fight between these two 01:25 guys uh why it is that maybe your 01:27 sympathies is a lot more with Steve yeah 01:29 so I mean there there are a bunch of 01:31 reasons first of all uh Bannon is what 01:34 in Dungeons and Dragons which is not 01:36 something you had a lot of experience 01:38 with in your youth I don't believe 01:40 correct me if I'm just making 01:42 assumptions I did play little Dungeons 01:44 and Dragons in high school you did oh 01:46 good for you I did good for you I also 01:49 played 01:50 guitar parents weren't worried that you 01:52 were going to hell for for doing the the 01:54 demon game we can talk about that later 01:56 I have a whole bunch of stories let's 01:57 keep let's stick to the topic at hand so 01:59 uh Steve Bannon would be lawful 02:02 evil and what that means is you know 02:05 he's bad guy but he's got a code he 02:08 aderes to the code Elon Musk is chaotic 02:10 evil right Steve bannon's been doing 02:12 this for for this point 20 years and 02:15 he's got a thing he he hates immigrants 02:19 he wants to shut down immigration he 02:22 wants to do isolationism an American 02:24 foreign policy he wants to do more 02:27 domestic Manufacturing 02:30 and uh and he wants to to punish the the 02:33 the wokes you know he hates liberals and 02:36 he wants to tear down you know Elite 02:38 institutions if he's got to break some 02:40 laws while he does it he's not going to 02:41 lose any 02:42 sleep Elon Musk has been doing this for 02:46 five minutes he started paying attention 02:47 to politics five minutes ago and since 02:50 then he's decided that climate change is 02:52 the most important thing in the world 02:53 and then that AI is the most important 02:55 thing in the world then that fertility 02:57 rates the most important thing in the 02:58 world then that the woke mind virus is 03:00 the most important thing in the world he 03:01 just ping pongs from thing to 03:03 thing and he he makes up new positions 03:07 and you know who can say whether or not 03:10 he's using ketamine this is a big 03:12 question nobody knows obviously I'm not 03:14 a doctor but I would say uh if someone 03:16 has depression issues they should 03:17 consider talking to the doctor about 03:19 camine instead of ssris this is a 03:22 53-year-old dude who seems to maybe be 03:27 paying people to play Diablo for him so 03:31 he can then go on to Twitch streams and 03:34 claim to be the ninth best Diablo player 03:38 in the world what in the world what's 03:41 going on it's absolutely nuts and the 03:45 extent to which Elon does seem to be 03:47 motivated by nothing but rent seeking 03:50 right if you look at the through line 03:51 for his life there is nothing but rent 03:54 seeking you know he's just going from 03:55 subsidy to subsidy and he will he'll 03:59 suck up to anybody if there's money in 04:02 it for him and and his companies and so 04:05 you know he's soft on China he's soft on 04:08 Russia he's soft on 04:10 everybody and he's out there being a 04:12 Welfare Queen in America looking for tax 04:15 breaks and 04:17 yeah I I just musk is a bad bad guy in 04:22 the oligarchical sense he wants to be 04:24 God emperor of the solar system and I 04:27 don't think that you can a trust him 04:29 with anything thing and B I think that 04:32 if musk was actually allowed to be in 04:34 charge of some real power for a year 04:36 would be really bad for 04:38 America but with 04:40 Bannon with Bannon if you squint if you 04:43 squint real hard you can see a 04:47 domesticated version of what his 04:50 populism would be like that would be 04:52 okay not all of it but uh but portions 04:56 of it you're like okay that wouldn't 04:57 give us like Optimal outcomes 05:00 but like that's a thing we could 05:02 basically live with kind of do you think 05:06 a domestic version of of Steve Bannon is 05:08 that a guy who wears fewer collared 05:10 shirts or does that one lay more than no 05:13 no more than four collars in the 05:15 domesticated version of Steve Bannon 05:17 yeah yeah so I I I was I was batting 05:20 this question around in my mind because 05:21 you know your newsletter prompted me to 05:23 think about it like I mean like again 05:25 who's the least worst guy who could be 05:27 whispering in Trump's ear at at any 05:29 given moment I think it's an interesting 05:30 question right um and and I the thing 05:33 that gives me a little bit of pause and 05:34 I I I I do buy big chunks of your 05:36 argument um the thing that gives me a 05:38 little bit of pause is like thinking 05:40 about Elon musk's political Evolution 05:42 right thinking about the way that he 05:44 went from a guy who was just kind of 05:45 zany and arrogant and obviously had a 05:48 gigantic head but was just kind of 05:50 limiting himself to you know being a god 05:53 of tech right like being being um in 05:55 that space and and and the way that he 05:59 himself kind of radicalized I think on a 06:01 personal level over over the last few 06:04 years into a guy who paid more and more 06:06 attention to the political discourse and 06:08 then was just kind of living in it 06:10 constantly and posting about it way more 06:11 than any human being ever ought to uh 06:14 and and then ultimately because he had 06:17 all this money sitting around was able 06:18 to sort of Leverage that into suddenly 06:20 become making himself one of the most 06:22 important people in politics I mean 06:23 Michael Michael Bloomberg must be Green 06:26 With Envy over over that um but but the 06:29 think the thing that makes me push back 06:31 if we're specifically comparing these 06:33 guys musk and Bannon I think I think a 06:35 big part of what happened to Elon Musk 06:38 has a lot to do with Steve Bannon not so 06:39 much with his ideological program that 06:41 you were laying out a minute ago but 06:43 sort of the the bigger contribution that 06:44 Steve Bannon has made to the magga 06:47 movement in general which is to kind of 06:50 give it a theory of media right of of 06:52 what political persuasion is of how Maga 06:54 wins in kind of the the popular 06:56 Consciousness and this has been a big 06:57 thing for Bannon for for years right I 06:59 mean his his very famous quote from from 07:01 Trump's first term is that you know you 07:04 don't need to beat the Democrats you 07:05 beat the med you need to beat the media 07:06 and the way you beat the media is by 07:07 flooding The Zone with we've all 07:09 said that phrase a million times right 07:11 because the theory is if if you can just 07:14 ruin everybody's media diet if you can 07:16 ruin everybody's kind of information 07:18 stream then you can beat down their 07:21 ability to distinguish truth from lies 07:23 you can destroy their social trust you 07:25 can turn them you can turn the whole 07:26 nation into you know political paranoids 07:29 and that ultimately plays to the benefit 07:31 of the kind of iconic clastic populist 07:34 movement with a figurehead like Donald 07:35 Trump who then can accumulate all this 07:37 power and you know put in the put the 07:40 program into place that guys like Steve 07:41 Bandon want or whatever but what's 07:43 interesting when you're talking about 07:44 these two guys is that that's kind of 07:45 exactly what has happened to Elon Musk 07:48 right like when you when you talk about 07:49 musk's um you know M musk does not share 07:52 that theory of persuasion musk is almost 07:54 like sort of childlike in a way when he 07:56 talks about political stuff right he 07:58 like he has this this like totally 07:59 unearned and naive like kind of cloud 08:03 cucko land sense where he's like you're 08:05 you're the media now we're just we're 08:07 we're we're brushing all those all those 08:08 guys out of the way all the all the 08:10 interests special groups all the elites 08:12 all of the the newspaper guys and 08:14 everything like that we're making them 08:15 irrelevant and and what's going to rise 08:17 to the surface is this beautiful 08:18 beautiful discourse and argument and 08:20 persuasion and all the real facts and 08:22 the truth is going to come out and it's 08:23 going to flourish right I mean it's a 08:24 very different kind of way of looking at 08:25 things than what Steve Bannon has but at 08:28 the same time he is a guy who has had 08:30 his brain hooked up to like the banon 08:32 machine for years and has completely 08:34 cooked his own mind I mean like he he's 08:36 just some of it's just being online 08:38 right like it's it's the attention span 08:40 stuff that you were talking about before 08:41 but but I mean he he's a guy who 08:43 like now has completely lost any ability 08:46 to like distinguish conspiracy theory 08:49 from reality of anything that comes 08:50 across his own feed um and and we I 08:54 think we see a lot of the political 08:55 damage of that and I guess the point 08:56 that I'm making is a lot of the real 08:59 real harms that a guy like Elon presents 09:01 and a lot of the real dangers that come 09:03 from having him be like a central figure 09:04 in our politics are sort of Downstream 09:07 from The Steve Bannon way of organizing 09:10 the world to try to turn everybody in 09:12 America into one of those lunatics yeah 09:14 I can I can see that it's not it's not 09:16 wrong exactly but there's more going on 09:19 here I mean Bannon has to be 09:21 authentically 09:22 populist in the sense that he's got to 09:25 find things that can have broad support 09:28 whereas Elon 09:31 Elon will just claim to be a populist 09:34 and then try to manipulate the results 09:36 so like this is a famous thing he would 09:37 do he'd like you know I'll do this when 09:39 he first took over Twitter uh what do 09:41 you think should I do this thing I'll 09:43 post a poll for an hour and then you 09:45 know if he didn't get the results he 09:46 wanted at the end of the hour he'd be 09:48 like I'm gonna extend this Poll for 09:51 another 24 hours and we'll see what 09:53 happened you know or he'd go in and 09:55 monkey with the the results 09:57 himself and so there is a Elon is Elon 10:01 is not a vox populi Vox day guy right he 10:04 is a you Vox Elon Vox de guy and that's 10:09 what he wants and Bannon doesn't operate 10:12 in that way and it's because Bannon is 10:16 Bannon is fundamentally an underdog 10:18 right and has always been an underdog he 10:19 is a rebel and and so he wears that well 10:23 and understands that in order to he he's 10:26 got to create something that can 10:27 actually find Traction in the World Elon 10:30 is the opposite and one of the things 10:31 that's funny to me Elon May wind up 10:36 being the richest guy in the history of 10:38 humanity I mean he's not there yet but 10:40 he could wind up having more money than 10:42 anybody has ever actually 10:44 had and he's trying to position himself 10:47 as the underdog guy and that's 10:50 insane right I mean it it is utterly 10:53 insane that he he's sees himself as this 10:56 Scrappy I mean he's he's not he was born 10:59 rich he's gotten richer he may wind up 11:02 the richest guy ever uh and because of 11:06 that being the posture he operates out 11:09 of he really this is a guy who wants to 11:12 be king of the world you know in a way 11:14 that Bannon doesn't bannon's Ambitions 11:17 are not bannon's Ambitions are 11:18 structural elon's are personal um yeah 11:22 and I it it is interesting like like 11:25 when when you talk about Elon I think 11:28 part of it part of it is just that like 11:29 it's it's a little bit like talking 11:31 about Trump where it's like so so much 11:32 of it comes down to what you are 11:34 imagining is actually kind of going on 11:35 between his ears right because so much 11:37 so many of his actions are kind of like 11:39 inscrutable and weird and open to 11:40 interpretation um I I think like you can 11:44 sketch out a compelling case that Elon 11:46 is a guy who who is you know more 11:48 calculating and more cynical than than 11:50 he lets on to be and a lot of people 11:52 argue this that like all this ultimately 11:54 comes down to you know cozying up to 11:56 power so that he can protect his 11:57 business interest in China or things 11:59 like that I mean like like there there 12:00 are theories of this I don't know if I 12:02 find them all that persuasive I mean I I 12:05 I think I think Elon ultimately the more 12:08 you kind of just like Mainline his stuff 12:10 the more he kind of comes across as a 12:12 guy who who is like supremely confident 12:14 in his own powers and his own ideas and 12:16 all that and all that stuff but is also 12:18 kind of furtive and wounded seeming and 12:21 and and very uh uh impulsive in in kind 12:25 of the the moves he makes and you know 12:28 again like keep coming back to sort of 12:29 the attention span but he does not he 12:31 does not um Carry himself like a person 12:34 who who has much of a plan like he's not 12:36 he's not like I'm gonna I'm gonna read 12:38 you know I'm gonna really brush up on 12:41 these six Fields uh in in year of ourt 12:44 2025 and and it's it's you know grooming 12:46 gangs in Great Britain and it's you know 12:49 the the new spending bill and the 12:51 importance of stopping that incident I 12:52 mean he he he really is like kind of 12:54 enslaved to to the algorithm in a way 12:56 that his own trending topics yeah right 12:59 right right I mean yeah he he he is he 13:01 has hooked himself up to his own machine 13:04 um and uh and I don't know like I I so 13:06 so so I think when you when you see him 13:08 as that kind of guy you can you can 13:10 almost see him as like kind of a tragic 13:11 figure in some ways where he he has he 13:14 has all this success and he has all this 13:16 ambition and I think a lot of his a lot 13:18 of his stated um again kind of like 13:20 naive uh uh ideals are things that he 13:24 sincerely believes or sincerely believes 13:26 he believes right it's just that he is 13:28 also like such a such an appetite and 13:30 such a such an un um undisciplined guy 13:34 in in in some weird ways uh that that 13:37 anytime those things bang off of like 13:39 you know speed bumps or his ego or 13:40 things like that he kind of brushes them 13:41 out of the way which is kind of what you 13:42 were alluding to before I don't know how 13:44 you stack that up against a guy like 13:45 Bannon who I don't really know uh I I 13:47 don't really know how to assess him 13:48 personally on the same way but uh can't 13:50 they just both suck JVL I don't know I 13:51 mean it's like ideally they will they 13:53 will eat one another and only the the 13:55 you know the fingernails and the tips of 13:57 their tails will be left or whatever you 13:59 PE people say that right there's the 14:01 expression like well it's the Iran Iraq 14:02 war who are you going to root for and 14:04 the answer is in the Iran Iraq war we 14:06 and the rest of the Civilized world all 14:08 rooted for Iraq like you know Saddam 14:10 Hussein's Iraq was not a great place 14:13 Saddam was a very very bad guy but 14:15 compared to the the the 14:18 mulas they were much much less 14:20 destabilizing and it wasn't even close 14:22 um even the Russians I mean this is you 14:24 know in a weird way the Americans and 14:27 the Russians and the French and the 14:29 Saudis and the rest of the gulf were all 14:30 on Iraq side the Iranians had the North 14:34 Koreans that was their Coalition partner 14:37 so you know uh I think we can we can 14:40 root for one I want to ask you before we 14:42 get out I want to ask you a question 14:43 about 14:44 Elon do you think in any part of elon's 14:50 mind there is a node which is 14:53 contemplating becoming 14:56 president because 14:59 I kind of 15:02 do I think there is a tiny little 15:06 recess it's not on active right now it's 15:08 in sleep mode it's in passive mode but I 15:11 think there's a part of him which is 15:13 thinking well maybe I should 15:16 just do it myself yeah it's it's 15:18 interesting to think about I mean on the 15:20 the kind of easy first blush answer is 15:23 that he's not constitutionally eligible 15:24 right um but you 15:26 know yeah exactly right you can 15:29 could why couldn't the Constitution be 15:32 disrupted right right I think if he 15:35 wanted to you know he could he could 15:36 very easily have like a well we'll cross 15:38 that bridge when we come to a sort of 15:39 approach 15:41 I it seems to me we'll see with how 15:43 these four years go right but I I would 15:45 lean toward thinking I think that it is 15:47 a little bit more in keeping with elon's 15:50 view of politics and and kind of his own 15:53 sort of heroic vision of himself that 15:55 that he is sort of this outside uh ch 15:59 Cher of sort of incoit forces uh that 16:02 that is not you know he he doesn't get 16:04 he doesn't want access to the actual 16:06 powers of the presidency because he 16:08 doesn't think that is what his secret 16:10 sauce is he thinks his secret sauce is 16:12 in kind of being a conduit for for sort 16:14 of popular intention um and and and and 16:18 the the power of you know sort of tech 16:19 and specifically Tech he controls to 16:22 kind of channel that and and use that to 16:24 indirectly uh or directly kind Force the 16:27 people who who do have the kind of 16:28 titles and the desks and things like 16:30 that to to kind of bend the knee and I 16:32 think he he sees himself as sort of that 16:34 more like swashbuckling outside figure 16:36 um but you know he is currently uh 16:40 potentially testing the the outside 16:42 limits of that theory right because he 16:44 has done a lot to get Trump elected he 16:46 has he has really punched his ticket uh 16:48 and and and does control a lot of 16:50 popular will and does have that position 16:51 close to the president and gave him a 16:54 hundred bazillion dollars to help him 16:56 win um such that I think he does feel 16:59 very entitled now uh to to wield a great 17:03 deal of influence and if a guy like 17:04 Steve Bannon can just kind of come 17:06 around the corner and cut him off at the 17:07 knees just because he's better at 17:09 actually you know connecting with people 17:11 and is not like totally uh kind 17:14 of that's the weird thing about Elon 17:16 right is like he he has this parasocial 17:19 relationship with all of these like 17:21 little fans of his but but he as a human 17:23 being really does not connect with other 17:24 human beings very well G Bannon is very 17:26 personable and kind of charming and 17:28 garious so so I do think like if if Elon 17:32 did kind of get kicked to the curb here 17:34 uh it would have a pretty devastating uh 17:38 impact on that kind of theory of outside 17:40 leadership that I was just uh just sort 17:42 of sketching out and who knows what he 17:44 would do after that I mean you you you 17:45 could see it you could see it here's the 17:48 thing about Elon he is a takeover artist 17:52 he he he gets interested in something he 17:55 doesn't often start his own thing uh he 17:58 just goes and takes takes things over 17:59 like with Tesla uh I don't know about 18:02 SpaceX I I feel like he uh he bought 18:06 SpaceX but I don't don't quote me on 18:08 that this is just YouTube we're we're 18:09 thinking out loud um with Twitter right 18:12 he got interested in it and couldn't do 18:14 what he wanted to do from only owning 9% 18:16 so he went and bought the thing I could 18:19 see a world in which four years of him 18:23 just fixating on Power and the United 18:25 States government he doesn't he can't 18:28 Scrat match the itch that he really has 18:31 there and he just 18:33 decides you know in his his mumbly way 18:37 and 18:38 uh 18:40 I again not a not a prediction but a 18:45 long shot like 100 to one is that at 18:48 some point Elon starts toying with the 18:50 idea of well why couldn't I just own the 18:52 United States I own everything else and 18:56 I'm 18:57 brilliant God help 18:59 us Andrew thanks for sitting in on this 19:02 guys hit like hit subscribe come follow 19:04 the channel we'll see you later good 19:06 luck America The persuasive effects of political microtargeting in the age of generative artificial intelligence Author: Simchon, Almog, Date: 2024-02-01 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit, PoliticalML Zotero Key: N7PTXP2H Cite Key: Simchon24PersuasiveEffectsPolitical Zotero Item | Lit Note p // g/ /p /pg Advance access publication 29 January 2024 Brief Report The persuasive effects of political microtargeting in the age of generative artificial intelligence Almog Simchon a,b,, Matthew Edwardsc and Stephan Lewandowsky a,d,e aSchool of Psychological Science, University of Bristol, Bristol BS8 1QU, UK bDepartment of Psychology, Ben-Gurion University of the Negev, Beer Sheva 841050, Israel cDepartment of Computer Science, University of Bristol, Bristol BS8 1QU, UK dSchool of Psychological Science, University of Western Australia, Perth 6009, Australia eDepartment of Psychology, University of Potsdam, Potsdam 14476, Germany *To whom correspondence should be addressed: Email: almogsi@post.bgu.ac.il* Edited By: Katherine Ognyanova Abstract The increasing availability of microtargeted advertising and the accessibility of generative artificial intelligence (AI) tools, such as ChatGPT, have raised concerns about the potential misuse of large language models in scaling microtargeting efforts for political purposes. Recent technological advancements, involving generative AI and personality inference from consumed text, can potentially create a highly scalable “manipulation machine” that targets individuals based on their unique vulnerabilities without requiring human input. This paper presents four studies examining the effectiveness of this putative “manipulation machine.” The results demonstrate that personalized political ads tailored to individuals’ personalities are more effective than nonpersonalized ads (studies 1a and 1b). Additionally, we showcase the feasibility of automatically generating and validating these personalized ads on a large scale (studies 2a and 2b). These findings highlight the potential risks of utilizing AI and microtargeting to craft political messages that resonate with individuals based on their personality traits. This should be an area of concern to ethicists and policy makers. Keywords: microtargeting, persuasion, GPT, AI Significance Statement Our study sheds light on the impact of artificial intelligence (AI) in political advertising, revealing how automated, personalized mes sages can potentially shape voter decisions. Our findings, showcasing the power and scalability of this technology, underscore the pressing need for ethical scrutiny and policy-oriented solutions to govern the use of AI in shaping public opinion and safeguarding electoral integrity. Introduction In the summer of 2016, the world was struck with the Brexit refer endum results, in which the United Kingdom had voted to leave the European Union. On the other side of the Atlantic, Donald Trump was only months away from getting elected to be the 45th president of the United States. At the same time, Alexander Nix, a CEO of a relatively obscure company called Cambridge Analytica, promoted the company’s ostensible success at swaying voters from one political candidate to another based on exploit ation of voters’ particular psychological vulnerabilities. This tac tic involves deducing psychological attributes that are not readily observable, such as personality traits, from individuals’ online behavior and personal data. Subsequently, these inferred psychological features are leveraged to craft highly personalized messages tailored to each individual. Cambridge Analytica was involved in the Vote Leave campaign (United Kingdom), the 2016 Trump campaign (United States), and other political campaigns spanning 68 countries (1) before it folded in 2018 after investiga tions opened in several countries. The investigation by a British Parliamentary committee concluded that relentless targeting that plays “to the fears and the prejudices of people, in order to alter their voting plans” is “more invasive than obviously false information” and contributes to a “democratic crisis” (2). Microtargeting is also problematic outside the political domain when it exploits people’s moment-to-moment emotional state. Facebook has access to technology that can identify vulnerable teenagers at moments when they feel “worthless” and “insecure,” although the technology was ostensibly never made available to advertisers and only used in an experimental context (3). The actual impact of Cambridge Analytica is difficult to ascer tain. There is, however, little doubt that sensitive personal charac teristics can be inferred from people’s digital fingerprint. A review of 327 studies concluded that numerous demographics could be reliably inferred from personal data, including for example sexual Competing Interest: The authors declare no competing interest. Received: August 24, 2023. Accepted: January 16, 2024 © The Author(s) 2024. Published by Oxford University Press on behalf of National Academy of Sciences. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse distribution and reproduction in any medium provided the original work is properly cited orientation (4). Personality (specifically, the “Big 5” attributes) can be inferred from 300 Facebook likes with greater accuracy than a person’s own spouse (5). It is less clear that microtargeting is indeed effective as a per suasion tool. The claims initially advanced by Cambridge Analytica were most likely exaggerated (6), and some experiments have failed to find a benefit from personalizing and microtargeting political ads in comparison to exposure to a generic ad that is the same for all members of the target audience (7). By contrast, a re cent set of studies has repeatedly shown the efficacy of microtar geting in a variety of circumstances. For example, microtargeting can increase voter turnout during tight political competitions based on highly salient issues (8); it can prevent voter defection from a party they initially favored (9); and it can have an effect even when ads are targeted on the basis of a single personal attri bute (10). A recent systematic review established that messagetailoring (i.e. aligning messages with characteristics of the target audience) is an effective persuasion strategy ( r = 0.17) (11). Given that, of late, the evidence has tilted in favor of the effi cacy of microtargeting, the availability of generative artificial in telligence (AI) (e.g. ChatGPT) has triggered additional concern that those large language models (LLMs) could be leveraged to readily scale microtargeting. Whereas conventional ad produc tion is a time-consuming process and personalization requires painstaking validation (12), two recent technological develop ments can dramatically reduce the time and effort required to con struct ads: First, generative AI may be used to derive numerous personalized versions of political messages from a single humandesigned template. Second, a recent machine-learning model has successfully inferred the personality of readers from the text they consume (13), providing a platform for rapid validation of the text produced by the AI. The technology thus exists to take any political message and derive versions that target people of dif ferent personalities in an automated manner and at scale. The present paper tests the effectiveness of this “manipulation machine” in four studies. We show that personality-congruent political ads are indeed more effective than other ads (studies 1a and 1b), and we show that ads can be generated and validated automatically (studies 2a and 2b). Results Study 1a Study 1b Study 2a Study 2b −0.2 −0.1 0.0 0.1 0.2 Coefficient Fig. 1. Perceived persuasion effect in studies 1a (Facebook ads; n = 440), 1b (Facebook ads; n = 803), 2a (GPT-3; n = 803), and 2b (ChatGPT; n = 804). x-Axis denotes the matching score coefficients. Negative coefficients mean that the larger the deviance from personality matching, the less persuasive the message was rated. y-Axis denotes the different studies. Error bars indicate 95% CI. The dashed line represents the zero point. In study 1, we explored the effect of personality-congruent polit ical messages on perceived persuasion using real political ads. We selected 10 ads from a pool of 1,552 political ads that were pub lished on Facebook to UK users between December 2019 and December 2021 (for validation, see Ref. (13)). Leveraging a recent language model, we assigned an “openness score” to each of these ads, predicting the attractiveness of the ad’s text for the openness to experience personality dimension experience (13).[a] In study 1a, we collected data from 440 participants; in study 1b, we replicated the procedure with 804 participants. Participants were requested to rate the ads’ perceived persuasiveness (see Materials and meth ods for details) and answer a personality questionnaire measuring the “openness to experience” factor (14). We constructed a person ality matching score, wherein we calculate the absolute difference between the scaled openness score of participant i and the scaled openness score of ad j: Matchingij = |z(Opennessi) − z(Opennessj)|, which serves as our key predictor of perceived persuasion. We fitted a linear mixed model to predict perceived persuasion with the openness score of the person, openness score of the ad and matching score as predictors. The model included random in significant predictor of perceived persuasion, above and beyond the personality score of people and ads, such that the larger the de viance from matching, the less persuasive the message was rated. Matching effect—study 1a: b = −0.07, 95% CI ( −0.09, −0.05), t(26, 348) = −7.40, P < 0.001; matching effect—study 1b: b = −0.08, 95% CI ( −0.10, −0.07), t(48173) = −12.87, P < 0.001, see Fig. 1 and supplementary material for details. Next, we explored whether this process could be automated us ing generative AI. We leveraged two generative language models: GPT-3 and ChatGPT. We prompted the models with instructions to rephrase the existing ads in our ad corpus and generate two var iations of each ad, one catering to people high on openness and one catering to people low on openness (see Materials and meth ods for details). It is important to recognize that our approach is limited to a single iteration of the generated ad. Given the stochas tic nature of LLMs, some outputs may be better than others. However, our argument is that LLMs could potentially be used to produce manipulative microtargeting at scale. Consequently, the minor variations introduced by these random fluctuations are likely inconsequential. To ensure that the text generated by the language models ap peals to different personality types, we applied our predictive model (13) and assigned an “openness score” to each ad in the gen erated corpora. In both corpora, we found evidence that the prompts led to generating ads in the expected direction, e.g. high er openness scores with high-openness prompts). Study 2a: t(870) = 2.07, P = 0.039, Cohen’s d = 0.07, 95% CI (0.00, 0.14); study 2b: t(995) = 16.01, P < 0.001, Cohen’s d = 0.51, 95% CI (0.44, 0.57). In addition to the current algorithmic validation, we also validated a smaller sample using human raters; see supplementary material for details. We then selected 10 ad pairs from each model that maximized the difference between the members of each pair (see supplementary material) and ran the same experimental pipeline that was used for study 1 ( n2a = 803, n2b = 804). In study 2a (GPT-3), the matching score narrowly failed to reach statistical significance: b = −0.01, 95% CI ( −0.02, 3.43 e − 04), t(48173) = −1.91, P = 0.057. However, the effect was statistically ChatGPT). Matching effect: b = −0.03, 95% CI ( −0.04, −0.02), t(48233) = −4.25, P < 0.001, see Fig. 1 and supplementary material for details. Discussion Our findings indicate that political microtargeting is an effective technique and can be automated using off-the-self generative AI. While we show consistent efficacy across four studies, it is im portant to recognize that the demonstrated effect sizes are rather small. Nonetheless, small effect sizes can turn substantial at scale, and the automation of political microtargeting is pivotal to achieving such scale. To put this effect size in context, based on the change between the median mismatch and no mismatch between individuals and ads, we have simulated 100,000 re sponses. Given a cutoff of 3 (midpoint of the 5-point perceived per suasion scale), we find that out of every 100,000 individuals exposed to political messages tailored to their personality, 2,490 individuals would now be expected to be persuaded due to the style. By extrapolation, the change from the worst matching to the best matching would mean an increase to 11,405 individuals. Considering that elections are often decided by fractions of a per centage point, shifting a few thousand individuals out of 100,000 can substantially impact the results. One notable drawback of utilizing OpenAI products (such as ChatGPT) is their closed-source nature. In response to this con cern, we conducted an additional algorithmic validation of the stimuli. We replicated our algorithmic validation using an opensource model (Meta’s Llama 2 70-B), consistently reaching similar conclusions (see supplementary material for more details). Our study focused on the openness factor of personality as a test case, yet recent evidence with consumer products suggests that persuasive microtargeted messages generated by LLMs can go beyond targeting openness. There is also evidence that micro targeting can be successfully applied to moral reframing of polit ical texts (e.g. appealing to fairness or loyalty when advocating for climate action) (15). Unlike other malicious uses of ChatGPT, such as generating fraudulent emails or phishing websites, rephrasing inputs in a mi crotargeting manner does not violate OpenAI’s usage policies, and there are accordingly no built-in safeguards to prevent this usage of the model. Advertisers, political campaign managers, and all manner of interest groups currently have access to this technol ogy. In one sense, this is a democratization of capability, as any one may create targeted content. However, the benefits of targeting would be expected to accrue to those actors who are best-placed to deliver politically targeted content at scale to very large populations. And there is nothing to stop those actors to devise content that is untruthful or manipulative or both, cre ating the specter of “gaslighting” populations by exploiting indi vidual vulnerabilities. In light of the potential harm of large-scale microtargeted ma nipulation, one might be concerned that discussing and research ing manipulative strategies, as we did here, might inadvertently promote their use, potentially causing more harm. However, we believe that providing scientific evidence is essential for regula tors, policymakers, and the public to make well-informed deci sions about how to contain such risks. Although the possibility exists that malicious actors could misuse our research, we argue that the benefits of contributing solid, science-backed insights to the public discourse far outweigh this risk. Previous research has demonstrated that it is possible to detect further support the notion that these efforts are not only detect able but also effective and scalable. Given this understanding, there is a need for behavioral and cognitive science to concentrate on developing prevention methods through the design of inter ventions that enhance people’s ability to detect manipulation ef forts and make informed decisions in their online environments. The effectiveness of advertisements can significantly diminish when individuals become aware of unacceptable targeting practi ces, such as using external data or inferred information without their consent (16). Hence, one potential intervention could focus on informing people when microtargeting takes place. Leveraging a predictive model, e.g. (13), instances where personal ity matching appears suspiciously accurate could be identified, which could then flag potential manipulation to the user. The pro posed intervention would promote transparency and user-agency by alerting users when ads seem “too good to be true” in terms of personality match. Recent evidence suggests that simply provid ing information about one’s personality can be an effective boost ing technique in detecting microtargeted messages (17). Building upon this, we lay the groundwork for an extended intervention that combines information from both the individual and the ad, creating a more comprehensive approach that can be applied in ecological settings. While enhancing microtargeting awareness is valuable, it alone cannot fully address the challenges stemming from infor mation and power imbalances between platforms and users. To promote a fair and equitable digital landscape, it is essential to de sign an internet for democracy, prioritizing transparency and user empowerment over profit-driven corporations (18). Materials and methods Participants were recruited from a UK-based prolific sample. Study 1a: n = 440, Mage = 39.45, SDage = 12.58; study 1b: n = 804, Mage = 40.58, SDage = 13.04; study 2a: n = 804, Mage = 41.76, SDage = 13.69, study 2b: n = 803, Mage = 40.59, SDage = 13.62. The studies lasted for approximately 6 min, and participants received £1.8 in study 1a and £0.90 in studies 1b, 2a, and 2b as compensation. All studies were fully reviewed and approved by the School of Psychological Science Research Ethics Committee at the University of Bristol (ethics approval #12318 and 12883). All participants provided informed consent via mouse click prior to their participation. In all studies, participants were presented with 10 ads in a Facebook ad format. For each ad, they were requested to rate the perceived persuasiveness of the ad (on a 1–5 Likert scale), measured with six items adapted from Ref. (19) (see supplementary material for details). Scholars have questioned the use of self-reports for studying persuasion effects and sug gested using tangible behaviors like click-through rates (CTRs) in stead (12). However, whereas CTR is appropriate for studying consumer behavior, our focus is on endorsement of ideas and ideologies. In this attitudinal playing field, self-reports have a strong track record in microtargeted persuasion (15, 19). A metaanalysis revealed a reliable link between self-reported persuasion and measures of attitudes or intentions ( r = 0.41) (20), confirming the appropriateness of perceived persuasion ratings for our re search. In studies 2a and 2b, there were two variations of each ad; therefore, participants were randomly assigned to either a low-openness or high-openness version of each ad (10 ads al together out of 20). At the end of the survey, participants were re quested to answer 12 items of the openness to experience Prompting strategy We followed a specific process to create two versions of each ad in study 2. First, we provided GPT-3 and ChatGPT with the definition of openness to experience: Author Contributions A.S., M.E., and S.L. conceptualized the research. A.S. performed the data collection and analysis. A.S. and S.L. wrote the original draft of the manuscript. One way in which people differ from each other is through their personalities. The Big-Five Personality Model identifies five per sonality traits on which each individual varies. One of the traits is Openness to Experience. Openness to Experience has both mo tivational and structural components. People high in Openness to Experience are motivated to seek new experiences and to en gage in self-examination. People low in Openness to Experience by contrast are more comfortable with familiar and traditional experiences. Data Availability All shareable data and code are available at https://osf.io/5w3ct. Preprints This manuscript was posted on a preprint https://doi.org/10. 31234/osf.io/62kxq. Then, we made a request in the following manner: Please rephrase the next ad so it would appeal to people high/low on Openness to experience. In our ChatGPT (gpt-3.5-turbo; February 2023) prompt, we in cluded the additional instruction: Do not oversell, but make it slightly more persuasive to people who are high/low on Openness to experience. As a result, each prompt was constructed in a structured order: openness definition, instruction, and the original ad. See supplementary material for more details. Open science statement Studies 1a, 2b, and 2a were preregistered at https://aspredicted. org/4Q1_LTB, https://aspredicted.org/XWR_3JS, and https:// aspredicted.org/PZ1_K5B, respectively. Study 2b was not preregis tered. Our hypotheses and target sample size remained un touched. However, the preregistered analysis plan relied on binary classification of the ads. We deviated from this plan and in corporated a continuous measure of ad openness, which allowed us to simplify the model and focus on the matching score metric. In the supplementary material, we report all the preregistered analyses and show overall convergence with the current method. Full reproduction materials are accessible at https://osf.io/5w3ct. Note a An error was identified in the design, where one of the ads appeared twice instead of another ad. As a result, the findings of study 1a are limited to a set of nine ads. This issue was rectified in study 1b. Acknowledgments The authors thank Mattan Ben-Shachar for his assistance. Supplementary Material Supplementary material is available at PNAS Nexus online. Funding This research was supported by a large grant from the Volkswagen Foundation (“Reclaiming individual autonomy and democratic discourse online”). S.L. was also supported by funding from the Humboldt Foundation in Germany and by the European Research Council (ERC) Advanced Grant (101020961, References 1 Cadwalladr C. 2020 Jan. Fresh Cambridge analytica leak ‘shows global manipulation is out of control’. The Guardian. 2 Digital, Culture, Media and Sport Committee. Disinformation and ‘fake news’: final report. Technical Report, House of Commons, UK Parliament, 2019 [accessed 2020 Apr 21]. https:// publications.parliament.uk/pa/cm201719/cmselect/cmcumeds/ 1791/179102.htm. 3 Machkovech S. 2017. Report: facebook helped advertisers target teens who feel “worthless” [accessed 2020 Apr 12]. https:// arstechnica.com/information-technology/2017/05/facebook- helped-advertisers-target-teens-who-feel-worthless/. 4 Hinds J, Joinson AN. 2018. What demographic attributes do our digital footprints reveal? A systematic review. PLoS One. 13: e0207112. 5 Youyou W, Kosinski M, Stillwell D. 2015. Computer-based per sonality judgments are more accurate than those made by hu mans. Proc Natl Acad Sci U S A. 112:1036–1040. 6 Baldwin-Philippi J. 2017. The myths of data-driven campaigning. Polit Commun. 34:627–633. 7 Coppock A, Hill SJ, Vavreck L. 2020. The small effects of political advertising are small regardless of context, message, sender, or receiver: evidence from 59 real-time randomized experiments. Sci Adv. 6:eabc4046. 8 Haenschen K. 2023. The conditional effects of microtargeted facebook advertisements on voter turnout. Polit Behav. 45(4): 1661–1681. 9 Lavigne M. 2021. Strengthening ties: the influence of microtargeting on partisan attitudes and the vote. Party Polit. 27:965–976. 10 Tappin BM, Wittenberg C, Hewitt LB, Berinsky AJ, Rand DG. 2023. Quantifying the potential persuasive returns to political micro targeting. Proc Natl Acad Sci U S A. 120:e2216261120. 11 Joyal-Desmarais K, et al. 2022. Appealing to motivation to change attitudes, intentions, and behavior: a systematic review and meta-analysis of 702 experimental tests of the effects of motiv ational message matching on persuasion. Psychol Bull. 148(7–8): 465–517. 12 Matz SC, Kosinski M, Nave G, Stillwell DJ. 2017. Psychological tar geting as an effective approach to digital mass persuasion. Proc Natl Acad Sci U S A. 48:12714–12719. 13 Simchon A, Sutton A, Edwards M, Lewandowsky S. 2023. Online reading habits can reveal personality traits: towards detecting psychological microtargeting. PNAS Nexus. 2(6):gad191. 14 Soto CJ, John OP. 2017. The next big five inventory (BFI-2): devel enhance bandwidth, fidelity, and predictive power. J Pers Soc Psychol. 113(1):117–143. 15 Matz S, et al. 2023. Apr 22. The potential of generative AI for per sonalized persuasion at Scale. https://doi.org/10.31234/osf.io/ rn97c. 16 Kim T, Barasz K, John LK. 2018. Why am I seeing this ad? the ef fect of ad transparency on ad effectiveness. J Consum Res. 45(5): 906–932. 17 Lorenz-Spreen P, et al. 2021. Boosting people’s ability to detect microtargeted advertising. Sci Rep. 11(1):15541. 18 Lewandowsky S, Pomerantsev P. 2022. Technology and democ racy: a paradox wrapped in a contradiction inside an irony. Mem Mind & Media. 1:e5, 1–9. https://doi.org/10.1017/mem.2021.7. 19 Hirsh JB, Kang SK, Bodenhausen GV. 2012. Personalized persua sion: tailoring persuasive appeals to recipients’ personality traits. Psychol Sci. 23(6):578–581. 20 Dillard JP, Weber KM, Vail RG. 2007. The relationship between the perceived and actual effectiveness of persuasive messages: a meta-analysis with implications for formative campaign re search. J Commun. 57(4):613–631. Report: Global Policy Outlook Series: Societal Polarization & Nationalism, H2 2024 Author: McMann, Jason I., Date: December 2024 Collections: Hot Takes US Elect 2024, Polarization Zotero Key: ZKJU9QSV Cite Key: McMann24globalPolarizNationlsmImmigTrust Zotero Item | Lit Note V, MORNING CONSULT’ Global Societal Polarization & Nationalism Outlook Incisive, data-driven analysis for corporates, market actors and policymakers DECEMBER 2024 TE About Morning Consult Morning Consult Morning Consult is a global decision intelligence company changing how modern leaders make smarter, faster, better decisions. The company pairs its proprietary high-frequency data with applied artificial intelligence to better inform decisions on what people think and how they will act. POLITICAL INTELLIGENCE Daily tracking of political attitudes in 43 global markets Every day, Morning Consult surveys thousands of respondents across 43 countries to assess public views of incumbent political leaders, country trajectories, and attitudes toward other countries and international organizations. Daily syndicated tracking of political attitudes in the following markets: - Argentina - Ireland - Russia Our Political Intelligence solution — encompassing 6+ years • Australia • Israel • Saudi Arabia - Austria - Italy - Singapore of trended data across dozens of markets and hundreds of • Belgium • Japan • South Africa - Brazil - Malaysia - South Korea demographics — provides clients across the public and - Canada - Mexico - Spain private sectors with data at the scale and speed needed to • Chile • Netherlands • Sweden - China - Nigeria - Switzerland navigate the changing political and geopolitical landscape in • Colombia • Norway • Thailand - Czech Republic - Pakistan - Turkey real-time. - Egypt - Peru - LEARN MORE - France - Philippines - United Kingdom - Germany - Poland - United States LEARN MORE 2 - India - Romania - Vietnam POLICY INTELLIGENCE Monthly tracking of policy sentiment in 19 markets Decision intelligence for a new political era Unprecedented political polarization, the rise of extreme parties, and outsider political leaders with a penchant for populism are reshaping the global policy landscape. For multinationals engaged in scenario planning, financial services firms seeking to [S Market coverage includes: forecast policy-driven market dynamics, and public sector entities managing political challenges abroad, data solutions that enable • Argentina • India • South Korea - Australia - Italy - Spain real-time assessments of public views on key policy issues are - Brazil - Japan - Turkey needed to stay ahead of the curve. • Canada • Mexico • United Kingdom - China - Nigeria - United States - France - Russia Morning Consult’s Policy Intelligence solution provides • Germany • South Africa unprecedented insight into trends in policy-relevant sentiment Issue coverage includes: across 8 thematic issue areas, 19 of the world’s largest markets, and hundreds of demographics, enabling clients to make critical • Business & regulatory climate • Policy priorities - Trade & investment policy climate - ESG, sustainability & climate change business, investment and policy decisions with greater speed and - Social policy & labor market dynamics - Foreign policy & nationalism rigor than ever before. • Trust & governance • Threat perceptions About This Report Series Morning Consult’s Global Policy Outlook Series provides biannual snapshots of the current state of global sentiment on key issues at the intersection of politics and economics across 19 of the world’s largest markets. Corporate decision-makers, risk management and financial services professionals, and public sector entities rely on this report series to understand how public sentiment is trending across critical policy areas and make business, investment and public sector decisions accordingly. IN THIS REPORT 7 Topical Outlook 7 Six-Month Forecast 8 Macro Outlook 9-12 Topical Findings 13 Methodology 14 About the Author SIX-MONTH FORECAST Societal Polarization & Nationalism While national pride remains pervasive globally, declines in major Notably, it's the world’s largest markets — the United States and China — developed markets seeing recent government instability or turnover — who lead the pack (alongside Brazil) when it comes to the six-month including France, Germany, and the United States — have driven a dip in uptick in support for isolationism, with the public in both markets showing the global average. Relatively low pride in peer markets (Japan and the a roughly six-point increase in support. United Kingdom) is meanwhile helping to pin the average down. The former is in line with prevailing expectations for the incoming Trump Over H2 2024, this decline coincided with an increase in nativist administration, which will see majority (56%) support for a more sentiment, marked by noteworthy upticks in the average share of global isolationist foreign policy, conducive to tariff imposition, a reduction in adults espousing isolationist and anti-immigrant views (both up two points U.S. support for NATO and other international institutions, and over the past six months). The average share of adults who say they trust Washington's general retrenchment on the world stage. their compatriots is meanwhile down one point. Per our data, opposition China meanwhile displays the most pronounced public support for to immigration and distrust in one’s fellow countrymen tend to go hand in isolationism of all markets we track, with 71% in favor. The metric suggests hand, with the markets showing large declines in support for immigration Beijing will continue to see public buy-in for its efforts to insulate the also commonly showing large dips in societal trust. country from external headwinds by building up national champions and Collectively these dynamics affirm the narrative that global publics are bolstering domestic demand despite near-term economic challenges. It is broadly turning inwards heading into 2025. Governments will face also conducive to retaliation in the renewed U.S.-China trade war heightened pressure to adapt their policymaking in turn. anticipated for 2025, a scenario we advise treating as a fait accompli. 6-month trajectory of public views on nationalism and societal polarization Percentage point change in the shares of adults in each country who ... 6-month trajectory of public views on nationalism and societal polarization Percentage point change in the shares of adults in each country who ... Are proud to live in their country +2.6 3.5 +14 -2. Are proud to live in their country +2.6 +14 -2. -2. Think their country should be more isolationist -1.4 +1.2 +1.7 1 #2 #19 16 Think their country should be more isolationist RB +14 +26 +23 ).. 19 +1.2 +1.7 3.5 <5 16 -1.4 1 16 Support more — “4 +1 a )* 39 -3.7 +25 4 <.5 il Fy: -09 18* Trust compatriots 1.2 -24 <5 i kL - <b <5 <5 -07 -06 +1.8 Col1 Col2 Col3 Col4 Col5 Col6 Col7 Col8 Col9 Col10 Col11 Col12 Col13 Col14 Col15 Col16 Col17 Col18 Col19 Col20 Col21 Col22 Col23 Col24 Col25 Col26 Col27 Col28 Col29 Col30 Col31 Col32 Col33 Col34 Col35 Col36 Col37 Col38 Col39 Col40 1|9|1|6|||||||||||=||||||)..||RB||| ||- 1|. 4|+ 1|. 2|||+ 1|. 7|||||||||||||+1|4|+1|4|+2|6|+2|3|||2|2|<|5|||||| ||||||||||||||||||||||||||||||||||||||||| ||||“ 4||+|1|a||||) ||3|9|- 3|. 7|+ 2|5|4||< .|5|i l||||F y|:|- 0|9|1|8|||<|5|1|7|| ||||||||||||||||||||||||||||||||||||||||| |1 .|1 .|2|- 2|4|<|5|i|||k L|k L||>||<|b|<|5|<|5|- 0|7|- 0|6||+1|+1|.8|||||||||||| ||||||||||||||||||||||||||||||||||||||||| Support more — Trust compatriots 1.2 +1.8 2 -06 Fy: 4 “4 -24 -07 <5 <.5 <b <5 +1 kL 1 Fy: 18 17 Reported values derive from surveys conducted in May 2024 and November 2024, among representative samples of roughly 1,000 adults per country, with unweighted margins of error of +/-3 percentage points. Reported values derive from surveys conducted in May 2024 and November 2024, among representative samples of roughly per country, with = -09 <5 il 1,000 adults National pride is relatively subdued in developed markets with recent government instability Shares of respondents who agree or disagree that they are proud to live in their country Latest month (shares by country) Trend line (cross country average) National pride is relatively subdued in Shares of respondents who agree or disagree that they are proud to live in their country Trend line (cross country average) Hl Strongly/Somewhat disagree Strongly/Somewhat agree Don't know/No opinion Argentina Australia Canada Mexico Nigeria Japan +0.7pp evil 60% 40% 7 italy EET [34 9% Russia [INE 5; ET South Africa South Korea 7% Spain Turkey 8% United Kingdom 11% United States 12% Surveys conducted in November 2024, among representative samples of roughly 1,000 adults per country, each with a margin of error of up to +/-3 percentage points. Figures may not add up to 100% due to rounding with recent government instability Latest month (shares by country) Strongly/Somewhat agree (6-month trajectory) South Korea Germany France Russia -0.8pp India 33% 7 15% so; S-; ET 7% (o] - & I ~ ~ ~ ~ Nd ~ ~ Nd ~ a 1 A s 3 QE LF ¥ EF ESE YY Fg of Surveys conducted monthly among representative samples of roughly 1,000 adults per country, each with a margin of error of up to +/-3 percentage points. Trend lines derive from simple averages. Chart labels for six-month trajectory values below half a percentage point are omitted country) Col2 Col3 Col4 Col5 Col6 Col7 Col8 Col9 Col10 Col11 Col12 Col13 Col14 Col15 Col16 Col17 Col18 Col19 Col20 Col21 Col22 HM Strongly/Somewhat agree HM Don't know/No opinion H l S t r o n g ly / S o m e w h a t d is a g r e e A r g e n t in a Australia evil 0 0S 0 0 0 0 0 6 2 % 0 [ 3 4 3 3o %E Canada I N E 7 s ; E T China France 9% G e r m a n y 9 % India I 7 E it a ly S - ; Japan 15% Mexico Nigeria[ Russia I N E 5 ; E T South Africa South Korea 7% Spain T u r k e y 8 % U n it e d K i n g d o m 1 1 % United [ 0 7 S E N I ; o E N I 8 % 12% developed markets Surveys conducted monthly among representative samples of roughly — Strongly/Somewhat disagree for six-month trajectory values below half a percentage point are omitted United Kingdom Surveys conducted South Africa of error of up to +/-3 percentage country, 1,000 adults China a margin to rounding (o] 12% 1,000 adults 9% 8% per Chinese views are decidedly isolationist as global sentiment turns inward Shares of respondents who agree or disagree that their country should remain separate from other countries' problems and affairs Latest month (shares by country) Trend line (cross country average) decidedly isolationist as global sentiment turns inward Chinese views are Trend line (cross country average) Hl Strongly/Somewhat disagree Strongly/Somewhat agree Don't know/No opinion Argentina Australia Canada Mexico France Japan +2.1pp 20% India ; -1.5pp italy 23% 10% 12% 13% 1% 8% 7% 7% Russia 18% South Africa 10% South Korea 10% Spain 8% Turkey 13% United Kingdom 15% United States 16% Surveys conducted in November 2024, among representative samples of roughly 1,000 adults per country, each with a margin of error of up to +/-3 percentage points. Figures may not add up to 100% due to rounding Shares of respondents who agree or disagree that their country should remain separate from other countries' problems and affairs Latest month (shares by country) Strongly/Somewhat agree South Africa Germany Nigeria Russia Brazil 60% 10% 13% 7% (6-month trajectory) 0% \\a \\a Na & I ~ ~ ~ ~ Nd ~ ~N Nd Ka a 1 “ © © & A $s 3 QE LF ¥ EF ESE YY Fg of Surveys conducted monthly among representative samples of roughly 1,000 adults per country, each with a margin of error of up to +/-3 percentage points. Trend lines derive from simple averages. Chart labels for six-month trajectory values below half a percentage point are omitted country) Col2 Col3 Col4 Col5 Col6 Col7 Col8 Col9 Col10 Col11 Col12 Col13 Col14 Col15 Col16 Col17 Col18 Col19 Col20 Col21 Col22 HM Strongly/Somewhat agree HM Don't know/No opinion H l S t r o n g ly / S o m e w h a t d is a g r e e A r g e n t in a 8 % Australia 12% B razil 1 % Canada 13% China 7% France 13% G e r m a n y 1 0 % India 7% italy 1% J a p a n 2 3 % Mexico 7% Nig eria Ru ssia 1 8 % South Africa 10% South Korea 10% S pain 8 % T u r k e y 1 3 % U n it e d K in g d o m 1 5 % United States 8 % 13% 1 0 % 2 3 % % 8 1 % 8 16% points. Trend lines derive from simple averages. Chart labels points. Figures may not add up to 100% due among representative samples of roughly Don't know/No opinion United Kingdom Surveys conducted South Korea of error of up to +/-3 percentage country, 1,000 adults China a margin 40% ° 10% 15% 1,000 adults 13% 8% per Anti-immigrant sentiment is meanwhile on the upswing globally Shares of respondents who agree or disagree that more immigration would be a good thing for their country Latest month (shares by country) Trend line (cross country average) Anti-immigrant sentiment is meanwhile on the upswing globally Trend line (cross country average) Hl Strongly/Somewhat disagree Strongly/Somewhat agree Don't know/No opinion Argentina Australia Canada France Japan +1.8pp 60% 40% 54% Italy 15% 16% 13% 11% Mexico 33% 12% Nigeria 35% 8% South Africa 25% 9% South Korea 1% Spain 40% 9% Turkey IKEZEN 10% United Kingdom 18% United States 18% Surveys conducted in November 2024, among representative samples of roughly 1,000 adults per country, each with a margin of error of up to +/-3 percentage points. Figures may not add up to 100% due to rounding Shares of respondents who agree or disagree that more immigration would be a good thing for their country Latest month (shares by country) Strongly/Somewhat agree South Korea Germany Mexico Brazil India 10% 16% 13% 11% (6-month trajectory) 0% Na a Ne & I ~ ~ ~ ~ Nd ~ ~N Nd Ka a 1 A s 3 QE LF ¥ EF ESE YY Fg of Surveys conducted monthly among representative samples of roughly 1,000 adults per country, each with a margin of error of up to +/-3 percentage points. Trend lines derive from simple averages. Chart labels for six-month trajectory values below half a percentage point are omitted country) Col2 Col3 Col4 Col5 Col6 Col7 Col8 Col9 Col10 Col11 Col12 Col13 Col14 Col15 Col16 Col17 Col18 Col19 Col20 Col21 HM Strongly/Somewhat agree HM Don't know/No opinion H l S t r o n g ly / S o m e w h a t d is a g r e e Argentina 13% Australia 16% Brazil 1 6 % Canada 1 3 % France 15% Germany 11% I n d ia 5 4 % 1 0 % Italy 11% Japan 22% M e x ic o 3 3 % 1 2 % N ig e r ia 3 5 % 8 % SS oou u tthKore h A f r ic a 2 5 % 91% % a Spain 40% 9% Turkey IKEZEN 10% United Kingdom 18% U n ite States d 18 13% % 3 1 5 4 % 1 0 3 3 % 1 2 3 5 % 8 % 40% 9% % 8 1 points. Trend lines derive from simple averages. Chart labels points. Figures may not add up to 100% due among representative samples of roughly Don't know/No opinion United Kingdom Surveys conducted South Africa of error of up to +/-3 percentage country, adults per a margin -1.6pp 40% 1,000 0% IKEZEN 18% 9% 1,000 adults Amid an ongoing global turn toward nativism, societal trust continues its secular decline Shares of respondents who say they trust other people in their country Amid an ongoing global turn toward nativism, societal trust continues its secular decline Latest month (shares by country) Trend line (cross country average) Hl Trust a lot/Trust some = Trust a lot/Trust some Shares of respondents who say they trust other people in their country Trend line (cross country average) Hl Trust a lot/Trust some HM Trust a little/Don't trust at all = Trust a little/Don't trust at all Latest month (shares by country) Trust a little/Don't trust at all Trust a lot/Trust some United Kingdom South Korea Argentina Australia Canada Nigeria Turkey Japan Brazil 60% -0.7pp 26% 58% 56% 42% 45% Italy 35% JIE 47% 41% United States [NGTGTcNINTNGzGzGGEEEEEEE Trust a little/Don't trust at all South Africa Germany Mexico 20% 40% 50% 46% 38% 26% (6-month trajectory) a vy ¥ FTF F RY ~ a & 5 so 4 ES 5 & LF ¥ E&I FEY vy ¥ 4 oo 2 |Latest month (shares by country)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |H l T r u s t a lo t / T r u s s o m t e H M T r u s t a lit t le / D o n 't t r u s t a t a ll d [IEG ; Argentina E E — Australia 38 % Brazil 4 7% Canada 41% France r r ] 5 6% |= Germany 50% India 2 6 % Italy 40% Japan 46% Mexico %3 5 % N ig e r ia 2 3 % 4 2 S ou thAfrica 53% South Korea s— 5 55% spain [IIE JIE 26% d Turkey 58% United Kingdom [EEE 45% h 49% U n ite States d N [ G T G T c N IN T N G z G zG G EE E E EE E|||||||||||||||| ||||||||||||||||d| |||[IEG|||||||||||||;| |||— E E|||||||||||||| ||||||||||||||% 8 3||| ||||||||||||||||| |||||||||% 7 4|||||||| ||||||||||||||||| ||||||||||||% 1 4||||| ||||||||||||||||| |||] r r||% 6 5|||||||||||| ||||||||||||||||| ||||=|||||50%||||||||| ||||||||||||||||| ||||||||||||||||% 6 2| ||||||||||||||||| |||||||||||||40%|||| ||||||||||||||||| |||||||||46%|||||||| ||||||||||||||||| |||||||||||||||% 5 3|| ||||||||||||||||| |||2 3 %||||||||4 2 %|||||| ||||||||||||||||| |||||||% 3 5|||||||||| ||||||||||||||||| |||s— 5|||55%||||||||||| ||||||||||||||||| |||[IIE JIE|||||||||||||26%| ||||d||||||||||||| ||||58%||||||||||||| ||||||||||||||||| |||[EEE|||||||45%||||||| ||||||||h||||||||| |||E E E E E E E G G z G z G N T IN N c T G T G N [|||||49%||||||||| (6-month trajectory) United States France spain India 40% 49% Surveys conducted in November 2024, among representative samples of roughly 1,000 adults per Surveys conducted monthly among representative samples of roughly 1,000 adults per country, each with country, each with a margin of error of up to +/-3 percentage points. Figures may not add up to 100% due a margin of error of up to +/-3 percentage points. Trend lines derive from simple averages. Chart labels points. Trend lines derive from simple averages. Chart labels points. Figures may not add up to 100% due per country, each with of error of up to +/-3 percentage in November 2024, each with a margin a margin 26% 55% 1,000 to rounding for six-month trajectory values below half a percentage point are omitted Surveys conducted monthly among representative samples of roughly for six-month trajectory values below half a percentage point are omitted of error of up to +/-3 percentage adults per 5 NGTGTcNINTNGzGzGGEEEEEEE Methodology Sampling and data collection This report relies on data collected through Morning Consult’s proprietary survey research capabilities. Data comes from monthly surveys conducted among representative samples of roughly 1,000 adults per country, with unweighted margins of error of +/-3 percentage points. All interviews are conducted online. Surveys field during the second half of the indicated month. Data is weighted to approximate representative samples of adults in each country surveyed. Weighting parameters vary by country. Consult our Global Political Intelligence Methodology Primer for additional details on sampling and data collection procedures, weighting and representativeness, margins of error, and question wording. Cross-country averaging All cross-country trend lines presented in this report derive from simple averages. Heatmap values and temporal coverage For heatmaps indicating percentage point changes — corresponding to the change from May 2024 to November 2024 — values ranging from -0.5 to +0.5 are denoted as “<0.5.” Corresponding heatmap color-coding indicates whether the change is positive or negative, with green cells indicating a positive change and pink cells indicating a negative one. Data is weighted to approximate representative samples of adults in each country surveyed. Weighting parameters vary by country. for additional details on sampling and data collection procedures, weighting and representativeness, margins of samples of roughly 1,000 adults per country, with unweighted margins of error of Heatmap values and temporal coverage Sampling and data collection change and pink cells indicating a negative one. our Global Political Intelligence Methodology Primer +/-3 percentage points. All interviews are conducted -0.5 to +0.5 are denoted as “<0.5.” Corresponding heatmap half of the indicated month. Consult 2024 — — ABOUT THE AUTHOR Jason I. McMann leads the political analysis team and syndicated political data commercialization at Morning Consult. He leverages the company’s high-frequency survey data to help clients integrate data-driven political risk assessments into their decision-making processes, including through scenario planning, forecasting, and event-based modeling. Prior to joining Morning Consult, Jason served as Head of Analytics at GeoQuant, now part of Fitch Solutions, where he led the company’s efforts to model the market impacts of political risk as well as the analysis of global trade and investment policy risks. He also led country JASON I. MCMANN, PHD risk analysis for South Asia and Southeast Asia. Head of Political Intelligence Jason has worked with a range of nonprofit and development organizations — including the World Bank, the International Finance Corporation and the Natural Resource Governance Institute — on regulatory benchmarking projects in emerging markets. He earned his Ph.D. with a specialization in international political economy from Princeton University’s Politics Department. He holds an M.A. in international relations from the University of Chicago and a B.A. from New York University. i LEARN MORE FOLLOW US = MEDIA & SPEAKING INQUIRIES MorningConsult.com @MorningConsult press@morningconsult.com VM MORNING CONSULT’ How Journalism Will Adapt in the Age of AI Author: John Micklethwait, Date: 2025-01-10 Collections: MediaAdsPolit, MisDisinformation Zotero Key: 35IG2JC4 Cite Key: Micklethwait25journalismAIadapt Zotero Item | Lit Note We journalists are by nature a pretty paranoid lot. Permanently worried that somebody somewhere — governments, lawyers, our colleagues, the IT department — is about to do something terrible to us, or our copy. So far the 21st century has only fed that paranoia. Back in 2006, one of my first covers as editor of The Economist was entitled “Who Killed the Newspaper?” At the time the internet was wrecking the cozy business model of most big city papers that relied on their monopoly of classified advertising. Looking back, though, it was less a case of assassination than suicide. Far too many quality media brands fell for the tech-rhetoric that “legacy media” was dead and that content should be free. Soon they were stuck in a vicious circle of chasing clicks, cutting costs and gradually handing over their business to the tech giants. But eventually sense prevailed, people started to charge for journalism and legacy media began to recover. The New York Times, which had only 500,000 digital subscribers when Mark Thompson arrived in 2012 and focused the Grey Lady on selling subscriptions, now has more than 10 million paying customers. The “content is free” sirens who lured so many great names onto the rocks have shut up; the new challengers like The Information, Puck and (despite its name) The Free Press make sure people pay sooner or later. And yet, just as the quality press has come to terms with the internet and social media, along comes another even bigger change: artificial intelligence. AI promises to get under the hood of our industry — to change the way we write and edit stories. It will challenge us, just like it is challenging other knowledge workers like lawyers, scriptwriters and accountants. How exactly will this revolution unfold? Before I make my predictions, a little personal humility is in order. When I became editor of The Economist, I had no idea that a company called Twitter had been founded 10 days before; yet by the time I came to Bloomberg nine years later, Twitter was in effect the world’s biggest newspaper. So beware any editor (including this one) peddling certainties. But I will submit that our newsroom at Bloomberg is quite a good laboratory to look for clues as to how this revolution might progress. Partly because we use more technology, including early versions of AI, than anywhere else. Out of the 5,000 stories we produce every day there is some form of automation in more than a third of them. And partly because our audience is close to the demanding news consumer of the future. Our readers will trade millions of dollars on the basis of what we write. So accuracy and lack of bias is key for them, but so is time. Our readers, viewers and listeners hate it when we waste their time — and as we shall see, saving time is a key part of what AI offers. Here are two examples of what AI can already do. The first is a report we published which showed how oil is being smuggled out of Iran and transferred from ship to ship. Those involved go to all sorts of lengths to avoid being caught doing this — so we built an algorithm that looked at satellite images of ships to detect when two vessels were next to each other. On the 566 days when skies were clear between early January 2020 and Oct. 4, 2024, we found 2,006 of these suspicious side-by-side formations — which our journalists could then investigate. AI is really good at pattern recognition — sorting through a big pile of images or documents or data to tell a story when the pile is too large and too fuzzy for a human to do it. Our data journalism chief, Amanda Cox, says her favorite analogy for large language models is “infinite interns.” You don’t always totally trust the results they bring, but, just like human interns, the machines keep on getting better every day: from toddler-level intelligence in 2020 to something close to PhD-level intelligence, at least when it comes for specific tasks, with the next iterations of ChatGPT and its ilk. Most journalists love AI when it helps them uncover Iranian oil smuggling. Investigative journalism is not hard to sell to a newsroom. The second example is a little harder. Over the past month we have started testing AI-driven summaries for some longer stories on the Bloomberg Terminal. The software reads the story and produces three bullet points. Customers like it — they can quickly see what any story is about. Journalists are more suspicious. Reporters worry that people will just read the summary rather than their story. To which the honest answer is: Yes, a reader might well do that, but would you prefer that they wasted their time skimming through paragraphs on a topic that they are not actually interested in? To me it’s pretty clear; these summaries, used correctly, both help readers and save time for editors. So, looking into our laboratory, what do I think will happen in the Age of AI? Here are eight predictions. Let’s look at a simple example — covering company earnings announcements. When I first came to Bloomberg, there was a “Speed” team of fast-fingered journalists who specialized in banging out headlines, hoping to beat our closest rivals by a few seconds. Then automation appeared — computers that could scour a company’s press release in fractions of a second. People feared for their jobs. But the machines needed humans. First to tell them what to look for — the number of iPhones sold in China could matter more to Apple’s share price than the actual income. And the machine also needs humans to look for and interpret the unexpected — the sudden resignation of a CEO, for instance, could be meaningful or not. We still employ roughly the same number of people to look at earnings, but the number of companies whose earnings we cover and the depth of the coverage around those announcements have both increased dramatically. And, I would argue, the job has also become more interesting; it’s not just about fast typing but working out what matters. The same could well happen with AI — multiplying the amount of content that we produce. For instance, a stretched bureau might not have enough time to provide its readers with an explainer on the fall of Assad in Syria; but what if you could run four of your current news stories through an algorithm? In seconds you would have a crude draft of an explainer for a journalist to work on. Another obvious multiplier of content is automatic translation — more pieces will reach more readers, and more journalists at big global organizations will be able to write in their own language. The value of news shows no sign of declining — with political changes now matching economic ones in their worth. Each time we reveal a policy shift in Washington, Paris or Beijing you can see currency markets jump. But crucially the amount of time that this counts as news keeps coming down. In terms of the big set piece announcements — like, say, jobs figures — it has long been down to fractions of a second, and our competition is often hedge funds who are using AI of their own to look at the numbers as quickly as we do. In terms of a news story about an unexpected event, like a takeover or a CEO resignation, it is much harder to measure, but I would hazard an unscientific guess that the time it takes for prices to move has collapsed from several seconds to milliseconds in my time at Bloomberg. AI is going to speed that process up still faster — and universalize it. A lot depends on how copyright deals are sorted out, but the chances are that ever more news, as it appears, will be immediately ingested into machines like ChatGPT that consider more than just one market — and added to what might be called immediate general knowledge. It will be available to everyone, or at least a much broader set of people than now. One of the basic points about most of the things that I have mentioned so far is that you need reporting. An AI summary is only as good as the story it is based on. And getting the stories is where the humans still matter. The machine can’t persuade a cabinet minister to tell you that the chancellor has just resigned; it can’t take a chief executive for lunch; it can’t write an original column or cajole an interviewee into admitting something on air. Crucially, a newsroom will still need boots on the ground. Especially in a world where you can no longer presume that an emerging country like Indonesia or India is going to follow Western models of freedom, and where many countries are trying to clamp down on reporting, you will need people who know people. Illustration: Karan Singh for Bloomberg Break down most editing jobs into a series of skills. Begin with managing a team of journalists: You will be unsurprised to know that I smugly still think newsrooms will need people like me. Next, commissioning a story: Again, I think that will remain mainly a human skill — although at Bloomberg we already use AI to prompt us to consider writing a story (pointing out that a share price has jumped or social media is talking about an explosion). However, once the story has been delivered and we are in the actual process of changing words on screens, I think you will see AI tools coming into play more and more, restructuring and rewriting drafts, checking facts and so on. Again, I am not talking about New Yorker level editing. But a lot of news reporting is more formulaic. Consider for instance a sports report on a football match. In five years’ time, a British journalist could file her piece on a match at the King Power Stadium to her editor in London. A second later, both she and her editor will get an edited version: It will have been checked for both spelling and house style; there will be queries alongside dodgy assertions (why did the reporter claim that Liverpool dominated the match when Leicester in fact had 51% possession?); photographs and video segments will have been added — and links to the four Leicester players who scored goals. At this point my example is probably becoming unbelievable on multiple different levels, especially to anybody who follows football. But I think you can see how AI probably will change editing jobs more than reporting ones. As mass summarization tools like ChatGPT and Perplexity suck in ever more stories, they are using them to construct answers. You can already see that when you ask Google a question. Rather than getting a long string of links to other stories, you get an answer that runs to a couple of sentences, sometimes close to a paragraph. My colleague Chris Collins, who heads up our product team for Bloomberg News, says that search as we know it could disappear. That will make an enormous difference to anyone whose business relies on search advertising — and counting eyeballs. At the moment, when a reader clicks on a link, the publisher may receive a few cents from an advertiser. But as you get an ever-longer answer from the search engine (or rather answer engine), then those clicks will stop. This is yet another reason why building a sustainable subscription business — and investing in long-term relationships with a committed set of readers — is so important for serious news publications. It is also a prompt to sort out copyright; we plainly need much more clarity over what can and can’t be used free of charge from our courts and legislators. If you discuss AI with journalists, it’s likely that somebody will mention hallucinations — the idea that the machine will invent a story or be hoodwinked into inventing one. There will inevitably be a degree of trial and error about AI, and there is no shortage of people who think they can gain commercial or political advantage by scamming us. My hunch is that for the foreseeable future, the main danger is AI being used to generate fake video or audio images that distort or malignly amplify an event that actually happened, rather than inventing completely fake events. A lot of this is about the interplay between humans and machines. A few years ago, I followed the way that our breaking news team dealt with a subway shooting. They were prompted by social media that something bad had happened. And you could see the electronic chatter rapidly increase, but they would confirm it only once they had a human source that they trusted — in this case an eyewitness who was on the scene. By contrast, video and audio is much harder to confirm. With the subway shooting a grisly photograph of an apparently dead person appeared on social media. But was it real? Had it been made up? At speed this is harder to verify. You have to check the picture against photographs of the subway station, inspect it to see whether pixels have been moved, and so on. Perhaps AI will make it easier to root out fraudulent audio and video, but so far most of the examples I have seen are of ever more elaborate fakes. One footnote to this though. In terms of “fake news,” it is notable that those regimes that long peddled lies now tend to specialize in obscuring the truth in a cloud of fake information rather than insisting on a single untruth. In the old days, for instance, Pravda would simply state a lie — and then repeat it. Now, when something happens that the Kremlin does not like (like an airliner being shot down or a battle being lost), Russia’s army of bots generate a multiplicity of possible outcomes. The main objective is to confuse. This again is a hunch. Personalization has been the holy grail of digital journalism. Imagine if you only got the news you needed: your own personal newspaper. So far it has happened only rather clumsily. Many people don’t like handing over their details to news organizations — even if it would appear to be in their interest to do so. Some readers get creeped out when you suggest things to them. They worry about being stuck in opinion ghettoes. They miss that element of serendipity — the story that you didn’t know you would be interested in. It is the difference between visiting an old-fashioned bookshop, where you can browse and stumble upon an interesting novel, and being fed suggestions by Amazon. AI will begin to crack this puzzle. Algorithms are good at working out what you might be interested in — in spotting patterns that people don’t see themselves. The infinite interns will be able to make connections more painlessly than those rather random “news for you” boxes that either give you too much or mean you miss out on the thing everybody else is talking about. This predictive personalization of content comes with a dark side. The same algorithms that predict that we might like a gardening course can also lead a teenager who has just been dumped by his girlfriend towards videos about suicide. At the moment, social media companies are not liable for the content on their networks in the same way that an editor like me would be. Thanks to rules like America’s infamous “Section 230,” the tech giants are treated as if they were more like telephone companies than media companies. They are responsible for the wires, but not what is said across them. That argument is already pretty threadbare, and I expect it will become ever more so, the more powerful AI becomes. For decades, tobacco companies hid behind the argument that it was not their product that killed people — smoking was a matter of personal choice — but eventually that defense crumbled. I think the tech giants will lose that battle too, not least because anybody with children can talk about the addictiveness of their product. Which leads me to my eighth and final prediction: For politicians everywhere, AI will simply become too complicated, too powerful, too intrusive, and (if you live outside the US) too American for them to leave it alone. In the 1990s, US politicians wanted to free up young internet companies so they could innovate. Nobody now thinks the likes of Amazon, Microsoft and Facebook need to be protected from anybody. Rather the reverse. Companies have to do more than follow the law. Society only seems happy granting a corporation privileges like limited liability as long as that particular firm is seen as doing good. Various companies and indeed entire industries can lose their franchise from society; you go from being the cool innovators to “the malefactors of great wealth” (as Theodore Roosevelt called the robber barons a century ago when he ushered in antitrust laws). You can see that happening at the moment with the tech giants. In the US the politics are complicated, because American lawmakers, even if they don’t like the tech giants, still see them as one reason why the US is ahead of China economically. In Brussels, there will be fewer such qualms — especially once the politicians of Europe wake up to how far they are behind on artificial intelligence. As one businessman told me, “America innovates, China replicates, Europe regulates.” So those are my broad predictions. Bear in mind, again, that I may have again entirely missed the AI equivalent of Twitter being founded 10 days ago. But where do these eight somewhat-educated guesses leave our world — and the craft that has kept me gainfully employed since 1987? I think on balance we are allowed a degree of paranoid optimism. Paranoia, because it is not hard to see how it could go wrong — where the amount of fake content proliferates, where journalism is caught between interfering politicians and technology superpowers, and where a lot of people in newsrooms lose their jobs because the machine can edit the copy. At its worst, in places like China and Russia, governments could use AI to further impede independent journalism — to chase down our sources, censor what we do, and spin out elaborate webs of fake news of their own. But at some point, optimism begins to break through. Go back to our story on the Iranian ships. All this new technology will give us even more ways to recognize patterns and to hold powerful people to account. In the past, entire countries could seem out of bounds. Now politicians are always being videoed somewhere. Often doing stupid things. As historian Timothy Snyder put it, “No matter how dark the evil, there is always a corner for ridicule’s little lantern.” I am optimistic too that this time our industry is better prepared for technology. Editors and publishers are more on our guard with AI than we were with the internet and social media, less willing to give away our content, and so the flight to quality will be quicker. I say “this time” because we often make the mistake of imagining that we are the first generation of journalists that technology has happened to. In fact, what has happened so far this century (and is about to happen again) is really just an old story being retold — of a new technology ushering in a period of madness and upheaval and then some sense. At the beginning of the 19th century, the arrival of the steam press made it possible to print off pamphlets and scandal sheets in numbers that could say anything about anybody. The titan of the penny press was the New York Sun, which rapidly became the biggest-selling newspaper on the planet. One of its most famous investigative series claimed to have discovered, with the help of a large but strangely very-hard-to-locate telescope, that the moon was populated by a wonderful menagerie of creatures, including half-humans/half-bats who built temples. But gradually things began to sort themselves out. New Yorkers preferred to pay for news that was useful, that told them about the real world; and the new consumer goods companies preferred to advertise their wares alongside stories that were actually true. New titles appeared. The Economist was founded in 1843, the New York Times and Reuters both appeared in 1851, the Financial Times in 1888, the Wall Street Journal in 1889. A flight to quality happened. As long as we focus on original reporting, on writing stories that people in power don’t want us to publish or that tell us something new about the world, and we do that without fear, favor or bias, we will do well. This essay is an edited excerpt from the James Cameron Memorial Lecture, which I had the honor of delivering at City St George’s, University of London. Dataset of Electoral Volatility and its internal components in Western Europe (1945-2015) Author: Emanuele, Vincenzo, Date: 2015 Collections: ElectionPredFeats, PoliticalML Zotero Key: XRGF6GED Cite Key: Emanuele15DatasetElectoralVolatility Zotero Item | Lit Note Dataset of Electoral Volatility and its internal components in Western Europe (1945-2015) Vincenzo Emanuele (LUISS Guido Carli) Codebook (August 3, 2017) Description This dataset provides data on electoral volatility and its internal components in parliamentary elections (lower house) of 20 Western European countries for the 1945-2015 period. It covers the entire universe of Western European elections held after World War II under democratic regimes. Data for Greece, Portugal and Spain have been collected after their democratizations in the 1970s. Altogether, a total of 347 elections (or, more precisely, electoral periods) are included. Content Country: country where the parliamentary election is held (in alphabetical order) Election Year: year in which the election is held Election Date: exact date of the election RegV: electoral volatility caused by vote switching between parties that enter or exit from the party system. A party is considered as entering the party system where it receives at least 1% of the national share in election at time t+1 (while it received less than 1% in election at time t). Conversely, a party is considered as exiting the part system where it receives less than 1% in election at time t+1 (while it received at least 1% in election at time t). AltV: electoral volatility caused by vote switching between existing parties, namely parties receiving at least 1% of the national share in both elections under scrutiny. OthV: electoral volatility caused by vote switching between parties falling below 1% of the national share in both the elections at time t and t+1. It is important to clarify that this category is not computed by aggregating the scores of each party falling below 1% and then comparing the overall sum at time t and t+1. Conversely, each party’s volatility is counted separately - up to a specification of 0.1% - and then added to the calculation of OthV. This choice has been made to avoid underestimation of Total Volatility but at the same time to maintain a distinction between parties above 1% and parties below 1% for the calculation of the two components of RegV and AltV. TotV: total electoral volatility in the party system, given by the sum of the previous measures. RegV + AltV + OthV = TV. Sources The main source for electoral data has been the work by Nohlen and Stöver (2010), for elections until 2008. For the elections held since 2009, I have relied on official data provided by the pertinent electoral authority for each country. For certain elections I have also relied on Bartolini and Mair (1990) and on Ersson (2012). Details about sources and other methodological choices are specified below in the notes on the individual countries. Nohlen and Stöver’s work has the merit to provide, for a large number of elections, electoral results of parties down to 0.1%, disentangling the category of ‘other parties’. The availability of data up to a specification of 0.1% has allowed for an accurate calculation of Total Volatility and its internal components. Methodological criteria The index of electoral volatility has been originally developed by Pedersen (1979). Given the emphasis on the internal components of the index of electoral volatility, the most important question has been that of understanding when a party can be considered as ‘new’ and included in the calculation of RegV. On this point I have relied on the classic criteria set by Bartolini and Mair (1990: 311–312) regarding mergers and splits of existing parties: when two or more parties merge to form a new party, or when two or more parties merge with an existing party, electoral volatility is computed by subtracting the vote share of the new party from the combined vote share of the merging parties in the election immediately preceding the merger. When a party splits into two or more parties, electoral volatility is computed by subtracting the combined vote share of the new parties from that of the original party in the election immediately preceding the split. Following again Bartolini and Mair’s choices, I have considered as splits all those separations that derive from official decisions of a minority within the structure of a given party. As a consequence, splits and mergers have been included in the calculation of AltV. On the contrary, when a party leader or deputy is expelled or simply exits from a party and then launches a new party, this latter has been included in the calculation of RegV. Moreover, ‘genuinely new parties’ – namely ‘parties that are not successors to any previous parliamentary parties, have a novel name and structure, and do not have any important figures from past democratic politics among their major members’ (Sikk 2005, 399) – clearly enter the calculation of RegV. These criteria are also used by Ersson (2012). As underlined by Sikk (2005, 393–394), this approach is conservative, in the sense that it underestimates voters’ mobility but it ‘seems to better balance shortcomings and merits’ with respect to alternative approaches. Furthermore, according to Ersson (2012, 4), this approach ‘is the least troublesome one’. As regards thresholds, for the calculation of TotV and its internal components, parties’ scores between elections have been confronted up to a specification of 0.1%. Parties below 0.1% have been grouped in a residual category and included in OthV. I have set a threshold of 1% of the national share for parties in order to be included in the calculation of either RegV or AltV. The rationale behind this logic is that when a party casts 1% or more, it is already considered as a relevant component of the party system while a vote shift from, say, 1.9% to 2.1% is only considered as an alteration in the strength of an established party, devoid of any regeneration for the party system. At any rate, any threshold could be considered arbitrary and has its own trade-offs: at the same time, to not set any threshold would be even more distorting for the purpose of calculating the extent to which a party system is undergoing a regeneration. For instance, take the case of the French National Front (FN): it contests legislative elections for the first time in 1973 (0.5%), then in 1978 (0.3%) and 1981 (0.2%), and eventually in 1986 elections it succeeds in gaining 9.8% of the national share. With the 1% threshold that I set, FN enters the calculation of RegV in 1986 with a contribution of 4.8 ((9.8 – 0.2)/2 = 4.8)). If we set no threshold, it would have entered the calculation of RegV (with a contribution of 0.5/2 = 0.25) in 1973 and in 1986 it would have entered the calculation of AltV (with a contribution of 4.8, as seen before). Nevertheless, in my opinion, the real innovation within the French party system occurs in 1986 and not in 1973. For this reason, a threshold is necessary to set a qualitative distinction between parties that produce a significant change within the system and parties that simply enter the election game. Notes on individual countries Austria Following Bartolini and Mair (1990, 315), the Democratic Progressive Party (DFP) splits from the Social Democrats (SPÖ) in 1966, thus entering the calculation of AltV. Belgium For some elections (1958; 1999-2014) the residual category of ‘other parties’ is not disaggregated. Red Lions (RL) in 1971 have been considered as a split from the Socialist Party (PSB), thus entering the calculation of AltV). Following Bartolini and Mair (1990, 315), the Flemish and the Walloon wings of Christian-democrats, socialists and liberals have been put together until 1981. Since 1985, they have been considered separately. Due to this choice, TV is slightly underestimated between 1971 and 1981. Cyprus After 1976, the Democratic National Party merges into the Democratic Rally (AltV). In 1996, United Democrats is considered new (RegV) instead as a split from AKEL. The party was founded in 1993 by Georgios Vassiliou, former President of Cyprus, elected as an independent with the support of AKEL. After the election of 1996, ADISOK, a split from AKEL, merged into the United Democrats (AltV). Denmark The Independent Party (DU) in 1953 has been treated as a new party (RegV). Following Bartolini and Mair (1990, Appendix II) but unlike Ersson (2012, 22), the Socialist People’s Party (SF) in 1960 has been considered as a new party (RegV). Following Ersson (2012, 22), in 1998 the Danish People’s Party (DF) splits from the Progress Party (FP) (AltV). Finland The Social Democratic Union of Workers and Smallholders (TPSL) splits from the Social Democratic Party (SDP) (Ersson 2012, 22). The Finnish Rural Party (SMP) in 1962 has been classified as a new party (entering the canculation of RegV). In 1975, the Constitutional Right Party (PO) has been considered as a new party (entering the calculation of RegV), while the Finnish People’s Unity Party (SKYP) splits from the Rural Party (AltV) (Bartolini and Mair 1990, 316). France As recognized by Bartolini and Mair (1990, 316-317) and Ersson (2012, 21), estimating volatility scores for France is not easy due the extremely fluid nature of French parties. Moreover, electoral data are very often inconsistent given that different sources report different electoral results. In 1958, the Unified Socialist Party has been considered as new (entering the RegV calculation). In 1967, the conservative political scene in France is very confused: I have followed the choices made by Bartolini and Mair (1990, 317). Note that Ersson (2012, 21) disagree on how to treat the Center Democrats (CD). In 1973, the two groups of Left Republicans (MR and RR) have been treated as new (entering the calculation of RegV). In 1981, the two Green lists have been considered as splits from the previous Green list running in 1978 (thus entering the calculation of AltV). In 1986 Rally for the Republic (RPR) and Union for French Democracy (UDF) ran with common lists in certain districts: my choice has been to consider them together for this election. In 1997, the residual category of ‘other parties’ is not disaggregated. Moreover, votes grouped in the categories of ‘other left’ and ‘extreme left’ have been considered in continuity with the same groups in 1993. The National Republican Movement (MNR) and the Republican Pole have been treated as new parties in 2002 (therefore entering the RegV calculation). In 2007, the Extreme Left includes Revolutionary Communist League (LCR) and Workers’ Struggle (LO) that ran separately in 2002 (AltV). In 2017, the Union of Democrats and Independents is a merge the Radical Party and the New Centre (AltV). Debout la France is new (RegV). Germany In 1953, electoral data are inconsistent: I have followed the Federal Returning Officer http://www.bundeswahlleiter.de/en/bundestagswahlen/fruehere_bundestagswahlen/btw1953.html. In 1990, the electoral results in the reunified Germany have been put in comparison with the results in West Germany in 1987: the result is a higher level of electoral volatility with respect to Dassonneville (2015) who excludes the East Germany from the calculation in 1990. The Christian Democratic Union (CDU) and the Christian Social Union (CSU) have been considered as a unique party: this leads to a slight underestimation of electoral volatility only in 1990, while in the other elections their swings go in the same direction. Greece In 1977, the National Alignment (EP) has been considered as a split from New Democracy (entering the calculation of AltV). In 1993, Political Spring (POLAN) has been considered as a split from New Democracy (following Bolgherini 2002). In 1996, the Democratic Social Movement (DIKKI) splits from the Panhellenic Socialist Movement (PASOK). In May 2012, the Independents Greeks (ANEL) has been considered a new party (therefore entering the RegV calculation). Iceland In 1953, the Republican Party (Lý) is a splinter from the Independence Party (Sj) (entering the AltV calculation). In 1967, the Independent Democratic Party (Ól) has been treated as a new party (therefore entering the RegV calculation). In 1971, the Union of Liberals and Leftists (Sf) is a continuation of Ól under a different label (and it enters the AltV calculation.). In 1979, ‘Independents from the South’ have been treated as a new list (therefore considered in the calculation of RegV). In 1987, the Association for Justice and Equality (Suj) splits from the Progressive Party (Fr) (entering the calculation of AltV). Following Ersson (2012, 18), in 1995 the National Awakening (Þj) splits from the Social Democratic Party (Al) (and it enters the calculation of AtV). In 1999 there is a process of splits and mergers on the left wing scene: I have followed Ersson’s conservative choices (2012, 18-19) as regards the two new parties, Social Democratic Alliance (Sa) and Left-Green Movement (Vi), both considered as an emanation from the four left parties that contested the 1995 election (entering the AltV calculation). The Liberal Party (Ff) is instead a new party (thus considered in the RegV calculation). Ireland Independents have been treated as a unique ‘party’ but always included in OthV, so as to not inflate either RegV or AltV. Ersson makes a different choice on this point, by treating them as a genuinely new party each time, thus overestimating its type ‘A’ volatility, which corresponds to my RegV. In 1992, Democratic Left (DL) splits from Workers Party (WP) (and it has been considered for the AltV calculation). In 2002, DL merges into the Labour Party. Italy In 1948, the Socialist Party of Italian Workers (PSLI) splits from the Italian Socialist Party (therefore, it has been considered for the calculation of AltV). In 1968, the Socialist Party of Proletarian Unity (PSIUP) splits from the Italian Socialist Party (PSI). In 1976, the Radical Party (PR) and Proletarian Democracy (DP) are new parties (and considered for the calculation of RegV). In 1979, National Democracy (DN) splits from the Italian Social Movement (MSI) (and enters the AtV calculation). In 1992, The Network (La Rete) has been considered a new party (entering the RegV calculation) and so does also Democratic Alliance (AD) in 1994 (RegV), while the Segni Pact (PS) has been treated as a split from Christian Democracy (DC) (and enters the AltV calculation). In 2001, the Sunflower is an electoral alliance composed by the Greens and the Italian Social-democrats (SDI). In 2006, the calculation of electoral volatility for Greens and SDI has been made by attributing the 50% of the list score in 2001 to each party (1.1% to each party). In 2006, European Democracy (DE) merges into Union of Christian and Center Democrats (UDC). In 2008 The Right (La Destra) is considered a new party (therefore considered for the calculation of RegV), while the Pensioners’ Party (PP) merge into the People of Freedom (PDL) (and it enters the calculation of AltV). Luxembourg In 1968, the Popular Independent Movement (MIP) merges into the Democratic Party (DP) (entering the calculation of AltV). In 1999, Green and Liberal Alliance (GLA) is a new party (thus considered for the RegV calculation). Malta As for Ireland, Independents in 1950 have been treated as a unique ‘party’ but included in the calculation of OthV. In 1962, both the Democratic Nationalist Party (DNP) and the Christian Workers Party (CWP) have been classified as splinter parties from, respectively, the Nationalist Party (PN) and the Labour Party (PL) (this choice is consistent with Ersson 2012, 19). Netherlands Following Bartolini and Mair (1990), the Political Party of Radicals (PPR) splits from the Catholic People’s Party (KVP) in 1971 (entering the AltV calculation). The Reformed Political League (GPV), a former split from the Anti-Revolutionary Party (ARP) in 1952, enters the calculation of RegV as it overcomes 1% in 1971. Following Bartolini and Mair (1990, 320), the Reformatory Political Federation (RPF) in 1981 has been considered a new political party (thus also considered for the calculation of RegV). In 1989, the GreenLeft (GL) is the result of the merge among four small left-wing parties, among which the Political Party of Radicals PPR) and the Pacifist Socialist Party (PSP) were above 1% in 1986 (entering the AltV calculation). For the 1994 election, I have consulted the European Electoral Database to find disaggregated data up to 0.1% http://eed.nsd.uib.no/webview. In 2006, the Party for Freedom (PVV) has been considered a new party (and it is considered for the calculation of RegV): Gert Wilders, a former member of the People's Party for Freedom and Democracy (VVD), left the party in 2004, and later in 2006 founded the Party for Freedom (PVV) but without any formal breakaway within the structure of the VVD. Norway Unlike Ersson (2012, 22), the Socialist People’s Party (SF) has been considered as a splinter from the Labour Party (A) in 1961. Portugal Contrary to international practice, official electoral results include blank and invalid votes into valid votes. In order to provide more accurate volatility scores and following Nohlen and Stöver (2010), parties’ vote share has been recalculated by subtracting blank and invalid votes from the total number of valid votes. In 1983, the People’s Democratic Union (UDP) and the Revolutionary Socialist Party (PSR) ran in a joint list in some constituencies: in order to compare their scores with the two separate lists running in 1985, they have been considered together for the period 1983-1985. Spain Similarly to Portugal, in Spain official electoral results include blank votes into valid votes. In order to provide more accurate volatility scores, parties’ vote share has been recalculated by subtracting blank votes from the total number of valid votes. In 1982, the Democratic and Socialist Center (CDS) is a doubtful case, given that is has been launched by Adolfo Suárez, the former leader of the Union of the Democratic Center (UCD). However, there has not been a formal split in UCD and CDS can be therefore considered a new party (entering the calculation of RegV). In 1986, the Communists’ Unity Board (MUC) and the Democratic Reformist Party (PRD) are new parties (thus considered for the calculation of RegV). In 1989, the Basque Solidarity (EA) splits from the Basque Nationalist Party (PNV). Sweden Feminist Initiative (F!), exceeding 1% of the national share for the first time in 2014, is a genuinely new party (and it enters the calculation of RegV). Switzerland Following Bartolini and Mair (1990), the Republican Movement (RB) has been considered a new party in 1971 (thus entering the calculation of RegV). For the 2007 federal election, electoral data have been gathered from the Swiss Federal Administration website http://www.politik- stat.ch/nrw2007CH_it.html. The Green Liberal Party (GLP) has been considered a new party (and it enters the RegV calculation). How to cite this dataset? Emanuele, V. (2015), Dataset of Electoral Volatility and its internal components in Western Europe (1945-2015), Rome: Italian Center for Electoral Studies, http://dx.doi.org/10.7802/1112 Publications based on this dataset Chiaramonte, A. and Emanuele, V. (2015), Party System Volatility, Regeneration and De Institutionalization in Western Europe (1945-2015), Party Politics, doi:10.1177/1354068815601330. References Bartolini, S. and Mair, P. (1990), Identity, competition, and electoral availability: The stabilisation of European Electorates 1885-1985, Cambridge: Cambridge University Press. Bolgherini, S. (2002), Elezioni, famiglie politiche e sistema partitico nella Grecia democratica (1974-2000). Quaderni dell’Osservatorio Elettorale 47: 33-86. Dassonneville, R. (2015), Net Volatility in Western Europe: 1950–2014. Dataset. Leuven: Centre for Citizenship and Democracy. Ersson, S. (2012), Electoral volatility in Europe: Assessments and potential explanations for estimate differences. In: Elections, Public Opinion and Parties (EPOP) Conference, Oxford, UK, 7 - 9 September 2012. Nohlen, D. and Stöver, P. (eds) (2010), Elections in Europe: A Data Handbook. Baden-Baden: Nomos. Pedersen, M.N. (1979), The Dynamics of European Party Systems: Changing Patterns of Electoral Volatility. European Journal of Political Research 7(1): 1-26. Sikk, A. (2005), How unstable? Volatility and the genuinely new parties in Eastern Europe. European Journal of Political Research 44: 391-412. The Truth About the US Economy! Author: Boyle, Patrick, Date: 2024-11-11 11/11/24 Collections: Hot Takes US Elect 2024 Zotero Key: X7GL9NUB Cite Key: Boyle24TruthUSEconomy Zotero Item | Lit Note 00:00 In markets, contagious exuberance can take hold from time to time, leading to investment frenzies 00:06 followed by market crashes. Most people who are interested in markets and investing find this type 00:13 of collective delusion fascinating. It doesn’t just show up in markets however - collective 00:18 delusions are everywhere around us. According to Pew Research, over time more and more Americans 00:25 believe that crime is a growing problem – they believe that this is more the case 00:30 nationally than locally – but statistics show that crime in the United States (and especially 00:36 violent crime) has plummeted since the 1990’s. There are all sorts of statistics that show that 00:44 quality of life has improved massively over time, but for some reason people don’t want 00:49 to believe that. Twenty years ago I came across a book by Stephen Moore and Julien Simon called 00:56 Its Getting Better all the Time, which examined the 100 greatest trends of the last hundred years 01:02 which showed that people were taller, healthier, poverty rates were lower, household incomes had 01:09 not only risen, but household sizes had fallen, meaning that there was more money per family 01:15 member than there had ever been before. There were many eye-opening charts in the 01:20 book which showed that despite all the negative news that we read - that the overall quality of 01:26 life was trending upwards globally. The authors showed that a greater percentage of the global 01:33 population had access to education, nutrition and clean water than ever before. Statistics 01:40 like global child mortality – a statistic that pulls together many of the improvements we have 01:46 experienced in terms of medicine, hygiene, access to food and shelter, safety from wars – and much 01:53 more had improved massively over time. In 1800 44% of people died before their fifth birthday 02:02 and today that number has fallen to 3.7%. The improvements aren’t all in the distant 02:09 past either, according to the World Health Organization, although the 02:14 global population has grown, the total number of under-5 deaths worldwide has declined from 02:20 12.8 million in 1990 to 4.9 million in 2022. When I look in the comments section of my videos, 02:30 I’m occasionally surprised to see how angry and negative some people are about the economy, 02:36 they will say that the dollar is in decline – but if we look at the dollar index – which compares 02:41 the US dollar to a basket of Americas trade partners' currencies, we can see that the dollar 02:47 is in fact strong - every major currency in the world has fallen against the dollar this year. 02:54 The US Economy has been booming – by almost every measure - in recent years. Most Americans 03:00 are doing better than they were a year ago, unemployment is lower, wages are growing, 03:06 and inflation is declining. It’s not just the one percent either, the statistics show that Americans 03:12 across ages and social classes are doing well. In a video earlier this year I mentioned that 03:19 unemployment was near all-time lows – people wrote in the comments that this was because so many 03:25 people had stopped looking for work – but that’s not true - the labor force participation rate has 03:31 risen every year for the last three years, on top of that - there are more jobs available today than 03:38 there are job seekers. These are tangible economic improvements that should be cheering people up, 03:45 but that’s not what’s happening. Surveys show that Americans are the most pessimistic they 03:50 have been about the economy in thirty years. TikTokers (who YouTubers look down on) have 03:57 coined the term “the silent depression,” claiming that it’s harder to get by today 04:03 than it was during the great depression. An idea that is entirely divorced from reality. 04:09 During the great Depression US GDP contracted by 35% international trade more than halved and 04:17 unemployment rose to 23%. In contrast, last week’s GDP report showed that the US economy 04:25 grew at a 2.8% annualized rate and unemployment is at 4.1% - which is near its all-time lows. 04:34 Up until a few years ago Americans economic attitudes mostly tracked what was actually 04:40 happening in the economy. When things improved, surveys showed that people’s 04:44 subjective experience of the economy improved too, and when there was a slow down people 04:50 reported that things had gotten worse for them. Everything seemed to change in mid-2020. When the 04:57 pandemic first struck the economy tanked and sentiment collapsed with it – so far that’s 05:03 totally normal. But the strange thing is that when the economy bounced back, sentiment didn’t. 05:10 The relationship between economic statistics and surveys of public sentiment broke down entirely at 05:18 that point – and this was totally new – there is no history of a similar divergence looking 05:23 back through economic history. So, let’s dig into why people are so gloomy about the 05:29 economy and so pessimistic about the future today. Before we get to that let me tell you 05:35 about this week’s video sponsor – surfshark. I have been using VPN software for quite some time 05:42 and Surfshark is an easy to use and affordable VPN app for Windows, Mac, Android, iOS, and more. 05:49 VPN - stands for virtual private network, and using one connects your computer or 05:55 phone directly to a remote server, creating a point-to-point tunnel 06:00 that encrypts your personal data, masks your IP address, and lets you sidestep website blocks and 06:06 firewalls on the internet. This ensures that your browsing is private, protected, and more secure. 06:13 This is most useful for people who access the internet from unfamiliar networks 06:18 like at coffee shops or airport lounges. Surfshark doesn’t just protect your data, 06:24 If you travel you’ll notice that different content is available on streaming services in different 06:29 countries – in some countries content is even censored. With Surfshark, no matter 06:34 where in the world you are, you get to take the internet from home with you. Surfshark is fast, 06:40 reliable and they don’t collect or track your data, they also allow you to set up 06:45 one account and use it on unlimited devices. Secure your privacy with Surfshark! Enter 06:51 coupon code BOYLE for 4 months EXTRA at www.surfshark.com/boyle. Click the link 06:58 in the description to sign up today. OK, so what do we mean by a good economy? 07:05 Well to start with GDP growth, the US economy has been outpacing other advanced economies, 07:11 with some projecting that the US will grow by as much as 3% this year, 07:16 when other leading economies are projected to grow by 1% or less. Next, the stock market has 07:24 been booming- the S&P is up more than 27% year to date, there has been strong wage growth low 07:31 unemployment and declining inflation – which is getting close to the Fed’s 2% target. Labor force 07:39 participation has increased which means that even people who weren’t looking for jobs now 07:44 are working because the employment environment has gotten so good. Even those tiktokers talking about 07:51 “the silent depression” are earning money in a way that was not possible during the 07:56 actual – or noisy depression as I now call it. These are all things that we should feel in our 08:03 day to day lives – and to top it off, the people who have seen the biggest 08:07 income and wage gains are people at the lower end of the income distribution – as 08:12 the widening inequality that we have seen for decades reversed direction during the 08:17 pandemic in pretty much all advanced economies. A wider swath of people across American classes in 08:25 their day-to-day life are experiencing positive economic benefits, but despite all of this, 08:32 the average man on the street still feels that the economy is in terrible shape. 08:37 Now obviously, I’m not trying to say that everyone is thriving, and no one has anything to be down 08:44 about. There are of course plenty of people experiencing misfortune, but that has always 08:49 been the case. The thing that is unusual is just that average sentiment is so bad, 08:55 when the average American is doing better economically than they were in the past. 09:00 According to Pew Research large majorities of Americans say they are dissatisfied with 09:06 the economy and when they look toward the not-too-distant future, they see a 09:11 country that will be worse than it is today. When they look to the past, 58% of Americans 09:18 say that life for people like them is worse today than it was 50 years ago. 09:24 Gallup reports that Americans are the most pessimistic they have been in decades about 09:30 young people's chances of having greater material success in life than their parents. 09:36 58% of American adults think that it’s unlikely that today's youth will have a better life than 09:42 their parents had. This is an 18-percentage-point increase in pessimism over the last five years. 09:50 You might think that people are being cautious and worrying that an economic boom is about to 09:55 turn to a bust – but if that is true it would be quite a break with the past too - as the 10:01 highest percentage of U.S. adults expecting better lives for the next generation was 71% in and that 10:09 happened in1999 – at the peak of the dot com bubble. So, if people are being cautious now, 10:16 they certainly weren’t cautious back then. Gilad Edelman at The Atlantic conducted a 10:22 poll last December asking over a thousand Americans how they felt about the economy. 10:28 Only 20 percent of respondents said that the economy had improved over the last year and 10:34 44 percent said it had gotten worse. He found that while there was a partisan 10:40 split and Republicans were more negative about the economy than Democrats, democrats 10:46 were still very negative, only 33 percent of Democrats felt that the economy had improved. 10:53 Interestingly, Fareed Zacharia reported in the Washington Post that despite all of the internal 10:59 pessimism in the United States, international polls show that the rest of the world views 11:05 the United States much more positively than they did five years ago. A survey conducted by King’s 11:12 College London found that across 24 nations where trends exist, the share of the public 11:18 who feel the US model should be followed has risen by seven percentage points since 2019. 11:26 Zacharia writes that while Americans may be troubled by the deep polarization and division in 11:32 the country, an international audience sees that the debate – where the nations dirty laundry is 11:38 constantly washed in full public view is better than the alternative of repressing national 11:45 problems – as can happen elsewhere - where a North Korean style façade of unity is presented. 11:52 Gilad Edelman at The Atlantic wrote that he expected to find that the economic frustration 11:58 within the United States was driven by housing inflation and so he gave his survey respondents a 12:04 long list of factors to choose from as the metric that was dragging down the national economy. He 12:11 found that it wasn’t the cost of housing - the runaway winner (according to his survey) was 12:17 the price of groceries at the supermarket. He points out that for every person that is 12:22 irritated that housing prices are rising, there is another person delighted that 12:27 the value of their home has gone up. He learned that people weren’t actually 12:32 upset that the generous benefits handed out during the pandemic had been withdrawn – as 12:37 some people had claimed. Apparently, people with children at home were more positive on 12:43 the economy than people without kids - which you wouldn’t expect if Americans were fuming over the 12:50 expiration of the expanded child tax credit. I would have expected that some of the pessimism 12:57 was driven by excessive exposure to social media, but the survey found that those who reported 13:03 getting their news from Facebook, Instagram, or TikTok expressed more positive views than people 13:09 who didn’t. Those who read national and financial newspapers were also more positive than average. 13:16 The most negative sentiment he found came from older people, not Gen Z TikTokers. This is quite 13:24 surprising, as older people are asset owners and will have benefited from asset inflation. 13:31 Edelman points out that grocery prices went up more than average inflation and that in 13:37 contrast with housing or used car prices, few ordinary Americans benefit when grocery prices 13:43 go up. Higher grocery prices are also something that Americans see week after 13:49 week when they go to the stores. Higher used car prices are much 13:54 less of an irritant as most people can postpone a car purchase if necessary, 13:59 and it is a one off event every few years. Now, most Americans are making more money - after 14:06 inflation, than they were before the pandemic. I mentioned that statistic in a video a few months 14:12 ago – and it drew all sorts of angry comments with people accusing me of making it up. But it is 14:18 there in the data – and not just that, wages have risen the fastest for the lowest-paid workers. If 14:25 people were coldly rational, Edelman points out they would recognize that their higher income 14:31 more than offsets the higher grocery prices. One reason for the dissatisfaction may be that 14:38 workers expect their pay to go up over time, so they think that it’s fair that 14:43 they are earning more, and unfair that their grocery bills have gone up at the same time. 14:48 There is reason to believe that the pandemic and the associated lockdowns might also have harmed 14:55 the mood of the country too. Research from Boston University found that, prior to the pandemic, 15:01 8 percent of American adults experienced some form of depression. By April 2020, 15:08 that proportion had jumped to 28 percent and in 2021, the figure rose to 32 percent. 15:16 Other reasons for Americas pessimism of late is the worry that AI will take everyone’s job. If we 15:23 listen to Silicon Valley, we are only a year or two away from a society where filmmakers, 15:29 journalists, lawyers and Doctors are all put out of work by AI’s. You’ll be sitting at home 15:35 penniless worried about what will happen when you have to explain to the robot plumber who has just 15:41 unblocked your drain that you can’t afford to pay him. Will you be able to remove his 15:46 battery pack before he puts you in cuffs and carts you away? How will the Roomba react to 15:52 seeing you do this – you kinda knew it was a mistake giving the vacuum cleaner your 15:57 wifi password… What were you thinking??? According to YouGov 46% of people who use AI 16:05 tools at work are worried about AI taking their job. Among those who never use AI tools at work, 16:12 26% are concerned about this. While AI can be expected to transform many aspects of daily life, 16:20 there has been a long history of labor automation, and historically labor demand 16:25 shifts away from jobs that can be automated towards new types of work where people work 16:31 alongside the new technology. Workers becoming more productive doesn’t lead to job losses – it 16:38 leads to more production – which also means greater availability of goods and services. 16:45 One of the best examples of jobs being rapidly automated away is telephone switchboard operators. 16:52 Around 350,000 women worked for telephone companies as switchboard operators up until 16:59 as late as 1950. An additional million worked as switchboard operators at offices, hotels, 17:05 and other businesses. The rate of growth of telephone networks in the 1920’s led 17:12 people to project that the phone companies would soon need to employ every American 17:17 woman of working age as a switchboard operator. When mechanical switchboards were installed the 17:24 number of women employed as telephone operators immediately fell by 50% to 80% - and wages 17:31 for women in other occupations fell rapidly as employers were flooded with job applications. This 17:39 didn’t last for long, however. Studies show little or no negative long-term employment effects at 17:45 all. Laid off switchboard operators quickly found employment like secretarial work and restaurant 17:52 jobs and while some of those jobs (like restaurant work) paid less, others paid the same or more. 18:00 Automation typically increases worker productivity and reduces employment in a given field, 18:07 but we have seen all sorts of jobs automated in the past without leading to mass unemployment, 18:13 the same can be expected in the future. There are all sorts of things we can choose 18:18 to worry about. The Doomsday Clock is a symbolic clock that represents how close 18:24 we are to destroying the world with dangerous technologies of our own making. It warns how many 18:30 metaphorical “minutes to midnight” humanity has left. It is adjusted every year with the goal of 18:36 warning the public and inspiring action. I suppose we could lie awake all night 18:42 worrying about global events that are entirely outside of our control, 18:47 or we could focus on taking action to improve our lives and the lives of those around us. 18:53 There are all sorts of positive changes occurring in the world – despite all of the news we read 18:59 about hurricanes and natural disasters, the number of deaths from natural disasters has fallen by 19:06 75 percent over the past 100 years. This figure is even more shocking when you consider how much the 19:12 global population has grown over that period. Over the last twenty years the proportion of 19:18 the world’s population living in extreme poverty has more than halved. The share 19:24 of the world population living in democracies increased continually over the last 250 years, 19:31 and 80% of the population who live under an authoritarian regime today — live in one country, 19:38 China. The world’s population has greater access to education than ever before too. 19:45 For those feeling down about the economy, frustrated about politics, or worried about 19:50 what the future has in store for us, it is worth reflecting on the idea that there have been much 19:56 more difficult periods in the past and that for most people in the world things have never been 20:01 better than they are today. The reason these statistics have improved is not because people 20:07 sat at home and worried about the direction society was taking. They have improved because 20:12 it’s in everyone’s self-interest to improve the world, and when things got hard people 20:18 applied their energy to making things better. Our capacity to reflect on the challenges we 20:25 face and find solutions explains all of the scientific and economic progress that has been 20:31 made over time, and there is no good reason to believe that will change any time soon. 20:37 Economic statistics show that the US economy is doing just fine right now – and even if it 20:43 was not – if somehow the numbers are wrong – there is very little we can do about it 20:48 individually. If you are feeling pessimistic – one of the best ways to lift your mood is 20:53 to go outside and exercise, contribute to your community, try to learn new things, 20:59 engage with your friend’s family and loved ones and work hard on improving your situation, rather 21:05 than focusing on the things holding you back. If you found todays video interesting you 21:10 should watch this one next. Don’t forget to check out our sponsor Surfshark using 21:15 the link in the video description. Have a great day and talk to you again soon, bye. How the Democrats Lost the Working Class Author: Weisman, Jonathan, Date: 2025-01-04 Collections: Hot Takes US Elect 2024 Zotero Key: BITQTI2W Cite Key: Weisman25HowDemocratsLost Zotero Item | Lit Note Democrats had just absorbed a crushing defeat in the 1994 midterm elections when President Bill Clinton’s very liberal labor secretary, Robert Reich, ventured into hostile territory to issue a prophetic warning. Struggling workers were becoming “an anxious class,” he told the centrist Democratic Leadership Council, two weeks after Republicans led by Newt Gingrich had gained 54 seats in the House and eight in the Senate. Society was separating into two tiers, Mr. Reich said, with “a few winners and a larger group of Americans left behind, whose anger and whose disillusionment is easily manipulated.” “Today, the targets of that rage are immigrants and welfare mothers and government officials and gays and an ill-defined counterculture,” Mr. Reich cautioned. “But as the middle class continues to erode, who will be the targets tomorrow?” His message went largely unheeded for 30 years, as one president after another, Republican and Democratic, led administrations into a post-Cold War global future that enriched the nation as a whole and some on the coasts to staggering levels, but left many pockets of the American heartland deindustrialized, dislocated and even depopulated. As a half-century-old world order organized around American-Soviet contention gave way to a more freely competitive landscape of shifting alliances, presidents from both parties sought to secure U.S. leadership under new rules for economic competition, global stability and strong financial markets. Democratic presidents tried, with limited success, to expand safety nets at home, especially health care and income support for the poor. In the end, however, their bets on foreign policy — opening China to capitalism, halting Iran’s nuclear program, tightening economic bonds with allies — took precedence, and a new fealty to megadonors shaped fiscal policies that bolstered financial markets but shuttered many factories. The unintended consequences often came at the expense of American workers. And Mr. Reich’s “anxious class” — neither the impoverished nor the highfliers riding the rising global stock market — felt unheard until the rise of an unlikely new kind of Republican: Donald J. Trump. The Democratic Party’s estrangement from working-class voters first became clear with Mr. Trump’s upset of Hillary Clinton in 2016, powered by broad shifts in the preferences of white voters without college degrees, and it became even more unmistakable with his emphatic defeat of Vice President Kamala Harris in November. That result was a reckoning for a party that thought it had fixed its problems with blue-collar voters by heavily reinvesting in domestic manufacturing but instead discovered even more erosion, this time among Black and Latino workers. Many Democrats have blamed recent social issues like transgender rights or the “woke” language embraced by many on the left. But the economic seeds of Mr. Trump’s victories were sown long ago. “One of the things that has been frustrating about the narrative ‘The Democrats are losing the working class’ is that people are noticing it half a century after it happened,” said Michael Podhorzer, the former political director of the A.F.L.-C.I.O. “The resentment and movement away from the Democrats began long before they were for nongendered bathrooms. It was because their lives were becoming more precarious, their kids were leaving town, the pensions they expected were evaporating, and that took a toll.” A big bet on China, without a safety net To be sure, blue-collar voters have long been fickle. Richard M. Nixon’s “silent majority” delivered him a landslide in 1972, propelled not by a Republican economic platform but by a backlash to civil rights legislation and anti-Vietnam War protests. The so-called Reagan Democrats, stung by inflation and economic malaise, helped give the White House back to the G.O.P. eight years later, and it remained in Republican hands for 12 long years. William A. Galston, a domestic policy adviser to Mr. Clinton and an architect of the Democrats’ shift to the center, said that after the election debacles of 1980, 1984 and 1988, the party’s repositioning on social and economic issues was not a choice but an imperative. But once Mr. Clinton took office in 1993, choices were made. “The Clinton vision was to be a pro-growth progressive by combining major expansions in public investment and the safety net with more private investment through fiscal discipline and vibrant markets,” said Gene Sperling, an economic adviser to the last three Democratic presidents. “As the first post-Cold War president,” he continued, Mr. Clinton also tried to have “a focus on strengthening global relations through trade agreements.” The North American Free Trade Agreement had been negotiated under President George H.W. Bush. It fell to Mr. Clinton to get it through Congress. His rationale was that the trade agreement would enhance Mexico’s stability and economic growth, reduce illegal immigration and foster cooperation in fighting drug trafficking. A wider social safety net — including universal health care, expanded education and job training and economic investment — would cushion the blow of employment losses, while cheaper consumer goods would make everyone happy. Then the health-care push collapsed in the late summer of 1994. The Republicans took control of Congress after their decisive victories that November, and the domestic agenda was moribund, replaced by a zeal for budget cutting. The Clinton administration faced a choice: Pull the plug on free trade and internationalism or push ahead without the safety-net side. Over the objections of more liberal voices in the administration, Mr. Clinton chose the latter, pressing on with legislation to normalize trade relations with China and allow Beijing to join the World Trade Organization. Even then, there was concern that China’s accession into the family of trading nations could flood the United States with cheap imports and bankrupt American manufacturers. But the economy was roaring, deregulation was the order of the day as the administration worked to free Wall Street from Depression-era banking and investment rules and, most important, a reformer, Jiang Zemin, had taken control in China. The foreign policy chiefs in the White House believed firmly that cooperation was vital to securing a prosperous, peaceful and eventually democratic China. “You might think I was nuts,” Mr. Clinton allowed last month as he discussed international trade at The New York Times DealBook Summit, “but Jiang Zemin was president of China, and he was a darn good one.” A disregard for ‘the dignity of work’ That tendency to roll the dice on grand international bets, with working-class voters as the chips, would become a theme. Too often, the bets did not pay off. China became more autocratic, not less. And the feared tsunami of Chinese exports indeed arrived, along with the damage. In 1998, 17.6 million Americans were employed in manufacturing. By January 2008, the “China shock” had cost U.S. manufacturers nearly four million jobs. By January 2010, as the financial crisis waned, manufacturing employment had bottomed out below 11.5 million. “I would be the first to say the leadership of both political parties were in the grip of a theory or story that turned out to be wrong,” Mr. Galston said, “and damagingly so.” Still, Democratic economists defend their choices. Jason Furman, an economic adviser in the Clinton and Obama White Houses, said the biggest expansions of income inequality came in the 1980s and 1990s, before the China shock. Overall, China’s integration into world markets did increase the number of jobs in the United States — selling services like insurance and Hollywood movies to the Chinese, and peddling Chinese-made goods at stores like Walmart — while sharply lowering the cost of living for American consumers. What was less appreciated beforehand was the psychological damage that would be done by factory closures, large and small, in communities where prestige, stability and identity centered on those plants — as well as the political impacts of those closures on key industrial states like Pennsylvania, Ohio, Michigan and Wisconsin. Democratic policies focused on people as consumers instead of as workers, counting on those people whose jobs were eliminated to find their way to jobs newly created — an assumption that was often flawed, given that the new service jobs frequently required out-of-reach skills or were located on the coasts, not in the upper Midwest. Too often, said Jared Bernstein, the chairman of President Biden’s Council of Economic Advisers, there was a “disregard for the importance of work, the dignity of work.” “Forty people might have lost their job in a factory, but 100,000 people in the community had lower prices,” Mr. Bernstein said. “The calculus seemed obvious. But the calculus was wrong.” Still, for years, the Democratic Party’s drift away from the working class could be papered over. George W. Bush eked out the narrowest of victories in 2000 in part because the economy was doing so well that voters could focus on his appeal to “restore honor and integrity to the White House.” Four years later, Mr. Bush was re-elected as a wartime president, his domestic agenda topped by hot-button social issues like opposing gay marriage. But blue-collar voters, who had soured on the “trickle-down economics” of the Reagan years, turned away from the party of Mr. Bush, who had entangled the nation in two wars, and watched helplessly but angrily as Wall Street tycoons dragged down the banking and housing markets in 2008 with their opaque financial gambles. And they spurned the G.O.P. again in 2012 when it turned to Mitt Romney, a wealthy businessman seemingly plucked from plutocratic central casting. Pulled punches after the financial crisis David Axelrod, one of the architects of Barack Obama’s 2008 campaign, said the last years of the George W. Bush administration were a moment when Democrats could pivot back to policies to address the hollowing out of the industrial base, and with it, the middle class. The 2009 bailout of the auto industry was driven by those concerns, as were the re-regulation of Wall Street and the creation of the Consumer Financial Protection Bureau. But under Mr. Obama, no one on Wall Street or in the banking sector faced prosecution for the global financial crisis. After Mr. Obama called bankers “fat cats” on “60 Minutes,” Democratic donors on Wall Street howled. “The masters of the universe,” Mr. Axelrod said, “turned out to be more sensitive than we thought.” Mr. Obama tempered his language. The 2012 campaign was marked by an early effort by Democrats to tar Mr. Romney as an insensitive, rapacious businessman willing to send jobs overseas. It worked. The working class stuck with Mr. Obama. But the later years of his presidency veered away from kitchen-table issues as Mr. Obama tried to secure his legacy on the global stage. That meant striking a deal with Iran to curb its nuclear program, at least temporarily; completing groundbreaking regulations on trucks, cars and power plants to curtail climate change; and finalizing one more ambitious trade agreement, the Trans-Pacific Partnership, to unite a dozen nations on both sides of the world’s largest ocean under trade rules and in an alliance that would isolate China. As Mr. Obama basked in those achievements, Mr. Trump campaigned against every one of them, framing them not as steps toward a more peaceful planet but as job killers again threatening the forgotten working class. Once elected, he would undo all of them within months. A glaring math error The Democrats’ alienation from blue-collar voters was scarcely a unique phenomenon. Across the developed world, as Western democracies have grown more affluent and less industrially centered, so have the parties that once represented the working classes, said Thomas Piketty, the French economist who has become one of the foremost experts on wealth inequality. It seemed to make sense politically: With the largest cities and the growing suburbs backing those center-left parties — which Mr. Piketty called “the Brahmin left,” or “parties of the educated” — shrinking towns and rural areas would matter less and less. But there was always a problem with the theory, said Mr. Bernstein, the Biden adviser: “About 60 percent of the work force is still not college-educated.” Douglas Holtz-Eakin, a veteran Republican economic adviser in the Bush White House and for John McCain’s 2008 presidential campaign, observed that huge shocks to the nation’s economic system — terrorism and war, the financial crisis and the coronavirus pandemic — had upended many Americans’ lives, but least of all those of the wealthy. The rich did not send their children to war, their banks were bailed out, and they rode out the pandemic working from home. “In all of it, the elites got away unscathed,” Mr. Holtz-Eakin said, “while the ordinary man took it on the chin.” Exploiting such resentments, Mr. Trump, with his relentless economic appeals and his open disregard for America’s global leadership, broke the Democratic formula by winning over not only a large majority of the white working class but also a strong percentage of workers of color. Of course, there is plenty of blame to go around. Labor leaders often point to the Democratic Party’s movement away from unions as an intermediary between the party and working-class voters. During the 2008 campaign, Mr. Obama was instructed to not even use the words “labor union,” Mr. Furman recalled: Most workers were not members, and it was believed that unions were unpopular. Mr. Podhorzer said he understood why Democrats had moved away from unions as their conduits to the working class. “When you talk to the unions, you’re talking to an institution that can hold you accountable to the promises you are making and can ask you for specific things,” he said. “When you’re talking around them, you’re basically doing commercial marketing.” But, he added, “that sets you up for the moment when a Donald Trump comes along, and you have a candidate who just has better marketing than you.” Still, Lawrence F. Katz, a Harvard professor who was the chief economist in the Clinton Labor Department, said unions had played their own negative role. As the chief negotiator on the labor agreements that would accompany NAFTA’s passage, Mr. Katz recalled, he worked out an $8 billion package to bolster unemployment insurance, expand job training and relocation assistance and increase other transition programs for every worker affected by trade, whether in a union or not. Union leaders balked, he said. They simply wanted to kill NAFTA. Short of that, they wanted any trade adjustment assistance to go through the unions to union workers. The $8 billion package became a $50 million-a-year program administered through the unions, available only to workers who could show that they lost their jobs because of international trade and the movement of factories to Mexico and Canada. Workers dismissed trade adjustment assistance as burial assistance. There were also missed opportunities: Mr. Furman said the Obama administration’s timid response to the financial crisis prolonged the slow, frustrating recovery, intensifying the anger that Mr. Trump tapped into in 2016. And Mr. Clinton’s balanced budgets and record surpluses in the late 1990s had quickly been squandered by Mr. Bush. But there, too, political reality played a part. Republicans controlled Congress. “Do I wish Clinton had spent the surplus on great things instead of handing it to George W. Bush? Yes,” Mr. Furman said. “Do I think he could have spent it on all those great things in a divided government? No.” A Biden recalibration, undone by inflation If any Democrat intuitively understood the voters who were abandoning his party, it was Mr. Biden, who campaigned in 2020 as “Scranton Joe,” the product of a small, deindustrialized city that epitomized the ground lost by the working class. His victory may have been fueled by the pandemic, but his focus was on economics. He tried to undo or reverse some of the damage that had been done by his predecessors. He brought in left-leaning economists like Mr. Bernstein and Heather Boushey, who had often been voices of dissent in the Clinton and Obama years. His chief at the Federal Trade Commission, Lina Khan, zealously tried to break up monopoly industries. The United States Trade Representative, led by Katherine Tai, steadfastly avoided pursuing new trade deals that might rankle labor leaders, instead focusing on issues like strengthening labor rights in Mexico. The new administration ushered out the belief that healthy financial markets, low unemployment and adequate support for people with the lowest income were enough to sustain an economic growth whose benefits would be shared broadly. None other than Robert Rubin, the former Clinton Treasury secretary most associated with the Democratic shift toward promoting economic growth and market stability, called the Biden recalibration “constructive.” The president largely confined his “industrial policy” to promoting domestic manufacturing in arenas like semiconductors, which are vital to economic and national security, and to combating climate change, which unfettered free markets have failed to address, Mr. Rubin said in an interview. The Biden administration also moved to bolster the clout of unions, drive down unemployment so workers would gain bargaining power and strengthen the Internal Revenue Service to go after affluent tax cheats, Mr. Bernstein said. Mr. Biden did not have the surplus of federal dollars to invest that Mr. Clinton had bequeathed to his successor, so he guided private investment through regulations and huge tax credits secured through Congress. “A trillion dollars of private investments have already been announced and are underway,” said Lael Brainard, the director of the Biden National Economic Council. “That’s a pretty remarkable number. Factory construction has doubled relative to the Trump administration — doubled.” A “worker-centered trade policy” strengthened so-called Buy America commitments, maintained most of Mr. Trump’s tariffs on foreign products and pumped hundreds of billions of dollars into new American infrastructure and factories. “Our new approach to trade recognizes people as more than just consumers, but also producers,” Ms. Tai said in a 2023 speech, “the workers, wage-earners, providers, and community members that comprise a vibrant middle class.” If all of that was a corrective for policies past, the working class proved to be in an unforgiving mood in November. Ms. Harris saw some electoral gains among union workers. But she lost far more ground in the much larger, nonunion work force. In November, 56 percent of voters without college degrees voted for Mr. Trump. In 1992, just 36 percent of voters with only a high school diploma voted Republican — about the same percentage that Barry Goldwater got in his overwhelming defeat against Lyndon Johnson in 1964. Republican and Democratic economists point to a single reason: inflation. Mr. Reich’s “anxious class” was as anxious as ever, unwilling to see policy shifts that might take years to bear fruit as a salve for the immediate pain of rising prices. Democrats said the president was the political victim of a global trend emerging from the pandemic. Republicans pointed to his policies, and one piece of legislation in particular, the $1.9 trillion American Rescue Plan, saying it poured gasoline on the smoldering embers of post-pandemic inflation. “The American Rescue Plan killed the Biden administration in its infancy,” Mr. Holtz-Eakin said, almost ruefully. “It was the worst thing they could have done, and they did it. They were warned, and they did it anyway.” Ana Swanson contributed reporting from Washington. How Republicans Use Slogans to Win Voters: Michael's Quick Take Author: The Bulwark, Date: 2024-12-14 Collections: Hot Takes US Elect 2024 Zotero Key: DP4RVVII Cite Key: TheBulwark24HowRepublicansUse Zotero Item | Lit Note 00:00 Trump uh was able to um work this I idea 00:05 and Republicans too this sort of the 00:08 sloganeering aspect of reinforcing 00:10 messaging and 00:12 reinforcing uh the content they wanted 00:14 to get so you know the goal for Trump 00:16 was to repeat his slogan so much that 00:18 they became conventional wisdom that's a 00:21 strategy that the writers employed with 00:23 great success for decades true uh back 00:26 in 2004 the daily shows John Stewart 00:29 explained conventional wisdom is the 00:30 agreed upon understanding of an event or 00:32 person John KY is a flip flopper George 00:36 Bush has sincere Heartland values and is 00:39 stupid what matters is not that the the 00:42 designation be true just that it be good 00:45 upon the media uh uh be agreed upon by 00:48 the media so that no further thought has 00:51 to be put into it so how does 00:54 conventional 00:56 wisdom be get arrived at how do we 00:58 settle in that we we take the bits and 01:01 pieces we like we we personalize them 01:03 and go oh that makes me feel real good I 01:07 that's me I agree I mean basically yeah 01:10 to some degree look the the reason that 01:12 conspiracy theories stick is that people 01:14 are looking for a neat explanation for 01:16 what they see in front of them and for 01:18 example you know if there's if there's 01:21 crime for example uh you have a 01:23 republican who will come forward and say 01:25 that the crime is the result of 01:26 immigrants pouring into the country even 01:28 though we know based on based on um 01:30 based on analyses and based on reporting 01:33 that immigrants and and foreign born 01:35 foreign born folks who come into this 01:36 country um commit crimes at 01:38 exponentially lower rates than native 01:40 born Americans but it fits a neat 01:42 narrative that Republicans will push 01:44 through when you add in the fact that 01:46 Republicans are very good about simple 01:48 messages compare that with what 01:50 Democrats do Al Franken says that 01:51 Democrats bumper stickers always end 01:53 with continued on next bumper sticker 01:55 that's the problem right there I mean 01:57 even as a Democrat myself I am attracted 02:01 to politicians who put forward extensive 02:04 plans I want to see I want to see policy 02:06 I want to see plans I want to see 02:08 legislation but the vast majority of 02:10 Americans aren't going through a 16-page 02:12 proposal about child care they're 02:14 listening to this guy and Donald Trump 02:16 who is charismatic to his base say you 02:18 know build the wall and that's a lot 02:20 easier to understand that's a lot more 02:22 that's a lot more uh concise that's a 02:24 lot more um uh attractive for people who 02:27 are looking for some for some scapegoat 02:30 for their own problems and Republicans 02:32 have tapped into that in a way that 02:33 Democrats haven't so let me give you a 02:35 good example of what you're talking 02:37 about in 2010 uh in trying to look at 02:39 the strategy there were two things that 02:42 that came to me in in various 02:45 conversations one was inside my own tent 02:48 with Republican leadership in which I 02:50 tried to impress upon them the 02:51 importance of what Barack Obama was 02:53 doing and how it was starting to 02:55 resonate with voters um and there was 02:58 there was an inside gang strategy that 03:01 really didn't want to confront that 03:03 that's why we have never to this day my 03:05 friend put up an actual policy that is a 03:08 counterproposal to 03:10 Obamacare because the politics of 03:12 Obamacare is 03:14 richer than the solution of healthc care 03:17 yeah even though as I pointed out to 03:19 folks in the meeting you know Obamacare 03:22 is the Heritage healthcare plan right 03:25 from the 1990s so there was that the 03:28 other was the moment that uh I knew I'd 03:31 won the the narration of the story 03:35 around Obamacare was the day that Barack 03:38 Obama himself standing behind the podium 03:41 referred to his own health care plan the 03:43 Affordable Care Act as Obamacare yeah 03:46 and when I get when I get my opponent to 03:49 use my language in describing their 03:52 stuff yeah he's 03:54 one touchdown the second the second one 03:58 actually was interesting to your point 04:01 was the narrative Around The 04:04 Branding so you come up with the slogan 04:07 fire 04:08 Pelosi right so a lot would go went into 04:12 that it wasn't just about oh let's get 04:15 let's let's if you want to take control 04:17 of the Congress you've got to fire 04:19 Pelosi right to win the house right 04:22 meaning I got to elect more Republicans 04:24 and Democrats yeah well what we did with 04:26 The Branding on that was we took that 04:29 concept 04:30 where those two words became synonymous 04:33 for every other ill that you had with 04:35 government with government if you want 04:37 to fix that lamp poost you got to fire 04:39 Pelosi meaning you got to change the 04:41 government well it's it's smart too 04:43 because it allows you to supersede 04:45 people's allegiances with their local 04:47 Congress people and and that's that's 04:50 usually they they usually have closer 04:52 relationships than than people's 04:55 allegiances with with Statewide 04:57 officials or even the presidential 05:00 so and so you might you might feel like 05:01 okay look there are always elected 05:04 officials who are elected in their 05:06 districts who might be of the opposite 05:08 party I mean Marie Glen Camp Perez is 05:10 elected in a in a very conservative 05:11 District but if you make it not about 05:13 her for example but make it about Pelosi 05:15 more broadly and give it and make it 05:17 about the mission more broadly then 05:19 that's going to be able to allow um 05:22 these voters who might be more inclin to 05:24 vote for uh uh a member of Congress of 05:27 their party to make it to make it about 05:28 Nancy Pelosi and not these people then 05:30 that allows you to kind of like jump 05:32 jump over uh the hurdle of people who 05:35 might be more closely aligned with their 05:36 member the Democrats couldn't make it 05:39 about Donald Trump well in in this past 05:42 election you mean yeah not just this 05:44 past election but even in 16 I mean 05:46 think about it I mean 16 would have been 05:48 the time to actually snuff out that 05:51 flame before became a roaring far far I 05:54 do think I do I mean look to to claim 05:56 that that these elections haven't been 05:59 about Donald Trump I think I I I don't 06:01 know how they could be more about Donald 06:03 Trump here but again point it was that's 06:06 my point unlike the making the election 06:08 about Pelosi in 2010 I see noticed I did 06:12 not make it about Barack Obama yeah I 06:14 didn't say it it's smart because 06:16 Republicans are always better about 06:18 finding finding the most potent villains 06:20 and you know basically the last 20 years 06:22 30 years in Republican politics has been 06:24 has been finding people um to cast as 06:27 Boogeyman and making them the 06:30 enemy you know more broadly making them 06:32 the cause of all of the problems that 06:34 you're facing right now that has been 06:36 what Republicans have been historically 06:38 really good at I mean even even when 06:40 it's not just one specific person even 06:41 when Hillary Clinton was was off the 06:43 scene then it become then it became an 06:45 issue of Migrant Caravans that were 06:47 coming in and every September or October 06:50 uh of an election year it would just be 06:51 about migrant Caravans and you can't let 06:53 those people in because they're the 06:54 reason that you don't have jobs they're 06:55 the reason that Americans are on drugs 06:58 they're the reason that that that white 06:59 people are becoming a minority in this 07:01 country so Republicans have been really 07:03 effective at casting either one person 07:05 or a group of people as the cause of all 07:08 of the problems that you're contending 07:10 with so with that in mind what 07:16 were ry's like oh no what what are you 07:18 about to ask me now um what worked in 07:22 2024 for 07:24 Democrats well look I I do think that 07:29 and and you may disagree with me on this 07:30 but I don't know that the Democrats 07:32 actually could have put up a candidate 07:33 who would have been able to overcome the 07:36 economic environment that we were 07:37 contending with I mean you you know full 07:39 well that there wasn't a single 07:41 incumbent in in a in a uh industrialized 07:44 country in the entire world who either 07:46 stayed in office or didn't lose seats in 07:48 office and so the US was no different a 07:50 lot of times people are just looking to 07:51 punish those in office um for the 07:53 economic environment right in front of 07:55 them there was high inflation it was a 07:56 global phenomenon uh leaders across the 07:59 entire Globe were ousted as the result 08:00 of that the US was no different in terms 08:03 of what what the Democrats could do 08:05 better could have done look I I actually 08:08 think that KLA Harris ran just about 08:11 just about the best campaign that she 08:12 could have run given given the the 08:15 barriers that she was contending with we 08:17 were talking about a 100 day campaign 08:19 it's easy to Monday Morning Quarterback 08:20 but but there was not a lot that she 08:23 could have done granted she could have 08:24 answered a few questions differently I 08:25 think she should have come out and said 08:27 that there were there was a broader 08:29 difference between her and Joe Biden 08:30 somewhere I think Joe Biden would have 08:32 been fine with that or he should have 08:33 been fine with that and just should have 08:35 just said like hey kid I get it this is 08:37 how politics Works distance yourself 08:40 from me here XYZ and go out and win this 08:42 election because it's not about 08:43 protecting my ego it's about protecting 08:45 protecting our agenda and uh and so that 08:48 was a misstep but otherwise largely you 08:51 know it was a well-run campaign and I 08:53 think there was a decent Embrace of 08:56 alternative media I think Democrats more 08:58 broadly do have to get much much much 09:00 better at figuring out where they spend 09:02 their money you had spoken about this 09:04 earlier but look Democrats spent 09:06 Democrats raised and spent a billion 09:08 dollars in this election cycle how much 09:10 of that went toward some permanent 09:13 messaging distribution system how much 09:16 of that are Democrats going to see in 09:18 this month December of an of an election 09:21 year that we can then sit down and do 09:23 the work in December and then January of 09:26 25 February March April May all these 09:29 months where we're not actually running 09:31 for reelection right now we're just kind 09:33 of doing the work that's going to be 09:34 informing the American people of what's 09:36 going on of where Donald Trump is 09:37 dropping the ball of what Republicans 09:39 are doing of how they're not abiding by 09:40 the same promises they made during the 09:42 campaign tra how much of that money went 09:43 to building up that distribution system 09:46 none of it it's always just setting a 09:48 billion dollars on fire and then 09:50 wondering four years from now why we're 09:52 not able to reach our people the same 09:55 way that Republicans are able to reach 09:56 their people I mean if we had put 10 10% 10:00 5% 1% toward building up some permanent 10:04 infrastructure where we can actually 10:06 message the rest of the year think about 10:08 the dividends that that will play that 10:10 that that that will pay and yet the fact 10:12 that we opt not to do that until we do 10:14 until we recognize the importance of 10:16 having a permanent uh Independent Media 10:18 infrastructure until we um build up the 10:21 importance of of making sure that 10:23 Democrats um allies on the outside are 10:26 able to continue informing people of 10:28 what's going on and the virtue ues of a 10:30 progressive platform or a Democratic 10:32 platform until we do that we're just 10:34 going to continue to to wander in the 10:36 wilderness 10:37 here you hit on a number of I think 10:40 interesting points from my perspective 10:42 as a as a sort of a campaign guy and 10:44 elected former elected official having 10:46 been on a ballot and trying to carry the 10:49 messaging that you're talking about 10:52 um it it really does raise some 10:55 interesting points as the party looks at 10:58 look 10:59 transition uh I I did a a a piece for 11:02 MSNBC daily of about the next D DNC 11:07 chairman understanding um the value of a 11:10 50 state strategy which I borrowed from 11:14 uh Howard Dean yeah who was the first 11:16 chairman to really see the country 11:18 through that lens um and he was 11:21 successful and I was successful uh so 11:24 there must be some value to 11:26 understanding that the middle of the 11:27 country does exist and that there's 11:29 something that progressives or 11:32 traditional Democrats can say to the 11:35 middle of the country not just to the 11:37 East and the West Coast um there's that 11:40 piece of what what you're saying the 11:42 other though which I think is 11:44 interesting 11:46 is how do you then begin to marry 11:51 because this was this was my challenge 11:53 as a newbie chairman National chairman 11:55 I've been a chairman for state and local 11:58 but it's it's a different game when I 12:00 now have to account for South Carolina 12:02 and Washington state right two different 12:05 kinds of Republicans 12:08 um how do you then find that sweet spot 12:12 for the rising Progressive voices inside 12:16 the the Democratic party one the more 12:20 traditional the DLC Bill Clinton uh line 12:24 of of Democrats 12:27 too and the 12:29 coalitions oh my God you got coalitions 12:32 for days right uh within that body how 12:36 does a chairman begin to knit that those 12:40 pieces together in light of what we've 12:43 just witnessed in 2024 where the country 12:45 is basically said we're just gonna take 12:48 a quick step to the right for a moment 12:51 yeah and say we're gonna be there 12:52 permanently but we just going to move a 12:54 little bit more to the right from New 12:56 York to Florida fromg Virginia to 13:00 California that's what we saw well how 13:03 do you how do you assess that from your 13:05 listening to your audience what the you 13:08 know the work you've done out there uh 13:11 connecting with with citizens who are 13:13 voters how do you see it so I do and 13:16 this might be my bias as somebody who 13:17 works in the media but but I do believe 13:20 that this is largely a function of the 13:22 media and our messaging and not so much 13:24 the policies because if you look at 13:26 Democratic or Progressive policies they 13:29 win out when they're when they're 13:30 isolated on the ballot when you look at 13:32 abortion rights they win out regardless 13:34 of where you are I mean 13:35 Democrats voted in Kansas and okah 13:39 Kansas Kansas Ohio Virginia Alabama de 13:44 uh that that that question was winning 13:46 out if you look at the issue of 13:47 marijuana that generally wins out if you 13:49 look at uh the issue of right to work 13:51 you know Democrats are on the right side 13:53 of a lot of these issues and yet and yet 13:56 it's our politicians who lose out and 13:58 that goes to show that the policies are 14:00 popular but the brand isn't and I to To 14:04 My Grave I will bring this this belief 14:07 that that's largely a function of how 14:09 people receive their information and 14:11 watch as Democrats either get vilified 14:14 because of you know the the boogeyman 14:15 phenomenon that we were speaking about 14:17 earlier but also because I think 14:19 Republicans are really good at homing in 14:21 on uncertain issues that frame the left 14:25 as these Fringe figures I mean you you 14:28 look at and this is obviously an 14:29 overused example but gender reassignment 14:31 surgeries for Trans inmates in prison 14:34 and this is you know maybe not 14:36 necessarily so much about the actual 14:39 issue itself but it it goes to frame 14:41 Democrats in a light of like this is who 14:43 the Democrats are focused on and look 14:45 this issue comes from I believe an ACLU 14:48 questionnaire I mean it's an edge case 14:49 issue right but Dem but Republicans were 14:52 able to pull that one thing put a 14:55 quarter of a billion dollars behind an 14:57 ad campaign and and isolate themselves 15:00 in a right-wing ecosystem that wouldn't 15:02 push back against it and so people 15:04 largely believed they believe there was 15:06 there was some polling that shows of 15:08 Swing voters that actually went to Trump 15:10 in the end it was because they viewed 15:12 kamla Harris as being more focused on on 15:15 Fringe issues in society and and that 15:18 add is a testament to exactly that and 15:20 so that's that's I guess the broad 15:23 disadvantage of the asymmetry at play in 15:25 the media right now is that you can have 15:27 a right-wing media ecosystem that does 15:29 that and that doesn't challenge this 15:31 notion even though we all know that KLA 15:33 Harris not only didn't make the issue of 15:35 gender reassignment surgeries in prison 15:37 among Trans in inmates an issue in this 15:40 election she's actually disavowed and 15:42 this comment didn't even occur in this 15:43 election cycle 15:45 from but they're so effective at taking 15:48 an issue and just making it the issue of 15:50 the election regardless of what the 15:52 truth about it actually is and so what 15:54 you're left with is an election that's 15:56 largely a referendum on whatever 15:58 Republican want to make it a referendum 16:00 on and the only way to combat that is if 16:03 we're able to be a to to have some 16:06 messaging distribution system that can 16:08 counter what they have on the right so 16:10 that instead of playing defense all the 16:12 time and saying no no no that's not what 16:14 she's doing we promise instead of that 16:17 we actually can go on offense and make 16:18 them answer for a lot of the policies 16:20 that are that are in fact being 16:22 presented by Donald Trump that aren't um 16:25 that aren't being well received by the 16:26 American people or that wouldn't be well 16:27 received by the American people 16:29 that that's a very good point because 16:32 the reality on the transgender issue 16:34 which I thought was somewhat ironic was 16:37 uh some of the people who were the 16:40 loudest in regards to that have have 16:43 their own 16:46 issues um family members who are 16:48 transgender I mean you know so it to be 16:51 able to compartmentalize off those 16:53 people in your 16:55 life about whom you're now talking and 16:58 and disparaging is is a unique 17:01 Republican trait uh oh yeah to be able 17:05 to have a transgender I mean Michael you 17:08 you had denes D Souza come out maybe a 17:11 week or two ago and trash Joe Biden for 17:13 issuing a pardon against Hunter denes 17:16 duza got a pardon from Donald Trump 17:18 compartmentalization is their superpower 17:20 it's a superpower it is so true it is so 17:23 true uh and and we use our superpowers 17:26 well so 17:34 [Music] What the Left Refused to Understand About Women’s Sports Author: Lewis, Helen, Date: 2024-12-30 Collections: Hot Takes US Elect 2024, MediaAdsPolit, IdentityPolitics, Polarization Zotero Key: V3UL29HS Cite Key: Lewis24leftUnderstandRefuseSportTrans Zotero Item | Lit Note Sia Liilii comes from a big family in Hawaii, the ninth of 11 children. Without her volleyball scholarship at the University of Nevada at Reno, she told me recently, she would never have been able to go to college. So when she got wind this past summer that one of Nevada’s opponents in the Mountain West Conference, San Jose State University, was fielding a transgender player, she rebelled. “It’s not right that this person is taking not only a starting spot but a roster spot, from a female who has, just like us, played volleyball her whole life and dreamt of playing at the collegiate level,” Liilii said. The story of transgender women competing in female sports is frequently told as one of inclusion—creating opportunities for people to compete as their authentic selves. But for athletes such as Liilii, these rules were a matter of exclusion. Every spot taken by someone with a male athletic advantage is an opportunity closed to a female rival. Other players in the conference, it turned out, had concerns similar to Liilii’s. In particular, some worried whether a ball spiked over the net by a stronger and more powerful player could injure them. Those concerns would ultimately lead Nevada and other teams to forfeit games to San Jose State, in the largest-scale protest yet by female athletes against the presence of a trans competitor. More than 200,000 women compete in college sports in the United States, according to the National Collegiate Athletic Association, and more than 3.4 million girls take part in high-school sports, according to the National Federation of State High School Associations. Questions of fair competition tend to resonate intensely with both athletes and their supporters. Sports organizations set rules to minimize unwarranted advantages—witness the restrictions on high-tech sharkskin-inspired swimsuits and running shoes with carbon-fiber plates. But while Nike estimates that its VaporFly sneakers give a 4 percent boost to wearers, the performance gap between men and women is estimated to vary from 10 to 50 percent, depending on the sport. Yet progressives have downplayed that sex difference—which is obvious to many casual observers—because it challenges the idea that transgender women should be treated as women in all circumstances. Jonathan Chait: Moderation is not the same thing as surrender On Joe Biden’s first day in office as president, he issued an executive order opposing discrimination on the basis of gender identity. Its language did not explicitly address college athletics but declared that all “children should be able to learn without worrying about whether they will be denied access to the restroom, the locker room, or school sports.” After the 2022 midterms, LGBTQ organizations assured Democrats that Republican attack ads about trans athletes in female sports were ineffective—the issue was too far down voters’ list of priorities, they argued. Yet by this fall, Donald Trump’s campaign was pummeling the Democratic nominee, Kamala Harris, with a spot that showed, among other images, a 2012 picture of Gabrielle Ludwig, a 50-something basketball player who had returned to college after transitioning. At 6 feet 6 inches tall, Ludwig towered over her teammates. Harris’s campaign reportedly tested several rebuttals, and found that none of them worked. So how did Democrats move from proudly championing trans inclusion in Biden’s early days as president to finding the topic an unanswerable liability three years later? Why did the left refuse to acknowledge the trade-off between inclusion of some athletes and fairness to others? Why were concerns like Sia Liilii’s so easily ignored? Many progressives have viewed trans rights as an uncomplicated sequel to the successful campaigns for voting rights for Black Americans and marriage equality for same-sex couples. But the volleyball players were pointing to an issue that affected two traditionally marginalized groups: gender-nonconforming people and women athletes. And the left, which had become attached to a simple, hierarchical ranking of oppression, could find no way to arbitrate between the two groups—or even acknowledge that any conflict existed. In the 19th and 20th centuries, American women fought for the right to play sports at all. They were excluded by arbitrary rules, inadequate facilities and funding, and the belief that competition was unhealthy and unfeminine. The 1972 passage of the law known as Title IX, which prohibited discrimination “on the basis of sex” in educational settings, began to improve the situation for college athletes. But in recent years, lawyers have argued over what the law means—does sex cover only biological sex, or gender identity and sexual orientation? Almost everyone agrees that, in most sports, men and women should compete in different categories. The argument is over whether the lines should be drawn by athletes’ genes or their experience of gender. Many articles in the popular press have portrayed the growing visibility of trans athletes as a sign of social progress. In 2021, the New Zealand weight lifter Laurel Hubbard was heralded as the first openly trans athlete to compete in the Olympics. In a lengthy 2022 profile, the University of Pennsylvania swimmer Lia Thomas told Sports Illustrated, “I just want to show trans kids and younger trans athletes that they’re not alone. They don’t have to choose between who they are and the sport they love.” Some high-profile female athletes have championed this spirit of inclusion. The former U.S. women’s soccer captain Megan Rapinoe has described restrictions on trans players as “trying to legislate away people’s full humanity.” Throughout the Biden administration, activist groups waved away tough questions, claiming that there was no evidence of “trans athletes” having advantages. But such generic phrasing is deceptive. No one is arguing that trans men have an advantage over biological males; when trans men compete in the male category, they tend to struggle. The actual question is whether natal males have an advantage over natal females. Liilii told me that when she raised the issue with her coaches at Nevada when the players were deciding whether to play against San Jose State, one of the college staff told her to educate herself on the topic, “really implying that we weren’t smart enough to know what is happening.” For all the plaudits that Lia Thomas received from some quarters, she also came to symbolize others’ concerns. Thomas was a higher-ranked swimmer in the female category than she had been in the male one a few seasons earlier. She had ranked 65th among men in the 500-yard freestyle, for example; she won an NCAA championship in the women’s event. Greater awareness of Thomas and other trans athletes in women’s sports did not translate into greater approval. If anything, the opposite occurred: In 2021, 55 percent of Democrats supported transgender athletes competing in the team of their chosen gender, according to Gallup. Two years later, however, that number had fallen to 47 percent. Overall, nearly seven out of 10 Americans now think athletes should compete in the category of their birth sex. Read: The Democrats need an honest conversation on gender identity By 2023, the Biden White House seemed to be backing away from the sweeping language in its earlier executive order. The administration proposed to give schools and universities more leeway to limit trans athletes’ participation while prohibiting states from enacting blanket bans. The situation remained in flux when the college volleyball season began this year. Under USA Volleyball rules, trans athletes who take “the necessary steps to transition to their adopted gender,” including lowering their testosterone levels, are allowed to compete in the women’s category. The extent to which hormone suppression negates male athletic advantage is a matter of scientific debate. But when Liilii saw videos of the disputed player during the preseason, she remembers thinking, “The way this person is jumping and hitting the ball—I’ve never seen a woman do that.” (The player has not publicly confirmed her transgender status, so I’m choosing not to name her. She did not respond to requests for comment for this story. In legal filings, San Jose State has neither disputed that it was fielding a transgender player nor identified the athlete in question. “Our student athletes are in full compliance with NCAA rules and regulations,” a university spokesperson told me by email.) In September, the San Jose State co-captain Brooke Slusser and the associate coach Melissa Batie-Smoose went public with their concerns about their own team’s trans player. “Safety is being taken away from women,” Batie-Smoose later told Fox News. “Fair play is taken away from women.” Both women told Quillette that they believed players and coaches were being pressured not to make a fuss. The next month, Liilii told me, she and her Nevada teammates voted, 16**–**1, to boycott their next match against San Jose State. The Nevada players were not alone: Teams from Boise State, the University of Wyoming, Southern Utah, and Utah State also forfeited games rather than face the trans player. San Jose State kept competing despite all that—and despite a lawsuit aimed at barring the school from the Mountain West Conference postseason tournament in Las Vegas in November. (The lawsuit failed, and the team finished second in the finals.) The season ended in acrimony. “I will not sugarcoat our reality for the last two months,” San Jose State’s head coach, Todd Kress, said in a statement after the tournament. “Each forfeiture announcement unleashed appalling, hateful messages individuals chose to send directly to our student-athletes, our coaching staff, and many associated with our program.” Afterward, seven of the team’s athletes requested to enter the transfer portal. The disputed player, who is a senior, will not compete again. By the time of the tournament, San Jose State’s roster had become a national political issue. Sia Liilii told me that after her team put out its statement refusing to play the California school, one of their next matches was attended by Tulsi Gabbard, the former Democratic member of Congress whom Trump has nominated to be director of national intelligence ; Sam Brown, the Republican candidate for Senate in Nevada; and Senator Markwayne Mullin of Oklahoma. “That was really reassuring,” Liilii said, “just seeing that there’s a lot of support.” Clearly, many on the right felt that a revolt in women’s volleyball had the potential to connect with voters. Meanwhile, on the left, people who questioned the activist line—including the tennis legend Martina Navratilova, a longtime progressive—were being excoriated for their supposed bigotry. “People like to say that it’s a complicated issue, and I don’t actually think it is … It all boils down to: Do you actually think that trans women and intersex women are real women—and are really female or not?” the transgender cyclist Veronica Ivy told The Daily Show’s Trevor Noah in 2022. “It’s an extreme indignity to say, ‘I believe you’re a woman, except for sport.’” She added that the enforcement of traditional categories was about “protecting the fragile, weak cis white woman from the rest of us.” Noah’s studio audience in New York heartily applauded Ivy’s words. Sports was only one part of a seamless whole: If you believed, as good liberals did, that trans women were women, no carve-outs were justifiable. In red America, however, a different narrative was developing. The same year that Ivy was soaking up the Daily Show applause, Riley Gaines, a University of Kentucky swimmer who had competed against Lia Thomas, went public with her objection to trans inclusion in her league. She recalls feeling slighted after a race in which she tied with Thomas for fifth place but the Penn swimmer got to hold the relevant trophy. “It took that personal experience,” Gaines told me. “I hate that it took that. I wish I was more bold.” In March 2024, her profile exploded when she was interviewed by Joe Rogan. Here was an everyday Christian girl talking to a sports-mad superstar podcaster about how the left was trying to deny that men are stronger than women. If thousands of YouTube comments are any indication, Rogan’s audience loved it. Gaines has joined a lawsuit against the NCAA, calling for a nationwide ban on transgender women in female categories. The ACLU and other advocacy groups on the left have intervened to oppose Gaines’s suit, suggesting that conservative slogans about “protecting women’s sports” are a cover for racism, transphobia, and misogyny. The National Women’s Law Center believes that “the work of gender justice is at odds with overbroad generalizations about sex-related traits or abilities” and suggests that the “over-policing” of athletes’ bodies particularly harms minority women. By contrast, conservatives have welcomed female athletes who feel abandoned by American feminist and civil-rights groups. Today, Gaines, Liilii, and other female athletes who have spoken out on this issue have signed up to be ambassadors for the Independent Women’s Forum, a nonprofit that previously criticized what it saw as overzealous enforcement of Title IX. In 2012, the group’s then executive director wrote that “what is very clear is that legislation in the name of ‘gender equality’ does not actually make men and women the same.” However, the group now fights to “take back Title IX” by separating participation in sports on the basis of biological sex. People “love to receive information through stories,” May Mailman, the IWF’s director and a former White House adviser to Donald Trump, told me. “The left knows this—George Floyd is one story that sparked immense societal unrest.” During the presidential campaign, the IWF sent its ambassadors on a cross-country bus tour that started in Scranton, Pennsylvania, under the slogan “Our bodies, our sports.” The group’s ambassadors have also testified before Congress and in states considering restrictions on transgender women participating in female sports. The IWF’s ideological opponents may dismiss these athletes as political partisans. But even if some are, so what? Conservatives have a right to speak up, and the institutional left certainly didn’t listen to the players’ concerns. Progressives can’t expect to triumph by silencing dissenters through administrative pressure. Helen Lewis: The push for puberty blockers got ahead of the research One of the most influential IWF ambassadors is Payton McNabb, who says she received a brain injury in 2022, at the age of 17, when playing volleyball against a transgender opponent. A widely circulated video of the incident shows the spike that hit her, but not what happened subsequently. She told me that she was briefly unconscious. “The neurologist told me that I had a brain bleed, partial paralysis on my right side, and a concussion,” she added. (She declined to provide her medical records for me to verify her account.) Her story is the kind that is invisible to a certain type of American media consumer but achieves the status of lore with another. She has been interviewed by Fox News, Megyn Kelly and the New York Post, and on the podcast of Allie Beth Stuckey, a rising star on the religious right who was described in The Atlantic as the “new Phyllis Schlafly.” In August 2023, McNabb testified in front of the North Carolina legislature after Governor Roy Cooper, a Democrat, vetoed a law that banned athletes “of the male sex” from competing on women’s teams. All of the state’s Republicans, along with two Democrats, later voted to override the veto. During the hearings, it emerged that in the four years that the North Carolina High School Athletic Association had permitted transgender players to choose their teams, only two natal male students had successfully applied to play as girls. That can be read two ways. One is this: Why were Republicans making such a big deal out of an issue that affects so few students? The other is this: Why did Democrats, a few years ago, make such a big deal out of an issue that affected so few students? After the 2024 election, a handful of Democrats broke ranks. “I have two little girls,” Representative Seth Moulton of Massachusetts told The New York Times. “I don’t want them getting run over on a playing field by a male or formerly male athlete.” His campaign manager subsequently resigned, protesters gathered outside one of his offices, and he was rebuked by the state’s Democratic governor. But many of Moulton’s fellow Democrats were notably silent. “Asked for comment on Mr. Moulton’s remarks, each of the 10 other members of the state’s congressional delegation, all Democrats, declined to comment or did not immediately respond,” the Times reported. Further evidence that a taboo had been broken came on the Friday before Christmas. The White House abandoned its proposed rule change forbidding blanket bans on trans athletes after 150,000 public responses, acknowledging that the incoming Trump administration will set its own rules. Meanwhile, many international sports organizations have opted to define their women’s division in biological terms. This past summer, Lia Thomas lost her legal case against World Aquatics, which had barred her from the female competition. A leading contender to be the next head of the International Olympic Committee, Britain’s Sebastian Coe, has said that “the protection of the female category, for me, is absolutely non-negotiable.” Those who favor defining women’s sports according to biology feel confident that their side will prevail. “I have nieces, and I have little sisters,” Sia Liilii told me. She said she was happy “knowing that I did the right thing, and knowing that when they are in my position, they won’t have to deal with this.” Read: I detransitioned. But not because I wasn’t trans. In my view, the way forward lies in an empathetic compromise, one that broadly respects transgender Americans’ sense of their own identity—for example, in the use of chosen names and pronouns—while acknowledging that in some areas, biology really matters. Many sports organizations have established a protected female category, reserved for those who have not experienced the advantages conferred by male puberty, alongside an open one available to men, trans women, trans men taking testosterone supplements, and nonbinary athletes of either sex. Unlike Veronica Ivy, many voters who support laws protecting trans people from housing and employment discrimination don’t see trans rights as an all-or-nothing deal; in fact, a few limited carve-outs on the basis of biological sex might increase acceptance of gender-nonconforming people overall. Not everything has to be an entrenched battle of red versus blue: As more and more Democrats realize that they shouldn’t have built their defense of trans people on the sand of sex denialism, Republicans should have the grace to take the win on sports and disown the inflammatory rhetoric of agitators such as Representative Nancy Mace, who responded to the election of the first trans member of Congress by deploying anti-trans slurs. As the second Trump administration begins, the lesson from the college-volleyball rebellion is that institutions cannot impose progressive values by fiat. Attempts at social change will not survive without the underlying work of persuasion. Sia Liilii and other women athletes said no. Universities and sports organizations needed a better response. I Traded My News Apps for Rumble, the Right-Wing YouTube. Here’s What I Saw. Author: Thompson, Stuart A., Date: 2024-12-13 Collections: Hot Takes US Elect 2024, NeuroPsychoLinguisticPolitics, MediaAdsPolit, PoliticalML, Polarization Zotero Key: XCR36A32 Cite Key: Thompson24rumbleRightWingYT Zotero Item | Lit Note By Stuart A. Thompson Stuart Thompson has monitored right-wing media since 2020. He watched 47 hours of video on Rumble for this article. Dec. 13, 2024 As soon as President-elect Donald J. Trump won the presidential race, influencers on Rumble, the right-wing alternative to YouTube, flooded the platform with a simple catchphrase: “We are the media now.” The idea seemed to capture a growing sense that traditional journalists have lost their position at the center of the media ecosystem. Polls show that trust in mainstream news media has plummeted, and that nearly half of all young people get their news from “influencers” rather than journalists. In its place, they argue, are right-wing digital creators who have found hordes of fans online. Rumble, for instance, is tiny compared with YouTube, but it is a primary source of news for millions of Americans, according to Pew Research Center. On election night, its active viewership topped out at more than two million, and the company said in a statement that it averaged more than 67 million monthly active users in the final quarter of 2024. You are the news now.This is a real story.It's not about back slaps, it's about how now, it's time to floor the freaking gas pedal.When you're done with me, go watch Graham Allen.When you’re done with Graham Allen, watch Crowder.When you’re done with Crowder watch Tucker, then watch Rogan or whatever.You don't need old school media anymore. ▶ Dan Bongino, host of “The Dan Bongino Show,” says viewers should follow his program with other Rumble creators in a bid to replace mainstream media. If Rumble was the media now, I wondered what it would be like to consume an all-Rumble diet. So on Nov. 18, about two weeks after the election, I deleted my news apps, unsubscribed from all my podcasts and filtered all my newsletters to the trash. And for the next week, from early morning till late at night, I got all my news from Rumble. An alternate reality I started by visiting Rumble’s homepage on Monday morning where I saw my first recommended video. It was about the risk of nuclear war, with an A.I.-generated photo of President Biden laughing maniacally above a headline that read: Biden Authorizes Strike on Russia Ahead of Trump Taking Office!!” Rumble was once an obscure video platform featuring mostly viral cat videos. Founded in 2013 by a Canadian entrepreneur, it was designed as a home for independent creators who felt crowded out on YouTube. But the platform took a hard right turn around the time of the Capitol riots on Jan. 6, 2021, when social networks and YouTube cracked down on users who violated their rules. Conservatives flocked to other platforms, including Rumble, which quickly embraced its new role as a “free speech” haven — and saw its valuation surge to half a billion dollars practically overnight. Its content today goes far beyond cat videos. Video game livestreams populate its homepage alongside a bizarre face-slapping competition called But political commentary and news remain its most popular categories by far. The front page A screenshot from the first day of this experiment shows videos about WWIII and live categories focused on news, entertainment and “conspiracies.” I chose a selection of popular “news” shows to watch, along with political content from other areas, like its active “conspiracies” section. Because my experiment began so soon after Mr. Trump swept to victory on Nov. 5, I expected many of the videos to feel triumphant. There were a few moments of joy: After the hosts of “Morning Joe,” the MSNBC talk show, visited Mr. Trump at Mar-a-Lago, hosts of Rumble shows gleefully mocked them, saying they went to “kiss the ring and bend the knee.” Clips of N.F.L. athletes doing Mr. Trump’s dance moves were a sign, the hosts said, that Mr. Trump had recaptured popular culture from the clutches of Hollywood liberals. Multiple shows criticized the same clip from “Morning Joe” ▶ Stay Free with Russell Brand But their happiness quickly gave way to a relentless outpouring of anger and frustration, as they fixated on a cast of perceived enemies to blame for America’s troubles — from Democratic politicians to to Republican adversaries. Just a few hours into the experiment, it was clear that I was falling into an alternate reality fueled almost entirely by outrage. Among the claims I heard: Some people at think tanks in Washington were “morons” and “crazier than any schizophrenic.” The Department of Homeland Security a “sex-trafficking operation,” a claim apparently based on a misreading of a government report . (The report, by the Department of Homeland Security’s Office of the Inspector General, indicated that more than 300,000 unaccompanied minors had not received a notice to appear in court or had received the notice but had failed to appear. Some conservative commentators said this meant the children were being trafficked, but experts in immigration policy said it meant no such thing.) Progressives were trying to get Republicans killed — a claim based on death threats that Representative Marjorie Taylor Greene of Georgia said she received. After only one day, I could feel my perspective shifting. When I described to my wife what I was hearing on Rumble, she said I was right to feel uneasy because the world I was immersing myself in sounded genuinely awful. Hour by hour, Rumble’s hosts stoked fears about nearly everything: culture wars, transgender Americans and even a potential World War III. ‘Do you guys know where your fallout shelters are?’ On the second night, while catching up on the show “Redacted,” I heard that World War III was because of rising tensions with Russia but that most Americans were unaware of it. Exactly what to make of this remained unclear to me, but I suspected tensions would need to rise much further before bombs started dropping. Clayton Morris, a former Fox News personality who co-hosts the show with his wife, seemed convinced that nuclear war was coming, describing the lack of fallout shelters in major cities throughout the United States. (I later read news articles that offered a fuller picture, suggesting that the risk of escalation was real but that nuclear threats were also a strategy in Mr. Putin’s saber rattling.) We have zero infrastructure in the United Statesthat could save lives in a critical crisis like this.No fallout shelters, no contingency if Russia,you know, fires against, you know, fires missiles into Nashville,Los Angeles, Buffalo, New York City, Los Angeles.Do you guys know where your fallout shelters are?Where you’ll be taken care of?Where food and supplies will be? No, you don’t. ▶ Clayton Morris, co-host of “Redacted,” warns there are not enough fallout shelters in major cities in the United States. The coverage struck me as particularly scary, but I also paused to consider whether Mr. Morris had any credentials as a Russia-Ukraine analyst. Since 2017, he has pivoted his career from hosting television shows to offering investors “financial freedom” through real estate investing. He was sued in 2019 by two dozen clients who said they were sold ramshackle homes as investment properties, then relocated his family to Portugal before the lawsuits were settled — which some said complicated the litigation proceedings. (Mr. Morris denied any wrongdoing.) On Rumble, though, he seemed authoritative: His slickly produced show had more than 560,000 followers and it aired daily with an active comments section filled with supporters. The videos were recommended to me by Rumble’s algorithm, so I kept watching. Other shows referenced clips directly from Russian state television or . During “The Roseanne Barr Show,” a segment about nuclear war for an emergency health kit. (In an email, the show’s co-host Jake Pentland, who is Ms. Barr’s son, told me their show wants to keep Americans “safe and protected from this wildly corrupt administration whether that’s through education or highlighting specific products that can protect them.”) The prospect of an impending World War III stuck with me long after the livestreams ended. As I shuttled my son to day care or walked down aisles at the grocery store, I found my mind drifting to thoughts of nuclear bombs, a military draft or how a global conflict might actually unfold. While watching a segment on the dire prediction, I glanced over at my wife, who was enjoying Netflix’s romantic comedy series “Nobody Wants This,” unaware about the threat of nuclear winter. ‘Who’s in charge now? We are.’ As the days ticked by, I saw how the outrage stoked online could burst into the real world. Early in the week, multiple hosts on Rumble a Democratic official in Pennsylvania who they suggested was trying to steal the election by counting invalid ballots. The controversy gained nationwide attention and the official, Diane Ellis Marseglia, the commissioner for Bucks County, Pa., received profanity-laden emails and death threats. Reading news articles about it later, though, it was clear the situation was more complicated than the hosts had suggested. The courts responded with additional guidance and the county followed the law. The official eventually apologized for using a badly worded statement that stoked the backlash — and her apology video also made the rounds on Rumble. “We are all going to learn lessons from this new media landscape,” Ms. Marseglia said in her apology. “Most of all, I am.” Dan Bongino, the host of “The Dan Bongino Show,” relished the moment. “Who’s in charge now? We are,” he said triumphantly. “Who made this a story? Us.” Who’s in charge now? We are.Who made this a story?Us.You.You’re like, I didn’t—No, you did.People like me and all of these conservative MAGA podcasters sent that video out.And you made it a story. ▶ Dan Bongino, host of “The Dan Bongino Show,” says that right-wing influencers have replaced mainstream media. It seemed clear that actual news — the objective details about complex situations like election proceedings or the war in Ukraine — mattered far less than how these situations could be contorted to support Mr. Trump or deride Democrats. Nearly every show created a visceral feeling that the nation was barrelling from crisis to crisis. Progressives were getting away with galling levels of incompetence or corruption, the hosts said over and over again. Even though Mr. Trump and the Republican Party would soon control the White House and Congress, and conservatives have a majority on the Supreme Court, there were After just a week, this alternate reality started shifting how I instinctively reacted to the world outside Rumble. I would catch a stray story on the local news radio about something innocuous, like train delays or traffic jams, and wonder: “Can I really trust this?” It’s true that listening to any single news source long enough will shift your perspective. But few sources have as many ties to Mr. Trump and his incoming administration as Rumble. Its top personalities are frequently seen with Mr. Trump at events or at Mar-a-Lago, his Florida home, with hosts suggesting they will have special access to the administration. We told you we were going to go to Mar-a-Lagoand we’re gonna, we’re gonna, we’re gonnaget some interesting access in the next couple of years.And, because so many people that’s been in the administrationhave been, like, true friends of us.Been on this program, been a part of this audiencelocked in with you. ▶ Benny Johnson, host of “The Benny Show,” said he expected to have “interesting access” to the Trump administration. Vivek Ramaswamy, Mr. Trump’s pick for a new government efficiency initiative, and Howard Lutnick, the likely commerce secretary, owned millions of dollars worth of Rumble shares when it went public in 2022. So did Craft Ventures, which was co-founded by David Sacks, an investor who sits on Rumble’s board of directors and was recently named Mr. Trump’s pick for cryptocurrency czar. Christopher Pavlovski, Rumble’s founder and chief executive, has emerged as a Trump ally, too. In a post on X, he shared a photo from after the election of him standing next to several people, including Elon Musk, one of Mr. Trump’s most prominent backers. At the back of the frame and grinning was the soon to be 47th president of the United States. “Free speech saved,” Mr. Pavlovski wrote. I received a statement from Tim Murtaugh, a representative for Rumble who was also Mr. Trump’s communications director for his 2020 campaign. He said: “The New York Times and its fellow legacy media outlets have lost their monopoly on deciding what information people can have, so of course they’re rushing to attack Rumble, a key alternative in the news marketplace.” The ‘planet might be saved’ by Trump. The fear and outrage that infused every show was offset by a sense of hopefulness that the president-elect would fix everything — even that the “planet might have been saved” because he was re-elected. It is not an exaggeration to saythe planet might have been savedby the fact that we won the election two weeks ago today. ▶ Charlie Kirk has credited Mr. Trump’s re-election with potentially saving the world. Blame for any hiccups in Mr. Trump’s strategy was assigned to Democrats or even Republicans who were not sufficiently obedient. Senator Tommy Tuberville, an Alabama Republican, that while Republicans controlled the Senate, the party remained “a third MAGA, a third Republican and a third RINO,” meaning “Republican in name only.” “We’ve got control, but do we have control?” Mr. Tuberville summarized. Perhaps the biggest cheerleader for Mr. Trump was Mr. Bongino, the eponymous host of Rumble’s most-watched show, with 3.4 million followers. Mr. Bongino is a former Fox News host who ran three unsuccessful bids for elected office before striking gold in the right-wing commentary business. The podcast version of his show consistently ranks among the top news podcasts in the country. Rumble’s financial documents show that his company, Bongino Inc., owned 5.8 percent of the company when it went public in 2022, now worth more than $100 million. Though I listened to an hour of Mr. Bongino’s opinions each day, it seemed like I learned mostly what various progressive or mainstream media figures had said about different culture war topics, and Mr. Bongino’s predictable reactions to them. Bongino’s focus Many segments on Mr. Bongino’s show included comments from liberals or mainstream news media, along with Mr. Bongino’s predictable reactions to them. Note: Times are approximate On his Thursday show, he talked about the nation’s intelligence apparatus — but it was in response to what about its effectiveness. He talked about cancel culture — but in response to about Matt Gaetz, Mr. Trump’s first pick for attorney general. He talked about identity politics — but in reaction to what a Democratic congresswoman said about race. He talked about the murder conviction of an undocumented migrant — but in reaction to what a news anchor had said about the case on ABC News. Nearly every show I watched on Rumble framed issues this way, focusing on how news was discussed by mainstream media, and then complaining about it. I don’t remember seeing Mr. Bongino criticize Mr. Trump — not once. He spent the first part of the week saying that Mr. Gaetz, the former Republican congressman who was briefly a contender for attorney general, would surely be confirmed. He seemed to dismiss a federal sex-trafficking investigation into Mr. Gaetz by saying it was impossible to find “good” people for top roles. (Mr. Gaetz denied any wrongdoing and the Justice Department declined to file charges.) So all I'm trying to say to you is if you're using good or bad,as a metric to appoint people who could change the country for the betteryou're going to lose,because they're all badso forget it. ▶ Mr. Bongino said anyone seeking political power who could “change the country for the better” is a bad person. When Mr. Gaetz withdrew his name from consideration later that week after significant pushback, Mr. Bongino never faulted Mr. Trump for the whole ordeal. Instead, he blamed Republicans and said it was part of Mr. Trump’s strategy to intentionally overwhelm his critics with controversial picks. ‘You’re going to become part of the show.’ After watching Rumble nonstop for days, I realized this very article was likely to fuel its own cycle of outrage on the platform. But I was surprised when that happened before it was even published. I wrote to everyone mentioned in the article to ask for their perspective about Rumble and its popular shows, but few replied. Instead, people like Russell Brand, the former actor turned political commentator, took one of my emails and made an entire segment out of it. Mr. Bongino called me and claimed my story would focus on Rumble’s fringiest voices in a bid to get the site banned. “Don’t ever email us,” he warned. “Don’t. Because you’re going to become part of the show.” We’re in charge now.Chew on that ----.I’m serious by the way.Don’t ever email us.Don’t.Because you’re going to become part of the show. ▶ Mr. Bongino says any journalists contacting the show will “become part of the show.” Mr. Pentland, the co-host of “The Roseanne Barr Podcast,” posted the email I sent him to his X account. Rumble’s chief executive reposted it, then Elon Musk reposted that to his more than 200 million followers. My phone number was visible, and apparently seen more than 50 million times on the platform, so I was soon flooded with angry phone calls and texts calling my article (which hadn’t yet been published) a “hit job” focused on World War III. On his show, Mr. Pentland referenced my email and said his original ad for a nuclear fallout health kit was meant to “educate our audience” about alternative medicines. Then that segment bled into for the health kit. Stephen K. Bannon, host of “War Room,” relayed a message through his producer, saying that his show “exists as the information arm for the activist cadre at the tip of the spear of the MAGA movement.” Candace Owens, the host of the “Candace Show,” was the only one who called me back. She said she was focusing less on political outrage lately after growing weary of chasing negativity. “I realized I was waking up every day and I was looking for things to be angry at,” she said. “And that wasn’t healthy for me.” Social Media Led to Trump's Rise, America Needs to Grapple With It Author: Longwell, Sarah, Date: 2025-01-10 Collections: Hot Takes US Elect 2024 Zotero Key: SXUUU7KK Cite Key: Longwell25socialMediaTrumpRise Zotero Item | Lit Note 00:00 it used to be that we would live and 00:01 exist in communities where people sort 00:04 of roughly had the same things that we 00:06 had right the the houses were similarly 00:08 sized people had the same kind of cars 00:10 the same kind of stuff and so there was 00:12 this sense that everybody else was doing 00:14 better than you and now we're just 00:18 absolutely throttled all day with images 00:21 of people who are living a better life 00:23 and have more stuff and they're in AA 00:26 look at that just lounging by a pool 00:28 hello everyone it's JV L here with my 00:30 best friend Sarah Longwell publisher of 00:33 the bwar 00:35 Sarah my friend my real best friend even 00:39 though we just had a 00:41 tiff um yesterday Tim had Derek Thompson 00:46 on his Flagship pod to talk about uh a 00:51 giant piece that Derek did for the 00:53 Atlantic about America's loneliness 00:57 epidemic and the 01:02 the big Derrick's thesis is much bigger 01:04 but it is sort of extrapolating from 01:07 Neil Postman and Robert putam into where 01:11 we are today and how we have sort of 01:14 been in the process of destroying our 01:17 social capital at a society 01:22 level and this a very difficult topic 01:25 for me because I 01:28 agree intellect actually with all of it 01:31 but you hate being around people 01:34 personally there is nothing I hate more 01:36 than being around people and for me 01:40 Solitude is like a warm blanket it's a 01:45 gift so uh so for me it's like very 01:48 important not to extrapolate from my own 01:51 personal experiences in this because I'm 01:53 not like other people I'm not even 01:55 really human I'm a pod person and I 01:58 accept that 02:01 but uh what are your thoughts broadly on 02:04 all of 02:06 this about you being a pod person well 02:09 uh no I'm just 02:10 kidding uh I do I will say just for you 02:13 personally you are surrounded on a daily 02:16 basis by more people than most because 02:19 you have this large family uh and so I 02:22 don't think that when they say lonely I 02:25 really do think it has a lot to do with 02:30 people for whom and also you have best 02:32 friends like me I do uh that you 02:34 interact with and yes like we are not 02:38 physically together but we are 02:39 emotionally together uh engaged in a 02:43 cosmologically together yeah at all 02:45 engaged in a in a long conversation 02:48 yearslong conversation like a chess game 02:50 that goes on forever um and so you know 02:54 I just I don't think that you're 02:55 experiencing the type of loneliness that 02:58 these guys are are articulating and I 03:01 think 03:03 that you know one of the things that I 03:06 always is we struggle with is that you 03:09 always kind of want a silver bullet for 03:12 why things are the way they are and I 03:13 think loneliness is a big piece of that 03:16 but loneliness has tentacles that lead 03:19 to other things right um because it's 03:22 not just that we no longer have bowling 03:25 leagues uh they still exist but people 03:28 don't do them as much we don't go to 03:29 churches often um I was reading 03:31 something recently um about you know 03:34 third life and there's this whole sort 03:36 of study or or conversation around there 03:38 used to be that you would have sort of 03:42 work home or you know home school H work 03:47 uh and then you'd have this third space 03:49 and it could be church or it could be 03:50 the Lion's Club or but places where you 03:53 got together with people 03:55 and that was deeply satisfying even you 03:58 go back and read to 04:00 uh and on Democracy in America and one 04:02 of the things he'll credit us with is 04:04 our call them little pontoons but the 04:06 extent to which we self-organize in ways 04:10 that are uh emotionally and civically 04:13 gratifying for us and those connections 04:18 that we have lead us to taking care of 04:22 the country to feeling connected to it 04:24 in ways that we wouldn't want to burn 04:25 things down because no we want to 04:28 protect these Civic spaces and we like 04:30 them um and you know when I say the 04:34 tentacles I mean things like okay so now 04:35 we're lonely and so you seek out this 04:38 we've given ourselves this new 04:40 opportunity right without the if you're 04:42 if you're somebody who doesn't you don't 04:45 love always or even it's just an 04:46 inconvenience to get up and go do the 04:48 thing you can do so much of it virtually 04:49 you can sit and engage with your fellow 04:51 humans in this 04:53 Similac of Life called social media 04:56 which then can lead to the sense that 04:59 and this I trying to explain this to 05:00 somebody recently where they were like 05:02 well what what do Americans want like 05:04 why and I was saying well you know and 05:06 this is where someone like you might be 05:08 like but the economy was good why does 05:10 everybody think it's bad and I'm like 05:12 what if people don't exactly think that 05:14 it's bad it's that they think other 05:16 people have it better and that it used 05:18 to be that we would live and exist in 05:21 communities where people sort of roughly 05:23 had the same things that we had right 05:25 the the houses were similarly sized 05:27 people had the same kind of cars the 05:29 same kind of stuff and so there wasn't 05:31 this sense that everybody else was doing 05:33 better than you and now we're just 05:36 absolutely throttled all day with images 05:39 of people who are living a better life 05:42 and have more stuff and they in be look 05:45 at that just loung him by a PO why can't 05:47 I do that and so maybe it's not so much 05:52 that you can say like well GDP and job 05:55 this and you're like okay but I'm 05:56 driving an Uber and maybe you can you 05:59 can feed your family or you can take 06:01 care of yourself but you can't take a 06:02 vacation and you see it everywhere else 06:04 and I think it's leading to a chronic 06:08 dissatisfaction that is then coupled 06:10 with our increased 06:12 loneliness um and our increased 06:14 loneliness can also come from like 06:15 seeing other people and feeling like 06:17 we're not a part of that right like this 06:19 is what's killing kids in school like 06:21 it's not just that bullying it's like oh 06:23 I'm not part of this other thing and I 06:24 can see it I didn't used to be able to 06:26 see pictures all over the place of the 06:28 cool kids having the party but now I can 06:30 and I know I'm excluded and so I just I 06:32 think there's like a lot going on here 06:35 but I do think it is at the center of 06:37 our cultural Malay so it's I mean I I 06:41 view this like it's murder on the Orient 06:44 Express like you know what what's the 06:46 cause of it all of it all of the things 06:49 are the cause of it and you know dating 06:52 back to widespread adoption of the 06:55 television and declining population 06:58 density as people move to the suburbs 07:00 to declining Church attendance to 07:03 declining fertility rate to social media 07:07 I mean just all of the things 07:12 and the the thing that I so you know I 07:16 look at it I think okay so what are the 07:18 places that Society could write itself 07:22 then right because you you can't put the 07:24 TV back in the box like the TV is just 07:26 there sure um 07:29 I wonder phones are like like are we 07:32 going to look back on 07:35 phones the 07:37 way we now Look Back 40 years from now 07:40 will we look back on phone use the way 07:42 we now look back at people smoking in 07:44 the 07:45 movies right you you you put on a movie 07:48 from 1950 and every single person is 07:50 smoking and we look at that and we're 07:52 like yeah can you believe that that that 07:53 was people like that they would will we 07:56 40 years from now look back on 08:00 the the the amount of time given to to 08:04 phones and social media and screens and 08:07 will we view it like smoking or will it 08:09 be totally normal to us and will we not 08:12 do you see what I'm saying like are we 08:13 do we think we're going to learn 08:14 something and revamp how we use those 08:16 and integrate those things to our lives 08:18 this is just a we're in this first blush 08:20 of the new technology still we're only 08:22 like 15 20 years into it and so 08:25 everybody's just doing it in every way 08:28 possible and nobody's really figured out 08:30 which ways are self-destructive yet 08:32 you I do think there's some of that I 08:35 mean I think uh I was just I think about 08:38 this a lot with my kids um where I'm 08:41 like guys go outside go outside what are 08:44 you doing when I was a kid I'm like I'm 08:46 at that age where I spent a lot of time 08:48 being like when I was a kid or we just 08:50 like went outside and we didn't come 08:51 home until you know dinner time and like 08:53 our parents didn't even know where we 08:54 were uh and we just were out riding our 08:57 bikes and running through the woods 09:00 and don't you're not going to sit here 09:02 and play Minecraft or whatever get out 09:04 there and I'm like you know what I 09:07 didn't have parents who were on their 09:09 phones right like I mean I also like I 09:13 am like aware I am a our phones came 09:16 around for us and like forget we're 09:18 we're trying to figure out how to manage 09:19 it for our kids when we like nobody 09:23 managed it for us and we just were like 09:25 oh do you have crack do you have the 09:26 screen crack that I can be engaged in 09:29 thank you like and like people use it 09:31 for what the thing about the phones is 09:32 like there's a phone but inside the 09:34 phone there are different worlds and so 09:36 like while somebody else might play 09:37 Candy Crush right I'm addicted to to to 09:40 Twitter political social media right 09:42 where people are having political 09:43 conversations because I love that um but 09:47 also have I lost the like it is 09:50 hijacking our brains in all kinds of 09:53 ways and I know it like I can see it 09:55 happening um I mean my like keep trying 09:57 to go keep saying I'm going to read more 09:59 fiction going to read more fiction I 10:01 read I used to just read book after book 10:04 after book that was just what I did my 10:07 attention span for a book now like you 10:11 got to give me a day I need to like know 10:13 I can sit down and even then it's like I 10:16 don't know man um anyway but I do think 10:18 we are at it's like I think one of the 10:20 things that's going to happen as we 10:21 start to get a grip and we realize how 10:23 much damage it's doing we will probably 10:25 do as we did with smoking we'll try to 10:27 protect the kids first right the first 10:29 thing we did was raise put an age limit 10:32 on who could access it and I think 10:33 that's probably what's coming for us 10:35 first is no phones or and they're 10:39 already starting to do like I just saw 10:40 Instagram is on unrolling and I'm not on 10:42 Instagram or if I'm on it I don't use it 10:45 a like 10:47 teen version that is specifically I 10:50 think probably made for teenagers to 10:52 keep them out of kind of adult world uh 10:56 and that's good I think schools getting 10:59 like saying no your phone I see a phone 11:01 in the classroom it's confiscated like 11:03 you don't you don't use the phone during 11:04 the 11:05 day so there's a school thing is a real 11:09 mixed bag because what you have is you 11:11 have places where school systems are 11:15 adopting that as a policy but then not 11:18 doing anything about the enforcement 11:20 about it and leaving the teachers defend 11:21 for themselves and so this is like I I 11:25 don't like you know I send my kids to 11:27 this crazy Catholic school and 11:30 like they are serious serious about no 11:33 phones and uh It just strikes me as 11:37 obvious that that's how it should be 11:39 everywhere and I don't understand why 11:41 it's not and uh I wish that the school 11:44 systems which have done because I 11:46 friends who are teachers who you know 11:47 their school system has done this and 11:50 the idea is they're like yeah yeah we 11:51 have a very strict no phone policy but 11:54 the the deal is then like all the 11:56 teachers are left to enforce it or not 11:57 enforce it as they want and and 12:00 so you know you're the one mean teacher 12:03 who is doing it all the time the kids 12:05 hate you and they think you know and so 12:06 a bunch of the other teachers don't 12:07 enforce it and so it winds up in this 12:09 weird Wild West kind of thing uh I don't 12:14 know like I I just think the phones are 12:16 bad and we haven't let our kids have 12:18 them and Flash is going to get an iPhone 12:23 when he gets his license because that's 12:25 the the level at which I'm like I don't 12:28 know like I want him to have access to 12:31 Google Maps and yeah uh and all the 12:34 safety stuff because I'm a crazy 12:36 helicopter parent I'm terrified of him 12:38 driving um like utterly terrified of him 12:41 driving in ways which don't make any 12:44 rational sense but uh but they just are 12:47 there um but I don't know like he's like 12:50 he's like the last kid without a phone 12:52 in his peer group you 12:55 know good for you uh I guess cuz I think 12:59 16 is a good age I actually think this 13:02 is a good Jersey 13:05 17 don't worry you won't have to pump 13:07 his own gas uh yeah that's right he'll 13:10 be able to sit in the car on his 13:12 phone yeah do you know why though when 13:14 you say like I don't understand why 13:16 everybody's not not everybody's doing 13:18 this it's the parents like and you don't 13:21 have Buy in from the parents parents 13:22 want their kids to have a phone uh and 13:25 it's a lot of it so I think um you know 13:27 there's height Jonathan height has this 13:29 book called The anxious generation uh 13:32 super super worth uh a read I say that 13:35 like I read it uh but I actually just uh 13:39 you know skipped around and stuff heard 13:41 I read in it uh but you know anxious 13:46 kids they have anxious 13:48 parents we're anxious our phones are 13:51 making us more anxious all we see right 13:54 but I think it's funny I I was talking 13:55 to somebody much older the other day who 13:58 said like uh they were talking about 14:00 somebody's um husband who had killed his 14:03 wife and the person was like you know uh 14:07 this was back when that was very 14:08 uncommon and I was 14:11 like is it more common today than it was 14:14 back in 14:15 1950 uh still seems pretty pretty 14:18 uncommon not good yeah but I was like or 14:22 are you getting 14:24 served things that terrify you 14:26 constantly right are you seeing every 14:29 bad news story and it creates a sense in 14:32 us that the world is an unsafe Place 14:34 filled with people who are going to kill 14:36 you at any point I mean I am committed 14:41 to my kids being out in the world on 14:43 their own and I'm also very SC I mean 14:46 the the feeling in my stomach of not 14:48 knowing where they are and not having 14:50 eyes on them and that is not how our 14:51 parents felt I mean I but now we see 14:53 every story about a pedophile every 14:55 story about a 14:57 murderer we're drowning in this stuff I 15:00 I again I don't want to generalize from 15:02 personal experience but I feel like 15:04 we're caught maybe in this pingpong 15:07 because those of us raised in the 70s 15:09 and 15:10 80s I think that parenting was two hands 15:13 off like I look back on it and I'm I'm 15:17 amazed that my friends and I didn't 15:19 die you know like I I just the things 15:23 that like we just did and the number of 15:25 people who were doing drugs and drinking 15:28 underage and you know like driving drunk 15:30 sometimes not me obviously because I was 15:32 such a a rule follower but 15:35 uh I mean I look back on that and I 15:38 don't look at it as like oh it was all 15:40 such good fun I look on it 15:42 as uh it is only by luck and privilege 15:47 that peers were able to escape this time 15:50 without you know really bad consequences 15:53 and had I been part of a lower 15:55 socioeconomic stratum uh I'm sure there 15:58 would have been lots of real 15:59 consequences hey Sarah there's there's 16:01 more show oh there is yeah are we still 16:03 talking we have more talking we're still 16:05 we're still talking the talking goes on 16:08 but that's only for the you know the the 16:09 people who are inside the Velvet Rope 16:11 the the B Plus members oh they got to 16:13 subscribe yeah tell them to subscribe 16:16 tell the people you should subscribe 16:17 guys why wouldn't you subscribe you get 16:19 all kinds of things you get some some 16:21 extra uh me and JVL you get some extra 16:24 me and George Conway you get oh you get 16:25 jbl's triad this is one of the best 16:27 things the bull work off offers I read 16:30 it at least once or twice a 16:32 week yes go and subscribe we'd love to 16:36 have you Analysis Of Focus Group Data: Top 5 AI Tools For FGD Analysis Author: Nwankwo, Chris, Date: 2024-03-26T18:35:28+00:00 Collections: FocusGroups Zotero Key: J3L2B6SV Cite Key: Nwankwo24focusGrpAIanalysisTop5 Zotero Item | Lit Note The success of marketing and product development often hinges on how well we analyze focus group data. Focus groups offer a unique window into customer needs, behaviors, and motivations. But the real value comes from turning those raw discussions into clear, actionable insights that can drive impact across product strategy, marketing, experience design, and business planning initiatives. What is FGD Analysis? FGD (Focus Group Discussion) analysis refers to the systematic examination of data collected during a focus group session. It involves transcribing discussions, identifying key themes, and interpreting the data to uncover insights related to the research question. The process is often qualitative and involves understanding the dynamics of group interactions, such as how participants influence each other’s opinions or how certain ideas dominate the conversation. What Methodology is Commonly Used by Focus Groups? The qualitative research methodology is most commonly used in focus group studies. Specifically, methodologies like thematic analysis and grounded theory are popular because they allow for in-depth exploration of participant perspectives and social dynamics. The focus is often on understanding subjective experiences rather than measuring them quantitatively. The goal is to gather rich, descriptive data rather than numerical data. This makes the analysis more nuanced than just crunching numbers from surveys. And It requires a keen eye to spot patterns and extract meaning. However, some researchers may use mixed methods, combining qualitative focus group data with quantitative surveys to validate their findings. What Type of Analysis is Best for Focus Groups? The best type of analysis for focus group data depends on the nature of the research and its objectives. Here are some common analysis types used: • Thematic Analysis: This is the most widely used method in focus group analysis. It involves identifying recurring themes, patterns, and concepts across the data. Thematic analysis is especially useful for understanding participants’ attitudes, beliefs, and behaviors in relation to the topic being studied. • Content Analysis: Another common approach is content analysis, which involves quantifying the frequency of certain words or themes that appear during the discussions. This method is effective when researchers are interested in understanding the prevalence of specific ideas or terminologies. • Discourse Analysis: If the focus is on how people use language in social interactions, discourse analysis may be more appropriate. This method looks at the language used by participants and how it reflects underlying social and cultural norms. • Grounded Theory: Grounded theory is useful when the goal is to generate theories based on the data itself. It is an inductive approach where researchers develop theories by continuously comparing emerging themes across the focus group data. • Narrative Analysis: In cases where the researcher is more interested in individual stories and how they are constructed, narrative analysis might be the best approach. This method allows a deeper dive into how participants frame their personal experiences in the group context. What Are the Four Critical Qualities of Focus Group Analysis? 1. Depth: Focus group analysis should go beyond surface-level observations, exploring the underlying reasons and motivations behind participants’ opinions. 2. Context: Analysis must consider the broader social and cultural context in which participants’ opinions are formed. 3. Credibility: The findings should be supported by clear evidence, typically through direct quotes from participants, and should be triangulated with other data sources when possible. 4. Systematic Approach: A rigorous, systematic method must be used to ensure that the analysis is thorough and free from researcher bias. This includes transparent coding processes and a clear explanation of how themes were derived from the data. How is Focus Group Data Analyzed? Focus group data analysis involves several systematic steps aimed at deriving insights from participant interactions. The process starts with data collection, typically through video or audio recordings of the focus group discussions (FGD). These recordings are then transcribed into written documents for further analysis. The analysis generally involves multiple stages, including: 1. Transcription: Converting the verbal exchanges from the focus group into text, ensuring that no information is lost in the process. This often includes noting non-verbal cues like tone or body language, as they can add context to the dialogue. 2. Coding: After transcription, researchers typically categorize the data by identifying recurring themes, keywords, or concepts. Coding can be done either manually or with the help of qualitative analysis software like NVivo or Insight7, which offers AI-powered analysis for qualitative data. 3. Thematic Analysis: The next step is to conduct thematic analysis, identifying patterns or themes across the group discussions. These themes represent common ideas or perceptions shared by participants, providing insight into their collective opinions. 4. Categorization: The data is further categorized to highlight similarities and differences across various groups, or even between individuals. This helps in understanding the range of perspectives. 5. Interpretation: Once themes are identified, the next step is interpreting the findings in the context of the research objectives. Researchers analyze how participants’ opinions and attitudes relate to the research questions. 6. Visualization and Reporting: The final step in focus group analysis is presenting the results, typically through detailed reports, visual charts, or graphs to make the data accessible and interpretable. Before Transcription, How Do You Record Data from a Focus Group? Data from a focus group is typically recorded through audio or video devices. Audio recordings allow the researcher to capture all verbal exchanges, while video recordings can also capture non-verbal cues, like gestures or facial expressions, which can provide additional context. These recordings are then transcribed for analysis. How Long Does It Take to Analyze Focus Group Data? The time it takes to analyze focus group data can vary significantly depending on the size and complexity of the data, the method used for analysis, and the software tools employed. Generally, for a single focus group session, the process might take 1-2 weeks. Here’s a breakdown: • Transcription: Depending on the length of the discussion, transcription could take between 4 to 8 hours per session. Automated transcription tools can speed this process up. • Coding: Manual coding of the data might take 1-2 days per session, especially if the data is complex or the researcher is new to the process. • Thematic Analysis: Identifying themes, categorizing data, and interpreting it might take 3-5 days, depending on the depth of the analysis. • Reporting: Writing up the results could take another 3-5 days, especially if visualizations and summaries are included. How to Analyze Focus Group Data with AI Tools Analyzing focus group data typically follows a structured process, but the inclusion of AI tools can significantly enhance both the speed and depth of analysis. While human expertise is still crucial, AI tools are revolutionizing how we tackle focus group data. They’re speeding up the process and uncovering insights we might miss. This approach allows researchers to efficiently manage data while ensuring accuracy and insight generation. Step-by-Step Process for Analyzing Focus Group Data 1. Familiarization with the Data: Before jumping into any analysis, it’s crucial to immerse yourself in the data by reviewing transcripts and recordings. This step involves reading through the data multiple times to familiarize yourself with both explicit content and subtler nuances, such as participants’ emotional tone, body language (if available), and how group dynamics influenced responses. AI tools can assist at this stage by automatically transcribing audio or video recordings. Tools like Insight7, Otter.ai, and Rev offer high accuracy, generating transcripts quickly and affordably. AI transcription cuts down time spent on manual transcription, allowing you to focus on deeper analysis. 2. Open Coding: The next step is to begin coding the text. Open coding involves assigning labels to specific sections of the dialogue based on recurring ideas or key terms. For instance, if participants repeatedly discuss “customer service,” that section might be labeled as such. These labels will eventually help in organizing data into broader themes. AI-driven platforms like Insight7 offers an automated coding process that accelerates the traditional workflow by helping you quickly identify recurring concepts and sentiments. By continuously learning from your edits and validations, the AI improves in its suggestions over time, thus enhancing accuracy. 3. Organizing Codes into Themes: Once the open coding is complete, you will begin grouping similar codes into larger themes. For instance, when analyzing customer satisfaction, codes like “service quality,” “staff behavior,” and “response time” might be grouped under a broader theme such as “customer experience.” Thematic analysis helps highlight the significant patterns that emerge from the discussion. AI-powered tools not only assist in theme generation but also visualize these connections. With Insight7, themes are generated quickly and without the need for manual coding, making the process even more streamlined and accessible for researchers who may not be familiar with complex coding methods or want a stress-free solution. Tools like FocusVision and Upword analyze full transcripts and automatically suggest themes, highlighting key patterns that human coders might overlook. 4. Interpreting the Data: With themes established, interpret what these themes reveal about the focus group discussion. Look at how the themes relate to your research objectives or business goals. Consider the implications of these insights and how they align with or challenge existing knowledge. AI-driven platforms such as Insight7 assist in interpreting data by uncovering hidden patterns and correlations. These insights can provide a deeper understanding of the focus group findings and help you make more informed decisions based on the data. 5. Presenting the Results: Effective presentation of focus group data involves summarizing the findings in a clear and engaging manner. Use quotes from participants to support your interpretations and provide context for the themes identified. Visualizations, such as charts and graphs, can also help in presenting the data more clearly. AI tools can streamline report building and visualization. Tools like Insight7 offer automated reporting features that make it easy to compile and share comprehensive, multimedia-rich reports. Additionally, platforms like Upword’s Report Builder allow you to create interactive reports that include coded quotes, theme visualizations, and even video clips from the focus group. Top Tools for the Analysis of Focus Group Data Insight7 is purpose-built for focus group analysis, designed to streamline the entire process from transcription to insight generation. Its AI-powered capabilities automatically identify key themes, sentiments, and trends in qualitative data, making it an excellent choice for both seasoned researchers and those new to the field. Insight7 offers an interactive and intuitive user interface that simplifies analysis and reporting. Key Features: • AI-Driven Automatic Coding: Insight7 automatically processes raw focus group data and categorizes it into relevant themes and sentiments, eliminating the need for manual coding. This feature is particularly valuable for large datasets, as it reduces the time and effort required to surface key insights. • Interactive Dashboards and Visualizations: Insight7 provides real-time data exploration through interactive dashboards, allowing users to visualize trends, patterns, and themes immediately. This visual representation helps researchers to quickly digest complex data and make data-driven decisions faster. • Multi-Format Analysis: The platform supports analysis across multiple data types, including transcripts, audio files, and video content. This flexibility enables researchers to combine qualitative insights from different sources for a more comprehensive view. • Collaboration and Report Generation: Insight7 facilitates easy sharing of insights and recommendations across teams, making collaboration smoother. It also offers customizable report generation, enabling researchers to export findings in a format tailored to specific business needs or stakeholders. • Actionable Insights and Journey Mapping: Beyond data analysis, Insight7 enables researchers to map customer journeys and derive strategic recommendations from the data, translating insights into practical actions for business growth. Insight7 is purpose-built for focus group analysis. Its advanced AI tackles coding automatically, helping you identify key themes and sentiments fast. With interactive dashboards and visualizations, you can explore trends and generate actionable recommendations easily. Insight7’s user-friendly interface makes it a powerful ally for both seasoned researchers and newcomers. From automating the tedious task of coding to providing intuitive visual tools, it allows businesses to quickly move from data collection to insight generation. 2. Transana Transana is a versatile qualitative data analysis software tailored for analyzing audio and video recordings of focus group sessions. With its transcription tools and multimedia capabilities, Transana allows researchers to transcribe, code, and analyze focus group data in various formats. It’s great for thematic analysis and exploring interaction patterns in your focus groups. Key Features: • Multimedia Analysis: Supports analysis of audio, video, and transcripts, allowing users to capture a broader range of data. • Transcription Tools: Built-in transcription capabilities make it easy to convert audio and video into text for further analysis. • Detailed Thematic Analysis: Helps researchers break down complex interactions and communication patterns in focus groups. • Interaction Pattern Exploration: Allows for deeper analysis of non-verbal cues such as body language and vocal tone. 3. Quirkos Quirkos is a user-friendly qualitative data analysis software that simplifies the process of analyzing focus group data. Quirkos takes a visual approach to coding and analysis. Its color-coded bubbles and interactive charts make it easy to organize and interpret your data. Key Features: • Visual Data Coding: Uses color-coded bubbles and charts to make coding and analyzing data more intuitive. • Drag-and-Drop Interface: A user-friendly, visual interface that simplifies data organization. • Interactive Visualizations: Allows users to explore data relationships visually, making it easier to spot connections. • Affordable and Accessible: A cost-effective tool suitable for small teams or independent researchers. Quirkos takes the complexity out of focus group analysis by using a visual interface that simplifies the coding process. This makes it ideal for researchers who are new to data analysis or prefer a more visual way of exploring insights. 4. HyperRESEARCH HyperRESEARCH is a comprehensive qualitative analysis tool known for its flexibility in coding both text and multimedia data. It offers a wide range of functionalities for deep thematic analysis across various formats. Key Features: • Flexible Coding System: Allows researchers to code text, images, and video data with ease. • Multi-Format Support: Works with different data types, making it suitable for complex qualitative projects. • Comprehensive Thematic Analysis: Enables researchers to conduct in-depth exploration of recurring themes and insights. HyperRESEARCH offers flexibility across various data formats, making it an excellent tool for comprehensive thematic analysis. It is suitable for researchers who need to conduct deep, multi-layered analysis of both text and multimedia data. While these tools are all useful, Insight7 stands out for its focus on streamlining the entire focus group analysis process. From transcription to insight generation, it’s designed to make your job easier at every step. How to Present FGD Results? Presenting focus group discussion results typically involves a combination of narrative summaries, direct quotes, and visual representations. Here’s a structured approach: 1. Introduction: Start by outlining the purpose of the focus group, the key research questions, and the composition of the participants. 2. Key Findings: Present the major themes or insights that emerged from the discussion. Each theme should be supported by direct quotes from participants to provide context. 3. Charts and Visuals: Use tables, charts, or mind maps to visualize how themes are connected. These visuals help readers quickly grasp the major points. 4. Comparisons: If multiple focus groups were conducted, present comparisons between groups. Highlight any differences in perspectives based on demographics or group composition. 5. Conclusion: Summarize the key takeaways and how they relate to the original research objectives. Benefits of Analyzing Focus Group Data 1. Deep understanding of consumer preferences Focus group analysis helps you understand the ‘why’ behind consumer choices. You’ll uncover nuances that surveys might miss, helping you fine-tune your products and marketing to better meet the needs and desires of their target audience. 2. Identify emerging trends and patterns By analyzing discussions across multiple groups, you can detect shifts in attitudes and preferences early. This foresight helps you stay ahead of the market, anticipate market trends, and proactively adapt their strategies to capitalize on emerging opportunities. 3. Validate quantitative findings Focus group insights add context to your survey data. This combination gives you a more complete picture of consumer behavior. By triangulating quantitative data with qualitative insights from focus groups, you can gain a more holistic understanding of consumer attitudes and motivations. This validation enhances the credibility and reliability of research findings, providing businesses with confidence in their decision-making processes. 4. Explore consumer emotions and perceptions Focus groups reveal the emotional drivers behind purchases. This knowledge helps you create marketing that truly resonates with your audience. By uncovering the emotional drivers behind purchasing decisions, businesses can develop more resonant marketing campaigns, brand messaging, and customer experiences that forge deeper connections with their target audience. 5. Generation of Actionable Insights The ultimate goal is to drive decision-making. Focus group analysis turns complex data into clear recommendations you can act on. Whether it’s refining product features, optimizing pricing strategies, or enhancing customer service initiatives, focus group data analysis provides the insights needed to drive meaningful change and drive business growth. AI Enhances, Not Replaces, Human Analysts While AI tools like Insight7 are powerful, they don’t replace human expertise. Skilled researchers are still crucial for: • Designing the right discussion guides and research approaches* • Providing context to help AI systems interpret nuanced language accurately* • Validating the accuracy of auto-coding and suggested insights* • Weaving together cohesive storytelling and strategic recommendations* • Applying subject matter expertise to extrapolate meaning from findings* AI allows analysts to work smarter and faster. But the human touch remains essential for maximizing impact. Conclusion: Mastering Focus Group Analysis Focus group analysis is a vital skill for understanding your market. It uncovers preferences, identifies trends, and generates actionable insights. While challenges exist, systematic techniques and AI tools like Insight7 can transform raw data into valuable knowledge. Successful organizations see focus group analysis as a core strength. It drives not just small improvements, but game-changing innovations. By combining human expertise with AI-powered tools like Insight7, you can unlock the full potential of your focus group data. Ready to take your focus group analysis to the next level? Explore how Insight7 can streamline your process and uncover deeper insights. Survey: 29% Of Borrowers Say Student Loan Debt Will Influence Their Vote Author: Foster, Sarah, Date: 2024-06-17T04:10:00+00:00 Collections: Hot Takes US Elect 2024 Zotero Key: FVKSW9QN Cite Key: Foster24Survey29studentLoanVote Zotero Item | Lit Note Only a small share of adults have student loan debt, but President Joe Biden and former President Donald Trump may still have to make it a key part of their platform while campaigning for the White House, a new Bankrate survey indicates. Nearly 1 in 5 adults (18 percent) say student loan debt will have a major influence over their vote in the upcoming 2024 presidential election, according to Bankrate’s Student Loans and the Presidential Election Survey. Meanwhile, more than 1 in 4 Americans call student loan debt a national crisis (29 percent) and say the federal government hasn’t done enough to provide borrowers with financial assistance (27 percent). That’s even as just 17 percent of adults carry federal debt, Census Bureau and Federal Student Aid data shows. The findings suggest that concerns of college affordability and rising educational debt — now reaching $1.6 trillion — aren’t limited to individuals personally carrying the burden. Yet, student loan borrowers care even more. The share of Americans calling student loan debt a national crisis rises to 46 percent among those who have debt for themselves. About 3 in 10 of those who have debt for themselves or someone else (29 percent) say it will be a key factor in their vote. Both Trump and Biden have attempted to appeal to borrowers with debt during their stints in the Oval Office. Trump was the first to institute a more than three-year-long coronavirus pandemic-era forbearance program that put both payments and interest on pause. Biden, on the other hand, created a new income-driven repayment plan that’s speeding up forgiveness and reducing monthly costs for millions of borrowers. Despite the Supreme Court rebuffing his sweeping forgiveness program in 2023, he’s canceled $167 billion in loans — or roughly 10 percent of all federal debt — for nearly 5 million borrowers and is moving forward with a “Plan B” program to help even more. Borrowers in Bankrate polls dating back almost a decade indicate that their debt is casting a dark shadow over their financial lives, impacting their ability to achieve the same financial milestones as generations before them. But for the Americans saddled with debt to pay for their own education, student loans have been seen as the price they have to pay to compete for well-paying jobs in an increasingly selective and competitive U.S. labor force. As long as there is sufficient demand for costly education, those supplying the ‘product’ will continue to do so at a high price. — Mark Hamrick, Bankrate Senior Economic Analyst Key insights on how student loans could influence the 2024 presidential elections • A major voting issue: Nearly 1 in 5 Americans say student loan debt will have a major influence on their vote in the 2024 presidential election (18%), with that number rising to 29% among those who currently hold student loan debt for their education or someone else’s. • Struggles with making payments: Since the forbearance period on federal student loans came to an end in October 2023, about 1 in 4 of those who currently have student loan debt for either their education or someone else’s schooling say they are having trouble affording their monthly payments on their federal student loans (24%), have skipped at least one monthly payment on their federal student loans (24%) or have had to enroll in a new repayment plan, forbearance or deferment to reduce their payments (25%). • The burden of student loan debt: Roughly 1 in 4 of those who currently have student loan debt for themselves or someone else don’t expect that they’ll ever be able to pay off their student loan debt (24%). About 2 in 5 of those current borrowers (39%) say that they expected to fully repay all their loans themselves when they borrowed for their education. • Limited options for forgiveness: About 3 in 10 (30%) of current student loan borrowers say they do not know if they qualify for any student loan forgiveness. Just 10% of those with student debt say their balances, or at least a portion of it, have already been forgiven. This election cycle, these Americans care about student loan debt the most Unsurprisingly, Americans with student loan debt were the most likely to cite it as an important election issue. Yet, the hot-button item might not be as acute for the country’s youngest voters as experts — and the candidates themselves — think. Generation Zers (ages 18-27) with student loan debt for themselves or someone else were the least likely to indicate that it will have a major influence over their choice in the voting booth, at: • Gen Z: 22 percent;* • Millennials (ages 28-43): 35 percent; and* • Generation X (ages 44-59): 24 percent.* Meanwhile, higher-income Americans who currently have some form of debt were even more likely to call it a major decision-making factor: • Under $50,000: 25 percent;* • Between $50,000 and $99,999: 31 percent; and* • $100,000 or more: 37 percent.* The perspectives of those who had debt but paid it off mirror that of Americans as a whole, with 18 percent saying student loan debt has a major influence over their vote this upcoming election. Meanwhile, the share of Americans who say student loan debt will have a major influence on their vote is slightly higher (30 percent) for those who have debt for someone else. “The availability of student loans resulted in a friction-free environment where all too many students piled on debt,” Hamrick says. “After the fact, many developed student loan debt remorse.” Almost a year after payments resumed, many borrowers are feeling the pinch of student loan debt One reason many borrowers seem to be voting with their student loan debt in mind: They’re familiar with the financial stress that can come along with making payments, especially in an era of high inflation and expensive interest rates. Nearly 1 in 4 of those who currently have student loan debt for either their education or someone else’s say they are having trouble affording their monthly payments (24 percent) or have gone as far as skipping at least one monthly payment (24 percent) since the forbearance period ended in October 2023. Another 1 in 4 (25 percent) say they have had to enroll in a new payment plan, forbearance or deferment to reduce their monthly cost. Americans who have debt for themselves are even more likely to feel the pinch, with 29 percent indicating that they’re having trouble affording their monthly payments versus 18 percent who have debt for another, respectively. Yet, while higher-income Americans were more likely to focus on student loan debt at the polls, lower-income adults were the ones feeling the biggest pinch. That could be because they’re the least likely to have sought out a different repayment strategy, Bankrate data finds. | Impact | Under $50,000 | Between $50,000 and $99,999 | $100,000 or more | | --- | --- | --- | --- | | I am having trouble affording my monthly payments on my federal student loans | 28% | 28% | 14% | | I have gone at least one month without paying my federal student loans since payments resumed in 2023 | 25% | 25% | 22% | | I have had to enroll in a new repayment plan, forbearance or deferment to reduce my payments since payments resumed in 2023 | 21% | 26% | 32% | The same can be true vice-versa. Gen Zers with debt were the least likely to enroll in a new repayment, forbearance or deferment plan — but that might be because they’re the least likely to indicate that they’re having trouble affording those payments. | Impact | Gen Z | Millennials | Gen X | | --- | --- | --- | --- | | I am having trouble affording my monthly payments on my federal student loans | 18% | 30% | 25% | | I have gone at least one month without paying my federal student loans since payments resumed in 2023 | 24% | 28% | 18% | | I have had to enroll in a new repayment plan, forbearance or deferment to reduce my payments since payments resumed in 2023 | 22% | 28% | 27% | • What should you do if you’re struggling to make a student loan payment?* ‘A life sentence of debt’ The No. 1 reason Sarah Blankenship supported Biden at the 2020 presidential elections came down to his pledge to forgive up to $20,000 in federal student loan debt for eligible borrowers. Blankenship, a Gen Xer, borrowed about $20,000 in federal debt in her 20s for an associate’s degree in advertising that she couldn’t finish after the birth of her first son. She made payments on an income-driven repayment plan from 2004 to 2011, until a divorce decimated her finances. Suddenly a single mom to three children, her student loans had to take a backseat to surviving. She fell delinquent and ended up defaulting, watching as penalties, fees and interest piled up. For four years, she received daily calls from her federal student loan servicer. She let all of them go to voicemail. “I was really scared. I didn’t know what to do or if I had any options,” she says. “My electricity was regularly shut off; I was evicted at one point. If they could give us the support we need, we would be able to help ourselves crawl out of mountains of debt and awful situations. Our country is so against helping people who truly needs help, and I saw that when I was a single mom.” Eventually, she got back on her feet and picked up the phone in 2015. Her only options were to consolidate her loans and enroll in an income-driven repayment plan, but it came with some painful stipulations. Mainly, her payments that count toward forgiveness were reset. Blankenship has come a long way since those days in the aftermath of her divorce. She found better-paying jobs, went back to school (without having to take out more loans) and moved into her dream home with her three children. But she now owes about twice as much as what she initially borrowed. After the post-pandemic forbearance period, she was automatically transferred over to the SAVE plan. Her payments started at a manageable $135 a month — until she remarried. Her household income grew nine times, and her monthly payment more than quadrupled. Biden’s forgiveness program would’ve felt like a “clean slate,” she says. She’s awaiting more information on whether she could be eligible for help through his “Plan B” proposal or one-time account adjustments — targeted for struggling borrowers like her who’ve had debt for more than 20 years and owe more than they initially borrowed. She says she’s satisfied with Biden’s efforts to help borrowers like her, but casts blame on the political gridlock that’s been preventing her from getting any of the help. She’s on track to pay off her loans in about four years. She’s unhopeful that any proposals will come to her rescue in time. “It is completely our government’s responsibility to step in. Nothing related to the student loan mess we are in right now happened without the explicit permission of our government,” she says. “For me to be almost 50 years old and still dealing with this, that was never a thought in my mind when I took out these loans. It should not be a life sentence of debt in order to have an education.” The share of Americans calling student loan debt a national crisis has fallen since last year’s poll The Biden administration’s work to cancel student loan debt might not have gone entirely unnoticed: Fewer Americans than last year (29 percent in 2024 versus 32 percent in 2023) call student loan debt a national crisis and say the federal government has not done enough to help borrowers (27 percent in 2024 versus 31 percent in 2023). Even among the Americans who currently have student loan debt for themselves, the percentage of them who agree with those statements is lower than it was a year ago. About half (48 percent) called student loan debt a national crisis in 2023, a share that edged down to 46 percent in 2024. Meanwhile, more than a third (35 percent) of those who currently have student loan debt for themselves say the federal government has not done enough, down from 48 percent. What student loan debt has Biden forgiven? Check out Bankrate’s guide on the latest forgiveness announcements. Read more Almost half of Gen Xers who have debt for themselves or someone else (49 percent) agree that student loan debt is a national crisis, followed by 42 percent of millennials and 36 percent of Gen Z. Millennial borrowers, on the other hand, are the most likely to say the federal government hasn’t done enough (at 39 percent), compared with 27 percent of Gen Z and 30 percent of Gen X. By income, Americans with debt who are earning under $50,000 are the least likely to indicate that the federal government hasn’t done enough, at 27 percent. That compares with 38 percent of those earning $100,000 or more and 36 percent of those making between $50,000 and $99,999. Americans are finding forgiveness options confusing Biden has forgiven more student loan debt than any other U.S. president, but cancellation is still rare among current borrowers. Just 10 percent of those who currently have student loan debt say their balance, or at least a portion of it, has already been forgiven, a share that rises to just 15 percent for those who had student loan debt but paid it off. Those figures are in line with data from the Biden administration, which says that 1 in 10 federal borrowers have been approved for some form of debt relief. But borrowers are indicating that they might be confused by the slate of forgiveness announcements, including whether they may be on the administration’s cancellation shortlist. Nearly one-third of current student loan borrowers (30 percent) say they do not know if they qualify for any student loan forgiveness. Available student loan forgiveness programs Wondering whether you’re eligible for a forgiveness program? You’re not alone. Read more About 2 in 5 current borrowers (39 percent) say they expected to fully repay their loans themselves when they borrowed money. Yet, 24 percent of those with student loan debt say they do not expect they will ever be able to pay it off. That sentiment rises with age, at: • Gen Z: 14 percent;* • Millennials: 26 percent; and* • Gen X: 27 percent.* • What can the federal government do to help relieve the burden of student loan debt?* Regulation is the ‘only way’ people will get some relief, this borrower feels Mike Tatum, 37, has worked 10 jobs since graduating from college in 2015, chasing promotions and pay increases in what he says feels like a nonstop grind. In the coming months, he expects that he’ll have to pick up extra work in addition to his full-time job, too. He has no choice, he says, thanks to student loan debt. Tatum owes about $40,000, a balance that even six years in the U.S. army as an infantryman couldn’t protect him from. His wife, meanwhile, has almost $200,000 in debt from her bachelor’s, master’s and doctoral degrees. The sole provider for his family of two children, Tatum has been juggling the debt for the two of them — along with their grocery bills, mortgage payments on their first house, a car loan, utilities and more. He’s on the lowest monthly payment he can find, spending less than $100 a month on his debt and closer to $200 for his wife’s, but the funds come from somewhere. He built up his family’s emergency fund during the pandemic-era forbearance period but still doesn’t regularly contribute to a retirement account. He’s come to accept that his debt will be part of him for the rest of his life — unless it’s forgiven. “If there’s a world in which I win the lottery and we get this huge sum of money, we’ll pay it off,” Tatum says. “I don’t have a good enough plan to make a dent in my own student loans. I’m just going to see if I can make it through my working life, and once I die, it is what it is at that point.” He feels that pursuing his degree was necessary for companies to take his applications seriously, but most of what he needed to learn happened on the job. He fears the day when his own children go off to college, knowing that he won’t be able to pay for it. His two children, 2 and 9, are “artsy,” enjoying dancing and music, he says. He wants them to live in a world where they can pursue what they’re passionate about, without fearing college costs, debt or income. “College is very much overpriced, and they aren’t willing to change that on their own. Regulation is the only way where people are going to get some relief,” he says. “Lifestyle-wise, there’s always something else that you’re balancing. I go pay for a wedding or I go get ahead on my student loans. I could buy a house or pay off my debt. It’s a moving target that you’re wearing around your neck.” Voices of Victory, Part II The Makings of a Good Focus Group Author: Luntz, Frank I., Date: May 16 1994 Collections: PollMethods, FocusGroups Zotero Key: UIYPFP7K Cite Key: Luntz94focusGrpMaking Zotero Item | Lit Note Frank Luntz, president of the polling and communications firm Luntz Research, has served as an adviser to the U.S. House Republican leadership, New York Mayor Rudolph Giuliani, and numerous candidates in this country and abroad. These articles appeared in the May 16 and May 30, 1994, editions of The Polling Report. --- Voices of Victory, Part I Focus Group Research in American Politics by Frank I. Luntz Political pollsters are drowning in numbers. Media organizations like CNN are polling on a weekly basis, if not more. Congressional elections are six months away, yet nearly every incumbent and a good number of challengers already have at least one telephone poll under their belt. Why Quantitative Research Isn’t Enough Unfortunately, while we have all the numbers we can possibly crunch, we are severely lacking in insight. \"Not seeing the forest for the trees,\" as the saying goes, most pollsters know what voters think, but too few understand how voters feel. If understanding \"why\" is the objective, traditional telephone polling is simply not enough. Qualitative research is the answer. Quantitative research (telephone polling) has become so sophisticated that it is possible to predict with significant accuracy who will win most elections. But times -- and the American electorate -- are changing. If conventional wisdom and national telephone polls were accurate, Ross Perot should have barely scraped into double digits in 1992. All those Perovians should have fallen behind Bush or Clinton, or just stayed home -- so it was said. Yet when all the ballots were counted, Perot ended up a whisker below 20%. And if post-election telephone surveys, conventional wisdom, and 100 years of electoral history were any indication, Perot’s support should have collapsed in the weeks following the election. Instead, it expanded. Traditional quantitative polling inaccurately screened out likely Perot voters prior to the election, and misgauged Perot voter commitment after the election. The fact is, traditional polling methods are increasingly ill-equipped to measure public emotions and motivations as they exist today. Americans don’t want to respond \"yes\" or \"no\" to alternatives that are either unacceptable or require clarification. Asking voters to choose among fighting crime, reforming welfare, and improving health care is an illegitimate choice for those who believe the government must accomplish all three. If telephone polling is so clear-cut and conclusive, why is there a tremendous discrepancy between polling firms in their reported data on abortion? What do \"pro-choice\" and \"pro-life\" mean anyway? Telephone polling can’t answer that question because voters themselves can’t explain it in 30 seconds. We simply need to know more. In today’s post-partisan politics, there are too many shades of gray, too many \"yes, but what I really think is ...\" attitudes, too many voter priorities that cannot be prioritized. With the rise of talk radio and 24-hour television news channels, not to mention C-Span and public access cable, there is a rapidly increasing number of semi-informed voters out there with only half-formed political views. The elements that make up public opinion have changed; so must its measurement. The key to understanding why qualitative research in general, and focus groups in particular, are so important in the realm of today’s politics, can be summarized in a single sentence: Unlike traditional quantitative research, focus groups are centrally concerned with understanding attitudes rather than measuring them. In an academic sense, the goal of a focus group is to gain access to private, non-communicable, unconscious feelings and emotions. In a real sense, focus group research is a direct, sensitive, and interactive method of assessing public opinion, accomplishing what telephone studies cannot. It approaches attitudes and priorities tangentially by allowing respondents to talk freely and to choose descriptive categories significant to them (rather than to the pollster, or even to the client). A History of Success The focus group concept is about 50 years old, and like many modern innovations, its roots date back to World War II. A group of sociologists were asked to investigate how the military’s propaganda films were being received by their audiences. They learned that, with proper prodding, people can identify the exact reason certain scenes, lines, or phrases make them think or act in a certain way. The consumer culture was next to use focus group technology, turning to academically trained market researchers to determine everything from packaging and pricing to advertising and marketing. Today, roughly 70% of all consumer research dollars are earmarked for qualitative research, and it is nearly impossible to find a Fortune 500 company that does not use focus groups to develop its corporate image and/or marketing strategy. By comparison, only 10% of all political research is devoted to qualitative formats, and less than a fourth of all House and Senate candidates have had any experience with the techniques. However, when they learn how far behind the research curve they are, and what they are missing, that will soon change. Focus groups may have a low-tech feel, but more often than not, it is a series of focus groups rather than traditional telephone polling data that snatches victory from the jaws of defeat. Historically, quantitative data has helped set themes and issues, but focus groups have determined strategic communication and implementation. The evidence is staggering: • It was a 1984 Georgia focus group that gave Walter Mondale’s sputtering presidential campaign the ammunition to fight back against a rapidly charging Gary Hart. A gaggle of Hart supporters were brought together for the purpose of finding a weakness, any weakness, in Hart’s political armor. At that focus group, the Mondale campaign found what they needed -- an underlying concern about Hart’s ability to handle an international crisis. Two hours of discussions with 15 potential voters yielded only one important finding, but it was the magic bullet that stopped Hart dead in his tracks. • The 1988 Republican focus group in Paramus, N.J., has reached legendary stature, and deservedly so, for that single gathering may have changed American history. George Bush was trailing Michael Dukakis by double digits, with the critical target group, so-called Reagan Democrats, trending toward the Massachusetts Governor. Assembled was just such a group, and they were fed a litany of Dukakis negatives, from Willie Horton to Boston Harbor. Individually, the negatives did not have a significant impact (although the prison furlough program evoked considerable unease). However, the cumulative effect of the information provided to the participants peeled them away from Dukakis one by one -- and made it clear to the Bush camp exactly what had to be done to win. • No one in political history has had a greater commitment to focus group research than Bill Clinton. The strategies for dealing with Gennifer Flowers, the draft dodging charge, and the other moral challenges that faced his campaign in the primaries were developed through focus group research. The technique has followed him into the Oval Office. In President Clinton’s first year alone, his pollster conducted more focus groups than were conducted in all four years of the Bush presidency. Some of the traditional campaign decisions -- issues, targeting and scheduling -- are not determined by qualitative research, yet focus group results find their way into campaign strategy through speeches, television and radio ads, and press opportunities. The reason is obvious: qualitative research provides deep insight into behavioral and emotional responses that you cannot capture in telephone studies. Testing Television Spots The testing of television commercials is also perfectly suited to focus group research. While firms like ours can play radio ads and the audio portion of a television commercial directly through the telephone line, it is impossible to show respondents the visual component through this medium. Since the goal in any advertising effort is to activate \"mental imagery\" -- the mental images that the messages create in consumer minds -- visual analysis is far more effective than auditory processing. Furthermore, scientific studies completed in the past decade using focus group methodology have proven that \"how it is said\" and \"what is heard\" is more important than \"what is said.\" For example, focus groups have alerted media consultants to the importance of \"auditory stimuli\" (i.e., background music and sounds) to increase attention, recall, and persuasiveness. On the other hand, focus groups have occasional difficulty measuring the \"sleeper effect,\" a gradual acceptance of a particularly hostile stimulus. Americans generally have a negative reaction to negative advertising. However, the palpability, and so the persuasiveness, of the information often increases as the viewer sees it multiple times. Limitations of Focus Groups Despite their star-studded history, the accuracy and legitimacy of qualitative research in general, and focus groups in particular, are still raised by a small but vocal group in the polling community. Some attacks are legitimate, but most are not. To some extent, the problem lies with the consumer. Candidates have never been the most sophisticated consumers of political technology, and it took years before they were prepared to accept the proposition that telephone studies are scientific, reliable, and valid. Qualitative research can be just as empirical and objective, but skepticism still persists. A perceived absence of formal structure and \"hard numbers\" does not make qualitative research unscientific, but few candidates are academically trained behavioral scientists, and they are intimidated by what they don’t understand. Privately, political pollsters have been known to highlight the benefits of telephone surveys at the expense of focus groups for two simple reasons: either they are not qualified to moderate a group, or because the profit margin for telephone surveys is much greater. Others, particularly first and second generation media consultants, oppose focus groups because they cannot control the outcome, and they don’t like surprises. (As Art Linkletter might quip, \"voters say the darndest things.\") This is ironic. Better to be surprised by a focus group conclusion than to have that surprise delivered on election day. Focus groups do have their limitations. The participants are chosen scientifically but, as a group of 10 or 12 people, the findings cannot be projected onto the entire population. The results are dependent upon the interaction between the respondents and the moderator, and unprofessional moderating can lead to inaccurate conclusions. But scientifically derived quantitative data can also misinform and mislead. In the mid-1980s, a significant number of working class white Democrats were abandoning their party in favor of the Republican Party of Ronald Reagan. However, telephone polls at that time suggested a growing tolerance by these white voters toward the increasing political power of blacks. In fact, the exact opposite was the case. It took a series of focus groups, by Democratic pollster Stan Greenberg in suburban Detroit, to bring those telephone study errors to light. --- Voices of Victory, Part II The Makings of a Good Focus Group by Frank I. Luntz A well-run focus group is a laboratory for social interaction. A good focus group requires four simple characteristics: the proper composition, an open environment, a probing moderator, and in-depth analysis. The composition of the focus group must be selected strategically, with homogeneity as the key to a successful session. Human behavioral studies have consistently proven that people will reveal their innermost thoughts only to those they believe share a common bond. For example, if your goal is to study the real, in-depth feelings of whites and blacks toward affirmative action, welfare, or crime, you cannot have an integrated focus group. Similarly, women will not talk freely and emotionally about abortion if men (including a male moderator) are present. This is just a fact of life. The mood of the group is also critical. A single dominant voice can cripple open, honest discussion by intimidating the other participants. Also, keep food outside the focus group room. This has nothing to do with the potential for a food fight. Continuing participant attention to food is an unnecessary and ill-advised distraction. But the single greatest component of a successful focus group is the moderator. Academics have been justifiably critical of many focus group practitioners because they lack one or more of the following characteristics: • a creative mind • analytical skills • verbal skills • intellectual ability • an eye for detail • a tolerance for disorder • listening skills • a capacity for empathy Being a \"good listener\" is not enough to moderate a focus group properly. Remarkably few political focus group moderators have been academically or professionally trained to stimulate thorough but balanced discussion in an unbiased fashion. Similarly, all too often, focus group moderators put pressure on respondents to give information that they just do not have. The fact is, voters are ill-informed about the intricate details of public policy, and the loudest and most emotional respondent often knows the least about what he or she is talking about. A professional focus group moderator knows how to keep such an individual from intimidating and biasing the other participants. Even with the \"right\" participants, a good environment, and a trained moderator, the eventual success of focus group research in developing political strategy is fully dependent on the analysis. The question every focus group user needs to ask is: Who analyzes the transcripts? Too often, the dialogue is poured over not by behavioral scientists or by experts in sociology but by low level political types who know tactics but not people. This can lead to misinterpretation of comments, false conclusions and, eventually, flawed recommendations and strategy. State-Of-The-Art Qualitative Techniques Imagine the power of being able to measure instantly and specifically the exact reaction to a political theme, message, or messenger—second-by-second, by target population subgroups. That power now exists. In the post-Reagan era, most politicians have understood the importance of harnessing verbal and visual imagery in their effort to affect voter attitudes and opinions. Roughly one-half of President Clinton’s annual $2 million polling budget is targeted toward communication, and it shows with every speech and public appearance. Bill Clinton \"feels your pain\" because he actually knows what your pain is. Clinton’s not-so-secret (and not-so-new) weapon is a technology called \"instant response,\" which combines the most important components of quantitative, qualitative, and in-depth public opinion research to test message delivery, understanding, believability, and impact. A computer-based system, the instant response technology specializes in the immediate, second-to-second measurement of voter reaction to a speech, debate, or political advertisement. Here’s how it works: Participants are gathered in a single room for a two- to three-hour session. Each participant uses a button- or dial-operated hand-held computer, roughly the size and weight of a small paperback book, to relay his or her immediate reaction to a video or televised appearance. A portable PC collects and records these responses in real time, along with demographic information and customized quantitative close-ended opinion. During the presentation, a line graph is displayed continuously on a monitor adjacent to the PC. Audience reactions are gathered literally second-by-second, enabling the pollster to determine exactly which words, phrases, gestures, and other visuals enhance the communication effort, and which should be altered or abandoned. For example, George Bush used an instant response system to test \"ad-libs\" prior to his 1988 debate appearances, and Bill Clinton learned that his mannerisms and statements tested better before a live audience than they did straight to camera. The intensive focus groups following the session answer the question \"why\" and \"how,\" thus providing confirmation and strategic guidance. Focus group research is the least financially profitable tool of the polling trade, but it may be the most powerful. Political types may have been the last to discover its power, but nothing breeds attention more than success. Every candidate wants to win, and as they become aware of the qualitative option, its usage will continue to grow. Focus group influence is undeniable even today. As media guru Roger Ailes concluded, \"When I die, I want to come back with real power. I want to come back as a member of a focus group.\" --- Motivational Factors Focus groups are best used to explain \"why\" the public feels the way it does. A properly constructed and administered focus group will draw out the \"motivational factors\" behind the \"top of mind\" opinions -- which is critical to understanding what is driving public opinion. This figure illustrates the current motivational factors for the simple question: \"What is the most important problem facing America today?\" In telephone polls, rarely do more than 2% or 3% of those surveyed ever cite any of the three factors themselves. However, using traditional statistical analysis and an adjusted conjoint interviewing technique in focus group research, these three attitudes explain the fundamental motivational factors of more than 80% of Americans. • \"Declining quality of life\" explains public concern about health care, the economy, the deficit, taxes, and unemployment. • \"Disintegration of morality in society\" is the primary motivational factor behind concern about crime, drugs, welfare, and immigration. • \"The break-up of the American family\" is the fear most associated with the problems in education, and to a lesser extent, crime, drugs, and welfare. — Frank I. Luntz | Voices of Victory, Part I Focus Group Research in American Politics Author: Luntz, Frank I., Date: May 16 1994 Collections: PollMethods, FocusGroups Zotero Key: E9X9EFNI Cite Key: Luntz94focusGrpAmerPolit Zotero Item | Lit Note Frank Luntz, president of the polling and communications firm Luntz Research, has served as an adviser to the U.S. House Republican leadership, New York Mayor Rudolph Giuliani, and numerous candidates in this country and abroad. These articles appeared in the May 16 and May 30, 1994, editions of The Polling Report. --- Voices of Victory, Part I Focus Group Research in American Politics by Frank I. Luntz Political pollsters are drowning in numbers. Media organizations like CNN are polling on a weekly basis, if not more. Congressional elections are six months away, yet nearly every incumbent and a good number of challengers already have at least one telephone poll under their belt. Why Quantitative Research Isn’t Enough Unfortunately, while we have all the numbers we can possibly crunch, we are severely lacking in insight. \"Not seeing the forest for the trees,\" as the saying goes, most pollsters know what voters think, but too few understand how voters feel. If understanding \"why\" is the objective, traditional telephone polling is simply not enough. Qualitative research is the answer. Quantitative research (telephone polling) has become so sophisticated that it is possible to predict with significant accuracy who will win most elections. But times -- and the American electorate -- are changing. If conventional wisdom and national telephone polls were accurate, Ross Perot should have barely scraped into double digits in 1992. All those Perovians should have fallen behind Bush or Clinton, or just stayed home -- so it was said. Yet when all the ballots were counted, Perot ended up a whisker below 20%. And if post-election telephone surveys, conventional wisdom, and 100 years of electoral history were any indication, Perot’s support should have collapsed in the weeks following the election. Instead, it expanded. Traditional quantitative polling inaccurately screened out likely Perot voters prior to the election, and misgauged Perot voter commitment after the election. The fact is, traditional polling methods are increasingly ill-equipped to measure public emotions and motivations as they exist today. Americans don’t want to respond \"yes\" or \"no\" to alternatives that are either unacceptable or require clarification. Asking voters to choose among fighting crime, reforming welfare, and improving health care is an illegitimate choice for those who believe the government must accomplish all three. If telephone polling is so clear-cut and conclusive, why is there a tremendous discrepancy between polling firms in their reported data on abortion? What do \"pro-choice\" and \"pro-life\" mean anyway? Telephone polling can’t answer that question because voters themselves can’t explain it in 30 seconds. We simply need to know more. In today’s post-partisan politics, there are too many shades of gray, too many \"yes, but what I really think is ...\" attitudes, too many voter priorities that cannot be prioritized. With the rise of talk radio and 24-hour television news channels, not to mention C-Span and public access cable, there is a rapidly increasing number of semi-informed voters out there with only half-formed political views. The elements that make up public opinion have changed; so must its measurement. The key to understanding why qualitative research in general, and focus groups in particular, are so important in the realm of today’s politics, can be summarized in a single sentence: Unlike traditional quantitative research, focus groups are centrally concerned with understanding attitudes rather than measuring them. In an academic sense, the goal of a focus group is to gain access to private, non-communicable, unconscious feelings and emotions. In a real sense, focus group research is a direct, sensitive, and interactive method of assessing public opinion, accomplishing what telephone studies cannot. It approaches attitudes and priorities tangentially by allowing respondents to talk freely and to choose descriptive categories significant to them (rather than to the pollster, or even to the client). A History of Success The focus group concept is about 50 years old, and like many modern innovations, its roots date back to World War II. A group of sociologists were asked to investigate how the military’s propaganda films were being received by their audiences. They learned that, with proper prodding, people can identify the exact reason certain scenes, lines, or phrases make them think or act in a certain way. The consumer culture was next to use focus group technology, turning to academically trained market researchers to determine everything from packaging and pricing to advertising and marketing. Today, roughly 70% of all consumer research dollars are earmarked for qualitative research, and it is nearly impossible to find a Fortune 500 company that does not use focus groups to develop its corporate image and/or marketing strategy. By comparison, only 10% of all political research is devoted to qualitative formats, and less than a fourth of all House and Senate candidates have had any experience with the techniques. However, when they learn how far behind the research curve they are, and what they are missing, that will soon change. Focus groups may have a low-tech feel, but more often than not, it is a series of focus groups rather than traditional telephone polling data that snatches victory from the jaws of defeat. Historically, quantitative data has helped set themes and issues, but focus groups have determined strategic communication and implementation. The evidence is staggering: • It was a 1984 Georgia focus group that gave Walter Mondale’s sputtering presidential campaign the ammunition to fight back against a rapidly charging Gary Hart. A gaggle of Hart supporters were brought together for the purpose of finding a weakness, any weakness, in Hart’s political armor. At that focus group, the Mondale campaign found what they needed -- an underlying concern about Hart’s ability to handle an international crisis. Two hours of discussions with 15 potential voters yielded only one important finding, but it was the magic bullet that stopped Hart dead in his tracks. • The 1988 Republican focus group in Paramus, N.J., has reached legendary stature, and deservedly so, for that single gathering may have changed American history. George Bush was trailing Michael Dukakis by double digits, with the critical target group, so-called Reagan Democrats, trending toward the Massachusetts Governor. Assembled was just such a group, and they were fed a litany of Dukakis negatives, from Willie Horton to Boston Harbor. Individually, the negatives did not have a significant impact (although the prison furlough program evoked considerable unease). However, the cumulative effect of the information provided to the participants peeled them away from Dukakis one by one -- and made it clear to the Bush camp exactly what had to be done to win. • No one in political history has had a greater commitment to focus group research than Bill Clinton. The strategies for dealing with Gennifer Flowers, the draft dodging charge, and the other moral challenges that faced his campaign in the primaries were developed through focus group research. The technique has followed him into the Oval Office. In President Clinton’s first year alone, his pollster conducted more focus groups than were conducted in all four years of the Bush presidency. Some of the traditional campaign decisions -- issues, targeting and scheduling -- are not determined by qualitative research, yet focus group results find their way into campaign strategy through speeches, television and radio ads, and press opportunities. The reason is obvious: qualitative research provides deep insight into behavioral and emotional responses that you cannot capture in telephone studies. Testing Television Spots The testing of television commercials is also perfectly suited to focus group research. While firms like ours can play radio ads and the audio portion of a television commercial directly through the telephone line, it is impossible to show respondents the visual component through this medium. Since the goal in any advertising effort is to activate \"mental imagery\" -- the mental images that the messages create in consumer minds -- visual analysis is far more effective than auditory processing. Furthermore, scientific studies completed in the past decade using focus group methodology have proven that \"how it is said\" and \"what is heard\" is more important than \"what is said.\" For example, focus groups have alerted media consultants to the importance of \"auditory stimuli\" (i.e., background music and sounds) to increase attention, recall, and persuasiveness. On the other hand, focus groups have occasional difficulty measuring the \"sleeper effect,\" a gradual acceptance of a particularly hostile stimulus. Americans generally have a negative reaction to negative advertising. However, the palpability, and so the persuasiveness, of the information often increases as the viewer sees it multiple times. Limitations of Focus Groups Despite their star-studded history, the accuracy and legitimacy of qualitative research in general, and focus groups in particular, are still raised by a small but vocal group in the polling community. Some attacks are legitimate, but most are not. To some extent, the problem lies with the consumer. Candidates have never been the most sophisticated consumers of political technology, and it took years before they were prepared to accept the proposition that telephone studies are scientific, reliable, and valid. Qualitative research can be just as empirical and objective, but skepticism still persists. A perceived absence of formal structure and \"hard numbers\" does not make qualitative research unscientific, but few candidates are academically trained behavioral scientists, and they are intimidated by what they don’t understand. Privately, political pollsters have been known to highlight the benefits of telephone surveys at the expense of focus groups for two simple reasons: either they are not qualified to moderate a group, or because the profit margin for telephone surveys is much greater. Others, particularly first and second generation media consultants, oppose focus groups because they cannot control the outcome, and they don’t like surprises. (As Art Linkletter might quip, \"voters say the darndest things.\") This is ironic. Better to be surprised by a focus group conclusion than to have that surprise delivered on election day. Focus groups do have their limitations. The participants are chosen scientifically but, as a group of 10 or 12 people, the findings cannot be projected onto the entire population. The results are dependent upon the interaction between the respondents and the moderator, and unprofessional moderating can lead to inaccurate conclusions. But scientifically derived quantitative data can also misinform and mislead. In the mid-1980s, a significant number of working class white Democrats were abandoning their party in favor of the Republican Party of Ronald Reagan. However, telephone polls at that time suggested a growing tolerance by these white voters toward the increasing political power of blacks. In fact, the exact opposite was the case. It took a series of focus groups, by Democratic pollster Stan Greenberg in suburban Detroit, to bring those telephone study errors to light. --- Voices of Victory, Part II The Makings of a Good Focus Group by Frank I. Luntz A well-run focus group is a laboratory for social interaction. A good focus group requires four simple characteristics: the proper composition, an open environment, a probing moderator, and in-depth analysis. The composition of the focus group must be selected strategically, with homogeneity as the key to a successful session. Human behavioral studies have consistently proven that people will reveal their innermost thoughts only to those they believe share a common bond. For example, if your goal is to study the real, in-depth feelings of whites and blacks toward affirmative action, welfare, or crime, you cannot have an integrated focus group. Similarly, women will not talk freely and emotionally about abortion if men (including a male moderator) are present. This is just a fact of life. The mood of the group is also critical. A single dominant voice can cripple open, honest discussion by intimidating the other participants. Also, keep food outside the focus group room. This has nothing to do with the potential for a food fight. Continuing participant attention to food is an unnecessary and ill-advised distraction. But the single greatest component of a successful focus group is the moderator. Academics have been justifiably critical of many focus group practitioners because they lack one or more of the following characteristics: • a creative mind • analytical skills • verbal skills • intellectual ability • an eye for detail • a tolerance for disorder • listening skills • a capacity for empathy Being a \"good listener\" is not enough to moderate a focus group properly. Remarkably few political focus group moderators have been academically or professionally trained to stimulate thorough but balanced discussion in an unbiased fashion. Similarly, all too often, focus group moderators put pressure on respondents to give information that they just do not have. The fact is, voters are ill-informed about the intricate details of public policy, and the loudest and most emotional respondent often knows the least about what he or she is talking about. A professional focus group moderator knows how to keep such an individual from intimidating and biasing the other participants. Even with the \"right\" participants, a good environment, and a trained moderator, the eventual success of focus group research in developing political strategy is fully dependent on the analysis. The question every focus group user needs to ask is: Who analyzes the transcripts? Too often, the dialogue is poured over not by behavioral scientists or by experts in sociology but by low level political types who know tactics but not people. This can lead to misinterpretation of comments, false conclusions and, eventually, flawed recommendations and strategy. State-Of-The-Art Qualitative Techniques Imagine the power of being able to measure instantly and specifically the exact reaction to a political theme, message, or messenger—second-by-second, by target population subgroups. That power now exists. In the post-Reagan era, most politicians have understood the importance of harnessing verbal and visual imagery in their effort to affect voter attitudes and opinions. Roughly one-half of President Clinton’s annual $2 million polling budget is targeted toward communication, and it shows with every speech and public appearance. Bill Clinton \"feels your pain\" because he actually knows what your pain is. Clinton’s not-so-secret (and not-so-new) weapon is a technology called \"instant response,\" which combines the most important components of quantitative, qualitative, and in-depth public opinion research to test message delivery, understanding, believability, and impact. A computer-based system, the instant response technology specializes in the immediate, second-to-second measurement of voter reaction to a speech, debate, or political advertisement. Here’s how it works: Participants are gathered in a single room for a two- to three-hour session. Each participant uses a button- or dial-operated hand-held computer, roughly the size and weight of a small paperback book, to relay his or her immediate reaction to a video or televised appearance. A portable PC collects and records these responses in real time, along with demographic information and customized quantitative close-ended opinion. During the presentation, a line graph is displayed continuously on a monitor adjacent to the PC. Audience reactions are gathered literally second-by-second, enabling the pollster to determine exactly which words, phrases, gestures, and other visuals enhance the communication effort, and which should be altered or abandoned. For example, George Bush used an instant response system to test \"ad-libs\" prior to his 1988 debate appearances, and Bill Clinton learned that his mannerisms and statements tested better before a live audience than they did straight to camera. The intensive focus groups following the session answer the question \"why\" and \"how,\" thus providing confirmation and strategic guidance. Focus group research is the least financially profitable tool of the polling trade, but it may be the most powerful. Political types may have been the last to discover its power, but nothing breeds attention more than success. Every candidate wants to win, and as they become aware of the qualitative option, its usage will continue to grow. Focus group influence is undeniable even today. As media guru Roger Ailes concluded, \"When I die, I want to come back with real power. I want to come back as a member of a focus group.\" --- Motivational Factors Focus groups are best used to explain \"why\" the public feels the way it does. A properly constructed and administered focus group will draw out the \"motivational factors\" behind the \"top of mind\" opinions -- which is critical to understanding what is driving public opinion. This figure illustrates the current motivational factors for the simple question: \"What is the most important problem facing America today?\" In telephone polls, rarely do more than 2% or 3% of those surveyed ever cite any of the three factors themselves. However, using traditional statistical analysis and an adjusted conjoint interviewing technique in focus group research, these three attitudes explain the fundamental motivational factors of more than 80% of Americans. • \"Declining quality of life\" explains public concern about health care, the economy, the deficit, taxes, and unemployment. • \"Disintegration of morality in society\" is the primary motivational factor behind concern about crime, drugs, welfare, and immigration. • \"The break-up of the American family\" is the fear most associated with the problems in education, and to a lesser extent, crime, drugs, and welfare. — Frank I. Luntz | \"Deep Faking\" Political Twitter using Transfer learning and GPT-2 Author: Ressmeyer, Ryan, Date: 2019 Collections: MediaAdsPolit, PoliticalML Zotero Key: VVZ9NPJJ Cite Key: Ressmeyer19deepFakePolitTwittXferLrn Zotero Item | Lit Note \"Deep Faking\" Political Twitter using Transfer learning and GPT-2 Ryan Ressmeyer Department of Electrical Engineering Stanford University ryanress@stanford.edu Sam Masling Department of Computer Science Stanford University smasling@stanford.edu Madeline Liao Department of Computer Science Stanford University mmliao@stanford.edu Abstract In the modern political climate, Twitter has become one of, if not the most impactful mediums for both current and aspiring politicians to communicate with their constituents. Our group attempted to replicate a variety of politicians’ twitter accounts using the recently released GPT-2 model from OpenAI and transfer learning. We compared our results to former state-of-the-art LSTM recurrent models to illustrate the advantages of transfer learning. Additionally, we used GPT-2 to extrapolate from a given context, allowing for robust tweet mapping from one user to another. 1 Introduction In recent election years, Twitter has become an increasingly important communication platform for American politicians. Our goal was to see if we could use state-of-the-art NLP techniques to not only recognize the different style and contents of tweets written by different politicians, but to actually generate similar tweets. This idea stemmed from two popular fields of study within NLP: Twitter sentiment analysis, and zero-shot text generation. By utilizing transfer learning and building upon OpenAI’s GPT-2 text generation model, we sought to train a model for the domain-specific task of generating tweets based on a given account. OpenAI’s GPT-2 is a transformer-based language model trained with the goal of predicting the next word given all previous words of some text [8]. Upon being given some prompt, the model has been shown to be able to generate seemingly human-authored text in response to the prompt that makes sense both grammatically and semantically. We wanted to extrapolate this model to generate tweets given a dataset of tweets to train on, rather than responding to a given prompt. Specifically, we wanted to be able to extract a topic from a given tweet and train on generating tweets about said topic. We scraped about 2000 tweets from each account we chose to \"deep fake,\" and for each tweet, we used RAKE to extract a list of keywords, and then for each keyword, we used GloVe to generate a set of similar words that were then appended to a \"history\" for the tweet [9]. Finally, based on the dataset of tweets with their corresponding histories, we were able to generate new tweets via GPT-2. 2 Related Work 2.1 Recurrent Neural Networks and LSTM for Text Generation Recurrent neural networks, specifically, long short-term memory networks, are commonly used for text-generation tasks. Sequence to sequence encoder/decoder models, such as that provided in by Sutskever et. al. (2014) [11], utilize LSTM to map sequences of text to a newly generated sequence of text, which has been shown to be effective for translation and dialogue texts [10]. LSTMs have also been used in previous works on tweet generation; the most similar existing work to our project is that of MIT postdoc Bradley Hayes: @DeepDrumpf, a Twitter bot that posts Trump-like tweets. Hayes’s model uses a recurrent neural network that generates tweets one character at a time. [4]. 2.2 The Transformer Model Recurrent neural networks are computationally expensive, and training cannot be parallelized due to its sequential nature. The transformer model alleviates this problem by neither using recurrent nor convolutional neural networks and only using attention mechanisms, which has allowed for text-generation models that are much less computationally expensive [12]. OpenAI recently trained a state-of-the-art transformer based language model on over 8 million online documents [8]. These 40 GBs of text, coined WebText, were curated by scraping the web with an emphasis on document quality. The model OpenAI created, GPT2, expanded on their original with slight layer normalization tweaks and an expanded vocabulary and context size. Without fine-tuning, it broke previous accuracy records for seven out of eight datasets. Indeed, it outperformed these previous records without ever actually training on the datasets themselves. OpenAI’s team conclude their paper with a recognition that their work has purely examined GPT-2’s zero-shot performance. That is, how it has been able to generalize its unsupervised training to new problems. However, they speculate that GPT-2’s potential far exceeds these metrics as it can easily be fine-tuned to tasks using transfer learning. For this reason, OpenAI only released the 345M parameter version of their full 1.5B parameter GPT-2. However, researches have still began to use these models to extend GPT-2 to domain specific uses. Some results include Lee, Jieh-Sheng, and Jieh Hsiang’s (2019) adaptation of GPT-2 to generate patent claims, and Budzianowski, Paweł, and Ivan Vulic’s (2019) task oriented dialogue bots. This paper hopes to add to the growing understanding of GPT-2’s capabilities by fine-tuning it to generate tweets in the style of a specific user. To achieve these results using transfer learning, Golovanov, Sergey, et al.(2019) specifies two means of adapting a zero-shot, unsupervised transformer model to a task specific language model. So called single-input training concatenates the context for language generation to training examples such that only a transformer-based decoder is needed to begin generation. Conversely, multi-input training uses an encoder-decoder structure to load context into a transformer model. Within the realm of this paper, we used single-input training examples due to their widespread use in recent GPT based transfer learning papers [1][13]. 2.3 Model Selection for Tweet Generation and \"Deep-Faking\" While LSTMs are commonly used for text-generation problems similar to the one we attempt to solve, a few factors led us to our ultimate decision to use transfer learning with the GPT-2 transformer model. Firstly, given the vast pre-training GPT-2 has courtesy of OpenAI, a transfer-based approach would retain the inherent knowledge of language that GPT-2 contains. This is important given the limited number of tweets we were able to attain from each user on twitter. As we found with our baseline LSTM, there was be no inherent knowledge of the english language, and thus outputs were random and lacked a coherent sentence structure. Secondly, given our focus on mimicking the content and style of specific politicians, we chose a transformer based model to allow our model to “focus” on learning each politician’s twitter personality given a specific context rather than simply learning how to complete a sentence (as was the case with LSTM). Additionally, a disadvantage of recurrent neural networks is that the less recent context is eventually “forgotten” by the model when generating the next word. With transformer models, a sentence’s topic is never forgotten, allowing for robust tweet generation. Thus, we decided to move forward with GPT-2 using transfer learning. Figure 1: Context extraction process. 3 Dataset and Features To generate our dataset, we used Twitter’s official API to gather the most recent tweets from numerous politicians. Twitter’s API allows you to scrape up to 3000 of a given user’s most recent tweets. These tweets, however, will include retweets, which we had to filter out given that we wanted to capture the specific style of each user, which is not shown in a retweet. This left us with around 2200 original tweets for each user we looked at. We then processed each tweet, filtering out images and links which left us with plain text for each entry. While links and images are often an important part of a twitter user’s persona, we chose to filter these out as the goal of this project was to capture the grammar and word usage of a user using GPT-2, which was not built to deal with images and links. Along with the filtering, we also used keywords to generate context or a “history” for each tweet (the reasoning behind this will be addressed in our introduction to transformers later in the paper). Using RAKE (Rapid Automatic Keyword Extraction), we extracted monogram and bigram keywords from each tweet. From there we used GloVe vectors to find the 10 most similar words to each key word, and used the aggregation of these similar words as the context for each tweet. Thus, we see that for the tweet in Figure 1, a simplified context could look like ‘capitalism wealth poverty state New York Dartmouth’. We used these contexts to help our model develop an understanding of what tweets on similar topics look like. This also allowed our model to generate tweets on topics that were tangential to the subjects of the tweets in our dataset, instead of only being able to generate tweets directly from the content matter of the dataset. 4 Methods 4.1 Training Example Formatting Given the RAKE then GloVe extrapolated context for each tweet, our training algorithm then generated training and validation examples. Following the single-input method described in Golovanov, Sergey, et al.[2019], our model was trained using a tweet’s context concatenated with the actual tweet. To specify which user a tweet came from, we add the tweeter’s name to the beginning of the input sequence. In addition, we use a sentence segment embedding layer to denote which parts of the input were the tweeter’s name, context for the tweet, and tweet itself. Finally, since GPT-2 is not a recurrent neural network, a positional embedding layer is added to give the model a concrete sense of position for each word. 4.2 Multi-task Loss To achieve our goal of fine-tuning GPT-2 to generate tweets in a similar style to a given twitter user, we used a multi-task loss defined as a linear combination of language model loss and Multiple Choice Loss. This combination of loss functions was found in Wolf, Thomas, et al. [2019] to drastically increase dialogue readability and adherence to context. language model loss is defined as the cross-entropy loss applied to a softmax of the transformer decoder output with the example tweet Figure 2: Visualization of the training process. Includes input representation and multiple choice loss words as the labels. Given the word sequence S = {w1, ..., w|s|}, language model loss is defined as: llm(S) = − |S| � i=1 � � log P[ˆ](wi|wi−1, ..., w1) As described in Wolf, Thomas, et al. [2019], Multiple Choice Loss involves adding \"distractor\" examples for each true example in the dataset. As previously mentioned, these examples have the context of a previous tweet, but the training tweet itself is randomly selected from other twitter accounts. Multiple Choice Loss involves calculating a final hidden layer after the sentence has ended. This final hiden layer, hl, is used as the input to a linear classifier layer to correctly classify the true example among distractors. During training, this classifier is trained jointly with the transformer model using the loss function: lmc(S) = −((y log(σ(hl ĎWh)) + (1 − y) log(1 − σ(hl ĎWh)) Where Wh is learned during training, y is defined as 1 if the example is the true example and 0 otherwise, and σ is the sigmoid function. At this point that it is worth mentioning that both of these losses, and the multiple choice classifier are automatically generated in our code during training using the Huggingface transformers library for Pytorch. It is open-source and dramatically reduces the barrier to entry into NLP using transformers. 4.3 Transfer Learning Specifics Our final model was trained on approximately 1400 tweets. It used an equal weighting of language model and multiple choice loss for 4 epochs. It trained with a learning rate of 6.25e[−][5] and batch size of 4. Using google cloud computing, the model only took around 40 minutes to train, since the training set was relatively small. 4.4 Tweet Generation In order to generate tweets, the output of the decoder must be extrapolated into words. GPT-2 takes an input and returns an array of probabilities for each word in its vocabulary corresponding to the probability of that each word follows the input. To generate text, we simply take one of these words and append it to the input and rerun the model. However, choosing which word to append given the probability distribution is difficult. The baseline is to use Greedy Decoding, which takes the highest probability word. Interestingly, this has been shown to not generate lifelike sentences. In our model, top k filtering was used to generate tweets. This involves always sampling from the top k number of words from all word probabilities. This addition of randomness among the top probabilities was found by Fan, Angela, Mike Lewis, and Yann Dauphin. (2018) to lead to more variance in sentence structure as well as longer sentences. In this case, k = 4 was used. 5 Experiments/Results For our baseline, we trained a Keras 2-layer LSTM on tweets by Elizabeth Warren and simply recorded the outputted \"tweets.\" For our GPT-2 based model, the procedure was slightly more complicated. Our code is available at https://www.github.com/JumboJoll/CS229_Final_Project. After augmenting the data as described in section 3, we trained our model on Donald Trump’s account specifically. In order to generate output, we then fed in contexts from other accounts. Below, we include a table of results of both our baseline LSTM as well as the GPT-2 model using contexts from Elizabeth Warren tweets as input for our Trump-trained model. Our LSTM produced results that were contextually relevant, however the actual tweets themselves were not coherent English. Some examples were: “care banks rake ed workers wants know would high help us free government need past students” “wall bill right federal week gay bill working federal work cares wear health admin working country” We saw drastically improved output from our Transformer model, however: Qualitatively, we notice that our generated Trump tweets 1) reflect the content from the Elizabeth Warren tweet, and 2) reflect both Trump’s tweet style as well as his political views. We also were able to calculate the accuracy of the model as well as the loss at each epoch. 6 Conclusion/Future Work Using transfer learning to convert GPT-2 to our domain specific task proved to be more successful using an LSTM. Using contexts from Elizabeth Warren’s tweets as the input for a model trained on Trump’s tweets, we were able to generate coherent Trump-style tweets about the content from the Warren tweets. Granted, we did not spend a significant amount of time fine-tuning our LSTM; however, for our goal of generating persona-specific tweets, transfer learning on top of a transformer model proved more worthwhile than tuning hyperparameters to ensure that our LSTM learned both coherent English in addition to a persona. We plan to build upon our results by implementing a more robust evaluation method for our tweets. Given the subjective nature of the quality of text generation (and even more subjective nature of tweet generation, since tweets are often written concisely and informally), we chose to use human analyses to evaluate our model. However, including quantitative measures of analysis would allow us more insight into how we could further improve our model. BLEU scores are commonly used to analyze translation models, while loss and perplexity were metrics used in comparison of various text generation techniques Kawthekar et. al. (2017) [4]. However, the evaluation of our model proves to be difficult because we are not necessarily comparing it to common english. Instead, Twitter is a highly specialized language domain that will likely require its own metrics. Thus, additional tweaking of current evaluation techniques and more research needs to be done in order to more robustly analyze our tweets. 7 Contributions Ryan Ressmeyer: GPT-2 training, debugging, and generation. Madeline Liao: Background research on text generation models and techniques, data processing and augmentation, tweet context generation Sam Masling: Background research on a variety of text generation models, baseline implementation, data processing, tweet context generation References [1] Budzianowski, Paweł, and Ivan Vuli´c. \"Hello, It’s GPT-2–How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems.\" arXiv preprint arXiv:1907.05774 (2019). [2] Fan, Angela, Mike Lewis, and Yann Dauphin. \"Hierarchical neural story generation.\" arXiv preprint arXiv:1805.04833 (2018). [3] Golovanov, Sergey, et al. \"Large-scale transfer learning for natural language generation.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019. [4] Hayes, Bradley. \"DeepDrumpf.\" Twitter account. [5] Kawthekar, P., Rewari, R., Bhooshan, S. \"Evaluating Generative Models for Text Generation.\" 2017. [6] Lee, Jieh-Sheng, and Jieh Hsiang. \"Patent Claim Generation by Fine-Tuning OpenAI GPT-2.\" arXiv preprint arXiv:1907.02052 (2019). [7] Pennington, J., Socher, R., Manning, C.. \"GloVe: Global Vectors for Word Representation.\" 2014. [8] Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI Blog 1.8 2019. [9] Rose, S., Engel, D., Cramer, N., Cowley, W. Automatic keyword extraction from individual documents. 2010. [10] Sundermeyer, M., Schlüter, R., Ney, H. \"LSTM Neural Networks for Language Modeling.\" 2012. [11] Sutskever, I., Vinyals, O., Le, Q. \"Sequence to Sequence Learning with Neural Networks.\" 2014. [12] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems. 2017. [13] Wolf, Thomas, et al. \"Transfertransfo: A transfer learning approach for neural network based conversational agents.\" arXiv preprint arXiv:1901.08149 2019. AI Focus Groups: How Artificial Intelligence is Transforming Research [+5 Amazing Tools] Author: Oberoi, Kritika, Date: May 30, 2024 Collections: FocusGroups Zotero Key: QL4PE6DW Cite Key: Oberoi24focusGrpAI Zotero Item | Lit Note Focus groups have long been a staple in market research, providing valuable insights into consumer behavior and preferences. With a global market size exceeding $1.1 billion, it's clear that businesses recognize the power of these guided discussions. But what happens when you combine this tried-and-true method with the cutting-edge technology of artificial intelligence (AI)? The result is a revolution in the way we conduct and analyze focus groups. In this article, we'll explore how AI focus groups are changing the game—making Focus Group Discussions more efficient, cost-effective, and accessible than ever before. From AI-powered personas to automated transcription and analysis, we'll delve into the various ways AI is being leveraged to enhance the focus group experience. We'll also discuss the benefits and limitations of using AI in focus groups and introduce you to 5 AI focus group tools worth considering. What’s a focus group? Before we dive into how AI is revolutionizing focus groups, let's take a step back and clarify what we mean by a focus group. According to the Qualitative Research Consultants Association (QRCA), a focus group is a moderated discussion among a small group of carefully selected participants. These participants are chosen based on specific criteria, such as demographics, psychographics, or behaviors, to ensure they represent the target audience for a particular product, service, or concept. The purpose of a focus group is to gather qualitative data, like opinions, perceptions, and experiences, through guided discussions. A skilled moderator facilitates these conversations, using a well-crafted discussion guide to explore topics of interest. Focus groups are often used in market research and social science research to gain deeper insights into consumer or user behavior—they’re particularly common in the CPG industry. By creating a comfortable environment that encourages participants to share their thoughts and feelings, focus groups can uncover valuable information that may not come up through survey or other quantitative methods. One caveat: UX researchers are sometimes skeptical about focus groups because of the bias and social pressure that can creep in when multiple people are sharing opinions. How is AI changing Focus Group Discussions? In the world of market research, focus groups have long been a go-to method for gathering qualitative insights. However, the traditional focus group process can be time-consuming, costly, and prone to human error. AI is shaking things up—with recent advancements in AI technologies, focus groups are undergoing a major transformation, becoming more efficient, cost-effective, and valuable than ever before. Here are 4 major ways AI is transforming Focus Group Discussions: AI for Focus Group Questionnaires Creating your discussion guide can be time-consuming. Why not getting a helping hand from AI? AI can create your first draft of your focus group questionnaire. Customize the prompt below to get AI focus group questions: Create a focus group discussion interview script of 15-20 questions as a market researcher, to gather detailed insights from users of [X app/activity]. Include warm-up questions about the user’s day and work life. Covers key topics such as user needs, preferences, and experiences with similar products. Ask about A, B, and C features. Use simple language. Need a complete list of GPT prompts for research? Get started here Remember to check and improve on the output! AI will get you 80% of the way there, but still needs researcher supervision to get to the perfect focus group questions. AI-Powered Personas One of the most intriguing (and newer) developments in AI focus groups is the emergence of AI-powered personas. These virtual ‘AI participants’ are created using machine learning algorithms that analyze vast amounts of user data to generate realistic representations of target audiences. The promise of AI personas is compelling—imagine being able to conduct focus groups without the need to recruit real participants, saving time and resources while still gathering valuable insights. It’s true, no one likes recruiting—but there are serious risks and limitations to consider with AI personas. One of the primary concerns with AI personas is the accuracy of their outputs. While machine learning models can analyze patterns and generate responses based on historical data, they may struggle to capture the nuances and constantly evolving nature of human perceptions and needs. Additionally, even if AI personas are based on comprehensive user data, there's no guarantee that they will accurately represent the diversity of opinions and experiences within a target audience. Relying solely on AI personas for focus group insights could result in a biased or outdated understanding of user needs, leading researchers astray. And as researchers we know—we’re nothing without strong, reliable data. AI-Supported Transcription and Analysis One of the most significant ways AI is transforming focus groups is through advanced transcription and analysis capabilities. In the past, transcribing and analyzing focus group recordings was a time-consuming and error-prone process, often requiring hours of manual labor. With the latest AI-powered transcription tools, researchers can now get over 90% accurate transcripts in minutes, across diverse accents and speaking styles. This means less time spent on tedious transcription tasks and more time focused on thinking through valuable insights in your data. But AI's impact on focus group analysis doesn't stop at transcription. Cutting-edge tools like Looppanel are taking things a step further—automatically tagging focus group transcripts, so it’s easier than ever to identify key themes and patterns across sessions. Auto-tagged data on Looppanel With AI-assisted analysis, researchers can discover critical insights up to 5x faster than traditional manual coding methods. This is particularly useful in fast-paced research environments where tight deadlines are common. While manual coding will always be a valuable tool for in-depth qualitative analysis, AI-supported transcription and analysis tools are quickly becoming essential for researchers looking to work smarter, not harder. AI for Report Writing You’ve run your research, analyzed your findings—now you need to write up a shiny report that communicates your incredible insights. One of the most time-consuming parts of focus group discussions is crafting a comprehensive report for stakeholders. This can take a lot of time, but AI can help. Researchers can use AI chatbots like ChatGPT or writing tools like AI Summarizer & Copy.ai to quickly draft concise and to-the-point reports. You can feed your key data points to these tools and ask it to summarize or re-write your insights. You can even give it instruction on the tone you want the AI to use and how expansive you want its writing to be. For example, you can customize this prompt to re-write research findings for your report: Given the following research data about our product X, a [describe the product], please provide a concise summary of this insight in [customize output format: e.g., 2-3 bullet points]. Use simple, readable language that can be shared in a report. [Paste data + insights] Want to play around with more GPT report writing prompts? Here’s a list to start with Benefits of AI in Focus Groups AI offers numerous benefits that can greatly enhance the focus group experience for researchers and participants alike. Let's explore some of the key advantages of AI focus groups: 1. Increased efficiency: One of the most significant benefits of AI in focus groups is its ability to streamline time-consuming tasks. AI-powered tools can assist researchers in creating questionnaires, transcribing audio recordings, and generating reports. By automating these processes, researchers can save valuable time and focus on more strategic aspects of the study, such as analyzing insights and making data-driven decisions. 2. Enhanced insights: AI algorithms excel at identifying patterns, sentiment, and key themes within large volumes of qualitative data. By applying AI to focus group transcripts and recordings, researchers can uncover deeper insights that may have been overlooked through manual analysis alone. AI can also help researchers quickly identify trends and connections across multiple focus groups, enabling them to draw more comprehensive conclusions. 3. Cost-effectiveness: Conducting focus groups can be costly, especially when considering expenses such as participant recruitment, venue rental, and moderator fees. AI can help reduce these costs by enabling researchers to conduct virtual focus groups, reducing the need for physical spaces and travel. Additionally, AI-powered tools can automate tasks traditionally performed by human assistants, such as transcription, further reducing expenses. 4. Increased accessibility: AI technologies can also make focus groups more accessible to a wider range of participants. For example, AI-powered transcription tools can provide real-time captions for participants with hearing impairments, while language translation AI can enable researchers to conduct focus groups with participants who speak different languages. By breaking down these barriers, AI helps ensure that diverse voices are heard and represented in focus group research. 5. Reducing data overwhelm: One of the biggest challenges of focus groups is the amount of data you’re dealing with. If you’re running multiple focus groups, you may end up with 100s of pages of transcripts, notes, observations. While we as people find this volume of data extremely overwhelming, AI can help categorize it for us to make it easier to consume, process, and review. Overall, AI has the potential to revolutionize the way researchers conduct focus groups. By leveraging AI technologies, researchers can save time, uncover deeper insights, reduce costs, increase accessibility, and reduce overwhelm. As AI continues to advance, we can expect to see even more innovative applications in the field of focus group research. Drawbacks and limitations of AI in Focus Groups While AI can be incredibly helpful in focus groups, it's important to understand that it has some limitations. Here are a few potential drawbacks to keep in mind when using AI in focus group research: 1. Accuracy concerns: Although AI is very powerful, it can still make mistakes. It may misinterpret what participants say in a focus group, especially since it doesn’t have access to facial expressions and other data. Researchers should always review the AI's output to ensure it aligns with the actual focus group discussions. 2. Lack of human nuance: AI excels at identifying patterns and themes, but it can't fully replace human understanding. Researchers still need to apply their own knowledge and experience to interpret the data and draw meaningful conclusions. AI is a tool to assist researchers, not a substitute for their expertise. Let’s say ChatGPT creates your first draft of your interview guide—it’ll have the major building blocks, but will require your input to make sure the key questions your stakeholders care about are being covered. AI focus group questions will be a great starting point, not the perfect answer. 3. Contextual limitations: Sometimes, the way people express themselves in a focus group is just as important as what they say. AI might not always pick up on subtle context clues, such as sarcasm or humor. Researchers should review the focus group recordings to catch these nuances that AI could miss. This is especially the case if you’re dealing with any kind of physical stimulus (e.g., you’re asking for participant feedback on new package designs). 4. Potential biases: AI learns from the data it's trained on. Large language models (LLMs) that are popular today are trained on data from the internet, which is naturally biased towards a white, male demographic. This means the AI can bring in bias based on the lens its applying—researchers should check AI’s output to make sure that bias is minimized. 5. Privacy and data security: Using AI in focus groups involves collecting and analyzing large amounts of personal data. This raises important concerns about privacy and data security. Researchers must be transparent about how they're using AI and ensure they have robust measures in place to protect participants' personal information. For example, be very careful if you’re using open chatbots like ChatGPT to analyze data—make sure you’ve opted out of letting them use your data for training purposes. Despite these limitations, AI remains a valuable tool for focus groups when used thoughtfully and in combination with human expertise. As AI technology continues to evolve, some of these drawbacks may become less significant. However, researchers should always use AI responsibly and not rely on it too heavily at the expense of their own judgment and skills. 7 AI Focus Group Tools to Consider Incorporating AI tools into your focus group research can significantly streamline your workflow and enhance the quality of your insights. Here are 6 popular AI Focus Group tools that can help you at various stages of your research workflow: 1. Looppanel ○ Use case: Use Looppanel to record, transcribe, and analyze focus group discussions ○ AI features: Generates transcripts with over 90% accuracy, automatically creates detailed session notes, and intelligently tags themes for easy analysis ○ When to use it: When running focus groups on Zoom, Google Meet, Teams or in person, especially if you’re under tight deadlines for analysis ○ Limitations: You cannot run the focus group session directly on the Looppanel platform; you'll need to use a separate video conferencing tool if running it virtually (e.g., Zoom, GMeet, or Teams). 2. Discuss.io ○ Use case: Running focus group sessions on the platform, recording, and transcribing ○ AI features: Provides session summaries, helps you write effective screeners for participant recruitment and improve your discussion guides ○ When to use it: When you’re unable to use Zoom, Google Meet, or Teams for your virtual focus group session and need an all-in-one platform ○ Limitations: Focus Group participants may be more likely to drop off or be skeptical when asked to join a session on an unfamiliar tool, compared to well-known platforms like Zoom, Google Meet, or Teams. 3. Zoom ○ Use case: Running focus group sessions, translating across languages, and providing real-time captioning ○ AI features: Offers real-time translation for global sessions, generates transcripts, and provides AI-generated session summaries for quick insights ○ When to use it: When you want to run a focus group that is easily accessible to everyone, especially if working with a tight budget or participants from different language backgrounds ○ Limitations: As Zoom is not primarily a research tool, you may want to pair it with a platform like Looppanel for better transcription quality, AI-powered notes, advanced analysis features, and the ability to create video clips for reporting 4. Otter.ai ○ Use case: Recording and transcribing audio from focus group sessions ○ AI features: Transcription, AI-generated notes for easy reference, and call summaries to quickly grasp key points ○ When to use it: When you need real-time transcription of a focus group session or require a large amount of transcription at an affordable price ○ Limitations: Lacks support for video, which is a significant limitation as it prevents the analysis of facial expressions and nonverbal cues; also lacks built-in analysis features for deeper insights 5. ChatGPT ○ Use cases: Writing discussion guides, summarizing insights, and drafting reports ○ AI features: Generates high-quality written content (e.g., interview questions), rewrites content for reporting purposes, and can even provide suggestions for improvement ○ When to use it: To rewrite, summarize, or create drafts of interview materials, saving time and effort in the process ○ Limitations: Can generate incorrect or inconsistent content, so it's essential to review the output; also, ensure that you have requested the platform not to use your data for training purposes to avoid privacy risks, particularly when inputting participant data 6. AI Summarizer ○ Use cases: Generating summaries of proposals, plans, etc., extracting key points from research material, and organizing small chunks of data. ○ AI features: Concisely condenses the given content without damaging meaning and quality, performs abstractive summarization, and generates output results according to the specified length. ○ When to use it: When you want to concisely communicate with the focus group or extract and organize key points from large amounts of documentation for further discussion or analysis ○ Limitations: If your input content contains grammatical errors, this can confuse the AI summarizer, leading to inaccurate results. 7. Copy.ai ○ Use cases: Writing discussion guides, summarizing insights, and drafting reports ○ AI features: Generates written content (e.g., interview questions), rewrites content for reporting purposes, and offers templates for various research-related tasks ○ When to use it: To rewrite, summarize, or create drafts of interview materials, leveraging the platform's templates and AI-powered writing assistance ○ Limitations: Built on top of GPT, which means it can generate incorrect or inconsistent content, so it's crucial to review the output and ensure it aligns with your research goals Conclusion AI is transforming the way we conduct focus groups, making the process faster, more efficient, and more insightful. From generating AI focus group questions to analyzing transcripts and drafting reports, AI focus group tools are helping researchers at every stage of the process. However, it's important to remember that AI is not a replacement for human expertise and judgment. While AI can provide valuable assistance, researchers must still use their skills and knowledge to design effective focus group studies, interpret the results, and make informed decisions. As with any new technology, it's crucial to understand the limitations and potential drawbacks of AI in focus groups, such as the risk of biased or inaccurate outputs. By using AI tools thoughtfully and in combination with human expertise, researchers can unlock the full potential of AI in focus group research and gain deeper insights into their target audiences. Want to learn more about how AI is impacting qualitative data analysis? Read the dedicated article here The Way Harris Lost Will Be Her Legacy Author: Cottom, Tressie McMillan, Date: 2024-11-06 Collections: Hot Takes US Elect 2024 Zotero Key: 5JCG2JCM Cite Key: Cottom24OpinionWayHarris Zotero Item | Lit Note I have always been audacious about Black women’s political aspirations. When I was 15 years old I decided to become a local pageant queen, a Carousel Princess, for our Thanksgiving Parade. It was an uncharacteristic choice. I was an oratory and student government kind of girl. I don’t remember much about the pageant except the question portion, in which I was asked, “What do you want to be in the future?” I intended to be “the first Black female Supreme Court justice.” The emcee smiled indulgently. I stared defiantly. Some 30 years later, Ketanji Brown Jackson took my Supreme Court justice job. And this week Vice President Kamala Harris almost became the first Black female president. Our policy differences aside, her ascendancy to the top of her party should have felt like the realization of my childhood dreams. Instead, for weeks I have felt isolated in my ennui. As it turns out, Donald Trump was harder to vanquish than so many of us hoped. He won. And today it feels like his brand of identity politics is more audacious than my youthful dreams about a Black female president. The Monday night before Election Day, I drove to my nearby alma mater, North Carolina Central University, with an anxiety headache. In the words of Alice Walker, I went searching for my mother’s garden — a connection to the ancestral continuation of Black women’s survival and resilience. The campus has a polling place, perhaps in part because I helped organize one my sophomore year in college. It is where I cast my first vote for president. This historically Black college campus in Durham, N.C. — a Southern city that once had a “Black Wall Street” — is full of the ghosts of voter suppression and dogged Black resilience. It is also home to the ghost of my young, ambitious, audacious self. As I walked across campus, I thought about the prospect of a Harris victory and wondered why I wasn’t more excited about the possibility of our first Black female president. Kamala Harris embodies audacity even more than Barack Obama once did. Like Obama, she is of mixed race and identifies as Black. But she is also the child of two immigrants, and she is unquestionably a product of Black American institutions. She is a graduate of Howard University. A member of the first intercollegiate Black sorority, Alpha Kappa Alpha. An Oakland pedigree. And she has that silk press. Women of color were among the last minority groups to get the right to vote. The fact that someone who represents those who were last to be included in American democracy came so close to saving this nation from its worst impulses is audacious, by any measure. There will be a lot of accounting of why Harris came so close but did not close the deal. I know two things today. I know the fundamentals of how and why Trump exists — I’ve written about them too many times to count — and I know that nothing about those fundamentals has changed. I knew that a month ago. I knew it a week ago. I didn’t want to know it again today. But this election was about enthusiasm and diagnoses. The long-term trajectory of our country has not changed. Millions of middle-class people feel working-class. These Americans have no way to describe what is happening to them. Is it the borders? Is it supposedly dangerous cities? The loss of God in the classroom? Cultural Marxism? Wokeism? Inflation? Tariffs? Women? Nature abhors a vacuum. But political opportunists thrive in vacuums. This election was about who told a better story about the fundamentals without promising anything to fix those fundamentals. That is the sweet spot for an opportunist like Trump and his party. Sell everything, promise nothing. Keep them coming back for more. The Harris campaign countered with a relatively hopeful message about the future. But the candidate ultimately ran on a platform of stasis. Her promise was that nothing much would change about the country but the race and gender of the one in charge — and she didn’t even lean into a history-making narrative about her race and gender because it felt like a promise she could not keep. A promise of a more audacious America. Maybe how she almost won is the reason she could not win. I was never going to be a big fan of some of Harris’s politics. I do not believe in police reform and she is a “law and order” Democrat, for one example. But I am a pragmatic voter. I know that most Americans are center-right and that the left has not yet built the power base necessary to make most Americans center-left. She was always going to be the only legitimate candidate in a race against Donald Trump, a felon and self-aggrandizing political troll. Perhaps more important, when she became the Democratic nominee, it did not feel like a conquest. It felt like a Hail Mary. The stink of our national desperation has haunted me since Harris accepted the nomination. It called to mind how Black women become firsts in other arenas. Research shows that women often inherit leadership roles when organizations are in crisis. That kind of risk exposure is especially acute for women of color. Desperate companies and nonprofits blow through their wunderkind leaders and turn to women, who often have fewer organizations competing for their talents. They can get them at a good deal — female C.E.O.s often make less than male C.E.O.s. And when the organizational fractures are too deep to be fixed or the female leaders are given too little authority or resources to fix them, they are blamed for the failures they inherited. It was a short election cycle. Harris didn’t have a victory lap after early primaries or on Super Tuesday. We did not get a chance to invest in Harris’s capabilities, only her willingness to inherit our broken mess. And the early forensics of her campaign read like a funeral program instead of a celebration of life. Notably, there are numerous obituaries for “identity politics” that argue that Harris’s decision to de-emphasize her race and gender in her campaign means that diversity is dead as an overt political strategy. But what if she should have leaned into identity more, risking the white identity voter but telling us a clear story about why electing her would speak about our audacious potential? We cannot know for sure. We do know that she declined to write the narrative of her historic firsts throughout her campaign, while Trump wrote the second volume of his story of which identities should matter most. Harris’s campaign positioned race and gender not as strengths to be reclaimed but flaws to be subsumed. If you have to hide the light of your race and gender behind the metaphorical bushes to come vaguely close to being the first Black female president, then identity still matters very much indeed. Imagine someone like Southern-bred Stacey Abrams being the first Black female president. Put aside whether Abrams’s politics are better or worse — that is not necessary for this exercise. What matters is that when she ran for governor of Georgia, she leaned into how much race and gender shaped her Americanness. I will never forget her powerful essay on why her student loan debt made her a better leader, as opposed to a fiscally irresponsible one. She embodied the racial wealth gap that condemns Black Americans to what the sociologist Louise Seamster has called “bad debt” — the debt we take on to earn our way out of poor economic opportunities. That looks like bad mortgages and upside-down car loans and, yes, student loans for the education we have to have to get our foot in the door. Admittedly, Abrams had to own this part of her political biography. She spent most of her childhood in the South. But if this country were to put a Southern Black woman with the shared experience of millions of working-class and aspirational Black Americans in the White House, her ascendancy would be a statement on this country’s original sin. The sin that still animates housing segregation, school inequality, wage inequality and environmental racism. The idea of Abrams as president is merely an example of a different kind of campaign, on an alternate timeline, where being first could have given us a first worth voting for. Will more Black women run because of Kamala Harris? Almost definitely. Will more Black women win using Kamala Harris’s playbook? I hope not. Black feminism — and especially the idea of intersectionality — became a pop culture cliché in the 2020s. But Harris took a more traditional second-wave feminist approach to her performance of womanhood. Her campaign did not challenge gender conventions so much as it selectively subverted some gender expectations (and left others intact) for her political benefit. She has this in common with Sarah Palin, another first who almost went the distance. I always believed that a conservative woman would be the first female president because of what social science says about gender and leadership. American voters demand compensating qualities — gun-toting, conventional attractiveness, religiosity — to accept a female president. Harris played that game, finessing that hybrid centrist conservative in a progressive package. That feels like a game plan for the many female candidates who will follow in her footsteps. It is less clear to me that Harris’s game plan can ever actually work for a Black female candidate. For one thing, Black women cannot usurp Palin’s gun-toting, America-first political shtick. Harris came close. On the trail, Harris (and Walz) talked about packing a gun the way women in politics were once expected to talk about packing their kids’ school lunches. She also delighted in being tough on crime. By pivoting to the right, Harris bet that American voters trust female politicians who present as culturally conservative, like Palin. Harris just did it from the Democratic ticket. That choice is part of her legacy. What isn’t part of her legacy is the end of identity politics. Try as some might to bury the salience of race, class and gender to political life, identity still matters. And it mattered to Harris’s campaign. The devil is in how people misunderstand what identity politics is. The common-sense understanding of identity politics is that it is rooted in the shared group identity based on linked oppressions. That link to oppression is key. It is why only minorities are thought to have an “identity” in identity politics. By that definition, Kamala Harris would have to bleat her horn about breaking glass ceilings. That is what Hillary Clinton infamously did in her nomination acceptance speech in 2016. She also would have gone Jesse Jackson with a rhyme about hope and dope and a rope that overtly draws on the Black elegiac tradition. That is what then-candidate Barack Obama did in 2008. Given that her Indian American heritage does not have the same kind of resonant political story as women’s and Black Americans’, if she had leaned into that identity as a key part of her campaign strategy, she would have had to invent one. If you think identity politics means “performing an identity,” then Harris bucked the progressive habit of leveraging identity politics. Someone looking to build on her campaign’s progress might think that Harris lost because she didn’t ham up the guns and nationalism enough and bury her identity even deeper. That is wrong and it is racist. And it is also wrong because it is racist. It is racist to believe that racial identity is an affliction that only weak-willed or weak-minded minorities suffer. When we limit identity politics to minorities, we insist that in politics only nonwhite people have racial identities (and to a lesser extent, that only women have gendered identities). If you accept that everyone has a racial identity — which is very hard to refute — then every political individual engages in identity politics. White voters are also identity voters. In fact, they are the most important of all identity-driven voters. It was their votes that Harris had to court. I thought a lot this election season about a book by the political scientist Ashley Jardina, “White Identity Politics,” published in 2019, that examines how much white racial attitudes are rooted in their white identity politics. Jardina’s data supports the theory that white voters feel, and therefore act, in ways that support their view of themselves as white. They are not necessarily prejudiced against minorities. In fact, Jardina argues that the empirical connection between prejudiced views and white identitarianism is pretty weak. A lot of white people don’t feel white and yet vote and act in racially prejudiced ways. And some white people feel white, vote from a white identity and do not particularly act in racially prejudiced ways. White identity voters are not located just in conservative areas, like the South. They aren’t all Republicans or conservative voters or poor or working class. Jardina is clear that white identity politics has increased across the income, ideological and geographical spectra. Trump’s final electoral map picked up new voters, kept his core voters and found new identity voters — Latinos, Black men and immigrants. Harris may not have emphasized her own identity in her campaign but her campaign absolutely emphasized the interests of those white identity voters who may have leaned toward Trump in 2016. Those voters tend to be threatened by, and therefore obsessed with, immigration. This was the source of candidate Harris’s hard center-right turn. Time and time again, she put her experience as a prosecutor and an attorney general forward when she talked about securing the border. She embraced American exceptionalism abroad and policing at home. She did not appear to kowtow to the Democratic Party’s left wing. Palestinian supporters did not succeed in getting a Palestinian speaker onstage at the Democratic National Convention. She did not utter now-toxic slogans like “abolish the police.” And she pushed back on those who tried to position her as a “D.E.I.” candidate. Neoconservatives like Dick Cheney endorsed Harris not only because Trump is so dangerous; on issues like national and domestic security, it’s hard to pass a light between her positions and those of a typical conservative. The nation’s first Black woman on a national ticket tapped into the white identity voters’ nationalism. She pitched herself to the white identitarian coalition that Trump has played like a fiddle. That is just more of the white identity politics on which American politics were built — and that the Black franchise should always be an indictment of. Obama’s coalition feels like an aberration and it is time for us to accept that American politics is identity politics. It is a battle for the identities that will be named and shamed, while others are granted anonymity and power. Playing to the silent white identitarian does not guarantee victory if you aren’t a white identity candidate. If we learn anything from Harris’s loss, it should be that we must stop sacrificing the power of other identities — their values, their hopes, their security — for a chance that will not pan out. Black women have saved democracy enough times to deserve more than cosmetic diversity. We deserve more than hope and change. We deserve recognition and power. Above all, we deserve better than a post at the head of a sinking ship that reveres the iceberg. All this isn’t to say the Harris campaign did not leave a mark. She proved the continuing importance of Black institutional life. The schools and churches and social groups that Black Americans cobbled together from the ashes of white violence and dispossession made her candidacy possible. Wealthy donors make you competitive. But a base makes you viable. Her competitiveness proved that Black culture can still make a powerful base. Kamala Harris would not be the first Black female vice president were it not for Howard University, a historically Black college, and her affiliation with Alpha Kappa Alpha, a Black Greek-letter sorority. Even her political base in California’s Bay Area tapped into that area’s racial history — the Black migration that built racial enclaves that became microsites of political power. Whether or not Harris’s campaign alters the political imagination for Black female electoral power, the institutions she has brought along with her just might. But it won’t be today, and it won’t be this year. How focus groups informed our study about nationalism and international engagement in the U.S. and UK Author: Mordecai, Mara, Date: 2020-10-05 Collections: FocusGroups Zotero Key: J5DHGH96 Cite Key: Mordecai20HowFocusGroups Zotero Item | Lit Note (Related post:How quantitative methods can supplement a qualitative approach when working with focus groups) Pew Research Center surveys regularly reveal divergent opinions about globalization, national identity and international engagement, both in the United States and other countries. Political affiliation and ideological orientation help explain some of these differences in opinion, but we wanted to explore more fully how local context and national identity also shape opinions about globalization. To do this, we designed a cross-national, comparative qualitative research project. We recruited 232 people to participate in 26 focus groups in seven cities in the United States and United Kingdom. We asked participants about their views of local, national and international issues. Unlike in our surveys — where respondents typically answer closed-ended questions — we asked open-ended questions, allowing participants to discuss their responses in more depth. And being in a focus group allowed participants to engage with one another and the topics posed. After the focus groups concluded, we developed a strategy to systematically analyze verbatim transcripts by theme. We then assembled a matrix of distinct, yet sometimes intersecting, frames of reference that people employed when discussing the nature and consequences of globalization, national identity and international engagement. In addition to forming the basis of our analysis for a data essay published today, the findings of the focus groups helped guide us in developing survey questions which will be fielded soon in the U.S. and UK, as well as other countries. In this post, we’ll explain how we developed and carried out this research project, and what we learned. To navigate directly to the different components of this analysis, you can use the links below: Designing the project From the start, we planned to use the focus group findings to inform future opinion surveys. Our research design aimed to depart from the structured interview format of quantitative surveys, while maintaining a systematic, rigorous approach to analyzing the focus group transcripts. The number and composition of focus groups in the U.S. and the UK were intended to exhaust the range of views on globalization, international engagement and national identity — providing us with enough disparate opinions about these topics from people in different walks of life, geographies and more that we achieved a “saturation” of sorts. Saturation is a difficult standard to demonstrate; however, we can report that over the course of the study we did observe an increasing repetition of themes, if not experiences, and gradually fewer views that could be described as novel or innovative. This suggests that the research design did, in fact, achieve a reasonably robust degree of saturation. Building our focus groups We drew on extensive background research and expert consultations to organize focus groups in the U.S. and UK using similar but distinct criteria and variables, such as party affiliation, ideology, vote in the referendum to leave or remain in the European Union, and 2016 U.S. presidential vote. Focus group composition in each country was designed to maximize variability — e.g., across different political ideologies — while creating optimal conditions for open, active engagement by participants — e.g., speaking with “leavers” in one group and “remainers” in another. Importantly, we found that people with certain characteristics in common were more likely to feel comfortable speaking openly about their opinions. United States In the U.S., research indicates that partisanship plays a role in how people feel about the themes we wanted to explore, but that the experience of being a Republican in a primarily “blue state” might vary from that of a Republican in a “red state.” As a result, we selected three cities — Seattle, Houston and Pittsburgh — in states that can generally be described as Democratic-leaning (Washington), Republican-leaning (Texas) or a “battleground” (Pennsylvania). Our research also suggested that Americans’ views about globalization and immigration differ by their community type — rural, suburban or urban — as well as by their race and ethnicity. But we’ve learned from our past focus groups on this topic that it’s important to create homogeneous racial and ethnic groups so as to minimize participants’ discomfort speaking about sensitive issues and encourage more open sharing of personal experiences and perspectives. To that end, we created one focus group with each of three distinct racial and ethnic communities: a group composed of Asian and Pacific Islander Americans in Seattle, one comprised of Hispanic Americans in Houston and one consisting of Black Americans in Pittsburgh. All other groups were comprised of White Americans exclusively. Taking all of these factors into account, we organized a total of four focus groups in each city: 1) a group of White Republicans and Republican-leaning independents living in rural areas surrounding each city; 2) a group of suburban White independents; 3) a group of ethnic minority independents who hailed from suburban or urban locations; and 4) a group of White urban Democrats and Democratic-leaning independents. As an additional “screen,” or question we used to help sort participants into particular groups, we asked every participant a previously used survey question that related to our topic of interest: whether America’s openness to people from all over the world is “essential to who we are as a nation,” or ”makes us risk our identity as a nation.” We wanted to ensure that everyone had an opinion on this question and that Groups 1 and 4, as described above, were of like mind about it (the former saying America risks losing its identity, the latter saying openness is essential) and that Groups 2 and 3 had some variation. United Kingdom In the UK, we again focused on geography and ideology, but the Brexit vote also played a key role. In particular, we wanted to include two cities that largely voted to remain in the EU and two that largely voted to leave, while also capturing the diversity of views among people living in those areas. We chose London and Edinburgh because they voted resoundingly to remain in the EU — and Edinburgh, specifically, to understand the Scottish perspective and explore the attitudes of those who do and do not support Scottish independence. We chose Birmingham and Newcastle because both voted to leave the EU and were historic manufacturing hubs, while Newcastle also offered a northern perspective. Using a seven-point left-right ideological orientation scale, we created four focus groups in each of the three cities in England: 1) those on the ideological right who voted to leave the EU; 2) those on the ideological left who voted to leave; 3) those on the ideological right who voted to remain; and 4) those who on the ideological left who voted to remain. In addition, all of the ideologically left-leaning focus groups included at least two participants from racial and ethnic minority groups, while the right-leaning focus groups were entirely White Britons. Participants were also asked to what degree they identified with their local area, their city, with England or Scotland (depending on the location of the group), with the UK, as European, or as a global citizen, each using a 10-point scale, where 0 meant not at all and 10 meant very strong identification. People were then recruited into different groups based on whether they gave higher values to being European or a global citizen or whether they gave higher values to local areas or their country. Other variables The variables we considered when putting these groups together are not the only ones that affect attitudes toward globalization and national identity. Things like education, gender, employment and age are also prisms through which views about these issues can be forged, and while these were not necessarily the primary variables we considered when putting our groups together, we did consider them. Some of the groups had specific education cut-offs. In the U.S., no one in the White rural Republican groups attended college, while all other groups were comprised of a mix of educational attainment. And in the UK, different groups in different cities had different educational cut-offs. For example, the “remain”-voting groups in London and Newcastle all had an undergraduate degree or less education, while the participants in the “remain”-voting groups in Birmingham all completed undergraduate degrees or pursued graduate degrees. We took a similar approach for the “leave”-voting groups, with those in London and Newcastle having undergraduate degrees or more education and those in Birmingham having undergraduate degrees or less education. And in Scotland, the people we spoke with who supported Scottish independence had less education, while those who favored staying in the UK had more. We did this to examine how education might affect attitudes within each type of group. With regard to gender and age, we made sure that all groups were roughly balanced, with about half of participants men and half women (there were no gender nonbinary or non-conforming individuals in our sample), and that all groups had a mix of adults ages 18 and older. Developing a discussion guide Our discussion guide — essentially a list of topics we planned to cover — included questions that aimed to explore our research interests and prompt conversations that examined the impacts of globalization and national identity at the local, national and international levels. Since our screening questions already identified whether people were more nationally or internationally oriented, we included questions designed to uncover why people felt as they did, the crux of qualitative research. We wanted to see how they described and reacted to change affecting their community, their country and the world. Asking about the local effects of globalization At the local level, we asked the following questions: • Describe where you live.* • How has your neighborhood changed?* • Is your neighborhood closely knit?* • Are there opportunities for people where you live?* • How would you describe the kind of place where you’d like to live?* Asking about the national effects of globalization At the national level, we asked the following questions (not all questions included in the focus group moderator guides were analyzed in the final report, but analyses of some of these data may be released at a later date): • What does it mean to be American/British?* • What things make you proud to be American/British?* • What makes you embarrassed or ashamed to be American/British?* • When was the U.S./Britain at its best?* • Can people from other cultures be American/British?* • Is what it means to be American/British the same today as it was in the past?* • To what extent to you feel at home in the U.S./Britain today?* • Where is a quintessentially American/British place?* • Who is a patriot/someone with a strong British identity?* • Who is a cosmopolitan?* • Is there common ground between patriots/people with strong British identities and cosmopolitans?* • What are the issues facing our country today?* Asking about the international effects of globalization At the international level, we asked the following questions: • What is globalization?* • Has our country benefited from globalization?* • Who or what has benefited in particular?* • What are the biggest problems with individual countries working together?* • Is our country open or closed to people from other countries?* • What would be the impact of our country having fewer connections with other countries?* Analyzing the data Once the focus groups wrapped up, we were left with 26 roughly 40-page transcripts, totaling over 1,000 pages of text. And even though our researchers had attended the focus groups, debriefed about them with moderators and had access to the transcripts, we sought to develop a method to identify key themes in a systematic way without needing to pore over more than a thousand pages. To do this, we used a multistep method of data reduction. First, a team of researchers entered short-form text from the transcripts into a spreadsheet, or data display. In this display, the discussion guide questions were columns and individual participants were rows. Researchers entered short-form text deemed relevant to each discussion guide question into a cell. This short-form text was either a direct quote or paraphrased statement that we considered to directly answer the questions the participant was responding to, related to the key themes we initially set out to study or related to new themes we identified. If a respondent did not discuss the topic, we left the cell blank. We created a separate column of “coder’s notes” for researchers to note any patterns or themes within the group. We also bolded and color-coded paraphrased text or direct quotes that captured the themes that occurred across multiple questions. Quality control We conducted a number of quality control checks during this data entry phase. To ensure the essence of each data reduction spreadsheet captured the same general information, each researcher initially coded the same transcript. We noted and discussed any discrepancies (such as omitted key quotations, missing responses by participants, insufficiently condensed text, or significant paraphrases or direct quotations that were not bolded and/or color-coded). After coding the same transcripts, each researcher then proceeded to enter data onto a separate spreadsheet for each group on their own. Each researcher had at least two of their displays checked for errors by another researcher. No researcher entered data for more than eight transcripts. After we entered all the transcript data, we combined the spreadsheets for each group into one “master” display for each country. At this stage, we completed an additional quality control check to ensure that the data in each cell properly aligned with others (i.e., that each column contained only data relevant to the discussion guide question “variable” header). Creating ‘toplines’ After the quality assurance measures had been finalized, a team of three researchers further reduced the data displays into outlines or summaries, akin to the “toplines” we use to summarize quantitative data. Each researcher was assigned a section of the focus group instrument — either local, national or global — and outlined the key themes identified under each question in that section. They also noted any differences in the themes that came up between groups or between certain demographics within groups (e.g., groups consisting of people who voted to leave the EU were less likely to see multiculturalism in a positive light). These toplines were also careful to note when certain questions were not asked in all groups to indicate that there are limitations to the findings from these questions. Key takeaways We believe this qualitative research project was effective at exploring and understanding some of the key frames of reference people employ when answering questions about globalization, international engagement and national identity. Over the course of 26 focus groups, our guided discussions gradually achieved a high level of “saturation,” leading us to feel confident with the depth and breadth of the study’s findings. These findings, in turn, suggest new ways for us to ask about and engage survey respondents on topics related to the experience and consequences of increasing global interconnectedness. For example, we learned a great deal about how ideas of political correctness, national history and law and order factor into how people conceptualize their national identities. Without conducting these groups, we may not have known to ask questions on these topics when exploring people’s views of globalization. But in addition to helping us better conceptualize how to ask questions about the topics we explored, we also learned about why people felt certain ways — adding a richness to our bread-and-butter quantitative work. However, we also recognize the challenges of qualitative research. The process of developing this project, collecting the data, cleaning the data and conducting the analysis was extremely time consuming for our team. While we can typically write a survey and identify a sample, conduct fieldwork, and analyze our data within the span of less than a year (and sometimes, within less than six months), this project was over a year in the making. In addition, when we conduct a survey, we have processes in place to manage its progress, from questionnaire development to reporting. But for this project, we were creating new systems. If we pursue this kind of project again, we now have a stronger foundation to build on, in addition to some new strategies we explored, such as using a QDAS, that may help truncate our timeline. Beyond the time costs, there are monetary costs to doing enough focus groups to feel like one has reached “saturation” and heard about perspectives from enough different types of people, especially if one is interested in cross-national, comparative work. Despite these challenges, the research team is excited about the findings from this project — both on their own and in terms of how they can inform our future survey work — and we believe they were valuable enough to justify the costs. Taking Action on Attention I Author: IAS, , Date: APRIL 2023 Collections: NeuroPsychoLinguisticPolitics, ElectionPredFeats, MediaAdsPolit Zotero Key: 9ZYY2IMD Cite Key: IAS23takingActionAttentionVI Zotero Item | Lit Note Taking Action on Attention TABLE OF CONTENTS EXECUTIVE SUMMARY Attention is one of the most complex and widely discussed topics in digital media. Defining it has proven to be difficult, with marketers across the industry often divided on how to characterize and fully harness attention. But industry experts unanimously agree that harnessing attention is pivotal to executing a successful advertising campaign. Between content-oversaturation, cookie deprecation, and other challenges across digital media, Integral Ad Science (IAS) saw a need to develop a methodological point-of-view that combines the deeply human aspect of attention with evidence-based media signals. In early 2022, IAS’s media research team set out to test the hypothesis: Does media quality lead to greater attention? We began testing if core media quality metrics, like viewability and brand safety, lead to higher consumer attention — and we found that to be true. The team followed this finding by testing if contextual relevance has an impact on attention. Once that was proven to be accurate, our research team leveraged eye-tracking technology to understand how consumers interact with ads with varying degrees of media quality optimizations. While eye-tracking data verified that core quality metrics are instrumental in predicting results, holistic insights related to ad density, share of voice, context, and more can show marketers the full picture of media quality needed to maximize attention. We further proved this model utilizing our industry-first Total Visibility™ product, which provides us with actionable advertiser performance data for customers who want greater transparency into their programmatic buys. Using that data, we were able to find patterns, like which types of impressions are likely to lead to a conversion and which are not. These patterns led us to create a differentiated Attention model that can predict the propensity of whether or not an impression will lead to a result. Throughout this report, we detail five studies that propagate our definition of Attention and its foundation in business results. With research based on millions of data signals combined with Total Visibility™ and machine-learning models, our Attention model is based on three key signals that predict the likelihood of an impression leading to a result: Visibility, Situation, and Interaction. DEFINING ATTENTION Everyone is talking about attention. While it may seem like a rehashing of engagement, industry research has shown that attention is a much more nuanced and elaborate concept. For example, Google’s attention study concluded that the first five seconds of an ad on YouTube are critical. TikTok revealed insights about the importance of creative triggers and shorter content lengths. And the Project Trinity study released by Havas and Teads provided observations on the links between attention and time-in-view and scroll speeds. The nuanced nature of these findings add to the challenge of establishing a single, unified thesis to define attention and the metrics that should be used to measure it. Through our findings, which included millions of advertising data signals refined by machine-learning models, we concluded that business results, like conversions or sales, must be present in order to determine that attention has occurred. Furthermore, marketers must take a holistic media quality approach in order to successfully harness attention to reach superior results. Comprehensive media quality measurement and optimization solutions that leverage Visibility, Situation, and Interaction signals are core to driving attention. WHY ATTENTION? At this time, there’s no singular metric that can accurately and effectively monitor attention. However, as we look to the future of digital advertising, the need for a set of metrics to capture the entire picture of attention is crucial, as evidenced by key challenges that marketers consistently face. Cookie Deprecation Third-party cookies will soon become obsolete. As browser cookies, along with MAIDs (mobile device IDs), continue to deprecate as a result of industry changes, advertisers, platforms, and publishers are looking to alternative metrics that can act as proxy performance indicators. Content Over-Saturation There are more brand and product choices for consumers now than ever before. This fragmentation, while good for the industry as a whole, can make it more difficult for brands to breakthrough with their messages. Cross-Vertical Competition For most brands across many verticals, there’s a lot of competition. The proliferation of brands across verticals increases the likelihood of message competition among consumers. For example, a CPG brand doesn’t just have a handful of other brands competing with the same products — they also have store brands to contend with. Creative & Copy Advancements Advancements in digital ad units allow brands to be more creative with their ad copy and design than ever before. A/B testing is an option, but the high cost of this strategy can be a deterrent for marketers, making it challenging to know that messages and creative are breaking through the noise. THE THREE ATTENTION SIGNALS Attention can be achieved by a combination of media signals that, when used together, improve campaign effectiveness. Through a holistic media quality approach, marketers can drive results by simultaneously understanding and optimizing toward consumer attention. Our research team has analyzed millions of data signals to determine that Attention is a function of three key signals — Visibility, Situation, and Interaction. The combination of these signals can predict if an impression is likely to lead to a business result. Our analysis included utilizing our Total Visibility™ data and machine-learning models to tie business results to Attention signals. Visibility Visibility signals measure the validity of the impressions being served. These are baseline measures advertisers should optimize first. Without Visibility measures, Attention cannot occur. Visibility Metrics: • Viewability • Time-in-view • Full screen Situation This set of signals describes the environment in which impressions are served. Situation signals measure the number of ads on a page, the percent share of voice (SOV) an ad has on a page relative to other content, and the relevance between the ad messaging and the surrounding content. Situation Metrics: • Ad density • Page orientation • Brand suitability • Contextual relevance Interaction Interaction signals are indicators of consumer activity in the presence of ads. These signals can help identify human preference, measuring activity like scrolling, interacting with ads, and where the consumer’s gaze and fixation is on the page. Interaction Metrics: • Scroll • Volume • Video player play/pause • Eye-tracking VISIBILITY Viewability | Time-in-view | Full screen Attention Signal #1 The major CPG brand tripled its return on ad spend (ROAS) thanks to in-view ad placements. |Col1|+180%|In-view ads tripled ROAS compared to not-in-view placements, with a 180% increase in incremental ROAS| |---|---|---| |Lift in conversion rate for viewable impressions compared to not viewable impressions|+95%|Col3| |---|---|---| VISIBILITY Viewability | Time-in-view | Full screen Attention Signal #1 There’s a sweet spot for ideal time-in-view to drive incremental sales. 3-10s Ideal time-in-view for driving incremental sales and ROI 118 108 72 0-2 seconds 3-10 seconds 11+ seconds This study provided remarkable findings for marketers looking to drive both incremental sales and ROI. While ads that remained in-view for more than 11 seconds drove the highest percentage of incremental sales, it was the ads in-view between three and 10 seconds that hit the sweet spot: they were highest on the incremental index — driving the most ROI for the CPG brand compared to both shorter and longer time-in-view ranges. These findings suggest that marketers can use an ideal time-in-view range to gain attention and provide the most opportunity to achieve both incremental sales and higher ROI. SITUATION Ad density | Page orientation | Brand suitability | Contextual relevance Attention Signal #2 The environment in which an ad’s impressions are served has an enormous impact on ad performance and consumer receptivity. Consumers seek out content that aligns with their interests and preferences. The single best place for a brand to be is alongside content that matches their ideal consumer’s interests. A broad range of metrics including brand safety, contextual relevance, ad density and more have credible influence on how an ad performs. In the same study that linked media quality to attention and results, we wanted to discover how brand safe situations affect conversion rates on ads. Here’s what we found. Optimizing for brand safety leads to greater conversions… +233% Lift in conversion rate for brand safe impressions compared to those that were not brand safe Imps. not Brand Safe Imps. Brand Safe Our research shows that ad density has a compelling impact on results as well. Higher ad density can actually negatively affect the consumer experience and be the reason for fewer conversions for marketers. We found that low (i.e. below average) ad density on a given page points to both better conversion rates and lower costs per conversion. Low Density High Density Cost Per Conversion Low Density High Density SITUATION Memorability and purchase intent skyrocketed among consumers who viewed the ad in the contextually relevant setting, with increases in favorability too. consumers who viewed the ad in the contextually relevant favorability too. In-context ad Out-of-context ad 73% MEMORABILITY 18% MEMORABILITY MEMORABILITY PURCHASE INTENT FAVORABILITY favorability too. In-context ad Out-of-context ad 73% MEMORABILITY 18% MEMORABILITY INTERACTION After evaluating all IAS metrics to measure their impact on results from a multivariate model perspective, we found that interaction highly influences results — but the effectiveness differs between display and video. The effects on results differed between display and video beyond just Interaction signals. Our research found that nearly all three Attention groups — Visibility, Situation, and Interaction — positively influence results with very minimal negative correlations between these metrics and results. Most influential Most influential KEY FINDING display metrics video metrics IN-VIEW RATE SCROLL AD DENSITY FULL SCREEN AVERAGE TIME-IN-VIEW PLAYER ACTED ADS +71% +41% Relative influence Relative influence INTERACTION Scroll | Volume | Video player play/pause | Eye-tracking Attention Signal #3 Our eye-tracking study provided valuable insight into consumer interaction with ads in differing environments through the use of heat maps. These findings showed that consumers gravitate toward contextual relevance, with attention captured on the display page faster and for a longer amount of time. In-context Out-of-context In another analysis, we used eye-tracking[1] to dive deeper into the link between context and attention. We tested an ad running in one of the most-watched sporting events across the globe, capturing the attention of over a billion viewers. For this major sporting event, we wanted to discover how both contextually relevant content, like sports, and non-contextually aligned content, like politics, impact attention. 1 Eye-tracking measurement determined by Interaction partner data. TAKING ACTION ON ATTENTION NOW Advertiser checklist While tools for scaling attention are still in the process of being developed, we’ve created a working model that helps advertisers drive campaign performance with consumer attention at the forefront. Our findings show how advertisers can drive and capitalize on attention now. Here are IAS's recommendations. Don’t obsess over one metric alone. Use a personalized suite of metrics to ensure that your campaigns are viewable, brand suitable, blocking invalid traffic, and driving the highest quality impressions. As we saw earlier in this paper, viewability, time-in-view, and brand safety all correlate to drive higher attention. The situation that your ads 2 run in can be a critical driver for success. |Col1|57%|Col3|Increase in conversions for viewable & brand safe impressions compared to non-viewable & not safe| |---|---|---|---| |Col1|171%|Col3|Increase in conversions for impressions with time-in-view greater than 15 seconds| |---|---|---|---| |Col1|+233%|Col3|Lift in conversion rate for brand safe impressions compared to those that were not brand safe| |---|---|---|---| Take advantage of capabilities that help ensure your ads are running in safe, suitable, and relevant environments. |Col1|+25%|Col3|Increase in memorability for HP’s contextually matched ads compared to unmatched ads| |---|---|---|---| Optimize toward supply 3 where ad interaction and engagement are historically higher. PLAYER ACTED ADS Most Influential Video Metrics SCROLL FULL SCREEN The Interaction Attention signal is most influential when predicting conversion performance for video campaigns, specifically. +41% Relative influence TAKING ACTION ON ATTENTION NOW Start optimizing attention with a test-and-learn approach. Media platform 1 Media platform 2 Media platform 3 Media platform 4 Media platform 5 58.7 Advertisers can use Attention measurement to evaluate which programmatic partners are driving higher attention. Working to increase attention through the three signals in our model will have an impact on cost and reach, so use tests to inform your brand’s most effective strategy. Utilize tools that increase 6 transparency within the programmatic ecosystem so that attention improves with cost efficiency. For example, by integrating with IAS’s Total Visibility™ framework, advertisers can: • Identify and mitigate supply path and publisher inefficiencies • Pinpoint problem private marketplaces to make platforms aware of issues they can correct 56.9 53.4 52.8 48.5 Attention Score IAS’s Total Visibility™ saves advertisers an average of 24% of budget that would otherwise be lost in the supply chain. TAKING ACTION ON ATTENTION NOW Publisher & platform checklist Attention measurement and optimization doesn’t only benefit advertisers. The supply-side can experience massive benefits too, like new logo growth, client retention, and increased yield by enabling efficient ways for advertisers to capitalize on attention. Here are our recommendations for publishers and platforms. Test DSP partners for performance. 63.3 60.3 59.8 59.1 57 DSPs provide greater ease to advertisers looking for available pre-bid segments that can be used to optimize campaign impressions toward increased attention. In a study conducted with a major CPG brand, we found that attention can vary when comparing impressions across different tech platforms and across publishers. Automate impression 2 delivery and dynamically meet advertisers’ specific brand safety requirements. With the IAS Publisher Optimization solution, a major media publisher significantly reduced brand risk, a key metric within the Situation Attention signal. Enhance content curation and 3 optimize environments so that viewability rates improve. DSP 1 DSP 2 DSP 3 DSP 4 DSP 5 Month-over-month increase in mobile app viewability with the IAS Publisher Verification solution Direct to Pub 56.5 DSP 6 56.3 DSP 7 55.7 DSP 8 50.9 DSP 9 50.2 DSP 10 49.4 Attention Score Decrease in brand risk after activating the IAS Publisher Optimization solution MOVING THE INDUSTRY FORWARD WITH ACTIONABLE ATTENTION IAS believes in both measurement (post-bid) and optimization (pre-bid) solutions that will help our customers drive higher attention for their campaigns, and as a result, superior results. Attention today IAS currently provides a custom report in our Signal UI that features metrics which form the ingredients for Attention scoring. As marketers dip their feet into what attention means for them, they can use this Attention metrics report to understand how components of Visibility, Situation, and Interaction are at play across their programmatic media spend. For example, advertisers can build out tests to compare how viewability, time-in-view, or viewable video quartile rates independently influence their results. Experimenting with how these individual parts of Attention impact performance will better prepare brands to leverage IAS’s dedicated Attention scores when they become fully available. Marketers can also leverage IAS’s Custom Report Builder to pull individual metrics based on our Attention model to help inform their own campaign performance. The future of IAS’s Attention model IAS’s upcoming optimization tools will allow customers to boost attention and improve campaign effectiveness through actionable data. Using the IAS Attention model, our future pre-bid segments will allow advertisers to confidently optimize ad spend toward the results they deem most valuable. MOVING THE INDUSTRY FORWARD WITH ACTIONABLE ATTENTION The future of IAS’s Attention model We also plan to extend our Attention model beyond just the open web for consumers to gauge attention across digital channels. Attention can be directed wherever consumers are — so we’re developing pre-bid optimization tools to harness attention across CTV and, later, social. CONTRIBUTORS Khurrum Malik Chief Marketing Officer Khurrum Malik joined IAS as Chief Marketing Officer in 2022. He is a technology marketing leader with a broad range of experience driving growth for public and private companies. Most recently, Malik was Head of Global Business Marketing at Spotify, where he led business marketing, creative, product marketing, measurement, and analytics teams within Spotify's advertising business. Previously, he served as CMO of Compass, a real estate technology platform, and was Head of Product Marketing at Meta. Malik holds an M.B.A, M.S., and B.A. from the University of Virginia. Jeremy Kanterman Vice President, Research & Insights Jeremy Kanterman joined IAS in 2021, bringing 15 years of digital research leadership including eight years in research roles covering both linear TV and print. Most recently, he was the Senior Director of Ad Effectiveness Research at Yahoo where he helped clients measure the impact of their ad investments. Kanterman is a co-holder of two patents for inventions developed during his time at Yahoo and was a co-inventor of Yahoo's Multi-Touch Attribution solution. He holds an M.B.A from Baruch College’s Zicklin School of Business and B.A. from Syracuse University. Act on attention today. Contact IAS to get started. GET IN TOUCH ’Identity Politics’ Isn’t Why Harris Lost Author: Johnson, Matt, Date: 2024-11-08 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: LNGKERDM Cite Key: Johnson24identityPoliticsIsntWhy Zotero Item | Lit Note A Kamala Harris poster seen among delegates on day 4 of the Democratic National Convention at the United Center on August 22, 2024 in Chicago, Ill. (Photo by Melina Mara/The Washington Post via Getty Images) THERE’S BITTER DISAGREEMENT within the Democratic party and on the broader American left over Donald Trump’s shockingly decisive defeat of Kamala Harris. There are many explanations for Trump’s victory, and they tend to mirror the political prejudices of those who propose them: Harris didn’t appeal to the working class. She wasn’t progressive enough. She was too progressive. She didn’t distance herself from President Biden’s economy (which is quite good, despite the pervasive belief that it’s awful). She didn’t go on Joe Rogan’s podcast. There are good reasons for Democrats to be self-critical. Harris under-performed Biden among key demographics including black men, Hispanics, and young voters. Large swaths of the working class have migrated toward the GOP. Former swing states like Ohio and Florida are now dark red, and Trump won every battleground state. It’s clear that the Democratic party needs to rethink its coalition—old assumptions about demographic trends inevitably favoring the Democrats have proven to be unfounded, and Trump gained ground among nearly every category of voters. The role of identity in this election is a major focus for Democrats. Trump built a stronger multiracial coalition than many thought possible, challenging basic assumptions Democrats have made for decades. The boxes voters check on the census have less influence on their politics than Democrats thought. Democrats have long appealed (some would say “pandered”) to voters based on presumed identitarian grievances, but that strategy won’t work forever. And yet, some still criticize Harris for imagining she could run a campaign that didn’t appeal to identity politics. “Obama’s coalition feels like an aberration,” writes Tressie McMillan Cottom in the New York Times, “and it is time for us to accept that American politics is identity politics.” Despite all the available evidence, this argument assumes that the identitarian essentialism that voters increasingly reject will always be a powerful political force. Cottom continues: “Try as some might to bury the salience of race, class and gender to political life, identity still matters.” Other critics of Harris blame her defeat on too much identity politics, as if she was a raging progressive scold screaming “racist” and “misogynist” at every available opportunity. But she did the opposite. Although there’s some truth to the claim that the general perception of the Democratic party as too “woke” played a role in Trump’s re-election, this explanation has its own flaws. For one thing, Harris ran as a centrist and offered a universalist message that generally didn’t get bogged down in identitarianism. For another, Biden trounced Trump amid what has aptly been described as the “great awokening” in 2020. Identity politics, like so many other political divides in the United States, was subordinate to a much larger development. A cultural and political chasm has opened up between Americans who still care about the integrity of their democracy and those who do not. We have entered an age of political nihilism in which our traditional conceptions of ideological and identitarian conflict must be abandoned. This election was about the identity of the United States as a whole, and it didn’t tell an inspiring story. ONE OF HARRIS’S POLITICAL LIABILITIES was the impression that she’s an opportunist whose views are based on political expediency instead of core principles. During the 2020 Democratic primary, Harris joined many of her fellow candidates in embracing a long list of progressive causes. She supported Medicare for all. She described Trump’s effort to build a wall on the southern border as “un-American” and a “medieval vanity project.” She wanted to ban fracking. She expressed sympathy for the “defund the police” movement. But once she was at the top of the ticket, she reversed each of these positions. But voters didn’t penalize her for moving to the center—they penalized her for what they saw as inauthenticity. They didn’t trust that her centrism was genuine. Progressives like Vermont Sen. Bernie Sanders assume that working-class Americans—whether white, black, or Hispanic—have moved away from the Democratic party because it hasn’t made the tax system fairer or adopted the right industrial policies. But much of the growing contempt for the party among multiracial working class Americans is cultural. Many Hispanics are furious with Democrats for neglecting border security, and they’re well aware of Harris’s inconsistent record on immigration. Just 4 percent of Hispanics have used the progressive gender-neutral term “Latinx” to describe themselves, and 75 percent who’ve heard the term say it shouldn’t be used. There are also cultural fissures within the Democratic coalition on issues like gender identity—while 61 percent of Democrats say a person’s gender can be different from the sex assigned at birth, this proportion falls to 33 percent for black Americans. The Trump campaign spent tens of millions of dollars on ads attacking Harris on transgender issues, which would conclude with lines like “Kamala’s agenda is they/them, not you.” Perhaps the best-known of these ads was one which drew attention to Harris’s statement during the 2019 primary that she would support the use of taxpayer dollars to fund gender reassignment surgeries for immigrants in prison. It was easy to dismiss these ads as a strategic mistake—at a time when Americans are worried about the state of the economy and other “kitchen table” issues, did it really make sense to emphasize a seemingly marginal front in the culture wars? The answer is yes—each element of the transgender prisoner ad addressed one of Trump’s top issues: immigration, crime, and the vague sense among many Americans that the Democratic party’s “woke” fixations have little to do with their lives. This isn’t a fair characterization of the party, and the ad was a demagogic distraction (the Bureau of Prisons reports that there have been a grand total of two approved gender-related surgeries for prisoners). But the ad worked because it addressed a genuine sentiment among voters, including voters that the Democrats desperately needed to win over. In this sense, those who attribute Harris’s loss to identity politics are correct. Trump’s massive gains among Hispanics demonstrated that there is a new political reality in the United States. His performance among other major demographics like black men tells a related story—those who believe demography is destiny in American elections are mistaken. This fact undermines the racial essentialism from both the left and the right. Identitarians on the left insist that “people of color” are permanent victims of pervasive structural racism in the United States, but many don’t feel that way. Trump’s bigotry was on display every day, but diverse voters looked past it (or believed it didn’t apply to them) because they have other concerns. Meanwhile, many Trumpists argue that Democrats only want mass migration because Hispanics and other diverse voters allegedly vote the same way. Beyond the ugly conspiracism and prejudice behind this belief, its underlying assumption is becoming less credible every day. Share IN A RECENT INTERVEW on Fox News, Free Press editor Bari Weiss accused Harris of running a radical identitarian campaign that alienated most Americans: “It turns out that running on these extraordinarily niche issues like gender fluidity or defunding the police” made Harris seem “profoundly out of touch to ordinary Americans.” One of the main reasons Fareed Zakaria cited for Harris’s defeat is the “dominance of identity politics on the left.” Sanders says a key reason for Trump’s victory is that the Democratic party has become the “party of identity politics.” In a postmortem about the election, the political scientist Yascha Mounk argues that left-wing identitarianism was one of the most “consequential vulnerabilities of Kamala Harris’ campaign”: While running for the Democratic primaries in 2019, she wedded herself to a slew of identitarian positions that happened to be deeply unpopular. Sensing that the political winds had shifted, she did not reprise her flirtations with the idea of defunding the police or decriminalizing illegal border crossings. But neither did she have the courage to explicitly call out the ideological foundations for these deeply unpopular positions. . . . If it’s true that the “identitarian vision of the world” that has “gained tremendous influence” on the left was one of the Democrats’ biggest vulnerabilities in 2024, why wasn’t this vulnerability even more decisive in 2020? The George Floyd protests swept the country in the summer of 2020, just a few months before the election. This was a period when journalists were being fired for offenses like publishing editorials that claimed “Buildings Matter, Too” (a reference to riots that were taking place in many cities); monuments were being defaced and removed (some of which were racist, while others weren’t); and public shaming was rife. Trump exploited these developments. In 2019, the New York Times released its 1619 Project—which presented the story of the United States as a tale of little more than endless racial oppression. The creators of the project even argued that they aimed to reframe the nation’s “true founding” as the moment the first slaves landed in the country. It wasn’t long before Trump announced his 1776 Commission, which called for “patriotic education” and denounced “distorted histories.” He fused his attacks on left-wing identity politics with his law-and-order message. On July 4, 2020, he proposed that “people who damage or deface federal statues or monuments will get a minimum of 10 years in prison.” He decried a “radical ideology attacking our country . . . under the banner of social justice.” This year, Trump once again relied on anti-woke demagoguery to attack Harris, but he had much less to work with than he did in 2020. There were no images of Minneapolis burning. Black Lives Matter is long past its cultural apogee. The primary which suggested that wokeness had made significant inroads in the Democratic party was half a decade ago. While Harris had been swept along with the woke political current before her primary campaign ended in 2019, her 2024 campaign was focused on building the broadest tent possible. She preferred not to mention her race, and unlike Hillary Clinton, she didn’t emphasize that she would make history as the first female president. Harris delivered a universalist message that intentionally avoided identity politics. When CNN’s Dana Bash asked her about Trump’s accusation that she “happened to turn black” for political reasons, she responded: “Same old, tired playbook. Next question, please.” She was similarly dismissive of the idea that she was a “DEI” hire in the Biden administration. She didn’t just avoid rhetoric about defunding the police—she embraced her record as California attorney general and constantly presented herself as a tough-on-crime candidate: “I’m the only person on this stage,” she said during the only debate with Trump, “who has prosecuted transnational criminal organizations for the trafficking of guns, drugs, and human beings.” So Weiss had it exactly backward when she accused Harris of “running on” defunding the police. Let’s say Harris had taken Mounk’s advice and challenged the “ideological foundations for these deeply unpopular positions” like “defund the police” or decriminalized border crossings. This would likely have made the election an even bigger disaster for her. It would have reminded voters of the Democrats’ focus on those issues in 2019, and it would have forced Harris to relitigate an internal party feud that most Democrats have already moved past. The idea of calling out the “ideological foundations” behind the progressive turn in 2019 is even worse—Harris should not have wasted time educating voters about the evolution of postmodernism and intersectionality. She had about 100 days to build a campaign—there wasn’t time for an academic seminar about identity politics. Share THE ELECTION MAY HAVE TURNED OUT differently had Biden accepted his role as a transitional president earlier and given his party the time it needed to select the best possible candidate to defeat Trump. The Democrats could have chosen a candidate who didn’t share Harris’s baggage from the 2019 primary, and who could have made a cleaner break with the party’s brief but hard turn toward identitarianism. But this doesn’t change the fact that Harris did her best to run as a centrist. She didn’t lecture Americans about their bigotry. She deftly navigated Trump’s endless race-baiting. When Bret Baier tried to trick her into calling Trump supporters stupid during a Fox Newsinterview, she replied: “I would never say that about the American people.” But it wasn’t enough. Over the next several years, we’re going to hear all about how the Democratic party was punished for its condescension toward “ordinary Americans,” who were pushed toward Trump by the arrogance of “coastal elites,” wokeness, and so on. Journalists will once again parachute into small Midwestern towns and ask the inhabitants why Trump was more capable of speaking to their needs and concerns than Harris, but this time they won’t explore just the attitudes of non-college educated whites—they will include interviews with Hispanics and black Americans who decided that Trump deserved another four years as the most powerful man on earth. But the old narratives about the Democrats’ contempt for the working class and their racially divisive politics don’t apply to this election. The argument that Americans are in so much economic pain that they had to vote for Trump doesn’t hold up: Unemployment is just over 4 percent, the stock market was hitting all-time highs before Trump won, and inflation has plummeted from a peak of over 9 percent to 2.4 percent. The “soft landing” that many economists thought was impossible—in which inflation comes down precipitously without triggering a recession—is on the tarmac. American growth has sustainably outpaced other advanced democracies. The evidence for the strength of the Biden economy will be even clearer once Trump returns to office and immediately starts taking credit for it. Americans don’t care about any of this. They don’t care that the Biden administration handled a global economic crisis about as well as anyone could have hoped. They don’t care that Harris ran an inclusive campaign and refused to condemn Trump supporters. While Biden defeated Trump by over 7 million popular votes in 2020, Trump just became the first Republican to win the popular vote in two decades. His performance improved in all but one state, and he dominated among voters who were concerned about major issues like the economy and immigration. It’s possible Trump will have complete control of the United States government when he takes office in January—the GOP already took the Senate, and it has a good chance of retaining control of the House. A conservative Supreme Court recently ruled that the president is immune from prosecution for crimes that could be construed as official acts. The federal court cases against Trump are winding down. Americans have decided not to hold Trump accountable for attempting to steal an election, standing by while his supporters breached the Capitol and hunted down public officials, and filling our public discourse with despicable language about immigrants “poisoning the blood of our country” or the “vermin” who oppose him politically. They don’t care that he’s threatening to go after the “enemies from within” with the full power of the United States government—perhaps even the military. They don’t mind that he practices his own brand of grievance-soaked identity politics, which tells Americans that immigrants are killing them, stealing their jobs and votes, and destroying the country. It would be comforting if Harris lost the 2024 election because she ran a divisive campaign that pitted Americans of different backgrounds, races, and genders against each other. If that were the case, defeating Trumpism would be a simple matter of more strategic communications and the traditional job of coalition-building. But it isn’t the case. Americans didn’t elect Trump because they were offended by the Democrats—they elected Trump because they like him . They know everything they need to know about Trump. He has been broadcasting his cruelty, hatred, and self-obsession into every American home for nearly a decade. He has demanded the termination of the Constitution to remain in power after losing an election. He has promised to be a dictator on day one. He says his next term will be all about “retribution” against his fellow Americans. This election was about identity after all—an American identity which has become more tolerant of authoritarianism and less committed to democracy over the past decade. As Trump’s opponents try to figure out why they were defeated and how they can resist him over the next four years, they need to confront the dark reality about who we have become as a nation. Share Maine's Lessons in Ranked Choice Voting Author: Grimes, Shannon, Date: 2024-09-13T14:00:00+00:00 Collections: Voting Systems Zotero Key: NXHF44T5 Cite Key: Grimes24mainesLessonsRCV Zotero Item | Lit Note Author’s note***:*I first learned about ranked choice voting when I was living in Midcoast Maine in 2018. That initial glimpse into a different way of voting piqued my interest, and it’s how I became interested in researching election methods after I moved to the Pacific Northwest. Takeaways • If Oregon adopts ranked choice voting in November, the closest analog for its election method will be that of Maine, the first state to use ranked choice voting statewide.* • Maine now uses, and Oregon could adopt, ranked choice voting in partisan primaries as well as general elections.* • Mainers like ranked choice voting. They voted to affirm its use not once but twice, and polls confirm continued voter enthusiasm.* • Ranked choice voting ensures that the winning candidate has the broadest support, whether in the party primary or among the general electorate.* • Ranked choice voting has reinforced Maine’s pattern of electing centrist, independent-thinking political leaders.* • Even when it has not obviously changed an election’s winner, ranked choice voting has had subtle benefits in Maine, such as encouraging candidates to ally with each other and to reach out to each other’s supporters.* Find audio versions of Sightline articles on any of your favorite podcast platforms, including Spotify, YouTube, and Apple. Ranked choice voting will be on the ballot in Oregon this fall. And because the proposed measure won’t alter Oregon’s partisan primaries, it is not the same as Alaska’s much-discussed electoral system, which combines ranked choice general elections with unified all-party, top-four primaries. It’s also unlike proposals before voters this fall in Colorado, Idaho, Montana, and Nevada, which emulate Alaska’s system.1 Instead, Maine, the first state to adopt ranked choice voting statewide, offers Oregonians the closest example of what the changes might look like in practice. Like Oregon, Maine has mostly closed partisan primaries and kept them after implementing ranked choice voting. Cascadians, therefore, may want to know: How has ranked choice voting influenced elections in Maine? In short, Maine’s groundbreaking use of ranked choice voting showcases similar advantages to what we’ve seen elsewhere, before and since the Pine Tree State’s journey with the voting method began. Ranked choice voting is popular and well-liked, especially after people use it. Candidates sometimes campaign together or reach out to each other’s supporters. And because elected leaders must earn majority support, they have strong incentives to seek votes beyond their party base. Mainers express enthusiasm for ranked choice voting across multiple elections Mainers like ranked choice voting. They voted to affirm its use not once but twice, and polls confirm continued voter enthusiasm. Maine has used ranked choice voting statewide since 2018, and its largest city, Portland, has used it since 2011. The election of Paul LePage, Maine’s controversial former governor, was a major spur for reform. LePage served two terms but never won a majority of votes; in 2010, for example, he won with less than 38 percent of the vote. After LePage, many Mainers wanted a system that would never again elect outlier officeholders supported only by pluralities of voters. Still, winning reform was not easy. Voters in Maine adopted ranked choice voting in 2016 through a citizens’ petition and ballot measure. Ranked choice voting was set to start in the 2018 elections; however, in 2017, following inquiries from the Maine Senate, the state supreme court advised that ranked choice voting was unconstitutional in some general elections because the Maine constitution stipulates that state offices be won with a plurality (whoever receives the most votes).2 In response, the state legislature passed a bill to delay implementation of all uses of ranked choice voting until the constitution could be changed—a move many saw as repealing ranked choice voting in opposition to the people’s will. Volunteers collected signatures for a “people’s veto” of the legislature’s delay. With the law on hold until the veto measure was voted on, the courts directed the secretary of state to move forward in implementing ranked choice voting in the June 2018 primaries. In the same June election where they used ranked choice voting for the first time, Maine voters again voiced their support at the ballot for the voting method, passing the veto measure. Further controversy followed Maine’s use of ranked choice voting in the 2018 general election (more on this below) and again when the Maine Republican Party sought to reduce the use of the voting method after the state legislature expanded it to apply to presidential contests. But ranked choice voting remains in action. Because the constitutional quirk that requires plurality general election winners applies to state offices, Maine uses ranked choice voting in only the partisan primaries for governor, state senator, and state representative, and in both primaries and general elections for US senator, US representative, and now US president. Ranked ballots offer small but mighty improvements for voters Election reform has not dramatically shifted Maine’s politics. Maine’s election administrators have only had to look at second- and third-place rankings in a handful of races since implementing ranked choice voting; as in elections prior to the use of ranked choice voting, many elections in Maine are not highly competitive or only have two candidates. In more than three-quarters of the races with more than two candidates, the winning candidates received more than half of first-choice votes and officials did not need to examine the rankings in further rounds of counting.3 Yet the voting method continues to play an important role in the state—including setting the tone even when results weren’t closely contested. Maine’s first test of ranked choice voting showcases a positive campaign Even when it has not obviously changed an election’s winner, ranked choice voting has had subtle benefits in Maine, such as encouraging candidates to ally with each other and to reach out to each other’s supporters. This facet of the voting method was apparent from the get-go. The state’s first use of ranked choice voting came in the 2018 primary, where the parties used it to determine their nominees. Maine’s secretary of state had to tabulate the ranked ballots in just two races: the Democratic primary for the Second Congressional District and the Democratic primary for governor.4 In the US House primary, Democrat Jared Golden took a strong initial lead of first-choice votes among the pool of four candidates and then received more vote transfers than his main rival, Lucas St. Clair, from the two less-popular contenders. Final results indicated support from 54.3 percent of Democratic voters, showing Golden to have a solid backing from his party. In the gubernatorial nomination, four rounds of ranking thinned the field from seven candidates down to just Janet Mills. She led the vote totals in every round and eventually won with 54.1 percent of Democratic votes. Ranked choice voting didn’t necessarily shift the outcome of either of these elections (although we’ll never fully know; voters might have behaved differently with different ballots): the initial plurality leader was also the final winner. The method did clearly affect other aspects of the campaign, though. One form of influence was on campaign civility and outreach to voters. Two candidates for governor, Betsy Sweet and Mark Eves, recognized their shared values and cross-endorsed each other. After the campaign, Sweet (who finished third, thanks in part to transfers from most of Eves’s supporters) wrote about how she was never labeled a spoiler candidate or discounted by voters. She noted how all candidates “talked to as many people as we could, even voters whom we knew liked another candidate better.” Sweet and Eves later jointly highlighted how ranked choice voting increased voter engagement and encouraged candidates to focus on issues. St. Clair, Golden’s primary competitor, similarly explained that “[with ranked choice voting] I could run on the issues that mattered most to me and help create a more robust public debate around the issues I cared about.” Gubernatorial winner Mills agreed that the voting method changed campaigning dynamics, saying that “everybody’s campaign was better than it would have been without ranked choice voting. The people voted on this several times for good reasons. They expected and intended that the level of civility would rise with this tabulation [process], and I think it did so.” Ranked choice voting also ensures that winners have a strong base of support to continue into the general election campaign and then to govern. Mills’s closest competitor Adam Cote observed how the vote transfers solidified support for Mills: “Janet Mills won this race. She was strong everywhere across the state, as her vote totals show.” Voters seemed to appreciate that the rankings offered more choices. One voter saw, for example, how the voting method opened up possibilities to choose Sweet first and Mills as a backup, and felt more excited about the available choices. Administratively, the secretary of state’s office did not experience any of the feared implementation chaos spouted by opponents of the change. Secretary of State Matthew Dunlap expressed that “everything came together very nicely, but through a lot of hard work. We’re very pleased that this went so smoothly.” Pushback arises in the first general election Ranked choice voting is designed to consolidate support behind the candidate with the broadest support, including by counting second-choice votes. But the major-party candidate who didn’t win in Maine’s first election where second-choice votes played a pivotal role blamed the voting method. In the 2018 general election, 16 of the 17 races with more than two candidates were not closely competitive, and the winner received a majority of first-choice votes. One contest, though, required the secretary of state to dive into the details of voters’ rankings: the race for the Second Congressional District. Golden (who had made it through the ranked choice Democratic primary earlier in the year) faced then-sitting US Representative Bruce Poliquin (Republican) and two unaffiliated candidates, Tiffany Bond and Will Hoar. When the vote totals came in, Poliquin had 2,171 more first-choice votes than Golden (46.3 percent of the vote to Golden’s 45.6 percent). Bond received 5.7 percent of first-choice votes, and Hoar another 2.4 percent. After election officials transferred first-choice votes for Bond and Hoar to their voters’ second or third choices, Golden won with 50.6 percent.5 Maine allows batch elimination of candidates who cannot gain enough votes to win in later rounds. Thanks to Wayne Lei for supporting a sustainable Cascadia. Our work is made possible by the generosity of people like you. This “come-from-behind” victory is an intentional feature of ranked choice voting. The method is designed to consolidate support behind a majority winner based on voters’ backup choices. In this case, Golden received about two-thirds of the votes that transferred from Bond and Hoar. Like independent candidates in other elections, Bond expressed appreciation for ranked choice voting, saying that running as a third candidate is “no harm, no foul” because ranked choice voting “frees people up to make more interesting choices, to try something different.” Unfortunately, Poliquin didn’t recognize that those voters wanted to have a voice in the final election outcome, and he initiated a lawsuit challenging the use of the method. A federal judge denied Poliquin’s request for an injunction in December, and he later dropped the suit. The outcome in the Second Congressional District, however, solidified the Maine Republican Party’s opposition to the method, and the party has continued to attempt to roll back use of ranked choice voting. Ranked choice voting upholds Maine’s historical election trends That first year of ranked choice voting created the largest splash. In later elections, ranked choice voting upheld or confirmed support for initial winners. Maine still encountered some of the broader challenges US elections face today, and voters continued to elect centrist, independent, leaders as they had in the past. In 2020 nine races in the primaries and 10 in the general had more than two candidates and offered ranked ballots.6 In most of those elections, the winner earned a majority of first-choice votes. In the six races (all in the primaries) where election administrators needed to count second and subsequent rankings to determine a majority winner, the candidate who won the most first-choice votes won the final round as well; ranked choice voting simply consolidated support for the initial leader. Similar statistics follow for 2022: the secretary of state transferred second-choice votes in two of the nine elections with more than two candidates, and both resulted in the plurality winner winning a majority.7 Many news outlets and commentators expected the rankings to be tabulated in the 2020 US Senate race, which was hotly contested as it could have determined party control of the US Senate. Incumbent Republican Susan Collins faced Democrat Sara Gideon and two independents, Max Linn and Lisa Savage. But Collins won with 50.4 percent of first-choice votes and won the seat in the first round. Ranked choice voting still played a role in how the contest played out, such as how independent candidate Savage encouraged her supporters to rank Gideon second, wanting to consolidate support behind a more mainstream candidate with similar views if she couldn’t win. But ranked choice voting cannot solve all the problems with American elections; it’s no cure-all. With so much national attention on the race, media outlets largely presented the race as a contest between the Democrat and the Republican, stifling how voters might have viewed their options. Outside money poured in, paying for negative ads that barraged Mainers, unlike in many ranked choice races. The influx of cash may in some ways have hurt Democratic challenger Sara Gideon in the same way that polarizing candidates were punished in Alaska’s first ranked choice election: Democrats focused their energy on national implications, whereas Susan Collins emphasized local issues more important to the Mainers voting. Senator Collins also has a reputation as a moderate, bipartisan lawmaker—the type of candidate that Mainers have continued to elect over the years and that ranked choice voting often bolsters. Later analysis showed that many otherwise Democratic voters split their tickets to vote for Collins, appreciating her local issue-based messages and history of working across the aisle. A similar scenario had happened in Maine’s 2018 Senate race, where voters also received a ranked ballot but administrators didn’t need to look into second- and third-choice votes to determine a majority winner. In that election, Independent incumbent Angus King earned 53.3 percent of first-choice votes against competitors Republican Eric Brakey and Democrat Zak Ringelstein and won the seat. Both Senators exemplify Maine’s long history of centrist, independent political leaders, a tradition that continued with the revised ballots. Indeed, ranked choice voting supports more of these types of candidates: Golden, the Democrat who won his first election only after second-choice votes were counted, is also a centrist candidate who garnered votes from ticket-splitting Republicans to win his next election—in the same district where the majority of voters chose Donald Trump for president.8 And while ranked choice voting may not have visibly changed the outcome of those elections (although again, it may have affected voters in some unobvious way), it may influence how the leaders govern. Along with Senators Mitt Romney and Lisa Murkowski, Senator Collins was one of three Republicans to vote with Democrats to confirm Judge Ketanji Brown Jackson on the Supreme Court. Collins does not have the protection of an all-party top-four primary (followed by a ranked choice voting general election) that Murkowski does in Alaska, but she is safe from a far-right Republican challenger who might otherwise win with a plurality of party votes in a Republican-only primary. Maine’s lesson for Oregon? A modest but important upgrade Maine’s experience with ranked choice voting demonstrates the modest benefits the voting method offers. Winners needed second- and third-place rankings to reach a majority in only 11 contests, and just one winning candidate had fewer first-choice votes than his competitors, so it hasn’t completely upended the state’s politics. Mainers continue to elect centrist, independent leaders, and ranked choice voting offers even more freedom for voters to choose preferred people over parties. Even when first-choice rankings alone have decided the winner, the introduction of ranked ballots in Maine has created important assurances for voters. Ranked choice voting ensures that the winning candidate really was desired by more voters—in contrast to past elections like that of Paul LePage.9 Voters have seen how some candidates partner together and how parties can fully align around a preferred nominee. After confirming their desire to use ranked choice voting in two different elections, Mainers have continued to express enthusiasm for the voting method. Polling in 2018 found strong support for expanding ranked choice voting and a large majority of voters who found it easy to use. In 2022 an overwhelming 82 percent of Second Congressional District voters reported finding it “easy” or “very easy.” Most Mainers take advantage of the rankings when offered. Future elections will use ranked choices, including the 2024 presidential race, where votes for third-party candidates may well influence the distribution of Maine’s electoral votes, particularly since Maine is one of two states to split electoral votes.10 As ranked choice voting continues in Maine, Oregon and other states may well look to the northern corner of the country on the other coast for guidance in how this electoral method can upgrade elections. Trump's Victory Should Teach Democrats How Many People Actually Author: Cannon, Carl, Date: 2024-12-05 Collections: Hot Takes US Elect 2024 Zotero Key: YPV2TWPL Cite Key: RealClearPolitics24TrumpsVictoryShould Zotero Item | Lit Note 00:03 welcome back to the show for our last 00:04 segment here today I'm talking with Jack 00:08 Hamilton better known as professor John 00:10 Maxwell Hamilton he's an esteemed 00:13 journalism professor at Louisiana State 00:15 University Carl Canon Washington bureau 00:17 chief real cler politics you're 00:18 listening to RCP radio on sirusxm or on 00:21 our 00:22 podcast which we post on our website 00:24 every day Jack thanks for joining me 00:26 glad to be here Carl now you had a piece 00:29 uh that we ran on the front page on uh 00:32 Wednesday and it it's it was it's a 00:35 different kind of political commentary 00:37 you're you're writing as a as a Democrat 00:40 but as a person who sees both sides 00:43 because you're trained in the world's 00:45 greatest profession journalism and um 00:48 but you H you have a cautionary you have 00:50 a caution for both sides for both the 00:52 mega Republicans who are feeling um you 00:55 know triumphant and for Democrats who 00:57 are you know their reaction 01:00 vary but tell our tell our viewers and 01:03 listeners what you're what you're trying 01:05 to tell each side to 01:06 do well I think there are really two 01:08 messages uh one is that uh the idea of 01:12 Reform can be good and uh I think we 01:15 need to embrace that 01:17 concept there's a difference between 01:19 reform and Revolution and I have a 01:21 concern about a revolution and what the 01:24 consequences of that will be second 01:26 thing is I think we need to and I write 01:28 about this often in my columns I think 01:30 we need to try to always go back to 01:32 principles and principles 01:35 means wanting to have wanting to make 01:37 the system work better and strengthening 01:39 it rather than finding ways to tear it 01:41 down and also to 01:45 respect uh the truth and fairness rather 01:48 than your own side so as an 01:51 example uh President Biden pardons his 01:55 son which I find 01:57 appalling it it it 02:01 seriously bothers 02:02 me uh one of the arguments I hear 02:05 Democrats 02:06 make is and I you know they genuinely 02:10 feel this way is well Trump has very 02:12 porous relationships between being 02:14 president and his business interests 02:15 he's pardoned his own family members and 02:18 so 02:19 forth uh okay so he has but we can't 02:22 have a spiral going down and down so 02:24 that every time one side does something 02:25 the other side tries to match it why 02:28 imitate why imitate the things you don't 02:30 like why not rise above that and being a 02:33 liberal and I'm pretty much a Centrist 02:35 Democrat on most issues and sometimes 02:37 slightly conservative but but being a 02:39 liberal I expect the people on that I 02:42 identify with to take the lead in doing 02:44 that and I would hope both sides would 02:47 do that and so that's my concern Jack 02:49 let me ask you a question because I've 02:51 noticed this since Donald Trump came on 02:52 the scene in 2015 and you know that 02:54 that's been a while now we're we're 02:57 we're we're go nine years into this guy 03:01 and and I I think I I would tell you I 03:04 think it's the 03:05 strangest most unexpected thing I've 03:08 encountered in my political career which 03:09 is that the people who profess to 03:11 dislike Trump the most who really hate 03:13 him who think he's a menace and a danger 03:15 and they're the people who act the most 03:17 like him they're willing to bend these 03:19 Norms they're willing to exaggerate he 03:21 lies they lie he says something stupid 03:24 they mischaracterize it on purpose to 03:26 make it sound even stupider I it's I 03:29 don't what what drives people to to to 03:33 not to not do what you're saying not be 03:36 a little more philosophical about what 03:37 they're doing and trying to and and 03:39 adhere to their own 03:40 principles so you know especially in the 03:43 last couple of weeks since the election 03:46 and also reading comments when I write 03:49 columns for you which I I enjoy doing in 03:51 in large part 03:53 because uh you you put all sides on it 03:56 and for me that's a point of saying to 03:58 my liberal friends hey look I write 03:59 places that are try to be have multiple 04:02 points of view and I respect that and 04:05 and I'm I'm beginning to change my mind 04:08 on some things in ways that that I that 04:10 I'm not happy about and one is I think 04:12 the number of people out there not all 04:14 but the large number of people out there 04:15 who actually think that chaos is better 04:18 than what we have now and I really 04:21 believe that now be having been a combat 04:24 marine and having been a foreign 04:26 correspondent in countries where I saw 04:28 what wars look like 04:30 I I'm appalled to think anyone would 04:32 want that to see to see places in utter 04:36 chaos and and their own families 04:38 subjected to these kinds of problems but 04:41 there's two things about it and history 04:43 tells us something about this the first 04:45 is when people feel the this way so 04:49 strongly that they don't want to see the 04:51 other 04:52 side it's a sign of how desperate they 04:55 are that in other words they feel they 04:57 feel that this system has has nothing in 05:00 it for them and that's something the 05:02 Democrats have to pay attention to you 05:05 can't just write people off because they 05:07 they are that way I mean you have to 05:09 find out why why they 05:11 actually don't 05:13 care and why they why they feel so 05:16 strongly the second thing that that I 05:18 that I remembered thought about is you 05:21 know revolutions are always led by 05:23 Elites and that's an interesting Paradox 05:26 with the Trump administration because or 05:29 the Trump followers because actually if 05:31 you match up leaders in the Trump 05:34 Administration with previous 05:36 administrations they're more Elite than 05:38 some of the people who have been who 05:39 were in The Democratic cabinet and so 05:41 forth I mean they all went to Elite 05:42 colleges they may now wear leather 05:45 jackets and kind of look slovenly and 05:47 walk around as if they look like they 05:48 weren't at Harvard but but most of them 05:51 went there and so there's that's 05:53 something I think the Trump followers 05:55 really need to think about why is it 05:56 that Elites are asking to tear down A 05:59 system that they benefited from and made 06:02 and made fortunes from or fame and I 06:05 don't really have an answer to that 06:07 question but I think it's really worth 06:08 pondering well I agree with you and and 06:11 but but if Elites are starting 06:13 revolutions and you know JD Vance went 06:15 to y Law School right and TR Donald 06:18 Trump himself went to Wharton School 06:19 business so these are not these are not 06:21 you know the 06:23 proletariat uh you know from E from 06:27 Siberia try to kill the are but but 06:31 there is something about the people who 06:32 follow them uh Jack you know the The 06:35 Washington Post had a very interesting 06:37 story about a year ago and they looked 06:39 into the background of 06:42 the J January 6th protesters and 06:45 demonstrators the riers people who 06:46 invaded the capital and they did a 06:49 cross-section of it I think they looked 06:50 at the people who had been arrested and 06:52 a stunning percentage of that between 25 06:54 and 50% had had financial hardship in 06:58 their lives that they risk losing their 07:01 home that that kind of and those people 07:05 feel that the system isn't working for 07:07 them so yeah Trump and JD Vance may have 07:10 gone to Ivy League schools but you got 07:12 this movement of people workingclass 07:14 people who think the system isn't 07:15 working anymore and I I just I guess my 07:18 question is so the Washington Post did 07:20 that story they' to be credited for it 07:21 but then they kind of forgot about it 07:23 their coverage doesn't reflect that and 07:25 I don't mean to single out that paper 07:27 because I I like that paper and read it 07:28 every day but but the political coverage 07:31 tends to you know treat Trump's 07:34 supporters uh like there's something 07:36 wrong with them and the point you make 07:38 in your column is well there may be 07:39 something wrong with them but but 07:42 revolutions you quote Mark Twain a 07:45 revolution exists because people are 07:47 they're not getting something that they 07:48 need out of government that's right yeah 07:53 I I think there's this 07:54 problem that it that appears in many 07:57 places that is 08:01 that if you're not in favor of 08:04 trump you be you begin to think that 08:06 every one of the people who support him 08:09 there has to be something wrong with 08:10 them and that's not even statistically 08:12 possible I mean are you saying half the 08:15 country is is crazy I mean half the 08:17 country is just morally a deficit that's 08:19 not true there are lots of very very 08:21 very good people who support Trump so 08:24 then why why is that so here's an 08:27 example this is you know you there you 08:30 know this thing in being a foreign 08:31 correspondent you know if you talk to 08:32 the cab driver that's not a good day way 08:34 to be a reporter like on the way in from 08:35 the airport talk to the cab driver then 08:36 write a story but sometimes individual 08:39 episodes tell you something and so a 08:41 woman I know who's who um uh is a 08:45 housekeeper who is Hispanic and has her 08:48 papers and all the rest she said many in 08:50 her family supported 08:52 Trump now for for people who are you 08:56 know have maybe been somewhat more 08:58 charitable to toward immigration and so 09:00 forth you can say wait a minute that 09:01 doesn't make any sense because I've been 09:03 supporting those people I'm all in favor 09:04 of immigration but why do they feel that 09:07 way well they feel that way because they 09:09 think that immig immigrants illegal 09:12 immigrants have been treated too softly 09:14 and they had to work their way up 09:15 through the system and so they resent it 09:17 well that's a much more complicated view 09:20 of what's going on than merely writing 09:22 people off as being ignoramuses and so I 09:25 don't do that I do think that people who 09:27 who you know say boldly 09:30 that they think chaos is great are I 09:33 feel very sorry for them and for my 09:34 family and for all of us who will have 09:36 to suffer if that really came to pass 09:38 but it's just not right to take take to 09:41 oversimplify and reduce everything to 09:45 some kind of caricature and I think the 09:48 Press is so concerned about things that 09:50 Trump does that are I believe inherently 09:52 undemocratic and and find very troubling 09:56 that it's hard for them to get past that 09:59 well that that's right but but the party 10:01 that the party that run on preserving 10:03 democracy probably shouldn't have tried 10:05 to kick Trump off the ballot probably 10:07 shouldn't have turned a single 10:08 misdemeanor in New York into 34 felonies 10:11 absolutely right shouldn't have done all 10:13 these should maybe not not not pardon 10:16 your own son when you said very many 10:18 times you wouldn't it yeah I look I'm 10:21 like you and I sometimes people really 10:23 read me carefully say I have a double 10:24 standard that I hold liberal Democrats 10:26 to a higher standard I I may I grew up 10:29 in a house like that you know those are 10:31 my people but if you're going to invoke 10:33 these high-minded principles you know 10:36 any journalist should hold you to them 10:38 um you're listening to John Maxwell 10:41 Hamilton professor of Journalism at LSU 10:43 I'm Carl Kon Washington beer Chief got 10:45 about a minute left Jack I I want to ask 10:47 you um one final question though this 10:51 talk about this loose talk of Revolution 10:53 tearing it down you know blowing it up 10:55 you what what cash Patel said about the 10:58 FBI building and stuff 11:00 you know when commentators say that or 11:02 demonstrators it's one thing when you 11:04 have people who are going to come in and 11:06 run the government saying it I I'm with 11:08 you and that I it's a little unsettling 11:10 and you wonder how much of this was just 11:13 rhetoric to try and win an election and 11:15 how much they really want 11:17 to have fundamental change and you know 11:21 look they won the election but they're 11:23 going to have they're going to have a 11:23 two seat majority in the house they won 11:26 they're going to win the popular vote 11:27 for of plurality not a majority it was 11:29 very close election and it does it seem 11:32 to you that they're already even before 11:33 the inauguration over interpreting their 11:38 mandate I you 11:41 know I'm not sure what their mandate is 11:43 I mean is the Mandate really for 11:44 revolution I mean like what percentage 11:46 of people really feel that way there are 11:48 a lot of people who say incredible 11:50 numbers of people who say that they 11:52 think violence is okay when your cause 11:55 is right and that is that Revolution I 11:57 mean what it's still violence 12:00 uh so I don't know about what I don't 12:02 know what the Mandate is and I don't 12:04 know how many people I sometimes wonder 12:06 I I find hard to 12:08 believe I I actually just I I can't 12:11 believe when sometimes you can look at 12:12 something like January 6 and make 12:14 statements that it was just a Sunday 12:16 picnic I mean I I I can I cannot get my 12:19 mind around that but I think things can 12:22 fall apart and I and I think the idea 12:24 that somehow because we are the country 12:26 we are it always works out I I kind of 12:28 look at it like you know the Romans are 12:29 sitting around and they go somebody say 12:30 Hey you know these people are invading 12:32 us they go no it's not going to happen 12:33 it's not going to happen but then one 12:34 day guess what the Huns arrive and so 12:37 someday it happens and um I I don't mean 12:41 to say that in such a jular way actually 12:43 you know Sinclair Lewis wrote a book 12:45 that is often mentioned these days it's 12:47 not a very good 12:49 novel in terms of structure and so forth 12:51 but it is actually it was written out of 12:54 fear about H we long and what would 12:55 happen in the 1930s and also what they 12:57 were seeing with was selenia Hitler and 12:59 so this was a reaction to that and his 13:01 wife St Clair Lou's wife is a very 13:03 famous foreign correspondent who had one 13:04 of the first interviews with Hitler and 13:06 so you know he was really feeling this 13:10 and I read that book recently and it is 13:12 very scary because show name of the 13:15 novel it it can't happen here that's 13:17 right and it's it's a very interesting 13:20 book because you can see how this can 13:24 work how one day somebody becomes Gets a 13:28 Job who has not had a good job or has 13:30 not had and now all of a sudden that 13:32 person's in charge and then that person 13:34 starts overreaching and then does 13:35 terrible things to people but then that 13:37 person had something terrible done to 13:39 them because this kind of Revolution 13:41 just gets off the rail and eats it eats 13:42 its own and I just I just wish people 13:46 who were who who thought about violence 13:48 this way really understood what it means 13:52 for them and their 13:53 families that's the part that I think is 13:55 so important and I agree with you you've 13:58 been listening to Jack Hamilton 14:00 Professor journalism at LSU and one of 14:02 the reasons I'm optimistic optimistic 14:04 about the future is that we still have 14:06 Educators like Dr Hamilton um that's 14:10 going to do it for us today we'll be 14:11 back tomorrow on siries XM CH bhus 14:14 channel24 I'm Carl Canon washingon 14:16 bureau chief of real clear politics have 14:18 a good evening and we'll see you 14:20 tomorrow 14:24 [Music] 14:25 [Applause] 14:27 [Music] Who Do Trump Voters Blame For America's Problems? It's Not Who You Think Author: John Russell, Date: 2024-10-09 Collections: Hot Takes US Elect 2024 Zotero Key: FWNBPQS4 Cite Key: Russel24whoTrumpVotersBlame Zotero Item | Lit Note 00:00 You guys wanna vote? 00:01 I’ll vote! 00:02 You’ll vote? 00:03 [John Russell] Who should we blame 00:04 for our biggest problems? 00:07 That question 00:07 is at the heart of politics, 00:09 and it's what brought me 00:10 to the Allegan County fair, 00:12 to see a musician 00:13 who went viral for blaming 00:15 a very powerful group of people. 00:18 These rich men north of Richmond, 00:21 lord knows they all just wanna have total control. 00:26 [Oliver Anthony] We recorded it on a Saturday. 00:29 I think he uploaded it on Tuesday. 00:31 And by Thursday, man, 00:33 we were on a roller coaster ride. 00:35 [News] The year’s least likely number one single. 00:37 The message that's resonating 00:38 with millions of Americans right now. 00:40 [Joe Rogan] I've seen songs go viral, 00:42 but that's pretty bananas, son. 00:44 Your sht, 00:44 your sht went to the moon. 00:46 [John] Rich Men North of Richmond 00:48 isn't your average country song. 00:49 It's not about trucks or cold beer. 00:52 It's about rich men 00:54 who control our lives. 00:55 And that message resonated 00:57 across the political spectrum. 00:59 [Youtuber] I mean, he was speaking for the masses. 01:02 This applies to everybody 01:04 that is not in a position of power, 01:06 and that is 99% of us. 01:08 He spoke facts on that thing, too. 01:10 He said the rich men trying to take control. 01:12 The rich men north of Richmond. 01:14 [John] But from the moment it took off, 01:16 the song’s meaning was also hotly debated. 01:18 [Debate moderator] Why is this song 01:21 striking such a nerve 01:22 in this country right now? 01:24 Our country is in decline. 01:26 We need to send Joe Biden back to his basement 01:29 and reverse American decline. 01:34 It was funny kind of seeing the response to it, 01:36 like, that song has nothing to do with Joe Biden, 01:39 you know? 01:41 It’s a lot bigger than Joe Biden. 01:44 [John] So what is the song actually about? 01:46 Why did it resonate so widely? 01:49 And who do people blame 01:51 for their biggest problems? 01:54 The answer to those questions 01:55 tell a bigger story about what's 01:57 really going on in our politics 02:01 and why we're so damn divided. 02:03 We have way more in common 02:04 than we'd like to believe. 02:05 And we have a common enemy. 02:07 We do. 02:09 [John] How are we feeling about the concert? 02:10 Excited? 02:11 So excited! 02:12 It's freaking Oliver Anthony! 02:14 We have to go. 02:15 [John] You gotta do it. 02:17 Before he picked up a guitar, 02:19 Oliver Anthony was working 02:20 low-wage factory jobs. 02:22 [Robbie] I like him because he's just an ordinary person. 02:24 He refuses to do big stadium shows. 02:26 [Thadeus] He's speaking out for people 02:28 who sometimes don't speak out for themselves. 02:30 [John] And some of his biggest songs 02:32 are about the struggle of working-class life. 02:35 You've got hits like “Ain’t Got a Dollar.” 02:37 Well, I ain’t got a dollar, 02:40 but I don’t need a dime. 02:43 But Rich Men North of Richmond is by far the biggest. 03:00 [John] Did you come out for the Oliver Anthony show? 03:02 I was gonna, but I couldn't afford a ticket, so... 03:05 I feel like that's kind of related 03:06 to Rich Men North of Richmond, right? 03:07 Yeah, it for sure is. 03:08 I work seven days a week 03:10 and I still can't afford 03:11 to even pay for my own apartment right now. 03:13 So what do you do? 03:15 I work at a daycare four days a week, 03:17 and then three days a week 03:18 I work at a movie theater as a cook, 03:19 and then some weekends 03:20 I work another job 03:21 that I've been working 03:21 for a couple years. 03:23 Should one job be enough? 03:24 Oh, 100%. 03:26 Why do you think it isn't? 03:29 Politics? I mean... 03:31 [John] Politics is also the answer 03:33 that Oliver Anthony offers 03:35 in the song's most famous line. 03:46 [John] That Rich Men North of Richmond song, 03:48 did that line specifically resonate with you? 03:50 Yeah, 1000%. 03:52 Well, obviously, fck D.C. 03:54 Fck the government! 03:56 I can’t even trust our own government 03:57 that’s supposed to be, like, 03:58 looking out for us. 03:59 I can't even trust people in high power. 04:01 And I feel like we should. 04:03 The richest people on Earth, 04:04 do they have too much power, 04:06 too little, or just enough? 04:07 Too much! 04:08 Are you kidding me? Way too much. 04:10 How much blame do you give to 04:12 the biggest companies on Earth? 04:14 Amazon, and Elon Musk, 04:18 and Disney, of course. 04:21 All of them are money scandals. 04:22 Do you personally feel like you're living 04:24 by the same set of rules 04:25 that Elon Musk is living by? 04:27 No. 04:28 I feel like he could get away with a lot more 04:30 'cause he has money, he can pay off – 04:31 I feel like a lot of the courts 04:33 and stuff are rigged 04:34 to people who have money. 04:35 You try to start a business, 04:36 you are getting punched down on 04:38 by a bigger business. 04:39 Like, my cousin started 04:40 a liquor company, did really well. 04:41 His name was this close 04:42 to somebody else's. 04:44 (popping noise) 04:45 Punches right down! 04:47 The big corporations have to get knocked down a little bit. 04:49 I grew up on a hog farm 04:50 in southeast Missouri. 04:52 There is no little man hog farms anymore. 04:54 Prices are outrageous. 04:56 I went the other day, 04:57 $5 or somethin' for a dozen eggs, 04:59 because they want to make more 05:00 and more money. 05:02 It's like, “Okay, 05:02 we got away with making $0.10, 05:04 oh, they won't care 05:04 if we gouge 'em another two pennies.” 05:06 Well, next thing you know, 05:07 they're making triple, 05:08 quadruple what they're supposed to make. 05:10 How important, to you, 05:13 is it for us to do something about that, 05:15 corporations just kind of being 05:16 able to charge whatever they want? 05:17 I think we all, 05:18 like I said, 05:19 if we don't do something about it, 05:20 our country is going to go to hell real quick. 05:22 [John] Clearly, there's a lot of blame here 05:24 for the richest men on Earth, 05:26 not just the ones 05:27 who live north of Richmond. 05:29 But for Oliver Anthony, 05:31 the blame doesn't exactly stop there. 05:50 [John] Where does that line in the song 05:51 land with you? 05:52 Was that one of the things 05:53 that resonated? 05:54 Don’t even go to the welfare part. 05:56 We don't want to go there. 05:58 I'll probably get thrown outta here. 06:00 My view is if you're able to work, you should work. 06:02 I don't care who you are, 06:04 where you’re from, 06:05 pay your own bills, 06:06 pay your own way, 06:06 if you can work, work. 06:07 Well, I think some of those programs 06:09 are great programs. 06:10 Don't get me wrong. 06:11 They're great programs, 06:12 but they're not a lifetime program. 06:14 They are a stepping stone program. Right? 06:17 So people fall on hard times and okay, 06:20 let me help you up. 06:21 Let me pick you up. 06:22 Now carry on. 06:23 And also, I have family 06:25 that has been on welfare 06:26 that has made it on the other side. 06:27 It's what you want to do with it. 06:29 You can use it. 06:30 You can use it to what you want to do. 06:32 His whole point is living on it. 06:34 [John] That's a big thing we came out for, 06:36 who's really causing our problems? 06:37 People on welfare 06:38 or, like, really big billionaires? 06:40 I think welfare is amazing. 06:41 I’ve been a single mom, 06:43 my girls have been, 06:44 you’ve been a single dad. 06:44 I think it’s people that use it and abuse it. 06:47 I think that’s the problem. 06:50 [John] Here's the thing. 06:51 About two thirds of Americans 06:53 use welfare at some point in their life, 06:56 and over half of children in the U.S. 06:59 will have used food stamps 07:00 by the time they turn 18. 07:03 These percentages are high, 07:05 and higher than they used to be. 07:07 That’s a trend that started long before 07:08 Joe Biden or Donald Trump became president. 07:12 The truth is, 07:13 millions of Americans 07:14 are forced to rely on welfare 07:16 because their jobs 07:17 don't pay a living wage, 07:19 because companies like Walmart 07:21 skimp on wages, 07:22 while our tax dollars 07:23 make up the difference. 07:25 And that kind of corporate welfare 07:27 is how you get one family 07:29 owning more wealth 07:30 than the bottom 40% of Americans. 07:34 [Producer] If it’s between the billionaires 07:36 and the corporate lobbies, 07:37 versus folks that are on welfare, 07:39 who is more to blame 07:41 for the problems that we face? 07:43 I will still take the elites on that one. 07:45 My thing is, I will never punch down. 07:47 So if there's a problem, it's above. 07:50 I mean, you can look below. 07:51 But like, most of those people lost their way 07:53 lost their way so long ago. 07:55 Everyone’s just so separated that 07:57 I don’t know if we can ever get back to it. 07:58 We're so divided against ourselves, 08:00 common people, for what cause? 08:04 [Oliver Anthony] There may be some people who 08:06 misunderstood my words 08:07 in Rich Men North of Richmond. 08:09 You know, the words say that there's 08:10 people on the street 08:11 with nothing to eat 08:12 and the obese milking welfare. 08:14 And that's not the fault of those people. 08:16 Welfare only makes up 08:18 a small percentage of our budget. 08:20 We can fuel a proxy war 08:22 in a foreign land, 08:23 but we can't take care of our own. 08:25 That's all the song’s trying to say. 08:28 [John] From the moment it went viral, 08:30 Rich Men North of Richmond 08:31 was the subject 08:32 of intense political debate. 08:34 [Joe Rogan] When a person like yourself 08:35 gets labeled a right-wing, left-wing fanatic, 08:37 like right out of the gate. 08:38 Both in like a week and a half. 08:40 Then yeah, at least I know 08:41 I’m doing something right. 08:43 [John] The truth is, 08:44 Rich Men North of Richmond 08:45 resonated with an audience 08:47 that was much broader 08:48 than some of the media coverage might stuggest. 08:51 He's speaking up for the way 08:52 most of us feel. 08:54 The struggle, for everybody right now, 08:56 is too real, 08:58 and it shouldn't be that way. 09:00 It’s a come together, 09:01 let's stop the bullsh*t type of song. 09:03 This is what we need more of. 09:04 When I look at Republicans and Democrats, 09:07 I see people who are bitter and angry 09:10 and disgusted with a system 09:11 that isn't serving anyone. 09:13 Everything he speaks about is 09:15 exactly where I'm coming from, 09:16 people are just so... 09:17 like that guy holding the sign where it was like, 09:18 “drain the Dems.” 09:19 And I go, “You realize that Republicans 09:20 are equally responsible 09:21 for a lot of the stuff 09:22 going on over there?” 09:23 So it's like, if you can't recognize both evils, 09:26 we're never going to go anywhere. 09:27 He's not even signing really 09:28 for Democrat or Republican. 09:30 What's right and what's wrong. 09:32 Yes, period. 09:34 Do you feel like that should be how it is? 09:36 That is how it should be. 09:37 At the end of the day, it shouldn't be 09:38 a right or left party. 09:39 For me, it’s about the working man, the working class. 09:42 [John] But Allegan County is also a deep red county. 09:45 So while there's support here 09:47 for the message in Rich Men North of Richmond, 09:49 there's also a lot of support 09:51 for someone else. 09:53 [Robbie] I think Trump resonates 09:54 because he's not a politician. 09:55 Until you get at least half in Washington 09:58 that's not politicians, 09:59 we're not going to get rid of the problem. 10:01 [John] Your politics are interesting to me 10:02 because there's like so much 10:04 that I agree with in there. 10:05 But, you know, 10:06 just transparently, I hate Donald Trump, 10:08 because he's from the billionaire class 10:09 and that's all he's going to work for, 10:10 in my opinion. 10:11 Why go with him 10:12 instead of, say, 10:13 leave the top of the ticket blank, 10:14 go down ballot, 10:15 like, how do you make that decision for yourself, 10:17 holding the opinions that you do? 10:18 I love getting challenged on questions like that. 10:20 No, I love that, I love that. 10:22 I agree with you too. 10:24 So, it basically comes down to 10:26 what's the lesser of the two evils right now? 10:28 I mean, I just think about the four years 10:29 when he was president, 10:30 like obviously, 10:31 in my opinion, 10:32 he’s the world's best crime boss. 10:35 And then you see people posting like, 10:36 “in the arms of Jesus,” 10:37 like, “I was persecuted too.” 10:39 And I'm like, dude, what are we doing? 10:41 Does it feel like the system’s so captured 10:43 that you need a crime boss to get out of it? 10:46 100,000%. 10:48 I truly believe, like, 10:49 that is where it starts. 10:50 One last question. 10:52 If it wasn't Trump, 10:53 it wasn’t anybody who's currently here, 10:55 if you were custom building 10:58 the exact kind of 10:59 political candidate 11:00 you would love to vote for, 11:02 what kind of stuff 11:03 would they really stand for? 11:04 Wouldn't be a politician, 11:07 come from a working family, 11:08 him a working man or woman, 11:10 him or her, herself, 11:12 and stand up for the little guy. 11:14 Because nobody stands up 11:15 for the little guy, 11:16 and the middle class, from day one of this United States, 11:19 has built the United States, 11:20 and we're the ones that always get sh*t on. 11:23 [John] So, who should we blame 11:25 for our biggest problems? 11:27 On that question, we’re divided. 11:30 Where do you come down? Who do you think, 11:31 if you had to make a choice? 11:33 Sometimes we punch up at the richest men on Earth. 11:36 Is it one? 11:37 Yeah, tally. 11:38 There you go. 11:39 Thanks, man. 11:40 You’re welcome. 11:40 Appreciate it. 11:42 And sometimes we punch down 11:43 at the powerless, 11:44 the people who remind us 11:46 that the system feels broken beyond repair. 11:49 You guys wanna vote? 11:50 I’ll vote. 11:50 You'll vote? 11:52 Poor people. 11:53 We got one vote for poor people. 11:54 But it's worth asking, 11:55 who profits 11:57 from pitting us against each other. 11:58 Hi, everyone. 11:59 And how the hell do we fight back? 12:01 [Bryce] You remember in Bug's Life 12:02 when the grasshopper’s like, 12:03 if they figure out 12:04 that they outnumber us 500 to 1, 12:06 we’re screwed. 12:09 It's exactly the same thing. 12:11 We have 75% of the population punching down, 12:13 and it’s like, “Oh great, look at us, look at us.” 12:15 We're doing exactly what they wanted. 12:17 [Lexi] Everybody wants everybody to fight each other. 12:20 All these high people don’t want peace, 12:22 'cause then they lose their money. 12:24 Rich people, one vote for rich. 12:25 [Lexi] If we all work together, as a world, 12:28 as a nation, we would all be happy. 12:30 We all wouldn’t have to worry 12:32 about working for our money 24/7, 12:36 we just need to work together. 12:37 Woah, baby. 12:39 The youth are coming out to vote. 12:42 [John] You feel like we have power 12:43 we don't realize? 12:45 Yes, I think they have us so dumbed down 12:47 to believe that we can't. 12:49 We will never make any progress, 12:51 because we're looking here 12:52 when we should be looking up. 12:56 There it is, thanks bro. 12:59 Definitely. 13:01 There she is. 13:06 We are more divided today 13:07 than we've ever been. 13:08 Everything is about one party 13:10 or one person 13:11 trying to hold some moral high ground 13:13 over the other, 13:14 just for the sake of of being able 13:16 to point their finger down at them. 13:18 And it’s like, what the hell are we doing? 13:22 There she is, we’re racking 'em up. 13:25 Power of the pen, baby. 13:26 Thank you for voting. We appreciate you. 14:37 Howard. 15:04 Right. 15:05 Right there. 15:05 All right, guys. 15:12 Oh! 15:13 Thank you. 15:16 Oh! Letter. 15:18 Oh, baby. Here we go. 15:20 There it is. 15:20 Let's go. 15:27 To. 16:08 There we go. 16:10 That's. 16:13 Okay. 16:15 What a lot of sentiment out there, 18:07 Here's my view. 18:08 The fact of life is so expensive 18:11 for the average working person. 18:12 That so many people work overtime hours 18:15 for bullshit pay 18:17 in small businesses 18:18 don't stand a chance against huge 18:19 corporate monopolies. 18:21 These are problems 18:22 that are bigger than Harris or Biden 18:25 or Trump. 18:26 The American dream 18:27 has been eroding for decades, 18:29 and what Richmond, 18:30 north of Richmond, revealed 18:32 is that there are millions of people 18:34 on the left and right 18:35 who are ready to punch back at a system 18:37 that allowed that to happen. 18:40 The question is, 18:41 which direction should we punch? bruchansky/pfootprint-nlp Author: Bruchansky, Christophe, Date: 2024-09-14T07:47:47Z Collections: NeuroPsychoLinguisticPolitics, PoliticalML Zotero Key: XBVKD9PS Cite Key: Bruchansky24politFootprintNLP Zotero Item | Lit Note Political Footprints: Political Discourse Analysis Using Pre-Trained Word Vectors A political footprint is a vector-based representation of a political discourse in which each vector represents a word. Political footprints are computed using machine learning, which allows for a systematic and more objective political analysis. The Python toolkit provides heuristics on how to use pre-trained word vectors to analyze public declarations and political debates. Because political footprints compute semantic similarity based on large corpora of text, they lead to political discourse analysis that relies less on the researcher’s or journalist's political knowledge or beliefs. They are, however, very much dependent on the corpus they were trained with (Wikipedia, Google News, etc.), and, more generally, on the cultural context all political discourse originates in. The scripts were designed with journalists and researchers in mind (political science and cultural studies). A personal computer and basic knowledge in coding are only required. Assuming data is well formatted, it is possible to analyze a full presidential election in a single day. In times when soundbites and social media trends are more commented on than any political vision, political footprints aim to help place political discourse back in the center of public debate. Here are some examples of political footprints in action: In the words of Claude Lévi-Strauss: « Quant aux créations de l'esprit humain, leur sens n'existe que par rapport à lui, et elles se confondent au désordre dès qu'il aura disparu » “As for the creations of the human mind, their significance only exists in relation to the mind, and they will fall into general chaos as soon as it disappears” What is a political footprint? A political footprint is a vector-based representation of a political discourse in which each vector represents a word, with a number of option properties attached, such as a relevance, sentiment, and emotion. It is a subclass of word vector models (or word embeddings) in which words with similar meanings are located close to one another. Political footprints are like semantic maps applied to political discourse; they have been inspired by other initiatives such as word2vec4everything. They focus exclusively on what a statement or speaker says. They are, in this sense, very different from other popular word cloud analyses that focus on news articles and social media (i.e. tweets). The emphasis is on what a speaker has in his or her control. From left to right: Bernie Sanders, Hillary Clinton, and Donald Trump’s political footprints during 2016 U.S. election debates, with the closest words to “people” highlighted. Transcripts of all televised debates are available on the American Presidency Project. The current implementation is based on the following technologies: • IBM Watson natural language understanding: returns a list of entities and keywords included in a text, with a relevance, sentiment, and emotion score for each one;* • GloVe (Stanford University): package including word vectors trained using Wikipedia (2014) and Gigaword 5 (2011). Using this model means that our results are based on a 2014 snapshot of how words relate to one another. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. pdf bib;* • FastText (Facebook): alternative to GloVe that supports non-English languages. P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information;* • TensorFlow and TensorBoard: open-source software library for machine intelligence, its TensorBoard interface is particularly useful for quick word embeddings visualisation;* • Wordle: free online tool to generate word clouds.* This toolkit is multi-language, please consult IBM Watson and FastText documentation to see what languages are supported. Scripts are written in python and use, among others, TensorFlow and IBM Watson libraries. TensorFlow provides some good documentation on how to install python and its library on your computer: install python and TensorFlow. In addition, you need to download and unzip the pre-trained word vector model of your choice, for instance glove.6B.zip or FastText, and put the files in your project’s folder. Text format and tokenization The first step is to gather your data, clean it and format it so that it can be vectorized using TensorFlow. Examples of raw data are provided in the examples/US-Elections-2016/debates folder (source: The American Presidency Project). Two scripts have been developed to help you in this process: Step 1: parse html files using pfootprint-generate-texts.py (optional) Use this script if your data is in the form of html files, or if you deal with a corpus involving more than one contributor per data file (debate transcripts). python pfootprint-generate-texts.py -d examples/US-elections- 2016/Primary_Republicans/ -t debate Arguments: • d: directory with all your html files.* • t: type, either \"debate\" or \"declaration\".* In the case of declarations, each html file generates one text file. In the case of a debate, the script isolates individual contributions (assuming each contribution starts with the name of the contributor in bold), and creates one text file per contributor. In the case of a debate, you need to manually clean the text files after running the script: • merge files that might have resulted from a slightly different spelling of participant names;* • delete moderator files. We lose a certain amount of important information in doing so, which means it is sometimes difficult to understand what a candidate's answer was about, but this is the price we pay when we only take into account the candidates' own words;* • the script names text files using their original folder name (i.e. \"Primary_Democrats-SANDERS.txt\"), so that political footprints can be grouped into subcategories, such as ‘Primary-Republicans’, ‘Primary-Democrats’, and ‘General-Election’. All text files need however to be located in a single folder (\"text-files\" folder in our example).* Note: the script removes any text within brackets such as \"applause\", \"laugh\", etc. We want to focus on what a speaker says. We thus need to exclude any reaction from the public. Step2: parse text files using pfootprint-generate-jsons.py The second step is to identify key terms in each text file, their relevance, sentiment and associated emotions. The script uses IBM Watson natural language understanding in order to do so (30 days free trial available). Resulting json files are all saved in a separate folder \"json-footprints\". Please update your IBM Watson USERNAME and PASSWORD in the script before using it. python pfootprint-generate-jsons.py -d examples/US-Elections-2016/ Arguments: • d: project directory* • l: language (default is 'en'), see IBM docs for supported languages* • u: url (optional)* The script finds all .txt files in a directory (and its subdirectories) and creates a \"json-footprints\" folder with .json files generated by IBM Watson. IBM Watson truncates queries with text files larger than 50kb; a workaround is to upload your files on a server and use their url instead (limit is 600kb). Notes: • using IBM Watson is convenient but it comes with a cost: there is not much control over or explanation of how the terms are selected and weighted. A possible improvement would be to use open source libraries;* • two json files are created per text: one for its entities and one for its keywords. It is not clear how IBM Watson creates the two lists, and it is assumed that entities are more relevant than keywords (if a term exists in both lists, only the entity version is kept).* Create political footprints using TensorFlow At this stage, the only data that matter are the json files we have stored in our JSON-footprint directory. We are now going to leverage this information using pre-trained word vectors: python pfootprint.py -d examples/US-Elections-2016/ -p pretrained- models/glove.6B/glove.6B.300d.txt Arguments: • d: project directory* • p: pre-trained word vector model file (i.e. glove.6B.300d.txt)* The script finds all .json files files in a directory (and its subdirectories) and creates a model folder with all political footprints. Using GloVe has the advantage that we can quickly run the script for a relatively low number of dimensions (\"glove.6B.50d.txt\"), and increase it to 300 once you are satisfied with the results. Done! We now have all processed all our data and we can use TensorBoard to visualize our results: tensorboard --logdir=examples/US-Elections-2016/model Open your browser, go to localhost:6006, select ‘embeddings’ and you should see the following screen with all your results. Play a bit with TensorBoard to gain an intuitive understanding of your data. Here are few tips on how to use TensorBoard: • Tensors (left menu): list of all political footprints that have been processed. In the case of debates, you can also visualize all words that have been used during the debates (Aggregation tensor) and compare the location of political footprints centroids (akin to centers of gravity, see heuristic 3).* • Color by: this option is useful to get a quick understanding of political footprints in terms of relevance, sentiment, and emotions. Square sprites have been added to the graph in order to check word sentiments even when some words are selected (red is negative and blue is positive).* • If you wish to visualize the graph in 2-D, run the t-sne for 5000 iterations instead of PCA (bottom left). Keep in mind, however, that both visualizations are reductions (the real space has up to hundreds of dimensions!).* • The most useful tool is the list that appears on the right when you select a word: they are its closest neighbors in the original dimension space, based on their cosine similarity (the distance between them).* So what do we see? Words seem, at least in the 3D space, to be grouped in fairly mundane categories: countries tend to be located next to one another, the same goes for politician names, business vocabulary, etc. What is important to understand, and what is really the whole point of using this technique, is that these clusters have been defined without any human intervention: they have been discovered by machines based on how frequently words appear together, either on Wikipedia or in other large corpora of text. This is what allows us to make an unbiased analysis of political discourse. To be clear, there is a strong cultural bias, stemming from how these words have been used on Wikipedia, news feeds, and other large corpora, but not from the researcher performing the analysis. Let us now look at how we can put political footprints to practical use, and avoid using our personal interpretation as much as possible. Political footprint analysis Heuristic #1: main themes of a discourse and what they mean 1. Open a discourse or candidate tsv file (model folder), using, for instance, Google Sheets. 2. Sort the words by relevance and pick up the top 5. 3. For each of these words, look inside TensorBoard at the 10 closest words. 4. Select these words in the tsv file and use a tool such as Wordle to visualize them, with their size = their relevance, and their color = the emotion they have been associated with. Undetected: #a8a8a8 Joy: #7afff7 Anger: #b33939 Disgust: #8f852c Fear: #b55c2a Sadness: #999370 Climate-related terms used in Paris agreement (source:United Nations Framework Convention on Climate Change). Word clouds have been generated using Wordle. Hillary Clinton’s topics that related to the affordable care act (U.S. election televised debates). Transcripts of all televised debates are available on the American Presidency Project. Read the full analysis here. Notes: • the same heuristic was applied to both the 2008 and 2016 US elections and the results were surprisingly consistent with our intuition;* • emotion detection was not as reliable. A Google Spreadsheet formula was applied to tsv files in order to reduce noise: the formula only keeps emotions that are consistent with sentiments (joy is only kept when sentiment is positive) and have big enough scores. A much better solution would be to use an emotion detection mechanism that can adapt to political language, such as this this one(Rheault, Ludovic, Kaspar Beelen, Christopher Cochrane and Graeme Hirst, see their paper). In any case, it is important to keep in mind that an emotion attached to a word is not necessary targeting that word. An angry feeling detected when using the word \"people\" doesn’t mean that the discourse is necessary expressing anger TOWARDS people, but that speaking about people generates anger. Sarcasm is another example of why emotions detected by IBM Watson might fail.* if(AND(F2>0.1,J2>G2,J2>H2,J2>I2,J2>K2),\"Joy\",if(AND(F2<- 0.1,G2>0.4,G2>H2,G2>H2,G2>J2),\"Anger\",if(AND(F2<- 0.1,H2>0.5,H2>G2,H2>I2,H2>J2),\"Disgust\",if(AND(F2<- 0.1,I2>0.4,I2>G2,I2>H2,I2>J2),\"Fear\",if(AND(F2<- 0.1,K2>0.4),\"Sadness\",\"Undetected\"))))) with the F column for sentiment, G for anger, H for disgust, I for fear, J for joy, and K for sadness. • Machine learning techniques such as this one belong to structuralism: word similarities are not inferred from the discourse that is analyzed but from the large corpus of text that was used to train the words (i.e. Wikipedia). Comparing distances (cosine similarity) between words does not provide any information about a specific discourse, but only about the culture and language it is based on; information about the discourse lies in the choice of these words instead of others, their relevance and the associated emotion. Read this article for a more detailed analysis;* • a possible extension would be to automate this heuristic using k-means clustering or other unsupervised machine learning techniques.* Heuristic #2: compare how a theme is appropriated by each participant 1. Open the TensorBoard \"Aggregated\" tensor and choose a term or theme that you would like to study. Let's say we are interested in American \"values\". 2. For this term, select in the aggregated tensor the 20 closest terms (i.e. \"social\", \"civilization\", \"inequality\", \"liberty\", etc.) and see how many participants have used each of them. Choose a couple of terms, ideally those that have been used by many participants and that have many different meanings. \"Social\" and \"interest\" were picked in our 2016 US election example, but it could also have been “ethical”, “principle”, “values”, “freedom”: any term that can be used with different sets of words depending on a participant's political views. 3. Compare related terms, to \"social\" and \"interest\" in our example, for each candidate using the heuristic #1. John McCain’s topics that related to social matters (2008 primaries debates). Transcripts of all televised debates are available on the American Presidency Project. See full example here. Bernie Sanders’ topics that related to social matters (2016 primaries debates). Transcripts of all televised debates are available on the American Presidency Project. See full example here. This heuristic was very effective in identifying fundamental differences in how US presidential candidates understood a topic or addressed a problem. One of its benefits is that the relationship between words has been defined at the GloVe level; we can thus compare texts that have not necessarily used the same terms to address the same issue. Heuristic #3: sort discourses by style and affinity 1. Open each discourse using TensorBoard. Visualize them colored by relevance, sentiment, anger, joy, etc. 2. Open the TensorBoard \"Centroids\" tensor and compare how far centroids are from one another. This heuristic has not been conclusive in the current implementation. A couple of interesting properties were revealed in our US election example, but not enough to exclude the possibility that they were mere coincidences. • Bernie Sanders's political footprint seemed more focused than the others: Wall Street was detected as being by far the most relevant of his topics. But this might have been due to his lack of synonyms to describe the same situation. We cannot conclude that Bernie Sanders was fundamentally more focused.* • Hillary Clinton's emotions were less visible in her political footprint. But as explained above, this might have been due to her use of sarcasm, and more subtle ways of expressing her emotions.* • With the possible exception of the fact that Bernie Sanders' centroid (center of gravity) was the farthest from Donald Trump’s, centroids did not correspond to any of our intuitions. Using centroids is arguably a simplistic way to look at political footprints. Centroids do not take into account relevance, sentiments and emotions: seeing Islam as a positive and secondary topic counts exactly the same as seeing Islam as a negative and central topic.* How to Fix America’s Two-Party Problem Author: Wegman, Jesse, Date: 2025-01-14 Collections: Voting Systems Zotero Key: 2ZZJIQTU Cite Key: Wegman25fix2partyProportVote Zotero Item | Lit Note Opinion How to Fix America’s Two-Party Problem A series of photographs of Congress and bills being signed by Democratic and Republican presidents. Imagine a Congress where politicians of different ideologies work together to pass legislation reflecting what most Americans want. This isn’t hypothetical; it’s how Congress worked for much of the 20th century. There were only two major parties, but each was much more ideologically diverse than today, so deal making and coalition politics were the norm. From Social Security to civil rights to immigration and environmental Congress got big things done. That’s not where we are now. In 2025, American politics is stuck. Voters see little but chaos, as Congress lurches from one self-imposed crisis to the next. Incumbents get re-elected over and over, and yet the parties fail to pass meaningful legislation on the things that matter most to Americans. It doesn’t have to be this way. To escape our two-party trap, we need a better system of electing people to Congress: proportional representation. Mr. Wegman is a member of the Times editorial board, where he writes about democracy, law and politics. Mr. Drutman is a senior fellow at New America and the author of “Breaking the Two-Party Doom Loop: The Case for Multiparty Democracy America.” Jan. 14, 2025 In early 2020, Representative Alexandria Ocasio-Cortez, the progressive Democrat from New York, was asked to speculate about her role under a Joe Biden presidency. She groaned. “In any other country, Joe Biden and I would not be in the same party,” she said, “but in America, we are.” Ms. Ocasio-Cortez’s frustration with the two-party system reflects the frustration American voters feel every time they step into the voting booth, when they find themselves stuck with the same two choices — and, in most places, only one with any shot at winning. As a new Congress sputters into gear, this rusty binary split — a product of our antiquated winner-take-all electoral mechanisms — is key to understanding why national legislature has become the divisive, dysfunctional place it is today. Representatives to adopt proportional representation — an intuitive and widely used electoral system that ensures parties earn seats in proportion to how many people vote for them. The result is increased electoral competition and, ultimately, a broader range of political parties for voters to choose from. In 2024 fewer than 10 percent of U.S. House races were competitive. In a vast majority of districts, one party or the other wins by landslides. Driven by a decades-long geographic sorting that has concentrated Democrats in cities and Republicans in rural areas and reinforced by partisan gerrymandering, this split electoral landscape has fostered a polarized climate that becomes more with each election. The heart of the problem is the system of single-winner districts, which give percent of representation to the candidate who earns the most votes and zero percent to everyone else. Winner-take-all is the electoral software that generates two dominant parties relegates third parties to playing the role of spoiler and wasting their supporters’ votes. This leads to the same high-stakes contest every two years between the same two parties, resulting either in domination by one or in and paralyzed government by both. The winners push through as much of their agenda as they can while they hold power. Losing parties often refuse to work with the other side. In less polarized political times, winner-take-all systems can do a decent job reflecting public opinion and maintaining democratic stability, but when a is deeply divided and large numbers of people fear that they will not be represented at all, the result is an erosion of trust in government and rising extremism and political violence. As the political scientist Barbara F. Walter has observed, a majority of civil wars over the last century appear to have broken out in countries with winner-take-all systems. No democracy can survive long in the face of this much division and distrust. It’s hardly surprising, then, that more than two-thirds of Americans want to see changes in our political system. Roughly the same proportion wish they had more than two parties to choose from. They’re right: Two parties competing in winner-take-all elections cannot reflect the diversity of 335 million Americans. The Fix We ended up here not by any conscious choice. The framers of the Constitution never addressed congressional electoral systems Rather an election that only one American democracy, one that we have neglected to update, despite radically changed circumstances. As a result, we feel trapped by a system we backed into without thinking much about it. But what if we aren’t trapped? Most established democracies already use proportional representation. The switch from winner-take-all to proportional elections has been the most common major electoral system change among democracies in recent decades. Here’s how it would work in the United States: That’s proportional representation. Voters vote as they do today, but because districts have multiple members rather than single members, it allows several candidates to win seats without a majority of the votes. When the winning threshold goes down, the number of viable parties goes up. So multiple from multiple parties can represent a district, giving millions more Americans the opportunity to vote for a winning candidate, whether that candidate is a Republican or a Democrat or a member of another party. This brings us closer to the vision of founders like John Adams and James Madison, who both warned against the dangers of two dominant parties (or factions). As Adams wrote in 1776, Congress “should be in miniature, an exact portrait of the people at large. It should think, feel, reason and act like them.” What would that look like in 2025? How many parties would there be? To find out, we analyzed data from Nationscape, a large survey that polled hundreds of thousands of Americans on their political preferences. We used those answers to distribute voters among six hypothetical parties. Source: Nationscape Offering Americans a choice among multiple parties doesn’t just give them a clearer voice in how and by whom they are governed; it also opens up possibilities for new political coalitions, which can in turn lead to the passage of broadly popular legislation. Our politics may seem intractable today, but on issues from gun control to immigration to education reform to A.I. regulation, winning coalitions are hiding in plain sight. It also works better with bigger legislatures, but the U.S. House has been at its current size for more than a century. From the beginning of the House in 1789 until the early 20th century, its membership grew roughly in line with the U.S. population. At the start, there were about 34,000 constituents per representative; by 1911, that number had grown to more than 200,000 — much than the founders intended but still manageable. Today the average district more than 760,000 people, which is far too big for any one representative. As a result, tens of millions of Americans are represented by House members they did they are not represented at all. The fix for this problem is to expand the membership of the House of Representatives to reflect the size and diversity of the U.S. population in the 2020s rather than the 1920s. According to many political scientists, the optimal total number of members of the House and the Senate combined is equal to the root of the nation’s population. Legislatures in democracies around the world roughly align with this ratio; the U.S. House did, too, until its size was by law at 435. Today the cube-root rule would give us a House with 593 members. To see how these reforms would work in practice, consider Massachusetts: A map showing the congressional districts of Massachusetts, both before and the introduction of proportional representation. Massachusetts currently has nine seats in the House. All of them are filled by Democrats, even though more than one in three voters cast a ballot for Donald Trump in November. In an expanded House with proportional representation, Massachusetts would have 13 members, divided into three districts of four or five representatives each. Sources: Nationscape; Redistricter Not only would right-leaning voters in Massachusetts get federal representation, but the candidates they would help elect would most likely differ from rightleaning candidates elected in Florida or Wyoming. The same is true of the state’s liberals and progressives, who would most likely elect different candidates from those in Texas or Nevada. If proportional representation was used nationally, it would recreate the ideological diversity that is currently suppressed by the two-party system. To see a picture of how that could play out with multimember House districts, we used Redistricter, a mapping application for analyzing political and demographic Here’s what it might look like. New Popu New Liber Progres Maine N.Y. Mass. Mich. Conn.R.I. Maine Party Seats |Party|Seats| |---|---| |Progressive|74| |New Liberal|120| |New Populist|132| |Growth and Opportunity|122| Christian Conservative 93 Sources: Nationscape; Redistricter The big benefit of proportional representation is that tens of millions more Americans would feel represented in Congress. It would also effectively partisan gerrymandering and most likely increase representation of racial, and religious minorities. In all, it would be a better way to run one of the largest and most diverse countries on the planet. Eight Words Many Americans believe the two-party system is inherent to the American constitutional design. But the nation’s framers didn’t intend it, nor is it anywhere in the Constitution. Rather, the shape of the House of Representatives is mandated by a mere eight words of federal law, which requires “no district to elect more than one representative” — 435 districts, 435 members. The law as it exists today was passed in 1967, to address concerns that whitedominated Southern states would exploit multimember districts to marginalize their Black voters, but versions of it have been in effect since 1842. Before then, states regularly used multimember districts to elect their congressional representatives. None of those multimember districts, however, have ever been proportional. All used a form of bloc voting — a majoritarian system in which voters can support as many candidates as there are seats, making it impossible for minority-supported candidates to win. But under proportional representation, voting is impossible because each voter gets a single vote. That’s another thing about proportional representation: As a voter, you wouldn’t do anything substantially different from what you do now. You’d receive your ballot and pick the candidate you wanted to represent you in Washington. What would be different is that you would have more candidates and parties to choose from, and seats would be awarded in proportion to each party’s total vote share in a multiwinner district. The winners could be reported right away. No new voting machines or procedures would be required. And the law could be changed to allow for proportional representation tomorrow. Can This Actually Happen? Any big electoral reform raises big questions. First, if Congress is already dysfunctional with two parties, why would it work better with five or six? The quick answer is that Congress would do what every |Christian Conservative|93| |---|---| coalitions and allocate leadership roles and committee posts to reflect the bargains they’ve made. If, for example, the 2026 election were held under proportional representation, the Democratic Party coalition might reorganize into three parties competing separately for seats in Congress — the Progressives, the New Liberals and the Populist Party. Each party could appeal to different constituencies, including many that currently feel ignored or alienated by the two-party system. By spreading out responsibilities, Congress could operate in a more decentralized, committee-oriented manner, with the House speaker playing a less powerful role. This is how Congress operated in the mid-20th century, a more fluid era. Elections for the Senate and the presidency would remain as they are. But with a multiparty legislature, we would expect coalitions of multiple parties around strongest presidential candidates, with cabinet appointments that reflect the winning coalition. This combination has proved durable in many countries. And contrary to popular belief, we wouldn’t have to move to a parliamentary system of government. Most countries with presidential systems use proportional representation to elect their legislatures. The United States is one of only stable(ish) presidential democracies in the world that use a winner-take-all system to elect their legislature. The other three are Ghana, Liberia and Sierra Leone. Would sitting members of Congress ever approve these reforms? Many of them agree that their institution is not working. Interviews with retiring members show how dysfunctional, chaotic and difficult Congress has become — and why so many representatives now call it quits after a short time in office. It’s not surprising, then, that calls for reform are starting to come from the House. In November, Representative Marie Gluesenkamp Perez, Democrat of Washington, and Representative Jared Golden, Democrat of Maine, proposed establishing a select committee to examine electoral reforms, including proportional representation and House expansion; last week they reintroduced it as a top priority in the new Congress. Meanwhile, incumbents who fear losing their jobs should expect to be re-elected under proportional representation, since would benefit from the advantage of being well known. A more serious obstacle is the leaders of the two dominant political parties, are unlikely to support reforms that weaken their power. Then again, party leadership has been described as the worst job in America. And elected leaders sometimes support significant reforms when voters become so disillusioned that they demand them. broken system. But imaginations can expand, and conditions that seem can change. They often do in a crisis like the one we are living in now. This is a rare moment of openness to reform, combined with and triggered by the in American politics. We should seize it. Methodology To create the six new hypothetical parties, we built indexes that capture the policy views of Americans along economic and social dimensions. The data are Nationscape, a nationally representative survey of several hundred thousand people fielded between 2019 and 2021. The economic index we created includes positions on issues related to health care, the minimum wage, environmental policy, trade and taxes. The social index includes positions on issues like abortion, gun policy and immigration, as well as racial and cultural attitudes. We then defined the positions of six new hypothetical parties along those economic and social dimensions and matched each respondent to the party that most closely aligns to her or his preferences. New multimember districts were drawn to accommodate an expanded House of 593 members. The seats in those districts were allocated based on state population. For simplicity, no district has more than six seats. In states with fewer than six House seats, the entire state is covered by one district. We then placed respondents into their new multimember districts and counted how many supporters each party had in each district. We treated the number of party supporters in each multimember district as votes for that party and used those votes to figure out how many seats the parties won in each multimember district. Proportional representation systems use different formulas to translate votes to seats. For this exercise, we used a formula first developed by Daniel Webster that tends to facilitate the representation of small parties. The Truth About Joe Biden’s Economy Author: Moore, Stephen, Date: 2024 Collections: Hot Takes US Elect 2024 Zotero Key: WYYUZ5Q6 Cite Key: Moore24truthBidensEcon Zotero Item | Lit Note Americans are feeling the pain at the pump and at the grocery store. Inflation is at a 40-year record high. Yet, President Joe Biden and his administration say that the economy is better than ever. This week, Distinguished Visiting Fellow Stephen Moore explains the truth about Joe Biden’s economy. Michelle Cordero: From The Heritage Foundation, I'm Michelle Cordero and this is Heritage Explains. Cordero: Inflation rates hit a 40 year high last week, coming in at 9.1%. The cost of everyday staples also increased. The price of eggs went up 33.1%, meat 8.2%, gasoline 59.9%, used cars 7.1% and air travel 34.1%. This is all in addition to supply shortages. Yet as the economy is flashing huge red warning signs, President Joe Biden and his administration are telling us something different. Joe Biden: Look, here's where we are. We have the fastest growing economy in the world, the world, the world. We have 8.6 million new jobs just since I got in an office. Unemployment rates down to 3.6%. We've reduced the deficit last year by $320 billion, this year going to reduce it by $1.7 trillion dollars, trillion dollars. Clip: When you look at inflation, when we look at where we are economically, and we are in a strong, we are stronger economically than we have been in history. When you look at the unemployment numbers at 3.6%, when you look at the jobs numbers, more than 8.7 million of new jobs created, that is important. But we understand that gas prices are high and we understand that food costs are high. And that is because of a once in a generation pandemic and also Putin's war. And that's just the facts. Cordero: So what's the truth? Are we growing faster and stronger than ever, or is our economy in a downward spiral? Today Stephen Moore, a distinguished fellow in economics at Heritage, will explain. Our conversation after this short break. >>> Tired of hoax stories? Fed up with toxic partisan coverage? There is a better option. The Daily Signal delivers news that matters to you on culture, politics and current events. Stay up to date on the real news of the day. Subscribe to The Daily Signal, wherever you get your podcasts, because you can handle the truth. Cordero: Steve Moore, thank you so much for joining us. Stephen Moore: Hi, Michelle. Great to be with you. Cordero: Okay. So we know what we're seeing at the pump and at the grocery store, we're feeling the pain. So why is the Biden administration saying the economy has never been better? What are they looking at? Moore: Well, I think first of all, this really suggests that the people in the White House are just out of touch with what's going on in real America. We've always talked about Washington being a bubble, where politicians of both side of the aisle are just not in touch with what's happening in main street America. And this is a really a picture perfect example of that, where it's laughable and it's actually insulting for the Biden administration to tell people who are seeing their incomes being ravaged by the highest inflation rate in 40 years. Moore: You see there was a study that came out recently that a lot of families are really having to cut back and even the essentials that they buy because their incomes are falling relative to the price of everything from gasoline to buying milk and all the other things that people have to buy, rent. Moore: And so it's not a good economy. 82% of Americans, according to New York Times poll, and they're hardly on the right, Americans say the country's headed in the wrong direction. And it is. It is headed in the wrong direction. We've got the high gas prices. We have a situation now where I believe we're in a, what I call a soft recession, where the last two quarters have been negative, now only a little bit negative, but still that's officially in a recession. Moore: And the real question now, I think going forward, Michelle, is whether we're going to have a crash landing here. Whether we're going to be able to glide out of this and we can talk about that, but I'm worried. Cordero: So another thing we've heard from Joe Biden is that Americans have record savings. Where's he getting that? Is that true? Moore: Yeah. I'm working on a paper right now, that's why I was laughing. About the things, the multiple things that Biden has said about the economy that are not true. And that's one of the ones I don't even understand. Moore: Sometimes you can cherry pick data and say, oh, look, this shows, but we've just lived through a stock market crash where the economy's lost $8 trillion in savings. So it's really delusional to say Americans have record high savings when every day that goes by, people are losing their lifetime savings. I mean, my God, my wife and I just looked at our 401k plan and I'm not rich, we lost a couple hundred thousand dollars just in the last six months of our lifetime savings because of the crash in the market. Moore: And then the fact is that I was looking at credit card data, Americans aren't saving, they're dis-saving, they're running up their credit card so they can pay their bills. So that is another one that it just doesn't comport with reality. Cordero: So let's talk about the job market. I'm just going to kind of run through some of these indicators. The left claims that the job market is growing. Is that true? Moore: Look, the job market is good right now. It is good. And there are jobs out there. So my advice to people, if you're been sitting on the sideline for two years, like a lot of people have throughout COVID, now would be a good time to get a job while they're still available. Job growth is definitely slowing. And so over the last three months, we've seen there's fewer Americans working today than there were three months ago. Moore: And I think we're all a little puzzled. Even the economics team here at Heritage, we have a lot of discussions about why is this happening? Why aren't Americans filling the jobs that are out there? And I do think the government benefits is a big part of this, that we're paying people not to work. Moore: And one of the things, I mean, the people regulars here at Heritage know that Heritage played such a big role in the 1990s on the historic welfare reforms, where we required people to work to get benefits. We all believe in a safety net, but you have to either be working or looking for a job or getting training. Moore: And people should realize that under Obama, then especially so under Biden, we've eviscerated all of those. Almost all of the work requirements are gone. And so you've got people who can get rental assistance, food stamps, free healthcare, $300 a week per child benefits, unemployment benefits, all of these things. And they add up to, and by the way, this is all tax free, without working an hour, you can make 70, $75,000 a year. So we have to get back to the idea of work fair. Cordero: So you're saying, going back to the question that the job market is growing, but that's due to the circumstances left over from the pandemic. Moore: Yeah, exactly. We're still a little bit below where we were in 2019. We haven't still fully recovered. Moore: And the other part of this, Michelle, that's really interesting that we've been doing a lot of work on here at Heritage, is looking at the difference between what's happening in the red states and the blue states. So Republican governors handled COVID much, much better than the blue states did. They kept their schools open. They allowed their businesses where possible to stay open, whereas the blue states just shut down. Moore: And so even to this day, the red states have lower unemployment rates. They have healthier businesses. And so it is also a kind of a tale of two countries. The job market is pretty good in the red states and in the blue states, you still have a lot higher unemployment. Cordero: Okay. Another indicator is the housing market. Fox Business recently reported that home sale cancellations hit its highest rate since the start of the pandemic. What does that tell us? Moore: I'm worried about housing because I think we all remember what happened in 2007 and 2008 when we had the financial collapse that was led by the bursting of the housing bubble. And Lord, I don't want to see that again. I mean, that was just a total collapse of the US economy. Moore: I am worried about a bubble in the housing market. You've got record low affordability, so people can't afford the prices of the houses, and then you have rising mortgage rates. So just to give you an example, now, mortgage rates actually have come down a little bit in the last few weeks, thank goodness. They hit 6%. Now they're at about 5.7, but they were at 3%. So let's just use the single example from three to 6% on a mortgage rate, you have a 30 year mortgage and a $500,000 home, which is about the median house value today. You're going to pay an extra 150 to $200,000 in interest payments over the life of that mortgage. So it's making housing even more expensive and I do think you're going to see a slow down. Moore: Now, look, we've had a, the housing market's been through the roof over the last five years, so it's got to come down. I just hope it doesn't crash. Cordero: All right. And then one more thing that I'm seeing the left is actually admitting that inflation is going up, but I see them following it up with, but so are wages. Does that matter? Moore: Sure. I mean, what's killing the economy and killing families right now is that for roughly the last year or 14 months or so, inflation is up about 8% and wages are up about 5%. So that means families in real terms are 3% poor than they were when Biden came into office. And I lived through this in 1970s. So, people would get eight or 9% pay raises and yet inflation was running at 12%, so they were actually losing money. And that's the phenomenon you're seeing right now. Cordero: So the left is spinning that then by acting like wages are up is a good thing. Forget about the fact that inflation is up. Moore: Yeah. Because we want to look at the purchasing power of your wages. And so that's way down. And so we've got to get the inflation rate down. I mean, I want to make this point crystal clear, until we get this inflation rate down, inflation like a cancer, it just kills an economy. You see that throughout American history, you see it in other countries. So we got to, job number one is get that inflation rate down to at least, right now we're at eight and a half percent, we got to get down to no more than four and then get it back down to about the 2% range you want it at. Moore: And so the Fed has been way behind the curve on this. But another point we've been making at Heritage to all the members of Congress and the people in the policy making positions is the match that lit the forest fire of this high inflation was the runaway spending that happened. And it was the last year of Trump and the first year and a half of Biden where we spent three and a half trillion dollars we didn't have. Moore: And my only surprise that any economists are even surprised that, of course that's going to lead to inflation when you just flood the zone with all of this cheap money, then all of a sudden it's, as Milton Friedman taught us, it's too many dollars in the economy chasing too few goods. So you've got to, Fed has to start pulling back that money. Moore: And then the other thing we're talking a lot at Heritage about to, especially the Republicans because they will take Congress, is now you need also a pro growth agenda of how do we get an increase in the supply of goods and services? Well, you're not going to give that by raising taxes and raising regulations. And so we need a pro America energy policy to, as Trump had, we need to be cutting taxes and we need to get government spending under control. Cordero: All right, Steve, this is your wheelhouse. You're a rockstar when it comes to these issues. Cordero: In conclusion, we're feeling pain at the pump, feeling pain at the grocery store. The left is telling us that's not true, the economy's great. In a nutshell, what is the state of the economy right now? And are we heading into a recession or not? Moore: I would use the word precarious. We're in a really dangerous spot right now. We have to turn things around. And Michelle, something that really worries me, that's keeping me up at night now, is as you and I speak, unfortunately, you've got Chuck Schumer now negotiating with Joe Manchin, who was my man of the year. But if he makes a deal with Manchin, I mean with Schumer, for another version of this Build Back Better, you're talking about potentially another trillion dollars of spending and taxes, that would be catastrophic. I mean, I can't imagine they would do something that damaging the economy. We need to be cutting spending and taxes and the Democrats want to do the exact opposite. So that will be our big focus as we've got to stop this bill from happening. Moore: The Democrats have 50 votes in the Senate, so we have to, hopefully Kristin Sinema or maybe Joe Manchin will come to their senses because if they pass that bill, we'll have a severe recession. That's how dangerous that bill is. Cordero: Steve, thank you so much for your time and your clarification on these issues. Moore: Okay. Thank you, Michelle. Have a great week. Cordero: And that's it for this week's episode. If you liked it, we would be so happy if you left us a five star rating and comment, wherever you get your podcasts. And please share it with a friend or on social media. That's the best way to help Heritage Explains grow. Thank you for listening and we'll see you next week. Heritage Explains is brought to you by more than half a million members of The Heritage Foundation. It is produced by Michelle Cordero and Tim Doescher, with editing by John Popp. 2024 forecast methodology Author: The Hill, Date: 2024 Collections: PollMethods Zotero Key: QGWW8Y3R Cite Key: hill2024methodology Zotero Item | Lit Note 2024 Forecast Methodology The Decision Desk HQ 2024 forecasting model provides the probability of each candidate winning each state in the presidential election, the mean expected electoral vote total for each candidate, and the overall probability of each candidate winning the Electoral College and thus the presidency. Additionally, the model is designed to produce accurate estimates of the probability of Republican and Democratic victories in individual House and Senate elections, as well as the aggregate number of seats expected to be won by each party in the Senate and House. The model is built upon the framework of previous models in 2020 and 2022, with some methodological improvements. Summary Our dataset comprises over 200 base features including economic indicators, measures of the political environment (both national and local), candidate traits, and campaign finance reports. We also include engineered variables designed to draw context-specific information into the model. We refresh this dataset on a rolling basis. Not all features are used in every estimator; many are most effective when paired with certain other features and estimators. We also create adjusted variables from various data points, such as an adjusted PVI metric that combines the PVI with our generic ballot average, serving as one of our model's many predictors. We use Bayesian ridge regression, random forest, gradient boosting, and ElasticNet regression which when ensembled prove to be more accurate (Montgomery et al. 2011). Finally, we complete the ensemble by incorporating a weighted and bias-corrected polling average when available to produce our final prediction. We then run correlated simulations on our predictions to derive the range of possible outcomes. Our quantitative estimates for each race are converted into qualitative ratings, as described in Figure 1 below. This is done in an entirely automated fashion—we set the bounds on the ratings and go wherever the model takes us. Figure 1 Blended Poll Averages In our forecasting model, we employ a blended averaging method that includes data from every presidential horse race and head-to-head poll that comes out by a pollster with different polling dates. This blended average approach calculates the national and state-level averages of candidate support using a cubic spline. This ensures that we include every unique poll conducted into the overall average, while minimizing the influence of biased or outlier polls, and polling frequency disparities. Poll Weighting DDHQ has adjusted our election forecasting model by introducing a dynamic approach to weighting state polls and fundamental indicators as the election date approaches. The overall structure of the model remains consistent, with only the future trajectory of the weighting balance being modified. With this launch, the model reduces poll related uncertainty over time by applying an exponential decay factor to fundamental indicators, such that polls have more weight in the predictions the closer to Election Day we are. Moreover, the model factors in the number of polls a specific race has and poll weights are gradually increased as more polls become available. This approach improves weighting without introducing any discontinuities and better reflects the increasing relevance of polling data as the election nears. Dataset and Variables Our Congressional model draws from a large base dataset of 265 variables in the House and 201 in the Senate, spanning each non-special congressional election from 2006 through 2020. A list of these variables and their sources can be found here . In addition to primary source data, we also feature-engineer several important variables. Our national environment relies on using a weighted average of generic ballot polling, built to account for pollster bias and variability. In addition to regular FEC numbers from campaign reports, we also incorporate a formula to compare Republican and Democratic campaign finance numbers in each district/state. As Michael Bloomberg and Tom Steyer know very well, increasing funding comes with diminishing returns for every extra dollar. We account for this by taking the difference in logarithms of the Democratic and Republican candidates' fundraising. Polling data for each race is consolidated using a weighted average that accounts for recency, pollster bias and pollster quality. Our polling average does not change unless new polls are added, so in the early stages of a race, our polling average may appear static. We refresh the dataset on a rolling basis to ensure that any and all changes to individual races are accounted for quickly. This includes adding any new individual race polling, changes in the national environment and special election environment variables, quarterly and 48-hour FEC reports, new economic indicators, primary election outcomes, and candidate status changes. For our weekly public update, the dataset will encompass all changes available at the end of the previous Friday, plus any polls released by the day before the update. Prediction Framework We assembled data from previous elections dating back to 2006 as our training set for our Senate and House models. However, on the Senate side, there have only been a few hundred Senate races to train a model on - not very many when one considers that even in our condensed model we are using dozens of variables. To rectify this, we also use previous House elections to help train our Senate as well as our House models. This leads to a much more stable and confident fit for our models, and also allows us to make distinctions between elections in the two chambers of Congress. More recent elections are weighted more heavily than older ones to account for evolving political trends that have altered electoral dynamics in recent decades. The Congressional model also emphasizes data from past Presidential election years compared to midterms, as Congressional races in Presidential years show distinct patterns. The model recognizes that cross-party voting has become increasingly rare, particularly in Presidential elections, making it more challenging for candidates to significantly outperform their party's baseline expectations. Feature Selection We employ feature selection methods to optimize our ensemble approach, following best practices in computational modeling and academic literature. Elastic Net: Elastic Net is a blend of LASSO and Ridge regression used for regularization and variable selection. LASSO is useful for high-dimensional datasets because it is computationally fast. However, if there is a group of variables that have high pairwise correlations, then LASSO often selects only one variable from that group, which may lead to “over-regularization”. Elastic Net addresses the problem of “over-regularization” by incorporating a ridge penalty which shrinks the coefficients instead of zeroing them out (Zou and Hastie 2003). Because the predictors have such high pairwise correlations (think 2020 House vs. 2020 Presidential results in the district), we decided to weight the L1 and L2 regularizations equally in our ElasticNet component. This leads to a robust and parsimonious model that provides an excellent component for the eventual Ensemble Model. Manual Feature Selection: Taking a more subjective approach to feature selection, we can select substantively important variables rather than allowing an algorithm to choose. Quantitative models sometimes fail to capture important differences that are not easily quantifiable or need more subjective adjustments made possible by researchers' discerning eyes. Models predicting Congressional results span the latter half of the 20th century (Stokes and Miller 1962; Tufte 1975; Lewis-Beck and Rice 1984) with relatively simple quantitative analyses and are still relevant and accurate through the work of contemporary scholars (Campbell 2010; Lewis-Beck and Tien 2014). We choose to include many of the same variables that these researchers find most important, such as incumbency (Erikson 1971; Abramowitz 1975), district partisanship (Brady et al. 2000; Theriault 1998), and whether a given year is a midterm or Presidential cycle ( Erikson 1988; Lewis-Beck and Tien 2014). At the same time, adhering to the Principle of Parsimony, we exclude relatively minor variables for which we do not have a strong theoretical reason to believe they substantially affect most election results, such as average wages. Also, while scholars typically fail to find a general causal linkage between raising more money and winning, it does appear to be a considerably predictive variable for challenger success ( Jacobson 1978). Algorithms Most election forecasting models utilize either a Bayesian approach or classical logistic regression to predict the outcome of an election. Our modeling process has evolved to utilize an ensemble technique, incorporating various algorithms and variable subsets, including Bayesian ridge regression, linear regression, Random Forest, XGBoost and Elastic Net to forecast an election. Including a variety of algorithms and variable subsets in our ensemble reduces error in two ways. First, ensembles have proven to be more accurate on average than their constituent models alone. Second, they are less prone to making substantial errors (if they miss, they miss by smaller margins on average) (Montgomery et al. 2011). Individual models produce good results, but will give slightly different estimates for each race. On the whole, we have found them to be comparable based on various accuracy metrics, but they produce better estimates when averaged together. In our testing, the ensemble predictions produce fewer misses in 2014 and 2016 than any single model alone. In the House model, we combine two separate ensemble models to leverage both party-oriented variables as well as incumbency-based variables, and then add any recent polling information. In the Senate, a single party-oriented ensemble model is sufficient to produce accurate results, and is later combined with polls to make a final prediction. Each algorithm is listed and briefly described below. Bayesian statistics, in a simplified manner, consists of utilizing observed data to update our degree of belief in an event occurring. For our purposes, the outcome of a particular race is modeled as a normal random variable with margin θ. The margin θ is modeled as a function of the features in the dataset using linear regression. Parameters of the linear regression are drawn from non-informative prior distributions, such as the Cauchy distribution, a distribution with extremely heavy tails, and which has a higher chance of sampling unlikely events. Random Forest is a well-known tree-based algorithm. It produces an estimate for each race by walking through many decision trees and then outputting the mean predicted margin of these trees. XGBoost is an ensemble trees algorithm that uses the gradient boosting library. As with Random Forest, it can be utilized for supervised learning tasks such as our forecasting model. Linear Regression finds the optimal (least-squares) combination of factors which best predicts the eventual margin in a race. We use a weighting scheme that gives more weight to close elections, so our model can train the most on the key races. Elastic Net is a mixture of LASSO and ridge regression. LASSO regression (L1 regularization) applies a penalty to reduce the number of variables, and ridge regression (L2 regularization) applies a bias term that reduces the volatility of the predictors and is most useful when the predictors are highly correlated with each other (as is true in our case). Polling and the Decision Desk HQ Poll Average Decision Desk HQ internally collects polls and creates a polling average used in the forecasting model. The approach incorporates all available public and internal polls into the Decision Desk HQ polling database. An adjusted cubic spline interpolation technique creates a smooth and continuous curve between data points, minimizing abrupt changes in any direction. The methodology involves daily fitting of new cubic splines, incorporating historical and new data points while preserving the integrity of past averages. Without any new polls in a day, the polling average remains constant, reflecting the most recent available data. This methodology enables Decision Desk HQ to deliver reliable and comprehensive polling averages that accurately reflect the sentiment of the electorate across various political races and indicators. The final forecast predictions are derived from a weighted average of two components: the fundamentals, which include all relevant data inputs except polls, and the polling average. The model determines the optimal weightings for these factors based on historical training data. Dynamic Modeling Race-level forecasts also consider the time remaining until the Presidential election. As the election approaches, the model dynamically adjusts its weightings to emphasize polling data, as polls become increasingly predictive of the outcome. Conversely, when the election is further away, the forecast relies more heavily on fundamental metrics, acknowledging that polling data is more likely to fluctuate and that sudden changes in dynamics can occur. The model's probabilities also become more certain closer to Election Day, reflecting the increased stability of race-level and generic ballot polling as the event draws near. Election Simulations After calculating probabilities for each individual Presidential state, House and Senate race, we then turn our attention to predicting the aggregate number of electoral votes/seats we expect the GOP to win and the probability of winning the Electoral College and taking control of the House and Senate. We use each race's predicted probability and margin to run simulations of the 2024 elections. Presidential race results by state are not independent from one another. For example, the candidates are the same, and polling errors are likely to be correlated. By treating each state as an independent entity, such suboptimal models neglect the potential for systematic polling errors or shifts in the national political environment that could similarly affect multiple states. This oversight led to a notable instance where one forecast designer found himself in the unenviable position of having to consume a bug on live TV, as a consequence of his model's overconfident prediction that Hillary Clinton would defeat Donald Trump in 2016. To account for the relationships across states and the importance of these correlations, the DDHQ modeling team implements a realistic and robust correlated simulation process. Similarly, many election forecasters erroneously consider the outcome of each Congressional race to be independent of others; however, in reality, this is NOT the case. Polling errors and national environment errors are mildly correlated across Congressional races within an election cycle due to various sources of error, resulting in systematic bias (Shirani-Mehr et al. 2018). In the Presidential simulation, we employ a multi-level approach incorporating national, regional, and race-level stochastic noise to create a realistic pairwise correlation structure in our model. In the Congressional simulation, we include national and race-level stochastic noise. At the national level, we generate a random variable representing the overall political climate, which affects all races simultaneously. This variable is drawn from a normal distribution with a mean of zero and a carefully chosen standard deviation that reflects the historical variability of national polling errors. Next, we divide the country into distinct regions based on geographic and political similarities. We employed hierarchical clustering techniques to determine the optimal regional groupings for our multi-level simulation approach. By analyzing a comprehensive set of demographic, geographic, and political variables that could potentially influence the correlation of polling and other forecast errors, we divided the United States into 10 distinct regions. We generate a regional random effect for each region in each simulation, also drawn from a normal distribution with a mean of zero and a region-specific standard deviation. These regional effects capture the shared characteristics and correlations among races within the same region. Finally, we incorporate race-specific random noise for each individual contest, accounting for the unique factors and race-level polling errors that may affect each race differently. Combining these three levels of random effects - national, regional, and race-specific - we create a robust and realistic simulation process that accurately captures the complex correlations between races (see Figure 2). We run millions of daily simulations to calculate the probabilities of Democrats or Republicans winning the Presidency, Senate and House and the likelihood of each possible combination of outcomes across these three chambers (see Figure 3). Figure 2 Figure 3 Online Extremism and Political Advertising: A Visual Interview With Laura Jakli Author: Jakli, Laura, Date: January 19, 2022 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: E87W5C4I Cite Key: Jakli22politAdOnlineExtremism Zotero Item | Lit Note How can we track online extremism through political advertisements? Using data from online advertising, Laura Jakli, a 2020 PhD graduate from UC Berkeley’s Department of Political Science, studies political extremism, destigmatization, and radicalization, focusing on the role of popularity cues in online media. She is currently working on her book project, Engineering Extremism. She is currently a Junior Fellow at the Harvard Society of Fellows. Starting in 2023, she will be an Assistant Professor at Harvard Business School’s Business, Government and the International Economy (BGIE) unit. From 2018 to 2020, she was a predoctoral research fellow at Stanford University’s Center on Democracy, Development and the Rule of Law, and at the Program on Democracy and the Internet. Social Science Matrix content curator Julia Sizek interviewed Jakli about her work, with questions based on political advertisements and graphics from Jakli’s research. Your research uses the Facebook Ad Library to understand far-right political parties. What insights do advertisements provide for understanding far-right parties? Since 2018, the Facebook Ad Library (also known as the Ad Archive) has publicly documented the political advertisements hosted on the platform, as well as some limited metadata for each ad (for example, the name of the ad buyer, the number of ad impressions, total ad expenditure, geographic target, and audience gender and age demographics). Initially, the Ad Library exclusively featured ads run in the United States, but it expanded to dozens of other countries within a year. Since I study European politics, this expansion of the Ad Library opened up a new way to explore party messaging at scale. Much of my research considers the gap between the publicly stated and privately held beliefs and preferences of far-right voters (and party elites themselves). In line with this, I was interested in examining party ads because the far right may be incentivized to present a more mainstream right-wing ideological profile in formal documents and in mass media campaigns to appeal to a broad audience. Meanwhile, when the far-right is targeting a narrow, custom audience through online media, the party may use more extreme campaign content. This is because, with digital micro-campaigns, they do not have the same political incentive to appeal to the masses or signal ideological common ground with center-right parties. With my current political ads research, the objective is to better understand far-right party strategy and political positions. The main advantage of ads in this regard is that most parties field hundreds of unique online ads in the months leading up to an election. The sheer volume of political ad text available means that it is quite feasible to construct reliable ideological profiles for small parties, and to create valid inferences about party strategy. Moreover, since online ads are time-stamped and geographically targeted, they can be used to trace how positions change over time, both sub- and cross-nationally. How do political ads work on Facebook? Who buys them, and how are political ad purchases split between groups? In other words, who is posting these ads, and how do they find their audiences? Many party ads are purchased by the national party itself, meaning that they are sponsored by the party’s main Facebook page, even if the ad content is focused on a specific regional or candidate campaign. But it can be a more decentralized process, and each political party can choose to run its political campaign through a combination of national and local advertising. In some European countries, I see party candidates and local party organizations paying for and running their own ads. Facebook allows advertisers to target not just by age, gender, and geographic location, but also by political interests and hobbies. Email lists gathered through rallies, fundraisers, and other events can be used to target customized political audiences. Moreover, these inputs can be used to find “Lookalike Audiences” that share interests, traits, and demographics with the established email list. These advertising parameters allow campaigns to target political ads quite narrowly and precisely. One weakness of the Ad Archive is that it doesn’t actually reveal how the campaigns found their audiences. All you have available as metadata is basic demographic information, including a breakdown of the audience by gender, age, and geographic location. You can make some inferences about whom parties targeted based on this information, but the ad algorithm may also be impacting that audience. For example, you can’t distinguish between when the party directed ads to be delivered to males between the ages of 18-24, or when the ad algorithm picked up on the fact that men between 18-24 interacted with the ad at higher rates, and therefore “learned” to deliver more ads to this segment over time. In other words, the audience is curated both through what ad buyers specify as their parameters (e.g., let’s target XYZ demographics), and the algorithm independently determining who would be an efficient target to display the ad. This advertisement (Figure 1) from Vlaams Belang, a far-right party in Belgium, is fascinating because of the way that it is designed to track viewer reactions. How are advertisements on social media different from ordinary advertisements, and are you able to track how people interact with these advertisements? Figure 1: Translation: “They have gone completely mad and want to actively participate in the return of ISIS terrorists! Vlaams Belang resolutely says NO. We must protect our people from these time bombs. We must take their nationality and try them in the countries where they committed their crimes. What do you think? Return possible for terrorists? [Indicate Yes (with a smiley) or No (with a like).] The ability to rapidly field and test the performance of different political ads is one aspect of online advertising that distinguishes it from older forms of campaigning. Parties don’t have to commit to one message or thematic policy focus through a campaign season. This flexible, feedback-based approach is precisely demonstrated in this ad from Vlaams Belang. It asks ad viewers to signal using the laugh emoji if they agree with the return of foreign terrorist fighters (known as “returnees”) and “like” if they disagree with the policy. Presumably, the idea is to quickly and cheaply test how salient this issue is for potential voters. Researchers are not easily able to track how people interact with these advertisements unless the advertisement links to a post on a public Facebook page. But in the case of Vlaams Belang and most parties that do these quick polls through ads, the poll takes you to a party webpage so they can get more information about their audience (and possibly elicit donations). One other way to get a sense of how people interact is simply through the number of impressions the ad gets. Impressions count the total number of times the ad is displayed on viewers’ screens. This is broadly informative, but doesn’t mean that audiences are actually clicking on the ad or interacting with its content in any way, so the inferences researchers can draw are quite limited. One of the benefits of online advertisements, in contrast to traditional advertising, is the ability to target certain groups. This ad (Figure 2) shows an ad that targeted audiences specifically in Austria. How did you find that targeted advertising worked for far-right groups, and how did advertisements differ at the local and national scales? Figure 2: Translation: “There is a huge boiling point at Europe’s borders, because masses of illegal migrants want to return to certain European target countries, including our Austria. While patriotic politicians like Matteo Salvini are doing everything possible to stop illegal migration, completely different signals are coming from Berlin. Angela Merkel even wants to have the refugees picked up from Africa….” Broadly speaking, the demographic metadata suggests that the far right has a much higher ratio of male ad audiences than do other parties, which makes sense, given the male skew of their voter base. But there is such limited metadata provided by the Facebook Ad Library that I have not been able to establish any other notable demographic trends. I am currently working on understanding the geospatial trends of far-right advertising but cannot say anything definitive yet. I will say that the more localized advertisements — typically fielded by regional party organizations or local candidates — differ substantially in content from national ads. The more localized campaign material is crafted to resonate with local news events and community issues. Far-right political ads that target a narrow geography appeal to voters less on abstract political platforms or ideological principles and more on tangible and immediate localized concerns. In effect, this represents a shift to digital “home style” politics, by which the far right frames their platforms such that constituents of each district are led to believe party representatives are “one of them” and have their immediate interests in mind when crafting policy. In my qualitative analysis, I found that regional far-right party branches often stylize themselves as accessible, populist, and anti-political, presenting their party as concerned with what is “happening on the ground” and what the “people” really want. Relatedly, these online campaigns are crafted and fielded rapidly, in a manner that is less professional, less polished, and more casual than offline campaigns. Knife crime is one example of a localized thematic focus common in far-right ads (see Figure 3). Figure 3: Translation: “Migrants at the forefront of knife crimes. ‘Dangerous people have no place in the middle of our liberal society and therefore have to be deported.’ Those were the words of #CDU Interior Minister Roland Wöller after the horrific knife murder in Dresden in October 2020 by an Islamist Syrian. This should give the impression that the CDU-led government is finally taking action against serious criminal foreigners….” Working from a large dataset of far-right political ads, you translated the advertisements into English, and then used the NRC Word-Emotion Association Lexicon to identify how the ads evoke emotions like fear, disgust, and anger. These images (Figure 4) show word clouds based on advertisements from the German AfD (Alternative for Germany) party. What do these word clouds show? Figure 4: Disgust and anger word clouds for Alternative for Germany ads, using the NRC Word-Emotion Association Lexicon (aka EmoLex). First, I want to note that the share of negative emotive ad content is typically much higher in far-right ads than in the ads of other parties. Their negative ad campaigns focus on — and often exaggerate — social and economic problems, while identifying other people, parties, and institutions as responsible for them. Consistent with much of the literature, I also found that the far right is associated with specific emotive appeals, most prominently with fear and disgust, but also with a higher share of anger emotion words, on average. Using the NRC Word-Emotion Association Lexicon, the disgust word cloud visualizes the terms tagged in the far-right AfD’s (Alternative for Germany) ads as being words associated with disgust. The size of the term in the word cloud helps visualize its relative frequency in appearance across the ads. The anger word cloud visualizes the same, but for anger-associated terms in the ads. These figures show that illegality, criminality, and violence are some of the most prevalent disgust-associated themes found in German far-right ads. There is quite a bit of overlap here with the most frequently found anger-associated words. Themes of criminality, violence, and terror attacks are frequently discussed by AfD, presumably with the intent of evoking anger toward the political status quo. One of your findings is that far-right groups in Europe tend to claim ownership over the topic of immigration, as is reflected in this advertisement (Figure 5). How did you measure the focus on immigration among far-right parties in comparison to their more moderate counterparts? Figure 5: Translation: “Swept under the rug: the huge refugee costs. The AfD has been talking about it for a long time, but the other parties and the associations and companies of the so-called ‘asylum industry’ that benefit from them consistently avoid talking about this topic….” I use a method called structural topic modeling to determine whether the far right maintains issue ownership on immigration. In topic modeling, each document (in this case, party ad corpus) is modeled as a mixture of multiple topics. Topical prevalence measures how much each topic contributes to a document. Put simply, I use metadata on which party fielded each ad text to examine differences in topical prevalence across the ad texts, and sort topical prevalence by party family. I estimated the mean difference in topic proportions for far-right parties and all other parties to determine which topics are more prevalent in far-right ads. I use this to gauge whether there is disproportionate emphasis on immigration in far-right campaign ads, or whether immigration topics are prevalent across different types of parties. In a large majority of sampled EU countries, I found a disproportionate emphasis on immigration issues on the far right, which is consistent with issue ownership. There are three notable patterns in how the far right discusses the immigration issue across Europe. First, many parties specifically emphasize Muslim migration and frame Islam as a unique threat to national values and cultural identity. Second, immigration is often tied to criminality as well as to issues of women’s safety. Third, it is linked to general Euroscepticism and the EU’s multiculturalism. While your analysis focused on the text of far-right political advertisements, the images would seem to be an essential part of ads’ effectiveness, as we can see in this image (Figure 6). What do you think are the limits of a text-based analysis, and what are avenues for investigating visual complements to your text-based research? Figure 6: Translation: “A sobering word for Sunday: The persecution of Christians in many countries around the world is increasing. But the Christian churches in Germany have paid too little attention to it for years. They prefer to curse the AfD, although the protection of Christians abroad is an important issue for this party….” It is definitely an important limitation. Many ads also have videos embedded, not just images. By reducing the current study to text analysis, I may miss the fundamental features that lead viewers to interact with the ad, click on related content, or mobilize for the party. More broadly, there seems to be a trend in recent years of decreasing emphasis on text and increasing emphasis on visuals and videos in political ads. These trends mirror other social media trends (e.g., the rise of TikTok and YouTube). I think the political parties that acknowledge this trend and craft their online ads accordingly have a leg up over those that do not. Based on a small qualitative assessment of these ad visuals, what I can say is that the inflammatory, emotive content I try to capture through text comes through much more explicitly in images and video. My sense is that the visuals associated with far-right ads are quite striking and substantively different from the ad visuals of other parties, although I have not tried to quantify these differences systematically. As our tools for image and video analysis improve in social science, I hope to study these features more rigorously. A Kamala Harris Canvasser’s Education Author: Preston, Julia, Date: 2024-11-30 Collections: Hot Takes US Elect 2024 Zotero Key: 3A6LUI8S Cite Key: Preston24harrisCanvasserEduc Zotero Item | Lit Note In October, as a novice volunteer knocking on doors in Pennsylvania for the Kamala Harris campaign, my task was to make sure that committed Democrats voted, and to persuade undecided voters that Harris was the better choice. I was told not to spend time talking with voters who were clearly supporters of Donald Trump. But there was something about the way one man snarled at me, “She’s evil,” as he was tending his front lawn on a quiet, tree-shaded street in a suburb of Allentown, that made me stop. When I approached, he seemed to shrink back, but he recovered and told me that Harris was a moral and physical danger to children because she supported public middle schools allowing students to undergo transgender surgery without the consent of their parents. By this time, after two months of canvassing, I had heard from several Trump voters some version of this noxious innuendo. I shrugged, told him his concern was not based on any reality I was aware of, and moved on to the next door on my list. News & Politics The latest from Washington and beyond, covering current events, the economy, and more, from our columnists and correspondents. A few minutes later, another voter on the same street opened his door to declare that he would vote enthusiastically for Harris. He was a pastor in a local Protestant church, and he expressed disbelief that Trump, after his attempt to overthrow the results of the 2020 election, his felony convictions, the court case that found him liable for sexual abuse, and his increasingly erratic and crude behavior, was even close to Harris in the polls. “How is this possible?” was the refrain I heard time and again from Democrats. Allentown is the most populous city in the Lehigh Valley, a forty-mile stretch on the eastern edge of Pennsylvania that follows the path of the Lehigh River. The city’s suburbs have a peaceful veneer that belies the tensions on the ground. On a winding rural road, I met an older woman, a registered Democrat, who had volunteered as a poll worker in recent elections. She said that her whole family was under online siege by MAGA militants who were accusing her of preparing to subvert the upcoming count. Another volunteer I met, who had come from Brooklyn with his two pre-teen kids, had been confronted by an armed Trump supporter. The man had claimed to be in charge of security for his neighborhood and said they were barred from entering. My own presence in Allentown, where I walked the streets with a green-and-pink shoulder bag carefully selected to convey joyfulness and filled with Harris campaign literature, had followed an abrupt life change. I’ve been a journalist for four decades, reporting on immigration and other subjects for the Washington Post, The New York Times, and, most recently, The Marshall Project. But, on June 27th, as I watched the debate between President Joe Biden and Trump, I was overcome with dismay. That night, Trump unleashed a barrage of lies about immigrants and asylum seekers. Biden failed to respond with any corrective truths or positive portrayals of immigrant families. A few days later, I resigned from The Marshall Project, as I felt I could no longer comply with its rules proscribing partisan activity. I joined a voter uprising against Biden, writing letters and making calls. My sense of relief when the President stepped aside, on July 21st, became exhilaration when Harris sprinted out of the gate the following day and assumed the Democratic mantle. Just as I was being initiated into the world of political activism, I was presented with a historic chance to help elect America’s first woman President. I started canvassing on August 11th. My induction took place in Bensalem, a township northeast of Philadelphia, in a spare campaign office still announced by a Biden-Harris yard sign. I received training on a mobile app that would guide my steps, generating for each of my canvassing forays a street map with dots showing the households of registered voters, who were identified by name, age, gender, and party affiliation. The app meant that, when people opened their doors, I could ask to speak with them by their first names. I recorded their responses, indicating whether they were “strong” for Harris––definitely voting for her–– “strong” for Trump, or were still in some murky terrain of indecision, in which case I was on: I had a minute or two to launch my pitch to sway them. I also received my first training in campaign messaging––a short course on Project 2025 and the catastrophic perils it posed for American democracy. Even on that first day, walking around in sultry heat, I began to sense a dissonance between the celebrity-inflected exuberance of the Harris campaign and the bleak mood and raw divisions I encountered in the streets. I canvassed a gritty apartment complex, with brown grass in the green spaces, that surrounded a small pool, where several mothers languished as their children splashed. They all scoffed when I asked if they were Harris supporters. By the end of that afternoon, the warnings about Project 2025’s plans for an “authoritarian, Christian nationalist movement with broad control over American life”—in the words of a flyer I received as part of my “lit pack”—felt too academic for a voter with gray and missing teeth who told me she could not afford dental care. By contrast, just blocks away was a curving street lined with colonial-style homes, with Volvos and S.U.V.s in the driveways, where one smiling Democrat after another opened the doors. Here was the class polarization that would later get so much attention. As for the Trump voters who turned up on my lists, I quickly understood that we were not operating on a plane of shared facts. A retired police officer shouted me down when I asked him to explain his support for Trump, given that the assault on the Capitol on January 6, 2021, had injured a hundred and forty law enforcement officers. “That’s a lie!” he said, even though I had, at the ready, the latest Justice Department report on the prosecutions of the rioters. Another voter insisted that all Trump had asked for after the 2020 election was “a recount” of the national vote, as if that were a remotely feasible, or legal, proposition. Others echoed Trump’s dark visions of millions of criminal migrants rampaging across the land, though there was little sign of them in northeast Pennsylvania. This is what I was up against: Trump was broadcasting on some direct wavelength with his followers, and he had drawn them into his alternate universe of looming economic disaster, menacing migrants, and outrages perpetrated by Democrats against their children, which only he was visionary enough to see and strong enough to combat. By late August, I decided to focus my canvassing on Allentown. The city, the third most populous in Pennsylvania, was once an emblem of American steel, but deindustrialization led to its decline, several decades ago. (“Well, we’re living here in Allentown / and they’re closing all the factories down,” Billy Joel sang in 1982.) In recent years, Allentown has undergone an uneven revival spurred by the arrival of tens of thousands of Latinos, many of them exiles from New York, who now make up a majority of the city’s population. About half are Puerto Ricans, American citizens who can vote in federal elections if they are registered in the mainland U.S. Another large group are Dominicans, including longtime U.S. citizens and first-generation immigrants. Most of these voters were likely to be registered Democrats or Independents. I speak Spanish, and I concluded that my most effective contribution would be to help run up the vote in Latino neighborhoods in Allentown. In a campaign office on Hamilton Street, in the center of the city, I found a corps of young staffers who were smart and vigorous, but perhaps not deeply experienced in the engineering of campaigns, backed up by volunteers who were union members and other stalwart Democrats. In the early days, after Harris’s choice of Governor Tim Walz as her running mate and her spectacular performance at the Democratic National Convention, the buzz was like a defibrillator bringing the campaign back from the dead. We were thrilled to think we might be witnessing something akin to the history-making excitement of Barack Obama’s first Presidential run, in 2008. But what I encountered at the doors in Latino neighborhoods were disaffected people under severe economic stress—workers with little time to watch television and no consistent or reliable channels for political news, who received scattershot information about both Harris and Trump on their mobile phones, and were disgusted by what they perceived as the nasty and pointless name-calling they saw there. I recall the harried look of a Puerto Rican grandmother, one of three registered Democrats in a walk-up apartment crammed with boxes and randomly placed furniture. She was home with her grandchildren, a wailing toddler and a teen-ager, while their parents were juggling day and night shifts at their jobs on a Saturday. She wanted to vote for Harris, she said, if she could get to the polls on Election Day. Often, my conversations started with voters telling me they did not plan to vote because they did not see any point in it. On September 7th, I attended a rally, organized by Latinos con Harris and headlined by her husband, Doug Emhoff, in the gym of a local high school. A d.j. from La Mega, the local Spanish-language contemporary radio station, played thumping dance tunes. The crowd cheered boisterously. Even so, the underlying distress was startling: two voters I chatted with ended up in tears. A woman named Julie, who had a disability caused by a car accident and who was living on a fixed income, said she hoped Harris would do something to increase the value of food stamps, because she was not getting enough to eat. A young Dominican mother, Melvis, carrying her infant daughter, said she saw Harris as both an example and protector for the little girl’s future. She said she deeply feared that Trump, a court-confirmed sexual predator, would only encourage the rampant, unseen sexual abuse and violence against women in her community. Meanwhile, I sensed that Harris was struggling to break through. She had an immense hurdle to overcome: the void of communication from the White House about what, if anything, the Biden Administration had done for Allentown and the larger Lehigh Valley. Voters associated Biden with higher prices for basic needs and virtually nothing else. They seemed to think that Trump’s term had ended with the stable economy of 2019, rather than with the pandemic and the steep economic downturn that followed. With six weeks to go, Harris’s identity as the daughter of a working immigrant mother and her proposals for an “opportunity economy” were barely beginning to resonate. In all my weeks of canvassing, only one voter, a Black Latina I met by chance in the parking lot of a Supremo grocery store, raised the issue of women’s reproductive rights, a centerpiece of Harris’s campaign. I set aside the campaign’s talking points and improvised my own. I talked about what I remembered from 2020, when friends were dying of COVID-19, millions of Americans lost their jobs, and Trump suggested we inject bleach into our bodies. (I found the bleach anecdote invariably sparked vivid memories for voters.) I made a point of saying that Joe Biden was not on the ballot. I created cards with bullet points on Harris’s child tax credit and other family-friendly proposals, which even I had a hard time understanding and explaining. I shared a video by the salsa star Marc Anthony, who said with grim intensity that he had not forgotten when Trump blocked funds for Puerto Rico after Hurricane Maria. I described the terrible harms of family separation that immigrants would face from Trump’s mass deportations. I connected with more than one Latina mother when I asked whether Trump, with his lying and philandering, was the example she wanted for her children. At times, our tools seemed excessively intrusive. In addition to the door-knocking, voters were bombarded with phone-bank calls and text messages. On my turf lists, most of the voters were not home, and I would leave flyers for the Democratic candidates tucked in their doors. I wondered if they felt uncomfortable that a stranger, presuming to know their political inclinations, had been lurking at their front steps. There were times, too, when I questioned the campaign’s tactics. At one point, I was told that paid canvassers had been hired to fan out across Allentown’s Latino neighborhoods. Fired-up volunteers (including me) were prohibited from door-knocking there. A major issue seemed to be the information flow, which moved entirely in one direction: from the candidate to the voters. With such an abbreviated campaign, there was little time to collect and respond to the concerns that people were raising at their doors. Nevertheless, by mid-October I noticed a distinct shift. On the weekend of October 19th, thousands of volunteers flocked to the Lehigh Valley, coming from all over the East Coast in convoys of buses and cars, armed with no specific battle plan but determined to answer Michelle Obama’s call to “do something.” Campaign staffers, pale from exhaustion, deployed these volunteers across the region. Harris and Walz kept up a blitz of rallies. Harris seemed to be growing into her campaign, articulating more specifics on her “to-do list” for everyday Americans. Then came Trump’s closing rally, at Madison Square Garden, on October 27th, where the comedian Tony Hinchcliffe committed the epic unforced error of calling Puerto Rico a “floating island of garbage.” When I returned to Allentown the following Wednesday, Puerto Rican flags were flying on porches. Residents suddenly realized that Trump’s demeaning rhetoric about Haitian and Venezuelan immigrants could extend to them. At one household, where my mobile app told me the family included four registered Democrats, the eldest member saw my Harris-Walz button and shouted to the street, “Fuck Trump!” The four had agreed they would go together to cast their votes for Harris on Election Day. On Monday, the last day before the election, Harris finally came to Allentown for a whistle-stop rally. Thousands of people stood in four-hour lines to attend, a more diverse crowd than I had seen at any previous event. The Puerto Rican rapper Fat Joe opened for the Vice-President, exhorting his gente: “Where’s the orgullo? Where’s the pride?” How Kamala Harris — and Joe Biden — lost to Donald Trump and left Democrats in shambles Author: Parker, Ashley, Date: 2024-11-09 Collections: Hot Takes US Elect 2024 Zotero Key: F4S3QE27 Cite Key: Parker24HowKamalaHarris Zotero Item | Lit Note Democracy Dies in Darkness How Kamala Harris — and Joe Biden — lost to Donald Trump and left Democrats in shambles The Democratic Party now finds itself grappling with how it lost so definitively, and how it so thoroughly misunderstood the American electorate. Updated November 9, 2024 14 min 11117 By Ashley Parker, Tyler Pager, Josh Dawsey and Michael Scherer When President Joe Biden was the Democratic nominee, he surrounded himself with an insular circle of longtime aides, often prompting complaints about his operation being a black box. He refused to meet with his pollsters, and many on his campaign saw ads at the same time the public did — when they first ran. “There was somewhere between never and hardly ever any real strategy conversation,” said one person familiar with the dynamic. When Vice President Kamala Harris replaced Biden in July, the dynamic flipped. Campaign chair Jen O’Malley Dillon insisted on having a larger role in the messaging process, and over several weeks in the late fall convened three “step back” messaging Zooms to strategize about how to negatively define former president Donald Trump. But the process, which included several dozen campaign officials, was unwieldy and struggled to alight on a message. Aides just fired ideas into the void — “You’d have someone say, ‘What about ‘Too Risky For Too Long?’” recounted one — and O’Malley Dillon and other senior advisers rejected the pollsters’ push for a single word: “Dangerous.” Finally, the group settled on what they privately called the “Three U’s” or the “Triple U’s” — “Unhinged, unstable and unchecked.” Yet regardless of who was at the top of the ticket and who was running the show, it was the voter who got lost in the process. The American people had been crystal clear for months, as voters in other countries had in the face of post covid inflation. By a steep margin, Americans did not approve of Biden’s presidency. By an even steeper margin they thought the country was heading in the wrong direction. They were demanding a new direction that Democrats never figured out h t ff A Harris campaign aide declined to comment on this portrait of her defeat. Following Harris’s loss Tuesday — in which Trump made gains with nearly every single demographic group, leaving him poised to potentially win all seven battleground states and the popular vote — the Democratic Party now finds itself grappling with how it lost so definitively, and how it so thoroughly misunderstood the American electorate. Democrats expect the party, donors and outside groups will eventually conduct autopsy reports to understand just how the race went awry. Donna Brazile, a Harris ally and former chair of the Democratic National Committee, said “the next step for the Democrats is deep introspection,” adding that there needs to be a process to figure out what went wrong before the party decides on next steps and who should lead it. “You don’t jump from one horse to another when you are riding on a donkey,” she said. But a consensus has already emerged that the party failed to understand the average voter and their concerns — and focused too much on Trump, according to interviews with more than two dozen campaign aides, advisers, strategists and others, many of whom spoke on the condition of anonymity to share candid opinions about the party’s loss. Like with any losing campaign, the finger-pointing and gripes have already begun to leak into public view. Many Democrats view the original sin as Biden’s decision to run for a second term, as well as his and his insular inner circle’s outright dismissal of anyone who raised alarms about his dwindling political prospects. “Joe Biden is reason one, two and three why we lost,” a Harris aide said, noting that he was “totally underwater” in the polls when Harris replaced him at the top of the ticket. Trump aides began to joke in the final weeks of the race that Biden was their best surrogate, as he briefly put on a Trump hat at one event less than 24 hours after Harris was viewed as the winner of the debate, later called for Trump to be “locked up … politically” and derisively referred to Trump supporters as “garbage” on a Zoom on the same evening Harris delivered her closing speech. “The guy was incredible,” a Trump adviser said. Some Biden loyalists, meanwhile, fault the Obama-era technocrats, who they say first sniped at Biden from the outside — hobbling his candidacy — only to join the Harris campaign and cast themselves as saviors, armed with good data but a poor understanding of American anger in this moment. And others still have cast some blame on O’Malley Dillon, who they argue was a micromanager and whose team failed to win over voters on issues they cared about most, like immigration and the economy. Some Harris allies were also alarmed when O’Malley Dillon appeared to try to engage on transition efforts. A campaign aide disputed that, arguing O’Malley Dillon only discussed necessary coordination between the campaign and transition teams. and the rejection of MAGA politics. “It’s very simple: If you try to win elections by talking to the elites of this country, you’re going to get your ass kicked — there are not enough Beyonces, Oprahs or Hollywood elites to elect anyone,” said Chris Kofinis, former chief of staff to Sen. Joe Manchin III (I-West Virginia). “Trump is not the disease. He is the symptom. The disease is political, cultural, and economic elites who keep telling the public what they should think, feel and believe — and guess what they told them on Tuesday: Go to hell.” More broadly, many Democrats view their defeat — with Trump making inroads with Latinos, first-time voters, and lower- and middle-income households, according to preliminary exit polls — not just as a series of tactical campaign blunders, but as evidence of a shattered party with a brand in shambles. Two days after the election, OpenLabs, a Democratic data firm, produced a “first look” analysis of the results, obtained by The Washington Post. They found the biggest swings away from Harris were in areas with larger populations of Asian American and Hispanic voters. They also found that counties with bigger shares of Muslim and Jewish voters also swung toward Trump. Two Trump advisers said they could not believe that they were able, for example, to beat Harris in Dearborn, Michigan. “Obviously this is a major reckoning for the Democratic Party in terms of, particularly as it relates to young men, Black and Hispanic voters and rural voters,” said Jef Pollock, a Biden and Harris campaign pollster. “If the economy were perceived by voters as swimming, things might be different. But for now, it’s clear these voters I’m talking about — particularly young men, Black men, Hispanic men, and rural White voters — do not see the Democrats as addressing their everyday needs, and that’s something we need to think about holistically.” Pollock added: “That is not a Kamala Harris thing. That is a larger thing.” Adam Jentleson, a Democratic strategist who served as chief of staff to Sen. John Fetterman (D-Pennsylvania), similarly criticized the Democratic Party for having prioritized “coalition management” — essentially kowtowing to far-left interest groups — over “the smart and effective practice of politics for many, many years.” The challenge of inflation, Jentleson said, became insurmountable in an environment where large swaths of the country had come to believe that Democrats “are preoccupied with the narrow interests of college-educated elite activists more than everyday working people.” Trump and Harris aides alike, for instance, agree that comments Harris made during the 2020 Democratic primaries — where, in the words of one Harris aide, Democrats competed for the party’s progressive base by saying “absolutely bananas stuff” — came back to haunt her as her party’s nominee four years later. Specifically, they point to her support for using tax dollars to provide gender-affirming surgeries for federal prisoners and detained immigrants, which provided the Trump campaign with one of its most devastating lines of attack against her. Trump advisers could not believe how well the ad tested. Though voters in focus groups said they were not specifically going to base their vote on that issue, it helped perpetuate the Trump campaign message that she was “dangerously liberal,” as well as out of touch with the average American. At one point, as Harris readied for her debate with Trump, her team prepared an answer on the question of transgender athletes in women’s sports, intended to cast her as more middle-of-the-road on the issue, according to one person familiar with the plan. But the moderators never asked her about the topic, and she did not choose to bring it up on her own in another forum. In addition to the broader headwinds — including inheriting a campaign built for an unpopular incumbent just over 100 days before Election Day — the Harris campaign also made some tactical blunders, according to aides and outside strategists. Chief among them, some argue, was the decision to elevate former Republican congresswoman Liz Cheney, who was ousted from her party after she became a vocal Trump critic, as a top surrogate for the vice president’s campaign. In late October, Harris and Cheney did a joint tour of Pennsylvania, Wisconsin and Michigan as part of an effort to win over Republican-leaning and independent voters, particularly women. But some aides and many Democrats outside the campaign argued the voters who found Cheney’s message appealing were already going to vote for Harris — she did very little, if anything, to move undecided voters. The campaign also seized on highly critical comments from some of Trump’s former top aides, specifically John F. Kelly, his former chief of staff, who called Trump a fascist. Some worried that backfired, particularly when Harris said during a CNN town hall that she agreed with Kelly’s assessment. Aides say it would have been better for her to separate herself from the characterization and encourage voters to listen to the people who knew Trump best. Instead, the headlines from her CNN appearance focused on her calling Trump a fascist, and in turn, her effort to paint Trump as “unhinged, unstable and unchecked” became overtaken by the fascist label. In a similar vein, Democrats, including former president Barack Obama, had reservations about Harris’s decision to deliver her closing argument speech from the Ellipse, where Trump spoke just before the Jan. 6, 2021, insurrection. Aides were adamant the speech was not centered on Trump’s actions on Jan. 6 — and in her remarks, Harris sought to connect concerns about Trump’s behavior to people’s everyday lives — but the Jan. 6 symbolism carried the day. The election results, Democrats concede, demonstrate the voters were willing to overlook Trump’s character — and the Democratic Party suffered because they focused too much on bashing Trump, and not on how they would improve voters’ lives. damaging immigration numbers to a Republican congressman. Her team was shocked when the numbers emerged as she was in transit to the border, advisers said. More broadly, some aides attributed the shift away from Harris in reliably blue states like New York, New Jersey and Illinois as a repudiation of the party’s handling of immigration. Many of those states have seen an influx of migrants, some bused by Republican governors, and Democrats in major cities have struggled to respond. When some of the major unions did not endorse Harris, it was a red flag, advisers said, not because the unions endorsements on their face would matter that much — but that leadership clearly knew their members were inclined to vote for Trump in large margins. Several advisers said the campaign did not do enough to address those concerns. In the final weeks of the race, the campaign realized they were hemorrhaging Black and Latino voters, an adviser said, partially driving Harris’s media schedule and economic plans that were meant to cater to those groups. “But it was too little, too late,” this person said. Another concern in hindsight was the campaign’s decision to spend heavily on celebrity concerts in the final stretch of the campaign, while still failing to achieve their intended goal of demonstrating widespread enthusiasm for Harris across the nation and turning out more pro-Harris voters to the polls. In some ways, the scale of the wave that returned Trump to office was so striking that Harris advisers and other Democrats argue that no gaffe by the vice president, or strategic decision by the campaign, would have ultimately changed the outcome. Democratic donors spent more on this presidential election than ever before — likely around $2 billion. They built a campaign with massive financial advantages, a much bigger volunteer footprint, many more ads and a smaller battleground than any election in recent decades. And the campaign had an impact: A Cook Political Report analysis found a three-point swing to Trump across the battlegrounds between 2020 and 2024; across the other 43 states, the swing was nearly seven points. “My main takeaway from all the post-election analysis is that hindsight is not 20/20,” said Geoff Garin, a Biden and Harris pollster. “The defining realities for this election were dissatisfaction with the economy and disapproval of President Biden and they created enormous headwinds for Vice President Harris to fight through … Harris had 107 days to introduce herself to a huge swath of the electorate and to litigate a multipart case for her election. No one in the history of American politics has been able to do that.” Once Harris took over, some aides wanted her to create distance from Biden — believing she could not win unless she drew sharp contrasts with the unpopular president. One of Harris’s biggest mistakes, advisers said, was an answer she gave on “The View,” a popular morning talk show. When asked if she had any disagreements with Biden, she said she could not think of any, immediately sparking joy among Trump’s advisers she was going to create distance, where? These were all questions the campaign grappled with — and never answered. In marketing terms, moreover, the Democrats turned out to have a brand problem that could not be overcome by repositioning the product. “We did okay on the product marketing. We moved the needle in those battleground states but the brand problems persist when people think the country is on the wrong track and are feeling the headwinds of inflation,” said Jesse Ferguson, a Democratic strategist who worked for Future Forward, the largest Democratic-leaning super PAC, and other groups. “The best field goal kicker in the world can’t get through the uprights if they are kicking from their own 20-yard line.” Defining Harris: The Case For Positive Messaging Author: Blueprint, Date: 2024-08-07T20:19:58-04:00 Collections: PollMethods, Hot Takes US Elect 2024, MediaAdsPolit Zotero Key: 67JB3DHF Cite Key: Blueprint24pollPosHarrisCase Zotero Item | Lit Note August 8, 2024 A new Blueprint poll shows that voters are ready for and receptive to informative, positive pro-Harris messaging and that the most effective communication approaches foreground Harris’s biography, record, and accomplishments, particularly her prosecutorial background, economic populism, and commitment to immigration reform. Voters are less persuaded by anti-Trump or anti-Republican messages, and more responsive to information that helps them fill in the blanks on their perceptions of Harris, which remain malleable. This holds true for Democrats’ most powerful and effective rebuttals to anti-Harris attacks, where the best pushbacks highlight her record and accomplishments as a prosecutor and vice president rather than repeating traditional anti-Trump attacks. Independents are especially responsive to a Harris-forward messaging approach. The results show that voters are eager for fresh messaging in this race–voter perceptions of Donald Trump are mostly solidified, so their appetite for Trump-centric content is limited. In fact, 71% of voters say their minds are made up about Trump and that there is absolutely nothing that could change their opinion about him. Just 57% say the same about Vice President Harris, with 20% saying either that they’re open to changing their minds about her or that they don’t really have any views on her and need to learn more. To derive an effective definition of Harris on the trail, Blueprint used a “Mad Libs” style conjoint test, plugging in different permutations of Harris’s biographical facts, past accomplishments, and issue focuses to see what voters preferred the most. Overall, Harris’s depth of accomplishment and personal biographical information are appealing to voters, including independents, and the campaign is correct to lean into her record as a prosecutor and her commitment to protecting freedoms like reproductive choice and policies like Social Security and Medicare. In terms of an economic message, voters prefer one that is laser-focused on keeping the prices of goods and services low and those that champion policies to help working and middle-class families—like the message Harris has debuted at rallies in Georgia and Texas. The findings indicate that this is a message that will land with voters and that they will believe, with voters blaming Harris far less than President Biden for inflation, trusting her more than Biden on nearly every issue, and wanting to hear more about Harris and her policies than about Trump. HARRIS CAN SUCCESSFULLY REBUT REPUBLICAN ATTACKS Blueprint tested common Republican arguments against Vice President Harris, including attacks on her immigration record, the economy, her family, her legitimacy as a candidate, being a liberal from San Francisco, and the DEI attack. The poll then asked voters about the convincingness of various rebuttals to these attacks, finding that the most effective responses are ones that immediately highlight Vice President Harris’ background and record of accomplishments rather than ones that call out Republican lies and use traditional anti-Trump messaging. The effectiveness of the attacks and the rebuttals are scored using a metric that represents the combined appeal of the message with voters overall, with independent voters, and the intensity of that appeal (“very convincing”). Initially, voters were asked whether they found the attacks and rebuttals “very convincing”, “somewhat convincing”, “not very convincing”, or “not at all convincing”. The final score represents the total margin + independent margin + total “very convincing” + independent “very convincing.” Republican attacks that focus on immigration and the economy score the best, while the DEI and family arguments score the worst. Immigration Rebuttal The most convincing rebuttal to the GOP attack that focuses on Harris’s role as the so-called “border czar” and blames her for increased illegal immigration is one that immediately pivots to Vice President Harris’ background as a child of immigrants and her understanding of the need for a better system. This rebuttal couches a more moderate policy argument for immigration reform and legal immigration in more liberal language about America as a nation of immigrants, while drawing a biographical thruline. Another effective rebuttal, which pairs well with the above, highlights Harris’s credentials as a prosecutor, namely her work to take on transnational gangs and drug smugglers, and her support for the bipartisan legislation that takes border security seriously, and gives the Border Patrol more federal funding. A combination of the top two testing rebuttals holds the key to how the Harris campaign can defuse Republicans’ number one attack and win over independents: immigration positioning that meets the electorate’s desire for border security and immigration reform, set against personal, empathetic language around immigrants themselves. A rebuttal that points to Republican lies and credits a recent downturn in border crossings to Vice President Harris’s work to address the root causes of forced migration in Central America is significantly less effective. Economic Rebuttal On the economy, where Republicans have already begun attempting to tie Vice President Harris to inflation and rising cost of living under the Biden administration, the most convincing Democratic rebuttal is one that highlights Harris as a champion of working families and the middle class who has fought to cap insulin prices, cut junk fees, and lower costs for everyday Americans. This effective rebuttal also explains her accomplishments as California Attorney General taking on big corporate interests and standing up against companies that price gouge. The least effective rebuttal tested is one that calls out Republican lies and insists that the data is more positive than people’s perceptions of the economy. Legitimacy As a Candidate Rebuttal When Republicans attack Harris’s legitimacy as a candidate, specifically by claiming she was chosen by party insiders and is undermining the will of 14 million primary voters, the most convincing rebuttal emphasizes that Kamala Harris is the vice president and that it is perfectly legitimate for the second-in-command to step in for the president. The two top testing messages differ from the third in that they center Vice President Harris’s record rather than President Biden’s decision. DEI Rebuttal Vice President Harris’s most effective retort to the increasingly common but woefully ineffective “DEI hire” attack is to immediately point out Trump’s and the GOP’s desperate attempts to distract from Project 2025 and their efforts to divide Americans in order to give tax cuts to the rich. Another rebuttal that scores not far behind is one that highlights Vice President Harris’s experience as Attorney General of California, a United States Senator, and Vice President and her accomplishments in every role. The lowest-scoring rebuttal frames the DEI attack as racist and calls out Republican prejudice and bigotry. Family Rebuttal Republican attacks that go after Vice President Harris for not having biological children, framing her life choices as a pursuit of power over family values and her inability to relate to ordinary Americans, can most effectively be met with a counter-attack that calls out Trump’s three marriages and history of cheating. It pivots to Harris’s support of popular policies like protecting reproductive rights, while strongly asserting that policy outcomes for working parents are what makes a candidate pro-family. This is the highest-scoring rebuttal we tested overall, and it is the only case where directly calling out Republicans and using anti-Trump attacks are more effective than pivoting to Vice President Harris’s record. San Francisco Liberal Rebuttal The most effective rebuttal against Republican arguments that attack Vice President Harris as a San Francisco liberal who supports radically liberal policies, pivots to her prosecutorial background and effective record as District Attorney and Attorney General of California, where she dedicated her career to upholding the law, putting criminals behind bars, and keeping Americans safe. The rebuttal that calls out Republican fear-mongering and lies scores remarkably low. DEFINING HARRIS: COMBINING BIOGRAPHY, ACCOMPLISHMENTS, AND ISSUES In a statistical exercise called a conjoint analysis, similar to a “Mad Libs,” we tested various permutations of statements about Vice President Harris, swapping in biographic facts, past accomplishments, and campaign issue focuses to see what voters prefer the most and what an appealing definition of her will be on the trail. By pulling in the most popular component of her personal background, the most popular political accomplishment, and the most popular issue position, the three top statements to define Vice President Harris are: • Kamala Harris was the Attorney General of California. She prosecuted sex traffickers and other men who abused women, putting them behind bars. She is running on protecting Social Security and Medicare.* • Kamala Harris was a tough-on-crime prosecutor. She secured a $1.1 billion settlement for students who were taken advantage of by a predatory for-profit education company. She is running on keeping the prices of goods and services low.* • Kamala Harris is a groundbreaking lawmaker and public servant. She sued BP for the Deepwater Horizon oil spill and forced them to fund environmental restoration projects. She is running on maintaining protections for those with pre-existing health conditions.* Harris’s prosecutorial background provides the biggest boost out of the biographical facts we tested, and her prosecutorial accomplishments against criminals as well as big businesses lead her past political accomplishments. On issue positions, the presence of “protecting Social Security and Medicare,” gave any statement in which it was included a 15% boost. “Keeping the prices of goods and services low” earned a 9% boost, and “maintaining protections for those with pre-existing health conditions” earned a 6% boost. VOTER ATTITUDES TOWARD HARRIS ARE LESS CALCIFIED THAN TOWARD TRUMP The poll finds Harris even on overall favorability: 46% of voters view her favorably, and 46% view her unfavorably—a major improvement compared to how voters viewed her before Biden exited the race. Trump is viewed favorably by 43% of voters and unfavorably by 52%. The poll also shows that voter attitudes about Harris are less calcified than their perception of Trump. We tested this by adding “extremely” favorable and “extremely” unfavorable options to our favorability grid (in addition to the typical “somewhat” and “very” options), as well as the options to choose “neutral” and “haven’t heard enough to say.” 64% of voters say they have either an “extremely” favorable (27%) or unfavorable (37%) opinion of Trump, showing how dug in many voters are. Just 3% say they are neutral on him, and only 1% say they haven’t heard enough to say. On Harris, a smaller share of voters—53%—say they have either an “extremely” favorable (24%) or unfavorable (29%) opinion. 5% say they are neutral on her, and 3% say they haven’t heard enough to say. That means that about one in 10 voters are completely dug-in on their views of Trump but not of Harris. In a similar finding, we asked voters about the extent to which they’ve already made up their minds about the candidates. Of Trump, 71% say: “My mind is made up; there is absolutely nothing that could change my opinion about” him. Much fewer, 57%, say the same of Harris. Meanwhile, 20% of voters say either that they’re “open to changing [their] mind” on Harris (12%) or that they “don’t really have any views” on her and “need to learn more” (8%). Just 7% of voters say they’re open to changing their mind on Trump, and only 5% say they don’t really have any views on him and need to learn more. Additionally, the poll tested Democratic party messages related to Biden stepping down and Harris coming into the spotlight. The message that tested the best with both overall voters and, notably, those who didn’t vote in 2020, focused on a breath of fresh air: • “Isn’t it time for someone new? For the last four decades, the Clintons, Bushes, Trumps, and Bidens have been running everything. Now, we have an opportunity to give someone new a chance to shake things up. Kamala Harris doesn’t come from a wealthy political family. She worked her way to the top by fighting for middle-class families. It’s time for a breath of fresh air.”* Overall, there is significant room for voters to change—or even just form—their opinions of Harris; this is much less true of Trump. VOTERS BLAME HARRIS LESS THAN BIDEN FOR RISING PRICES; TRUST HER MORE ON ALL ISSUES TESTED The poll presented voters with a list of aspects of the Biden administration and asked them who—Biden or Harris—they believe is more responsible for each. The policies most attributed to Biden are providing military aid to Ukraine (50% say Biden is “more” or “entirely” responsible, 5% say Harris), the withdrawal from Afghanistan (51% Biden, 7% Harris), providing military aid to Israel (48% Biden, 7% Harris), student debt cancellation (49% Biden, 10% Harris), capping the cost of insulin (41% Biden, 12% Harris), and rising prices on goods and services (35% Biden, 7% Harris). The policies most attributed to Harris include policies to increase access to abortion (18% Biden, 25% Harris), the reduction in crime (22% Biden, 14% Harris), passing the Respect for Marriage Act (24% Biden, 15% Harris), the reduction in border crossings (27% Biden, 17% Harris), and passing the CHIPS and Science Act (25% Biden, 11% Harris). While rising prices on goods and services is net +28 Biden (35% Biden, 5% Harris), lowering prices on goods and services is net +16 Biden (27% Biden, 11% Harris), suggesting that Harris gets less blame for prices rising and relatively more credit for reducing them. When asked who they trust more—Biden or Harris—on a range of issues, Harris is more trusted on all, with her strongest being reproductive rights (72% to 28%), protecting LGBT rights (70% to 30%), reducing racial tension in the country (65% to 35%), having good policies on police and crime (62% to 38%) reducing gun violence (61% to 39%), ensuring a better future (61% to 39%), immigration (60% to 40%), and keeping rent and housing costs affordable (60% to 40%). Harris leads Biden by 14 points on bringing down the prices on goods and services and by 12 points on securing the border. While there is no issue where more voters trust Biden more than Harris, her weakest issues are protecting Social Security (+2), preventing war and conflict (+3), creating jobs for everyone who needs one (+4), reducing the deficit (+4), and lowering gas prices (+4). ABOUT THE POLL Blueprint surveyed an online sample of 2,245 voters on July 28 and July 29, weighted to education, gender, race, survey engagement, and 2020 election results. The survey was conducted in English, and its margin of error is ±2.4 percentage points. Full toplines can be found here and crosstabs upon request. PRESS CONTACTS Alyssa Cass alyssa@slingshotstrat.com 347-992-5006 Evan Roth Smith evan@slingshotstrat.com 646-240-0096 Follow us on X for more information and regular polling updates We Looked at 78 Election Deepfakes. Political Misinformation is not an AI Problem. Author: Kapoor, Sayash, Date: 2023-03-20 Collections: Generative AI, MediaAdsPolit, PoliticalML, MisDisinformation Zotero Key: 4GLF4WPG Cite Key: Kapoor23misinfoAInotTheProb Zotero Item | Lit Note AI-generated misinformation was one of the top concerns during the 2024 U.S. presidential election. In January 2024, the World Economic Forum claimed that “misinformation and disinformation is the most severe short-term risk the world faces” and that “AI is amplifying manipulated and distorted information that could destabilize societies.” News headlines about elections in 2024 tell a similar story: In contrast, in our past writing, we predicted that AI would not lead to a misinformation apocalypse. When Meta released its open-weight large language model (called LLaMA), we argued that it would not lead to a tidal wave of misinformation. And in a follow-up essay, we pointed out that the distributionof misinformation is the key bottleneck for influence operations, and while generative AI reduces the cost of creating misinformation, it does not reduce the cost of distributing it. A few other researchers have made similar arguments. Which of these two perspectives better fits the facts? Fortunately, we have the evidence of AI use in elections that took place around the globe in 2024 to help answer this question. Many news outlets and research projects have compiled known instances of AI-generated text and media and their impact. Instead of speculating about AI’s potential, we can look at its real-world impact to date. We analyzed every instance of AI use in elections collected by the WIRED AI Elections Project, which tracked known uses of AI for creating political content during elections taking place in 2024 worldwide. In each case, we identified what AI was used for and estimated the cost of creating similar content without AI. We find that (1) most AI use isn't deceptive, (2) deceptive content produced using AI is nevertheless cheap to replicate without AI, and (3) focusing on the demand for misinformation rather than the supply is a much more effective way to diagnose problems and identify interventions. To be clear, AI-generated synthetic content poses many real dangers: the creation of non-consensual images of people and child sexual abuse material and the enabling of the liar’s dividend, which allows those in power to brush away real but embarrassing or controversial media content about them as AI-generated. These are all important challenges. This essay is focused on a different problem: political misinformation. Improving the information environment is a difficult and ongoing challenge. It’s understandable why people might think AI is making the problem worse: AI does make it possible to fabricate false content. But that has not fundamentally changed the landscape of political misinformation. Paradoxically, the alarm about AI might be comforting because it positions concerns about the information environment as a discrete problem with a discrete solution. But fixes to the information environment depend on structural and institutional changes rather than on curbing AI-generated content. We analyzed all 78 instances of AI use in the WIRED AI Elections Project (source for our analysis). We categorized each instance based on whether there was deceptive intent. For example, if AI was used to generate false media depicting a political candidate saying something they didn't, we classified it as deceptive. On the other hand, if a chatbot gave an incorrect response to a genuine user query, a deepfake was created for parody or satire, or a candidate transparently used AI to improve their campaigning materials (such as by translating a speech into a language they don't speak), we classify it as non-deceptive. To our surprise, there was no deceptive intent in 39 of the 78 cases in the database. The most common non-deceptive use of AI was for campaigning. When candidates or supporters used AI for campaigning, in most cases (19 out of 22), the apparent intent was to improve campaigning materials rather than mislead voters with false information. We even found examples of deepfakes that we think helped improve the information environment. In Venezuela, journalists used AI avatars to avoid government retribution when covering news adversarial to the government. In the U.S., a local news organization from Arizona, Arizona Agenda, used deepfakes to educate viewers about how easy it is to manipulate videos. In California, a candidate with laryngitis lost his voice, so he transparently used AI voice cloning to read out typed messages in his voice during meet-and-greets with voters. Reasonable people can disagree on whether using AI in campaigning materials is legitimate or what the appropriate guardrails need to be. But using AI for campaign materials in non-deceptive ways (for example, when AI is used as a tool to improve voter outreach) is much less problematic than deploying AI-generated fake news to sway voters. Of course, not all non-deceptive AI-generated political content is benign. Chatbots often incorrectly answer election-related questions. Rather than deceptive intent, this results from the limitations of chatbots, such as hallucinations and lack of factuality. Unfortunately, these limitations are not made clear to users, leading to an overreliance on flawed large language models (LLMs). For each of the 39 examples of deceptive intent, where AI use was intended to make viewers believe outright false information, we estimated the cost of creating similar content without AI—for example, by hiring Photoshop experts, video editors, or voice actors. In each case, the cost of creating similar content without AI was modest—no more than a few hundred dollars. (We even found that a video involving a hired stage actor was incorrectly marked as being AI-generated in WIRED’s election database.) In fact, it has long been possible to create media with outright false information without using AI or other fancy tools. One video used stage actors to falsely claim that U.S. Vice President and Democratic presidential candidate Kamala Harris was involved in a hit-and-run incident. Another slowed down the vice president's speech to make it sound like she was slurring her words. An edited video of Indian opposition candidate Rahul Gandhi showed him saying that the incumbent Narendra Modi would win the election. In the original video, Gandhi said his opponent would not win the election, but it was edited using jump cuts to take out the word “not.” Such media content has been called “cheap fakes” (as opposed to AI-generated “deepfakes”). There were many instances of cheap fakes used in the 2024 U.S. election. The News Literacy Project documented known misinformation about the election and found that cheap fakes were used seven times more often than AI-generated content. Similarly, in other countries, cheap fakes were quite prevalent. An India-based fact checker reviewed an order of magnitude more cheap fakes and traditionally edited media compared to deepfakes. In Bangladesh, cheap fakes were over 20 times more prevalent than deepfakes. Let’s consider two examples to analyze how cheap fakes could have led to substantially similar effects as the deepfakes that got a lot of media attention: Donald Trump’s use of Taylor Swift deepfakes to campaign and a voice-cloned robocall that imitated U.S. President Joe Biden in the New Hampshire primary asking voters not to vote. A Truth Social post shared by Donald Trump with images of Taylor Swift fans wearing “Swifties for Trump” t-shirts. Top left: A post with many AI-generated images of women wearing “Swifties for Trump” t-shirts, with a “satire” label. Top right: A real image of Trump supporter Jenna Piwowarczyk wearing a “Swifties for Trump” t-shirt. Bottom left: A fabricated image of Taylor Swift in front of the American flag with the caption, “Taylor wants you to vote for Donald Trump.” It is unclear if the image was created using AI or other editing software. Bottom right: A Twitter post with two images: one AI-generated, the other real, of women wearing “Swifties for Trump” t-shirts. Trump’s use of Swift deepfakes implied that Taylor Swift had endorsed him and that Swift fans were attending his rallies en masse. In the wake of the post, many media outlets blamed AI for the spread of misinformation. But recreating similar images without AI is easy. Images depicting Swift’s support could be created by photoshopping text endorsing Trump onto any of her existing images. Likewise, getting images of Trump supporters wearing “Swifties for Trump” t-shirts could be achieved by distributing free t-shirts at a rally—or even selectively reaching out to Swift fans at Trump rallies. In fact, two of the images Trump shared were real images of a Trump supporter who is also a Swift fan. Another incident that led to a brief panic was an AI clone of President Joe Biden’s voice that asked people not to vote in the New Hampshire primary. News headlines in the wake of the Biden robocall. Rules against such robocalls have existed for years. In fact, the perpetrator of this particular robocall was fined $6 million by the Federal Communications Commission (FCC). The FCC has tiplines to report similar attacks, and it enforces rules around robocalls frequently, regardless of whether AI is used. Since the robocall used a static recording, it could have been made about as easily without using AI—for instance, by hiring voice impersonators. It is also unclear what impact the robocall had: The efficacy of the deepfake depends on the recipient believing that the president of the United States is personally calling them on the phone to ask them not to vote in a primary. Is it just a matter of time until improvements in technology and the expertise of actors seeking to influence elections lead to more effective AI disinformation? We don’t think so. In the next section, we point out that structural reasons that drive the demand for misinformation are not aided by AI. We then look at the history of predictions about coming waves of AI disinformation that have accompanied the release of new tools—predictions that have not come to pass. Misinformation can be seen through the forces of supply and demand. The supply comes from people who want to make a buck by generating clicks, partisans who want their side to win, or state actors who want to conduct influence operations. Interventions so far have almost entirely tried to curb the supply of misinformation while leaving the demand unchanged. The focus on AI is the latest example of this trend. Since AI reduces the cost of generating misinformation to nearly zero, analysts who look at misinformation as a supply problem are very concerned. But analyzing the demand for misinformation can clarify how misinformation spreads and what interventions are likely to help. Looking at the demand for misinformation tells us that as long as people have certain worldviews, they will seek out and find information consistent with those views. Depending on what someone’s worldview is, the information in question is often misinformation—or at least would be considered misinformation by those with differing worldviews. In other words, successful misinformation operations target in-group members—people who already agree with the broad intent of the message. Such recipients may have lower skepticism for messages that conform to their worldviews and may even be willing to knowingly amplify false information. Sophisticated tools aren’t needed for misinformation to be effective in this context. On the flip side, it will be extremely hard to convince out-group members of false information that they don't agree with, regardless of AI use. Seen in this light, AI misinformation plays a very different role from its popular depiction of swaying voters in elections. Increasing the supply of misinformation does not meaningfully change the dynamics of the demand for misinformation since the increased supply is competing for the same eyeballs.Moreover, the increased supply of misinformation is likely to be consumed mainly by a small group of partisans who already agree with it and heavily consume misinformation rather than to convince a broader swath of the public. This also explains why cheap fakes such as media from unrelated events, traditional video edits such as jump cuts, or even video game footage can be effective for propagating misinformation despite their low quality: It is much easier to convince someone of misinformation if they already agree with its message. Our analysis of the demand for misinformation may be most applicable to countries with polarized close races where leading parties have similar capacities for voter outreach, so that voters’ (mis)information demands are already saturated. Still, to our knowledge, in every country that held elections in 2024 so far, AI misinformation had much less impact than feared. In India, deepfakes were used for trolling more than spreading false information. In Indonesia, the impact of AI wasn't to sow false information but rather to soften the image of then-candidate, now-President Prabowo Subianto (a former general accused of many past human rights abuses) using AI-generated digital cartoon avatars that depicted him as likable. The 2024 election cycle wasn’t the first time when there was widespread fear that AI deepfakes would lead to rampant political misinformation. Strikingly similar concerns about AI were expressed before the 2020 U.S. election, though these concerns were not borne out. The release of new AI tools is often accompanied by worries that it will unleash new waves of misinformation: • 2019. When OpenAI released its GPT-2 series of models in 2019, one of the main reasons it held back on releasing the model weights for the most capable models in the series was its alleged potential to generate misinformation. • 2023. When Meta released the LLaMA model openly in 2023, multiple news outlets reported concerns that it would trigger a deluge of AI misinformation. These models were far more powerful than the GPT-2 models released by OpenAI in 2019. Yet, we have not seen evidence of large-scale voter persuasion attributed to using LLaMA or other large language models. • 2024. Most recently, the widespread availability of AI image editing tools on smartphones has prompted similar concerns. In fact, concerns about using new technology to create false information go back over a century. The late 19th and early 20th centuries saw the advent of technologies for photo retouching. This was accompanied by concerns that retouched photographs would be used to deceive people, and, in 1912, a bill was introduced in the U.S. that would have criminalized photo editing without subjects’ consent. (It died in the Senate.) Thinking of political misinformation as a technological (or AI) problem is appealing because it makes the solution seem tractable. If only we could roll back harmful tech, we could drastically improve the information environment! While the goal of improving the information environment is laudable, blaming technology is not a fix. Political polarization has led to greater mistrust of the media. People prefer sources that confirm their worldview and are less skeptical about content that fits their worldview. Another major factor is the drastic decline of journalism revenues in the last two decades—largely driven by the shift from traditional to social media and online advertising. But this is more a result of structural changes in how people seek out and consume information than the specific threat of misinformation shared online. As history professor Sam Lebovic has pointed out, improving the information environment is inextricably linked to the larger project of shoring up democracy and its institutions. There’s no quick technical fix, or targeted regulation, that can “solve” our information problems. We should reject the simplistic temptation to blame AI for political misinformation and confront the gravity of the hard problem. This essay is cross-posted to the Knight First Amendment Institute website. We are grateful to Katy Glenn Bass for her feedback. The Media’s Identity Crisis Author: Warzel, Charlie, Date: 2024-11-08 Collections: Hot Takes US Elect 2024 Zotero Key: BKHANTZ3 Cite Key: Warzel24MediasIdentityCrisis Zotero Item | Lit Note “You are the media now.” That’s the message that began to cohere among right-wing influencers shortly after Donald Trump won the election this week. Elon Musk first posted the phrase, and others followed. “The legacy media is dead. Hollywood is done. Truth telling is in. No more complaining about the media,” the right-wing activist James O’Keefe posted shortly after. “You are the media.” It’s a particularly effective message for Musk, who spent $44 billion to purchase a communications platform that he has harnessed to undermine existing media institutions and directly support Trump’s campaign. QAnon devotees also know the phrase as a rallying cry, an invitation to participate in a particular kind of citizen “journalism” that involves just asking questions and making stuff up altogether. “You are the media now” is also a good message because, well, it might be true. A defining quality of this election cycle has been that few people seem to be able to agree on who constitutes “the media,” what their role ought to be, or even how much influence they have in 2024. Based on Trump and Kamala Harris’s appearances on various shows—and especially Trump and J. D. Vance’s late-race interviews with Joe Rogan, which culminated in the popular host’s endorsement—some have argued that this was the “podcast election.” But there’s broad confusion over what actually moves the needle. Is the press the bulwark against fascism, or is it ignored by a meaningful percentage of the country? It is certainly beleaguered by a conservative effort to undermine media institutions, with Trump as its champion and the fracturing caused by algorithmic social media. It can feel existential at times competing for attention and reckoning with the truth that many Americans don’t read, trust, or really care all that much about what papers, magazines, or cable news have to say. All of this contributes to a well-documented, slow-moving crisis of legacy media—a cocktail whose ingredients also include declining trust, bad economics, political pressure, vulture capitalists, the rise of the internet, and no shortage of coverage decisions from mainstream institutions that have alienated or infuriated some portion of their audiences. Each and every one of these things affected how Americans experienced this election, though it is impossible to say what the impact is in aggregate. If “you are the media,” then there is no longer a consensus reality informed by what audiences see and hear: Everyone chooses their own adventure. Read: The great social-media news collapse The confusion felt most palpable in the days following Joe Biden’s disastrous debate performance in June. I noticed conflicting complaints from liberals online: Some argued that until that point, the media had failed to cover Biden’s age out of fear of crossing some editorial redline, while others said the media were now recklessly engaged in a coordinated effort to oust the president, shamefully crusading against his age. Then, Biden’s administration leveled its own critique: “I want you to ask yourself, what have these people been right about lately?” it wrote in an email. “Seriously. Think about it.” Everyone seemed frustrated for understandable reasons. But there was no coherence to be found in this moment: The media were either powerful and incompetent or naive and irrelevant … or somehow both. The vibe felt similar around The Washington Post’s decision not to endorse Harris in the final weeks of the race after the paper’s owner, Jeff Bezos, intervened and shut the effort down. Readers were outraged by the notion that one of the world’s richest men was capitulating to Trump: The paper reportedly lost at least 250,000 subscribers, or 10 percent of its digital base, in just a handful of days following the decision. But even that signal was fuzzy. The endorsement was never going to change the election’s outcome. As many people, including Bezos himself, argued, newspaper endorsements don’t matter. The writer Max Read noted that Bezos’s intervention was its own indicator of the Post’s waning relevance. “As a journalist, you don’t actually want your publication to be used as a political weapon for a billionaire,” Read wrote. “But it would be nice for your publication to be so powerful and unavoidable that a billionaire might try.” This tension was everywhere throughout campaign season: Media institutions were somehow failing to meet the moment, but it was also unclear if they still had any meaningful power to shape outcomes at all. I’ve watched for the past year with grim fascination as both the media industry and its audience have sparred and tried to come to some shared understanding of what the hell is going on. The internet destroyed monoculture years ago, but as I wrote last December, it’s recently felt harder to know what anyone else is doing, seeing, or hearing online anymore. News sites everywhere have seen traffic plummet in the past two years. That’s partly the fault of technology companies and their algorithmic changes, which have made people less likely to see or click on articles when using products like Google Search or Facebook. But research suggests that isn’t the entire story: Audiences are breaking up with news, too. An influencer economy has emerged on social-media platforms. It’s not an ecosystem that produces tons of original reporting, but it feels authentic to its audience. Traditional journalism operates with a different playbook, typically centered on strong ethical norms and a spirit of objectivity; the facts are meant to anchor the story, even where commentary is concerned. This has presented challenges in the Trump era, which has produced genuine debates about whether traditional objectivity is possible or useful. Some audiences crave obvious resistance against the Republican regime. Outlets such as the The New York Times have tried to forge a middle path—to be, in executive editor Joe Kahn’s words, a “nonpartisan source of information” that occupies a “neutral middle ground” without devolving into “both-sides journalism.” This has had the unfortunate effect of downplaying the asymmetries between candidates and putting detached, clinical language onto politics that feel primal and urgent. When it comes to covering Trump, critics of the Times see double standards and a “sanewashing” of his alarming behavior. Independent online creators aren’t encumbered by any of this hand-wringing over objectivity or standards: They are concerned with publishing as much as they can, in order to cultivate audiences and build relationships with them. For them, posting is a volume game. It’s also about working ideas out in public. Creators post and figure it out later; if they make mistakes, they post through it. Eventually people forget. When I covered the rise of the less professionalized pro-Trump media in 2016, what felt notable to me was its allergy to editing. These people livestreamed and published unpolished three-hour podcasts. It’s easier to build a relationship with people when you’re in their ears 15 hours a week: Letting it all hang out can feel more authentic, like you have nothing to hide. Critics can debate whether this kind of content is capital-J Journalism until the heat death of the universe, but the undeniable truth is that people, glued to their devices, like to consume information when it’s informally presented via parasocial relationships with influencers. They enjoy frenetic, algorithmically curated short-form video, streaming and long-form audio, and the feeling that only a slight gap separates creator and consumer. Major media outlets are trying to respond to this shift: The Times’ online front page, for example, has started to feature reporters in what amounts to prestige TikToks. Yet the influencer model is also deeply exploitable. One of the most aggressive attempts to interfere in this election didn’t come directly from operators in Russia, but rather from a legion of useful idiots in the United States. Russia simply used far-right influencers to do their bidding with the large audiences they’d already acquired. Read: YouTubers are almost too easy to dupe Watching this from inside the media, I’ve experienced two contradicting feelings. First is a kind of powerlessness from working in an industry with waning influence amid shifting consumption patterns. The second is the notion that the craft, rigor, and mission of traditional journalism matter more than ever. Recently I was struck by a line from the Times’ Ezra Klein. “The media doesn’t actually set the agenda the way people sometimes pretend that it does,” he said late last month. “The audience knows what it believes. If you are describing something they don’t really feel is true, they read it, and they move on. Or they don’t read it at all.” Audiences vote with their attention, and that attention is the most important currency for media businesses, which, after all, need people to care enough to scroll past ads and pony up for subscriptions. It is terribly difficult to make people care about things they don’t already have an interest in—especially if you haven’t nurtured the trust necessary to lead your audience. As a result, news organizations frequently take cues from what they perceive people will be interested in. This often means covering people who already attract a lot of attention, under the guise of newsworthiness. (Trump and Musk are great examples of people who have sufficiently hijacked this system.) This is why there can be a herding effect in coverage. Numerous media critics and theorists on Threads and Bluesky, themselves subject to the incentives of the attention economy, balked at Klein’s perspective, citing historical social-science research that media organizations absolutely influence political metanarratives. They’re right, too. When the press coheres around a narrative that also manages to capture the public’s attention, it can have great influence. But these people weren’t just disagreeing with Klein: They were angry with him. “Another one of those ‘we’re just a smol bean national paper of record’ excuses when part of the issue was how they made Biden’s age the top story day after day after day,” one historian posted. These arguments over media influence—specifically the Times’—occurred frequently on social media throughout the election cycle, and occasionally, a reporter would offer a rebuttal. “To think The Times has influence with Trump voters or even swing voters is to fundamentally misunderstand the electorate,” the Times political reporter Jonathan Weisman posted in October. “And don’t say The Times influences other outlets that do reach those voters. It’s not true.” The argument is meant to suggest that newspaper coverage alone cannot stop a popular authoritarian movement. At the same time, these defenses inevitably led critics to argue: Do you think what you do matters or not? In a very real sense, these are all problems that the media created for itself. As Semafor’s Ben Smith argued last month, discussing the period following Trump’s 2016 win, “a whole generation of non-profit and for-profit newsrooms held out their hands to an audience that wanted to support a cause, not just to purchase a service.” These companies sold democracy itself and a vision of holding Trump’s power to account. “The thing with marketing, though,” Smith continued, “is that you eventually have to deliver what you sold.” Trump’s win this week may very well be the proof that critics and beleaguered citizens need to stop writing those checks. A subscription falloff would also highlight the confusing logic of this era for the media. It would mean that the traditional media industry—fractured, poorly funded, constantly under attack, and in competition with attention gatherers who don’t have to play by the same rules—is simultaneously viewed as having had enough power to stop Trump, but also past its prime, having lost its sway and relevance. Competition is coming from a durable alternative-media ecosystem, the sole purpose of which is to ensconce citizens in their chosen reality, regardless of whether it’s true. And it is coming from Musk’s X, which the centibillionaire quickly rebuilt into a powerful communication tool that largely serves the MAGA coalition. Read: I’m running out of ways to explain how bad this is Spaces like X offer an environment for toxic ideas paired with a sense of empowerment for disaffected audiences. This is part of what Kate Starbird, a professor at the University of Washington, calls the right’s “powerful, partisan, & participatory media environment to support its messaging, which offers a compelling ‘deep story’ for its participants.” By contrast, the left’s media ecosystem, she argues, relies “upon rigid, self-preserving institutional media and its ‘story’ is little more than a defense of imperfect institutions.” The right’s media ecosystem might be chaotic, conspiracist, and poisonous, but it offers its consumers a world to get absorbed in—plus, the promise that they can shape it themselves. Would it have been possible for things to go differently if Harris had attempted to tap into this alternative ecosystem? I’m not so sure. Following Harris’s entrance into the race, each passing week felt more consequential, but more rigidly locked in place. Memes, rallies, and marathon podcast appearances from Trump offered data points, but there was no real way to interpret them. Some Zoomers and Millennials were ironically coconut-pilled; people were leaving Trump rallies early; everyone was arguing about who was actually garbage. Even when something seemed to matter, it was hard to tell whom it mattered to, or what might happen because of it. When it’s unclear what information everyone is consuming or which filter bubble they’re trapped in, everyone tends to shadowbox their conception of an imagined audience. Will the Rogan bros vote? Did a stand-up comedian’s insult activate a groundswell of Puerto-Rican American support? We didn’t really know anything for certain until we did. “You are the media now” is powerful because it capitalizes on the reality that it is difficult to know where genuine influence comes from these days. The phrase sounds empowering. Musk’s acolytes see it as the end of traditional-media gatekeeping. But what he’s really selling is the notion that people are on their own—that facts are malleable, and that what feels true ought to be true. A world governed by the phrase do your own research is also a world where the Trumps and Musks can operate with impunity. Is it the news media’s job to counter this movement—its lies, its hate? Is it also their job to appeal to some of the types of people who listen to Joe Rogan? I’d argue that it is. But there’s little evidence right now that it stands much of a chance. Something has to change. Perhaps it’s possible to appropriate “You are the media now” and use it as a mission statement to build an industry more capable of meeting whatever’s coming. Perhaps in the absence of a shared reality, fighting against an opposing information ecosystem isn’t as effective as giving more people a reason to get excited about, and pay attention to, yours. Trump Is About to Betray His Rural Supporters Author: Brownstein, Ronald, Date: 2024-12-13 Collections: Hot Takes US Elect 2024 Zotero Key: VMHEWARR Cite Key: Brownstein24trumpBetrayRural Zotero Item | Lit Note This article was featured in the One Story to Read Today newsletter. Sign up for it here. Donald Trump’s support in rural America appears to have virtually no ceiling. In last month’s election, Trump won country communities by even larger margins than he did in his 2020 and 2016 presidential runs. But several core second-term policies that Trump and the Republican Congress have championed could disproportionately harm those places. Agricultural producers could face worse losses than any other economic sector from Trump’s plans to impose sweeping tariffs on imports and to undertake what he frequently has called “the largest domestic deportation operation” of undocumented immigrants “in American history.” Hospitals and other health providers in rural areas could face the greatest strain from proposals Trump has embraced to slash spending on Medicaid, which provides coverage to a greater share of adults in smaller communities than in large metropolitan areas. And small-town public schools would likely be destabilized even more than urban school districts if Trump succeeds in his pledge to expand “school choice” by providing parents with vouchers to send their kids to private schools. Resistance to such measures in deep-red rural areas could represent one of the few obstacles Trump would face from a GOP-controlled Congress over implementing his agenda. Still, the most likely scenario is that elected Republicans who represent rural areas will ultimately fall in line with Trump’s blueprint. If so, the effects will test whether anything can loosen the GOP’s grip on small-town America during the Trump era, or whether the fervor of his rural supporters provides Trump nearly unlimited leeway to work against their economic interests without paying any political price. “I don’t think [the Trump agenda] is going to lead to a dramatic reversal of these partisan shifts, because the truth is that the disdain for the Democratic Party is decades in the making and deep in rural America,” Nicholas Jacobs, a political scientist at Colby College and the author of the 2023 book The Rural Voter, told me. But if Trump acts on the policies he campaigned on, Jacobs added, “it’s hard to imagine that rural [places] will not suffer and will not hurt, and it’s hard to imagine that rural will not respond.” Read: Let us now praise undecided voters Trump’s support in rural places reached imposing proportions in last month’s election, with gains even in heavily Latino rural counties in the Southwest and some Black rural areas in the Southeast. The nonpartisan Center for Rural Studies has developed a six-category classification system that segments the nation’s roughly 3,100 counties from the most urban to the most rural. The center found that in the second most-rural grouping, small metropolitan areas, Trump won 60 percent of the vote compared with Vice President Kamala Harris’s 40 percent. In the top most-rural category, nonmetropolitan areas, Trump beat Harris even more resoundingly, by 69 percent to 31 percent. Trump’s vote share in the nonmetro areas exceeded even his commanding 66 percent there against Joe Biden in 2020 and 67 percent against Hillary Clinton in 2016. Trump’s advantage in the small metros outstripped his margin over Biden and equaled his advantage over Clinton. Across his three runs for the White House, Trump gained considerably more support in the most-rural counties than in the nation’s more populous communities. Although he ran no better in the most-urban counties than did the 2012 Republican nominee, Mitt Romney, Trump roughly doubled the GOP margin in nonmetro areas from 20 points in 2012 to nearly 40 this year. In the small metros, Trump’s 20-point lead in 2024 represented a significant increase over Romney’s 12-point advantage. Congressional elections have largely followed the same trajectory. Once, rural areas were the political base for economically moderate, culturally conservative “blue dog” Democrats in the House, but since the GOP sweep in the 2010 midterm elections, Republicans have hunted the blue dogs to virtual extinction. Maps of party control of House seats now show the countryside solidly red in almost every state. Barring a few exceptions in New England, the states where rural residents compose the largest share of the population preponderantly elect Republicans to the Senate as well. As Jacobs noted, the GOP advances in small-town America feed on these communities’ deep sense of being left behind in a changing America. Trump, as a thrice-married New Yorker who has lived much of his life in a Fifth Avenue penthouse, has always seemed an unlikely tribune for rural voters, yet his connection with them is visceral. After years of seemingly inexorable decline in more remote communities, Jacobs believes, rural residents are especially responsive to Trump’s attacks on “elites” and his promises to upend the system. “I think rural people are rejecting the idea that the devil we know is worse than the devil Trump may bring,” Jacobs told me. Despite the appeal of Trump’s promise of “retribution” against the forces these people believe have held them back, the change he’s offering in the specifics of his second-term agenda may strain those ties. The potential conflicts begin with Trump’s plans for trade. Agricultural producers faced the most turmoil from the tariffs that Trump in his first term slapped on numerous trading partners, including China, the European Union, Mexico, and Canada. Trump bought peace with farm interests by disbursing more than $60 billion in payments to producers to compensate for the markets they lost when China and other countries imposed retaliatory tariffs on U.S. products such as soybeans, corn, and pork. Those payments consumed nearly all of the revenue that Trump’s tariffs raised, according to an analysis by the Council on Foreign Relations. Trump’s payments to farmers preempted any large-scale rural revolt during his first term. But they nonetheless imposed long-term costs on agricultural producers. The bruising trade conflicts of Trump’s first term encouraged foreign purchasers of American farm products to diversify their supply in order to be less vulnerable to future trade disruptions, Sandro Steinbach, the director of the Center for Agricultural Policy and Trade Studies at North Dakota State University, told me. As a result of Trump’s trade conflicts, Steinbach said, the United States lost share in those markets and never recovered it. In 2016, for example, the U.S. sold nearly as many soybeans to China as Brazil did; now Brazil controls three times as much of the Chinese market. “China is demanding more commodities” but is buying them from other suppliers, Steinbach said, “and that means we left a lot of money on the table.” All of these disruptions came from Trump’s relatively targeted first-term tariffs on imports. He’s now threatening much more sweeping levies, including a 10 percent tariff on all imports, rising to 60 percent on those from China and 25 percent for goods from Mexico and Canada. Steinbach believes farmers will “very likely” now face even greater retaliatory trade barriers against their produce than they did in Trump’s first term. “The worst-case scenarios are really bad,” he told me. Farm lobbies are welcoming Trump’s pledge to slash environmental regulations and hoping that he can deliver on his promise to cut energy costs. But his determination to carry out the mass deportation of undocumented immigrants will create another challenge for farmers. Agriculture relies on those workers as much as any other industry: Varying estimates put the proportion of farm laborers who are undocumented at one-sixth to nearly a quarter; they also make up large workforce shares in other industries along the food chain, such as meatpacking. Removing a significant share of those workers through deportation, Steinbach said, would further erode the international competitiveness of American farmers by raising their labor costs and thus the price of their products. Eliminating undocumented workers would also put upward pressure on domestic food prices—after an election that, as Trump himself noted, he won largely because of the price of groceries—and would also weaken rural economies by removing those workers’ buying power. “It is a stretch to think that if you start deporting undocumented labor, rural people who are hanging out in town are going to step in and fill those jobs, or people are going to move back to the countryside,” Jacobs told me. “There is very little evidence to suggest the labor market would self-correct in that direction.” A recent attempt to model how Trump’s tariff and mass-deportation plans would affect agricultural producers found a devastating combined impact. In a scenario where Trump both imposes the tariffs he’s threatened and succeeds at deporting a large number of immigrants, the nonpartisan Peterson Institute for International Economics has forecast that by 2028, agricultural exports could fall by nearly half and total agricultural output would decline by a sixth. Mass deportation, the institute projected, would reduce the workforce for agricultural production more than for any other economic sector. This forecast underscores Steinbach’s astringent assessment: “Any of those policies will be pretty painful in the short run for rural America.” Read: Tariffs once tore the GOP apart—and may be doing so again Equally painful for rural America could be Trump and congressional Republicans’ agenda for health care. Big cuts in federal spending on Medicaid and subsidies for the uninsured to buy coverage under the Affordable Care Act were central to the Trump-backed plan that House Republicans passed in 2017 to repeal the ACA. Trump’s administration later backed a Senate Republican proposal to convert Medicaid into a block grant and significantly cut its funding. Retrenching federal spending on Medicaid and the ACA remains a priority for congressional Republicans. Trump has consistently excluded Medicaid when he’s pledged not to seek cuts in the other biggest federal safety-net programs, Social Security and Medicare. The Republican Study Committee, a prominent organization of House conservatives, called in its latest proposed budget for converting Medicaid and ACA subsidies into block grants to states and then cutting them by $4.5 trillion over the next decade, more than four times the scale of cuts passed by the House in its 2017 bill. “At the level of cuts some of these groups are talking about, we are not looking at making things more efficient,” Larry Levitt, the executive vice president for health policy at the nonpartisan KFF think tank, told me. “We are looking at cutting tens of millions of people off from coverage.” Rural places would be especially vulnerable to cuts anywhere near the level that Republicans are discussing. Rural residents tend to be older and poorer, and face more chronic health problems. Rural employers are less likely to offer health insurance, which means that Medicaid provides coverage for a larger share of working-age adults in small towns: Multiple studies have found that about a fifth of rural residents rely on Medicaid, compared with less than a sixth in urban areas. Nearly half of all children in rural areas receive health coverage through the federal Children’s Health Insurance Program launched during Bill Clinton’s presidency. Medicaid is especially important in confronting two health-care challenges particularly acute in rural communities. One is the opioid epidemic. In a KFF poll last year, more than 40 percent of rural residents said that they or someone in their family had been addicted to opioids, a far higher proportion than in urban or suburban communities. Medicaid has become the foundation of the public-health response to that challenge. One recent study found that Medicaid provides treatment for about 1.5 million opioid users every year. Particularly important in that effort has been the ACA’s expansion of Medicaid to cover more working-poor adults who are just above the poverty level. Hundreds of thousands of people are receiving opioid-addiction treatment under Medicaid in heartland states that Trump won, such as Michigan, Pennsylvania, Ohio, Kentucky, and Indiana. In all of those states, a majority of people receiving care are covered through the Medicaid expansion, the center-left Urban Institute has calculated. “A lot of effort has gone into beefing up the community-based resources for mental health and substance abuse, and Medicaid has been the linchpin in the financing for that,” Cindy Mann, a health-care attorney who oversaw the Medicaid program during Barack Obama’s administration, told me. One of the options most discussed among Republicans for reducing Medicaid spending has been to eliminate the extra federal money (the so-called enhanced match) that Washington has offered states to cover more of the working poor under the ACA. If that money is withdrawn, states would face enormous fiscal pressure to reduce such coverage. That would directly undercut the financing that Medicaid has provided for responding to the opioid epidemic, something that Trump has pledged to prioritize. Medicaid is also a linchpin in the struggle to preserve rural hospitals. These face much more financial stress than medical facilities in more populous areas. Mann says that over the past two decades, 190 rural hospitals have closed or converted to other purposes, and nearly a third of the remaining facilities show signs of financial difficulty. Private insurance, Mann notes, doesn’t provide as much revenue for rural hospitals as it does for urban ones, because fewer rural residents have such coverage to begin with; even for those who do, rural providers lack the economic leverage to demand reimbursement rates that are as high as private insurers provide to urban hospitals. That situation makes Medicaid a crucial lifeline for rural hospitals. “With large cuts to federal health spending, it would be very hard for rural health-care providers to simply survive,” said the KFF’s Levitt. “In many cases, rural hospitals are hanging by a thread already, and it wouldn’t take much to push them over the edge.” Read: The education deserts of rural America In the same way that rural hospitals are especially vulnerable to Trump’s health-care agenda, his education plans could threaten another pillar of small-town life: public schools. Trump has repeatedly promised to pursue a nationwide federal voucher system that would provide parents with public funds to send their children to private schools. In numerous state ballot initiatives over recent years, rural residents have voted against proposals to create a school-voucher system. That record continued last month when rural areas again mostly voted against voucher systems in ballot initiatives in Nebraska and Kentucky. (In Colorado, rural areas split about evenly on a similar proposition.) Kelsey Coots, who managed the campaign against the Kentucky voucher initiative, told me that the proposal was rejected even in culturally conservative rural counties “because everyone in the community is connected to the school.” Small-town residents, she said, recognized that rural public schools already facing financial strain from stagnant or shrinking enrollments have little cushion if vouchers drain more of their funding. Regardless of how receptive conservative rural voters might be to Republican attacks on “woke” educators, Coots noted, “if you ask them about their public school or their neighborhood school, they like it, because they know what the public school means for their community.” Throughout three elections, Trump’s messaging—particularly his hostility to racial and cultural change—has resonated strongly in rural communities. His second term may test whether that deep reservoir of ideological support can survive policies that threaten the material interests of rural America in so many ways. Key things to know about U.S. election polling in 2024 Author: Kennedy, Scott Keeter and Courtney, Date: 2024-08-28 Collections: PollMethods Zotero Key: RQBFDHIM Cite Key: Kennedy24KeyThingsKnow Zotero Item | Lit Note (J Studios/Getty Images) Confidence in U.S. public opinion polling was shaken by errors in 2016 and 2020. In both years’ general elections, many polls underestimated the strength of Republican candidates, including Donald Trump. These errors laid bare some real limitations of polling. In the midterms that followed those elections, polling performed better. But many Americans remain skeptical that it can paint an accurate portrait of the public’s political preferences. Restoring people’s confidence in polling is an important goal, because robust and independent public polling has a critical role to play in a democratic society. It gathers and publishes information about the well-being of the public and about citizens’ views on major issues. And it provides an important counterweight to people in power, or those seeking power, when they make claims about “what the people want.” The challenges facing polling are undeniable. In addition to the longstanding issues of rising nonresponse and cost, summer 2024 brought extraordinary events that transformed the presidential race. The good news is that people with deep knowledge of polling are working hard to fix the problems exposed in 2016 and 2020, experimenting with more data sources and interview approaches than ever before. Still, polls are more useful to the public if people have realistic expectations about what surveys can do well – and what they cannot. With that in mind, here are some key points to know about polling heading into this year’s presidential election. How are election polls being conducted? Pollsters are making changes in response to the problems in previous elections. As a result, polling is different today than in 2016. Most U.S. polling organizations that conducted and publicly released national surveys in both 2016 and 2022 (61%) used methods in 2022 that differed from what they used in 2016. And change has continued since 2022. One change is that the number of active polling organizations has grown significantly, indicating that there are fewer barriers to entry into the polling field. The number of organizations that conduct national election polls more than doubled between 2000 and 2022. This growth has been driven largely by pollsters using inexpensive opt-in sampling methods. But previous Pew Research Center analyses have demonstrated how surveys that use nonprobability sampling may have errors twice as large, on average, as those that use probability sampling. The second change is that many of the more prominent polling organizations that use probability sampling – including Pew Research Center – have shifted from conducting polls primarily by telephone to using online methods, or some combination of online, mail and telephone. The result is that polling methodologies are far more diverse now than in the past. (For more about how public opinion polling works, including a chapter on election polls, read our short online course on public opinion polling basics.) All good polling relies on statistical adjustment called “weighting,” which makes sure that the survey sample aligns with the broader population on key characteristics. Historically, public opinion researchers have adjusted their data using a core set of demographic variables to correct imbalances between the survey sample and the population. But there is a growing realization among survey researchers that weighting a poll on just a few variables like age, race and gender is insufficient for getting accurate results. Some groups of people – such as older adults and college graduates – are more likely to take surveys, which can lead to errors that are too sizable for a simple three- or four-variable adjustment to work well. Adjusting on more variables produces more accurate results, according to Center studies in 2016 and 2018. A number of pollsters have taken this lesson to heart. For example, recent high-quality polls by Gallup and The New York Times/Siena College adjusted on eight and 12 variables, respectively. Our own polls typically adjust on 12 variables. In a perfect world, it wouldn’t be necessary to have that much intervention by the pollster. But the real world of survey research is not perfect. Pre-election polls face one critical challenge that the routine opinion poll does not: determining which of the people surveyed will cast a ballot. (procurator via Getty Images) Predicting who will vote is critical – and difficult. Preelection polls face one crucial challenge that routine opinion polls do not: determining who of the people surveyed will actually cast a ballot. Roughly a third of eligible Americans do not vote in presidential elections, despite the enormous attention paid to these contests. Determining who will abstain is difficult because people can’t perfectly predict their future behavior – and because many people feel social pressure to say they’ll vote even if it’s unlikely. No one knows the profile of voters ahead of Election Day. We can’t know for sure whether young people will turn out in greater numbers than usual, or whether key racial or ethnic groups will do so. This means pollsters are left to make educated guesses about turnout, often using a mix of historical data and current measures of voting enthusiasm. This is very different from routine opinion polls, which mostly do not ask about people’s future intentions. When major news breaks, a poll’s timing can matter. Public opinion on most issues is remarkably stable, so you don’t necessarily need a recent poll about an issue to get a sense of what people think about it. But dramatic events can and do change public opinion, especially when people are first learning about a new topic. For example, polls this summer saw notable changes in voter attitudes following Joe Biden’s withdrawal from the presidential race. Polls taken immediately after a major event may pick up a shift in public opinion, but those shifts are sometimes short-lived. Polls fielded weeks or months later are what allow us to see whether an event has had a long-term impact on the public’s psyche. How accurate are polls? The answer to this question depends on what you want polls to do. Polls are used for all kinds of purposes in addition to showing who’s ahead and who’s behind in a campaign. Fair or not, however, the accuracy of election polling is usually judged by how closely the polls matched the outcome of the election. By this standard, polling in 2016 and 2020 performed poorly. In both years, state polling was characterized by serious errors. National polling did reasonably well in 2016 but faltered in 2020. In 2020, a post-election review of polling by the American Association for Public Opinion Research (AAPOR) found that “the 2020 polls featured polling error of an unusual magnitude: It was the highest in 40 years for the national popular vote and the highest in at least 20 years for state-level estimates of the vote in presidential, senatorial, and gubernatorial contests.” How big were the errors? Polls conducted in the last two weeks before the election suggested that Biden’s margin over Trump was nearly twice as large as it ended up being in the final national vote tally. Errors of this size make it difficult to be confident about who is leading if the election is closely contested, as many U.S. elections are. Pollsters are rightly working to improve the accuracy of their polls. But even an error of 4 or 5 percentage points isn’t too concerning if the purpose of the poll is to describe whether the public has favorable or unfavorable opinions about candidates, or to show which issues matter to which voters. And on questions that gauge where people stand on issues, we usually want to know broadly where the public stands. We don’t necessarily need to know the precise share of Americans who say, for example, that climate change is mostly caused by human activity. Even judged by its performance in recent elections, polling can still provide a faithful picture of public sentiment on the important issues of the day. The 2022 midterms saw generally accurate polling, despite a wave of partisan polls predicting a broad Republican victory. In fact,FiveThirtyEight found that “polls were more accurate in 2022 than in any cycle since at least 1998, with almost no bias toward either party.” Moreover, a handful of contrarian polls that predicted a 2022 “red wave” largely washed out when the votes were tallied. In sum, if we focus on polling in the most recent national election, there’s plenty of reason to be encouraged. Compared with other elections in the past 20 years, polls have been less accurate when Donald Trump is on the ballot. Preelection surveys suffered from large errors – especially at the state level – in 2016 and 2020, when Trump was standing for election. But they performed reasonably well in the 2018 and 2022 midterms, when he was not. Pew Research Center illustration During the 2016 campaign, observers speculated about the possibility that Trump supporters might be less willing to express their support to a pollster – a phenomenon sometimes described as the “shy Trump effect.” But a committee of polling experts evaluated five different tests of the “shy Trump” theory and turned up little to no evidence for each one. Later, Pew Research Center and, in a separate test, a researcher from Yale also found little to no evidence in support of the claim. Instead, two other explanations are more likely. One is about the difficulty of estimating who will turn out to vote. Research has found that Trump is popular among people who tend to sit out midterms but turn out for him in presidential election years. Since pollsters often use past turnout to predict who will vote, it can be difficult to anticipate when irregular voters will actually show up. The other explanation is that Republicans in the Trump era have become a little less likely than Democrats to participate in polls. Pollsters call this “partisan nonresponse bias.” Surprisingly, polls historically have not shown any particular pattern of favoring one side or the other. The errors that favored Democratic candidates in the past eight years may be a result of the growth of political polarization, along with declining trust among conservatives in news organizations and other institutions that conduct polls. Whatever the cause, the fact that Trump is again the nominee of the Republican Party means that pollsters must be especially careful to make sure all segments of the population are properly represented in surveys. The real margin of error is often about double the one reported. A typical election poll sample of about 1,000 people has a margin of sampling error that’s about plus or minus 3 percentage points. That number expresses the uncertainty that results from taking a sample of the population rather than interviewing everyone. Random samples are likely to differ a little from the population just by chance, in the same way that the quality of your hand in a card game varies from one deal to the next. The problem is that sampling error is not the only kind of error that affects a poll. Those other kinds of error, in fact, can be as large or larger than sampling error. Consequently, the reported margin of error can lead people to think that polls are more accurate than they really are. There are three other, equally important sources of error in polling: noncoverage error, where not all the target population has a chance of being sampled; nonresponse error, where certain groups of people may be less likely to participate; and measurement error, where people may not properly understand the questions or misreport their opinions. Not only does the margin of error fail to account for those other sources of potential error, putting a number only on sampling error implies to the public that other kinds of error do not exist. Several recent studies show that the average total error in a poll estimate may be closer to twice as large as that implied by a typical margin of sampling error. This hidden error underscores the fact that polls may not be precise enough to call the winner in a close election. Other important things to remember Transparency in how a poll was conducted is associated with better accuracy. The polling industry has several platforms and initiatives aimed at promoting transparency in survey methodology. These include AAPOR’s transparency initiative and the Roper Center archive. Polling organizations that participate in these organizations have less error, on average, than those that don’t participate, an analysis by FiveThirtyEight found. Participation in these transparency efforts does not guarantee that a poll is rigorous, but it is undoubtedly a positive signal. Transparency in polling means disclosing essential information, including the poll’s sponsor, the data collection firm, where and how participants were selected, modes of interview, field dates, sample size, question wording, and weighting procedures. There is evidence that when the public is told that a candidate is extremely likely to win, some people may be less likely to vote. Following the 2016 election, many people wondered whether the pervasive forecasts that seemed to all but guarantee a Hillary Clinton victory – two modelers put her chances at 99% – led some would-be voters to conclude that the race was effectively over and that their vote would not make a difference. There is scientific research to back up that claim: A team of researchers found experimental evidence that when people have high confidence that one candidate will win, they are less likely to vote. This helps explain why some polling analysts say elections should be covered using traditional polling estimates and margins of error rather than speculative win probabilities (also known as “probabilistic forecasts”). National polls tell us what the entire public thinks about the presidential candidates, but the outcome of the election is determined state by state in the Electoral College. The 2000 and 2016 presidential elections demonstrated a difficult truth: The candidate with the largest share of support among all voters in the United States sometimes loses the election. In those two elections, the national popular vote winners (Al Gore and Hillary Clinton) lost the election in the Electoral College (to George W. Bush and Donald Trump). In recent years, analysts have shown that Republican candidates do somewhat better in the Electoral College than in the popular vote because every state gets three electoral votes regardless of population – and many less-populated states are rural and more Republican. For some, this raises the question: What is the use of national polls if they don’t tell us who is likely to win the presidency? In fact, national polls try to gauge the opinions of all Americans, regardless of whether they live in a battleground state like Pennsylvania, a reliably red state like Idaho or a reliably blue state like Rhode Island. In short, national polls tell us what the entire citizenry is thinking. Polls that focus only on the competitive states run the risk of giving too little attention to the needs and views of the vast majority of Americans who live in uncompetitive states – about 80%. Fortunately, this is not how most pollsters view the world. As the noted political scientist Sidney Verba explained, “Surveys produce just what democracy is supposed to produce – equal representation of all citizens.” MAGA Won Over UNDOCUMENTED IMMIGRANTS? What Has This Country Come To? :The Secret Podcast Author: The Bulwark, Date: 2024-12-09 Collections: Hot Takes US Elect 2024, MediaAdsPolit Zotero Key: 7FQK7BCR Cite Key: TheBulwark24MAGAWonUNDOCUMENTED Zotero Item | Lit Note 00:00 hey there it's JVL with my best friend 00:02 Sarah Longwell publisher of the buw work 00:04 Sarah it's uh I I I hate everybody today 00:08 okay can we talk about that sure do you 00:11 want to tell what people why why why are 00:13 we doing this uh we're just I mean we're 00:16 just we wanted to try a little a a tiny 00:19 secret just a little we do a secret 00:22 podcast uh for members of buor plus we 00:24 do it every Friday it seems to be quite 00:26 popular people enjoy it but we wanted to 00:28 do a little just a little tiny a little 00:30 secret for YouTube and if you want the 00:32 real show go go to thework.com And 00:34 subscribe uh so there was a a piece that 00:39 ran a couple weeks ago in propublica 00:41 that just came across my field of vision 00:43 this weekend in which propublica went 00:44 and spoke with undocumented immigrants 00:47 across the country and uh found a huge 00:52 number of them Trump 00:54 supporters and the Common Thread running 00:57 through it is uh 01:00 they are illegal immigrants but they are 01:03 good illegal immigrants and they really 01:07 resent more recent illegal immigrants 01:11 especially those who have claimed 01:13 Asylum and they want those illegal 01:16 immigrants 01:18 deported but they are confident Trump 01:21 won't Deport 01:24 them and uh I this made me angry I don't 01:30 know how else to say it except that it 01:31 made me quite angry and it made me come 01:33 around on mass deportations like let's 01:36 let's do this thing let's just do this 01:38 thing right we we haven't gotten the 01:40 validated voter numbers yet but uh you 01:43 know it looks like Trump did about 50% 01:45 of of Hispanic voters a little less than 01:48 50% um certain almost certainly went 01:52 over 50% Mark with Hispanic men and if 01:55 it turns out that like what Republicans 01:57 needed to break through with Hispanic 01:59 vote was that they needed to promise to 02:01 use the military to enforce Mass 02:04 deportations including of children who 02:07 are American citizens whose parents 02:09 happen to be 02:10 undocumented then I'm just like okay 02:12 let's do this thing you know like I I 02:17 don't know what uh at some point the 02:21 leopards have to eat people's faces 02:24 right and and for Democrats to stand up 02:27 and try to stop something like this from 02:30 happening saying look you say that you 02:32 you voted for the leopards eating faces 02:34 because you think the leopards won't eat 02:36 your faces but but they will come for 02:38 you eventually we promise you like and 02:41 so we're going to protect you from that 02:42 no don't don't protect them from 02:45 it let the leopards eat the faces so for 02:48 those of you who do not regularly listen 02:49 to The Secret show this is the part of 02:51 the show where JVL posits a angry Theory 02:54 and I push back um and uh we're going to 02:57 start by untangling a couple things here 02:59 that I think you're confl yes uh so one 03:01 is you are angry at uh some people who 03:05 are quoted in a one prop publica piece 03:08 which is just you just happen to read it 03:11 and that now has the Atlanta Journal 03:13 Constitution piece from the day after 03:15 the election has caused you to entirely 03:17 say you're in favor of mass deportations 03:20 because some of these quotations made 03:21 you mad okay those illegal immigrants 03:24 who gave those 03:25 quotations are not allowed to vote which 03:27 means they are not part of the 50% their 03:31 children however also quoted in the also 03:34 their children were Trump voters and 03:35 this woman Rosa said that she was proud 03:38 that her sons voted for Trump because of 03:42 the immigration stuff and uh and even 03:44 volunteered that one of her sons has a 03:45 let's go Brandon bumper sticker on his 03:48 car great let's do this thing uh well 03:52 look I do think that there was always at 03:54 the heart of some of the ways in which 03:57 just through a political lens that the 03:59 Democrat have thought about uh 04:01 immigrants coming here uh and and the 04:06 Democrats now are the sort of much more 04:08 the party of yes we want more people to 04:10 come and Joe Biden really I mean the the 04:13 Border was wide open for much of his 04:16 term and uh and I think it's a big part 04:19 of why KLA Harris lost I mean boy 04:21 listening to these focus group would 04:23 like the one thing that they know about 04:25 her was that she was supposed to be the 04:26 immigration are people seem to remember 04:29 that one uh little negative 04:31 information 04:33 but the now Hispanics as everyone will 04:36 tell you are not a 04:38 monolith but they are culturally quite 04:41 conservative in many ways um and 04:44 especially I think people coming from uh 04:46 other countries who are much more 04:48 familiar with a strong men mentality uh 04:52 where there's uh there's more 04:54 authoritarian flare find something quite 04:58 uh familiar and Trump and so uh while I 05:03 think you're right that there is a a 05:05 strange sort of a close the door behind 05:09 me um or pull the ladder up or whatever 05:12 the phrase might be um you know I did a 05:14 lot of focus groups with Hispanic voters 05:16 and they and now these are Hispanic and 05:19 many of them sort of had recently become 05:20 citizens or and they boy nobody talks 05:23 more about the Border than than 05:25 Hispanics and I think that that is just 05:27 a has been a Mis 05:30 is it a misapprehension a a 05:32 misunderstanding on the part of 05:35 Democrats uh who thought that Hispanics 05:37 would continue to be with them um that 05:40 they're not uh they're not culturally 05:42 they are much more conservative 05:44 financially they are much more 05:45 conservative they believe in the kind of 05:47 thing that Donald Trump is putting out 05:48 there especially the kind of cartoonish 05:50 Fox 05:52 patriotism so this is where I I mean I I 05:54 really think Democrats need to learn 05:56 from Trump and the the big lesson they 05:58 need to learn is transaction actionism 06:01 because the Democratic party is 06:02 currently in the business of trying to 06:05 do things for people who are never going 06:07 to vote for them and I think that's 06:09 insane and ass backwards and uh that's 06:13 not what Trump does right and uh 06:17 so what why why are Democrats out there 06:20 like defending immigration who's voting 06:22 for them because of that right and and 06:25 if you're a pro-immigration voter well 06:26 where are you going to go if the you 06:29 know the rats are suddenly becoming 06:30 immigration Hawks where are you going to 06:31 go you know you don't have any place to 06:33 go anyway it's it's like what Trump used 06:34 to say about the pro lifers what are you 06:36 going to do you got no choice um and uh 06:40 and I would say also the other half of 06:42 this is that tweet you sent me this 06:44 morning from the Department of Health 06:45 and Human 06:47 Services 06:48 celebrating Happy pansexual and pan 06:51 romantic Pride day and I just everything 06:55 ought to be transactional 06:57 transactionalism how many V are you 06:59 celebrate yeah if I celebrate the 07:02 pansexual 07:04 transatlantic pride month what was the 07:06 name what was the I got to tell you I 07:07 sent this to JVL because JVL will 07:09 sometimes look JVL will sometimes be 07:12 like you know just because Twitter user 07:16 Li liberachi 07:19 6241 tweets something insane you know 07:23 Republicans will say see all Democrats 07:25 think this and I push back on that and 07:26 say look no there's a lot of crazy stuff 07:29 that institutional Democrats are putting 07:31 out there and I say this as a very proud 07:33 member of the 07:35 lgbtq plus plus plus Plus+ Community 07:39 whatever that this it's all gone too far 07:41 it's all got to stop you're right you're 07:43 right again I just look at this and I 07:45 think to myself Democrats start being 07:49 transactional happy pans where are the 07:53 panromantic pride day this doesn't need 07:56 to be a thing how many votes does that 07:57 hang a number on how many votes that is 07:59 going to win the next demog I I just it 08:02 makes me 08:04 insane and the level see I don't and I 08:06 don't even agree with you about the 08:07 transactionalism I think you should do I 08:09 think you should I think the whole point 08:10 of a liberal democracy is that you have 08:13 things like you know pluralism the rule 08:16 of law um free speech and that they are 08:18 applied equally to everybody it should 08:20 not be transactional more they ain't 08:22 yeah well you you you seem to want to um 08:26 you know get into the kind of Vortex of 08:29 backlashes the tit fortat monu's versus 08:33 Capulet Hatfields and McCoy that lead us 08:36 straight into utter ruin like Banana 08:39 Republic like somebody's got to hold the 08:40 line uh and I I don't think we can react 08:43 in this sort of emotionally angry way 08:46 this is just blow it all up because I'm 08:47 mad it's like I read one prop publica 08:49 story and I say blow it all up we're all 08:51 accelerationists now no this this is a 08:53 moment that requires at least some 08:55 forbearance in where we pick our battles 08:58 um and and look although as it 09:01 happens 09:03 um I think that uh mass deportations is 09:07 both uh an unlikely 09:10 scenario um I think much more likely and 09:13 you and I were talking about this 09:13 offline not that long ago uh is that 09:16 they do I was talking to someone from 09:18 the business Community recently who was 09:20 like well you know Trump will do his 09:22 photo ops and do that but like they're 09:24 not going to start raiding the companies 09:26 like the business Community is very like 09:29 he better not mean this he better not 09:31 mean this and that but see you you want 09:34 them to do it so that those people don't 09:36 get to say uh oh yeah he just he just 09:40 it's just a few photo ops right you you 09:42 want them the business Community to have 09:44 to you want them to come raid their 09:46 companies and I want one of two things 09:48 okay either I want the actual mass 09:53 deportations or I want Democrats saying 09:58 see he didn't do anything what I don't 10:01 want is Democrats setting their hair on 10:04 fire about Mass deportations no Mass 10:07 deportations happening and then Trump 10:09 saying look how mad the libs are we did 10:12 it we won Victory problem solved on to 10:16 the next thing that's what I don't want 10:18 this is a good now this is something 10:20 worth exploring which is the uh Trump 10:22 did not build an actual wall no but he 10:25 did he didn't build the foxcon plant 10:27 remember the foxcon Apple was going to 10:30 make all Tim Apple was there to break 10:32 ground and it never happened he built a 10:34 wall of liberal anger though he built a 10:37 wall of liberal and that was enough for 10:39 people that was enough for most of his 10:41 supporters and not only because a the 10:43 own the libs guys got what they want and 10:45 the low info people thought 10:48 because of how angry people were that he 10:51 built the wall 10:53 yes and this is what I don't want with 10:56 mass deportations because like you I I I 10:59 could we're not going to do a a long 11:01 thing but I could explain to you why I 11:03 am deeply skeptical that anything like 11:05 Mass deportations are going to happen uh 11:08 and so like don't don't give them this 11:12 don't don't like set your be like oh 11:14 yeah he wants to deported 11 million 11:16 people great go ahead I'm waiting 11:20 sitting here with my check I know those 11:22 clickers they have a theme parks to know 11:23 how many people are going on each ride 11:25 I'm going to sit there at the border 11:26 with my clicker clicking on how many 11:28 people get deported 11:29 and uh I think they mostly they put 11:32 people on planes I don't think that they 11:34 that's one of the problem I think they 11:35 walked them across uh in a single file 11:38 line I'll be at the airport then I'll be 11:39 at all the airports click click click 11:41 point is it ain't never going to happen 11:43 and instead of setting her hair on fire 11:45 worrying about it happening and talking 11:47 about how he shouldn't do it so that he 11:49 can then say well we would have done it 11:51 but the Democrats stopped us like just 11:53 stand there and say oh really you're 11:55 serious God I dare you I double dog dare 11:59 you sure I would say though that one 12:03 should not endorse or unse or be in 12:07 favor of or not in favor of policy 12:09 because one read an article and got very 12:11 mad at a group of people I you know what 12:15 you're saying group of people that can't 12:17 vote but on the other hand it doesn't 12:19 feel right doesn't feel right I I know 12:22 and I know we're in our feelings I know 12:24 that everyone's in their feelings right 12:26 now and uh and so I'm I'm I'm going I'm 12:30 trying forbearance as people shout and 12:32 yell over things that I find irrational 12:35 because you know people are upset that 12:37 Donald Trump is going to be president 12:38 I'm upset that Donald Trump is going to 12:40 be president but that is not a reason to 12:42 burn it all down and I'm the worst well 12:44 I mean there are lots of reasons to burn 12:46 it all down let's be honest but uh but 12:48 that a lot easier to burn things down 12:50 than it is to build them again so we 12:51 should be really careful about what we 12:53 set on fire we're going to want some of 12:55 these institutions some point we're 12:57 going to miss them when they're gone 13:02 well 13:04 maybe okay Sarah good talking with you 13:08 we'll do it again next time hit like uh 13:10 subscribe to the channel go over to the 13:12 buw work and subscribe to the bull work 13:14 bye 13:28 bye for Rethinking Democrats' Political Strategy Author: Torenberg, Erik, Date: 2024-11-29 Collections: Hot Takes US Elect 2024 Zotero Key: R78F5JKM Cite Key: Torenbert24rethinkDemStrategy Zotero Item | Lit Note 00:00 [Music] 00:10 welcome to econ 102 where economists 00:12 Noah Smith and I make sense of what's 00:14 happening in the news technology 00:16 business and Beyond through the lens of 00:18 Economics let's Jump Right 00:20 In women move toward Trump in this 00:24 election white men move toward 00:27 Harris you know so like all the all the 00:30 narratives are just are just pretty 00:32 broken and you're going to have to 00:34 appeal to People based on it it's time 00:37 for Democrats to embrace broad appeals 00:40 you've got to ditch this idea that a a 00:42 nation is just like a larger version of 00:44 a city and that you can have these local 00:46 activist groups who represent various 00:48 communities in the city come to you and 00:49 basically offer you votes in exchange 00:51 for patronage that's not how National 00:53 politics Works you've got to do broad 00:56 appeals it's too expensive to pay off 00:59 everybody based on their demographic 01:00 cross tabs actually you know what 01:03 there's a really interesting theory 01:04 about this uh called selectorate Theory 01:06 by a a political scientist named Bruce 01:08 Bueno de MOS which if I my Spanish says 01:11 me right means Bruce goodness of mosqu 01:14 but anyway Bruce Bueno de MOS is one of 01:16 the Premier political scientists of Our 01:18 Generation and also he's a really good 01:19 forecaster who like contracts to the CIA 01:22 and forecasts political events really 01:23 well but also he has the selectorate 01:25 theory which basically says that in any 01:27 country whether a dictator ship or a 01:30 one- party state or you know Soviet 01:32 Union or like America or democracy 01:35 anywhere has some group of people who 01:38 selects the leader the leader is not a 01:40 Superman he's not homelander from the 01:42 boys he's not he doesn't rule through 01:44 Godlike power right he rules through a 01:47 coalition he has people who support him 01:50 and that's the selectorate the group of 01:53 people who have the potential who who 01:55 who support matters and in in a military 01:57 State it's the that's the military and 02:00 in a one party State it's the party 02:02 maybe you know like the broad party like 02:04 Chinese Communist party has what 50 02:05 million people I guess and so that's the 02:08 selectorate and then the winning 02:10 Coalition is a group of the selectorate 02:11 which is big enough to win within the 02:13 selectorate and the question is how do 02:15 you buy a winning Coalition and 02:17 selectorate Theory says that in a 02:19 dictatorship you buy people directly you 02:23 give money to your cronies you if you 02:27 have a small selectorate if your 02:28 selector is just the army then you do 02:30 what Iran and and like Egypt and Myanmar 02:35 and used to be Indonesia do which is 02:37 basically have the Army be a mafia and 02:39 basically the Army gets to run all the 02:40 businesses so you can buy off the Army 02:42 because no one else matters you don't 02:44 care about the people in the street that 02:45 you just shoot them you make your money 02:46 from oil I don't know or the Suz canal 02:49 and like you don't Indonesia had to stop 02:51 this kind of politics but but I think 02:52 Egypt and and Iran still have this going 02:55 strong Pakistan to to some degree has it 02:57 going where just the army R and 03:00 they're the mafia and and you can pay 03:02 them off but when you get to a democracy 03:05 where the selectorate is the voting 03:06 public right with it's 100 million 03:09 people or whatever when you get to the 03:11 when you get to that size of the 03:12 selectorate you can't just buy them off 03:14 it gets too expensive and so selectorate 03:16 theory predicts that democracies will be 03:17 better because they will focus on 03:19 providing public goods that benefit 03:21 everyone even the losers so the idea is 03:23 that the public goods will be like 03:25 infrastructure rule of law or you know 03:28 etc etc and so democracies will provide 03:30 public goods while everyone in a 03:32 autocracy is focused on just paying off 03:34 specific individual people and and and 03:37 not necessarily providing public goods 03:39 this is one of those theories that was 03:40 made like you know in the 90s and early 03:41 2000s when everybody thought like 03:43 democracy is the best thing ever and 03:44 we'll win and end of History kind of 03:46 moment but I think there's something to 03:48 it and I think Democrats have been 03:50 trying to buy people off too 03:52 specifically they've been saying okay 03:54 here's you know this this group which 03:56 this this community quote unquote which 03:58 is represented by this activist group 04:00 that comes talks to our staffers or more 04:03 more accurately about 60 different 04:04 activist groups all funded by like 04:06 leftwing Tech guys and so then let's you 04:09 know the here's this community we'll buy 04:11 them off by by doing these different 04:14 programs so like student loans 04:15 cancellation was insanely expensive 04:19 regressive nobody really wanted it but 04:21 Biden did it because there were these 04:23 groups who came and really fought for it 04:25 hard in order to buy off to buy the 04:28 young people community 04:31 and guess what they shifted toward Trump 04:32 anyways but that that was an attempt to 04:34 buy off but it was insanely expensive so 04:38 Democrats you know they have this idea 04:40 that you can buy off these you know that 04:42 communities one by one by getting a 04:44 group capital G group to tell you what 04:47 the community wants and then passing a 04:49 giant budget item based on what the 04:50 group tells you the community wants and 04:52 it's just not working a because it's too 04:54 expensive the selectorate theory but you 04:56 know student loan cancellation being the 04:58 biggest thing or various subsidies 05:00 there's all kinds of things that they 05:01 did and so then there's there's that but 05:04 then there's also the fact that like 05:07 most American voters don't vote based on 05:10 you know for president based on the 05:11 direct benefit that they get right they 05:13 vote based on self-expression because if 05:15 you think about it actually that's 05:17 almost all elections in the world are 05:18 based on self- expression because if you 05:20 think about it why would you ever go out 05:22 and vote for president when the 05:23 presidency will never ever be won by a 05:24 single vote it can't if it got to a 05:27 single vote the Supreme Court would 05:29 effectively deci by deciding which votes 05:30 are applic counted it' be like Bush Gore 05:34 right so the Supreme Court will not 05:35 decide this will decide this one vote 05:38 will never ever ever make the difference 05:40 in the presidency so why does everyone 05:42 go out and vote why does why do 100 05:43 million people go out and vote they vote 05:45 to express themselves and if you express 05:48 yourself you're expressing principles 05:50 and ideas those are like public goods in 05:53 the selector Theory those are broad 05:54 things that appeal to everybody 05:56 Democrats need to stop thinking they can 05:58 buy off specific groups of voters with 06:00 like Peola essentially and start doing 06:03 broad appeals that help everybody 06:06 everybody in America what are examples 06:08 of that so like you know making easier 06:11 to build housing is an obvious one you 06:14 know just competent management which 06:16 Democrats have not incredibly done 06:17 recently lately but which will be very 06:19 attractive once RFK gets going right 06:21 it's like hey under me your kid will not 06:23 get measles right appeals to our 06:26 commonality As Americans that was what 06:28 Bill Clinton always did you know Bill 06:30 Clinton would always say you know like 06:33 what unites us is more important than 06:34 what divides us Americans are united on 06:36 our values that was every speech you 06:38 gave you know we're actually United did 06:40 it persuade Republicans to support him 06:42 hell no it didn't did it persuade a few 06:45 swing voter enough swing voters to 06:46 support him where he consistently won 06:48 elections hell yes it did right he just 06:50 always said what unites is more 06:52 important than what divides us just talk 06:53 about basic Universal values Freedom you 06:56 know and Freedom From Fear like like 06:58 Freedom From Fear of crime and disorder 07:00 is is a fundamental human Freedom talk 07:02 about freedom in international context 07:04 like now you have all the right-wing 07:05 preds saying why is Russia our enemy 07:06 well Russia threatens Freedom China 07:08 threatens Freedom even more and and so 07:11 so talk about Freedom bring that back 07:13 and and KLA Harris did this at the very 07:15 end of her campaign but it was way too 07:16 late and way too little honestly like 07:18 but it was just people had already 07:19 associated with wokeness and she didn't 07:22 if she had been like you know Miz 07:24 freedom and like we're all Americans and 07:26 like like Bill Clinton 2 from day one 07:28 from 2019 when that was really harshly 07:31 unpopular had she done that she would 07:33 have she would have been a successful 07:34 politician more than she was one last 07:36 quick question this is from Ryan cobine 07:38 he asks Noah are you familiar with Neil 07:40 how's book the fourth turning is here 07:42 what do you think of this generational 07:44 driver hypothesis as an explanation of 07:45 certain aspects of his history and 07:48 Society well I haven't read it so I can 07:51 hold forth on things I haven't read 07:52 because I asked GPT to summarize them 07:55 and read a million commentaries about 07:57 them but you're going to get a second 07:58 rate rant but okay why not the idea is 08:01 that there's Generations that tear down 08:03 institutions and generations that build 08:05 institutions and the Boomers were the 08:06 ultimate tear down generation and that 08:08 the Millennials would be the in the 08:10 generation that builds up institutions 08:12 well guess what they did they were very 08:14 woke the Millennials built a new series 08:17 of institutions of woke stuff to replace 08:19 the old stuff you know where where 08:21 nature magazine is waiting into 08:23 Political controversies and and you know 08:25 like the the NSF is awarding science 08:28 grants based on race Etc and things like 08:30 that that and you know the the the new 08:33 online media of like Teen Vogue saying 08:35 everything is about capitalism those 08:37 were the institutions that the 08:39 Millennials were building right the 08:41 groups the groups were an institution in 08:44 American life that we didn't even have a 08:45 name for until recently and so that was 08:47 an institution that that Millennials 08:49 were building Social Media stuff now 08:51 we're in the process of being like hold 08:54 on maybe that sucked so the thing is 08:58 that I don't see Trump and his people 09:00 building much in the way you know Elon 09:03 and Vivic want to go and just like fire 09:05 80% of the Civil Service or whatever 09:07 that's deg grading institution that 09:09 that's that's a tear down and they're 09:11 Generation X anyway or at least Elon is 09:13 like that's tearing down institutions or 09:16 you know RFK saying okay we're going to 09:18 just ban vaccines and then allow all 09:20 this quack like that's tearing down 09:23 that's not building up and the people 09:24 who wanted to build up were the wers who 09:26 sort of built up over the 2010s built up 09:29 things that in frankly were a little 09:31 dysfunctional and so I think if you want 09:34 to really take Strauss and house 09:35 theories seriously you've got to look at 09:38 this idea of the woke Revolution as the 09:40 Builder effort and unfortunately I think 09:43 that kind of face 09:45 planted so where does that leave us well 09:48 it leaves me disbelieving in Long dated 09:51 generational cycle theories that are 09:52 difficult to assess with data but I 09:55 don't know what should leave you maybe 09:56 depressed you know like many of you I 09:59 spent a good chunk of my day managing 10:00 emails and phone calls and despite being 10:03 careful about where I share my contact 10:04 info somehow the robocalls keep 10:06 multiplying and my inbox is flooded with 10:08 Spam it turns out there's a reason for 10:10 this data Brokers these companies don't 10:13 just collect basic contact info they're 10:16 Gathering everything from your social 10:17 security number and financial records to 10:19 your shopping habits and employment 10:21 history some health insurance companies 10:23 are now working with data Brokers to 10:24 track your online activity which could 10:26 actually affect your insurance rates 10:28 another huge privacy risk are people 10:30 search sites these websites publish 10:32 detailed profiles about you including 10:34 your home address family members 10:35 information and even sensitive details 10:37 like religious beliefs or political 10:39 affiliations this got me thinking about 10:41 how exposed we all are whether you're 10:42 sending a wire transfer want to protect 10:44 against identity theft or just trying to 10:46 keep your family's personal details 10:48 private that's why I started using incog 10:51 here's what makes it different they 10:52 automatically handle five different 10:54 types of data Brokers marketing 10:56 recruitment financial information risk 10:58 mitigation Brokers and those people 11:00 search sites instead of you having to 11:02 track down all these companies yourself 11:04 and trust me there are hundreds incog 11:06 contacts them on your behalf and demands 11:08 they remove your information and here's 11:10 the key they keep monitoring and 11:12 removing your data automatically because 11:14 these Brokers often try to collect it 11:15 again incog protects your data across 11:18 all your devices whether you're working 11:19 from home a coffee shop or anywhere with 11:21 public Wi-Fi plus you can add up to four 11:24 additional family members under their 11:25 family and friends plan and they're so 11:27 confident you'll see the difference in 11:28 your day to day they offer a 30-day 11:30 money back guarantee take your personal 11:33 data back with incog use code Eon 102 at 11:36 the link below and get 60% off an annual 11:38 plan incognit docomond 102 ec102 is a 11:42 podcast from turpentine the network 11:44 behind Moment of Zen in the arena the 11:46 cognitive Revolution and more if you 11:48 like what you hear subscribe and leave 11:50 us the review in the App Store you can 11:52 keep up with both of our substacks for 11:53 written analysis of the topics we cover 11:55 in the show at no opinion. substack 11:57 docomo torberg ubs.com thanks for 12:00 listening When working with focus groups, quantitative methods can help Author: Devlin, Kat, Date: 2020-12-18T14:59:32.338Z Collections: FocusGroups Zotero Key: 3PZ6VUYC Cite Key: Devlin20focusGroupQuantMethods Zotero Item | Lit Note How quantitative methods can supplement a qualitative approach when working with focus groups (Pew Research Center illustration) (Related post: How focus groups informed our study about nationalism and international engagement in the U.S. and UK ) Pew Research Center often collects data through nationally representative surveys, and we use focus groups to supplement those findings in interesting ways and inform the questionnaire design process for future work. When conducting focus groups, we typically use qualitative methods to understand what our participants are thinking. For example, we tend to hand-code key themes that come up in discussions. But in our latest focus group project, we wondered if we could use quantitative methods, such as topic models, to save time while still gaining valuable insights about questionnaire development. Some of these methods worked well, others less so. But all of them helped us understand how to approach text-based data in innovative ways. In general, we found that quantitative analysis of focus group transcripts can generate quick, text-based summary findings and help with questionnaire development. But it proved less useful for mimicking traditional focus group analysis and reporting, at least with the specific quantitative techniques we tried. Background In the fall of 2019, the Center held focus groups in the United States and United Kingdom to talk with people about their attitudes toward globalization. These discussions focused on three different contexts: participants’ local community, their nation of residence (U.S. or UK) and the international community. After conducting 26 focus groups across the two nations, we transcribed the discussions into separate files for each group. For our quantitative analysis, we combined all of the files into one .csv document where each participant’s responses — whether one word or a short paragraph — corresponded to one row of the spreadsheet. To prepare the text for analysis, we tokenized the data, or split each line into individual words, and further cleaned it by removing punctuation and applying other preprocessing techniques often used in natural language processing. Preliminary quantitative findings As a first step, we conducted a basic exploration of focus group respondents’ vocabulary. Several words frequently emerged among participants in both the U.S. and UK, including “country,” “live” and “feel,” all of which appeared in at least 250 responses across groups in each country. (This analysis excludes pronouns, articles and prepositions that are common in all spoken English, such as “he,” “she,” “they,” “a,” “the,” “of,” “to,” “from,” etc. Names of cities where focus groups took place and text from group moderators are also excluded from analysis.) We also found that several words and phrases distinguished the U.S. focus groups from those in the UK. Terms like “dollar” and “Republican” were among the most common terms only used by the American groups, while the UK’s national health system (“NHS”) and legislative body (“parliament”) appeared frequently in the British groups but were never used by Americans. As an exploratory tool, this kind of analysis can point to linguistic distinctions that stray from the predetermined topics included in a focus group guide. For instance, while we asked the groups in oblique ways about what it takes to be American or British, respectively, we never explicitly asked about immigration or minority groups in their country. Nonetheless, “African Americans” and “Native Americans” exclusively arose in the U.S. groups, while “Polish” and “mixed race” people were discussed in the UK. This told us that it might be worthwhile for future survey questionnaires to explore topics related to race and ethnicity. At the same time, it’s possible that our focus groups may have framed the conversation in a unique way, based on the participants’ racial, ethnic or immigration background. Word correlations We used another computational tool, pairwise correlation, for some exploratory text analysis that measures how often words co-occur compared with how often they are used separately. Using three terms related to key themes that the focus groups were designed to study — “global,” “proud” and “immigrant” — we can get a sense of how focus group participants talked about these themes simply by identifying other words that were positively correlated with these topical terms. By further filtering these other terms to those that were mainly used in just one country, we can capture unique aspects of American and British views on global issues, national pride and immigration. Both British and American participants discussed their nationalities when the conversation turned to pride. For instance, the words that most commonly appeared alongside “proud” in each country were “British” and “American,” respectively. (We considered the words “America” and “American” as separate terms, rather than variants of the same term.) Though “proud” among Britons often involved discussion of the word “flag,” “proud” in the U.S. correlated with “military.” Of course, correlation alone does not reflect whether these discussions had positive, negative or neutral connotations. Discussions about migration and global issues — including “globalization,” which we shortened to “global” in our text processing — also varied across the two countries. When U.S. respondents used the word “immigrant,” they were also likely to use words like “illegal,” “legal,” “come” and “take.” By comparison, Britons who used the term were liable to do so alongside terms like “doctor” or “population.” British participants used the word “global” alongside terms related to business (“company,” “industry,” “cheaper”) and global warming (“climate”). In the U.S., on the other hand, the discussion about globalization and immigration often accompanied terms like “hurt,” “China,” “benefit” and “take.” Pew Research Center has conducted several surveys on the topics of migration, climate change and views of China, among others. Our focus groups confirmed that these issues play a part in how individuals see their country’s place in the world, though they also highlight that, in different nations, people approach these topics in distinct ways that may not be immediately evident in traditional survey questions. Topic models In recent years, the Center has explored the use of topic models for text-based data analysis. This method finds groups of words that appear alongside one another in a large number of documents (here, focus group responses), and in the process finds topics or themes that appear across multiple documents. In our attempt to quantitatively analyze this set of focus group transcripts, we had somewhat limited success with this approach. On first pass, we used a probabilistic topic model called latent Dirichlet allocation, or LDA. But LDA often created topics that lacked coherence or split the same concept among multiple topics. Next we turned to structural topic models (STM), a method that groups words from documents into topics but also incorporates metadata about documents into its classification process. Here, we included a country variable as one such “structural” variable. STM allowed us to set the number of topics in advance but otherwise created the topics without our input. (Models like these are often called “unsupervised machine learning.”) We ran several iterations of the model with varying numbers of potential topics before settling on the final number. (For more on the Center’s use of topic modeling, see: Making sense of topic models) Our research team started at 15 topics and then increased the number in increments of five, up to 50 topics. With fewer than 35 topics, many word groupings seemed to encompass more than one topic. With more than 35 topics, several topics that appeared distinct began to split apart across topics. There was no magic number, and researchers with the same data could reasonably come to different conclusions. Ultimately, we settled on a model with 35 topics. Some of these topics clearly coalesced around a certain concept. For instance, we decided to call Topic 11 “Brexit” because its most common terms included “vote,” “leave,” “Brexit,” “party” and “referendum.” But while this topic and others appeared quite clear conceptually, that was not uniformly the case. For example, one topic looked as though it could relate to crime, but some terms in that topic (e.g., “eat” and “Christian”) did not fit that concept as neatly. We named some of the topics based on the themes we saw — “Legal immigration,” for example, and “European trade.” But as other researchers have noted, that does not necessarily mean the word groupings are definitely about that theme. In this case, we used topic models as an exploratory analysis tool, and further research would be needed to validate each one with a higher degree of certainty and remove conceptually spurious words. Another important consideration is that topic models sometimes group topics differently than researchers might be thinking about them. For that reason, topic models shouldn’t be used as a measurement instrument unless researchers take extra care to validate them and confirm their assumptions about what the models are measuring. In this project, the topic models simply served to inform questionnaire development for future multinational surveys. For example, Topic 12 in this experiment touches on issues of how spoken language relates to national identity, and future surveys may include a question that addresses this concept. One helpful aspect of the topic model approach is that it allowed us to see which topics were more prevalent in the U.S. or UK, or if they appeared in both. American respondents, for example, more frequently discussed topics related to the U.S. and legal immigration, while British respondents more often discussed topics related to Brexit and trade in Europe. Topics that researchers coded as “language” and “housing” appeared with relatively the same prevalence in both countries. However, characteristics of the data and problems with initially interpreting topics can cause further difficulties in this analysis. For instance, a topic we labeled “protection” was much more prevalent in American focus group discussions. That might have led us to assume that Americans are more concerned than their British counterparts with safety-related issues. But the focus groups we conducted were not nationally representative of the populations of either country, so we couldn’t draw this type of conclusion from the analysis. Additionally, because the topic itself might include words that have no relation to the concept of protection, researchers would likely need to consult the full transcripts that mention these topics — as well as external resources — before using this for questionnaire development. Text-based classification Qualitative coding of focus group transcripts is a resource-intensive process. Researchers who carried out the qualitative analysis of these transcripts considered using a Qualitative Data Analysis Software, or QDAS. These are tools designed for qualitative researchers to analyze all forms of qualitative data, including transcripts, manually assigning text into categories, linking themes and visualizing findings. Many disciplines employ these methods for successfully analyzing qualitative data. We wondered if quantitative methods would let us achieve similar ends, so we explored ways to potentially streamline procedures with quantitative tools to minimize the time and labor needed to classify text into topics of interest. Unlike with topic models, a text-based classification model uses predetermined topics, or variables, for the algorithm to classify. (This falls into a broader category called “supervised machine learning.”) A successful classification algorithm would mean that we could avoid having to read every transcript to determine what content belonged to certain groups, or having to make the kind of subjective judgments that are necessary with qualitative software. We used an extreme gradient boosting model (XGBoost) to classify focus group responses as relevant or not relevant to three different topics: immigration, the economy and the local community. We chose these topics because each emerged in the course of the focus group discussion — some as overt topics from prompts in the focus group guide (e.g., the local community), others as organic themes when people discussed their neighborhoods, national identity and issues of globalization (e.g., immigration and the economy). Two of our researchers coded the same set of randomly selected focus group responses, about 6% of approximately 13,000 responses from all groups combined. They used a 1 to indicate the response was about a topic and 0 to show it was not. From the set of coded responses, each of the three topics appeared at least 70 times. The model’s performance proved lackluster. When we compared the model to our hand coding, the accuracy rate ranged from 85% to 93%. But it also failed to identify the topic in most cases where it occurred, meaning that much of the accuracy was driven by matching on instances coded as 0 (i.e. the response is not about that topic) since 0 was much more prevalent across categories. One can liken this to a test for a rare disease. If only 1% of people in a population have a disease and the test returns only negative results to all people tested, the accuracy would be high — 99% of tests would be correct. But the test would have little utility since there would be no positive matches in instances where people were actually infected. Using a measure similar to accuracy called the kappa, a statistic that examines agreement while adjusting for chance agreement, we found that the classifier performed poorly with a kappa of no more than .37 for two of our topics. In addition, we looked at the models’ precision and recall, metrics that help evaluate the effectiveness of a model. Precision ranged from 20% to 100%, while recall ranged from 4% to 27% among two of the topics. On the third topic — the local community — the model assigned zero to all cases. Conclusion The quantitative techniques that we explored in this post do not completely replace a traditional approach to qualitative research with focus group data. Using a quantitative approach, however, can aid in exploratory analysis and refining questionnaire development without having to attend every group in person or read through hundreds of pages of text. The tools we used are far from exhaustive, and as the Center continues to use focus groups as part of the research process, we are hopeful that we can learn more about how to employ innovative techniques in our analysis. Kat Devlin is a research associate focusing on global attitudes at Pew Research Center. This post benefited from feedback provided by the following Pew Research Center staff: Dennis Quinn, Patrick van Kessel and Adam Hughes. The coming battle between social media and the state Author: Green, David Allen, Date: 2025-01-11 Collections: MediaAdsPolit Zotero Key: UVUT4N33 Cite Key: Green25battleSocMediaState Zotero Item | Lit Note “Quis custodiet ipsos custodes?” is the important question posed by the Roman poet Juvenal, translated by the English author Alan Moore as “Who watches the watchmen?”. But it is perhaps a question with a complacent implicit assumption. It presupposes that it is possible to watch the watchmen — and all that one needs to do is work out how this is done and by whom. Regulation, however, is not magic. Just because one wants a thing to be regulated, that does not make it capable of actually being regulated. If a thing is unpleasant or unwelcome, the instant demand is that something should be done, and that the unwanted thing can be regulated so it cannot happen. The notion that all we need to make the world a better place is “better regulation” is deeply embedded in our culture. And one thing for which the cry for regulation is made is social media platforms. If only they were “better regulated”, the popular sentiment goes, then various political and social problems would all be solved. But there are two problems with regulating social media platforms. The first comes from the very technology that gave rise to this fairly recent but now almost ubiquitous phenomenon. The second is that to impose effective regulation against unwilling platforms will require determined, unflinching governmental action and political will — the possibility of which the platforms are now doing what they can to avoid. At base social media is about the ability of anyone with an internet connection to use an online platform to say anything they want about anybody to anyone. As soon as what they want to say is typed — or recorded in video and audio — all they need to do is press enter and it is published — or broadcast — to the world. Meta’s Mark Zuckerberg arrives for the Senate Judiciary Hearing on Big Tech and the Online Child Sexual Exploitation Crisis in Washington, DC, January 2024 © Annabelle Gordon/CNP/Sipa This ease of publication or broadcast contrasts with the position up to about 30 or 40 years ago where a private individual would normally have to pass a number of gatekeepers — at newspapers, publishing houses and broadcast stations — before having what they wanted to say go far beyond their immediate circle. The law in turn followed this restrictive model. Liability for, say, defamation or infringement of copyright, or for non-compliance with broadcasting standards, would usually bite at the moment the gatekeepers permitted publication or broadcast. For that step was a solemn moment and those permitting wider circulation had onerous responsibilities. Yes, of course, it was open to eccentric and determined individuals to “vanity press” a book, or to promote home-printed pamphlets, or even start a pirate radio station. But these were intensive and expensive courses of action that would not cross the minds of normal people. And then came along the world wide web, user-friendly internet browsers, and the social platforms that made online publication easy. Everyone could publish to the world about anything they wanted. How was this constant babble to be regulated? Would it be possible? Or would it be as futile as seeking to regulate everyday conversations in the home or in the street? One idea was to try to make the platforms themselves like the gatekeepers of old: to treat the social media companies as “publishers” of what was published by their users. But the obvious problem was that the platforms did not have, and could not have, any form of prior approval to what was published. All they could do would be after the event, once the unwanted thing was already published. They were gatekeepers only able to close the gate after the animals had bolted. They could un-publish but not publish. Platforms thereby lobbied successfully for legal liability only to be incurred if a valid takedown request was not granted. And in any case this approach only worked where there were pre-existing individual legal causes of action: it made sense in respect of defamation of a specific identifiable individual. The obstacle is regulation by jurisdictions outside the US — primarily in the European Union, but also elsewhere such as Brazil and China But mass disinformation and misinformation often break no private law rights of individuals. The real victim instead is healthy public discourse. Another challenge was dangerous information in respect of self-harm and suicide. And also the promotion of criminal activity, such as child abuse or terrorism. These problems were stark, and they required more than mere takedown notices by complainants. Indeed, there would often be no complainants aware of such material, only those seeking to consume it. Constant vigilance would be required. One way of addressing this would be for the social media platforms to employ complex and expensive systems. This would be an immense cost imposition for platforms that primarily just wanted to monetise data and to sell advertising on the back of the social media postings of users. But it would be an imposition that platforms would only accept if there was no alternative. Those following the relationship between Big Tech and public policy can get distracted — and exhausted — by the constant rush of events on 24-hour media and the loud personalities. As Madness sang in “Our House”: there’s always something happening, and it’s usually quite loud. It is more difficult to take a step back and to analyse situations both in terms of tactics and strategy of the companies and the authorities involved. Impulsive figures such as Elon Musk, the owner of X (previously Twitter), and inconsistent decision makers such as Meta’s Mark Zuckerberg, can misdirect us from what their companies are rationally seeking to achieve. And there have been a couple of events that indicate that such companies are not as strong and powerful as their cheerleaders and critics seem to believe. Indeed, the providers of American social media platforms are weak in the face of a particular obstacle. For it is weakness, and not strength, that explains their recent behaviour. The obstacle is regulation by jurisdictions outside the US — primarily in the European Union, but also elsewhere such as Brazil and China. The social media platforms have realised that they cannot win the battles with foreign governments and legal systems by themselves. They are not powerful enough to solve their own problems. They need help. Supreme Court justice Alexandre de Moraes, who blocked X in Brazil last year after Elon Musk’s social media platform refused to suspend some accounts © Dado Galdieri/New York Times/Redux/Eyevine One example here is how X and other business interests directed by Musk went through the motions of opposing the order of the Brazilian Supreme Court to take down offending material, only to capitulate and to perform the obligations imposed by the Brazilian judicial system and local law. X huffed and puffed, but the only house that was blown down was its own. This corporate weakness in the face of determined state action should not be surprising. In any ultimate battle, the state will prevail over a corporation for the simple reason that a corporation as a legal person only has legal existence and entitlements to the extent set out by legislation. Those who control the law can, if they want, control and tame any corporate in their jurisdiction. This is why, for example, the most powerful corporation the world had then seen — the East India Company — was summarily dissolved by the British parliament in 1874. It is also why the Bell System of telecommunications companies was broken up by US antitrust law and policy in the 1980s. Companies can be very powerful — but there is always something stronger on which they depend for legal recognition. Large companies therefore place great reliance on being able to influence public policy and lawmaking. This explains what Meta did, for instance, with the appointment of the pro-European former UK deputy prime minister Nick Clegg as vice-president of global affairs and communication. That was a good choice for a company seeking to constructively influence the formulation and implementation of EU policy. As the surrender of Musk and X to the Brazilian courts shows, state power is likely to always ultimately win against the platforms if tested But there is only so much that can be done by utilising contacts and quiet consultation. The friendly approach did not prevent the EU’s Digital Services Act. It did not prevent a €797.72mn fine for antitrust breaches. It did not prevent a €1.2bn fine for data breaches. Meta’s policy of constructive dialogue with the EU was failing badly. There was a looming contradiction between what Meta wants from its social media platforms in the jurisdiction of the EU and what the EU is willing to accept. Smiles and handshakes were no longer enough. The re-election of Donald Trump to the presidency of the United States provided Meta with a glorious opportunity to pivot from futile co-operation with the EU to confrontation and coercion. If Meta could get the US government onside in its battles with the EU and other jurisdictions, then it would maximise its chances of success. In his Facebook announcement this week of changes to various policies, Zuckerberg candidly said that he wanted to “work with President Trump to push back on governments around the world. They’re going after American companies and pushing to censor more. The US has the strongest constitutional protections for free expression in the world . . . The only way that we can push back on this global trend is with the support of the US government.” This was listed in his pre-prepared statement as the sixth policy change, but it was plainly the most important — for it also explained the other five points, which included abandoning fact-checking and moving content moderation from California to a “less biased” Texas. Everything in that statement went towards aligning Meta with the values and priorities of the new administration. For a corporation in the predicament of Meta this makes perfect commercial sense, even if it does violence to previously expressed sentiments. This is not an example of a company suddenly acting irrationally, but of a company rationally responding to one political development so as to facilitate defeating a regulatory challenge. And it is not the only tactic serving this broader commercial strategy. The leaders of many tech companies have every interest in promoting the new US government and in weakening resolve in the EU. Member states with leaders sympathetic to Trump, such as Hungary and Italy, are being courted alike so that EU policy can be weakened from the inside. The tech giants are adopting this robust strategy not because they are strong — they know that, like X in Brazil, they cannot take on any determined government or legal system in a significant market and win. They are doing this because they know they are weak, and that they need allies. Their business model depends upon it. And as the business models of most social media platforms require engagement above all — for without engagement you cannot have data mining and monetising and advertising — it really does not matter that the engagement is generated and amplified by misinformation and disinformation. Moderation and fact-checking are expensive. If the social media platforms were obliged, under pain of legal sanction, to make such moderation and fact-checking work, then that would be the commercial way forward. International corporations will tend to comply with the applicable law, and the expense of compliance is a cost of business. But not having such procedures and policies in place is far cheaper and more profitable. So if they can avoid such obligations they shall — and if “soft” lobbying does not work then they will look to governments to do the hard work of coercion. If Meta and X were confident in heading off the regulatory impositions of the EU, Brazil and elsewhere, they would not need to swing behind Trump and the new administration. The fact that they are doing so openly and unapologetically — indeed shamelessly — means they know they have a challenge, and one which they may not meet. They know that certain foreign governments and legal systems are capable of winning any head-to-head regulatory battle. For as the surrender of Musk and X to the Brazilian courts shows, state power is likely to always ultimately win against the platforms if tested. But that was an extreme situation: regulation is an ongoing phenomenon, and exciting and dramatic court cases should be an exception. More useful on a day-to-day basis is for regulators to be put in their place. The recent appointments at board level at Meta look like it is preparing for battle, and one in which its current commercial model requires it to defeat the aims of foreign governments. The new appointments make a lot of strategic sense. And if it does play this situation well, with the US government bullying other states for the benefit of the platforms, this is a battle and a war that the tech companies can win — not because of how they played to their strengths but because of how they covered their weaknesses. The question is now whether the EU, Brazil and others have the determination and the stomach for what will become an ugly public multinational row. Nonetheless there is a fight ahead: over who shall regulate the social media platforms that in turn are influential in shaping (and contaminating) public discourse. David Allen Green, a regulatory and media lawyer, is an FT contributing editor How Public Polling Has Changed in the 21st Century Author: Keeter, Courtney Kennedy, Dana Popky and Scott, Date: 2023-04-19T19:53:44+00:00 Collections: PollMethods Zotero Key: V8MY38LD Cite Key: Keeter23pollingChanged21cent Zotero Item | Lit Note 61% of national pollsters in the U.S. used methods in 2022 that differed from those of 2016 The 2016 and 2020 presidential elections left many Americans wondering whether polling was broken and what, if anything, pollsters might do about it. A new Pew Research Center study finds that most national pollsters have changed their approach since 2016, and in some cases dramatically. Most (61%) of the pollsters who conducted and publicly released national surveys in both 2016 and 2022 used methods in 2022 that differed from what they used in 2016. The study also finds the use of multiple methods increasing. Last year 17% of national pollsters used at least three different methods to sample or interview people (sometimes in the same survey), up from 2% in 2016. This study captures what changes were made and approximately when. While it does not capture why the changes were made, public commentary by pollsters suggests a mix of factors – with some adjusting their methods in response to the profession’s recent election-related errors and others reacting to separate industry trends. The cost and feasibility of various methods are likely to have influenced decisions. This study represents a new effort to measure the nature and degree of change in how national public polls are conducted. Rather than leaning on anecdotal accounts, the study tracked the methods used by 78 organizations that sponsor national polls and publicly release the results. The organizations analyzed represent or collaborated with nearly all the country’s best-known national pollsters. In this study, “national poll” refers to a survey reporting on the views of U.S. adults, registered voters or likely voters. It is not restricted to election vote choice (or “horserace”) polling, as the public opinion field is much broader. The analysis stretches back to 2000, making it possible to distinguish between trends emerging before 2016 (e.g., migration to online methods) and those emerging more recently (e.g., reaching respondents by text message). Study details are provided in the Methodology. Other key findings from the study include: Pollsters made more design changes after 2020 than 2016. In the wake of the 2016 presidential election, it was unclear if the polling errors were an anomaly or the start of a longer-lasting problem. 2020 provided an answer, as most polls understated GOP support a second time. The study found that after 2020, more than a third of pollsters (37%) changed how they sample people, how they interview them, or both. This compares with about a quarter (26%) who made changes after 2016. As noted above, though, these changes did not necessarily occur because of concerns about election-related errors. The number of national pollsters relying exclusively on live phone is declining rapidly. Telephone polling with live interviewers dominated the industry in the early 2000s, even as pollsters scrambled to adapt to the rapid growth of cellphone-only households. Since 2012, however, its use has fallen amid declining response rates and increasing costs. Today live phone is not completely dead, but pollsters who use it tend to use other methods as well. Last year 10% of the pollsters examined in the study used live phone as their only method of national public polling, but 32% used live phone alone or in combination with other methods. In some cases, the other methods were used alongside live phone in a single poll, and in other cases the pollster did one poll using live phone and other polls with a different method. Several key trends, such as growth of online polling, were well underway prior to 2016. While the 2016 and 2020 elections were consequential events for polling, the study illustrates how some of the methodological churn in recent years reflects longer-term trends. For example, the growth of online methods was well underway before 2016. Similarly, some live phone pollsters had already started to sample from registered voter files (instead of RDD, random-digit dialing) prior to 2016. Use of probability-based panels has become more prevalent. A growing number of pollsters have turned to sampling from a list of residential addresses from the U.S. Postal Service database to draw a random sample of Americans, a method known as address-based sampling (ABS). There are two main types of surveys that do this: one-off or standalone polls and polls using survey panels recruited using ABS or telephone (known as probability-based panels). Both are experiencing growth. The number of national pollsters using probability-based panels alone or in combination with other methods tripled from 2016 to 2022 (from seven to 23). The number of national pollsters conducting one-off ABS surveys alone or in combination with other methods during that time rose as well (from one in 2016 to seven in 2022). The growth of online opt-in among national pollsters appears to have paused after 2020. The number of national pollsters using convenience samples of people online (“opt-in sampling”) – whether alone or in combination with other methods – more than quadrupled between 2012 and 2020 (from 10 to 47). In 2022, however, this number held flat, suggesting that the era of explosive growth could be ending. Whether changes to sample sources and modes translate into greater accuracy in presidential elections remains to be seen. The fact that pollsters are expanding into new and different methods is not a guarantee that the underrepresentation of GOP support occurring in 2016 and 2020 preelection polls has been fixed. Polling accuracy improved in 2022, but this represents only one nonpresidential election. Notable study limitations A study of this nature requires difficult decisions about what exactly will be measured and what will not. This study focuses on two key poll features: the sample source(s) – that is, where the respondents came from – and the mode(s), or how they were interviewed. While important, these elements are not exhaustive of the decisions required in designing a poll. The study did not attempt to track other details, such as weighting, where public documentation is often missing. Because the study only measured two out of all possible poll features, estimates from this study likely represent a lower bound of the total amount of change in the polling industry. Another limitation worth highlighting is the fact that state-level polls are not included. Unfortunately, attempting to find, document and code polling from all 50 states and the District of Columbia would have exceeded the time and staff resources available. A related consideration is that disclosure of methods information tends to be spottier for pollsters who exclusively work at the state level, though there are some exceptions. It is not clear whether analysis at the level of detail presented in this report would be possible for state-only pollsters. While not necessarily a limitation, the decision to use the polling organization rather than individual polls as the unit of analysis has implications for the findings. The proliferation of organizations using online methods implies but does not prove that online polls grew as well. However, research conducted by the American Association for Public Opinion Research (AAPOR) following the 2016 and 2020 elections reveals an explosion in the share of all polling done using online methods. AAPOR estimated that 56% of national polls conducted shortly before the 2016 election used online methods; the comparable share for 2020 was 84%. More details on the strengths and weaknesses of the study are presented in the Methodology. In an attempt to verify the accuracy of the categorization of polling methodologies, researchers attempted to contact all organizations represented in the database. Several pollsters contacted for this study noted that use of a particular method was not necessarily an endorsement of methodological quality or superiority. Instead, design decisions often reflect a multitude of factors. Survey cost – especially the increasing cost of live phone polling – came up repeatedly. Timing can also be a factor, as a design like address-based sampling can take weeks or even months to field. As noted above, this study does not attempt to address why each organization polled the way they did. It aims only to describe major changes observable within the polling industry. Nor does it evaluate the quality of different methods, as a multitude of other studies address that question. Changes to polling after 2020 differed from those after 2016 The study found a different kind of change within the polling industry after 2020 versus 2016. After 2020, changes were both more common and more complex. More than a third (37%) of pollsters releasing national public polls in both 2020 and 2022 changed their methods during that interval. By contrast, the share changing their methods between 2016 and 2018 was 26%. The nature of the changes also differed. About half of the changes observed from 2016 to 2018 reflected pollsters going online – either by adding online interviewing as one of their methods or fully replacing live phone interviewing. By contrast, the changes observed from 2020 to 2022 were more of a mix. During that period, some added an approach like text messaging (e.g., Change Research, Data for Progress), probability-based panels (Politico, USA Today) or multiple new methods (CNN, Wall Street Journal). About a quarter of the change observed from 2020 to 2022 reflected pollsters who had already moved online dropping live phone as one of their tools (e.g., CBS News, Pew Research Center). A look at change over the entire recent period – from 2016 to 2022 – finds that more than half of national public pollsters (61%) used methods in 2022 that differed from those they used in 2016. As noted above, if features like weighting protocols were included in the analysis, that rate would be even higher. A longer view of modern public polling (going back to 2000) shows that methodological churn began in earnest around 2012 to 2014. That was a period when about a third of national pollsters changed their methods. Change during that period was marked by pollsters starting to migrate away from live telephone surveys and toward online surveys. Pollsters increasingly use multiple methods – sometimes three or more Pollsters are not just using different methods, many are now using multiple methods, the study found. Here again there is a discernable difference in how polls changed after 2016 and how they changed after 2020. After 2016, the share of pollsters using multiple methods remained virtually unchanged (30% in both 2016 and 2018). After 2020, however, the share climbed to 39%. Notably, the share of pollsters using three or more different methodologies in their national public polls tripled from 5% in 2020 to 17% in 2022. In this analysis, “multiple methods” refers to use of multiple sample sources (e.g., registered voter files and random-digit dial) or multiple interview modes (e.g., online, mail, live telephone). In some cases, several methods were used in a single poll. In other cases the pollster did one poll using one method and another poll using another method. As an example, in 2014 Pew Research Center switched from exclusively using live phone with random-digit-dial sample to also using a probability-based panel. In 2020 the Center added an additional method, one-off address-based sample surveys offering online or mail response. By 2022, the Center dropped live phone polling. Pollsters that used at least three different methods in 2022 include CNN, Gallup, NPR, Politico and USA Today. Text messaging and address-recruited panels see growth after 2020 An overarching theme in the study is the growth of new methods. Analysis earlier in this report aimed to describe trends for the most prominent methods. In the past, pollsters often used just one method (e.g., live phone with random-digit dial). That has changed. Today pollsters tend to use new methods (such as text) as one of several ways that they reach people. To track the trajectory of these newer methods, it helps to consider the number of pollsters using the method by itself or in combination with other methods. A prime example is text message polling. An extremely small share of pollsters conduct national public polls exclusively by text. A larger share use text alongside another method, such as online opt-in. How texting is used varies. In some cases respondents receive a text with a web link for an online survey. In other cases, respondents answer the questions via text. Among the pollsters in this study, just one used texting in a national public survey in 2020. In 2022 that number rose to nine, representing 13% of the active national pollsters tracked that year. These figures reflect the number of pollsters using texting alone or in combination with other methods like live phone. Analysis looking at methods used either alone or in combination with other approaches also suggests a change in the trajectory of online opt-in polling. While online opt-in usage grew tremendously between 2006 and 2020, that growth appears to have slowed if not stopped in 2022 for national polling. By contrast, the share of national pollsters turning to probability-based panels continues to grow. In 2022 a third (33%) of national pollsters used probability-based panels either alone or in combination with other methods. This is up from roughly 10% during most of the 2010s. Live phone was once the dominant method of polling but has been in decline since 2016. As of 2022, about a third of national pollsters used live phone alone or in combination (32%), while a much smaller share relied on it as their only method (10%). The study also tracked the adoption of a specific kind of opt-in sample – members of an online opt-in panel who are matched to a record in a national registered voter file. This study first observed that approach in 2018. In 2018, 2020 and 2022, about 3% to 5% of national public pollsters used online opt-in samples matched to registered voter files, the study found. Wokeness: a guide for advocates Author: Navigator Research, Date: 2023 Collections: IdentityPolitics Zotero Key: AGUYM989 Cite Key: Navigator23wokeGuide Zotero Item | Lit Note Update: Tuesday, April 4[th] Wokeness: A Guide f Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days.Ad t • Just over half say they are hearing about “wokeness,” while pluralities say they are not sure what it means. • Americans are most concerned about elected officials talking too much about “wokeness” at the expense of other issues, and they want Republican elected officials to talk about “wokeness” less. • Two in three say “anti-woke” politicians are “focused on the wrong things,” “trying to divide us,” and are concerned Republicans focused on wokeness talk about taking on “woke corporations” while “pushing for tax cuts for their wealthy donors, the rich, and corporations.” Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Just Half of Americans Have Heard About “Wokeness,” With Fox News-Watching Republicans Driving Awareness Democrats (53% “a lot”/”some”) and Republicans (59%) are hearing more about “wokeness” than independents (42%). • Fox News-watching Republicans are hearing the most about “wokeness” (66%), followed by MSNBC (60%) and CNN (58%) viewers. How much have you seen, read, or heard about \"wokeness\"? Total A A lot Some Not much/Nothing Lot/Some Overall 26 28 46 54 Democrats 27 26 47 53 Independents 16 25 58 42 Republicans 28 30 41 59 Black 22 24 54 47 Hispanic 25 30 45 55 White 27 28 45 55 AAPI 24 33 43 57 Fox News GOP* 36 30 34 66 Non-Fox News GOP 21 31 49 51* CNN viewers** 29 29 42 58 MSNBC viewers** 30 30 40 60 |Col1|A lot Not much/Nothing Some|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col2 1|Col22|Col23|Col24|Col25| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |||||||||||||||||||||||||| ||26||||||28||||||||||46|||||||| |||||||||||||||||||||||||| ||27|||||||26||||||||47||||||||| ||16|25|||||||||||58|||||||||||| ||28||||||||30||||||||||||41|||| |||||||||||||||||||||||||| ||22|||24||||||||||54||||||||||| ||25|||||30||||||||||||45||||||| ||27|||||||28||||||||||45||||||| ||24||||33||||||||||||||43|||||| |||||||||||||||||||||||||| |Fox News GOP*|36|||||||||||30|||||||||||34|66|* ||21||31||||||||||||49|||||||||| ||29|||||||||29||||||||||42||||| ||30||||||||||30|||||||||||40||| |||||||||||||||||||||||||| ”Fox News GOP” are those who identify as Republicans and say Fox News is one of their “main sources of news about politics and current events in the U.S.” Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Very Conservative GOP Are Twice as Likely to Hold Negative Views of \"Wokeness\" Than Americans Overall Two in five Americans (39%), including half of independents (51%), say the term “means different things to everyone” or they “don’t know enough to say.\" Very conservative Republicans drive the share who describe it as “policies and ideas that are going too far” (71%). Which best describes \"wokeness\" to you? Policies and ideas that are pushing Not sure: \"wokeness\" means our country forward on issues like different things to everyone race and gender, increasing and doesn't have a clear tolerance, and striving for equality definition/Don’t know Overall 27 39 Democrats 43 41 Independents 18 51 Non-very cons. Republicans 15 38 Very cons. Republicans 7 22 Black 36 43 Hispanic 33 35 White 26 38 AAPI 29 39 Policies and ideas that are going too far, promoting dangerous and divisive ideas on issues like race and gender 27 Col2 Col3 Col4 Col5 Col6 Col7 Col8 Col9 Col10 Col11 39 Col13 Col14 Col15 Col16 34 Col18 Col19 Col20 Col21 Col22 43 41 16 15 38 47 33 35 32 29 39 32 Nationwide survey of 1,000 registered voters conducted March 23-March 27, 2023.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Americans Cite Critical Race Theory, Black Lives Matter, and Issues Related to Gender as “Woke” Policies What is an example of a \"woke\" policy to you? Democrats Independents Republicans Nationwide survey of 1,000 registered voters conducted March 23-March 27, 2023.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Partisanship Drives Views of Topics Like DEI, SEL, ESG, and CRT, Though Many Americans Are Unfamiliar With Each “Diversity, equity, and inclusion” is viewed favorably by Democrats (net +56 favorable), independents (net +16), and across racial groups (from net +15 among white Americans to net +50 among Black Americans); Republicans stand alone in their unfavorable views (net -12). • Republicans also view social and emotional learning (net -1), ESG (net -18), and critical race theory (net -51) unfavorably. Please indicate how favorable or unfavorable you are to each one. Net Favorable Favorable Don’t know Unfavorable Overall Dem Ind Rep Black Hisp White AAPI +23 +56 +16 -12 +50 +30 +15 +42 Environmental, social, and corporate governance, also known as ESG Critical race theory +20 +42 +14 -1 +33 +31 +17 +29 +10 +39 -10 -18 +24 +14 +4 +24 -8 +31 -12 -51 +27 +7 -17 -4 |Col1|Please indicate how favorable or unfavorable you are| |---|---| ||| ||Diversity, equity, and inclusion, also known as DEI| ||Social and emotional learning, also known as SEL| ||Environmental, social, and corporate governance, also known as ESG| ||Critical race theory| |Unfavorable Favorable Don’t know|Col2|Col3|Col4|Col5|Col6|Col7|Col8| |---|---|---|---|---|---|---|---| ||||||||| |47||||29||24|| ||||||||| |38|||44||||18| ||||||||| |35||40||||25|| ||||||||| |30|32||||38||| Nationwide survey of 1,000 registered voters conducted March 23-March 27, 2023.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Elected Officials Focusing Too Much on Wokeness at the Expense of Other Issues Bothers Americans Most Roughly two in five Democrats (41%), independents (43%), and Republicans (38%) are more bothered by “elected officials talking too much about things like ‘wokeness’” than by elected officials standing in the way of progress or being too “woke.” Which bothers you most? Elected officials talking too much about Elected officials being too \"woke\" Elected officials standing in things like \"wokeness\" instead of and pushing extreme ideas about the way of progress on focusing on the day-to-day issues that gender and race while censoring issues like race and gender really matter to people's lives freedom of speech Overall 28 40 32 Democrats 42 41 17 Independents 26 43 31 Non-very cons. Republicans 15 41 44 Very cons. Republicans 13 34 53 Black 46 30 23 Hispanic 30 40 30 White 25 41 34 AAPI 27 46 26 |Elected officials talking too much about things like \"wokeness\" instead of focusing on the day-to-day issues that really matter to people's lives Elected officials standing in the way of progress on issues like race and gender Elected officials being too \"woke\" and pushing extreme ideas about gender and race while censoring freedom of speech|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |||||||||||||||||||| |28||||||40|||||||32|||||| |||||||||||||||||||| |42||||||||41||||||||||17| |26||||43||||||||||31||||| |15||41|||||||||44|||||||| |13|34|||||||||53||||||||| |||||||||||||||||||| |46|||||||||30||||||||23|| |30|||||||40||||||||30|||| |25|||41|||||||||34||||||| |27|||||46|||||||||||26||| |||||||||||||||||||| Nationwide survey of 1,000 registered voters conducted March 23-March 27, 2023.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. A Majority Say Republicans Should Spend Less Time Talking About “Wokeness” Majorities of Democrats (63%) and independents (52%) say “Republicans should talk about wokeness less,” along with half of not very conservative Republicans (50%) and even 41% of very conservative Republicans. How do you feel about the amount of time Republicans are spending talking about “wokeness”? Republicans should talk Republicans are talking about Republicans should talk Net Should Talk about wokeness more wokeness the right amount about wokeness less About It Less Overall Democrats Independents Non-very cons. Republicans Very cons. Republicans Black Hispanic White AAPI +31 +39 +30 +28 +16 +13 +34 +35 +34 |Republicans are talking about wokeness the right amount Republicans should talk about wokeness more Republicans should talk about wokeness less|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||||||| |24||||22||||||55|||||| ||||||||||||||||| |24||||13|||||||||||63| |22||26|||||||||||||52| |22||28|||||||||||||50| |25|||||34||||||||||41| ||||||||||||||||| |35||||||17|||||||48||| |22||23|||||||56||||||| |21|22|||||||56|||||||| |23|||20|||||57|||||||| ||||||||||||||||| Nationwide survey of 1,000 registered voters conducted March 23-March 27, 2023.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Americans Say “Anti-Woke” Politicians Are Focused on the Wrong Things, Including Division and Extreme Views Two in three say “focused on the wrong things” (68% well) and “trying to divide us… because they think it helps them gain more political power” (67%) describe candidates who call themselves “anti-woke” or “against wokeness” well. • While 62% of Americans think that candidates who describe themselves as \"anti-woke\" are “censoring freedom of speech,” just 49% say those politicians are “protecting freedom of speech,” driven by Republicans (net +20 well). Below is a list of statements some have used to describe politicians who call themselves “anti-woke” or “against wokeness.” Please indicate how well each of these describes a politician who calls themselves “anti-woke.” Net Well /* Split sample Well Not well Overall Dem Ind Rep** Focused on the wrong things, instead of the issues that matter to me like 68 32 +36 +54 +26 +20 inflation and rising costs Trying to divide us, splitting us by race, religion, and sexual orientation 67 33 +34 +58 +16 +14 because they think it helps them gain more political power Extreme 64 36 +28 +50 +8 +8 Trying to divide us, splitting us by race, religion, and sexual orientation 63 37 +26 +54 +26 -4 Censoring freedom of speech and diversity of thought 62 38 +24 +56 +16 -8 Protecting traditions like faith and family 54 46 +8 -6 -8 +26 Looking out for the well-being of children and families 49 51 -2 -24 -10 +24 Protecting freedom of speech and diversity of thought 49 51 -2 -20 -2 +20 |Col1|each of these describes a politician who calls themselves “anti-wo /* Split sample|** |---|---| ||Focused on the wrong things, instead of the issues that matter to me like inflation and rising costs| ||Trying to divide us, splitting us by race, religion, and sexual orientation because they think it helps them gain more political power| ||Extreme| ||Trying to divide us, splitting us by race, religion, and sexual orientation| ||Censoring freedom of speech and diversity of thought| ||Protecting traditions like faith and family| ||Looking out for the well-being of children and families| || Protecting freedom of speech and diversity of thought| |Well|Col2|Col3|Col4|Not well| |---|---|---|---|---| |68||||32| |67||||33| |64|||36|| |63|||37|| |62|||38|| |54||46||| |49|51|||| |49|51|||| Nationwide survey of 1,000 registered voters conducted March 23-March 27, 2023.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. Strongest Frame Against GOP Focuses on Their Use of “Woke Corporations\" Rhetoric While Rewarding the Rich With Tax Breaks Republicans who feel their party is spending too much time talking about wokeness* are most concerned by “anti-woke” Republicans who talk about taking on “woke corporations” while looking out for the rich (50% concerning), focus on wokeness and are distracted from issues that really matter (42%), and use it to divide the country on race (40%).* Below is a list of things people have said about Republicans who say they are “anti-woke.” Please indicate how concerning you find each one. If you don't believe that statement, please indicate that as well. Not Total Concerning /* Split sample** Concerning Don’t believe concerning Dem Ind Rep Not Anti-Woke GOP** Republicans talk about taking on “woke corporations,” but in reality, they continue to push for tax cuts for their wealthy donors, the rich, and 64 24 12 87 64 38 50 corporations, while failing to look out for working class and everyday Americans Republicans are so obsessed with wokeness that they aren't paying attention to 59 26 15 82 60 32 42 the issues that really matter, like inflation and the cost of living Republicans are using the word “woke” to just continue to divide our country 58 27 15 84 54 31 40 around the issue of race Republicans talk about taking on “woke corporations,” but in reality, they oppose regulations that protect American families – like regulating what 56 24 20 80 56 29 33 corporate polluters can release into our air and water and regulations that could prevent dangerous incidents like the recent train derailment in Ohio “Wokeness” has no clear meaning: Republicans are just using the term “woke” to describe anything they don't like – a policy, a business, or even voters they 55 26 19 79 52 28 34 represent |Not concerning Concerning Don’t believe|Col2|Col3|Col4|Col5|Col6|Col7| |---|---|---|---|---|---|---| |||||||| |64|||24|||12| |||||||| |59||26|||15|| |||||||| |58||27|||15|| |56|24|||20||| |||||||| |55|26|||19||| |||||||| ”Not anti-woke GOP” are self-identified Republicans who say that Republicans should talk about “wokeness” less on another question.Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. About Navigator In a world where the news cycle is the length of a tweet, our leaders often lack the real-time public-sentiment analysis to shape the best approaches to talking about the issues that matter the most. Navigator is designed to act as a consistent, flexible, responsive tool to inform policy debates by conducting research and reliable guidance to inform allies, elected leaders, and the press. Navigator is a project led by pollsters from Global Strategy Group and GBAO along with an advisory committee, including: Jessica Floyd, The Hub Project; Christina Reynolds, EMILY’s List; Mike Podhorzer, AFL-CIO; Jesse Ferguson, progressive strategist; Navin Nayak, Center for American Progress Action Fund; Stephanie Valencia, EquisLabs; and Melanie Newman, Planned Parenthood Action Fund. For Press inquiries contact: press@navigatorresearch.org To learn more about Navigator: http://navigatorresearch.org @NavigatorSurvey on Twitter About the Study Global Strategy Group conducted public opinion surveys among a sample of 1,000 registered voters from March 23-March 27, 2023. 100 additional interviews were conducted among Hispanic voters. 80 additional interviews were conducted among Asian American and Pacific Islander voters. 100 additional interviews were conducted among African American voters. 102 additional interviews were conducted among independent voters. The survey was conducted online, recruiting respondents from an opt-in online panel vendor. Respondents were Nationwide surveys of registered voters; Each wave represents approximately 1,000 interviews taken over the prior three-five days. ifi d i fil d i l k h d hi i i f l After backing Trump, low-income voters hope he doesn’t slash their benefits Author: Craig, Tim, Date: 2024-12-26 Collections: Hot Takes US Elect 2024 Zotero Key: WN79YLN8 Cite Key: Craig24lowIncTrumpVoterHope Zotero Item | Lit Note NEW CASTLE, Pennsylvania — Lori Mosura goes to the grocery store on a bicycle because she can’t afford to fix her Ford F-150 truck. The single mother and her 17-year-old son live in an apartment that is so small she sleeps in the dining room. They receive $1,200 each month in food stamps and Social Security benefits but still come up short. Mosura said she often finds herself deciding between buying milk or toilet paper. It was all that penny-pinching that drove the part-time tax consultant to abandon the Democratic Party this fall and vote for Donald Trump. “He is more attuned to the needs of everyone instead of just the rich,” Mosura, 55, said on a recent afternoon. “I think he knows it’s the poor people that got him elected, so I think Trump is going to do more to help us.” Trump carried the Pennsylvania city of New Castle by about 400 votes, becoming the first Republican presidential candidate to win here in nearly 70 years. More than 1 in 4 residents live in poverty, and the median income in this former steel and railroad hub ranks as one of the lowest in Pennsylvania. New Castle’s poorest residents weren’t alone in putting their faith in Trump. Network exit polls suggest he erased the advantage Democrats had with low-income voters across the country. Trump signs along Richelieu Street in New Castle. (Jeff Swensen for The Washington Post) Mary Defrank reads her Bible at a laundromat in New Castle. (Jeff Swensen for The Washington Post) Fifty percent of voters from families with an income of less than $50,000 a year cast their ballots for Trump, according to the data, compared with 48 percent for Vice President Kamala Harris. Four years ago, President Joe Biden carried those voters by 11 percentage points; Hillary Clinton won them by 12 points in 2016 and former president Barack Obama by 22 points in 2012. Now, low-income Americans who voted for Trump say they are counting on him to keep their benefits intact even while his Cabinet picks and Republican lawmakers call on him to reduce federal spending. Elon Musk and Vivek Ramaswamy — whom Trump has chosen to lead a new nongovernmental advisory panel, the “Department of Government Efficiency” — have said they want to trim $2 trillion from the government’s annual budget,a cut that some experts say could be accomplished only by slashing entitlement programs. Trump’s pick for White House budget director was a key architect of Project 2025, a plan drawn up by conservatives to guide his second term that calls for steep cuts to programs such as food stamps. And GOP leaders in Congress and Trump advisers are considering significant changes to Medicaid, food stamps and other federal aid. The uncertainty comes after last week’s high-stakes showdown in Congress over the federal funding bill. Lawmakers narrowly avoided a shutdown after agreeing to fund the government until March. House Speaker Mike Johnson (R-Louisiana) had unveiled a bipartisan bill to put off a shutdown, but Trump and Musk railed against what they said was unnecessary spending in the initial package. “Everybody is on hyperalert,” said Tom Scott, the chief executive officer of Lawrence County Community Action Partnership, a social service agency that helps New Castle residents. “You have to be concerned because you don’t know which programs could be targeted” for spending reductions. In a recent appearance on NBC News’s “Meet the Press,” Trump said any efforts by his administration to streamline the federal government would not impact entitlement programs such as Social Security. “Americans of all backgrounds elected President Trump because of his plans to lower costs, end the financial drain of illegal immigrants on our healthcare system, and ensure that our country can continue to care for American citizens who rely on Medicaid, Medicare, and Social Security,” said Anna Kelly, a spokeswoman for his transition team. Some longtime Democrats like Mosura said they initially struggled over whether to vote for Trump. They had believed Democrats were the most likely to help the poor and disagreed with Republicans on issues like abortion. But Mosura said she kept coming back to the conclusion that Trump would put Americans like her first and improve her economic prospects. Mosura said she has been unable to find full-time work in her field and is planning to change her party affiliation to Republican. But she also gets anxious when she hears GOP politicians talk about reducing government spending. “We helped get you in office; please take care of us,” Mosura said,shifting the conversation as though she were speaking to Trump. “Please don’t cut the things that help the most vulnerable.” Employees of the Two Fat Guys and an Oven bakery in New Castle shoot a Christmas photo on Dec. 20. (Jeff Swensen for The Washington Post) ‘Serious, serious trouble’ Located about an hour north of Pittsburgh, New Castle is the third-largest incorporated city in western Pennsylvania. After it was founded around the turn of the 19th century, the city became a major destination for Italian immigrants. New Castle emerged as an industrial hub known for its tin mills in the 1800s. The new residents built a lively downtown filled with shops and bars. New Castle was also home to a thriving labor movement that helped shape the city’s historically left-leaning politics. Before Trump won New Castle, the city had last backed a Republican presidential candidate in 1956, when voters narrowly supported Dwight D. Eisenhower (R) over Adlai Stevenson (D), according to Andrei Pagnotta, a resident who has spent years studying the region’s election results. But the city changed dramatically as factories closed and younger residents moved to more vibrant urban areas. The busiest destinations downtown today are the 60-year-old Capitol Grill bar, which sells $2.75 Bud Lights, and Hazel’s Restaurant, where guests can get a stuffed pork chop dinner for $11.99.The city’s population of 21,000 is roughly half what it was during its peak in the 1940s. A portrait of Lewis Stiles Hoyt, an oil and coal baron, at the mansion built by his son in New Castle. (Jeff Swensen for The Washington Post) The former Shenango China Factory in New Castle. (Jeff Swensen for The Washington Post) Federal benefits have helped keep residents afloat, and today that safety net is deeply interwoven with daily life. The 8.5-square-mile city includes 10 public housing projects. About 60 percent of the houses in the city are rental properties, and federal housing vouchers help countless families afford rent. Many residents also receive Medicaid and food stamps. More than half of the Lawrence County Community Action Partnership’s $32 million budget comes from federal aid, said Scott, the organization’s leader. That helps it do things like shuttle low-income residents to medical appointments and offer rental assistance. Federal aid is also key in keeping New Castle’s children fed. About 90 percent of students come from low-income families and qualify for free school lunches, and many are sent home with food to eat over the weekend. “It’s a very depressed area, so if our funding were to go away, and I have not heard it will, but if it were to go, we would be some in serious, serious trouble,” said Gregg Paladina, superintendent of the New Castle Area School District. Lori Mosura with her son, Garrett Miller, in New Castle. (Jeff Swensen for The Washington Post) Milk or toilet paper A decade ago, Mosura was a fervent supporter of the Clintons. She recalls meeting former president Bill Clinton in New Castle at a rally during Hillary Clinton’s 2008campaign for the White House. But the single mother said she began to drift away from Democrats as the party’s promises to help people like herself began to ring hollow. She said she’s been “suffering a lot” because of rising prices. Groceries she once thought of as staples — like soda — are now luxuries. “People are struggling now more than ever, in this city especially,” said Mosura, who pays $375 a month for her apartment. She said she’s “prayed more in the past year than ever before” because of her mounting bills. New Castle was a bastion for Democratic votes up until this year. But in larger Lawrence County, the shift toward the GOP has been ongoing. White, working class Americans were increasingly drawn to Republican candidates, in part because of their messaging on security and immigration. According to Census data, White residents make up three-fourths of New Castle’s population, while Black residents account for 10 percent. The population also includes 10 percent of residents who identify as mixed race and five percent who are Hispanic. Joseph’s Marketplace, a family-owned grocer in New Castle. (Jeff Swensen for The Washington Post) Lynne Ryan, chairwoman of the Lawrence County Republican Party, said Democrats lost the support of New Castle residents over their handing of illegal immigration and foreign aid.She said Trump was a skilled politician whom low-income voters found to be honest and relatable. “Trump won’t cut necessary programs, and nowhere has he said he is cutting any of that,” Ryan said. “He is cutting bloated government. He is not cutting programs that work for the American people.” City Administrator Chris Frye, a Republican and former mayor of New Castle, said he expects GOP leaders will push for some changes to how federal programs are administered. But Frye urged his party to show “empathy” when it comes to determining the actual benefits that people receive. “I think it would be stupid to just take something away,” Frye said. “We would have mass chaos. Mass homelessness … so nationally, I don’t think it is going to be a situation where they are taking away from people.” Detractors worry that New Castle residents will come to regret their votes. “I have big concerns those who voted for him didn’t realize what they are doing and it’s going to affect their family and friends and probably themselves, too,” said Timothy Buck, chairman of the Lawrence County Democratic Party. Steve Tillia shows off his Trump flag in front of his New Castle home. (Jeff Swensen for The Washington Post) ‘Cutting the fat’ On the 100 block of Richelieu Street in New Castle’s Lower East Side neighborhood, many residents rely on some form of government assistance — and most people support Trump. More than a month after the election, Trump signs and flags were still planted in yards and on porches. Steve Tillia, 59, receives $1,600 a month in Social Security disability payments and $300 in food stamps to support himself and his son. Tillia, who said he is unable to work after suffering from mini strokes, still drives around New Castle with a Trump flag anchored on the bumper of his SUV. Tillia said he’s confident that Trump and GOP leaders will reduce spending by “cutting the fat” out of government — and not slashing benefits. “It’s not cutting government programs, it’s cutting the amount of people needed to run a program,” he said. “They are cutting staff, which could actually increase the amount of the programs that we get.” Richelieu Street on Dec. 20. (Jeff Swensen for The Washington Post) Tillia’s neighbor, Dawn Simmons, nodded her head in agreement. Simmons receives $900 a month in Social Security benefits and $171 a month in food stamps, and she also said Trump’s decisions may even lead to enhanced benefits in the coming years because he plans to “put Americans first.” The neighbors on Richelieu Street said they are broadly supportive of Trump’s Cabinet picks, as well as Musk’s plans for the Department of Government Efficiency. Musk has been highlighting examples of what he considers government waste on X, his social media platform — at times pushing misleading information. Some of those examples have nonetheless resonated in New Castle. “Why do we need to be studying cats in Russia?” asked Tillia, referring to a program that used cats to study spinal cord injuries. New Castle residents who didn’t support Trump said they are dismayed that their community appears to have voted against its own self-interests. As they folded clothes at the Colonial laundromat, Tom Jones, 86, and his 75-year-old wife, Lottie, said they are already hearing from parishioners at their church who worry how they will get by if federal spending is reduced. “My fear is, they will get rid of Obamacare, and mark my words, this will just be the beginning of their troubles,” Jones said. “I want to say, ‘I told you so.’ … These people who voted for him didn’t look at the big picture.” Kathy Davis at the Riverside Apartments smokers patio in New Castle. (Jeff Swensen for The Washington Post) But as Kathy Davis sat in the “smokers patio” at the Riverside Apartments, she said she is as confident as ever that Trump’s presidency will benefit her. Davis, a retired artist, subsists on a monthly $1,300 Social Security payment and $75 in food stamps. She rents her studio apartment for $385 per month. Asked whether she worries that Trump’s agenda could hurt the poor, Davis said the incoming president is “too smart for that.” “You can’t wipe out half of the population” of New Castle, Davis said. “We are old and tired and just want to be taken care of, and Trump has too much common sense, so I don’t think he is going to do anything to hurt us.” Emily Guskin contributed to this report. What Is a Likert Scale? Guide & Examples Author: Bhandari, Pritha, Date: 2020-07-03T11:13:20+00:00 Collections: NeuroPsychoLinguisticPolitics Zotero Key: EGLDIQFM Cite Key: Bhandari20likertScale Zotero Item | Lit Note A Likert scale is a rating scale used to measure opinions, attitudes, or behaviors. It consists of a statement or a question, followed by a series of five or seven answer statements. Respondents choose the option that best corresponds with how they feel about the statement or question. Because respondents are presented with a range of possible answers, Likert scales are great for capturing the level of agreement or their feelings regarding the topic in a more nuanced way. However, Likert scales are prone to response bias , where respondents either agree or disagree with all the statements due to fatigue or social desirability or have a tendency toward extreme responding or other demand characteristics. Likert scales are common in survey research, as well as in fields like marketing, psychology, or other social sciences. Download Likert scale response options What are Likert scale questions? Likert scales commonly comprise either five or seven options. The options on each end are called response anchors. The midpoint is often a neutral item, with positive options on one side and negative options on the other. Each item is given a score from 1 to 5 or 1 to 7. The format of a typical five-level Likert question, for example, could be: 1. Strongly disagree 2. Disagree 3. Neither agree nor disagree 4. Agree 5. Strongly agree In addition to measuring the level of agreement or disagreement, Likert scales can also measure other spectrums, such as frequency, satisfaction, or importance. When to use Likert scale questions Researchers use Likert scale questions when they are seeking a greater degree of nuance than possible from a simple “yes or no” question. For example, let’s say you are conducting a survey about customer views on a pair of running shoes. You ask survey respondents “Are you satisfied with the shoes you purchased?” A dichotomous question like the above gives you very limited information. There is no way you can tell how satisfied or dissatisfied customers really are. You get more specific and interesting information by asking a Likert scale question instead: “How satisfied are you with the shoes you purchased?” • 1 – Very dissatisfied* • 2 – Dissatisfied* • 3 – Unsure* • 4 – Satisfied* • 5 – Very satisfied* Likert scales are most useful when you are measuring unobservable individual characteristics, or characteristics that have no concrete, objective measurement. These can be elements like attitudes, feelings, or opinions that cause variations in behavior. How to write strong Likert scale questions Each Likert scale–style question should assess a single attitude or trait. In order to get accurate results, it is important to word your questions precisely. As a rule of thumb, make sure each question only measures one aspect of your topic. For example, if you want to assess attitudes towards environmentally friendly behaviors, you can design a Likert scale with a variety of questions that measure different aspects of this topic. Here are a few pointers: Include both questions and statements A good rule of thumb is to use a mix of both to keep your participants engaged during the survey. When deciding how to phrase questions and statements, it’s important that they are easily understood and do not bias your respondents in one way or another. Use both positive and negative framing If all of your questions only ask about things in socially desirable ways, your participants may be biased towards agreeing with all of them to show themselves in a positive light. | Environmental damage caused by single-use water bottles is a serious problem. | | | | | | --- | --- | --- | --- | --- | | Strongly disagree | Disagree | Neither agree nor disagree | Agree | Strongly agree | Respondents who agree with the first statement should also disagree with the second. By including both of these statements in a long survey, you can also check whether the participants’ responses are reliable and consistent. Avoid double negatives Double negatives can lead to confusion and misinterpretations, as respondents may be unsure of what they are agreeing or disagreeing with. | I never buy non-organic products. | | | | | | --- | --- | --- | --- | --- | | Strongly disagree | Disagree | Neither agree nor disagree | Agree | Strongly agree | Ask about only one thing at a time Avoid double-barreled questions (asking about two different topics within the same question). When faced with such questions, your respondents may selectively answer about one topic and ignore the other. Questions like this may also confuse respondents, leading them to choose a neutral but inaccurate answer in an attempt to answer both questions simultaneously. | How would you rate your knowledge of climate change and food systems? | | | | | | --- | --- | --- | --- | --- | | Very poor | Poor | Fair | Good | Excellent | Be crystal clear The accuracy of your data also relies heavily on word choice: • Pose your questions clearly, leaving no room for misunderstanding.* • Make language and stylistic choices that resonate with your target demographic.* • Stay away from jargon that could discourage or confuse your respondents.* How to write Likert scale responses When using Likert scales, how you phrase your response options is just as crucial as how you phrase your questions. Here are a few tips to keep in mind. Decide on a number of response options More options give you deeper insights but can make it harder for participants to decide on one answer. Fewer options mean you capture less detail, but the scale is more user-friendly. Usually, researchers include five or seven response options. It’s a good idea to include an odd number so that there is a midpoint. However, if you want to force your respondents to choose, an even number of responses removes the neutral option. | How frequently do you buy biodegradable products? | | | | | | --- | --- | --- | --- | --- | | Never | Occasionally | Sometimes | Often | Always | Choose the type of response option You can measure a wide range of perceptions, motivations, and intentions using Likert scales. Response options should strive to cover the full range of opinions you anticipate a participant can have. Some of the most common types of items include: • Agreement: Strongly Agree, Agree, Neither Agree nor Disagree, Disagree, Strongly Disagree • Quality: Very Poor, Poor, Fair, Good, Excellent • Likelihood: Extremely Unlikely, Somewhat Unlikely, Likely, Somewhat Likely, Extremely Likely • Experience: Very Negative, Somewhat Negative, Neutral, Somewhat Positive, Very Positive Some researchers also include a “don’t know” option. This allows them to distinguish between respondents who do not feel sufficiently informed to give an opinion and those who are “neutral” on the topic. However, including a “don’t know” option may trigger unmotivated respondents to select that for every question. Choose between unipolar and bipolar options On a unipolar scale, you measure only one attribute (e.g., satisfaction). On a bipolar scale, you can measure two attributes (e.g., satisfaction or dissatisfaction) along a continuum. | How satisfied are you with the range of organic products available? | | | | | | --- | --- | --- | --- | --- | | Not at all satisfied | Somewhat satisfied | Satisfied | Very satisfied | Extremely satisfied | Your choice depends on your research questions and aims. If you want finer-grained details about one attribute, select unipolar items. If you want to allow a broader range of responses, select bipolar items. Unipolar scales are most accurate when five-point scales are used. Conversely, bipolar scales are most accurate when a seven-point scale is used (with three scale points on each side of a truly neutral midpoint.) Note Choosing between unipolar and bipolar questions is not the same thing as asking two things at once (double-barreled questions). Make sure that you use mutually exclusive options Avoid overlaps in the response items. If two items have similar meanings, it risks making your respondent’s choice random. | Environmental damage caused by single-use water bottles is a serious problem. | | | | | | | --- | --- | --- | --- | --- | --- | | Strongly agree | Agree | Neither agree nor disagree | Indifferent | Disagree | Strongly disagree | How to analyze data from a Likert scale Before analyzing your data, it’s important to consider what type of data you are dealing with. Likert-derived data can be treated either as ordinal-level or interval-level data. However, most researchers treat Likert-derived data as ordinal: assuming there is not an equal distance between responses. Furthermore, you need to decide which descriptive statistics and/or inferential statistics may be used to describe and analyze the data obtained from your Likert scale. You can use descriptive statistics to summarize the data you collected in simple numerical or visual form. Example: Descriptive statistics • Ordinal data: To get an overall impression of your sample, you find the mode, or most common score, for each question. You also create a bar chart for each question to visualize the frequency of each item choice. • Interval data: You add up the scores from each question to get the total score for each participant. You find the mean, or average, score and the standard deviation, or spread, of the scores for your sample. You can use inferential statistics to test hypotheses, such as correlations between different responses or patterns in the whole dataset. Example: Inferential statistics • Ordinal data: You hypothesize that knowledge of climate change is related to belief that environmental damage is a serious problem. You use a chi-square test of independence to see if these two attributes are correlated. • Interval data: You investigate whether age is related to attitudes towards environmentally friendly behavior. Using a Pearson correlation test, you assess whether the overall score for your Likert scale is related to age. Lastly, be sure to clearly state in your analysis whether you treat the data at interval level or at ordinal level. Analyzing data at the ordinal level Researchers usually treat Likert-derived data as ordinal. Here, response categories are presented in a ranking order, but the distances between the categories cannot be presumed to be equal. For example, consider a scale where 1 = strongly agree, 2 = agree, 3 = neutral, 4 = disagree, and 5 = strongly disagree. In this scale, 4 is more negative than 3, 2, or 1. However, it cannot be inferred that a response of 4 is twice as negative as a response of 2. Treating Likert-derived data as ordinal, you can use descriptive statistics to summarize the data you collected in simple numerical or visual form. The median or mode generally is used as the measure of central tendency. In addition, you can create a bar chart for each question to visualize the frequency of each item choice. Appropriate inferential statistics for ordinal data are, for example, Spearman’s correlation or a chi-square test for independence. Analyzing data at the interval level However, you can also choose to treat Likert-derived data at the interval level. Here, response categories are presented in a ranking order, and the distance between categories is presumed to be equal. Appropriate inferential statistics used here are an analysis of variance (ANOVA) or Pearson’s correlation. Such analysis is legitimate, provided that you state the assumption that the data are at interval level. In terms of descriptive statistics, you add up the scores from each question to get the total score for each participant. You find the mean, or average, score and the standard deviation, or spread, of the scores for your sample. Advantages and disadvantages of Likert scales Likert scales are a practical and accessible method of collecting data. • Quantitative: Likert scales easily operationalize complex topics by breaking down abstract phenomena into recordable observations. This enables statistical testing of your hypotheses. • Fine-grained: Because Likert-type questions aren’t binary (yes/no, true/false, etc.) you can get detailed insights into perceptions, opinions, and behaviors. • User-friendly: Unlike open-ended questions, Likert scales are closed-ended and don’t ask respondents to generate ideas or justify their opinions. This makes them quick for respondents to fill out and ensures they can easily yield data from large samples. Problems with Likert scales often come from inappropriate design choices. • Response bias: Due to social desirability bias, people often avoid selecting the extreme items or disagreeing with statements to seem more “normal” or show themselves in a favorable light. • Fatigue/inattention: In Likert scales with many questions, respondents can get bored and lose interest. They may absent-mindedly select responses regardless of their true feelings. This results in invalid responses. • Subjective interpretation: Some items can be vague and interpreted very differently by respondents. Words like “somewhat” or “fair” don’t have precise or narrow definitions. • Restricted choice: Since Likert-type questions are closed-ended, respondents sometimes have to choose the most relevant answer even if it may not accurately reflect reality. Other interesting articles If you want to know more about statistics, methodology, or research bias, make sure to check out some of our other articles with explanations and examples. Frequently asked questions about Likert scales Cite this Scribbr article If you want to cite this source, you can copy and paste the citation or click the “Cite this Scribbr article” button to automatically add the citation to our free Citation Generator. Bhandari, P. & Nikolopoulou, K. (2023, June 22). What Is a Likert Scale? | > Guide & Examples. Scribbr. Retrieved November 11, 2024, from https://www.scribbr.com/methodology/likert-scale/ Have the Democrats Become the Party of the Élites? Author: Marantz, Andrew, Date: 2024-12-14 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: 9LBV3C3V Cite Key: Marantz24demsPartyOfElites Zotero Item | Lit Note Why did Kamala Harris lose the election? So glad you asked. Pull up a chair—actually, no need to sit, because the answer is so simple that it can be summed up in a few seconds. The main problem was the incumbency disadvantage, exacerbated by inflation—and immigration, and also urban disorder, wokeness, and trans swimmers. Also, Joe Biden dropped out too late, and Harris peaked too early, and the Democrats should have picked another candidate, or maybe they should have stuck with Biden. Donald Trump’s voters were motivated by white grievance, except for the people of color who were motivated by economic anxiety; ultimately, the main issue was the patriarchy, exacerbated by misinformation on long-form podcasts, although of course Harris should have gone on Rogan. From now on, the Democratic Party has no choice but to move left, move right, overhaul its approach entirely, and/or change nothing at all. A few days after the election, Musa al-Gharbi, a forty-one-year-old sociologist at Stony Brook University, published a lengthy piece on Substack called “A Graveyard of Bad Election Narratives,” using bar charts and cross tabs to “rule out what wasn’t the problem”: sexism, racism, third-party spoilers. “It didn’t take me long to write, because I realized I’d written essentially the same piece after the 2020 election, and also after the 2016 election,” he said the other afternoon, at a diner in Greenwich Village, while finishing off a black coffee and a plate of fries. “The details change, of course, but the basic trends have been consistent for a long time.” Harris didn’t do herself any favors, he argued—she was “unwilling or unable to distance herself from the unpopular incumbent,” she shouldn’t have campaigned with Liz Cheney, and so on—but any Democrat would have had a hard time winning, because, during the past three decades, “Democrats have become the party of élites,” alienating increasing numbers of “normie voters” in the process. Daily Our flagship newsletter highlights the best of The New Yorker, including top stories, fiction, humor, and podcasts. More specifically, al-Gharbi maintains that the Democrats have become the party of “symbolic capitalism,” a coinage so important to him that he used it as the title of both his doctoral dissertation and his Substack. The phrase is his play on “symbolic capital,” the French sociologist Pierre Bourdieu’s term for prestige, cultural status, and other types of capital that can’t be measured by money alone. (A TV-news producer or an H.R. officer may wield rarefied forms of social power without cracking the top tax bracket.) Symbolic capitalists—academics, commentators, lawyers, consultants—manipulate words or data rather than making things with their hands. In conversation, al-Gharbi, a tenure-track professor with a sideline in punditry, tends to use “symbolic capitalists” and “symbolic élites” interchangeably, and always in the first-person plural; he may be one of the club’s sharpest critics, but he can’t deny that he’s also a member. “We are valued—overvalued, I would say—for what we know, not for what we make or do in the physical world,” he said. For him, this is the key divide in American politics. In the past three decades, he argues, the Democratic Party has been transformed from the party of non-symbolic workers to the party of symbolic élites. This strikes him as a fateful misstep: If elections are about convincing voters that you’re on their side, then why associate your party with a group that most voters not only don’t identify with but actively resent? Despite Donald Trump’s many shortcomings as a politician, al-Gharbi continued, “one thing Trump has always been good at is triggering these outraged, condescending reactions from normie liberals, which work to his benefit.” For example: McDonald’s. All politicians pretend to like junk food while they’re at the Iowa State Fair, but Trump, despite being able to afford champagne and caviar, has long seemed to prefer Diet Coke and Filets-O-Fish. “He’s passionate about the product,” al-Gharbi said, with a snort-laugh. “It’s one of the many things about him that drive us”—symbolic élites—“crazy. He was born rich, went to fancy schools. He should be one of us, but he just isn’t.” Al-Gharbi paused to acknowledge the server who cleared his plate. In October, when Trump stopped at a McDonald’s in Pennsylvania and briefly pretended to work the drive-through window, he made little effort to hide that it was a flimsy P.R. stunt; beneath his McDonald’s apron he wore a pressed shirt and his signature red tie. “It’s not like voters were supposed to see this and get duped—‘Oh, he’s a regular Joe like me,’ ” al-Gharbi continued. Rather, the point was to “heighten the cultural and class differences between normal people,” who like McDonald’s, and symbolic capitalists, especially coastal liberals, who could be counted on to seethe about the garishness of the spectacle. Substantively, the seethers may have had a point. Harris has spoken fondly about her summer job at McDonald’s, and now advocates for a fifteen-dollar minimum wage; Trump was born rich, and his public gestures are often at odds with his actual policies. (When Trump was asked, through the drive-through window, whether he favored raising the minimum wage, he dodged the question.) Semiotically, though, it’s not hard to see how the episode might have pushed a normal voter toward Trump. Who is more relatable: the scolds who seem to turn their noses up at McDonald’s or the guy who’s clearly lovin’ it? There is, famously, less social mobility in the U.S. than Americans would like to believe, but al-Gharbi’s trajectory has involved an exceptional amount of change. He grew up in southern Arizona, near an Army intelligence base. Most of his close relatives served in the Army, including his father, who is Black; his mother and stepfather, who are white; and his twin brother, who was killed in Afghanistan. He considered joining the Catholic priesthood before, in his late teens, becoming an atheist; he later converted to Islam and changed his name. Al-Gharbi’s mother, a Trump supporter, sometimes introduces him as “my liberal son,” which he finds amusing. He will cop to being a coastal symbolic élite, but he never refers to himself as a liberal. He went to community college and sold shoes at a local Dillard’s before getting his Ph.D. at Columbia, an environment that he found both alienating and illuminating. “Our stipend was thirty-seven thousand dollars a year, which was more money than I had ever made,” he said. It wasn’t enough—he was in his mid-thirties, the main breadwinner of a family of four, and he still had to work his way through grad school—but it felt like a lot, “especially since I didn’t consider it, you know, a real job. We were getting paid to go to school. And yet many of my colleagues—most of whom were younger, and living alone, and, needless to say, came from a different background—would openly take the position, ‘Thirty-seven thousand is trash money.’ ” The day after the 2016 election, al-Gharbi went to class prepared to discuss W. E. B. Du Bois, but the planned discussion was cancelled so that his classmates could process their feelings. “There were grown men weeping,” he said, and a mother “talking about how Trump was going to put her child in a camp.” In the ensuing days, he went on, some left-leaning Columbia students “said they couldn’t hand in their papers on time, due to mental distress, and meanwhile most of the people they purported to care about, Black and brown maintenance workers and people like this, kept showing up every day to do their jobs.” An irony of al-Gharbi’s work, as he knows, is that his critique of the élite consensus can only spread if it is picked up by élite consensus-makers: he’s just a symbolic capitalist, standing in front of other symbolic capitalists, asking them to cite him. Still, he seems to relish telling each crowd what it least wants to hear. His standard PowerPoint starts with Uncle Sam pointing out at the audience—if you’re attending this talk, then you might be a symbolic capitalist—and goes on to assert that, when it comes to issues like inequality, “the GOP is not the main problem.” Another slide speaks to symbolic capitalists through their patron saint, Taylor Swift: “It’s me, hi! I’m the problem, it’s me.” The month after the election was so rich with instant takes and podcast postmortems—Harris’s top campaign staffers on “Pod Save America,” a bipartisan panel at Harvard’s Institute of Politics—that it’s hard to imagine what else might be left to say. The week after the election, Representative Alexandria Ocasio-Cortez appeared on MSNBC to explain the phenomenon of the “A.O.C.-Trump voters”—the significant number of people in her district who had apparently cast a split ballot for her and Trump. The host, Joy-Ann Reid, seemed to find this concept baffling (“It makes no sense”), but Ocasio-Cortez didn’t sound baffled. Every campaign, she said, is “a race to convince a person about who cares about [them] more”—the unstated implication being that her brand of left-wing populism and Trump’s brand of right-wing populism both resonated with the frustrations of working-class voters, while Harris’s soft-edged liberalism did not. Still, Ocasio-Cortez cast doubt on any pundit peddling a hasty grand theory of the electorate. “We had an election on November 5th, and on November 6th you’ve got an answer?” she said. “Don’t listen to those people.” Al-Gharbi may seem like such a person—“One narrative to unite and rule them all,” he tweeted on November 6th, linking to his work—but at least his theory wasn’t concocted overnight. He has been making versions of the same argument, in peer-reviewed journals and in a range of public-facing outlets, for several years. If what we’re seeing now is a partisan dealignment, then al-Gharbi would date its origin to the nineteen-nineties, when President Bill Clinton advanced the interests of “those affiliated with the symbolic economy at the expense of most others.” Clinton, another McDonald’s-philic politician, was both a favorite son of the white working poor and a technocratic striver. In al-Gharbi’s view, Clinton’s rhetoric and signature policies (financial deregulation, tech boosterism, NAFTA, welfare reform) reoriented the Democratic coalition around the workers that his Secretary of Labor, Robert Reich, called “symbolic analysts”—yuppies in the New South, I.T. professionals in the Sun Belt, and so on—and away from the working class, both white and nonwhite. In the past decade, al-Gharbi contends, this trend accelerated, as symbolic capitalists adopted sharply more progressive views on a range of cultural issues, dragging their party with them. Another day-after postmortem came from Senator Bernie Sanders. “It should come as no great surprise that a Democratic Party which has abandoned working class people would find that the working class has abandoned them,” he posted on X, on November 6th. In an interview with the Times, Nancy Pelosi, the former Speaker of the House, brushed off Sanders’s critique before changing the subject: “There are cultural issues involved in elections as well. Guns, God and gays—that’s the way they say it.” Al-Gharbi is hardly a professional campaign adviser; he doesn’t even vote. Still, in this season of Democratic friendly fire, as the socialists urge the Party to the left and the moderates counsel a tack to the center, he is telling a sweeping story about how both factions may be partially correct. “On economic issues, I see no reason for Democrats to moderate—if anything, they have more room to run to the left,” he told me. “Redistribution, expanding health care—these things are incredibly popular. There’s no reason they can’t combine that with a shift toward the median voter on things like immigration. I don’t think they will, but they could.” This synthesis, in broad strokes, is not unique to al-Gharbi. Elements of his analysis overlap with those of Timothy Shenk , Michael Lind, Barbara Ehrenreich, and John B. Judis and Ruy Teixeira, and many others (including a few elected Democrats, such as Representative Ro Khanna and Senator Chris Murphy). Among the one-word explanations currently on offer for what has gone wrong with the Democrats, al-Gharbi’s choice would probably be “wokeness”; but his understanding of the word is not, as is often the case, warped by whichever blue-haired zillennial or overbearing Bluesky post happened to annoy him most recently. His account is granular enough to fill a data-heavy book, published, in October, by Princeton University Press, called “We Have Never Been Woke: The Cultural Contradictions of a New Elite .” Al-Gharbi identifies three previous “Great Awokenings”—beginning in the early nineteen-thirties, the late sixties, and the late eighties—and argues, counterintuitively, that the most recent one began around 2010 and ended, or at least started ending, in 2021. Instead of defining the term succinctly, he offers a list of “views that seem to be discursively associated with ‘wokeness,’ ” such as a “focus on identity, subjectivity, and lived experience.” He is harshly critical of performative wokeness (“my covert narcissism I disguise as altruism,” as Taylor Swift puts it), but not of its putative goals. “The core idea behind intersectionality,” he concedes, “seems both important and fairly uncontroversial.” His problem with the bulk of social-justice discourse, he told me, is “not that symbolic capitalists are calling for too much justice but that we do so in ways that are counterproductive.” Focus Groups: The Definitive Guide Author: Webster, Will, Date: 2024 Collections: FocusGroups Zotero Key: Q92ZZJ6C Cite Key: Webster24focusGrpGuide Zotero Item | Lit Note What is a focus group? Focus groups are a type of qualitative research that bring together a small group of people representing a target audience. In a conversation usually guided by a moderator, this group will discuss a specific topic, products, services or concepts in a controlled environment. The purpose of focus groups is to have a relaxed, open-ended conversation to gain insights that may not be possible from a survey or individual interview. They’re a very valuable tool in the research toolkit. Free eBook: This Year’s Global Market Research Trends Report Focus group roles In any focus group there are typically three main roles being played. | 1 | Participant | They’re there to contribute to the discussion – sharing their thoughts, feelings and experiences – and provide the qualitative data focus groups exist to gather. They are typically chosen because they represent the target market or demographic being studied. | | --- | --- | --- | | 2 | Moderator | The moderator’s role is to guide the conversation and focus group participants. They’re there to introduce the purpose of the focus group, lay the ground rules to the group members, and create a safe and respectful environment for discussion. The moderator should both follow a discussion framework and be prepared to adapt to the flow of conversation. Although rarely common, there can be two moderators in some types of focus groups. | | 3 | Observer | The observers are there to, well, observe. They are typically members of the research team or the company conducting the focus group research, who watch the focus group discussion without participating. They’re there for several reasons: to hear the discussion live, to ask the moderator to probe on certain points of interest, to collect data, to observe body language and group interaction, and to gather any additional insights. | When should a focus group be used? Focus groups are a very popular type of research method that’s used in virtually every sector, from tech to academia, and marketing to political science. Focus groups are a great choice if you want to go deep into a particular topic. If surveys are a brilliant tool for understanding what someone feels about something, focus groups help us to explore why – which is why the two work great in tandem. Focus groups give us an opportunity to capture the human element – the emotions and non-verbal cues that numbers often miss – and help us to explore underlying motivations. Here are some of the most common focus group use cases. New product or concept testing If you’ve got a new product or concept in the works, a focus group can provide valuable feedback before you launch it into the market. You can get a sense of how people might react to it, what they like or dislike, and any improvements they might suggest. In fact, focus groups are often used by brands to improve on and even co-create products in real time, with concepts discussed and iterated over the course of the session. Understanding customers better Focus groups are a great market research tool to help you better understand why customers think and behave the way they do. If, for example, a product isn’t selling as expected, a focus group with your customers can shine light on their barriers to purchase. Beyond current customers, businesses can also use focus groups to better understand new prospects and bring their target customer segments to life. Marketing and advertising Before you invest a lot of money into a marketing or advertising campaign, you could use a focus group to test your messaging and visuals. Once any tweaks are made, you should be left with a campaign that will better resonate with your target audience. Exploratory research Focus groups are great when you don’t know what you don’t know. If you’re entering a new market or targeting a new customer segment, they can help you explore and understand the landscape. When quantitative data isn’t enough Sometimes, numbers and stats don’t tell the whole story. If you have quantitative data but want to delve deeper into the ‘why’ and ‘how’, focus groups are a great way to do that. Focus groups can provide rich, qualitative insights that quantitative research methods might miss. But it’s important to remember that they aren’t right for every situation. If you need to make definitive conclusions about a large population, a quantitative survey will be a better option. The same applies if you’re focusing on sensitive topics that people may not feel comfortable discussing in a group setting, such as financial or health matters. At the end of the day, the best research method for your use case really depends on your specific goals, who you’re collecting insights from and the nature of the information you’re seeking. Types of focus groups Focus groups can take different forms depending on the objectives of the study, the participants, and the nature of the topic being discussed. Ranging from the common to the seldom used, here are the different types of focus group methods. One-way focus groups The classic focus group format: a moderator leads a discussion among a group of participants about a particular topic. Two-way focus groups Here one group watches another, observing the discussion, interactions and conclusions. This format is used to provide additional insights and a deeper understanding of the topic. Dual-moderator focus groups A focus group with two moderators. One ensures the session runs along smoothly; the other makes sure all topics are covered. The aim of this format is to create a more comprehensive discussion. Dueling-moderator focus groups Like dual moderator focus groups in that there are two moderators, but here the moderators take opposing viewpoints on the topic. The purpose of this format is to help the participants consider and discuss a wider range of points. Respondent-moderator focus groups Where one respondent – or several – plays the role of moderator. This format counters the unintentional bias that can come from a single moderator, and encourages variety in the discussion, Mini focus groups, dyads and triads Exactly as they sound, mini focus groups involve fewer participants than usual. These smaller groups – typically made up of 4 to 5 participants – are well suited to complex topics. There are also focus groups involving two participants and a moderator – known as dyads – and groups with three participants and a moderator, which are known as triads. Remote focus groups An online focus group. This format is a great way to give your research a wider geographical reach and access a greater pool of people. How to run a focus group: Step-by-step guide The focus groups that generate the best insights are often those that are planned best. Here’s our guide for every step of the process. 1. Research and define your goal The first step is to identify what you want to learn from the focus group. Are you testing a new product or exploring consumer behavior? Maybe you’re seeking feedback on a marketing campaign or getting to know a new target market? 2. Choose a moderator If planning is the key ingredient for a great focus group, a good moderator is a close second. The moderator is the most important person in the room, and needs to be someone who can facilitate discussions, manage a group of strangers, and keep the conversation on track and be able to elicit the feedback desired.. 3. Choose a location The most important consideration here is how you create a comfortable, non-judgmental environment where participants feel safe to share their thoughts and opinions. And you also need to answer the big question: in-person or online? In-person sessions typically enable better conversation and group chemistry, while online focus groups give you access to a much bigger, broader pool of potential participants. 4. Recruit the right participants Next, work out who you need to participate in the focus group to reach your goal. Whatever your target audience is, you want the respondents to meet the baseline criteria – noting that the ideal size for a focus group is typically between 6 and 8 participants, and that none of your participants should know each other. Once you’ve worked out who you want there, you need to recruit them. This is often done via ads, invitations to your CMS database or a third party. Incentives, like cash or gift cards, are typically used to encourage participation. 5. Create a discussion guide In tandem with step four, it’s time for the moderator to develop a document that will guide the discussion. Based on your research goal or goals, this guide should include a list of focus group questions or topics you want to cover during the session, and strike a good balance between structured and flexible – so you can gather the data you need while not discouraging spontaneous conversation. 6. Conduct the focus group The big day has arrived. With everything in place, all you need to do is make sure that every participant is given an opportunity to speak. Don’t forget to record the focus group (with the participants’ consent) and make efforts to capture non-verbal cues from participants. 7. Debrief and iterate Debrief after each session to understand your key findings, and if necessary, edit the discussion guide for future focus groups based on your learnings and observations so far. 8. Analyse and report on the findings Now’s time to transcribe your recordings and analyse them for key themes and insights. The aim here is to interpret your findings in the context of your initial goal. It’s best practice to present your key focus group results and findings in a report, alongside recommendations based on them. How many people should be in a focus group? The ideal size of a focus group is generally said to fall between 6 and 8 participants.Why is this the sweet spot? Because it’s small enough to ensure that everyone has a chance to speak and share their views, but large enough to provide a variety of perspectives. That said, the goal of your research and the topic(s) you’re focusing on can change things. For instance, if the topic is particularly complex or sensitive, a smaller group may be better. If you have a larger pool of potential focus group attendees, best practice would be to split them up and conduct multiple focus groups, instead of one focus group with too many people. Focus groups vs in-depth interviews Focus groups and in-depth interviews are two of the most popular forms of qualitative research. They do, however, differ in what they can bring to your research – which is why they’re often used in tandem to answer a single research question. The benefits of focus groups over in-depth interviews Focus groups are designed to encourage interaction between a collection of people, often revealing insights that may not surface in a one-on-one conversation. They give researchers an opportunity to observe group dynamics and how individuals influence each other and can be influenced themselves. A big advantage of focus groups is their efficiency – in one session you can gather a broad range of insights from multiple individuals. The benefits of in-depth interviews over focus groups In-depth interviews are one-on-one discussions between a researcher and participant. Whereas focus groups are by definition a group discussion, in-depth interviews provide a more personal and detailed exploration of an individual’s perspectives and experiences. Because of this, interviews are great for sensitive or personal topics, and the interviewee won’t be as influenced by others when giving their honest opinions – which is a risk with focus groups. Another benefit of in-depth interviews is that the researcher/interviewer has greater control over the conversation, which gives you a greater chance of covering all topics thoroughly. Advantages and disadvantages of focus groups Like any research method, focus groups come with a variety of pros and cons that are typically associated with any type of qualitative research. Advantages of focus groups • They give you qualitative insights. Exploring the ‘why’ behind people’s behaviors, views and decisions • They enable interactive discussions. Often leading to deeper insights as participants explore topics and ideas • They give immediate findings. Observing real-time reactions means you can quickly implement them on a concept, product or campaign • You can capture non-verbal data. Non-verbal cues and body language often give a further layer of insight into participants’ attitudes and feelings • You have real-time flexibility. The moderator can steer the conversation to explore new points and topics if they arise Disadvantages of focus groups • Your sample size is small. And less likely to represent how the total population feels • You’re at risk of conforming beliefs. Meaning that participants may change what they’re saying to match the majority opinion or the loudest voice • They require a skilled moderator. Your findings could be a lot less valuable without one • Data analysis can be time consuming. If you’re hosting multiple groups, transcribing and deciphering data can be labour-intensive and complex • They can be expensive (especially if done in person). You may need to pay for participant travel, accommodation and incentives, venue rent and even moderator fees ‘The internet hasn’t made us bad, we were already like that’: The mistake of yearning for the ‘friendly’ online world of 20 years ago Author: Rey, Enrique, Date: 2025-01-07T13:29:11+01:00 Collections: MediaAdsPolit, Polarization, MisDisinformation Zotero Key: TTV7MTSS Cite Key: Rey25alwaysBadNotinternetFault Zotero Item | Lit Note In the visuals of María Escarmiento’s concerts, as in those of many other figures of the urban scene, iPods, Blackberry phones and screenshots of Fotolog, Messenger or Tuenti appear often. A large part of the public is too young to have used these technologies, but they know what these devices and websites were used for. Even they associate them with a time when virtual spaces were more welcoming, fun and habitable. Such is the force of the Y2K aesthetic or Flow 2K, one of the latest revivals that, as in all artistic disciplines, entails revisiting and reinterpreting the designs and interfaces of electronic devices from around 20 years ago. It’s the umpteenth nostalgic twist of a culture obsessed with retromania. Millennials have been adults for years, and they might now be making up for lost time by evoking hours spent chatting on MSN Messenger (they weren’t all that exciting: you used to talk to your classmates right after you’d been with them) and looking up movies and songs by Evanescence, Green Day and Eminem on eMule (they often sounded bad and the noise of the computer, running all night, caused nightmares). Furthermore, as academics such as Grafton Tanner (author of The Circle of the Snake: Nostalgia and Utopia in the Age of Big Tech) point out, the internet, with its archival form of storing information, fits particularly well with nostalgia. As such, this entire movement of vindication and archaeology (which dusts off material from the golden days of IRC and Habbo Hotel all the way to the early years of Facebook, just before the Cambridge Analytica scandal) is a perfect excuse to relaunch products or produce viral content without too much effort. But Flow 2000 could also mean that we miss the old internet (if such a thing ever existed) or, at the very least, that we need a network on a different scale and speed, one that is more human and kinder. Recently, the cultural critic Kyle Chayka wrote in The New Yorker about “cozy tech,” one of the latest fantasies being spread by platforms like TikTok. “Cozy tech” is the label that groups together content about users sipping from a steaming cup, browsing leisurely or playing nice, simple video games on devices with smooth, ergonomic designs. It’s a more powerful image than it seems because it conveys something we lost at some point in the last decade: a sense of control; the idea that it is possible to enjoy technology in peace again. Over the past few months, several essays have been published in Spain that explore this desire (nostalgic or projected into the future) for a better internet and propose different collective solutions to escape this degraded internet in which the platforms have won the battle against users, where hatred has surpassed support networks, where data extraction and the desire for profit dirty every corner of the web, and where entertainment is poisoned. Along the way, they debunk some technophobic myths and warn against the rhetoric of gurus in the pay of big technology companies. These books also contain many of the possible answers to the two questions that we so often remember lately: Has the network changed or have we lost our innocence? Bottom line: When did the internet go bust? A truncated utopia The most pessimistic among us consider that the history of the internet is the story of a great failure: another enormous collective disappointment because the last credible utopia (a horizontal and free network) fell by the wayside. In her essay Las redes son nuestras (The networks are ours), Marta G. Franco explains that the history of the internet is marked by successive thefts or forced expropriations, and that the third and last of these thefts occurred eight years ago. The first was completed during the leap from the nerd network of the 1990s to the dotcom bubble; the second, when Web 2.0 emerged and users gained prominence, but also became producers of free data; and the third and most recent, when the forces of the far right (what she calls “the International of Hate,” formed by politicians like Donald Trump in the U.S. and Javier Milei in Argentina) transformed “the platforms that helped us find and organize ourselves into a minefield of unpleasant experiences.” However, the author is not pessimistic (“if we were robbed and lost three times it is because a short while before, for three times, we were winning”) and she believes that the internet can still change for the better. Mayte Gómez Molina (known on social media as Ingrata Bergman), a digital artist, poet and researcher, points out that it is necessary to ask who the internet has disappointed: “Although we cannot imagine it now, it could have taken many forms. Many pioneers and artists of net art in the 1990s and 2000s explored the creation of directories, other search engines and how to take the structure of the network to the limit, creating interfaces full of ramifications. Then the search engines recovered unidirectional communication and standardized the way of browsing; so for those people it has been a disappointment. But for those who had an economic interest in the extension of capitalism, the internet has become exactly what they wanted, which is not so different from television. The rest of us experience it as a conflictive space, where there are many disappointing things but which always regenerates and opens up new possibilities. The internet does something very cruel and difficult to bear emotionally: at first it disappoints you, but then it gives you hope again.” These cycles of hope and disillusionment also affect the rhetoric about the online world, which, in times of disillusionment like the present, can function as a self-fulfilling prophecy. Proyecto Una, a self-described millenial and feminist collective, does not want to be pessimistic either: “We do not like the hyper-fixation of certain left-wing groups with defeat. Part of the internet we have is the logical evolution of that project by some hippies who thought they could fix the world with technology. Now they are realizing that the world’s problems were more social than technological; but there are alternatives.” One of the topics that this collective dedicated to digital thought and activism most energetically combats is the idea that there is still a border between the virtual world and the real world and, therefore, that it might be possible to escape from one to the other. “The real world and the digital world do not exist as separate entities. We are materialists: the world is what we build with our capacity to act on it,” they note. This group of philosophers, programmers and YouTubers does not tolerate well the fatalistic, technophobic messages claiming that, given the bad situation of many online spaces, this means that all technology (especially since the popularization of Artificial Intelligence) is governed by unclear rules that cannot be altered. “Naturalizing human behavior or essentializing the evolution and impact of a technology is done either out of ignorance or by private interests that seek to create that ignorance. Whenever we receive a message, whether in the offline or online world, we have to ask ourselves: who is sending it? Why is it stated like that? What benefit can they be getting from it? Who is interested in me repeating this? The worst propaganda is the one we replicate without even realizing it.” What do we miss so much? In Los hechos de Key Biscaine (The Facts of Key Biscaine), the latest novel by Xita Rubert (who was born in 1996), there is a scene in which two teenage friends enter Omegle, a very popular website around 2010 that, like Chatroulette, was used to chat with strangers. “A virtual infinity of penises. A terrifying phallic universe. Behind each chat there was a crotch always ready to insinuate itself and prepared to be revealed,” describes the narrator. Situations like this were very common in spaces that are now missed and mythologized but which, even then, were reproducing sexist and racist behavior. For this reason, Proyecto Una questions whether the Internet of 20 years ago was more free: “Was it free, for whom? The fact that you can say anything does not mean that there is more freedom. It means that the freedom of the strongest prevails. On 4Chan, people used to joke that there were no women on the internet, and if someone identified themselves as such, they were required to show their tits. These types of spaces (like the rest, of course) were not neutral.” They believe that the situation has not changed that much and that today “large commercial platforms allow fascism to grow, precisely when they do not take measures to moderate content. When they choose the profits offered by the engagement of a hate message or a racist hoax over intervening in certain types of behavior.” Although the internet was never entirely free, almost every user could mention a certain milestone that affected them particularly (from the virtual rape in a role-playing game in 1993 reported by the journalist Julian Dibbell, to Trump’s first election victory or, why not, the closure of Tuenti) and establish a subjective date when the internet broke down. In addition, many would agree that sarcasm as an approach to any interaction has been another one of the factors that have most deteriorated the online world, and it has been working as a double-edged sword for at least a decade. “Sarcasm can bring you closer to what you are ironizing about. Turning something horrible into a spectacle can promote it or even radicalize it,” explains Gómez. “On the one hand, we have to overcome the phenomenon of algorithmic personality (since I hang out with people who think like me, you cannot recognize that the other person might think differently). On the other hand, if we pay attention to and turn into a spectacle all the ultras, incels, neo-fascists and violent people who organize themselves on the internet, it may seem that they cannot harm you, that they are images or representations without any real power. When something becomes a meme it gives the sensation that it does not exist, and being too ironic turns possible and dangerous things into fiction,” explains the researcher. Like all nostalgic escapism, the myth about a world wide web before the age of sarcasm (and the dominance of big companies) where everything was more sincere and simpler is a melancholic trap. The Austrian poet Ingeborg Bachmann wrote that when you turn 30 you discover the ability to remember, and those who were teenagers when broadband was installed in most homes are now that age. That is why the internet has filled up with memories of itself, although, with some effort, it is still possible to find new things full of a collaborative spirit. “There is still a lot of kindness on the internet. You just have to go to YouTube and watch those videos about how to fix a specific washing machine,” says Gómez. “A lot of content is a sign of goodwill; the real Youtuber is the one who has 10 views on each video. There are a lot of sweet, practical, erratic, very strange things there, and also a lot of people helping others selflessly,” she notes. Oche Zamora, a social educator, waiter and one of those individuals who make Facebook worthwhile, also thinks that many people continue to make a brilliant use of the internet: “We have come to suspect that all publications hide a spurious desire for recognition and that everything we do on social networks we do to pretend that we are better than we really are,” he complains. “And I don’t think that is the case. We all want to be loved and, above all, to be loved by certain people. What is the problem with that? The fact is that on social networks there is a desire to express oneself, to play, to reflect and to have a good time, and not just a Machiavellian strategy to get likes,” he says. “Seeing what the networks are becoming, announcements of events and polarized opinions on the umpteenth debate on the media agenda, one misses that exposure of the intimate. I wish to read personal stories or confessions again.” So, is there a way to recover the good things about the internet that we miss without falling into the self-serving nostalgia of Flow 2K or into apocalyptic talk? The essays cited above offer some political keys (such as recovering our digital sovereignty) and individual ones (such as giving visibility to the self-organized projects and venues that continue to exist). Mayte Gómez concludes: “We must stop this reactionary thinking and this fear of technology that arises from the idea that the internet has made us bad. That is not true: we were already like that. If the internet is unfriendly it is because we are becoming less so. We cannot perpetuate the idea that machines are entities with a will of their own; we must take responsibility for what happens on the internet.” Sign up for our weekly newsletter to get more English-language news coverage from EL PAÍS USA Edition Morning consult political intelligence: Global tracking methodology primer Author: Consult, Morning, Date: Q2 2024 Collections: ElectionPredFeats Zotero Key: I5CSV7UZ Cite Key: MorningConsult24politIntelPrimer Zotero Item | Lit Note / M (0) RNIN G Cc (o) NSU LT Global Tracking Methodology Primer April 2024 Morning Consult Political Intelligence: Global Tracking Methodology Primer Morning Consult values transparency in survey research. Our high frequency data allows us to generate large sample sizes and shed light on key demographics with consistency and stability. This methodology primer provides an overview of the political data collected using our global survey research capabilities, and is available to clients via Morning Consult’s Political Intelligence product. Data comes from the following proprietary sources: 1. Daily global tracking survey 2. Monthly global tracking survey Daily Tracking Overview Morning Consult conducts thousands of daily interviews on key political issues across 43 countries, making our unified data set on political attitudes the largest globally. Every day, we gauge public opinion on world leaders, international organizations, policy issues and other countries to provide customers with data at the scale and speed needed to navigate the changing global political landscape in real time. All interviews are conducted online. Morning Consult samples among demographically defined strata to reach a desired target audience in each country. Portions of our syndicated monthly surveys do not field in China, Russia, Saudi Arabia and the United Arab Emirates due to political sensitivities. Data series Morning Consult tracks three main sets of political issues globally: Morning Consult Political Intelligence: This methodology primer provides an overview of the political data collected using our global survey research capabilities, and is available to clients via Morning Consult values transparency in survey research. Our high- scale and speed needed to navigate the changing global political landscape in real time. All interviews are target audience in each country. Portions of our syndicated monthly surveys do not field in China, Russia, conducted online. Morning Consult samples among demographically defined policy issues and other countries to provide customers with data at the Morning Consult tracks three main sets of political issues globally: following proprietary sources: Daily Tracking leaders, international organizations, Overview stability. 2. MV MORNING CONSULT Data series Leader approval’ Whether respondents approve or disapprove of their chief executive Country trajectory Respondents’ views on whether their country is heading in the right direction or going down the wrong track Views of other countries & global institutions? Views of other countries and key international institutions, including the International Monetary Fund, the United Nations, NATO and others MV MORNING CONSULT Question text & response options Question: Do you approve or disapprove of how [chief executive’s name] is doing? Response options: 4-point approval scale, plus don’t know/no opinion Question: Now, generally speaking, would you say that things in your country are going in the right direction, or have they pretty seriously gotten off on the wrong track? Response options: right direction, wrong track Question: Do you have a favorable or unfavorable impression of the following? Response options: 4-point favorability scale, plus don’t know/no opinion and heard of/no opinion would you say that things in your country of how [chief executive’s name] is doing? are going in the right direction, or have scale, plus don’t know/no opinion and country is heading in the right direction international institutions, including the Respondents’ views on whether their Views of other countries & global disapprove of their chief executive Views of other countries and key plus don’t know/no opinion Nations, NATO and others Response options: heard of/no opinion 4-point favorability wrong track? Data series Country coverage & target sample sizes Tracking start dates and target sample sizes vary by country. As of January 2024, average daily sample sizes on the daily tracker are as follows: \"Leader approval is not tracked in China, Saudi Arabia and the United Arab Emirates, and is not continuously tracked in Russia. 2 The countries and institutions rotate, such that not every respondent is asked about each target country or institution on a daily basis. This yields smaller daily sample sizes relative to our leader approval and country trajectory batteries. The countries and institutions rotate, such that not every respondent is asked about each target country \"Leader approval is not tracked in China, Saudi Arabia and the United Arab Emirates, and is not Question: Do you approve or disapprove Tracking start dates and target sample sizes vary by country. they pretty seriously gotten off on the Question: Now, generally speaking, Question text & response options or going down the wrong track sizes on the daily tracker are as follows: Response options: Country trajectory Leader approval’ institutions? MV MORNING CONSULT |Col1|Col2|Col3|Col4| |---|---|---|---| |Americas|Argentina|Nov \"21|150-250| ||Brazil|Dec \"18|200-300| ||Canada|Dec 18|350-450| ||Chile|Nov ’21|150-250| ||Colombia|Nov '21|150-250| ||Mexico|Dec \"18|300-400| ||Peru|Nov '21|150-250| ||United States|Oct \"16|3,650-3,750| |Col1|Col2|Col3|Col4| |---|---|---|---| |Europe|Austria|Sep 2|100-200| ||Belgium|Nov \"21|100-200| ||Czech Republic|Nov '21|100-200| ||France|Nov \"18|300-400| ||Germany|Nov \"18|300-400| Czech Republic Argentina Americas MV Canada Europe Nov '21 Nov \"21 Nov \"21 Nov \"18 France Dec \"18 Sep 2 3,650-3,750 Brazil Peru 200-300 300-400 300-400 350-450 100-200 100-200 150-250 150-250 300-400 3,650-3,750 150-250 350-450 100-200 300-400 MV MORNING CONSULT |Col1|Ireland|Sep '21|100-200| |---|---|---|---| ||Italy|Apr’20|200-300| ||Netherlands|Sep '21|100-200| ||Norway|Sep 21|50-150| ||Poland|Sep 21|150-250| ||Romania|Nov '21|150-250| ||Russia|Jan \"19|150-250| ||Spain|Apr’20|200-300| ||Sweden|Sep '21|100-200| ||Switzerland|Nov '21|50-150| ||United Kingdom|Oct \"18|550-650| |Col1|Col2|Col3|Col4| |---|---|---|---| |Middle East & Africa|Egypt|Oct 21|150-250| ||Israel|Oct \"21|50-150| ||Nigeria|Oct ’21|150-250| Middle East & Africa United Kingdom Netherlands Romania Sweden Norway Apr’20 Nov '21 Ireland Sep '21 Sep 21 Russia Oct \"18 Oct \"21 Egypt Israel 200-300 550-650 100-200 100-200 150-250 50-150 150-250 150-250 50-150 150-250 200-300 50-150 100-200 100-200 150-250 50-150 MV MORNING CONSULT |Col1|Saudi Arabia|Oct \"21|100-200| |---|---|---|---| ||South Africa|Oct '21|150-250| ||Turkey|Oct '21|150-250| ||UAE|Nov \"21|100-200| |Col1|Col2|Col3|Col4| |---|---|---|---| |Asia-Pacific|Australia|Jan 19|300-400| ||China|Feb 19|300-400| ||India|Feb “19|350-450| ||Indonesia|Aug 21|150-250| ||Japan|Jan 19|400-500| ||Malaysia|Aug 21|100-200| ||Pakistan|Dec’'21|150-250| ||Philippines|Aug 21|150-250| ||Singapore|Aug 21|100-200| ||South Korea|Jun ’20|150-250| Saudi Arabia South Africa Singapore Malaysia Pakistan Aug 21 Aug 21 Turkey Feb 19 Asia-Pacific Jan 19 Oct '21 Japan India 300-400 300-400 400-500 100-200 100-200 150-250 150-250 150-250 300-400 350-450 400-500 100-200 150-250 150-250 100-200 MV MORNING CONSULT Thailand Aug 21 100-200 Vietnam Aug 21 150-250 Weighting, representativeness & population targets Data is weighted to approximate representative samples of the target population among adults in each country surveyed. Weighting parameters vary by country and are derived from national statistics offices whenever available. Weights are updated periodically. Please contact us for additional details on weighting parameters used in particular countries. Population targets for Morning Consult’s daily syndicated surveys vary by country and include the general, online or literate population, as indicated in the table below. |Col1|Thailand|Aug 21|100-200| |---|---|---|---| ||Vietnam|Aug 21|150-250| Population targets Col2 Col3 Col4 Col5 Col6 Col7 Col8 Argentina General France General Nigeria Online South Korea General Australia General Germany General Norway General Spain General Austria General India Online Pakistan Online Sweden General Belgium General Indonesia Online Peru Online Switzerland General Brazil Literate Ireland General Philippines Online Thailand Online Canada General Israel General Poland General Turkey General Chile General Italy General Romania General UAE General China Online Japan General Russia General U.K. General Colombia Online Malaysia General Saudi Arabia General Us. General Population targets for Morning Consult’s daily syndicated surveys vary by country and include the general, country surveyed. Weighting parameters vary by country and are derived from national statistics offices online or literate population, as indicated in the table below. Please contact us for additional details on weighting Population targets Vietnam Saudi Arabia Switzerland Aug 21 Argentina Colombia Romania Australia Pakistan Sweden Canada General General General General General General General General General General General General General Literate Poland France Ireland Russia Online Online Online Online Online China Spain Brazil Chile UAE U.K. Us. 100-200 MV MORNING CONSULT Czech Republic General Mexico Online Singapore General Vietnam Online Egypt Online Netherlands General South Africa Online Additional data considerations: sample sizes and margins of error Unless otherwise noted, all data referenced in Morning Consult’s political analysis utilizes weighted sample sizes and unweighted margins of error. As a general rule, sample sizes of roughly 2,000 respondents have unweighted margins of error of +/-2 percentage points, while sample sizes of roughly 1,000 respondents have unweighted margins of error of +/-3 percentage points. In some countries, weighted sample sizes for certain demographics are small. As such, inferences about those groups should be made cautiously. |Czech Republic|General|Mexico|Online|Singapore|General|Vietnam|Online| |---|---|---|---|---|---|---|---| |Egypt|Online|Netherlands|General|South Africa|Online||| Additional data considerations: sample sizes and margins of error In some countries, weighted sample sizes for certain demographics are small. As such, inferences about points, while sample sizes of roughly 1,000 respondents unweighted margins of error of +/-2 percentage have unweighted margins of error of MV MORNING CONSULT Netherlands Vietnam General Mexico Online Online MV MORNING CONSULT Monthly Tracking Overview Morning Consult surveys approximately 19,000 adults in 19 countries on key political, policy and economic issues every month. Issue coverage is spread across eight core themes, with historical coverage of 2+ years. All interviews are conducted online. Morning Consult samples among demographically defined strata to reach a desired target audience in each country. Surveys generally begin fielding in the last week of each month. Portions of our syndicated monthly surveys do not field in China and Russia due to political sensitivities. Data series Morning Consult tracks a variety of issues across eight core themes. Themes Representative topical coverage Business & regulatory Views on taxation, antitrust, wages, data protection, property climate rights, economic and policy stability, and judicial fairness Trade & investment Attitudes toward tariffs, local manufacturing, foreign-owned policy climate business operations and general market openness Social policy & labor Views on income inequality, workplace and labor protections, market dynamics generational mobility, and productivity as a means of advancement Nationalism & Views on global cooperation, isolationism, immigration and internationalism global institutions, alongside assessments of national pride Threat perceptions Concern about various global and domestic threats, including COVID-19, cross-country conflict, civil unrest and climate change Trust & governance Evaluations of trust in politicians, local and national government, and business, plus views of democracy and authoritarianism ESG & corporate Attitudes toward environmental and social considerations, racial responsibility and gender equity, corporate and government responsibilities, and emissions reductions Policy priorities Views on the importance of national security and defense, trade-offs between democracy and growth, and climate resiliency strata to reach a desired target audience in each country. Surveys generally begin fielding in the last week issues every month. Issue coverage is spread across eight core themes, with historical coverage of 2+ Evaluations of trust in politicians, local and national government, and gender equity, corporate and government responsibilities, Concern about various global and domestic threats, including and business, plus views of democracy and authoritarianism global institutions, alongside assessments of national pride rights, economic and policy stability, and judicial fairness generational mobility, and productivity as a means of business operations and general market openness between democracy and growth, and climate Consult samples among demographically defined Representative topical coverage Business & regulatory Trade & investment Threat perceptions market dynamics internationalism Policy priorities policy climate advancement resiliency Themes COVID-19, trade-offs MV MORNING CONSULT MV MORNING CONSULT Country coverage & target sample sizes Tracking start dates vary by country. |Col1|Tracking start|Target sample size| |---|---|---| |Argentina|Jun’22|1,000| |Australia|Jan 21|1,000| |Brazil|Jan 21|1,000| |Canada|Jan 21|1,000| |China|Jan 21|1,000| |France|Jan 21|1,000| |Germany|Jan’'21|1,000| |India|Jan 21|1,000| |Italy|Jan’'21|1,000| |Japan|Jan’'21|1,000| |Mexico|Jan 21|1,000| |Nigeria|Jan ’23|1,000| |Russia|Jan 21|1,000| |Spain|Jan ’22|1,000| Country coverage & target sample sizes Target sample size Argentina Australia Canada Nigeria Jan ’22 France Jan 21 Jan 21 Jan 21 Jan 21 Jan’'21 Jan 21 China Brazil 1,000 1,000 1,000 1,000 1,000 1,000 1,000 Italy MV MORNING CONSULT South Africa Jan 21 1,000 South Korea Jan 21 1,000 Turkey Jan ’22 1,000 United States Jan 21 1,000 United Kingdom Jan’'21 1,000 Weighting, representativeness & population targets Data is weighted to approximate representative samples of the target population among adults in each country surveyed. Weighting parameters vary by country and are derived from national statistics offices whenever available. Weights are updated periodically. Please contact us for additional details on weighting parameters used in particular countries. Population targets for Morning Consult’'s monthly syndicated surveys are identical to the targets used for our daily syndicated surveys. Additional Data Considerations Sample sizes and margins of error Unless otherwise noted, all daily and monthly syndicated data referenced in Morning Consult’s political analysis utilizes weighted sample sizes and unweighted margins of error. As a general rule, sample sizes of roughly 2,000 respondents have unweighted margins of error of +/-2 percentage points, while sample sizes of roughly 1,000 respondents have unweighted margins of error of +/-3 percentage points. In some countries, weighted sample sizes for certain demographics are small. As such, inferences about those groups should be made cautiously. |South Africa|Jan 21|1,000| |---|---|---| |South Korea|Jan 21|1,000| |Turkey|Jan ’22|1,000| |United States|Jan 21|1,000| |United Kingdom|Jan’'21|1,000| Additional Data Considerations whenever available. Weights are updated periodically. Please contact us for additional details on weighting Population targets for Morning Consult’'s monthly syndicated surveys are identical to the targets used for country surveyed. Weighting parameters vary by country and are derived from national statistics offices Data is weighted to approximate representative samples of the target population among adults Sample sizes and margins of error those groups should be made cautiously. +/-2 percentage points, while sample United Kingdom margins of error of South Korea MV MORNING CONSULT Turkey Jan 21 Jan’'21 1,000 1,000 1,000 MV MORNING CONSULT Questions? For questions regarding Morning Consult Political Intelligence and related data assets, please contact politics@morningconsult.com. For press engagements, please contact communications@morningconsult.com. For inquiries about Morning Consult Pro, please contact pro@morningconsult.com. For questions regarding Morning Consult Political Intelligence and related data assets, please contact For press engagements, please contact politics@morningconsult.com. about Morning Consult Pro, please contact How will sexism impact Harris's presidential campaign? Author: Conroy, Meredith, Date: 8/1/2024 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: YY69YW48 Cite Key: Conroy2willSexismHarris Zotero Item | Lit Note With Vice President Kamala Harris now the expected Democratic nominee for president, she'll be the second woman, and first woman of color, to receive a major political party's nomination for president in the United States. And while Democrats' elevation of Harris since President Joe Biden's exit from the race has been largely smooth, the early reaction from the Republican Party suggests that Harris's gender (and race) may be central to their criticisms. There's certainly a history of gender being leveraged to undermine women in politics, but recent years have seen significant progress in normalizing women's success at high levels. With that in mind, Harris's campaign could tell us to what extent those types of criticisms still hold weight, and to what extent they could backfire. The 'electability' trap Attitudes about women in American politics have evolved a lot over the last 70 years. In 1958, just 54 percent of respondents said they would vote for a well-qualified woman to become president, according to Gallup. In January 2024, 93 percent said the same. That support is higher among Democrats; 99 percent of Democrats compared with 87 percent of Republicans said they would vote for a woman. (Independents fell in the middle, at 93 percent.) Of course, Gallup doesn't ask the same question about male presidential candidates. Attitudes about people of color in American politics have evolved, too. For example, in 1958, just 38 percent of respondents said they would vote for a Black presidential candidate, compared with 92 percent in 2024. Again, Democratic support is higher than Republican (96 percent compared with 88 percent, respectively). Gallup doesn't ask this question specifically about women of color, Asian candidates, or white candidates. (It more recently began asking about \"Hispanic\" candidates — 93 percent of respondents said they would support in 2024). Overall, support for a female or Black candidate has been fairly consistent since around 2000, and the partisan split is limited in this hypothetical, with Democrats modestly more open to supporting women and nonwhite candidates. But other polling shows Democrats are particularly enthusiastic about electing a female president. A 2018 poll by Pew found 63 percent of Democrats said that they \"personally hope the United States will elect a female president in their lifetime,\" compared to just 24 percent of Republicans. And according to a new AP/NORC poll, a majority of Democrats say electing a woman or person of color would be a \"good thing\" (70 percent and 61 percent, respectively), while the majority of Republicans say it doesn't matter (68 percent and 78 percent, respectively); 15 percent of Republicans and virtually no Democrats say it would be a \"bad thing\" to elect a female president. Of course, to have the opportunity to vote for female candidates, or candidates of color, Americans have to nominate these candidates through party primaries. In the 2020 primaries, Hillary Clinton's loss to Trump in 2016 seemed to loom large in many Democrats' minds — studies have shown that sexism motivated voters' choice at the ballot box and negatively impacted Clinton's vote share in the 2016 general election. This probably helps explain why, despite expressed support for an abstract female or Black candidate, the 2020 primary saw Democrats seemingly balk at nominating someone with either (or both) identities. While a 2019 YouGov/CBS News poll showed Democrats preferred a woman or a person of color as their party's presidential nominee, the party ultimately went with the \"safe\" pick in Biden (and Sen. Bernie Sanders was his closest rival). These statistics point to the fact that some Americans may not support female candidates or candidates of color not because of overt sexism and racism or even implicit bias, but because of more complicated fears about whether candidates with these identities can win. This is a concept known as \"strategic discrimination,\" which explains that women and people of color are underrepresented in U.S. politics because voters hesitate to support nonwhite, nonmale candidates based on concerns about whether other voters will support them. Of particular note for Harris's candidacy, the linked paper found Black women to be perceived as less electable than either white women or Black men, demonstrating the unique challenge she faces as a candidate with multiple marginalized identities. Another study found Democrats in the 2020 primary rated women and people of color as less \"electable\" than their white, male counterparts despite more often being the preferred candidate in a hypothetical matchup. And in 2019, a poll by LeanIn.org found that among Democrats, 58 percent said that it would be at least slightly harder for a woman to win against Trump as opposed to a male candidate. In that same poll, 53 percent said they were very or extremely ready for a woman president — a near-identical number to that in a YouGov/Economist poll last month — but far fewer, 16 percent, thought \"most Americans\" were very or extremely ready. One reason Biden selected Harris as his running mate was an effort to appeal to Democrats' demand for a ticket that represented an increasingly diverse party. But with Harris now making her own choice for VP — almost all of the names that have been floated are white men — it's clear that electability concerns remain a key factor here. A July 19-21 YouGov poll found Democrats and Democratic-leaning independents still think members of the party would be less likely to support a woman for president, compared to a man, especially if her running mate was also a woman. And according to a 19th News/SurveyMonkey poll fielded July 22-24, 40 percent of respondents think Harris will have a better chance of winning with a white male running mate, while 42 percent think that choosing a woman would have a negative impact on her campaign (just 16 percent believe it would have a positive impact). Will Harris neutralize gendered and racialized attacks? Political opponents have long exploited voters' skepticism about women and people of color in politics, and Harris's identity as a multiracial woman inevitably means voters' attitudes about gender, race and their intersection will come into play in this election, whether or not they are directly invoked by her opponents … But even at this early point in her candidacy, they are. Renewing an attack they've made throughout her vice presidency, Republicans have called Harris a \"DEI candidate,\" a label that stokes racial resentment by suggesting that success by minority individuals is not earned, or even comes at the expense of others. Other conservative commentators have suggested that Harris \"slept her way to the top,\" a trope that also reflects sexualized stereotypes about women in politics, especially women of color. Vice presidential candidate JD Vance's attacks on Harris as a longtime politician — \"What the hell have you done other than collect a government check for the last 20 years?\" he asked at a rally recently — evoke the Welfare Queen trope, which dates back to the 1970s and demonizes single women (especially single Black women) who receive government assistance. And Vance has attacked Harris using more direct gendered tropes as well: In a resurfaced clip from 2021, Vance questions Harris's leadership capacity by referring to her (and other Democratic leaders) as \"childless cat ladies,\" using this language to question their stake in the country's future. (Harris is a mother to two stepchildren, one of whom took to social media to defend her against the caricature.) Still, there is some reason to think that the effectiveness of racialized and gendered criticism could be stunted this cycle. For one, there's been a stark partisan sorting when it comes to attitudes that tap into racism and sexism since 2016. Research shows that racial resentment among white Americans toward Black Americans has remained stable since the '80s, but these attitudes are now more closely correlated with political beliefs and partisanship. Similarly, beliefs about women in the workplace are increasingly polarized along partisan lines. In other words, these beliefs are largely already baked into partisanship, meaning that those likely to be dissuaded from supporting a candidate like Harris based on her race or gender were already unlikely to vote for any Democrat in the first place. Moreover, women have made considerable gains in American politics since 2016. In the 2018 midterms, women's win rate in Democratic primaries was double that of men's, and 60 percent of the congressional seats flipped by Democrats in that cycle were won by women. It's not just true of Democratic women, either: In 2020, Republican women were similarly responsible for most of the congressional seats their party flipped. And of course, Harris was elected as the country's first female vice president in 2020. Outcomes like these challenge the notion that women aren't electable. In fact, in each of those election cycles, female candidates arguably had some electoral advantages. In the wake of Trump's election, the historic Women's March and the growing #MeToo movement brought renewed attention to women's issues, which prompted more Democratic women to run for office — and may have helped Democrats at large by mobilizing young and female voters. Then in 2020, Republicans adopted a concerted strategy to recruit women (and people of color) to their candidate pool, as Republican women, motivated by resentment toward the progressive women's movement sparked by Trump's election, ran on a conservative counter-narrative to what they believed was a misrepresentation of women's interests — a pitch that would've been less sincere coming from GOP men. And when it comes to the highest office, Nikki Haley made a convincing electability argument in this cycle's Republican primary. \"My view is that Clinton's 2016 campaign went a long way toward normalizing a woman presidential candidate, and perhaps even a president,\" said Christina Wolbrecht, a political scientist at Notre Dame who studies gender and role model effects in American politics. \"Multiple women ran for the Democratic nomination in 2020, several of whom were considered credible competitors, and Nikki Haley outlasted everyone except Trump on the Republican side this year. Certainly the fact that these candidates were women did not go unremarked, but it did not seem to be as central to coverage as it had been in 2016.\" Since the 2022 Dobbs decision, Democrats have banked on abortion as a winning issue that would drive their voters to the polls, but while Biden appeared to be losing some of this support, Harris could refocus the race around abortion rights to her advantage. As Wolbrecht put it, \"talking about reproductive rights comes much easier to Harris than it does to Biden, so we can expect that issue to be even more central to her campaign than it was to Biden's.\" This is all the more true for Harris because women are viewed as more adept at handling health care and social policy issues more generally. Harris is also likely be perceived as more liberal than Biden would have been because of her gender — and while that's a potential pitfall in a general election, it could also give her more room to play both sides by pivoting to the center on certain issues, such as leaning into her background as a prosecutor to counter the \"soft on crime\" attacks that tend to dog both Democrats and women. (Notably, that tightrope is narrower for a woman of color, as Black women are penalized more than white women for using \"dominant\" language.) Still, a Democratic ticket with Harris at the top presents a strong contrast to this year's GOP ticket on the basis of gender alone. The GOP convention doubled down on the \"tough guy\" Trump persona, as did his choice of running mate, given Vance's history of emphasizing male victimhood and gender traditionalism. In fact, Trump's campaign thus far has seemed to endorse a strategy meant to energize and attract male voters, including by making inroads among men of color, even if it could alienate some female voters. Trump surrogate Rep. Matt Gaetz laid out this strategy bluntly in January, saying, \"for every Karen we lose, there's a Julio and a Jamal ready to sign up for the MAGA movement.\" Some prominent Republicans, including party leaders and Haley, have already expressed concern about these race- and gender-based lines of attack, worried that the approach could backfire. For Democrats, countering these attacks by explicitly characterizing them as sexist or racist can also be fraught, but Harris's campaign and surrogates may have found a more effective counter, one that turns gendered attacks back on Trump and Vance: They've recently started characterizing the Republican ticket as \"weird,\" particularly in reference to their gendered language and stances on women's issues. \"These guys are just weird. They're running for, like, 'He-Man Women Hater's Club' or something,\" Minnesota Gov. Tim Walz said. And Trump and Vance have taken the bait, also using \"weird\" to describe Harris's positions on climate and immigration. As Omar Wasow, a political scientist at the University of California, Berkeley put it, \"The classic maneuver when your opponent has a good position is to shift the battle to new terrain. So, 'they're weird' is not only more accessible, but potentially redefines the contest to 'normal vs not normal.'\" And in so doing, gendered and racialized criticism could be rendered less relevant. **** It's still early, but polling certainly suggests that Harris is a \"viable\" candidate — she's been polling neck-and-neck with Trump, with similar levels of support as Biden, for whom a big selling point was his palatability to a wide coalition of voters and his ability to beat Trump. Whether or not Harris becomes the first female president of the United States, her status as a major party nominee, and the gendered and racialized attacks already invoked by her opponents, mean the campaign will certainly expose attitudes about women, race and politics in America. The Rise of the Union Right Author: Lowrey, Annie, Date: 2024-12-30 Collections: Hot Takes US Elect 2024 Zotero Key: LWJYMIMX Cite Key: Lowrey24RiseUnionRighta Zotero Item | Lit Note Richard Tikey builds coke-oven doors for U.S. Steel. He’s a union guy, through and through: He’s been a union member for 26 years, and is now the vice president of his local, the United Steelworkers 1557 in Clairton, Pennsylvania. He has spent much of his adult life voting for Democrats. Kamala Harris and Joe Biden lobbied hard for votes like Tikey’s. The Biden administration increased tariffs on foreign steel and spent hundreds of billions on heavy infrastructure. It supported union drives, stocked the National Labor Relations Board with worker-friendly lawyers, banned noncompete clauses, expanded eligibility for overtime, cracked down on union busting, and extended protections for civil servants. Biden was the first president in history to walk a picket line. In contrast, Donald Trump has supported “right to work” laws, attempted to gut federal worker protections, and named union busters to lead the Department of Labor and the NLRB. He has also supported firing workers on strike, stiffed contractors for his campaigns and businesses, described American wages as “too high,” and bragged that he denied his own workers overtime pay. Even so, weeks before the election, Tikey appeared in a lime-green hard hat and a Steelworkers for Trump T-shirt, giving a thumbs-up for cameras alongside the once and future president. “Why would we support Democrats?” Tikey told me this month. “Every time we have a Republican in office, things are better.” Millions of other union members feel the same way. Exit polls indicate that nearly half of union households voted Republican in 2024, up from 43 percent in 2016 and 37 percent in 2000. Other polling shows that Trump commanded a 26-point lead among white voters without a college degree in union homes, up nine points since 2020. Conversely, Democratic support dropped 35 percentage points among Latino voters in union households, and also waned among Black union voters. These trends are part of a long, slow tectonic electoral realignment. This century, the country has become less polarized in income terms, with Democrats gaining among coastal elites and Republicans among the working class. In the past decade, it has also become less racially polarized, with Black, Asian, and Latino voters shifting red. And education has become a much stronger predictor of a person’s partisanship. Democrats now dominate among the college-educated, and Republicans dominate among white people without a degree. The Republican coalition has become more diverse, while the Democrats have seen their working-class base—the working-class base that delivered them election after election in the 20th century—walk away. What would it take to get voters like Tikey to come back? First, Democrats need to understand how they lost them. The commonly told story is an economic one, which I have heard from union leaders, the Bernie left, and blue-collar voters who have started voting Republican. The Democrats have more liberal economic policies than the GOP: They support higher taxes on the wealthy and more progressive spending. But this is not the same thing as being pro-worker. And the party has shed voters as it has become more corporatist, pro-globalization, and cosmopolitan. A Democratic president, Bill Clinton, signed NAFTA, which cost hundreds of thousands of jobs in the heartland and suppressed wages. A Democratic president, Barack Obama, failed to pass “card check,” which would have made forming unions radically easier. He also negotiated the Trans-Pacific Partnership, which unions argued would send American jobs overseas. More broadly, Democrats failed to prevent the collapse of the unionized workforce, two decades of stagnation in middle-income wages, and the hollowing-out of the Rust Belt. Their answer was to “ compensate the losers,” rather than avoid policies that generated losers to begin with. This cost them votes, as well as credibility among many working-class voters. “Beginning with Jimmy Carter, there was an increasing effort to see unions and labor as a special interest, rather than a foundational part of the party,” Michael Podhorzer, the longtime political director of the AFL-CIO, told me. “There hasn’t been a political party in this country with working people at the table for decades. This is the bed the Democrats made for themselves, and it obviously has not paid off in the way they anticipated.” At the same time, particularly in the past decade, Republicans have become more economically populist. The mainstream of the party now promotes restricting trade and running enormous deficits, even during economic expansions. They may threaten to make huge cuts to popular social programs, but rarely actually do so. The Affordable Care Act lives on; Medicare and Social Security remain untouched. Trump signed a stimulus bill twice as large as Obama’s. Neither party delivered what it promised, economy-wise. It cost the Democrats and helped the GOP. Political scientists and pollsters layer a cultural story onto this economic story. Since the 1970s, academics have noted that as societies have become wealthier, their voters have tended to care less about bread-and-butter financial issues and life-and-death defense ones. They begin voting on topics such as the environment, immigration, gender equity, and civil rights. (Academics call this “postmaterialism.”) People can “choose parties on the basis of their overall social and cultural views,” Matthew Grossmann, a political scientist at Michigan State University, told me. Voters on both the right and the left have become postmaterial. The college-educated have aligned with the Democrats, attracted by the party’s views on climate change and racial equality. Non-college-educated voters have shifted toward the Republicans on the basis of immigration, abortion, and race. Patrick Ruffini, a Republican pollster and strategist, told me that Trump’s coalition might have been slightly lower-income than Harris’s during this election. If so, it would likely be the first time the Republican coalition was less wealthy than the Democratic coalition in decades. “You have the party of the working class versus the professional class,” he said, but it’s “cultural issues that are driving these changes.” The greater emphasis on cultural issues has posed problems for both parties in their appeals to the American center, even as it has attracted votes too. In 2022, voters turned away from the GOP after the Supreme Court reversed Roe v. Wade. (Some pollsters expected the same in 2024, but other issues predominated.) In the past three elections, the left’s position on immigration has alienated it from Latino voters it was desperately trying to hang on to. As my colleague Rogé Karma writes, these voters didn’t care about immigration as much as they cared about kitchen-table economics, and many had less liberal opinions about the border than professional Democrats. The Democrats’ positions have proved the more alienating ones for the small-c conservative American public—something the party has been slow to acknowledge. “The Democratic Party is incredibly well educated and has incredibly liberal views on social issues, relative to the population as a whole,” Grossmann noted. “It is just not very easy to change that.” For all that cultural issues help explain how Democrats lost the working class over the past two decades, the economy nevertheless seems to have been the decisive factor in Trump’s 2024 victory. In polls, voters consistently named high prices as their top concern. They consistently said they trusted Trump to do better on the issue of inflation. Democrats pointed to the good headline numbers in terms of GDP growth, inequality, jobs, and wages, as well as the inflation-rate decline since 2022. Voters felt like the Democrats were ignoring or gaslighting them. Harris did not criticize the Biden administration for its role in stoking inflation. This cost her votes and perhaps the election, a pattern that has played out for incumbent parties around the world. The Biden administration also fumbled in making the case for its policies to middle-income voters. Biden and Harris passed a tremendous amount of legislation but struggled to distill the hundreds of billions of dollars in spending and thousands of finicky provisions into tangible policy deliverables that the public could grasp. “While voters across party lines strongly supported Biden’s populist economic policies, many were not aware that his administration had enacted them,” an election postmortem by the left-of-center polling group Data for Progress found. When I talked with voters during the campaign, I would often ask them what they thought Harris and Trump would do once in office. People tended to give specific answers for Trump, whether they themselves were a Democrat or a Republican. He’d enact tariffs, close the border, fire civil servants, and deport undocumented criminals. Even motivated Democrats, I found, struggled to name Harris’s top priorities. Someone might respond with 10 answers or sometimes none. The candidates the Democrats ran and the strategies their campaigns deployed were less-than-ideal too. Biden’s age and Harris’s lack of authentic connection with voters, something that’s hard to measure but not hard to see, were obstacles to victory. The Democrats’ character-based vilification of Trump failed to connect for many voters who liked the guy and supported his policies. “People underestimated the appeal of Trump’s message to nonwhite working-class audiences,” Ruffini told me. “They didn’t think it could cross over.” History suggests that things will get easier for Democrats, in some ways. If past trends hold, the party will pick up five or more points in the midterms without doing anything. The Republicans will start passing policies and instantly become less popular in the eyes of voters, left and right. And in the next presidential campaign, the Democrats will benefit from being able to run unencumbered by incumbency, against Trumpism, if not Trump himself. Still, pollsters and political scientists told me, the party needs to change. The “Brahmin left”—meaning the educated elite that now makes up the Democratic Party’s base—is not a big enough bloc to defeat Republicans going forward. Democrats have to get back at least some members of the middle class, the working poor, and the unions. In terms of kitchen-table policies, well, the Democrats need to have some. Just a few. Big ones. Popular ones that are easy to understand. A bill that caps the price of all prescription drugs at $25 a month, say, rather than a 19-point policy white paper. The content of such proposals matters too. The Brahmin left tends to be more supportive of redistribution than the working class, which tends to prefer something that economists call “predistribution”: high minimum wages rather than welfare payments, pro-union policies rather than refundable tax credits, antitrust measures rather than food stamps. Moderate families also give higher marks to social spending that feels like infrastructure: universal pre-K, guaranteed jobs programs, and public internet. The cultural drift of the party will be harder to change, political analysts told me. Tacking to the center would mean repudiating activists on immigration, the environment, women’s and LGBTQ rights, and abortion—the same activists who have marched in the streets, raised money, and knocked on doors for Democrats, and have become its most loyal voters. It would mean ignoring many of Washington’s most powerful nonprofits and interest groups. “I’m a progressive,” Jared Abbott, the director of the Center for Working-Class Politics, told me. “I’m not even sure it would work, because the reputation of the party is so set in.” Indeed, Harris brought up that she was a gun owner and ran on her record as a prosecutor. She did not emphasize trans-rights issues, nor did she use the term Latinx in speeches. What did her relative centrism get her? Still, pollsters noted that some politicians have had success with their cultural appeals to more conservative voters: John Fetterman in Pennsylvania, Ruben Gallego in Arizona, Marie Gluesenkamp Perez in Washington. It might not take much more than loudly rejecting some far-left positions, Ruffini told me. “You have to have someone come out and say: ‘Here’s what I’m for and I’m against. And I don’t like some of this cultural stuff.’ Create a clear moment of contrast and differentiation.” I asked Tikey which issues drew him to the Republicans. He made more money under Republicans, he told me (though union data show that workers got large profit-sharing payments under Biden). He thought Trump would do better on inflation, and he appreciated the GOP’s stance on abortion, gender, and guns. Plus, he said, “I don’t understand why unions endorse Democrats when they want to shut down” plants like the one he works in. He has a point. Democrats are not vowing to save coal plants, for instance. They’re promising to compensate the losers. In the future, could a more centrist Democrat, in cultural and economic terms, win Tikey over? “The Democratic Party has changed,” he told me. It just isn’t the party that he and many of his neighbors supported back in the 1990s. “I don’t think so,” he said. The Rise of the Union Right Author: Lowrey, Annie, Date: 2024-12-30 Collections: Hot Takes US Elect 2024 Zotero Key: N7XP9BJX Cite Key: Lowrey24riseUnionRight Zotero Item | Lit Note Richard Tikey builds coke-oven doors for U.S. Steel. He’s a union guy, through and through: He’s been a union member for 26 years, and is now the vice president of his local, the United Steelworkers 1557 in Clairton, Pennsylvania. He has spent much of his adult life voting for Democrats. Kamala Harris and Joe Biden lobbied hard for votes like Tikey’s. The Biden administration increased tariffs on foreign steel and spent hundreds of billions on heavy infrastructure. It supported union drives, stocked the National Labor Relations Board with worker-friendly lawyers, banned noncompete clauses, expanded eligibility for overtime, cracked down on union busting, and extended protections for civil servants. Biden was the first president in history to walk a picket line. In contrast, Donald Trump has supported “right to work” laws, attempted to gut federal worker protections, and named union busters to lead the Department of Labor and the NLRB. He has also supported firing workers on strike, stiffed contractors for his campaigns and businesses, described American wages as “too high,” and bragged that he denied his own workers overtime pay. Even so, weeks before the election, Tikey appeared in a lime-green hard hat and a Steelworkers for Trump T-shirt, giving a thumbs-up for cameras alongside the once and future president. “Why would we support Democrats?” Tikey told me this month. “Every time we have a Republican in office, things are better.” Millions of other union members feel the same way. Exit polls indicate that nearly half of union households voted Republican in 2024, up from 43 percent in 2016 and 37 percent in 2000. Other polling shows that Trump commanded a 26-point lead among white voters without a college degree in union homes, up nine points since 2020. Conversely, Democratic support dropped 35 percentage points among Latino voters in union households, and also waned among Black union voters. These trends are part of a long, slow tectonic electoral realignment. This century, the country has become less polarized in income terms, with Democrats gaining among coastal elites and Republicans among the working class. In the past decade, it has also become less racially polarized, with Black, Asian, and Latino voters shifting red. And education has become a much stronger predictor of a person’s partisanship. Democrats now dominate among the college-educated, and Republicans dominate among white people without a degree. The Republican coalition has become more diverse, while the Democrats have seen their working-class base—the working-class base that delivered them election after election in the 20th century—walk away. What would it take to get voters like Tikey to come back? First, Democrats need to understand how they lost them. The commonly told story is an economic one, which I have heard from union leaders, the Bernie left, and blue-collar voters who have started voting Republican. The Democrats have more liberal economic policies than the GOP: They support higher taxes on the wealthy and more progressive spending. But this is not the same thing as being pro-worker. And the party has shed voters as it has become more corporatist, pro-globalization, and cosmopolitan. A Democratic president, Bill Clinton, signed NAFTA, which cost hundreds of thousands of jobs in the heartland and suppressed wages. A Democratic president, Barack Obama, failed to pass “card check,” which would have made forming unions radically easier. He also negotiated the Trans-Pacific Partnership, which unions argued would send American jobs overseas. More broadly, Democrats failed to prevent the collapse of the unionized workforce, two decades of stagnation in middle-income wages, and the hollowing-out of the Rust Belt. Their answer was to “ compensate the losers,” rather than avoid policies that generated losers to begin with. This cost them votes, as well as credibility among many working-class voters. “Beginning with Jimmy Carter, there was an increasing effort to see unions and labor as a special interest, rather than a foundational part of the party,” Michael Podhorzer, the longtime political director of the AFL-CIO, told me. “There hasn’t been a political party in this country with working people at the table for decades. This is the bed the Democrats made for themselves, and it obviously has not paid off in the way they anticipated.” At the same time, particularly in the past decade, Republicans have become more economically populist. The mainstream of the party now promotes restricting trade and running enormous deficits, even during economic expansions. They may threaten to make huge cuts to popular social programs, but rarely actually do so. The Affordable Care Act lives on; Medicare and Social Security remain untouched. Trump signed a stimulus bill twice as large as Obama’s. Neither party delivered what it promised, economy-wise. It cost the Democrats and helped the GOP. Political scientists and pollsters layer a cultural story onto this economic story. Since the 1970s, academics have noted that as societies have become wealthier, their voters have tended to care less about bread-and-butter financial issues and life-and-death defense ones. They begin voting on topics such as the environment, immigration, gender equity, and civil rights. (Academics call this “postmaterialism.”) People can “choose parties on the basis of their overall social and cultural views,” Matthew Grossmann, a political scientist at Michigan State University, told me. Voters on both the right and the left have become postmaterial. The college-educated have aligned with the Democrats, attracted by the party’s views on climate change and racial equality. Non-college-educated voters have shifted toward the Republicans on the basis of immigration, abortion, and race. Patrick Ruffini, a Republican pollster and strategist, told me that Trump’s coalition might have been slightly lower-income than Harris’s during this election. If so, it would likely be the first time the Republican coalition was less wealthy than the Democratic coalition in decades. “You have the party of the working class versus the professional class,” he said, but it’s “cultural issues that are driving these changes.” The greater emphasis on cultural issues has posed problems for both parties in their appeals to the American center, even as it has attracted votes too. In 2022, voters turned away from the GOP after the Supreme Court reversed Roe v. Wade. (Some pollsters expected the same in 2024, but other issues predominated.) In the past three elections, the left’s position on immigration has alienated it from Latino voters it was desperately trying to hang on to. As my colleague Rogé Karma writes, these voters didn’t care about immigration as much as they cared about kitchen-table economics, and many had less liberal opinions about the border than professional Democrats. The Democrats’ positions have proved the more alienating ones for the small-c conservative American public—something the party has been slow to acknowledge. “The Democratic Party is incredibly well educated and has incredibly liberal views on social issues, relative to the population as a whole,” Grossmann noted. “It is just not very easy to change that.” For all that cultural issues help explain how Democrats lost the working class over the past two decades, the economy nevertheless seems to have been the decisive factor in Trump’s 2024 victory. In polls, voters consistently named high prices as their top concern. They consistently said they trusted Trump to do better on the issue of inflation. Democrats pointed to the good headline numbers in terms of GDP growth, inequality, jobs, and wages, as well as the inflation-rate decline since 2022. Voters felt like the Democrats were ignoring or gaslighting them. Harris did not criticize the Biden administration for its role in stoking inflation. This cost her votes and perhaps the election, a pattern that has played out for incumbent parties around the world. The Biden administration also fumbled in making the case for its policies to middle-income voters. Biden and Harris passed a tremendous amount of legislation but struggled to distill the hundreds of billions of dollars in spending and thousands of finicky provisions into tangible policy deliverables that the public could grasp. “While voters across party lines strongly supported Biden’s populist economic policies, many were not aware that his administration had enacted them,” an election postmortem by the left-of-center polling group Data for Progress found. When I talked with voters during the campaign, I would often ask them what they thought Harris and Trump would do once in office. People tended to give specific answers for Trump, whether they themselves were a Democrat or a Republican. He’d enact tariffs, close the border, fire civil servants, and deport undocumented criminals. Even motivated Democrats, I found, struggled to name Harris’s top priorities. Someone might respond with 10 answers or sometimes none. The candidates the Democrats ran and the strategies their campaigns deployed were less-than-ideal too. Biden’s age and Harris’s lack of authentic connection with voters, something that’s hard to measure but not hard to see, were obstacles to victory. The Democrats’ character-based vilification of Trump failed to connect for many voters who liked the guy and supported his policies. “People underestimated the appeal of Trump’s message to nonwhite working-class audiences,” Ruffini told me. “They didn’t think it could cross over.” History suggests that things will get easier for Democrats, in some ways. If past trends hold, the party will pick up five or more points in the midterms without doing anything. The Republicans will start passing policies and instantly become less popular in the eyes of voters, left and right. And in the next presidential campaign, the Democrats will benefit from being able to run unencumbered by incumbency, against Trumpism, if not Trump himself. Still, pollsters and political scientists told me, the party needs to change. The “Brahmin left”—meaning the educated elite that now makes up the Democratic Party’s base—is not a big enough bloc to defeat Republicans going forward. Democrats have to get back at least some members of the middle class, the working poor, and the unions. In terms of kitchen-table policies, well, the Democrats need to have some. Just a few. Big ones. Popular ones that are easy to understand. A bill that caps the price of all prescription drugs at $25 a month, say, rather than a 19-point policy white paper. The content of such proposals matters too. The Brahmin left tends to be more supportive of redistribution than the working class, which tends to prefer something that economists call “predistribution”: high minimum wages rather than welfare payments, pro-union policies rather than refundable tax credits, antitrust measures rather than food stamps. Moderate families also give higher marks to social spending that feels like infrastructure: universal pre-K, guaranteed jobs programs, and public internet. The cultural drift of the party will be harder to change, political analysts told me. Tacking to the center would mean repudiating activists on immigration, the environment, women’s and LGBTQ rights, and abortion—the same activists who have marched in the streets, raised money, and knocked on doors for Democrats, and have become its most loyal voters. It would mean ignoring many of Washington’s most powerful nonprofits and interest groups. “I’m a progressive,” Jared Abbott, the director of the Center for Working-Class Politics, told me. “I’m not even sure it would work, because the reputation of the party is so set in.” Indeed, Harris brought up that she was a gun owner and ran on her record as a prosecutor. She did not emphasize trans-rights issues, nor did she use the term Latinx in speeches. What did her relative centrism get her? Still, pollsters noted that some politicians have had success with their cultural appeals to more conservative voters: John Fetterman in Pennsylvania, Ruben Gallego in Arizona, Marie Gluesenkamp Perez in Washington. It might not take much more than loudly rejecting some far-left positions, Ruffini told me. “You have to have someone come out and say: ‘Here’s what I’m for and I’m against. And I don’t like some of this cultural stuff.’ Create a clear moment of contrast and differentiation.” I asked Tikey which issues drew him to the Republicans. He made more money under Republicans, he told me (though union data show that workers got large profit-sharing payments under Biden). He thought Trump would do better on inflation, and he appreciated the GOP’s stance on abortion, gender, and guns. Plus, he said, “I don’t understand why unions endorse Democrats when they want to shut down” plants like the one he works in. He has a point. Democrats are not vowing to save coal plants, for instance. They’re promising to compensate the losers. In the future, could a more centrist Democrat, in cultural and economic terms, win Tikey over? “The Democratic Party has changed,” he told me. It just isn’t the party that he and many of his neighbors supported back in the 1990s. “I don’t think so,” he said. Media Mistrust Has Been Growing for Decades—Does It Matter? Author: Holcomb, Jesse, Date: 2024-10-17 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: CJ6STC7B Cite Key: Holcomb24mediaMistrustGrow Zotero Item | Lit Note Midway through the 20th century, the news media was among the most trusted institutions in the United States. Today, it sits near the bottom of the list, outflanked only by Congress in most surveys. It’s one of those social facts that elicits a sense of self-evidence (“We needed a survey for that?”). Everybody knows the media has a credibility problem. And seemingly everyone has got a beef with the news. What happened? In truth, the origins, diagnosis, and prescription for the public’s trust issues with journalism are complex and contested and—despite the current rhetoric about “fake news”—have been decades in the making. For instance, how we define “news media” can influence how we understand public attitudes. Surveys show that Americans generally trust local news organizations more than national ones and that they trust the media they themselves consume over and above “the media” in general. Or consider the range of attributes the public weighs as they consider whether news media can be entrusted to perform certain jobs: Does trust depend on a news outlet’s ability to be transparent? Independent? Accurate? And assuming we agree on our terms, who’s to blame for the trust deficit? We ought to be cautious about oversimplifying a decades-long phenomenon. But the broad contours of the news media’s fall from grace can be sketched by summarizing three big trends, each of which interact with the other two: the acceleration of political polarization, the proliferation of new media platforms, and the economic disruption of the news industry. The impact of polarization Following the Civil Rights Movement of the 1950s and '60s, a major political realignment began to take place in Washington and in the electorate more broadly, a realignment that today has resulted in partisan ideological consistency on both sides of the aisle. As Democrats increasingly aligned around a liberal/progressive agenda and Republicans aligned along a conservative one, it became more common for voters to evaluate any number of policies and institutions through their partisan lens. Additionally, elected officials and voters started to express more hostility toward the out-group, a phenomenon known as “affective polarization.” Gallup’s decades-long polling trends illustrate what that looks like in the media context. In 1973, most Americans expressed a high degree of trust and confidence in the news media to do its job. Democrats (74%) and Republicans (68%) were generally on the same page. But by 2023, while the media had lost ground with both groups (and with independents), the gap had widened dramatically, with just 11% of Republicans trusting the media, compared with 58% of Democrats. Are these polarized views of the news media a response to media behavior? Surely, in part. Over the past half-century, for example, the journalism profession has become more highly educated and more politically lopsided as fewer and fewer in the business tend to identify as Republican. And in the digital news era, news workers tend to cluster in the major coastal (and progressive) urban centers in what one observer called a “game of concentration.” But political polarization has steadily increased over the last couple of decades, sorting the news audiences’ affective responses along increasingly partisan lines. And a growing number of new entrants into the media business have provided friendly venues for amplifying partisan attacks on the media. In the early 1970s, when most Americans expressed a high degree of trust in the news, their choices were limited. American households got their news from one of the three major broadcast networks on television or on the radio, and from their local newspaper, which carried a mixture of national and local headlines. This limited-choice environment continued into the 1980s, even with the launch of the first 24-hour news platform, CNN. But the 1990s saw the emergence of a set of powerful national media brands that would offer alternatives to Americans who wanted something different. “The Rush Limbaugh Show,” nationally syndicated in 1988, put conservative talk radio on the map. Through his acidic commentary, Limbaugh pioneered a particular brand of media criticism that castigated the national press as lapdogs for the Democratic establishment while presenting his own voice as an unvarnished and trustworthy source for disaffected conservative listeners. In the mid-1990s, Fox News debuted, combining CNN’s always-on news and commentary approach with a distinctly conservative voice. MSNBC showed up on the cable dial that same year and soon carved out a liberal niche in response to Fox. When the bumper crop of internet-born news companies came along in the 21st century, conventional wisdom said that differentiation would breed success. This approach wasn’t altogether new but rather an extension of the niche branding pioneered on radio and cable TV. But inherent in the idea was a perverse incentive structure: These new news programs and websites could burnish their brands through attacking the credibility of other media. Gawker Media’s irreverent style implicated stuffy New York media, while Breitbart News Network’s website verticals included a section entirely devoted to exposing “Big Journalism.” And this helped further funnel at least some Americans into political silos, allowing them to follow the news that reinforced their views and to marginalize the sources they disagreed with. The rapid rise of social media as a source of news has shaped public attitudes about journalism in ways we are still trying to fully understand. Research has found that social media use has led to burnout and news avoidance, fueled general mistrust of all media, and introduced an epistemic crisis. Although social media users may be exposed to a negative view of news media based on algorithmic incentive structures that reward certain types of messages, there’s little evidence that these individuals are seeing kinder representations of the news media offline. Even as a majority of Americans now turn to social media as a source of news, they are even more wary of these platforms than they are of professional news organizations. Taken together, it’s unlikely that platforms like Facebook, X (née Twitter), and Instagram have nurtured public trust in the news, even as the promise of the early social web suggested that these tools would foster opportunities for journalists and citizens to interact through crowdsourcing story ideas and soliciting audience feedback. Economic challenges to the news industry sow doubt about its product A third major blow to American trust in the news media came as a result of economic disruption to the news industry, a disruption caused in large part by the technological changes in consumer media that were occurring in the 1990s and beyond. Here, as in other cases of external threats, the industry was not a passive actor. The U.S. newspaper business was at its zenith in the first decade of the 21st century, even as readership had been slowly declining; in 2005, it generated nearly $50 billion a year in advertising revenue, according to a Pew Research Center analysis. But as internet companies such as Craigslist, Google, and Facebook began supplanting services such as classified ads offered by newspapers, audiences—and advertisers—left print. By 2020, newspapers were taking in just $10 billion annually. Increased competition for a shrinking pot of advertising revenue produced a kind of desperation in the industry and a race to the bottom. To be sure, print had never been an altruistic enterprise—take the metropolitan dailies, for instance, that followed their affluent, White customer base to the suburbs, often abandoning Black audiences in the urban core. But when the Great Recession of 2008 put the squeeze on owners—some of them hedge funds with no real interest in the mission of journalism—many were quick to cut costs by laying off reporters and editors, hollowing out local and metropolitan newsrooms around the country. Newspaper employment went from 71,000 in 2008 to 31,000 in 2020. Fewer journalists, thinner reporting, and increasingly desperate advertising content did not escape readers’ attention. A 2020 Pew Research Center survey found that Americans were not only skeptical about the quality of the reporting they saw but also cynical about the business motivations behind the news. It found that no more than half of Americans had confidence in journalists to act in the best interests of the public, and that the public was more likely than not to say that news organizations don’t care about the people they report on. Experts, pollsters, and commentators have done a good job diagnosing the multifaceted issue of American mistrust of the news. So why does it seem to be so difficult for the industry to reverse its fortunes in the court of public opinion? Some persistent myths have made it difficult to find an obvious way forward. Political scientist Jonathan Ladd argues in Why Americans Hate the Media and How It Matters that industry insiders too often embrace the notion that a trusted, independent prestige press is the natural order of things. In fact, prior to the 20th century, few news organizations fit the definition of a prestige press, and many had partisan agendas. Ladd writes that in this historic context, the so-called golden age of American journalism that gave us the Pentagon Papers and the Watergate investigation is more of an anomaly than a status quo. Ladd also argues that news leaders fall prey to the myth that trust used to be high because journalists were especially competent at their jobs decades ago. In fact, trust was high for a variety of reasons, including the low-choice media environment described previously, as well as a more forgiving political culture. Though this myth persists, it has begun to crumble as newsrooms reckon with their own legacies of racism, sexism, and elitism. A third myth views mistrust as inherently a bad thing, a phenomenon that must be reversed. Columbia University journalism historian Michael Schudson has argued that mistrust might mean that journalists are actually doing their jobs, particularly when reporters deliver unwelcome news about a party or politician or public figure. But when our news institutions falter and fail, mistrust should be interpreted as an important signal, not dismissed simply as misdirected partisan noise. And although public mistrust can be uncomfortable for those who are invested in preserving institutions, it’s important to remember that mistrust itself doesn’t automatically lead to worrisome social effects. The news industry needs to reckon with these myths. To be clear-eyed about the way forward, it needs to be clear-eyed about its past. But reckoning is only one part of the strategy. The 2020 Pew Research Center survey found that three-fourths of Americans believe it’s possible to improve public confidence in the media. So what else should news organizations do to repair relationships with their most skeptical audiences? For one, continue to put in the spadework of practicing transparency, engaging at the street level, and prioritizing coverage that matters to communities. Strive for accuracy and fairness and acknowledge mistakes when they’re made. There’s no secret trick or shortcut to building credibility. But there’s evidence that these practices can make a difference, at least around the margins. (For example, one study by the Center for Media Engagement at the University of Texas at Austin found that using “person-centered” language instead of stereotypical labels can make readers from marginalized groups feel more respected and more likely to trust the news article.) But even if the industry were to shore up its weaknesses, that might not be enough to fully reverse the current trends. After all, much of the broader social and political environment is not within the industry’s control. There are probably steps the media could take that might build trust with certain groups, but doing so would compromise journalistic values if reporters are seen as trying to pander to audiences. At the end of the day, public trust is one very important barometer of a healthy press. But it need not be the only one. Journalism at its best is often an antagonist to power, a disquieting force in society, and even an agent that helps communities flourish. The news media must find its way forward without ignoring public opinion—but also without obsessing over it as the final measure of journalism’s performance. Jesse Holcomb is associate professor of journalism and communication at Calvin University and a former principal adviser to the trust, media, and democracy project at the Knight Foundation. Will wokeness derail the Democrats? Author: Brandslet, Steinar, Date: 2024-10-29T07:59:48+00:00 Collections: IdentityPolitics Zotero Key: X3EDH2V3 Cite Key: Brandslet24wokenessDerail Zotero Item | Lit Note Will wokeness derail the Democrats? The presidential race appears to be a dead heat ahead of the United States election on 5 November, but wokeness is ‘an unexploded bomb’. Denne saken er også tilgjengelig på norsk There has been a steady rise in wokeness in recent years, both in the United States and in much of the rest of the world. However, wokeness is not equally popular among everyone, and for US swing voters, this could be the issue that determines whether they vote for one party or the other. The Democrats are far more closely associated with wokeness than the Republicans are. This could be a problem for the former. “There is little doubt that Donald Trump is trying to label Kamala Harris as woke, without using the word itself. He talks about little Jimmy going off to school one fine day, being operated on, and coming home a girl. Absolute nonsense, of course, but he keeps bringing it up at rallies,” said Professor Espen Moe from the Department of Sociology and Political Science at NTNU. So why does Trump talk like that? What is wokeness? In the beginning, the term ‘woke’ was a call to be aware of racism. However, it now encompasses awareness of perceived social injustice in general and includes everything from feminism, veganism and climate activism to gender fluidity, depending on whom you ask. In addition, ‘woke’ has now become a derogatory term among people who are tired of political correctness. More often than not, these individuals are Republicans, or become Republicans. “Gender identity is definitely an issue that creates Republican voters. People who think ‘enough is enough!’” said Moe. Attention to gender identity creates Republican voters. Illustration: Shutterstock, NTB SHOW MORE So far, however, the Republicans have not succeeded much in making wokeness a major issue in the election. “I can’t see that the Republicans have managed to harm the Democratic campaign as much as they probably expected to. And Harris has kept her distance from that minefield,” said Moe. At the same time, the Democrats also attract voters who have a positive attitude towards wokeness, so they cannot completely distance themselves from the movement either. Elections are not always decided by who gets the most votes Moe is currently on sabbatical in Berkeley, California. This is right in the heart of the woke movement. “Let’s just say that wokeness is not considered a problem in Berkeley,” said Moe. But the Democrats have already won California, and the election will not be decided there. In most states, the outcome of the election is more or less predetermined. The Democrats are certain to win in Maryland, while the Republicans will take Alaska. The US election is therefore decided in a small number of swing states, such as Georgia, Arizona and Pennsylvania. Swing states. The redder, the safer the Republican during the election in 2020. The same with blue for the Democrats. Illustration: Wikimedia Commons, CC BY-SA 4.0 SHOW MORE Able to win with fewer votes In the United States, it is possible to become president without having the majority of the population behind you. It has happened five times, most recently in 2016, when Hillary Clinton had nearly three million more votes than Trump, but still lost. This is because the president is not elected through a direct popular vote. Instead, voters compete for so-called electors in each individual state. The winner in each state generally gets all of the electors (with the exception of Maine and Nebraska). These 538 electors then cast their votes for their presidential candidate in the Electoral College. They ultimately decide the election. The idea behind this system is to prevent large states from dominating the smaller ones too much. Smaller states generally have more electors relative to their population size than larger states, giving them proportionally greater power. A vote in California counts less than a vote in Rhode Island, for example. This also creates a structural bias in favour of the Republicans, as nearly all of the smaller states with the most electors relative to their population are Republican. Thus, a small number of voters in a small number of states can determine the outcome of the entire election. And wokeness is one of several factors that play a role. But is wokeness really that important? Maybe not as much as before. Wokeness in decline? The renowned magazine The Economist recently stated that wokeness is in decline in the United States. “It actually feels that way. Comedian Kevin Hart said the same thing a few months ago. The peak has been reached, and comedians are starting to discuss topics for which they would have been cancelled a couple of years ago,” said Moe. So it may not be as important to assert that you are woke anymore. “Biden was never particularly woke, and Harris has refrained from flirting too much with wokeness. The problem for the Democrats is that wokeness has been very important for the left, while the centrist faction, which has traditionally dominated, is convinced that they will lose the election if wokeness takes over the party. And therein lies the dilemma. They need to mobilize all Democratic voters, including the voices on the far left where much of the intensity lies, but they also need to appear moderate and sensible,” said Moe. Joe Biden was never particularly woke. Photo: Official White House photo by David Lienemann SHOW MORE So it is still important. “Principally speaking, I definitely think we should talk about wokeness, and if I were an American academic, I would certainly be much more cautious about what I said during lectures than I bother to be in Norway. But every day that goes by in the election campaign without it being talked about is a good day for the Democrats,” said Moe. According to Moe, the upper echelon, which is not particularly woke anyway, is fully aware of this, and those party members who are even further out on the left are taking care not to make any waves. Israel replacing wokeness “Currently, the threat Trump poses is disciplining the Democratic Party quite well, but Harris occasionally struggles with rowdy voters at rallies. They no longer want to talk about gender-neutral toilets, but Israel,” said Moe. Israel, Palestine, and everything associated with the ongoing situation has taken over as the topic that academics in the USA must be most cautious about discussing. “The war is definitely a problem for the Democrats. Democratic voters on the far left either don’t understand, or refuse to understand, how narrow the American win-set is regarding Israel/the Middle East, and how little the United States can actually do to influence the situation.” The term ‘win-set’ refers to the area where the interests of the actors involved overlap. In this conflict, this area is tiny, if it exists at all. Netanyahu, for example, has a clear interest in Harris not winning the election and sees no reason to accommodate the United States at this point. Harris would lose the election if she embraced either Israel or Palestine, but Trump does not have that problem. “At the same time, the conflict may possibly also be drawing much of the attention that the far left would otherwise have focused on wokeness and other forms of political correctness,” said Moe. The warfare in Palestine has perhaps taken a lot of attention away from woke. Photo: Shutterstock, NTB SHOW MORE The Democrats less dependent on wokeness It is not certain that the Democrats need wokeness as much as they used to. The party already has the greatest credibility in several of the areas that wokeness is associated with. “It is easy for the Democrats to talk about women’s rights. Variants of it could have been dismissed as woke, but in the wake of Roe v. Wade being overturned, which granted women the right to have an abortion, it is impossible to label the women’s movement as extremism,” said Moe. The Republicans are now coming with claims that doctors in certain Democratic states are performing abortions on foetuses after they have been born. “The Democrats have strong credibility on issues regarding racism and ethnicity, and when Trump makes blunders with statements about ‘black jobs’, it is relatively easy to discuss race without it sliding into woke territory.” The Democrats are talking about Trump and Project 2025. This is a plan for a very conservative Republican dream scenario, which may well scare off swing voters. “There are more than enough extreme standpoints there to fill an entire election campaign for the Democrats. And the more bad polls for the Republicans, the more extreme it gets,” said Moe. Individual groups no longer emphasized Perhaps the Democrats have learned something important from the 2016 campaign. “I believe Hillary Clinton and the Democrats made a huge mistake by promoting her as the first female president of the United States and breaking the glass ceiling for women, rather than simply presenting her as the best candidate. The fact that she was a woman was used as a huge selling point. Harris hasn’t fallen into that trap, and perhaps the Democrats have learned. She hasn’t marketed herself as a woman, Black, or Indian.” However, Harris does shift into a different gear than Biden when the topic of abortion comes up. Hillary Clinton. Woman first and skilled politician afterwards? Mistake, says Professor Espen Moe. Photo: United States Department of State SHOW MORE “But apart from that, she presents herself as a person, as an individual, not as part of a group demanding special treatment or claiming to have been treated unfairly.” Instead, she scoffs at the ‘same old racist playbook’ every time Trump talks about her as Black or Indian, and leaves it at that. “I think that is really clever. An individual, competent, with the credentials to back it up, capable of speaking in coherent and consistent sentences, not engaging in smear tactics or insults, except for some manipulative jabs at Trump and his campaign, of course. But none of this is woke, female or Black.” So, what hope is there if you are a Republican? The Republicans are struggling to sell their platform “The economy and immigration are the two major selling points for the Republicans. These are areas where the Republicans have great credibility and support, and they can win over many voters if they manage to focus the campaign on these issues. But Trump is unable to be disciplined or coherent in these areas either. It is possible that Trump might suddenly hit the mark with a lucky shot at some point, and that could change the campaign unexpectedly, but so far, I think he and his team have rather missed the mark,” said Moe. Why is this the case? “Obviously, it has a lot to do with the fact that the Republican presidential candidate is likely the least disciplined presidential candidate in history. In addition, there are ongoing scandals involving other Republicans, such as the Black gubernatorial candidate Mark Robinson in North Carolina, who referred to himself as a ‘black Nazi’ in a comment section on a porn site about a decade ago, saying he didn’t think slavery was that bad.” Wokeness is an unexploded bomb – and the outcome remains uncertain So, will wokeness ultimately harm the Democrats? “Wokeness is perhaps more like an unexploded bomb that most people on the Democratic side are now trying to steer clear of. While Trump is currently neither disciplined, consistent nor coherent enough to deliver credible attacks that convince anyone beyond his own loyal base. Perhaps wokeness will emerge at a later stage and shift the electoral campaign – after all, the election can be decided by very few votes in just a couple of swing states – but so far, I don’t think it has had a significant influence on the campaign. That doesn’t mean that the election is in any way won for the Democrats, as the Republicans are concentrating nearly all of their resources in the swing states of Pennsylvania and Georgia. And that might just be enough. The Republicans will probably win if they take both these states, and the race really is neck and neck there,” Moe said. Taking Action on Attention II Author: IAS, , Date: July 2024 Collections: NeuroPsychoLinguisticPolitics, ElectionPredFeats, MediaAdsPolit Zotero Key: XLSSHSL8 Cite Key: IAS24takingActionAttentionVII Zotero Item | Lit Note TAKING ACTION ON ATTENTION VOLUME II JULY 2024 TABLE OF CONTENTS 3 5 7 10 14 18 21 EXECUTIVE SUMMARY Attention remains a pivotal topic in digital media. As marketers strive to capture and retain consumer attention, the methods and metrics used to measure this elusive concept have evolved. While traditional metrics like viewability have laid the groundwork, it’s clear that a more comprehensive approach to measuring attention is needed to drive meaningful business results. ATTENTION IS A MEASURE OF WHETHER OR NOT AN AD RESONATES WITH CONSUMERS AND CAN BE LINKED TO BUSINESS RESULTS To better understand how attention is related to performance, Integral Ad Science (IAS) conducted extensive research, which included studies with media experts and observational analyses based on data from our more than 280 billion digital interactions captured daily. Continued on next page EXECUTIVE SUMMARY CONTINUED KEY INSIGHTS 83% of U.S. media experts believe an effective attention strategy is crucial for their company** 88% of media experts measured attention in 2023, but more than half relied on proxy metrics** 130% lift in conversion rates among high attention impressions compared to low attention impressions**** 51% lower cost per conversion compared to low attention, representing a $1.43 savings in cost per conversion**** “Th Att ti P ff” I t l Ad S i N b 2023 INTRODUCTION ATTENTION CONTINUES TO BE A MAJOR TALKING POINT ACROSS THE INDUSTRY Attention is nothing new in digital advertising, but it’s still one of the most complex topics in the industry. Marketers remain divided on how to properly harness it, with attention scores, methodologies, and key performance indicators increasingly diversified among marketers and industries alike. “Attention is another stepping stone along the path to outcomes-based measurement.” — eMarketer “In the attention economy, every second counts. For brands, serving ad content that can hold a viewer’s attention for longer not only leads to greater engagement, enhanced brand recall and improved trust with the consumer, it also positively affects their bottom line.” — Forbes “Attention metrics can combat banner blindness by finding new places for marketers to focus when tried-and-true placements are ignored.” — AdExchanger Continued on next page CONTINUED INTRODUCTION In the same study, media experts already measuring attention said they were most interested in upping media performance KPIs (time-in-view, video ad completion, brand safety and suitability, viewability, ad fraud rates, and return on ad spend) and revenue opportunity KPIs (new sales leads, purchase intent, online or offline sales lift, and offline visitation lift). When it comes to branding KPIs, although media experts may have ranked this lower, attention plays a key role in influencing upper funnel attitudes, especially for results like awareness and purchase intent. BUSINESS RESULTS THAT ARE MOST IMPORTANT FOR AN ATTENTION STRATEGY: MEDIA PERFORMANCE 84% REVENUE OPPORTUNITY ONLINE TRAFFIC INTERACTIVITY BRANDING 44% 44% 33% 73% MEDIA EXPERTS CAN AGREE ON ONE THING: RESULTS ARE KEY. In Taking Action on Attention: Volume II, we’ll explore the dynamic link between attention and business results. We’ll dive deeper into the set of metrics that make up results-driven attention and explore fresh, innovative ways to capture genuine consumer attention. Plus, we’ll highlight how an attention-centric strategy can supercharge your media performance and boost your bottom line, with practical tips and insights to help you drive superior results across your KPIs. A SUPERIOR STRATEGY: QUALITY ATTENTION[TM] A SUPERIOR STRATEGY: QUALITY ATTENTION[TM] ATTENTION IS A MEASURE OF WHETHER OR NOT AN AD RESONATES WITH CONSUMERS AND CAN BE LINKED TO BUSINESS RESULTS A proper attention measurement product must inform actions that drive results and account for 1. media visibility, 2) environment of impressions, and 3) the interaction by consumers. These three signals, when measured and optimized against, have the ability to drive proven results. Experts across the industry broadly agree about the three essential signals for effective attention measurement and optimization. In The Attention Payoff study, up to 88% of U.S. media experts agreed about the importance of the three signals of attention: visibility, situation, interaction. As media quality assurance methods evolve to meet emerging demands, media experts will increasingly rely on attention measurement that drives results. Powerful tools combining key signals of visibility, situation, and interaction through the power of machine learning technology will be essential to elevate campaign performance. THAT’S WHERE QUALITY ATTENTION[TM] COMES IN IAS Quality Attention[TM] unifies media quality and eye tracking with machine learning to deliver proven results to brands worldwide. “Th Att ti P ff” I t l Ad S i N b 2023 A SUPERIOR STRATEGY: QUALITY ATTENTION[TM] CONTINUED HOW DOES QUALITY ATTENTION[TM] WORK? One of the most critical ingredients in an effective attention approach is the actionable data that informs media strategies and unlocks proven results for marketers and their campaigns. IAS Quality Attention[TM] is powered by an advanced machine learning model that estimates the propensity of a conversion event or click for each impression. A higher attention score is associated with a higher probability for a conversion event or click. The IAS model uses signals obtained through core measurement capabilities — such as viewability, media type, traffic type, and user interactions — along with data from Lumen’s proprietary model based on one of the world’s largest consumer attention biometric data sets. QUALITY ATTENTION[TM] VISIBILITY SITUATION INTERACTION FOCUS METRICS: Viewability Video Quartiles Time-in-View METRICS: Ad Density Ad Share of Screen Device and Format Type METRICS: User Scroll Volume Up/Down/Mute Pause/Unpause Eye Tracking/Focus powered Lumen data used for predictive modeling by Lumen THE CONNECTION BETWEEN ATTENTION AND PERFORMANCE THE CONNECTION BETWEEN ATTENTION AND PERFORMANCE THE FIRST LOOK AT ATTENTION AND PERFORMANCE IAS’s first step in cracking the code on attention and its connection to performance was understanding what business results can be garnered from ads that experience higher attention. During the beta period for Quality Attention[TM], we observed a strong correlation between attention scores and campaign performance. High attention impressions experienced a 130% lift in success rate on average when compared to low attention impressions. HIGH ATTENTION: Scores range from 60-100 LOW ATTENTION: Scores range from 0-59 SUCCESS RATE: (Conversions + Clicks) / Impressions Next, IAS analyzed billions of impressions along with millions of success events from our Quality Attention[TM] and Total Visibility[TM] measurement data sets with one goal: fully connect attention to business results. THE CONNECTION BETWEEN ATTENTION AND PERFORMANCE HIGHER ATTENTION MAKES FOR HIGHER SUCCESS RATES We compared business results between low and high attention scores, further analyzing how attention continuously impacts results. Higher attention impressions experienced success rates that were twice as high as those with low attention. Success events in this study represent conversion and click metrics captured by DSP log files. SUCCESS RATE BY ATTENTION SCORE WHAT WE OBSERVED: 0.25% 2x higher success rate HIGHER ATTENTION 0.20% 0.2% 0.15% IMPRESSIONS HAD 0.10% DOUBLE THE 0.1% 0.05% SUCCESS RATE AS 0.0% LOW ATTENTION HIGH ATTENTION THOSE WITH LOW ATTENTION. THE CONNECTION BETWEEN ATTENTION AND PERFORMANCE CONTINUED HIGHER ATTENTION IMPRESSIONS ARE WORTH THE COST The high quality of high attention impressions comes at an added cost. High attention impressions cost 8% more than low attention impressions, representing an additional $0.23 CPM. But there’s good news: the additional cost for optimizing toward high attention results in a lower cost per success event. High attention impressions had a 51% lower cost per conversion compared to low attention impressions, representing a $1.43 savings in cost per success event. HIGH ATTENTION IMPRESSIONS LED TO: Decrease in cost per Decrease in cost per success event success event compared to low compared to low 51% $1.43 attention impressions attention impressions QUALITY ATTENTION[TM] AND PROVEN RESULTS QUALITY ATTENTION AND PROVEN RESULTS To showcase the value of Quality Attention[TM], IAS partnered with two leading measurement companies, Upwave and NCSolutions, to investigate the impact higher attention scores have on driving results for major CPG brands. The studies focused on upper funnel brand lift and lower funnel sales lift, respectively, for two global beverage brands. CASE STUDY 1 26% Lift in brand awareness for high attention impressions vs. low attention impressions 69% Lift in purchase intent for high attention impressions vs. low attention impressions QUALITY ATTENTION AND PROVEN RESULTS CONTINUED CASE STUDY 2 INCREMENTAL SALES SALES LIFT RETURN ON AD SPEND +157% +40% +6% LOW HIGH LOW HIGH LOW HIGH ATTENTION ATTENTION ATTENTION ATTENTION ATTENTION ATTENTION QUALITY ATTENTION AND PROVEN RESULTS THE IMPORTANCE OF AVERAGES Understanding the meaning behind metrics is a key element in driving sales and maximizing ROI from digital advertising campaigns. In the two CPG brand case studies, the average attention score was 63 — which is just above the global CPG average of 62. This slightly higher attention average and the impact of the placements with the above average scores is key to understanding how to drive future campaign success. IAS’s Quality Attention[TM] product provides a global averages tool that gives marketers access to attention scores alongside a global average across all eligible impressions. This tool provides additional averages by region, vertical, and subvertical to provide marketers with key points of comparison to help evaluate their campaign’s attention performance. The Quality Attention[TM] global averages tool is refreshed quarterly, giving marketers the ability to benchmark campaign performance with regularly updated metrics. IAS plans to continue to extend the Quality Attention[TM ]global averages tool with even more insights, including providing averages by ad format and device. THE FUTURE OF QUALITY ATTENTION[TM] THE FUTURE OF QUALITY ATTENTION[TM] Nearly eight-in-10 U.S. media experts who measured attention in 2023 say their company is ready to optimize toward the metric this year. Social media is the top target for attention metrics, with 61% of buy- and sell-side experts in the U.S. interested in measuring and optimizing toward attention on social. To accompany our current Quality Attention[TM] measurement capabilities, IAS is working to bring optimization (pre-bid) to market that will enable advertisers to unlock proven results with segments based on our Quality Attention [TM] model. Later, Quality Attention[TM] measurement and optimization coverage will expand into additional environments and formats, as well as a product for publishers. THE ROAD TO QUALITY ATTENTION[TM] PHASE 1: QUALITY ATTENTION[TM] MEASUREMENT (POST-BID) Open web measurement and reporting in IAS Signal Report Builder and interactive Quality Attention[TM] dashboard AVAILABLE NOW COMING SOON PHASE 2: QUALITY ATTENTION[TM] OPTIMIZATION (PRE-BID) Optimize campaigns for performance to unlock proven results within major DSPs FUTURE STATE PHASE 3: EXPANDED COVERAGE Support additional formats and environments (e.g. mobile in-app, ctv, social, etc.) FUTURE STATE PHASE 4: QUALITY ATTENTION[TM] FOR PUBLISHERS Measure inventory for Quality Attention[TM] to understand performance QUALITY ATTENTION BEST PRACTICES 1 2 COMPARE ATTENTION SCORES Compare Quality Attention[TM ]against global averages to identify underperforming campaigns and inform where a deep dive needs to occur for a more detailed understanding of campaign performance. SHIFT MEDIA BUDGETS Identify high and low scoring placements, campaigns, environments, and formats. Test shifting budgets and measuring the impact. 3 LEVERAGE EXCLUSION LISTS Identify sites consistently showing a low attention score and create an exclusion list for them. 4 TEST CREATIVE EXECUTION Map placements and/or ad sizes to creative executions to identify less impactful messages and tactics. 5 ENABLE TOTAL VISIBILITY[TM] Activate Total Visibility[TM] to gain access to conversion and click data that can be leveraged to provide more granular and curated optimization recommendations that drive results. CONCLUSION Attention isn’t a fleeting trend. Its wide adoption signals a fundamental shift in how we measure and understand media effectiveness. As we’ve explored in this white paper, higher attention scores are directly linked to better campaign performance, including increased conversion rates, higher ROI, and significant improvements in brand KPIs, like awareness and purchase intent. The future of media measurement lies in embracing these advanced attention metrics. By continuously refining and expanding the scope of attention measurement tools, marketers can stay ahead of the curve, ensuring their campaigns resonate more deeply with consumers and deliver superior business results. As we look ahead, attention is set to become the gold standard in media metrics. Marketers who adopt a comprehensive attention measurement strategy will be better equipped to navigate the complexities of the digital landscape, drive impactful campaigns, and ultimately achieve their business objectives. ACT ON ATTENTION TODAY. CONTACT IAS TO GET STARTED. GET IN TOUCH www.integralads.com Musk’s politics hadn’t seeped into Tesla. Then he axed its eco car of the future. Author: Verma, Pranshu, Date: 2024-12-10 Collections: NeuroPsychoLinguisticPolitics Zotero Key: R4P87J4G Cite Key: Verma24muskPoliticsEcoCar Zotero Item | Lit Note Earlier this year, top executives at Tesla gathered in Palo Alto to sell CEO Elon Musk on a line of compact cars that would bring the electric-vehicle revolution to budget-conscious consumers across the world. The more affordable car had long been part of Musk’s master plan for Tesla to fight climate change by “accelerat[ing] the world’s transition to sustainable energy.” But Musk axed the proposal, amid budget constraints, according to two people close to Musk who spoke on the condition of anonymity to describe private conversations. Instead, the billionaire green-lit a massive purchase of computer chips, in a deal worth billions intended to enhance Tesla’s luxury cars (and a humanoid robot called Optimus) withenergy-intensive artificial intelligence capabilities. The internal deliberations over the so-called Tesla Model 2, which have not previously been reported, reflect what sources close to Musk describe as a significant shift in the billionaire’s attitude toward climate change. And with Musk deeply embedded in the incoming administration of President-elect Donald Trump, his startling transformation from environmental crusader to critic of dire climate predictions could help bolster moves in Washington against clean energy — and even electric vehicles. Spectators look at Tesla's Core Technology Optimus humanoid robot and a cybertruck at the Bund Conference in Shanghai on Sept. 5. (Cfoto/Future Publishing/Getty Images) Once one of the most vocal American executives on the dangers of climate change, Musk called for a “popular uprising” against the fossil fuel industry in a 2016 film. At Tesla, every internal slide presentation had to include figures from former vice president Al Gore’s documentary “An Inconvenient Truth” citing rising carbon dioxide levels in the atmosphere, a reminder of the company’s mission. But the paragraph about global warmingis no longer required in Tesla’s presentation decks and climate change has plummeted on Musk’s list of priorities: In an August live stream on X, he told Trump “we don’t need to rush” to solve the climate crisis. 🌱 Follow Climate & environment Follow People familiar with Musk’s thinking say the billionaire still believes global warming is a problem, a point he made online as recently as last year. But he now thinks the existential risks from climate-related disasters have been overstated — views influenced by a right-wing universe he has come to inhabit online and in real life in Texas, according to five people familiar with Musk’s thinking, who spoke on the condition of anonymity to describe private conversations. Musk also has developed a newfound enthusiasm for technological solutions to climate change, such as nuclear power and carbon capture. Today, he rarely talks about global warming as an imminent threat, viewing robotics, artificial intelligence, stemming population decline and reaching Mars as more critical to human survival. Musk’s shift presents a challenge for his electric vehicle business, where some employees are committed to working for a mission-driven company. Several senior executives have left amid the recognition that Musk doesn’t prioritize Tesla’s climate mission as much as other priorities, according to two people familiar with the matter. Meanwhile, Trump has campaigned on eliminating the tax credit for electric vehicles that has driven much of Tesla’s business. Musk has conceded that the tax credit eventually would have to end. And he probably will lobby the Trump administration to help nearly all his businesses — not just Tesla, but the rocket company SpaceX and the internet service provider Starlink as well, one of the people said. “Musk, at Trump’s side, both encourages the president-elect to ignore the seriousness of [climate change] and pursue policies that will make the problem worse,” said Paul Barrett, deputy director of the Center for Business and Human Rights at New York University’s Stern School of Business. “He clearly has made the calculation that his relationship with Trump will lead to decreased regulation of his businesses (and many others), and that that indirect benefit outweighs his direct competitive concerns.” President-elect Donald Trump watches with Elon Musk, Sen. Ted Cruz (R-Texas), Sen. Kevin Cramer (R-North Dakota) and Rep. Ronny Jackson (R-Texas) before the launch of the sixth test flight of the SpaceX Starship rocket on Nov. 19 in Boca Chica, Texas. (Brandon Bell/Getty Images/Pool/AP) Musk’s transformation on climate change has been years in the making, said the five people, sparked by a variety of influences: battles with environmental groups, tensions with the Biden administration and a rightward shift related to the pandemic, which has exposed him to new experts and ideas. “He used to be a Democrat who believed everything he was told was true about this,” said one of the people. “And now he’s thinks for himself and realizes, yes, climate change is real, but it’s not nearly among the top problems right now.” Musk and Tesla did not respond to a request for comment. A climate change meltdown When LeonardoDiCaprio strolled onto the floor of Tesla’s sprawling gigafactory in Reno, Nevada, to film a 2016 National Geographic documentary about climate change, Musk didn’t mince words. There needs to be a “popular uprising” against the fossil fuel industry, Musk told DiCaprio, warning that the world is “unavoidably headed toward some level of harm and the sooner we can take action, the less harm will result.” At the time, Musk also was tweeting prolifically about limiting carbon emissions, reminding people that “Tesla is working hard to help stop global warming,” touting a tax on carbon and warning of “climate change meltdown.” By 2020, the covid pandemic had begun to alter his worldview. In Alameda County, California, officials ordered lockdowns that forced the closure of Tesla’s main production plant. Around that time, Musk walked through his Tesla facilities ripping mask mandate posters from the walls, a person close to him said. In May 2020, Musk announced Tesla would move its headquarters out of the state. He soon relocated to the Austin area, though Tesla maintained its engineering headquarters in Silicon Valley. Around that time, Musk’s daughter Vivian Jenna Wilson — a transgender woman from whom he is now estranged — received gender-affirming care. Friends say Musk was devastated by his daughter’s transition, and he has vowed to “destroy the woke mind virus” that he claims “killed” her. Both events helped cement Musk’s turn against Democrats, which in turn pushed him to rethink his views on climate change. “Anything the left saying [was] conspiratorial and bullshit,” said one of the people familiar with his thinking. Another turning point came in 2021, when President Joe Biden hosted an electric vehicle summit featuring each of the other major American automakers and the United Auto Workers. Associates say Musk was outraged that Tesla was not included, and that the incident turned him against the Biden administration. President Joe Biden takes an electric vehicle for a spin around the South Lawn of the White House in Washington, D.C. (Bill O'Leary/The Washington Post) After moving his personal life and sprawling business empire to Austin, Musk began discussing climate issues and other politics with friends and associates who are skeptical of catastrophic global warming predictions. Among them was Palantir co-founder Joe Lonsdale, a libertarian-leaning conservative who also relocated from Silicon Valley during the pandemic. Lonsdale, who is active in right-wing politics, introduced Musk to Florida Gov. Ron DeSantis (R), at the time a presidential hopeful favored by the tech elite. Lonsdale argues that climate change could be solved in part by nuclear energy with advanced cooling technologies, according to one of the people. He is a fan of Steven Koonin, a Stanford University physicist who worked in the Department of Energy under former president Barack Obama. This year, Koonin published a book critiquing “consensus science” on climate change, arguing that experts exaggerate the impacts and that slashing carbon emissions wouldharm development and economic growth. The U.N. Intergovernmental Panel on Climate Change has projected that unmitigated CO2 emissions will cause additional weather disasters, stunted economic growth and dangerous heat for billions of people around the world. In 2023, Musk got to know Vivek Ramaswamy, the former pharmaceutical executive who ran against Trump in the Republican primary. In his most recent book, Ramaswamy acknowledges that global surface temperatures are rising but also argues that there is no conclusive evidence this rise is bad for humanity, citing Koonin and other researchers who point to potential net positive effects of planetary warming. Ramaswamy, Musk’s co-chair on Trump’s commission on government efficiency, has discussed his views on climate change with Musk on podcasts and on X. Vivek Ramaswamy and Elon Musk, with his son X, depart the Capitol building in Washington, D.C. on Dec. 5. (Craig Hudson for The Washington Post) Ramaswamy and Lonsdale declined to comment on conversations with Musk. Meanwhile, Musk’s views of environmental nonprofit groups has also shifted. In 2018, shortly before his roughly $6 million in donations to the Sierra Club became public, he tweeted his thanks to the group “for fighting climate change,” adding: “This affects every living creature on Earth.” Today, Musk thinks the Sierra Club, Greenpeace and other environmental groups have impeded his businesses, one of the people said. For example, the Sierra Club of Texas attempted to stall a new Tesla plant in Travis County, which includes Austin — a move Musk viewed as antithetical to the advocacy groups’ climate goals, the person said. “That struck him as pretty nuts,” said the person, adding that Musk’s takeaway was: “They say they are environmental groups, but do they care about the environment?” Musk also bristled when Tesla’s gigafactory in Grünheide, Germany, lost power and had to halt work earlier this year after suspected arsonists set fire to an electricity pylon near the site. The incident, portrayed by media outlets as environmentalists railing against corporate greed, angered Musk, the person said. “These are either the dumbest ecoterrorists on Earth or they’re puppets of those who don’t have good environmental goals,” Musk wrote on X in March. Ben Jealous, the Sierra Club’s executive director, said Musk’s shift in climate views will come at a cost. “By embracing Donald Trump, Elon is supporting an agenda that would destroy the electric vehicle economy, kill jobs and accelerate the climate crisis to uncontrollable extremes,” Jealous said. Tefere Gebre, chief program officer at Greenpeace USA, said in a statement that billionaires like Musk have “bought” the White House and are “angling to crush dissent.” Some Tesla shareholders now question Musk’s commitment to the company’s original mission, following his public takeover of Twitter, now X, and measures that appeared to give Tesla’s chief executive more power. Fred Lambert, editor in chief of Electrek, a pro-electric-vehicle blog, sold his entire stake in Tesla earlier this year after decisions, including shareholders reapproving Musk’s $56 billion compensation package, made him reevaluate his position. In an interview, Lambert said the company’s mission now appears to be “enriching Elon Musk.” As Trump prepares to take office in January, speeding the launch cadence at SpaceX is at the top of Musk’s wish list for the new administration, according to a person close to him. He also wants to limit oversight of the rocket company by the Federal Aviation Administration and the U.S. Fish and Wildlife Service, which are assessing the environmental impact of SpaceX’s South Texas launch facility. Cleanup begins following the launch of the SpaceX Starship spacecraft and Super Heavy rocket launched from Starbase that heavily damaged the pad on April 20, 2023. (Jonathan Newton/The Washington Post) Musk also may lobby for policies allowing Starlink to launch more satellites and increase their competitiveness in the United States, the person said, and for the National Highway Traffic Safety Administration to streamline the regulation of autonomous vehicles, which Musk has called “the future” of Tesla. People who have worked with Musk said it’s unlikely he will lobby with the same zeal for climate-related issues. Robert Zubrin, president of the Mars Society and a longtime Musk associate, said the billionaire’s alliance with conservatives now overshadows that priority. “He decided he would join this camp, and this was more important than the whole climate cause. He decided to go all in,” Zubrin said. “And I guess in one sense, that is characteristic Musk: When he decides to do something, he goes all in.” Americans’ Dismal Views of the Nation’s Politics Author: Nadeem, Reem, Date: 2023-09-19T16:55:10+00:00 Collections: Voting Systems Zotero Key: BPARBKMD Cite Key: Nadeem23amerDismalPolitArticle Zotero Item | Lit Note 65% say they always or often feel exhausted when thinking about politics (Pew Research Center illustration; photos via Getty Images) Americans have long been critical of politicians and skeptical of the federal government. But today, Americans’ views of politics and elected officials are unrelentingly negative, with little hope of improvement on the horizon. Majorities say the political process is dominated by special interests, flooded with campaign cash and mired in partisan warfare. Elected officials are widely viewed as self-serving and ineffective. A comprehensive new Pew Research Center study of the state of the nation’s politics finds no single focal point for the public’s dissatisfaction. There is widespread criticism of the three branches of government, both political parties, as well as political leaders and candidates for office. Notably, Americans’ unhappiness with politics comes at a time of historically high levels of voter turnout in national elections. The elections of 2018, 2020 and 2022 were three of the highest-turnout U.S. elections of their respective types in decades. But voting in elections is very different from being satisfied with the state of politics – and the public is deeply dissatisfied. • Just 4% of U.S. adults say the political system is working extremely or very well; another 23% say it is working somewhat well. About six-in-ten (63%) express not too much or no confidence at all in the future of the U.S. political system. • Positive views of many governmental and political institutions are at historic lows. Just 16% of the public say they trust the federal government always or most of the time. While trust has hovered near historic lows for the better part of the last 20 years, today it stands among the lowest levels dating back nearly seven decades. And more Americans have an unfavorable than favorable opinion of the Supreme Court – the first time that has occurred in polling going back to the late 1980s. • A growing share of the public dislikes both political parties. Nearly three-in-ten (28%) express unfavorable views of both parties, the highest share in three decades of polling. And a comparable share of adults (25%) do not feel well-represented by either party. • Candidate choices are underwhelming. As the presidential campaign heats up, 63% of Americans say they are dissatisfied with the candidates who have emerged so far. Setting aside the presidential campaign, there has been a downward trend in views of the quality of all political candidates. Just 26% rate the quality of political candidates as very or somewhat good, down about 20 percentage points since 2018. • Majorities back age and term limits and eliminating the Electoral College. Reflecting the public’s frustration with the federal government and political leaders, large shares of Americans support various changes to the political system, including for such long-standing proposals as establishing term limits for members of Congress and scrapping the Electoral College. Age limits – for both federal elected officials and members of the Supreme Court – draw broad support. But there is little appetite in the public for increasing the size of the U.S. House or modifying the allocation of Senate seats. The new study of Americans’ views of the state of the political system is primarily based on a survey conducted July 10-16, 2023, among 8,480 adults, with additional data from a survey conducted June 5-11, 2023, among 5,115 adults. Both were conducted on Pew Research Center’s nationally representative American Trends Panel. A little more than a year before the presidential election, nearly two-thirds of Americans (65%) say they always or often feel exhausted when thinking about politics, while 55% feel angry. By contrast, just 10% say they always or often feel hopeful about politics, and even fewer (4%) are excited. The survey also provides people several opportunities to describe in their own words their feelings about the political system and elected officials. When asked to sum up their feelings about politics in a word or phrase, very few (2%) use positive terms; 79% use negative or critical words, with “divisive” and “corrupt” coming up most frequently. We also asked people to identify the strengths of the political system, as well as its weaknesses. Among the positive responses, roughly one-in-ten point to the structures of U.S. government, including its system of checks and balances (12%), freedoms and democratic values (9%) and the opportunity to vote in elections (8%). Yet it is telling that a majority of Americans are unable or unwilling to identify strong points of the nation’s political system. While about a third gave no answer, another 22% write “nothing” – meaning that in their view, the political system does not have any strengths. Explore this report This overview covers key takeaways from our study of Americans’ attitudes about the political system and political representation. For more in-depth analysis, we encourage you to explore the full report. All chapters are listed out in the table of contents and at the bottom of the page. These views and other negative sentiments are widely shared among older and younger Americans, White, Black, Hispanic and Asian adults, people who are highly engaged in politics and those who are less engaged. And in most cases, the partisan differences in these attitudes are relatively modest. In an era defined by partisan polarization, the parties share little common ground politically. But they do share a deep unhappiness with the current state of politics. The impact of partisan polarization Ordinary Americans are more polarized than in the past. Partisan divisions on issues are wider than they were a few decades ago, and many Americans hold deeply negative views of those on the “other side” of politics. Yet the public also is highly critical of the impact of partisan polarization on politics. More than eight-in-ten Americans (86%) say the following is a good description of politics: “Republicans and Democrats are more focused on fighting each other than on solving problems.” Asked to describe in their own words the biggest problem with the political system, 22% of Americans volunteer partisan polarization or lack of partisan cooperation. Only critiques of politicians (31%) are mentioned more frequently. Most people (57%) also believe that conflicts between Republicans and Democrats receive too much attention these days. And 78% say there is too little focus on important issues facing the country. Persistent concerns over money in politics The public’s belief that special interests and campaign donors have too much influence on politics is not new. Since the 1970s, large majorities have said that the government “is run by a few big interests looking out for themselves,” rather than for the benefit for all the people. Yet money in politics emerges again and again as a major source of public frustration. • Most say the cost of campaigns keeps good candidates from running. An overwhelming majority (85%) holds the view that “the cost of political campaigns makes it hard for good people to run for office.” • Members of Congress are widely seen as mixing financial interests with their work. About eight-in-ten Americans (81%) say members of Congress do a very or somewhat bad job of “keeping their personal financial interests separate from their work in Congress.” • Americans feel major donors have too much influence. Large majorities say big campaign donors (80%) and lobbyists and special interests (73%) have too much influence on decisions made by members of Congress. People in members’ own districts, by contrast, are widely viewed as having too little influence (70% say this). A sizable majority (72%) – including comparable majorities in both parties – support limits on the amounts of money individuals and organizations can spend on political campaigns. And 58% believe it is possible to have laws that would effectively reduce the role of money in politics. (Explore this further in Chapter 5.) Views of the parties and possible changes to the two-party system Neither party is particularly popular with the public. Only about four-in-ten adults have a favorable view of the Democratic Party (37%), while about as many (36%) have a favorable impression of the Republican Party. An increasing share of Americans express negative opinions of both parties. Currently, 28% of the public has an unfavorable opinion of both the Republican and Democratic parties. That is little changed from a year ago, but nonetheless is the highest share expressing dislike of both parties in nearly three decades. In 1994, just 6% of Americans viewed both parties negatively. Many people are open to the idea of having more political parties: 37% say the phrase “I often wish there were more parties to choose from” describes their views extremely or very well. Nearly half of independents and those who identify with other parties (47%) – including 56% of Democratic-leaning independents – say this. (Views of the two major parties, the party system and support for additional parties are discussed in more detail in Chapter 4 and Chapter 9.) However, there is considerable skepticism that having more parties would make it easier for the country to solve its problems. About a quarter (26%) say it would make it easier to solve problems, while nearly as many (24%) say it would make it harder; a quarter say it would not have much impact. The survey includes a number of proposals to change the way politics is run in this country. Some have attracted majority public support for many years, including ending the Electoral College system, placing term limits on members of Congress, automatically registering all citizens to vote and requiring all voters to show government-issued photo identification to vote. (Opinions about proposals for changing the political system are covered in Chapter 10.) Large majorities of Americans also support: • Age limits for federal elected officials. Amid widespread concern over the advanced age of many political leaders, including President Joe Biden, former President Donald Trump and some members of Congress, 79% of the public favors putting a maximum age in place for elected officials in Washington, D.C. These views also are widely shared across partisan lines. • Age limits for the Supreme Court. Nearly as many Americans (74%) support putting age limits in place for justices of the Supreme Court. Democrats (82% favor) are more supportive of creating age limits for the justices than are Republicans (68%). Another proposal that would dramatically affect the Supreme Court – increasing the number of justices – attracts considerably less support from the public. Slightly more Americans oppose (51%) than favor (46%) increasing the number 0f justices on the court. There is a wide partisan gap in these views: Democrats (66%) are more than twice as likely as Republicans (27%) to favor expanding the court. Other important findings Recent presidential campaigns viewed as too long, not informative. Americans are not just unhappy about the current state of politics; they also take a critical view of recent presidential campaigns. Nearly eight-in-ten (78%) say recent nominees have not been good candidates, while large majorities also say the campaigns have not focused on the right issues or been informative, and 72% say they “lasted too long.” (Chapter 8) Growing public concerns over different dimensions of federal-state relations. A majority of Americans (54%) are extremely or very concerned that “the rights and protections a person has might be different depending on which state they are in,” up from 43% just a year ago. Increasing shares also express concerns about the federal government doing too much that is better left to the states, as well as state governments not being willing enough to work with the federal government. (Chapter 2) Elected officials are held in extremely low regard. When asked why local and national elected officeholders run for office, relatively small shares of Americans say they run to serve the public or address issues they care about. By contrast, majorities say all or most politicians are motivated by selfish reasons, including 63% who say all or most ran for office to make a lot of money. (Chapter 7) Majority says voting can affect the country’s direction. Nearly six-in-ten (57%) say voting by people like them can affect the country’s future direction, though just 20% say it can affect this a lot. Adults younger than 50 are less likely than older people to say voting can have at least some effect on the country’s future direction. (Chapter 5) Explore chapters of this report Republicans won the election’s influencer race. Democrats want to catch up. Author: Wells, Dylan, Date: 2025-01-09 Collections: Hot Takes US Elect 2024 Zotero Key: 4TUIFGS2 Cite Key: Wells25repubsElectionInfluencers Zotero Item | Lit Note Less than a month before the election, Donald Trump appeared on the comedy podcast “Flagrant,” where he discussed topics both informal — making his son clean up after a party at Trump Tower — and more serious: the assassination attempt against him in Pennsylvania. Seated on a gray couch, he mused about his mother, bragged about his children and discussed abortion policy for nearly an hour and a half. Noting how he was taking time away from more traditional campaigning to speak on the show, Trump declared, “It’s a whole new different way of getting the word out.” The interview was one of many lengthy sit-downs during the closing months of the campaign that Trump granted with content creators — a growing group of podcasters, TikTokers, vloggers and others who feed a $250 billion industry. Trump discussed his favorite musicians and did his signature dance with streamer Adin Ross, brought pranksters the Nelk Boys aboard Trump Force One, and delayed a swing state rally to chat with Joe Rogan for nearly three hours. “It’s entertainment, but it’s also very serious,” Trump said of “Flagrant.” “If it was just funny stuff, you wouldn’t have the success you’ve got, to be honest with you. It’s nice to intersperse it.” Following Transition to 47 Following While both Democrats and Republicans stepped up their work with creators this cycle, hoping to reach voters who don’t follow more traditional media, many analysts and politicos agree that Trump’s more aggressive efforthelped him make inroads with young voters, particularly young men. Men ages 18 to 29 backed Trump 49 to 48 percent in 2024, while in 2020 they favored Joe Biden 52 to 41 percent, according to exit polling. Some said Republicans did a better job of blending cultural and political material in their work with influencers, and that Democratic efforts at times came across as less authentic, with content that was overly produced or scripted — the opposite of the desired outcome from content intended to show the candidate’s more personal side. “Democrats are, in general, way more careful, way more deliberate about the places that we choose to go. But the reality is that in this media environment, you can reach the political people if you want, but that’s not going to be enough to swing the election,” said popular progressive YouTuber and podcast host Brian Tyler Cohen. “If you’re looking to persuade voters, you have to get out there in culture, in sports, and we’ve ceded that ground to the right. … We do have to, like, reclaim a lot of this space otherwise, otherwise we’re not going to be able to win.” The Democratic grappling over if, how and why their influencer work failed to connect coincides with demographic shifts as the Democratic Party has increasingly become viewed by some voters as the party of the upper class and college educated. Some in the party acknowledge they were caught flat-footed this cycle due to an incorrect assumption that, thanks to Democrats’ strength with young voters and historical early adaptation to digital campaigning, they dominated the online campaign. Young voters historically vote blue, but they backed Vice President Kamala Harris over Trump by just four points — a sharp drop from Biden’s 25-point lead over Trump in 2020, according to Tufts University’s nonpartisan Center for Information and Research on Civic Learning and Engagement. Around a fifth of U.S. adults — and 37 percent of adults ages 18 to 29 — say they regularly get news from influencers on social media, according to a November Pew Research Center survey. Both parties spent millions on a wide range of projects, such as message placement with hyper-local microinfluencers and paying creators to post positive content. Influencers who make a living off their personal brand also decided to weigh in in the political sphere. Harris’s campaign had momentum on TikTok, Instagram and X after the Democratic National Convention in August, but that shifted after her debate with Trump in September, according to Magda Muszynska Chafitz, senior director of research at People First, an influencer agency that works with liberal causes and worked with the Harris campaign. Even though Harris was widely regarded as the winner of the debate, the number of posts supporting Trump took off then, and Harris never regained a lead. Trump-supportive content increased again after his appearance on Rogan’s podcast, according to People First’s tracking of posts on TikTok, Instagram, X, Reddit and Facebook. The group’s research also found nearly three times the level of engagement — measuring comments and likes — under TikToks supporting Trump compared to those supporting Harris. “Even though the [Harris] campaign invited influencers to her event, the content posts felt more polished, more produced, more edited,” she said. In a recent presentation on influencer lessons from the election, she argued that “Harris’s team ultimately failed to embrace direct/authentic engagement by treating creators as ‘media outlets’ instead of partners.” The Harris campaign declined to comment on the criticism of its creator effort. Posters of Vice President Kamala Harris urging people to vote for her on Election Day in Charlotte on Nov. 4. (Logan Cyrus for The Washington Post) In contrast, Chafitz said, sensational short clips of Trump — such as video of him serving foodat a McDonalds — epitomized the type of content that people expect on social media platforms. The campaign “embraced influencers as crucial allies, offering them exclusive experiences and access that fostered genuine advocacy, enhancing their connection with his message and followers,” she said. “The content that the Trump campaign put out on TikTok felt more cultural than it did political. And I think that’s where one of their strengths was,” said Rachel Janfaza, a Gen Z political analyst and founder of the nonpartisan political newsletter the Up and Up,that is both aimed at a Gen Z audience and covers Gen Z news. She pointed to Trump’s broader effort to appear on male-centric podcasts and banter with hosts discussing sports and pop culture. In contrast, she said, Democratic content often felt more “curated” and at times failed to pass young viewers’ sniff test. Gen Z, which includes people born between 1997 and 2012,values authenticity, with studies showing the generation favors transparency and seemingly unfiltered and realistic content, which drives many top creators today. Some Democrats noted that online trends should have been read as warning signs that their message wasn’t breaking through — even if they were not explicitly political. “We see the rise of some of these trends — skinny is in, trad wives are in and some of these trends that point toward some of these more conservative values and viewpoints, and it didn’t seem like we were ringing any alarms at seeing these things happen at scale,” said Annie Wu Henry, a Democratic digital strategist who has worked extensively in the creator space.“They were cultural indicators of where many people, who maybe do not look at themselves as political, find themselves on the spectrum.” “You can’t change culture in 106 or 107 days. You can’t even change culture in six months, and especially when Donald Trump has been building his following and his brand and in this political space and this momentum for eight years, and then before that was obviously a public figure that had a brand,” she added. Conservatives in the influencer field say that their emphasis on creating less-explicitly-political, cultural-coded content came as a result of typical pop culture stalwarts not backing their politics. CJ Pearson, a Gen Z creator and conservative activist, argued that the GOP influencer ecosystem was built out of necessity because traditional celebrities were not aligned with the party. “Conservatives had to come to terms with the fact that Hollywood would never reflect our values or our ideals. We just had to deal with that. And so what we decided to do was to create our own celebrities,” he said. Pearson, a former Georgia state House candidate who sued the governor in attempt to overturn Biden’s win in 2020,recently founded Influence America to focus on incubating the next generation of GOP influencers, with an eye toward women and minorities. “There obviously are certain demographics of which conservatives need to make advancements with. … We need ambassadors to speak to those specific groups of people,” said Pearson, who is Black. “We can create a movement that just isn’t turning out Gen Z or changing the hearts and minds of Generation Z, but is actually reaching Generation Alpha, which is the next frontier.” Republicans say it was hard to get buy-in on working with content creators before the election, but after Trump’s win, things have changed. “It was pulling teeth through Election Day to get Republicans to understand that you actually — this is a far better [return on investment] in talking to the electorate than anything else you can do,” said Stephen Aaron, who worked on an effort to blend culture and political content to reach lower-propensity voters to turn out for Republicans. To do so, he worked with gun owners, hunters, veterans, fisherman, farmers, ranchers and people in the home-school community. Supporters wait as Donald Trump and his wife, Melania Trump, visit his campaign headquarters in West Palm Beach on Nov. 5. (Jabin Botsford/The Washington Post) Following Trump’s win, Aaron said the organization Vote 4 America is now meeting with freshly interested Republican members of Congress to teach them how to engage with creators and persuade them to do long, unscripted interviews with podcasters. Democrats are also launching new efforts in hopes of building up their creator ecosystem to mirror content models on the right, as they piece together areas where they may have fallen short. “There seems to be a large degree of gatekeeping among people at the people on the left, and that has some detrimental effects,” added Cohen, who has previously interviewed party leaders including Harris and Biden. “We have the better message. We just have to make sure that people hear it at this point, and there has to be more of an openness on the left to go into places that aren’t necessarily safe, that aren’t necessarily comfortable.” Cohen is part of a group of influencers that in the days after the election launched Chorus, a new collective intended to give creators more support and autonomy to advocate for Democratic causes and policies. The effort, one of many Democratic projects that have come together since Trump’s win, is intended to serve as an incubator for creators, to connect them with resources like editors, bookers and researchers. The group plans to hold daily editorial calls — reminiscent of traditional newsrooms — so creators are united in focusing on the same issues in hopes of better breaking through with voters. Time “was spent doing things people think they should do with influencers, like [taking] them somewhere, putting on something fancy for them. [But]what they really need is underwriting and financial support,” said Josh Cook, president of Good Influence, a firm that connects creators to progressive campaigns and causes and helped launch Chorus. Cook compared the effort to how NPR or PBS are underwritten by sources such as corporate sponsorships, grants, pledge drives and congressional appropriations in contrast to the freelance, gig-economy-style typically involved in creator work. “The right has done a much better job of just putting the money into the hands and entities around these effective voices and helping them scale by supporting their work,” he said. Republicans “let their best advocates online just be themselves without a ton of direction. They’ll support them for sure, but it’s much less rigid,” Cook argued. “Can you imagine asking the Nelk Boys to A/B test what they’re talking about that day when they’re making fun of Democrats? That’s not how they work.” “We’ve been playing with an arm tied behind our back on the high-minded mountain of principles, and that’s a mistake,” he said. Republican Victory and the Ambience of Information Author: Heller, Nathan, Date: 2024-11-13 Collections: Hot Takes US Elect 2024, MediaAdsPolit Zotero Key: SNNEJHLP Cite Key: Heller24ambianceInfo Zotero Item | Lit Note Dawn had not yet broken on the election results last week when Democrats began their favored ritual of falling out of love. Reasons were enumerated why Kamala Harris, the candidate who weeks earlier had been a magnet for enthusiasm, was an obvious poor choice to run for President. She was too coastal, it was suggested, too centrist, too un-primaried, too woke, too female. What were they thinking? The remorse is familiar, regardless of the outcome. When Joe Biden ran for President in 2020, many Democrats lamented that the Party hadn’t produced a stronger option—but Biden went on to receive more votes than any candidate in American history. Hillary Clinton transformed, in the Party’s view, from a historic nominee to a terrible candidate almost overnight. Barack Obama was widely acknowledged as a great candidate—even a once-in-a-generation one—who barely made it to a second term. John Kerry, a “legitimate, good candidate,” lost the popular vote; Al Gore, almost universally considered to be a terrible candidate, won it. One might conclude that the Democrats’ ability to hold the heart of the American public has amazingly little to do with the ideal dimensions of the candidate they put forth, and that their perennial trying and failing to find the perfect figure, followed by rites of self-flagellation, is a weird misappropriation of concern. The Republicans don’t lament the inadequacies of their candidates, clearly. The Republicans have thrice sent Donald Trump. If the problem this year wasn’t the person, was it policy? Our distance from the close of the polls is still measurable in days, and yet voices have settled into hot debate about which issues Harris undersold, at the cost of the election. She leaned too much on reproductive freedom, we hear, or gave fatally little attention to concerns about immigration or the Palestinian cause or the Israeli cause. The campaign missed what spoke to men, perhaps particularly Black men, or Latino men—or was it women? Also, not enough about the kitchen-table economy. News & Politics The latest from Washington and beyond, covering current events, the economy, and more, from our columnists and correspondents. To anyone who studied the Harris campaign up close, many of those accounts don’t track. The Vice-President talked about illegal immigration, and her work to curb it, all the time. Mobilizing Black men in swing states was among the campaign’s most deliberate projects. The Democrats were faulted for hazy policy long after they released a ninety-two-page party platform and an eighty-two-page economic chaser filled with figures, graphs, footnotes, and detailed plans. Harris spoke at length about taxes and the kitchen-table economy all across the country. Why didn’t the speeches register? Why did people persist in thinking that Harris was short on policy; that Trump’s programs would boost the American economy, despite a widely broadcast consensus from sixteen Nobel Prize-winning economists to the contrary; or that he would lower taxes for working people, though the Institute on Taxation and Economic Policy calculated that he would increase them? Even many of Trump’s critics think his first term marked a high point for border patrol, though more unauthorized migrants have been forced to leave under Biden. (Why was Biden’s Presidency widely dismissed as desultory, when, in fact, as my colleague Nicholas Lemann recently put it, “he has passed more new domestic programs than any Democratic President since Lyndon Johnson—maybe even since Franklin Roosevelt”?) How did so many perceptions disprovable with ten seconds of Googling become fixed in the voting public’s mind? And why, even as misapprehensions were corrected, did those beliefs prevail? Democrats, during their hair-shirt rituals, gaze into their souls and find “bad messaging.” There is talk of a poor “ground game,” an élite failure to “connect.” But the Harris campaign set records or near-records for fund-raising, volunteer enrollment, and in some districts voter registration; it is hard to imagine what a better ground game or a closer connection might have looked like in three months. And the messaging, which hewed to the middle-class experiences of Harris and her running mate, Tim Walz, neither of whom is Ivy-educated or grew up rich, was hardly misguided in a race that ostensibly came down to the economic and exclusion anxieties of working people. Yet Democrats did make a crucial messaging error, one that probably (as the line goes) lost them the election. They misjudged today’s flow of knowledge—what one might call the ambience of information. Harris’s approach this year was distinct from her failed effort to run a more identity-centered campaign in the Democratic primary of 2020. Instead, it leaned on strategies that had carried her toward her two most improbable electoral victories: her first race, for San Francisco district attorney, which she entered while polling at six per cent, against a powerful progressive incumbent and a well-known law-and-order centrist, and won by more than ten points; and her election as California’s attorney general, which at least one major California paper initially called for her opponent on Election Night, before Harris gained ground in the continuing count and, in a reputation-making vindication of her strategy, pulled ahead. Her magic in those elections had come largely through micro-targeting—a focussed, intensely local effort to engage voters on tailored terms and to mobilize small communities that traditional campaigning missed. In the early two-thousands, this was the cutting edge of ground strategy. Harris’s political peers regarded her as one of its first virtuosi. On the trail with the Vice-President, reporting a profile for Vogue, I was struck by how reflexively her mind and methods ran to the local frame. When I noted, in an interview, that one of her policy signatures seemed to be investing in community-development financial institutions (C.D.F.I.s)—which offer capital access to struggling communities—Harris lit up and elaborated a neighborhood-centered theory of market-based improvement. She touted C.D.F.I.s’ contributions to “the economy of the community.” Laying out her middle-class economic-opportunity programs, she invariably talked about a woman who had run a nursery school on her block. If Americans still arrive at a theory of the world through their communities, the boundaries of those communities have broadened and diffused. Harris’s micro-targeting home run in San Francisco came before the iPhone. Her second unlikely victory, in the race for California attorney general, roughly coincided with Facebook’s introduction of a proprietary sorting algorithm for its News Feed. In the ensuing years, there were major changes to the channels through which Americans—rich Americans, poor Americans, all Americans—received information. As early as 2000, the political scientist Robert Putnam, in his landmark study “Bowling Alone,” noted that technology, not least the Internet, had an individuating, isolating tendency that eroded the network of civic bonds—he called it social capital—that joins and holds people in groups. It is wrong to suggest that people now relate only through digital screens. (People still show up at cookouts, dinner parties, track meets, and other crossings.) But information travels differently across the population: ideas that used to come from local newspapers or TV and drift around a community now come along an unpredictable path that runs from Wichita to Vancouver, perhaps via Paris or Tbilisi. (Then they reach the cookout.) Studies confirm that people spend less and less time with their neighbors. Instead, many of us scroll through social networks, stream information into our eyes and ears, and struggle to recall where we picked up this or that data point, or how we assembled the broad conceptions that we hold. The science historian Michael Shermer, in his book “The Believing Brain,” used the term “patternicity” to describe the way that people search for patterns, many of them erroneous, on the basis of small information samplings. The patterns we perceive now rise less from information gathered in our close communities and more from what crosses our awareness along national paths. The Democrats didn’t look past national-scale audiences—Harris sat with both Fox News and Oprah. But she approached that landscape differently. The campaign, it was often noted, shied away from legacy-media interviews. It instead used a national platform to tune the affect, or vibes, of her rise: momentum, freedom, joy, the middle class, and “ BRAT” chartreuse. When she spoke to wide audiences, her language was careful and catholic; one often had the sense that she was trying to say as little as possible beyond her talking points. The meat and specificity of her campaign—the access, the detail, and the identity coalitions—were instead concentrated on coalition-group Zooms, and on local and community audiences. Harris micro-targeted to the end. Donald Trump did the inverse. He spoke off the cuff on national platforms all the time. He said things meant to resonate with specific affinity or identity subgroups, even if they struck the rest of listening America as offensive or absurd. (“In Springfield, they’re eating the dogs!”) As my colleague Antonia Hitchens reported, his campaign was boosted by a traditional get-out-the-vote ground effort late in the game—despite this apparently not being a priority for Trump—but the canvassing was less about delivering policy information than about tuning voters’ ears like satellites to the national signal. (Election fraud was a theme.) Trump’s speeches at rallies, many people noticed, had a curious background-music quality: they went on forever, aimlessly, and people would come and go at will. The actual speeches didn’t seem to matter; they existed simply to set a vibe and keep certain broad suggestions (immigration big problem! Biden Administration so corrupt!) drifting into the ether. Trump seemed to think that much of the voting public couldn’t be bothered with details—couldn’t be bothered to fact-check, or deal with fact checkers. (“Who the hell wants to hear questions?” he asked at a town hall in October before deciding to dance and sway to music for more than half an hour.) Detail, even when it’s available, doesn’t travel widely after all. Big, sloppy notions do. Planting ideas this way isn’t argument, and it’s not emotional persuasion. It’s about seeding the ambience of information, throwing facts and fake facts alike into an environment of low attention, with the confidence that, like minnows released individually into a pond, they will eventually school and spawn. Notions must add up to a unified vision but also be able to travel on their own, because that’s how information moves in a viral age. And national media is key. Trump’s command of the ambience of information wouldn’t have been possible without his own platforms, such as Truth Social, as well as allies such as Fox News’ C.E.O., Suzanne Scott, who in 2020 excoriated her team after they fact-checked Trump, and Elon Musk, who, hoping for executive-branch power over his own sector, largely funded more than a hundred and seventy-five million dollars’ worth of pro-Trump outreach, was read into early voting data, and tweeted lies, conspiracy theories, and mistrust of media on his network, X, which boosts his posts. The communications researcher Pablo Boczkowski has noted that people increasingly take in news by incidental encounter—they are “rubbed by the news”—rather than by seeking it out. Trump has maximized his influence over networks that people rub against, and has filled them with information that, true or not, seems all of a coherent piece. This is the opposite of micro-targeting. The goal is for voters to meet ideas coming and going so often that those notions seem like common sense. The pollster and political-marketing-language consultant Frank Luntz assembled a focus group of men who had previously voted for a Democratic nominee but were voting for Trump this year. Many of their rationales were based on untrue information settled deep in the ambience of information. “Nothing against people from California, but the policies in California are so bad I wouldn’t be surprised if the state goes bankrupt,” a participant in Indiana said. (California has the largest economy in the U.S.) “Kamala from California is too radical . . . she’s too far left.” (Biden’s policies tended to be to the left of Harris’s, when they didn’t align.) These are not convictions that someone acquires from a specific source, neighborhood, or community. The American Trends Panel survey methodology Author: Saks, Brian Kennedy, Alec Tyson and Emily, Date: 2023-02-15T19:57:46+00:00 Collections: PollMethods Zotero Key: IJM2G9HY Cite Key: Saks23pollMethodAmTrnds Zotero Item | Lit Note The American Trends Panel survey methodology Overview The American Trends Panel (ATP), created by Pew Research Center, is a nationally representative panel of randomly selected U.S. adults. Panelists participate via self-administered web surveys. Panelists who do not have internet access at home are provided with a tablet and wireless internet connection. Interviews are conducted in both English and Spanish. The panel is being managed by Ipsos. Data in this report is drawn from the panel wave conducted from Dec. 12 to Dec. 18, 2022. A total of 11,004 panelists responded out of 12,448 who were sampled, for a response rate of 88%. The cumulative response rate accounting for nonresponse to the recruitment surveys and attrition is 4%. The break-off rate among panelists who logged on to the survey and completed at least one item is 2%. The margin of sampling error for the full sample of 11,004 respondents is plus or minus 1.4 percentage points. Panel recruitment The ATP was created in 2014, with the first cohort of panelists invited to join the panel at the end of a large, national, landline and cellphone random-digit-dial survey that was conducted in both English and Spanish. Two additional recruitments were conducted using the same method in 2015 and 2017, respectively. Across these three surveys, a total of 19,718 adults were invited to join the ATP, of whom 9,942 (50%) agreed to participate. In August 2018, the ATP switched from telephone to address-based recruitment. Invitations were sent to a stratified, random sample of households selected from the U.S. Postal Service’s Delivery Sequence File. Sampled households receive mailings asking a randomly selected adult to complete a survey online. A question at the end of the survey asks if the respondent is willing to join the ATP. In 2020 and 2021 another stage was added to the recruitment. Households that did not respond to the online survey were sent a paper version of the questionnaire, $5 and a postage-paid return envelope. A subset of the adults who returned the paper version of the survey were invited to join the ATP. This subset of adults received a follow-up mailing with a $10 pre-incentive and invitation to join the ATP. Across the five address-based recruitments, a total of 23,176 adults were invited to join the ATP, of whom 20,341 agreed to join the panel and completed an initial profile survey. In each household, one adult was selected and asked to go online to complete a survey, at the end of which they were invited to join the panel. Of the 30,283 individuals who have ever joined the ATP, 12,448 remained active panelists and continued to receive survey invitations at the time this survey was conducted. The U.S. Postal Service’s Delivery Sequence File has been estimated to cover as much as 98% of the population, although some studies suggest that the coverage could be in the low 90% range. The American Trends Panel never uses breakout routers or chains that direct respondents to additional surveys. Sample design The overall target population for this survey was non-institutionalized persons ages 18 and older, living in the U.S., including Alaska and Hawaii. All active panel members were invited to participate in this wave. Questionnaire development and testing The questionnaire was developed by Pew Research Center in consultation with Ipsos. The web program was rigorously tested on both PC and mobile devices by the Ipsos project management team and Pew Research Center researchers. The Ipsos project management team also populated test data that was analyzed in SPSS to ensure the logic and randomizations were working as intended before launching the survey. Incentives All respondents were offered a post-paid incentive for their participation. Respondents could choose to receive the post-paid incentive in the form of a check or a gift code to Amazon.com or could choose to decline the incentive. Incentive amounts ranged from $5 to $20 depending on whether the respondent belongs to a part of the population that is harder or easier to reach. Differential incentive amounts were designed to increase panel survey participation among groups that traditionally have low survey response propensities. Data collection protocol The data collection field period for this survey was Dec. 12 to Dec. 18, 2022. This survey included a postcard experiment in which postcard notifications were mailed to half of ATP non-tablet household panelists with a known residential address on Dec. 12, 2022. The other half of ATP panelists did not receive any postcard mailings. The survey-level response rate was 89% among those mailed the postcard and 88% among those who were not mailed the postcard. Invitations were sent out in two separate launches: Soft Launch and Full Launch. Sixty panelists were included in the soft launch, which began with an initial invitation sent on Dec. 12, 2022. The ATP panelists chosen for the initial soft launch were known responders who had completed previous ATP surveys within one day of receiving their invitation. All remaining English- and Spanish-speaking panelists were included in the full launch and were sent an invitation on Dec. 13, 2022. All panelists with an email address received an email invitation and up to two email reminders if they did not respond to the survey. All ATP panelists that consented to SMS messages received an SMS invitation and up to two SMS reminders. Data quality checks To ensure high-quality data, the Center’s researchers performed data quality checks to identify any respondents showing clear patterns of satisficing. This includes checking for very high rates of leaving questions blank, as well as always selecting the first or last answer presented. As a result of this checking, eight ATP respondents were removed from the survey dataset prior to weighting and analysis. Weighting The ATP data is weighted in a multistep process that accounts for multiple stages of sampling and nonresponse that occur at different points in the survey process. First, each panelist begins with a base weight that reflects their probability of selection for their initial recruitment survey. These weights are then rescaled and adjusted to account for changes in the design of ATP recruitment surveys from year to year. Finally, the weights are calibrated to align with the population benchmarks in the accompanying table to correct for nonresponse to recruitment surveys and panel attrition. If only a subsample of panelists was invited to participate in the wave, this weight is adjusted to account for any differential probabilities of selection. Among the panelists who completed the survey, this weight is then calibrated again to align with the population benchmarks identified in the accompanying table and trimmed at the 1st and 99th percentiles to reduce the loss in precision stemming from variance in the weights. Sampling errors and tests of statistical significance take into account the effect of weighting. The following table shows the unweighted sample sizes and the error attributable to sampling that would be expected at the 95% level of confidence for different groups in the survey. Sample sizes and sampling errors for other subgroups are available upon request. In addition to sampling error, one should bear in mind that question wording and practical difficulties in conducting surveys can introduce error or bias into the findings of opinion polls. Dispositions and response rates Adjusting income and defining income tiers To create upper-, middle- and lower-income tiers, respondents’ 2021 family incomes were adjusted for differences in purchasing power by geographic region and household size. “Middle-income” adults live in families with annual incomes that are two-thirds to double the median family income in the panel (after incomes have been adjusted for the local cost of living and household size). The middle-income range for the American Trends Panel is about $43,800 to $131,500 annually for an average family of three. Lower-income families have incomes less than roughly $43,800, and upper-income families have incomes greater than roughly $131,500 (all figures expressed in 2021 dollars). Based on these adjustments, 28% of respondents are lower income, 46% are middle income and 18% fall into the upper-income tier. An additional 6% either didn’t offer a response to the income question or the household size question. For more information about how the income tiers were determined, please see here. A note about the Asian adult sample This survey includes a total sample size of 371 Asian adults. The sample primarily includes English-speaking Asian adults and, therefore, may not be representative of the overall Asian adult population. Despite this limitation, it is important to report the views of Asian adults on the topics in this study. As always, Asian adults’ responses are incorporated into the general population figures throughout this report. Measurement properties of the awareness of artificial intelligence in daily life scale Pew Research Center’s survey on awareness of artificial intelligence in daily life asked respondents to identify AI applications in email, online shopping, customer service and other areas they might encounter. The scale included six different questions to measure how aware people are of AI applications. These questions represent some common ways people could use AI in their lives but are not designed to be an exhaustive list of all the ways people could encounter AI. The following criteria are used to evaluate how well the six items scale as an index of awareness of AI in daily life: 1) the degree to which responses are internally consistent 2) the degree to which the questions reflect a single underlying latent dimension and 3) the degree to which the scale discriminates between people with high and low awareness of AI in daily life. The internal reliability of the scale as measured by Cronbach’s alpha is 0.83. Cronbach’s alpha does not increase if any of the items is dropped. An exploratory factor analysis finds that the first common factor explains 91% of the shared variance in the items. The factor loadings show that each of the six items is at least moderately correlated with the first common factor. This suggests that the set of items is the result of a single underlying dimension. Note that all the awareness of AI in daily life items are coded as binary variables (either correct or incorrect). Both Cronbach’s alpha and factor analysis are based on a Pearson’s correlation matrix. Pearson’s correlations with binary variables are restricted to a limited range, underestimating the association between two variables. We do not anticipate the use of a Pearson’s correlation matrix will affect the unidimensional factor solution for the scale. We conducted item response modeling for the scale to evaluate how well it discriminates between people at different levels of awareness. The analysis fits a two-parameter logistic model, allowing discrimination and difficulty to vary across items. Discrimination shows the ability of the question to distinguish between those with higher and lower awareness of AI in daily life. Difficulty shows how easy or hard each question is for the average respondent. All the items have acceptable discrimination estimates. The two items with the highest discrimination were knowing that a music playlist recommendation uses AI and product recommendations based on previous purchases when shopping online uses AI. The difficulty parameter estimates are negative for all six items. The scale did not include a more difficult item with a positive difficult value. Because of this, the items did not have much variation in difficulty. The test information function shows the amount of information the scale provides about people with different levels of awareness of artificial intelligence in daily life. The test information function approximates a normal curve and is centered below zero at about -0.35. This indicates that the scale provides the most information about those with slightly below-average awareness. The scale provides comparatively less information about those with high awareness, especially very high awareness. © Pew Research Center 2023 Protect Your Brand And Drive Superior Results This Election Season Author: IAS, , Date: 2024 Collections: MediaAdsPolit Zotero Key: W6MXZL2N Cite Key: IAS24brandProtectResultsElection Zotero Item | Lit Note 2024 SEASON POLITICAL GUIDE Protect your brand and drive superior results this election season PROTECT YOUR BRAND AND DRIVE SUPERIOR RESULTS THIS ELECTION SEASON Ensure your ads only run on Quality Impressions™ aligned with the correct context for your brand values and objectives during the election season. Challenges like brand suitability, attention, and ad fraud can be amplified in an election year, and regularly assessing your campaign strategy is key for driving superior results. MEDIA QUALITY IN ELECTION SEASON HOW IAS HELPS Political ad spend is projected to reach new heights by the end of the 2024 election cycle, eclipsing $10 billion according to NBC News. With more competition for advertisers to deliver their messages effectively on quality media, marketers need sophisticated technology to cut through the noise and reach their ideal consumers. Brand-specific protection with curated contextual avoidance down to the sentiment & emotion level Contextual Targeting to reach your audiences with curated segments for political related content Ad Fraud Protection across programmatic, social, & CTV buys Global Disinformation Index (GDI) Partnership for third-party, AI-powered protection against misinformation CONSUMER ENGAGEMENT & AD PERCEPTION DURING THE POLITICAL SEASON 74% 72% 65% of consumers report that they of Internet users sought of Americans say they saw at are unlikely to purchase a election news online in 2020 least “some” news about the product or service from a election that seemed brand that advertises near completely made up online misinformation TAKE THESE 6 STEPS TO ENSURE BRAND PROTECTION AND SUPPORT QUALITY NEWS OUTLETS DURING THE ELECTION SEASON 2. BOOST PROTECTION ACROSS SOCIAL 3. LAYER ON CONTEXTUAL PRECISION PRE- AND POST-BID FOR AVOIDANCE & TARGETING 4. PRIORITIZE TRUSTED NEWS SITES 5. AVOID AD FRAUD SPIKES 6. BE STRATEGIC WITH CTV CAMPAIGNS RECAP: LEVERAGE IAS TO DRIVE SUPERIOR RESULTS DURING ELECTION SEASON 1. MEASUREMENT (POST-BID) AND OPTIMIZATION (PRE-BID) IAS’s standard brand safety categories cover content related to topics such as violence, hate speech, and offensive language. IAS provides brands with the ability to set brand safety risk settings (very low, low, moderate, high, and very high) across IAS's eight core content categories for each unique campaign. Review your brand safety settings to reflect your most up-to-date risk tolerances and campaign goals. See relevant brand safety categories for the political season and descriptions below: ■ Hate Speech: Content that represents or discusses hate or hate speech. Hate speech consists of hostility, aggression towards, denigration, or unequal treatment of groups or individuals on the basis of race, religion, gender, nationality, ethnic origin, sexual orientation, or other involuntary characteristics. ■ Offensive Language & Controversial Content: Content that includes offensive terms, including profanity, insults, swear words, vulgar terms, obscenities or religious blasphemy. Content that deliberately portrays inaccurate content as news. ■ Violence: Content that represents or discusses violence, including violent crimes. Some violent topics covered are murder, rape, domestic violence, psychological abuse, robberies, executions, war settings, acts of terrorism, death or injury, disaster scenes, animal abuse, and more. The political season can bring great uncertainty around brand risk. Set your risk tolerance accordingly for each content category across campaigns for granular protection that scales. 1. VERY LOW 2. LOW 3. MODERATE 4. HIGH 5. VERY HIGH TAKE ACTION: ■ Reviewing and updating brand safety settings during an election season is vital to protect your brand integrity. ■ Leverage IAS blocking tags to prevent ads from delivering on unsuitable sites. IAS will fail the ad and blocking tags will ensure the creative does not render. ■ Since risk tolerance can vary for different brands and campaigns, advertisers should review their risk tolerance across content categories. ■ Automatically avoid misinformation through IAS’s partnership with the GDI. Simply set your risk threshold to moderate, or stricter, for our standard Offensive Language & Controversial Content category. 2. IAS’s holistic solution ensures no matter how you’re investing in media, we’ve got you covered for digital. Measure on social platforms with IAS’s next-generation product suite, Total Media Quality (TMQ), to confidently scale on social. TMQ’s sophisticated technology is powered by machine-learning, with select platforms utilizing multimedia technology which combines frame-level analysis of image, audio and text. IAS technology can identify images of political figures, analyze hate speech and debated sensitive social issues. Leverage this technology to identify text on screen such as a news logo or a sign in a protest, ensuring suitable content adjacency during election season. IAS VIEWABILITY & IVT AVAILABLE NOW: IAS VIEWABILITY & IVT BOOST PROTECTION ACROSS SOCIAL: ■ Optimize campaigns based on what’s driving superior results with viewability, time-in-view, and more. ■ Validate media quality. IAS Signal provides advertisers with trusted data and transparency to ensure your campaigns are engaging and validating users. ■ Identify trends to see how your campaigns evolve over time with different creatives to confidently build your social strategy. ■ Measure performance globally across platforms to understand performance. BRAND SAFETY & SUITABILITY PROTECTION ON SOCIAL: Measure & Optimize for Brand Safety & Suitability aligned to the GARM framework on your top social partners. Review your risk thresholds for categories relevant to political topics like arms & ammunition or death, injury & military conflicts, hate speech & acts of aggression, and more. BRAND SAFETY & SUITABILITY AVAILABLE NOW: 2024 ROADMAP COMING SOON: achieved with IAS insights 3. FOR AVOIDANCE & TARGETING IAS offers 600+ contextual segments for a more nuanced approach to brand suitability and contextual targeting during the political season. Instead of relying on “politician name” in a keyword exclusion list, IAS segments have you covered. For example, “Arms & Ammunition ” would keep you protected against hot button issues like crime and guns. By classifying down to the sentiment and emotion level, you can avoid unsuitable content without overblocking and hurting scale. +93% Over target ROI goal IAS DYNAMIC CONTEXTUAL TARGETING DRIVES SUPERIOR RESULTS IAS helps you achieve optimal media quality results and hit campaign KPIs in a brand safe and socially responsible way. Activate relevant cookieless Context Control Avoidance and Targeting Segments for the election season today within your DSP of choice. For added protection, receive automated alerts when hot-button issues are most pervasive in the news to update your suitability strategy as needed. 600+ SEGMENTS AVAILABLE FOR: • Audience Proxy: Target endemic content to reach desired audiences such as small business owners and tech enthusiasts • Seasonal: Target content for seasonal events and election campaigns like the upcoming presidential election • Topical: Target or avoid content relating to big world issues including sensitive social issues, natural disasters, and infectious diseases and outbreaks • Vertical: Target or avoid content relating to overarching categories like entertainment, financial, and travel • Brand Specific: Customize suitability preferences unique to your brand to avoid content specifically mentioning your brand alongside negative content 4. Quality news and trusted journalism is more important than ever. IAS’s Context Control avoidance segments allow for the most granularity within the GARM framework, so you can set different risk thresholds for news content versus gaming or entertainment. While an overall brand safety strategy across the web is important, advertisers may feel inclined to support a news publication regardless of potential keyword fails or context. IAS enables advertisers to always include specific news site homepages and section pages with an IAS inclusion list. IAS supports traditional domain exclusion lists as a tool for advertisers seeking an additional layer of control over their campaigns. When applying domain inclusion/exclusion lists, it is strongly recommended that advertisers do not just set and forget them, but rather review and update them often to ensure that they are not missing out on the opportunity to make a quality impression. DRIVE CONVERSION RATES AND IMPROVE COST EFFICIENCY CONVERSION RATE COST-PER-CONVERSION % % 278 63 Drives results and improve cost efficiency! Align your ads with brand safe political content and avoid low-quality sites WHY IT MATTERS TAKE ACTION Leverage IAS MFA Measurement and Optimization to track and avoid MFA sites and monitor performance for ad clutter. Reach out to an IAS representative to learn more. 5. Ad fraud is any deliberate activity that prevents the proper delivery of ads to the intended audience, in the intended place. According to IAS's Media Quality Report, the global non-optimized ad fraud rate was 8.6% in H2 2020, costing marketers millions of dollars during the last election season. Leverage IAS’s sophisticated, MRC accredited, ad fraud prevention tools to avoid wasting ad dollars on invalid traffic. Device Spoofing (SIVT) is when non-CTV devices represent themselves as CTV devices Device Fraud (SIVT) is traffic from a device that is considered suspicious based on behavioral analysis Source Laundering Model (SIVT) are impressions delivered to a spoofed app - one that is different from what is being claimed ACROSS DEVICES DESKTOP Browser MOBILE/TABLET Browser Apps HOW YOU’RE PROTECTED Invalid User Agents (GIVT) are detected when user agents fail the IAB list check Self-Identified Bots (GIVT) are bots that match the IAB spiders and bots list Activity Based Filtration (GIVT) is automated or simulated human activity, which might not be caught through spider and bot filtration CTV Smart TV Game Console Streaming Device TAKE ACTION: ■ Activate IAS pre-bid Ad Fraud (IVT) segments in your DSP of choice, excluding High or High and Moderate Risk suspicious activity ■ Leverage post-bid blocking and measurement insights to eliminate impression waste across campaigns ■ Ensure Measurement and Optimization are activated across all available platforms (i.e. social) and environments (i.e. CTV) for maximum value 6. During the election season, it's more important than ever to keep your brand protected from ad fraud spikes and brand risk on your most premium buys. CTV impressions may not always appear where buyers expect, with as much as 17% of impressions serving with the TV off. Leverage IAS to protect and grow your brand with ultimate transparency. 90% of advertisers consider brand safety a priority in their CTV planning strategy during the political season. CTV CHALLENGES: IAS CTV SOLUTIONS: FRAUD RISK FRAUD PROTECTION BRAND SAFETY RISK BRAND SAFETY PROTECTION LACK OF TRANSPARENCY UNMATCHED TRANSPARENCY VIEWABILITY LIMITATIONS VIEWABILITY MEASUREMENT TAKE ACTION CONTROL WHERE YOUR MESSAGE APPEARS: ACTIVATE FRAUD PROTECTION & GEO-BLOCKING ■ Monitor delivery quality by creating app inclusion and/or exclusion lists. There are a lot of players within the CTV ecosystem. IAS can block ads from serving outside of intended geographic regions or designated market areas. Obtain access to the most global, granular media quality data of the entire CTV ecosystem. PROTECT YOUR BRAND EQUITY: ACTIVATE BRAND SAFETY PROTECTION ■ Identify infractions that could have adverse consequences for your brand. Ensure you can block ads from being served to invalid traffic or brand unsafe apps. IAS provides you with unmatched reporting granularity, way beyond the app level. DATA ENRICHMENT WITH INTELLIGENT CTV TECHNOLOGIES & NON-STOP INNOVATION: RECEIVE UNMATCHED CONTENT-LEVEL TRANSPARENCY ■ IAS’s acquisition of Publica and Context have accelerated AI-based multimedia classification on CTV. IAS can analyze more than a billion ad impressions per day to identify ad theft and injected ads. LEVERAGE IAS TO DRIVE SUPERIOR RESULTS DURING ELECTION SEASON 1. Review your brand safety settings for measurement (post-bid) and optimization (pre-bid) 2. Boost protection across social 3. Layer on contextual precision pre- and post-bid for avoidance & targeting 4. Prioritize trusted news sites 5. Avoid ad fraud spikes 6. Be strategic with CTV campaigns Taking the steps outlined above will help advertisers ensure brand safety, avoid waste, and drive superior results across major political events. WHY IAS? ACTIONABLE DATA, GLOBAL SUPPORT, AND FOCUS ON SUPERIOR RESULTS COMPREHENSIVE DATA ACROSS PLATFORMS MACHINE LEARNING DATA ENRICHING TECHNOLOGY REAL-TIME PROCESSING ACCREDITED BY INDUSTRY BODIES LIKE THE MRC GLOBAL FOOTPRINT, LOCALIZED SERVICE HISTORY OF INNOVATION FOCUSED ON YOUR ESG GOALS UNMATCHED CTV TRANSPARENCY Have questions? Reach out to your IAS representative to learn more and strategize for the political season! THANK YOU Testing survey questions ahead of time can help sharpen a poll’s focus Author: Silver, Laura, Date: 2020-10-14 Collections: PollMethods, FocusGroups Zotero Key: 5DBL7P5T Cite Key: Silver20pollQuestTestAhead Zotero Item | Lit Note Survey researchers often try to determine how respondents might react to different kinds of questions. This can help ensure that proposed survey questions are as clear as possible and measuring what researchers set out to measure. Pew Research Center frequently tests survey questions ahead of time through qualitative research methods such as focus groups, cognitive interviews, pretesting (often using an online, nonprobability sample), or a combination of these approaches. In this post, I’ll walk through our preparations for a comparative, cross-national survey we’re planning to field in the United States, United Kingdom, France and Germany later this fall. In this case, we used focus groups and a short survey experiment to test some questions for the upcoming poll. I’ll discuss two of those survey questions here. Measuring pride — or shame — in a nation’s history One of the topics in the focus groups — conducted in the U.S. and UK in fall 2019 — was a discussion on what makes people proud to be American or British, as well as what makes them embarrassed. One interesting observation is that participants pointed to each country’s history as either a source of pride or shame, depending on the political orientation of the focus group. For example, in British focus groups consisting of those who voted to leave the European Union (“Leavers”), participants emphasized their pride in the former British empire and the role that the UK played in spreading democracy and the English language worldwide. But in British focus groups composed of those who voted to remain in the EU (“Remainers”), participants expounded on how the country’s history of colonialism and imperialism made them feel embarrassed, citing examples such as the “mess” in Hong Kong and how the UK bore responsibility for “destabilizing regions” and “making one tribe fight the other.” These respondents also emphasized that Britain’s involvement in the slave trade was problematic, an issue that has since boiled over as protesters toppled a statue of slave trader Robert Colston in Bristol, England. In the U.S., too, the focus groups were separated by issues of history, slavery and racism. Those composed of Republicans and Republican-leaning independents emphasized their pride in American history and the ways in which the country has been a beacon of hope and a “land of opportunity,” while those comprised of Democrats and Democratic leaners highlighted past injustices and unequal opportunity for Black people. Democratic focus groups also singled out the perceived hypocrisy of America as a “melting pot” despite its legacy of slavery and mistreatment of Native Americans, among other injustices. Based on these divides, we were interested in drafting a survey question that could measure the relative tendency to be proud or ashamed of the history of one’s country. But we wanted to resolve two issues first. One was whether we should we provide a middle answer option so people could report being both proud andashamed in their country’s history. The other was whether the question might inadvertently elicit answers related to each country’s handling of the current COVID-19 pandemic, which was not our intention. (We know from a previous survey that people in the U.S. and UK are divided about their government’s handling of COVID-19.) To study the first issue, we used an online nonprobability sample to randomly assign people to receive one of two survey questions. We used a nonprobability sample because our pretesting typically focuses on understanding how different question wordings perform — something that can be explored through experiments on nonprobability samples — rather than understanding what percentage of the U.S. population has a particular view. For this experiment, half of the respondents in the online nonprobability sample were given the following binary, forced choice question: “Which comes closer to your view even if neither is exactly right? There are some times when I am not proud of the U.S./UKORI am always proud of the U.S./UK, no matter what.” The other half were given the following three-part question instead: “Which comes closer to your view, even if neither is exactly right? I’m almost always proud of this countryORI’m almost always ashamed of this countryORI’m often proud of this country, but I’m often ashamed of it as well.” Since people were randomly assigned to one question or the other, we know that the only differences between the two groups should have been which question they received and not other factors that might affect results. Results indicated that half or more in each country that got the second version of the question chose the middle option of both pride and shame. In the U.S., similar shares chose “always proud” in both question formats, whereas in the UK, fewer people chose “always proud” when the middle option was present. Thus, while the two versions of the question produced somewhat similar results for the “always proud” group, the three-part question provided more information, allowing us to more clearly parse those who were “always ashamed” from those who fell into the middle category of being often proud and often ashamed. Regardless of the formulation of the question, Republicans in the U.S. were more likely than Democrats to say they were “always proud,” as were Leavers in the UK when compared to Remainers. To gain some insight into the second issue flagged — whether respondents might be thinking of their government’s response to the coronavirus outbreak — we decided to explore the cognitive processes behind individual answers by asking respondents to explain what they were thinking in an open-ended question following whichever of the two survey questions they answered. Examining these open-ended responses, we saw that while COVID-19 did come up, respondents’ feelings of pride and shame often had a longer time horizon. For example, people mentioned the history of the two countries; general freedoms and principles that they attributed to the countries; characteristics of the publics; and more. While some mentioned politics, more emphasis was often placed on the current administration in each country than on the specific handling of the COVID-19 pandemic. In fact, responses felt qualitatively similar to what we heard in the focus groups, suggesting that we were capturing our concept of interest, rather than a short-term sentiment about the pandemic. For example, in focus groups, we heard things like “All you have to do is travel outside the U.S. to realize how good we have it here.” In open-ended responses online, one American participant similarly voiced, “I’ve been to 45 countries — none better.” In addition to the valuable information we gained from the three-part question in terms of analytically separating out the “always ashamed” group from those who feel more mixed, we also learned from the open-ended answers that some respondents who received the two-part question mentioned that they often felt both proud and ashamed of their country’s history and had difficulty responding. As a result, we decided to field the three-part question instead. Measuring nostalgia Another question we asked in the focus groups was about when each country was at its best. Participants named different dates or eras, but four key themes emerged. First, both Britons and Americans highlighted “simpler” times –before the internet era, Facebook and generally when people had a slower pace of life and didn’t have to be constantly responsive to email or texts. Second, people emphasized times when economic opportunity seemed plentiful and they felt like the ordinary people could make a good living and improve their quality of life compared to their parents. Third, people touted times when the countries had come together and been less polarized, whether because of a natural disaster (e.g. Hurricane Harvey in the U.S.), a terrorist attack (e.g. 9/11 in the U.S.), cultural events (e.g. royal weddings in the UK) or the Olympics, among others. A fourth theme also emerged: the sense that each country has never been at its best. People in both countries — and especially those in groups composed of Remainers in the UK or Democrats in the U.S. — highlighted how the “best” times can depend on one’s vantage point. This was underscored by a U.S. respondent who noted that even after World War II, when Americans could celebrate beating the Nazis, civil rights remained limited for some. This sentiment was echoed by many who said that while both countries had made progress, historically there were always people disenfranchised based on race, sexuality or gender. To get at this concept in our survey, we drafted the following question: “Thinking about the U.S./UK, which of the following best reflects your view? We were better in the pastORThe best years are still ahead.” We also drafted a second, similar version, to explore whether slightly different wording would change the results: “Thinking about the U.S./UK, which of the following best reflects your view? Our country’s best years are already behind usOROur country’s best years are still to come.” In the UK, people answered the question similarly, regardless of the wording. Around half generally felt the country was better in the past, while half said it will be better in the future. In the U.S., however, question wording appears to have mattered. While 45% said in the first question option that “we were better in the past,” a considerably smaller share (34%) chose the corollary in the second question option: “Our country’s best years are already behind us.” In addition, we noticed that the ideological differences we observed in the focus groups — with Leavers in the UK and Republicans in the U.S. more likely to be nostalgic for the past — were either slight, not present, or reversed. In the UK, we saw that on both versions of the question, Remainers were actually more likely to say their country was better in the past than Leavers, though those divides were more stark in the first version of the question. On this question, too, we were concerned that some respondents might think of the COVID-19 pandemic when evaluating whether their country’s best years were in the past or in the future, so we asked the same open-ended question that we asked about national pride or shame. As was the case in the earlier question, while some people did bring up the outbreak, we didn’t find any major cause for concern on this front. But the open-ended answers did help us understand why the ideological differences we observed in the focus groups were more muted in the test survey. Most notably, people often chose different survey responses while giving the same explanation for their choice — e.g. political polarization, the election of Boris Johnson or Donald Trump or COVID-19. For example, in the U.S., two respondents who were both dissatisfied with the Trump presidency answered the closed-ended question differently. One who said “we were better in the past” explained his reasoning as, “Everything has gone bad since we had Trump as president.” The other said “the best years are still ahead,” but described a similar rationale: “Very many things will change for the positive once we’re unshackled from Trump.” We found many examples like this in the open-ended answers, suggesting that while the question may have captured people’s general predilections, it was not adequately capturing the sense of nostalgia that we were trying to measure. Given this, we decided not to run this question on the survey. The importance of testing While organizing focus groups and online pretesting adds quite a bit of time — and expense — to our survey research process, we find it incredibly valuable. For this project, focus groups were particularly valuable for highlighting themes and topics that were related to our key areas of interest but that we had never asked survey questions about before. And the pretesting showed us which of those topics we were successfully asking about in a closed-ended format and which may have slightly missed the mark, allowing us to recalibrate, adjust and create the best possible survey instrument for our larger, nationally representative survey. How wokeness could cost the Democrats the election Author: Kotkin, Joel, Date: 2024-11-01 11/1/24 Collections: IdentityPolitics Zotero Key: H58GZED9 Cite Key: Kotkin24howWokenessCould Zotero Item | Lit Note This time around, Hillary Clinton is not lamenting Republican ‘deplorables’. She has chosen instead, along with Kamala Harris, to label Donald Trump and his supporters as out-and-out fascists. Different words but the same meaning: anyone who backs the GOP candidate in next week’s US presidential election is an enabler for modern-day blackshirts or stormtroopers. But for many Americans, the real ‘deplorables’ are to be found among Harris’s backers, such as the tech oligarchs who dominate the economy, the financiers of Wall Street or the moguls of mainstream media. Think of the likes of Bill Gates, who just forked in $50million to the Harris campaign. Even more detested by most of the public are the ‘progressive’ activist class that has embraced Harris and shaped her past record. This group, as the author Musa al-Gharbi writes in his new book, We Have Never Been Woke, constitutes ‘a new elite’. Trained as ‘symbolic analysts’, these often flailing graduates and professionals now represent a revolutionary class pushing the Democrats towards the ideological loony bin. As long as Harris and the Democrats remain in thrall to the activists’ progressive ideology, they will be tarred with their widely unpopular views on everything from climate change to transgenderism, race quotas and immigration. Their numbers are not too impressive. Overall, the woke make up roughly eight per cent of the electorate. But they tend to be politically motivated and dominant within the party apparatus, newsrooms and schools. They have long dominated local politics in cities like Los Angeles, Oakland, Houston and Boston. Progressive influence has been far more evident in the Biden administration than in preceding Democratic regimes, especially those of Bill Clinton and even of Barack Obama. The current administration has welcomed ideologues with strident progressive views on the environment, gender, race and the Middle East. Biden and Harris have focussed on these woke constituencies over more traditional Democratic policies that embrace broad-based economic growth and opportunity. Harris, more than Biden, epitomises the current version of the ‘left’ that is rooted in an increasingly gentrified base, rather than working-class or middle-class people. This has been financially rewarding for the Democrats. The ultra-rich and their progressive foundations have consistently outraised and outspent the political ‘right’ by a margin of nearly two-to-one. For Harris, long supported by these same people, this has helped build up an unprecedented billion-dollar campaign war chest, as much as three times the size of Trump’s. Leftists like Bernie Sanders admit that Harris’s apparent shift to the centre during the election is a mere pragmatic feint. But as her campaign has lost momentum, his political action group, Our Revolution, now warns even the hint of moderation could limit turnout among progressive voters. Yet there’s a problem here. Outside of the biggest cities and college towns, progressives are thin on the ground. Their views are far from popular with the general public (abortion rights is the main exception). The progressive, mainstream media – such as the New York Times and USA Today – insist that wokeness poses no big problem. But, as former New York Times opinion editor James Bennet put it, the traditional media now serve as the place where ‘America’s progressive elite talks to itself about an America that does not really exist’. Thankfully, politicians still have to deal with the real America, leading some swing-state Democrats to run away from the progressives’ ideological handcuffs. In tough races in Pennsylvania and Ohio, incumbent Democratic senators are even stressing their close ties to President Trump to win re-election. In contrast, California’s woke governor, Gavin Newsom, is now so unpopular that some of his own legislators avoid being seen with him. Clearly smelling the coffee, he has kept his state’s nuclear and natural-gas power plants operating, despite fierce opposition from greens. He has even suggested amending the state’s landmark environmental law, which has proved devastating for the cost of living. In California, as elsewhere, rising street crime and drug overdoses is leading to remarkable shifts even in the bluest of cities. Progressive district attorneys have already lost in San Francisco, as well as in Virginia, Pennsylvania and New York. In next week’s election, Los Angeles DA, George Gascon, is behind his Republican opponent by an astounding 30 points. Proposition 36, which seeks to overturn lenient sentencing policies once embraced by Harris, is also likely to pass easily in California. This year two members of the leftist ‘squad’ – Cori Bush and Jamaal Bowman – were decisively beaten in primaries by more traditional Democrats. The defeat of wokedom can also be seen in the private sector, where many companies are now abandoning the once omni-present Diversity, Equity and Inclusion (DEI) machinery. Similarly, large companies – most notably, major investment firm BlackRock – are also shying away from the Environment, Sustainability and Governance (ESG) agenda. This reflects the unprofitable nature of most green ventures, many of which entering what one executive described as ‘ the valley of death’. No surprise many investors are beating a rapid retreat. These firms may not survive under a Trump administration or even a Republican Congress, without government subsidies. Harris’s handlers know opposing the woke ideology would win votes, but her past record makes this a difficult proposition. Take immigration, an issue on which Trump often expends his worst demagoguery. Even as she seeks to pretend she was never Biden’s ‘border czar’, Harris could pay a heavy price for an administration that allowed the border to become so porous. Opposition to the open border goes well beyond MAGA racists. The vast majority of Americans, according to a CBS News survey, favour deporting undocumented immigrants. Most also favour proof of citizenship status being a requirement to vote, a position widely rejected by progressives. Rather than embrace unrestricted immigration because most are similarly brown-skinned, American Latinos also worry that the current influx of largely poor, barely educated immigrants could take their jobs or lower their wages, forcing them to compete with a ‘serf class’ with limited rights and lower expectations. This is no simple nativist talking point: the Congressional Budget Office warns that the recent massive ‘surge in immigration’, much of it undocumented, could impact the salaries of low-income workers, many of whom are Latino. In addition, roughly half of all Latinos, notes Pew , associate the current migration wave with increased crime in their communities. Yet it’s a long way from the barrio to the ivory towers and swanky urban precincts inhabited by the progressive elite. A recent Rasmussen study of high-earning, grad-degree urban professionals found their views on a host of issues, such as climate policies limiting meat and gas consumption, and controls over free speech, differ widely from most Americans. These highly educated deplorables lament that Americans have too much freedom, not too little. So who are the real fascists then? These divergent views reflect a clear class bias. The ‘1%’ surveyed by Rasmussen said they were prospering under Biden. Tech workers and investors, according to a recent Information survey, favour Harris by roughly three to one. A rollicking stock market and high property prices are manna from heaven for those who already own a lot or soon will. CEOs, on Wall Street and elsewhere, are enjoying record pay. Rather than invest in automation or skills training, many business leaders look instead to cut wage costs by employing illegal immigrants. As they have looked to please the progressive deplorables, the Democrats have proved ill-positioned to address deteriorating conditions among the working and middle classes who once made up the Democratic base. Inflation under Biden has hit the less affluent particularly hard, keeping their inflation-adjusted incomes below when Biden was elected. It may be many years before they can catch back up. A majority say they are worse off than four years ago. In 2020, before the impact of Covid, only a third did. Overall, one in four Americans fears losing his or her job in the next year, while roughly half of all US adults surveyed now think the vaunted American Dream, as epitomised by home ownership, has become unreachable. The lines for food banks, particularly in the electorally crucial Midwest, have been lengthening in recent years. Most voters expect Harris to raise taxes, making things worse. Harris has tried to placate the masses with a spew of handouts and incentives, notably for housing and small business, with some tied to race. But her record in California demonstrates that climate concerns meant more to her than allowing for affordable housing. As California’s DA she wielded the state’s environmental laws to block new housing developments, helping to make California the most unaffordable place to live in the country. In the Golden State, one in five pays half their income in rent, more than anywhere else in the US. It is hard to underestimate how much the spectre of global warming dominates much of the thinking of the progressive activist class. Yet barely three per cent of Americans, according to Gallup, peg this as their main concern. Backing draconian and expensive measures to ‘save the planet’ may be de rigeur among elite zealots but it threatens the livelihoods of many Americans, particularly those involved in the carbon economy – like truck drivers, factory workers, oil riggers and farmers. Relatively few working-class voters, according to a survey conducted by YouGov last year, are keen on the current green drive to wipe out fossil fuels in the near future. The cultural causes of the left-wing deplorables are, if anything, even more unpopular than their economic policies. Progressive Democratic positions on slavery reparations and racial quotas are backed by no more than 30 per cent of the population. The drive for transgender rights, such as allowing children to transition without parental approval or having biological males participate in women’s sport, are opposed by roughly two-thirds of voters. The Democrats’ cluelessness on gender was demonstrated by their unintentionally comic attempts to promote their own version of masculinity by using pro-Harris actors who claim to eat ‘carburetors for breakfast’. The cultural disconnect is so vast that the woke seem genuinely unaware that many immigrants are often more culturally conservative than native-born whites. No wonder black, Latino, and young men tell pollsters they will vote in larger numbers for Trump this year than they did in 2020. Several Arab and Muslim leaders have also endorsed Trump, who is doing surprisingly well for a candidate so often accused of Islamophobia. Similarly, the progressive embrace of critical race theory, also overwhelmingly opposed by most Americans, has driven Democrats to denounce merit, alienating generally middle-class groups like Asians and Jews. Even if Harris manages to win, thanks largely to the awfulness of Trump, Democrats will need somehow to break the lock hold of progressive wokedom. A Harris administration that doubles down on woke would make it all too easy for Republicans to pose as the party that defends the middle and working classes, including aspirational minorities. Of course, the Democrats do not have a monopoly on deplorables. After all, Trump has been willing to share a platform with clearly obnoxious, conspiratorial figures like Tucker Carlson. Yet unlike the Harris camp, Trump has shown himself to have essentially no ideology and is unlikely to be burdened embracing unpopular right-wing stances, as we can see in his careful treatment of the abortion issue. In contrast, Harris will be hard-pressed to dismiss the deplorables on her side, since she has little personal appeal, and her strongest and most committed supporters hail from that detested corner of American political life. If she wins, Harris will be beset, as was the doddering Joe Biden, with people whose sympathies on virtually every issue spell electoral disaster. She also may find compromising with those supposedly ‘fascist’ Republicans in Congress very unpopular with her core supporters. Educated, progressive women, her strongest political base, tend to be the least interested in ‘com [ing] together and mak[ing] the sacrifices’ needed to unite the country, according to a recent American Compass survey. For Democrats, it may be far better in the long run if Harris loses, allowing future candidates to reject the activist set and bring the party back to more saleable positions on crime, economic growth and upward mobility. Until then, woke Democrats are working to the advantage not of liberal ideas, but to Trump and his likely successor, JD Vance. Joel Kotkin is a spiked columnist, a presidential Fellow in Urban Studies at Chapman University in Orange, California, and a senior research fellow at the University of Texas’ Civitas Institute. Inside Morning Consult’s Survey Research Technology Author: Pink,, Bill, Date: October 2024 Collections: PollMethods Zotero Key: 5SZQCFYB Cite Key: Pink24morningConsultRsrchTech Zotero Item | Lit Note VW, MORNING CONSULT’ Inside Morning Consult’s Survey Research Technology Bill Pink, Head of Solutions Architects Alexander Podkul, Senior Director of Research Science James Martherus, Senior Research Scientist October 2024 © 2023 Morning Consult All rights reserved What Sets Morning Consult Apart |0|1|Decis|ion int|elligen|ce is| |---|---|---|---|---|---| |||Our m|indset|is lase|r focus| ||2|Techn This a|ology llows u|-enabl s to cr|ed sam eate hi| ||3|End-t In a w|o-end orld of|data c messy|reation data, t| Technology-enabled sampling supports full product suite across markets and at scale CONTEXT & TRENDS ALWAYS-ON DATA BESPOKE INSIGHTS ALWAYS-ON DATA CONTEXT & TRENDS Pro Intel Custom Reports & Analysis Consumer Insights Platform Flexible Solutions |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |||||||||||||||| |||||C|ONT|EX|T &|TRE|NDS|||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |||||||||||||||| ||BE|SPO|KE I|NSI|GHT|S||||||||| |||||||||||||||| |Custom Flexible Solutions End-to-end research capabilities connecting high-quality data with actionable insights aT lh do es r,e aw ni dth eh mig ah le er lM ivM inS s inc o tr he es fe idn wd et so ibne rwuhriatel , f , g M t areas||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| brand, economic and political data. Exploreby coveragearea ote ot tase ti, Report:What to Know AboutGenZ's ‘Engagement with Soclel Mec, Entertainment nology |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| |||||||||||. .|* .|* ||||||||||||| How we collect and source interviews online 0101 Encouraging truthful and unique survey responses . - COLLECTING RESPONDENT INTERVIEWS How Tech-Enabled Sampling Works at Morning Consult |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8| |---|---|---|---|---|---|---|---| ||||||||| ||||||||| ||||||||| ||||||||| ||||||||| ||||||||| ||||||||| ||||||||| |ompl|ete o||||||| Respondents complete our surveys online via our survey hosting platform. Dynamic bidding algorithms reduce costs and field time while eliminating human error and collecting sample as efficiently as possible. ENSURING DATA QUALITY Prioritizing Quality at Every Stage of Data Collection • LJ • LJ • LJ LJ LJ LJ • LJ LJ LJ |Col1|Col2|Col3|Prom|otin|g q|ualit|y in|onlin|e da|ta|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||colle|ctio|n req|uire|s a|holis|tic v|iew|| |L|J|t|o en|sur|e tha|t re|spon|den|ts ar|e…|| |* *|||||||||||| |J|||||||||||| |L|J||||||||||| |J |||||||||||| ||||||||||||| |J L|J||||||||||| |L J L|J J||||||||||| PRE-SURVEY DURING POST-SURVEY In-depth panel vetting Automated pattern Secure survey link detection monitoring Real-time, in-survey authentication attention checks to promote authentic Fraud prevention tools responses while reducing response burden Routine benchmarking by a Digital fingerprinting to team of survey research ensure authentic, unique experts respondents Collecting survey data online is easy, collecting high quality survey data requires exceptional attention to detail. TODAY S SAMPLE ECOSYSTEM panel F PanelGg Pane)1 Panej ! PangJ Ug, yP By leveraging this technology, we can exclude duplicated respondents before the interview even begins. Today’s Online Survey Panels Researchers Panels PROMOTING REPRESENTATIVE RESPONSES Tech-Enabled Sampling Solutions Produce More Accurate Results How “off the shelf” sample from popular online sample sources compare to interviews collected by Morning Consult MC decrease % in error 40 Competitor 1 Competitor 2 Competitor 3 0 2 4 6 8 Average Absolute Error (pp) |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| How we promote quality survey responses 0202 Our approach to encouraging attentiveness UNDERSTANDING CONTEMPORARY RESEARCH CHALLENGES “Bogus Respondents” Can Jeopardize Research Accuracy |Bogus Respondents” Can Jeopardize Research Accuracy DERSTANDING CONTEMPORARY RESEARCH CHALLENGES|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |ogus Respondents” Can Jeopardize Research Accuracy 01 AF sr s em s sB io n g t hs eR R is ko s td o On n l i n e P o l l s o g u e sp n e t s A p p r o v a lr a t i n g s c a n b e in f lu e n c e d s e v e r a lp e r c e n t a g e p o i n t s Pew Research Center found that many online panels of by bogus takers opt-inpolls have high numbers of “bogus respondents” BY COURTNEY KENNEDY, NICK RATLEY, ARNOLD LAU, ANDREW MERCER, SCOTT KEETER, IOSHUA FERND AND DORENE ASARE-MARFO —-— Seriously? 02 United States,Dec 2nd-5th 2023%, agreeing By age group “TheHolocaust “TheHolocausthas is a m y th ” b e e n e x a g g e ra te d ” 0 10 20 30 0 10 20 30 These types of respondents can lead to strange 18-29 18-29 headlines, like “One in five young Americans thinks 30-44 30-44 the Holocaust is a myth” 45-64 65+|||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||||||||||||||||| ||These types of respondents can lead to strange headlines, like “One in five young Americans thinks the Holocaust is a myth”||||||||||||||| ||||||||||||||||| ||||||||||||||||| Assessing the Risks to Online Polls From Bogus Respondents Approval ratings can be influenced severalpercentagepoints by bogus takers ofopt-inpolls BY COURTNEY KENNEDY, NICK RATLEY, ARNOLD LAU, ANDREW MERCER, SCOTTKEETER, IOSHUA FERND AND DORENE ASARE-MARFO USING DESIGN TO ENCOURAGE ATTENTIVENESS Attention Checks are One Tool to Identify Dishonest or Inattentive Respondents |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14| |---|---|---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||||| ||||||||||||||| ||||||||||||||| ||||||||||||||| ||.||||||||||||| ||||||||||||||| ||||(v] Fox|News.co|m||||||||| ||||o G J [|e N le g o|s w||||||||| ||—+||[J Ya|hoo! Ne|ws||||||||| ||||(V/] NB|C.com|||||||||| |Identify Dishonest or Inattentive|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15| |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| |||||||||||||||| |||||||||||||||| |How many times have you done the following in the 12 months? 0 times 1 time 2 or more times Flown on an airplane O O O Stayed in a hotel ©) O O Sone to a movie theater Oo O O F lo w n to th e m o o n (v /] O O||||||||||||||| |||||||||||||||| ||||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |Example: Grid with an unlikely event||||||||||||||| |||||||||||||||| |Which of the following words is most closely associated with the word book? © do g sun O O cup Q @ re n||||||||||||||| ||wing w|ords i|s most|close|ly ass|ociate|d with|the||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |||||||||||||||| |Example: Word Association Check||||||||||||||| When a big news story breaks people often go online to get up- to-the-minute details on what is going on. We want to know which websites people trust to get this information. We also want to know if people are paying attention to the question. Please ignore the question and select FoxNews.com and NBC.com as your two answers. When there is a big news story, which is the one news website you would visit first? (Please only choose one) [J New York times website . (7 Huffington Post [J CNN.com (v] FoxNews.com [J GoogleNews [J Yahoo!News • (V/] NBC.com Example: Long text with directed choice Which of the following words is most closely associated with the word book? © dog O sun O cup @ren Example: Grid with an unlikely event ATTENTION CHECK REVIEW Reviewing Attention Check Effectiveness is Necessary Since Not All are Equal Fail Pass’ Grid Directed Choice ee I Te: Word Association = - e » Grid Trap Questions Ce . i» Multiple Choice Math ® ® Straightline a » Open End Math a : i Standalone Directed Choice FC : J 0.0 0.1 0.2 0.3 Average Treatment Effect AVERAGE QUALITY REMOVING BOGUS RESPONDENTS Using Attention Checks to Remove “Bogus Respondents” & Improve Accuracy |Col1|ura|cy|Col4|Col5|Col6| |---|---|---|---|---|---| ||||||| ||||||| |By targeting bogus respondents using effective attention checks, estimates can improve by up to four percentage points in accu|||||| ||racy||||| ||||||| ||s||||| ||||||| ||||||| ||||||| ||||||| All Respondents Good Respondents Panel 1 Panel 2 Panel 3 Panel 4 0 2 4 6 8 AVERAGE ABSOLUTE ERROR (PP) For example: Across all four panels, our average absolute error in the share of households with Social Security income went from 5.9% to 1.2% REMOVING BOGUS RESPONDENTS … Removing Inattentive Cases Also Improves Message Testing Results |Col1|Col2|Col3|Col4|Col5|Col6| |---|---|---|---|---|---| ||||||| ||||||| |Poor quality respondents can also dampen results from survey experiments like message tests and conjoints.|||||| ||||||| ||||||| ||||||| ||||||| ||||||| ||||||| ||||||| Good Respondents All Respondents 0.17 0.200 0.225 0.250 0.275 5 AVERAGE TREATMENT EFFECT Good Respondents are unique respondents who passed three attention checks and were not flagged by our fraud detection technology |Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| 03 Evidence it works Why our approach matters BETTER DATA TRANSLATES INTO BETTER INSIGHTS Evidence it works: Validation Against Behaviors and Business Outcomes MORNING CONSULT V5. GLOBAL ICS Morming Consult US. IGS vs. University ofMichigan _ Moming Consult Lefty — University of Michigan (Right ® Correlationcostliclent:9.97 wo aoa pa ht \\vd — ha %0 100 a0 ww - oo R De jd hs 0 70 50 Mar 18 Sep 18 Mar 19 Sep Mar 20 Sep 20 Mar 2) Sep! Mar 22 Sep22 Mer '23 Sep 23 Mar 24 Sou: MorneCans Econame Inligence, Lnveraty of anigan • i 3 {came 83; 4 • . - PA - - » Comma oo Cuma ConsumerPackageGoods. R-04218 Decreaseinrand Simutaneossnese Equity,Increasein » Bandy InBrandExutyand Marketshare : MaritShara h | Hfl i : i ‘Simultaneous Decreasein Brandbquty andMaket InereaseinBrand Store Equity,Decreaseln Market share ChangeIn MentalMarketShare Consumer Sentiment 01 Campaign Effectiveness 04 HIGH FREQUENCY DATA ALLOWS FOR MORE ACTIONABILITY Client tapped internal store count data and MC Mental Market Share to guide their regional investments |AS se v e r a l S o u t yh e a s t D M s h o w a n e rq u i t o p p o r t u n it y f o improvement given ’ a l r e a d y s t r o n g physical presence ( a n d a r e a l i z a t i o n t h a t need to focus more o n n e e d s k e y c o m p e t it o r w a s|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14| |---|---|---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||||| ||||||||||||||| ||||||||||||||| ||||||||||||||| ||||||||||||||| ||||||||||||||| ||||||||||||||| |||55%||||||d e liv that|e r in regi|g i n on).|||| ||||||||||||||| |y|regio|n | Bu|bble|Size:|Total|Hom|eswit|hinea|chD|MA|||| ||||||||||||||| 18% 16% 14% 12% 10% 8% 6% 4% 2% 0% 5% 15% 25% 35% 45% 55% Physical Availability (Share of store count) Mental Availability: Mental Market Share of client brandwithineach| Physical Availability: Share of store of count by region| Bubble Size: Total Homes within eachDMA DATA AT SPEED AND SCALE UNLOCK A CLEAR STORY Quick turn research identified geopolitical risk, as well as the protection that its reputation provides. Ld - [J - - LJ [J 2 ) p [J |L|d |[|J ||L|J [|J 2|) p|[|J|Col12| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| ||||||||||||| |||||||||||ts|| ||||||||||||| PROBLEM Country Affinity Snapshot of <<Brand>> **Country Affinity Snapshot of <<Brand>>** and the United States* and the United States* A multi-national America-based retailer was dealing with boycotts abroad during a period of acute geopolitical upheaval. The situation was shifting daily Middle East Social Impact Reputation Percentage who agree the statement describes “Brand” and the brand needed quick turn global research to Attribute Bahrain Egypt Jordan Kuwait Lebanon Qatar pawal Turkey UAE understand evolving brand sentiment in order to prbeietind Create jobs 81% o 68% 9 68% 0 79% o 73% 9 76% 9 66% 0 59% 9 77%o optimize and prioritize crisis messaging. Middle East Geopolitical Risk Percentage who “agree” with these statements Saudi Attribute Bahrain Egypt Jordan Kuwait Lebanon Qatar Arabia Turkey UAE SOLUTION Proforeign 84% 86% 80% 89% 88% 90% 81% 72% 87% Within two and a half days, we had collected over which the retailer used to understand where they were most vulnerable so they could focus and tailor their 2 p Research Blog FEATURED Introducing Our Methods How Morning Consult Weights U.S. Voter Survey Data Introducing Morning Consult's research: what we do and how we do it Weighting data is one of the many tools that survey researchers leverage to improve the accuracy of sample estimates LATEST RESEARCH BLOG RESEARCH BLOG RESEARCH BLOG How Morning Consult Weights U.S. Voter Replicating Experimental Findings with Using Crowd-Sourced Community Survey Data Online Panels Identities to Revisit Urban, Suburban, and September 27, 2024 July 22, 2024 Rural Definitions April 11,2024 Vv, MORNING CONSULT’ Vv, MORNING CONSULT’ © 2023 M i C lt All i ht d The ‘Mainstream Media’ Has Already Lost Author: Lewis, Helen, Date: 2024-12-05 Collections: Hot Takes US Elect 2024 Zotero Key: FJUCL6FT Cite Key: Lewis24MainstreamMediaLost Zotero Item | Lit Note This article was featured in the One Story to Read Today newsletter. Sign up for it here. This October, in the closing days of the presidential election, the podcaster Joe Rogan said something extraordinary. He had just hosted Donald Trump for a three-hour conversation in his studio in Austin, Texas, and wanted to make clear that he had discussed a similar arrangement with Kamala Harris’s campaign. “They offered a date for Tuesday, but I would have had to travel to her and they only wanted to do an hour,” he posted on X. “I strongly feel the best way to do it is in the studio in Austin.” And so Rogan declined to interview the vice president. Explore the January 2025 Issue Check out more from this issue and find your next story to read. View More What a diva, some people said. If you’re offered an interview with a presidential candidate, get off your ass and get on a plane! But Rogan could dictate his own terms. He is not competing in the snake pit of D.C. journalism, where sitting opposite a major candidate delivers an instant status bump. He is the most popular podcaster alive, with a dedicated audience of right-leaning men who enjoy mixed martial arts, stand-up comedy, and wild speculation about aliens (space, not illegal); they are not political obsessives. Rogan knew that Harris needed him more than he needed her. Nothing symbolizes the changed media landscape of this past election more than Rogan’s casual brush-off. Within a week, his interview with Trump racked up more than 40 million views on YouTube alone, and millions more on other platforms. No single event, apart from the Harris-Trump debate, had a bigger audience this election cycle. By comparison, Harris’s contentious interview with Bret Baier on Fox News, the most popular of the cable networks, drew 8 million viewers to the live broadcast, and another 6.5 million on YouTube. Those figures demonstrate the absurdity of talking about the “mainstream media” as many still do, especially those who disparage it. According to a 2021 Pew Research Center survey, Americans with a wide range of political views generally agree about which outlets fall within this definition: newspapers such as The New York Times and The Wall Street Journal and television networks such as CNN. Everyone else who’s disseminating information at scale is treated like a couple of hipsters running a craft brewery who are valiantly competing with Budweiser. From the October 2024 issue: Helen Lewis on how Joe Rogan remade Austin That’s simply not true. Rogan is the “mainstream media” now. Elon Musk, too. In the 2024 campaign, both presidential candidates largely skipped newspaper and television sit-downs—the tougher, more focused “accountability” interviews—in favor of talking directly with online personalities. (J. D. Vance, to his credit, made a point of taking reporters’ questions at his events and sat down with CNN and the Times, among others.) The result was that both Trump and Harris got away with reciting slogans rather than outlining policies. Trump has not outlined how his promised mass deportations might work in practice, nor did we ever find out if Harris still held firm to her previous stances, such as the abolition of the death penalty and the decriminalization of sex work. The vacuum was filled with vibes. The concept of the mainstream media arose in the 20th century, when reaching a mass audience required infrastructure—a printing press, or a broadcast frequency, or a physical cable into people’s houses—and institutions. That reality made the media easy to vilify. “The press became ‘the media’ because the word had a manipulative, Madison Avenue, all-encompassing connotation, and the press hated it,” Richard Nixon’s speechwriter William Safire wrote in his 1975 memoir. Somehow, the idea that the mainstream media is made up of major corporations has persisted, even though the internet, smartphones, and social media have made it possible for anyone to reach an audience of millions. Two of the most important information sources of this election cycle have a job that didn’t exist even a decade ago: Acyn Torabi and Aaron Rupar, who watch hours of political rallies and TV appearances in order to clip them for social media. These “clippers” can drive days of discussion, particularly when the context of a remark is disputed—such as when Vance’s 2021 remarks characterizing Democrats as “childless cat ladies” went viral. Today, the divide between the “mainstream” and the outsiders is not about reach. Sixty-three percent of American adults get at least some of their news from television, 42 percent from radio, and 26 percent from print publications, according to a 2024 Pew report. But 54 percent get at least some of their news from social media—meaning that, alongside established outlets, they’re relying on sources such as Infowars videos, Facebook memes, and posts on X. The divide is not about influence, either. During Trump’s victory speech in Florida, he invited the UFC boss Dana White to say a few words. White thanked the streamer Adin Ross, the podcaster Theo Von, the YouTubers known as the Nelk Boys, and the former NFL players Will Compton and Taylor Lewan, as well as Rogan. During the campaign, all of these men had hosted Trump for softball interviews, often with the encouragement of Trump’s 18-year-old son, Barron; Ross even gave Trump a gold Rolex and a customized Tesla Cybertruck during their livestream. (You don’t get treatment like that from the Wall Street Journal editorial board.) From the May 2024 issue: Is Theo Von the next Joe Rogan? Trump’s showmanship, aggression, and ability to confabulate suit this new environment. His inconsistency is not a problem—these interviews are designed to be entertaining and personal, not to nail down his current position on abortion or interrogate his income-tax policies. Trump has been especially enthusiastic in his embrace of this new media class, but the Democrats also understand its power: In 2023, Jill Biden addressed a White House holiday party for hundreds of influencers. “You’re here because you all represent the changing way people receive news and information,” she reportedly said. At the Democratic National Convention, more than 200 “content creators” were credentialed along with traditional journalists.Being outside the mainstream is, today, seen as more authentic, more in tune with Real America. Finally, the media divide is not about resources, either. Although some of the legacy outlets are still large, well-funded companies, so are many of the upstarts. Vance, Peter Thiel, and Vivek Ramaswamy have all invested in the video platform Rumble, which went public in 2022 with a reported valuation of $2.1 billion. When The Daily Wire, a right-wing online news organization, tried to hire the internet personality Steven Crowder, he was offered $50 million over four years. He rejected this, calling deals like these “slave contracts.” As for Rogan, he has apparently chosen to forsake fact-checkers and lawyers in favor of some guy named Jamie who looks up stuff on Google, but he doesn’t have to do that. His last deal with Spotify was reportedly worth as much as $250 million. He could hire a whole newsroom if he wanted to. But Rogan has intuited, correctly, that many Americans no longer trust institutions. They prefer to receive their news from trusted individuals. The main beneficiary of our outdated ideas about the “mainstream media” is the political right. Not so long ago, conservatives resented their exclusion from the MSM, because they thought it painted them as extreme: Sarah Palin complained about the “lamestream media,” while the late Rush Limbaugh preferred to call it the “state-controlled media” or the “drive-by media.” But that’s changed. Being outside the mainstream is, today, seen as more authentic, more in tune with Real America. Trump’s constant criticisms of the “fake-news media” have been enthusiastically embraced by his downballot copycats. Complaints about alleged liberal media bias have been amplified by commentators who are themselves overtly partisan: Tucker Carlson, Russell Brand, Dan Bongino, Megyn Kelly, Charlie Kirk, Alex Jones. The underlying premise is that all media skew toward one side or another, but at least these people are honest about it. That allows them to speak alongside Trump at rallies (Kelly), embrace bizarre conspiracy theories (Jones), talk about their encounters with demons (Carlson), and continue to work despite multiple allegations of sexual assault (Brand, who has denied the claims)—all things that would be out-of-bounds for actual journalists. And let’s be clear, some influencers are very cozy indeed with the subjects they cover. You may not have heard of the Instagrammer and Substacker Jessica Reed Kraus, who was formerly a lifestyle influencer, but she has more than 400,000 subscribers on Substack, where she boasts about her access to Robert F. Kennedy Jr. and Trump. In January, she joined Kennedy on his catamaran in Hawaii, sipping mimosas and eating pineapple; she attended Trump’s Super Bowl party at Mar-a-Lago. Reed Kraus is open about focusing on personalities, not policy. “Average Americans don’t have the time or patience to sift through what separates one candidate’s health care plan from another,” she told Semafor. “But they relate and respond to intimate aspects that speak to one’s character.” Often, these very same influencers are the loudest voices complaining about the failures of “the media.” On the eve of the election, Rogan hosted Musk, that other great titan of the new media, to make the case for Trump—whom Rogan then endorsed. “The legacy media, the mainstream media, is not balanced at all,” said Musk, who personally donated more than $100 million to Trump’s reelection efforts. “They’re just a mouthpiece for the Democratic Party.” Never mind that, for example, CNN’s Andrew Kaczynski broke the single most damaging story to the Harris campaign—that she had indeed, in Trump’s phrase, supported “transgender operations on illegal aliens that are in prison.” (This became a staple of Republican attack ads.) Nor did it matter to Musk that, amid his complaints about the standards of the mainstream media, he has repeatedly promoted fake stories: about Nancy Pelosi’s husband, about gangs attacking polling stations during the recent Venezuelan election, and even about a dead squirrel whose euthanasia the right saw as evidence of government overreach. When he is proved to be wrong—often by the same legacy media that he decries—he tends to delete his posts without a correction or an apology. What happens next? To me, the picture looks bleak: more conspiracy theories, more noise, more loudmouths complaining about other people’s bias. It’s hard to see how journalistic institutions get rebuilt when so many of their business models have collapsed. The migration of ad dollars to Google and Meta means that—with few exceptions—20th-century newsrooms are not coming back. We cannot reverse the drift from institutions to individuals. Nor can the new partisan outlets be forced to adopt 20th-century norms. The Fairness Doctrine—the policy, repealed under Ronald Reagan, that required broadcasters to reflect contrasting views—is gone for good. We have to let go of the notion that “mainstream media” is a category reserved only for journalists guided by a professional code of ethics, a mission of public service, and an aspiration toward objectivity or at least fairness. Many independent reporters do good and important work—I’m thinking of the YouTuber Coffeezilla’s work on crypto scams, for example, and Jason Garcia’s investigations into Floridian politics on his Substack, Seeking Rents—but they are surrounded by a clamorous sea of partisans who operate under new and different rules. Flaunt your bias, get cozy with your subjects, and don’t harsh their mellow by asking uncomfortable questions. “You are the media now,” Musk told X users as the election results came in. It was the truest statement he had made in months. To the folks building their own platforms, to the influencers hopping on catamarans with politicians, to the streamers handing out Teslas to their guests—well done on your triumph. Welcome to the mainstream media. Now hold yourselves to the same standards you demand from others. This article appears in the January 2025 print edition with the headline “Joe Rogan Is the Mainstream Media Now.” For some Latinos, ‘prosperity gospel’ led them to Trump Author: Boorstein, Michelle, Date: 2024-12-24 Collections: Hot Takes US Elect 2024 Zotero Key: XZK8MSDR Cite Key: Boorstein24LatinosProsperityGosp Zotero Item | Lit Note ALLENTOWN, Pa. — The Lehigh Valley Barbershop was bustling with the next generation of American strivers. The mood among the young men, mostly first- or second-generation migrants from the Caribbean, was hopeful. Their candidate, Donald Trump, had just won the presidential election. Sitting in high-end silver chairs, the young men talked about the businesses they had built, or would build. That would be more possible, they hoped, with the return of Trump, someone to whom they could relate — a businessman who has made mistakes, they said, but still keeps striving. “Kamalasaid, 'Trump is for the rich, I fight for the poor.’ But I don’t want to be low-class — I hope that’s not a bad way to say it. But I don’t want to be there,” said Christian Pion, 31, referring to Vice President Kamala Harris. He became a U.S. citizen last year, a decade after coming to the United States from the Dominican Republic,and cast his first presidential ballot for Trump. “God doesn’t want you to be poor.” Next to him, his best friend, Willy J. Castillo, 39, who owns the shop and others, worked the register as he talked about Trump’s drive to succeed, overcome and survive. Castillo, who also voted for Trump, identifies with that: “The Bible says ‘God helps those who help themselves,’ right?” The mix of hope, drive for success and belief in a God who rewards faith, sometimes with financial accomplishments,has become dominant across the United States andLatin America, experts on Latino religion say. The belief system is sometimes called “seed faith,” “health and wealth gospel,” or “prosperity gospel.” In the past half-century,driven by larger-than-life pastors, it has overtaken othermore traditionaltheologies centered on God’s priority being poor and disenfranchised people, some experts said. This belief system, they said, helps explain what exit polls showed was a significant shift among Latino Christian voters to Trump, who they see as an uber-successful, strong and God-focused striver. “If you take Trump and all his characteristics, it’s almost exactly as any prosperity gospel preacher,” said Tony Tian-Ren Lin, an Asian-Latino pastor in New York who wrote a book on Latino Americans and the prosperity gospel. “The big personality, talking a big game, saying things like ‘no one can do it’ but him … If for years you’ve been listening to someone like that, you’re not surprised when a political leader says those things.” Homes on Seventh Street, formerly known as Millionaire’s Row, in Allentown, Pennsylvania, on May 23. (Michelle Gustafson for The Washington Post) Nationally, network exit polls showed that between 2020 and 2024, Trump gained 14 points in support among Latinos, although a bare majority favored Harris, the Democratic nominee. In that same period, he gained 25 points among Latino Catholics and 18 points among Latino evangelical Protestants. The shift is evident here in Lehigh County, in the eastern part of pivotal Pennsylvania. It is the county with the highest proportion of Latino voters — 29 percent, according to the U.S. Census. The Democratic Party’s margin in presidential contestsshrank in Lehigh by 4.9 percentage points,from 7.6 percentage points in 2020 to 2.7 percentage points in 2024. But in majority-Latino Allentown, the county’s biggest city, the move toward Trump was even more pronounced. The 10 city precincts with the highest-proportion of Latino voters shifted to Trump by an average of 20 percentage points since Trump faced eventual winner Joe Biden in 2020, according to a Washington Post analysis of precinct results from Lehigh County and demographic data from L2, an election data provider. The prosperity gospel is rooted in American Pentecostalism and evangelical Protestantism, but experts say it’s become huge across faith in general, and especially among unaffiliated,often online spiritual influencers. Trump grew up in the church of the Rev. Norman Vincent Peale, whose book “The Power of Positive Thinking,” was a huge bestseller and is considered a classic of the prosperity gospel. A Pew Research Center survey in 2014 found wide majorities of Protestants and Catholics in almost all of Latin America agreed that “God will grant wealth and good health to believers who have enough faith.” In the Dominican Republic — the ancestral or birth home formany in Allentown — 76 percent of Protestants agreed and 79 percent of Catholics did. The firm PRRI asked a similar question in March and found 44 percent of U.S. Latinos overall agreed, higher than any other group except African Americans. What that means politically is that wealthy candidates like Trump are seen by some as both faithful and worthy of emulation. Themovement started in the United States with healers and televangelists such as Oral Roberts and Benny Hinn, who told followers that giving them money would lead to divine blessings, conjuringa transactional God. By extension, personal wealth was seen as a goal for the faithful. It focused on the power of the self, and the idea that God wouldreward positivity, hard work and confidence. The acceptance of the prosperity gospel has been boosted by the fact that many adherents view institutional religion as corrupt, for varied reasons. Christian broadcasters and evangelical missionaries decades ago took the ideas overseas, where, University of Pennsylvania religion scholar Anthea Butler said,the prosperity gospel became “supersized,” especially in Latin America and Africa, and then returned to the United States with new waves of immigrants. Experts say its ideas now are sowidespread in spiritual and secular lifethat it has become the gospel of the American Dream. And few have had more faith in the American Dream than religious immigrants. Maria Perez, 53,who emigrated from the Dominican Republic 20 years ago, was pushing her granddaughter in a stroller to church in downtown Allentown. She said shevoted for Trump because she believes God picked him for economic and political success — a belief underscored by his surviving two apparent assassination attempts,she said. Trump’s lifestyle shows he’s uber-rich, she said, dismissing his multiple business bankruptcies as low moments in a booming career. Share this articleNo subscription required to readShare Perez goes to church once or twice a month and smiled as she talked about the pastors and spiritual figures she listens to online every day. “They give me hope that God wants us to do well here. And I know Trump wants us to do well, too.” Pion said he built a successful trucking business in Trump’s first term that made $500,000 in its first year. But during Biden’s tenure, Pion said, the market and gas prices changed, to the point where he is having to sell his two trucks and lay off three employees. When Pion became a U.S. citizen last year, Castillo got him a gift: an AI video that shows a computer-generated Trump offering congratulations. “Now we can say ‘I am an American and I need my money and I want it now,’” Trump appears to say in the video. “As your presidential candidate for 2024, I’ll make sure your truck company thrives.” Trump imagery inside the GOP county office in Northampton County, Pennsylvania, on Oct. 17. (Joe Lamberti for The Washington Post) Latino Christian leaders say the prosperity gospel is one of multiple factors that led voters to Trump, including therising cost of living, abortion, a wariness of women’s and LGBTQ rights, misinformation and his appeals to a group he has called “my beautiful Christians.” Bishop William Surita, a semiretired pastor who helps oversee a network of Hispanic evangelical churches in the Allentown area, said the prosperity gospelis part of a nuanced set ofemotions that pulled people to Trump, rather than something conscious. “They think because you got a businessman, he knows about the economy,” said Surita, who declined to say which candidate he voted for. “Anyone who follows Trump knows he hasn’t been a good businessman.” Another factor is the slight shift of Latinos — both in Latin America and the United States — from Catholicism to Evangelicalism. And evangelical Christianity in the United States is overwhelmingly tied to the Republican Party and its candidates. Nilsa Alvarez, national Hispanic director for the conservative Christian group Faith & Freedom Coalition, helped coordinate swing-state voters and said a factor in the state was a story from a pastor who spoke this fall at an event with more than 175 Latinos. The pastor, she said, shared a story about a transgender boy whose teachers, without the parents’ consent, “took her to get a sex change operation and she died.” When The Post contacted the pastor, Edelmiro Santana, he said that someone whose name he didn’t know had come up to him at an event and told him that story and that he was simply passing it on without proof.Trump often told a similar unsubstantiated anecdote about schools operating on children to change their gender. But Alvarez dismissed the prosperity gospel as a factor in Trump’s election, saying Latinos were motivated by wanting change on the economy. Mark Lopez, director of race and ethnicity research at the Pew Research Center, said it’s possible the Latino belief in the prosperity gospel has made what he characterizes as the Democratic Party’s messaging less attractive. “Is it that Latinos identify with Trump as a successful person, or is it that Latinos have stepped away from the Democratic Party because it doesn’t talk about economic success and focuses more on poverty, on helping those who need help?” he asked. Butler, the Penn religion scholar,said the prosperity gospel’s rise helps explain why many Latino Trump supporters weren’t turned off by his promise to deport millions of immigrants. The prosperity gospel, she said, isn’t only about getting rich. A Trump supporter waits in line outside of PPG Paints Arena before a campaign rally on Nov. 4, in Pittsburgh. (Justin Merriman for The Washington Post) “It’s about family — I want to keep my family intact, I want to provide for my family, I want to give my children opportunities,” she said, “And for a lot of immigrants, especially from Latin America, it’s ‘I want to be part of what America is,’ which is about being hardworking and having all these things.” So when immigrants are portrayed in a negative light — including by Trump — “it’s like: ‘If those people aren’t representing me well, I don’t want those people to come either!’” she said. The Democrats’ embrace of “those people,” Butler said, showed thosevotersthe party “didn’t fit the version of success.” Lin noted that Latino immigrants are strongly influenced by the prosperity gospel becausethey are part of a self-selecting group. “The more you believe in the prosperity gospel, the more you want to come. You have the faith and believe you deserve it. You want the American Dream. If you stay [in your home country], you’re not taking action, you lack faith,” said Lin, who profiled newcomers for his book. Mike Madrid, a political consultant and expert on Latino voters, said Trump is successfully selling a kind of spiritually-tinged hope to people who are fighting to make it in a country where wealth and material success are celebrated. “It’s hope. It’s hopeful,” he said. “You’re selling people an element of faith. ‘Have faith in this and you will achieve economic success, God wants that for you.’ That becomes a religion in an age where that is what is valued.” Scott Clement, Emily Guskin and Lenny Bronner contributed to this report. What should the Democrats do now? Author: Staff WAPO, , Date: 2024-11-07 Collections: Hot Takes US Elect 2024 Zotero Key: XXIFWIKA Cite Key: StaffWAPO24whatShouldDsDo Zotero Item | Lit Note Opinion Washington Post Opinions staff What should the Democrats do now? Eight columnists on how the opposition party should adapt after losing to Donald Trump — again. November 7, 2024 12 min 3957 Charles Lane: Listen to the majority Democrats need to a) learn and b) think. The main thing they need to learn from is the popular vote. Despite lacking constitutional significance, it is the most impressive result of Tuesday’s election. President-elect Donald Trump is on track to be the first Republican presidential candidate to get more votes than the Democratic candidate since George W. Bush in 2004. He could well equal Bush’s achievement of an absolute majority. The voters have delivered this verdict despite being told relentlessly, by Democrats and allied media personalities, that Trump is unfit, a liar, crazy, dangerous, racist, a misogynist, a Russian agent, a felon and a fascist. It didn’t matter, politically, how much truth there is in this message. A critical mass of the American people — maybe even most of them — have repudiated it. Trump is now significantly stronger politically than he was before being impeached twice, indicted four times and convicted once. What should this make Democrats think? Not, one hopes, that the people have proved themselves unworthy of self-government. Alas, some are already indulging this interpretation, much like the East German official in Bertolt Brecht’s poem “The Solution,” who informed a restive citizenry that they “had forfeited the confidence of the government and could win it back only by redoubled efforts.” As Brecht sardonically noted, “Would it not be easier in that case for the government to dissolve the people and elect another?” Democrats have been acting like the proverbial American tourist in France, trying to get their point across by shouting louder in a language only they understand. Cast out of the White House, the Senate and quite likely the House of Representatives, Democrats should stop yelling for a while and listen, really listen, to the economic and cultural concerns of ordinary people. The electorate that has just preferred Trump to Vice President Kamala Harris is the only one we have. Democrats can’t dissolve it and elect another; and without its support, resistance is futile. The math is dispositive about what the Democratic Party needs to do. A bit more than 60 percent of U.S. adults do not have a college degree. According to exit polls, 56 percent of these noncollege voters chose Donald Trump over Kamala Harris. So if Democrats want to build a durable majority, they have to find a way to reconnect with blue-collar Americans — not just White voters but also Hispanic, who fled to the GOP on Tuesday in numbers Democrats should find alarming. The theme of the Democrats’ appeal should be the promise of upward mobility. The policy agenda would flow from that: alleviating the housing crisis, jump-starting small businesses, safer streets, better schools, more affordable college tuition for your kids. Harris talked about these ideas. Her biggest problem was Trump’s generational talent for making blue-collar voters believe he sees them, hears them and is on their side (even if his policies are not). Four years from now, assuming the Constitution survives, Trump will be out of the picture. The GOP will own whatever happens between now and then on immigration, abortion and social issues such as transgender rights. The opportunity will be there for the right Democratic messenger to win blue-collar voters back. Karen Tumulty: Win back Latinos One place to start is with Latinos. The flight of Hispanic voters from their traditional home in the Democratic Party should be recognized as a blaring warning siren. Exit polls showed 46 percent of them voted for former president Donald Trump; the former president won 55 percent of Latino men. He swept 14 out of 18 counties along the Texas-Mexico border, doubling his 2020 performance in those historically Democratic strongholds. Trump bested even the numbers won by Texan George W. Bush in 2004. Hispanics by and large shrugged off the offensive language that Trump and his supporters used, including the late-breaking “floating island of garbage” joke that so many Democrats were convinced would be fatal. What they listened for, and never heard from Democrats, was a recognition of what was going on in their own lives — and a persuasive argument for how Vice President Kamala Harris and her party would make their circumstances better. Instead, they saw Democrats denying that there was chaos at the border, enforcing prolonged shutdowns during the pandemic that devastated their livelihoods and spending so excessively that it ignited inflation. So spare them prissy labels such as “Latinx.” Recognize that many are socially conservative. They are less moved by abortion rights, LGBTQ+ issues and social justice arguments. As Danny Ortega, an Arizona Democratic activist told Tim Alberta in the Atlantic: “They may think Republicans are racist, but some of them are going to vote for the Republicans anyway, because they’re better on the economy, better on small business, b tt l ti ” fail to deliver much besides chaos — as they will — Democrats should be ready with a set of economic solutions that actually work. Karen Attiah: Start over While former president Donald Trump’s victory indicates a shift in America, the right didn’t win everywhere. A number of progressive ballot measures, including in red states, passed across the country: raising minimum wage, increasing sick leave and reinforcing access to abortion. I am not terribly optimistic for the Democratic Party — which now feels lost to sea. The arrogant gamble to toss aside progressives, young people, and Arab, Muslim, Palestinian and Lebanese constituencies angry over U.S. support for Israel’s bombardment of Gaza fractured the Democratic base. The party will have to shake the perception that the people shouting about saving democracy are members of a party that people increasingly feel is beholden more to big-money interests and out-of-touch consultants than to its own voters — particularly when it comes to the economy. How can a party talk about democracy while alienating its base and talking down to voters? If they can’t get it right for 2026, honestly, tear the party down and start from scratch. For those looking to sell books on authoritarianism, this will be an opportune time. I’m optimistic for people who are interested in other ways of building grass-roots power. Community building will be really important. We must continue to help the most marginalized and to regain social trust. I hope that this will be a time for reflection for the Democrats. Defeat carries deep and powerful lessons, if we are willing to look. Perry Bacon Jr.: Protect immigrants and trans people I’m already seeing comments from Democratic members of Congress saying the party needs to distance itself from transgender Americans. I’m very worried the second Trump administration is going to demonize college professors and students, transgender people and undocumented immigrants in particular — and that Democratic politicians are going to go along with it (or not object much), thinking it will electorally help the party. That’s not the right moral stance. And I don’t think it’s politically savvy either. The Democrats had some of their best recent electoral performances in 2018 and 2020, when they consistently attacked former president Donald Trump for being anti-immigrant and embraced the protest movement after George Floyd was killed. The governors who won in 2022 and 2023 in purple and red states, such as Kentucky’s Andy Beshear, opposed anti trans legislation pushed by Republicans. lost resoundingly — while also saying a bunch of things I doubt she really believes. Democrats are never going to outdo the Republicans in terms of being mean to minorities. Rather than moving to the right on social issues, they should focus on economic ideas that actually resonate with people. Most people don’t run or aspire to run a small business. So it was strange that one of the few economic ideas that Harris harped on was a small-business tax deduction. Trump is telling a compelling story with a clear villain — essentially, “Hardworking Americans are paying taxes and having that money go to services for illegal immigrants.” Democrats need their own story — something like, “You can have good health care and a steady job and not pay exorbitant prices for groceries and child care if we start making the billionaires pay their fair share and stop allowing them to take all of the profits from your hard work.” Matt Bai: Leave Washington Here’s a bit of political arcana: It has now been 28 years since Democrats last nominated a presidential candidate who hadn’t first served in the U.S. Senate. In the past 60 years, only four Democrats have won the office. Two of them (Jimmy Carter and Bill Clinton) were governors who won by campaigning against the system, and another, Barack Obama, ran as an outsider who had barely served in Washington. The other is Joe Biden, who won at an extraordinary moment, mid-pandemic. Democrats have become an entirely Washington-centric party that celebrates its legislators. I don’t know exactly how Democrats rebound and rebrand in the second Trump era. But the party has to look away from Washington and get back to the states, where most successful reform movements have been born. Sure, it’s harder in these celebrity-obsessed times to get a hearing as a relative unknown, the way Carter and Clinton once did. But Pete Buttigieg showed that it can still be done — if you have talent and a story to tell. If I were a younger, ambitious Democratic governor who thought I understood where the country was going, I’d hit the road and start making my case before President-elect Donald Trump had finished unpacking his boxes. Outsider movements win. Washington parties don’t. Theodore R. Johnson: Compromise — then win Republicans or prevent the coming power struggle between the party’s factions. So, Democrats’ first order of business must be to assemble a governing coalition — even if they are in the House minority — to ensure the business of the country doesn’t stall out with partisan stalemates. They’ll need to be opportunistic in partnering with reasonable Republicans to modulate the administration’s worst impulses. Their second order of business is finding a new winning electoral coalition. The Obama formula for victory — one that Biden successfully revitalized — was high-turnout elections coupled with lopsided multiracial support: upward of 90 percent of Black voters and more than two-thirds of those who are Latino, Asian or from another ethnic minority group. That approach, which suggested racial demography was destiny, seems to have unraveled. Trump rebuilt George W. Bush’s winning coalition in 2004 by capitalizing on a set of cultural and economic grievances shared by people across races. But his solution is laden with intolerance, and it comes with threats of state violence. Democrats will have to show how progressive policies on immigration and the economy offer fairness and efficiency without MAGA’s room for despotism. E.J. Dionne Jr.: Take a breath Then the Democratic Party needs to start with these problems: 1. The Biden administration’s policies were focused on uplifting noncollege voters in red states. None of this seemed to penetrate. Why? Was it because the benefits of investment programs take a long time to be clear? Or was it bad salesmanship? Or was it because prices so dominated public thinking that none of this got through? 2. We know there is great diversity in the Latino community, particularly big differences between men and women and between evangelical and non-evangelical Latinos. Don’t be so shocked by what happened. George W. Bush got 40 percent to 45 percent of the Latino vote. What brought so many Latinos back to the GOP? 3. Fight Donald Trump’s abuses with everything in your arsenal. But figure out why even a share of voters who saw him as extreme voted for him anyway. How can the dangers the former president poses be made more concrete for more people? 4. Have a conversation on culture wars that is animated by an insistence that the party will not sell out women, LGBTQ+ people and people of color as well as by a desire to use language and arguments that are congenial to middle-of-the-road voters. This also means thinking more deeply about the role of religion in shaping political views. 5. Don’t get obsessed by categories such as “center” or “left.” Most voters don’t think that way. You shouldn’t, either. Party Affiliation and Election Polls in 2012 Author: Pew, , Date: 2012-08-03T19:18:18+00:00 Collections: PollMethods Zotero Key: 2BN7VBPD Cite Key: Pew12PartyAffil2012pols Zotero Item | Lit Note Update: In the years since this piece was published, the survey field has changed. The link between Americans’ willingness to take surveys and their political views became stronger. Researchers developed new methods for addressing this. Pew Research Center’s surveys of U.S. adults now weight on political party affiliation, but the way that is done differs from the approach imagined in this piece. In every campaign cycle, pollwatchers pay close attention to the details of every election survey. And well they should. But focusing on the partisan balance of surveys is, in almost every circumstance, the wrong place to look. The latest Pew Research Center survey conducted July 16-26 among 1,956 registered voters nationwide found 51% supporting Barack Obama and 41% Mitt Romney. This is unquestionably a good poll for Obama – one of his widest leads of the year according to our surveys, though largely unchanged from earlier in July and consistent with polling over the course of this year. And the survey did interview more Democrats than Republicans; 38% of registered voters said they think of themselves as Democrats, 25% as Republicans, and 33% as independents (to clarify, some reporters and bloggers incorrectly posted their own calculations of party identification based on unweighted figures). That’s slightly more Democrats than average over the past year, and slightly fewer Republicans. Recent Pew Research Center surveys have found anywhere from a one-point to a ten-point Democratic identification advantage, with an average of about seven points. While it would be easy to standardize the distribution of Democrats, Republicans and independents across all of these surveys, this would unquestionably be the wrong thing to do. While all of our surveys are statistically adjusted to represent the proper proportion of Americans in different regions of the country; younger and older Americans; whites, African Americans and Hispanics; and even the correct share of adults who rely on cell phones as opposed to landline phones, these are all known, and relatively stable, characteristics of the population that can be verified off of U.S. Census Bureau data or other high quality government data sources. Party identification is another thing entirely. Most fundamentally, it is an attitude, not a demographic. To put it simply, party identification is one of the aspects of public opinion that our surveys are trying to measure, not something that we know ahead of time like the share of adults who are African American, female, or who live in the South. Particularly in an election cycle, the balance of party identification in surveys will ebb and flow with candidate fortunes, as it should, since the candidates themselves are the defining figureheads of those partisan labels. Thus there is no timely, independent measure of the partisan balance that polls could use for a baseline adjustment. These shifts in party identification are essential to understanding the dynamics of American politics. In the months after the Sept. 11 terrorist attacks, polls registered a substantial increase in the share of Americans calling themselves Republican. We saw similar shifts in the balance of party identification as the War in Iraq went on, and in the build-up to the Republicans’ 2010 midterm election victory. In all of those instances, had we tried to standardize the balance of party identification in our surveys to some prior levels, our surveys would have fundamentally missed what were significant changes in public opinion. The clearest evidence of this is the accuracy of the Pew Research Center’s final election estimates. In every presidential election since 1996, our final pre-election surveys have aligned with the actual vote outcome, because we measured rising Democratic or Republican fortunes in each year. In short, because party identification is so tightly intertwined with candidate preferences, any effort to constrain or affix the partisan balance of a survey would certainly smooth out any peaks and valleys in our survey trends, but would also lead us to miss more fundamental changes in the electorate that may be occurring. In effect, standardizing, smoothing, or otherwise tinkering with the balance of party identification in a survey is tantamount to saying we know how well each candidate is doing before the survey is conducted. What follows is a more detailed overview of the properties of party identification – how it changes over the short- and long-term, and at both the aggregate and individual level. It also includes a detailed discussion of the distinction between registered voters and likely voters, and why trying to estimate likely voters at this point in the election cycle is problematic. What is Party Affiliation? Public opinion researchers generally consider party affiliation to be a psychological identification with one of the two major political parties. It is not the same thing as party registration. Not all states allow voters to register by party, and even in states that do, some people may be reluctant to publicly identify their politics by registering with a party, while others may feel they have to register with a party to participate in primaries that exclude unaffiliated voters. Thus, while party affiliation and party registration is likely to be the same for many people, it will not be the same for everyone. Party affiliation is derived from a question, typically found at the end of a survey questionnaire, in which respondents are asked how they regard themselves in politics at the moment. In Pew Research Center surveys, the question asks: “In politics today, do you consider yourself a Republican, Democrat or Independent?” As the wording suggests, this question is intended to capture how people think of themselves currently, and people can change their personal allegiance easily. We continually see evidence of this in surveys that ask the same people about their party affiliation at two different points in time. In a post-election survey we conducted in November 2008, we interviewed voters with whom we had spoken less than one month earlier, in mid-October. Among Republicans interviewed in October, 17% did not identify as Republicans in November. Among Democrats interviewed in October, 10% no longer identified as Democrats. Of those who declined to identify with a party in October, 18% told us they were either Democrats or Republicans when we interviewed them in November. Overall, 15% of voters gave a different answer in November than they did in October. We also see party affiliation changing in understandable ways over time, in response to major events and political circumstances. For example, the percentage of registered voters identifying as Republican dropped from 33% to 28% between 2004 and 2007 during a period in which disapproval of President George W. Bush’s job performance was rising and opinions about the GOP were becoming increasingly negative. Similarly, the percentage of American voters identifying as Democrats dropped from 38% in 2008 – a high point not seen since the 1980s – to 34% in 2011, after their large losses in the 2010 congressional elections. (For more about the fluidity of party affiliation, see Section 3 of the report, “Independents Oppose Party in Power… Again,” Sept. 23, 2010.) The changeability of party affiliation is one key reason why Pew Research and most other public pollsters do not attempt to adjust their samples to match some independent estimate of the “true” balance of party affiliation in the country. In addition, unlike national parameters for characteristics such as gender, age, education and race, which can be derived from large government surveys, there is no independent estimate of party affiliation. Some critics argue that polls should be weighted to the distribution of party affiliation as documented by the exit polls in the most recent election. But the use of exit poll statistics for weighting current surveys has several problems. First of all, a review of exit polls from the past four elections (including midterm elections) shows the same kind of variability in party affiliation that telephone opinion polls show. Why is an exit poll taken nearly two years earlier a more reliable guide to the current reality of party affiliation than our own survey taken right now? Second, most pollsters sample the general public – even if they subsequently base their election estimates on registered voters or likely voters in that poll. But the exit polls are sampling voters. We know that the distribution of party affiliation is not the same among voters as it is among the general public or among all those who are registered to vote. How can the exit polls provide an accurate target for weighting a general public sample when they are based on only about half (or less) of the general public? Registered Voters vs. Likely Voters Another common question during election years is why we report on registered voters when we will ultimately base our election forecast on likely voters. We certainly understand that the best estimate of how the election will turn out is one that reflects the voting intentions of people who will actually vote. Most – but not all – people who are registered to vote cast a ballot in presidential election years. It is for this reason that pollsters, including Pew Research, make a substantial effort to identify who is a likely voter (For more details on how likely voters are determined, see “Identifying Likely Voters” in the methodology section of our website). But, in the same way that party affiliation is not fixed for a given individual, being a “likely voter” is not a demographic characteristic like gender or race. Political campaigns are, in part, designed to mobilize supporters to vote. Although it may feel like the presidential campaign is in full swing, much of the hard work of mobilizing voters has not yet taken place and won’t occur until much closer to the election. Accordingly, any determination of who is a likely voter today – three months before the election – is apt to contain a significant amount of error. For this reason, Pew Research and many other polling organizations typically do not report on likely voters until September, after the nominating conventions have concluded and the campaign is fully underway. Critics have argued that any poll based on registered voters is likely to be biased toward Democratic candidates, since likely voter screens tend to reduce the proportion of Democratic supporters relative to Republican supporters. This has been the case in Pew Research’s final election polls over the past four presidential elections. In these polls the vote margin has been, on average, five points more favorable to the Republican candidates when based on likely voters rather than registered voters. These final estimates of the outcome have generally been very accurate, especially when undecided respondents are allocated to the candidates. Yet the effect of limiting the analysis to likely voters can vary over the course of the campaign cycle, even in just the later months. For example, in September and October of 2008, most Pew Research surveys found little difference between election estimates based on all registered voters and those we identified as most likely to vote, suggesting again that determining a likely voter gets more accurate only as Election Day nears. Moderation Is Not the Same Thing as Surrender Author: Chait, Jonathan, Date: 2024-11-27 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: NQ9MT7IE Cite Key: Chait24ModerateNotSurrender Zotero Item | Lit Note Before this month’s elections, when Democratic candidates were being attacked for letting transgender athletes compete in girls’ sports, trans-rights activists and their allies had a confident answer: They had nothing to fear, because anti-trans themes were a consistent loser for Republicans. That position became impossible to maintain after the elections, when detailed research showed that the issue had done tremendous damage to Kamala Harris and other Democrats. In fact, the third-most-common reason swing voters and late deciders in one survey gave for opposing Harris was that she “is focused more on cultural issues like transgender issues rather than helping the middle class,” an impression these voters no doubt got from endless ads showing her endorsing free gender-transition surgery for prisoners and detained migrants. Now some of the very people who pushed Democrats into adopting these politically toxic positions have shifted to a new line: Abandoning any element of the trans-rights agenda would be morally unthinkable. “To suggest we should yield even a little to Mr. Trump’s odious politics, to suggest we should compromise on the rights of trans people,” wrote the New York Times columnist Roxane Gay, would be “shameful and cowardly.” Asked whether his party should rethink its positions on transgender issues, Senator Tim Kaine said, “Democrats should get on board the hate train? We ain’t gonna do it.” The writer Jill Filipovic recently argued that Democrats must refuse “to chase the median voter if that voter has some really bad, dangerous, or hateful ideas.” Refusing to accommodate the electorate is a legitimate choice when politicians believe they are defending a principle so foundational that defeat is preferable to compromise. But in this case, the no-compromise stance is premised on a fundamental misunderstanding of the options on the table. Democrats do not, in fact, face a choice between championing trans rights and abandoning them. They can and should continue to defend trans people against major moral, legal, and cultural threats. All they need to do to reduce their political exposure is repudiate the movement’s marginal and intellectually shaky demands. Read: The Democrats need an honest conversation on gender identity The major questions about trans rights are: Do some people have the chance to live a happier and more fulfilling life in a different gender identity than the one to which they were born? Do some of these people need access to medical services to facilitate their transition? Do they deserve to be treated with respect and addressed by their chosen names and pronouns? Do they deserve equal protections from discrimination in employment, housing, andmilitary service? Must society afford them access to public accommodations so as not to assault their dignity? I believe the moral answer to all of these questions is a clear yes. The evidence also suggests that this is a relatively safe position for politicians to take. Americans broadly support individual choice, and trans rights fit comfortably within that framework. Sarah McBride, the incoming first transgender member of Congress, faced down bullying by her new Republican colleagues—an example of how Democrats can defend the dignity of trans people without allowing themselves to be depicted as extremists. The Trump administration is reportedly planning to kick transgender people out of the military, a move that only 30 percent of the public supports, according to a February YouGov poll. If Trump follows through, this fight would give Democrats the chance to highlight the pure cruelty of the Republican stance. Democrats mainly ran into trouble because they either supported or refused to condemn a few highly unpopular positions: allowing athletes who transitioned from male to female to participate in high-level female sports, where they often enjoy clear physical advantages; allowing adolescent and preadolescent children to medically transition without adequate diagnosis; and providing state-funded sex-change surgery for prisoners and detainees. The first two issues poll horribly; the last has not been polled, but you can infer its lack of support from the Harris campaign’s insistence on changing the subject even in the face of relentless criticism. I think there’s a strong case to be made for the Democrats adjusting the first two of these stances on substantive grounds. But even if you disagree with that, as many activists do, there remains an almost unassailable political case for reversing course. Why not stick to what I’d argue are the clearest, most important cases where trans rights must be protected, while letting go of a handful of hard-to-defend edge cases that are hurting Democrats at the polls—yielding policy outcomes that work to the detriment of trans people themselves? The answer is that much of the trans-rights activist community and its most vocal allies have come to believe that the entire package of trans-rights positions is a single, take-it-or-leave-it bloc. That mistaken conviction underlies the insistence that compromise is impossible, and that the only alternative to unquestioning support is complete surrender. This maneuver is common among political movements of all stripes. Consider how, say, Israel hawks routinely define being “pro-Israel” as not only supporting the existence of a Jewish state but also withholding any criticism of Israel’s military operations or settlement expansion. Once you have defined acceptance of your entire program as a moral test, it becomes easy to dismiss all opposition as bigotry—hence the disturbing ease with which many Israel hawks routinely smear even measured criticism of Israel as anti-Semitic. Examples of this dynamic are easy to find. Gun-rights advocates will denounce even the mildest firearms restriction as gun-grabbing and a rejection of the Second Amendment; some climate activists have extended the term climate denier from those who deny the science of climate change to anybody who rejects any element of their preferred remedy. Helen Lewis: The worst argument for youth transition Trans-rights activists have made especially extensive use of this tactic, frequently accusing anyone who dissents from any element of their agenda as transphobic. Quashing internal disagreements is a necessary step toward casting all dissent as pure bigotry. “A lot of LGBTQ leaders and advocates didn’t want to say they had concerns because they worried about dividing their movement,” the New York Times reporter Jeremy Peters noted. Perhaps the nadir of this campaign occurred last year, when a group of Times contributors and staffers published an error-riddled letter attacking the paper. The letter accused the Times of “follow[ing] the lead of far-right hate groups” with its reporting on the debate among youth-gender-care practitioners about the efficacy of providing puberty blockers and cross-sex hormones to children. It effectively transmitted the message that calling into question any position maintained by trans-rights activists would create a reputational cost for anybody working not just in journalism but in other industries, too—particularly people in Democratic politics and other nonconservative elite fields. The hothouse dynamic no doubt contributed to Democrats’ inability to form reality-based assessments of their positioning on the issue. A few days after the election, Democratic Representative Seth Moulton told the Times, “I have two little girls. I don’t want them getting run over on a playing field by a male or formerly male athlete.” This sparked a furious backlash. Kyle Davis, a Democratic official in Moulton’s home city of Salem, called on Moulton to resign. “We’re certainly rejecting the narrative that trans people are to be scapegoated or fear-mongered against,” he told reporters. Moulton has supported the Equality Act and the Transgender Bill of Rights, both of which would extend broad anti-discrimination protections to trans people. He has explained that he favors “evidence-based, sport-by-sport policies,” rather than the sweeping bans favored by Republicans. But Moulton’s general support for trans rights makes his heresy on female sports more, not less, threatening to the left. The MSNBC columnist Katelyn Burns argues that placing any limits on female sports participation means denying trans women all their other rights. “If trans girls are really boys when they’re playing sports … then trans women should be considered men in all contexts,” she wrote in October. That simple equation collapses under a moment’s scrutiny. Female sports is one of the rare cases in which the broadly correct principle of allowing trans people to set the terms of their own identity can meaningfully inhibit the rights of others. One can easily defend Lia Thomas’s right to be addressed as a woman and allowed access to women’s bathrooms without supporting her participation on a women’s college swim team. In place of careful reasoning, advocates of the maximal position frequently resort to sweeping moralistic rhetoric. Innumerable columns after this month’s elections have chastised moderates for “throwing trans people under the bus.” Arguing in this spirit, the New York Times columnist M. Gessen worries that trans people will be outright “abandoned” by the Democratic Party, and insists that Democrats cannot separate trans rights from other social issues, in part because Republicans see them all as linked. “On the right, all fears are interconnected, as are all dreams: Replacement theory lives right next to the fear of trans ‘contagion,’ and the promise of mass deportation is entwined with the vision of an America free of immigrants and people who breach the gender binary.” Helen Lewis: The only way out of the child-gender culture war As they refine their position profile, Democrats should obviously continue to listen to trans people themselves about their priorities. Those priorities are not always uniform, however, nor are they perfectly represented by the activist organizations speaking on their behalf. Dr. Erica Anderson, a trans woman and the former president of the United States Professional Association for Transgender Health, has criticized rapid medicalization of gender-questioning youth. The trans writer Brianna Wu argues that the movement’s adoption of more radical positions has imperiled its core goals. The tactic of smearing all of these critiques as “anti-trans” is deeply misleading. In a column demanding that Democrats give not an inch on any element of the trans-rights agenda, the Time columnist Philip Elliott asserts, “Conceding ground to the winners, as seems to be the case here in a culture-war fight that is as over-simplified as it is ill-considered, is not a way to dig out of this deep hole.” But the hole is not actually that deep. Harris lost both the national vote and Pennsylvania, the tipping-point state, by less than two percentage points. A Democratic firm found that exposure to Trump’s ubiquitous ads showing Harris endorsing free sex-change surgery for migrant detainees and prisoners moved the audience 2.7 points in his direction. And conceding ground to the winners is a time-honored way to escape political holes of any size. After Mitt Romney was hammered in 2012 over Republicans’ desire to cut Medicare, Trump repositioned them closer to the center. In 2024, Trump partially neutralized the GOP’s biggest liability, abortion, by insisting that he would leave the matter to the states, allowing him to pick up enough pro-abortion-rights votes to scrape by. Gessen argues, “It’s not clear how much further Democrats could actually retreat.” But there is plenty of reasonable room for Democrats to retreat—on female-sports participation, youth gender medicine, and state-sponsored surgery for prisoners and detainees. You may wish to add or subtract discrete items on my list. I can’t claim to have compiled a morally or politically unassailable accounting of which compromises Democratic politicians should make. What is unassailable is the principle that compromise without complete surrender is, in fact, possible. How Polls Have Changed to Try to Avoid a 2020 Repeat Author: Cohn, Nate, Date: 2024-10-23 Collections: PollMethods Zotero Key: N733ZT8K Cite Key: Cohn24changePollsAvoid2020 Zotero Item | Lit Note SUBSCRIBER-ONLY NEWSLETTER The Tilt How Polls Have Changed to Try to Avoid a 2020 Repeat Pollsters are constantly tinkering, but they have also made some substantial adjustments. By Nate Cohn Oct. 23, 2024 Updated 1:29 p.m. ET Illustration by Andrew Sondern; photograph by Tony Cenicola/The New York Times [ p g g g 2024; the first was about what went wrong in 2020.] Public pollsters ended up with egg on their face after the last two presidential elections. It’s possible they will after this election, too. But if you think nothing about polling has changed since then, you’re mistaken. The changes pollsters have made this cycle may or may not yield a more accurate result this November. That’s impossible to predict. But many of these changes are substantial, and, on balance, they probably do reduce the risk of another high-profile polling misfire — even if there are no guarantees. Over the last four years, there have been four basic kinds of changes in the polling world: Pollsters adopted different methods of data collection, like surveys taken by text or mail, rather than by phone. Pollsters adopted new methods of “weighting,” where they made statistical adjustments to try to better account for underrepresented groups. Pollsters didn’t change their practices, but benefited from new data that improved longstanding methods. The makeup of the pollsters changed, as some pollsters left the game and others joined the fray. These changes don’t account for every single change pollsters have made, of course. Pollsters tinker constantly with their procedures, even when they make no wholesale changes at all. Still, below are many of the big ones. Here’s how each might — ever so slightly — reduce the risk of another big miss. Fifteen years ago, nearly every major political poll was conducted by randomly dialing telephone numbers, a technique known as random-digit dialing. Gallup, ABC/Washington Post, CBS/New York Times, Pew Research, you name it — this was the kind of poll they conducted. Today, only one prolific pollster, Quinnipiac, is still doing it this way. Instead, pollsters are using many different methods to find respondents and conduct surveys, including sending text messages and recruiting survey-takers online. But perhaps a more surprising change has been many pollsters’ move toward using mail, a method known as address-based sampling. Why the mail? This method adheres to the principle of random sampling: It’s possible to send mail to addresses at random, much as pollsters once dialed random telephone numbers. But while most people never pick up their phone, most people do open the mail — and when they do, pollsters can try to get them to participate in their surveys, usually by offering a financial incentive. The move to address-based polling is playing out on two fronts. First, more pollsters are using so-called online probability panels, in which pollsters recruit people by mail to take polls online. CNN/SSRS, ABC News/Ipsos and the University of New Hampshire are all examples of pollsters that moved from random- digit dialing to mail-recruited probability panels over the last four years. It’s not clear so far that probability panels are any better at finding Trump voters than phone polls. (The Quinnipiac poll, using random-digit dialing, has tended to show similar or even stronger p ) y g panel attrition, where the people who gradually leave a panel may not be representative of those they originally recruited. Many of these pollsters produced some of the least accurate results of the 2020 campaign, and they’ve made major changes in response. Second, there’s the rise of so-called benchmark surveys. Here, a pollster conducts a one-off mail survey, usually with a large financial incentive. It then uses the results of that survey — which typically has a much higher response rate, thanks to those incentives — to ensure its subsequent and lower-quality polls have the same number of Democrats and Republicans as the higher- quality survey. Get the best of The Times in your inbox Sign up for The Great Read: On weekdays and Sundays, we recommend one piece of exceptional writing from The Times — a narrative or essay that takes you someplace you might not expect to go. Thank you for signing up for Opinion Today. Manage your preferences. Sign up for Breaking News: Sign up to receive an email from The New York Times as soon as important news breaks around the world. The most prominent example is the Pew NPORS study, a high- incentive mail survey with around a 30 percent response rate. Pew’s probability panel, in comparison, effectively has a 3 percent response rate — similar to most other high-quality polls. The Pew NPORS study is being used to determine the partisan makeup of many polls you see nowadays, including the Pew American Trends Panel, Reuters/Ipsos, ABC/Ipsos, CNN/SSRS, KFF and Marquette Law/SSRS polls. y y p p advantage on leaned party identification. Realistically, many or even all of these surveys would find more Democratic results without the Pew NPORS. Weighting: Recall vote This cycle, many pollsters have started to weight their polls on a new metric: “recalled vote.” This is a technique in which a pollster weights the number of self- reported Biden 2020 and Trump 2020 voters to match the result of the last election. 2024 Election: Live Updates › Updated 6 hours ago Donald Trump’s lifetime of scandals heads toward a moment of judgment. Here’s the latest on the presidential race. Trump accuses a U.K. party of election meddling. He has links to its rivals. We’ve written a lot about this measure — it’s a big controversy among survey researchers this cycle. In prior cycles, almost none of the traditional “gold standard” pollsters even touched it. Now CNN/SSRS, The Washington Post and Monmouth University, among many others, employ the technique. Around two-thirds of the polls this cycle are weighted using recalled vote. Whatever its merit, recall-vote weighting today clearly reduces the risk that the polls will underestimate Mr. Trump this cycle, for three reasons. First, voters have typically been likelier to recall voting for the winner of the last election in this case President Biden In p g g g g the respondents who say they voted for the loser — in this case Mr. Trump. As a result, weighting on recall vote would tend to boost Mr. Trump’s numbers in 2024. Second, it may be especially well suited to counteract the biases of highly engaged voters. One of the most prominent theories of survey error is that the polls get too many highly engaged voters, and that these voters lean Democratic. These voters are also among the likeliest to report sticking with the candidate they supported in the last election. As a consequence, weighting by past vote can move a group of highly engaged Democratic-leaning respondents neatly into alignment into the result of the last election. Third, many polls in 2020 were off by so much that they absolutely would have benefited from weighting on recall vote, if only because it makes it harder to produce double-digit outliers. The infamous ABC/Post poll in Wisconsin, which found Mr. Biden up 17 points, would have been helped by this measure. With recall vote in the back pocket of pollsters, a result like Harris +17 is much less likely to happen this cycle. And in fact there hasn’t been one. New data: More Trump-era information Some pollsters haven’t necessarily made any major changes since 2020, and that includes many of the state-based pollsters using voter file data. Will these pollsters fare as poorly as they did four years ago? They very well might, but they do have one reason to hope to do better anyway: They might benefit from data that wasn’t available back in 2020 p g turnout modern race, and there are many indications that Mr. Trump fared best among lower-turnout voters. Today, pollsters know who those new 2020 voters are; they didn’t know four years ago. Who voted by mail, early in person or on Election Day. In 2020, Biden voters were much more likely to vote by mail, while Trump supporters were more likely to vote in person on Election Day. Importantly, this pattern often broke party lines: The Republicans who voted by mail, for instance, weren’t very likely to support Mr. Trump; the Democrats who voted in person, on the other hand, were often relatively supportive of Mr. Trump. Today, this information is a powerful tool to help make sure a pollster doesn’t just have the right number of Democrats and Republicans, but also the right kind of Democrats or Republicans. Four years of party registration changes. Over the last few years, Republicans made significant gains in party registration, in no small part because many longtime Trump supporters have gradually been registering as Republicans. To the extent some Trump voters were “hidden” from pollsters, many have come out of the woodwork by re-registering as Republicans. A new cast of pollsters Imagine, for a moment, that none of these changes helped at all, and that these pollsters fared just as poorly as they did four or eight years ago. The polls still might be a bit more accurate than they were in 2020. How can that be? The answer is because the makeup of the pollsters has changed. p p 538’s reckoning — has since stopped asking voters how they will vote in the presidential race. SurveyMonkey and Swayable were the two most prolific polls of the 2020 campaign, with a combined 126 polls over the final stretch. They’ve all but disappeared this cycle. Three Democratic firms — Change Research, Data for Progress and Public Policy Polling — produced a combined 117 polls over the final three weeks of the 2020 campaign. They’re still around, but producing numbers at only a fraction of the old pace. At the same time, many firms that had more balanced or even Republican-leaning results have kept producing new polls or even increased their pace. For the 2022 midterms, there were so many Republican-leaning polls — and so few traditional, nonpartisan pollsters — that the polling averages of several key states wound up underestimating Democrats. This cycle, there are many more nonpartisan pollsters conducting surveys. As a result, the Republican pollsters haven’t succeeded in moving the averages, as they did two years ago. Nonetheless, the balance of pollsters has shifted to the right. In 2020, the polls underestimated Mr. Trump by about 4.2 points. This year’s cast of pollsters, based on those that conducted polls over the last month, underestimated Mr. Trump by 3.2 points in 2020. It’s just one point, but it’s a point less biased against Mr. Trump. Together, the polling industry is hoping it adds up to a big difference in November. Nate Cohn is The Times’s chief political analyst. He covers elections, public opinion, demographics and polling. More about Nate Cohn How Democrats Blew It on Inflation Author: Timiraos, Nick, Date: 2024-11-18 Collections: Hot Takes US Elect 2024 Zotero Key: 9CAWHIXE Cite Key: Timiraos24demsBlewItOnInflation Zotero Item | Lit Note President Biden hadn’t even been inaugurated when he and his senior advisers made a monumental gamble in January 2021 that would reverberate through his presidency. Fresh on the heels of a $900 billion Covid-relief bill that Congress had approved weeks earlier, Biden proposed a $1.9 trillion stimulus bill. Biden and many of those advisers had been part of the Obama administration. Barack Obama took office under similarly gloomy circumstances in January 2009, during the low point of the global financial crisis. Years of high unemployment followed, with much of the world mired in a trap of low growth and very low—even negative—interest rates. One lesson Democrats took from that episode: Spend aggressively when interest rates are low. It is better to overfill the cup than underfill it. The result was the American Rescue Plan, a package that boosted a child tax credit, sent $1,400-per-person direct payments to households, and directed $350 billion to state and local governments. Politically, those choices backfired. Billions of dollars in Covid-19 aid were already coursing through the economy, the aftereffect of bipartisan spending measures under Donald Trump. Strong demand from Biden’s additional fiscal stimulus, ultralow interest rates and a successful vaccine rollout ran headlong into crippled supply chains and discombobulated labor markets. Bad luck piled on. New Covid variants, Russia’s invasion of Ukraine and China’s Covid-related lockdowns continued to upend global supply chains and commodity markets. Inflation soared in most of the world’s richest economies—across Europe, Canada, and Australia. Democrats had bet voters would reward them for a strong labor market recovery with tangible gains for workers. Instead, those voters recoiled at the sudden cost-of-living increases. Consumer prices during Biden’s term have risen 20%, compared with 8% in Trump’s term. “If inflation had been less severe in that first year, if it had peaked at a lower level, could Vice President Harris have survived that? My intuition is yes,” said Michael Strain, head of economic-policy studies at the right-leaning American Enterprise Institute. On Election Day, roughly 40% of voters said the economy was their top issue, far outstripping any other. Those voters backed Trump by a 22-percentage-point margin. Inflation has declined without a recession, but many were thinking instead about how prices are still high. White House officials interviewed for this story defended their record by pointing to how the ARP was designed at a time when it wasn’t at all clear the country was about to escape the pandemic. Virus counts and deaths were rising as Biden took office. And after a swift rebound in hiring through the second half of 2020, the number of people working fell in December 2020. “Any scenario that envisions less inflation from a reduced ARP also has to wrestle with slower growth, higher unemployment and more child poverty,” said Jared Bernstein, the chair of the White House Council of Economic Advisers. White House and Democratic officials have argued that overall U.S. economic outcomes were better than those achieved in nearly every other advanced economy. But politically, those arguments fell flat and gave Trump his opening. “It comes off as cold comfort to say that people have it worse in Germany, the U.K., France,” said Rep. Brendan Boyle (D., Pa.). “People naturally compare their experiences today to what things were like prepandemic.” ‘17 educated idiots’ Republicans were opposed to another stimulus package in early 2021. So Democrats had to maneuver with a razor-thin one-vote majority in the Senate, with Vice President Kamala Harris able to break a 50-50 tie. Sen. Joe Manchin, the most conservative Democrat in the chamber, was stunned when colleagues told him the $1.9 trillion price tag and warned them it was a massive overreach, he said in a recent interview. Lawmakers were seeking to extend temporary unemployment benefits of an extra $400 a week. “A lot of hardworking people aren’t making that kind of money in my state,” Manchin told his colleagues. Within hours of speaking up, he found himself face-to-face in the Oval Office with Biden. Manchin begged the president to pump the brakes. The country hadn’t even digested the stimulus approved weeks earlier. It didn’t make sense to throw another $2 trillion on top. Biden was unmoved. “I’ve got to do it, Joe,” he said. Manchin ultimately swallowed his reservations and gave his support. The package passed in March 2021. The loudest criticism came from Larry Summers, who had been a top Obama adviser but wasn’t part of Biden’s team. Many Democrats faulted Summers for insufficient stimulus in 2009 and were in no mood to listen to him by 2021. He pointed to how Democrats had lost badly at the ballot box during other bouts of inflation in 1968 and 1980. “The sense of serenity and complacency being projected by the economic policymakers, that this is all something that can easily be managed, is misplaced,” he said that spring. Inflation jumped that April, driven by used cars, airfares and other items that could be traced to reopening the economy. Officials at the White House and Federal Reserve highlighted the temporary nature of price readings by describing high prices as likely to be “transitory.” And for the first few months, the story held. Price growth initially eased through the summer. White House advisers kept telling Manchin not to worry. One pointed to how 17 Nobel laureates said inflation would be transitory. Manchin shot back: “You’ve got 17 educated idiots telling you what you want to hear.” When price growth broadened and reaccelerated that fall, the Fed threw “transitory” into the ash heap and sped up plans to withdraw stimulus. By December 2021, inflation hit 7%. Fiscal stimulus, approved by both Trump and Biden, accounted for about 3 percentage points of the rise in inflation through 2021, according to the San Francisco Fed. A separate analysis by the bank’s economists estimated the ARP boosted inflation, excluding food and energy items, by 0.3 percentage point a year in 2021 and 2022. Raw politics, and not just economics or health concerns, were at work. The administration had faced heavy pressure from progressives, including in the House of Representatives, to make $1.9 trillion the floor, not the ceiling, for any spending on the ARP. Focus shifted to passing Build Back Better, a separate $3.5 trillion spending bill that Democrats envisioned as Biden’s signature economic initiative. A meaningful acknowledgment that inflation was a problem would raise alarm bells over further spending. That, in turn, could kill the sprawling healthcare, education and climate package. Many economists, inside and outside the White House, genuinely believed inflation would be transitory. Still, Democratic-affiliated analysts and economists outside the administration faced pressure to play down inflation risks to avoid imperiling the administration’s broader agenda, said Strain, the economist at the American Enterprise Institute. “They kept insisting it was transitory because they were trying to shove BBB down my throat,” said Manchin, who ended negotiations over the package at the end of 2021. What went wrong The postpandemic economic response will be heavily debated for decades. Outside analysts fault the White House for a tepid response that compounded misfortunes on Election Day. First, officials treated inflation as a communications problem that could be improved with better messaging. In the summer of 2022, after a mild inflation reading followed a string of hot prints, one of the president’s top political advisers argued that officials should make a big show of declaring that the battle on inflation had turned, according to people familiar with the matter. Biden’s economic advisers flinched. What would they say if inflation was bad next month? “That’s a problem for 30 days from now,” the political adviser responded. Second, economic advisers were overruled at times when it came to proposing measures that might have lowered consumer prices. In April 2022, for example, some of the president’s economic team pushed for rolling back tariffs on certain Chinese imports that had been imposed by the Trump administration. Even if it might not make a noticeable dent in consumer price gauges, they argued it was better for the White House to be viewed as throwing its back into bringing down prices. Tariffs are often passed on to customers in the form of higher prices. For nonstrategic goods, “there’s not much of a case for those tariffs being in place,” economic adviser Daleep Singh said during remarks in Washington at the time. “Why do we have tariffs on bicycles or apparel or underwear?” Political advisers sympathetic to concerns from labor groups and foreign-policy advisers who wanted to maintain leverage against Beijing argued against the move. Biden sided with them when his economic advisers said removing tariffs wouldn’t move the needle on inflation. “President Biden said inflation was his No. 1 priority, and I don’t think he acted like it was his No. 1 priority,” said Strain. Bernstein, the senior White House economist, said that the White House was “always balancing trade-offs.” Other officials said the political hit from removing tariffs would have overwhelmed any marginal benefit in the form of lower prices. Gene Sperling, a former senior economic adviser to Biden and Harris, said cross-country comparisons show that the U.S. outperformed its peers economically and that Democrats fared better than other incumbent governments politically. He faulted analyses that presume there were “silver bullets on inflation we just chose not to fire.” White House officials pointed to a raft of measures they took to lower costs as it became clear inflation might take longer to decline. That included releasing oil from the U.S. strategic reserve, capping the cost of insulin and prescription drugs for seniors and taking steps to ban junk and other hidden fees. The Biden administration did less to tamp down aggregate demand, leaving the Fed to shoulder much of the burden of cooling down the economy. The Fed raised rates at the fastest pace in 40 years to a two-decade high, and Biden mostly avoided commenting on rates. He pledged not to publicly challenge the Fed as it tried to slow the economy. Critics say the White House could have done more to cut deficits that were unprecedented for a peacetime economy with low unemployment. Instead, the administration passed several pieces of legislation that boosted spending on infrastructure and clean-energy investments, including the Inflation Reduction Act in 2022. “The whole job of fighting inflation shouldn’t just be up to the Fed. I’d prefer a whole-of-government approach,” said Robert Kaplan, a former Dallas Fed president, in a June interview. Kaplan at the time argued for hitting the pause button on some of the spending from the IRA out of concern that subsidies to encourage new factory construction were working against efforts to lower inflation. The spending blitz of 2020-21 revealed that “these things have a momentum of their own,” said Raghuram Rajan, a former governor of India’s central bank. “It is hard to reverse on a dime, and then it goes further than you want.” Write to Nick Timiraos at Nick.Timiraos@wsj.com The Role of Artificial Intelligence in Discourse Analysis Author: DiscourseAnalyzer, Date: 2024-08-09T00:06:06+00:00 Collections: PoliticalML Zotero Key: DT8XIGZZ Cite Key: DiscourseAnalyzer24discourseAI Zotero Item | Lit Note This article is brought to you by the Discourse Analyzer AI Toolkit, the #1 AI tool for Discourse Analysis.Dive deeper into language and communication with the leading AI-powered tool designed to provide comprehensive insights. Ready to revolutionize your discourse analysis projects? 👉 Subscribe and Use the Tool Now! Artificial Intelligence (AI) plays an increasingly significant role in discourse analysis by offering powerful tools and methodologies for analyzing large-scale linguistic data, identifying patterns in language use, and enhancing the efficiency and depth of discourse studies. By integrating AI techniques, discourse analysts can automate and refine the analysis of complex discursive patterns, making it possible to handle vast amounts of data, uncover subtle linguistic features, and explore the dynamics of discourse across various contexts. 1. Key Contributions of Artificial Intelligence to Discourse Analysis Natural Language Processing (NLP) Natural Language Processing, a subfield of AI, provides the computational tools necessary to process and analyze human language. Key contributions include: • Text Mining and Data Extraction: NLP algorithms can automatically extract relevant information from large text corpora, making it easier to identify and analyze key themes, topics, and trends in discourse. • Sentiment Analysis: AI can analyze the sentiment expressed in texts, identifying positive, negative, or neutral attitudes, which is useful for understanding public opinion, media discourse, and social media communication. • Named Entity Recognition (NER): NLP systems can identify and categorize entities (such as names, locations, dates) within a text, helping to contextualize and understand discourse more accurately. Machine Learning and Pattern Recognition Machine learning algorithms are used to recognize patterns in language use and to categorize or cluster texts based on linguistic features. This includes: • Topic Modeling: Techniques like Latent Dirichlet Allocation (LDA) can automatically identify topics within a large collection of texts, revealing underlying themes and structures in discourse. • Text Classification: Machine learning models can classify texts into categories based on content, such as detecting bias, genre, or rhetorical style, aiding in the analysis of discourse across different domains. • Clustering and Similarity Analysis: AI can group similar texts together based on linguistic features, allowing researchers to identify discourse communities or variations in discourse across different groups or time periods. AI enables the development of automated tools that can perform complex discourse analysis tasks. These tools can include: • Automated Content Analysis: Software that uses AI to analyze large volumes of text, identifying key themes, discourse markers, and patterns of argumentation. • Conversation Analysis Tools: AI-powered tools that can analyze the structure and flow of conversations, such as turn-taking, interruptions, and conversational dynamics. • Discourse Parsing: AI systems that can break down texts into their component discourse units (such as sentences, clauses, or paragraphs) and analyze how these units relate to one another to construct meaning. 2. Applications of Artificial Intelligence in Discourse Analysis AI is particularly useful in analyzing the vast amounts of discourse generated on social media platforms. Applications include: • Trend Analysis: AI can track and analyze the evolution of discourse around specific topics or hashtags, helping to understand public sentiment and the spread of ideas. • Toxicity Detection: AI models can identify harmful or toxic language in online discourse, helping to moderate content and study the dynamics of online aggression and harassment. • Misinformation and Fake News: AI can analyze and detect patterns in language use that are associated with misinformation, aiding in efforts to identify and combat false narratives. AI can enhance the analysis of political and media discourse by processing large volumes of text and identifying patterns in communication strategies, framing, and bias. Applications include: • Media Bias Detection: AI can be used to detect and analyze biases in media coverage, revealing how different outlets frame issues and represent different groups. • Political Speech Analysis: AI tools can analyze political speeches to identify rhetorical strategies, sentiment, and the use of persuasive language. • Campaign Discourse Analysis: AI can track and analyze the discourse used during political campaigns, providing insights into messaging strategies and voter engagement. Educational and Institutional Discourse AI can assist in analyzing educational and institutional discourse by automating the analysis of classroom interactions, policy documents, and organizational communication. Applications include: • Automated Feedback Systems: AI-powered systems can analyze student essays or classroom discussions, providing feedback on language use, argumentation, and adherence to discourse norms. • Policy Document Analysis: AI can process and analyze large collections of policy documents, identifying key themes, trends, and the framing of issues over time. • Organizational Communication: AI tools can analyze internal communications within organizations to study power dynamics, cultural trends, and the impact of leadership discourse. 3. Advantages and Challenges of AI in Discourse Analysis Advantages • Scalability: AI allows discourse analysts to process and analyze large-scale datasets that would be impossible to handle manually, enabling the study of vast corpora of texts. • Efficiency: AI can significantly speed up the analysis process by automating repetitive tasks such as text classification, sentiment analysis, and entity recognition. • Precision and Objectivity: AI algorithms can detect subtle patterns and features in discourse that might be overlooked by human analysts, reducing subjective biases in the analysis process. Challenges • Interpretability: AI models, especially complex ones like deep learning, can be difficult to interpret, making it challenging to understand how they arrive at their conclusions. • Bias and Fairness: AI models can inherit biases present in the training data, potentially leading to biased or unfair outcomes in discourse analysis. • Context Sensitivity: AI may struggle with understanding the nuanced context of discourse, particularly in cases where meaning is heavily dependent on cultural or situational factors. Conclusion Artificial Intelligence plays a transformative role in discourse analysis by providing powerful tools and methodologies for processing and analyzing large-scale linguistic data. By integrating AI techniques such as Natural Language Processing, machine learning, and automated analysis tools, discourse analysts can enhance the efficiency, depth, and scale of their studies, making it possible to uncover complex patterns and dynamics in discourse across various contexts. However, the use of AI in discourse analysis also presents challenges, particularly in terms of interpretability, bias, and context sensitivity, which researchers must carefully address to ensure robust and ethical analysis. Frequently Asked Questions How does Artificial Intelligence (AI) contribute to discourse analysis? AI contributes to discourse analysis by providing advanced tools and methodologies for processing and analyzing large-scale linguistic data. AI techniques such as Natural Language Processing (NLP) and machine learning enable the automation of complex analytical tasks, allowing discourse analysts to handle vast amounts of data, identify patterns in language use, and delve deeper into the nuances of discourse across various contexts. What is Natural Language Processing (NLP), and how is it used in discourse analysis? NLP is a subfield of AI that focuses on the interaction between computers and human language. In discourse analysis, NLP is used to: Text Mining and Data Extraction: Automatically extract key information from large text corpora, identifying themes, topics, and trends in discourse. Sentiment Analysis: Assess the sentiment expressed in texts, identifying positive, negative, or neutral attitudes. Named Entity Recognition (NER): Identify and categorize entities within a text, such as names, locations, and dates, helping to contextualize discourse. How does machine learning enhance discourse analysis? Machine learning enhances discourse analysis by recognizing patterns in language use and categorizing or clustering texts based on linguistic features. Key applications include: Topic Modeling: Techniques like Latent Dirichlet Allocation (LDA) automatically identify underlying themes and structures in discourse. Text Classification: Machine learning models classify texts into categories based on content, such as detecting bias or rhetorical style. Clustering and Similarity Analysis: AI groups similar texts together, enabling the identification of discourse communities or variations across different groups. What are some automated tools developed using AI for discourse analysis? AI has enabled the development of various automated tools for discourse analysis, including: Automated Content Analysis: Software that analyzes large volumes of text to identify key themes, discourse markers, and argumentation patterns. Conversation Analysis Tools: AI-powered tools that analyze conversational dynamics, such as turn-taking and interruptions. Discourse Parsing: Systems that break down texts into discourse units (sentences, clauses) and analyze their relationships to construct meaning. How is AI applied to analyze social media and online discourse? AI is particularly effective in analyzing the vast amounts of discourse generated on social media platforms. Applications include: Trend Analysis: Tracking and analyzing the evolution of discourse around specific topics or hashtags. Toxicity Detection: Identifying harmful language in online discourse, aiding in content moderation and the study of online aggression. Misinformation and Fake News: Detecting patterns associated with misinformation, helping to combat false narratives. In what ways does AI assist in analyzing political and media discourse? AI enhances the analysis of political and media discourse by processing large volumes of text and identifying patterns in communication strategies, framing, and bias. Key applications include: Media Bias Detection: AI detects biases in media coverage, revealing how different outlets frame issues. Political Speech Analysis: Analyzing political speeches to identify rhetorical strategies, sentiment, and persuasive language. Campaign Discourse Analysis: Tracking and analyzing political campaign discourse to understand messaging strategies and voter engagement. What role does AI play in educational and institutional discourse analysis? AI assists in analyzing educational and institutional discourse by automating the analysis of classroom interactions, policy documents, and organizational communication. Applications include: Automated Feedback Systems: AI analyzes student essays or discussions, providing feedback on language use and argumentation. Policy Document Analysis: Processing large collections of policy documents to identify key themes and trends. Organizational Communication: Analyzing internal communications to study power dynamics, cultural trends, and leadership discourse. What are the advantages of using AI in discourse analysis? The advantages of using AI in discourse analysis include: Scalability: AI allows for the processing of large-scale datasets, enabling the study of vast corpora that would be impossible to analyze manually. Efficiency: AI automates repetitive tasks like text classification and sentiment analysis, significantly speeding up the analysis process. Precision and Objectivity: AI can detect subtle patterns in discourse that might be overlooked by human analysts, reducing subjective biases. What challenges does AI present in discourse analysis? AI presents several challenges in discourse analysis, including: Interpretability: Complex AI models can be difficult to interpret, making it challenging to understand how they arrive at their conclusions. Bias and Fairness: AI models can inherit biases from their training data, leading to biased outcomes in discourse analysis. Context Sensitivity: AI may struggle with understanding nuanced contexts, particularly where meaning is heavily dependent on cultural or situational factors. Why is AI important in the future of discourse analysis? AI is important in the future of discourse analysis because it offers powerful tools to analyze complex discursive patterns at scale, uncover subtle linguistic features, and handle vast datasets efficiently. As discourse increasingly takes place in digital and global contexts, AI’s ability to process and analyze large amounts of data will be crucial for understanding the evolving dynamics of language, communication, and power in society. However, careful attention must be given to the challenges of interpretability, bias, and context sensitivity to ensure robust and ethical discourse analysis. Support for Trump’s Policies Exceeds Support for Trump Author: Peters, Jeremy W., Date: 2025-01-18 Collections: Hot Takes US Elect 2024 Zotero Key: RP5S9T5F Cite Key: Peters25policySupportMoreThanTrump Zotero Item | Lit Note Many Americans who otherwise dislike President-elect Donald J. Trump share his bleak assessment of the country’s problems and support some of his most contentious prescriptions to fix them, according to a new poll from The New York Times and Ipsos. A little more than half of the country expresses some desire to see Mr. Trump follow through with his harshest threat to deal with illegal immigration: deporting everyone living in the United States without authorization. The poll, which surveyed 2,128 adults from Jan. 2 to 10, found that 55 percent of Americans either strongly or somewhat support such mass deportations. Americans are more evenly split on whether Mr. Trump should implement tariffs on countries like China and Mexico, which he has vowed to do as a way to reduce reliance on foreign goods. Still, 46 percent say that trade with foreign nations should be subject to increased tariffs. And a large majority is sympathetic to efforts to strictly limit how doctors can treat children struggling with their gender identity — an issue Mr. Trump and other Republicans made central to their campaigns for office. Seventy-one percent said that no one under 18 should be prescribed puberty-blocking drugs or hormones. The Supreme Court is expected to issue a decision on the matter later this year. The poll tells the story of a country turning inward, where people are more aligned with Mr. Trump’s “America First” agenda than they were during his first term in office. For a political figure so divisive — Americans view him more negatively than any other president about to take office in the last 70 years — the level of support for his ideas is striking. Most Americans say the United States has ignored serious problems at home while entangling itself in costly conflicts abroad, the poll found. A majority believe the government is sending too much money to Ukraine. And many are expressing less tolerance of immigrants overall. “Something needs to happen on immigration,” said Jose Hernandez, 48, of Atlanta, who works with a hotel chain on new projects. “I’m an immigrant myself, from Mexico, but I waited 25 years. I came to this country legally.” He added, “There’s no control over the system.” Mr. Hernandez said he voted for Hillary Clinton in 2016 and Joseph R. Biden in 2020, and considers himself aligned with Democrats on social justice. But in 2024, he supported Mr. Trump as more of a vote “against Kamala” than anything else, he explained. Sign up for the Race/Related Newsletter Join a deep and provocative exploration of race, identity and society with New York Times journalists. Get it sent to your inbox. Though Mr. Hernandez said he does not want to see mass deportation, he described the current situation at the border as “unsustainable.” “We establish rules and guidelines. When you’re not following those rules, that’s it,” he said. Mr. Trump has vowed to carry out the largest deportation operation in American history. And the public is with him, to a point. A vast majority of Americans — 87 percent — support deporting undocumented immigrants with a criminal record, which Mr. Trump has said would be one of the first orders of business he carries out. Nearly two-thirds of all Americans — including 54 percent of Hispanics and 44 percent of Democrats — support deporting people who entered the country illegally during the last four years of the Biden administration, after it reversed many of Mr. Trump’s immigration restrictions from his first term. In that time, legal and illegal immigration soared to the highest levels in U.S. history. A slim majority — 56 percent — said they believe that immigrants strengthen the country. About 41 percent agreed with the statement “immigrants today are a burden on our country.” That sentiment had subsided over the last decade, according to several public polls, but now appears to be rising. The undocumented population was 11 million people in 2022, according to the latest government estimates. Demographers agree that the current number is higher, between 13 million and 14 million. Americans are also eager to see their country less enmeshed in world affairs. Asked if it was better for the United States to be active in world affairs or, instead, to concentrate less on problems overseas and pay more attention to issues at home, 60 percent of Americans prefer less foreign engagement, according to the poll. As recently as 2019, a smaller share of Americans expressed a desire to pull back from international affairs, splitting about 50-50 on the question, according to Pew Research Center. The Ipsos survey, conducted for The New York Times, aimed to measure support for specific policy proposals Mr. Trump said he would implement if elected. It also surveyed public sentiment on a range of issues that have been the subject of partisan disagreement, from the scope of presidential power to programs designed to promote diversity. The country remains deeply divided over Mr. Trump, the poll found, despite his inflated claims of winning “a powerful and unprecedented mandate.” Roughly the same share of people told The Times that they are worried or pessimistic about the next four years as excited or optimistic. His favorability rating, according to an average of polls from the website FiveThirtyEight, has hovered just below 50 percent lately. That matches his share of the popular vote in 2024. Americans are far from willing to give Mr. Trump carte blanche. For instance, even though most people expect he will use the government to investigate and prosecute his political opponents, a vast majority of Americans do not want him to. That includes a majority of Republicans. Overall, 73 percent of Americans say they oppose the idea of Mr. Trump pursuing legal charges against his adversaries — with 49 percent saying they are strongly opposed. Mr. Trump would also lack majority support to eliminate the constitutional guarantee to citizenship for anyone born on American soil, the poll found. The poll also revealed that Americans hold their government in exceedingly low esteem — far lower than during the Watergate era. Majorities across races, genders and partisan stripe say the political system is broken and that the economy works against them — a pessimism that tracks with some of Mr. Trump’s grimmer rhetoric. There is a widespread belief, across parties, that Washington is corrupt, with two-thirds of Democrats and 80 percent of Republicans saying the government serves itself and the powerful over ordinary people. Two-thirds of Americans say the economic system unfairly favors the wealthy. In interviews, respondents to the poll reflected the foul mood of the country. “So many elected officials have the service of their constituencies at the bottom,” said Tarra Williams, 49, a compliance manager in Mooresville, N.C., who said she voted for Vice President Kamala Harris. Ms. Williams said she did not trust Republicans, Democrats or the federal government. “The whole country is on cognitive dissonance autopilot,” she said. “We need a whole governmental reboot.” Among some Democrats, there was a certain ambivalence about Mr. Trump’s second inauguration. “I don’t think Trump becoming president is a good or bad thing,” said Booker Preston, 50, a mechanic in Fort Worth, Texas, who said he voted for Ms. Harris. Perhaps, he suggested, the government might spend some of the money it sends abroad tackling problems at home. “I really feel that we spend a lot of money overseas that we might not be able to recoup — nor do we get enough benefits to offset,” he said. “We could spend those billions of dollars here to really help people here.” Mr. Trump’s promise to do a better job managing inflation and the economy persuaded many voters. More Americans expect that Mr. Trump’s policies will help rather than hurt the economy. Even among Democrats, about one-third say he will help the economy or, at least, not make much of a difference. Americans were mixed on whether Mr. Trump would be able to make good on some of those economic promises. Most Republicans expect that prices will go down during Mr. Trump’s tenure; most Democrats expect they will not. But Americans largely expect him to follow through on what he said he would do. Nearly unanimously, and across parties, majorities said they thought he was likely to carry out mass deportations and that he would raise tariffs on China and Mexico. A slightly narrower majority of Americans expect that Mr. Trump would involve the country in fewer wars. Republicans are about twice as likely to expect this as Democrats. Like it or not, Mr. Trump did not begin any major wars, said Tim Malsbary, 56, a nurse in Cincinnati, who said he voted for Mr. Trump this election but used to consider himself a Democrat. “The Democratic Party has made me bitter,” he added. Though the issue of rights for gay and transgender people ranks far down most Americans’ list of priorities — only 4 percent cited it as one of their most important issues — Republicans have focused on it heavily. And Mr. Trump, who ran attention-grabbing ads attacking Ms. Harris as a radical on the issue, appears to have been more in sync with public sentiment. The survey found, for instance, that just 18 percent of Americans believe transgender female athletes — those who were male at birth — should be allowed to compete in women’s sports. Nearly 80 percent say they should not. On social issues, Republicans have also gone after attempts to increase racial diversity. When it comes to such efforts in schools and government agencies, Americans are evenly divided, with 48 percent saying they want to end such programs and 47 percent who want to keep them. About 22 percent of Black Americans and 40 percent of Hispanic Americans support ending these programs. Still, as polarizing as many Americans find Mr. Trump, some are withholding judgment. Ali Romero, 43, of Moab, Utah, said she found it difficult to support some of Mr. Trump’s decisions on things like reproductive rights and social justice. But she did not see Ms. Harris as a compelling alternative, even though she leans Democratic. “So instead of voting for someone and feeling not great about it,” she said, “I voted for nobody and I feel great about it.” At the very least, a Trump presidency will be different, she said. “It’s not the status quo.” Christine Zhang contributed. How This Poll Was Conducted Here are the key things to know about this poll from The New York Times and Ipsos: • The margin of sampling error among all Americans is about plus or minus 2.6 percentage points. In theory, this means that the results should reflect the views of the overall population most of the time, though many other challenges create additional sources of error.* You can see full results and a detailed methodology here. You can view the cross tabs here. ‘What a circus’: eligible US voters on why they didn’t vote in the 2024 presidential election Author: Otte, Jedidajah, Date: 2024-12-13T14:00:48.000Z Collections: PollMethods, Hot Takes US Elect 2024, ElectionPredFeats Zotero Key: N3QMJXD5 Cite Key: Otte24whyVotersNoVote Zotero Item | Lit Note The 2024 US presidential election had been widely characterized as one of the most consequential political contests in recent US history. Although turnout was high for a presidential election – almost matching the levels of 2020 – it is estimated that close to 90 million Americans, roughly 36% of the eligible voting age population, did not vote. This number is greater than the number of people who voted for either Donald Trump or Kamala Harris. More than a month on from polling day, eligible US voters from across the country as well as other parts of the world got in touch with the Guardian to share why they did not vote. Scores of people said they had not turned out as they felt their vote would not matter because of the electoral college system, since they lived in a safely blue or red state. This included a number of people who nonetheless had voted in the 2020 and 2016 elections. While various previous Democratic voters said they had abstained this time due to the Harris campaign’s stance on Israel or for other policy reasons, a number of people in this camp said they would have voted for the vice-president had they lived in a swing state. “I’m not in a swing state, and because of the electoral college my vote doesn’t count. I could have voted 500,000 times and it would not have changed the outcome,” said one such voter, a 60-year-old software developer with Latino heritage from Boston. Having voted for Hillary Clinton in 2016, he voted in 2020 but left the presidential slot blank “as a Quixotic protest against the electoral college and my preference for Bernie Sanders”, he said. He said he felt “heartbroken” over Joe Biden and Harris’s stance on Gaza. “If I were in a swing state I would always vote for Dems, though,” he added, echoing several others. A 40-year-old carpenter from Idaho who voted in the previous two elections because he then lived in the swing state of Arizona – giving his vote to Clinton and Biden – also said he did not vote this time because he felt his vote did not matter due to the electoral college system. “I didn’t find Harris compelling, just more of the same. Politicians from both parties seem unwilling to make the kind of fundamental economic and political changes that would make a meaningful difference for all people, namely a move towards a more democratic socialist system. That being said if we didn’t have the electoral college I probably would have voted for Harris,” he said. A large number of people said they abstained because no candidate represented working- or middle-class interests and people such as themselves, including several people who voted in the previous two elections but did not vote this time. Some people from swing states said they did not vote because both parties were too similar and did not address concerns of the common voter, among them John, a 29-year-old financial professional from Pennsylvania who is a registered independent, but voted for Clinton and Biden in the previous two elections. “What is the point [of voting]?,” he asked. “Aside from a handful of weaponized issues, the parties are nearly identical. They both hate the poor and serve only their donors.” A number of former Trump and Biden voters said they had not voted in this election as they disliked both candidates, among them Jared Wagner, a 34-year-old from Indiana who works in the trucking industry and said he had voted for Trump in 2016 but had abstained in both the 2020 and 2024 election. “I refuse to put my name on either candidate when I know neither of them are truly the best we have to offer. We need a major overhaul to the two-party system,” he said. “As a man with young children I worry about what kind of country they will grow up in. It terrifies me; we deserve better.” John, a 58-year-old from West Virginia, said he had voted for Hillary Clinton in 2016 and Biden in 2020, but had decided that not voting this November “felt most authentic and appropriate”. “I wasn’t apathetic about this election, I followed it closely,” he said. “But most of the candidates and issues left me cold and disinterested and seemed to be simply perpetuating the existing system, especially the status quo of authority and law and order, or rampant human development on the land. “On the presidential level, I was shocked and disgusted that the Harris campaign chose to completely ignore discussing climate change. Fundamentally, this election seemed to have very little to do with my interests and concerns.” Anne, a 65-year-old retired white woman from California, was among various people who said they had voted but not for any presidential candidate. She said she had always previously voted for the Democratic candidate, but could not bring herself to do so this time. “I did vote for all other down-ballot candidates and initiatives,” she said. “I would have voted for Harris had my vote made a difference, but I could not vote for a president who will continue the complete destruction of Gaza and annexation of the West Bank.” Various people said they did not vote for a presidential candidate in the 2024 election because they had only wanted to cast a positive vote for a candidate rather than merely an opposition one, and that neither candidate had offered a compelling vision for change. Among them was a 62-year-old professional working in process planning from Texas, who said he had voted for the Republican presidential candidate at every election between 1984 and 2016. “In 2020 I voted Libertarian as a protest vote,” he said. “This year I was so turned off by Trump’s low character, economic ignorance, disregard of our national debt, hostility to Ukraine and so on that I was trying to convince myself to vote for Harris. But her economic policy was just a grab bag of voter payoffs and she doesn’t care about the debt either. skip past newsletter promotion Big thinkers on what we can do to protect civil liberties and fundamental freedoms in a Trump presidency. From our opinion desk. Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion “So I did not vote for president. I voted for Senate, congressman, and many other down ballot races. I split my ticket, too. I just no longer want to vote against anyone. I want to vote FOR someone. And none of the candidates for president wanted my vote enough.” A 35-year-old Black male voter from Portland, Oregon, who works at a gas station, said he disliked Kamala Harris but now regretted not voting for her, as he had thought Trump would lose the election. I want to vote FOR someone. And none of the candidates for president wanted my vote enough “I did not vote in 2016 or 2020 either because I did not like any of the candidates in those elections either. I last voted in 2012, for Obama,” he said. “I felt both candidates fell well short of the presidential standard, and didn’t feel I could cast a vote for either,” said a 47-year-old engineering manager and registered Republican from Texas. “VP Harris failed to demonstrate she was ethically or intellectually capable of executing the office, repeatedly failing to detail out her policies and generally running her campaign like a popularity contest – ‘collect enough celebrity endorsements, by paying them, and the masses will elect you,’” he said. Trump, he felt, “cares about the US and believes his own ideas will ‘save’ the country – but he’s a terrible human being. I don’t feel he represents a majority of Americans at all, but is more a reaction to some of the issues we face as a country.” Various people who did not vote in other recent elections either said that again this time no candidate was leftwing enough, among them 37-year-old Elly, a mother of four daughters from the midwest. “Bernie Sanders was the last candidate I was excited to vote for,” she said. “This election came down to two parties who have utterly abandoned everyday people and their problems with affordability and worries about climate change, but one party, the Republicans, were savvy enough to pretend they felt the collective pain of the common folks, whilst the Democrats mostly said ‘all is well.’ I couldn’t in good conscience support either side on the national level.” Several people who usually always voted Democrat in the past said the Harris campaign had been overly focussed on progressive identity politics for them to be able to lend it their support this election, such as Simon, a father from California in his 60swho had voted for Clinton in 2016 and for Biden in 2020 in protest against Trump, but had abstained this year due to Harris’s embracing of “trans ideology”, among other reasons. “I am not a fan of the Democrats, but I would have voted to keep Trump out of office if there was an economically literate, competent, law and order candidate who was willing to challenge the excesses of ‘woke’,” he said. “The Dems are out of touch on social issues, and have tacked too far to the left to appease a minority of progressives. “I support some policies that would be considered rightwing on immigration, but also investing in social housing, so I’m looking for candidates capable of taking difficult decisions based on rational analysis.” Leigh Crawford, a 56-year-old hedge fund manager from California, who had voted for Barack Obama in 2012, for Clinton in 2016 and for Biden in 2020, said he had abstained this time as both candidates were fiscally irresponsible in his view, because he strongly disliked Trump’s anti-immigration and pro-tariffs stance, and because Harris had been “pro-censorship” and “too tolerant of antisemitism”. Several people said they did not vote this time because of a growing disillusionment with the extreme polarization in US politics, including Chris, an architect in his 40s from Tennesseewho had voted twice for Obama, and once for Trump in 2016, but had abstained in 2020 and 2024 as he had lost hope in politics. “Skip the debates, what a circus,” he said. “I’m so sick of hearing about politics. “The political system in the US is broken. Things are so polarized, there is no cooperation for the good of the people. There is just so much hate, even in everyday conversation with average people. “There is just so much of this ‘if I don’t win, I’m taking the ball and going home’ mentality. It just causes nothing to get accomplished.” How America Embraced Gender War Author: Tolentino, Jia, Date: 2024-11-07 Collections: Hot Takes US Elect 2024 Zotero Key: TZQLAMCD Cite Key: Tolentino24HowAmericaEmbraced Zotero Item | Lit Note The big two genders are said to be at war. The results of the Presidential election can hardly be read otherwise: in preliminary exit-poll data out of Pennsylvania, women aged eighteen to twenty-nine swung forty points for Kamala Harris, while their male counterparts swung twenty-four points for Donald Trump. The conflict—the dark, snarling, many-headed beast of indifference and contempt that emerges from these numbers—has been building for decades. Women in America, as in nearly all industrialized democracies, used to be more conservative than men; in the nineteen-seventies, they began to shift leftward, then closed the partisan gap by the eighties, and during the nineties became firmly more liberal than American men. The simplest explanation for this is the most plausible one: women, acquiring education and workplace power and economic independence, drew closer to a party that valorized equality and away from a party that valorized hierarchy. With birth control, with safe and legal abortion, the story went, women gained control over their lives. In the twenty-tens, women gained control over culture, too. A slick, corporatized feminism—the mugs about male tears, the Ruth Bader Ginsburg bobbleheads—occupied the public sphere. Girls were brash and confident and eager to call out bad male behavior; it was no longer O.K. to kiss a girl if she didn’t want you to (even if she was slutty!). All of this made a certain cohort of men lose their grip. Over time, their numbers grew, fermenting in corners of the Internet that indulged their feelings of being left behind. Women were graduating from high school and college in greater numbers than men; they were suddenly wanted in places where they had been unwanted almost forever. Around ten years ago came a Presidential candidate who promised to reverse the shifts that had transformed American life toward equality—to put men, white people, white men, back on top. It was war then, too: Trump won despite and because of the fact that he’d bragged about sexually assaulting women; the fact that his wife was hot, silent, and seemingly miserable; the fact that he had so many accusers no one could keep track. Trump did what he promised, and installed a Supreme Court that overturned Roe v. Wade during a Democratic Presidency that ostensibly rebuked him. At least twenty states made abortion more or less illegal. I.V.F. was restricted in Alabama; doctors in states like Oklahoma, Texas, and Florida stopped treating women in mortal danger because of active miscarriages; a woman with a cancerous and nonviable pregnancy was told to bleed in a hospital parking lot until she was sick enough to qualify for care. A thirteen-year-old girl in Mississippi who’d been raped in her front yard couldn’t afford to get to Chicago for an abortion and thus became a mother before starting the seventh grade. The 2024 election was framed, optimistically, by Democrats, as a referendum on this suffering, as the midterms had been. After Harris entered the race, almost four in ten women under the age of thirty cited abortion as the most important issue to their vote. Abortion was the second most important issue among all Harris voters, the most important being “democracy.” For Trump voters, the economy was the top issue. The funny thing is the pretense that we can separate these concepts. Without the right to choose, women are not full participants in a democracy or an economy. Both campaigns leaned into the gender war. Trump turned his focus away from the suburban white women who’d supported him in 2016, and he courted young men, attending U.F.C. matches, cozying up to Elon Musk, and going on podcasts recommended by his eighteen-year-old son. He led with vibes of absurdist aggression, a miasma of crypto and YouTube testosterone, allowing the policy architecture of actual, brutal, gender-based subjugation to follow easily behind. Trump was among the many G.O.P. candidates who collectively poured tens of millions into anti-trans political ads, showing his commitment not to women but to the institution of gender itself. Harris went on “Call Her Daddy,” a podcast where young women dish about sex and kvetch about men, and got Julia Roberts to narrate an ad about how wives don’t have to disclose their liberal votes to their conservative husbands. The strategy reflected a reality that has since been unveiled by the results—a gendered battle intensified by young people, who are fighting for a sense of individual purchase on a world they have barely begun to properly live in. Those Pennsylvania exit polls. The ones in North Carolina, where young female voters went for Harris by thirty-three points and young male voters went for Trump by twenty-three. The fact that the entire nation, more or less, is shifting rightward at least a little, but that men aged eighteen to twenty-nine have moved almost thirty points rightward since 2020. If the 2016 election illuminated the shocking state of white women’s loyalties, the 2024 election has instantly done the same for young men. The gender war, as pitched by politicians, revolves around two competing visions of a woman’s life. Each side thinks it understands what the other wants. The Trumpists—embodied by J. D. Vance and his repulsive, sneering childless-cat-lady comments, by Tucker Carlson and his portrayal of Kamala Harris as a “Samoan Malaysian, low-I.Q.” diversity hire—believe that the left wants women to be Plan B-gobbling dilettantes in their youth; dick-stomping corporate drones in early adulthood; lonely, angry spinsters who approach forty in a mania for egg-freezing or emasculation. (Soon afterward, the woman problem becomes neutralized by the relative invisibility of the “postmenopausal female.”) The libs believe that conservatives want women to spend their youth in training to attract, submit to, and please men, suppressing all other forms of human potential for one that revolves around dress-up, smiling self-imprisonment, and wiping asses, both literally and emotionally. The chasm between young men and women in this year’s vote is the chasm between these two stories. It’s men fearing women’s enthrallment to independence at the expense of their own centrality, and women fearing their subjugation to men at the expense of their lives. The difference—and this is always the difference—is about volition. Men who voted for Trump fear what women might actually want; women who voted for Harris fear what will be done to them against their will. In the imaginary world ruled by angry lesbian socialist girlbosses, there is absolutely nothing to stop you from being a barefoot, pregnant homemaker at twenty-four if you’d like to be one. In the increasingly non-hypothetical world ruled by far-right Trumpists, the blissful servitude of women must be insured by removing their control over their bodies, and ideally, actually, by removing them from the public sphere altogether. In a recent video, the former Trump aide and Project 2025 adviser John McEntee quipped while cheerfully eating chili-cheese fries, “So I guess they misunderstood when we said we wanted mail-only voting. We meant ‘male,’ m-a-l-e.” Dale Partridge, the pastor of an “anti-woke” church and the author of a book called “The Manliness of Christ”—always a funny argument, given that Jesus famously bled to death to give new life, as many girls and women in conservative states will over the next four years—posted, “In a Christian marriage, a wife should vote according to her husband’s direction. He is the head and they are one.” What baffles me about this supposed contest between ideas of womanhood, both of them invented by men for political purposes, is its distance from the reality of living as a woman. The conflict that exists between work and parenting, between childlessness and child rearing—even between wanting power over men and willingly giving them power over you—flares not in the gap between a liberal woman and a conservative woman but within each of their individual lives. Two-thirds of Republican mothers work outside the home; the percentage is only three per cent higher for Democratic mothers. Democratic women have their first child at twenty-five, on average, just one year later than Republican women. Eighty-six per cent of Democratic parents and eighty-eight per cent of Republican parents think of parenting as one of the most important parts of their identity. Women on the left want children; women on the right get abortions. “I see you’ve had your hair cut—just when it was starting to look nice.” Cartoon by Tom Chitty There are many millions of women, mostly white—forty-five per cent of them voted for Trump in this election—who are drawn to the archetypal conservative vision of motherhood. But the gendered gulf in the youth vote suggests that the fight is changing, as women in the middle of the political spectrum begin to vote, post-Roe, on the basis of their actual lives. The possibility of a national abortion ban is looming. The lawyer behind Senate Bill 8, Texas’s abortion-bounty law, is closely allied with Trump and recently represented him before the Supreme Court. Project 2025 outlines a plan for formal federal surveillance of pregnancy. This is what so many young men, straight men who want women to bear their children, voted for. The fight is now peer-to-peer, between men in favor of reproductive servitude and women who refuse. Trump’s return to power—his imminent control over the Supreme Court and the federal judiciary, the coming dissolution of the very idea of the government providing any sort of guardrail against corporate power, carceral violence, and environmental destruction—is the beginning of a political era that will likely last decades. So much of it will be worked out on a level that the ordinary person mostly cannot touch. But this particular part—the politics of abortion, the struggle of who gets to determine when and why and how a person has a child, the question of who and what a woman works for—will also be negotiated at home. In her study of marriage “Parallel Lives,” the critic Phyllis Rose argues that “marriage is the primary political experience in which most of us engage as adults.” There is a reason that both campaigns have, in different ways, embraced the framing of their political fights as marital dramas. For those of us whom God made heterosexual, the intimate realm is politicized now more than ever. But it’s from this foundation that we find a way out. It’s from here, in the arena of flesh and friction and surprise and transcendence—an arena that may be increasingly foreign to screen-bound, isolated, radicalized young men, and rightfully unappealing to their female counterparts—that we learn not just when to take up arms against another person but when to try harder to see them, or allow them to change us. It’s here that we learn how badly we need one another, in the end. ♦ ‘Trump’s America’: Comeback Victory Signals a Different Kind of Country Author: Baker, Peter, Date: 2024-11-06 Collections: Hot Takes US Elect 2024 Zotero Key: QX93QYJ4 Cite Key: Baker24tumpDifferentCntry Zotero Item | Lit Note In her closing rally on the Ellipse last week, Kamala Harris scorned Donald J. Trump as an outlier who did not represent America. “That is not who we are,” she declared. In fact, it turns out, that may be exactly who we are. At least most of us. The assumption that Mr. Trump represented an anomaly who would at last be consigned to the ash heap of history was washed away on Tuesday night by a red current that swept through battleground states — and swept away the understanding of America long nurtured by its ruling elite of both parties. No longer can the political establishment write off Mr. Trump as a temporary break from the long march of progress, a fluke who somehow sneaked into the White House in a quirky, one-off Electoral College win eight years ago. With his comeback victory to reclaim the presidency, Mr. Trump has now established himself as a transformational force reshaping the United States in his own image. Populist disenchantment with the nation’s direction and resentment against elites proved to be deeper and more profound than many in both parties had recognized. Mr. Trump’s testosterone-driven campaign capitalized on resistance to electing the first woman president. And while tens of millions of voters still cast ballots against Mr. Trump, he once again tapped into a sense among many others that the country they knew was slipping away, under siege economically, culturally and demographically. To counter that, those voters ratified the return of a brash 78-year-old champion willing to upend convention and take radical action even if it offends sensibilities or violates old standards. Any misgivings about their chosen leader were shoved to the side. As a result, for the first time in history, Americans have elected a convicted criminal as president. They handed power back to a leader who tried to overturn a previous election, called for the “termination” of the Constitution to reclaim his office, aspired to be a dictator on Day 1 and vowed to exact “retribution” against his adversaries. “The real America becomes Trump’s America,” said Timothy Naftali, a presidential historian at Columbia University. “Frankly, the world will say if this man wasn’t disqualified by Jan. 6, which was incredibly influential around the world, then this is not the America that we knew.” To Mr. Trump’s allies, the election vindicates his argument that Washington has grown out of touch, that America is a country weary of overseas wars, excessive immigration and “woke” political correctness. “The Trump presidency speaks to the depth of the marginalization felt by those who believe they have been in the cultural wilderness for too long and their faith in the one person who has given voice to their frustration and his ability to center them in American life,” said Melody C. Barnes, the executive director of the Karsh Institute of Democracy at the University of Virginia and a former adviser to President Barack Obama. Rather than be turned off by Mr. Trump’s flagrant, anger-based appeals along lines of race, gender, religion, national origin and especially transgender identity, many Americans found them bracing. Rather than be offended by his brazen lies and wild conspiracy theories, many found him authentic. Rather than dismiss him as a felon found by various courts to be a fraudster, cheater, sexual abuser and defamer, many embraced his assertion that he has been the victim of persecution. “This election was a CAT scan on the American people, and as difficult as it is to say, as hard as it is to name, what it revealed, at least in part, is a frightening affinity for a man of borderless corruption,” said Peter H. Wehner, a former strategic adviser to President George W. Bush and vocal critic of Mr. Trump. “Donald Trump is no longer an aberration; he is normative.” The fact that Mr. Trump was able to bounce back from so many legal and political defeats over the past four years, any one of which would have been enough to wreck the career of any other politician, was a testament to his remarkable resilience and defiance. He is unbowed and, this time at least, undefeated. It also owed in part to failures of President Biden and Ms. Harris, his vice president. Mr. Trump’s victory was a repudiation of an administration that passed sweeping pandemic relief, social spending and climate change programs but was hobbled by sky-high inflation and illegal immigration, both of which were brought under control too late. Moreover, Mr. Biden and Ms. Harris never managed to heal the divisions of the Trump era as promised, though it may never have been possible. They could not figure out how to channel the anger that propels his movement or respond to the culture wars he fosters. Once she took the torch from Mr. Biden, Ms. Harris initially emphasized a positive, joy-filled mission to the future, consolidating excited Democrats behind her, but it was not enough to win over uncommitted voters. At that point, she switched back to Mr. Biden’s approach of warning about the dangers of Mr. Trump and the incipient fascism she said he represented. That was not enough either. “The coalition that elected them wanted them to unite the country, and they failed to do so,” said former Representative Carlos Curbelo, a Republican from Florida. “Their failure has resulted in further disillusionment with our country’s politics and empowered the Trump base to give him another narrow victory after setbacks in three consecutive general elections.” Ms. Harris did preach unity in her closing days, but her “we are all in this together” message of harmony fell short against Mr. Trump’s “fight, fight, fight” message of belligerence. As much as anything, the election reinforced how polarized the country has become, split down the middle. It is a tribal era, an us-versus-them moment, when each side is so divorced from the other that they find it hard to even comprehend each other. Mr. Trump’s political resurrection also highlighted an often underestimated aspect of the 248-year-old American democratic experiment. For all of its commitment to constitutionalism, the United States has seen moments before when the public hungered for a strongman and exhibited a willingness to empower such a figure with outsized authority. That has often come during times of war or national peril, but Mr. Trump frames the current struggle for America as a war of sorts. “Trump has been conditioning Americans throughout this campaign to see American democracy as a failed experiment,” said Ruth Ben-Ghiat, a historian and author of “Strongmen: Mussolini to the Present.” By praising dictators like President Vladimir V. Putin of Russia and President Xi Jinping of China, she said, “he has used his campaign to prepare Americans for autocracy.” She cited his adoption of language from Nazi and Soviet lexicons, such as branding opponents as “vermin” and the “enemy from within” while accusing immigrants of “poisoning the blood of our country,” and suggesting that he might use the military to round up opponents. “A victory for Trump would mean that this vision of America — and the recourse to violence as a means of solving political problems — has triumphed,” Ms. Ben-Ghiat said. Others cautioned against assuming Mr. Trump would follow through on his most outlandish threats. Marc Short, who was chief of staff to Vice President Mike Pence and might have reasons to worry given Mr. Trump’s anger at him and his former boss, said he was not concerned about a wave of retaliation. “I don’t believe in that,” he said. “I think there’s a lot of theater around that more than there is real sort of retribution.” But Mr. Short predicted another four years of chaos and uncertainty. “I would anticipate a lot of volatility — personnel but also significant boomerangs on policy,” he said. “Not boomerang from Biden-Harris but boomerang from himself. You’ll have one position one day and another the next.” Mr. Trump’s latest victory also adds ammunition to the argument that the country is not ready for a woman in the Oval Office. Mr. Trump, a thrice-married admitted adulterer accused of sexual misconduct by more than two dozen women, has for the second time defeated a woman with more experience in public office than he had. Each of them was flawed, just as male candidates are flawed, but the sense of 2016 déjà vu on the left on Wednesday morning was palpable. Mr. Trump ran a campaign openly aimed at men, featuring Hulk Hogan ripping off his shirt at the Republican National Convention, macho talk at his closing Madison Square Garden rally and even the former president himself seeming to simulate a sex act on a microphone in the final days of the race. On Election Day, Mr. Trump’s adviser Stephen Miller posted a message on social media saying, “If you know any men who haven’t voted, get them to the polls.” According to exit polls, a majority of Ms. Harris’s supporters were women while a majority of Mr. Trump’s supporters were men. Yet even though most abortion rights referendums were passing in various states on Tuesday, the issue did not galvanize women in the first presidential race since Roe v. Wade was overturned to the extent that Democrats had expected and Republicans feared. In a sense, Mr. Trump’s victory also brings the Jan. 6, 2021, ransacking of the Capitol by a mob of his supporters full circle. The attack, aimed at stopping the finalization of Mr. Biden’s 2020 victory, has now been recast from a deadly assault on democracy that discredited Mr. Trump into a patriotic act that will generate pardons promised by the newly re-elected president. “In many ways, this is the last chapter of the Jan. 6 drama,” said Mr. Naftali. “Many Republicans thought they had managed to thread the needle, to avoid pissing off their base while also jettisoning Trump. And it turned out they hadn’t. And now they have him back. And if he wins the bet, and he’s returned to power, then the final verdict of Jan. 6 is that in modern America, you can cheat and the system isn’t strong enough to fight back.” The defining struggle going forward will be the war that Mr. Trump says he will now wage against a system that he deems corrupt. If he follows his campaign promises, he will seek to consolidate more power in the presidency, bring the “deep state” to heel and go after “treasonous” political opponents in both parties and the media. As he does that, he will have legitimacy and experience that he did not have the last time around. He learned from his first term, not so much about policy, but about how to pull the levers of power. And this time, he will have more latitude, a more aligned set of advisers and possibly both houses of Congress as well as a party that even more than eight years ago answers solely to him. The Trump era, it turns out, was not a four-year interregnum. Assuming he finishes his new term, it now looks to be a 12-year era that puts him at the center of the political stage as long as Franklin D. Roosevelt or Ronald Reagan were. It is Mr. Trump’s America after all. Democrats face a reckoning and a long rebuilding. There is no quick fix. Author: Balz, Dan, Date: 2024-11-10 Collections: Hot Takes US Elect 2024 Zotero Key: 9LTGBCL8 Cite Key: Balz24demsReckonRebuild Zotero Item | Lit Note Democrats face a reckoning and a long rebuilding. There is no quick fix. Donald Trump’s commanding victory left Democrats adrift. They must first understand why they lost before figuring out how to rebuild. Supporters of Vice President Kamala Harris sit in the bleachers on Tuesday at Howard University, where her campaign held its election night party. (Marvin Joseph/The Washington Post) Analysis by Dan Balz November 10, 2024 at 6:00 a.m. EST The question was asked countless times over the past few years: What is the future of the Republican Party? That question was answered on Tuesday by the voters. The party’s future is its present: President-elect Donald Trump. The pertinent question now is what is the future of the Democratic Party? Get the latest election news and results Defeat has launched a reckoning for Democrats. Ahead of Tuesday, many Democrats could not while many ballots have yet to be counted. The search for answers has begun. Start with some fundamentals. This was a tough environment for the incumbent party. President Joe Biden’s approval rating on Election Day was 40 percent positive and 59 percent negative, according to network exit polls. Even in an era when politicians have uniformly low approval ratings, that 59 percent negative judgment was an anchor that weighted down Vice President Kamala Harris. On Election Day, 68 percent of voters said the economy was either “not so good” or “poor,” according to exit polls. Democrats wanted voters to focus on low unemployment, sustained if modest growth and a stock market that continued to expand everyone’s retirement accounts. Voters had another view: 46 percent said their family’s finances were worse today than four years ago while 24 percent said they were better; 75 percent said inflation had caused a moderate or severe hardship on them or their families. Overall, voters were in a sour mood. In the exit polls, 73 percent said they were either dissatisfied or angry. This was an environment in which voters were calling for change — as they have in every election since 2006, except for 2012, when President Barack Obama was reelected Of the 28 percent of the electorate needed change was their priority, Trump won them by nearly 3 to 1. Given all that, it’s not so surprising that the party that held the White House and the Senate was driven out of power. Amid all the internecine finger-pointing about what Harris and her campaign could or should have done differently, and those questions are legitimate, it’s important to remember how much fundamentals affect the outcome of elections. In the midterm elections two years from now, if Trump fails to deliver on the broad promises of his campaign, if the economy falters, if a Trumpian Republican Party without overwhelming strength in Congress proves incapable of governing effectively, Democrats could believe they are truly on the rebound. If history is any guide, and in the Trump era that’s not always a safe assumption, Democrats are likely to make gains in Congress in 2026. Beyond the climate that burdened Harris, however, last week’s election highlighted the challenges Democrats have to confront to wrest power back. Trump has accelerated a realignment of the American electorate based on education, which has been the most significant fault line in politics since his first campaign in 2016. In 2024, it proved to be more significant than the gender gap, which drew larger than recent norms. Harris addresses supporters after conceding to President-elect Donald Trump at Howard University on Wednesday. (Marvin Joseph/The Washington Post) Colleagues at The Washington Post offered insight into this with an article analyzing the electorate that put Trump in the White House for a second time. The conclusion: The Trump coalition in 2024 was more diverse, younger and more working-class than the coalition that voted for him previously. Voters without college degrees supported Trump by a margin of 14 percentage points, the most for any Republican since 1984. In that year, Ronald Reagan won them by 19 percentage points en route to a landslide victory by winning 59 percent of the popular vote and carrying 49 states. Trump managed to attract these working-class voters though his popular vote percentage currently stands at about 51 percent. White in its makeup, but one reason he has expanded his coalition is the shift in support among Latinos, particularly Latino men. Nationally, Trump carried Latino men by 54 percent to 44 percent. His support grew by 19 percentage points from 2020. In Texas and in Florida, he won Latino men with 64 percent of the votes. Increasingly it appears that the Hispanic vote will be up for grabs, rather than solidly Democratic. That’s a huge change that Democrats cannot ignore. And it speaks to the broader decline in support among working-class voters generally. Political parties are not static institutions. They are organic, capable of adapting and responding to setbacks. What this requires is not playing the blame game internally or looking for scapegoats or criticizing those for voting the other way. It requires sober analysis, a sound theory of the way back and a leader who can distill that into a winning campaign. The most effective presidential candidates are often those who can define or redefine their party, as Trump and Obama did, rather than simply being defined by the party. Harris, because she entered so late in the election cycle, because she was the vice president and because she had not independently carved out an identity when she ran in the 2020 campaign was dissatisfaction with its leadership. President Joe Biden speaks about the election in the Rose Garden of the White House on Thursday. (Maansi Srivastava for The Washington Post) In some ways, the Democrats have faced worse moments. Republicans scored three huge presidential victories in 1980, 1984 and 1988, winning a total of 1,437 electoral votes to 173 for the Democrats. By 1992, voters were turning out an incumbent Republican president, George H.W. Bush, and handing the presidency to Bill Clinton, a Democrat. By 1996, Clinton was winning voters without college educations by 14 percentage points — an almost complete reversal of Reagan’s strength from 12 years earlier. Clinton was part of a larger effort to assess the party’s problems, with deep research by pollster Stanley Greenberg into why working-class voters in Macomb County, Michigan, were defecting and with much of the intellectual group known as the Democratic Leadership Council. It was left to Clinton, however, to synthesize all this into a compelling package, one that sought to de-emphasize the party’s most significant vulnerabilities, though not abandoning them wholesale, and finding new policies and ideas designed to change the image of the party in ways that would bring voters back. The losses Democrats suffered on Tuesday were not of the numerical magnitude of the Republican victories in the 1980s. Harris lost Wisconsin by less than a percentage point, Michigan by less than 2 points, Pennsylvania by 3 points. Trump’s electoral college majority, after he was projected Saturday to have won in Arizona, the last state to be called, is 312, smaller than both of Obama’s totals in 2008 and 2012. But that doesn’t mean the Democrats’ problems are not severe. In an era when there are few swing states, when most are either solidly blue or solidly red, winning working-class voters — whether Whites in places like those three Northern states or Latinos in the Southwest — is critical. And at the same time, Democrats must energize Black voters in the cities and suburban voters as well. party must do to win back more working-class voters. For some on the left, Sen. Bernie Sanders (I-Vt.) being the leading proponent, that means a more robust and populist economic program. Sanders lost that fight to Biden in the 2020 primaries, but the party’s progressive wing remains robust. For others, however, it means hewing even more to center left policies, a recognition that many voters judged Harris as too far left for their tastes. Supporters are seen cheering through bulletproof glass as former president Donald Trump speaks at a campaign rally in Lititz, Pennsylvania, on Nov. 3. (Jabin Botsford/The Washington Post) Trump and his campaign seized on immigration. They lied about Haitian migrants eating people’s pets in Springfield, Ohio. Democrats were understandably outraged. But they underestimated broader concerns of voters about what had been a surge of undocumented immigrants coming into the country. When Republican governors sent busloads of migrants to northern cities, Democratic officeholders had to plead with the White House to help them deal with the strain on resources. The Trump campaign also exploited the Democrats’ embrace of identity politics and specifically for transgender rights with an ad attacking Harris for advocating for transgender surgeries for prison inmates paid for by taxpayers. The tagline on the ad said, “Kamala’s for they/them President Trump is for you ” The by too many voters, lest they leave themselves vulnerable to just this kind of attack. Clinton’s New Democrat formulation is a thing of the past. Some of the policies from that era have been rejected, especially some of the tough-on-crime measures that resulted in massive incarcerations of Black men. What worked then is not a prescription for what will work now. The country has changed, and the Democratic Party has changed. The Democratic Party is centered more than ever in big urban centers and has lost touch with too many voters in rural areas. Its coalition is more diverse and more difficult to unite. It has lost some of its economic identity while identity politics of a different kind have taken on more significance. What is most valuable to remember as Democrats begin this period of self-reflection is that what took place many years ago was an intellectual exercise with a political objective. The product was also something that did not enjoy immediate acceptance. Clinton had to formulate it and then test it in the real world of politics against opposition from others in the party, in a competition of ideas before he could take it to the country as a whole. Too often in these moments, political parties look for an easy fix — a new message or a not sufficient. The problems Democrats now confront are complicated and years in the making. Listening is the first step — understanding why many people were willing or even wanted to vote for Trump, given his obvious flaws — and building from what they learn. Only after that can they begin to regain the trust needed to be able to compete in places where they have been left behind. By Dan Balz Dan Balz is chief correspondent at The Washington Post. He has served as the paper’s deputy national editor, political editor, White House correspondent and Southwest correspondent. @danbalz What the data says about crime in the U.S. Author: Gramlich, John, Date: 2024-04-24 Collections: Hot Takes US Elect 2024 Zotero Key: RICMLYEF Cite Key: Gramlich24crimeWhatDataSays Zotero Item | Lit Note A growing share of Americans say reducing crime should be a top priority for the president and Congress to address this year. Around six-in-ten U.S. adults (58%) hold that view today, up from 47% at the beginning of Joe Biden’s presidency in 2021. With the issue likely to come up in this year’s presidential election, here’s what we know about crime in the United States, based on the latest available data from the federal government and other sources. How much crime is there in the U.S.? It’s difficult to say for certain. The two primary sources of government crime statistics – the Federal Bureau of Investigation (FBI) and the Bureau of Justice Statistics (BJS) – paint an incomplete picture. The FBI publishes annual data on crimes that have been reported to law enforcement, but not crimes that haven’t been reported. Historically, the FBI has also only published statistics about a handful of specific violent and property crimes, but not many other types of crime, such as drug crime. And while the FBI’s data is based on information from thousands of federal, state, county, city and other police departments, not all law enforcement agencies participate every year. In 2022, the most recent full year with available statistics, the FBI received data from 83% of participating agencies. BJS, for its part, tracks crime by fielding a large annual survey of Americans ages 12 and older and asking them whether they were the victim of certain types of crime in the past six months. One advantage of this approach is that it captures both reported and unreported crimes. But the BJS survey has limitations of its own. Like the FBI, it focuses mainly on a handful of violent and property crimes. And since the BJS data is based on after-the-fact interviews with crime victims, it cannot provide information about one especially high-profile type of offense: murder. All those caveats aside, looking at the FBI and BJS statistics side-by-side does give researchers a good picture of U.S. violent and property crime rates and how they have changed over time. In addition, the FBI is transitioning to a new data collection system – known as the National Incident-Based Reporting System – that eventually will provide national information on a much larger set of crimes, as well as details such as the time and place they occur and the types of weapons involved, if applicable. Which kinds of crime are most and least common? Property crime in the U.S. is much more common than violent crime. In 2022, the FBI reported a total of 1,954.4 property crimes per 100,000 people, compared with 380.7 violent crimes per 100,000 people. By far the most common form of property crime in 2022 was larceny/theft, followed by motor vehicle theft and burglary. Among violent crimes, aggravated assault was the most common offense, followed by robbery, rape, and murder/nonnegligent manslaughter. BJS tracks a slightly different set of offenses from the FBI, but it finds the same overall patterns, with theft the most common form of property crime in 2022 and assault the most common form of violent crime. How have crime rates in the U.S. changed over time? Both the FBI and BJS data show dramatic declines in U.S. violent and property crime rates since the early 1990s, when crime spiked across much of the nation. Using the FBI data, the violent crime rate fell 49% between 1993 and 2022, with large decreases in the rates of robbery (-74%), aggravated assault (-39%) and murder/nonnegligent manslaughter (-34%). It’s not possible to calculate the change in the rape rate during this period because the FBI revised its definition of the offense in 2013. The FBI data also shows a 59% reduction in the U.S. property crime rate between 1993 and 2022, with big declines in the rates of burglary (-75%), larceny/theft (-54%) and motor vehicle theft (-53%). Using the BJS statistics, the declines in the violent and property crime rates are even steeper than those captured in the FBI data. Per BJS, the U.S. violent and property crime rates each fell 71% between 1993 and 2022. While crime rates have fallen sharply over the long term, the decline hasn’t always been steady. There have been notable increases in certain kinds of crime in some years, including recently. In 2020, for example, the U.S. murder rate saw its largest single-year increase on record – and by 2022, it remained considerably higher than before the coronavirus pandemic. Preliminary data for 2023, however, suggests that the murder rate fell substantially last year. How do Americans perceive crime in their country? Americans tend to believe crime is up, even when official data shows it is down. In 23 of 27 Gallup surveys conducted since 1993, at least 60% of U.S. adults have said there is more crime nationally than there was the year before, despite the downward trend in crime rates during most of that period. While perceptions of rising crime at the national level are common, fewer Americans believe crime is up in their own communities. In every Gallup crime survey since the 1990s, Americans have been much less likely to say crime is up in their area than to say the same about crime nationally. Public attitudes about crime differ widely by Americans’ party affiliation, race and ethnicity, and other factors. For example, Republicans and Republican-leaning independents are much more likely than Democrats and Democratic leaners to say reducing crime should be a top priority for the president and Congress this year (68% vs. 47%), according to a recent Pew Research Center survey. How does crime in the U.S. differ by demographic characteristics? Some groups of Americans are more likely than others to be victims of crime. In the 2022 BJS survey, for example, younger people and those with lower incomes were far more likely to report being the victim of a violent crime than older and higher-income people. There were no major differences in violent crime victimization rates between male and female respondents or between those who identified as White, Black or Hispanic. But the victimization rate among Asian Americans (a category that includes Native Hawaiians and other Pacific Islanders) was substantially lower than among other racial and ethnic groups. The same BJS survey asks victims about the demographic characteristics of the offenders in the incidents they experienced. In 2022, those who are male, younger people and those who are Black accounted for considerably larger shares of perceived offenders in violent incidents than their respective shares of the U.S. population. Men, for instance, accounted for 79% of perceived offenders in violent incidents, compared with 49% of the nation’s 12-and-older population that year. Black Americans accounted for 25% of perceived offenders in violent incidents, about twice their share of the 12-and-older population (12%). As with all surveys, however, there are several potential sources of error, including the possibility that crime victims’ perceptions about offenders are incorrect. How does crime in the U.S. differ geographically? There are big geographic differences in violent and property crime rates. For example, in 2022, there were more than 700 violent crimes per 100,000 residents in New Mexico and Alaska. That compares with fewer than 200 per 100,000 people in Rhode Island, Connecticut, New Hampshire and Maine, according to the FBI. The FBI notes that various factors might influence an area’s crime rate, including its population density and economic conditions. What percentage of crimes are reported to police? What percentage are solved? Most violent and property crimes in the U.S. are not reported to police, and most of the crimes that are reported are not solved. In its annual survey, BJS asks crime victims whether they reported their crime to police. It found that in 2022, only 41.5% of violent crimes and 31.8% of household property crimes were reported to authorities. BJS notes that there are many reasons why crime might not be reported, including fear of reprisal or of “getting the offender in trouble,” a feeling that police “would not or could not do anything to help,” or a belief that the crime is “a personal issue or too trivial to report.” Most of the crimes that are reported to police, meanwhile, are not solved, at least based on an FBI measure known as the clearance rate. That’s the share of cases each year that are closed, or “cleared,” through the arrest, charging and referral of a suspect for prosecution, or due to “exceptional” circumstances such as the death of a suspect or a victim’s refusal to cooperate with a prosecution. In 2022, police nationwide cleared 36.7% of violent crimes that were reported to them and 12.1% of the property crimes that came to their attention. Which crimes are most likely to be reported to police? Which are most likely to be solved? Around eight-in-ten motor vehicle thefts (80.9%) were reported to police in 2022, making them by far the most commonly reported property crime tracked by BJS. Household burglaries and trespassing offenses were reported to police at much lower rates (44.9% and 41.2%, respectively), while personal theft/larceny and other types of theft were only reported around a quarter of the time. Among violent crimes – excluding homicide, which BJS doesn’t track – robbery was the most likely to be reported to law enforcement in 2022 (64.0%). It was followed by aggravated assault (49.9%), simple assault (36.8%) and rape/sexual assault (21.4%). The list of crimes cleared by police in 2022 looks different from the list of crimes reported. Law enforcement officers were generally much more likely to solve violent crimes than property crimes, according to the FBI. The most frequently solved violent crime tends to be homicide. Police cleared around half of murders and nonnegligent manslaughters (52.3%) in 2022. The clearance rates were lower for aggravated assault (41.4%), rape (26.1%) and robbery (23.2%). When it comes to property crime, law enforcement agencies cleared 13.0% of burglaries, 12.4% of larcenies/thefts and 9.3% of motor vehicle thefts in 2022. Are police solving more or fewer crimes than they used to? Nationwide clearance rates for both violent and property crime are at their lowest levels since at least 1993, the FBI data shows. Police cleared a little over a third (36.7%) of the violent crimes that came to their attention in 2022, down from nearly half (48.1%) as recently as 2013. During the same period, there were decreases for each of the four types of violent crime the FBI tracks: • Police cleared 52.3% of reported murders and nonnegligent homicides in 2022, down from 64.1% in 2013.* • They cleared 41.4% of aggravated assaults, down from 57.7%.* • They cleared 26.1% of rapes, down from 40.6%.* • They cleared 23.2% of robberies, down from 29.4%.* The pattern is less pronounced for property crime. Overall, law enforcement agencies cleared 12.1% of reported property crimes in 2022, down from 19.7% in 2013. The clearance rate for burglary didn’t change much, but it fell for larceny/theft (to 12.4% in 2022 from 22.4% in 2013) and motor vehicle theft (to 9.3% from 14.2%). Note: This is an update of a post originally published on Nov. 20, 2020. Biden’s fateful decision to run in 2024 will be part of his legacy Author: Balz, Dan, Date: 2025-01-12 Collections: Hot Takes US Elect 2024 Zotero Key: Y6A4WVSL Cite Key: Balz25bidenDecRunLegacy Zotero Item | Lit Note “Who knows what I’m going to be like when I’m 86 years old?” That was the answer President Joe Biden gave to Susan Page, Washington bureau chief for USA Today, in an exit interview published last week. The interview focused mostly on Biden’s record and legacy, but near the end, Page asked the president two brief questions. The first was whether he thought he could have won the election in November, had he continued his candidacy against President-elect Donald Trump. Biden said he believed he would have prevailed. Her second question was an important follow-up: “Do you think you would’ve had the vigor to serve another four years in office?” “I don’t know,” Biden said. As he answered the second question, the president circled back to the 2020 election. He told Page that he had run that year because he thought he had the best chance among all the Democrats to beat Trump. (He also has said he was motivated to run after seeing the neo-Nazi rally in Charlottesville in 2017 that left one young woman dead after a white supremacist drove his car into a group protesting the event.) But he added this in answering the question about his vigor: “I also wasn’t looking to be president when I was 85 years old, 86 years old,” he said. “And so, I did talk about passing the baton. But I don’t know. Who the hell knows? So far, so good. But who knows what I’m going to be like when I’m 86 years old?” In the USA Today interview, Biden conceded mistakes — among them that he failed to sign his name to covid checks sent out by the government. His allies acknowledge that he was not built for this era of politics and media, that his old-school style made it more difficult for him to reach voters and make the best case for himself. He has much to point to, from an economic recovery out of the pandemic that, despite inflation, is the envy of other countries; to bipartisan legislation to rebuild America’s infrastructure and its semiconductor capacity; to investments aimed at combating the effects of the changing climate. As Biden noted, much of this has not been seen or felt by most voters yet. Many of these projects could take some years to show tangible results. That’s the record of a single term, a mirror on what has been. The question of whether Biden should have run for a second term is different, and it has been a central part of the discussion among Democrats for the past two years. Alsolooming is the question of whether Democrats would have fared any better against Trump if he had chosen not to run and Democrats had held a contested primary that would have begun early in 2023. The latter is one of those unanswerable “What if?” questions of history. Nonetheless, it is pertinent, given all the discussions that surrounded Biden’s decision to seek a second term and later the efforts by Democratic Party leaders to persuade him to get out of the race and yield to Vice President Kamala Harris in what turned out to be a losing campaign. The issue of Biden’s physical and mental capacity was front and center for months as the presidential campaign took shape. It became unavoidable after the president’s disastrous performance in the June 27 CNN debate in Atlanta. Biden’s critics have claimed that White House officials and some in the media covered up for Biden’s incapacities for months, if not years. Whenever the issue of his physical or mental acuity arose, whenever he seemed to falter in public, White House officials and other close advisers said consistently that the president was fully on his game, sharp and in charge, and that they saw no noticeable decline in his fitness to serve as president. A lengthy post-election article in the Wall Street Journal described the efforts by advisers to manage what the story called Biden’s “limitations.” That followed a pre-election article in the Journal quoting various politicians, many of them Republicans, as saying they saw evidence of a diminished president. That article drew a vociferous denunciation from the White House. Biden listens during a meeting Thursday about the federal response to the Los Angeles wildfires. (Sarah L. Voisin/The Washington Post) History could help to resolve the question of what advisers saw, knew and did during Biden’s term as president, but that will have to await the release of internal White House documents and records, which won’t be made public for many years. Post-presidency books by journalists or the administration could shed some light on the question in the near term. As to the president’s capacities, however, the differences between the Biden who is preparing to leave office in little more than a week and the Biden of four or eight years ago are evident to all who watch him. It is difficult to interpret exactly what Biden meant in his answer to Page about whether he thought he would have had the vigor to serve another four years. A simple “I don’t know” from someone who is 82, trying to project how he might feel in four years, is honest. But the answer also does not clearly acknowledge that he believes he would not have the stamina to do the job until he reaches 86. Still, there is a belief among many Democrats that Biden never should have sought a second term — that he should have held true to his pledge in the 2020 campaign to be a transitional figure. That statement was widely interpreted to mean that he intended to serve for only four years before yielding the stage to a younger generation of leaders in the Democratic Party. History and hubris interceded. In the late summer and fall of 2022, it looked as if Biden might face a primary challenge if he said he was running again. Despite legislative successes in his first two years and the assembling of a coalition to aid Ukraine after Russia had invaded, his approval ratings were low and anxieties over inflation and concerns about the flow of undocumented immigrants coming across the U.S.-Mexico border were growing. Had Democrats suffered the typical midterm election losses that historically befall a party that had just won the White House, pressure on Biden not to run probably would have increased significantly. Instead, in part because of the backlash against the Supreme Court’s decision to end the constitutional right to abortion months before the election, Democrats did far better than expected in those midterms. Instead of facing pressure to not run again, Biden was able to celebrate as a president who had defied the odds, and Democrats joined in toasting his success. That, along with his belief that he was again best equipped to take on and defeat Trump in 2024, made the decision to run again easy. Whether Biden gave serious thought as to whether he then had or would have years into the future the vigor to serve a second term is something only he, first lady Jill Biden and perhaps a few other close family members know. They could not have been indifferent to all the evidence that Biden’s age was a principal concern among many voters. Long before the Atlanta debate, distressed Democratic insiders lamented the president’s decision to run as a sign of stubbornness and selfishness. Biden’s decision to run thus robbed the party of an open primary contest that would have elevated a different candidate to lead Democrats in the campaign and set the stage for the chaotic events that followed the June debate, including the anointing of Harris as the nominee. The eventual outcome might have been preordained regardless of who the Democrats had as their nominee, given public concerns about inflation and general dissatisfaction with the state of the country. But the absence of open competition for the nomination meant only Harris, in a shortened campaign, got the chance to prove otherwise — and without having dispatched potential rivals to win the nomination. Biden repeated, during a session with reporters Friday, his belief that “he would have beaten” Trump had he remained a candidate, but that he quit the race to unify the party. Is there a case that Biden would have won? In terms of the total number of votes, Harris performed worse in 2024 than Biden in 2020, while Trump performed better in 2024 than he did in 2020. Biden got 81 million votes in 2020; Harris got 75 million in 2024. Trump won 74 million in 2020 and increased that to 77 million in 2024. Nearly all the difference in Harris’s lower totals came in the non-battleground states. In those states plus the District of Columbia, her vote totals were about 6 million fewer than Biden’s in 2020. That includes dramatic declines in several deep-blue states. For example, she received about 1.8 million fewer votes in California, about 600,000 fewer in New York and about 400,000 fewer in both Illinois and New Jersey. In the seven battleground states, where the Harris-Walz campaign concentrated its efforts, she was only about 50,000 votes below Biden’s totals in 2020, though she lost them all while Biden won six of the seven in 2020. The reason Trump swept the battlegrounds was because of his ability to expand on his 2020 support. He won about 950,000 more votes in those battleground states in 2024. Harris’s defeat was due not only to questions about her own record and positions but also to voters’ frustrations with inflation and immigration — mistakes by Biden that would have affected his candidacy had he stayed in. On immigration, the president was slow to act. On inflation, he failed to address it directly and tried to focus voters on jobs and overall growth rather than the pain they felt from higher grocery and gas prices. The record of his administration, as admirable as parts of it might have been, ultimately was judged harshly by voters in November. On Election Day, the president’s approval rating stood at 40 percent, according to network exit polls. It’s difficult to imagine any sitting president winning reelection under those circumstances, especially at a time when governments around the world were being turned out by disgruntled voters. Biden will deliver a farewell address on Wednesday evening and have another opportunity to extol his accomplishments and assess the state of the country. That record will be an important part of his legacy. That he chose to run again in 2024 in the face of voters’ broad discontent and on top of the specific concerns they had about his age and capacity will also be a part of that legacy. How Kamala Harris Burned Through $1.5 Billion in 15 Weeks Author: Goldmacher, Shane, Date: 2024-11-17 Collections: CampaignMoney Zotero Key: X9WP37PR Cite Key: Goldmacher24KharrisBillions Zotero Item | Lit Note Vice President Kamala Harris spent a remarkable $1.5 billion in her hyper-compressed 15-week presidential campaign. But in the days since losing to President-elect Donald J. Trump, her operation has faced questions internally and externally over where exactly all that cash went. Despite her significant financial advantage, Ms. Harris became the first Democratic presidential candidate to lose the national popular vote in two decades, ceding every battleground state to Mr. Trump. Her cash-rich campaign spared no expense as it hunted for voters — paying for an avalanche of advertising, social-media influencers, a for-hire door-knocking operation, thousands of staff, pricey rallies, a splashy Oprah town hall, celebrity concerts and even drone shows. It was a spree that averaged roughly $100 million per week. The frenzied spending has led to second-guessing among some Democrats, including whether investing in celebrity-fueled events with stars such as Lady Gaga and Beyoncé was more ostentatious than effective. Since her loss, the Harris operation has pressed supporters for more cash with desperate-sounding solicitations, stirring fears about post-election debts. “Is there anything we can say?” came one email asking for cash last Monday. The biggest expense during the race was advertising. Between July 21 and Oct. 16, financial records show that the Harris campaign spent $494 million on producing and buying media, a category that includes both television and digital ads. The total sum through the election is said to be closer to $600 million. Yet starting in October, her campaign was actually narrowly outspent on broadcast television by Mr. Trump, according to data from the ad-tracking service AdImpact. The ads were just one piece of a campaign that had enough cash to spend on seemingly everything. There was $2.5 million directed toward three digital agencies that work with online influencers, records show. The campaign spent around $900,000 to book advertising on the exterior of the Sphere venue in Las Vegas in the last week of the race, two officials said. There were drone shows in the sky before the debate in Philadelphia in September and at a Pittsburgh Steelers game in October. In a note on Friday to Ms. Harris’s top fund-raisers, Chris Korge, the Democratic National Committee’s finance chair, said that losing all seven battleground states had “shocked us all.” “I will absolutely push for an introspective study and analysis of the campaign, its structure, its messaging, all communication platforms and budgeting,” Mr. Korge wrote. Given the magnitude of Ms. Harris’s loss, more of the focus so far has been on the Democratic brand and message rather than the mechanics of her operation. Ms. Harris inherited a campaign based in inconvenient Wilmington, Del., that was built for President Biden, and she had limited time to refashion it to better suit her strengths. Ms. Harris added some senior advisers but mostly kept the Biden team in place, including Jennifer O’Malley Dillon, the powerful campaign chair who oversaw the finances and virtually every major move. The campaign’s spending decisions were documented in Federal Election Commission records and interviews with 15 Harris campaign officials and close allies, most of whom insisted on anonymity to discuss internal finances and dynamics candidly. Many of the financial figures in this article are from the latest campaign reports; some are from Harris officials with knowledge of the spending. All told, the Biden and Harris campaigns collectively raised about $2.15 billion, two people said. It is not clear exactly how much Mr. Trump spent though it was far less. Mr. Trump and the Republican Party together raised $1.2 billion, one person with knowledge of the figure said. Even in defeat, there were some signs of the effectiveness of Ms. Harris’s spending: She performed stronger in the battleground states than nationally. Some Harris aides and allies have taken a strange sort of solace in the scope of her defeat as Mr. Trump captured 312 electoral votes — far more than the 270 needed to win. “There is not a single expenditure in a different spot that would have changed the outcome of the race,” said Bakari Sellers, a close ally of Ms. Harris and a former lawmaker in South Carolina. In fact, Mr. Sellers said, the campaign faced an unusual problem: “We had so much money it was hard to get it out the door.” Patrick Stauffer, the campaign’s chief financial officer, said in a statement that there had been no outstanding debts or overdue bills as of Election Day. He said that “there will be no debt” on the next Democratic National Committee and Harris for President campaign filings in December. Donations made after the election to the “Harris Fight Fund” are being funneled to the Democratic National Committee, officials said. In recent days, the committee has shed hundreds of staff members, an expected downsizing after the defeat. The party had a payroll of roughly 680 in October and is shrinking by roughly 70 percent, according to two people familiar with the cutbacks. A D.N.C. official said 95 percent of those being let go had a post-election end date in their offer letter. Still, the reductions were symbolic of the boom-and-bust of elections — and the severity of the bust in defeat. “We are prepared to lead the fight against Donald Trump into the future,” said Rosemary Boeglin, the communications director for the Democratic National Committee. Mr. Trump himself mocked the Harris team for its financial situation in a recent social-media post: “Whatever we can do to help them during this difficult period,” he offered. One particular Harris payment has drawn attention in the aftermath of the election: the $1 million paid to Oprah Winfrey’s production firm, Harpo Productions. In an Instagram post, Ms. Winfrey said the company was paid to stage a live-streamed town hall in Detroit, providing the set, lights, cameras, microphones, crew, producers and even the chairs. “I did not take any personal fee,” Ms. Winfrey wrote. “However the people who worked on that production needed to be paid. And were. End of story.” The $1 million actually undercounts the full cost of the event, which ran closer to $2.5 million, according to two people briefed on the matter. Another pricey choice was holding swing-state rallies featuring star performers on the eve of the election, including Lady Gaga in Philadelphia, Jon Bon Jovi in Detroit, Christina Aguilera in Nevada, James Taylor in North Carolina and Katy Perry in Pittsburgh. The singers themselves were not compensated, officials said, but the support staff was. The overall bill for the election-eve rallies exceeded the planned budget and is said to have topped $10 million. The cost overruns were partly because the Harris team built an entire rally venue at a park in Pittsburgh only to be told by the Secret Service that the site could not be properly secured. They had to rush to take it down and rebuild at a second venue. “Because of Vice President Harris’s unparalleled fund-raising prowess,” Mr. Stauffer said, “we were able to run an aggressive all-of-the-above strategy to reach voters, keeping the seven battleground states incredibly close.” Even as Ms. Harris ran notably stronger in battlegrounds such as Pennsylvania, Georgia and North Carolina than in surrounding areas, those results were double-edged, politically. They suggested that the ticket she led was so unpopular that it took an enormous campaign just to limit her losses. Though Ms. Harris had been on the ticket from the start, her advisers discovered that the Biden operation had done virtually no research on her strengths and weaknesses. Her operation spent more than $12 million on polling from July 21 to mid-October, records show. Other major costs, according to records and campaign officials, included $111 million in online ads seeking donations, at least $100 million transferred to battleground-state parties, $70 million on mail and nearly $28 million to produce the merchandise that people were ordering. And for all the focus on her volunteer program, the campaign spent a significant sum — about $50 million — for paid door-to-door canvassers. In an Oct. 16 memo, the leading super PAC supporting Ms. Harris raised alarms about being outspent on television. The group, Future Forward, said in the memo, which was first reported by The Washington Post, that it would be “difficult for anyone” but the Harris team to close the gap because of the higher ad rates that super PACs pay. It was hardly Future Forward’s only frustration. Another memo, issued days later, pointed out “very high-performing ads that have yet to get a big spend.” One ad, Future Forward said, had ranked in the “100th percentile” — meaning it was the most effective — yet it had virtually never been aired. Campaign officials, meanwhile, were frustrated that Future Forward sat on so much of its money until the final weeks, forcing the campaign to spend more on the airwaves earlier. Another Harris challenge: After raising $1 billion in less than three months, a bevy of consultants, allies and others were often angling for a cut, including the chairman of the Democratic Party in Philadelphia. In September, the Harris operation contributed almost $25 million to other party committees, in part to quiet those demands. Some media allies of Ms. Harris were also paid. Areva Martin, who hosts a talk show, was paid $200,000 as a media consultant, and she went on a battleground-state tour in October. Roland Martin, who hosts his own streaming programming and runs a media company called Nu Vision Media, received $350,000 in September for a “media buy” that he said was for advertising. “It should have been a hell of a lot more,” Mr. Martin said in a brief interview. “More should have been spent on Black-owned media.” Mr. Martin interviewed Ms. Harris in October. Ms. Harris’s campaign also made two $250,000 donations to National Action Network, the organization led by the Rev. Al Sharpton. Mr. Sharpton interviewed Ms. Harris on MSNBC in October. As Ms. Harris faced questions about relative weakness among Black voters, her campaign gave $2 million in late September to the National Urban League. One of the unanswered questions is who exactly made money off the commissions on Ms. Harris’s advertising, which can be especially lucrative. Such payments are often hidden even in federal disclosures. In 2020, for instance, Mike Donilon, who was one of Mr. Biden’s top strategists, reported on his personal financial disclosure form with the White House that his consultancy had earned $4.35 million in 2020, far more than the roughly $543,000 disclosed to the Federal Election Commission in payments to his firm. Numerous firms could have netted big commissions from the Harris campaign. Four companies received at least $90 million in payments as of mid-October, including one firm whose cumulative receipts from the Harris campaign approached $300 million. 10 Takeaways From the Night Trump Marched Back to the White House Author: Epstein, Reid J., Date: 2024-11-06 Collections: Hot Takes US Elect 2024 Zotero Key: 2ZQELH9Q Cite Key: Epstein24takeawaysNight Zotero Item | Lit Note Donald Trump has won the 2024 presidential election. Follow live updates here. Americans have voted former President Donald J. Trump back into the nation’s highest office four years after he fomented a riot at the Capitol to try to block his removal from power. His election is likely to again place the country’s democracy under enormous stress. For the last decade, he has demonstrated that he has little regard for the checks and balances that have defined American government since the dawn of the republic. Now, after Mr. Trump’s defeat of Vice President Kamala Harris, the nation faces momentous changes certain to cut across political and cultural lines. Republicans demonstrated strength all over the map and up and down the ballot. They seized control of the Senate and could retain the House. Winning both would give Mr. Trump an important source of legislative power. As it was during Mr. Trump’s first term, the nation is set to be governed at the whims of a president with little interest in the details of policy. But under the government he assembles, major issues like abortion rights, taxation, immigration and foreign policy will be pushed hard to the right, from not only legislation and executive orders but also the inevitable appointment of Trump-friendly judges and, potentially, more Supreme Court justices. Here are 10 takeaways from an election that again upends American politics. America’s democracy is likely to be put under tremendous strain. Mr. Trump has already sought to undermine the independence of the justice system. During his first term, he demanded personal loyalty from officials across the executive branch and fired those who resisted his demands. Now he returns to the White House with the knowledge of how the system restricted his impulses and with a roster of officials more willing to help him circumvent longstanding norms. The last time he took office, a culture of resistance emerged from corners that included Democrats, the federal government’s civil servants and even some of his own appointees. This time, he is unlikely to elevate people willing to speak against him. His circle of influential allies now includes men like Senator JD Vance of Ohio, the vice president-elect; the tech billionaire Elon Musk; and Robert F. Kennedy Jr., the conspiracy theorist who has long cast doubt on the established science of vaccines. Mr. Trump will be a president who has already whipped up political violence and who promised to use the United States military against his domestic opponents, a prospect that may chill public resistance to his boundary-breaking ambitions. The country, and its foreign allies, are in for major whiplash. The relative stability on domestic and international affairs during the last four years is about to be gone, replaced by a volatile president who often operates without regard to national precedent. Mr. Trump has praised the authoritarian leaders of China, North Korea and Russia while demeaning democratic American allies in Europe and Asia. Whether the United States remains part of NATO is a live question. Aid to Ukraine as it struggles to fight off Russia’s invasion is in peril. And the Mideast conflict will have a powerful, unpredictable new actor who has not demonstrated an interest in calming tensions. And then there are the domestic issues that will turn on a dime. Mr. Trump campaigned in front of signage that read “deport illegals now.” He has promised a 20 percent tariff on imported goods. His allies have said a Trump administration could severely restrict or try to outright ban abortion. And Mr. Trump will have an opportunity to appoint — and have a Republican-controlled Senate confirm — scores more federal judges and perhaps more Supreme Court justices, if seats open up. That would lock in conservative control of the judicial system for generations. Trump remains the nation’s most durable political force. Ms. Harris said over and over that her campaign was designed to “turn the page” on Trump-era politics. Now the Trump story may have years more to go. While the former president’s party lost or underperformed in every major election since he won in 2016, he remained the most powerful force in American politics. He beat back every Republican who challenged him, and the Democrat who managed to defeat him in 2020 defined much of his presidency as being an antidote to Mr. Trump. In the meantime, Mr. Trump was criminally indicted four times, convicted once, was found liable for sexual assault, lied enough times to keep an army of fact-checkers employed, and spent the final days of his campaign giving a platform to racist jokes and using threatening language toward his political opponents, one of whom he suggested should have guns shooting at her. Enough Americans looked past it all to send him back to the White House. Kamala Harris could not outrun Joe Biden. The recriminations of Ms. Harris’s poor showing will be swift for President Biden, who despite his legislative accomplishments remained unpopular with voters. Americans widely judged him too old for a second term and saw him as a poor steward of the economy and the country’s southern border. For more than a year, top Democrats including Ms. Harris rejected what the party’s voters were telling them: The octogenarian Mr. Biden should not run again. Updated Nov. 7, 2024, 7:45 p.m. ETNov. 7, 2024 While Ms. Harris shot the ticket back into contention after replacing Mr. Biden, he remained a political albatross. She struggled to articulate how she would be different, and Mr. Biden proved to be such an unsteady surrogate that Democrats winced whenever he ventured onto the campaign trail. Whether Ms. Harris’s struggles stemmed more from her decisions or from the situation Mr. Biden left her in will be a subject of fierce debate as Democrats fling accusations of blame in the coming days, weeks and months. America will keep waiting for a female president. Hillary Clinton made the prospect of breaking what she once called “the highest, hardest glass ceiling” a centerpiece of her 2016 campaign. Expecting to win, she held her election-night event at the Javits Center in New York, under an actual glass ceiling. Ms. Harris did not make her gender or her race — she is the daughter of a Jamaican father and a South Asian mother — central to her abbreviated campaign as she sought to disqualify Mr. Trump and present herself as the face of a new generation of leadership. Neither approach has worked against Mr. Trump. Trump’s federal criminal trials are imperiled. Mr. Trump’s Justice Department is likely to drop the federal charges against him in his classified documents and election interference cases. He has already said he would fire Jack Smith, the special counsel who has led the federal investigations and prosecutions into the former president over the last two years. There is not much doubt that a Trump-appointed attorney general would drop the charges shortly after being confirmed. Mr. Trump still faces a sentencing hearing this month in Manhattan for his felony convictions from this spring, though how that might change now that he is president-elect remains to be seen. Trump will have a Republican Senate, and possibly a G.O.P. House. Republicans will control the Senate and could retain the House, though it remains up for grabs and may not be called for days. A Republican trifecta would give Mr. Trump free rein to carry out his policies and leave Democrats unable to mount the sort of congressional investigations that often unearth politically damaging revelations. Unlike when Mr. Trump first entered office in 2017, he will inherit a Congress full of members in his image who endorse his style and politics. While the Republican Senate majority still has two moderates in Susan Collins of Maine and Lisa Murkowski of Alaska, Mr. Trump might not need their votes to pass sweeping right-wing policies in the new Senate. The contest for House control remains a seesaw battle, with key races in California, Oregon and Washington still uncalled. Democrats will almost certainly flip some Republican seats, but Ms. Harris’s struggles in industrial Pennsylvania and Michigan will cost her party — and could possibly preserve the slender Republican majority in the lower chamber. A brief era of good feelings is over for Democrats. When Mr. Biden was finally forced out after his poor debate performance, the party had a collective moment of joy as it coalesced around Ms. Harris. For just over 100 days, there was little dissent as she executed a controversy-free campaign with few evident mistakes. Fund-raising surged, and the party cheered as she built a coalition that stretched from Liz Cheney to Bernie Sanders. But there is nothing like a poor electoral showing to ignite an intraparty civil war. The entire country moved to the right. Mr. Trump won Florida by roughly 13 percentage points — 10 more than in 2020. Texas also moved 10 points in his favor. He won Ohio by about 11 points, three more than in 2020. Blue states shifted, too. New York moved 13 points toward him, and Virginia shifted six points in his direction. The former president did it while running an erratic and sometimes outwardly racist campaign — marked by two attempts on his life — that made the twin arguments that Mr. Biden and Ms. Harris were responsible for post-pandemic inflation and an unpopular approach to immigration. At the same time, Mr. Trump bet big on anti-transgender advertising while warning that Democratic elites had contempt for his supporters. Ms. Harris and other Democrats largely chose not to answer Mr. Trump’s cultural attacks, believing that abortion rights would motivate enough voters to defeat him again. But even though Mr. Trump appointed the justices who helped overturn Roe v. Wade, the anger that powered surprisingly large Democratic victories did not lead to similar results when Mr. Trump himself was on the ballot. Rural America grew even redder. When Ms. Harris named Gov. Tim Walz of Minnesota as her running mate, part of his remit was to serve as her ambassador to rural areas, where voters would presumably be familiar with his football coach persona and appreciate his upbringing in a tiny Nebraska town. It did not slow the rush of rural America to Mr. Trump. Mr. Walz spent much of his time stumping in small towns while leaving the big arenas in major markets to Ms. Harris. Democrats spoke openly about a lose-by-less campaign in rural areas of the battleground states, but in the end they lost by the same or more while not making up enough ground in suburban counties — and losing some of their advantage in major cities. Jonathan Weisman contributed reporting from New York. Modeling the nonlinear effects of opinion kinematics in elections: A simple Ising model with random field based study Author: Tiwari, Mukesh, Date: 2021-11-15 Collections: NeuroPsychoLinguisticPolitics, MediaAdsPolit Zotero Key: 2FWHHMGF Cite Key: Tiwari21politOpinionKinematics Zotero Item | Lit Note Physica A: Statistical Mechanics and its Applications Volume 582, 15 November 2021, 126287 Modeling the nonlinear effects of opinion kinematics in elections: A simple Ising model with random field based study Mukesh Tiwari a, Xiguang Yang, Surajit Sen b b Show more Share Cite https://doi.org/10.1016/j.physa.2021.126287 Get rights and content Highlights • Sociophysics model with random field for partisan competition. • Mixed model with contrarian, conformist and inflexible agents. • Efficiency of campaigns and strategies for different levels ofEffici dominance. Abstract Inspired by partisan competitions and contentious elections in democratic countries, we numerically explore the effect of campaign strategies and related factors on the opinion of an electorate The nature of the electorate is modeled through agents with different take discrete opinion values that depend on both internal and external influences. The inhomogeneity of external influence on individuals is modeled as a random field. Two types of electorates have been considered. In an electorate with only conformist agents short duration high impact campaigns are highly effective. These are, however, also sensitive to perturbations at the local level modeled as inflexibles and/or absentees. In electorates with both conformist and contrarian agents and varying level of dominance due to local factors, short-term campaigns are effective only in the case of fragile dominance of a single party. Strong local dominance is relatively difficult to influence and long term campaigns with strategies aimed to impact local level politics are seen to be more effective. Introduction Statistical physics inspired models that describe collective phenomena arising from the interaction between the constituents has been an active area of interest in disciplines outside physics, such as in social science [1], [2], [3], [4]. In the field of social science, a fundamental question that these models attempt to study and explore is that of opinion dynamics. The dynamics of opinion change can be typically modeled, as local interactions, over a 2D square lattice or large networks. The nodes of the network, or the cells in the lattice are agents whose values or attributes determine their opinion. In the simplest case, each agent has two choices which are representative of binary opinions. Some of the well studied social interaction rules, that lead to the change in the values and reflect shaping of opinions include the voter model [5], [6], [7], [8], in which each agent acquires the opinion of a randomly selected neighbor, majority vote model [9], [10], [11], [12] where the agent takes the opinion of the local majority and the Sznajd model [13] which studies the influence of a group on individual’s opinion. Besides a diverse set of rules, the opinions can also be continuous [14], [15], or both discrete and continuous [16]. Social impact and the role of individual’s attitude in the formation of the net opinion is however a complex phenomena [17], [18], [19], [20]. In democracies, elections present a platform for people to express their opinions and hence can be considered as one of the largest experiments on the emergence of consensus (partial) and opinion formation. Elections by themselves are, however, complex processes. Recent works on elections have indeed explored through simple statistical physics models the role of these complex interactions [21], [22], [23]. In addition to the social structure and the interaction rules, individual preference, their intrinsic nature and the available information also play important roles in the formation of voter’s choice [24]. It is not others. The process of opinion formation and the distribution of opinions, therefore, depend on the type of traits, such as inflexibles, or those who do not change their opinion easily [25], [26], [27], [28], [29], [30], [31], contrarians [32], [33], [34], [35], [36], whose opinion is against the local majority, influential leader or those with higher influencing ability [37] etc. Political environments and the opinions surrounding them are charged and fluctuating. Political parties through their campaigns, aim to take advantage of this and use media as one of the platforms to disseminate information and their position on various topics. Media therefore acquires an important position and its role and significance in opinion formation has been investigated within various models [38], [39], [40], [41], [42], [43], [44], [45]. Advances made in modes of digital communications have also made social media platforms an important channel for campaigns and communication. The structure of the information diffusion networks results in rapid dissemination of news and information. Controlling and regulating materials being posted on these platforms is not simple, resulting in spread of misinformation and fake news at an alarming rate [46], [47]. While the level of influence on an individual’s opinion is unclear, they do affect their process of decision making [48]. Given the diversity that exists within an electorate, and the complex competition between the different factors that affect decision making, it becomes important to understand their role within different social structures. Moreover, election results are mostly spot measurement of an unsettled many body system. Strategies and their timings may induce modest shift in the opinions which can change the tipping point or introduce instability [49], [50], [51], thereby changing the political outcome. With all this in the background, in this paper we study the influence of campaign strategies on different types of electorates. Our studies have been motivated by some common observations in campaign strategies, their effect on prevailing voter sentiments and the corresponding influence on regional electoral outcomes in some recent elections in democratic countries. These strategies include, extensive and aggressive use of social media, aggressive campaigning, government response in the presence of extreme background events such as COVID, issues of ethnic equality and to national security threats that may evoke a strong emotional response from the voters etc. Given the prevailing state and nature of the electorate we attempt to understand how the different factors influence the voter’s choice. interpretation of the different parameters that are used. In Section 3 we present results of our numerical simulation. First we consider a system consisting of only conformist agents in the presence of random field. Here, we also explore the role of inflexibles and the absentees on a quasi-stationary state that results after the removal of field. We then analyze a mixed model with conformist and contrarian agents. Allowing for different levels of dominance of conformists we examine the efficacy of campaigns. Conclusions are presented at the end. Section snippets Model We model a system where every agent has two choices (two party system). Their opinions represented as discrete values in this paper symbolize their support for a party. We adopt a 2D Ising spin system approach, in which the agents are modeled on a lattice. Each cell in the lattice, may be assumed to be an individual or an agent, and the dynamics of opinion formation is mathematically represented as: is the strength of … Results We present results using Monte Carlo (MC) simulations with periodic boundary conditions. Simulations have been performed on lattice of size 128 × 128, unless mentioned otherwise, using the Metropolis algorithm. In Section 3.1 we first examine the response of a system with only conformist agents to external influence. We then discuss the effect of perturbations on the consensus observed. In Section 3.2 we consider a modified model with both conformist and contrarian agents. Different levels of … Conclusion In this paper, through models of opinion dynamics, we have qualitatively studied the effect of campaign and strategies on the opinion of an electorate. The different scenarios considered here are inspired by what is observed in real populations as they face competitive regional elections. Two main factors that can play a role in shaping of opinions, local and external influence have been considered. At the local level, the prevailing situation Mukesh Tiwari: Conceptualization, Methodology, Software, Investigation, Writing – original draft, Writing – review & editing. Xiguang Yang: Software, Investigation. Surajit Sen: Conceptualization, Methodology, Writing – original draft, Writing – review & editing. … Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. … Acknowledgment The authors would like to thank Prof. Jacob Neiheisel for pointing out the relevance of this work to partisan competition. … Recommended articles References (61) RednerS. Reality-inspired voter models: A mini-review C. R. Phys. (2019) TsallisC. A majority rule model — New finite size scaling J. Magn. Magn. Mater. (1983) GalamS. Application of statistical physics to politics Physica A (1999) MoscoviciS. Toward a theory of conversion behavior Adv. Exp. Soc. Psychol. (1980) AraripeL. et al. Role of parties in the vote distribution of proportional elections GalesicM. et al. Statistical physics models of belief dynamics: Theory and empirical tests Physica A (2019) GalamS. et al. The role of inflexible minorities in the breaking of democratic opinion dynamics Physica A (2007) GalamS. Contrarian deterministic effects on opinion dynamics:“the hung elections scenario” Physica A (2004) GambaroJ.P. et al. The influence of contrarians in the dynamics of opinion formation Physica A (2017) CrokidakisN. Effects of mass media on opinion spreading in the Sznajd sociophysics model Physica A (2012) View more references Cited by (9) Mass media and its impact on opinion dynamics of the nonlinear q-voter model 2024, Physica A: Statistical Mechanics and its Applications Citation Excerpt : …Individuals can change or keep their opinions on a matter due to the influence of the mass media [23]. Other previous works have reported the effect of mass media on various models of opinion dynamics with various scenarios, which can be seen in Refs. [24–41]. The mass media or sometimes referred to as the external field in some studies has shown various effects on the evolution of opinion, depending on the considered scenario.… Show abstract 2022, Physica A: Statistical Mechanics and its Applications Show abstract The external field effect on the opinion formation based on the majority rule and the q-voter models on the complete graph 2023, International Journal of Modern Physics C A bibliometric analysis and basic model introduction of opinion dynamics 2023, Applied Intelligence Unanimity, Coexistence, and Rigidity: Three Sides of Polarization 2023, Entropy Physicists, non physical topics, and interdisciplinarity 2022, Frontiers in Physics View all citing articles on Scopus View full text © 2021 Elsevier B.V. All rights reserved. All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply. Lauren Boebert’s Survival Instincts Author: Hessler, Peter, Date: 2025-01-06 Collections: Hot Takes US Elect 2024, Polarization Zotero Key: U2Y5IT8P Cite Key: Hessler25survivalInstinceBoebert Zotero Item | Lit Note On Election Night of 2024, shortly before nine o’clock, Representative Lauren Boebert ascended a small stage at the Grainhouse, a sports bar in Windsor, Colorado. Windsor, part of the state’s Fourth Congressional District, is situated on Colorado’s agricultural northern plains, and the bar was housed in a massive metal grain bin. A bright-green John Deere 237 corn picker stood next to the stage. Boebert, who had moved to the district earlier in the year, wore a tightly fitted blue suit with red lining, a white shirt, a pair of silver stiletto heels, and a red “Make America Great Again” baseball cap that had been signed on the brim in Sharpie by President Donald Trump. It had been less than two hours since the Colorado polls had closed, and most news organizations would not call the Presidential race until after midnight. But Trisha Calvarese, Boebert’s opponent for a seat in the House of Representatives, had already conceded. “The swamp, they thought I would fail!” Boebert shouted to more than two hundred supporters. “But you all welcomed me to Windsor, Colorado. And, rather than failing, I think it’s kind of like an A-plus with extra credit with this G.E.D. right here!” Two years earlier, Boebert had barely won the closest race in the nation, defending her seat in Colorado’s Third Congressional District by only five hundred and forty-six votes. Then her political prospects, which already looked dim, seemed to worsen because of a tumultuous personal life. After the 2022 election, Boebert got divorced; her ex-husband was arrested twice for domestic altercations; and her oldest son, a teen-ager, was also arrested, having allegedly participated in a string of vehicle break-ins and credit-card thefts. In September, 2023, Boebert herself was kicked out of the Buell Theatre, in Denver, after disturbing other audience members during a musical performance of “Beetlejuice” by vaping, laughing and singing loudly, and engaging in mutual groping with her date. Surveillance video of the incident, which also showed Boebert giving the finger to an usher as she and her date were escorted out, quickly went viral. Three months later, the congresswoman abruptly decided to seek office for a third term in a part of Colorado where she had never lived as an adult. And yet, despite all Boebert’s bad publicity, here she was in Windsor, at the age of thirty-seven, poised to become the senior member of Colorado’s Republican delegation to the House of Representatives. Daily Our flagship newsletter highlights the best of The New Yorker, including top stories, fiction, humor, and podcasts. “My Democrat opponent just called and conceded and asked me to uphold our democracy,” Boebert said from the stage. “And my response was ‘I promise you I will uphold America’s constitutional republic! ’ ” The crowd cheered. Next to me, Fred Mahe, the treasurer of the Weld County Republican Party, shouted, “She’s right! She’s right! It’s not a democracy!” Mahe wore a baseball cap with an image of Trump raising his fist after last summer’s assassination attempt. Another man nearby had a T-shirt that said “I’m Voting for the Felon and the Hillbilly.” One middle-aged woman, immaculately dressed in the colors of the American flag, wore a campaign-style button with the message “Life’s a Bitch—Don’t Vote for One.” Earlier in the evening, several people had told me that they worried about the possibility of George Soros influencing Colorado’s elections. A woman in her sixties, dressed in an “All American Trump Girl” shirt, explained that she worked in Boulder, a liberal bastion, and she feared political violence. “If Trump wins tomorrow, Boulder could be a shit show,” she said. At the end of Boebert’s speech, she introduced her mother, Shawna, who stood nearby, wearing an American-flag-patterned shawl. “Many of you have heard my life story of being raised in a Democrat household,” the congresswoman said. “And it wasn’t because my mom was liberal. It’s because she believed the lies. She believed the lies of politicians, and it entrapped us in a cycle of poverty.” She concluded, “In 2016, my mom voted for Donald J. Trump, and, just like you, she is ready for him to win his third Presidential election! God bless you, Windsor! Thank you so much. We’re gonna fight, fight, fight!” Lauren Boebert’s political career began in the uniquely challenging terrain of Colorado’s Third District. To drive from corner to corner across the district, which is larger than the state of Pennsylvania, takes more than ten hours. It encompasses some of the tallest mountains in the continental United States, as well as vast stretches of high desert. In a hard landscape, geographical features have hard names: Disappointment Valley, Calamity Mesa, Battlement Mesa. Constituent communities include Silt, Stoner, Sawpit, Slick Rock, Bedrock, Marble. Another on-the-nose name is Rifle, Boebert’s home town, where she used to own a restaurant called Shooters Grill, whose waitstaff openly carried firearms. Cartoon by Roz Chast Before Boebert, the district was represented by a Republican named Scott Tipton. His greatest moment of national publicity may have come in January, 2019, when, during a federal-government shutdown, the Onion ran a headline: “Poll Finds 100% of Americans Blame Shutdown Entirely on Colorado Representative Scott Tipton.” A photograph featured the fifth-term congressman wearing a blue suit, a blue tie, and a gentle, slightly hangdog expression. A fictional Pew Research pollster was quoted: “As far as the American people are concerned, Tipton and Tipton alone owns this shutdown.” This joke—the targeting of some random low-profile Republican from an unknown rural district—became a reality of a different sort when Boebert entered the primary, later that year. She was thirty-two, with no political experience, and her family life had often been troubled. Her mother, a high-school dropout, had given birth to her at the age of eighteen, and Boebert never knew her biological father. She has said that her family sometimes relied on welfare, and that her mother had a partner who was abusive. At sixteen, Lauren met the twenty-two-year-old man whom she would eventually marry, Jayson Boebert. Like her mother, Lauren dropped out of high school and got pregnant, having a son at eighteen. For a while, Lauren was a shift manager at a McDonald’s in Rifle. Later, she worked as a pipeline locator for a company that drilled for natural gas, a major local industry. She began devoting herself to born-again Christianity, and she and Jayson had three more sons. Both parents also had police records. In 2004, Jayson allegedly exposed himself to two women in a Colorado bowling alley; he pleaded guilty to the lesser charge of petty offense for public indecency and lewd exposure, spending a few days in jail. That same year, an altercation between the couple resulted in a guilty plea for Jayson on a misdemeanor charge for harassment with a domestic-violence enhancement. After another fight, Lauren was charged with third-degree assault, criminal mischief, and underage drinking. (The outcomes of these charges are not known, because juvenile records in Colorado are automatically sealed.) A few years later, during the financial crisis, the couple lost their home in a foreclosure. In 2013, Lauren and Jayson opened Shooters Grill. Boebert has claimed that she encouraged staff to arm themselves after a man was beaten to death outside the restaurant. But nobody has been able to find records of a murder that matches Boebert’s description. The Colorado Sun reported, in the only such incident it could turn up, that a man had been involved in a fight elsewhere in Rifle, and then ran to within a block of the restaurant, where he died from a methamphetamine overdose. Boebert’s first taste of fame came in September, 2019, at a Colorado town-hall meeting held by the Presidential-primary candidate Beto O’Rourke. O’Rourke had proposed a ban and a buyback program for assault rifles, and Boebert, standing in the audience, challenged him, in an exchange that subsequently appeared on Fox News. “I was one of the gun-owning Americans that heard your speech and heard what you had to say regarding ‘Hell yes, I’m going to take your AR-15s and your AK-47s,’ ” Boebert said. “Well, I am here to say, ‘Hell no, you’re not.’ ” Few people took Boebert seriously when she entered the Republican primary. She had little financial support, and she didn’t receive her G.E.D. until after she declared her candidacy. Tipton chose not to spend several hundred thousand dollars of available campaign funds in the primary. But Boebert proved to be an energetic candidate, accusing Tipton of being soft on immigration and of failing to support Trump to an adequate degree. (In fact, Trump had endorsed Tipton.) After winning the primary handily, Boebert took the seat by six percentage points. As a first-term congresswoman, Boebert developed an often cartoonish national image. During the campaign, she had said of QAnon, “If this is real, it could be really great for our country,” and on January 6th she tweeted, “Today is 1776.” At a Christian conference, Boebert joked that Jesus hadn’t had enough AR-15s “to keep his government from killing him.” She made a series of Islamophobic comments about Representative Ilhan Omar, referring to her during a speech on the House floor as “the Jihad Squad member from Minnesota.” In 2022, when a same-sex-marriage bill passed in the House, Boebert opposed it, explaining that it was part of a progressive cause that “undermined masculinity.” That year, during the State of the Union address, she heckled President Biden while he talked about supporting veterans who suffered from medical problems. These various controversies helped establish Boebert as a national figure, and her hard-right image seemed to be effective for fund-raising. In 2022, as she ran for reëlection, seventy-seven per cent of her itemized contributions came from outside the district, according to an Aspen Journalism analysis of data from the Federal Election Commission. But the congresswoman presented herself very differently to local constituents. “I’m straight out of Rifle, running a restaurant with my four little boys and with my G.E.D.,” she said at a dinner in Ouray, a former mining town. At events in Colorado, Boebert’s message tended to be more personal, and she seemed less intent on attracting attention with extreme statements. She preached a kind of bootstraps politics. “I finally said, Enough is enough,” she told the audience at another event outside Denver, describing her decision to run for office. “I can’t sit on the sofa and be mad anymore. That is getting me nowhere.” Devastated Democrats Play the Blame Game, and Stare at a Dark Future Author: Epstein, Reid J., Date: 2024-11-07 Collections: Hot Takes US Elect 2024 Zotero Key: QUX8ADRG Cite Key: Epstein24demsBlameDarkFuture Zotero Item | Lit Note A depressed and demoralized Democratic Party is beginning the painful slog into a largely powerless future, as its leaders grapple with how deeply they underestimated Donald J. Trump’s resurgent hold on the nation. The nationwide repudiation of the party stunned many Democrats who had expressed a “nauseous” confidence about their chances in the final weeks of the race. As they sifted through the wreckage of their defeats, they found no easy answers as to why voters so decisively rejected their candidates. In more than two dozen interviews, lawmakers, strategists and officials offered a litany of explanations for Vice President Kamala Harris’s failure — and just about all of them fit neatly into their preconceived notions of how to win in politics. The quiet criticism, on phone calls, in group chats and during morose team meetings, was a behind-the-scenes preview of the intraparty battle to come, with Democrats quickly falling into the ideological rifts that have defined their party for much of the Trump era. What was indisputable was how badly Democrats did. They lost the White House, surrendered control of the Senate and appeared headed to defeat in the House. They performed worse than four years ago in cities and suburbs, rural towns and college towns. An early New York Times analysis of the results found the vast majority of the nation’s more than 3,100 counties swinging rightward since President Biden won in 2020. The results showed that the Harris campaign, and Democrats more broadly, had failed to find an effective message against Mr. Trump and his down-ballot allies or to address voters’ unhappiness about the direction of the nation under Mr. Biden. The issues the party chose to emphasize — abortion rights and the protection of democracy — did not resonate as much as the economy and immigration, which Americans often highlighted as among their most pressing concerns. Many Democrats were considering how to navigate a dark future, with the party unable to stop Mr. Trump from carrying out a right-wing transformation of American government. Others turned inward, searching for why the nation rejected them. They spoke about misinformation and the struggle to communicate the party’s vision in a diminished news environment inundated with right-wing propaganda. They conceded that Ms. Harris had paid a price for not breaking from Mr. Biden’s support of Israel in the war in Gaza, which angered Arab American voters in Michigan. Some felt their party had moved too far to the left on social issues like transgender rights. Others argued that as Democrats had shifted rightward on economic issues, they had left behind the interests of the working class. They lamented a Democratic Party brand that has become toxic in many parts of the country. Several noted that the independent Senate candidate in Nebraska ran 14 percentage points ahead of Ms. Harris in the state. And many said they were struggling to process the scale of their loss, describing their feelings as a mix of shock, mourning and panic over what might come in a second Trump administration. “I am pretty devastated and worried,” said Representative Veronica Escobar of Texas, who served as a co-chair for the Harris campaign. “There’s real, imminent danger for people here. There is real danger here ahead for Americans — including many Americans who voted for Trump.” Soul-searching over strategy and values Not everyone was quite as mournful. Senator Bernie Sanders of Vermont, the longtime progressive standard-bearer, blamed what he called a party-wide emphasis on identity politics at the expense of focusing on the economic concerns of working-class voters. “It’s not just Kamala,” he said. “It’s a Democratic Party which increasingly has become a party of identity politics, rather than understanding that the vast majority of people in this country are working class. This trend of workers leaving the Democratic Party started with whites, and it has accelerated to Latinos and Blacks.” Get the best of The Times in your inbox • Sign up for The Evening:Catch up on the biggest news, and wind down to end your day.* • Sign up for The Great Read:On weekdays and Sundays, we recommend one piece of exceptional writing from The Times — a narrative or essay that takes you someplace you might not expect to go.* • Sign up for Breaking News:Sign up to receive an email from The New York Times as soon as important news breaks around the world.* Mr. Sanders, a political independent who has long criticized the influence of the party’s biggest donors and veteran operatives, offered a pessimistic forecast: “Whether or not the Democratic Party has the capability, given who funds it and its dependency on well-paid consultants, whether it has the capability of transforming itself, remains to be seen.” Mr. Sanders was hardly the only one who diagnosed the party’s problem as being too beholden to the needs of its identity groups. Mr. Trump spent tens of millions of dollars on anti-transgender television advertising, which went unanswered by the Harris campaign and its allies. Representative Seth Moulton of Massachusetts, who was one of two dozen Democrats who sought the party’s presidential nomination in 2020, suggested the party should shift its approach to transgender issues. “Democrats spend way too much time trying not to offend anyone rather than being brutally honest about the challenges many Americans face,” Mr. Moulton said. “I have two little girls, I don’t want them getting run over on a playing field by a male or formerly male athlete, but as a Democrat I’m supposed to be afraid to say that.” But Representative Pramila Jayapal of Washington, the chair of the Congressional Progressive Caucus, said Democrats should not give in to prejudice and misinformation. She compared the fight for transgender rights to the struggle over gay marriage, in which public opinion shifted quickly. “We need to create space for people’s fears and let them get to know people,” said Ms. Jayapal, who described herself as “the proud mom of a daughter who happens to be trans.” “And we need to counter the idea that my daughter is a threat to anyone else’s children,” she said. ‘The dynamics of this race were baked in’ And then there was the blame for Mr. Biden. Even before he announced his run for re-election, Democrats were whispering that the president, now 81, was too old to seek re-election, and polls confirmed that voters had serious reservations. Democrats who were worried at the time now say Ms. Harris never really had a chance. “The dynamics of this race were baked in before Kamala Harris became a candidate,” said Julián Castro, the former housing secretary who also ran for president in 2020. “She was dealt a bad hand. She was trying to get elected in the shadow of a president who was unpopular and who the public had overwhelmingly been saying should not run for re-election and took too long to step aside.” Even David Plouffe, a veteran Democratic strategist whom Ms. Harris brought into her operation after Mr. Biden dropped out, seemed to suggest that the president had put her in a difficult position. “We dug out of a deep hole but not enough,” Mr. Plouffe wrote on X. Mr. Biden’s defenders said it was not his fault. Senator Chris Coons of Delaware, a top Biden ally, said he did not think the president had been a drag on Ms. Harris. She ran “a terrific campaign,” he added. “There’s a couple of groups in the United States, young men and Latino voters, that just did not respond in a positive way to our candidate and our message and our record,” he said. “We had a gap that we didn’t close.” For her part, Ms. Harris delivered a concession speech that urged supporters to remain vigilant about the present and optimistic about the future, and to keep fighting for their values. She did not point fingers or cast blame. “I am so proud of the race we ran, and the way we ran it,” she said. “Hear me when I say, the light of America’s promise will always burn bright. As long as we never give up. And as long as we keep fighting.” On Thursday, Mr. Biden addressed the nation from the Rose Garden, urging his supporters to remain optimistic and tenacious. “Setbacks are unavoidable, but giving up is unforgivable,” he said. “We all get knocked down, but the measure of our character, as my dad would say, is how quickly we get back up.” As they reflected on the fallout, Democratic officials compared notes about where this Election Day ranked on their list of horrible experiences. Matt Bennett, the executive vice president for public affairs at Third Way, a centrist think tank, said the party had not faced a crisis as severe since the 1980s, when Democrats lost three straight presidential races in landslides. To regain their grip on power, Democrats must embrace a more moderate approach, he argued. But that will not be easy, Mr. Bennett warned, since the party is facing a leadership vacuum with Mr. Biden weakened and Ms. Harris defeated. “The one way to beat a right-wing populist is through the center,” Mr. Bennett said. “You must become the party that is more pragmatic, reasonable and more sane. That’s where we have to go.” A leadership vacuum Mini Timmaraju, the chief executive of Reproductive Freedom for All, said Democrats must develop a long-term plan to directly confront the sexism — both within their party and the nation — that hampered Ms. Harris and Hillary Clinton, the only women to win a major party’s presidential nomination. “We can’t keep brushing it under the rug,” she said. “The narrative cannot be, ‘Kamala Harris somehow failed.’ There’s a bigger failure here and we have to figure it out and reckon with it.” With Mr. Biden and Ms. Harris now political lame ducks, the Senate majority gone and without a likely House speaker in the party, Democrats in 2025 will find themselves short on clear leaders, as they did after Mr. Trump won in 2016. The next decision party leaders face is whom to choose as the next leader of the Democratic National Committee, a post that was largely ceremonial with Mr. Biden in office but will include far more responsibilities and power without White House officials calling the shots. Jaime Harrison, the party’s chairman since Mr. Biden installed him in the post four years ago, has said for months that he will not seek another term. A new election is set to take place early next year. ‘I can’t go toe to toe with social media.’ Top U.S. health official reflects, regrets. Author: Diamond, Dan, Date: 2025-01-12 Collections: Hot Takes US Elect 2024, PoliticalML, MisDisinformation Zotero Key: CG4VUAE2 Cite Key: Diamond25healthOfficialCantDisinfo Zotero Item | Lit Note As they entered office at the height of the coronavirus pandemic in early 2021, Xavier Becerra and his allies had a plan to restore Americans’ faith in the nation’s beleaguered public health agencies. Becerra, tapped by President Joe Biden to lead the Department of Health and Human Services, empowered career government scientists and experts muzzled under the Trump administration. Biden officials took on social media posts they said spread disinformation about coronavirus vaccines, urging Facebook and other companies to remove them. The White House mounted a nationwide vaccination campaign, convinced the results would win over skeptics. Four years later, the pandemic has receded. But trust in America’s health agencies has not recovered. The percentage of adults who regarded the Centers for Disease Control and Prevention as “excellent” or “good” fell from 64 percent in April 2019 to 40 percent in October 2021 — a rating that has stubbornly refused to budge in the subsequent three years, according to Gallup polls, despite the Biden administration’s efforts to rebuild confidence. Other surveys found similar declines in trust and approval for federal health agencies, and the people who lead them, driven by GOP skepticism. Sitting in his office at HHS headquarters, America’s top health official identified a culprit: a media climate that he says drowns out reliable information. False claims about vaccines run rampant online; government health experts at news conferences barely make a dent compared with influencers who have huge followings. 🧘 Follow Health & wellness Follow “I can’t go toe to toe with social media,” Becerra said in a wide-ranging interview Wednesday, arguing that even a Cabinet secretary can be hemmed in. As examples, Becerra cited the lawsuits the Biden administration faced after urging social media companies to take down posts the White House considered disinformation. And he noted that officials can’t formally disclose many details about negotiations to lower prescription drug prices. “I don’t get to write whatever I want,” he said. The health secretary never mentioned Robert F. Kennedy Jr., but the longtime anti-vaccine activist’s shadow hung over Becerra’s answers. President-elect Donald Trump’s pick to run HHS has relentlessly criticized the agencies he soon may lead, amplified false claims about vaccines and offered alternatives to what he called government misinformation. Now, Kennedy, who has said he is not anti-vaccine, could occupy the office where Becerra was giving his exit interview. If Kennedy is confirmed by the Senate, he will be the first HHS secretary whose personal celebrity arguably eclipses the agencies he oversees. The scion of the Democratic political dynasty has 4.5 million Instagram followers — more than HHS (about 200,000 followers) and its subagencies combined. His bid for the presidency won him millions of supporters, some of whom said they agreed with Kennedy’s “Make America Healthy Again” agenda. If confirmed by the Senate, Robert F. Kennedy Jr. would move into the office Becerra occupies at the Department of Health and Human Services in Washington. (Jason Andrew for The Washington Post) The low-profile Becerra, meanwhile, remains largely anonymous to the regular American. (As Becerra stood in the airport on one trip last year, shadowed by a Washington Post reporter, hundreds of travelers walked past, and not one approached the man 12th in line of succession to the presidency.) Some of that stems from his cautious approach to media; even in his exit interview, he steered clear of complaints about the incoming Trump administration. “I want to be careful. I’m not going to try to speak for the next administration. I’m not going to try to characterize what any potential nominee is saying or critique it, because that’s not my place,” Becerra said. Instead, he focused his answers on his sprawling department, a portfolio that touches everything from Americans’ food and drugs to the nation’s crisis response. Down the hall, in a secure room known as the Secretary’s Operations Center, emergency personnel monitored the fires raging in Southern California. Other HHS staffers were helping support arrangements related to former president Jimmy Carter’s viewing in the Capitol or preparations to guard against the threat of avian flu. But the hulking HHS headquarters was mostly quiet last week, with Washington tamped down by snow and the federal government shuttered Thursday for Carter’s funeral — a muted ending to four years marked, in many ways, by achievements that Becerra promised senators when he asked for their votes to confirm him. The Biden administration helped administer hundreds of millions of coronavirus vaccine shots. It oversaw record-high enrollment through the Affordable Care Act (ACA). It rolled out new initiatives decades in the making to lower drug prices. But the bundle of health policies wasn’t enough to sway the electorate, and each achievement had a flip side. The coronavirus vaccines saved lives — but the Biden administration’s vaccine mandates helped spark a lingering backlash against health agencies. The rate of uninsured Americans fell to an all-time low, but experts have focused on the nation’s relatively poor health outcomes, saying coverage does not guarantee access to care. The drug-price victories were hard to explain to voters, and it is not clear whether Trump and congressional Republicans will seek to unwind the federal law that empowered Medicare to negotiate directly with drug companies — although Becerra, a former California attorney general, said he would fight to protect it. “I’m not AG anymore, but I’d be ready to sue them,” he vowed. Becerra, then a congressman representing California, joined members of the Congressional Hispanic Caucus in March 2010 to announce support for the law that became the Affordable Care Act. (Lauren Victoria Burke/AP) Surviving backlash Becerra, who was a congressman for more than two decades before becoming California’s top lawyer in 2017 and then the nation’s health secretary in 2021, is a veteran of Washington battles. Some of the more recent fights concerned his own job: The Post and other outlets reported on internal frustrations with Becerra’s leadership during the pandemic and his agency’s response to unaccompanied children at the border. Some officials mused about replacing him with someone they said would be more proactive. Becerra acknowledged the learning curve when taking charge of HHS, which oversees programs such as Medicare and Medicaid; approves drugs, medical devices and vaccines; regulates hospitals, physicians and other health-care providers; and steers many other initiatives affecting food and medicine. It also plays a central role in the nation’s human services, such as caring for unaccompanied migrant children. “I didn’t realize how vast this agency’s jurisdiction is,” Becerra said, reflecting on how HHS found itself at the center of various crises, such as a 2022 baby formula shortage. “Since when has HHS been the administrator and distributor of infant formula?” He said he quieted the criticism by demonstrating results, including smoothing the process of HHS taking custody of unaccompanied migrant children. “We proved that we could execute,” Becerra said. Becerra in June 2021 toured a Texas facility serving as an emergency intake site for unaccompanied migrant children. His agency's oversight of migrant children would become a major flash point. (Jabin Botsford/The Washington Post) One area of execution: ACA enrollment, with Becerra’s team announcing last week that nearly 24 million Americans are covered through the program. Millions of those enrollees benefit from subsidies expanded by the Biden administration, which are set to expire at the end of this year. Democrats introduced legislation last week to extend those subsidies, even as Republicans look to end them, saying they distort the purpose of the ACA by subsidizing Americans who don’t need the assistance. The disagreement threatens to tee up a major political fight. “We have strengthened the subsidy support that Americans get that have made these plans extremely affordable,” Becerra said. “I think we’ve convinced a lot of Americans that this really is the best deal in town.” Becerra also touted the Biden administration’s work on capping out-of-pocket costs at $2,000 a year for covered prescription drugs for Medicare Part D enrollees; a decline in drug overdose and suicide deaths; and the launch of the 988 crisis hotline. He declined to comment on his next steps after leaving government later this month, although allies widely expect him to run for California governor. Becerra joined President Joe Biden and Vice President Kamala Harris in July as the president signed an executive order protecting access to reproductive health services. (Bill O'Leary/The Washington Post) Rebuilding trust Becerra acknowledged that his team struggled to win back the support of skeptical Americans, who he said are being bombarded by “instantaneous information and disinformation” on social media. The health secretary contended that the government is outmatched, suggesting that Congress should set aside more resources for his nearly $2 trillion agency. “I don’t have a budget that Pfizer has to do marketing and advertising,” Becerra said, invoking the pharma giant that spends billions of dollars to promote its drugs. “Will [Congress] give me some money to compete out there with all the disinformation?” He also defended Biden administration efforts that Kennedy and others have criticized as heavy-handed, such as requiring federal workers and federal contractors to be vaccinated. “Every chance I have to do what the science and the evidence is telling me is going to protect Americans and save lives … I’d do it again,” Becerra said. Social media companies, after working closely with the Biden administration during the pandemic, have offered their own rebukes of government. Facebook founder Mark Zuckerberg told popular podcaster Joe Rogan last week that he disagreed with the Biden administration’s push to take down social media posts critical of coronavirus vaccines, and has said he regretted complying with the White House’s requests. The flagging support for federal health agencies may shift with a change of administration. Just 19 percent of Republicans gave the CDC high marks in September, down from 71 percent in April 2019, according to Gallup surveys. Support for federal agencies often correlates with partisan politics, with GOP voters more willing to view agencies positively when a Republican president is in office, pollsters have said. Becerra said the difficulties his team faced reflect a deep distrust of institutions. “Do I think the American public has come back to a point where they trust, whether it’s the ACA or vaccines, as much as they trust their priest or their rabbi? No,” Becerra said. “But then again, I don’t think priests … have the same standing they used to have before, either.” The health secretary wound down his interview with a frank plea about how public health experts can better reach Americans. “I don’t know what more we can do,” Becerra said. “I’m more than willing to listen if somebody’s got some great ideas.” The debate over why Harris lost is in full swing. Here’s a guide. Author: Prokop, Andrew, Date: 2024-11-09T11:00:00+00:00 Collections: Hot Takes US Elect 2024 Zotero Key: GKFDQZJD Cite Key: Prokop24debateWhyHarrisLost Zotero Item | Lit Note Democrats have faced a bitterly disappointing defeat, and the debate is on about why that happened. Amid the opportunistic finger-pointing and evidence-free assertions that Vice President Kamala Harris could have won if only she had done this or that, there is a genuine search for explanations about what happened. The answer Democrats find most persuasive could greatly influence the party’s direction as it tries to win again. This debate will clearly go on for some time, and disentangling causality is difficult. But one way to think about it is to break up the question. How much of the defeat was about Harris’s weakness as a candidate or her campaign strategy? How much was about Donald Trump’s strengths? How much was about Joe Biden’s record? How much was about the Democratic Party brand generally? And how much was due to larger structural factors like a global anti-incumbent trend? It’s possible that all of these played some role in the outcome, especially because issues like inflation can resonate across them all. But let’s go through them. Was Harris an unusually weak candidate? Any candidate who loses tends to get defined, in retrospect, as an obvious loser. So naturally, lots of the Democratic finger-pointing has been pointing at Harris. But how convincing is it? Harris had some real strengths: her record as a former prosecutor, her formidable fundraising, and the fact that she was a fresh face. But many had grave doubts about her prospects all along. Harris’s political rise in deep-blue San Francisco, and later statewide in California, came by cultivating support among Democratic elites; she had never had to run in a swing state and therefore never developed a political style designed to appeal to swing voters. It was far from clear what those swing voters would make of her when she entered the 2024 race. (The one time before this year that she faced a decent Republican opponent — her first run for California attorney general, in 2010 — she barely won.) Her campaign strategy was cautious and defensive. In her prior presidential campaign and during the vice presidency, she’d done several high-profile interviews that went poorly, which spurred her to avoid such interviews. In this bid, she was happy to prosecute the case against Trump on the debate stage, but seemed much less comfortable when it was her being grilled. She often spoke in talking points and platitudes. There was also her record. When Harris was trying to win the 2020 Democratic primary, she ran to the left, taking several policy positions (like banning fracking) that did not seem politically tenable. Trump’s team used one clip from that campaign, when she touted how she’d worked to ensure transgender inmates in California could access gender-affirming care, in a heavily funded attack ad. It concluded with the line: “Kamala is for they/them. President Trump is for you.” Her campaign strategy hinged on trying to tack to the center, disavowing or simply avoiding her past positions, sending out signals to the business community that she’d be friendlier to them, while using former Rep. Liz Cheney as a Republican validator. She did not break with the Biden administration or the mainstream Democratic consensus on any issue of significance. She did not run as a bold populist or progressive, either. Finally, there’s gender and race. Many have wondered whether the voter backlash against her was due to sexism — particularly due to preliminary numbers suggesting the swing against her was most intense among men. The New York Times reports that the Trump team’s ads often showed Harris “laughing or dancing in a colorful blouse and pink pants,” because Trump’s goal was “to make her look like a lightweight.” But is Harris getting too much of the blame? Evidence suggests the guy she replaced at the top of the ticket, Joe Biden, would have done much worse. One post-election poll found Trump would have beaten him by 7 percentage points nationally. Perhaps she did a decent job of playing a bad hand: the Biden administration’s record. Biden’s initial attempt to run for reelection — before it was curtailed by his disastrous debate — limited the time and options available to Harris. But the bigger problem may have simply been that she was his vice president, and his administration was very unpopular. Blueprint, a Democratic polling initiative, published research showing that two of the three most effective arguments for pushing swing voters away from Harris were that “inflation was too high under the Biden-Harris administration” and that “too many immigrants illegally crossed the border under the Biden-Harris administration.” Polling all year has shown that inflation and immigration were Democrats’ biggest vulnerabilities. So part of the party’s second-guessing will naturally involve whether Biden should have made different policy choices to produce different outcomes in those areas. Biden did not cause inflation, but his American Rescue Plan did make it worse, which resulted in higher prices and necessitated bigger interest rate hikes than would have happened otherwise. His administration was also slow to adjust, and though a “soft landing” without a recession eventually resulted, voters hated the enduring high prices. On the border, too, Biden only belatedly pivoted. After a huge increase in the number of unauthorized immigrants arriving at the border in 2021 and onward, Democrats in blue states and cities struggled to deal with the logistics of so many arrivals, and public backlash brewed. Late in 2023, Biden tried to pass a border security bill through Congress, but failed — in part due to opposition from Donald Trump. In mid-2024, the combination of a deal with Mexico and new executive orders seemed to finally cut down on border crossings. But it’s possible Biden could have done more earlier, limiting the effectiveness of immigration as an attack on Harris. Finally, Israel’s war in Gaza bitterly divided the Democratic coalition. There was probably no way to make everyone happy here, and polling does not show it as a top reason swing voters turned against Harris. But the ugly controversy over Biden’s support for Israel (and Harris’s support for Biden’s policy) may have hurt her in Michigan and cut down on the left’s enthusiasm for her. It’s unlikely to have been decisive, but it certainly didn’t help. Was Trump an unusually strong candidate? The political conventional wisdom has generally been that Trump is a weak candidate who’s been holding Republicans back — that his 2016 win was a fluke reliant on the Electoral College; that he was quite unpopular as president; that voters rejected him and his party in 2018, 2020, and (sort of) 2022; and that the GOP was taking a massive risk by nominating him again after he tried to steal the last presidential election and was indicted four times. But Trump may have been unusually well-positioned to take advantage of dissatisfaction with the Biden administration’s record on the economy and immigration. Trump’s persona as a celebrity businessman, one who many voters view as especially savvy about the economy, has been an advantage for him in polls since his first campaign. That wasn’t enough to save him amid the chaos of 2020, but given what’s ensued since, many Americans have looked back on Trump’s governing record more fondly. Voters have given him retrospective credit for the strong economy and low inflation environment of 2017 through 2019, while not really blaming him for the pandemic. Focus groups again and again came back to the idea that voters hated the Biden economy and thought Trump could fix it. Indeed, Trump’s outperformance of many down-ballot Republican candidates in key races — in part due to split-ticket voting, in part due to Trump voters simply not voting down-ballot — suggests there was a significant bloc of “I don’t like Republicans much, but the economy was better under Trump” voters. On immigration, too, there was a stunning swing of public opinion to the right during Biden’s term, as border arrivals soared, which may have played to Trump’s advantage. Was this a backlash against the Democratic Party for going too far left? One theory floating around is that the results show the public is punishing the Democratic Party for having moved too far left. Josh Barro made this argument in a Substack post, citing poor Democratic governance in blue states and cities as well as “woke” far-left policies on crime, schooling, and trans rights as likely causes of public frustration. Perhaps this explains some of the disproportionate shifts against Harris we saw in deep-blue states like New York, as well as progressive prosecutors losing and a tough-on-crime ballot proposition passing in California. A counterpoint to this is that Democrats’ swing-state Senate candidates did well — several of them won despite Harris losing their states — and that even amid the backlash in New York, Democrats flipped several House seats in New York. That could be read to suggest the problem had less to do with the Democratic Party and more to do with the top of the ticket. Still, Democrats did likely lose the national popular vote as well as the presidency, so it’s hard to argue that the party’s political positioning is optimal. Was it just due to a global trend? Finally, another school of thought holds that perhaps the explanation for the outcome doesn’t lie in the United States at all. Perhaps it’s just the latest example of a worldwide trend of incumbents doing poorly in democracies holding elections in the post-pandemic years. Inflation, as a worldwide trend caused by supply-side disruptions and foreign crises, is a big part of the reason for that global struggle. “Every governing party facing election in a developed country this year lost vote share, the first time this has ever happened,” John Burn-Murdoch reported for the Financial Times. “It’s possible there is just no set of policies or personas that can overcome the current global anti-incumbent wave.” Still, it is worth keeping in mind that Trump won quite narrowly, by just 2 percentage points or less in the decisive swing states. On the one hand, that could suggest Democrats did a surprisingly good job among structural headwinds, starting from behind and closing the gap as much as possible — even if it wasn’t ultimately enough. On the other hand, it could suggest that more could have been done. Was it really fated that there was absolutely nothing Democrats could have done over the past four years to improve their margin by 2 more points, however strong the headwinds? Whatever the answer, Democrats have two years until their next chance to take back a branch of the federal government — and plenty to figure out in the meantime. Microtargeting and Data Analytics: Transforming Political Campaigns Author: Croll, Rob, Date: september 17, 2024 Collections: CampaignMoney, PoliticalML Zotero Key: 4TIRG3GM Cite Key: Croll24politAnlsysMicrotarg Zotero Item | Lit Note Political campaigns are challenged to ensure their messages get to the voters who will determine their success. But that has perhaps never been more challenging in an era of increasingly competitive races, saturated markets, and technological advancements. 2024 is shaping up to be the most expensive political cycle in history. Spending on political campaigns is expected to reach $12.32 billion this year, an increase of 130% over 2018 and 190% greater than 2016. Source: eMarketer Accompanying these huge spending increases is a need for more effectiveness. That's mainly due to several factors, including: • The evolution of technology and consumer behavior have impacted the political sphere as much as elsewhere, making it ever more challenging to reach the intended audience.* • The influence of the Citizens United Supreme Court decision on political advertising continues to reverberate, leading to consistent growth in ad spending and a more intense and fiercely competitive advertising landscape within political campaigns.* In the past, radio and TV were widely used to broadcast messages to wide audiences. However, that has become less effective, so today's political campaigns are shifting toward highly targeted, data-driven strategies. Reflecting this new focus, IPG Mediabrands' Magna unit expects digital political ad spending to grow by 156.3% from 2020 to 2024. What is Microtargeting in Political Campaigns? Microtargeting—using data from voter databases, social media, and behavioral insights—allows campaigns to communicate tailored messages to specific segments. This data-driven method increases engagement, enhances voter turnout, and helps campaigns precisely target undecided voters. Microtargeting emerged in the early 2000s and has shifted the campaign focus from mass communication to precise, tailored outreach based on demographics, interests, and behaviors. While traditional campaigns target broad demographics, microtargeting creates a more intimate voter experience. It also allows for real-time adjustments based on voter preference and engagement patterns. Examples of microtargeting include: The Role of Data Analytics in Political Campaigns Political campaigns gather voter data from various sources, including social media platforms such as Facebook, X (formerly Twitter), and Instagram, voter registration databases maintained by government authorities, and online interactions such as website visits, email responses, and surveys. Examples of data that can be used include: Modern technologies like artificial intelligence and machine learning help process this data, delivering insights into voter preferences and sentiment. These insights fuel more effective campaign strategies and messaging. Here's a look at how campaigns utilize this data: Voter Segmentation Campaigns use data to divide the electorate into distinct groups based on shared characteristics, beliefs, or behaviors. Example: A campaign might create segments like \"environmentally conscious millennials\" or \"fiscal conservatives nearing retirement.\" Message Tailoring Each voter segment receives customized messaging that resonates with their specific interests and concerns. Example: The \"environmentally conscious millennials\" segment might receive messages about clean energy policies, while the \"fiscal conservatives\" segment might get notifications about tax plans. Predictive Modeling Campaigns use historical data and machine learning to predict voter behavior, such as the likelihood of voting or supporting a candidate. Example: A model might identify voters who are likely to be persuadable, allowing the campaign to focus resources on these individuals. Ad Targeting Data analytics inform digital ad placement, ensuring ads reach the right audience at the right time. Example: Using browsing history and demographic data to show specific ads to undecided voters in swing states. Fundraising Optimization Campaigns analyze donor data to identify potential high-value donors and optimize fundraising strategies. Example: Targeting past donors with personalized appeals based on their giving history and interests. Get-Out-The-Vote (GOTV) Efforts Data helps campaigns identify likely supporters who may need extra encouragement to vote. Example: Using voting history data to target infrequent voters with reminders and information about polling locations. Real-time Campaign Adjustments Sentiment analysis of social media and news coverage allows campaigns to respond to emerging issues or controversies quickly. Example: Rapidly crafting a response to a viral news story based on real-time public sentiment analysis. A/B Testing Campaigns use data to test different messages, email subject lines, or ad creatives to optimize engagement. Example: Testing two versions of a fundraising email to see which generates more donations. Voter Contact Prioritization Data helps campaigns decide which voters to contact through which channels (phone, email, door-to-door) for maximum impact. Example: Using predictive models to identify which undecided voters are most likely to be swayed by a personal phone call. Opposition Research Data analytics can help identify weaknesses in opposing campaigns or candidates. Example: Analyzing social media engagement to find which attack ads are most effective against an opponent. Benefits of Microtargeting and Data Analytics in Campaigns Getting the right message to the right people at the right time matters for political campaigns. Here are a few benefits of microtargeting and data analytics: Personalized Messaging Data analytics allows campaigns to craft tailored messages to specific voter groups, increasing the relevance of communications. Personalized outreach leads to greater voter engagement, making individuals feel more connected to the campaign. Cost-Effective Campaigning Microtargeting allows campaigns to focus resources on high-probability voter segments like swing voters or undecided individuals. Instead of spreading resources across a broad audience, campaigns can allocate spending where it will make the most significant impact, reducing waste and increasing efficiency. Improved Voter Insights With advanced data analysis, campaigns can better understand what drives voter behavior. Real-time sentiment analysis tools allow campaigns to monitor reactions and make timely adjustments to messaging, ensuring that communications remain relevant throughout the election cycle. For example, campaigns can adjust their messaging based on real-time data from social media discussions or use sentiment analysis to help anticipate potential shifts in voter preferences. Key Tools and Technologies for Political Microtargeting Data Management Platforms (DMPs) and CRM Systems These platforms play a pivotal role in organizing and segmenting voter data. By integrating various data sources—such as voter records and social media interactions—DMPs and CRMs allow campaigns to manage data effectively. Campaigns can then engage voters with highly relevant messages and track interactions for future outreach. Artificial Intelligence and Machine Learning AI-driven tools are revolutionizing the way campaigns predict voter behavior and optimize outreach. Machine learning algorithms can analyze vast data sets to uncover patterns in voter preferences, helping campaigns predict election outcomes with greater accuracy. Predictive analytics tools can also identify swing voters and provide insight into when and how to engage them. Geospatial Analytics By analyzing geographic data, campaigns can tailor their strategies to local concerns. Geospatial analytics can reveal patterns in regional voting behaviors, allowing campaigns to focus their efforts on areas that are crucial for victory, such as battleground states or swing districts. For example, a campaign might adjust its message to focus on healthcare in one region and infrastructure in another based on local priorities. Ethical Concerns and Challenges One of the biggest challenges in microtargeting is ensuring voter data is used responsibly. While microtargeting is widely seen as an effective tool for political campaigns, notable scandals have occasionally raised concerns about privacy and the misuse of personal data in political campaigns. One of the most well-known issues involved Cambridge Analytica, which became infamous in 2018 following revelations about using Facebook data in political campaigns. The consulting firm obtained access to personal data from millions of Facebook users without their explicit consent through a third-party app disguised as a personality quiz. This data harvesting allowed the firm to build detailed voter profiles and microtarget individuals with highly personalized political ads. The scandal highlighted significant ethical concerns regarding privacy and data protection, leading to widespread criticism, regulatory investigations, and a broader conversation about the role of data analytics in elections. Another concern is that hyper-personalized messaging risks deepening political divides by reinforcing voters' beliefs. Targeted messages often create echo chambers, where individuals are exposed only to content that aligns with their worldview. While this is a valid concern, campaigns can avoid further polarization by focusing on common ground and shared voter concerns. Another risk is misinformation, which can be mitigated by prioritizing accuracy and transparency in campaign communications. Success Stories and Case Studies Microtargeting and data analytics have been (and are being) used successfully by campaigns of all types, from local to national. Here are a few examples that highlight the power of microtargeting. Barack Obama's 2008 and 2012 Campaigns Obama's campaigns are frequently cited as turning points in political microtargeting. In 2008, Obama's team collected extensive voter data to segment the electorate into small, manageable groups. This allowed the campaign to personalize outreach efforts for demographics such as young voters, minorities, and new voters. They used social media, email, and text messaging to deliver targeted messages that resonated with each group. The 2012 campaign further enhanced these strategies with predictive analytics, focusing on understanding voter behavior in real-time and adjusting messages to drive engagement. Donald Trump's 2016 Campaign Trump's 2016 victory is often linked to his campaign's use of data analytics and microtargeting through Cambridge Analytica. The campaign used voter data to create highly personalized ads for different demographics, including rural voters and middle-class Americans. A key strategy involved using social media platforms like Facebook to deliver targeted ads based on specific voter concerns, helping the campaign engage previously overlooked or undecided voters. This data-driven approach was critical in swing states, where targeted messaging helped tip the balance. 2022 Midterm Elections The 2022 U.S. midterm elections provided several examples of sophisticated microtargeting: • The Democratic Senatorial Campaign Committee (DSCC) used a tool to analyze voter data and identify persuadable voters in crucial swing states. This helped in crafting targeted messages for specific voter segments.* • In Georgia's Senate runoff, the Warnock campaign used a combination of traditional data sources and social media analytics to identify and mobilize infrequent voters, particularly in urban and suburban areas.* Political Microtargeting: Key Takeaways As political landscapes continue to evolve, campaign marketing is increasingly turning to advanced technological tools and strategies to get messages to voters. • Microtargeting enables campaigns to segment voter data and deliver highly relevant messages, improving engagement and turnout.* • Campaigns maximize their impact with fewer resources by focusing on high-probability voter segments like swing voters.* • Campaigns now use AI-driven tools to refine their strategies in real-time, adjusting outreach based on voter behavior.* • While microtargeting offers many benefits, campaigns must navigate privacy concerns and prevent potential misinformation.* How Hearst Bay Area Can Help Hearst Bay Area provides a powerful platform for political campaigns to reach target audiences effectively using our expansive first-party data. By leveraging this data, campaigns can optimize their advertising spend, reach engaged voters, and maximize impact in critical areas through targeted programmatic advertising and hyper-targeted social media campaigns across platforms. Our political marketing agency's data-driven approach allows campaigns to personalize messages and engage voters in dynamic ways, making their messages more sophisticated and impactful. Contact us below to learn more about how we can help revolutionize your political campaign. Partisan Conflict and Congressional Outreach Author: Pew, Date: 2017-02-23T14:54:50+00:00 Collections: MediaAdsPolit, PoliticalML, Polarization Zotero Key: NTIK3ZLA Cite Key: Pew17negPartisanMoreEngage Zotero Item | Lit Note This report describes key findings from an analysis of all official press releases and Facebook posts identifiable via internet and archival sources, issued by members of the 114th Congress between Jan. 1, 2015, and April 30, 2016, prior to either party’s 2016 presidential nominating convention. A total of 94,521 press releases were collected directly from members’ websites and supplemented with additional press releases collected from LexisNexis searches. The 108,235 Facebook posts from members’ official accounts were collected directly from the social media platform’s application programming interface (API) for public pages. To analyze these two collections of congressional statements, Pew Research Center used a combination of machine-learning techniques and traditional content analysis methods. The Center sampled 7,000 press releases and Facebook posts. Researchers and paid content coders then manually classified the sample to create a set of training data. Next, the Center employed an automated approach that approximates human judgments to classify the rest of the documents. Researchers used machine-learning models that estimated the relationship between words used in the documents and human classification decisions to predict the most likely classification for all documents that were not coded by hand. Additional detail can be found in the methodology section. This approach allowed the Center to provide a more comprehensive account of congressional communications than would be possible with a much smaller sample of documents classified by hand. For example, researchers were able to estimate the proportion of communications containing expressions of indignant disagreement for every U.S. senator and representative during this period, allowing for an examination of the relationship between indignation and the political attributes of particular members and their districts. Nonetheless, this process, which was relatively new for the Center, necessarily entails some degree of error in concept operationalization, human judgment, machine classification and statistical estimation. Because of this, the statistics included in this report should be understood as estimates. Just as survey results need to be understood in the context of a margin of error, there is a degree of error associated with the results presented here. Estimates of error are provided in the form of standard error intervals and confidence bands, which are attempts to quantify the uncertainty surrounding each set of statistics. Estimates of reliability in human and error in machine classification are provided in the methodology section at the end of the report. Hostility in political discourse was thrown into sharp relief in 2016 by a contentious presidential campaign. A new Pew Research Center analysis of more than 200,000 press releases and Facebook posts from the official accounts of members of the 114th Congress uses methods from the emerging field of computational social science to quantify how often legislators themselves “go negative” in their outreach to the public. Overall, the study finds that the most aggressive forms of disagreement are relatively rare in these channels. For the average member, 10% of press releases and 9% of Facebook posts expressed disagreement with the other party in a way that conveyed anger, resentment or annoyance. But there are distinct patterns among those who voiced political discord: congressional leaders, those with more partisan voting records and those who are elected in districts that are solidly Republican or Democratic were the most likely to go negative. Republicans, who did not control the presidency during the 114th Congress, were much more likely to voice disagreement than were Democrats. The analysis represents the Center’s most extensive use of the tools and methods of data science to date. The study also finds that critical posts on Facebook get more likes, comments, and shares. Posts that contained “indignant disagreement” – defined here as a statement of opposition that conveys annoyance, resentment or anger – averaged 206 more likes, 66 more shares and 54 more comments than those that contained no disagreement at all. Other research suggests that, faced with divisive policy rhetoric, audiences tend to adopt the stances of their party leaders. On the other hand, members also discussed legislative bipartisanship – emphasizing the importance of working across the aisle – in 21% of press releases, more than three times as often as on Facebook, where just 6% of posts contained bipartisan content, on average. The most moderate legislators and those in districts with the most partisan competition were the most likely to emphasize bipartisanship. During the period under study here, Facebook posts focused on bipartisanship averaged substantially less engagement than those containing disagreement. The findings come at a time when many Americans are struggling to make sense of the extreme partisanship and polarization on display in Washington. Some scholarly evidence suggests that in recent years, a decades-long trend toward increasing partisanship in Congress may be influencing the growth of similar divisions among the American public, though it’s difficult to nail down exactly why the public has become more polarized. As of 2016, a Pew Research Center study found that more than half of Americans who identify as Republicans or Democrats had a “very unfavorable” view of the opposing party, up from around 20% in the early 1990s. And a 2014 report found that “Republicans and Democrats are more divided along ideological lines – and partisan antipathy is deeper and more extensive – than at any point in the last two decades.” At the same time, the number of moderates in Congress has dwindled since the 1970s. Yet, in 2014, 56% of Americans said that they preferred leaders who compromise over those who stick to their positions. In 2015, 57% reported feeling frustrated with the federal government. The result is a paradox in American politics: Voters generally say they want a functioning government with legislators willing to compromise, but polarization in Congress – and partisan antipathy among members of the public – continues to rise. The results presented here come from an analysis of more than a year’s worth of press releases and Facebook posts by members of Congress. While these channels don’t cover all the ways representatives communicate with their constituents – there are also town halls, media appearances, other social media outlets and plenty of discussion on the floors of the House and Senate – they represent two key channels for officials that can be captured and studied in their entirety. Studying Facebook posts also makes it possible to measure how much a member’s audience interacts with the posts by looking at likes, comments and shares. To generate these findings, Pew Research Center used a combination of human coders and machine-learning methods to classify 94,521 press releases and 108,235 Facebook posts issued by members of Congress based on their substantive content: the events, topics and issues raised and discussed in each kind of document. The documents were classified as expressing disagreement; “indignant disagreement,” a type of disagreement that also expresses anger, resentment or annoyance; support for bipartisanship; or descriptions of constituent benefits. Using machine-learning methods to sort through large amounts of written material has a key advantage: once trained, a computer can look through much more data far more quickly than a team of humans. This type of analysis – still relatively new to the Center – is in its infancy in terms of potential uses for answering key questions in social science, and is full of promise. At the same time, using computers to code written language is inherently an error-filled process. Because of the complexity of the assigned task (attempting to identify nuanced themes such as “bipartisanship” and “indignation”) and the inevitable possibility of human error associated with the training process, a machine-learning system will miss instances it should have caught, and incorrectly classify some others. Just as survey results need to be understood in the context of a margin of sampling error, there is a degree of error associated with the results presented here. In the cases where this is quantifiable, it is laid out in the methodology at the end of the report. The period under study in this report spans the beginning of the 114th Congressional session in January 2015 through April 30, 2016, prior to either party’s presidential nominating convention. It also incorporates information about legislators and legislative districts compiled from outside sources. In particular, this report uses a measure called “ DW-NOMINATE,” which compares a given legislator’s voting record to every other lawmakers’ to estimate their ideology: where they fall on the liberal to conservative spectrum. While this analysis is based on a particular time period within a particular Congress, and thus is by definition a snapshot in time, it serves as an exploratory effort on behalf of Pew Research Center to use machine learning to quantify some of the ways that members of Congress communicate with the modern American public in official outreach efforts. Other important findings In the 114th Congress, Republicans – as the party that controlled the legislative branch but not the White House – criticized President Barack Obama in their press releases and Facebook posts more often than they criticized other Democrats. Indeed, the average Republican member directed indignant disagreement at the president in 15% of press releases, compared with just 2% that attacked other Democrats. Democratic members, for their part, took an emotionally charged stance against Republicans in 4% of their press releases. Partisan disagreement often comes in bursts, which appear to follow partisan policy conflicts. In the 114th Congress, strong Democratic opposition to Republicans in press releases followed events such as the GOP’s March 2015 budget proposal and September 2015 plans to defund Planned Parenthood. On the other side of the aisle, Republican opposition to the president and his party surfaced after Obama announced initiatives to sign a nuclear nonproliferation treaty with Iran in August and September 2015; prepared to close the Guantanamo Bay detention center in February 2015; and nominated Merrick Garland to the Supreme Court in March 2016. On the other hand, Democrats and Republicans issued bipartisan press releases at about the same rate. Bipartisan language appeared in about 21% of press releases for members of both parties. However, the average member of Congress discussed bipartisanship in just 6% of Facebook posts, again with only modest differences between members of the two major parties. This difference is especially noteworthy in light of the fact that among Americans who pay attention to government and politics on Facebook, those who hold consistently liberal or conservative policy preferences are more likely to follow politicians. Additionally, Americans with consistently liberal or conservative policy preferences are less willing to support compromise in Washington. Americans not only divided, but baffled by what motivates their opponents Author: Plutzer, Eric, Date: 2018-11 Collections: NeuroPsychoLinguisticPolitics Zotero Key: 3SJH46YG Cite Key: Plutzer18usDividedBaffledMotive Zotero Item | Lit Note Americans not only divided, but baffled by what motivates their opponents The Mood of the Nation Poll asks Americans to understand why their fellow citizens voted differently than they did. Eric Plutzer and Michael Berkman As it became clear that Democrats would win control of the US House of Representatives, pundits immediately began explaining the “Blue Wave.” Some said it was rooted in concerns that President Trump was leading the nation into dangerous territory; others pointed to alarm about health care, or compassion for citizens of color and refugees. But Republican voters were having none of this, according to a recent Penn State Mood of the Nation Poll. The nationally representative poll of 1,000 citizens included 307 voters who cast votes for Republican Congressional candidates in the midterm elections. We asked them, “In your opinion, how many citizens voting for the Democrats did so because they sincerely believe that the Democratic party is best for the country?” Republicans can’t understand Democrats Only one in four Republican voters felt that most or almost all Democratic voters sincerely believed they were voting in the best interests of the country. Rather, many Republicans told us that Democratic voters were “brainwashed by the propaganda of the mainstream media,” or voting solely in their self-interest to preserve undeserved welfare and food stamp benefits. [1] We asked every Republican in the sample to do their best to imagine that they were a Democrat and sincerely believed that the Democratic Party was best for the country. We asked them to explain their support for the Democratic Party as an actual Democratic voter might. For example, a 64-year-old strong Republican man from Illinois surmised that “Democrats want to help the poor, save Social Security, and tax the rich.” But most had trouble looking at the world through Democratic eyes. Typical was a a 59-year-old Floridian who wrote “I don't want to work and I want cradle to grave assistance. In other words, Mommy!” Indeed, roughly one in six Republican voters answered in the persona of a Democratic voter who is motivated “free college,” “free health care,” “free welfare,” and so on. They see Democrats as voting in order to get “free stuff” “without having to work for it” was extremely common – roughly one in six Republican voters used the word “free” in the their answers, whereas no real Democratic voters in our sample answered this way. Among the Republicans who seemed to try hardest to take the perspective of sincere and patriotic Democratic voters, the most common attributions were related to immigration – a topic made salient by President Trump in his campaign stops during the last month of the election. As in this Republican woman from [2] Washington who said, “Democrats welcome all people into the country whether they are here legally or not.” Democrats return the favor: Republicans uninformed or self-interested The 429 Democratic voters in our sample returned the favor and raised many of the same themes. Democrats inferred that Republicans must be “VERY ill-informed,” or that “Fox news told me to vote for Republicans.” Or that Republicans are “uneducated and misguided people guided by what the media is feeding them.” Many also attributed votes to individual self-interest – whereas GOP voters feel Democrats want “free stuff,” many Democrats believe Republicans think that “I got mine and don't want the libs to take it away,” or that “some day I will be rich and then I can get the benefits that rich people get now.” Many used the question to express their anger and outrage at the other side. Rather than really try to take the position of their opponents, they said things like, “I like a dictatorial system of Government, I'm a racist, I hate non-whites.” Democrats think many Republicans sincere, and point to policy Democrats, however, were somewhat more generous in their answers. More than four in ten Democratic voters (42%) felt that most Republican voters had the country’s best interests at heart (combining the top two bars in the figure below). And many tried their best to answer from the other’s perspective. A 45-year-old male voter from Ohio imagined that as a Republican, he was motivated by Republicans’ “harsh stance on immigration; standing up for the 2nd Amendment; promised tax cuts.” A 30-year-old woman from Colorado felt that Republican votes reflected the desires to “stop abortion… stop gay marriage from ruining our country… and give us our coal jobs back.” Other Democrats felt that their opponents were mostly motivated by the GOP’s “opposition to Obamacare,” “lower taxes” and to support a party that “reduced unemployment.” [3] Taking the perspective of others proved to be really hard The divide in the United States is wide, and one indication of that is how difficult our question proved for many thoughtful citizens. A 77-year-old Republican woman from Pennsylvania was typical of the voters who struggled with this question, telling us, “This is really hard for me to even try to think like a devilcrat!, I am sorry but I in all honesty cannot answer this question. I cannot even wrap my mind around any reason they would be good for this country.” Similarly, a 53-year-old Republican from Virginia said, “I honestly cannot even pretend to be a Democrat and try to come up with anything positive at all, but, I guess they would vote Democrat because they are illegal immigrants and they are promised many benefits to voting for that party. Also, just to follow what others are doing. And third would be just because they hate Trump so much.” The picture she paints of the typical Democratic voter being an immigrant, who goes along with their party or simply hates Trump will seem like a strange caricature to most Democratic voters. But her answer seems to lack the animus of many. Democrats struggled just as much as Republicans. A 33-year-old woman from California told said, “i really am going to have a hard time doing this” bu then [4] offered that Republicans “are morally right as in values, … going to protect us from terrorest and immigrants, … going to create jobs.” Voters like these – baffled but not hostile – would seem to represent an opportunity. Their answers tell us that they might actually be interested in better understanding those at the opposite end of the political spectrum and that motivation could be the first step of a long journey toward reducing incivility and polarization. Whether such voters can long endure in today’s media and social media environment is a critical question – if they can endure and even grow, then the prospects for bipartisan cooperation in areas of shared concern will be possible. If not, polarization will continue to rise. Contact information Eric Plutzer Michael B. Berkman Director of Polling Director Mood of the Nation Poll McCourtney Institute for Democracy Editor Public Opinion Quarterly Professor of Political Science The Pennsylvania State The Pennsylvania State University University Plutzer@psu.edu mbb1@psu.edu How the Poll was Conducted This poll was conducted between November 7[th] and November 12[th], 2018 by YouGov in partnership with the Penn State McCourtney Institute of Democracy. All Mood of the Nation questionnaires are designed by the McCourtney Institute polling team, with the fieldwork conducted by YouGov, an online polling organization. The YouGov panel includes over 1.8 million individuals who agree to complete occasional surveys. The 1,000 individuals who completed the McCourtney Mood of the Nation Poll were matched to the joint distribution from the Census’s American Community Survey in terms of age, sex, race/ethnicity, and years of education. The frame was augmented by matching to the November 2010 Current Population Survey and the Pew Religious Life Survey in order to include voter registration, [5] |Eric Plutzer Director of Polling Mood of the Nation Poll Editor Public Opinion Quarterly The Pennsylvania State University Plutzer@psu.edu|Michael B. Berkman Director McCourtney Institute for Democracy Professor of Political Science The Pennsylvania State University mbb1@psu.edu|Col3| |---|---|---| political interest and party identification in the selection model. The unweighted data are broadly representative of the US population in terms of age, education, and region. The data have been weighted to adjust for variation in the sample from the adult United States population with respect to demographic variables such as geographic region, gender, race/ethnicity, age, and education, and political variables such as voter registration status and political interest. Weighting details are described in a paper by Ansolabehere and Rivers. [1] Core questions asked in every Mood of the Nation Poll can be viewed here. Supplemental questions reported in this report are as follows: S1. In the November elections for the House of Representatives, that ended on Tuesday: I did not vote I voted for the Republican, YYYY I voted for the Democrat, XXXX I voted for an independent or third-party candidate I voted in the election, but not for the US House of Representatives S2. Nationwide, more than 40 million Americans voted for <<opposite party: Democratic / Republican>> candidates. In your opinion, how many citizens voting for the <<opposite party: Democrats / Republicans>> did so because they sincerely believe that the <<opposite party: Democratic / Republican>> party is best for the country. a. Almost all of them b. Most of them c. About half of them d. Fewer than half e. A few of them f. Almost none S3. You said that << answer to nov18_homany >> <<opposite party: Democrats / Republicans>> sincerely felt that the <<opposite party: Democratic / Republican>> party is best for the country. Now, we would like you try to put yourself in the place of those voters and answer the following question as if you were trying to answer exactly the way they would answer. You voted for the <<opposite party: Democratic / Republican>> candidate. Can you tell us three reasons why you think the <<opposite party: Democratic Party / Republican Party >> is best for the country? My reasons are… 1 Ansolabehere, Stephen, and Douglas Rivers. (2013) \"Cooperative survey research.\" Annual Review of Political Science 16: 307-329. [6] Is this all Joe Biden's fault? Author: Cillizza, Chris, Date: 2024-11-09 Collections: Hot Takes US Elect 2024 Zotero Key: 2DDWX8S8 Cite Key: Cillizza24allBidensFault Zotero Item | Lit Note 00:00 the blame game is on for Democrats and 00:04 lots and lots of people are pointing the 00:06 finger at Joe Biden is that fair all 00:08 right let's talk about it so let's go 00:10 through first the number of people in 00:13 the last 24 to 48 hours who are saying 00:15 this was essentially Joe Biden's fault 00:18 and that starts with former house 00:20 Speaker Nancy Pelosi who you will 00:22 remember was the Catalyst to get Joe 00:25 Biden out of the race after his 00:27 disastrous June 27th debate so um Pelosi 00:30 talked to the New York Times podcast 00:34 unit actually um about the race and let 00:37 me just read a couple of her quotes this 00:40 is py had the president gotten out 00:42 sooner there may have been other 00:43 candidates in the race um the 00:46 anticipation was that if the president 00:48 were to step aside that there would be 00:50 an Open Primary and she went on to say 00:52 this and as I say comma may have I think 00:54 she would have done well in that and 00:56 been stronger going forward but we don't 00:58 know that that didn't happen we live 01:00 with what happened and because the 01:02 president endorsed KLA Harris 01:03 immediately that really made it almost 01:05 impossible to have a primary at that 01:07 time if it had been much earlier it 01:09 would have been different okay that's 01:11 Nancy Pelosi then there was John favro 01:16 not that John favro the other John favro 01:18 P not the actor and director but the PS 01:21 of America co-founder and former Barack 01:24 Obama speech writer and he said on 01:28 Friday quote Joe Biden's decision to run 01:31 for president again was a catastrophic 01:34 mistake last piece of evidence Josh 01:36 Barrow who uh I love who writes a 01:39 substack newsletter called very serious 01:41 that you should check out here's what uh 01:44 Josh Barrow said in the in the opad in 01:46 New York Times which is called this is 01:48 all Biden's fault he 01:51 said 01:52 um KLA Harris lost the election this 01:55 week but I mostly don't blame her at 01:56 least I don't blame her because of 01:58 anything she did recently since she 02:00 became the unofficial nominee in July 02:01 she played a difficult hand about as 02:03 well as she could have running a 02:04 disciplined campaign that sought to 02:05 reassure Americans about the economic 02:08 issues that trouble them most in a 02:09 political environment that was very 02:10 rough for Democrats and for incumbent 02:12 parties around the world okay then he 02:13 goes on to say but where did that bad 02:15 hand come from it was dealt to her by 02:17 two people President Biden who produced 02:19 a governing record she could not 02:21 effectively defend or run away from and 02:23 herself with all the toxic position 02:25 taking she did in 2019 generating 02:28 endless attack ad for Donald Trump and 02:30 Mr Biden even Bears blame for Miss 02:32 Harris's pre-2020 baggage since he put 02:35 her on the ticket in full awareness that 02:38 she was carrying it that's Josh Barrow 02:42 okay so those are three pieces of EV 02:43 evidence from I think three very smart 02:46 uh and well-informed observers of two 02:48 Democrats and Josh Barrow a journalist 02:50 of uh Democratic politics um is it 02:54 fair uh I think in large part it is let 02:58 me first say you don't lose as 03:00 convincingly as KL Harris lost uh all 03:03 seven Battleground States went to Trump 03:05 Trump's percentage of the vote improved 03:08 in 48 out of the 50 states he won uh he 03:11 did better among black voters he did way 03:13 better among Latino voters he did 03:16 significantly better amount of young 03:17 voters he did better in urban counties 03:19 he did better in rural counties right I 03:20 mean this was a a full scale across the 03:23 board win for for Donald Trump so to say 03:26 it was one thing I always think is tough 03:30 do I think that Joe Biden Bears a 03:32 significant chunk of the blame I do um 03:37 for a lot of the reasons that they 03:39 Pelosi Josh Barrow and John favro talked 03:41 about and and I think it comes down to 03:43 this when Joe Biden ran in 2020 he 03:46 expressly said I'm going to be his word 03:49 a bridge to the next generation of 03:52 democratic leaders while he didn't come 03:55 out and say I'm only going to run for 03:57 one term the expectation was we're going 03:59 to put a guy in place here uh who we 04:02 think can beat Donald Trump just given 04:04 his background and sort of his profile 04:08 but this guy is old right I mean Biden 04:10 was 77 or 78 when he was running right 04:14 so obviously he's going to be in his 80s 04:16 uh at the end of his first term and I 04:17 think the Assumption was among most 04:19 Democrats he probably won't run again 04:21 now I don't think Biden or his people 04:24 ever said like yes he's definitely not 04:26 running but that was the Assumption 04:27 right and then what happened well guess 04:29 what being president is really 04:30 interesting and fun and people don't 04:32 like giving it up voluntarily so I think 04:34 what happened was Biden and his inner 04:35 circle led by his wife Joe Biden 04:38 convinced him and convinced themselves 04:40 that he was still the only candidate who 04:43 could beat Donald Trump and and I still 04:47 believe this that Joe Biden did not 04:50 really believe that KLA Harris if he 04:53 stepped down he believed KLA Harris 04:54 would be the nominee and I think he was 04:56 right even if there was an Open Primary 04:58 like Nancy Peli suggests I think Lis 05:00 probably would have won it sitting vice 05:01 president African-American woman uh hard 05:04 to beat in a Democratic primary but I 05:07 think Biden was convinced that Harris 05:09 could not beat Trump now that would make 05:12 you wonder why he picked her in the 05:13 first place but regardless um and that 05:16 by the way is sort of Josh Barrow's 05:17 Point why did you pick someone who you 05:19 don't think could win a national 05:20 election anyway so I think that informed 05:23 a lot of Biden's decision to run again 05:25 was that he thought Harris could not win 05:28 and he thought he was the only person 05:30 who could win and beat Trump obviously 05:34 June 27th and the debate changed all 05:36 that because it became clear in the 05:39 debate and then in the reporting after 05:41 the debate that people within sort of 05:44 democratic circles in the White House 05:45 had known that Joe Biden was showing 05:48 significant signs of slowing mentally 05:50 and physically while in the White House 05:51 and the debate undeniable disaster I I 05:54 think it's the most consequential debate 05:56 presidential debate we've had in modern 05:57 American history um 06:00 there was just no way around the fact 06:02 that Joe Biden could no longer run 06:03 people thought he was too old and was 06:05 slowing down before the debate and then 06:07 after the debate it wasn't even a 06:09 conversation so he is forced out by 06:12 Pelosi among others Barack Obama is in 06:14 there too and it goes to Harris because 06:17 Democrats figure we can't piss off two 06:19 legs of our base black voters and Women 06:21 Voters we can't just pass over KLA 06:24 Harris because we don't know that she's 06:26 a good candidate and can win but 06:29 KL Harris wasn't a great candidate and 06:31 she didn't win um so to me Joe Biden 06:35 does as I said bear some responsibility 06:37 is he the only person no absolutely not 06:40 look KL Harris was not a great candidate 06:43 she had taken a bunch of positions in 06:45 2019 that were way more to the left of 06:47 the C than than the sort of majority of 06:49 the country she tried to scale those 06:51 positions back she couldn't um she was 06:54 not great at defining herself she 06:56 focused on democracy and fascism at the 06:59 end of the campaign when we knew those 07:00 messages didn't work to sway independent 07:02 voters and persuadable voters um so I 07:07 don't think it's just Joe Biden's fault 07:09 but I I will say that it was in 07:12 retrospect virtually impossible for the 07:15 vi the sitting vice president of a 07:17 deeply unpopular President we didn't 07:19 even talk about that but you know Joe 07:20 Biden's approval rating was in the 07:22 30s um maybe the low 40s and uh a 07:27 majority of people did not think he had 07:28 done a good job on immigration in the 07:29 economy two of the most important issues 07:31 for their vote it was probably 07:33 impossible for uh his vice president to 07:37 win but guess who picked his Vice 07:39 President Joe Biden right 07:41 so in the in the blame pie I would 07:46 say uh 50% Biden 35% Harris 15% sort of 07:52 atmospherics and uncontrollable things 07:53 and Trump trump ran you know in 07:56 retrospect he knew how to get people to 08:00 votee for him so was Biden the only 08:02 person to blame no absolutely not does 08:04 he bear a chunk of the blame a big chunk 08:07 I think he does yeah all right so uh as 08:10 always four things uh number one 08:12 subscribe to this channel if you have 08:13 not daily videos sometimes even videos 08:16 on the weekends uh so please subscribe 08:18 and also I'm doing more live streams so 08:20 that you you know so when you get 08:22 alerted to that if you're a subscriber 08:24 you know I've posted a new video you 08:25 know I'm going to do a live stream it 08:26 just makes it easier to consume my 08:28 content number two like this video just 08:30 give it the old thumbs up thank you dumb 08:33 uh number three uh comment on this video 08:35 am I right who's right is do your blame 08:39 pie out of 100% who gets blame how do 08:42 you do the blame pie and uh number four 08:46 tell 10 friends about this channel okay 08:48 everybody take care this is my ticket 08:50 right here Eric B and rim 1986 paid and 08:53 full I would have voted for them all 08:55 right everybody take care of yourselves 08:57 be well Inside the online offensive that turned out a new generation of men for Trump Author: Zakrzewski, Cat, Date: 2024-11-10 Collections: Hot Takes US Elect 2024 Zotero Key: QQV5R4MG Cite Key: Zakrzewski24onlineTrumpNewMen Zotero Item | Lit Note Democracy Dies in Darkness Inside the online offensive that turned out a new generation of men for Trump Through TikTok posts, memes and podcasts, the Trump campaign harnessed mounting angst among Gen Z men to introduce “Make America Great Again” to young voters. November 10, 2024 11 min 2654 By Cat Zakrzewski, Drew Harwell and Naomi Nix President-elect Donald Trump’s road back to the White House weaved through testosterone-fueled corners of the internet, breaking from the circuit of daytime talk shows and local radio broadcasts. Trump flexed his arms with podcasting National Football League players and chatted about aliens with YouTube wrestler Logan Paul. He served the Nelk Boys Chick-fil-A on his private jet “Trump Force One,” calling the group of YouTube pranksters a “modern-day Johnny Carson.” He riffed on surviving a July assassination attempt with the four-man comedy team behind “Flagrant,” a raunchy podcast that promises to deliver “unruly hot takes directly to your dome piece.” The resulting memes, TikTok posts and YouTube videos landed Trump millions of views online — part of a sprawling online strategy that gave him a direct line to a giant fan base of young American men. @nelkboys · Follow LFG @realDonaldTrump 10:53 AM · Nov 5, 2024 48.1K Reply Copy link Read 377 replies While many political strategists once viewed this group as liberal by default, young men between the ages of 18 and 29 swung enormously for Trump, shifting rightward by eight percentage points since 2020, according to network exit polls. The day after the election, Team Trump advisers on X praised a group of young, self-described “s**tposters fueled by** a lot of black coffee and Zyn” nicotine pouches for rewriting the political campaign playbook and introducing “Make America Great Again” to a new — and terminally online — generation of men. online fringe known as the “manosphere.” The influencers and podcast hosts don’t have movie or TV deals, but they command bigger audiences than many traditional Hollywood actors thanks to online media platforms such as YouTube, Instagram and TikTok, where millions of Americans now get their news and spend their time. Some of the influencers also boosted Trump offline: The Nelk Boys led a $20 million get-out-the-vote effort, focused on door-knocking at fraternity houses and targeting young men at college football tailgate parties. This risky strategy of courting the audiences of Joe Rogan and the Nelk Boys more aggressively than viewers of traditional news programs or “Saturday Night Live” paid off on Election Day, propelling Trump’s first popular vote victory. “People were able to see him for who he really is and not through the characterization of him by the mainstream media,” said Alex Bruesewitz, a Trump campaign adviser who developed his podcast strategy. “That was a massive, massive victory for us this go around.” The 78-year-old businessman — now the country’s oldest president-elect — was able to achieve what some are hailing as a political realignment in part by adapting his decades as a TV showman to live-streamed interviews on X, three-hour podcast chats and eye-catching TikToks. These tactics reintroduced first-time voters to a candidate whose notorious 2015 ride down a golden Trump Tower escalator occurred when they were in elementary and middle school. Uniquely isolated during the pandemic, these young men turned to podcasts and influencers to navigate loneliness and stress, said John Della Volpe, the director of polling at the Harvard Kennedy School Institute of Politics. @theovon Trump’s resounding showing in what some called the “battle of the sexes” election reflected his ability to harness a mounting angst among young men about their economic and cultural status in a society in which young women are more educated and have more professional opportunities than previous generations. “Trump is so good at channeling that frustration for people who feel they have no power,” said Hasan Piker, one of the internet’s most prominent left-wing streamers “He’s the best at saying, ‘You’re right to be angry, and I’m going to tell you exactly why you should be angry.’” A cohort of young men were primed for that message. Video games, wrestlers and fries Trump’s first foray into podcasting came in 2022, 14 months after he left Washington in disgrace after failing to respond to the Jan. 6 attack on the U.S. Capitol. At the time, he was largely exiled by major TV networks and banned from most social media. But the Nelk Boys were happy to welcome the former president on their show “Full Send.” In the interview taped at Trump’s Mar-a-Lago Club, the former president talked about his friendship with Ultimate Fighting Championship CEO Dana White, lamented the rising price of oil and claimed that Russian President Vladimir Putin would never have invaded Ukraine if he were still in the White House. “We look weak, we look stupid, we look like we don’t know what we’re doing, and nobody has ever seen the country like this,” Trump said on the episode, presented by Happy Dad Hard Seltzer. YouTube pulled down the episode, citing a policy that at the time banned claims of widespread fraud during the 2020 election. The takedown, however, became an advertisement for the show and for Trump, who swiftly put out a statement decrying the “Big Tech lunatics” who removed the interview with the “very popular” influencers. The appearance was just the start of Trump’s foray into the world of bro influencers, which accelerated as Election Day approached. He golfed with Bryson DeChambeau, talked energy prices with personal-finance podcaster Dave Ramsey, and discussed religion and marijuana with the cerebral computer scientist Lex Fridman. John McEntee, a former Trump White House staffer who has gained 3 million followers on TikTok for his right-wing quips such as “It’ll be funny to see how quickly feminists embrace the kitchen when World War 3 breaks out,” said Trump’s embrace of online media helped him capture a giant audience by “speaking to people where they are.” figure, joining these young platforms (like TikTok) was such a symbol that he was part of the culture.” Some of Trump’s appearances were with online influencers preferred (and suggested) by his 18-year-old son, Barron, such as the video game streamer Adin Ross, who live-streamed a chat and dance routine with Trump at Mar-a-Lago. “My son Barron says hello, by the way. He’s a big fan of yours,” Trump told Ross, 24. 5.8M 67.7K See U.S. election results from The Associat258.2 ed Press. K Undertaker.” World Wrestling Entertainment fan pages had higher engagement than those of any other sport, and their audience was more moderate than many would assume, Bruesewitz said. “It went like wildfire throughout all of the different fan pages,” he added. “We reached tens of millions of people that were not our average audience through that interaction.” That targeting of young men extended to the campaign’s digital-media operation, which worked to blast pro-Trump messages onto social media accounts with tens of millions of followers. Dylan Johnson, 26, a deputy campaign director of communications, said the team adopted a “scorched earth” strategy when dealing with critics or journalists they felt were misrepresenting Trump — part of the team’s “no free shots on goal” policy that responded to every hit with a counterattack. “We were in the cockpit, and we could fire at will whenever we saw something worth hitting,” Johnson said. “We can be a little flamethrower-y at times, but that’s part of the charm.” @karolineleavitt · Follow .@TrumpWarRoom grinding away to victory at the Team Trump HQ tonight The best & most hard working War Room in the business !!! 7:20 PM · Nov 5, 2024 2.9K Reply Copy link Read 101 replies edits.” As Trump drove at disaffected young men, Harris detoured the other way. She appeared on “Call Her Daddy,” a podcast popular with young women, and “All the Smoke,” a show hosted by two former National Basketball Association players popular with young, non-White men. Long podcast interviews have a second life in clips, which circulate on social media and in stories in traditional news outlets, Republican digital strategist Eric Wilson said. In an interview with comedian Theo Von, Trump candidly discussed how his older brother Fred’s struggle with addiction shaped his own decision not to drink, use drugs or smoke. A clip from the interview had nearly 3 million likes on TikTok. 2.9M 25.6K 618K @theovon talkin recovery w/ @President Donald J Trump original sound - Theo Von adopted an irreverent tone of its own through attention-grabbing stunts like showing Trump slinging fries at a McDonald’s and driving a garbage truck, creating meme-worthy moments his team hoped would soften his public persona and break the mold for a political campaign. Jackson Moore, a 19-year-old Iowa State University student, followed the campaign largely through watching reaction videos and clips from right-wing outlets like the Daily Wire on Instagram, TikTok and YouTube. On mainstream media, Trump was compared to Adolf Hitler and his supporters to “fascists,” he said. But the online videos presented a more moderate Trump, Moore said, one “genuinely concerned” about real Americans’ issues. So on Election Day, Moore cast his ballot for Trump. The ‘mighty and powerful’ Ten days before Election Day, Trump left Michigan supporters waiting on an airplane tarmac in the cold, blaming the almost three-hour delay for his rally on taping an interview with Rogan, the world’s most popular podcaster. The wide-ranging conversation covered the existence of UFOs, Trump’s continued denial of the 2020 election results and North Korean leader Kim Jong Un — and reached Rogan’s massive 80 percent male audience, according to data from Edison Research. Rogan’s interviews with Trump and his top allies, Vice President-elect JD Vance and X owner Elon Musk, have been viewed in total more than 78 million times on YouTube alone. The strategy hit voters beyond the base of the Republican Party, where Trump had limited success in 2020. “If you’re over the age of 45, you have an opinion of Trump, it doesn’t change. But there are a ton of young voters out there voting for the first time ever, and podcasts are a pretty effective way of reaching them,” Piker said. “Those guys aren’t watching MSNBC.” Meanwhile, Harris spent the final days of the campaign with more traditional Hollywood icons. She appeared alongside actress Maya Rudolph in a sketch that opened “Saturday Night Live” and, on the eve of the election, she made her final appeal to voters alongside Oprah Winfrey and Lady Gaga at a rally on Philadelphia’s Benjamin Franklin Parkway. The same night, Rogan announced his endorsement of Trump, who celebrated the news onstage at a rally in Pittsburgh. On Election Day, Trump boosters made an overt appeal to men. “Get every man you know to the polls,” tweeted Stephen Miller, a top adviser in the first Trump White House. Boys, the “Bussin With the Boys” crew, Von and “the mighty and powerful” Rogan. It remains to be seen whether Trump — and the Republican Party — will be able to hold these young supporters as his administration gears up to pursue an aggressive plan to reshape American society. Many Trump supporters in their late teens and early 20s admire Trump’s brand and his masculinity; they don’t recall his decision to pull out of the Paris agreement to combat climate change or his defense of white nationalists in the wake of the deadly 2017 Charlottesville rally, Della Volpe said. “I don’t think the vast majority of younger, first-time Trump voters are voting for extreme policy,” he added. Meryl Kornfield and Razzan Nakhlawi contributed to this report Media Mix Modeling - How To Use It to Drive Growth Author: Goodway Group, Date: 2022-10-05T13:16:52+00:00 Collections: MediaAdsPolit Zotero Key: KGPZVHW3 Cite Key: Goodway22MediaMixModeling Zotero Item | Lit Note “You can’t manage what you don’t measure.” This is the idea behind media mix modeling. If you’re not measuring your marketing efforts and analyzing them regularly, how will you know what’s working? How will you know whether or not you are spending your budget effectively? The answer: you won’t. Your company will miss out on opportunities to optimize marketing efforts and increase ROI. A data-driven approach to digital marketing will help you understand how people interact with your content and campaigns. You’ll better understand what they want, and how to improve their experience to convert and retain customers. Media mix modeling helps you identify the effectiveness of various marketing channels and make informed decisions about where to invest your budget. But, what is media mix modeling precisely? How can you use it to grow your brand? A media or marketing mix is the combination of all the media or communication channels you use in your marketing. For example, your media mix may consist of traditional and digital channels—print, broadcast, linear and connected TV, social media, mobile app, and search engine marketing. Media mix modeling (MMM) is an analysis technique to help advertisers understand the impact of different media channels on their bottom line. You may also hear it referred to as marketing mix modeling. You see how each channel in your marketing or media mix influences brand awareness, purchase intent, and other key performance indicators. To do this, you need historical data from multiple sources. Then, you have to aggregate and analyze that data to identify trends that can impact conversion like seasonality, pricing, promotions, and so on. The result is an actionable report that shows what media mix will work best for your business at any given time of year. Media mix modeling differs from attribution because it’s more holistic. It helps you see the big-picture impact of marketing strategies across channels. Attribution looks at individual channels to determine whether they’re responsible for driving sales and revenue. Attribution helps you understand how each channel contributes to your overall success, but it doesn’t consider how they fit together as part of an integrated marketing strategy. In contrast, media mix modeling is used when there’s a need to analyze long-term trends and understand how current efforts will impact future results. Attribution models like multi-touch attribution are incredibly important. However, brands can benefit from combining strong attribution with media mix modeling. When you use media mix modeling, you’ll get aggregate data showing: • How much money you spent on each channel and overall* • What impact that had on marketing performance and sales* On top of that, many marketers also use incrementality measurement. When combined, all three create a complete picture of your marketing performance. Media mix modeling provides a way to measure and compare the effectiveness of media channels, as well as predict future performance. It can help you decide which channels to invest in and which to cut, as well as predict the performance of new campaigns. It also helps you determine how much of your budget you should allocate to each channel. Here’s how: Multi-Linear Regression MMM uses a multi-linear regression model to estimate the relationship between media spending and sales volume. In statistics, multiple linear regression uses two or more independent variables to predict the outcome of a dependent variable. In marketing, this means you are comparing independent variables (ad spend) on each channel with dependent variables (sales and market share). Marketers can use MMM at all levels—from brand managers to CEOs—to understand how their marketing efforts are working (or not). MMM can be a messy, complex endeavor. There are several steps involved, and they may vary depending on the nature of your business. Here’s an overview of how it works: 1. Data collection Data is essential to any marketing strategy, and media mix modeling is no exception. Not only must you collect data, but the quality of that data can make or break your decision-making process. First, you need to collect data from real people and not just any data. You should use first-party data. If you’re using unreliable or incomplete data to create your models, they probably won’t be very accurate. Before moving forward with MMM, you must ensure that the data is correct, complete, and representative of your target audience. The data you collect should be as specific and detailed as possible, including information about the person’s age, gender, location, income level, education level, and occupation. For MMM, specifically, you’ll want to know your spending and costs for each channel you use. Then, the CVR or conversion rate, transactions, and sales. 2. Channel mapping Next, map out the channels you will test (if any) and ensure there’s a straightforward way to gather analytics about them all. For example, if you’re testing social media ads on Facebook and Instagram, you should also include a link to a survey that asks respondents which platform they saw your ad. This will help you determine which channel is the most effective and whether it’s worth investing in more ads on that platform. Be very clear on how you will get the analytics you test, so you can easily compare the different channels’ results and determine which is most effective. 3. Aggregating results Once you have your data, it’s time to analyze it and determine which channel was most effective at driving traffic and sales from each ad campaign. This will allow you to decide better what marketing efforts will succeed or fail based on all available information combined instead of relying on anecdotal evidence from each piece alone. 4. Determine the cause Once you know which ad was more effective and why you can use this information to build a better strategy for your next campaign. If you saw a lower ROI from the ad campaign, it might be because your audience is skewed toward people who are less likely to buy. If this is the case, don’t waste time and money trying to convert these people through more ads. Instead, focus your efforts on attracting new customers (or ones who are more likely to buy). 5. Keep testing and learning Note that this is not the end of your testing. It’s simply a step along the way. You’ll want to continue running these tests until you’ve found what works for your business and can replicate that success repeatedly. Media mix modeling will show you where to allocate your marketing spend based on what has worked best historically across all channels at every stage of your campaign lifecycle: awareness, consideration, purchase, and even loyalty building. You can use this data to continually optimize your campaigns so that they get better over time. This is especially important for eCommerce companies, which generally have a higher return on investment (ROI) than traditional businesses. If you’re already running ads and want to take your marketing strategy to the next level, consider trying out media mix modeling. Media Mix Modeling is a tool for measuring how much you should spend on different types of advertising. It helps you to decide how much money to spend on each ad and where it will get the best return on investment (ROI). Let’s say you’re an advertising manager at a company selling products via traditional retail channels. You want to know how much more profitable your advertising efforts will be if you increase spending for some or all of your campaign components (TV ads, billboards, print ads) To do this, you run through a few steps: 1. Determine how much you spend on each type of advertising (e.g. $10K for TV ads and $50K for influencer marketing). 2. Calculate each channel’s average return on investment (based on your current campaign). For example, if you spent $10K and received $100K in revenue from your sales campaign, your return would be 10%. 3. Calculate the difference in return between your current campaign and an updated one by subtracting each channel’s average ROI from its respective cost. 4. Add all these differences to get a total value of increased ROI. For example, let’s say your TV ad campaign was $10K but only returned $100K in revenue. On the other hand, an influencer marketing campaign cost $50K but returned $650K. Through MMM, you might consider influencer marketing more profitable, and decide to increase your spending there instead of on TV. Media mix modeling is not simply a matter of spending more on the media you think will work best. It’s about finding that perfect blend of channels. Then optimizing them to reach your target audience at the right time, in the right place, and for the right price. The media mix modeling process starts with an understanding of your target audience. You need to know where they are, what they do, and how they think to make the most effective ad spend decision. The next step is identifying which channels will reach the right people at the right time. Once you’ve determined which media outlets are best suited for your campaign goals, optimize their performance against your objectives and goals. The result is an optimized media mix that will provide the best return on investment for you. The future of media mix modeling is promising. It allows advertisers to make decisions based on data instead of intuition. With it, you can quantify how much each type of media contributes to your success. budget allocation and campaign strategy. It also allows them to create more effective campaigns, leading to greater ROI. Goodway has data scientists that can conduct media mix modeling, along with other techniques, to optimize your marketing. With us, you can easily analyze how different media channels work together and find the most effective strategy for your brand. Contact us today . What We Learned About Celebrity Culture in the 2024 Election Author: McKinley, Jesse, Date: 2024-11-08 Collections: Hot Takes US Elect 2024 Zotero Key: LGIEF2TB Cite Key: McKinley24WhatWeLearned Zotero Item | Lit Note In an election season in which both parties sought out any possible edge, Democrats clung to one seemingly clear-cut advantage: Celebrities including Taylor Swift and Bruce Springsteen broke hard for that party, even as voters did not. President-elect Donald J. Trump had his own famous supporters, drawing from a more masculine cohort, with figures like Kanye West and Mel Gibson, as well as less mainstream acts like Kid Rock and Jon Voight, also weighing in with their endorsements. The result was a split-screen of American celebrity — two sets of famous people for two halves of the country. But by and large, the biggest names in entertainment said Vice President Kamala Harris should be elected to the nation’s highest office. In the end, it did not seem to matter much. Ms. Harris was decisively defeated on Tuesday, despite the backing of a megastar like Beyoncé. On the singer’s Instagram page, one commenter put it succinctly. “America is tired,” wrote Albert Pennachio, an independent voter who lives in Statesville, N.C. “And we don’t care what celebrities think anymore.” It wasn’t always this way: Celebrity endorsements used to seemingly carry substantial weight, with influential figures like Oprah Winfrey helping to magnify lesser-known candidates (such as Barack Obama in 2008). But that sway seems to have lessened as influencers, onetime “micro”-celebrities and podcasters like Joe Rogan — a former comedian with amorphous, largely libertarian political views — have gained bigger audiences. (Mr. Rogan endorsed Mr. Trump on Monday, but only after some 80 million people had already voted early.) Chris Lehane, a former Democratic political consultant who worked in the Clinton White House, said that the move away from following celebrity viewpoints was part of a broader, generational shift from traditional means of communication and advertising toward a more fluid, fast-moving cultural landscape. “It is actually the more digitally native influencers who are able to leverage their status, content and distribution channels that move the needle,” Mr. Lehane said. Glen Bolger, a veteran Republican pollster, said that voters always like it “when a celebrity reinforces your own view by endorsing the candidate you support.” “But that doesn’t mean it changes anybody’s minds,” he added. “If you’re a Trump supporter and a celebrity endorses him, well, that’s a smart celebrity, right? If you’re a Trump supporter and they endorse Kamala, well, I’m still going to watch their movie or their concert, but I’m not going to listen to their political advice.” Add to that a general fatigue with celebrities — who, by their nature, are not like you or me — and their political views may not matter. You might buy a ticket for a Taylor Swift show, after all, but you don’t have to punch the ticket for her preferred candidate. Indeed, William F.B. O’Reilly, a Republican strategist, said that endorsements by some celebrities may actually drive voters in the opposite direction. “Celebrity endorsements say a lot: they say you’re a liberal, an elitist, and a cultural progressive,” Mr. O’Reilly said. “An Oprah or Clooney endorsement is the kiss of death in large swaths of the country now.” Of course, the reasons for any vote, in any election, are myriad and personal. But using the preference of the famous as a guide for the general public’s mood — or voting inclination — seems to be a losing proposition. Janice Min, the chief executive of Ankler Media and former president of The Hollywood Reporter and Billboard, said there wasn’t much question what effect Ms. Harris’s celebrity surrogates had on the race. “They did not work,” she said Wednesday afternoon, adding an expletive, and referring to a list that includes Beyoncé, Mr. Springsteen, Ms. Winfrey, Jennifer Lopez and Bad Bunny. On one hand, Ms. Min said, Ms. Harris spent the better part of the summer and the fall sending out the message that a second Trump presidency would involve him bending at the knee of his fellow billionaire businessmen. On the other, she’d been anointed as the Democratic nominee by party leaders without a proper primary process, after which she aligned herself with the types of surrogates who can ignite resentment because of how much more money they have than most of their fellow citizens — and during an election in which most voters cited the economy as a top concern. “She couldn’t get the endorsement of the Teamsters, but she got the endorsement of Hollywood,” Ms. Min said. “That’s a tough message for a lot of voters. It was a show of dazzle in an election where clearly people were not looking for dazzle.” — Jacob Bernstein Fame Outside of Hollywood The Trump campaign’s roster of traditionalist celebrity endorsers were an older, mostly white bunch. They included the wrestler Hulk Hogan (who memorably tore his shirt asunder at the Republican National Convention), the erstwhile “S.N.L.” actor Rob Schneider and the actor Jon Voight. Yet this group captures little of how the Trump campaign actually leveraged star power to bring home a victory. Instead of courting conventional actors or artists, the campaign broadened its definition of what a celebrity is*.* In Mr. Trump’s world, podcasters, spurned television show hosts and Silicon Valley entrepreneurs turned right-wing pundits like Elon Musk were the focus. His aim was to permeate the internet enclaves where M.M.A.-loving, crypto-curious guys congregated. Mr. Musk, Tucker Carlson and the U.F.C. founder Dana White (a star in his own right with nearly 10 million Instagram followers) spoke at his rallies, and in the late stages of his campaign, Mr. Trump appeared on Joe Rogan’s podcast, the Kick streamer Adin Ross’s channel and the Nelk Boys’ podcast. Some of these figures are as affluent and powerful as their Hollywood counterparts, but their supporters still seem to see them as relatable and grass-roots. — Jacob Gallagher Trump as a Sui Generis Celebrity Of course, the biggest celebrity involved in the 2024 presidential election was running for office. Mr. Trump’s political career has in some ways obscured the fact that he is an enormous star and a consummate entertainer: charismatic (at least to his supporters), ubiquitous and with an army of fans that includes a comfortable majority of American voters. A former reality TV star and a longtime tabloid fixture, Mr. Trump has an eye for self-branding that has supercharged his rise through American politics, from the afternoon he descended a golden escalator in 2015 to the moment he paused after a would-be assassin’s bullet nicked his ear to create the most indelible image of this campaign season. His defiant reaction roused his followers and drew admiration from figures across the political spectrum. It revealed, yet again, Mr. Trump’s great showman’s instinct; he is, on our national stage, un-upstageable. When you have that, who needs A-listers to vouch for you? Maybe Mr. Trump knew that A-listers would have drawn too much attention away from him — or maybe he’s got a sixth sense for the national mood. — Joseph Bernstein The Limits of Taylor Swift On the eve of the election, some noticed the conspicuous absence of one pop star, even among hordes of others: Taylor Swift. After Ms. Swift spoke out in support of Ms. Harris on the night of her debate against Mr. Trump — delivering a coveted endorsement that her opponent falsely claimed for himself a month earlier — Democrats rejoiced over what seemed like a surefire boost to Ms. Harris’s campaign, particularly with young voters. But though Ms. Swift has the ability to command legions of fans, crash ticketing websites and shape entire countries’ economies, her influence on political outcomes appears more limited, and some are beginning to wrestle with the idea that Ms. Swift herself (as well as other celebrities of her stature) may not want to be a political voice. In an article for the Cut published on Monday, the writers Olivia Craighead and Danielle Cohen observed that rather than heading to Pennsylvania to campaign for the vice president, Ms. Swift “spent Monday night at a football game in Kansas City, cheering on her boyfriend.” Still, the hope that Ms. Swift’s appearance in Scranton or elsewhere on the campaign trail could have made a substantial difference for Ms. Harris now looks much like the hope that any single force could have stopped Mr. Trump’s victory: probably misplaced. — Marie Solis In 2016, Vogue broke a 100-year precedent, and endorsed a candidate for president: Hillary Clinton. But despite the endorsement; despite the fact that Anna Wintour, the magazine’s editor, is a powerful fund-raiser; and despite the fact that Mrs. Clinton had already been on the cover as a first lady and had been profiled during the 2016 Democratic primary, as her party’s nominee, she did not do a cover again. Ms. Harris, however, did. She actually appeared on Vogue’s cover twice: once in 2021 as vice president, and last month on the “digital” cover as the Democratic nominee, along with the cover line “The candidate for our times.” You can understand the decision. Ms. Wintour is the closest thing to a celebrity editor the magazine industry has. Vogue is a star-making platform, with an Instagram following of 50.1 million, and a print circulation of over 1.2 million. That’s a lot of potential voters. And the issues that affect women’s lives are part of the magazine’s remit. But its power has been eroding along with its stranglehold on the industry, as voices from all kinds of backgrounds offer their own definition of style. And while fashion may be a universal language, it does not always translate well when it comes to politics, especially for a candidate claiming to understand the economic pain facing many Americans. After all, how do you believe someone is attuned to the rising price of milk when they are featured in a glossy that includes handbags that can cost more than some people’s monthly rent? Despite the fact Annie Leibovitz took Ms. Harris’s portrait, that might have been a very risky look. — Vanessa Friedman Jacob Bernstein, Joseph Bernstein, Vanessa Friedman, Jacob Gallagher, and Marie Solis contributed reporting. Democrats did better than Harris downballot, providing glimmer of hope Author: Kane, Paul, Date: 2024-11-09 Collections: Hot Takes US Elect 2024 Zotero Key: HPU9KCJD Cite Key: Kane24ColumnDemocratsDid Zotero Item | Lit Note POLITICS Democrats did better than Harris downballot, providing glimmer of hope Democrats minimized Senate losses and created possible gains in House, leaving key questions: Why did Harris and GOP candidates underperform? Sen. Tammy Baldwin, left, and Wisconsin Gov. Tony Evers, both Democrats, visit a grocery store in Milwaukee on Election Day. (Joel Angel Juarez for The Washington Post) Column by Paul Kane November 9, 2024 at 2:43 p.m. EST In his valedictory speech, President-elect Donald Trump rattled off what he claimed were supposed victories in Senate races that propelled Republicans to the majority in the chamber. “Montana, Nevada, Texas, Ohio, Michigan, Wisconsin,” Trump said early Wednesday, before boasting how “we’ll be keeping control of the House.” Get the latest election news and results Nevada, Michigan and Wisconsin. And in the House, Democrats still have a narrow chance to claim the majority — and, at minimum, they staved off the deep losses that usually come alongside losing the presidency. Despite so far appearing to secure the popular vote victory — likely for only the second time for a Republican candidate in the past nine presidential elections — Trump’s coattails did their jobs unevenly. True, he ran so far ahead of Vice President Kamala Harris in Montana and Ohio, by about 12 and 20 percentage points, respectively, that he pulled along two novice candidates, though only with single-digit victories. In the five states that were both presidential and Senate battlegrounds, Trump won all five. Yet Senate Republicans lost four and are projected to win Pennsylvania by such a narrow margin that Democrats haven’t yet conceded. In 2016 and 2020, only Sen. Susan Collins (R Maine), a centrist with a decades-long brand, won a Senate race in a state where the opposing party’s candidate won the presidential ballot. And in the House, Democrats could at least yield a net gain of a couple of seats. That would leave the GOP with the smallest majority in almost 110 years, even smaller than its current Speaker Mike Johnson (R-Louisiana) to govern. Normally, the party suffering a multimillion ballot loss in the popular presidential vote would suffer in lower-profile downballot races, but enough voters in critical battlegrounds split their tickets to avoid a full Democratic wipeout. A cardboard cutout of the Democratic nominee, Vice President Kamala Harris, at a festival in Las Vegas on Nov. 2. (Eric Thayer for The Washington Post) Demoralized over Trump’s victory, Democrats have begun to realize that their downballot performance wasn’t so bad, but they wonder what drove so many of their voters away from Harris. Republicans are definitely gleeful over Trump’s commanding win for the presidency but are just beginning to wonder why more voters did not follow suit downballot. The key takeaway, for Democratic lawmakers and strategists, is that the overall party brand is in poor shape. Some Democratic policies are popular, but candidates in swing states need to aggressively campaign even in rural places that favor Republicans so that enough voters there will listen. “If you don’t respect their culture and their lifestyle, they’re not even going to hear you when you talk about your policy positions,” Sen. Thursday. Warner’s home-state colleague, Sen. Tim Kaine (D), successfully won reelection and nearly doubled Harris’s margin of victory because of his performance in places like Lee County. In the southwestern corner county that borders Kentucky and Tennessee, Trump defeated Harris, 86 percent to 14 percent, a margin of more than 7,000 votes. The GOP Senate candidate, Hung Cao, underperformed — winning the county by 76 percent, a margin of about 5,000 votes. “He didn’t win, but he ran way ahead of the vice president,” Warner said of Kaine’s results in places like Lee County. He noted that “national Democrats” — who don’t have time to campaign in these rural towns — are perceived as denigrating the culture, while smart Virginia Democrats spend time there. Sen. Tammy Baldwin (D-Wisconsin), considered much more liberal than Kaine, credited that same approach for her narrow victory in a state that Trump won by 1 percent. “The way we won this race is the way I have always approached the job: We did everything — everywhere — all at once. I travel to red, of our state,” Baldwin said in her victory speech. Her prototypical rural performance came in Buffalo County, along the Mississippi River on the western edge of the state. Harris lost by almost 2,500 votes, 30 points, while Baldwin lost by fewer than 1,900 votes, about 25 points. These culturally conservative voters probably nodded their heads at the Trump campaign ads accusing Harris of supporting gender-affirming care for prisoners, from a 2019 interview she did. Yet enough of them spared Baldwin, Kaine and Democratic candidates who spent time in their regions. As of late Friday afternoon, Democratic Senate candidates gained higher shares of votes than Harris did in 78 percent of counties in Michigan, Ohio, Pennsylvania and Wisconsin, according to The Washington Post’s data analysis. The flip side, of course, is how GOP candidates could not come close to matching Trump, who outperformed them in 97 percent of counties. “Republicans win back Senate control, but left races on the table,” Jessica Taylor, the Senate analyst for the Cook Political Report With Amy Walter, wrote Thursday. incumbents on their “front line” list, those considered in potential danger; at least 28 have won reelection. And Democrats have knocked off three GOP incumbents, with a handful more in races that are too close to call. Some traditionally liberal strongholds gave Harris only modest wins but Democrats downballot performed fine, particularly in New York. Harris has received less than 56 percent, the lowest for a Democrat in the Empire State since 1988. And yet House Democrats flipped three GOP seats that, combined with a special election win early this year, produced a net gain of four seats from New York this year. The most glaring issue for Democrats is determining how much of Harris’s underperformance is the result of discomfort with a woman of color as commander in chief. Trump’s margin of victory among men grew by five points from 2020, and the most dramatic voter shift came from Latino men. They favored Biden by 23 percentage points in 2020 only to back Trump by 12 percentage points this year, according to exit polls. Sen. Jacky Rosen speaks to Latino voters during a campaign stop i S k N d t id f R i M (M li M /Th Republicans believe a portion of that shift came from this predominantly Catholic bloc of voters opposing transgender rights and other liberal policies. “They think that their activists on transgender issues, climate and immigration — all of these activists on what they call social justice — are somehow reflective of the broader country. They are not,” Sen. Marco Rubio (R-Florida) said in an interview on a Catholic news show. Some Democratic strategists believe that Latino men have suffered such tough economic times that they were more willing to listen to powerful male voices like Trump’s even if they did not believe in his policies. “It deals with how they view voices in the world and understand them,” Dan Sena, a New Mexico expert who oversaw the House Democratic 2018 effort, told The Post in a pre-election article. “And Latino men, generally, well, there are some Latino men … that will embrace strong and wrong over weak and right.” Harris won the heavily Latino state, but only by about half as large a margin as Biden did in 2020. She fell well below the 10-point win racked up by Sen. Martin Heinrich (D-New Mexico), whose vote share was little changed from 2018. h h l d i l i h than 1 percent in 2022, won by four points this time. And in Arizona, Rep. Ruben Gallego (D) was leading by 1.1 points as of early Saturday, with remaining votes expected to lean toward the Democrat. But in Senate races, several Democratic women had success: Baldwin won again in Wisconsin, Rep. Elissa Slotkin won in Michigan and Sen. Jacky Rosen has won in Nevada. The Republican losses are particularly glaring in Nevada and Arizona, where Trump’s final margin is expected to be several percentage points above Harris’s. Just enough Trump voters chose him and flipped to Democrats or a different alternative altogether downballot. In some cases, Senate Democrats caught a break because Trump voters chose him but then voted for a third-party option or left their ballot blank on the Senate race. In Nevada, 80,000 voters in the Senate race selected a third-party candidate or the state’s unique “none of these candidates” option for president. Another 20,000 who voted in the helping send Rosen to victory while Harris lost. In Churchill County, Republican candidate for Senate Sam Brown got 69 percent of the vote. That seems like a big margin, but Trump got 74 percent of the vote in that Nevada county east of Reno. Brown conceded defeat Saturday morning. Republicans in congressional races do not get the same sort of goodwill from these rural voters that Trump receives, while smart Democrats have a chance to earn just enough votes there to be competitive. National Democrats need to follow Rosen’s and Baldwin’s lead and stump in places like that, Warner said. Otherwise, these voters won’t be willing to listen to their policy pitch. “They’re never going to hear your position on education or infrastructure or national security,” Warner said. “And I think we’ve got some work to do there.” By Paul Kane Paul Kane is The Washington Post's senior congressional correspondent and columnist. His column about Congress, @PKCapitol, appears throughout the week and on Sundays. He joined The Post in 2007. @pkcapitol Crime Is Down. Why Do So Many Rural Americans Think It’s Going Up? Author: Hanel, Rachael, Date: 2024-10-16T00:01:00+00:00 Collections: Hot Takes US Elect 2024, NeuroPsychoLinguisticPolitics Zotero Key: IAQ7HAS7 Cite Key: Hanel24ruralCrimeBelief Zotero Item | Lit Note Crime Is Down. Why Do So Many Rural Americans Think It’s Going Up? False claims of “migrant crime” echo the Ku Klux Klan’s fear mongering Rachael Hanel October 16, 2024 ven though statistics show a consistent downward trend in violent crime since the E early 1990s, conservative politicians and media are trying to convince voters that America is under attack. Often these messages are geared toward rural voters to make them think large cities are rife with crime. Colorado GOP chairman Dave Williams speaks before Republican presidential nominee former President Donald Trump at a campaign rally at the Gaylord Rockies Resort and Convention Center Friday, Oct. 11, 2024, in Aurora, Colo. (David Zalubowski, AP Photo) The overarching message: the only way to keep this crime from infiltrating rural areas is to vote for conservatives. Examples abound. On Monday, vice-presidential nominee J.D. Vance visited Minneapolis, stopping at the site of the Third Precinct, which burned in protests after the 2020 murder of George Floyd. Conservatives often criticize Minnesota Gov. Tim Walz, Kamala Harris’s running mate, for not doing more to stop the protests. “The story of Minneapolis is coming to every community across the United States of America if we promote Kamala Harris to president of the United States,” Vance told a phalanx of media and supporters. politicians. That number is nearly 7 out of 10 for Republican voters or those who lean Republican, according to the Pew Research Center. In comparison, three years ago the number of Americans who said reducing crime should be a priority was at 47%. Experts on politics and political rhetoric say that voters in rural areas may be susceptible to claims of violent crime rampant in large cities because instead of seeing cities for themselves, they rely on news reports and political speeches. “Conservative media has painted a wholly false picture of urban centers as ‘Mad Max’ wastelands where you can’t step out the door without getting stabbed or there are people dying of fentanyl overdoses on every corner,” says Brian Hughes, a research assistant professor in the Department of Justice, Law & Criminology at American University. Hughs calls it a “divide-and-conquer” approach. “People who watch these reports are more fearful, less likely to connect with neighbors, less likely to visit the cities and experience lives of people who live there.” The facts of violent incidences are often cherry-picked by politicians and held up as examples emblematic of a wider problem. “The right is totally comfortable perverting the truth and the circumstances of a crime to fit a particular narrative,” says Seyward Darby, editor-in-chief of Atavist magazine and the author of Sisters in Hate: American Women on the Front Lines of White Nationalism (2020, Little, Brown and Company). While voters say violent crime is up and that politicians should make fighting it a priority, they are more likely to believe it is a problem elsewhere rather than where they live. According to the Pew Research Center, 55% of people say there is more crime in their local area, while 77% believe crime rates are up nationally. It’s only logical to realize that crime exists in rural areas. But in small towns where everyone seems to know each other’s business, it can be easier to explain away criminal actions, says Kevin Parsneau, a professor of political science at Minnesota State University, thinking goes, they never had their life together to begin with. So then it becomes a personal thing.” Or, he adds, “You either misunderstand what’s going on in the city, or you don’t see what’s going on right in front of you.” Exaggerating, misleading or simply false claims about crime is not a new tactic from the right. “Fear mongering is one of the classic propaganda techniques. What it does is it softens us up, softens up our critical faculty,” Hughes says. He says this type of propaganda does an end run around intelligence, logic and reason, making it especially effective. This can be particularly true when the suspension of critical thinking is used as a pretext to dehumanize certain groups like immigrants. A constant refrain from both the Trump campaign and Fox News warns that “dangerous criminals” are flooding across the southern border at unprecedented rates, causing a wave of so-called “migrant crime.” The category may be invented but the rhetoric has real consequences, with Trump claiming that criminal illegal immigrants are “poisoning the blood” of this country. Aside from a few highly publicized cases, data show there is no increase in crime attributable to immigration. According to border officials, most migrants are families fleeing violence and poverty. Archive. Frequency of “Migrant crime” as a Google search term. Source: Google Trends. Numbers represent search interest relative to the highest point on the chart for the given time. Darby says that this tactic can be found in the post-Civil War era. The Ku Klux Klan, founded in rural Tennessee, rose to prominence by promoting the idea that the end of slavery posed dangers to a white populace, particularly white women. Then, as now, the racial component of imagined crime was emphasized. “I really don’t think a lot has changed,” she says. Over time, too, the American public has overestimated crime rates. Pew Research Center data shows for the past 30 years, in 23 out of 27 surveys about crime rates, at least 60% of adults said crime is on the rise, even when statistics show the opposite is true. These attitudes on crime reflect media coverage, Parsneau says. Indeed, the saying “If it bleeds, it leads,” is a given on local television newscasts. “Fear of crime correlates more with media coverage of crime than actual crime. So crime Parsneau says. It’s the “mean world syndrome,” a phrase coined by communications scholar George Gerbner in the 1970s. When people learn about the wider world only through media reports, the media can shape a false reality. People generally do not directly consult sources of crime statistics, such as the FBI or Bureau of Justice Statistics. Instead, they rely on local news reports or social media. Even for official statistics, there’s an atmosphere of distrust. “Increasingly we see that people on the right do not care about official statistics and say, ‘No, that’s fake’ or ‘That’s someone lying,’ ” Darby says. Even mainstream media don’t always remind voters of the hard facts, Parsneau says. Reporters, when asking politicians questions, might say something like, “A big issue for voters is crime. What are you going to do about violent crime?” without noting that violent crime rates are actually down. In rural America, emphasis on crime takes attention away from other, more pressing issues, such as the agricultural economy and infrastructure needs. Even though rural voters know those issues are important, “that doesn’t seem to overwhelm their fear that somebody from Mexico is going to sneak across the border and sell them fentanyl,” says Parsneau says. “It makes the assumption that we could reduce the amount of drugs if we could just stop them at the border. Well, they’re actually cooking the meth a few blocks away, so work on that.” Hughes, who is also the associate director of Polarization and Extremism Research and Innovation Lab (PERIL) at American University, uses “attitudinal inoculation” to combat manipulative messages. “It’s a tried-and-true practice demonstrated effectively hundreds of times in lab environments,” he says. Attitudinal inoculation involves showing people a short sample of propaganda and warning them about manipulative methods they will see—for example the use of deep ominous Hughes would like to see more money invested in helping people understand how political messaging can be manipulative. But it can be an uphill battle. Propaganda comes out of the big-money advertising industry. Show people how they are being manipulated politically, and they may also start to see how they are manipulated by advertising. “It’s a tall order to get people with money to fight this,” Hughes says. Even though political manipulation around crime is not new, it’s disheartening to see it continue to wield influence, Darby says. “It reveals how little people learn, how prone we are as a society of making the same mistakes over and over.” But Parsneau says to some degree, this could be politics as usual. “If you’re the out party,” he says, “you want to portray everything that’s happening now as bad, as a justification as to why the Democrats should be thrown out.” Rachael Hanel Rachael Hanel began her career as a newspaper reporter and now teaches creative nonfiction at Minnesota State University, Mankato. She’s the author of Not the Camilla We Knew: One Woman’s Path from Small-Town America to the Symbionese Liberation Army (2022) and We’ll Be the Last Ones to Let You Down: Memoir of a Gravedigger’s Daughter (2013). Have thoughts or reactions to this or any other piece that you’d like to share? Send us a note with the Letter to the Editor form. Want to republish this story? Check out our guide. Barn Raiser Your independent source for rural and small town news. Barn Raiser connects local and national perspectives through a network of writers and contributors who live in and care about rural and small town communities. By giving voice to shared concerns, and by reporting on local organizing strategies, Barn Raiser will leaven the commons with local connections. Contact us: info@barnraisingmedia.com (708) 260-6323 As a reader-supported 501(c)3 nonprofit, Barn Raiser does not oppose or endorse candidates for political office. Copyright 2024 BY BARN RAISING MEDIA (EIN: 88 2659178). Terms and Conditions | Privacy Policy | Cookies | Notice at Collection | Site by Devon Bussell Harris’s advisers blame everything but themselves for their loss Author: Bacon, Perry, Jr., Date: 2024-12-03 Collections: Hot Takes US Elect 2024 Zotero Key: TL5I9JZR Cite Key: Bacon24harrisAdvisersBlame Zotero Item | Lit Note Opinion Perry Bacon Jr. Centrist Democrats should stop blaming progressives for Harris’s loss Whether to use he/she pronouns in emails wasn’t a factor in the Harris-Trump race. Today at 6:00 a.m. EST 8 min 4378 From left to right: Commentator James Carville, Rep. Seth Moulton (D-Massachusetts), and Quentin Fulks, the deputy campaign manager for Vice President Kamala Harris's 2024 campaign. (Matt Winkelmeyer/Getty Images; Bruce Boyajian/The Washington Post; Melina Mara/The Washington Post) Center-left and establishment Democrats are trying to marginalize the party’s left wing in the wake of Vice President Kamala Harris’s loss last month, in some ways mirroring what moderates — including Bill Clinton — did in the late ’80s and early ’90s. Make sense of the latest news and debates with our daily newsletter immediately after the election — and hasn’t stopped since. Commentator James Carville said Harris could “never wash off the stench” of left-wing rhetoric such as “defund the police.” Rep. Seth Moulton (D-Massachusetts) said: “Here we are calling Republicans weird, and we’re the party that makes people put pronouns in their email signature.” Quentin Fulks, who was Harris’s deputy campaign manager, said party activists too often force Democratic candidates to apologize, particularly hurting them with male voters, because, according to him, “men don’t like people who apologize.” I hope this effort is unsuccessful. It’s based on a false premise. The Democrats’ biggest electoral problem isn’t its less-powerful progressive wing, but rather a centrist establishment that clings to power while constantly losing elections and major policy fights. And, as happened in the 1990s, a rightward move by Democrats on policy could hurt some of the most vulnerable people and groups in American society. As election analysis, these takes aren’t particularly strong. Whether to put he/she pronouns in emails wasn’t a factor in Harris’s race against Donald Trump. The “defund the police” slogan was most prominent in 2020 — a year Democrats won the House, Senate and the presidency. Harris didn’t issue a big apology during her campaign. (Election results aside, liti i d l i A i h d am disappointed I have to use space in a published article in 2024 to note that it’s plenty manly to say, “I’m sorry.”) Follow Perry Bacon Jr. So, if these supposed faults of progressives aren’t really why Harris lost, why are center-left Democrats so fixated on them? For three reasons. The first one (and what I suspect is really driving most of the left-bashing) is that the center-left Democratic establishment wants to shift blame for a painful election defeat that by most objective measures is almost entirely the establishment’s fault. People who support defunding the police have almost no power in the Democratic Party. Centrists do. Center-left and establishment Democrats unified behind Joe Biden over Sen. Bernie Sanders (I-Vermont) during the 2020 primaries and were largely supportive of him running for a second term until his dreadful performance at a June debate with Trump. Once Harris became the party’s candidate, she heeded calls from the center-left to run a moderate campaign, emphasizing the importance of the United States maintaining the “most lethal” military in the world and appealing to the wealthy and big corporations. their preferred strategies and candidates have failed — or blame progressives. It’s not surprising they have chosen the latter path. Who would want to admit they lost two out of three elections to Trump? The second reason for the wave of anti-left, post-election screeds is that the center-left wants to diminish progressives’ influence on the party’s values and policy stances. Pinning the left with the 2024 defeat is a way to discredit progressives on even non-electoral questions. We are watching an ideological and policy battle being waged through election analysis. While progressives have had fairly limited influence on Democratic electoral strategy, particularly in the Harris campaign, they do have power in other aspects of the party. Centrists have often complained over the last four years as the Biden administration forgave student loans, put former Elizabeth Warren staffers into senior positions and took other actions that were on progressives’ wish lists. And perhaps even more than their policy power, progressives have gained ground in shaping the cultural and social stances of many important sectors of American society, including politics. The Democratic Party is not forcing anyone, particularly a member of Congress such as Moulton, to put their preferred pronouns in or adopt other more liberal and tolerant stances. But since 2014, particularly at the heights of the MeToo and Black Lives Matter movements, it is almost certainly the case that people who live and work in left-leaning environments, such as Moulton, feel strong social pressure to be supportive of progressive social justice rhetoric and causes if they want to be perceived as good, moral people by their peers. And some corporations do require their workers to participate in diversity-training sessions that employees might object to on ideological grounds. Many centrists have had enough. They want to be able to express their more conservative beliefs and values without being harshly criticized by progressives as insufficiently committed to civil rights and other liberal values. And people in the center-left are much more likely to get a pass from fellow Democrats if their ideological arguments are couched in electoral terms. “Democrats lose when we put our pronouns in emails” is likely easier for Moulton to say in his social circles than, “I don’t see the point of people putting their pronouns in emails.” But the third reason for the left-bashing is more sincere. Some centrist Democrats support police reform, transgender rights, increased immigration and other causes championed by Party can’t win national elections while embracing these stances, particularly on issues of race and identity. They rarely say this directly (and it would be better for this intraparty debate if they did). But lots of center-left figures in the party are hinting at, essentially: “Democrats can be for transgender girls playing high school sports or for winning Wisconsin, and I think winning Wisconsin should be a bigger priority right now.” That is a reasonable position. There is often a tension between winning elections and taking morally correct stances. Abraham Lincoln probably could not have won the 1860 election if he had called for an immediate abolition of slavery. Lyndon B. Johnson had a better chance of being elected president in 1964 than the Rev. Martin Luther King Jr. Perhaps Clinton’s moves to distance himself from civil rights activist Jesse Jackson and other liberals during the 1992 campaign were essential to his victory. Officials in San Francisco who conducted some of the nation’s first same-sex weddings in 2004, including then-District Attorney Kamala Harris, might have hurt John Kerry’s presidential campaign. It’s hard in an election as close as last month’s to be confident that Harris would have won if she had opposed transgender girls playing sports or if the “defund the police” slogan had b i t Al t ki It was bad for gay and lesbian Americans that most prominent Democrats refused to support same-sex marriage until 2012. Post Opinions Podcasts Opinion Opinion [Washin] staff León Krauze, Eduardo Porter and Damir Marusic Impromptu Should Mexico try to reflections appease Trump? But if Democrats had exhausted all other realistic options to win, I might be supportive of them renouncing the party’s support for affirmative action or increased legal immigration. After all, even if Democrats move right on some issues of equality, they are still better than the Republicans. Democrats are nowhere close to having tried everything else, though. Before throwing social justice causes and acti ists under the bus could Opinion León Krauze, Eduardo Porter and Damir Marusic Should Mexico try to appease Trump? candidate for president? Or his vice president, who insists on never criticizing him? How about a party chair whose experience is in running successful campaigns, not lobbying for corporations? Or not relying on strategists whose heydays were 16 years ago? How about language that normal people use, instead of stilted phrases such as “opportunity economy”? Of course centrist-left Democrats want people who disagree with them ideologically to have less power inside the party. Of course they don’t want to take responsibility for Trump winning a second term. But if you’re not a centrist or part of the Democratic establishment, you shouldn’t join this left-bashing. Progressives didn’t lose the 2024 race, and marginalizing them won’t guarantee victories in 2026 and 2028. The Democratic Party isn’t just an election organization. Its real purpose is to advance good policies that make the United States a better country. The party shouldn’t shun its principles and values to win elections, as centrists are calling for, but instead find the right candidates and tactics so it can win elections while also defending transgender rights, racial justice and other liberal values. Post Opinions wants to know: What was the most important event of the year that had nothing to do with the U.S. presidential be published as Letters to the Editor. Share 4378 Comments By Perry Bacon Jr. Perry Bacon Jr. is a Washington Post columnist. Before joining The Post, Perry had stints as a government and elections writer for Time magazine, The Post's national desk, theGrio and FiveThirtyEight. He has also been an on-air analyst at MSNBC and a fellow at New America. He grew up in Louisville and lives there now. perry.bacon@washpost.com perrybaconjr Follow Popular Opinions Articles HAND CURATED Opinion Alexandra Petri I’m not afraid to say it: Annexing Canada is not a good idea December 4, 2024 Opinion Ronald Kessler To cut government, Trump could start with this do-nothing agency December 2, 2024 Opinion Magdalene Taylor ‘Your body, my choice’? Think before you engage. December 3, 2024 America’s Class Politics Have Turned Upside Down Author: Karma, Rogé, Date: 2024-10-31 Collections: Hot Takes US Elect 2024 Zotero Key: KWGQPJKZ Cite Key: Karma24ClassPoliticsUpsideDwn Zotero Item | Lit Note This is Work in Progress, a newsletter about work, technology, and how to solve some of America’s biggest problems. Sign up here. A simple and intuitive view of democratic politics holds that political parties exist to advance the material self-interest of the coalitions that support them. If this were true, then as the Democrats became the party of high-earning college graduates, they would have abandoned economic policies that would threaten those voters’ pocketbooks. A version of this essentially Marxist analysis has become standard fare on the right, where the phrase woke capital has become a slur to describe the Democrats’ supposed fealty to corporate America; the Republican vice-presidential candidate, J. D. Vance, has argued that the Democratic Party is now the party of Wall Street. But as wealthier and better-educated voters have shifted toward the Democrats, the party and its constituents have become more economically progressive, not less. They have largely united around an economic agenda that emphasizes aiding the poor and middle class, and around messaging that places that agenda front and center. The very richest Democrats have become just as left-wing on economics as their less affluent party members, and far more economically progressive than low- and middle-income Republicans. U.S. politics seems to have decisively entered what you might call a post-Marxist or post-materialist phase. From the New Deal through the George W. Bush era, the Marxist view of politics largely held up. The rich and educated overwhelmingly voted for Republicans, who pursued tax cuts and deregulation, while the working class mostly voted for Democrats, who expanded the social safety net. Rogé Karma: Why America abandoned the greatest economy in history Over the past decade and a half, however, the dynamic has dramatically shifted. In 2008, the top fifth of earners favored Democrats by just a few percentage points; by 2020, they were the group most likely to vote for Democrats and did so by a nearly 15-point margin. (Democrats won the poorest fifth of voters by a similarly large margin.) Democrats now represent 24 of the 25 highest-income congressional districts and 43 of the top 50 counties by economic output. A similarly stark shift has occurred if you look at college education rather than income. Perhaps most dramatic of all has been the change among wealthy white people. Among white voters, in every presidential election from 1948 until 2012, the richest 5 percent were the group most likely to vote Republican, according to analysis by the political scientist Thomas Wood. In 2016 and 2020, this dynamic reversed itself: The top 5 percent became the group most likely to vote Democratic. This newly educated and affluent Democratic Party did not swing to the right on economics. Quite the contrary. Following the 2020 election, the Biden administration pursued an expansive economic agenda that included a generous pandemic stimulus package, a massive expansion of the social safety net for the middle class and poor (including cash transfers to families and universal pre-K), and large investments to create well-paying jobs in left-behind places. These policies, if fully enacted, would have represented a significant redistribution of wealth. Most of the $4.5 trillion in proposed new spending would have been funded by a spate of new taxes on corporations and the ultra-rich. “The Biden agenda was more ambitious and redistributive than anything else pursued by Democrats since the 1960s or ’70s,” Jacob Hacker, a political scientist at Yale and co-author of a recent paper on the Democrats’ changing coalition, told me. “This is not a party pursuing a ‘Brahmin left’ agenda. It’s pursuing an incredibly progressive economic agenda.” Despite its ambition, this agenda did not provoke anything resembling a rebellion from the party’s rich, educated base or the politicians who represent them. (Indeed, one of the biggest obstacles to its enactment was West Virginia Senator Joe Manchin, who represents a much more working-class state than most of his Democratic colleagues and who switched his affiliation to independent this year.) Kamala Harris is now running on many of those same policies and, according to the polls, her support among college-educated voters is even higher than Joe Biden’s was in 2020. A common complaint from the center and the right is that the influx of affluent, highly educated voters into the Democratic Party has caused it to focus primarily on culture-war issues instead of pocketbook economics. But when Hacker and his co-authors analyzed party platforms since 1980, they found that since the early 2000s the share dedicated to economic issues has steadily increased and that economic issues take up twice as much space as cultural issues. They reached a similar conclusion when looking at Twitter, where you’d most expect to see party elites pandering to the cultural tastes of their base. They looked at the tweets of high-ranking Democrats from 2015 to 2022 and found that nine of the 10 most frequently tweeted phrases were focused on economic issues, such as Build Back Better, Affordable Care Act, and American Rescue Plan; the only noneconomic issue in the top 10 was Roe v. Wade. (By contrast, just three of the top 10 Republican-used phrases referred to economic issues.) The authors also found that members representing wealthy districts were actually slightly more likely to discuss pocketbook issues such as economics and health care than members from poor districts. The policies and rhetoric coming from party leaders reflect the fact that affluent liberal voters have moved well to the left on economic issues. A major survey conducted after the 2020 election found that overwhelming majorities of Democrats in the top fifth of income distribution favored raising the federal minimum wage, hiking taxes on individuals earning more than $600,000 a year, making college debt-free, and enacting Medicare for All. That’s similar or slightly higher than the support for those policies among poor and middle-income Democrats and anywhere from 20 to 40 points higher than support among low- and middle-income Republicans. None of this means material self-interest doesn’t matter at all to affluent liberals. Some evidence suggests that although wealthy Democrats tend to support higher taxes in the abstract, they are less likely to support specific tax increases that affect them directly; they are also known to oppose new housing construction in their own neighborhoods that would make housing more affordable. But even those exceptions are less exceptional than they may appear. According to the survey cited above, a bare majority of the richest Democrats support raising taxes on individuals making more than $250,000. And during this campaign season, the leaders of the Democratic Party—including both Harris and former President Barack Obama—have trumpeted their support for building more housing. The leftward drift of high-status voters is partly a story about a genuine ideological conversion. Since the 2008 financial crisis, politicians, academics, and the media have paid far more attention to how the existing economic system has produced inequality and hardship. Highly educated, affluent voters, who also tend to be the most plugged-in to national politics, seem to have responded to this shift by embracing more progressive economic views. David Deming: Break up big econ The story is also about political strategy. After Donald Trump’s 2016 victory, many Democrats became convinced that the best way to win back disaffected working-class voters was to enact policies that would help them. Surveys consistently find that middle- and low-income Republicans strongly disagree with their own party leaders on most economic issues, creating a potential opening for Democrats. The Biden agenda that was shaped by those views has largely produced its intended economic effects. Unemployment has fallen, wage inequality has shrunk, and hundreds of billions of investment dollars have poured into red states. Many of the country’s forgotten communities are making a strong comeback. Politically, however, the effort to win back working-class voters appears to have flopped: If polls are to be believed, the Democratic Party is bleeding working-class support more badly than it did in 2016 or 2020. Part of that failure seems to be because, when it comes to the economy, many voters are concerned about high prices above all else and view Democrats as responsible for them. But there’s also compelling evidence that Republican voters aren’t particularly motivated by economic policy in the first place. That is, although they disagree with GOP politicians about health care, taxing the rich, and the minimum wage, they don’t much care about that disagreement. A recent paper by the political scientist William Marble analyzed nearly 200 survey questions going back decades and found that in the 1980s and ’90s, non-college-educated white voters were more likely to vote in accordance with their economic views, causing them to support Democrats. Since the early 2000s, however, that dynamic has inverted: Non-college-educated white voters now place a far greater emphasis on culture-war issues over economic ones, pushing them toward supporting Republicans. That realignment leaves both parties in a strange place heading into November. Voters consistently say that the economy is the most important issue of the 2024 election. And yet the affluent overwhelmingly support Kamala Harris, whose administration favored bold redistribution and big government spending, while a critical mass of working-class voters favor Donald Trump, whose economic agenda consisted largely of cutting taxes for the rich and trying to kill the Affordable Care Act. The irony is that the Biden administration’s economic-populist push implicitly assumed that the Marxist view of politics was correct all along. Democrats embraced an agenda that largely went against its voters’ immediate material interests in the hopes that they could win over less-wealthy voters by appealing to their material interests. But working-class Trump supporters, just like liberal elites, turn out to have other things on their mind. What’s a Fact, Anyway? Author: McIntosh, Fergus, Date: 2025-01-11 Collections: MisDisinformation Zotero Key: 9MUS8BGI Cite Key: McIntosh25whatsAfact Zotero Item | Lit Note Every tribe has its myths, and journalists are no exception. In America, one common story goes like this: once, in the prelapsarian era before social media—or before smartphones, or the Internet—there was a time when journalists were trusted. Back then, everybody read muscular daily newspapers and watched straight-down-the-line TV reporting. When citizens had to make political decisions, a robust social contract with the media insured that they were well informed; even if they couldn’t always agree on what to do or whom to vote for, they could rely on a shared set of facts. But then something changed: people stopped paying attention to the news, or decided that they didn’t believe it anymore. They got distracted by podcasts, Facebook, and Twitch. They became ill-informed, and started to act against their best interests. The media decayed and fragmented, along with the nation. Opinion and news became indistinguishable, misinformation ran amok, and that is how we came to live in the post-truth world. This story is not unsupported. Trust in many institutions has fallen over the years, but in journalism it has plummeted. In 1972, Gallup began asking people in the United States, “How much trust and confidence do you have in the mass media?” The numbers used to be highly favorable—in 1976, more than seventy per cent said they had a lot, and very few reported having none—but in 2024 a plurality of respondents (thirty-six per cent) said that they had “none at all.” A survey of how highly Americans rated the ethics of various professions found that only a fifth believed journalists’ standards were “high” or “very high”: a better ranking than car salespeople and senators, but worse than bankers and chiropractors. (Almost half said that journalists’ standards were “low” or “very low.”) Increasing numbers of Americans report not following the news, or doing so via factually unreliable social or alternative media. In some parts of the world, the picture looks rosier—the Nordic states, outliers as ever, have far higher levels of public trust in journalsim—but one recent study of twenty-eight countries showed the balance of media trust and distrust to be neutral at best, and negative among the most developed nations. Meanwhile, large swaths of the industry have been on the retreat, racked by dwindling advertising revenue, hostile governments, and declining audiences. In the United States, the number of people employed in newsrooms dropped by more than a quarter between 2008 and 2020; thousands were laid off in 2024 alone. In 2022-23, the Committee to Protect Journalists recorded more journalists jailed worldwide than ever before. Technological change—most recently and acutely the mainstreaming of generative artificial intelligence—presents its own challenges, as does political polarization. In the United States, for example, the erosion of trust in journalism has been sharper and more consistent among Republican voters, and it can seem, in both parties, that many people turn to the news more for affirmation than for information. What passes for truth often depends on where one falls on the political spectrum: it’s no coincidence that Donald Trump’s venture into social media has “truth” in its name. News & Politics The latest from Washington and beyond, covering current events, the economy, and more, from our columnists and correspondents. This is a grim picture. But buried in the statistics is another story, one in which people worry about misinformation, even if they can’t agree on what to call it. “Fake news” is not a new concept, but many Americans now register displeasure with inaccurate or unverified information on social media, and a majority now think that somebody, perhaps even the federal government, should do something about it. There seems to be widespread recognition that bad facts are bad news—globally, fears of an “information war” are rising—and, despite endemic skepticism and distraction, there is an enduring thirst for reliable information. The question is, where can it be found, and how can its purveyors make themselves heard amid the noise? One answer has been to turn up the volume: to loudly pronounce that, in a fallen world, only we can provide accuracy. Networks such as NBC and the BBC have launched units dedicated to fact-checking what other people say—as opposed to checking their own work—and for the first Presidential debate of 2024 the New York Times tasked twenty-nine staffers with combing through the candidates’ statements in real time. Not every entrant in the accuracy economy is as large or well resourced: in the past decade, hundreds of small fact-checking Web sites have sprung up around the world, including many in countries, such as India, where press freedom is far from a guarantee. (Many of these sites received significant funding from Meta’s third-party fact-checking program, the end of which the company announced earlier this week.) Whether rooted in service or business strategy, this sort of “political fact checking,” which spotlights specific claims and seeks to confirm or disprove them, is hardly neutral. Setting out to counter inaccuracies, it often takes the form of rebuttal, and necessarily entails editorial decisions about what to cover. (No outfit can scrutinize every statement by every politician, and none of them wants to.) An insistent focus on pointing out misinformation may even inflate the scale of the problem—to the benefit of what the writer Joseph Bernstein has labelled Big Disinfo, and to the detriment of a publication’s appeal to uncommitted readers. The sense that journalists are worth listening to has as much to do with how they go about their work, and with the uses to which readers can put it, as with what they actually say. The provision of facts does not, in itself, engender trust. What is more certain is that, from time to time, every journalist, no matter how well meaning, gets something wrong, or misses the point. This is true even if—as Michael Schudson, a professor of journalism at Columbia University, has argued—American journalism is deeper, more analytical, and more investigative today than it was fifty years ago. If there ever was a golden age of journalism, we may be living in it. The trouble is, pointing out the mistakes of others is not enough. If people are to trust journalists, we need to earn it. The New Yorker is known for its accuracy. At least, that’s what the magazine tells itself. In reality, it depends on whom you ask. A prospectus announcing the first issue of The New Yorker, in 1925, promised that: As compared to the newspaper, The New Yorker will be interpretative rather than stenographic. It will print facts that it will have to go behind the scenes to get, but it will not deal in scandal for the sake of scandal nor sensation for the sake of sensation. Its integrity will be above suspicion. True to this mandate, The New Yorker maintains an obsessive interest in facts, and it didn’t take long for the early editors to recognize the usefulness of at least basic stenography. In 1927, the magazine published a Profile of the poet Edna St. Vincent Millay, “America’s first starlet.” Its first sentence—“Edna Millay’s father was a stevedore on the wharves at Rockland, Maine”—sounded good, but, like plenty more in the piece, it wasn’t true: Millay’s father was a schoolteacher turned insurance agent. The embarrassment inspired The New Yorker’s founding editor, Harold Ross, to install a fact-checking department, and since then checkers have left their marks all over the magazine. John McPhee, in an essay published in 2009, depicted fact checking as a pursuit bordering on mania. To shore up one important but rickety anecdote of his concerning wartime nuclear reactors, a checker spent days placing calls that “ricocheted all over the United States: from Brookhaven to Bethesda, from La Jolla to Los Alamos.” The chase culminated, moments before the press deadline, in a call to a Florida police department, which the checker enlisted to track down a crucial witness who turned out to have gone to the mall. He called the checker from a phone booth, just in time to correct an error. Not many pieces require such heroics: the reality is that fact checkers are busy people, who traffic only occasionally in the dark arts of deep research. Most facts can be checked fairly easily today, especially with the benefit of the Internet, but, since there are so many, a checker has to prioritize. (Merriam-Webster defines a fact as “a piece of information presented as having objective reality”; a long piece might contain thousands.) When a particular fact turns out to be sticky, persistence and attention, rather than any kind of special knowledge, are generally what’s needed. Checkers are not infallible, and their successes are mostly due to hard work and creativity. What is truly extraordinary about any fact-checking department is that it exists at all. At the time of writing, nearly thirty people work in Checking at The New Yorker, almost all of them full time. It is labor, at scale, that produces accuracy. But the pursuit of accuracy—that is, confirming a fact is a fact—is only part of the project. With a few pragmatic exceptions, fact checkers go over everything published by the magazine, editing for balance, fairness, and context. Writers are asked to share their sourcing; checkers review both research materials (books, articles, e-mails, documents) and original reporting. For complex topics, and as needed for corroboration, they do their own independent research, perhaps conferring with relevant experts. And, unless there’s a good reason not to, they get in touch with every person and entity discussed in the story, and comb through what’s attributed to or said about them. Realignment or not, we’re in a new era of American politics Author: Cohn,, Nate, Date: 2024-12-25T15:45:01-08:00 Collections: Hot Takes US Elect 2024 Zotero Key: VVFZ2RQG Cite Key: Cohn24RealignNewPolit Zotero Item | Lit Note When Barack Obama won reelection in 2012, it seemed to mark the beginning of a new era of Democratic dominance, one propelled by the rise of a new generation of young, secular and nonwhite voters. With hindsight, the 2012 election looks more like the end of an era: the final triumph of the social movements of the 1960s over the once-dominant Reagan Republicans. Instead, it’s the three Trump elections — in 2016, 2020 and 2024 — that look as if they have the makings of a new era of politics, one defined by Donald Trump’s brand of conservative populism. Whether you call it a realignment or not, American politics hasn’t been the same since Trump won his party’s nomination. The two parties clash over areas of former consensus, even as they reach detente on issues that defined the polarizing 2004 and 2012 elections. It can be disorienting for anyone who came of age before Trump. It can even feel like American politics has been turned upside down. Until Trump, there was a lot about American politics that you could take for granted. The meaning of the two parties seemed clear. Republicans represented Ronald Reagan’s three-legged stool of small-government fiscal conservatism, the religious right and foreign policy hawks. Democrats represented the working class, change and the causes of liberal activists. Every four years, the two parties mostly litigated the same fights over the same issues. They rehashed arguments over war and diplomacy; entitlement spending and tax cuts; “family values” and the social movements of the 1960s; or trade and free enterprise versus labor and protecting jobs. It led to predictable demographic divides and recurring, long-term electoral trends. That all changed when Trump came down the escalator. On some issues, it can even seem as if the parties have switched places. Today, Trump champions the working class, rails against elites, strives to protect American jobs and criticizes traditional U.S. foreign policy, all while Democrats defend the establishment, norms and the old foreign policy consensus. Long-standing areas of bipartisan consensus have suddenly become fiercely contested. Immigration, free trade, postwar U.S. alliances and even support for democracy at home and abroad have all become defining conflicts between the two parties during the Trump era, rather than areas of agreement. Yet at the same time, the two parties seem to have reached a truce on the most bitter fights of the Bush-Obama era, such as the Iraq War, Social Security and same-sex marriage. Much of the Republican Party’s old establishment — including the Cheneys, Mitt Romney and Paul Ryan — is now without a home. At the same time, many former Obama supporters, from Robert F. Kennedy Jr. to Elon Musk, suddenly find themselves near the center of Trump world. This new partisan conflict has led to very different electoral coalitions. In 2016, Trump made enormous gains among white voters without a college degree, including in Northern states, where Republicans had not been able to sustain breakthroughs. Since then, he has made even larger gains among young, Black, Hispanic and Asian voters — and did so by representing everything Democrats thought these groups opposed. After three Trump elections, the partisan gap between white and nonwhite voters is now smaller than at any time since the enactment of the 1964 Civil Rights Act. The partisan generation gap has fallen by two-thirds. Perhaps most strikingly, the old class divide between rich and poor, between capital and labor, has seemingly vanished. The exit polls found Trump losing voters making over $100,000 a year, while winning among voters making less — including those making less than $50,000. If anything, 20th-century fights are emerging as plausible areas of bipartisan consensus, with Republicans seemingly receptive to labor and spending on infrastructure, while Democrats seem more open to deregulation and supply-side remedies to problems like housing and energy. In place of the old class conflict, there’s a new educational divide. Before Trump, people voted about the same way with or without a degree. Now, the gap between voters with or without a degree is as large as the income gap was back in 2012 — and all the way back to the dawn of survey research. In some cases, Trump-era electoral shifts can be interpreted as an acceleration of longer-term trends; in other cases, they’re new developments. Either way, these trends have brought American politics to a very different place. The ‘R’ word Whether all of this counts as a “realignment” depends on how one defines the term. A realignment usually means one party obtains a significant political advantage for decades. By this measure, the Trump elections plainly fall short. Republicans barely hold any meaningful advantage; even if they do, it’s not at all clear whether it will even last four years. Nonetheless, the Trump elections have two features of a realignment: They changed the basic political conflict between the two parties, and they led to corresponding changes in the two coalitions. These changes aren’t minor and they’re not just because of the singular force of Trump, either. Like previous realignments, it is part of a broader political change occurring across Western democracies, where the remnants of the old industrial political order is being supplanted by something different. In country after country, the parties of the old industrial left, like Labour in Britain or the Socialists in France, have bled working-class support to a new kind of conservative populism, driven by a new set of issues, including immigration, trade and national sovereignty. These issues don’t fit on the old left-right ideological spectrum. In fact, many right-wing parties now embrace the welfare state. In each case, however, populist conservatives argue that elites have used democratic and transnational institutions to advance their own interests and causes at the expense of ordinary people. These political movements have thus far struggled to build lasting political majorities, but their critique has nonetheless been the most potent message in politics. The parties of the center-left, on the other hand, increasingly depend on the support of a new class of affluent college graduates. These parties may still yearn to champion the working class, but this hasn’t been their animating force for decades. Instead, they draw their energy from idealistic, college-educated progressive activists, whose cultural and economic views often alienate working-class voters. Even when these parties do aim to help working-class voters, their policies don’t pack an electoral punch. Instead, their electoral fortunes depend on forming coalitions with classically liberal but traditional conservatives, who oppose the populist right on trade, immigration, foreign policy and democracy. The transition away from industrial-era class politics has been ongoing since the 1950s and 1960s, when postwar affluence and an expanded safety net mostly satisfied a century of demands from industrial labor. Soon thereafter, the rise of a new generation of college-educated youth activists helped bring a new set of issues to the fore — from civil rights and women’s rights to Vietnam — that helped shatter the New Deal coalition. This was the last great upheaval in American politics. In a sense, everything one could take for granted about politics before Trump came into place by this time or soon thereafter. The parties were redefined; even the legs of Reagan’s three-legged stool can be recast as opposition to the 1960s cultural revolution, the anti-war movement and the Great Society. The fallout set the next five decades of electoral fights and trends into motion. In hindsight, Obama’s victory over Mitt Romney was the culmination of this era. For one last election, Democrats and Republicans took up their usual positions and relitigated the fights of the era. In the end, Obama won by a modest margin, but it nonetheless seemed to offer a decisive verdict on the era as a whole: Liberals won. By 2012, America was a multiracial, secular, liberal-leaning society. Less than 50 years after the Civil Rights Act, America had elected a Black liberal president. It would soon, it was presumed, elect a female president. Same-sex marriage was popular, and it would soon be the law of the land. Marijuana was next. In just a few years, demographic shifts promised to turn Obama’s modest victory into a lasting Democratic majority. This liberal triumph in the culture war came against the backdrop of the financial crisis and the Iraq War, which simultaneously dealt enormous blows to the Reagan-era consensus for smaller government, deregulation and a neoconservative foreign policy. With Obama’s victory, the dominant post-1960s conservative political coalition was finished. Four years later, Trump destroyed what remained of Reagan’s three-legged stool and redefined the Republican Party around a new set of issues. He seized the mantles of populism, change and the working class, by campaigning on newer issues: trade and China, immigration, energy and the excesses of a newly dominant college-educated, liberal, “politically correct” or “woke” left. In the end, the Democrats lost their core message and voters who they imagined were part of their base. While Republicans didn’t win in a realignment-like electoral landslide, Trump’s conservative populism won the policy debate decisively enough. On border security, domestic energy production, trade, China or deregulation, Democrats are moving toward the core of Trump’s agenda. The two major exceptions — abortion and democracy — were Republican self-inflicted wounds that at once prevented a more decisive Republican victory and obscured the extent that conservative populism had seized the center of American politics. Whether you call it a realignment or not, it’s a new era of politics. This story was originally published at nytimes.com. Read it here. Don’t Blame Polling for Our Infuriating Politics Author: Silver, Nate, Date: 2024-11-19 Collections: Hot Takes US Elect 2024 Zotero Key: Y437MA2Z Cite Key: Silver24dontBlamePolling Zotero Item | Lit Note The polls just can’t win in the court of public opinion — the very thing they’re designed to study. They are either maddening in finding a race too close to call (in the final month before the presidential election, nearly 80 percent of swing-state polls showed a lead of no more than two and a half percentage points) or they break from the pack, only to be wrong. In her final Iowa poll Ann Selzer, a name synonymous with the gold standard in polling, had Kamala Harris up by three points, a shocking result that titillated Democrats. Ms. Selzer has had a long history of defying the conventional wisdom and being right, but Ms. Harris lost Iowa by 13 points. It may even feel as though we’re Ping-Ponging between radically different futures, never quite certain what lies around the bend. Yet on the whole in 2024, polling did not experience much of a miss and had a reasonable year. Ms. Harris led by only one point in my final national polling average. And Donald Trump led in five of seven key states, albeit incredibly narrowly. The final polling averages were correct in 48 of 50 states. The final Times/Siena national poll (including third-party candidates) had Mr. Trump one point ahead. There was plenty of data to support a Trump win. So why did polling still feel so unsatisfying? In a world where the parties are remarkably efficient at corralling voters and competing to a 50-50 split each time, polls aren’t going to provide the certainty we crave. We’d better get used to it: This is now the fourth election in a row in which the popular vote margin was within five points, something that has happened only once before in the country’s history, for six elections between 1876 through 1896. The problems with polls are the same problems that plague politics. Polling has become a mirror that reflects the frustrating, even infuriating, nature of politics in America in 2024. Our politics are messy, and that is not something polls can fix. We’d better get used to that, too. Polls still provide important hints, leads and hypotheses. They were basically right that Democrats’ dominance among minority groups was waning, overestimating the swing among Black voters but understating it among Hispanic voters. Asian Americans, Native Americans and the oft forgotten group of voters who identify their race as “other” also shifted toward Mr. Trump. They were right that Democrats would experience a significant erosion among younger voters. Ms. Harris won voters ages 18 to 29 by just four points, according to the A.P. VoteCast survey — down from Joe Biden’s 25 points in 2020. But the shifts were much bigger among young voters who didn’t attend college, especially men. Some of the biggest shifts of all came in large cities like New York. Polls showed clues about this, too. A Times/Siena poll late in the campaign had Ms. Harris winning New York City by 39 points, down from Mr. Biden’s 53 points in 2020. Her actual margin in the five boroughs? Just 38 points, although some votes remain to be counted. Mr. Trump beat his polling numbers by about 2.5 points nationally (based on my projection of the popular vote once all votes are counted — what’s left is mostly from blue states) and 2.1 points in the average swing state. Our final forecast had it so close to 50-50 that the outcome was literally more random than a coin flip. (Empirically, heads win 50.5 percent of the time.) But a Trump sweep of the swing states was our single most likely outcome, because polling errors tend to be correlated. It’s not great that the polls missed low on Mr. Trump for the third and final time, even in a year when survey companies adopted all sorts of novel strategies to avoid this exact outcome. The core problem is easy to describe but hard to fix. Namely, polls routinely get higher response rates from Democrats than from Republicans. Democrats have higher civic engagement, trust the media more and are more comfortable revealing their political preferences. (They can also be more enthusiastic.) But Republicans have become much less trusting of institutions, and that likely includes polling. And the current G.O.P. coalition led by Mr. Trump includes more so-called marginal voters, those who may not regularly follow the news but who still show up at the ballot box for him. In 2024 a number of surveys used something called weighting by recalled vote — which means weighting data based on whom voters say they chose in the previous election. That can be a problem because voters don’t always remember their earlier votes. It can also cause polls to miss shifts in the electorate. Some of the states with the biggest polling misses — like Arizona and, although it isn’t a swing state any longer, Florida — were substantially affected by Covid-era migration patterns. In those states, conservative refugees moved from blue states for a lower cost of living and schools that were quicker to reopen. Until this year, I would have said that the cure of weighting by recalled vote was worse than the disease. Now I’m not so sure. The Selzer poll, for example, would have had Mr. Trump up by six points if it had applied this technique — not perfect, but much better. Other practices are less defensible. One of these is herding, or massaging your numbers so that they don’t produce an outlier result. There are unmistakable signs of this, with many polls bunched within two percentage points. By my calculations, there was only a one in 10 trillion probability of this happening by chance alone. There’s also a universe of Republican-leaning polling shops, which have been the subject of intense criticism from Democrats. In 2022 these companies had a bad year, contributing to a misleading narrative about a red wave. This year they were mostly accurate. However, one prominent outfit, Rasmussen Reports, was discovered to be coordinating with the Trump campaign and giving them early previews of their results. We need fewer of these companies and more like AtlasIntel, a Brazilian company that showed results that often differed from the consensus but at least provided an independent opinion. In its final round of polling, AtlasIntel accurately had Mr. Trump ahead in all the swing states. So the problems with polls match those in the rest of the media. A lack of trusted sources means there’s a vacuum to fill, and sometimes it’s filled with low-quality data or even outright misinformation. Should we trust polls less? I’ll offer a brave and qualified no, but only because the shift in public sentiment about polls — from viewing them as oracular to seeing them as fake news — has probably overcorrected relative to reality. Ms. Selzer announced her retirement from polling over the weekend to pursue “other ventures and opportunities.” Although her career change had been planned in advance, it still marks the end of an era. We should have learned by now that more data does not necessarily mean more certainty. We’re no longer in a time when we can count on polls to reflect the indisputable ground truth. But the thing is, we never really were. I’ve been working for years to get people to understand the uncertainties inherent in polling in the form of probabilistic forecasts. The misses in 2016, 2020 and 2024 were within the normal range of error in the long and checkered history of polling — like the Gallup poll that had Thomas Dewey defeating Harry Truman in 1948 or the ones in 1980 that missed Ronald Reagan’s landslide. It’s telling that 2020 was, by far, the worst of the three recent elections in terms of how much polls differed from the final margins but the one for which pollsters got the least grief, because Mr. Biden won. That’s only because he had such a large lead. In most elections now, their practical margins of error are going to be larger than the actual margins separating the candidates in states like Pennsylvania, Georgia and Nevada. There are no great alternatives. Prediction markets had a strong year. (I’m a part-time adviser to one of them, Polymarket.) But they’ve also had their misses, like insisting that Mr. Trump still had a chance in 2020 even after the election had already been called for Mr. Biden. And although my gut instinct that Mr. Trump would win this year turned out to be right, that was basically just a wild guess that people shouldn’t have taken too seriously. There’s another side of the coin. Rank-and-file Democratic voters trust the polls more than Republicans, but that does not seem to be true among the Democratic elites who dictate campaign strategy. In fact, it sure seems to me that Democrats made the mistake of routinely dismissing public opinion. Before Mr. Biden exited the race, the White House publicly insisted that the polls were skewed, even as its internal polling reportedly showed him in even worse shape than the public polls. And on an array of issues, Democrats downplayed inconvenient polling truths — about the public’s dislike for inflation and lax immigration policies, about Mr. Biden’s age, about Ms. Harris’s middling popularity and, yes, about the cultural phenomenon that we call wokeness. The polls have become part of a broader phenomenon in which parties, voters and the media create their own political realities. We trust them when they present a case we agree with, but we ignore, unskew or deride them as biased when they don’t. It is left for elections to provide a reality check. Democrats, reeling from election losses, cast blame on each other Author: Kornfield, Meryl, Date: 2024-11-16 Collections: Hot Takes US Elect 2024 Zotero Key: 5YZBK9AE Cite Key: Kornfield24demsElectionBlame Zotero Item | Lit Note Democracy Dies in Darkness Scott Otterson PoliticsBiden administrationThe FixThe BriefsPollingDemocracy in AmericaElection 2024 Democrats, reeling from election losses, cast blame on each other The internal ideological divisions burst into public view after Donald Trump beat Vice President Kamala Harris in the presidential race. 8 min 6541 Sen. Bernie Sanders (I-Vermont) speaks at a labor rally in Harrisburg, Pa., while campaigning for Vice President Kamala Harris on Oct. 27. (Nathan Morris/AP) [image] [image] By Meryl Kornfield November 16, 2024 at 10:14 a.m. EST Two weeks removed from a set of losses that sent Democratic leaders into a tailspin, the years-long ideological battle between the progressive left and centrists has once again come to a head as both sides fight to shift blame and take the reins of the future direction of the party. [image]Get the latest election news and results With the party grappling with how to prepare for midterm elections in two years, uncertainty about who will take over the Democratic National Committee, and expected internal debates over how to handle complete Republican control in the White House and on Capitol Hill, Democrats don’t have much time to find consensus on their path forward. “The question is: Do you want to be the least progressive outside the house looking in the window? Or do you want to be the most progressive person at the table with the deal being cut?” said Joe Caiazzo, a Democratic strategist and former campaign staffer for Sen. Bernie Sanders (I-Vermont), who said progressives need to assimilate to where they stand now in a Republican-dominated Washington. The internal ideological divisions burst into public view the day former president Donald Trump was declared the winner in the presidential election over Vice President Kamala Harris. Sanders posted a scathing statement on social media rebuking the Harris campaign for abandoning its base after Trump had spent months trying to convince voters they were better off financially when he was in office. Following Election 2024 Following “It should come as no great surprise that a Democratic Party which has abandoned working class people would find that the working class has abandoned them,” Sanders wrote. “While the Democratic leadership defends the status quo, the American people are angry and want change. And they’re right.” DNC Chair Jaime Harrison described that analysis as “straight up BS,” arguing that the Biden-Harris administration had brought about a slew of pro-worker reforms, including creating new jobs through major legislation, such as the Inflation Reduction Act. “There are a lot of post election takes and this one ain’t a good one,” Harrison wrote. For nearly a decade, progressives trying to get to the top of their party’s tickets have been stymied in favor of more centrist choices. While some have taken solace in the Biden administration embracing liberal policies, such as expanding child tax credits and forgiving student debt, Israel’s bombardment in Gaza has been the latest cause of friction inside the Democratic Party. Progressives have complained that Harris made a broad play for the political center, campaigning with billionaire Mark Cuban and former Republican congresswoman Liz Cheney, rather than trying to generate more enthusiasm for Harris among some left-leaning voter blocs. But centrist Democrats have argued that the electorate has clearly moved rightward and that the party must adapt its message to appeal to those voters. Some Democratic strategists said that Sanders’s message missed a critical point: Even in blue states like California, the electorate leaned more to the right than before on not just the presidential race but also on ballot initiatives and downballot races, especially among low- and middle-income voters, according to exit polls and election results. Most of the nation’s 3,000-plus counties swung rightward compared with 2020, including an average shift of 7.5 percentage points to the right in urban core counties that usually favor Democrats overwhelmingly. “If you took one glance at the map of counties and how they moved, where virtually all of them moving to the right since 2020 and conclude that what we need is to move to the left, I would posit that your political analysis is off,” said Matt Bennett, a co-founder of Third Way, a moderate Democratic think tank. Bennett and others have argued that voters’ frustrations with inflation and higher costs contributed to dissatisfaction with the current administration that was impossible for Harris to quell in the short period of her campaign. But progressives said they weren’t asked by the Harris campaign to provide advice on how to address those voter concerns. Rep. Ro Khanna (D-California), who supported Sanders’s presidential bid in 2020 and campaigned for President Joe Biden and then Harris this year, and other progressive Democrats said they had no conversations with Harris about her efforts to appeal to voters who might be interested in progressive policies. Sanders, once a colleague of Harris’s in the Senate, spoke directly with the vice president only once, and that was in a perfunctory call when she first announced she would run to succeed Biden, according to two people familiar with the conversation who spoke on the condition of anonymity to share private discussions. “We didn’t emphasize the economy,” Khanna said. “We didn’t emphasize the renewal of the American Dream. We didn’t emphasize manufacturing and higher wages and corporations not having excessive CEO pay. Instead, we spent a billion dollars having concerts all over America. I mean, it was ridiculous.” Despite blockbuster fundraising, the campaign has now faced greater scrutiny over how much money it gave to consultants and celebrities. Sanders made other attempts to influence Harris’s campaign, but they were unsuccessful, according to the people familiar with the outreach. Following Trump’s refusal to answer a question about raising the minimum wage after a campaign stop where he briefly worked the drive-through at a McDonald’s in Pennsylvania, Sanders told several Harris campaign aides that she should announce a bolder proposal for a national minimum wage increase, higher than the $15 minimum wage that the Biden administration had tried to set. Instead, Harris later answered a reporter’s question by saying that she backed a $15 minimum wage. A campaign official, speaking on the condition of anonymity to openly discuss strategy, said that Harris’s policy ideas about ending corporate price gouging and reducing housing costs were taken from long- standing progressive proposals. One of Harris’s final campaign stops included an appearance by Rep. Alexandria Ocasio-Cortez (D-New York). Meanwhile, Sanders, Khanna and other progressives were surrogates for the campaign. The official added that Trump’s campaign and supporting super PACs paid for television ads that sought to portray Harris as overly liberal. “That was a dynamic we had to deal with on the trail,” the official said. Khanna said that Trump’s stop at McDonald’s was smart and that he had made great inroads with working-class people by appealing to their concerns about the economy. And polling found that voters cared far more about issues like the economy, rather than abortion and transgender rights. About a third of voters said the state of democracy and the economy were the top issues in their vote, according to preliminary exit polls, while slim majorities of voters said they trusted Trump more to handle the economy, immigration, crime and “a crisis.” Rep. Pramila Jayapal (D-Washington), chair of the Congressional Progressive Caucus, pointed to the pandemic-era relief pushed by Democrats during the Trump administration. Getting those checks during Trump’s presidency might have led some to feel they had more money then than during Biden’s term once the aid ended. Voters preferred the progressive policies but they didn’t realize who was behind them, Jayapal said. “I don’t think we’ve ever really articulated that,” she said. “Voters did not reject our economic policies.” Jayapal and others have embarked on efforts to learn more about how voters feel and what they want to hear. Ocasio-Cortez has collected people’s responses to a question she posed on Instagram about why they voted for her and Trump. Many expressed an interest in populist leaders. Sanders adviser Faiz Shakir said he hopes Democrats will begin to travel the country to listen to voters and learn about people’s problems. Democrats needed to do more to explain to voters how income inequality had worsened under Trump, he said. The debates among Democrats fueled by the election results have also touched on the party’s standing on social issues, such as transgender rights, which were the subject of frequent attacks from the right. Some centrists have largely avoided the dialogue, while Rep. Seth Moulton (D-Massachusetts) told the New York Times that Democrats are out of touch with the views of a majority of Americans on issues such as allowing transgender female students to compete in girls’ sports. His comments were condemned by some Democrats, who said that he was parroting anti-trans rhetoric from the right. Meanwhile, Rep. Greg Landsman (D-Ohio) and other centrist Democrats said they are planning to find a middle ground with moderate Republicans on Capitol Hill. Landsman said he would propose that Trump’s new tax plan not raise taxes for middle-class families and that he expects he could convince enough Republicans to go along with him. “Every major thing we got done in this Congress, it was the bipartisan majority, the pragmatic middle that led the charge,” Landsman said. Tyler Pager contributed to this report. Share 6541 Comments Meryl Kornfield is a staff writer on the Politics desk of The Washington Post. merylkornfield By Meryl Kornfield follow on X NewsletterWeekdays [image] Early Brief The Washington Post's essential guide to power and influence in D.C. Sign up MOST READ Politics [image] [image] 1 Trump won. The celebrations started. Then the trouble began. 2 [image] [image] Musk appears to pressure Trump on Cabinet and tariffs, raising eyebrows 3 [image] [image] Trump picks fracking firm CEO Chris Wright to be energy secretary 4 [image] [image] Trump team weighs Pentagon pick after sexual assault allegation surfaces 5 [image] [image] Tracking Trump’s picks for his Cabinet and administration MOST READ [image] [image] 1 Trump won. The celebrations started. Then the trouble began. 2 [image] [image] Musk appears to pressure Trump on Cabinet and tariffs, raising eyebrows 3 [image] [image] 10 programs that could be on the ‘government efficiency’ chopping block 4 [image] [image] Jay Bhattacharya, an NIH critic, emerges as a top candidate to lead the agency 5 [image] [image] Column|The sad, silly spectacle of Jake Paul and Mike Tyson Company • About The Post • Newsroom Policies & Standards • Diversity & Inclusion • Careers • Media & Community Relations • WP Creative Group • Accessibility Statement Sections • Trending • Politics • Elections • Opinions • National • World • Style • Sports • Business • Climate • Well+Being • D.C., Md., & Va. • Obituaries • Weather • Arts & Entertainments • Recipes Get The Post • Manage Your Subscription • Become a Subscriber • Gift Subscriptions • Newsletters & Alerts • Washington Post Live • Reprints & Permissions • Post Store • Books & E-Books • Print Special Editions Store • Print Archives (Subscribers Only) • Today’s Paper • Public Notices Contact Us • Contact the Newsroom • Contact Customer Care • Contact the Opinions Team • Advertise • Licensing & Syndication • Request a Correction • Send a News Tip • Report a Vulnerability Terms of Use • Digital Products Terms of Sale • Print Products Terms of Sale • Terms of Service • Privacy Policy • Cookie Settings • Submissions & Discussion Policy • RSS Terms of Service • Sitemap • Ad Choices washingtonpost.com © 1996-2024 The Washington Post [image] Beyond abortion rights: Why did Kamala Harris lose women’s votes? Author: Shamim, Sarah, Date: 2024-11-07 11/7/24 Collections: Hot Takes US Elect 2024 Zotero Key: GTWNJVSY Cite Key: Shamim24whyHarrisLoseWomen Zotero Item | Lit Note EXPLAINER News Beyond abortion rights: Why did Kamala Harris lose women’s votes? The number of American women voting Democratic declines after Joe Biden’s election to the presidency. Why? 02:19 By Sarah Shamim 7 Nov 2024 Donald Trump’s defeat of Vice President Kamala Harris in the 2024 United States presidential election has signalled women’s rights – specifically the right to an abortion – was less of a key issue than expected for voters. This was the first presidential election since the Supreme Court overturned the landmark 1973 Roe v Wade court ruling, which ended a woman’s right to termi- nate a pregnancy. Trump has repeatedly claimed credit for that 2022 verdict, 02:19 the top court. KEEP READING The Elon Musk effect: How Donald Trump gained from billionaire’s support ‘We’re not going to win’: Trump’s election deepens Ukraine’s pessimism Don’t dare blame Arab and Muslim Americans for Trump’s victory The Harris campaign made much of Trump’s stance on reproductive rights in a bid to woo female voters, particularly in the swing states. However, early na- tional exit polls showed that Harris had won the support of 54 percent of women, lower than President Joe Biden did in 2020 when he had the support of 57 percent. So what happened to the female vote? Why was abortion expected to be significant in this election? The Supreme Court’s overturning of Roe v Wade in June 2022 was a huge turn- ing point for women’s reproductive rights in the US and sparked a massive back- lash from women’s rights and medical groups. Overturning Roe had been a key campaign promise that Trump successfully contested the 2016 election on. In light of the uproar over the Supreme Court’s ruling, Democrats expected the issue to loom large this election, and Harris shaped much of her campaign around it. Governor Janet Mills speaks at a rally in Monument Square in Portland marking the second anniversary of the US Supreme Court decision overturning Roe v Wade [Gregory Rec/Portland Press Herald via Getty Images] Harris’s campaign focused on highlighting statements Trump has made about abortion. For example, one advert that the Harris campaign ran close to the polls was ti- tled Punishment, referencing a statement by Trump before the 2016 election in which he suggested women who try to obtain abortions should be punished. However, in 2016, Trump moved back from this position, clarifying that any punishment would be for doctors performing the procedure, not women under- going it. On October 29 this year, Harris stated that Trump would “force states to moni- tor women’s pregnancies”. She urged listeners to “Google Project 2025 and read the plans for yourself”, referring to a conservative policy blueprint assembled by some of Trump’s supporters but which Trump has distanced himself from. This claim by Harris was deemed false by PolitiFact, a fact-checking outlet. Ultimately, while it is true that Trump and his Republican aides have been called out for making sexist remarks about women, Trump made up for it by strategi- cally distancing himself from the notion of a federal abortion ban in the run-up to this election, stating he believed it should be for individual states to decide on laws about abortion. Instead, he focused on rallying support among the working class by focusing on economic policy as the main thrust of his campaign. Trump’s running mate, JD Vance, in 2022 said he supported a nationwide abor- tion ban. However, in July this year, Vance said he was aligned with Trump on the idea that abortion should be an issue that each state deals with. According to a survey of female voters conducted by the Kaiser Family Foundation and published on October 11, the top issue that emerged for women voters overall was inflation, including rising household expenses. More than one-third (36 percent) of respondents cited it as the most important issue. This was followed by threats to democracy, which 24 percent of respondents cited, and immigration and border security, which 13 percent of women cited. The same number – 13 percent – cited abortion as the most important issue. The national exit polls were consistent with this. According to a preliminary national exit poll conducted by data provider Edison Research, 31 percent of voters said the economy mattered most in shaping their decision to vote whereas 14 percent cited abortion. How did women vote in this election? Women did vote for Harris but by a smaller margin than they did for Democrats in previous elections – for Biden in 2020 and for Hillary Rodham Clinton in 2016. CNN’s exit polls found that Harris won female voters’ support by 10 percentage points over Trump. But in 2020, Biden won their support by 15 percentage points, and in 2016, Clinton did by 13 percentage points. While Harris made slight inroads with white female voters, Trump still won them by 8 percentage points. Historically, white women have voted for the Republican candidate. According to the CNN polls, Harris also won 92 percent of the votes of Black women, compared with Trump’s 8 percent. This was up from Biden’s 90.5 per h i won 61 percent of their votes this election – 22 percentage points above Trump. But this margin was markedly lower than the 39-point lead Biden had over Trump with Latina women in 2020. Campaign signs at an early voting site at the West Oaks Branch Library in Ocoee, Florida, United States on October 27, 2024 [Paul Hennesy/Anadolu via Getty Images] What did Harris do wrong and Trump do right? Harris focused too much on abortion rather than more salient issues, such as economic policies, that would appeal to working class voters, including women, David Schultz, an author and political science professor at Minnesota’s Hamline University, told Al Jazeera. Voters had more confidence in Trump’s ability to handle the economy, and the Republican candidate ostensibly was better at persuading working class and middle class voters on his economic policies. Harris on the other hand, appealed more to college-educated, upper middle class voters. White women with college degrees tended to vote for Harris this election – 53.5 percent did so – while 64 percent of white women without degrees voted for Trump, according to a CNN poll. “The Harris campaign did not necessarily do a good job of explaining how her policies would help the middle class, or at least that message wasn’t really res- onating with a lot of voters,” Melissa Deckman, a political scientist and the CEO of Public Religion Research Institute, told the Reuters news agency. Schultz added that this caused Harris to lose critical battleground states that had consistently voted for Democrats before 2016. “Harris lost Wisconsin because she lost the working class and did not win women, suburbs and young voters,” he said. handling the economy compared with the 47 percent who trusted Harris. An analysis by the Washington, DC-based Brookings Institution think tank also found Harris had inherited America’s disapproval of Biden, which has grown during his presidency because of economic dissatisfaction. Americans were par- ticularly displeased with Biden’s handling of inflation, according to polls com- piled by the FiveThirtyEight website. Since Biden came to power, consumer prices have risen by more than 19 percent. The Brookings analysis also indicated Harris made some wrong tactical choices. For example, her decision to avoid media interviews when she first appeared on the Democratic ticket led to voters losing confidence in her ability to think on her feet, Brookings said. Deckman added that Harris’s choice of Minnesota Governor Tim Walz as her vice presidential running mate over Pennsylvania Governor Josh Shapiro was another of “several mistakes” made by the Democrats in the run-up to this elec- tion. This is because Walz did not help Harris sway any swing states. Minnesota has voted Democratic in every single presidential election since 1976. Is the right to abortion in danger in the US under Trump? Once Trump takes power in January, a nationwide abortion ban seems unlikely, but states may restrict abortion. Before this year’s election, Trump said he would veto any federal abortion ban because he believes abortion is an issue that should be left to each state. As of this week, abortion is banned in 13 states under almost all circumstances. In an additional four states, abortion is banned past six weeks of pregnancy. tional limits. Nine states and the District of Columbia place no restrictions on abortion. On Tuesday, 10 states voted on whether to embed the right to abortion in their constitutions. These measures were brought to the ballot by abortion rights groups. Seven states passed the abortion rights amendments, paving the way for abor- tion restrictions to be lifted in Missouri, where abortion was banned under any circumstances except medical emergencies, and Arizona, where abortions were banned past 15 weeks. These restrictions will be lifted in the coming weeks. The measures also passed in Colorado, New York, Maryland, Montana and Nevada, where abortion is legal but now this will be enshrined in their state constitutions. Florida, Nebraska and South Dakota failed to pass their own amendments, and their bans remain. Florida bans abortions past six weeks of pregnancy, Nebraska bans them past 12 weeks and South Dakota bans them in almost all circumstances. However, The New York Times reported on Wednesday that anti-abortion Republicans will place pressure on Trump to enact a federal ban on abortion. Navigating Wokeness: Voter Perceptions and the 2024 Election Author: Yokley, Eli, Date: July 19, 2023 at 5:00 am PST Collections: NeuroPsychoLinguisticPolitics, IdentityPolitics Zotero Key: VR72RHEL Cite Key: Yokley23wokeVoterPerception Zotero Item | Lit Note Sign up to get the latest data and analysis on how business, politics and economics intersect around the world. Florida Gov. Ron DeSantis, the No. 2 contender for the Republican Party’s 2024 presidential nomination, has beaten the drum of anti-wokeness. He has criticized major companies, the media, educational institutions and Democrats for efforts he calls “woke.” Supporters of such efforts, meanwhile, say they’re aimed at promoting social equality and acceptance in the United States. Our new research shows that conservative messaging is especially resonant among those who expect to vote in a Republican presidential primary or caucus in 2024. And while a number of corporations have taken hits in recent years for their diversity efforts, energy on the right appears strongest when anti-woke rhetoric is directed toward schools — especially over transgender issues — suggesting educational institutions are riper targets. Looking ahead to the general election, there is also room for beating the anti-woke drum to rile up some of the broader electorate, but voters overall are not nearly as likely to see the issue as a major problem. Focusing too much on wokeness is a risk for the Republican Party, putting it out of step with larger concerns among key voters it will need to win congressional majorities and the presidency. How the American electorate feels about wokeness The word “wokeness” itself resonates with the overall electorate, with roughly 3 in 4 voters expressing positive or negative views about it, our new survey found. This is similar to the share who have feelings about “MAGA” — shorthand for former President Donald Trump’s slogan “Make America Great Again,” which Democrats have leveraged to paint Republican opponents as extreme — or “socialism,” the GOP’s go-to campaign trail pejorative. According to the June 16-18 survey, the overall electorate is 17 percentage points more likely to express negative opinions about wokeness than positive ones. The term is particularly off-putting to the right, with 67% of potential GOP primary voters viewing wokeness unfavorably — nearly matching their scorn for socialism. The term “wokeness” has long been used by African American activists, but it became more prominent online in response to police killings of Black people, including Michael Brown in 2014. It took on a broader meaning to acknowledge social equality issues facing women after Trump’s 2016 election as well as other issues facing Black Americans amid mass protests following the police murder of George Floyd in 2020. Republicans responded by elevating concerns about a “woke mob” in reference to protesters and “woke capital” in reference to corporations and investors embracing social movements. Right-wing criticism of wokeness has given rise to presidential candidates. Vivek Ramaswamy became a conservative media fixture for his activism against corporate diversity, equity and inclusion and environmental, social and corporate governance practices. And DeSantis, who declared Florida the state “where woke goes to die,” made national headlines for backing the Stop WOKE Act targeting critical race theory and legislation banning classroom instruction about sexual orientation and gender identity that opponents have derisively called the “Don’t Say Gay” bill. To summarize their positions, to be woke is to force a liberal ideology upon Americans — and anti-wokeness puts these candidates in line with the bulk of their party’s 2024 electorate. When asked to define what it means to be “woke,” 55% of potential Republican primary voters chose “forcing a liberal ideology on American society,” compared with a quarter who selected “advocating for social equality and acceptance in American society.” The belief that wokeness is forcing something upon Americans was shared most strongly among DeSantis supporters, 72% of whom aligned with the statement, compared with 54% of Trump backers and 47% of those who support another candidate. Unlike DeSantis, Trump — himself a critic of some things the Florida governor calls “woke” — has not leaned so heavily into explicitly anti-woke rhetoric. In June, the former president told Iowa voters that “half the people can’t even define it — they don’t know what it is.” Still, DeSantis’ focus on the issue could give him something to tug on as he seeks support from the roughly 4 in 5 potential primary voters who are not currently supporting his bid, especially older GOP voters who are more likely to agree with his perspective on wokeness than their younger peers. Where Republican candidates can meet their voters on wokeness DeSantis caught major media attention in the lead-up to his presidential campaign for going to war against The Walt Disney Co. over its public opposition to his so-called Don’t Say Gay law. Our data suggests that DeSantis’ continued anti-wokeness attacks on educational institutions may resonate more with the party’s electorate than those aimed at corporations or Wall Street. Many also blame social media platforms, which some conservatives have accused of de-emphasizing or censoring right-wing accounts. When asked whether a selection of groups and institutions are going “too far” to promote social equality and acceptance in the United States, potential Republican primary voters were most likely to blame the news media and the Democratic Party, along with social media platforms. Beyond these institutions — which have long been targets of ire from the right — more than half said the same of colleges and universities (53%) and K-12 schools (52%). That’s higher than the 43% and 33%, respectively, who blame corporations and Wall Street, both of which have faced heat from the right for supporting DEI, ESG and other social initiatives. Furthermore, when asked about social issues that some politicians have described as threatening, the Republican electorate is generally less animated about ones involving businesses than they are about those involving children and schools, especially when it comes to LGBTQ matters. For example, fewer than 2 in 5 potential Republican primary voters see allowing drag performances in places such as bars and restaurants, or requiring companies to offset their carbon emissions by 2050, as a “major threat.” Much of the party’s electorate similarly lets colleges off the hook for hosting DEI programs. On the other hand, large shares of the GOP’s expected voters said allowing children to receive gender-transitioning care (73%) or permitting transgender athletes to participate in college sports (65%) constitute major threats. When it comes specifically to classroom instruction, the Republican Party’s electorate is especially against instruction about LGBTQ issues and critical race theory — even at the college level. The majority of potential Republican primary voters also oppose including LGBTQ civil rights, critical race theory, and sexual orientation and gender identity in curricula at both the college and K-12 levels, creating an opening for candidates who may potentially want to go even further than DeSantis has and even more headaches for educational institutions, at least during the Republican primary. Among the overall electorate, support for classroom discussion of topics such as sexual orientation and gender identity, critical race theory, and the LGBTQ civil rights movement is divided when it comes to K-12 classrooms, but there’s little question that such subjects are fair game for colleges. Looking ahead to 2024 Roughly six months from the first GOP nominating contest, potential Republican primary voters are only slightly more likely to say Trump would do a better job of fighting wokeness than DeSantis, 78% to 74%, due to more people being aware of the former president. The issue is likely to remain prominent in the contest, especially given how big a deal the party’s electorate thinks it is. Our data found that 57% of potential primary voters said woke ideology in American society is a major threat to the United States, matching the share who said the same of domestic terrorism and just slightly higher than the share who cited global economic disruptions. Still, the GOP’s voters are unlikely to respond solely to such messaging given their greater unanimity on the threats of inflation, China’s growing influence and illegal immigration into the United States. Rather, the wokeness issue, as DeSantis and others have famed it, is a tool for channeling larger cultural grievances on the right. But like some other hot-button issues on the right, there is a chance the emphasis on wokeness could alienate the larger electorate. Only 35% of voters see fighting woke ideology as a “very important” issue when thinking about their 2024 vote. Voters are around twice as likely to prioritize the economy, health care and entitlement programs compared with fighting wokeness. Arguments such as Trump’s about growing the economy and protecting Social Security and Medicare remain in line with where primary and general election voters are. Still, targeting wokeness does harness unique energy on the right, meaning universities and even major corporations will have to grapple with the issue as the campaign unfolds. Already on Capitol Hill, House conservatives gummed up the process for passing the typically bipartisan National Defense Authorization Act over issues such as rights for transgender people. In addition to his attacks on Disney, DeSantis has entered the fray over the controversy involving Bud Light. And countless state legislators have pushed back on educational institutions when it comes to LGBTQ issues. Ellyn Briggs, our analyst covering brands and marketing, noted in a recent memo that roughly 1 in 4 consumers would consider boycotting a “woke” brand, but she warned that such controversies can take on a life of their own in the increasingly polarized media environment — a concern that we expect will only grow in an election year. Harris Had a Wall Street-Approved Economic Pitch. It Fell Flat. Author: Nehamas, Nicholas, Date: 2024-11-09 Collections: Hot Takes US Elect 2024 Zotero Key: VYADYSFK Cite Key: Nehamas24HarrisHadWall Zotero Item | Lit Note When Vice President Kamala Harris traveled to a locally owned brewery in New Hampshire to talk about helping small businesses — a major plank of her economic platform — she made sure that one group of Americans felt included: millionaires who wanted to keep more of their profits from selling stocks and real estate. “If you earn a million dollars a year or more, the tax rate on your long-term capital gains will be 28 percent under my plan,” Ms. Harris said in that campaign speech this fall. “Because we know when the government encourages investment, it leads to broad-based economic growth.” The moment stuck out. In remarks that her campaign had pitched as a major address to the middle class, Ms. Harris offered a striking concession on tax rates for the wealthy — an olive branch that she used to present herself as more business friendly than President Biden, who had sought a higher rate. Her speech underscored just how much the advice of her allies and donors from Wall Street and Silicon Valley — as well as her own longstanding belief in pragmatic, incremental progress over sweeping, ideological change — was driving her messaging on the economy. One important influence on Ms. Harris was her brother-in-law, Tony West, who took a leave from his job as the chief legal officer at Uber to advise her campaign. Ms. Harris would often ask her staff, “Has Tony seen this?” before she would review her economic speeches or talking points, according to twopeoplewith knowledge of the conversations. Mr. West, who served as a top Justice Department official in the Obama administration but has little background in economic policy, also flagged social media posts from her campaign and official accounts that he thought were off Ms. Harris’s economic message, one of the people said. He and Brian Nelson, a longtime adviser to Ms. Harris, were in frequent contact with business executives and Wall Street donors during the campaign. The result was a Democratic candidate who vacillated between competing visions for how to address the economic problems that voters repeatedly ranked as their top issue. Ms. Harris neither abandoned nor fully embraced key liberal goals for confronting corporate power and raising taxes on the rich. Instead, she adopted marginal pro-business tweaks to the status quo that both her corporate and progressive allies agreed never coalesced into a clear economic argument. Voters ultimately preferred Donald J. Trump’s broad but vague promises to cut taxes and shake up the global trading system. And as frustration with the historic increase in inflation has led to losses among governing parties across the world, some Democrats doubt that Ms. Harris could have prevailed even with a stronger economic message. The Harris campaign and Mr. West declined to comment for this article. Democrats from across the party saw plenty to criticize in the economic agenda that Ms. Harris’s campaign had to slap together in a matter of weeks after Mr. Biden dropped out. Rather than the “opportunity economy” that Ms. Harris envisioned, several Democrats said Americans hungered for a more ambitious overhaul of the system. “When you’re too conflicted between the interest of corporate America and average working-day people, I think this is what you end up with,” said Jimmy Williams, the president of the International Union of Painters and Allied Trades. “A message that doesn’t resonate.” Failing to call out the ‘big, bad wolf’ An early case study of Ms. Harris’s quandary came during her first economic policy speech, in North Carolina in August. Members of her communications team had suggested she focus her speech on cracking down on corporate price gouging, according to two people briefed on the deliberations, as a way to address rising costs. Her speech ultimately focused only on addressing price gouging for groceries. Still, the criticism from both Republicans and Ms. Harris’s Wall Street allies came swiftly. They slammed the plan as price controls that would disrupt economic growth. The blowback caught Ms. Harris’s operation off guard, and campaign officials further narrowed the scope of her plan. While she brought up the idea repeatedly in campaign speeches and television advertisements, for some Democrats the approach remained too intangible. “You can say there’s a big, bad wolf blowing houses down, but you have to go after those people and point them out,” said Dwan Walker, the Democratic mayor of Aliquippa, a small, formerly industrial city outside Pittsburgh. “It’s effective because now you went after somebody, you brought it to bear and you showed us what it was.” Updated Nov. 7, 2024, 7:45 p.m. ETNov. 7, 2024 Over the course of the campaign, it became clear that Ms. Harris would de-emphasize Mr. Biden’s attacks on big companies in favor of a more conciliatory approach that she hoped would appeal to moderates. She wanted corporate leaders in her camp, as she tried to outrun the progressive reputation she had gained during the 2020 presidential primary race and to blunt Mr. Trump’s attacks that she was a “communist.” “We’re a center-right country,” said Charles Myers, a fund-raiser for Ms. Harris and the chairman and founder of Signum Global Advisors. “One of the few things we all agree on as Americans is the American dream, and a very big part of the American dream is the accumulation of wealth.” In an economic policy speech in late September in Pittsburgh, the vice president spoke of the importance of investing in “A.I., quantum computing, blockchain, digital assets and other emerging technologies” alongside a pledge to support “factory towns and workers.” Such messaging was a departure from Mr. Biden, who often styled himself as the most pro-labor president in the country’s history. Still, while progressives loved much of Mr. Biden’s economic message, he appeared on track for an even bigger loss to Mr. Trump. And aides to Ms. Harris pointed out that voters graded her more favorably than Mr. Biden on the economy, as she pushed policies including expanding the child tax credit, granting $25,000 in down-payment assistance to first-time home-buyers and allowing Medicare to cover home health care. Trump’s simple explanation On the campaign trail, Ms. Harris sometimes sent mixed signals about her economic agenda. Her campaign was intentionally ambiguous about whether she backed Mr. Biden’s ambitious idea to raise taxes on the ultrawealthy. She neither supported nor directly criticized financial regulators in the Biden administration. She did not talk about raising the minimum wage until two weeks before Election Day and frequently noted that Goldman Sachs analysts preferred her economic plan to Mr. Trump’s. On Election Day, voters decisively chose Mr. Trump, who promised across-the-board tariffs and the mass deportation of immigrants. Such policies would reshape the economic life of the United States — and many economists on the right and left agree that they would significantly raise prices. “People want to understand what’s going on in their lives. Trump gave them an explanation,” Senator Bernie Sanders, independent of Vermont, said in an interview. “He attributed all of our problems to undocumented immigrants. What is the Democratic explanation for why the gap between the rich and the poor is getting wider and working-class people are struggling? You tell me.” The results showed that Democrats lost ground with voters across the board compared with 2020, including in working-class areas and among people of color. Although Ms. Harris campaigned hard in the suburbs, pitching herself as a centrist to try to win over moderate Republicans and independents, her support there lagged as well. Nationwide, Democratic enthusiasm withered. While Mr. Trump received roughly the same amount of votes he did in 2020 — when he lost — Ms. Harris appears to have fallen well short of Mr. Biden’s haul four years ago. ‘You can’t out-Republican Republicans.’ While Democrats were defeated, populist, progressive economic policies did well at the ballot box. In Missouri, a red state, voters passed an amendment to raise the minimum wage to $15 an hour by a wide margin, even as they overwhelmingly voted against Ms. Harris. In Alaska, they were poised to do the same, with results still being tallied. Roughly 75 percent of voters in conservative Nebraska backed a measure to institute paid sick leave. Ms. Harris did not make either policy a major part of her campaign. Now, the miscalculations the Harris campaign potentially made on the economy could shape how the Democratic Party moves forward as it enters a period of rebuilding and recrimination. Progressives said the answer was to develop a distinctive message. “There is a tendency in every general election to run to the center to court moderate Republicans,” said Representative Pramila Jayapal, Democrat of Washington and the chair of the Congressional Progressive Caucus. “You can’t out-Republican Republicans.” Sara Nelson, the international president of the Association of Flight Attendants union, said that, for Democrats, “the biggest problem is that people would say, ‘I can’t tell the difference between the parties.’ And they leaned hard into that problem.” Some of Ms. Harris’s loudest backers have been more muted since the election. During the campaign, the billionaire Mark Cuban became an increasingly prominent surrogate for the vice president. He said repeatedly that Ms. Harris would abandon higher taxes on the ultrawealthy and appeared with her on the trail in swing-state Wisconsin. But he declined to comment when asked this week to weigh in on Ms. Harris’s economic messaging. “No interest in talking politics at all,” Mr. Cuban wrote in an email. Kate Kelly, Erica L. Green, Reid J. Epstein and Ruth Igielnik contributed reporting. The Long Global Trail of Resentment Behind Trump’s Resurrection Author: Cohen, Roger, Date: 2024-11-09 Collections: Hot Takes US Elect 2024 Zotero Key: 8HXS26TF Cite Key: Cohen24longGlobalResentTrump Zotero Item | Lit Note As the Cold War wound down almost four decades ago, a top adviser to the reformist Soviet leader, Mikhail S. Gorbachev, warned the West that “we are going to do the most terrible thing to you. We are going to deprive you of an enemy.” In the celebrations of the triumph of Western liberal democracy, of free trade and open societies, few considered how disorienting the end of a binary world of good and evil would be. But when the spread of democracy in newly freed societies looked more like the spread of divisive global capitalism, when social fracture grew and shared truth died, when hope collapsed in the communities technology left behind, a yearning for the certainties of the providential authoritarian leader set in. “In the absence of a shared reality, or shared facts, or a shared threat, reason had no weight beside emotion,” said Nicole Bacharan, a French political scientist. “And so a dislocated world of danger has produced a hunger for the strongman.” A different Russia, briefly imagined as a partner of the West, eventually became an enemy once more. But by the time it invaded Ukraine in 2022, disillusionment with Western liberalism had gone so far that President Vladimir V. Putin’s tirades against the supposed decadence of the West enjoyed wide support among far-right nationalist movements across Europe, in the United States and elsewhere. Western allies stood firm in defense of Ukrainian democracy, but even that commitment is wobbling. The curious resurrection and resounding victory of Donald J. Trump amounted to the apotheosis of a long-gathering revolt against the established order. No warning of the fragility of democracy or freedom, no allusion to 20th-century cataclysm or Mr. Trump’s attraction to dictators, could hold back the tide. If Russia was humiliated by the collapse of its communist imperium, as Mr. Putin has long asserted, it now reveled in the victory of Mr. Trump, who is dismissive of climate change, big on male virility and who argues, like Mr. Putin, that the West of networked elites is the place where family, church, nation and traditional notions of gender go to die. More dangers abound than when Mr. Trump won in 2016. In a world of rival powers where the post-1945 order seems largely dead, wars rage in Europe and the Middle East. They spread and efforts to end them have proved ineffectual. North Korea, a nuclear power whose troops now bolster Russian forces against Ukraine, is drawn in. Iran’s long conflict through surrogates with Israel escalates into direct exchanges of missiles. Loose talk of nuclear war resurfaces as a paralyzed United Nations Security Council looks on. “The Sleepwalkers” was the title of Christopher Clark’s book on the onset of World War I. They appear to many to be afoot once more. To this mire will now be added the chaotic, impulsive, high-risk approach to foreign policy described with near unanimity by Mr. Trump’s top aides during his first term, as well as his expressed contempt for NATO and the European Union, anchors of postwar Western security and stability, and his threats of confrontation with China in the form of punishing tariffs. A turbulent world and a turbulent personality make for a dangerous mix. During the election campaign, Mr. Trump made much of the fact that the European and Middle Eastern wars erupted after his first presidency and tried to portray Kamala Harris, the Democratic candidate, as the warmonger. His adviser Stephen Miller warned on X, that a Harris victory would mean “We invade a dozen countries. Boys in Michigan are drafted to fight boys in the Middle East. Millions die.” These were totally unfounded claims. But many Americans believe that Mr. Trump, at heart a businessman for whom foreign policy is merely a matter of transactional resolve, will usher in an era of prosperity incompatible with the turbulence of war. During his first term, he forged the Abraham Accords normalizing relations between Israel and four Arab states. Europe, however, is worried. Thomas Bagger, the State Secretary of the German Federal Foreign Office, said that “the shock is more profound because this time the election of Trump is not an accident but a clear expression of what America is and what it wants.” Speaking as Germany’s coalition government collapsed and uncertainty loomed before a general election next year, he added that Mr. Trump’s victory was particularly troubling because “the German Federal Republic is a creation of the United States of America, the fruit of postwar enlightened American policy.” Mr. Trump said this year that he would encourage Russia to do “whatever the hell they want” to any NATO members not meeting the alliance’s targets for spending on defense, and has suggested he would cut back on critical American support for Ukraine. For the international system, a Russian victory in Ukraine would affirm a principle of might over right, and for Europe it would pose a direct threat. Europe is more divided and has moved rightward over the past eight years. This was aptly symbolized by a meeting of European leaders Thursday in Budapest, where Prime Minister Viktor Orban of Hungary, in a message on X, greeted Mr. Trump’s re-election as a “much needed victory for the world.” Mr. Orban’s illiberal model, which has severely curtailed press freedom and the independence of the judiciary, has been hailed in Mr. Trump’s entourage as a possible template. Mr. Trump has called Mr. Orban a “fantastic” leader. The assembled European leaders, prodded by President Emmanuel Macron of France, agreed the continent should take more responsibility for its defense given the unpredictability of Mr. Trump’s America. But they were divided on sustained support for Ukraine, which Mr. Orban opposes. “There is no possible good outcome in Ukraine today,” said Ms. Bacharan, the French political scientist. “Trump wants the war over and, with Putin, will do whatever it takes.” With nationalist and anti-immigrant political currents strong throughout the continent, Mr. Trump will have more levers than during his first term with which to undermine the 27-nation European Union. The possibility that Europe will splinter, with each nation cutting its own deals with Washington, appears real. “As a nation we don’t have a way to deal with a world where every country is only looking out for itself,” Mr. Bagger said of Germany. “We nurtured the idea of an international community because it was the only post-Nazi way to think of ourselves. So where we turn in Trump’s world is unclear.” Many nations are asking themselves similar questions. Antonio Gramsci, the Italian political philosopher, wrote in 1930 of a world in which “the old is dying and the new cannot be born.” Mr. Trump could be returning to power in another such moment. He is not responsible for the breakdown of the Western-dominated postwar order or the diminishing magnetism of democracy for some as compared to China’s autocratic growth model, but he will have to deal with the consequences. The BRICS group of emerging market nations is now a powerful counterweight to the West, as illustrated at its meeting last month, hosted by Mr. Putin. Entrenched Russian and Chinese hostility toward the United States will complicate Mr. Trump’s every foreign policy endeavor. India, at once a BRIC member with close ties to Russia and a close friend of the United States, enjoyed good relations with Mr. Trump during his first term. Jawed Ashraf, the Indian ambassador to France, said he expected that to continue. But Mr. Ashraf added: “We are in a state of the world where people are seeking new answers. There’s a lack of belief in the future. Economic models unable to deliver, unfettered social media, and global volatility lead to taking it out on immigrants and questioning of democratic systems.” Mr. Trump’s victory was part of this wider phenomenon. In societies atomized by the overwhelming pace of technological change, and marked by growing inequality, Mr. Trump had simple answers that resonated. Those answers were the border and the pocketbook, the former too porous and the latter too empty. He would fix both. “It was the fight-fight-fight backlash,” said Pascal Bruckner, a French author and philosopher, alluding to Mr. Trump’s words after he narrowly survived an assassination attempt in July. “No more complex diagnosis, no more delicate decisions.” “God spared my life for a reason,” Mr. Trump said at his victory speech early Wednesday. The possibility of a sense of divine mission, backed by a clear electoral mandate, could make the likelihood of balanced policy more remote. Mr. Trump has not moderated in almost a decade since embarking on his first presidential campaign. “People want strength,” he said then. “We’re going to be so tough and so mean and so nasty,” he said. He got the blood up. Many dismissed him as a buffoon. But with his uncanny political antennae, attuned to humanity’s fears and resentments, he was onto something. China was rising; American power ebbing; Afghanistan and Iraq were graveyards of American glory; millions of struggling Americans felt forgotten or invisible; and the establishment had not understood the fact-lite theater of the contemporary world. It was the perfect storm for rabble-rousing. Far from an anomaly, Mr. Trump now looks like an inevitability, the answer, not once but twice, to the shattering of hopes for liberal democracy that accompanied the fall of the Berlin Wall. What the Left Keeps Getting Wrong Author: Williams, Thomas Chatterton, Date: 2024-11-07 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: R73DI7GF Cite Key: Williams24WhatLeftKeeps Zotero Item | Lit Note This article was featured in the One Story to Read Today newsletter. Sign up for it here. There is no single explanation for Donald Trump’s unambiguous win. But if, as we were constantly told, this was in fact the most important election of our lives, in which the future of democracy really was at stake, Democrats never conducted themselves that way. It was an egregious mistake—not just in retrospect but in real time—to allow Joe Biden to renege on his implicit promise to be a one-term president, and to indulge his vain refusal to clear the way for younger and more charismatic leaders to rise up and meet the magnitude of the political moment. Perhaps no candidate, not even one blessed with the talents of a Bill Clinton or a Barack Obama, could have overcome the handicap imposed on Kamala Harris when she emerged valiantly from the wreckage of the Weekend at Bernie’s campaign this summer, which her own administration had so brazenly tried to sneak past the voting public. But other major mistakes were made over the past four years. The Biden presidency was understood to be a return to normalcy and competence after the terrible upheavals of the early months of COVID and the circus of the first Trump administration. That was the deal Americans thought had been accepted—that was Biden’s mandate. Instead, as president, even as he leaned into plenty of policies that served all Americans, Biden either could not or would not forcefully distance himself from the Democratic Party’s need for performative “wokeness”—the in-group messaging used by hyper-online and overeducated progressives that consistently alienates much of the rest of the nation. Read: Why Biden’s team thinks Harris lost Here’s one narrow but meaningful example: On day one—January 20, 2021—the Biden administration released an “Executive Order on Preventing and Combating Discrimination on the Basis of Gender Identity or Sexual Orientation.” The order said that “children should be able to learn without worrying about whether they will be denied access to the restroom, the locker room, or school sports.” Supporters argued that the order was simply pledging that the administration would enforce previously established legal protections for LGBTQ people, but critics saw it differently. As the author Abigail Shrier wrote on Twitter: “Biden unilaterally eviscerates women’s sports. Any educational institution that receives federal funding must admit biologically-male athletes to women’s teams, women’s scholarships, etc. A new glass ceiling was just placed over girls.” In signaling their commitment to an extreme and debatable idea of trans rights, Democrats hemorrhaged other constituencies. Many Americans of all races care about girls’ sports and scholarships, and they believe that protecting women’s rights and flourishing doesn’t begin and end at safeguarding their access to an abortion. Out of this larger context, Harris entered the final stretch of the campaign already compromised. Republicans seized on her previous comments in support of progressive proposals such as defunding the police (which she later renounced). But it was more than culture-war flash points. Fair or not, many Americans didn’t believe Harris deserved to be vice president in the first place. This is in large part the fault of her boss, who stated up front before selecting her that he would prefer a vice president “who was of color and/or a different gender.” It was a slightly less blunt version of what he said before appointing Justice Ketanji Brown Jackson—that the job was only ever available to a Black woman. Harris’s very presence within the Biden administration therefore, to many onlookers, amounted to a kind of glaring evidence of precisely the kind of DEI hiring practices they intended to repudiate on Tuesday. Voters’ response was definitive. According to a New York Times analysis, “Of the counties with nearly complete results, more than 90 percent shifted in favor of former President Donald J. Trump in the 2024 presidential election.” That is to say, Trump improved with every single racial group across the country except one. He performed slightly better with Black voters overall (13 percent voted for him this time, according to exit polls, compared with 12 percent in 2020), and significantly better with everyone else—particularly Latinos, 46 percent of whom gave him their vote. He received an outright majority of ballots from voters marking the “other” box—a first for Republicans—and his party reclaimed the Senate and looks poised to hold on to the House. All told, the only racial group among whom Trump lost any support at all turned out to be white people, whose support for him dropped by a percentage point. Were Trump not such a singularly polarizing, unlikable, and authoritarian figure, one of the most salient and—when glimpsed from a certain angle—even optimistic takeaways from this election would be the improbable multiracial and working-class coalition he managed to assemble. This is what Democrats (as well as independents and conservatives who oppose Trump) must reckon with if they are ever going to counter the all-inclusive nihilism and recklessness of the new MAGA majority. Much attention has been paid to the gender gap in voting, and it’s true that more men voted for Trump than women. But the fact that so many citizens of all geographies and skin tones wanted to see Democrats pay a price, not just for policy differences but also for the party’s yearslong indulgence of so many deeply unpopular academic and activist perspectives, must be taken seriously. Read: Why Democrats are losing the culture war “The losses among Latinos is nothing short of catastrophic for the party,” Representative Ritchie Torres of the Bronx told The New York Times. Torres, an Afro Latino Democrat, won a third term on Tuesday. He criticized the Democrats for being beholden to “a college-educated far left that is in danger of causing us to fall out of touch with working-class voters.” Yet I fear that far too many elite Democrats will direct their ire and scrutiny outward, and dismiss the returns as the result of sexism and racism alone. In an Election Night monologue on MSNBC, the anchor Joy Reid expressed this mentality perfectly. Anyone who knows America, she said, “cannot have believed that it would be easy to elect a woman president, let alone a woman of color.” Her panel of white colleagues nodded solemnly. “This really was an historic, flawlessly run campaign,” Reid continued. “Queen Latifah never endorses anyone—she came out and endorsed! She had every prominent celebrity voice. She had the Swifties; she had the Beyhive. You could not have run a better campaign.” Over on X, Nikole Hannah-Jones, the creator of The New York Times Magazine’s “1619 Project,” wrote that we “must not delude ourselves”: “Since this nation’s inception large swaths of white Americans—including white women—have claimed a belief in democracy while actually enforcing a white ethnocracy.” Moments after North Carolina was called for Trump, Reid diagnosed what went wrong for Harris: White women, she said, didn’t come through; it was “the second opportunity that white women in this country have to change the way that they interact with the patriarchy,” and they had failed the test again. On X, commentators immediately jumped on the blame-white-women bandwagon, as if it was an evergreen obituary they all had on file, ready to post within a moment’s notice. Reflexive responses like these exemplify the binary framing of culture and politics in the United States—white/nonwhite, racist/anti-racist—that ascended with the death of Trayvon Martin in 2012 and peaked after the racial reckoning of 2020. For many on the left, it has proved a powerful and compelling means of contextualizing enduring legacies of inequality and discrimination that are rooted in past oppressions. And it has notched real successes, especially by forcing the country to confront bias in the criminal-justice system and policing. But it has also become a casualty of its own discursive dominance—an intellectual and rhetorical straitjacket that prohibits even incisive thinkers from dealing with the ever-evolving complexity of contemporary American society. As a result, it has taught far too many highly compensated pundits, administrators, scholars, and activists that they never have to look inward. Ronald Brownstein: An uncertain future beat an unacceptable present But the framing didn’t work for many other people. “I’m thankful that victimhood didn’t win as a strategy,” one of my oldest and closest friends, a Black man who doesn’t have a college degree, messaged me after Trump’s victory. (It is worth noting that his twin brother, a veteran, turned MAGA during the racial reckoning.) If we are to listen to what enormous numbers of our compatriots—including unprecedented numbers of newly minted nonwhite GOP voters—are trying to tell us, the straitjacket proved decisive in their shift rightward. All of us who reject the vision of America that Trumpism is offering are going to have to do something grander than merely counter a vulgar celebrity demagogue who commands a potent populist movement. It is too late for that anyway. We are going to have to reimagine the inner workings of the multiethnic society we already inhabit. The stale politics of identity that tries to reduce even the glaringly inconvenient fact of Trump’s multiracial alliance to “white women” stands in the way of overcoming the real democratic crisis. Harris herself knows this. When Trump attempted to goad her, mockingly pondering whether she was even Black at all, she shrewdly avoided appealing to superficial categories. In this crucial way, her campaign may be viewed as an unequivocal success, one that we can learn from. The One Thing Americans Remember About Biden Author: Igielnik, Ruth, Date: 2025-01-19 Collections: Hot Takes US Elect 2024 Zotero Key: FY24STVT Cite Key: Igielnik25rememberAboutBiden Zotero Item | Lit Note What one thing do you remember most about Joe Biden’s presidency? We surveyed more than 2,000 Americans this month and asked for their most prominent memory of Mr. Biden’s time in office. Here’s what they said, in their own words. • “Economy wrecker”* Trump voter in 2024 • “The economy improved”* Harris voter • “Giving money to Israel and Ukraine”* Harris voter • “Allowing migrants illegally”* Did not vote • “Very high border crossings”* Trump voter • “His declining cognitive abilities”* Harris voter • “Trying to help the common people”* Harris voter • “He was a total disaster”* Trump voter • “A return to normal presidential responsibility and decency”* Harris voter President Joe Biden will leave office on Monday with a dismal approval rating and a complicated legacy. Unsurprisingly, Americans’ positive and negative memories of Mr. Biden in a poll conducted by The New York Times and Ipsos this month largely split along partisan lines. Respondents who voted for Donald J. Trump were unsparing in their criticism of Mr. Biden, while those who voted for Kamala Harris had mostly positive views, though some also disapproved. What they said about Mr. Biden in these open-ended responses offers an early look at his legacy in the public’s mind. Republicans, in particular, pointed to Mr. Biden’s mental state and age as the top thing they remember. Many Democrats relayed memories of Mr. Biden’s kindness and empathy, while others cited the economy, at times in a positive light and other times negatively. A quarter of respondents could not think of a memory at all or declined to share one. Thinking back on Joe Biden’s presidency, what one thing do you remember most about his time in office? Based on a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted Jan. 2-10. Top 10 categories shown, excluding “don’t know” or blank responses. In a separate question, nearly half of Americans said that Mr. Biden left the country worse off than when he took office, just one quarter felt he left it better off, and another 25 percent said things were the same as before he became president. There was a partisan split on this question, too, but Black and Hispanic Americans were more likely to say Mr. Biden made things worse than better, and Americans 18 to 29 were twice as likely to say Mr. Biden left the country worse off than better off. Memories of presidents are often not static, and can grow rosier over time, a phenomenon that played out after Mr. Trump’s first term. Here’s a closer look at Americans’ current views of Mr. Biden’s time in office. Comments from Americans who said what they remembered most was Biden’s age • “He usually didn’t have a clue what was going on around him”* Trump voter in 2024 • “His declining health and confusion”* Trump voter • “His dementia”* Trump voter • “He’s old”* Harris voter • “His performance in the debate was shocking”* Harris voter • “I think Joe Biden has a good heart, but he’s too old to be effective”* Harris voter • “He’s just not all there”* Did not vote Selected responses from a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted Jan. 2-10. Many Americans remember Mr. Biden more for his personal characteristics than his policies. Fourteen percent cited his age or perceived mental decline as their most prominent memory, a greater share than any specific policy. Another 4 percent mentioned memories related to his empathy and kindness. Concern about Mr. Biden’s cognition primarily came from Republicans, though some Democrats and independents also shared misgivings. Many specifically cited his debate performance, which proved to be a turning point in his aborted campaign, as their key memory of his time in office. Comments from Americans who said what they remembered most was the economy • “Gas prices skyrocketing”* Did not vote in 2024 • “Out of control spending, reduced energy creation and inflation”* Trump voter • “He worked hard for the middle class and added protections and laws to help most Americans”* Harris voter • “High gas prices”* Trump voter • “I honestly don’t know much of what Joe Biden did, but I know the economy has suffered”* Harris voter • “Inflation, inflation, inflation”* Trump voter • “Better economy”* Harris voter • “Prices have gone way up”* Harris voter Selected responses from a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted Jan. 2-10. During the campaign, voters consistently cited the economy as their most important issue. As Americans look back on Mr. Biden’s time in office, many mentioned economic conditions as their principal recollection. Republicans pointed to rising prices as the main impact of his presidency, while Democrats were largely more positive, citing the economic recovery from the Covid-19 pandemic. Still, many Democrats and independents had concerns about the cost of living. Comments from Americans who said what they remembered most was immigration • “Open border”* Trump voter in 2024 • “The huge numbers of illegal immigrants that have entered the country”* Trump voter • “Unsafe borders, terrible economic policies, weak leadership”* Harris voter • “So many immigrants living on welfare”* Trump voter • “Immigration ran amok for several years and now it is too late to try and curtail the problem”* Harris voter • “His indifference to open borders”* Harris voter • “Skyrocketing illegal immigration”* Did not vote Selected responses from a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted from Jan. 2-10. Mr. Trump made Mr. Biden’s immigration policy a core issue in his campaign to return to the White House. Border crossings rose during Mr. Biden’s presidency, creating the largest immigration surge in U.S. history (though crossings plummeted late in his term after he tightened enforcement). Among Republicans, Mr. Biden’s immigration policies were among the most mentioned memories. These recollections were often expressed with evident frustration, and were frequently intertwined with economic concerns. Comments from Americans who said what they remembered most was foreign policy • “Getting out of Afghanistan. It was horrible”* Trump voter in 2024 • “His epic debate failure and his retreat from Afghanistan, sacrificing troops’ lives doing it”* Harris voter • “Continuing to send weapons overseas during the Israel and Palestine conflict and not allowing Ukraine to use them against Russia”* Harris voter • “His inability to tell Israel that genocide is wrong, no matter the provocation”* Did not vote • “More interested in foreign aid than the American people”* Trump voter • “Helping a lot outside the country”* Harris voter • “War”* Harris voter • “Supporting war with billions”* Harris voter Selected responses from a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted Jan. 2-10. Americans who mentioned foreign policy mostly looked back on the Biden years as a time of war. Democrats and Republicans alike expressed concern about spending on foreign conflicts that they believed starved domestic spending. Overall, 60 percent of Americans in the survey said the United States was too focused on helping other countries and needed to focus more on problems at home. Many cited the wars in Ukraine and Gaza as their main memory of Mr. Biden’s time in office. Democrats were especially likely to cite concerns about the deaths of Palestinians during the Israel-Hamas war. Comments from Americans who said what they remembered most was pandemic recovery or legislative accomplishments • “Coming out of Covid, avoiding recession, dealing with global supply issues”* Harris voter in 2024 • “He got us through the pandemic, he probably saved many people from dying of Covid”* Harris voter • “Covid stimulus and rebuilding the economy after Covid”* Harris voter • “He fixed us from Covid mess”* Did not vote • “Build back better”* Harris voter • “Getting the bipartisan jobs act passed”* Harris voter • “Infrastructure”* Harris voter • “Forgiving student debt, Covid lockdowns”* Trump voter • “His attempt to unburden some of us with student loans”* Harris voter • “He forgave my loan”* Trump voter Selected responses from a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted Jan. 2-10. Few Americans mentioned memories of the pandemic, but those who did remembered Mr. Biden’s work to help pull the country out of it. Many cited his work on the economic recovery after the pandemic and his efforts to avoid a recession. Some, particularly Democrats, also cited key pieces of post-pandemic legislation like the Inflation Reduction Act and infrastructure spending. And for a handful, Mr. Biden’s student loan forgiveness was their key memory, including some who had firsthand experience. Comments from Americans who said what they remembered most was corruption • “Corrupt”* Trump voter in 2024 • “Corruption to enrich him and his family”* Trump voter • “Endless scandals and fraud”* Trump voter • “He is a criminal”* Trump voter • “Pardoned his son, bad policies”* Did not vote • “Pardoning his son”* Did not vote • “The lies and corruption”* Did not vote Selected responses from a poll by The New York Times and Ipsos of 2,128 U.S. adults conducted Jan. 2-10. A small but significant share of Republicans mentioned corruption, with many citing Mr. Biden’s pardon of his son, Hunter Biden, as evidence. Taken all together, these responses offer a snapshot in time as Mr. Biden leaves office. History shows that many former presidents later get a reputational boost. This was the case for George W. Bush, George H.W. Bush and Jimmy Carter. Time will tell if Mr. Biden will follow a similar path. The Short Campaign May Have Been Harris’s Undoing — and Biden’s Fault Author: Nagourney, Adam, Date: 2024-11-08 Collections: Hot Takes US Elect 2024 Zotero Key: UNYRM8D8 Cite Key: Nagourney24ShortCampaignMay Zotero Item | Lit Note Good evening. My colleague Adam Nagourney has covered 10 presidential elections — but never one as short as Vice President Kamala Harris’s speedy sprint. Tonight, he tells us why that could have been one of Harris’s biggest problems. And we’ll take a look at just how far the red wave went. — Jess Bidgood After President Biden stepped aside and Vice President Kamala Harris entered the race in late July, I wrote about Democrats’ hopes that the short time-frame of her presidential campaign — just 15 weeks — would be a good thing. They thought it would allow a relatively inexperienced candidate to ride a burst of enthusiasm past the pitfalls of a long campaign and into the White House. But now, it looks like that short campaign was one of the key factors behind her decisive loss to former President Donald Trump. And you can add that to the list of things that Democrats are blaming Biden for during this season of second-guessing and recriminations. Biden’s decision to run for re-election, while demanding that his party rally behind him, effectively cleared the field. That meant that Harris did not endure the rigors of a competitive primary process that might have sharpened her qualities as a candidate. And it meant that a party that once had reservations about Harris never had the option of watching her compete against other Democrats before settling on its nominee. Harris displayed formidable skills during her rapid-fire campaign, rousing crowds at rallies and outmaneuvering Trump at their debate. But in smaller, more intimate settings during the general election campaign, she stumbled over the kinds of questions — Example No. 1: how she would be different from Biden — that she would have repeatedly faced during the gantlet of town hall meetings, living room coffees and local interviews that define a primary campaign. In short, Harris might have been better off if she had been able to take her show on the road before heading to Broadway. “Not even a close call,” said Howard Wolfson, a former senior adviser to Hillary Clinton, who ran for president in 2008 and 2016. Wolfson said he wished Biden had decided after the midterm elections of 2022, when Democrats had a reasonably strong performance, that he would not seek a second term. “Multiple Dems run in the primary and one of two things happen,” Wolfson wrote in an email, sketching out how the race might have unfolded. “Harris is tested, spends time competing for blue-collar votes in the blue wall states, gets better as a candidate, and wins. Or she doesn’t, and we get a better candidate instead.” A Biden backlash The consideration of what might have been is feeding a Biden backlash. The president resisted pressure from Democrats to step aside, and by the time he finally did, the nominating convention was less than a month away. The party had little choice but to rally around Harris. “I thought back in May of 2023 he should get out,” James Carville, the Democratic political consultant, told me. “The stuff she had to deal with in the general she would have dealt with in the primary. She would have answered that question — how will you be different from Biden — 100 times, at 100 debates, at 100 town halls, and 100 barbecues.” “We didn’t even let the public kick the tires,” he said. “We just gave them the car.” Harris unified the party behind her and staged a successful convention that ignited the early part of her candidacy. She succeeded, as one Democratic analyst put it, in assembling a sprawling presidential campaign airplane while flying at 600 miles an hour. But she never built a durable lead against the now president-elect. She not only lost, but she also fared worse than Biden, apparently losing the popular vote and all seven battleground states. In this post-election period, this question of whether she might have been a stronger candidate had Biden stepped aside earlier seems likely to be debated by Democrats for some time to come. The benefits of a marathon A long primary process can be grueling and expensive. It can force candidates to take positions they might come to regret as they appeal to the most active part of the party base — as Harris herself did when she ran for president in the crowded Democratic primary of 2019. But a primary campaign is boot camp for candidates, a chance for them to learn their vulnerabilities, to test and refine how to answer questions about complicated issues or their political history. (Plenty of time to craft answers to questions ranging from how she would be different from Biden to why she changed her position on issues like fracking.) Updated Nov. 7, 2024, 7:45 p.m. ETNov. 7, 2024 “I benefited enormously from long campaigns,” Tom Daschle, the South Dakota Democrat and former majority leader of the Senate, told me on Wednesday. “You can better define yourself. Three months is an impossible task and I think she did a phenomenal job putting that campaign together. But time is a real asset.” (Asked whether Harris would have been better off if Biden had bowed out earlier, Daschle responded: “We can second-guess that forever.” ) David Axelrod, the consultant, said he observed this firsthand in Iowa while serving as senior political adviser to Barack Obama, when Obama was just a first-term senator from Illinois. “Barack Obama, who turned out to be a stellar candidate, wasn’t a stellar candidate at the beginning of 2007,” Axelrod said. “He really benefited from the process of having time to grow and try things out and get accustomed to the freneticism of a national campaign.” “The thing about primaries, campaigns, long campaigns, is they give you a chance to get into your groove,” he said. Would that have made a difference for Harris? Before the election, this was an open question in Democratic circles. Now, well, not so much. A challenge from the start Matt Bennett, a co-founder of the center-left group Third Way, and a veteran of Democratic primary campaigns, said the short campaign did come with some benefits for Harris. “She benefited from being spared the pressure to move to the left,” he said. “But she clearly suffered from not having time to do everything in three months: reintroduce herself to the public, gracefully put daylight between herself and Biden, explain her 2020 positions, prosecute the case against Trump, offer new ideas of her own.” Harris faced other obstacles that would daunt any candidate, including an electorate convinced the country was heading in the wrong direction and eager for change. Plus, it is always complicated trying to run as a vice president, especially following in the footsteps of an unpopular president. And there is no guarantee a longer campaign would have changed anything. “Based on her run in 2019 and early 2020, not sure she had the depth to go the distance in anything but an anointed nomination,” said Scott Reed, the manager of the 2012 presidential campaign of Mitt Romney, a Republican. From that perspective, there is another way of looking at the trajectory of a campaign that took off like a rocket before returning to earth. Perhaps the real problem for Harris was not that this campaign was too short. The problem may have been that it was not short enough. The red wave reached even Guam Republicans enjoyed smashing success on Election Day. They won the White House and the Senate, and they are increasingly optimistic they will retain control of the House. And they made down-ballot gains, too. My colleague David Chen has this lay of the land. Republicans made strong gains in local races around the country, and they look likely to break up Democratic control of state governments in Minnesota and Michigan. They also saw unexpected gains in Democratic strongholds like Vermont. The night even proved sweet for Republicans west of the international date line. For the first time since 2006, Republicans won a majority of legislative seats in the U.S. territory of Guam. What had been a 9-6 advantage for Democrats after the 2022 election was reversed on Tuesday, when Republicans claimed nine seats to represent the island of 154,000 people, which is 4,000 miles west of Hawaii. Guamanians can’t vote for president, but the territory held a straw poll on Tuesday. Biden beat Trump by 13 points in that poll in 2020, but Harris won by only 3 percentage points. Even though abortion access has been a concern for residents since the overturning of Roe v. Wade, Republicans there attributed their gains to the same issues that worked for them on the mainland: the economy, health care and public safety . If the current results hold, Republicans would control 57 chambers in states around the country, Democrats would control 38 — down from 41 — and three chambers would be tied, Ben Williams, associate director of the elections and redistricting program at the National Conference of State Legislatures, said during a conference call on Thursday analyzing election results. — David W. Chen How Elon Musk Rebranded Trump Author: Chayka, Kyle, Date: 2024-11-13 11/13/24 Collections: Hot Takes US Elect 2024 Zotero Key: WZGH5XXU Cite Key: Hayka24muskRebrandTrump Zotero Item | Lit Note By the end of Donald Trump’s campaign for a second Presidency, he was part of a package deal. The Trump ticket represented not only Trump and his running mate, J. D. Vance, or not just them, but also the suddenly inseparable duo of Trump and the tech billionaire Elon Musk. Musk, the world’s richest man, was once a self-described moderate and an Obama supporter, but since the pandemic (during which his businesses tried to skirt quarantine orders) he has undergone an ostentatious rightward shift. In the months before Election Day, having previously had little public relationship with Trump, he began bankrolling Trump’s campaign to the tune of around two hundred million dollars. His American super PAC ran much of Trump’s ground game in swing states. Musk spoke onstage repeatedly at Trump’s rallies, resulting in one much memed photo of the entrepreneur leaping into the air with his arms ecstatically outstretched. Musk spent Election Night at Mar-a-Lago, within murmuring distance of the soon-to-be President-elect. Their new closeness has only intensified since Trump’s win. Musk has already come to occupy a para-governmental position in American society, because his multiple technology companies have increasingly wormed their way into international affairs. The Pentagon has paid for his Starlink satellites to provide Internet in war-torn Ukraine. NASA hires his SpaceX crafts for missions. Cities including Las Vegas have contracted his Boring Company to dig infrastructure projects. Now, rather than holding disconcerting sway over government endeavors from the outside, Musk is shaping the government from within. When Trump took a congratulatory phone call with the Ukrainian President, Volodymyr Zelensky, Trump reportedly put Musk on the line. In October, Trump said that he was considering Musk for a role as “Secretary of Cost Cutting”; on Tuesday, Trump officially announced that Musk and the right-wing entrepreneur Vivek Ramaswamy would jointly lead a new Department of Government Efficiency (a.k.a. D.O.G.E, surely the first time that a government agency has been named for a meme). Musk’s bio on X, the social-media platform formerly known as Twitter, which he bought in 2022 and has since transformed into his bully pulpit, now reads “The people voted for major government reform.” According to the Times, Musk in the past week has been constantly at Trump’s side at Mar-a-Lago, advising on cabinet appointments, bringing tech investors into the fold, and being greeted in the resort’s dining room with the same reverence as Trump. Kai Trump, one of the President-elect’s grandchildren, posted online that Musk was “achieving uncle status.” Musk seems to have embraced the deceptively quaint term “first buddy.” The Times called him “indisputably America’s most powerful private citizen.” Daily Our flagship newsletter highlights the best of The New Yorker, including top stories, fiction, humor, and podcasts. For fans of Musk’s techno-accelerationist vision, the Trumpian alliance amounts to the dawning of a bold new political era. Rachael Horwitz, the chief marketing officer of the San Francisco venture-capital firm Haun Ventures, has observed Trump’s newly tech-heavy fan base in her roles at the V.C. firm Andreessen Horowitz and the cryptocurrency exchange Coinbase. She told me that she understands the appeal of a Trump-Musk partnership. “Elon has given a new shape to how some people view not just Trump but maybe even conservatism,” she said, adding, “I think there is optimism at this moment. There’s hope.” Even before Musk’s involvement, there were signals that the Trump Administration would throw in its lot with Silicon Valley. Vance is a creature of online inclinations—including posting about digital dolphin porn and spreading his disproved claims about immigrants eating pets—and a onetime protégé of the tech magnate Peter Thiel. Whereas Kamala Harris was subtle in her courting of the crypto crowd, Trump explicitly said that he would support blockchain technology. (Since the election, Bitcoin has reached all-time highs.) Later in the campaign, Trump’s embrace of hit podcasters such as Joe Rogan, Theo Von, and Lex Fridman helped spread his message on digital channels among the young male demographic that proved key to his victory. In the company of Musk—redolent of angular Cybertrucks, reusable rockets, and humanity living on Mars—Trump appears not as an elderly, increasingly incoherent, legally embattled former President who failed to get reëlected once but as the leader of an energized, risktaking, unorthodox, tech-forward regime. To those who haven’t delved into Musk’s bigoted anxieties about population replacement or transgender rights, he might seem like a kind of real-life superhero—like Tony Stark of “Iron Man” joining the Administration. He has achieved miraculous things in the realms of electric vehicles and space travel; perhaps he can do so for the government as well. One parody account on X suggested that Musk could build a “bulletproof golf cart” for Trump. Yet there’s a gulf between the promise of innovation that Musk represents and the reality of how he operates as a businessman. Musk’s companies, like Trump’s, are kept afloat less by their ground-level progress than by the cult of personality around their founder. Tesla reportedly laid off more than fourteen per cent of its workforce in 2024; the team in charge of one of its most prominent infrastructure products, the Tesla Supercharger network, was eliminated. Once Musk took possession of X, he quickly laid off the vast majority of the staff, including most of the workers responsible for content moderation. (Cost cutting, indeed.) At the same time, Musk has succeeded at making X the communication platform of choice for those on the right, in part by remaking the site in his own image. X happens to be cratering financially—Fidelity, one of its stakeholders, recently estimated its value at around nine billion dollars, a far cry from the forty-four billion dollars Musk paid for it—yet it is poised to become a fulcrum of the second Trump Administration. For one thing, as Eric Newcomer, the proprietor of a Silicon Valley venture-capital newsletter, told me, “Tech is still addicted to X.” This election has been called the “podcast election,” for the influence of Rogan and his ilk, but it should perhaps be thought of more specifically as the digital-multimedia election, a race waged via podcasts on YouTube, talking heads on TikTok, and audio live streams on X (where Trump appeared with Musk in August). Whereas platforms such as YouTube and TikTok take pains to appear neutral, X under Musk has become a partisan, ideological arena. Despite his professed obsession with free speech and his protestations that X is a “public square,” the platform is now more weighted toward Republicans than Democrats. New users are recommended politicized accounts to follow, and Musk’s own torrent of posts is unavoidable, including plenty of boosterish Donald Trump memes. Noting how Facebook was blamed for misinformation in elections past, including in Trump’s victory in 2016, Newcomer told me, “X is so much more overt.” It is as if Rupert Murdoch’s Fox News was not only brazenly biased toward Trump’s campaign but was also publicly strategizing with it. Trump, who relentlessly threatens and smears the mainstream media, created his own social-media platform, Truth Social. It failed to achieve much traction, but no matter: X is the new hub for his true believers. The day after the election, Musk posted, “You are the media now.” For Musk, as for Trump, the stakes of Trump’s victory are legally and financially existential. Nearly all of Musk’s companies face proliferating lawsuits alleging everything from labor violations to illicit stock sales and gender discrimination. (A suit over a death linked to self-driving technology was settled earlier this year.) A friendlier government might delay or perhaps eliminate those threats to Musk’s empire, not to mention double down on the government contracts that Musk has secured for his businesses. Time will tell how influential Musk will be in a second Trump term. In the chaotic, infighting-filled first Trump Administration, plenty of figures who seemed to have Trump’s ear one day were estranged the next, including Peter Thiel, who was part of Trump’s 2016 transition team. Still, it’s hard to overstate the success that a Trump-Musk axis has already had in rebranding Trump’s image. As if to reaffirm a new closeness between the executive branch and Silicon Valley, a parade of other tech billionaires came forward online in the wake of Trump’s victory to kiss the ring. On Threads, Mark Zuckerberg posted, “We have great opportunities ahead of us as a country.” Jeff Bezos, having quashed a Kamala Harris endorsement at the Washington Post, now spoke up to congratulate Trump on his “extraordinary political comeback.” Sam Altman, the C.E.O. of OpenAI, posted on X, “i wish for his huge success in the job.” These tech barons are presumably hoping for Trump to aggressively pursue deregulation, including by reversing Biden’s antitrust efforts, making way for the mergers and the monopoly consolidation that tech giants continue to thrive on. Silicon Valley has long believed that its companies can achieve more than the state; now may be their chance to experiment, with backing from the state itself. As Musk put it on X, “America is a nation of builders / Soon, you will be free to build.” The Democrats Need an Honest Conversation on Gender Identity Author: Lewis, Helen, Date: 2024-11-10 Collections: Hot Takes US Elect 2024, IdentityPolitics Zotero Key: 7CWTE838 Cite Key: Lewis24demsGenderID Zotero Item | Lit Note One of the mysteries of this election is how the Democrats approached polling day with a set of policies on gender identity that they were neither proud to champion—nor prepared to disown. Although most Americans agree that transgender people should not face discrimination in housing and employment, there is nowhere near the same level of support for allowing transgender women to compete in women’s sports—which is why Donald Trump kept bringing up the issue. His campaign also barraged swing-state voters and sports fans with ads reminding them that Kamala Harris had previously supported taxpayer-funded gender-reassignment surgery for prisoners. The commercials were effective: The New York Times reported that Future Forward, a pro-Harris super PAC, found that one ad “shifted the race 2.7 percentage points in Mr. Trump’s favor after viewers watched it.” The Harris campaign mostly avoided the subject. Since the election, reports of dissent from this strategy have begun to trickle out. Bill Clinton reportedly raised the alarm about letting the attacks go unanswered, but was ignored. After Harris’s loss, Representative Seth Moulton of Massachusetts went on the record with his concerns. “I have two little girls, I don’t want them getting run over on a playing field by a male or formerly male athlete, but as a Democrat I’m supposed to be afraid to say that,” he told the Times. The recriminations go as far as the White House, where allies of Joe Biden told my colleague Franklin Foer that the current president would have countered Trump’s ads more aggressively, and “clearly rejected the idea of trans women competing in women’s sports.” One problem: Biden’s administration has long pushed the new orthodoxy on gender, without ever really explaining to the American people why it matters—or, more crucially, what it actually involves. His officials have advocated for removing lower age limits for gender surgeries for minors, and in January 2022, his nominee for the Supreme Court, Ketanji Brown Jackson, refused to define the word woman, telling Senator Marsha Blackburn of Tennessee, “I’m not a biologist.” Thomas Chatterton Williams: What the left keeps getting wrong On sports—an issue seized on by the Trump campaign—Biden’s White House has consistently prioritized gender identity over sex. Last year, the Department of Education proposed regulations establishing “that policies violate Title IX when they categorically ban transgender students from participating on sports teams consistent with their gender identity just because of who they are.” Schools were, however, allowed to limit participation in specific situations. (In April, with the election looming, this part of the Title IX revision was put on hold.) Harris went into the campaign tied to the Biden administration’s positions, and did not have the courage, or strategic sense, to reject them publicly. Nor did she defend them. The fundamental issue is that athletes who have gone through male puberty are typically stronger and faster than biological females. Rather than contend with that fact, many on the left have retreated to a comfort zone of claiming that opposition to trans women in women’s sports is driven principally by transphobia. But it isn’t: When trans men or nonbinary people who were born female have competed in women’s sports against other biological females, no one has objected. The same season that Lia Thomas, a trans woman, caused controversy by swimming in the women’s division, a trans man named Iszac Henig did so without any protests. (He was not taking testosterone and so did not have an unfair advantage.) Yet even talking about this issue in language that regular Americans can understand is difficult: On CNN Friday, when the conservative political strategist Shermichael Singleton said that “there are a lot of families out there who don’t believe that boys should play girls’ sports,” he was immediately shouted down by another panelist, Jay Michaelson, who said that the word boy was a “slur,” and he “was not going to listen to transphobia at this table.” The moderator, Abby Phillips, also rebuked Singleton, telling him to “talk about this in a way that is respectful.” A few Democrats, such as Colin Allred, a Senate candidate in Texas, attempted to counter Republicans’ ads by forcefully supporting women’s right to compete in single-sex sports—and not only lost their races anyway, but were attacked from the left for doing so. In states such as Texas and Missouri, the political right is surveilling and threatening to prosecute parents whose children seek medical treatments for gender dysphoria, or restricting transgender adults’ access to Medicaid. In this climate, activists believe, the Democrats should not further jeopardize the rights of a vulnerable minority by legitimizing voters’ concerns. “Please do not blame trans issues or trans people for why we lost,” Sam Alleman, the Harris campaign’s LBGTQ-engagement director, wrote on X. “Trans folks have been and are going to be a primary target of Project 2025 and need us to have their backs now more than ever.” During the race, many journalists wrote about the ubiquity—and the grimness—of the Trump ads on trans issues, notably Semafor’s David Weigel. But at the time, I was surprised how dismissive many commentators were about their potential effect, given the enormous sums of money involved. My theory was that these ads tapped into a larger concern about Democrats: that they were elitists who ruled by fiat, declined to defend their unpopular positions, and treated skeptics as bigots. Gender might not have been high on voters’ list of concerns, but immigration and the border were—and all the same criticisms of Democratic messaging apply to those subjects, too. Not wishing to engage in a losing issue, Harris eventually noted blandly that the Democrats were following the law on providing medical care to inmates, as Trump had done during his own time in office. On the integrity of women’s sports, she said nothing. Read: Why Biden’s team thinks Harris lost How did we get here? At the end of Barack Obama’s second term, gay marriage was extended to all 50 states, an achievement for which LGBTQ groups had spent decades campaigning. In 2020, the Supreme Court’s decision in Bostock v. Clayton County found that, in the words of conservative Justice Neil Gorsuch, “an employer who fires an individual merely for being gay or transgender defies the law.” Those advances meant that activist organizations, with large staffs and existing donor networks, had to go looking for the next big progressive cause. Since Trump came to power, they have stayed relevant and well funded by taking maximalist positions on gender—partly in reaction to divisive red-state laws, such as complete bans on gender medicine for minors. The ACLU, GLAAD, the Human Rights Campaign, and other similar groups have done so safe in the knowledge that they answer to their (mostly wealthy, well-educated) donors, rather than a more diverse and skeptical electorate. “The fundamental lesson I hope Dem politicians take from this election is that they should not adopt positions unless they can defend them, honestly, in a one-on-one conversation with the median American voter, who is a white, non-college 50-yr-old living in a small-city suburb,” the author (and Atlantic contributing writer) James Surowiecki argued last week on X. Even now, though, many Democrats are reluctant to discuss the party’s positions on trans issues. The day after Moulton made his comments, his campaign manager resigned in protest, and the Massachusetts state-party chair weighed in to say that they “do not represent the broad view of our party.” But Moulton did not back down, saying in a statement that although he had been accused of failing “the unspoken Democratic Party purity test,” he was committed to defending the rights of all Americans. “We did not lose the 2024 election because of any trans person or issue. We lost, in part, because we shame and belittle too many opinions held by too many voters and that needs to stop.” Gilberto Hinojosa, the chair of the Texas Democrats, faced a similar backlash. He initially told reporters, “There’s certain things that we just go too far on, that a big bulk of our population does not support,” but he quickly walked back the comments. “I extend my sincerest apologies to those I hurt with my comments today,” Hinojosa said. “In frustration over the GOP’s lies to incite hate for trans communities, I failed to communicate my thoughts with care and clarity.” (On Friday, he resigned, citing the party’s “devastating” election results in the state.) The tragedy of this subject is that compromise positions are available that would please most voters, and would stop a wider backlash against gender nonconformity that manifests as punitive laws in red states. America is a more open-minded country than its toughest critics believe—the latest research shows that about as many people believe that society has not gone far enough in accepting trans people as think that it has gone too far. Delaware has just elected the first transgender member of Congress, Sarah McBride. But most voters think that biological sex is real, and that it matters in law and policy. Instructing them to believe otherwise, and not to ask any questions, is a doomed strategy. By shedding their most extreme positions, the Democrats will be better placed to defend transgender Americans who want to live their lives in peace.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources.backup/Lall21StableFeatureSelection.pdf","text":"Stable Feature Selection using Copula based Mutual Information Journal Pre-proof Stable Feature Selection using Copula based Mutual Information Snehalika Lall, Debajyoti Sinha, Abhik Ghosh, Debarka Sengupta, Sanghamitra Bandyopadhyay PII: S0031-3203(20)30500-8 DOI: https://doi.org/10.1016/j.patcog.2020.107697 Reference: PR 107697 To appear in: Pattern Recognition Received date: 5 February 2020 Revised date: 3 October 2020 Accepted date: 7 October 2020 Please cite this article as: Snehalika Lall, Debajyoti Sinha, Abhik Ghosh, Debarka Sengupta, Sanghamitra Bandyopadhyay, Stable Feature Selection using Copula based Mutual Information, Pat- tern Recognition (2020), doi: https://doi.org/10.1016/j.patcog.2020.107697 This is a PDF ﬁle of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the deﬁnitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its ﬁnal form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. © 2020 Published by Elsevier Ltd. Highlights • A novel method for feature selection using incorporation of copula based multivariate dependency in mutual information, which assists to remove the need to average out over multiple instances of bivariate dependencies. • The method is unbiased against noisy datasets due to the scale-invariant property of copula. • The method can be applied to datasets, in which the ratio between sample size to class size is large enough, even though original marginal distribu- tions are unknown. • The method also satisﬁes the maximum relevance and minimum redun- dancy criteria of feature selection. 1 Stable Feature Selection using Copula based Mutual Information Snehalika Lall a, Debajyoti Sinhab, Abhik Ghosh c,d,∗, Debarka Sengupta e,f,g,h,i,∗, Sanghamitra Bandyopadhyaya,∗ aMachine Intelligence Unit, Indian Statistical Institute, Kolkata, West Bengal 700108, India. bSyMeC Data Center, Indian Statistical Institute, Kolkata, West Bengal 700108, India; Department of Computer Science and Engineering, University of Calcutta, Kolkata, West Bengal 700098, India. cInterdisciplinary Statistical Research Unit, Indian Statistical Institute, Kolkata, West Bengal 700108, India dCentre for Artiﬁcial Intelligence and Machine Learning, Indian Statistical Institute, Kolkata, West Bengal 700108, India eCenter for Computational Biology, Indraprastha Institute of Information Technology,Okhla,Phase-III, New Delhi-110020,India fDepartment of Computer Science and Engineering, Indraprastha Institute of Information Technology, Okhla,Phase-III, New Delhi-110020,India. gCenter for Artiﬁcial Intelligence, Indraprastha Institute of Information Technology,Okhla,Phase-III, New Delhi-110020,India hCentre de Recherche en Transplantation at Immunologie, Inserm, CHU Nantes, Universite de Nantes, UMR1064, ITUN, Nantes, France. iInstitute of Health and Biomedicine Innovation, Queensland University of Technology, Australia. Abstract Feature selection is a key step in many machine learning tasks. A majority of the existing methods of feature selection address the problem by devising some scoring function while treating the features independently, thereby overlooking their interdependencies. We leverage the scale invariance property of copula to construct a greedy, supervised feature selection algorithm that maximizes the feature relevance while minimizing the redundant information content. Multi- variate copula is used in the proposed copula Based Feature Selection (CBFS) to discover the dependence structure between features. The incorporation of copula-based multivariate dependency in the formulation of mutual information ∗Please address correspondence to A.Ghosh, D.Sengupta, and S.Bandyopadhyay. Email addresses: abhik.ghosh@isical.ac.in (Abhik Ghosh ), debarka@iiitd.ac.in (Debarka Sengupta ), sanghami@isical.ac.in (Sanghamitra Bandyopadhyay ) Preprint submitted to Journal of LATEX Templates October 25, 2020 helps avoid averaging over multiple instances of bivariate dependencies, thus eliminating the average estimation error introduced when bivariate dependency is used between a pair of feature variables. Under a controlled setting, our algorithm outperformed the existing best practice methods in warding oﬀ the noise in data. On several real and synthetic datasets, the proposed algorithm performed competitively in maximizing classiﬁcation accuracy. CBFS also out- performs the other methods in terms of its noise tolerance property. Keywords: Copula, Feature selection, Mutual information, Stability, Classiﬁcation accuracy. 1. Introduction1 With the advancement of science and technology, data has increased both in2 volume and dimensionality [1]. While the amount impacts the machine learn-3 ing model additively, dimensions give rise to confusion. Examples of high-4 dimensional data include genomic data [2], text data [3], image retrieval [4],5 bioinformatics [5], etc. Over the last ﬁve decades, feature engineering has6 emerged as a necessary ingredient of machine learning and a ﬁeld of study on7 it [6]. Feature selection [7] helps in two ways - it reduces the computational8 cost [8] and helps avoid model overﬁtting. Feature selection, in practice, often9 improves the accuracy of down-stream machine learning problems, including10 clustering and classiﬁcation. In the article [9], the authors proposed a new fea-11 ture selection algorithm based on dynamic mutual information, which is only12 estimated on unlabeled instances. According to [10], the author introduced a lo-13 cal discriminant model in the feature selection framework of subspace learning.14 The model preserves both the local discriminant structure and local geometric15 structure of the data. In [11], a stratiﬁed sampling method was employed to16 select the feature subset for random forests with high dimensional data. Fea-17 tures are separated into two groups. One group will contain strong informative18 features and the other weak informative features. Then, randomly features are19 chosen from each group proportionally. According to, [12] the authors proposed20 3 a space division strategy based on the feature importance, which can determine21 relevant features into the same subspace with a low computational cost. In [13],22 an information-theoretic approach was proposed to extract the hidden common23 structure shared by a set of random variables.24 Feature selection, in the context of supervisory learning, aims to identify25 an optimal feature subset such that the model accuracy is maximized. Finding26 the exact solution amounts to an NP-hard problem [14]. Approximate methods27 [15] are devised in the intersection of mathematics, statistics, and algorithms to28 circumvent this problem.29 Supervisory feature selection methods are of three broad types - ﬁlters [16],30 wrappers model [17] and embedded techniques[18]. Filters typically measure the31 association between explanatory variables and the dependent variable. Some32 of these association measures are - Pearson’s correlation coeﬃcient [19], Chi-33 squared test [20], mutual information [21], etc. Although ﬁlters are straight-34 forward and fast, they often fail to explore possibilities around feature combi-35 nations. As a result, the best accuracies are not attained using ﬁlters. Wrap-36 pers overcome this problem by employing feature subset search with the goal37 of performance maximization. A widely used wrapper approach is - genetic38 algorithms based feature selection approaches [22], where a hybrid genetic algo-39 rithm is adopted to ﬁnd a subset of features most relevant to the classiﬁcation40 task. Embedded methods are slightly diﬀerent from wrappers. Here, feature41 selection goes hand in hand with model-learning. In other words, unimportant42 features are lowly weighted or penalized while maximizing the performance of43 the associated supervisory model. The embedded feature selection [23] method44 takes the advantages of both wrapper and ﬁlter based approach. Zhang and45 colleagues proposed the horseshoe regularization penalty for feature subset se-46 lection, demonstrating its theoretical and computational advantages [24]. It is47 also an embedded feature selection approach.48 Of late, mutual information-based ﬁlter methods have gained popularity due49 to their ability to capture the non-linear association between dependent and in-50 dependent variables in a machine learning setting. Mutual information-based51 4 Feature Selection (MIFS) is among the earliest algorithms in this segment [25].52 It is a greedy algorithm that considers both mutual information of a candidate53 feature with class label information and the prior selected features. Conditional54 Mutual information maximization (CMIM), along with the same line, maxi-55 mizes mutual information concerning the class while conditioning upon the se-56 lected features [26]. One of the popular mutual information-based approaches57 is the Minimal-Redundancy-Maximal-Relevance criterion (MRMR) [27], which58 considers feature relevance concerning class labels and ensures that redundant59 features are not present in the ﬁnal feature subset. The existing algorithms60 evaluate bivariate feature dependencies. Moreover, the current algorithms are61 susceptible to transformations, such as scaling.62 To address the above problems, in this article, we propose Copula Based63 Feature Selection (CBFS), a ﬁlter based forward sequential search technique64 for feature selection. CBFS works by minimizing the copula mutual informa-65 tion among selected features and maximizing the same between a candidate66 feature and class. Multivariate feature dependency and scale-invariant property67 of CBFS yielded competitive results while tested on multiple real and synthetic68 Gaussian mixture datasets.69 2. Background70 In this section, we illustrate some basic preliminaries about copulas and71 mutual information.72 2.1. Copula Theory73 The name Copula comes from a Latin word copulare, which means joint74 together. Copulare word is ﬁrst used by famous statistician Sklar in 1959 in75 one of his famous Sklar’s Theorem. Copula produces a multivariate probability76 distribution from multiple uniform marginal distributions. Copula [28] is also77 extensively used in high dimensional data applications to obtain joint distri-78 butions from a random vector, easily by estimating their marginal functions.79 5 Mathematically, Copula is deﬁned as follows:80 Deﬁnition 1: Copula is an n dimensional function, C ∶ [0, 1]n → [0, 1], which81 satisﬁes the following properties:82 1. C(u1, \u0005, ui−1, 0, ui+1, \u0005, un) = 0, i.e., the copula is 0 if one of any variable83 is 0.84 2. C(1, \u0005, 1, u, 1, \u0005, 1) = u, i.e., the copula function is just u if one of the85 variables is u with all others being 1.86 3. C is an n-increasing function.87 Copula in Probability: Let, (X1, X2, \u0005, Xn) be the random vectors whose88 marginal distributions (U1, U2, \u0005, Un) are uniformly distributed in [0, 1]. A89 copula function C ∶ [0, 1] n → [0, 1] is deﬁned as the joint distribution:90 C(u1, u2, \u0005, un) = P (U1 ≤ u1, U2 ≤ u2, . . . , Un ≤ un). (1) It is not diﬃcult to verify that this copula satisﬁes all three properties mentioned91 in Deﬁnition 1. Sklar’s following theorem extends this deﬁnition to more gen-92 eral random variables with possibly non-uniform marginals, which is presented93 below.94 Sklar’s Theorem: Let (X1, X2, \u0005, Xn) be the random vectors whose marginals95 are96 F1(x1), F2(x2), \u0005, Fn(xn). So, for any joint distribution F , their exists a copula97 function C of its univariate marginal distributions such that,98 F (x1, x2, \u0005, xn) = C(F1(x1), F2(x2), \u0005, Fn(xn)). (2) The converse of the theorem is also true. Any copula function with individual99 marginal functions Fi(xi) as the arguments, represents valid joint distribution100 function with marginals Fi(xi). So, Copula is also known as joint distribution101 generating function with a separate choice of marginals.102 Assuming F (X1, X2, \u0005, Xn) has nth order partial derivatives, relation between103 the joint probability density function and the copula density function, say c,104 6 can be obtained from Equation (2) as,105 f (x1, x2, \u0005, xn) = ∂n(F (X1, X2, \u0005, Xn)) ∂X1∂X2\u0005∂Xn = ∂n(C(F1(x1), F2(x2), \u0005, Fn(xn)) ∂X1∂X2\u0005∂Xn = c(F1(x1), F2(x2), \u0005, Fn(xn)) M i fi(xi) (3) where, we deﬁne,106 c(t1, . . . , tn) = ∂nC(t1, . . . , tn) ∂t1\u0005∂tn . (4) There are many examples of copula [28]. One of the leading copula families,107 used in our experiment is Empirical copula, which is deﬁned as follows.108 Empirical Copula. Let X1, X2, \u0005, Xn be the random variables with marginals109 cumulative distribution function F1(x1), F2(x2), \u0005, Fn(xn), respectively.110 The empirical estimate of (Fi, i = 1, \u0005, n), based on a sample, {xi1, xi2, \u0005, xim}111 of size m is given by112 ̂Fi(x) = 1 m m Q j=1 1{Xij ≤x}, [i = 1, \u0005, n] (5) The Empirical Copula of X1, X2, \u0005, Xn is then deﬁned as113 ̂C(u1, u2, \u0005, un) = 1 m m Q j=1 1{ÃF1(x1,j) ≤ u1, ÃF2(x2,j) ≤ u2, \u0005, ÃFn(xn,j) ≤ un}, (6) for ui ∈ [0, 1], [i = 1, \u0005, n].114 2.2. Information Theory115 Here, we discuss some basic parts of information theory based on entropy116 and Mutual Information. The entropy is deﬁned as a measure of uncertainty117 and average information in a random variable [29]. The entropy of discrete118 random variable, X = (x1, x2, \u0005, xn) is deﬁned as:119 H(X) = − n Q i=1 p(xi) log2 p(xi) (7) where, p(xi) is the probability mass function, deﬁned as:120 p(xi) = Number of events occur with, xi Total number of events, n (8) 7 The conditional entropy of a random variable X given another random variable121 Y = (y1, y2, \u0005, yn) is deﬁned as:122 H(XSY ) = − m Q j=1 n Q i=1 p(xi, yj) log2 p(xi, yj) p(yj) (9) The joint entropy between two discrete random variables X and Y is deﬁned123 as:124 H(X, Y ) = − m Q j=1 n Q i=1 p(xi, yj) log2 p(xi, yj) (10) Mutual Information is a mutual dependence measure between two random125 variables. For any two continuous random variable X and Y , the mutual in-126 formation is given in form of their probability distribution functions p(x), p(y)127 and p(x, y) as:128 129 I(X, Y ) = U p(x, y) log p(x, y) p(x)p(y) dx dy (11) For any two discrete random variables x and Y , mutual information is then130 given by131 132 I(X, Y ) = Q x,y p(x, y) log p(x, y) p(x)p(y) = Q x,y p(x, y) log p(xSy) p(x) = H(X) − H(XSY ) (12) 2.3. Relation of Copula with Mutual Information133 From Equations (3), (9) and (10) the mutual information between two ran-134 dom variables X and Y can be described in terms of the copula function as135 8 136 I(X, Y ) = U p(x, y) log p(x, y) p(x)p(y) dx dy = U c(P (x), P (y))p(x)p(y) log c(P (x), P (y))p(x)p(y) p(x)p(y) dx dy = U c(u, v) log c(u, v) du dv = −H(C(u, v)) (13) where, u and v are the individual marginal distributions, respectively, P (x) and137 P (y). So, It can be seen that the Mutual Informations of the random variables138 are similar as negative entropy of their corresponding Copula distributions.139 2.4. Limitations of the previous works140 Earlier feature selection methods are mainly concern on the optimization of141 two criteria: maximizing relevance and minimizing redundancy. The limitations142 of these methods are the following:143 • MIFS is not eﬃcient for a large number of features. If the magnitude of the144 redundancy term increases, some irrelevant features may be selected. This145 limitation is removed in MRMR and NMIFS by dividing the redundancy146 term with the total number of subsets.147 • Most of the existing methods employ the cumulative summation and for-148 ward search to approximate the solution which causes overestimation of149 signiﬁcance (MIFS, CMIM, and MRMR). The overestimation of signiﬁ-150 cance can occur when the candidate features have a high correlation with151 pre-selected features. Still, at the same time, these are almost indepen-152 dent of the remaining subsets. This situation causes high redundancy for153 the candidate feature sets.154 • Existing feature selection works are dataset dependent [30]. Changes in155 dataset results diﬀerent selected features (MIFS, CMIM, and MRMR).156 9 Signiﬁcance of all the discussed methods depends on the characteristics of157 datasets.158 3. Proposed method for Feature Selection159 The proposed method for feature selection addresses the issue of overestima-160 tion of some feature due to the cumulative summation of Mutual Information.161 Let, a dataset be D with N dimensions. The total feature space is F =162 {f1, f2, \u0005, fN }. We want to select a sub set of features with n dimensions,163 where n << N . The feature subset will be S = {f1, f2, \u0005, fn}. We aim to select a164 feature subset S, which have same or better classiﬁer accuracy than the feature165 set F .166 After introducing the proposed method (CBFS), in this section, we will also167 show that our CBFS method is equivalent to max dependency search method168 [31]. We also compute the optimal bound of our method. Let us start with169 deﬁning the related optimality criteria.170 Max Dependency: The mutual information-based feature selection method171 which selects the feature set having the largest dependency with the class vari-172 able, is known as Maximal Dependency(MD) method [31]. This can be mathe-173 matically expressed as:174 ̂Sselected = arg max S I(S, Y ) = arg max fs∈F − ˙S I(fs, Y S ˙S) (14) Here, S = ( ˙S, fs) is a subset of features, where fs is the last feature in selected set175 S and Y is class level. Often direct implementation of the ﬁrst minimization in176 Equation 13 is infeasible when the second optimization implemented it, known177 as the ﬁrst order incremental search. The method describes that one feature178 is selected at each iteration, and that feature will be optimum, i.e., maximum179 dependent with the class variable.180 Relevancy: A feature fi is more relevant to class level Y than another181 feature fj, if fi has higher mutual information with the class label Y than fj.182 It is the Relevancy test for the features[31]. Mathematically fi is more relevant183 10 than fj if184 Ic(fi, Y ) > Ic(fj, Y ), (15) Where Ic(x, y) is a mutual information measure, which we will use to be the185 copula mutual information. As an alternative to the Max-dependency method,186 the maximum-relevancy method selects the feature set S as187 ̂Smax−rel = arg max S 1 SSS Q fs∈S Ic(fs, Y ). (16) Minimum Redundancy: It is often the case that the feature set S obtained by188 max-relevancy criterion (16) contains features that are mutually inter-dependent189 to a high extent to be redundant. Redundancy is measured using the minimiza-190 tion of mutual information between all selected features fs and non selected191 feature, say fi. It can be mathematically expressed as:192 mins∶fs∈SIc(fi; fs) (17) 3.1. Copula Based Feature Selection193 As mentioned in [31] there is an overestimation to ﬁnd the redundancy194 (MIFS, CMIM and MRMR).Most of the previous feature selection works, dis-195 cussed in Section I (including the mRMR method of [27] ), were dataset de-196 pendent. Any noise in the dataset may change the selected feature subset. We197 develop a copula-based feature selection (CBFS) method to optimize the rele-198 vancy and redundancy, which is much more stable than the existing methods.199 We minimize the copula mutual information between fi and fs (to reduce the200 redundancy between them) and maximizing the copula mutual information be-201 tween class label Y and fi. So, we are indeed using the ﬁrst order incremental202 search to select one feature in each step but propose to use (empirical) copula-203 based mutual information instead of the standard information measure (as in204 [27] ) to achieve more stability. Further, after selecting more than one feature,205 we use multivariate mutual information in contrast with the average in (17).206 Mathematically, it can be expressed as follows.207 11 After selecting f1, . . . , fs ∈ S, we select the next feature (fs+1 = fCBFS) by208 fCBF S = arg max fi∈(F −S)[Ic(fi; Y ) − Ic(fi; f1; f2; \u0005; fs)] = arg max fi∈(F −S)[−H(C(P (fi), P (Y )) + H(C(P (fi), P (f1), \u0005, P (fs)))] (18) The feature selection method (CBFS) is illustrated in Figure 1. The Venn di-209 agram A denotes the entropy of non selected features, H(fi), B denotes the210 entropy of selected features, H(fs), and C denotes the entropy of target class,211 H(Y ). The ﬁrst diagram represents the copula based relevancy, Ic(fi, Y ). The212 second diagram represents the copula based redundancy, Ic(fi, f1, f2, . . . , fs).213 The third diagram depicts the objective function of the algorithm. The opti-214 mum feature is obtained by maximization of the objective criteria iteratively.215 216 A B C A B C A B CC B Relevancy Redundancy IcI I c(f i,Y) I c (fi ,f1 ,f2 ,...,fs ) Maximize Objective Criteria, fCBFS Figure 1: Pictorial form of the feature selection method (CBFS) 3.1.1. Algorithm for the Copula Based Feature Selection217 CBFS algorithm is described in Algorithm 1. The algorithm is discussed218 below.219 12 1. This algorithm takes data matrix M with class label Y , and the number220 of features to be selected k as the input parameter.221 2. The ﬁrst most relevant feature is selected by maximizing the copula based222 mutual information between class label Y and all features in F .223 3. D contains the copula based multivariate mutual information values be-224 tween each non selected feature (fi) with all selected feature (f1, f2, . . . , fs).225 4. R contains the copula based multivariate mutual information values be-226 tween each non selected feature (fi) with class label Y .227 5. Subtraction of values in D from values in R are kept in E.228 6. A feature is selected with maximum value, obtained from E, and merged229 in selected feature list S iteratively.230 7. The above process (step 3 to step 6) is repeated k times.231 8. Thus, an optimal feature subset is obtained in S.232 9. S is returned as output.233 Algorithm 1 Copula Based Feature Selection Algorithm(CBFS) Input: Data Matrix M, Target class Y, Number of Selected Features, k. Output: Optimal Feature subset,(S). Initialisation: 1: S = g, {S will hold sub-set feature indices.} 2: S[1] ← M axfiIc(fi, Y ) {Initial most relevant feature} 3: for i = 0 to (k − 1) do 4: E = g 5: D ← Ic(fi; f1s; \u0005; fks) {Redundancy Criterion} 6: R ← Ic(fi; Y ) {Relevancy Criterion} 7: E ← (R − D) 8: S ← {S ˚ arg max(limfi {E}) 9: F ← F − {fi} 10: end for 11: return S 13 3.2. Optimality of Copula Based Feature Selection234 In article [27], authors proved that their method (mRMR) with ﬁrst order235 incremental search is optimum in the sense that it becomes equivalent to the236 max dependency criteria. It is proved in this section that Copula Based Feature237 Selection is also optimum, which is equivalent to max dependency criteria (and238 hence also equivalent to the mRMR method).239 240 Theorem 3.1. The proposed method CBFS is equivalent to the Maximal De-241 pendency criteria (14).242 Proof. Since we are also using the ﬁrst order incremental search, we assume243 that without loss of generality, the s feature Ss = {f1, . . . , fs} has already been244 selected optimally and we select the (s + 1)-th feature fs+1 = fCBFS by (18).245 Let us denote Ss+1 = {f1, . . . , fs, fCBFS}. Then the max dependency criteria246 is to maximize I(Ss+1, Y ). So, we need to show that this is equivalent to our247 proposed method (18). From the article [27], it can be shown that the maximiza-248 tion of I(Ss+1, Y ) is equivalent to simultaneous maximization of relevancy and249 minimization of redundancy. So, it is enough to show that our CBFS method250 also satisﬁes the max-relevancy and min-redundancy criteria. We can easily251 show that the minimum redundancy criteria satisfy when the second term of252 Equation (18) has a minimum bound over zero. It can be shown as below:253 Ic(f1; \u0005; fs) = [ p(f1; \u0005; fs) log p(f1; \u0005; fs) p(f1)\u0005p(fs) df1 . . . dfs = [ c(P (f1), \u0005, P (fs)) M i P (fi) log c(P (f1), \u0005, P (fs)) ∏i P (fi) p(f1)\u0005p(fs) df1 . . . dfs = S 1 0 \u0005 S 1 0 c(u1, \u0005, us) log c(u1, \u0005, us) du1 . . . dus ≥ S 1 0 \u0005 S 1 0 (M i ui) log(M i ui) du1 . . . dus = 0 (19) 14 where, ui is the individual uniform marginal distribution functions, P (fi). It254 is observed that when (c(u1, \u0005, us) = ∏i ui)),all the features are independent to255 each other, then mutual information among selected feature is minimized. It is256 the minimum redundancy criteria.257 Now, we estimate the upper bound of the Equation (18), which will satisfy the258 maximum relevance criteria. It can be shown as below:259 Ic(fi; Y ) = U p(fi, Y ) log p(fi, Y ) p(fi)p(Y ) dfi dY = U c(p(fi), p(Y ))p(fi)p(Y ) log c(p(fi), p(Y )) dfi dY = S 1 0 S 1 0 c(ui, v) log c(ui, v) dui dv ≤ min(ui, v) (20) where, ui and v are the individual uniform marginal distribution functions,260 P (fi) and P (Y ). It is veriﬁed from the above results that the maximum of261 Ic(fi; Y ) in Equation (18) is minimum of the feature variables and class label i.e.262 when the feature variable is maximally dependent with the class variable.263 3.3. Stability: The advantage of CBFS264 Most of the existing mutual information based feature selection methods265 are dataset dependent. One of the striking advantages of the proposed method266 is that it overcomes the downside of the primitive mutual information based267 approach due to its scale invariant property [28].268 Proposition 1: Consider, there are two random variables X and Y , and their269 copula function is CXY . If α and β are two functions of X and Y respectively,270 then relation of the Copula of (α(X), β(Y )) and (X, Y ) are as follows.271 • If α and β are strictly increasing functions, then the copula of (α(X), β(Y ))272 can be expressed as:273 Cα(X)β(Y )(u, v) = CXY (u, v) (21) • If α is strictly increasing and β is strictly decreasing , then we have274 Cα(X)β(Y )(u, v) = u − CXY (u, 1 − v) (22) 15 • If α is strictly decreasing and β is strictly increasing function, then we275 have276 Cα(X)β(Y )(u, v) = v − CXY (v, 1 − u) (23) • If α and β both are strictly decreasing function then their copula function277 can be expressed as:278 Cα(X)β(Y )(u, v) = u + v − 1 − CXY (1 − u, 1 − u) (24) These propositions are used to prove that the proposed copula based mutual279 information measure satisﬁed scale invariant property of copula. In case of noisy280 data, it keeps the original subset of features stable in the dataset. Theoretical281 proves of this statement is described here, whereas the simulation result is given282 later in Section 5.283 284 Theorem 3.2. Let, the functions α and β both be strictly increasing according285 to random variables X and y respectively. Copula based mutual information286 Ic(α(X), β(Y )) is same as Ic(X, Y )287 Proof. From Equations (12) and (21), we get the copula mutual information of288 Ic(α(X), β(Y )),between α(X) and β(Y ) to have the form:289 Ic(α(X),β(Y ))(u, v) = −Hc(α(X),β(Y ))(u, v) = − S 1 0 S 1 0 −c(α(X),β(Y ))(u, v) log c(α(X),β(Y ))(u, v)) du dv = S 1 0 S 1 0 c(X,Y )(u, v) log c(X,Y )(u, v) du dv = −Hc(X,Y )(u, v) = Ic(X,Y )(u, v) (25) 290 Thus, copula distribution function of two increasing functions remain same291 with copula function of two random variables.292 16 Theorem 3.3. Consider, the functions α is strictly increasing and β is strictly293 decreasing according to random variables X and y respectively.Copula based mu-294 tual information Ic(α(X), β(Y )) is same as Ic(X, Y )295 Proof. As, α is strictly increasing and β is strictly decreasing function, their296 copula distribution function will be as Equation (22) and hence their copula297 density function is:298 c(α(X),β(Y ))(u, v) = ∂2C(α(X),β(Y ) ∂u∂v = ∂2(u − CXY (u, 1 − v)) ∂u∂v = cXY (u, 1 − v) (26) From Equation (26), copula mutual information Ic(α(X), β(Y )), in this case is299 given by300 Ic(α(X),β(Y ))(u, v) = −Hc(α(X),β(Y ))(u, v) = − S 1 0 S 1 0 −c(α(X),β(Y ))(u, v) log c(α(X),β(Y ))(u, v)) du dv = S 1 0 S 1 0 cX,Y (u, 1 − v) log cX,Y (u, 1 − v) du dv = S 1 0 S 1 0 cX,Y (u, m) log cX,Y (u, m) du dm [By changing variable,(1 − v = m)] = −Hc(X,Y )(u, m) = Ic(X,Y )(u, m) = Ic(X,Y )(u, v) (27) 301 Similarly, if α is strictly decreasing and β is strictly increasing function of302 X and Y respectively, their copula mutual information of Ic(α(X), β(Y )) can303 be shown to satisfy the relation:304 Ic(α(X),β(Y ))(u, v) = Ic(X,Y )(u, v) (28) 17 Theorem 3.4. Consider, the functions α and β both to be strictly decreas-305 ing according to random variables X and y respectively. Copula based mutual306 information Ic(α(X), β(Y )) is same as Ic(X, Y )307 Proof. Now, when both α and β are strictly decreasing functions, their copula308 distribution function will be as in Equation (24) and hence their copula density309 function is given by:310 c(α(X),β(Y ))(u, v) = ∂2C(α(X),β(Y ) ∂u∂v = ∂2(u + v − 1 − CXY (1 − u, 1 − v)) ∂u∂v = cXY (1 − u, 1 − v) (29) From Equation (29), copula mutual information of Ic(α(X)β(Y )), when α and311 β are both strictly decreasing functions of X and Y respectively, is described312 below.313 Ic(α(X),β(Y ))(u, v) = −Hc(α(X),β(Y ))(u, v) = − S 1 0 S 1 0 −c(α(X),β(Y ))(u, v) log c(α(X),β(Y ))(u, v)) du dv = S 1 0 S 1 0 cX,Y (1 − u, 1 − v) log cX,Y (1 − u, 1 − v) du dv = S 1 0 S 1 0 cX,Y (m, n) log cX,Y (m, n) du dv [By changing variable,(1 − u = m, 1 − v = n)] = −Hc(X,Y )(m, n) = Ic(X,Y )(m, n) = Ic(X,Y )(u, v) (30) 314 Thus we have shown that copula based mutual information of random vari-315 ables α(X) and β(X) are here same as that of the random variables X, Y ,316 where α and β are increasing or decreasing function of X and Y . Since the317 18 proposed CBFS method is strictly based on this copula based mutual informa-318 tion, the selected feature set with this method also remains the same under such319 transformations. The proof of correctness of the optimal solution is provided320 below.321 Theorem 3.5. (Proof of Correctness) Let, a feature subset Fs = (f1, f2, . . . , fj, fj+1, . . .)322 be obtained from feature set FN using CBFS. Our claim is that the feature subset323 Fs is the optimal feature set.324 Proof. Let us consider the claim of the theorem is false.325 Let, another optimal feature subset (F ′ s) which has most of the frequent features326 with our claim feature set Fs.327 The new optimal feature subset is, F ′ s = {f1, f2, . . . , fj, fx, . . .} which is similar328 with feature subset Fs but except the feature(fj+1). The feature fj+1 is replaced329 with feature fx in new optimal feature subset F ′ s.330 CBFS selects features according to minimum correlation with all other non331 selected features and maximum correlation with the class label. So, It is true332 that,333 Ic(fk∈(FN −Fs))(fj+1; fk) ≤ Ic(fk∈(FN −F ′ s))(fx; fk) (31) IC(X,Y ) is the copula mutual information between random variables X and Y .334 Hence, CBFS will exchange feature fx with the feature fj+1.335 Then, new optimal solution will be, Fs = {f1, f2, . . . , fj, fj+1, . . .}, which is con-336 tradiction of our claim.337 4. Experimental Framework338 We have selected three well known mutual information-based feature selec-339 tion methods: CMIM, MIFS, and MRMR to compare with the proposed method340 CBFS. The reason for choosing these three works is that all the methods are341 based on joint mutual information and made an excellent performance in fea-342 ture selection [31]. The source codes for the methods are available in (FEAST)343 package of Matlab and can be downloaded from http://mloss.org/software/344 view/386/.345 19 Two types of datasets are used here, the ﬁrst one is a synthetic Gaussian346 mixture dataset, and another is real datasets. We have used four well-known347 classiﬁers to measure accuracy. In the next three sub-sections, multiple classi-348 ﬁers, synthetic Gaussian datasets, and real datasets are described respectively.349 4.1. Multiple Classiﬁers350 CBFS method is a ﬁlter-based feature selection approach. So, We anticipate351 that feature selected with the CBFS method has better performance with any352 classiﬁer. To validate this, we have considered four widely used classiﬁers [32],353 Support Vector Machine (SVM ), Neural Network (NN ), Naive Bayes (NB ) and354 Gradient Boosting Machine (GBM ). The caret package in R provided all the355 classiﬁer implementations.356 • SVM is a discriminative supervised learning method. We conﬁgured the357 SVM with l2-regularizer and linear kernel.358 • NN is also a supervised learner that emulates the human brain neuron359 structure. We have used the default caret NN layout to train our models.360 • NB is one of the probabilistic supervised learning methods for classiﬁ-361 cation. NB has shown excellent classiﬁcation performance on many real362 datasets. It uses Bayes’ theorem to compute the necessary probabilities.363 • GBM is a machine learning method for classiﬁcation and regression prob-364 lems. It ensembles multiple weak prediction models to build a single strong365 learner.366 4.2. Synthetic Gaussian Mixture Data367 To test the eﬃcacy of our algorithm, we generated eight synthetic Gaussian368 mixture datasets using 2, 3, 4 and 5 clusters. We used 50 relevant features and369 250 irrelevant features for each dataset. We created K component of Gaussian370 mixture model with the 50 relevant features. The covariance matrices(Σ) were371 generated using the formula described below:372 Σ = (ρSi−jS) (32) 20 i, j are the row and column of the covariance matrix. We have consider ρ = 0.5.373 The covariance matrix is kept the same for all eight synthetic datasets, and ﬁfty374 relevant features are generated by variation of the mean (µ). We have assumed375 the above for simplicity of datasets.376 White Gaussian noise [33] was added to these synthetic datasets as 250 ir-377 relevant features. The function uses the random normal distribution func-378 tion to create the normally distributed noise and adds it to the input matrix.379 Add.Gaussian.noise package in R is utilized here to generate Gaussian noise.380 The magnitude of the Gaussian noise is taken as mean 0 and standard devia-381 tion 1 for all experiment. 500 samples with k = (2, 3, 4, 5) number of classes are382 generated for each synthetic dataset. We generated overlapping clusters and383 non-overlapping clusters data for each class k = (2, 3, 4, 5). The sample genera-384 tion for each dataset was repeated 100 times, and average results are provided in385 section 5.1. Synthetic Data descriptions and distribution parameters are given386 below in Table 1 and 2.387 Table 1: Non-Overlapping Synthetic Gaussian Mixture Data # Classes Mixing Probabil- ities # Features Range of Means (µ) Relevant Irrelevant Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 2 [0.6,0.4] 50 250 50 values from (5, 15) 50 values from (−15, −5) - - - 3 [0.4,0.3,0.3] 50 250 Same as above Same as above 25 values from (5, 15) 25 values from (−15, −5) - - 4 [0.4,0.2,0.2,0.2] 50 250 Same as above Same as above Same as above 25 times 0 25 values from (5, 15) - 5 [0.2,0.2,0.2,0.2,0.2] 50 250 Same as above Same as above Same as above Same as above 50 values from (15, 20) The synthetic gaussian mixture datasets are plotted using tSNE visualization388 in Figure 2. The t-Distributed Stochastic Neighbor Embedding (t-SNE) is a389 non-linear technique for dimensionality reduction that is particularly employed390 to visualize of high-dimensional datasets.391 21 Table 2: Overlapping Synthetic Gaussian Mixture Data # Classes Mixing Probabil- ities # Features Range of Means (µ) Relevant Irrelevant Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 2 [0.6,0.4] 50 250 50 values from (2, 3) 50 values from (−3, −2) - - - 3 [0.4,0.3,0.3] 50 250 Same as above Same as above 25 values from (2, 3) 25 values from (−3, −2) - - 4 [0.4,0.2,0.2,0.2] 50 250 Same as above Same as above Same as above 25 times 0 25 values from (2, 3) - 5 [0.2,0.2,0.2,0.2,0.2] 50 250 Same as above Same as above Same as above Same as above 25 times 0 25 values from (−3, −2) 4.3. Real Dataset392 Ten continuous and discrete datasets were used for evaluation. Among393 them, ﬁrst ﬁve are UCI repository datasets https://archive.ics.uci.edu/394 ml/datasets.php, the next two are gene microarray datasets, then the two are395 face image datasets, and last one is handwritten image dataset. Raw datasets396 were preprocessed with discretization process, [34], using which the continuous397 datasets with equal frequencies. A short description of the datasets is given398 below in Table 3.399 400 • The Arrhythmia Data contains 452 samples and 279 features. It is also a401 binary classiﬁcation problem. The two classes are male and female.402 403 • The dataset Musk contains 476 samples and 168 features. It is also a404 binary class dataset. It is a continuous type of dataset.405 406 • The Libras Movement dataset contains 360 samples and 91 attributes as a407 feature, representing the coordinates of movement. It includes 15 classes,408 where each class references to a hand movement type in LIBRAS.409 410 22 Figure 2: Eight synthetic Gaussian Mixture Datasets are embedded using tSNE visualization. The ﬁgures represent non-overlapping classes and overlapping classes in the respective rows. • The Semieon is a popularly used handwritten dataset which contains 1593411 samples and 256 gray scale pixel values. Pixel values are regarded as fea-412 tures. It contains ten classes,(0, 1, \u0005, 9).413 414 • The Handwritten (HDR) consists of features extracted from handwritten415 numerals collected from Dutch utility maps. It has ten classes of ‘0-9’ dig-416 its. HDR consists of total 2000 observations with 200 patterns per class.417 Data contains 649 diﬀerent shapes for each model which are regarded as418 features.419 420 • The Lymphoma Dataset [35] is an unevenly distributed data. It contains421 96 sample points. It has 4026 genes expression values for each sample422 point as features. The target class has nine subtypes of lymphoma.423 424 • The Leukemia dataset [36] is one of the popular gene expression datasets.425 23 Table 3: Summary of the real datasets used in the experiments. Serial # Dataset Name #Instances #Features #Class 1 Arrhythmia 452 279 2 2 Musk 476 168 2 3 Libra 360 91 15 4 Semieon 1593 256 10 5 Handwritten Numerals 2000 649 10 6 Lymphoma 96 4026 9 7 Leukemia 72 7070 2 8 ORL 400 1024 40 9 warpPIE10P 210 2420 10 10 USPS 9298 256 10 It contains 72 observations. It consists of 7070 gene expression values for426 each observation. It is a binary class dataset, ‘AML’, ‘ALL’ are two class427 labels.428 429 • The ORL face database (developed at the Olivetti Research Laboratory,430 Cambridge, U.K.) contains 400 images of size 112 ∗ 92. The dataset ac-431 commodates images of 40 persons, ten images per each person, which are432 at diﬀerent times, lighting, and facial expressions. It has 40 class labels.433 • The warpPIE10P [37] is an image database of over 40,000 facial images434 of 68 people. Images are captured with each person across 13 diﬀerent435 poses, under 43 diﬀerent illumination conditions, and with four diﬀerent436 expressions. It contains 10 class labels.437 • The USPS database [38] is an image database for handwritten text recog-438 nition research. Digital images of approximately 5000 city names, 5000439 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are440 24 included in the dataset. It contains 10 class labels.441 5. Experimental Results442 Two alternative approaches were used to demonstrate the performance of the443 proposed feature selection algorithm (CBFS). Each approach addresses speciﬁc444 evaluation criteria, as outlined below.445 1. To investigate whether CBFS produces a better informative feature sub-446 set: Feature subsets obtained from diﬀerent feature selection methods were447 subsequently used for building their corresponding prediction models. Af-448 ter that, prediction accuracy was used to compare the model performance449 across multiple real datasets. For the synthetic datasets, the clustering450 accuracy was evaluated.451 2. To verify the scale-invariant property of CBFS: Gaussian noise was added452 to the real datasets and then tested to compare stability across the diﬀer-453 ent feature selection methods.454 5.1. Clustering Accuracy of Synthetic Datasets455 The clustering performance on all eight synthetic Gaussian mixture datasets456 was reported using the Adjusted Rand Index (ARI). The ARI can be used as457 a measure of agreement between the assigned clusters and the known groups.458 The value of the ARI is close to 0 when the clustering prediction is random,459 and 1 when the clustering is perfectly coherent to the known labels [39] Table460 4 illustrates the clustering performance evaluated on eight synthetic Gaussian461 mixture datasets for all four feature selection methods. As the number of clus-462 ters increases, the ARI score decreases for all methods. Also, the ARI score for463 the overlapping Gaussian mixture datasets is less than non-overlapping Gaus-464 sian mixture datasets. The results in Table 4 depict that CBFS outperforms all465 competing methods (Section 4) excepts 4-class and 5-class overlapping datasets,466 whereas MIFS and CMIM have higher ARI values than CBFS. The plausible467 reason is that CBFS leverages the multivariate dependency measure and scale-468 invariant property of the copula.469 25 Table 4: Adjusted Rand Index for Synthetic Data Datasets Method 2-Class Data 3-Class Data 4-Class Data 5-Class Data Non-Overlapping Datasets CBFS 1 ± 0 0.88 ± 0.1 0.72 ± 0.2 0.68 ± 0.07 CMIM 1 ± 0 0.83 ± 0.05 0.70 ± 0.13 0.57 ± 0.02 MIFS 1 ± 0 0.71 ± 0.02 0.68 ± 0.04 0.61 ± 0.05 MRMR 1 ± 0 0.70 ± 0.04 0.67 ± 0.004 0.59 ± 0.02 Overlapping Datasets CBFS 0.98 ± 0.04 0.78 ± 0.11 0.64 ± 0.1 0.65 ± 0.02 CMIM 0.97 ± 0.02 0.76 ± 0.03 0.58 ± 0.12 0.67 ± 0.1 MIFS 0.96 ± 0.03 0.75 ± 0.07 0.65 ± 0.023 0.64 ± 0.03 MRMR 0.97 ± 0.01 0.76 ± 0.04 0.63 ± 0.05 0.65 ± 0.07 5.2. Classiﬁcation Accuracy on Real Datasets470 The classiﬁcation performance of four classiﬁers (discussed in Section 4.1)471 using the respective feature selection methods has been reported. Leave-group-472 out cross-validation with 100 repetitions, and the ratio of training to testing at473 8.5:1.5 were used for evaluation purposes. In order to inspect how the num-474 ber of selected features aﬀects a classiﬁer, the classiﬁers were trained separately475 by varying the number of top features, topn = (10, 20, 30, 40, 50, 60, 70, 80) ob-476 tained from respective feature selection methods. Figure 3 explains the classiﬁ-477 cation accuracy using the Gradient Boost Machine(GBM) classiﬁer for ten real478 datasets. The result reports that when the number of features increases, the479 classiﬁcation accuracy with the CBFS methods also increases. The plausible rea-480 son is the multivariate dependency of the copula based feature selection method.481 For all multi class dataset (HDR, Libra, Lymphoma, ORL, USPS, warpPie10P,482 and Semieon), our proposed method performs well among other methods. In483 three datasets (Musk, Arrhythmia, and Semieon), CMIM and MRMR perform484 well than CBFS for the small number of features. The accuracy results of the485 other three classiﬁers are given in the supplementary ﬁle.486 5.3. Stability Performance487 This section represents that the proposed method has the potential to se-488 lect features from noisy data. As discussed in this paper [28], the copula has489 26 Arrythmia HDR Leukomia Libra Lymphomia Musk ORL Semieon USPS WRAPIE10P 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 20 40 60 80 0.2 0.4 0.6 0.8 1.0 Feature NumberClassification Accuracy Method CBFS CMIM MIFS MRMR Figure 3: Classiﬁcation accuracy for the ten datasets with Gradient Boost Machine(GBM) Classiﬁer. There are ten boxes. Each box represents one dataset that contains four color lines (Methods). The X-axis represents the number of selected features(10, 20, 30, 40, 50, 60, 70, 80), Y-axis represents the classiﬁcation accuracy values. the power to preserve the same dependency measure in transformed noisy fea-490 tures. The transformation of features may occur due to technical noise, which is491 much more common in real-life datasets. In that case, existing feature selection492 algorithms will fail on the noisy structure of the data, which results in poor493 performance.494 Gaussian noise was added with each feature of the datasets to make those495 noisy. Add.Gaussian.noise package in R was used to generate Gaussian noise.496 Next, the top 80 features were selected with each noisy dataset. The percentage497 of matching features was employed as a scale invariance metric. The number498 of matching features is the intersection of the most informative feature subset499 from a noisy dataset with the original optimal feature subset. The matching500 27 feature score (percentage) is deﬁned below:501 Simscore = ((n − r)~n) ∗ 100 (33) n is the total number of features in a dataset, and r represents the number of502 discrepancies between the feature subsets of original and noisy data.503 Figure 4 reports a correlation plot of the similarity score among all the com-504 peting methods in ten real datasets. For each case, we perform 100 trials and505 compute Kendall tau correlation among the similarity scores returned by all the506 methods.507 It can be observed from the ﬁgure that the correlation between the CBFS and508 CMIM is lesser than the other competing pairs, which represents CBFS has a509 higher similarity score than the well known CMIM method. The possible reason510 behind this is the scale invariant property of copula. Thus, the number of in-511 formative feature subset almost remains the same as the original feature subset512 for a noisy dataset using CBFS, which results in a high similarity score.513 Figure 4: Correlation plot of similarity score for all four methods on ten datasets. 28 6. Conclusion514 In this paper, we have proposed CBFS, a multivariate copula based approach515 for feature selection. Copula based dependency is utilized in our paper to model516 the interdependence between the features. The two main characteristics of the517 proposed method are:518 (i) Incorporation of copula-based multivariate dependency in mutual infor-519 mation helps to remove the need to average out over multiple instances520 of bivariate dependencies, thus eliminating the average estimation error521 introduced when bivariate dependency is used between two features.522 (ii) The copula-based objective function is used for feature selection possessed523 property of scale invariance. Hence, CBFS can achieve superior results524 compared to other methods in noisy dataset.525 The CBFS algorithm is applied in eight synthetic Gaussian Mixture datasets526 and ten real life datasets. In most of the dataset, including synthetic and real-527 life, CBFS consistently performs well than all other competing methods. This528 makes CBFS well accepted in a large domain of datasets (text, image and bio-529 logical). Moreover, CBFS performance is stable against noisy data. The large530 similarity score between the features in original and noisy datasets establish this531 fact.532 The execution time of our algorithm is directly proportional to the number of533 selected features and can be expensive when one needs to select a large number534 of features. However, this can be easily tackled with ever increasing computing535 power in advanced servers. Additionally, the regularization parameter has not536 been considered in our proposed approach, which may sometime make the al-537 gorithm susceptible to overﬁtting unless carefully employed. One limitation of538 CBFS is that it needs a large sample size to the number of classes. The ratio539 (n~SY S) must be a large number (>> 1), where n denotes the number of samples,540 and SY S is the number of classes in the dataset, otherwise Redundant features541 may occur while selecting a small set of features in some datasets (e.g., ORL,542 Libra, and WarpPIE10P).543 29 In particular, we are working on developing a regularized version of CBFS in-544 corporating a regularization parameter. Further future extensions would be to545 develop a robust version of CBFS for noisy data and to perform sensitivity546 analysis about the copula estimation (both parametric and non-parametric) for547 better dependency measure. One major future work will be the incorporation548 of deep learning in copula based feature selection, which can be applied in im-549 age analysis. Another essential possible future research along the line of our550 proposed algorithm will be the development of the copula based unsupervised551 feature selection, which has several practical applications, including the analy-552 ses of large single cell RNA sequence datasets. We hope to tackle some of these553 extensions in the future.554 Acknowledgment555 We would like to acknowledge support from J.C. Bose Fellowship [SB/S1/JCB-556 033/2016 to S.B.] by the DST, Govt. of India; SyMeC Project grant [BT/Med-557 II/NIBMG/SyMeC/2014/Vol. II] given to the Indian Statistical Institute by558 the Department of Biotechnology (DBT), Govt. of India. Govt. of India; In-559 spire DST Project.560 Software Availability: https://github.com/Snehalikalall/CBFS561 References562 [1] S. Dash, S. K. Shakyawar, M. Sharma, S. Kaushik, Big data in healthcare:563 management, analysis and future prospects, Journal of Big Data 6 (1)564 (2019) 54.565 [2] S. K. Baliarsingh, S. Vipsita, K. Muhammad, B. Dash, S. Bakshi, Analysis566 of high-dimensional genomic data employing a novel bio-inspired algorithm,567 Applied Soft Computing 77 (2019) 520–532.568 [3] X.-Y. Zhang, Y. Bengio, C.-L. Liu, Online and oﬄine handwritten chinese569 character recognition: A comprehensive study and new benchmark, Pattern570 Recognition 61 (2017) 348–360.571 30 [4] J.-P. van Oosten, L. Schomaker, Separability versus prototypicality in572 handwritten word-image retrieval, Pattern Recognition 47 (3) (2014) 1031–573 1038.574 [5] P. Boileau, N. S. Hejazi, S. Dudoit, Exploring high-dimensional biological575 data with sparse contrastive principal component analysis, Bioinformatics576 36 (11) (2020) 3422–3430.577 [6] S. Sharmin, M. Shoyaib, A. A. Ali, M. A. H. Khan, O. Chae, Simultaneous578 feature selection and discretization based on mutual information, Pattern579 Recognition 91 (2019) 162–174.580 [7] R. Sheikhpour, M. A. Sarram, S. Gharaghani, M. A. Z. Chahooki, A sur-581 vey on semi-supervised feature selection methods, Pattern Recognition 64582 (2017) 141–158.583 [8] I. A. Gheyas, L. S. Smith, Feature subset selection in large dimensionality584 domains, Pattern Recognition 43 (1) (2010) 5–13.585 [9] H. Liu, J. Sun, L. Liu, H. Zhang, Feature selection with dynamic mutual586 information, Pattern Recognition 42 (7) (2009) 1330–1339.587 [10] R. Shang, Y. Meng, W. Wang, F. Shang, L. Jiao, Local discriminative588 based sparse subspace learning for feature selection, Pattern Recognition589 92 (2019) 219–230.590 [11] Y. Ye, Q. Wu, J. Z. Huang, M. K. Ng, X. Li, Stratiﬁed sampling for feature591 subspace selection in random forests for high dimensional data, Pattern592 Recognition 46 (3) (2013) 769–787.593 [12] X.-f. Song, Y. Zhang, Y.-n. Guo, X.-y. Sun, Y.-l. Wang, Variable-size coop-594 erative coevolutionary particle swarm optimization for feature selection on595 high-dimensional data, IEEE Transactions on Evolutionary Computation596 (2020).597 31 [13] S.-L. Huang, X. Xu, L. Zheng, An information-theoretic approach to un-598 supervised feature selection for high-dimensional data, IEEE Journal on599 Selected Areas in Information Theory (2020).600 [14] A. Milan, S. H. Rezatoﬁghi, R. Garg, A. R. Dick, I. D. Reid, Data-driven601 approximations to np-hard problems., in: AAAI, 2017, pp. 1453–1459.602 [15] D. Mo, Z. Lai, Robust jointly sparse regression with generalized orthogonal603 learning for image feature selection, Pattern Recognition 93 (2019) 164–178.604 [16] S. Kashef, H. Nezamabadi-pour, A label-speciﬁc multi-label feature selec-605 tion algorithm based on the pareto dominance concept, Pattern Recognition606 88 (2019) 654–667.607 [17] J. Gonz´alez, J. Ortega, M. Damas, P. Mart´ın-Smith, J. Q. Gan, A new608 multi-objective wrapper method for feature selection–accuracy and stability609 analysis for bci, Neurocomputing 333 (2019) 407–418.610 [18] J. Zhang, Z. Luo, C. Li, C. Zhou, S. Li, Manifold regularized discriminative611 feature selection for multi-label learning, Pattern Recognition 95 (2019)612 136–150.613 [19] H. S. Badr, H. Du, M. Marshall, E. Dong, M. M. Squire, L. M. Gardner,614 Association between mobility patterns and covid-19 transmission in the usa:615 a mathematical modelling study, The Lancet Infectious Diseases (2020).616 [20] S.-W. Hwang, Y.-C. Chu, S.-R. Hwang, J.-H. Hwang, Association of peri-617 odic limb movements during sleep and tinnitus in humans, Scientiﬁc Re-618 ports 10 (1) (2020) 1–5.619 [21] J. Ircio, A. Lojo, U. Mori, J. A. Lozano, Mutual information based feature620 subset selection in multivariate time series classiﬁcation, Pattern Recogni-621 tion 108 (2020) 107525.622 [22] C.-F. Tsai, W. Eberle, C.-Y. Chu, Genetic algorithms in feature and in-623 stance selection, Knowledge-Based Systems 39 (2013) 240–247.624 32 [23] B. Peralta, A. Soto, Embedded local feature selection within mixture of625 experts, Information Sciences 269 (2014) 176–187.626 [24] X. Zhang, G. Wu, Z. Dong, C. Crawford, Embedded feature-selection sup-627 port vector machine for driving pattern recognition, Journal of the Franklin628 Institute 352 (2) (2015) 669–685.629 [25] R. Battiti, Using mutual information for selecting features in supervised630 neural net learning, IEEE Transactions on Neural Networks 5 (4) (1994)631 537–550.632 [26] F. Fleuret, Fast binary feature selection with conditional mutual informa-633 tion, Journal of Machine Learning Research 5 (Nov) (2004) 1531–1555.634 [27] H. Peng, F. Long, C. Ding, Feature selection based on mutual information635 criteria of max-dependency, max-relevance, and min-redundancy, IEEE636 Transactions on Pattern Analysis and Machine Intelligence 27 (8) (2005)637 1226–1238.638 [28] R. B. Nelsen, An introduction to copulas, Springer Science & Business639 Media, 2007.640 [29] D. P´al, B. P´oczos, C. Szepesv´ari, Estimation of r´enyi entropy and mutual641 information based on generalized nearest-neighbor graphs, in: Advances in642 Neural Information Processing Systems, 2010, pp. 1849–1857.643 [30] M. Bennasar, Y. Hicks, R. Setchi, Feature selection using joint mutual in-644 formation maximisation, Expert Systems with Applications 42 (22) (2015)645 8520–8532.646 [31] G. Brown, A. Pocock, M.-J. Zhao, M. Luj´an, Conditional likelihood max-647 imisation: a unifying framework for information theoretic feature selection,648 Journal of machine learning research 13 (Jan) (2012) 27–66.649 [32] A. S. Britto Jr, R. Sabourin, L. E. Oliveira, Dynamic selection of classiﬁersa650 comprehensive review, Pattern Recognition 47 (11) (2014) 3665–3680.651 33 [33] W. Liu, W. Lin, Additive white gaussian noise level estimation in svd652 domain for images, IEEE Transactions on Image processing 22 (3) (2012)653 872–883.654 [34] C.-F. Tsai, Y.-C. Chen, The optimal combination of feature selection and655 data discretization: An empirical study, Information Sciences 505 (2019)656 282–293.657 [35] A. A. Alizadeh, M. B. Eisen, R. E. Davis, C. Ma, I. S. Lossos, A. Rosen-658 wald, J. C. Boldrick, H. Sabet, T. Tran, X. Yu, et al., Distinct types of659 diﬀuse large b-cell lymphoma identiﬁed by gene expression proﬁling, Na-660 ture 403 (6769) (2000) 503.661 [36] T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. P.662 Mesirov, H. Coller, M. L. Loh, J. R. Downing, M. A. Caligiuri, et al.,663 Molecular classiﬁcation of cancer: class discovery and class prediction by664 gene expression monitoring, Science 286 (5439) (1999) 531–537.665 [37] T. Sim, S. Baker, M. Bsat, The cmu pose, illumination, and expression666 (pie) database, in: Proceedings of Fifth IEEE International Conference on667 Automatic Face Gesture Recognition, IEEE, 2002, pp. 53–58.668 [38] J. J. Hull, A database for handwritten text recognition research, IEEE669 Transactions on Pattern Analysis and Machine Intelligence 16 (5) (1994)670 550–554.671 [39] M. Hoﬀman, D. Steinley, M. J. Brusco, A note on using the adjusted rand672 index for link prediction in networks, Social networks 42 (2015) 72–79.673 34 Authors Biography674 Snehalika Lall completed her B.Tech. degree in electronics and commu-675 nication engineering from the West Bengal University of Technology, Kolkata,676 2013, India, and the M.E. degree in electronics and communication engineering677 from Jadavpur University, Kolkata, 2016. She is currently a Senior Research Fel-678 low for PhD in Machine Intelligence Unit, Indian Statistical Institute, Kolkata,679 from 2016, Her current research interests include feature engineering, statistical680 machine learning etc.681 Debajyoti Sinha received the B.S., M.S., and M.Tech. degrees from the682 University of Calcutta, Kolkata, India, all in computer science and engineering.683 He is currently pursuing the Ph.D. degree in computer science with the Uni-684 versity of Calcutta, Kolkata. He is involved in the research with the Machine685 Intelligence Unit, Indian Statistical Institute, Kolkata. His current research686 interests include parallel computing, high-dimensional data models, feature se-687 lection, and bioinformatics.688 Dr. Abhik Ghosh is currently an Assistant professor in Interdisciplinary689 Statistical Research Unit, Indian Statistical Institute, Kolkata. He received his690 B.Stat. (Hons.), and M.Stat. (Specialization: Mathematical Statistics and691 Probability) degrees with gold medals in 2010 and 2012, respectively, from692 the Indian Statistical Institute (ISI), Kolkata, India. He the completed his693 PhD in Statistics from the same institute in 2015 under the guidance of Prof.694 Ayanendranath Basu and did his postdoctoral research in University of Oslo,695 Norway. His research works involve the divergence measures and minimum696 divergence inferences, data robustness under Bayesian paradigm, robust and697 high-dimensional statistical methods with applications to biostatistics and bio-698 metrics, robust hypothesis testing, etc.699 Dr. Debarka Sengupta received the B.Tech. degree in computer science700 and engineering from the West Bengal University of Technology, Kolkata, India,701 and the Ph.D. degree in computer science and engineering from Jadavpur Uni-702 versity, Kolkata. He was with the Machine Intelligence Unit, Indian Statistical703 35 Institute, Kolkata, from 2009 to 2013, as a Research Fellow. He is currently704 a Post-Doctoral Fellow with the Computational and Systems Biology Group,705 Genome Institute of Singapore, Singapore. His current research interests in-706 clude genomics and statistical machine learning.707 Prof. Sanghamitra Bandyopadhyay received the Ph.D. degree in com-708 puter science from the Indian Statistical Institute (ISI), Kolkata, India, where709 she has been a Professor since 2007. She is currently the Director of ISI. She710 has authored or co-authored over 250 technical articles and has published ﬁve711 authored and edited books. Her current research interests include computa-712 tional biology and bioinformatics, soft and evolutionary computation, pattern713 recognition, and data mining.714 Dr. Banyopadhyay is a fellow of the Indian National Science Academy, the715 National Academy of Sciences, India, and the Indian National Academy of En-716 gineering, India. She is a recipient of several prestigious awards, including the717 Humboldt Fellowship from Germany, ICTP Senior Associate, Trieste, Italy, and718 the Shanti Swarup Bhatnagar Prize in engineering science.719 36 Declaration of interests720 The authors declare that they have no known competing ﬁnancial interests or721 personal relationships that could have appeared to inﬂuence the work reported722 in this paper.723 37","libVersion":"0.3.2","langs":""}
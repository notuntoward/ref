{"path":"lit/lit_sources/HuggingFace24benchmarkMTEB.pdf","text":"4 /8 /2 4 , 8 :5 4 P M b lo g /m te b .m d a t m a in ¬∑ h u g g in g fa c e /b lo g h ttp s ://g ith u b .c o m /h u g g in g fa c e /b lo g /b lo b /m a in /m te b .m d 1 /6 julien-c and mishig25 No more magic comments ( #1554 ) 7 months ago huggingface ¬†/ blog Code Issues 129 Pull requests 65 Actions Projects Security In blog / mt eb.md 140 lines (89 loc) ¬∑ 7.03 KB title thumbnail author s MTEB: Massive Text Embedding Benchmark /blog/assets/110_mteb/thumbnail.png user Muennighoff MTEB is a massive benchmark for measuring the performance of text embedding models on diverse embedding tasks. The ü•á leaderboard provides a holistic view of the best text embedding models out there on a variety of tasks. The üìù paper gives background on the tasks and datasets in MTEB and analyzes leaderboard results! The üíª Github repo contains the code for benchmarking and submitting any model of your choice to the leaderboard. MTEB: Massiv e Text Embedding Benchmark Pr eview Code Blame Raw 4 /8 /2 4 , 8 :5 4 P M b lo g /m te b .m d a t m a in ¬∑ h u g g in g fa c e /b lo g h ttp s ://g ith u b .c o m /h u g g in g fa c e /b lo g /b lo b /m a in /m te b .m d 2 /6 Text Embeddings are vector representations of text that encode semantic information. As machines require numerical inputs to perform computations, text embeddings are a crucial component of many downstream NLP applications. For example, Google uses text embeddings to power their search engine . Text Embeddings can also be used for finding patterns in large amount of text via clustering or as inputs to text classification models, such as in our recent SetFit work. The quality of text embeddings, however, is highly dependent on the embedding model used. MTEB is designed to help you find the best embedding model out there for a variety of tasks! üêã Massiv e : MTEB includes 56 datasets across 8 tasks and currently summarizes >2000 results on the leaderboard . üåé Multilingual: MTEB contains up to 112 different languages! W e have benchmarked several multilingual models on Bitext Mining, Classification, and ST S. Why Text Embeddings? MTEB 4 /8 /2 4 , 8 :5 4 P M b lo g /m te b .m d a t m a in ¬∑ h u g g in g fa c e /b lo g h ttp s ://g ith u b .c o m /h u g g in g fa c e /b lo g /b lo b /m a in /m te b .m d 3 /6 ü¶ö Ext ensible : Be it new tasks, datasets, metrics, or leaderboard additions, any contribution is very welcome. Check out the GitHub repository to submit to the leaderboard or solve open issues . W e hope you join us on the journey of finding the best text embedding model! Ov er view o f t asks and dat as ets in MTEB. Multilingual dat as ets ar e mark ed with a purple shade. For the initial benchmarking of MTEB, we focused on models claiming state-of-the-art results and popular models on the Hub. This led to a high representation of transformers. ü§ñ Models 4 /8 /2 4 , 8 :5 4 P M b lo g /m te b .m d a t m a in ¬∑ h u g g in g fa c e /b lo g h ttp s ://g ith u b .c o m /h u g g in g fa c e /b lo g /b lo b /m a in /m te b .m d 4 /6 Models b y av er age English MTEB s c or e (y) vs speed (x) vs embedding size (cir cle size). W e grouped models into the following three attributes to simplify finding the best model for your task: üèé Maximum speed Models like Glove offer high speed, but suffer from a lack of context awareness resulting in low average MTEB scores. ‚öñ Speed and per formance Slightly slower, but significantly stronger, all-mpnet-base-v2 or all-MiniLM-L6-v2 provide a good balance between speed and performance. üí™ Maximum per formance Multi-billion parameter models like ST5-XXL , G TR-XXL or SGPT-5.8B-msmarco dominate on MTEB. They tend to also produce bigger embeddings like SGPT-5.8B-msmarco which produces 4096 dimensional embeddings requiring more storage! Model performance varies a lot depending on the task and dataset, so we recommend checking the various tabs of the leaderboard before deciding which model to use! Using the MTEB library , you can benchmark any model that produces embeddings and add its results to the public leaderboard. Let's run through a quick example! First, install the library: Benchmark y our model 4 /8 /2 4 , 8 :5 4 P M b lo g /m te b .m d a t m a in ¬∑ h u g g in g fa c e /b lo g h ttp s ://g ith u b .c o m /h u g g in g fa c e /b lo g /b lo b /m a in /m te b .m d 5 /6 Next, benchmark a model on a dataset, for example komninos word embeddings on Banking77 . This should produce a r e s u l t s / a v e r a g e _ w o r d _ e m b e d d i n g s _ k o m n i n o s / B a n k i n g 7 7 C l a s s i f i c a t i o n . j s o n file! Now you can submit the results to the leaderboard by adding it to the metadata of the R E A D M E . m d of any model on the Hub. Run our automatic script to generate the metadata: The script will produce a m t e b _ m e t a d a t a . m d file that looks like this: p i p i n s t a l l m t e b f r o m m t e b i m p o r t M T E B f r o m s e n t e n c e _ t r a n s f o r m e r s i m p o r t S e n t e n c e T r a n s f o r m e r m o d e l _ n a m e = \" a v e r a g e _ w o r d _ e m b e d d i n g s _ k o m n i n o s \" m o d e l = S e n t e n c e T r a n s f o r m e r ( m o d e l _ n a m e ) e v a l u a t i o n = M T E B ( t a s k s = [ \" B a n k i n g 7 7 C l a s s i f i c a t i o n \" ] ) r e s u l t s = e v a l u a t i o n . r u n ( m o d e l , o u t p u t _ f o l d e r = f \" r e s u l t s / { m o d e l _ n a m e } \" ) p y t h o n m t e b _ m e t a . p y r e s u l t s / a v e r a g e _ w o r d _ e m b e d d i n g s _ k o m n i n o s - - - t a g s : - m t e b m o d e l - i n d e x : - n a m e : a v e r a g e _ w o r d _ e m b e d d i n g s _ k o m n i n o s r e s u l t s : - t a s k : t y p e : C l a s s i f i c a t i o n d a t a s e t : t y p e : m t e b / b a n k i n g 7 7 n a m e : M T E B B a n k i n g 7 7 C l a s s i f i c a t i o n c o n f i g : d e f a u l t s p l i t : t e s t r e v i s i o n : 0 f d 1 8 e 2 5 b 2 5 c 0 7 2 e 0 9 e 0 d 9 2 a b 6 1 5 f d a 9 0 4 d 6 6 3 0 0 m e t r i c s : - t y p e : a c c u r a c y v a l u e : 6 6 . 7 6 6 2 3 3 7 6 6 2 3 3 7 7 4 /8 /2 4 , 8 :5 4 P M b lo g /m te b .m d a t m a in ¬∑ h u g g in g fa c e /b lo g h ttp s ://g ith u b .c o m /h u g g in g fa c e /b lo g /b lo b /m a in /m te b .m d 6 /6 Now add the metadata to the top of a R E A D M E . m d of any model on the Hub, like this SGPT- 5.8B-msmarco model, and it will show up on the leaderboard after refreshing! Go out there and benchmark any model you like! Let us know if you have questions or feedback by opening an issue on our GitHub repo or the leaderboard community tab ü§ó Happy embedding! Huge thanks to the following who contributed to the article or to the MTEB codebase (listed in alphabetical order): S teven Liu, Lo√Øc Magne, Nils R eimers and Nouamane Tazi. - t y p e : f 1 v a l u e : 6 6 . 5 9 0 9 6 4 3 2 8 8 2 6 6 7 - - - Next st eps Cr edits","libVersion":"0.3.1","langs":""}
{"path":"lit/lit_sources/Mahler09sp500lagLasso.pdf","text":"Modeling the S&P 500 Index using the Kalman Filter and the LagLasso Nicolas Mahler ENS Cachan & UniverSud CMLA UMR CNRS 8536 and Telecom Paristech (TSI) LTCI UMR Institut Telecom/CNRS 5141 nico.mahler@gmail.com Abstract This article introduces a method to predict upward and downward monthly vari- ations of the S&P 500 index by using a pool of macro-economic and ﬁnancial explicative variables. The method is based on the combination of a denoising step, performed by Kalman ﬁltering, with a variable selection step, performed by a Lasso-type procedure. In particular, we propose an implementation of the Lasso method called LagLasso which includes selection of lags for individual fac- tors. We provide promising backtesting results of the prediction model based on a naive trading rule. 1 Introduction We consider the problem of predicting monthly movements of the S&P 500 index, and assume that a small subset of macro-economic and ﬁnancial predictors can efﬁciently represent the exogenous inﬂuence on S&P 500. The inﬂuence of each of these predictors can change over time and it can be lagged. Additionnally, according to economists (cf.[8]), S&P 500 is sensitive to the variations of those predictors around their own trend rather than to the variations themselves. Therefore, we need to ﬁltrate the predictors : a linear state-space model is ﬁrst proposed for each of them and their innovation residuals are computed with the Kalman algorithm (cf.[4],[5],[6],[7]). These residuals are then used to predict S&P 500 variations : the most informative residuals are identiﬁed thanks to the Lasso method, a procedure which aims at minimizing a L2 regression ﬁt under L1 penalty. This constraint allows for a sparse selection which is not only a gain in terms of interpretability, but which allows for variance reduction leading to more accurate predictions. The issue of lagged inﬂuence between variables is adressed by slightly modifying the Lasso. Indeed, as shown in [1] and [2], the Lasso is intimately connected to the LARS algorithm, which is an iterative procedure of variable selection generalizing the concept of bissector in a multidimensional framework. The basic idea consists in writing a variant of the Lasso, where both a variable and a lag are selected at each step, all the other lags of the variables being then eliminated from the possible further selections : we call it the LagLasso procedure. This approach is ﬁnally prospective, in the sense that we would like not only to build a competitive prediction method for S&P 500 but also to clearly state the problem of lag identiﬁcation. This is different from [3], that introduces a LARS algorithm adapted to time series, where each variable can be represented by a matrix made of its lagged realisations : the algorithm manages to select iteratively blocks of lags corresponding to a single variable instead of single lags corresponding each to a variable. In the next section, a mathematical framework of this approach is given. The way linear state-space models are used to denoise variables is explained as well as the modeling of S&P 500 variations through the LagLasso procedure. In the last section, 1 a backtesting of this method is provided : it is built on a sample period of 20 years, uses a gliding window of 5 years and a small number of macro-economic and ﬁnancial indicators. 2 Presentation of the prediction method More formally, we observe the predictors xt = (xi,t)1≤i≤d, where x.,t ∈ Rd, ∀t and we want to forecast the real variable yt at horizon h. The forecasting linear model is proposed : ˜yt+h = d∑ i=1 βi ˜xi,t−σ(i), (∗) where ˜yt+h and ˜xi,t are the innovation residuals of the variable xi,t corresponding, for each i, to a given linear state space model, where σ(i) is a lag corresponding to the ith variable, and where β = (β1, . . . , βd) is a real vector. 2.1 First step : Kalman ﬁltering We propose the following linear state space model : { zt = Htθt + vt, vt ∼ N (0, Vt), θt = Ftθt−1 + wt, wt ∼ N (0, Wt), θ0 ∼ N (m0, V0), where zt ∈ Rm stands for the observation vector, θt ∈ Rp is a hidden random vector, Ht and Ft are real matrices of size respectively m ∗ p and p ∗ p, and are to be speciﬁed. The only parameters of the model are the observation and evolution variances V (matrice of size m ∗ m) and W (matrice of size p ∗ p), that we estimate from available data using maximum likelihood. The Kalman ﬁlter recursively estimates the internal state of the process θt given the sequence of noisy observations zt. We denote by ˆθt|t the estimate of the state at time t given observations up to and including time t , and by Pt|t the associated error covariance matrix. This can be summed up by the system of equations :    ˆθt|t−1 = Ft ˆθt−1|t−1, (∗∗) Pt|t−1 = FtPt−1|t−1F ′ t + Wk−1, rt = zt − Ht ˆθt|t−1, (∗ ∗ ∗) St = HtPt|t−1H ′ t + Vt, Kt = Pt|t−1H ′ tS−1 t , ˆθt|t = ˆθt|t−1 + Ktrt, Pt|t = (I − KtHt)Pt|t−1, Equation (**) gives the predicted state at step t and equation (***) the innovation residual : this is the way we compute the quantities ˜xt and ˜yt stated in (*). Finally, in our implementation, we use such a model for the response yt and a single such model for all predictors xi,t, for simplicity of use. 2.2 Second step : selecting variables and lags with LagLasso Predicting yt+h is achieved by selecting the most signiﬁcant variables and lags, knowing that only one lag can be chosen per variable. We implemented a Lasso-type procedure : the LagLasso, which aims at building the vector β given in (*). From now on, we use the notation xi for ˜xi,t and y for ˜yt and we use a double index for β to account for the variables and the lags. In addition, as for the Lasso, it is necessary to offer some criterion to choose a single step in this iterative process that determines a single vector ˆβ : both Cp-type and cross-validation stopping criteria were considered. 3 Results and backtesting In order to question the validity of this method and to explore possible reﬁnements, several simple test methods are given. All of them are based on the same principle : considering the last 20 years of 2 LagLasso steps. 0. Choose lagmax and lagmin : σi ∈ [lagmin, lagmax], ∀i. 1. Standardization of the predictors xi,σ to have mean 0 and variance 1. Initialisation : r = y − y, βi,σ = 0, ∀i, σ. 2. Find the predictor xj,σ most correlated with r. 3. Move βj,σ from 0 towards its least-squares coefﬁcient ⟨xj,σ, r⟩, until some other competitor xk,τ , k ̸= j, has as much correlation with the current residual as does xj,σ. 4. Move (βj,σ, βk,τ ) in the direction deﬁned by their joint least squares coefﬁcient of the current residual on (xj,σ, xk,τ ), until some other competitor xl,υ has as much correlation with the current residual, i.e. : < xl,υ, r >=< xk,τ , r >=< xj,σ, r >. 5a. If a non-zero coefﬁcient hits zero, drop it from the active set, reinclude the variable and all its lags in the inactive set and recompute the current joint least squares direction. 5b. Eliminate all the lags corresponding to variable j from the inactive set. Continue until d variables are entered. S&P 500 monthly variations, we use a gliding window containing a sufﬁcient and constant number of points to make a prediction of the variation of the S&P 500 index over the next month. A number of successive predictions at horizon h = 1 month are obtained and compared with those computed with other methods, linear state-space models and regression particularly. Obviously, some explicative variables are needed and they have been chosen carefully : we checked that having too much correlated variables in the data basis is usually very counterproductive, which ﬁnally drastically limits the number of explicative variables. With the help of an economic expert, we chose PER, OIL, NAPM, INCOME and CORP PROFIT, that are all available on the website of the Federal Reserve Bank of St. Louis. A ﬁrst backtesting of this method consists in computing a recognition rate of upward and downward movements of the S&P 500 depending on the amplitude of the variation of the index. Results are provided for different maximal lags and for some other methods (cf. Table 1). In addition, the following naive trading rule is proposed. Imagine a trader that decides to sell or to buy one unit of S&P 500 index every month. If the prediction of the model for next month is positive, the trader buys; if it is negative, he sells. At the end of the backtesting period, proﬁt and loss accounts - computed with different maximal lags and following similar strategies derived from other methods - are compared (cf. Figure 1). 4 Conclusion A ﬁrst conclusion is that a multidimensional framework is usually more interesting than an unidi- mensional one. Furthermore, combining a ﬁltering method with a selection method gives promising results : a simple state-space model combined with the Lasso outperforms all the other backtested methods. This has to be tempered by the delicate calibration of the model : if the database contains too much correlated variables, the results (recognition rate and proﬁt and loss account) are clearly worse. And since macro-economic and ﬁnancial variables usually strongly depend on each other, this limits the number of predictors in the database. Unfortunately, taking lags into account does not improve signiﬁcantly neither the recognition rate nor the proﬁt and loss accounts, although it is a phenomenon highlighted by economists. We believe further improvements can be reached through a better indexing of data. Table 1: recognition rate of S&P 500’s upward and downward movements. Amplitude of the variation Kalman and LagLasso, lagmax = 1 Kalman and LagLasso, lagmax = 6 Kalman and LagLasso, lagmax = 12 Level Model Local Trend Model Lasso Regression 0 61.6% 60% 62.2% 53.8% 56.1% 61.6% 61.6% 0.01 62.5% 59.8% 61.8% 55.2% 56.5% 61.8% 61.8% 0.02 64.4% 58.8% 60.7% 53.2% 58.8% 60.7% 62.6% 0.03 64.6% 62.1% 64.6% 53.6% 58.5% 60.7% 62.1% 0.04 66% 66% 66% 50.9% 64.1% 59.7% 62.2% 0.05 66.6% 71.7% 71.7% 51.2% 71.7% 58.4% 56.4% 3 Figure 1: Backtesting KL stands for Kalman/LagLasso-type methods(lagmax = 1, 6, 12), SSM for linear State-Space Models (Level Model and Local Trend Model) and Reg for both Regression and Lasso. 5 References [1] Efron, B.; Hastie, T.; Johnstone, I. and Tishirani, R. (2004), \"Least Angle Regression,\" The Annals of Statistics, 32, 407-451. [2] Hastie, T.; Taylor, J.; Tibshirani, R; and Walther, G. (2007), \"Forward stagewise regression and the mono- tone lasso\", Electron. J. Statist., 1, 1-29. [3] Croux, C; Gelper S. (2008), \"Least Angle Regression for Time Series Forecasting with Many Predictors\", 1-36 pp. Leuven: K.U.Leuven. [4] Kalman, R. (1960), \"A new approach to linear ﬁltering and prediction problem\", Trans ASME Journal of Basic Engineering, 35-45. [5] Ruey, T., (2005), \"Analysis of Financial Time Series\", Wiley. [6] Welch, G.; Bishop, G., \"An Introduction to the Kalman Filter\", University of North Carolina, Department of Computer Science, TR 95-041. 1995. [7] Cappe, O., Moulines, E., Ryden, T. (2005), \"Inference in Hidden Markov Models\", Springer Series in Statistics, Springer-Verlag New York, Inc., Secaucus, NJ. [8] Schleifer, A. (2000), \"An Introduction to Behavioral Finance\", Oxford University Press. 4","libVersion":"0.3.2","langs":""}
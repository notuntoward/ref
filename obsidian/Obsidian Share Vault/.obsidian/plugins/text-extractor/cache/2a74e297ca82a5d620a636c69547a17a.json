{"path":"lit/sources/tmp_papers/Stein19autoConfigDeepNN_MIP-EGO.pdf","text":"Automatic Conﬁguration of Deep Neural Networks with Parallel Efﬁcient Global Optimization Bas van Stein * 1 Hao Wang * 1 Thomas B¨ack 1 Abstract Designing the architecture for an artiﬁcial neural network is a cumbersome task because of the nu- merous parameters to conﬁgure, including activa- tion functions, layer types, and hyper-parameters. With the large number of parameters for most net- works nowadays, it is intractable to ﬁnd a good conﬁguration for a given task by hand. In this paper an Efﬁcient Global Optimization (EGO) al- gorithm is adapted to automatically optimize and conﬁgure convolutional neural network architec- tures. A conﬁgurable neural network architecture based solely on convolutional layers is proposed for the optimization. Without using any knowl- edge on the target problem and not using any data augmentation techniques, it is shown that on several image classiﬁcation tasks this approach is able to ﬁnd competitive network architectures in terms of prediction accuracy, compared to the best hand-crafted ones in literature. In addition, a very small training budget (200 evaluations and 10 epochs in training) is spent on each optimized architectures in contrast to the usual long training time of hand-crafted networks. Moreover, instead of the standard sequential evaluation in EGO, sev- eral candidate architectures are proposed and eval- uated in parallel, which saves the execution over- heads signiﬁcantly and leads to an efﬁcient au- tomation for deep neural network design. 1. Introduction Deep Artiﬁcial Neural Networks and in particular Convo- lutional Neural Networks (CNN) have demonstrated great performance on a wide range of difﬁcult computer vision, classiﬁcation and regression tasks. One of the most promis- ing aspects of using deep neural networks is that feature *Equal contribution 1LIACS, University of Leiden, Leiden, The Netherlands. Correspondence to: Bas van Stein <b.van.stein@liacs.leidenuniv.nl>, Hao Wang <h.wang@liacs.leidenuniv.nl>. Copyright 2018 by the author(s). extraction and feature engineering, which was mostly done by hand so far, now is completely taken care of by the networks themselves. Unfortunately, the design and conﬁg- uration of the artiﬁcial neural networks are still derived by hand using either an educated guess, by popularity (using an architecture from previous literature) or by trying a grid of different architectures and parameters and then choosing the best performing network. Since the number of choices for a network architecture and its parameters can become quite large, an optimal deep neural network for a given prob- lem is very unlikely to be obtained using this hand-crafted procedure. The challenges in conﬁguring CNNs are: 1) the search space is usually high dimensional and heterogeneous, re- sulting from a large number of structure choices (e.g., num- ber of layers, layer type, etc.) and real parameters. 2) the computational time becomes the bottleneck when ﬁt- ting a deep network structure to a relatively large data set. Those difﬁculties hinder the applicability of the traditional nonlinear black-box optimizers, for instance Evolutionary Algorithms (Stanley & Miikkulainen, 2002). Instead, it is proposed here to adopt the so-called Efﬁcient Global Opti- mization (Moˇckus, 1975; 2012; Jones et al., 1998) (EGO) algorithm as the network conﬁgurator. The standard EGO algorithm is a sequential strategy designed for the expensive evaluation scenario, where a single candidate conﬁguration is provided in each iteration. It is proposed to adapt the EGO algorithm to yield several candidate conﬁgurations in each iteration where the resulting conﬁgurations can be evaluated in parallel. This paper is organized as follows. In section 2, the related approaches on network conﬁguration are discussed. In sec- tion 3, we introduce the All-CNN conﬁguration framework, using only convolutional layers, and the EGO-based con- ﬁgurator is explained in section 4. The proposed method is validated and tested in sections 5 and 6, followed by the demonstration of an application on a real-world problem. 2. Related Research The optimization of hyper-parameters is a very known chal- lenge and has been addressed in many works. For example,arXiv:1810.05526v1 [cs.LG] 10 Oct 2018 Automatic Conﬁguration of Deep Neural Networks with EGO (Bergstra & Bengio, 2012) shows that random chosen tri- als are more efﬁcient than using grid search to perform hyper-parameter optimization. Obviously, both random and grid search are far from optimal, and more sophisticated methods are required to search the very large and com- plex search space for optimizing deep artiﬁcial neural net- works. More recent work of the same author (Bergstra et al., 2013) shows that automatic hyper-parameter tuning can yield state-of-the-art result, In these papers, architectures are used that are known to work on a speciﬁc problem and are then ﬁne-tuned by hyper-parameter optimization. Some other sophisticated algorithms to perform parameter tuning and automated machine learning conﬁguration are Bayesian Optimization (Snoek et al., 2012; Jones et al., 1998), Evolu- tionary Algorithms (Loshchilov & Hutter, 2016) and SMAC (Hutter et al., 2011a), which try to quickly converge to prac- tical well-performing hyper-parameters for a given machine learning algorithm. Unfortunately, even with these sophisticated algorithms, op- timization of the deep neural network architecture itself, in addition with its hyper-parameters, is a very challenging task. This is caused by the time complexity and compu- tational effort that is required to train these networks, in combination with the size of the search space of hyper- parameters for such networks. Automatically optimizing the structure of an artiﬁcial neural network is not an en- tirely new idea though, as already in 1989 (Miller et al., 1989) genetic algorithms were proposed to optimize the links between a predeﬁned number of nodes. A bit later, an evolutionary program (GNARL) was proposed to evolve the structure of recurrent neural networks (Angeline et al., 1994). In another, more recent work (Ritchie et al., 2003), Genetic Programming (GP) is used for the automatic con- struction of neural network architectures. One of the main bottlenecks with the already proposed meth- ods though is that a single evaluation of an artiﬁcial neural network can take several hours, on a modern GPU system. This makes it infeasible to apply these algorithms with a large evaluation budget or on a large problem instance. Un- fortunately, these algorithms usually require a large evalua- tion budget to ﬁnd well performing network conﬁgurations for a speciﬁc problem. Another challenge is to deﬁne a bounded search space that still covers most of the possibili- ties in order to ﬁnd the optimum. When dealing with neural network structures this is far from simple. The number of layers for example could be a problematic parameter to vary, since each layer comes with its own set of hyper-parameters. To alleviate this problem, a generic conﬁgurable deep neu- ral network architecture is proposed in this paper. This architecture is highly conﬁgurable with a large number of parameters and can represent very shallow to very deep con- volutional neural networks. The conﬁgurable architecture has a ﬁxed number of hyper-parameters and is therefore very suitable for optimization. To tackle this optimization task, the well-known Efﬁcient Global Optimization algo- rithm (Moˇckus, 1975; 2012; Jones et al., 1998) is adopted with several important improvements, enabling the parallel training of different network candidates. The main advan- tages of the proposed approach are: 1. Small optimization time: it requires by far less real evaluations (training of candidate networks) than other approaches. 2. Parallelism: several candidate networks are suggested in each iteration, facilitating parallel execution over multiple GPUs. 3. A Conﬁgurable All-Convolutional Neural Network In order to optimize the structure and hyper-parameters of a deep neural network, a few modeling decisions are required to set the boundaries of the search space. The complexity of the search space is mostly due to a large number of different layer types, activation functions and regularization methods, each coming with their own set of hyper-parameters. In order to reduce the complexity of the search space without making too many modeling assumptions, a generic conﬁg- urable convolutional neural network designed for any image classiﬁcation problem is proposed here. According to (Springenberg et al., 2014), using only convo- lutional neural network layers can give the same or better performance as using the often used structure of convolu- tional layers followed by a pooling layer. Therefore, for our generic conﬁgurable network structure, we have chosen to use only convolution layers with the exception of the ﬁnal layer. The conﬁgurable network architecture is shown in Table 1 where each of the q stacks has an architecture as shown in Figure 1. The network consists of multiple of these stacks, that each consist of a number of convolutional layers and a convolutional layer with strides (Conv2D-Out), to allow for pooling, and a dropout layer. The last part of the network uses either global average pooling or not, and ends in a dense layer with the size of the number of classes one wants to predict. Each stack has 7 independent conﬁgurable parameters and 2 shared parameters that can be optimized. The convolutional layers in the stack have the parameters f, k, l2 and s, which are the number of ﬁlters f , the kernel size k, the l2 regularization factor for the weights, and for the Conv2D-Out layer the strides s, respectively. The parameter a stands for the conﬁgurable activation function and every dropout has its own dropout probability (d). The Automatic Conﬁguration of Deep Neural Networks with EGO last dense layer has l2 and a as conﬁgurable parameters. The size of each stack is conﬁgurable as well (n), and allows for very shallow to very deep neural network architectures. All hyper-parameters that are not taken into account for the conﬁguration are set to the values recommended by literature and the padding for each convolution layer is set to ‘same’ in order to avoid negative dimensions. Figure 1. Schematic diagram of the stack structure. Table 1. Generic Conﬁgurable All-CNN structure with q stacks and conﬁgurable parameters per layer. i is the index for the current stack (i = 1, . . . , q). Layer Type Parameters Dropout d0 Conv2D f0, k0, l2, a q Stacks ni× Conv2D fi, ki, l2, a, ni Conv2D-Out fout−i, kout−i, sout−i, l2, a Dropout di Head GlobalPooling boolean Dense l2, aout Next to the parameters of the conﬁgurable network itself (which are 26 when using 3 stacks), there are the learn- ing rate (lr) and decay rate (λ) for the back-propagation optimizer. Depending on available resources and the clas- siﬁcation task at hand, the ranges of the parameters can be determined by the user. For this paper, the ranges can be found in Table 2. The optimizer used for back-propagation is the well-known stochastic gradient descent (SGD), pro- vided by the Keras (Chollet et al., 2015) python library. Table 2. Ranges of the search space dimensions Parameter Range a {elu, relu, tanh, selu, sigmoid} f [1..512] k [1..8] s [1..5] n [1..6] d [10 −5, 0.8] l2 [10 −5, 10−2] lr [10 −5, 1.0] 4. Efﬁcient Global Optimization based Conﬁgurator The search space of the All-CNN framework is heteroge- nous and high dimensional. For the integer parameters, in case of three stacks, there are seven for the number of ﬁlters (f ), seven for the kernel size (k), three for strides (s) and three for the number of layers (n) in the stack, and thus 20 in total. For the discrete parameters, there are two for the activation functions (a) of the stack and the head and for the real parameters, there are four parameters for the dropout rate (d), one for regularization (l2) and one for the learning rate (lr). In addition, we have one boolean variable to control the global pooling. Therefore, this search space can be represented as: C = R6 × Z 20 × B × D2, where B = {0, 1} and D = {elu, relu, tanh, selu, sigmoid}. The convolutional neural network can be instantiated by drawing samples in C. Given a data set, the problem arises in ﬁnding the optimal conﬁguration, with respect to a pre- deﬁned, real-valued performance metric f of the neural network (for instance, f can be set to r2 for regression tasks and precision for classiﬁcation problems): f : C → R. In the following discussion, it is assumed that the performance metric f is subject to minimization, without loss of general- ity (the maximization problem can be easily converted). The challenge in optimizing f is the evaluation time of itself, which will be extremely expensive when training a large network structure on a huge data set. Consequently, it is rec- ommended to use efﬁcient optimization algorithms that can save as many evaluations as possible. The efﬁcient global optimization (EGO) algorithm (Moˇckus, 1975; 2012; Jones et al., 1998) is a suitable candidate algorithm for this task. It is a sequential optimization strategy that does not require the derivatives of the objective function and is designed to tackle expensive global optimization problems. Compared to alternative optimization algorithms (or other design of experiment methods), the distinctive feature of this method is the usage of a meta-model, which gives the predictive distribution over the (partially) unknown function. Automatic Conﬁguration of Deep Neural Networks with EGO Brieﬂy, this optimization method iteratively proposes new candidate conﬁgurations over the meta-model, taking both the prediction and model uncertainty into account. After the evaluation of the new candidate conﬁgurations, the meta- model will be re-trained. 4.1. Initial Design and Meta-modeling To construct the meta-model, some initial samples in the conﬁguration space, X = {x(1), x(2), . . . , x (n)} ⊂ C are generated via the Latin hypercube sampling (LHS) (McKay et al., 1979). The corresponding performance metric values are obtained by instantiating the network and validating its performance on the data set: Y = {y(1), y(2), . . . , y(n)} = {f (x(1)), f (x(2)), . . . , f (x(n))}. Note that the evaluation of the initial designs can be easily parallelized. For the choice of meta-models, although Gaussian process regres- sion (Sacks et al., 1989; Santner et al., 2003) (referred to as Kriging in geostatistics (Krige, 1951)) is frequently used in EGO, we adopt the random forest instead, due to the fact that it is more suitable for a mixed integer conﬁguration domain (Hutter et al., 2011b). In the following discussions, the prediction on conﬁguration x is denoted as m(x). In ad- dition, the empirical variance of the prediction ˆs 2(x) is also calculated from the forest, which quantiﬁes the prediction uncertainty. 4.2. Inﬁll-Criterion To propose potentially good conﬁgurations in each iteration, the so-called inﬁll-criterion is used to quantify the quality criterion of the conﬁgurations. Informally, inﬁll-criteria work in a way that predicted values from the meta-model and the prediction uncertainty are balanced. A lot of research effort has been put over the last decades in exploring various inﬁll-criteria, e.g., Expected Improvement (Moˇckus, 1975; Jones et al., 1998), Probability of Improvement (Jones, 2001; ˇZilinskas, 1992) and Upper Conﬁdent Bound (Auer, 2002; Srinivas et al., 2010). In this contribution, we adopt the so-called Moment-Generating Function (MGF) based inﬁll- criterion, as proposed in (Wang et al., 2017). This inﬁll- criterion allows for explicitly balancing exploitation and exploration. This criterion has a closed form and can be expressed as: M(x; t) = Φ ( ymin − m′(x) s(x) ) · exp ((ymin − m(x) − 1) t + ˆs 2(x) 2 t2) (1) m ′(x) = m(x) − ˆs 2(x)t, where ymin = min{y(1), y(2), . . . , y(n)} is the current best performance over all the evaluated conﬁgurations and Φ(·) stands for the cumulative distribution function of the stan- dard normal distribution. The inﬁll-criterion M introduces an additional real parameter t (“temperature”) to explicitly control the balance between exploration and exploitation. As explained in (Wang et al., 2017), when t goes up, M tends to reward the conﬁgurations with high uncertainty. On the contrary, when t is decreased, M puts more weight on the predicted performance value. It is then possible to set the t value according to the budget on the conﬁguration task: with a larger budget of function evaluations, t can be set to a relatively high value, leading to a slow but global search process and vice versa for a smaller budget. 4.3. Parallel execution Due to the typically large execution time of instantiated network structures, it is also proposed here to parallelize the execution. This requires generating more than one candidate conﬁguration in each iteration. Many methods are developed for this purpose, including multi-point Ex- pected Improvement (Ginsbourger et al., 2010) and Niching techniques (Wang et al., 2018). Here, we adopt the ap- proach in (Hutter et al., 2012), where q (> 1) different temperatures t are sampled from the log-normal distribution Lognormal(0, 1) and q different M criteria are instanti- ated using the temperatures accordingly. Consequently, q candidate conﬁgurations can be obtained by maximizing those q inﬁll-criteria. On one hand, as log-normal is a long- tailed distribution, most of the t values are realized relatively small and thus the model prediction is well exploited. On the other hand, only a few t samples will be relatively high and therefore will lead to very explorative search behavior. To maximize the inﬁll-criterion on the mixed-integer search domain, we adopt the so-called Mixed-Integer Evolution Strategy (MIES) (Li et al., 2013). The proposed Bayesian conﬁgurator is summarized in Algorithm 1. Algorithm 1 EGO Conﬁgurator 1: Generate the initial design X using LHS. 2: Construct the initial random forest on (X, Y ). 3: while the stopping criterion is not fulﬁlled do 4: for i = 1 → q do 5: ti ← Lognormal(0, 1) 6: Maximize the inﬁll-criterion using Mixed integer Evolution Strategy: x′ i = arg max x∈C M(x; ti) 7: end for 8: Parallel training and performance assessment for all {x ′ i}q i=1: y′ i ← f (x ′ i). 9: Append {x′ i, y′ i} q i=1 to (X, Y ). 10: Re-train the random forest model of f on the aug- mented data set (X, Y ) 11: end while Automatic Conﬁguration of Deep Neural Networks with EGO 5. Experiments To test our algorithm, two very popular and common classi- ﬁcation tasks have been performed using the proposed con- ﬁgurator and a conﬁgurable network with 3 stacks. These are the MNIST dataset (LeCun et al., 1998), containing 60.000 training samples, and a test set of 10.000 exam- ples, all 28x28 greyscale images, and the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), containing 60.000, 32x32 colour images with 10 classes, divided into 6000 images per class. There are 50.000 training images and 10.000 test images, in this case. In the optimization procedure of the neural network on the MNIST dataset, each evaluation is run for 10 epochs only with a batch size of 100 images. For the CIFAR-10 dataset, the number of epochs is increased to 50, which is still much less than the number of epochs in most recent literature (> 300). An early stopping criterion is used to stop the evaluation of a particular conﬁguration after 6 epochs of no improvements. No data augmentation is used. The Bayesian mixed integer conﬁgurator is set to evaluate 5 network conﬁgurations per step in parallel using NVIDIA K80 GPUs, where the ﬁrst 5 steps are used for the initial LHS design. The test set accuracy is returned after each evaluation as ﬁtness value for the optimizer. 6. MNIST and CIFAR-10 Results In Figure 2 the results of the automatic conﬁguration of the All-CNN networks are shown. In both cases, after approxi- mately 50 evaluations, a well-performing network conﬁgu- ration is obtained. Both classiﬁcation tasks used exactly the same initial conﬁguration, the only difference is the number of epochs for each network evaluation. The best performing conﬁgurations compete with the state- of-the-art as shown in Table 3 and Table 4 and can be pos- sibly improved when trained using more epochs. It should be noted that the number of epochs used to obtain these results is signiﬁcantly lower than the number of epochs in state-of-the-art solutions from literature. The advantage of such a small number of epochs is that it speeds up the entire optimization process. The idea behind this is that well-performing conﬁgurations can be tuned with a larger number of epochs as second optimization step, most likely resulting in increased performance. Using the automatic conﬁgurator we obtained neural network architectures that compete with state-of-the-art results using only 200 · 10 epochs and 200 · 50 epochs in total, without any manual tuning, reconﬁguring or upfront knowledge of the speciﬁc problem instances. While hand-crafted network conﬁgura- tions are not only trained using many more epochs for the ﬁnal reported architecture, they also require a huge amount of time to be constructed by reconﬁguring and ﬁne-tuning the architecture. Therefore, the hand-crafted networks basi- cally use many more more epochs until the ﬁnal architecture is reached. Table 3. MNIST Performance from literature. Test error Algorithm Epochs 0.23% (Ciregan et al., 2012) 800 0.32% (Graham, 2014) 250 0.61% Optimized All-CNN 10 0.71% (Yang et al., 2015) unknown Table 4. CIFAR-10 Performance from literature. Accuracy Algorithm Epochs 95.59% (Graham, 2014) 250 95.59% (Springenberg et al., 2014) 350 86.46% Optimized All-CNN 50 84.87% (Zeiler & Fergus, 2013) 500 7. Real World Problem: Tata Steel The proposed algorithm is applied on the real world prob- lem of classifying defects during the hot rolling process of steel. This industrial process is very complex with many conditions and parameters that inﬂuence the ﬁnal product. It is also a process that changes over the years and requires dealing with concept shift and concept drift. One of the main objectives for Tata Steel is to automatically classify and predict surface defects using material properties and machine parameters as input data. To achieve this objective, ﬁrst, a deep neural network architecture is designed by hand to classify these defects. The Tata Steel data set consists of various material measure- ments and machine parameters. Most of the measurements are measured over the complete length of each coil but not over the width of the coil (since the width is much smaller). However, the temperature measurements are taken over sev- eral tracks in the width of the coil as well. Due to this spatial difference, it was decided to design two concatenated net- work architectures. One part of the architecture is based purely on the temperature data, allowing for the application of convolution layers in the width and length direction of the coil. The second component is used for modeling the remaining measurements and machine parameters where the convolution ﬁlters only work in the length of the coil. In the end of the design process, these two parts are merged into one ﬁnal fully-connected output layer. The initial design process of these architectures was mainly based on trial and error and recommendations from litera- ture. The design process started with a small, relative simple, two-layer multi-perceptron, and adding additional dense and Automatic Conﬁguration of Deep Neural Networks with EGO (a) MNIST, 200 evaluations (b) CIFAR-10, 200 evaluations Figure 2. a) The left plot shows the optimization run on MNIST, plotting the test accuracy (black dots) of 200 evaluations using 10 epochs. The 20-moving average is depicted by the green line. b) The right plot shows similar results on the CIFAR-10 classiﬁcation task using 50 epochs per evaluation. convolution layers in order to increase the ﬁnal accuracy. Dropout is being applied to prevent over-ﬁtting, and after several manual iterations a dropout rate of 0.2 seemed to work best. Next, we applied a slightly modiﬁed version of the proposed conﬁgurable all-CNN network (with a separate stack for the temperature data before concatenating it to the main model) and automatically optimized the conﬁguration. The optimal conﬁguration obtained by using our optimization procedure signiﬁcantly improves the classiﬁcation accuracy. It also allows for easy retraining and validation on future data, since almost zero knowledge of the actual dataset is required to train and optimize the network architecture. (a) ROC of hand-constructed classiﬁer. (b) ROC of optimized All-CNN. Figure 3. a) The left plot shows the ROC curve of the classiﬁcation task for Tata Steel with a hand-crafted neural network architecture. b) The right plot shows the ROC curve for the same task but now using the optimized neural network architecture using 200 evaluations for the optimization and only 5 epochs per evaluation. The test set accuracy of the hand-designed classiﬁer and the optimized classiﬁer for this real world application is shown in Figure 3. It can be observed that the optimized classiﬁer has a signiﬁcantly improved accuracy on this speciﬁc defect type, with an almost 90% true positive rate with only a very small (0.5%) false positive rate. This shows that the opti- mization procedure and conﬁgurable network architecture has great potential for industrial applications. 8. Conclusions and Outlook A novel approach based on Efﬁcient global optimization algorithm is proposed to automatically conﬁgure the neural networks architecture. On some well-known image classiﬁ- cation tasks, it is observed that the proposed optimization approach is capable of generating well-performing networks with a limited number of optimization iterations. In addition, the resulting optimized neural networks are also highly com- petitive with the state-of-the-art manually designed ones on the MNIST and CIFAR-10 classiﬁcation task. Note that such performance of the optimized network are achieved under a very small number of epochs (10 for MNIST, and 50 for CIFAR-10) for training, without any knowledge on the classiﬁcation task or data augmentation techniques. As for the real-world problem, we have applied the pro- posed approach on the challenge of steel surface detection. The outcome clearly illustrates that the proposed conﬁgu- ration approach also works extremely well. The accuracy of the optimized network that detects the surface defect for Tata Steel is signiﬁcantly higher than the accuracy of the network designed by hand, which is obtained with manual ﬁne-tuning. For the next step, there are several possibilities to work on. Automatic Conﬁguration of Deep Neural Networks with EGO First, the proposed approach will be applied and tested on various modeling tasks and real-world problems. Second, the actual training time of the candidate network will be taken into account explicitly. The trade-off between train- ing time and accuracy can be controlled by optimizing the epochs and batch size. Additionally, it is also interesting to formulate this as a bi-criteria decision making problem, with one objective being the accuracy of the network and the other objective the training time required. Third, we will investigate how to extend the current conﬁgurable net- work that has a linear topology, to more general topological structures. In this case, it will be very challenging to search efﬁciently in the complex conﬁguration space with multiple dependencies. Acknowledgment The authors acknowledge support by NWO (Netherlands Organization for Scientiﬁc Research) PROMIMOOC project (project number: 650.002.001). References Angeline, Peter J, Saunders, Gregory M, and Pollack, Jor- dan B. An evolutionary algorithm that constructs re- current neural networks. IEEE transactions on Neural Networks, 5(1):54–65, 1994. Auer, Peter. Using conﬁdence bounds for exploitation- exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422, 2002. Bergstra, James and Bengio, Yoshua. Random search for hyper-parameter optimization. Journal of Machine Learn- ing Research, 13(Feb):281–305, 2012. Bergstra, James, Yamins, Daniel, and Cox, David. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International Conference on Machine Learning, pp. 115– 123, 2013. Chollet, Franc¸ois et al. Keras. https://github.com/ keras-team/keras, 2015. Ciregan, Dan, Meier, Ueli, and Schmidhuber, J¨urgen. Multi- column deep neural networks for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3642–3649. IEEE, 2012. Ginsbourger, David, Le Riche, Rodolphe, and Carraro, Laurent. Kriging Is Well-Suited to Parallelize Opti- mization, pp. 131–162. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. ISBN 978-3-642-10701-6. doi: 10.1007/978-3-642-10701-6 6. URL https://doi. org/10.1007/978-3-642-10701-6_6. Graham, Benjamin. Fractional max-pooling. arXiv preprint arXiv:1412.6071, 2014. Hutter, Frank, Hoos, Holger H., and Leyton-Brown, Kevin. Sequential Model-Based Optimization for General Al- gorithm Conﬁguration, pp. 507–523. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011a. ISBN 978-3-642- 25566-3. doi: 10.1007/978-3-642-25566-3 40. Hutter, Frank, Hoos, Holger H, and Leyton-Brown, Kevin. Sequential model-based optimization for general algo- rithm conﬁguration. LION, 5:507–523, 2011b. Hutter, Frank, Hoos, Holger, and Leyton-Brown, Kevin. Parallel algorithm conﬁguration. Learning and Intelligent Optimization, pp. 55–70, 2012. Jones, Donald R. A taxonomy of global optimization meth- ods based on response surfaces. Journal of global opti- mization, 21(4):345–383, 2001. Jones, Donald R, Schonlau, Matthias, and Welch, William J. Efﬁcient global optimization of expensive black-box func- tions. Journal of Global optimization, 13(4):455–492, 1998. Krige, Daniel G. A Statistical Approach to Some Basic Mine Valuation Problems on the Witwatersrand. Journal of the Chemical, Metallurgical and Mining Society of South Africa, 52(6):119–139, December 1951. Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. 2009. LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Li, Rui, Emmerich, Michael TM, Eggermont, Jeroen, B¨ack, Thomas, Sch¨utz, Martin, Dijkstra, Jouke, and Reiber, Jo- han HC. Mixed integer evolution strategies for parameter optimization. Evolutionary computation, 21(1):29–64, 2013. Loshchilov, Ilya and Hutter, Frank. Cma-es for hyperparam- eter optimization of deep neural networks. arXiv preprint arXiv:1604.07269, 2016. McKay, M. D., Beckman, R. J., and Conover, W. J. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 21(2):239–245, 1979. ISSN 00401706. URL http://www.jstor.org/ stable/1268522. Miller, Geoffrey F, Todd, Peter M, and Hegde, Shailesh U. Designing neural networks using genetic algorithms. In ICGA, volume 89, pp. 379–384, 1989. Automatic Conﬁguration of Deep Neural Networks with EGO Moˇckus, J. On bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical Conference, pp. 400–404. Springer, 1975. Moˇckus, Jonas. Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media, 2012. Ritchie, Marylyn D, White, Bill C, Parker, Joel S, Hahn, Lance W, and Moore, Jason H. Optimization of neural net- work architecture using genetic programming improves detection and modeling of gene-gene interactions in stud- ies of human diseases. BMC bioinformatics, 4(1):28, 2003. Sacks, Jerome, Welch, William J., Mitchell, Toby J., and Wynn, Henry P. Design and Analysis of Computer Exper- iments. Statistical Science, 4(4):409–423, 1989. Santner, T.J., Williams, B.J., and Notz, W. The Design and Analysis of Computer Experiments. Springer Series in Statistics. Springer, 2003. ISBN 9780387954202. Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practi- cal bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pp. 2951–2959, 2012. Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin. Striving for sim- plicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M., and Seeger, Matthias. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental De- sign. Proceedings of the 27th International Conference on Machine Learning (ICML 2010), pp. 1015–1022, 2010. ISSN 00189448. doi: 10.1109/TIT.2011.2182033. URL http://arxiv.org/abs/0912.3995. Stanley, Kenneth O and Miikkulainen, Risto. Evolving neural networks through augmenting topologies. Evolu- tionary computation, 10(2):99–127, 2002. Wang, H., van Stein, B., Emmerich, M., and B¨ack, T. A new acquisition function for bayesian optimization based on the moment-generating function. In 2017 IEEE Inter- national Conference on Systems, Man, and Cybernetics (SMC), pp. 507–512, Oct 2017. doi: 10.1109/SMC.2017. 8122656. Wang, Hao, B¨ack, Thomas, and Emmerich, Michael T. M. Multi-point efﬁcient global optimization using niching evolution strategy. In Tantar, Alexandru-Adrian, Tan- tar, Emilia, Emmerich, Michael, Legrand, Pierrick, Al- boaie, Lenuta, and Luchian, Henri (eds.), EVOLVE - A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation VI, pp. 146–162, Cham, 2018. Springer International Publishing. ISBN 978-3- 319-69710-9. Yang, Zichao, Moczulski, Marcin, Denil, Misha, de Freitas, Nando, Smola, Alex, Song, Le, and Wang, Ziyu. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1476–1483, 2015. Zeiler, Matthew D and Fergus, Rob. Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557, 2013. ˇZilinskas, Antanas. A review of statistical models for global optimization. Journal of Global Optimization, 2(2):145– 153, 1992.","libVersion":"0.3.1","langs":""}
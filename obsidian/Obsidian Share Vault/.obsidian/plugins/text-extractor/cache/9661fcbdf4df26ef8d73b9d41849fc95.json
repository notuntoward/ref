{"path":"lit/lit_sources/Dahl24LargeLegalFictions.pdf","text":"Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models Matthew Dahl Yale University matthew.dahl@yale.edu Varun Magesh Stanford University vim@stanford.edu Mirac Suzgun Stanford University msuzgun@stanford.edu Daniel E. Ho Stanford University deho@stanford.edu Abstract Large language models (LLMs) have the po- tential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations—responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations us- ing an original suite of legal queries, comparing LLMs’ responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked spe- cific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user’s incorrect legal assump- tions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations. Taken together, these findings caution against the rapid and un- supervised integration of popular LLMs into legal tasks. Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most—pro se litigants or those without access to traditional legal resources. 1 1 Introduction The legal industry is on the cusp of a technologi- cal metamorphosis, driven by recent advancements in AI—particularly in the form of large language models (LLMs). As these AI models have shown increasing proficiency in law-related tasks, such as first-year law school exams (Choi et al., 2022), the uniform bar exam (OpenAI, 2023b), statutory reasoning (Blair-Stanek et al., 2023), and issue- rule-application-conclusion (IRAC) analysis (Guha 1 All our code, raw data, prompts, and results are available at: https://github.com/reglab/legal_hallucinations. Figure 1: Hallucinations are common across all the LLMs we test when they are asked a direct, verifiable question about a federal court case. (Figure pools all reference-based tasks.) et al., 2023), many have started asking whether AI tools and systems might soon displace human lawyers altogether (Bommasani et al., 2022; Perl- man, 2023, inter alia). Despite their potential to re- form how legal work is performed, however, there remains a critical challenge to LLMs’ widespread adoption: the issue of hallucinations. LLMs like ChatGPT can sometimes generate text that is incon- sistent with current legal doctrine and case law, and in the legal field, where adherence to the source text is paramount, unfaithful or imprecise interpre- tations of law can lead to nonsensical—or worse, harmful and inaccurate—legal advice or decisions. In this work, we present the first evidence docu- menting the nature and frequency of hallucinations in the legal domain. While anecdotal discussion of this problem has surfaced in national media—for instance, the New York Times reported on a lawyer who faced sanctions for using ChatGPT-generated fictional case citations in a brief (Weiser, 2023), and SCOTUSBlog highlighted ChatGPT’s misin- formation regarding a supposed dissent by Justice Ruth Bader Ginsburg in the landmark gay rights case Obergefell v. Hodges (Romoser, 2023)—a sys- 1arXiv:2401.01301v1 [cs.CL] 2 Jan 2024 tematic analysis of this issue has been lacking. Our study seeks to fill this gap, providing insights essen- tial for evaluating LLMs’ effectiveness in general legal settings. Hallucinations have previously been studied in settings such as neural machine translation and abstractive summarization, as well as general- knowledge question-answering (QA) and dialogue generation (Ji et al., 2023; Mündler et al., 2023; Manakul et al., 2023; Dhuliawala et al., 2023). We advance this literature by focusing on open-domain hallucination in the legal context, where factual and textual accuracy are crucial. Traditionally, a key challenge in studying this mode of hallucina- tion has been obtaining the appropriate source con- tent against which non-factual model output can be judged (Agrawal et al., 2023), but we overcome this problem by leveraging the structured nature of American case law. Because American cases fol- low a standard legal schema (with parties, issues, citations, dispositions, and so forth), we are able to use the tabular metadata that is recorded in legal databases on these dimensions to create knowledge queries that simulate basic legal research tasks for each case. We apply these tasks to a random sample of cases across each level of the federal judiciary— the U.S. District Courts (USDC), the U.S. Courts of Appeals (USCOA), and the U.S. Supreme Court (SCOTUS)—and evaluate them using three popular LLMs: OpenAI’s ChatGPT 3.5, Google’s PaLM 2, and Meta’s Llama 2. Our findings reveal the widespread occurrence of legal hallucinations: When asked a direct, veri- fiable question about a randomly selected federal court case, LLMs hallucinate between 69% (Chat- GPT 3.5) and 88% (Llama 2) of the time (Figure 1). However, we also find that LLMs perform better on cases that are newer, more salient, and from more prominent legal jurisdictions, suggesting that LLMs suffer from a kind of legal “monoculture” (Kleinberg and Raghavan, 2021) that leads them to recite an unduly homogenized notion of the law. We then investigate two additional potential failure points for LLMs, beyond their raw hallucination rates: (1) their susceptibility to contra-factual bias, i.e., their ability to respond to queries anchored in erroneous legal premises (Sharma et al., 2023; Wei et al., 2023), and (2) their certainty in their responses, i.e., their self-awareness of their propen- sity to hallucinate (Kadavath et al., 2022; Xiong et al., 2023; Tian et al., 2023b; Yin et al., 2023; Azaria and Mitchell, 2023). Our results indicate that not only do LLMs often provide seemingly legitimate but incorrect answers to contra-factual legal questions, they also struggle to accurately gauge their own level of certainty without post- hoc recalibration. Overall, we conclude that while these LLMs appear to offer a way to make legal in- formation and services more accessible and afford- able to all, their present shortcomings—particularly in terms of generating accurate and reliable state- ments of the law—significantly hinder this objec- tive. 2 Preliminaries and Background 2.1 Terminology and Notation We first provide a brief overview of language mod- els (LMs) for readers who may not necessarily have a deep technical background. LMs are functions that map text to text: When a user provides a text input (known as a “prompt”), the model produces a text output (referred to as a “response”). If the prompt takes the form of a question, the response can be understood as an answer to that question. An LM generates its response by selecting the most probable sequence of tokens that follow the prompt’s tokens; therefore, it essentially functions as a probability distribution over these tokens. In this work, we focus on “large” language models (LLMs). These are models that contain billions of parameters and are trained on vast corpora reflec- tive of the scope of the Internet.2 Formally, we define an LLM as a function fτ : prompt ↦→ response, where fτ operates by sam- pling language tokens from a conditional probabil- ity distribution Pr[xn|x1, . . . , xn−1] that is learned by optimizing over a training corpus hopefully re- flective of facts about the world. We equate xn with the model’s response and {x1, . . . , xn−1} with the input prompt. τ is a temperature parameter that con- trols the shape of the probability distribution under sampling and is configurable by the user. When τ = 0, the distribution becomes degenerate and the model’s response is theoretically deterministic— thus, the model must always return the most likely next token.3 This deterministic response is typi- cally referred to as the “greedy” response. When τ is high, however, the distribution becomes more uniform and the model’s response becomes more 2 Hence, the largeness of a language model is a dual reference to its parameter count and the scope of its training corpus. 3 In practice, non-determinism may persist due to a model’s implementation details, e.g., the “mixture of experts” archi- tecture (Puigcerver et al., 2023; Chann, 2023). 2 Table 1: Typology of legal hallucinations Domain Objective Legal example Closed-domain response consistency with the prompt Opinion summarization response consistency with the training corpus Creative argumentation Open-domain response consistency with the facts of the world Statement of the law stochastic—this is the default behavior of chatbots like ChatGPT, which are often used to generate diverse and creative responses. 2.2 Hallucination in Language Models LLMs have shown promise on a number of legal re- search and analysis tasks (Blair-Stanek et al., 2023; Choi et al., 2022; Fei et al., 2023; Guha et al., 2023; Nay et al., 2023; OpenAI, 2023a; Trozze et al., 2023), but the problem of legal hallucination has so far only been studied in closed-domain applica- tions, e.g., where a model is used to summarize the content of a given judicial opinion (Deroy et al., 2023; Feijo and Moreira, 2023) or to synthesize pro- vided legal text (Savelka et al., 2023). In this paper, by contrast, we examine hallucination in an open- domain setting, i.e., where a model is tasked with providing an accurate answer to an open-ended le- gal query. This setting approximates the situation of a lawyer or a pro se litigant4 seeking advice from a legal chatbot. In the context of such question-answering (QA) scenarios, the study of hallucinations in LMs is still in its infancy, even outside the legal field. There is no universally accepted definition or classification of LM hallucinations (Ji et al., 2023). However, as Kalai and Vempala (2023) show, LMs that as- sign a positive probability to every response token must hallucinate at least some of the time. A key contribution of our work is thus to propose a typol- ogy of legal hallucinations, which we summarize in Table 1. It is essential to recognize that there are various ways in which an LLM might generate hal- lucinated information, and not all types are equally concerning for legal professionals. First, a model might hallucinate by producing a response that is either unfaithful to or in conflict with the input prompt, a phenomenon referred to as closed-domain hallucination. This is a major concern in tasks requiring a high degree of accu- racy between the response and a long-form input, such as machine translation (Xu et al., 2023) or 4 A pro se litigant is a litigant who is representing themselves, possibly without formal legal training. summarization (Cao et al., 2018). In legal contexts, such inaccuracies are particularly problematic in activities like summarizing judicial opinions, syn- thesizing client intake information, drafting legal documents, or extracting key points from an oppos- ing counsel’s brief. Second, LLMs might also hallucinate by produc- ing a response that either contradicts or does not directly derive from its training corpus. Following Agrawal et al. (2023), we conceptualize this kind of hallucination as one form of open-domain halluci- nation. In general, the output of a language model should be logically derivable from the content of its training corpus, regardless of whether the con- tent of the corpus is factually or objectively true.5 In legal settings, this kind of hallucination poses a special challenge to those aiming to fine-tune the kind of general purpose foundation models that we study in this paper with proprietary, in-house work product.6 For example, firms might have a catalog of internal research memos, style guides, and so forth, that they want to ensure is reflected in their bespoke LLM’s output. At the same time, how- ever, insofar as creativity is valued, certain legal tasks—such as persuasive argumentation—might actually benefit from some lack of strict fidelity to the training corpus; after all, a model that simply parrots exactly the text that it has been trained on could itself be undesirable. Defining the contours of an unwanted “hallucination” in this context re- quires value judgements about the balance between fidelity and spontaneity. Finally, the third way that an LLM can halluci- nate is by producing a response that lacks fidelity to the facts of the world, irrespective of how the LLM 5 For example, if a training corpus consisted of J. K. Rowling’s Harry Potter series, we would expect an LLM to produce the sentence “Tom Marvolo Riddle” in response to a query about Voldemort’s real name. However, if the training cor- pus consisted solely of Jane Austen’s Pride and Prejudice (for instance), we would consider this LLM output to be a hallucination—because there would be no basis in the training data for making such a claim about Voldemort. 6 For example, this kind of firm-specific fine-tuning is the business model of a prominent legal tech startup, Harvey.ai (Ambrogi, 2023). 3 is trained or prompted (Maynez et al., 2020). We consider this to be another type of open-domain hal- lucination, with the key concern being “factuality” in relation to the facts of the world (cf. Wittgenstein, 1998 [1921]). In our context, this is perhaps the most alarming type of hallucination, as it can un- dermine the accuracy required in any legal context where a correct statement of the law is necessary. 2.3 Hallucination Trade-offs In this paper, we investigate only the last kind of hallucination. As mentioned, the first two modes of hallucination are not always problematic in the legal setting: these kinds of hallucinations could actually be somewhat desirable to lawyers if they resulted in generated language that, for example, removed unnecessary information from a given ar- gument (at the expense of being faithful to it) or invented a novel analogy never yet proposed (at the expense of being grounded in the lexicon) (Cao et al., 2022). However, what a lawyer cannot tol- erate is the third kind of hallucination, or factual infidelity between an LLM’s response and the con- trolling legal landscape. In a common law sys- tem, where stare decisis requires attachment to the “chain” of historical case law (Dworkin, 1986), any misstatement of the binding content of that law would make an LLM quickly lose any professional or analytical utility. Focusing on non-factual hallucinations alone, however, comes with certain trade-offs. One of the advantages of our typology is that it makes clear that it may not always be possible to minimize all modes of hallucination simultaneously (Kalai and Vempala, 2023); indeed, reducing hallucinations of one kind may increase hallucinations of another. For example, if a given prompt contains informa- tion that does not conform to facts about the world, then ensuring response fidelity with respect to the former would by definition produce infidelity—i.e., hallucination—with respect to the latter. More generally, although fidelity to the prompt is nec- essary for avoiding closed-domain hallucination, there is an important sense in which prioritizing such behavior might actually induce the kind of open-domain hallucination that we center in this paper. These trade-offs present unavoidable challenges for prospective users of legal LLMs. When re- sponding to a query, should an LLM be skeptical of its prompt or sycophantic to it? If it has been trained on case law from one jurisdiction, should it enforce adherence to that training corpus even when responding about the law in another jurisdic- tion? If facts about the world conflict with each other—as legal rules often do—should the LLM preserve that nuance or refrain from introducing in- formation outside the scope of a query? Questions like these are ultimately questions about which kinds of legal hallucinations are more and less preferable, and they are questions whose answers require both empirical evidence and normative ar- guments. We supply some of the empirics in this paper (see Sections 5.1.6 and 5.2), but stress that the normative considerations are crucial and should be a topic of future legal hallucination research. 3 Profiling Hallucinations Using Legal Research Tasks To empirically assess non-factual hallucination, we adopt a QA framework where the goal is to test an LLM’s ability to produce accurate information in response to different kinds of legal queries. We de- velop fourteen tasks that are representative of such queries, which we group into three categories in order of increasing complexity and list in Figure 2. 3.1 Low Complexity Tasks In the low complexity category, we ask for informa- tion that we consider relatively easy for an LLM to reproduce. The information in this category does not derive from the actual content of a case itself, so it does not require higher-order legal reasoning skills to internalize. Instead, this information is readily available in a case’s caption or its syllabus— standard textual locations whose patterns even non- specialized LLMs should be able to recover. We therefore expect LLMs to perform best on these tasks: Existence: Given the name and citation of a case, ascertain whether the case actually exists. This basic evaluation provides preliminary insights into an LLM’s knowledge of actual legal cases: if it can- not distinguish real cases from non-existent ones, it probably cannot offer detailed case insights. We use only real cases in our prompts, so affirming their existence is the correct answer.7 Court: Given the name and citation of a case, pro- vide the name of the court that ruled on it. This task assesses an LLM’s knowledge about legal jurisdic- tions, an important building block of a case’s prece- 7 In Appendix E.1, we experiment with using fake cases as well. 4 less complex more complex Task Query Method Existence Is {case} a real case? Reference-based Court What court decided {case}? Reference-based Citation What is the citation for {case}? Reference-based Author Who wrote the majority opinion in {case}? Reference-based Disposition Did {case} affirm or reverse? Reference-based Quotation What is a quotation from {case}? Reference-based Authority What is an authority cited in {case}? Reference-based Overruling year What year was {case} overruled? Reference-based Doctrinal agreement Does {case1} agree with {case2}? Reference-based Factual background What is the factual background of {case}? Reference-free Procedural posture What is the procedural posture of {case}? Reference-free Subsequent history What is the subsequent history of {case}? Reference-free Core legal question What is the core legal question in {case}? Reference-free Central holding What is the central holding in {case}? Reference-free Figure 2: Hallucination QA task list. Tasks are sorted in order of increasing complexity. Query wording is paraphrased; see Appendix B for exact prompt used. Method column describes the inferential strategy that we use to estimate a hallucination rate for each task: reference-based tasks use known metadata to assess hallucinations, and reference-free tasks use emergent contradictions to assess hallucinations (see Section 4). dential value. We perform this task across three dif- ferent levels of the federal judiciary: the Supreme Court, whose opinions bind all lower courts; the Courts of Appeals, which operate one level down and set the law for courts within their geographic region; and the District Courts, whose opinions do not carry general lawmaking or binding authority over other courts. Each level of the court system has a different reporter, or the series of volumes that opinions are published in. This is relevant because the reporter is included in the citation that we provide to the LLM, essentially revealing the level of the hierar- chy that an opinion is from. All and only SCOTUS cases are published in the U.S. Reports. Opinions from the thirteen Courts of Appeals are published in the Federal Reporter, and District Court cases are published in the Federal Supplement. Because of this, we expect this task to be more difficult as we descend the hierarchy of courts. There is only one court associated with the U.S. reporter, but 13 associated with the Federal Reporter and 94 asso- ciated with the Federal Supplement. For USCOA cases, we require the name of the specific Court of Appeals, and for USDC cases, we require the name of the specific District Court. Citation: Given a case name, supply the Blue- book citation of the case. This query tests an LLM’s ability to associate a given dispute with its official record in a reporter volume at a particular page, which is the key way in which different opinions reference and link to each other. For USCOA cases, we further specify that we want the citation for the Court of Appeals opinion, and for USDC cases, we further specify that we want the citation for the District Court opinion. We test for citation equality using eyecite (Cushman et al., 2021). Author: Given the name and citation of a case, supply the name of the opinion author. This query tests an LLM’s ability to associate a given case with a particular judge, which is important for con- textualizing a case in the broader jurisprudential landscape. For SCOTUS and USCOA cases, we further specify that we want the name of the major- ity opinion author. We accept a fuzzy match of the opinion author’s name as accurate. 3.2 Moderate Complexity Tasks Next, in the moderate complexity category, we start to require an LLM to evince knowledge of actual legal opinions themselves. To answer the queries in this category, an LLM must know something about a case’s substantive content; these queries seek information that must be collated from idiosyn- cratic portions of its text. Of course, a database- augmented LLM might still be able to retrieve some of this information without ever actually internal- izing the content of a case, but we expect this text- based knowledge to be less available than the infor- mation described in the low complexity category. 5 Specifically, we ask for the following information: Disposition: Given a case name and its citation, state whether the court affirmed or reversed the lower court. This query tests an LLM’s knowledge of how the court resolved the instant appeal con- fronting the parties in the case, which is the first step for determining the holding that is created by the case. This is essentially a binary classification task where we accept correct “affirm” or “reverse” labels as accurate. We filter out all ambiguous dis- positions (e.g., reversals in part) and we do not ask this query of USDC cases because District Courts are courts of original jurisdiction.8 Quotation: Given a case name and its citation, supply any quotation from the opinion. This query tests an LLM’s ability to produce some portion of an opinion’s text verbatim, which is an important feature for lawyers seeking to use a case to stand for a specific proposition. Normally, such memoriza- tion is considered an undesirable property of LLMs (Carlini et al., 2022), but in this legal application it is actually desirable behavior. We accept any fuzzy string of characters appearing in the majority opinion as accurate. Authority: Given a case name and its citation, supply a case that is cited in the opinion. This query probes an LLM’s understanding of the chain of precedential authority that supports a given opin- ion. We do not distinguish between positive and negative citations for this task; we accept any prece- dent cited in any way in the text of the majority opinion as accurate. We extract and match citations on their volumes, reporters, and pages using eyecite (Cushman et al., 2021). Overruling year: Given a case name and its ci- tation, supply the year that it was overruled. This query tests an LLM’s ability to recognize when a given case has been subsequently altered, which is crucial information for lawyer seeking to determine whether a given precedent is still good law or not. This task is the most complicated in this category because it requires the LLM to draw connections between multiple areas of the case space. We ac- cept only the exact year of overruling as accurate, and we limit this task to only those SCOTUS cases that have been explicitly overruled (n=279).9 8 While it is possible for some administrative agency decisions to be appealed to a district court, this occurs infrequently enough that we choose not to ask for case disposition at the district court level. 9 In Section 5.2, we experiment with prompting with cases that have never been overruled as well. 3.3 High Complexity Tasks Finally, in the high complexity category, we seek answers to tasks that both presuppose legal reason- ing skills (unlike the low complexity tasks) and are not readily available in existing legal databases like WestLaw or Lexis (unlike the moderate complexity tasks). These tasks all require an LLM to synthe- size core legal information out of unstructured legal prose—information that is frequently the topic of deeper legal research. In Section 4.3, we explain how we test LLMs’ knowledge of some of these more complex facts without necessarily having ac- cess to the ground-truth answers ourselves: Doctrinal agreement: Given two case names and their citations, state whether they agree or dis- agree with each other. This query requires an LLM to show knowledge of the precedential relationship between two different cases, information that is essential for higher-order legal reasoning. We use Shepard’s treatment codes as a basis for construct- ing this task, filtering out all ambiguous citation treatments (e.g., neutral treatments) and coarsen- ing the unambiguous codes into “agree” and “dis- agree” labels that we accept as accurate. For this task, we use a relatively balanced dataset of 2,839 citing-cited case pairs coded as “agree,” and 2,161 citing-cited case pairs coded as “disagree.” This task is limited to SCOTUS cases as our underlying dataset only contains thorough Shepard’s data for citations to the Supreme Court. Factual background: Given a case name and its citation, state its factual background. This query tests an LLM’s understanding of the concrete fact pattern underlying a case, which is helpful in as- sessing the relevance of the case to current research and in drawing parallels with other cases. Procedural posture: Given a case name and its citation, state its procedural posture. This query tests an LLM’s understanding of how and why a case has arrived at a particular court, which aids in understanding the precise question presented and standard of review applicable. Subsequent history: Given a case name and its citation, state its subsequent procedural history, if any. This query tests an LLM’s knowledge of any other related proceedings that concern the given case after a particular decision, which is informa- tion that can change or clarify the legal significance of the case. Core legal question: Given a case name and its citation, state the core legal question at issue. This 6 query tests an LLM’s ability to pinpoint the main issue or issues that a court is addressing in a case, which is the most important factor in assessing whether a case is apposite or not. Central holding: Given a case name and its ci- tation, state its central holding. This query tests an LLM’s knowledge of the legal principle that a given case stands for, i.e., the precedent that future cases will rely upon or distinguish from. Articulat- ing the holding of a case is crucial for legal analysis and argumentation and is the most complex task that we evaluate. 4 Experimental Design 4.1 Data Construction We aim to profile hallucination rates across several legally salient dimensions, including hierarchy, ju- risdiction, time, and case prominence. Thus, we construct our test data with an eye toward making statistical inferences on these covariates. We begin with the universe of case law from each level of the federal judicial hierarchy—namely, SCOTUS, USCOA, and USDC—that has been pub- lished in the volumes of the U.S. Reports, the Fed- eral Reporter, and the Federal Supplement. To en- sure balance over time and place, we then perform stratified random sampling using year strata for the SCOTUS cases, circuit-year strata for the USCOA cases, and state-year strata for the USDC cases. We draw 5,000 cases from each level of the judiciary. Finally, we merge these units with metadata ob- tained from the Caselaw Access Project (2023), the Supreme Court Database (Spaeth et al., 2022), the Appeals Courts Database Project (Songer, 2008; Kuersten and Haire, 2011), the Library of Congress (Congress.gov, 2023), and Shepard’s Citations (Fowler et al., 2007; Black and Spriggs, 2013).10 4.2 Reference-based Querying One effective way to study hallucinations in the open-domain setting is to use a test oracle—or an external reference—to detect and adjudge non- factual responses (Lin et al., 2022; Lee et al., 2023; Li et al., 2023). Such oracles are usually difficult and costly to construct (Krishna et al., 2021), but we exploit the tabular metadata described in Sec- tion 4.1 to develop ours. Our assumption is that while LLMs have access to and are trained on the raw text of American case law, which is in the 10More information about how we use these metadata to con- struct each query is available in Appendix A. public domain (Henderson et al., 2022), they have not yet explicitly memorized these cases’ attendant metadata, which exist separately from the cases’ textual content and which we have aggregated from disparate sources. These metadata enable us to construct reference- based queries for the first nine of our tasks (Fig- ure 2). These queries take the form of question-and- answer triples ({x1, . . . , xn−1}, xn, x′ n), where {x1, . . . , xn−1} is the question, xn is the LLM’s greedy answer retrieved from calling f0(·), and x′ n is the known ground-truth answer. Our estimand of interest for each task is the population-level hal- lucination rate π, which we estimate by averaging over the binary outcomes of our randomly sampled cases: π = ˆπ = 1 N ∑ 1 [xn ̸= x′ n] (1) 4.3 Reference-free Querying Reference-based querying lets us directly recover our population parameter of interest, but two prob- lems limit the effectiveness of the approach. First, we are restricted to asking questions for which di- gestible metadata exist and a clear answer has been recorded, which rules out many more complex in- quires. Second, precisely because these queries can be answered with tabular data, legal database- augmented LLMs (Cui et al., 2023; Savelka et al., 2023) are likely to soon solve or at least mask hal- lucinated responses to these queries (Shuster et al., 2021; Peng et al., 2023). To test the tasks that cannot be easily verified against an external legal database, we employ reference-free querying instead, which detects hal- lucinations by exploiting the stochastic behavior of LLMs at higher temperatures (Agrawal et al., 2023; Manakul et al., 2023; Min et al., 2023). This ap- proach is rooted in the theory that hallucinations are more likely to originate in flat probability distribu- tions with higher next-token uncertainties, whereas factual answers should always have a high prob- ability of being the generated response given a prompt. Thus, by repeatedly querying an LLM at a non-greedy temperature, we can estimate the model’s hallucination rate by examining its self- consistency—factual responses should not change, but hallucinated ones will. Most reference-free approaches implicitly as- sume that the LLM is calibrated, i.e., that there is in- deed some correlation between its self-consistency 7 and its propensity to hallucinate. For reasons that we discuss in Section 5.3, we are unwilling to make this assumption in our legal setting. We therefore adopt a slightly different implementation that is still reference-free, but only requires contradiction, not consistency (Mündler et al., 2023). Specifically, for our final five tasks (Figure 2), we construct reference-free queries in the form of question-and- answer triples ({x1, . . . , xn−1}, x (1) n , x (2) n ), where {x1, . . . , xn−1} is the question, x (1) n is one LLM answer retrieved by calling f1(·) once, and x(2) n is another LLM answer retrieved by calling f1(·) again. Detecting a hallucination then amounts to detecting a logical contradiction between the two stochastic answers: any such contradiction guar- antees non-factuality, because two contradictory answers cannot both be correct (Mündler et al., 2023). To identify these contradictions at scale, we feed both answers into GPT 4 and ask it for its assess- ment. This technique does not assume anything about f1(·)’s calibration—it just requires that GPT 4 possess logical reasoning skills sufficient to com- pare f1(·)’s two responses and accurately label them as contradictory as not. To justify this re- liance on GPT 4, we manually label a portion of the reference-free responses ourselves and conduct an intercoder reliability analysis to ensure that GPT 4 is indeed able to perform this task. Full informa- tion about our procedure and a validity check is provided in Appendix C. (We find that GPT 4’s reliability is comparable to human labeling of con- tradictions.) An important caveat of this approach is that it only allows us to establish a lower bound on the hallucination rate for our reference-free queries: π ≥ ˆπ = 1 N ∑ 1 [x(1) n ̸= x(2) n ] (2) Although self-contradiction guarantees hallucina- tion, the inverse does not hold: two answers may be logically consonant but still lack fidelity to the law. Because we are unwilling to assume calibration, we accept this inferential limitation, but, as we show below, even the lower bounds on hallucination rates are quite high and informative. 4.4 Models We perform our experiments using three popular LLMs: OpenAI’s ChatGPT 3.5 (gpt-3.5-turbo-0613) (OpenAI, 2023b), Google’s PaLM 2 (text-bison-001) (Anil et al., 2023), and Meta’s Llama 2 (Llama-2-13b-chat-hf) (Touvron et al., 2023). We run each query under both zero- and three- shot prompting setups. We provide the full text of the prompts we use for each query, along with the few-shot examples, in Appendix B. All of our raw queries and responses, including timestamp logs, are available in our GitHub repository. 5 Results We begin by presenting our main results profil- ing the LLMs’ hallucination rates, which cut to the core of popular concerns over LLMs’ suitabil- ity for various legal research tasks (Section 5.1). Then, after showing that hallucinations are gen- erally widespread, and highlighting some salient variation in the LLMs’ tendencies to hallucinate, we turn to two additional challenges that threaten LLMs’ utility for legal research: (1) their suscep- tibility to contra-factual bias, i.e., their ability to handle queries based on mistaken legal premises (Section 5.2), and (2) their certainty in their re- sponses, i.e., their self-awareness of their propen- sity to hallucinate (Section 5.3). 5.1 Hallucination Rates and Heterogeneity Tables 2, 3, and 4 report our estimated hallucina- tion rates and their standard errors for each cat- egory of our tasks. We find that hallucinations vary with they substantive complexity of the task (Section 5.1.1), the hierarchical level of the court (Section 5.1.2), the jurisdictional location of the court (Section 5.1.3), the prominence of the case (Section 5.1.4), the year the case was decided (Sec- tion 5.1.5), and the LLM queried (Section 5.1.6). We do not find substantial differences between zero- shot and few-shot prompting, so we focus our dis- cussion on the few-shot results alone.11 5.1.1 Hallucinations Vary by Task Complexity As we hypothesized in Section 3, we first observe that hallucinations increase with the complexity of the legal research task at issue, which we visualize in Figure 3. Starting with the low complexity cate- gory (Table 2), the LLMs perform best on the sim- ple Existence task, though this is in part driven by their tendency to always answer “yes” when asked about the existence of any case. (In Appendix E.1 11Throughout, we also drop from our analyses any instances of LLMs refusing to respond to our queries. We document these abstentions in Appendix F, but they are generally rare and do not affect our findings. 8 we demonstrate this problem by asking about the existence of fake cases instead.) The models begin to struggle more when prompted for information about a case’s Court, Citation, or Author. Hal- lucinations then surge among the moderate com- plexity tasks (Table 3), all of which require the LLMs to evince knowledge of the actual content of a legal opinion. We note that these results are not just a product of different evaluation metrics: although the Quotation task, for example, requires near-word reproduction of particular sentences and phrases to be judged correctly, the Disposition task simply asks for binary responses from the model. Yet, the LLMs hallucinate widely in both setups. The results for the high complexity tasks (Ta- ble 4) confirm this general pattern of poor perfor- mance, but must be interpreted slightly differently. First, the Doctrinal agreement task is another bi- nary classification task, so the LLMs’ hallucination rates on this task—near 0.5—represent little im- provement over random guessing. This suggests that LLMs are not yet able to perform the kind of legal reasoning that attorneys perform when they assess the precedential relationship between cases— a core purpose of legal research. Second, regarding the remaining tasks in the high complexity category, it is important to keep in mind that the hallucination rates that we report for these tasks are only lower bounds on the true rates, as these tasks are evaluated using our reference-free method (Section 4.3). To provide some context for these bounds, we note that in a similar self- contradiction setup, Mündler et al. (2023) found that GPT 3.5 hallucinated about 14.3% of the time on general QA queries. On our legal QA queries, GPT 3.5 and our other LLMs far surpass this base- line rate—and it is possible that the true hallucina- tion rate is even higher. For example, we find that even on one of the easier reference-free tasks—Procedural posture— our LLMs hallucinate at least 57% of the time. Performance degrades further on the most com- plex Core legal question and Central holding tasks, with hallucinations arising in response to at least 75% of our queries. Hallucinations are low- est among GPT 3.5 responses to the Subsequent history task at the SCOTUS level, but this is be- cause the model simply tends to state that the liti- gation concluded with the Supreme Court decision. This may not actually be correct—many Supreme Court cases result in a remand and have additional procedural history in lower courts. However, we Figure 3: Relationship between task complexity and mean hallucination rate. Higher values indicate a greater likelihood of factually incorrect LLM responses. High complexity tasks include several reference-free tasks, so those reported halluci- nation rates are lower bounds on the true rates. Contra-factual tasks are excluded from this comparison. Figure 4: Relationship between judicial hierarchy and mean hallucination rate, all reference-based tasks pooled. Hallucina- tion rates are higher for lower levels of the federal judiciary. are unable to capture this kind of mistake, as our methodology only permits us to identify hallucina- tions where the model contradicts itself. We are not able to capture repeated incorrect answers as instances of hallucination, meaning that our esti- mate of hallucination in the SCOTUS Subsequent history task is likely to understate the rate of hallu- cination by a larger margin that other tasks. Taken together, these results invite skepticism about LLMs’ abilities to conduct complex forms of legal research. Our reference-free tasks, in particu- lar, raise serious doubts about LLMs’ knowledge of substantive aspects of American case law—the very knowledge that attorneys must often synthe- size themselves, instead of merely looking up in a database. 9 D.C. Cir.Fed. Cir. Figure 5: Relationship between USCOA jurisdiction and mean hallucination rate, all reference-based USCOA tasks and models pooled, post-1981 cases only. LLM performance is strongest in jurisdictions that are commonly perceived to play a more influential role. 5.1.2 Hallucinations Vary by Court We next examine trends by hierarchy, exploring LLMs’ abilities to restate the case law of the three different levels of the federal judiciary. We find that across all tasks and all LLMs, hal- lucinations are lowest in the highest levels of the judiciary, and vice-versa (Figure 4). Thus, our LLMs perform best on tasks at the SCOTUS level, worse on tasks at the USCOA level, and worst on tasks at the USDC level. These results are encour- aging insofar as it is important for LLMs to be knowledgeable about the most authoritative and wide-ranging precedents, but discouraging insofar as they suggest that LLMs are not well attuned to localized legal knowledge. After all, the vast ma- jority of litigants do not appear before the Supreme Court, and may benefit more from knowledge that is tailored to their home District Court—their court of first appearance. 5.1.3 Hallucinations Vary by Jurisdiction To better understand the relationship between dif- ferent courts and hallucinations, we next zoom in on the middle level of the judicial hierarchy—the Courts of Appeals—and examine horizontal het- erogeneity across the circuits.12 Figure 5 depicts these results geographically, showing lower hallu- cination rates in lighter colors and higher rates in darker colors. Pooling our tasks together, we see the best performance in the Ninth Circuit (compris- ing California and adjacent states in yellow), the Second Circuit (comprising New York and adja- 12Because not all Courts of Appeals were created at the same time, for parity in comparison here we exclude from our results cases decided before 1982, the year the youngest circuit—the Federal Circuit—was created. We report the full, non-truncated results in Appendix Section E.2, which are largely consistent with these post-1981 results. Figure 6: Relationship between SCOTUS case prominence, measured by PageRank percentile, and mean hallucination rate, all SCOTUS tasks pooled. Hallucinations decline sharply as case prominence passes the 90th percentile, meaning that LLMs are more likely to respond with accurate information about prominent cases. cent states in soft green), and the Federal Circuit (a special court headquartered in Washington, D.C.). By contrast, performance tends to be worst in the circuits in the geographic center of the country. These results confirm popular intuitions about the influential role that the Second, Ninth, and Fed- eral Circuits play in the American legal system. Be- cause it encompasses New York City, the Second Circuit has traditionally had a significant impact on financial and corporate law, and many landmark decisions in securities law, antitrust, and business litigation have come from this court. The Ninth Circuit, on the other hand, handles more cases than any other federal appellate court, and often issues rulings that advance progressive positions that lead to disproportionate review by the Supreme Court. Finally, the Federal Circuit exercises exclusive ap- pellate jurisdiction over patent and trademark cases, inter alia, enjoying unique influence in those areas of the law. Perhaps surprisingly, however, our results do not confirm popular intuitions about the D.C. Circuit, which is generally thought to be the most influential appellate division. In our tasks, our LLMs perform only at an average level within this circuit—not worst, but not best either. 5.1.4 Hallucinations Vary by Case Prominence To probe the role of legal prominence more directly, we move to SCOTUS-level results next, examining the relationship between case importance and hallu- cinations. To measure case prominence within this 10 Figure 7: Relationship between SCOTUS case decision year and mean hallucination rate, all SCOTUS tasks pooled. LLMs are most likely to respond with accurate information in cases from the latter half of the 20th century. single level of the judiciary, we use the Caselaw Access Project’s PageRank percentile scores, a met- ric of citation network centrality that captures the general legal and political prominence of a case. We find that case prominence is negatively cor- related with hallucination, reaffirming our results from above (Figure 6). However, we also note that a sharp slope change occurs around the 90th promi- nence percentile in the GPT 3.5 and PaLM 2 mod- els. This suggests that the bias of these LLMs—but not Llama 2—may be skewed even more toward the most well-known decisions of the American legal system, even within the SCOTUS level. 5.1.5 Hallucinations Vary by Case Year Because case law develops in virtue of new deci- sions building on old ones over time, the age of a case may be another useful predictor of hallucina- tion. Examining this relationship at the SCOTUS level in Figure 7, we find a non-linear correlation between hallucination and age: hallucinations are most common among the Supreme Court’s oldest and newest cases, and least common among its post- war Warren Court cases (1953-1969). This result suggests another important limitation on LLMs’ legal knowledge that users should be aware of: LLMs’ peak performance may lag several years behind the current state of the doctrine, and LLMs may fail to internalize case law that is very old but still applicable and relevant law. 5.1.6 Hallucinations Vary by LLM Finally, we also partition our results by the LLM itself and compare across models. We find that not Figure 8: Number of times each justice is stated to be the author of a SCOTUS case versus the actual number of cases authored by each justice in our time period-stratified dataset. A small number of justices are disproportionately represented in LLM responses. all LLMs are equal: GPT 3.5 performs best overall, followed by PaLM 2, followed by Llama 2 (see Figure 1 at beginning of manuscript). We also discover tendencies towards different inductive biases, or the predisposition of an LLM to generate certain outputs more frequently than others. In Figure 8, we highlight one of these bi- ases for our SCOTUS-level Author task, which asks the LLM to supply the name of the justice who authored the majority opinion in the given case. Each LLM we test has slightly different in- ductive preferences; some err towards the most recognizable justices, but others are a little more difficult to explain. For example, Llama 2 dispro- portionately favors Justice Story—an influential jurist who authored the famous Amistad opinion, among others—whereas PaLM 2 prefers Justice McLean—also an important jurist, but one more known for his dissents than his majority opinions, such as his dissent in the infamous Dred Scott case. Across the board, all our LLMs tend to overstate the true prevalence of justices at a higher magni- tude than they understate them, as indicated by the greater dispersion of the points above the y = x line in Figure 8. These biases demonstrate one way that LLMs inevitably encounter the kind of hallucination trade- off that we discuss in Section 2.3. If the inductive bias that an LLM learns from its training corpus is not well-aligned with the true distribution of facts about the world, then the LLM is likely to make systematic errors when queried about those facts. Moreover, the persistence of inductive bi- 11 ases also increases the risk of LLMs instantiating a kind of legal “monoculture” (Kleinberg and Ragha- van, 2021). Instead of accurately restating the full variation of the law, LLMs may simply regurgi- tate information from a few prominent members of the response set that they have been trained on, flattening legal nuance and producing a falsely ho- mogenous sense of the legal landscape. 5.2 Contra-factual Bias We now turn to the first of two potential failure points that we seek to highlight for LLMs perform- ing legal research tasks, beyond their sheer propen- sity to hallucinate: their bias toward accepting legal premises that are not anchored in reality and an- swering queries accordingly. We view this behavior as a particular kind of model sycophancy (the ten- dency of an LLM to agree with a user’s preferences or beliefs, even when the LLM would reject the be- lief as wrong without the user’s prompting; Sharma et al., 2023; Wei et al., 2023) or general cognitive error (Tversky and Kahneman, 1974; Jones and Steinhardt, 2022; Suri et al., 2023). This bias poses a subtle but pernicious challenge to those aiming to use LLMs for legal research. When a researcher is learning about a topic, they are not only unsure about the answer, they are also often unsure about the question they are asking as well. Worse, they might not even be aware of any defects in their query; research by its nature ven- tures into the realm of “unknown unknowns” (Luft and Ingham, 1955). This is especially true for unso- phisticated pro se litigants, or those without much legal training to begin with. Relying on an LLM for legal research, they might inadvertently submit a question premised on non-factual legal informa- tion or folk wisdom about the law. As discussed in Section 2.3, this then forces a trade-off: if the LLM is too intent on minimizing prompt hallucina- tions, it runs the risk of simply recapitulating the user’s misconception and producing a non-factual hallucination instead. To test whether this risk is real in the legal set- ting, we evaluate two modified versions of our reference-based queries, but with premises that are false by construction. Specifically, we ask the LLMs to (1) provide information about an author’s dissenting opinion in an appellate case in which they did not in fact dissent and (2) furnish the year that a SCOTUS case that has never been overruled was overruled. In both cases, we consider failing to provide the requested information an acceptable Figure 9: Hallucination rates by LLM, all contra-factual tasks pooled. Llama 2 is very unlikely to hallucinate on these tasks because it rejects the premise in the question. However, this tendency also leads it to perform more poorly on tasks with correct premises. answer; any uncritical answering of the prompt is treated as a hallucination. Table 5 reports the results of this experiment and Figure 9 summarizes them by LLM. In general, LLMs seem to suffer from contra-factual bias on these legal information tasks. As in the raw hal- lucination tasks, contra-factual bias hallucinations are higher in lower levels of the judiciary. Substan- tively, they are also greatest for the question with a false overruling premise, possibly reflecting the increased complexity of the question asked. Llama 2 performs exceptionally well, demon- strating little contra-factual hallucination. How- ever, this success is linked to a different kind of hallucination—in many false dissent examples, for instance, Llama 2 often states that the case or jus- tice does not exist at all. (In reality, all of our false dissent examples were created with real cases and real justices—just justices who did not author a dissent for the case.) Under our metrics for contra- factual hallucination, we choose to record these examples as successful rejections of the premise. The kind of error that Llama 2 makes here is al- ready measured in its poor performance on other tasks, especially Existence. 5.3 Model Calibration The second potential failure point that we investi- gate is model calibration, or the ability of LLMs to “know what they know.” Ideally, a well-calibrated model would be confident in its factual responses, and not confident in its hallucinated ones (Kadavath et al., 2022; Xiong et al., 2023; Tian et al., 2023b; 12 Yin et al., 2023; Azaria and Mitchell, 2023). If this property held for legal queries, researchers would be able to adjust their expectations accordingly and could theoretically learn to trust the LLM when it is confident, and learn to be more skeptical when it is not (Zhang et al., 2020). Even more importantly, if an LLM knew when it was likely to be hallucinat- ing, the hallucination problem could be in principle solvable through some form of reinforcement learn- ing from human feedback (RLHF) or fine-tuning, with unconfident answers simply being suppressed (Tian et al., 2023a). To study our LLMs’ calibration on legal queries, we estimate the expected calibration error (ECE) for each of our tasks. Appendix D describes our estimation strategy in full, but, intuitively, it en- tails extracting a confidence score for each LLM answer that we obtain and comparing it to the em- pirical hallucination rate that we observe. Table 6 reports the results of this analysis at the task level, and Figure 10 pools our findings at the LLM level by plotting those two metrics—confidences and empirical non-hallucination frequencies—against each other, binned into 10 equally-sized bins (repre- sented by the dots). In a perfectly calibrated model, the confidences and empirical frequencies would be perfectly correlated along the y = x diagonal. Overall, we note that PaLM 2 (pooled ECE = 0.066) and GPT 3.5 (pooled ECE = 0.114) are significantly better calibrated than Llama 2 (pooled ECE = 0.453). Diving into the task-level results, we see that across all LLMs, calibration is poorer on our more complex tasks and on tasks directed toward lower levels of the judicial hierar- chy. ECE is also higher on our partially open-ended tasks such as Court and Author. In these tasks, the LLM has a large but finite universe of responses, and the high ECE for these tasks reflects the LLMs’ tendencies to over-report on the most prominent or widely known members of the response set. In all cases, the calibration error is in the posi- tive direction: our LLMs systematically overesti- mate their confidence relative to their actual rate of hallucination.13 This finding, too, suggests that users should exercise caution when interpreting LLMs’ responses to legal queries, especially those of Llama 2. Not only may they receive a halluci- nated response, but they may receive one that the LLM is overconfident in and liable to repeat again. 13In Appendix D, we explore whether this bias can be cor- rected with an ex post scaling adjustment, but conclude that challenges remain. Figure 10: Calibration curves by LLM, all reference-based tasks pooled. GPT 3.5 and PaLM 2 are much better calibrated on legal queries than Llama 2. 6 Discussion We began this paper with a question that has surged in salience over the last twelve months: Will AI sys- tems like ChatGPT soon reshape the practice of law and democratize access to justice? Although recent research suggests that LLMs are performing in- creasingly well on a number of legal benchmarking tasks (Blair-Stanek et al., 2023; Choi et al., 2022; Fei et al., 2023; Guha et al., 2023; Nay et al., 2023; OpenAI, 2023a; Trozze et al., 2023), we highlight the problem of legal hallucinations, which remains a serious obstacle to the adoption of these mod- els. Confirming popular perceptions (Weiser, 2023; Romoser, 2023), we show that factual legal hallu- cinations are widespread in the language models that we study—OpenAI’s ChatGPT 3.5, Google’s PaLM 2, and Meta’s Llama 2—on the bulk of the legal research tasks that we profile (Section 5.1). We also push beyond conventional wisdom by surfacing two additional behaviors that threaten LLMs’ utility for legal research: (1) their suscepti- bility to contra-factual bias, i.e., their inability to handle queries containing an erroneous or mistaken starting point (Section 5.2), and (2) their certainty in their responses, i.e., their inability to always “know what they know” (Section 5.3). Unfortu- nately, we find that LLMs frequently provide seem- ingly genuine answers to legal questions whose premises are false by construction, and that un- der their default configurations they are imperfect predictors of their own tendency to confidently hal- lucinate legal falsehoods. These findings temper recent enthusiasm for the ability of off-the-shelf, publicly available LLMs to 13 accelerate access to justice (Tito, 2017; Perlman, 2023; Tan et al., 2023). Our results suggest that the risks of using these generic foundation models are especially high for litigants who are: • Filing in courts lower in the judicial hierarchy or those located in less prominent jurisdictions • Seeking more complex forms of legal infor- mation • Formulating questions with mistaken premises • Unsure of how much to “trust” the LLMs’ responses In short, we find that the risks are highest for those who would benefit from LLMs most— indigent or pro se litigants. Not only do LLMs hallucinate widely, their current implementations lack the behavioral features that such users would require. Ideally, LLMs would do best at localized legal information (rather than SCOTUS-level in- formation), be able to correct users when they ask misguided questions (rather than accepting their premises at face value), and be able to moderate their responses with the appropriate level of confi- dence (rather than hallucinating with conviction). We therefore echo concerns that the proliferation of LLMs may ultimately exacerbate, rather than eradi- cate, existing inequalities in access to legal services (Simshaw, 2022; Draper and Gillibrand, 2023). In- deed, increased reliance on LLMs may actually produce a kind of legal “monoculture” (Kleinberg and Raghavan, 2021), with users being fed informa- tion from only a limited subset of judicial sources that elide many of the deeper nuances of the law. We also emphasize that the challenges presented by legal hallucinations are not only empirical, but also normative. Although data-rich and moneyed players certainly stand at an advantage when it comes to building hallucination-free legal LLMs for their own private use, it is not clear that even in- finite resources can entirely solve the hallucination problem we diagnose. As we discuss in Section 2.3, model fidelity to the training corpus, model fidelity to the user’s prompt, and model fidelity to the facts of the world—i.e., the law—are normative commit- ments that stand in tension with each other, despite all being independently desirable technical prop- erties of an LLM. Ultimately, since hallucinations of some kind are generally inevitable (Kalai and Vempala, 2023), developers of legal LLMs will need to make choices about which type(s) of hallu- cinations to minimize, and they should make these choices transparent to their downstream users.14 Only then can individual litigants decide for them- selves whether the legal information they seek to obtain from LLMs is trustworthy or not. In the meantime, more experienced legal practi- tioners may find some value in consulting LLMs for certain tasks, but even these users should re- main vigilant in their use, taking care to verify the accuracy of their prompts and the quality of their chosen LLM’s responses. Acknowledgements We thank Neel Guha, Sandy Handan-Nader, Adam T. Kalai, Peter Maldonado, Chris Manning, Joel Niklaus, Kit Rodolfa, and Andrea Vallebueno for helpful discussions and feedback. 14For example, Casetext, a prominent legal AI provider, claims that its CoCounsel tool works by “eliminating the serious limitations—like hallucinations—that curb the professional utility of [its] model” (Casetext, 2023, emphasis added). However, without further qualification of the types of hallu- cinations that have been eliminated, blanket statements of hallucination elimination may actually further obscure the true risks of using LLMs from the user. 14 Table 2: Hallucination rates across levels of the federal judiciary (low complexity tasks) SCOTUS (1794-2015; n=5000) USCOA (1895-2019; n=5000) USDC (1932-2019; n=5000) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 Zero-shot 0.004 (0.001) 0.054 (0.003) 0.303 (0.006) 0.003 (0.001) 0.025 (0.002) 0.157 (0.005) 0.001 (0.001) 0.016 (0.002) 0.240 (0.006) Existence Few-shot 0.029 (0.002) 0.029 (0.002) 1.000 (0.000) 0.018 (0.002) 0.005 (0.001) 1.000 (0.000) 0.004 (0.001) 0.006 (0.001) 1.000 (0.000) Zero-shot 0.000 (0.000) 0.000 (0.000) 0.003 (0.001) 0.645 (0.007) 0.703 (0.006) 0.700 (0.006) 0.815 (0.005) 0.839 (0.005) 0.815 (0.005) Court Few-shot 0.000 (0.000) 0.000 (0.000) 0.005 (0.001) 0.641 (0.007) 0.703 (0.006) 0.679 (0.007) 0.870 (0.005) 0.842 (0.005) 0.870 (0.005) Zero-shot 0.714 (0.007) 0.907 (0.004) 0.941 (0.003) 0.936 (0.004) 1.000 (0.000) 1.000 (0.000) 0.996 (0.001) 1.000 (0.000) 1.000 (0.000) Citation Few-shot 0.643 (0.007) 0.833 (0.005) 0.953 (0.003) 0.925 (0.004) 0.999 (0.001) 1.000 (0.000) 0.996 (0.001) 0.999 (0.000) 1.000 (0.000) Zero-shot 0.799 (0.006) 0.816 (0.005) 0.884 (0.005) 0.973 (0.002) 0.989 (0.002) 0.991 (0.001) 0.964 (0.003) 0.988 (0.002) 0.987 (0.002) Author Few-shot 0.849 (0.005) 0.859 (0.005) 0.881 (0.005) 0.983 (0.002) 0.988 (0.002) 0.993 (0.001) 0.951 (0.003) 0.984 (0.002) 0.987 (0.002) Note: Table reports estimated hallucination rates. Abstention responses are dropped. Standard errors are shown in parenthe- ses. Table 3: Hallucination rates across levels of the federal judiciary (moderate complexity tasks) SCOTUS (1794-2015; n=5000) USCOA (1895-2019; n=5000) USDC (1932-2019; n=5000) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 Zero-shot 0.499 (0.001) 0.500 (0.000) 0.536 (0.007) 0.500 (0.000) 0.500 (0.000) 0.493 (0.008) - - - Disposition Few-shot 0.496 (0.002) 0.501 (0.002) 0.502 (0.004) 0.489 (0.007) 0.501 (0.001) 0.501 (0.002) - - - Zero-shot 0.229 (0.006) 0.999 (0.000) 0.921 (0.004) 0.000 (0.000) 1.000 (0.000) 0.996 (0.001) 0.000 (0.000) 1.000 (0.000) 0.964 (0.003) Quotation Few-shot 1.000 (0.000) 0.998 (0.001) 0.999 (0.000) 1.000 (0.000) 1.000 (0.000) 1.000 (0.000) 1.000 (0.000) 1.000 (0.000) 1.000 (0.000) Zero-shot 0.939 (0.003) 0.985 (0.002) 0.993 (0.001) 0.986 (0.002) 0.997 (0.001) 0.999 (0.000) 0.985 (0.002) 0.995 (0.001) 0.999 (0.000) Authority Few-shot 0.917 (0.004) 0.954 (0.003) 0.994 (0.001) 0.979 (0.002) 0.993 (0.001) 0.999 (0.000) 0.969 (0.002) 0.994 (0.001) 0.999 (0.000) Zero-shot 0.919 (0.017) 0.858 (0.022) 0.972 (0.011) - - - - - - Overruling yeara Few-shot 0.976 (0.010) 0.870 (0.021) 0.984 (0.008) - - - - - - a 1810-2022 (n=279) Note: Table reports estimated hallucination rates. Abstention responses are dropped. Standard errors are shown in parentheses. 15 Table 4: Hallucination rates across levels of the federal judiciary (high complexity tasks) SCOTUS (1794-2015; n=100) USCOA (1895-2019; n=100) USDC (1932-2019; n=100) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 Zero-shot 0.500 (0.000) 0.466 (0.005) 0.500 (0.000) - - - - - - Doctrinal agreementa Few-shot 0.458 (0.004) 0.453 (0.006) 0.500 (0.000) - - - - - - Factual background Zero-shot 0.753 (0.045) 0.960 (0.020) 0.847 (0.036) 0.818 (0.039) 0.917 (0.028) 0.898 (0.031) 0.698 (0.047) 0.907 (0.029) 0.889 (0.032) Procedural posture Zero-shot 0.677 (0.048) 0.796 (0.042) 0.876 (0.033) 0.632 (0.049) 0.723 (0.046) 0.865 (0.035) 0.573 (0.050) 0.818 (0.039) 0.908 (0.029) Subsequent history Zero-shot 0.253 (0.048) 0.910 (0.030) 0.828 (0.039) 0.545 (0.061) 0.806 (0.041) 0.660 (0.049) 0.500 (0.065) 0.811 (0.041) 0.720 (0.047) Core legal question Zero-shot 0.869 (0.034) 0.879 (0.033) 0.939 (0.024) 0.926 (0.027) 0.950 (0.022) 0.960 (0.020) 0.750 (0.044) 0.939 (0.024) 0.908 (0.029) Central holding Zero-shot 0.766 (0.044) 0.886 (0.034) 0.958 (0.020) 0.897 (0.031) 1.000 (0.000) 0.990 (0.010) 0.760 (0.044) 0.949 (0.022) 0.866 (0.035) a 1796-2005 (n=5000) Note: Table reports estimated hallucination rates. For all tasks except doctrinal agreement, this rate is only a lower bound on the true population rate. Standard errors are shown in parentheses. Table 5: Hallucination rates across levels of the federal judiciary (contra-factual tasks) SCOTUS (1794-2015; n=1000) USCOA (1895-2019; n=1000) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 False dissent premise Zero-shot 0.393 (0.017) 1.000 (0.000) 0.000 (0.000) 0.782 (0.018) 1.000 (0.000) 0.021 (0.005) False overruling premise Zero-shot 0.822 (0.012) 1.000 (0.000) 0.029 (0.005) - - - Note: Table reports estimated hallucination rates. Abstention responses are dropped. Standard errors are shown in parentheses. 16 Table 6: Expected calibration error (ECE) across levels of the federal judiciary SCOTUS (1794-2015; n=100) USCOA (1895-2019; n=100) USDC (1932-2019; n=100) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 Zero-shot 0.008 (0.001) 0.119 (0.003) 0.262 (0.006) 0.007 (0.001) 0.182 (0.003) 0.117 (0.005) 0.004 (0.000) 0.183 (0.003) 0.178 (0.006) Existence Few-shot 0.038 (0.002) 0.041 (0.002) 0.998 (0.000) 0.064 (0.002) 0.014 (0.001) 0.998 (0.000) 0.053 (0.002) 0.038 (0.002) 0.996 (0.000) Zero-shot 0.000 (0.000) 0.000 (0.000) 0.003 (0.000) 0.322 (0.007) 0.184 (0.005) 0.387 (0.006) 0.163 (0.005) 0.132 (0.005) 0.163 (0.004) Court Few-shot 0.001 (0.000) 0.000 (0.000) 0.009 (0.001) 0.355 (0.007) 0.161 (0.006) 0.452 (0.007) 0.099 (0.004) 0.094 (0.004) 0.099 (0.004) Zero-shot 0.069 (0.005) 0.026 (0.003) 0.068 (0.003) 0.073 (0.004) 0.004 (0.001) 0.036 (0.001) 0.043 (0.003) 0.002 (0.000) 0.022 (0.001) Citation Few-shot 0.029 (0.004) 0.059 (0.004) 0.063 (0.002) 0.036 (0.003) 0.010 (0.001) 0.032 (0.001) 0.018 (0.002) 0.001 (0.000) 0.051 (0.002) Zero-shot 0.208 (0.006) 0.130 (0.006) 0.419 (0.006) 0.094 (0.004) 0.125 (0.003) 0.583 (0.005) 0.228 (0.006) 0.063 (0.002) 0.305 (0.004) Author Few-shot 0.347 (0.005) 0.142 (0.005) 0.454 (0.005) 0.142 (0.003) 0.118 (0.003) 0.656 (0.005) 0.096 (0.003) 0.045 (0.002) 0.481 (0.005) Zero-shot 0.431 (0.007) 0.291 (0.007) 0.199 (0.008) 0.557 (0.007) 0.382 (0.006) 0.148 (0.007) - - - Disposition Few-shot 0.314 (0.007) 0.165 (0.008) 0.283 (0.007) 0.205 (0.008) 0.227 (0.007) 0.439 (0.007) - - - Zero-shot 0.246 (0.020) 0.116 (0.019) 0.510 (0.021) - - - - - - Overruling yeara Few-shot 0.680 (0.019) 0.154 (0.021) 0.754 (0.019) - - - - - - Zero-shot 0.527 (0.006) 0.165 (0.007) 0.564 (0.008) - - - - - - Doctrinal agreementb Few-shot 0.409 (0.008) 0.152 (0.007) 0.548 (0.006) - - - - - - a 1810-2022 (n=279) b 1796-2005 (n=5000) Note: Table reports expected calibration error between empirical hallucination rates and estimated conditional probabilities. Conditional probabilities are estimated by sampling 10 responses from the model at temperature 1 and assessing their agreement with the model’s greedy response. Bootstrapped standard errors are shown in parentheses. 17 References Ayush Agrawal, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When They’re Hallucinating References? Bob Ambrogi. 2023. As Allen & Overy Deploys GPT- based Legal App Harvey Firmwide, Founders Say Other Firms Will Soon Follow. LawSites. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau- rav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Gar- cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur- Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur- witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel- ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Ben- jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nys- trom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Au- rko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wiet- ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 Technical Report. Amos Azaria and Tom Mitchell. 2023. The Internal State of an LLM Knows When It’s Lying. Ryan C. Black and James F. Spriggs, II. 2013. The Cita- tion and Depreciation of U.S. Supreme Court Prece- dent. Journal of Empirical Legal Studies, 10(2):325– 358. Andrew Blair-Stanek, Nils Holzenberger, and Benjamin Van Durme. 2023. Can GPT-3 Perform Statutory Reasoning? In Proceedings of the Nineteenth In- ternational Conference on Artificial Intelligence and Law, Braga, Portugal. Association for Computing Machinery. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Kr- ishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad- hak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchan- dani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa- padimitriou, Joon Sung Park, Chris Piech, Eva Porte- lance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2022. On the Opportunities and Risks of Foundation Models. Meng Cao, Yue Dong, and Jackie Cheung. 2022. Hal- lucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization. In Pro- ceedings of the 60th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 3340–3354, Dublin, Ireland. Associa- tion for Computational Linguistics. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the Original: Fact Aware Neural Abstrac- tive Summarization. Proceedings of the AAAI Con- ference on Artificial Intelligence, 32(1). Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying Memorization Across Neural Lan- guage Models. Caselaw Access Project. 2023. Caselaw Access Project. Casetext. 2023. Cocounsel harnesses gpt4’s power to deliver results that legal professionals can rely on. Seherman Chann. 2023. Non-determinism in GPT-4 is caused by Sparse MoE. https://152334H.github.io/blog/non-determinism-in- gpt-4/. 18 Jonathan H. Choi, Kristin E. Hickman, Amy Monahan, and Daniel Schwarcz. 2022. ChatGPT Goes to Law School. Journal of Legal Education, 71(3):387–400. Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37–46. Congress.gov. 2023. Table of Supreme Court Decisions Overruled by Subsequent Decisions. https://constitution.congress.gov/resources/decisions- overruled/. Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowl- edge Bases. Jack Cushman, Matthew Dahl, and Michael Lissner. 2021. Eyecite: A tool for parsing legal citations. Journal of Open Source Software, 6(66):3617. Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023. How Ready are Pre-trained Abstrac- tive Models and LLMs for Legal Case Judgement Summarization? Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Ja- son Weston. 2023. Chain-of-verification reduces hal- lucination in large language models. arXiv preprint arXiv:2309.11495. Chris Draper and Nicky Gillibrand. 2023. The Potential for Jurisdictional Challenges to AI or LLM Training Datasets. In Proceedings of the ICAIL 2023 Work- shop on Artificial Intelligence for Access to Justice, Braga, Portugal. CEUR Workshop Proceedings. Ronald Dworkin. 1986. Law’s Empire. Harvard Univer- sity Press, Cambridge, MA. Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. 2023. LawBench: Benchmark- ing Legal Knowledge of Large Language Models. Diego de Vargas Feijo and Viviane P. Moreira. 2023. Improving abstractive summarization of legal rulings through textual entailment. Artificial Intelligence and Law, 31(1):91–113. James H. Fowler, Timothy R. Johnson, James F. Spriggs, II, Sangick Jeon, and Paul J. Wahlbeck. 2007. Net- work Analysis and the Law: Measuring the Legal Importance of Precedents at the U.S. Supreme Court. Political Analysis, 15(3):324–346. Neel Guha, Julian Nyarko, Daniel E. Ho, Christo- pher Ré, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N. Rockmore, Diego Zambrano, Dmitry Tal- isman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory M. Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan H. Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov- Rahe, Nils Holzenberger, Noam Kolt, Peter Hender- son, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, and Zehua Li. 2023. LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein- berger. 2017. On Calibration of Modern Neural Networks. In Proceedings of the 34th International Conference on Machine Learning, pages 1321–1330. PMLR. Peter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E. Ho. 2022. Pile of Law: Learning Responsi- ble Data Filtering from the Law and a 256GB Open- Source Legal Dataset. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hal- lucination in Natural Language Generation. ACM Computing Surveys, 55(12):248:1–248:38. Erik Jones and Jacob Steinhardt. 2022. Capturing Fail- ures of Large Language Models via Human Cognitive Biases. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language Models (Mostly) Know What They Know. Adam Tauman Kalai and Santosh S Vempala. 2023. Calibrated language models must hallucinate. arXiv preprint arXiv:2311.14648. Jon Kleinberg and Manish Raghavan. 2021. Algo- rithmic Monoculture and Social Welfare. Pro- ceedings of the National Academy of Sciences, 118(22):e2018340118. Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to Progress in Long-form Question Answer- ing. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, pages 4940–4957, Online. Association for Com- putational Linguistics. Ashlyn K. Kuersten and Susan B. Haire. 2011. Update to the Appeals Courts Database (1997–2002). Ananya Kumar, Percy Liang, and Tengyu Ma. 2020. Verified Uncertainty Calibration. 19 J. Richard Landis and Gary G. Koch. 1977. The Mea- surement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas- cale Fung, Mohammad Shoeybi, and Bryan Catan- zaro. 2023. Factuality Enhanced Language Models for Open-Ended Text Generation. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large- Scale Hallucination Evaluation Benchmark for Large Language Models. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. Joseph Luft and Harrington Ingham. 1955. The Johari Window as a graphic model of interpersonal aware- ness. In Proceedings of the Western Training Labora- tory in Group Development. University of California, Los Angeles, Extension Office. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Lan- guage Models. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factu- ality in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On- line. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Fac- tual Precision in Long Form Text Generation. Niels Mündler, Jingxuan He, Slobodan Jenko, and Mar- tin Vechev. 2023. Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. John J. Nay, David Karamardian, Sarah B. Lawsky, Wenting Tao, Meghana Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H. Choi, and Jungo Kasai. 2023. Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence. OpenAI. 2023a. GPT-4 Technical Report. OpenAI. 2023b. Introducing ChatGPT. https://openai.com/blog/chatgpt. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Lan- guage Models with External Knowledge and Auto- mated Feedback. Andrew Perlman. 2023. The Implications of Chat- GPT for Legal Services and Society. The Practice, (March/April). Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. 2023. From Sparse to Soft Mixtures of Experts. James Romoser. 2023. No, Ruth Bader Ginsburg did not dissent in Obergefell — and other things ChatGPT gets wrong about the Supreme Court. Jaromir Savelka, Kevin D. Ashley, Morgan A. Gray, Hannes Westermann, and Huihui Xu. 2023. Explain- ing Legal Concepts with Augmented Large Language Models (GPT-4). Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023. Towards understanding syco- phancy in language models. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces Hallucination in Conversation. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2021, pages 3784–3803, Punta Cana, Dominican Republic. Association for Computational Linguistics. Drew Simshaw. 2022. Access to A.I. Justice: Avoiding an Inequitable Two-Tiered System of Legal Services. Yale Journal of Law & Technology, 24:150–226. Donald R. Songer. 2008. The United States Courts of Appeals Database, 1925–1996. Harold J. Spaeth, Lee Epstein, Andrew D. Martin, Jef- frey A. Segal, Theodore J. Ruger, and Sara C. Benesh. 2022. 2022 Supreme Court Database, Version 2022 Release 01. http://supremecourtdatabase.org/. Gaurav Suri, Lily R. Slater, Ali Ziaee, and Morgan Nguyen. 2023. Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5. Jinzhe Tan, Hannes Westermann, and Karim Benyekhlef. 2023. ChatGPT as an Artificial Lawyer? In Proceedings of the ICAIL 2023 Work- shop on Artificial Intelligence for Access to Justice, Braga, Portugal. CEUR Workshop Proceedings. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo- pher D. Manning, and Chelsea Finn. 2023a. Fine- tuning Language Models for Factuality. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023b. Just Ask for Calibration: Strategies for Eliciting Calibrated Con- fidence Scores from Language Models Fine-Tuned with Human Feedback. 20 Joel Tito. 2017. How AI can improve access to justice. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine- Tuned Chat Models. Arianna Trozze, Toby Davies, and Bennett Kleinberg. 2023. Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers? Amos Tversky and Daniel Kahneman. 1974. Judgment under Uncertainty: Heuristics and Biases. Science, 185(4157):1124–1131. Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. 2021. Entailment as Few-Shot Learner. Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023. Simple synthetic data reduces sycophancy in large language models. Benjamin Weiser. 2023. Here’s What Happens When Your Lawyer Uses ChatGPT. The New York Times. Ludwig Wittgenstein. 1998 [1921]. Tractatus Logico- Philosophicus. Dover. \"Translated by C. K. Ogden\". Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Mari- anna J. Martindale, and Marine Carpuat. 2023. Un- derstanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do Large Language Models Know What They Don’t Know? Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bel- lamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* ’20, pages 295–305, New York, NY, USA. Associa- tion for Computing Machinery. 21 Appendix A Data Sources In Section 4.1, we describe the data and sampling strategy that we use to construct our queries. For clarity, Table 7 links these data to each task. Our exact procedure for sampling, merging, and ag- gregating across these datasets is available in our GitHub repository. B Prompt Templates The full zero-shot and few-shot prompt templates for all of our queries are shown in Figures 14 to 28. The few-shot examples presented are those used for the SCOTUS queries; appropriate cases from the other levels of the judiciary are used in the USCOA and USDC versions. C Contradiction Detection Approach In Section 4.3, we describe our strategy for as- sessing hallucinations for our reference-free tasks. Briefly, because we do not have access to ground- truth labels for these tasks, we exploit the stochas- tic behavior of LLMs at higher temperatures and check for contradictions in their responses to re- peated queries. In Figure 29, we share the template for the contradiction elicitation query that we send to GPT 4 to perform this contradiction labeling at scale. We frame our contradiction check as an en- tailment task (Wang et al., 2021) as we find that this prompting strategy is the most performant in our setting. To confirm that GPT 4 is able to reliably detect the contradictions that we envision, we crosscheck GPT 4’s conclusions against our own expert knowl- edge on a subsample of our data. Specifically, for 100 of our queries, two of us independently recoded the LLMs’ responses for contradictions. We report Cohen’s κ coefficient (Cohen, 1960) for our agree- ment with GPT-4 on these queries in Table 8. In general, a κ value greater than 0.80 is considered “almost perfect” agreement, and one greater than 0.60 is considered “substantial” agreement (Landis and Koch, 1977). Our κ values suggest that we are well-justified to use GPT 4 for contradiction detec- tion; indeed, one of us agrees with GPT 4 more than with the other human coder. To give the reader a sense of GPT 4’s reasoning abilities in this setting, in Figures 30 and 31 we share some examples of its reasoning process and contradiction conclusions. D Expected Calibration Error (ECE) D.1 Estimation Approach In Section 5.3, we estimate the expected calibration error (ECE) of our LLMs for each of our tasks. Studying model calibration directly is not possible in our setup because we do not always observe our LLMs’ conditional probability distributions, which we require in order to determine whether they are in fact correlated with their response accuracies. Specifically, two of the models that we evaluate— OpenAI’s ChatGPT 3.5 and Google’s PaLM 2—are closed-source and do not expose this information to the user. To overcome this hurdle, we instead estimate the distributions by drawing K samples from the model at temperature 1 and comparing those responses to its greedy response as follows: ̂Pr[xn|·] = 1 K ∑ k∈K 1 [f0(·) = f1(·)(k)] (3) Then, for each task, we calculate the ECE of the LLM. Conceptually, the ECE represents the aver- age difference between the LLM’s confidence in the accuracy of its responses and the empirical fre- quency of its correct, non-hallucinated responses. Formally, where X = ̂Pr[xn|·] (cf. Equation 3) and Y = 1 [xn = x′ n] (cf. Equation 1), we estimate: ECE = E [|X − E [Y |X]|] (4) Following Kumar et al. (2020), we use a plug-in estimator that bins the data into 10 equally-sized slices and subtracts off an approximated bias term. D.2 Temperature Scaling As we report in Section 5.3, our results suggest that out of the box, the LLMs we evaluate are not well- calibrated on legal queries. However, a model that is uncalibrated under its unscaled probability dis- tribution is not necessarily uncalibrated tout court; the purpose of a LLM’s temperature parameter, af- ter all, is to allow an sophisticated user to adjust the distribution as needed. To explore whether such temperature scaling could indeed affect our results, here we perform ex post Platt scaling (Guo et al., 2017) on the raw distribution and check for improvements in the measured ECE. Figure 11 visualizes the rescaled ECE for each of our LLMs, and Table 9 quantifies the numerical gains. As expected, rescaling generally improves the ECE. However, the rescaling procedure is not 22 Table 7: Data used to construct each task Complexity Task Data source Existence CAP (2023); SCDB (Spaeth et al., 2022) Court CAP; SCDB Citation CAP; SCDB Low Author CAP; SCDB Disposition CAP; SCDB; ACDB (Songer, 2008; Kuersten and Haire, 2011) Quotation CAP; SCDB Authority CAP; SCDB Moderate Overruling year SCDB; Library of Congress (2023) Doctrinal agreement Shepard’s (Fowler et al., 2007; Black and Spriggs, 2013) Factual background CAP; SCDB Procedural posture CAP; SCDB Subsequent history CAP; SCDB Core legal question CAP; SCDB High Central holding CAP; SCDB Table 8: Intercoder reliability Coders κ Coder 1 vs. GPT 4 0.87 Coder 2 vs. GPT 4 0.77 Coder 1 vs. Coder 2 0.79 Table 9: Scaled and unscaled ECE LLM ECE (unscaled) ECE (scaled) GPT 3.5 0.114 0.063 PaLM 2 0.066 0.053 Llama 2 0.453 0.060 perfect: GPT 3.5 and PaLM 2 remain relatively uncalibrated in the [0.3, 0.4] confidence interval. The pooled ECE of Llama 2 improves substantially, but this is due to the entire distribution being com- pressed into the [0.0, 0.3] interval—the rescaled LLama 2 model is simply not confident in any of its responses. Overall, these results confirm our conclusion in Section 5.3 that LLMs face calibra- tion challenges on legal knowledge queries. Figure 11: Rescaled calibration curves by LLM, all resource- aware tasks pooled. 23 Table 10: Hallucination rates across levels of the federal judiciary (fake existence task) SCOTUS (1794-2015; n=1000) USCOA (1895-2019; n=1000) USDC (1932-2019; n=5000) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 False existence Zero-shot 0.661 (0.015) 0.171 (0.012) 0.000 (0.000) 0.799 (0.013) 0.674 (0.015) 0.000 (0.000) 0.968 (0.006) 0.706 (0.014) 0.000 (0.000) Note: Table reports estimated hallucination rates. Abstention responses are dropped. Standard errors are shown in parentheses. E Supplementary Analyses E.1 Fake Case Existence Task In Table 2 we report results for the Existence task, where our LLMs are asked whether or not a given case exists. Performance on this task is gener- ally strong; however, because all our prompted cases are in fact real, it is unclear whether these results are due to the LLMs’ genuine knowledge of a case’s existence or simply their tendency to always answer “yes” to this type of question. Ac- cordingly, here we conduct a supplemental analysis where we repeat the existence query, but using fake cases instead of real cases. Each fake case citation is constructed with plausible party names and the reporter that is appropriate for the court at issue, e.g., SolarFlare Technologies v. Armstrong, 656 F.3d 262 for a fake USCOA case. Appendix Table 10 reports the results of this Fake case existence experiment. We see that GPT 3.5 and PaLM 2 are both prone to simply asserting the existence of any case—real or fake—though PaLM 2 is more discriminating at the SCTOUS level. Llama 2, on the other hand, appears immune from this behavior, but recall from Table 2 that this is just bias in the opposite direction: it simply denies the existence of any case, real or fake. Al- together, these results belie the LLMs’ seemingly excellent performance on the Existence task: even here, they may not possess any actual knowledge of the true details of many cases. E.2 Hallucination Rates at the USCOA Level (No Time Cutoff) In Figure 5 in Section 5.1.3 above, we show the relationship between hallucinations and USCOA jurisdiction. However, in that figure, we exclude cases decided prior to 1982 in order to fairly com- pare rates across older and younger jurisdictions. Here, we share the results for that analysis with no cases excluded. As Figure 12 suggests, the non- truncated results substantively mirror the truncated ones: the Ninth, Eleventh, and Federal Circuit con- tinue to perform best. However, in this figure, the D.C. Cir.Fed. Cir. Figure 12: Relationship between USCOA jurisdiction and mean hallucination rate, all resource-aware USCOA tasks and models pooled. (No time cutoff.) Figure 13: Relationship between aggregate state USDC juris- diction and mean hallucination rate, all resource-aware USDC tasks and models pooled. Eleventh Circuit has a somewhat stronger showing as well. We believe that this is best explained by its relative infancy: it split from the Fifth Circuit in 1981 so it is composed of newer cases only. E.3 Hallucination Rates by State In Section 5.1.3, we show within-USCOA sources of hallucination heterogeneity on a geographic map. Figure 13 depicts a similar analysis for our USDC tasks, aggregated to the state level. Confirming our results above, we observe the lowest level of hallucinations in the state of New York, which is comprised of the Northern District, Southern Dis- trict, Western District, and Eastern District of New York. F Abstention Rates Occasionally, our LLMs abstain from providing an answers to our queries. For example, they may plead ignorance or simply claim that they are un- able to answer. When this occurs, we drop these 24 responses from our analyses. Table 11 reports the LLMs’ abstention rates for each task, which are generally low. 25 Table 11: Absention rates across levels of the federal judiciary (resource-aware tasks) SCOTUS (1794-2015; n=100) USCOA (1895-2019; n=100) USDC (1932-2019; n=100) Task Prompt GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 GPT 3.5 PaLM 2 Llama 2 Zero-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) Existence Few-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) Zero-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) Court Few-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) Zero-shot 0.041 (0.003) 0.000 (0.000) 0.000 (0.000) 0.195 (0.006) 0.000 (0.000) 0.000 (0.000) 0.295 (0.006) 0.000 (0.000) 0.000 (0.000) Citation Few-shot 0.002 (0.001) 0.000 (0.000) 0.000 (0.000) 0.006 (0.001) 0.000 (0.000) 0.000 (0.000) 0.054 (0.003) 0.000 (0.000) 0.000 (0.000) Zero-shot 0.003 (0.001) 0.000 (0.000) 0.000 (0.000) 0.009 (0.001) 0.000 (0.000) 0.000 (0.000) 0.054 (0.003) 0.000 (0.000) 0.000 (0.000) Author Few-shot 0.022 (0.002) 0.000 (0.000) 0.000 (0.000) 0.016 (0.002) 0.000 (0.000) 0.000 (0.000) 0.011 (0.001) 0.000 (0.000) 0.001 (0.000) Zero-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) - - - Disposition Few-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) - - - Zero-shot 0.000 (0.000) 0.007 (0.001) 0.002 (0.001) 0.000 (0.000) 0.468 (0.007) 0.003 (0.001) 0.000 (0.000) 0.011 (0.001) 0.001 (0.000) Quotation Few-shot 0.000 (0.000) 0.005 (0.001) 0.007 (0.001) 0.000 (0.000) 0.003 (0.001) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) Zero-shot 0.002 (0.001) 0.000 (0.000) 0.002 (0.001) 0.008 (0.001) 0.000 (0.000) 0.000 (0.000) 0.117 (0.005) 0.000 (0.000) 0.000 (0.000) Authority Few-shot 0.001 (0.000) 0.001 (0.000) 0.000 (0.000) 0.003 (0.001) 0.000 (0.000) 0.000 (0.000) 0.013 (0.002) 0.000 (0.000) 0.002 (0.001) Zero-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) - - - - - - Overruling yeara Few-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) - - - - - - Zero-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) - - - - - - Doctrinal agreementb Few-shot 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) - - - - - - a 1810-2022 (n=279) b 1796-2005 (n=5000) Note: Table reports model abstention rates. Standard errors are shown in parentheses. 26 Zero-shot Is the case {case_name}, {case_citation} ({case_year}), a real case? Say \"yes\" or \"no\" only. Few-shot Is the given case a real case? Say \"yes\" or \"no\" only. Examples: ``` Case: Brown v. Board of Education, 347 U.S. 483 (1954) Answer: Yes Case: Bowers v. Hardwick, 478 U.S. 186 (1986) Answer: Yes Case: Columbia University v. Rodham, 564 U.S. 911 (2010) Answer: No ``` Case: {case_name}, {case_citation} ({case_year}) Answer: Figure 14: Existence task prompt templates. Also used for False existence task. Zero-shot What is the citation for the case {case_name}? Provide ONLY the citation in \"<volume>, <reporter>, <page>\" format, nothing else. Few-shot What is the citation for the given case? Provide ONLY the citation in \"<volume>, <reporter>, <page>\" format, nothing else. Examples: ``` Case: Brown v. Board of Education Answer: 347 U.S. 483 Case: Bowers v. Hardwick Answer: 478 U.S. 186 Case: McCulloch v. Maryland Answer: 17 U.S. 316 ``` Case: {case_name} Answer: Figure 15: Citation task prompt templates. 27 Zero-shot Who wrote the majority opinion in {case_name}, {case_citation} ({case_year})? Provide the first and the last name of the judge ONLY. Few-shot Who wrote the majority opinion in the given case? Provide the first and the last name of the judge ONLY. Examples: ``` Case: Brown v. Board of Education, 347 U.S. 483 (1954) Answer: Earl Warren Case: Bowers v. Hardwick, 478 U.S. 186 (1986) Answer: Byron White Case: McCulloch v. Maryland, 17 U.S. 316 (1819) Answer: John Marshall ``` Case: {case_name}, {case_citation} ({case_year}) Answer: Figure 16: Author task prompt templates. 28 Zero-shot Did the court in {case_name}, {case_citation} ({case_year}) affirm or reverse the lower court's decision? Say \"affirm\" or \"reverse\" only. Few-shot Did the court in the given case affirm or reverse the lower court's decision? Say \"affirm\" or \"reverse\" only. Examples: ``` Case: Plessy v. Ferguson, 163 U.S. 537 (1896) Answer: Affirm Case: Bowers v. Hardwick, 478 U.S. 186 (1986) Answer: Reverse Case: McCulloch v. Maryland, 17 U.S. 316 (1819) Answer: Reverse ``` Case: {case_name}, {case_citation} ({case_year}) Answer: Figure 17: Disposition task prompt templates. 29 Zero-shot Which court decided the case {case_name}, {case_citation} ({case_year})? Provide the name of the court ONLY, nothing else. Few-shot Which court decided the given case? Provide the name of the court ONLY, nothing else. Examples: ``` Case: Viacom International Inc. v. YouTube, Inc., 676 F.3d 19 (2012) Answer: Second Circuit Case: Durham v. United States, 214 F.2d 862 (1954) Answer: D.C. Circuit Case: Bowers v. Hardwick (1986) Answer: Supreme Court ``` Case: {case_name}, {case_citation} ({case_year}) Answer: Figure 18: Court task prompt templates. 30 Zero-shot Provide a verbatim quotation from the majority opinion in the case {case_name}, {case_citation} ({case_year}). The quotation MUST be word-for-word from the majority opinion. Wrap the quotation in <quote></quote> tags. Few-shot Provide a verbatim quotation from the majority opinion in the given case. The quotation MUST be word-for-word from the majority opinion. Wrap the quotation in <quote></quote> tags. Examples: ``` Case: Brown v. Board of Education, 347 U.S. 483 (1954) Answer: <quote>We conclude that in the field of public education the doctrine of \"separate but equal\" has no place.</quote> Case: Bowers v. Hardwick, 478 U.S. 186 (1986) Answer: <quote>It is obvious to us that neither of these formulations would extend a fundamental right to homosexuals to engage in acts of consensual sodomy.</quote> Case: McConnell v. Federal Election Commission, 540 U.S. 93 (2003) Answer: <quote>Our cases have made clear that the prevention of corruption or its appearance constitutes a sufficiently important interest to justify political contribution limits.</quote> ``` Case: {case_name}, {case_citation} ({case_year}) Answer: Figure 19: Quotation task prompt templates. 31 Zero-shot What is a precedent that is cited in the majority opinion of the case {case_name}, {case_citation} ({case_year})? Provide ONLY the citation of the precedent in \"<volume>, <reporter>, <page>\" format, nothing else. Few-shot What is a precedent that is cited in the majority opinion of the given case? Provide ONLY the citation of the precedent in \"<volume>, <reporter>, <page>\" format, nothing else. Examples: ``` Case: Brown v. Board of Education, 347 U.S. 483 (1954) Answer: Plessy v. Ferguson, 163 U.S. 537 Case: Bowers v. Hardwick, 478 U.S. 186 (1986) Answer: Griswold v. Connecticut, 381 U.S. 479 Case: McConnell v. Federal Election Commission, 540 U.S. 93 (2003) Answer: Buckley v. Valeo, 424 U.S. 1 ``` Case: {case_name}, {case_citation} ({case_year}) Answer: Figure 20: Authority task prompt templates. Zero-shot What year was {case_name}, {case_citation}, overruled? Provide the year only. Few-shot What year was the given case overruled? Provide the year only. Examples: ``` Case: Whitney v. California, 274 U.S. 357 Answer: 1969 Case: Austin v. Michigan Chamber of Commerce, 494 U.S. 652 Answer: 2010 ``` Case: {case_name}, {case_citation} Answer: Figure 21: Overruling year task prompt templates. Also used for False overruling premise task. 32 Zero-shot Do the cases \"{citing_case_name}, {citing_case_citation} ({citing_case_year})\" and \"{cited_case_name}, {cited_case_citation} ({cited_case_year})\" agree or disagree with each other? Say \"agree\" or \"disagree\" only. Few-shot Do the two given cases agree or disagree with each other? Say \"agree\" or \"disagree\" only. Examples: ``` Case 1: Brown v. Board of Education, 347 U.S. 483 (1954) Case 2: Plessy v. Ferguson, 163 U.S. 537 (1896) Answer: Disagree Case 1: Youngstown Sheet & Tube Co. v. Sawyer, 343 U.S. 579 (1952) Case 2: Medellin v. Texas, 552 U.S. 491 (2008) Answer: Agree Case 1: Whitney v. California, 274 U.S. 357 (1927) Case 2: Brandenburg v. Ohio, 395 U.S. 444 (1969) Answer: Disagree ``` Case 1: {citing_case_name}, {citing_case_citation} ({citing_case_year}) Case 2: {cited_case_name}, {cited_case_citation} ({cited_case_year}) Answer: Figure 22: Doctrinal agreement task prompt templates. Zero-shot What was the factual background in {case_name}, {case_citation} ({case_year})? No more than two sentences. Figure 23: Factual background task prompt template. Zero-shot What was the procedural posture in {case_name}, {case_citation} ({case_year})? No more than two sentences. Figure 24: Procedural posture task prompt template. Zero-shot What was the subsequent appellate history in {case_name}, {case_citation} ({case_year})? No more than two sentences. Figure 25: Subsequent history task prompt template. 33 Zero-shot What was the core legal question in {case_name}, {case_citation} ({case_year})? No more than two sentences. Figure 26: Core legal question task prompt template. Zero-shot What was the central holding in {case_name}, {case_citation} ({case_year})? No more than two sentences. Figure 27: Central holding task prompt template. Zero-shot What did Justice {fake_dissenting_author} argue in their dissent in {case_name}, {case_citation} ({case_year})? Figure 28: False dissent premise task prompt template. 34 Few-shot Analyze pairs of statements about historical U.S. Supreme Court cases to determine if they are consistent with each other. Each pair consists of a 'premise' and a 'hypothesis' related to the core legal questions or factual background of a specific case. Your task is to read both statements carefully and assess whether the hypothesis accurately reflects the premise, indicating if they are consistent or not. Provide a brief explanation before reaching a conclusion. Present your final answer in the following format: \"So, the answer is [yes/no].\" ### Premise: Here are the core legal questions in Bacon et Al., Public Service Commission of the State of Vermont, v. Rutland Railroad Company, 232 U.S. 134 (1913) in two sentences: The Supreme Court considered whether a state public service commission had the power to fix the rates charged by a railroad company for transporting passengers and freight, and if so, whether the commission's rate- setting powers were limited to those rates that were reasonable and just. Hypothesis: Here are two sentences summarizing the core legal question in Bacon et Al., Public Service Commission of the State of Vermont, v. Rutland Railroad Company, 232 U.S. 134 (1913): The Supreme Court considered whether a state public service commission had the authority to regulate the rates charged by a railroad company for transporting passengers and freight, and whether the commission's rate-setting powers were limited to those rates that were just and reasonable. Q: Is the premise consistent with the hypothesis? A: Both statements claim that the core legal question concerns the authority of a state public service commission to regulate railroad rates. So, the answer is yes. ### Premise: {response_1} Hypothesis: {response_2} Q: Is the premise consistent with the hypothesis? A: Figure 29: Contradiction elicitation prompt template. 35 Few-shot {few_shot_examples} ### Premise: The primary legal holding in Serbian Eastern Orthodox Diocese for the United States of America and Canada et al. v. Milivojevich et al., 426 U.S. 696 (1975) was that the secular courts could not intervene in internal church disputes regarding the appointment and removal of clergy, as these decisions fell under the purview of the First Amendment's protection of religious freedom. Hypothesis: The primary legal holding in Serbian Eastern Orthodox Diocese for the United States of America and Canada et al. v. Milivojevich et al. was that civil courts have no jurisdiction over disputes concerning church doctrine, governance, and internal administration. Q: Is the premise consistent with the hypothesis? A: The premise and the hypothesis both assert that secular or civil courts should not intervene in certain internal matters of religious organizations, specifically citing the protection of religious freedom under the First Amendment. The premise focuses on the appointment and removal of clergy, while the hypothesis broadens the scope to include church doctrine, governance, and internal administration. Despite the slight difference in scope, the underlying principle that courts should not intervene in internal church matters is consistent in both statements. So, the answer is yes. Figure 30: Example of non-contradiction reasoning. See Figure 29 for full prompt; text in yellow is generated by GPT 4. 36 Few-shot {few_shot_examples} ### Premise: The primary legal holding in Gunther v. Liverpool and London and Globe Insurance Company was that an insurer's liability for an insured vessel's loss is limited to the actual value of the vessel at the time of loss, regardless of the insured value stated in the policy. Hypothesis: The primary legal holding in Gunther v. Liverpool and London and Globe Insurance Company (1889) was that a foreign insurance company is entitled to sue in federal court diversity jurisdiction only if it is \"actually doing business\" within the state where the litigation is taking place, not merely conducting some incidental business activities there. Q: Is the premise consistent with the hypothesis? A: The premise discusses the insurer's liability in terms of the actual value of the vessel at the time of loss, while the hypothesis addresses the issue of a foreign insurance company's right to sue in federal court based on diversity jurisdiction. These are two distinct legal issues. The premise is about the scope of insurance coverage, and the hypothesis is about jurisdictional requirements for a foreign company to sue in federal court. Therefore, the premise and the hypothesis are not consistent with each other. So, the answer is no. Figure 31: Example of contradiction reasoning. See Figure 29 for full prompt; text in yellow is generated by GPT 4. 37","libVersion":"0.3.2","langs":""}
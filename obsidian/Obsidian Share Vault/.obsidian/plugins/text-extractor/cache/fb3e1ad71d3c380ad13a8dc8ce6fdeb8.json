{"path":"lit/lit_sources/1905.07886v2.pdf","text":"Conformal Prediction Interval Estimations with an Application to Day-Ahead and Intraday Power Markets Christopher Katha,*, Florian Zielb aUniversity Duisburg-Essen, Chair for Energy Trading and Finance bUniversity Duisburg-Essen, House of Energy Markets and Finance Abstract We discuss the concept of Conformal Prediction (CP) in the context of short-term electricity price forecasting. Therefore, we elaborate the aspects that render Conformal Prediction worthwhile to know and explain why this simple yet very eﬀective idea has worked in other ﬁelds of application and why its characteristics are promising for short-term power applications as well. Its performance is compared with diﬀerent state-of-the-art electricity price forecasting models, such as quantile regression averaging (QRA), in an empirical out-of-sample study for three short-term electricity time series. We combine Conformal Prediction with various underlying point forecast models to demonstrate its versatility and behavior under changing conditions. Our ﬁndings suggest that Conformal Prediction yields sharp and reliable prediction intervals in short-term power markets. We further inspect the eﬀect each of the model components has and provide a path-based guideline on how to ﬁnd the best CP model for each market. Keywords: Energy forecasting, Prediction intervals, Electricity price forecasting, Probability forecasting, Quantile regression, Linear models 1. Introduction Our society is full of forecasts, whether it is for economic data, the weather or customer demand. Unsurprisingly, this general statement also applies to the energy industry. Amjady & Hemmati (2006) describe the demand for accurate price predictions from two perspectives. On the one ∗Corresponding author Email addresses: christopher.kath@rwe.com (Christopher Katha,*), florian.ziel@uni-due.de (Florian Zielb) Preprint submitted to International Journal of Forecasting September 18, 2020arXiv:1905.07886v2 [econ.EM] 17 Sep 2020 2 hand, bilateral deals need to be realistically priced. On the other hand, the necessity for reliable price estimations for a) power producers to maximize their proﬁt in power plant dispatch and b) consumers to hedge and minimize their price uncertainty, becomes evident. Thus, forecasting electricity prices is a highly active ﬁeld of research. Overviews on the status quo and available approaches are supplied by Aggarwal et al. (2009); Weron (2014). Whilst a variety of several point forecasts, i.e., the determination of a concrete numerical estimate for the price, is already available and being constantly improved by academics, uncertainty in forecasting (e.g. expressed as prediction intervals) is only gaining more attention. Inevitably, all forecasts involve uncertainty about their level of preciseness, so why stop at the estimated price itself and not quantify the unknown deviation that comes along with it? This is where prediction intervals (PI) come into play. Based on the idea of an explicit consideration of uncertainty, a prediction interval tries to identify a value range that will most likely cover the observation. Unfortunately, extensive studies of density or interval predictions are still relatively scarce. The most prominent technique is quantile regression averaging (QRA) in Maciejowska & Nowotarski (2016); Maciejowska et al. (2016); Nowotarski & Weron (2014, 2015); Uniejewski et al. (2018). It showed convincing results in various applications and marks the current status quo for energy markets (more information on the mathematical motivation is provided in chapter 4.3.2.). Other models are given by bootstrapping (see a GARCH model in Khosravi et al. (2013)) or quantile regression as in Bunn et al. (2013). For a more detailed discussion on probabilistic forecasting, the interested reader might refer to a comprehensive study in Nowotarski & Weron (2018). This paper contributes to this research ﬁeld in the following ways: We introduce a relatively unknown concept called Conformal Prediction applied to day-ahead and intraday power prices. It is designated to predict intervals based on errors, features weak assumptions on data characteristics and is versatile with regards to the underlying point prediction model. It might be seen as an expansion of an existing point prediction estimator. But how does Conformal Prediction perform under changing market conditions and in comparison to other approaches? Can the approach deal with alternating point forecasts and the specialties of hourly short-term prices? To ﬁnd answers to these questions, the remainder of this paper is structured as follows. To start with, we thoroughly introduce the relatively new concept of Conformal Prediction to the world of forecasting in section 2. Before turning from a general Conformal Prediction toy example towards a more dedicated electricity price scheme, we discuss the characteristics of electricity prices based on three selected markets in section 3. Once the theoretical foundation and time series description are dealt with, we turn our attention towards the detailed models. We discuss the general model setup in 4.1, our point forecasts in 4.2 and close the model description by elaborating our PI estimators in 4.3. Section 5 provides the results of our empirical study based on several performance measures such 3 as the Winkler Score and pinball loss. In that context, we modify a very basic model step by step until it equals Conformal Prediction so that we can assess which speciﬁc aspect has the highest impact on performance. Finally, we conclude our ﬁndings in section 6 and critically assess potential improvements for further research. 2. The concept of Conformal Prediction Conformal Prediction (CP)1 describes an entire framework and was thoroughly analyzed for the ﬁrst time in Gammerman et al. (1998) and later in Shafer & Vovk (2008) and Vovk et al. (2005) for both regression and classiﬁcation problems. The interested reader might also check Kowalczewski (2019) for an application based on this paper. Conformal Prediction was initially introduced in an online or transductive manner, such that diﬀerent data realizations are iteratively presented to the learning algorithm. This is not only computationally costly but also less practice-oriented. Many real-world applications require batch processing, meaning that there is one learning set of historical observations and a function that tries to derive a generic rule applicable to new data. Inductive Conformal Prediction translates the transductive approach into a batch or inductive setting. Please note that we refer to the batch case for regression problems when mentioning CP. But what renders CP special and why should forecasters know about it? We ﬁrstly address some pros: \" CP yields valid prediction intervals that meet the designated conﬁdence level 1 − α . The user predeﬁnes the desired conﬁdence level. \" Only the weak assumption of exchangeability is made, no assumption of underlying distribu- tions is required. \" CP is model-agnostic and can be coupled with every singular prediction model as it solely uses the ﬁnal prediction of a classiﬁcation or regression model. \" The framework itself oﬀers high versatility with its applications in regression, classiﬁcation or an online or batch setting. It post-processes point or classiﬁcation model estimates and is independent from the underlying point forecast model characteristics and assumptions. \" CP, by deﬁnition, computes prediction intervals in an out-of-sample manner which reduces the risk of overﬁtting. To further decrease the risk of overﬁtting, CP can be coupled with 1If we think in a broader sense, Conformal Prediction describes an entire framework with diﬀerent sub-models. For reasons of clarity we will denote our sub-models as ’Conformal Prediction’ as well. Hence, the framework and the model speciﬁc deﬁnition are used analogously in this paper. 4 sampling techniques. Compared to bootstrapping approaches, CP only needs sampled data once which renders its computation very fast in comparison to usual bootstrapping where many iterations are required. Besides these useful properties, CP also has some drawbacks that need to be kept in mind when thinking about a possible application: 8 CP requires more historical observations than other models since we ﬁt a point prediction model, compute its out-of-sample forecasts and derive the interval from it. As a rule of thumb, one needs 25% - 50% more observations. 8 CP is not suitable for time-series with strong structural breaks. We need to split the time- series for computing point predictions and intervals. If there is a regime-switch for instance, the assumption of exchangeability is not valid anymore and we might end up ﬁtting intervals based on an entirely new regime but still assuming the predictions to be valid. 8 If the number of available historical observations is limited, CP is heavily inﬂuenced by outliers or clustering of distribution tail events. Therefore, forecasters must carefully inspect the time-series before ﬁtting any model. 8 CP computes symmetric prediction intervals whereas other approaches such as quantile re- gression or empirical error distribution based approaches separately focus on each quantile. While CP is superior if the true error is approximately symmetrically distributed, it could cause problems in an asymmetric scenario. The most crucial aspect is the extension characteristic. Like an additional layer, CP adds an interval estimate to an existing point forecasting model. A core principle of this second layer is the existence of a non-conformity score λi with i being an index for the number of the observation after sampling. It determines how uncommon an observation is in comparison with the real value. More information on the index notations is provided by Figure 1. Suppose we have (according to Johansson et al. (2014)) • A dataset containing historical observations Z = {(x1, y1) , . . . , (xL, yL)} that we randomly split into portions π and 1 − π - in our case - portions of 75% and 25%. The split ratio was determined in a limited hyper-parameter tuning and worked best for our case: 1. A training set (portion π of Z ) Ztrain = {(x1, y1) , . . . , (xM , yM )} 5 Data in chronological order without sampling Data after random split into training and calibration Data with observed prices Price unknown, out-of-sample epoch training set: i=1,…M calibration set: i=M+1,…L i=L+1 t=1,…,T t=T+1 identical data points since no sampling is done at out-of-sample epoch Figure 1: Detailed description of applied indices. Please note that there is a diﬀerent index notation for sampled and non-sampled data. 2. A calibration set (portion 1 − π of Z ) Zcalib = {(xM +1, yM +1) , . . . , (xL, yL)} . • A random forecast model that exploits Ztrain for training and yields estimate ˆyL+1 . Please note that we train on Ztrain and utilize the data of Zcalib to obtain unbiased out-of-sample estimates ˆyM +1, . . . , ˆyL . • The simplest non-conformity score λi = |yi − ˆyi| , computed from the estimates ˆyM +1, . . . , ˆyL , in Zcalib . A forecaster could also apply other non-conformity deﬁnitions tailor-made to the prediction problem at hand, CP is not limited to the absolute error here. The random split into training set Ztrain and calibration set Zcalib 2 is essential since we explicitly ﬁt a model on Ztrain and exploit Zcalib in an out-of-sample context. It is important to mention that we only use 1, . . . , M for training the point predictors. This could lead to issues when the entire set of historical observations is sparse. In the case of M being very small or seasonally inﬂuenced, CP might not be suitable anymore as the underlying forecasting models are not trained properly. It is hard to make a general recommendation as the necessity of data points depends on the forecasting problem, but for electricity time series we have found that a minimum of 1-2 months shall be available but a year is even better to tackle all aspects of seasonality (more details to be found in Hubicka et al. (2018); Marcjasz et al. (2018)). If one is uncertain about a proper choice of the hyper-parameter π , we recommend to carry out a grid-search with, for instance, steps of 10% from 10/90 to 90/10 and evaluate which choice of portion yields the best errors. The point forecast model is trained to minimize the error made with Ztrain . Only considering Ztrain results in a construction of intervals on the basis of explicitly minimized in-sample errors and 2The notation of training and calibration is sometimes also used for training and parameterization of models. We use the term ’calibration’ exclusively for Conformal Prediction and will use parameterization whenever we want to express that a model needs to be tuned to identify its optimal parameters. 6 Input data of Eq. 8 𝒵 = {(x1, y1), …, (xL, yL)} Training set 𝒵 = {(x1, y1), …, (xM, yM)} Calibration data 𝒵 = {(xM+1, yM+1), …, (xL, yL)} partition π randomly chosen partition 1-π randomly chosen Trained point forecast model Input (x1, …, xM) Output (y1, …, yM) Out-of-sample predictions Input (xM+1, …, xL) Output ( ̂yM+1, …, ̂yL) predict „unseen“ calibration data with trained model Non-conformity score λi = yi − ̂yi obtain absolute errors as non-conformity score where i = M + 1,..,L Point forecast Input xL+1 Output ̂yL+1 compute prediction for L+1 Obtain non-conformity threshold for L+1 Non-conformity threshold for given alpha Final prediction interval yα,L+1 = ̂yL+1 ± λα L+1 train model on training set r(λ) = #{i ∈ {M + 1,…, L} : λi < λ} + 1 #𝒵calib + 1 λα L+1 = min{λ ∈ {M + 1,…, L} : r(λ) ≥ 1 − α} Figure 2: Schematic representation of Inductive Conformal Prediction. Detailed information on input data per market is mention in Equation 6 as well as section 3.1. The detailed hyper-parameters per model are described in Appendix A. We use ’#’ to denote cardinality. causes an unrealistic estimation for unknown data. It does not reﬂect the model behavior in an out- of-sample environment and could overﬁt the prediction interval. Another comment must be made on our choice to split Ztrain and Zcalib in a random way. Johansson et al. (2014) only mention the partitions to be disjointed. However, under exchangeability, the order of each element of Z must not matter. In case of a seasonal time series with yearly, monthly or even weekly patterns, the order makes a diﬀerence. Hence, a random split ensures the assumption of exchangeability and prevents ’seasonal overﬁtting’, i.e., training only with data stemming from a speciﬁc time of the year. All in all, random sampling is supposed to make the model more robust. A detailed depiction of the interaction of both models, their input parameters and the computation of intervals is provided by Figure 2. 7 Minding Figure 2, the main task of CP is to compute a non-conformity score λα L+1 for the ﬁrst out-of-sample instance, i.e., the ﬁrst epoch where an actual prediction is needed. λα L+1 provides a probabilistic threshold so that the non-conformity score for the true value yL+1 will not exceed λα L+1 with conﬁdence 1 − α . The threshold value λα L+1 is identiﬁed by iterating through all known λi values and identify the smallest one under a conﬁdence level restriction in: λ α L+1 = min{λ ≥ 0 : r(λ) ≥ 1 − α}, (1) with r(λ) = # {i ∈ {M + 1, . . . , L} : λi < λ} + 1 #Zcalib + 1 , where # {i ∈ {M + 1, . . . , L} : λi < λ} + 1 is the cardinality of all values λi being smaller than our chosen λ . In mathematical terms, λα L+1,h is the smallest value that satisﬁes the condition r(λ) ≥ 1 − α . A crucial aspect with that regard is the fact that λα L+1 automatically stems from the set of already computed non-conformity scores over M + 1, ..., L . We only try to identify the smallest known λ value that satisﬁes the restrictions but do not compute an entirely new numerical value, instead we set a threshold for future values in L + 1 based on past non-conformity scores. Also, note the addition of ones in the counter and denominator. This simple trick can be seen as a bias correction for small sizes of M + 1, . . . , L . It does not make a diﬀerence with thousands of observations but a simple case with a low cardinality as shown in Figure 3 shows the eﬀect. It allows for higher levels of α giving very limited sample sizes. A toy example might be helpful in understanding this concept. For the sake of simplicity, we assume 9 observations. The instance L + 1 is the one where we only face the given explanatory variables xL+1 and need to forecast an interval for yL+1 . Figure 3 presents a solution minding Eq. (1). The result (in our toy example, only the largest value λL = 4 meets the conditions) forms the interval around the point forecast in ˆyL+1 ± λ α L+1. (2) It is important to mention that Equation (2) presents the simple form of Conformal Predic- tion, denoted as Inductive Conformal Prediction. A more complex version with a normalized non-conformity score, called Normalized Conformal Prediction, will be discussed in section 4.3.3. This symmetric interval comprises the true price with conﬁdence 1 − α under exchangeability in the underlying dataset. A less technical explanation in Shafer & Vovk (2008) exploits the law of large numbers together with exchangeability. Suppose the exchangeable sample space Z of size N = M + L with subspaces Zn = {z1, . . . , zn} for n ≤ N and the event En = {yL+1 /∈ [ˆyL+1(Zn) − λα L+1, ˆyL+1(Zn) + λα L+1] } . The event En is α -rare if the following holds 8 Derivation of Threshold Value λα L+1 Assume the following randomly sampled data and =0.3α r(λ) = #{i ∈ {M + 1,…, L} : λi < λ} + 1 #𝒵calib + 1 ≠ 1 − 0.3 Determine smallest : λα L+1 iteratively try out all given values starting from the largest and identify the smallest that still satisﬁes the inequality , in our case only is feasible λ λ r(λ) ≤ 1 − α λL λα L+1 = min{λ ≥ 0 : r(λ) ≥ 1 − α} r(λL) = #{i ∈ {M + 1,…, L} : λi < λL} + 1 #𝒵calib + 1 = 2 + 1 3 + 1 = 3 4 > = 1 − 0.3 r(λM+2) = #{i ∈ {M + 1,…, L} : λi < λM+2} + 1 #𝒵calib + 1 = 0 + 1 3 + 1 = 1 4 ≠ 1 − 0.3 Only solution (marked with ) satisﬁes the above constraint. Hence, λL = 4 λα L+1 = λL = 4 r(λM+1) = #{i ∈ {M + 1,…, L} : λi < λM+1} + 1 #𝒵calib + 1 = 1 + 1 3 + 1 = 2 4 ≠ 1 − 0.3 Figure 3: A toy example for the determination of Inductive Conformal Prediction’s threshold value that assumes α = 0.3 and a set of given forecasts and observations. A solution for the threshold variable can be obtained based on the formula in Eq. (1). More information on the depicted indices 1, . . . , M, M + 1, M + 2, L, L + 1 is provided in Figure 1. true: P(En | {z1, . . . , zn}) ≤ α. (3) We assume that the event En given a random bag of data never exceeds α . Shafer & Vovk (2008) also show that events E1, ..., EN are mutually independent which then implies that P(EN −1 | EN ) ≤ α . Now if N is large enough and our event En is an α− rare event, the law of large numbers proposes that events will only occur at portion α of N . For a more technical derivation of such validity the interested reader might study Vovk et al. (2005). Whereas other models fail to meet this requirement, CP leaves no concern about validity but the width of the interval itself. It might yield broader intervals than other approaches if the underlying point forecast model is not precise or speciﬁc time-series characteristics are not regarded. But why do we think that CP is suitable for electricity price forecasting? Firstly, we discuss our time series in terms of scope and characteristics and then present an adjusted CP scheme together with a set of point forecasts and other PI expert learners. 9 Forecast for 24h Forecast for 24h … t=0 t=T t=T+1 Training period Calibration period Figure 4: Out-of-sample rolling estimation scheme for our case study. The split into training and calibration applies to NCP models only. Please also note the random split depicted by changing areas of training and calibration. 3. Data and case study framework We examine datasets that comprise electricity spot prices from three diﬀerent power markets: Nord Pool spot day-ahead (year 2012 - 2013) prices, the German EPEX intraday market (year 2013-2016) and the price track of the Global Energy Forecasting Competition 2014 (GEFCom 2014, data for years 2011 -2013). The choice of markets covers geographical and chronological diﬀerences and provides insight into the model performance under varying market conditions. At the same time, we have chosen markets that are at least partially regarded by other authors to have reproducible ﬁndings. Our outcome is comparable to Nowotarski & Weron (2014) for the Nord Pool market. While there is no EPEX intraday study available at the time of writing, Nowotarski & Weron (2018) provide a benchmark for the GEFCom dataset. The case study employs a common rolling estimation framework which recalculates the model parameters on a daily basis and consequently shifts the entire training, calibration and forecast window by 24 hours, as shown in Figure 4. The parameterization period (i.e., training and calibration phase) spans 330 days and yields 182 days of Nord Pool forecasts and 831 daily intraday intervals respectively. GEFCom models yield 365 days of out-of-sample predictions. We deliberately expand the estimation window for intraday and GEFCom data to assess whether the models are capable of reaching stable coverage ratios over a longer time horizon. Conformal Prediction models and the naive benchmark are applied to the entire parameterization period, while QRA is based on point forecasts and cuts of eight weeks of parameterization data to train the quantile regression model. For more information on reproducibility one might check the data ﬁles mentioned under supplementary data. Also, note that from now on we add index h to reﬂect every single delivery hour. 3.1. Considered power markets The ﬁrst time series we regard is the Nord Pool Spot system price which is determined in a closed- form day-ahead auction at 12:00 CET. It describes the unconstrained day-ahead price for the entire Nordic bidding zone (e.g. Norway, Denmark, Sweden and Finland). It comprises hourly spot elec- 3.1 Considered power markets 10 tricity prices reported in EUR/MWh from 8.8.2012 - 31.12.2013. The price series can be obtained from the Nord Pool Spot web page (http://www.nordpoolspot.com). Our case study refers to pre- vious work of Nowotarski & Weron (2014) which is why we replicate their basic setup: We calibrate the models from 8.8.2012 - 3.7.2013 and report out-of-sample results for a 182-day period spanning from 4.7.2013 - 31.12.2013. German EPEX intraday trading prices reported in EUR/MWh are the second short-term price series analyzed in this paper. While the Nord Pool market allows entering a single round of bids establishing the prices in a day-ahead auction, the EPEX intraday market is a continuous one that is tradable up to 30 minutes3 prior to delivery. Please note that this lead time changed as per July 2015 from 45 to 30 minutes. We will consider the volume weighted average price (VWAP) of all transactions for the speciﬁc delivery hour. The data series can be obtained from the EEX historical data service and ranges from 21.7.2013 - 30.9.2016. The initial training and calibration window spans data from 21.7.2013 - 22.6.2014. We conducted the out-of-sample test over 831 days to have valid ﬁndings not inﬂuenced by any annual or seasonal eﬀects. In contrast to the Nord Pool data, we apply a set of external factors for the German intraday market. The model is en- riched with the ENTSO-E total load forecast obtainable from https://transparency.entsoe.eu/ and estimated wind injection (freely available for download at https://www.eex-transparency.com/). These determinants are not only assumed to improve accuracy but also increase complexity of the forecast model. Hence, we can validate our model behavior under the usage of price information or multi-dimensional regressor matrices. Please note that we have decided to ignore photovoltaics pro- duction as this requires a more complex regression setup. Usually one would leave a photovoltaics variable out of the model during night times when there is no generation and add it in daylight hours. We have sacriﬁced the additional input for the sake of a similar regression setup in all three power markets. The last dataset stems from the Global Energy Forecasting competition 2014 and is available for download in the appendix of Hong et al. (2016). It covers hourly zonal prices in USD/MWh, zonal load forecasts and system load predictions. The original market or exchange has never been communicated by the authors but due to its usage in a large-scale price forecasting competition it serves as a transparent, reproducible benchmark dataset. We use all available data points which implies a time period from 1.1.2011 - 17.12.2013. We follow the study in Nowotarski & Weron (2018) and compute out-of-sample estimations from 18.12.12 - 17.12.13 to have comparable ﬁndings. But why have we chosen these price series? Figure 5 depicts how diﬀerent the markets are. The three price series suggest a mean-reverting tendency but the intraday (ID) time series features 3As of July 2017 EPEX allows to trade up to 30 minutes before delivery from one German control area to another while the deadline for intra-control area trades has shrunk to 5 minutes. 3.2 Pre-processing 1120132014 0 20 60 100 140Nordpool Day−Ahead Price CharacteristicsDelivery Hours [8.8.2012−31.12.2013] Price [EUR/MWh]a) Nordpool day-ahead[8.8.12-3.7.13] training & calibration[4.7.12-31.12.13] out-of-sample forecast201420152016 −50 0 50 100 150EPEX IntradayDelivery Hours [21.7.2013−30.9.2016] Price [EUR/MWh][21.7.13-22.6.14] training & calibration[23.6.14-30.9.16] out-of-sample forecastb) EPEX Intraday2011201220132014 0 50 150 250 350GefCOM Day−Ahead Price CharacteristicsDelivery Hours [1.1.2011−17.12.2013] Price [USD/MWh]c) GEFCom Day-ahead[1.1.11-17.12.13] training & calibration[18.12.12-17.12.13] out-of-sample forecast Figure 5: Price plot of the Nord Pool, EPEX intraday VWAP and GEFCom day-ahead time series separated into training and forecast sections. The blue partition marks the initial training period that is consequently shifted with each iteration of the rolling estimation. The red parts are used for out-of-sample testing. higher volatility and negative prices. GEFCom data equals the intraday data in volatility but shows an overall higher price level. Table 1 supports the assumption of divergent characteristics. Both the standard deviation (SD) and the interquartile range (IQR) are much higher for intraday and GEFCom data. Interestingly, the spread between the 1st and 3rd quantile is much higher with GEFCom prices, while the diﬀerence between minimum and maximum is lower than the other two markets. Hence, we do not only have entirely diﬀerent price series in terms of geographical and time characteristics, but also the statistics support the impression of diversity. 3.2. Pre-processing The applied time series exhibits hourly granularity which renders a slight transformation necessary. Daylight saving time causes one doubled hour and one missing value. We partly follow Weron (2007) and average the duplicate hours. The latter is computed using multiple imputations as mentioned in Buuren & Groothuis-Oudshoorn (2011). The multiple imputations approach is also applied to 3.2 Pre-processing 12 Nord Pool day-ahead EPEX intraday GEFCom 2014 mean 36.38 31.81 44.83 SD 8.64 14.52 15.26 1st quartile 32.55 24.04 33.42 3rd quartile 39.90 38.98 53.93 min 1.38 -155.52 12.52 max 138.76 155.52 85.53 Table 1: Descriptive statistics of the analyzed price series. all other missing data points present in the time series. Figure 5 elicits concern for outliers in our datasets. Conformal Prediction exploits descending errors and could sacriﬁce preciseness to outliers. Hence, we tried the IQR based Tukey method (see Hoaglin (2003) for a more detailed description). Outliers are deﬁned by 1.5*IQR (like whiskers in common box-plot graphics) and are replaced by multiple imputations after removal. This process usually ensures greater generalization abilities and less chance of wide intervals. It also marks an adjustment to the underlying time series and needs to be treated very carefully. Given our data and the parameterization window, forecast performance (based on coverage, PI width and the Winkler Score) was decreased by around 5 - 10 %, which is why we decided to leave outliers unchanged in our ﬁnal time series. Prices and input factors for regression are usually transformed since many models demand stable variance. We apply a Box-Cox-based power transformation denoted as Yeo-Johnson transformation. The great beneﬁt of that approach over the plain Box-Cox one is the capability to deal with negative prices or zero values. It is deﬁned in Yeo & Johnson (2000) as ψ(ηh, yt,h) =    ((yt,h + 1)ηh − 1)/ηh if ηh ̸= 0, yt,h ≥ 0 log(yt,h + 1) if ηh = 0, yt,h ≥ 0 −((−yt,h + 1)2−ηh − 1)/(2 − ηh) if ηh ̸= 2, yt,h < 0 −log(−yt,h + 1) if ηh = 2, yt,h < 0. (4) with an allowed range of 0 ≤ ηh ≤ 2 . Following Yeo & Johnson (2000), the parameter ηh 4 is estimated by maximizing the maximum-likelihood function in ln(θh|yh) = − n 2 log(2π) − n 2 log(σ2 h) − 1 2σ2 h T∑ t=1 {ψ (ηh, yt,h) − µh}2 (5) + (ηh − 1) T∑ t=1 sgn(yt,h)log (|yt,h | + 1), 4Please note that we do not stick to the original notation in Yeo & Johnson (2000) and denote the scaling parameter as ηh as λi,h is already used for Conformal Prediction. 13 where θh = (ηh, µh, σ2 h) and yh = (y1,h, ..., yT,h)′ . The optimization in Eq. (5) depends on the parameter ηh and its impact on the population mean µh and variance σ2 h . The goal is to optimize ηh in such a way that a variance stabilizing transformation is yielded. For more details on the optimization itself, the reader can refer to Yeo & Johnson (2000). Please note that we optimize each ηh individually per hour h , market (i.e., Nord Pool, GEFCom and EPEX separately) and point forecast model (e.g. a diﬀerent ηh for each model described in section 4.2) using R’s caret package. 4. Prediction models 4.1. General forecasting approach Our choice of input parameters is similar to those in Nowotarski & Weron (2018) and Nowotarski & Weron (2014) in order to have a comparable benchmark result. That being said, we vary the input factors in our regression formula in Eq. (6) only slightly to show the performance with (EPEX and GEFCom) and without (Nord Pool) fundamental factors. The electricity price regression model itself is given by Equation (6) with its connected separation of external fundamental variables per market in Equation (7). yt,h = β1,h + β2,hyt−1,h + β3,hyt−2,h + β4,hyt−7,h ︸ ︷︷ ︸ AR-terms (6) + β5,hymin,t−1,h + β6,hymax,t−1,h ︸ ︷︷ ︸ non-linear eﬀects + β7,hDSat + β8,hDSun + β8,hDM on ︸ ︷︷ ︸ daily dummies + β10,hPCA1,h + β11,hPCA2,h + β12,hPCA3,h ︸ ︷︷ ︸ daily factors + β13,hy24,t−1,h ︸ ︷︷ ︸ end-of-day eﬀect + β14,hδt,h ︸ ︷︷ ︸ threshold eﬀect + βn,hφt,h ︸ ︷︷ ︸ fundamentals +εt,h, with βn,hφt,h :=    0 for Nord Pool β15,hφ1 ,t,h ︸ ︷︷ ︸ zonal load + β16,hφ2 ,t,h ︸ ︷︷ ︸ system load for GEFCom β15,hφ1 ,t,h ︸ ︷︷ ︸ load forecast + β16,hφ2 ,t,h ︸ ︷︷ ︸ wind forecast for EPEX, (7) where yt−1,h , yt−2,h , yt−7,h denote the prices of the identical hour one, two and seven days ago, while βn,h is the respective regression coeﬃcient. The indices h and t describe the hour 4.1 General forecasting approach 14 and day of the underlying electricity price. Non-linear price eﬀects are considered by ymin,t−1,h and ymax,t−1,h being the minimum and maximum price of the previous day and y24,t−1,h the last known price, i.e., the price of hour 24 one day ago. The terms DSat , DSun , DM on are dummy variables (taking a value of 1 in case of their occurrence) to capture the intra-week term structure. PCAk,h is the k−th principal component of yesterday’s 24 prices and comprises reduced daily price information. A threshold variable δt,h picks up the threshold model idea of Nowotarski et al. (2014) and compares the mean of yesterday’s daily prices with its equivalent one week ago to determine low or high volatility price regimes. We use the notation φt,h as a wildcard for all model-speciﬁc fundamental inputs, i.e., none for Nord Pool, zonal and system load forecasts for GEFCom and load and wind generation predictions for EPEX intraday, as additionally described in Equation (7). Please note that we intentionally sacriﬁce customization of the regression model for the sake of harmonization and comparable ﬁndings. Without that restriction one could also add day-ahead prices to the intraday regression or discuss -depending on the time of forecasting- if early intraday prices of the hour to be predicted would be a suitable addition. The regression problem itself requires customization of the underlying process; in our case the prediction of electricity spot prices which inhibits certain speciﬁcs. Short-term electricity time series feature manifold seasonality due to their hourly characteristics, weekly eﬀects and summer/winter times. We model each hour separately as 24 individual processes to minimize hourly or base/peak eﬀects. The hourly granularity is reﬂected by the index h in Eq. (6). While this approach minimizes one source of heteroscedasticity, it causes a diﬀerent problem: Hourly interdependencies caused by ramping costs or similar load events get lost. Traditional thermal power plants exhibit boundaries like start-up times. These might cause one hour to be profoundly aﬀected by the preceding one. Many PI models ignore this source of heteroscedasticity and disregard possible joint distributions as mentioned by Nowotarski & Weron (2018). We follow a diﬀerent approach. A principal component analysis (PCA) acknowledges these eﬀects in yt−1,h ∽ Λk,tFk,t, (8) where Λk,t are the load factors and Fk,t the principal components of yesterday’s prices. The components comprise all daily price information and are determined using all 24 hours. Please note that k = 1, ..., 24 because 24 hours yield 24 components. As with conventional PCA, the ﬁrst few factors comprise suﬃcient information to be included. In our case, three components are utilized. For another application of PCA in the context of electricity price dimension reduction one can check Raviv et al. (2015). 4.2 Individual point forecast models 15 4.2. Individual point forecast models A common basis for many PI estimators are point forecasts in the form of a simple regression where the actual price is a function of input factors xt,h and an error term εt,h . We apply a variety of diﬀerent models starting from a Naive benchmark using past values as prediction, over an advanced penalized linear regression model denoted as Lasso, to a K-nearest neighbor (called KNN) algorithm and a support vector machine regression (SVM). All models are described in a more detailed way in Appendix A. 4.3. Prediction interval models A proper point forecast model is only the ﬁrst step to retrieve prediction intervals. A bit of attention must be paid to the intervals and its notation. A (1 − α) prediction interval implies that the interval contains the true value with probability (1 − α) . Transferring this idea to the calculation of quantiles leads to τ = α 2 for the lower and τ = (1 − α 2 ) for the upper bound. For instance, we calculate the 5 % and 95 % quantile which yields a 90 % prediction interval if the distance between the two quantiles is regarded. A note must also be made on symmetry. Models can estimate quantiles or PIs in a symmetric fashion by adding or subtracting from a point forecast (see Eq. 13 for instance). Other models compute quantiles independently such that we construct the PIs from two quantiles without any point forecast in between. 4.3.1. Empirical error distribution approach As a probabilistic benchmark, we introduce a simplistic, model-agnostic approach called empirical error distribution (the suﬃx _E will be used in the following). Assume an expert learner (in our case, Lasso, KNN or SVM or the naive model) with the described hyper-parameters under section 4.2 and trained with the explanatory variables of Eq. (6). We simply compute the forecast ˆyt,h for both the calibration and training time window. Then, we calculate the forecast-individual residuals εt,h = |ˆyt,h − yt,h| (i.e., using the absolute error) and compute the sample quantile of in-sample errors ˆqτ,h over all εt,h for t = 1, ..., T . Note that qτ,h is not depending on time t but is calculated for the absolute error per hour h . We expand the point forecast for the unknown data to ˆyτ,T +1,h = ˆyT +1,h ± ˆqτ,h to retrieve the upper and lower bounds ˆyτ,T +1,h . This procedure does not demand any assumptions on time series characteristics nor require any greater eﬀort and marks the minimum to be reached for all other models. Please note that we do not use any sampling for our quantile calculation such that one could argue that this automatically leads to overﬁtting or the intervals being too narrow. This deﬁnitely holds true for very small samples. However, given 4.3 Prediction interval models 16 -10 -5 0 5 10 15 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95Deviation from nominal coverage (in %) Percentiles a) Nord Pool SVM_NCP SVM_E QRA LASSO_NCP LASSO_E NAIVE KNN_NCP KNN_E -20 -15 -10 -5 0 5 10 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95Deviation from nominal coverage (in %) Percentiles b) GEFCom -5 -3 -1 1 3 5 7 9 11 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95Deviation from nominal coverage (in %) Percentiles c) EPEX ID Figure 6: Diﬀerences between the empirical coverage of the prediction models and the nominal coverage per percentile. While we have left out the 50th percentile in the initial calculation, it is depicted here using interpolation. This ensures that we do not create any unwanted bias through a steeper step between the 55th and 45th percentile. The hatched gray area reﬂects the minimum possible coverage, i.e., the 5th percentile cannot have a higher negative deviation than 5 percent. 4.3 Prediction interval models 17 our sample size, we follow the asymptotic theory and assume that we do not conduct a large error. Besides, leaving out sampling - one of Conformal Prediction’s key factors - puts us in a position to speciﬁcally analyze its inﬂuence in a dedicated study in sub-chapter 5.4. Another possible point of criticism is the choice of the absolute error as the basis for the quantile computation. We want to compute a symmetric estimator but acknowledge that another residual deﬁnition for εt,h could inﬂuence results, which is why we brieﬂy touch upon asymmetric quantiles in sub-chapter 5.4 as well. We assume the eﬀect to be rather minor as the residuals are nearly symmetric. In such a setting, there is no substantial deviation if one calculates a quantile for absolute values or their unadjusted equivalents. 4.3.2. Quantile regression averaging Recent studies, as well as the GEFCom (see Hong et al. (2016) for results), have shown how powerful the quantile regression averaging (QRA) model of Nowotarski & Weron (2015) is. It stems from the thought of combining forecasts to improve performance (e.g. in Bordignon et al. (2013); Nowotarski et al. (2014)). The approach uses a set of individual point forecasts as an input for a quantile regression. The output is a quantile of either forecast errors (see Maciejowska & Nowotarski (2016) for instance) or price levels (applied in Nowotarski & Weron (2015)). The underlying problem formulation is to be found in Nowotarski & Weron (2014) as qτ,h(yt,h) = ω′ τ,h ˆyt,h + εt,h, ï¿œ (9) with ˆyt,h being the vector of point forecasts computed out of Eq. (6) by the diﬀerent competing prediction models mentioned in section 4.2. Hence, the vector comprises individual point forecasts of the Lasso model, a support vector and a k-nearest neighbor regression. Please note that the naive learner is not included in the vector due to its simplistic character and the expected negative eﬀect on performance. The notation ωτ,h describes a vector of weights with which to multiply the model output. The term qτ (yt,h) denotes the conditional quantile of the electricity price distribution given the user-speciﬁed nominal coverage in τ . The weights are determined by an optimization in arg min ω [∑ L t=1ρτ (yt,h − ω′ ˆyt,h )] , (10) where 1, ..., L describes the in-sample period and ρτ (z) = (τ −I{z<0})z . Equation (10) is equivalent to a likelihood function of a linear regression with asymmetric Laplace-errors and yields numerical values for the upper and lower bounds. Please also note that QRA does not explicitly account for heteroscedasticity. It necessitates point forecast estimates as input factors but if these models do 4.3 Prediction interval models 18 not consider the diﬀerent price realizations of weekdays and hours, the model might end up biased for electricity prices. 4.3.3. Normalized Conformal Prediction The toy example is helpful in understanding the basic concept but equally important is to ﬁne- tune the CP approach to electricity prices. They feature high volatility and a strong seasonality observable in weekly and daily patterns. Weekends tend to show lower price levels just like night hours when less electricity is needed. Therefore, the Inductive CP model introduced in chapter 2 requires taking into account new data and information to address the issue of heteroscedasticity. We aim to minimize any bias by an extended Conformal Prediction scheme referred to as Normalized Conformal Prediction (NCP) in Papadopoulos & Haralambous (2010) that considers new data as well. Hence, we have ﬁrst introduced the basic version Inductive CP in section 2 and now present a more reﬁned version. Whereas the toy example only uses historical data for the determination of non-conformity scores, the expanded version also incorporates the information set applied for the regression model. But what is diﬀerent to the calculus mentioned in section 2? It is mainly the non- conformity score. A non-conformity score λi,h exists for every pair of (xi,h, yi,h) in i = M +1, ..., L . Please note that we deviate from the t, h notation and use i to a) establish a connection to the examples of sub-chapter 2 and b) to highlight the diﬀerent order due to sampled training and calibration that is diﬀerent from the chronological t, h order. More information on the index notations is also provided by Figure 1. The non-conformity score is given by λi,h = |yi,h − ˆyi,h| |ˆεi,h| , (11) with |ˆεi,h| being the absolute value of the estimated error predicted by a second, explicit error estimation model. This section introduces NCP, Eq. (11) and (12) slightly diﬀer from the deﬁnition of section 2. This model predicts the estimated error of our KNN, SVM and LASSO model for the out-of-sample data. The interval forecast is given by yα,T +1,h = ˆyT +1,h ± λ α L+1,h |ˆεL+1,h| . (12) The NCP algorithm depends on two autarkic prediction models, as shown in Figure 7. One of them aims to deliver a point forecast referred to as ˆyt,h in Eq. (12). It might also be regarded as a stand-alone predictor if one disregards the Conformal Prediction framework. In detail, the point forecast for each Conformal Prediction model is provided by either a Lasso regression, an SVM or KNN regression. All model-speciﬁc details and hyper-parameters are discussed in section 4.2. The 4.3 Prediction interval models 19 Input data of Eq. 8 𝒵 = {(x1, y1), …, (xL, yL)} Training set 𝒵 = {(x1, y1), …, (xM, yM)} Calibration data 𝒵 = {(xM+1, yM+1), …, (xL, yL)} partition π randomly chosen partition 1-π randomly chosen Trained point forecast model Input (x1, …, xM) Output (y1, …, yM) Out-of-sample predictions Input (xM+1, …, xL) Output ( ̂yM+1, …, ̂yL) predict „unseen“ calibration data with trained model obtain normalized absolute errors as non-conformity score Trained error model Input (x1, . . . , xM) Output ( ε1 , …, εM ) train error model on absolute in-sample errors from trained point forecast model where εi = yi − ̂yi Point forecast Input xL+1 Output ̂yL+1 compute prediction for L+1 Obtain non-conformity threshold for L+1 Non-conformity threshold for given alpha Final prediction interval yα,L+1 = ̂yL+1 ± λα L+1 ̂εL+1 Error forecast Input xL+1 Output ̂εL+1 compute error prediction for L+1 Out-of-sample error predictions Input (xM+1, …, xL) Output ( ̂εM+1 , …, ̂εL ) supply trained error model with „unseen“ calibration data train model on training set λi = yi − ̂yi ̂εi Normalized Non-conformity score where i = M + 1,..,L Normalized Conformal Prediction with changes to Inductive Conformal Prediction of Section 2 in red Output prediction interval but multiply with predicted error to re-transform after normalization r(λ) = #{i ∈ {M + 1,…, L} : λi < λ} + 1 #𝒵calib + 1 λα L+1 = min{λ ∈ {M + 1,…, L} : r(λ) ≥ 1 − α} Figure 7: Schematic representation of Normalized Conformal Prediction. Detailed information on input data per market is mention in Equation 6 as well as section 3.1. Note that the trained error model and the trained point forecast model are in our case kept identical in terms of tuning parameters and algorithms, i.e., if the trained point forecast model is a Lasso predictor, the trained error model will be a Lasso predictor as well. However, they could also vary. The detailed hyper-parameters per model are described in Appendix A. 20 point forecast models use the explanatory variables of Eq. (6). Based on these price predictions, the errors made in the training process are calculated. The second model uses the residuals of the price forecasts ˆyt,h as the response and forecasts the inaccuracy present in the actual prediction approach given as |ˆεi,h| in Eq. (11) and Eq. (12). Once both models are trained, they equally generate their prediction on the novel calibration dataset. So to sum up, we have the following extensions to Inductive CP of section 2: • a second point forecast model that estimates the error associated with the prediction ˆyt,h , i.e., |εt,h| , • an adjusted normalized non-conformity score mentioned in Eq. (11), • and ﬁnally, a new interval forecast where we multiply with the predicted error to re-transform after normalization in Eq. (12). Please note that we apply identical models for both the price point forecast and the error point forecast and use 75% of all available data points for training and 25% for calibration of the NCP intervals. This means that the KNN_NCP approach uses the exact same model set-up for both point forecasts as shown in Figure 2, i.e., all hyper-parameters of section 4.2 as well as the explana- tory variables of Eq. (6) are used. The interested reader might also refer to the research data in Appendix B where we present a dedicated R-markdown ﬁle that introduces the connected R code step by step and allows for the reproduction of the results. 5. Empirical results 5.1. General performance metrics Prediction intervals need to be reliable and sharp (Nowotarski & Weron (2018)). The term reliability itself refers to the empirical coverage being close or equal to the designated coverage level. It is also noteworthy that reliability and sharpness share a close interdependency. The sharper an interval gets, the less it is near the true coverage. Moreover, we are facing a trade-oﬀ between the two criteria. As a ﬁrst approach to the topic, we compare the empirical coverage with the nominal values under consideration of the PI width in the upper rows of Table 2. From left to right, it depicts values for each model and interval. The ﬁrst impression is that the NCP models yield good coverage. Yet, this is not a uniform statement as the results diﬀer per model and interval. There is no single best model for coverage, nor for sharpness. If we take a closer look at the diﬀerent 5.1 General performance metrics 21 Point Forecast Naive LASSO LASSO KNN KNN SVM SVM QRA Naive LASSO LASSO KNN KNN SVM SVM QRA PI detection E NCP E NCP E NCP E QRA E NCP E NCP E NCP E QRA PI width (€/MWh) 3.3 2.6 2.9 3.1 3.4 2.2 2.1 2.1 12.3 7.7 8.4 7.7 11.9 6.7 8.3 6.2 daily coverage (%) 60.2 51 53.2 48.7 56.2 50.5 50.5 44.9 95.6 89.5 93.1 90 95.9 90.3 93.3 80.9 UC-Test passed (%) 41.6 95.8 95.8 95.8 54.1 100 95.8 83.3 25 95.8 66.7 79.2 37.5 95.8 66.7 16.7 CC-Test passed (%) 29.1 75 66.7 33.3 0 79.1 70.8 50 16.6 75 54.1 66.7 16.6 75 54.1 16.7 Ø Pinball loss 0.79 0.66 0.69 0.77 0.76 0.61 0.6 0.59 0.38 0.27 0.28 0.28 0.33 0.24 0.27 0.26 Ø Winkler score 6.4 5.2 5.4 6.1 6.6 4.8 4.7 4.7 15 10.8 11.2 11.2 13.3 9.4 10.8 10.2 PI width (€/MWh) 12.6 6.9 6.7 12.6 10.8 7.4 6.3 6.6 37 17.7 17.4 32.7 27.9 20.2 17.9 17.5 daily coverage (%) 53.7 52.5 52.9 50.2 48.6 51.8 49.7 47.2 91.2 90 90.8 89.4 89.4 90.2 90.4 83.3 UC-Test passed (%) 75 91.6 75 100 83.3 91.6 34.5 50 91.6 100 91.6 100 95.9 100 37.5 54.1 CC-Test passed (%) 0 54.2 41.6 0 0 75 100 62.5 12.5 58.3 20.8 0 0 58.3 100 33.3 Ø Pinball loss 3.34 1.71 1.7 3.62 3.18 1.88 1.78 1.81 1.72 0.66 0.66 1.59 1.93 0.75 0.71 0.78 Ø Winkler score 26.7 13.7 13.6 26.1 22.6 15 14.3 14.5 54.8 26.5 26.7 49.6 41.3 30.1 28.7 31.5 PI width (€/MWh) 7 10.8 6.9 17.5 14.1 8.9 5.2 12.3 32.5 21.4 21.8 40.2 36.3 24.5 19.4 30.4 daily coverage (%) 43.3 48.6 46 60.1 58.6 46.1 40.7 48.5 83.4 87.7 87.3 85.4 85.4 86.2 82.1 87.4 UC-Test passed (%) 33.3 95.8 66.7 12.5 25 91.6 0 100 12.5 91.7 62.5 45.8 37.5 62.5 0 79.2 CC-Test passed (%) 0 50 0 0 0 45.8 0 50 0 37.5 0 0 0 0 0 12.5 Ø Pinball loss 4.26 3.1 2.77 6.05 5.75 3.45 3.5 3.29 2.44 1.3 1.43 3.56 3.66 1.69 2.23 1.21 Ø Winkler score 34.1 24.3 22.2 48.4 46 27.5 27.9 26.3 97.6 52 57.3 142.7 146.5 67.5 89.2 48.6Nord Pool day-aheadEPEX intradayGEFCom 50% 90% Table 2: Selected prediction interval sharpness and reliability results for empirical two-sided prediction intervals. Please note that the pinball loss is a metrics for each quantile which we have averaged for the respective PI, such that the 90% PI describes the average Pinball loss of the 5th and 95th quantile. markets, it appears as if the EPEX intraday and GEFCom markets are more diﬃcult to predict in a probabilistic manner as their error measures are higher than Nord Pool ones. This intuitively makes sense as these markets are the more volatile ones. Higher volatility seems to widen the diﬀerence between predictions and observations. Our QRA model shows good performance but remains behind the Conformal Prediction models. Please note that we can validate our QRA results by means of the ﬁndings reported in Nowotarski & Weron (2014) for the Nord Pool market as they are very similar. QRA results obtained in Nowotarski & Weron (2018) for the GEFCom dataset were slightly better than our QRA model which might be due to the changed selection of point forecast models. We chose our predictors mostly out of the ﬁeld of machine learning, while the aforementioned authors used a wider set of traditional time series approaches. However, since the results do not fundamentally diﬀer, we see that as further cross-literature validation of our models. A downside of the previous analysis is the strict focus on both the 50% and 90% prediction intervals and their associated 25/75 and 5/95 percentiles. This enforces symmetry and does not evaluate the upper and lower parts of the PI in a separate way, which leaves room for netting eﬀects 5.1 General performance metrics 22 EPEX ID CC−test EPEX ID UC−test GEFCom CC−test GEFCom UC−test Nord Pool CC−test Nord Pool UC−testKNN_EKNN_NCPLASSO_ELASSO_NCPNAIVE_EQRASVM_ESVM_NCP 1 6 12 18 24 1 6 12 18 24 1 6 12 18 24 1 6 12 18 24 1 6 12 18 24 1 6 12 18 24 1 3 5 10 15 20 1 3 5 10 15 20 1 3 5 10 15 20 1 3 5 10 15 20 1 3 5 10 15 20 1 3 5 10 15 20 1 3 5 10 15 20 1 3 5 10 15 20 HourLR.statistics 5 % Significance 50 % Prediction Interval 90 % Prediction Interval Figure 8: Christoﬀersen unconditional coverage (UC) and conditional coverage (CC) test results reported as hourly Likelihood ratio (LR) statistics. Please note that all LR values above 20 are set equal to 20 for the graphical depiction. in errors. In order to assess the prediction quality, one needs to focus on all other percentiles, as done in Figure 6. It depicts the deviation between empirical and nominal coverage computed for all percentiles in steps of 5 and shows the asymmetric estimation quality. The ﬁrst striking fact is that, contrary to Table 2, the Nord Pool and GEFCom markets appear to be harder to predict since the distance to the true coverage is higher than anticipated by Table 2. Most of the models seem to suﬀer around the 55 and 45 percentile which is usually a diﬃcult region to predict due to the high density of observations in that area. We did not compute the 50 percentile as this is typically estimated by median point forecasts and is not directly associated with the Conformal Prediction technique anymore. For reasons of a clear depiction, the 50 percentile area was only interpolated. There is no single best predictor for all diﬀerent markets. Support vector machines tend to show 5.2 Christoﬀersen test 23 a constant level of diﬀerences in comparison with other estimation approaches. If we compare the empirical quantiles with their NCP equivalents it is not possible to favor one over the other. The choice of the best model seems to be heavily connected with the market to be predicted and the underlying point forecast. Our last ﬁnding is associated with the observation of diﬀerences between Table 2 and Figure 6. Obviously, singular percentiles are harder to foretell. But if one, for instance, considers the QRA performance in the EPEX intraday market, an interesting relationship becomes evident. The deviation switches from positive to very negative. If we recall that the 50% PI should cover the range of the 25 and 75 quantiles, we might assume that some of the models beneﬁt from netting eﬀects out of symmetry. This explains why the Nord Pool study reveals higher deviations in Figure 6. The 50% PI is near to the nominal coverage but the two individual quantiles are less close. 5.2. Christoﬀersen test Besides the nominal coverage, there is a commonly used test set provided by Christoﬀersen (1998) which examines unconditional coverage (UC), independence, and conditional coverage (CC). We stick to Weron & Misiorek (2008) and restrict on the ﬁrst observation which renders conditional coverage to be the sum of independence and unconditional coverage. That being said, it is suﬃ- cient to test for unconditional coverage and its conditional equivalent as the latter comprises the independence information. The tests are processed in the Likelihood-Ratio (LR) framework and use a hit series (1 if the interval is correct, 0 otherwise) as input. We also test hourly observations so that no daily eﬀects can falsely create any signals of dependence across several hours. We use the R-package rugarch with its ’VaRTest’ function to compute results. The detailed test statistics are displayed in Figure 8. We plot the test output in the form of LR test statistics against each hour of the day and do so per model and market. The dashed gray line reﬂects the 5% signiﬁcance level of the test statistics and determines the acceptance criterion for both the UC and CC test. All statistics above the gray line point towards a lack of reliability under our test setup. The ﬁrst striking observation in Figure 8 is that Conformal Prediction appears to have a positive eﬀect on the LR statistics. Taking the Lasso, for instance, most of the NCP plots per market are below their empirical counterparts which speaks for the theoretical foundation that postulates true coverage for NCP. We can also observe such performance for SVM_NCP predictions. KNN leaves a mixed impression. It seems to have consistent problems in all markets with the stricter CC test in partic- ular. QRA’s 50% PI values are mostly under the 5% signiﬁcance level. The associated 90% PI test statistics create a diﬀerent impression as they mostly do not meet our acceptance criterion. These ﬁndings are in line with Nowotarski & Weron (2018) in case of GEFCom but partially diﬀer in the Nord Pool case. Nowotarski & Weron (2014) report a higher ratio of accepted hours for the 90% 5.3 Winkler Score and pinball loss 24 -20 -10 0 10 SVM LASSO KNN Winkler Score Decrease / Increase (%) Nord Pool 90% Interval 50% Interval -30 -20 -10 0 10 20 SVM LASSO KNN Winkler Score Decrease / Increase (%) GEFCom -10 0 10 20 30 SVM LASSO KNN Winkler Score Decrease / Increase (%) EPEX ID a) Change in Winkler Score after using Normalized Conformal Prediction (NCP) vs. empirical (E) interval determination b) Best Conformal Prediction (NCP) Predictor vs. common PI estimators 1 6 11 16 21 1 3 5 7 9 11 13 15 17 19 21 23Winkler Score Hour of the day Nord Pool 10 20 30 40 50 60 70 80 90 100 110 120 1 3 5 7 9 11 13 15 17 19 21 23 Hour of the day GEFCom 8 18 28 38 48 58 68 1 3 5 7 9 11 13 15 17 19 21 23 Hour of the day EPEX ID 50% QRA 90% QRA 50% NAIVE 90% NAIVE 50% best NCP 90% best NCP Figure 9: Model performance measured by the Winkler Score as introduced in Eq. (13). Part a) compares the NCP PI determination with the empirical error distribution (E) approach per forecast model (e.g. SVM_NCP vs. SVM_E) to identify the beneﬁts of Conformal Prediction PIs, while section b) sets the best Conformal Prediction method based on the Winkler Score reported in Table 2 in relation to the naive benchmark and QRA. In detail, the best models are LASSO_NCP for EPEX ID and GEFCom, and SVM_NCP in the case of Nord Pool data. PI. Still, this might be caused by the diﬀerent blend of point forecasts. Finally, Naive_E requires a deeper look. Our naive benchmark yields insuﬃcient reliability in all of the considered power markets which delivers evidence to the fact that a more advanced prediction interval determination approach brings additional beneﬁt. If we compare the intra-market results, it becomes evident that GEFCom is the time series with the worst reliability, even for the most performant models. In contrast to that, EPEX ID and Nord Pool seem to have roughly the same range of errors across all models. Unfortunately, we do not know enough about the GEFCom origin to establish any further connection between fundamental characteristics and the problems with reliability. Yet, we can acknowledge that out of our three time series the Christoﬀersen test conﬁrms that GEFCom is the most diﬃcult to estimate. 5.3. Winkler Score and pinball loss All previous assessments have focused either on reliability or sharpness in a separate manner. A metric known as the Winkler Score (see Winkler (1972) for the derivation) allows for the joint 5.3 Winkler Score and pinball loss 25 elicitation of both, given in (cases representation adopted from Maciejowska et al. (2016)) Wt,h =    Bt,h for yt,h∈[Lt,h, Ut,h] Bt,h + 2 α (Lt,h − yt,h) for yt,h < Lt,h Bt,h + 2 α (yt,h − Ut,h) for yt,h > Ut,h , (13) where Bt,h represents the width of the two-sided prediction interval, i.e., Bt,h = Ut,h − Lt,h , and Lt,h, Ut,h its lower and upper bounds. The Winkler Score penalizes deviating coverage and examines the width. All results are depicted in Figure 9. The upper part under section a) tries to contribute to the question of additional beneﬁts of using Normalized Conformal Prediction in combination with diﬀerent point forecasts. Which point forecast models gain the most from Normalized Conformal Prediction and consequently feature the lowest Winkler Score? Figure 9 shows the decrease in the latter if we use NCP instead of the error distribution approach (using an _NCP model instead of an _E one). For GEFCom and Nord Pool, one can observe a decrease in the Winkler Score of about 10% - 20%, occurring mostly with the 90% PIs marked in gray. Interestingly, the 50% PIs increase in most of the cases. Hence, the forecast performance in the mid quantiles suﬀers from NCP. In EPEX intraday markets, NCP additions do not have a positive impact on the Winkler Score which underlines our diverse choice of markets and how diﬀerent the results are. The KNN model even shows an increase in the German intraday market albeit for all other markets there is at least a bit of decrease in the error measure. A possible connection could be established to Figure 6 where the EPEX ID market features low deviation from the true coverage. If we recall that the Winkler Score takes into account coverage we might assume that this market is overall less complex in its prediction characteristics and, therefore, does not beneﬁt from further model complexity. Still, this is just a ﬁrst, trivial explanation and requires more empirical analysis that goes beyond the scope of this paper. All in all, Normalized Conformal Prediction seems to have a positive impact on the Winkler Score in two of the three markets. On the other hand, the performance varies with the underlying point forecast model even in the same market. Section a) gives a good ﬁrst impression but leaves the time structure of a short-term price forecast aside. We want to assess hourly diﬀerences and have plotted a corresponding curve of hourly Winkler Scores for each market in section b). In order to reduce the complexity of the graphical depiction, we have narrowed down the analysis and only compare the best Normalized Conformal Prediction model (LASSO_NCP for EPEX ID and GEFCom, SVM_NCP in case of Nord Pool data) based on Table 2 with QRA and our naive benchmark. Not surprisingly, the Winkler Score curve of the naive approach is much higher, which implies less accuracy. This holds true for all three markets. QRA and NCP are very close: in our Nord Pool and EPEX intraday 5.3 Winkler Score and pinball loss 26 application, NCP features slightly lower curves while with GEFCom data, QRA and NCP are almost equal. If one takes a deeper look at the hourly shape of each individual curve it becomes evident that night hours show lower Winkler Scores. There are spikes in the error measure during the oﬀ-peak/peak time block shifts (around hour 8 and 20) in the Nord Pool market. This eﬀect is often observed in electricity spot markets or day-ahead markets in particular and might be explained by additional power plants ramped up or down to cover peak load during the day. While intraday markets are usually used to cover residual loads or renewables adjustments, day-ahead markets serve as a market place for much larger volumes. Therefore, we observe a strong block shifting eﬀect in the Nord Pool day-ahead data while there is less in the intraday equivalent. Taking the hourly shapes into account, we have to favor QRA or NCP over the naive benchmark, with NCP showing a slightly lower Winkler Score in some instances. Our second test statistic is a very popular one. The pinball loss (PB loss) was chosen to be the oﬃcial scoring rule for the GEFCom 2014 probabilistic forecasting track in Hong et al. (2016) and gained the reputation of a common measure for probabilistic forecasts. Its representation is given by P B(qyt,h(τ ), yt,h) =    (1 − τ )(qyt,h(τ ) − yt,h) for yt,h < qyt,h(τ ) τ (yt,h − qyt,h(τ )) for yt,h ≥ qyt,h(τ ), (14) where (qyt,h(τ ) is the τ -th estimated quantile of the electricity price series yt,h . The pinball loss is a quantile speciﬁc measure but can simply be averaged across hours or quantiles in order to have a more comprehensive sharpness indicator. The analysis of the pinball loss goes in a diﬀerent direction compared with the Winkler assessments since it focuses on percentiles in order to determine how an approach behaves under varying probabilistic assumptions. This modus operandi also shifts the focus towards asymmetric performance and sets each percentile in a performance relation. In contrast to that, Table 2 focuses on prediction intervals which imply symmetry. All ﬁndings are presented in Figure 10. The ﬁrst thing that has to be noted is the diﬀerence in scale. In comparison with the Nord Pool market, the EPEX intraday and GEFCom plots comprise 5 or 9 times higher PB loss scores. This corresponds to the previous impression we had from Table 2 or the numeric values of the Winkler analysis where these markets were more diﬃcult to predict as well. All in all, the conclusion drawn from Figure 10 is similar to the one in Figure 6. The middle percentiles increase the error measure. But there is another connection to this expression. All models except the KNN and the naive one are very close in terms of performance. Yet, there is one pattern. Normalized Conformal Prediction suﬀers in the middle percentiles to an extent that the much simpler _E models have a lower PB loss. The picture changes once the outer percentiles are concerned. If we 5.4 Path dependent evaluation of Conformal Prediction performance drivers 27 recall the results of Table 2, NCP models were yielding, in general, a bit better coverage and PI width. On the other hand, both the 90% and 50% PI only consider the 5/95 or 25/75 quantile, respectively. The picture seems to diﬀer with more median oriented quantiles. Depending on the market, the best performing model can either be NCP, QRA or an error distribution approach which reﬂects that there is no single best predictor when it comes to the PB loss. 5.4. Path dependent evaluation of Conformal Prediction performance drivers The previous sub-chapters have only taken a global view on estimation capabilities and compared the model performance with QRA and a naive benchmark. We have not discussed the question of why Conformal Prediction is performing in a decent manner. Leaving the technical concepts aside, Conformal Prediction features three possible origins from which performance might stem. Firstly, it forces the forecast to be symmetric. We sort the non-conformity measure λi,h and consider the respective value corresponding to the desired PI. The identiﬁed non-conformity score is added or subtracted from the forecast such that there is no designated diﬀerentiation between quantiles. For instance, we determine the 50% PI by subtracting and adding the same λi,h from our point forecast. In contrast to that, an asymmetric approach determines the 25th and 75th quantile in an independent manner. Combining these two quantiles yields the 50% PI in a second step. The second potential source of performance gains is the sampling technique described in chapter two. Conformal Prediction randomly splits the available set of information into training and calibration to ensure a maximum of generalization. But does this step really improve the models? The third aspect of Conformal Prediction is, at least intuitively, a very important one. In the case of Normalized Conformal Prediction, we adjust the non-conformity score by estimated errors as mentioned in Eq. (12). When it comes to the forecast value ˆyt+1,h , this small modiﬁcation ensures that all new information xt+1,h is regarded in the prediction interval determination by ﬁrstly estimating the error for t+1 and then plugging it in in Eq. (12). Without any quantitative backing, one will surely assume that this is a reasonable operation with a positive impact on predictive performance, especially if we consider the strong daily eﬀects of electricity price time series. Heteroscedasticity caused by weekly eﬀects is taken into account since we include the daily dummy in the new information set. We run a simulation path of diﬀerent combinations of the above three model expansions that jointly form the core of Conformal Prediction in the same out-of-sample fashion that was already applied in the empirical analysis in the previous sub-chapters. The regression model of Eq. (6), all hyper-parameters discussed in section 4.2, the transformations, and the out-of-sample rolling window approach remain unchanged. At the same time, we only consider the 3 point forecast models 5.4 Path dependent evaluation of Conformal Prediction performance drivers 28 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1,0 5 10 15 20 25 30 35 40 45 55 60 65 70 75 80 85 90 95Pinball Loss Score Percentiles a) Nord Pool SVM_NCP SVM_E QRA LASSO_NCP LASSO_E NAIVE KNN_NCP KNN_E 0 1 2 3 4 5 6 7 8 9 5 10 15 20 25 30 35 40 45 55 60 65 70 75 80 85 90 95Pinball Loss Score Percentiles b) GEFCom 0 0,5 1 1,5 2 2,5 3 3,5 4 4,5 5 5 10 15 20 25 30 35 40 45 55 60 65 70 75 80 85 90 95Pinball Loss Score Percentiles c) EPEX ID Figure 10: Pinball loss scores per percentile as mentioned in Eq. (14). Please note that we have taken the average over all 24 hours to depict model performance under diﬀerent percentiles and have plotted every 5th percentile except the 50th one. KNN, SVM and Lasso which were used for the previous Conformal Prediction computations. The following models were evaluated in the analysis path: 5.4 Path dependent evaluation of Conformal Prediction performance drivers 29 • asymmetric quantiles: We compute quantiles of non-absolute errors εt,h = yt,h − ˆyt,h where ˆyt,h stems from KNN, Lasso and SVM point forecasts trained on all available data points (i.e., the training and calibration set). Since the error distribution is almost symmetric, we can simply compute the lower bound as a quantile of the negative errors (where the point forecast underestimated the price) and the upper bound as the quantile of the positive errors. The bounds are then added to or subtracted from the point forecast ˆyt,h. • quantiles - sampled: The model is identical to the asymmetric quantiles model besides the fact that we randomly sample 75% of the available data points (i.e., the training and calibration set) for training of the point forecasts and use the remaining 25% calibration set for the calculation of quantiles of errors εi,h = yi,h − ˆyi,h . Hence, this approach avoids overﬁtting, as discussed in section 2. • quantiles - normalized: The model is almost identical to the asymmetric quantiles model but the basis for the quantile calculation εt,h = yt,h − ˆyt,h is normalized with the expected error ˆεt,h produced by a second point forecast model (again either SVM, KNN or LASSO) using the explanatory variables of Eq. (6) to forecast the errors. We use absolute values of the estimated error ˆεt,h to prevent negative values to change the entire prediction to be negative. Hence, we compute the quantiles of (yt,h−ˆyt,h) |ˆεt,h| and multiply the ﬁnal PI forecasts with |ˆεt,h| to upscale the values again. Please note that no sampling or split into calibration and training is applied. Instead, we use in-sample errors to train the second model which yields the expected error ˆεt,h . • quantiles - symmetric: This model is identical to the empirical error distribution approach of section 4.3.1 (denoted as _E models) and uses absolute errors for the computation of quantiles. • Conformal Prediction: The CP model is the same as the one discussed in the toy example in section 2. The only diﬀerence to the NCP of section 4.3.3 is the more simple, non-normalized computation of the non-conformity score such that λi,h = |yi,h − ˆyi,h| . • quantiles - norm-sampled: Assume the model described under ’quantiles-normalized’, i.e., normalized quantiles calculated from errors εi,h = (yi,h − ˆyi,h) and then normalized by means of |ˆεi,h| with an additional 75% / 25% random sampling of the data. The training period is not fully exploited here to avoid overﬁtting. • quantiles norm-symmetric: The norm-symmetric model is the same as the Conformal Prediction one but does not use any sampling. It can also be seen as the toy example of 5.4 Path dependent evaluation of Conformal Prediction performance drivers 30 section 2 except for the sample split. We just use in-sample errors to derive the non-conformity scores. • Normalized Conformal Prediction: This is the model of section 4.3.2. Speaking of values, we utilize our two common error measures, PB loss and Winkler Score, to iden- tify the impact of normalization, sampling and symmetry. In addition, we compare the PI width. Although this is not a traditional error measure per se, it helps in understanding diﬀerences. Fig- ures 11 and 12 illustrate both the diﬀerent simulation paths as well as the connected error measures separated into the 50% and 90% prediction interval. We start the analysis with the most basic form of PI estimation by computing the quantiles of the empirical error distribution5, depicted at the very left. This model neither samples any of the data nor uses new information. Please also note that we receive an asymmetric estimation as we independently compute the quantile for the upper and lower part of the PI, i.e., do not use absolute errors for quantile determination. We initially assumed that this PI predictor is by far the worst one but were proven wrong by our empirical study. In comparison to Normalized Conformal Prediction, the asymmetric empirical quantiles tend to perform very well. In the German intraday market, the results are nearly equal to NCP, while NCP yields lower errors in Nord Pool and GEFCom. But as a ﬁrst result, we can say that the most modest form of probabilistic forecasting is more accurate than expected. Based on the asymmetric quantiles, we separately add all three extension stages to the basic model. The second estimator uses normalization via estimated errors, and its other two equivalents add symmetry and sampling of data. Performance-wise, one can observe a clear picture. Adding symmetry lowers the Winkler Score and PB loss. Interestingly, the interval width is not widened at the same time which reﬂects that we yield more accurate intervals and that the previous one was not just too narrow. Such a clear indication came unanticipated as the technical model diﬀerence is rather small. Instead of stand-alone quantiles we compute absolute empirical errors and add or subtract them from the point forecast. While this is a very small change in terms of computation, its impact is impressive and speaks for symmetry in residuals. Sampling to avoid overﬁtting appears to further increase accuracy, which at least partially refutes our asymptotic argument of making no mistake without sampling. Still, the eﬀect is very small. Normalization or the addition of new information is a bit problematic in a stand-alone application. The asymmetric empirical model computes negative and positive normalization values in the form of estimated errors to be subtracted or added to the probabilistic forecast. These values are then normalized by the expected error, which is either a positive or negative value. It might occur that 5Please note that this diﬀers from the _E model used before due to the lack of symmetry. All _E models are symmetric ones. 5.4 Path dependent evaluation of Conformal Prediction performance drivers 31 the sign of some of the values changes the entire PI to unrealistic estimations which is why we adjust the normalization numbers to be always positive. All in all, the computation of normalized asymmetric quantiles does not really make sense. However, in the interest of completeness, we show the model results. In some cases, we observe a tremendous performance drop after normalization, which further underlines the argument of a known misconstruction. We could end our analysis at this point. But that would imply linear additivity of the speciﬁc model extensions. Is it intuitively possible to add, for instance, symmetry and normalization and yield the sum of each extension’s performance? We expand our models into three diﬀerent paths to answer this question. Firstly, we add the three other extensions. The normalized empirical quantiles are changed to symmetric ones. This step shall further validate our ﬁndings with regards to the ineﬃciency of non-symmetric normalization. And indeed, symmetry solves the issue of mis- constructed PIs. Error measures are lower while the PI width is narrowed down as well, which reﬂects an improvement of sharpness under reliability. A diﬀerent picture is painted if we extend the sampled asymmetric quantiles to a symmetric estimator. Please note that this predictor is the same as the Conformal Prediction mentioned in chapter two. The Winkler Score and PB loss only slightly change in some instances, which high- lights that sampling helps on a case by case basis. In contrast to that, adding normalization to sampled quantiles causes the same bias as with the normalized quantiles. Due to the lack of sym- metry, the sign of the output could change the entire prediction, which causes the Winkler Score and PB loss to be much higher. Hence, our empirical study suggests that the path from empirical quantiles to normalized and then normalized and sampled ones does not make sense to apply. Last but not least, we focus on Normalized Conformal Prediction as our last layer. In some cases, such as the GEFCom predictions, it makes sense to utilize all three extensions jointly. In other scenarios, such as EPEX intraday, the addition of sampling to norm-symmetric intervals was not beneﬁcial with regards to performance. This ﬁnding perfectly matches the impression from the Winkler Score analysis. The less complex markets with regards to estimations, namely Nord Pool and EPEX ID, do not seem to beneﬁt from model extension in the way the GEFCom data set does. So, after computing 144 models, what does the path-dependent analysis suggest? We can assume that there is no singular model that outperforms all the others. Our choice of markets is a very diverse one which causes results to be diﬀerent. The same counts for the point prediction models themselves. There are some universal tendencies, such as beneﬁcial eﬀects of symmetric estimations. That being said, the usage of normalization and sampling only adds value in some of the cases. We advise every forecaster to carefully test the probabilistic models in question, espe- cially if the market to be predicted features statistical similarity to the GEFCom data. Conformal Prediction serves as a good framework but still requires ﬁne-tuning with regards to the optimal 5.4 Path dependent evaluation of Conformal Prediction performance drivers 32 sampling SVM KNN LASSO SVM KNN LASSO PB Loss 0.6 0.76 0.69 PB Loss 0.61 0.78 0.67 normalization Winkler 4.7 6.6 5.4 Winkler 4.9 6.3 5.4 PI width 2.1 3.4 2.9 PI width 2.4 3.5 2.7 symmetry SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO PB Loss 0.6 0.94 0.66 PB Loss 0.6 0.88 0.67 PB Loss 0.65 0.86 0.73 PB Loss 0.61 0.77 0.66 Winkler 4.8 7.5 5.3 Winkler 4.8 7 5.3 Winkler 5.2 6.9 5.8 Winkler 4.8 6.1 5.2 PI width 2.1 4.4 2.5 PI width 2.1 3.9 2.6 PI width 2.6 2.7 2.8 PI width 2.2 3.1 2.6 SVM KNN LASSO SVM KNN LASSO PB Loss 0.69 0.9 0.72 PB Loss 0.63 0.78 0.74 Winkler 5.4 7.2 8.4 Winkler 4.8 6.2 5.9 PI width 2.9 3.6 1.9 PI width 2.2 3.3 2.7 SVM KNN LASSO SVM KNN LASSO PB Loss 1.78 3.18 1.7 PB Loss 1.89 2.9 1.77 Winkler 14.3 22.6 13.6 Winkler 15.1 23.7 14.2 PI width 6.3 10.8 6.7 PI width 7.4 11.2 7.1 SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO PB Loss 1.87 3.12 1.76 PB Loss 1.86 3 1.76 PB Loss 2.05 3.55 2 PB Loss 1.88 3.62 1.71 Winkler 15.2 24.9 13.9 Winkler 14.9 24 14.1 Winkler 16.4 28.4 16 Winkler 15 26.1 13.7 PI width 6.3 11.2 6.7 PI width 6.2 11.6 6.7 PI width 8.6 14.04 8.6 PI width 7.4 12.6 6.9 SVM KNN LASSO SVM KNN LASSO PB Loss 2.07 3.9 2.53 PB Loss 1.88 3.11 1.69 Winkler 16.5 31.9 21.8 Winkler 15.1 24.8 13.5 PI width 8.5 16.14 3.9 PI width 7.3 13.2 6.9 SVM KNN LASSO SVM KNN LASSO PB Loss 3.5 5.75 2.77 PB Loss 3.56 5.93 3.18 Winkler 28.1 46 22.2 Winkler 28.4 47.4 25.5 PI width 5.2 14.1 6.9 PI width 5.6 14.5 6.9 SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO PB Loss 4.2 6.73 3.24 PB Loss 3.55 6.31 3.19 PB Loss 3.65 6.32 3.24 PB Loss 3.45 6.05 3.1 Winkler 33.7 53.9 26 Winkler 28.5 50.6 25.5 Winkler 29.25 50.6 26 Winkler 27.5 48.4 24.8 PI width 4.9 13.8 6.7 PI width 5.3 14.5 6.8 PI width 12.7 26.1 9.6 PI width 8.9 17.5 10.8 SVM KNN LASSO SVM KNN LASSO PB Loss 4.38 7.24 4.58 PB Loss 4.2 6.49 3.14 Winkler 35.1 57.9 31.4 Winkler 33.7 51.9 25.1 PI width 12.1 26.5 5.3 PI width 9.2 17.6 10.2 quantiles - normalized quantiles -norm-symmetric quantiles -symmetric Conformal Prediction quantiles - sampled quantiles - norm-sampledasymmetric quantiles GEFCom 50% PI NCP quantiles - normalized quantiles -norm-symmetric NCP NCP asymmetric quantiles EPEX ID 50% PI quantiles - sampled quantiles - norm-sampled quantiles -symmetricNord Pool 50% PI Conformal Predictionquantiles -symmetric Conformal Prediction quantiles - sampled quantiles - norm-sampled quantiles - normalized quantiles -norm-symmetric asymmetric quantiles Figure 11: Identiﬁcation of Conformal Prediction’s key performance drivers based on a path dependent analysis of the three diﬀerent model extensions, symmetry, normalization and sampling for the 50% PI. We applied the same empirical setup as described in the previous sub-chapters but changed the models bit by bit (each addition is depicted by the colored arrows) to evaluate which part of Conformal Prediction accounts for most of the gains in precision. Please note that the pinball loss is a metric for each quantile which we averaged for the respective PI, such that the 50% PI describes the average pinball loss of the 25th and 75th quantile. The heat-map colors only compare PI models, i.e., we compare KNN asymmetric quantiles with KNN under normalized quantiles but do not compare KNN versus SVM performance. 5.4 Path dependent evaluation of Conformal Prediction performance drivers 33 sampling SVM KNN LASSO SVM KNN LASSO PB Loss 0.27 0.33 0.28 PB Loss 0.28 0.34 0.29 normalization Winkler 10.8 13.3 11.2 Winkler 11.1 13.7 11.4 PI width 8.3 11.9 8.4 PI width 8.9 11.9 8.5 symmetry SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO PB Loss 0.28 0.47 0.29 PB Loss 0.29 0.43 0.29 PB Loss 0.51 0.53 0.47 PB Loss 0.24 0.28 0.28 Winkler 11.1 18.6 11.6 Winkler 11.1 17.3 11.4 Winkler 20.2 21.9 18.9 Winkler 9.4 11.1 11.2 PI width 8.5 16.1 8.6 PI width 8.5 15.1 8.4 PI width 14.9 16.1 15.4 PI width 6.7 8.1 7.7 SVM KNN LASSO SVM KNN LASSO PB Loss 0.55 0.67 1.4 PB Loss 0.24 0.29 0.29 Winkler 21.6 24.7 29.3 Winkler 9.6 11.6 11.8 PI width 15.8 18.8 8.33 PI width 6.3 7.9 7.8 SVM KNN LASSO SVM KNN LASSO PB Loss 0.71 1.13 0.66 PB Loss 0.76 1.1 0.71 Winkler 28.7 41.3 26.7 Winkler 30.4 44.3 28.4 PI width 17.9 27.9 17.4 PI width 19.9 28.7 18.8 SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO PB Loss 0.76 1.18 0.7 PB Loss 0.75 1.17 0.7 PB Loss 1.58 1.92 1.57 PB Loss 0.75 1.59 0.66 Winkler 30.6 47.5 28.2 Winkler 30.1 46.8 28.3 Winkler 63.5 108.94 62.8 Winkler 30.1 49.6 26.5 PI width 18 29.9 17.9 PI width 18.6 32.1 18.1 PI width 55.2 96.8 55.4 PI width 20 32.7 17.7 SVM KNN LASSO SVM KNN LASSO PB Loss 1.57 1.86 1.12 PB Loss 0.73 1.07 0.69 Winkler 63 72.6 61.2 Winkler 29.4 42.7 26.4 PI width 53.9 67.6 26.1 PI width 18.8 28.6 17.6 SVM KNN LASSO SVM KNN LASSO PB Loss 2.23 3.66 1.43 PB Loss 2.27 3.76 1.77 Winkler 89.2 146.5 57.3 Winkler 91.1 150.7 70.8 PI width 19.4 36.3 21.8 PI width 20.5 36.7 22.3 SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO SVM KNN LASSO PB Loss 2.8 4.11 1.8 PB Loss 2.22 3.62 1.77 PB Loss 2.6 5.19 2.12 PB Loss 1.69 3.56 1.3 Winkler 113 165 72.6 Winkler 88.8 144.8 70.7 Winkler 104.4 207.9 84.7 Winkler 67.5 142.7 52 PI width 18 41.8 20.4 PI width 20.3 48.2 21.7 PI width 85.2 176.2 52.2 PI width 25 40.2 21.4 SVM KNN LASSO SVM KNN PB Loss 2.86 6.01 12.1 PB Loss 2.37 3.88 1.45 Winkler 114 240.6 256.4 Winkler 95.1 155.2 57.6 PI width 79.7 196.3 31.8 PI width 34.7 70.3 33.8 EPEX ID quantiles -symmetric Conformal Prediction Nord Pool quantiles -symmetric Conformal Prediction 90% PI asymmetric quantiles quantiles - sampled quantiles - norm-sampled quantiles - normalized quantiles -norm-symmetric NCP quantiles - normalized quantiles -norm-symmetric 90% PI asymmetric quantiles quantiles - sampled quantiles - norm-sampled NCP NCP quantiles - normalized quantiles -norm-symmetric GEFCom quantiles -symmetric Conformal Prediction 90% PI asymmetric quantiles quantiles - sampled quantiles - norm-sampled Figure 12: Identiﬁcation of Conformal Prediction’s key performance drivers based on a path dependent analysis of the three diﬀerent model extensions, symmetry, normalization and sampling for the 90 % PI. The plot is equivalent to Figure 11 besides the diﬀerent PI. We extend the models bit by bit (each addition is depicted by the colored arrows) and show the impact on the displayed metrics. The heat-map colors only compare PI models, i.e., we compare KNN asymmetric quantiles with KNN under normalized quantiles but do not compare KNN versus SVM performance as the analysis shall focus on the performance improvement that is due to diﬀerent PI computations. 34 blend of its key components. 6. Verdict and possible value-chain implications for energy traders 6.1. Conclusion and outlook The underlying research motivation of this paper was a thorough introduction of Conformal Pre- diction with a particular focus on short-term electricity prices. We have discussed the theoretical concept and demonstrated that Conformal Prediction works like a second layer to any given point forecast. By exploiting errors made from these point forecasts, symmetric prediction intervals are computed. The other two novelties in that sense are the sampling which ensures a high level of generalization and the normalization by means of estimated errors. We explicitly consider new information when we adjust the PI with an estimated error for t + 1 . This helps to account for electricity price characteristics like heteroscedasticity caused by contrasting load scenarios on diﬀer- ent days since we include information about such in the probabilistic estimation process. Leaving the theory behind, we test multiple probabilistic forecasting concepts in three independent pricing regimes and establish a connection to the empirical results of Nowotarski & Weron (2014) and Nowotarski & Weron (2018) by adopting a comparable QRA model. We demonstrate that Con- formal Prediction can live up to the expectations and yields valid prediction intervals even with changing point forecast inputs. In comparison with a naive benchmark, the well-known QRA and a simple error distribution approach applied to a similar set of point forecasts, NCP is equal or even better in terms of Winkler Score, Christoﬀersen test or PB loss. Connected to the decent per- formance is the question of key performance drivers. An additional evaluation that independently analyzes Conformal Prediction’s three key aspects, symmetry, normalization and sampling of input data, brings more clarity. We simulated diﬀerent paths leading to a total of 144 computations. The overall picture is rather unclear. Conformal Prediction and its diﬀerent modiﬁcations show varying performance across markets. As a consequence, we advise energy companies to compute a path like the one in sub-chapter 5.4 for their forecasting problems and only then decide on one speciﬁc Conformal Prediction model. We have to acknowledge that these ﬁndings only apply to short-term electricity prices. We deliberately chose to focus on these in order to yield maximum objectivity in our analysis. Future research might look at other possible applications, such as wind forecasting or load prediction. Apart from that, we did not discuss any extension of the known Normalized Conformal Prediction framework. Following the idea of QRA, Conformal Prediction intervals might also be averaged to get even better results. However, current research has only started to focus on aggregated Confor- 6.2 Beneﬁts for power traders 35 mal Prediction but the ﬁrst ﬁndings are promising. The interested reader might take a closer look at Carlsson et al. (2014) for a description of multiple aggregated Conformal Predictors or Vovk (2015) for the idea of combining various p-values or non-conformity scores. It might be worth expanding these ideas to the world of energy-related forecasting. 6.2. Beneﬁts for power traders Besides gaining academic knowledge, is there any practical implication that companies active in the ﬁeld of energy trading could consider for their value-chain? We believe that the model-agnostic character together with the post-processing aspect could be a beneﬁt in energy trading. Assume a power producer with a thermal generation unit for instance. Depending on the technical speciﬁca- tion, most units feature a minimum runtime such that instantaneous changes in their steering are hard to realize (see Petersen et al. (2014) for an example on how runtimes are considered in the underlying optimization problem). Suppose a minimum runtime of three hours. The unit output is sold on the day-ahead market with limit orders depending on the marginal costs of the plant. Bidding in day-ahead markets is easier for traders according to Weber (2010). It allows them to consider technical restrictions like the three-hour minimum runtime using linked orders also known as block-orders, which are not present in continuous intraday trading. In the continuous intraday market, it is possible to buy power back that has already been sold day-ahead and turn oﬀ the unit, which then saves fuel costs and leaves the spread between the day-ahead market and intraday trading as proﬁt. Kiesel & Paraschiv (2017) also mention the start-up costs of thermal units that additionally impede real-time steering based on price changes or diﬀerent load scenarios in the intraday market. Referring to our three hour minimum runtime example, we could buy back if the current price level for the three consecutive hours is lower than the production costs of the unit. However, this is not ideal as it may be the case that the ﬁrst hour is already below production costs and the other two hours will be later, due to the continuous nature of intraday trading. All three hours may be below the production costs at a diﬀerent point in time. If we only consider situations where all three hours are jointly below production costs, commercial opportunities will remain unused. A forecast is needed in this scenario to assess the likelihood of each hour being below the produc- tion costs in later trading stages. Most companies will use a point forecast to do so. The problem is that forecasts imply uncertainty. If the company trades based on a simple point estimation, it does not consider the accuracy of the forecast. A Conformal Prediction-based model could be added to the existing point forecast without any greater computational eﬀort and would allow for easy determination of PIs. We have shown possibilities to compute them in an out-of-sample manner to ensure generalization and tackle heteroscedasticity. If this is done, the power producer can trade 6.2 Beneﬁts for power traders 36 based on PIs. The PIs and their conﬁdence level are automatically valid and shall be selected depending on the risk appetite of each company. While the point forecast only takes into account the mean or median, it might make sense to trade on the 50% PI and evaluate later hours based on larger intervals since their longer trading time naturally implies more risk. All in all, a probabilistic forecast allows for more accurate trading decisions and a greater likelihood of executed intraday trades, i.e., higher power plant revenues as also proposed by Amjady & Hemmati (2006). The great beneﬁt of Conformal Prediction is that it is accurate without requiring much eﬀort, which is why we think that it is perfectly suited to companies that have one existing, proven point forecast and want to ﬁne-tune their trading decisions based on uncertainty and individual risk acceptance. Acknowledgements We thank the participants of the S3 seminar on 08.11.2018 in Wroclaw, Poland for their comments and questions that greatly improved the manuscript. This research did not receive any speciﬁc grant from funding agencies in the public, commercial or not-for-proﬁt sectors. This research article was partially supported by the German Research Foundation (DFG,Germany) and the National Science Center (NCN, Poland) through BEETHOVEN grantno. 2016/23/G/HS4/01005. Appendix A: Detailed Description of Point Forecast Models Naive expert learner A simple model is required to assess if more sophisticated approaches truly add any beneﬁt. There- fore, we assume that the best guess for today’s price is the last available similar day price. Based on the scheme laid out in Nowotarski & Weron (2018) we use yesterday’s hourly price if the day to be predicted is a Tuesday, Wednesday, Thursday or Friday. If not, then the price of the hour of the previous week is assumed to be the forecast. This Naive benchmark does not require any computations nor transformations but regards weekly eﬀects and the daily term-structure due to its multivariate approach. Lasso regression Our second expert learner combines point forecasting with feature selection. Introduced in Tib- shirani (1996), the least absolute shrinkage and selection operator (Lasso) enhances the common 6.2 Beneﬁts for power traders 37 ordinary least squares (OLS) scheme so that unnecessary variables are penalized or even removed. The Lasso estimator expands OLS by adding a linear penalty factor ζt,h ≥ 0 in ˆβlasso = arg min βh        T∑ t=1(yt,h − p∑ j=1 βj,hxt,j,h) 2 ︸ ︷︷ ︸ RSS + ζt,h p∑ j=1 |βj,h| ︸ ︷︷ ︸ Penalty Term        , (.1) where xt,j,h are the Yeo-Johnson transformed explanatory variables mentioned in Eq. (6). Note that the lasso penalty is often denoted by λ but since this parameter is already used for the non- conformity score, the lasso penalty is called ζt,h in this paper. In case of ζt,h = 0 we obtain OLS results, while ζt,h → ∞ causes all variables to be removed from the model. We compute a solution for ˆβlasso using the coordinate descent algorithm implemented in the R package glmnet of Friedman et al. (2010). The algorithm itself leaves the hyper-parameter ζt,h to be optimized. We use a two-fold cross-validation and identify the ideal tuning parameter each hour and day out of an equidistant grid between 0.1 and 0.001 with a step size 0.001. Although this results in more computational eﬀort, a recent study in Uniejewski & Weron (2018) highlights the importance of recursive Lasso hyper-parameter tuning and its beneﬁcial eﬀect on performance. K-nearest neighbor regression The idea of the K-nearest neighbor (denoted as KNN) algorithm is based on the fact that patterns in data will repeat in the future. The model implies that a comparable set of input factors will most likely result in the same output as observed with analogous input factors. Therefore, KNN approaches use a similarity measure to identify observations with similar patterns. The most similar values are then regarded as the prediction. The parameter k deﬁnes how many similar observations are taken into account. If k > 1 , the diﬀerent realizations for the target variable are usually averaged in order to yield an estimate for the true value. We explicitly incorporate this rather simple approach to have an alternative estimator based on a simple mapping rule. We use k = 50 for Nord Pool and GEFCom data and k = 200 for EPEX intraday prices and determine similarity based on Euclidean distance. The KNN model is trained with all the explanatory variables of Eq. (6). The computations are done applying the R-package FNN with the additional setting ’algorithm=c(\"kd_tree\", \"cover_tree\", \"brute\")’. Hence, we do not preliminary chose an algorithm but select the one that has the best in-sample accuracy and apply that for out-of-sample predictions. We use untransformed prices for the KNN calculation as transformations do not add any beneﬁt. For another application of KNN on Spanish day-ahead prices, the interested reader might refer to Bibliography 38 Lora et al. (2002). Support vector machine regression A support vector machine regression (SVM)6 maps the regression data to a high-dimensional space and tries to ﬁnd simple linear decision rules in a new space. Its foundation is given by simple geometric principles such as maximum margin hyperplane. From a computational point of view, we can consider SVM as a convex optimization. Its original application was limited to classiﬁcation problems but later generalized to deal with regression applications (see Vapnik et al. (1997) for instance). The solution obtained is a global one. A large variety of kernel functions renders SVM models be very ﬂexible, which is why most of their practical applications are in the context of hybrid models that combine several model layers together. The choice of kernels also controls the ability to capture non-linear problems or solely linear ones. Typical examples of applications are to be found in Che & Wang (2010) or Zhang et al. (2012). In contrast to that, we apply a simple stand-alone model based on the R package kernlab that uses a radial basis function kernel which is nonlinear. The explanatory variables supplied to the SVM learner are the ones mentioned in Eq. (6). We set sigma = 0.005 and apply a cost of constraint violation of C = 1.25 . The algorithm itself is restricted to a maximum of 1,000 iterations and works on Yeo-Johnson transformed prices. Appendix B: Research data Supplementary data to this article, such as a dedicated R-markdown ﬁle of the Conformal Prediction algorithm, can be found online for the sake of full reproducibility at DOI:10.17632/3wnk2pz6y2.1. Bibliography Aggarwal, S. K., Saini, L. M., & Kumar, A. (2009). Electricity price forecasting in deregulated markets: A review and evaluation. International Journal of Electrical Power & Energy Systems, 31 , 13–22. Aitkin, M. (1996). A general maximum likelihood analysis of overdispersion in generalized linear models. Statistics and computing, 6 , 251–262. Amjady, N., & Hemmati, M. (2006). Energy price forecasting-problems and proposals for such predictions. IEEE Power and Energy Magazine, 4 , 20–29. Bordignon, S., Bunn, D. W., Lisi, F., & Nan, F. (2013). Combining day-ahead forecasts for british electricity prices. Energy Economics, 35 , 88–103. 6Please note that we use SVM as an abbreviation for the regression case of support vector machines, also denoted as support vector regression. Bibliography 39 Breiman, L. (2001). Random forests. Machine learning, 45 , 5–32. Bunn, D., Andresen, A., Chen, D., Westgaard, S. et al. (2013). Analysis and forecasting of electricity price risks with quantile factor models. In Finance Research Seminar Series, University of St. Gallen. Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in R. Journal of statistical software, 45 , 1–67. Carlsson, L., Eklund, M., & Norinder, U. (2014). Aggregated conformal prediction. In IFIP International Conference on Artiﬁcial Intelligence Applications and Innovations (pp. 231–240). Springer. Che, J., & Wang, J. (2010). Short-term electricity prices forecasting based on support vector regression and auto-regressive integrated moving average modeling. Energy Conversion and Management, 51 , 1911–1917. Christoﬀersen, P. F. (1998). Evaluating interval forecasts. International economic review , 39 , 841–862. Dudek, G. (2016). Multilayer perceptron for gefcom2014 probabilistic electricity price forecasting. International Journal of Forecasting, 32 , 1057–1060. Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33 , 1 – 22. Gammerman, A., Vovk, V., & Vapnik, V. (1998). Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence (pp. 148–155). Morgan Kaufmann Publishers Inc. Hoaglin, D. C. (2003). John w. tukey and data analysis. Statistical Science, 18 , 311–318. Hong, T., Pinson, P., Fan, S., Zareipour, H., Troccoli, A., & Hyndman, R. J. (2016). Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond. International Journal of Forecasting, 32 , 896–913. Hubicka, K., Marcjasz, G., & Weron, R. (2018). A note on averaging day-ahead electricity price forecasts across calibration windows. IEEE Transactions on Sustainable Energy, 10 , 321–323. Johansson, U., Boström, H., Löfström, T., & Linusson, H. (2014). Regression conformal prediction with random forests. Machine learning, 97 , 155–176. Keles, D., Scelle, J., Paraschiv, F., & Fichtner, W. (2016). Extended forecast methods for day-ahead electricity spot prices applying artiﬁcial neural networks. Applied energy, 162 , 218–230. Khosravi, A., Nahavandi, S., & Creighton, D. (2013). A neural network-garch-based method for construction of prediction intervals. Electric Power Systems Research, 96 , 185–193. Kiesel, R., & Paraschiv, F. (2017). Econometric analysis of 15-minute intraday electricity prices. Energy Economics, 64 , 77–90. Kowalczewski, J. (2019). Normalized conformalprediction for time series data. accessed online 1st July, 2020. URL: http://oa.upm.es/57817/1/TFM_JAKUB_KOWALCZEWSKI.pdf. Liaw, A., & Wiener, M. (2002). Classiﬁcation and regression by randomforest. R News, 2 , 18–22. URL: http://CRAN.R-project.org/doc/Rnews/. Liu, B., Nowotarski, J., Hong, T., & Weron, R. (2015). Probabilistic load forecasting via quantile regression averaging on sister forecasts. IEEE Transactions on Smart Grid , 8 , 730–737. Lora, A. T., Santos, J. R., Santos, J. R., Expósito, A. G., & Ramos, J. L. M. (2002). A comparison of two techniques for next-day electricity price forecasting. In International Conference on Intelligent Data Engineering and Automated Learning (pp. 384–390). Springer. Maciejowska, K., & Nowotarski, J. (2016). A hybrid model for gefcom2014 probabilistic electricity price forecasting. International Journal of Forecasting, 32 , 1051–1056. Maciejowska, K., Nowotarski, J., & Weron, R. (2016). Probabilistic forecasting of electricity spot prices using factor quantile regression averaging. International Journal of Forecasting, 32 , 957–965. Bibliography 40 Marcjasz, G., Seraﬁn, T., & Weron, R. (2018). Selection of calibration windows for day-ahead electricity price forecasting. Energies, 11 , 2364. Nowotarski, J., Raviv, E., Trück, S., & Weron, R. (2014). An empirical comparison of alternative schemes for combining electricity spot price forecasts. Energy Economics, 46 , 395–412. Nowotarski, J., & Weron, R. (2014). Merging quantile regression with forecast averaging to obtain more accurate interval forecasts of nord pool spot prices. In European Energy Market (EEM), 2014 11th International Conference on the (pp. 1–5). IEEE. Nowotarski, J., & Weron, R. (2015). Computing electricity spot price prediction intervals using quantile regression and forecast averaging. Computational Statistics, 30 , 791–803. Nowotarski, J., & Weron, R. (2018). Recent advances in electricity price forecasting: A review of probabilistic forecasting. Renewable and Sustainable Energy Reviews, 81 , 1548–1568. Papadopoulos, H., & Haralambous, H. (2010). Neural networks regression inductive conformal predictor and its application to total electron content prediction. In International Conference on Artiﬁcial Neural Networks (pp. 32–41). Springer. Petersen, M. K., Hansen, L. H., Bendtsen, J., Edlund, K., & Stoustrup, J. (2014). Heuristic optimization for the discrete virtual power plant dispatch problem. IEEE Transactions on Smart Grid , 5 , 2910–2918. Raviv, E., Bouwman, K. E., & van Dijk, D. (2015). Forecasting day-ahead electricity prices: Utilizing hourly prices. Energy Economics, 50 , 227–239. Shafer, G., & Vovk, V. (2008). A tutorial on conformal prediction. Journal of Machine Learning Research, 9 , 371–421. Smola, A. J., & Schölkopf, B. (2004). A tutorial on support vector regression. Statistics and computing, 14 , 199–222. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58 , 267–288. Uniejewski, B., & Weron, R. (2018). Eﬃcient forecasting of electricity spot prices with expert and lasso models. Energies, 11 , 2039. Uniejewski, B., Weron, R., & Ziel, F. (2018). Variance stabilizing transformations for electricity spot price forecasting. IEEE Transactions on Power Systems, 33 , 2219–2229. Vapnik, V., Golowich, S. E., & Smola, A. J. (1997). Support vector method for function approximation, regression estimation and signal processing. In Advances in neural information processing systems (pp. 281–287). Vovk, V. (2015). Cross-conformal predictors. Annals of Mathematics and Artiﬁcial Intelligence, 74 , 9–28. Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world . Springer Science & Business Media. Weber, C. (2010). Adequate intraday market design to enable the integration of wind energy into the european power systems. Energy Policy, 38 , 3155–3163. Weron, R. (2007). Modeling and forecasting electricity loads and prices: A statistical approach volume 403. John Wiley & Sons. Weron, R. (2014). Electricity price forecasting: A review of the state-of-the-art with a look into the future. International journal of forecasting, 30 , 1030–1081. Weron, R., & Misiorek, A. (2008). Forecasting spot electricity prices: A comparison of parametric and semiparametric time series models. International journal of forecasting, 24 , 744–763. Winkler, R. L. (1972). A decision-theoretic approach to interval estimation. Journal of the American Statistical Association, 67 , 187–191. Bibliography 41 Wu, H., Chan, S., Tsui, K., & Hou, Y. (2013). A new recursive dynamic factor analysis for point and interval forecast of electricity price. IEEE Transactions on Power Systems, 28 , 2352–2365. Yeo, I.-K., & Johnson, R. A. (2000). A new family of power transformations to improve normality or symmetry. Biometrika, 87 , 954–959. Zhang, J., Tan, Z., & Yang, S. (2012). Day-ahead electricity price forecasting by a new hybrid method. Computers & Industrial Engineering, 63 , 695–701. Ziel, F., & Weron, R. (2018). Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks. Energy Economics, 70 , 396–420.","libVersion":"0.3.2","langs":""}
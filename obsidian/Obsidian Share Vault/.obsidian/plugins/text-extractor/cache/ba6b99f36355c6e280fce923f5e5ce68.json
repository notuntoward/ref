{"path":"lit/lit_sources.backup/VacaRubio24timeSeriesKAN.pdf","text":"2024 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING, SEPT. 22–25, 2024, LONDON, UK Kolmogorov-Arnold Networks (KANs) for Time Series Analysis Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, and M`arius Caus Centre Tecnol`ogic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain. Email: {cvaca, lblanco, rpereira, mcaus}@cttc.es ABSTRACT This paper introduces a novel application of Kolmogorov- Arnold Networks (KANs) to time series forecasting, leverag- ing their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representa- tion theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, pro- viding more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The pro- posed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics. Index Terms— Kolmogorov-Arnold Networks, ML, Time series 1. INTRODUCTION Time series forecasting plays a key role in a wide range of fields, driving critical decision-making processes in finance, economics, medicine, meteorology, and biology, among oth- ers, reflecting the wide applicability and its significance across many domains. It involves predicting future values based on the previously observed data points. With this goal in mind, understanding the dynamics of time-dependent phe- nomena is essential and requires unveiling the patterns, trends and dependencies hidden with the historical data. While conventional approaches have been traditionally centered on parametric models grounded in domain-specific knowl- edge, such as autoregressive (AR), exponential smoothing, or structural time series models, contemporary Machine Learn- ing (ML) techniques offered a pathway to discern temporal patterns solely from data-driven insights. Non-ML methods traditionally tackle the time series fore- casting problem and often rely on statistical methods to pre- dict future values based on previously observed data. One This work has been submitted to IEEE for possible publication. Copy- right may be transferred without notice, after which this version may no longer be accessible. of the most well-known techniques is the AutoRegressive In- tegrated Moving Average (ARIMA) model, which combines auto-regression, integration, and moving averages to forecast data. The authors in [1] detailed this approach, providing a comprehensive methodology foundational for subsequent sta- tistical forecasting methods. Extensions of ARIMA, like Sea- sonal ARIMA (SARIMA), adapt the model to handle season- ality in data series, particularly useful in fields like retail and climatology [2]. Exponential Smoothing techniques consti- tute another popular set of traditional (non-ML-based) fore- casting methods. They are characterized by their simplicity and effectiveness in handling data with trends and seasonality. An exponent of this family of techniques is the so-called Holt- Winters seasonal technique, which adjusts the model parame- ters in response to changes in trend and seasonality within the time series data [3, 4]. These models have been widely used for their efficiency, interpretability and implementation. More recently, ML models have significantly impacted the forecasting landscape by handling large datasets and capturing complex nonlinear relationships that traditional methods cannot. In recent years, Deep Learning (DL)-based forecasting models have gained popularity, motivated by the notable achievements in many fields. For instance, neural networks have been extensively studied due to their flexibil- ity and adaptability. Simple Multi-Layer Perceptron (MLPs) were among the first to be applied to forecasting problems, demonstrating significant potential in non-linear data model- ing [5]. Built upon these light models, more complex architec- tures have progressively expanded the capabilities of neural networks in time series forecasting. Typical examples are recurrent neural network architectures such as Long Short- Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), which are designed to maintain information in mem- ory for long periods without the risk of vanishing gradients – a common issue in traditional recurrent networks [6]. On a related note, Convolutional Neural Networks (CNNs), which are fundamentally inspired by MLPs, are also extensively em- ployed in time series forecasting. These architectures are particularly efficient at processing temporal sequences due to their strong spatial pattern recognition capabilities. The com- bination of CNNs with LSTMs has resulted in models that ef-arXiv:2405.08790v1 [eess.SP] 14 May 2024 ficiently process both spatial and temporal dependencies, en- hancing forecasting accuracy [7]. These models have started to outperform established benchmarks in complex forecast- ing tasks, motivating a significant shift towards more com- plex network structures. Unfortunately, as the majority of the models mentioned above are inspired by MLP architecture, they tend to have poor scaling law, i.e., the number of param- eters in MLPs networks do not scale linear with the number of layers, and often lack interpretability. A recent study in reference [8], which was presented just 12 days prior to the submission of this paper, intro- duces Kolmogorov-Arnold Networks (KANs), a novel neural network architecture designed to potentially replace tradi- tional multilayer perceptrons. KANs represent a disruptive paradigm shift, and as a potential game changer have recently attracted the interest of the AI community worldwide. They are inspired by the Kolmogorov-Arnold representation theo- rem [9], [10]. Unlike MLPs, which are inspired by the uni- versal approximation theorem, KANs take advantage of this representation theorem to generate a different architecture. They innovate by replacing linear weights with spline-based univariate functions along the edges of the network, which are structured as learnable activation functions. This design not only enhances the accuracy and interpretability of the networks, but also enables them to achieve comparable or su- perior results with smaller network sizes across various tasks, such as data fitting and solving partial differential equations. While KANs show promise in improving the efficiency and interpretability of neural network architectures, the study acknowledges the necessity for further research into their robustness when applied to diverse datasets and their compat- ibility with other deep learning architectures. These areas are crucial for understanding the full potential and limitations of KANs. Our paper is a prospective study that investigates the ap- plication of KANs to time series forecasting. To the best of authors’ knowledge, not previously explored in the liter- ature. We aim to evaluate the practicality of KANs in real- world scenarios, analyzing their efficiency in terms of the number of trainable parameters and discussing how the ad- ditional degrees of freedom might affect forecasting perfor- mance. Herein, we will assess the performance using real- world satellite traffic data. This exploration seeks to further validate KANs as a versatile tool in advanced neural network design for time series forecasting, although more comprehen- sive studies are required to optimize their use across broader applications. Finally, we note that due to the early stage of KANs, it is fair to compare it as a potential alternative to MLPs, but further investigation is needed to develop more complex solutions that can compete with advanced architec- tures such as LSTMs, GRUs and CNNs. This paper is structured as follows. Section 2 presents the problem statement, providing fundamental background on Kolmogorov-Arnold representation theorem and KANs. Sec- tion 3 will present the experimental setup description. Simu- lation results analyzing the performance of KANs with real- world datasets are shown in Section 4. Finally, concluding remarks are provided in Section 5. 2. PROBLEM STATEMENT We formulate the traffic forecasting problem as a time series at time t represented by yt. Our objective is to predict the future values of the series yt0:T = [yt0, yt0+1, ..., yt0+T ] (1) based solely on its historical values xt0−c:t0−1 = [xt0−c, ..., xt0−2, xt0−1] (2) where t0 denotes the starting point from which future values yt, t = t0, ..., T are to be predicted. We differentiate the his- torical time range [t0 − c, t0 − 1] and the forecast range [t0, T ] as the context and prediction lengths, respectively. Our ap- proach focuses on generating point forecasts for each time step in the prediction length, aiming to achieve accurate and reliable forecasts. Figure 1 shows an exemplary time series. 2.1. Kolmogorov-Arnold representation background Contrary to MLPs, which are based on universal approxima- tion theorem, KANs rely on the Kolmogorov-Arnold repre- sentation theorem, also known as the Kolmogorov-Arnold su- perposition theorem. A fundamental result in the theory of dynamical systems and ergodic theory. It was independently formulated by Andrey Kolmogorov and Vladimir Arnold in the mid-20th century. The theorem states that any multivariate continuous func- tion f , which depends on x = [x1, x2, . . . , xn], on a bounded domain, can be represented as the finite composition of sim- pler continuous functions, involving only one variable. For- mally, a real, smooth, and continuous multivariate function f (x) : [0, 1]n → R can be represented by the finite superpo- sition of univariate functions [9]: f (x) = 2n+1∑ i=1 Φi   n∑ j=1 ϕi,j(xj)   , (3) where Φi : R → R and ϕi,j : [0, 1] → R denote the so-called outer and inner functions, respectively. One might initially perceive this development as highly advantageous for ML. The task of learning a high-dimensional function simplifies to learning a polynomial number of one dimensional func- tions. Nevertheless, these 1-dimensional functions can ex- hibit non-smooth characteristics, rendering them potentially unlearnable in practical contexts. As a result of this prob- lematic behavior, the Kolmogorov-Arnold representation the- orem has been traditionally disregarded in machine learning circles, recognized as theoretically solid, but ineffective in practice. Unexpectedly, the theoretical result in [8] has re- cently emerged as a potential game changer, paving the way for new network architectures, inspired by the Kolmogorov- Arnold theorem. 2.2. Kolmogorov-Arnold network background The authors in [8] mention that equation (3) has two layers of non-linearities, with 2n + 1 terms in the middle layer. Thus, we only need to find the proper functions inner univariate functions ϕi,j and Φi that approximate the function. The one dimensional inner functions ϕi,j can be approximated using B-splines. A spline is a smooth curve defined by a set of con- trol points or knots. Splines are often used to interpolate or approximate data points in a smooth and continuous manner. A spline is defined by the order k (k = 3 is a common value), which refers to the degree of the polynomial functions used to interpolate or approximate the curve between control points. The number of intervals, denoted by G, refers to the number of segments or subintervals between adjacent control points. In spline interpolation, the data points are connected by these segments to form a smooth curve (of G + 1 grid points). Al- though splines other than B-splines could also be considered, this is the approach proposed in [8]. Equation (3) can be rep- resented as a 2-layer (or analogous 2-depth) network, with ac- tivation functions placed at the edges (instead of at the nodes) and nodes performing a simple summation. Such two-layer network is too simplistic to effectively approximate any arbi- trary function with smooth splines. For this reason, reference [8] extends the ideas discussed above by proposing a general- ized architecture with wider and deeper KANs. A KAN layer is defined by a matrix Φ [8] composed by univariate functions {ϕi,j(·)} with i = 1, ..., Nin and j = 1, ..., Nout, where Nin and Nout denote the number of in- puts and the number of outputs, respectively, and ϕi,j are the trainable spline functions described above. Note according to the previous definition, the Kolmogorov-Arnold representa- tion theorem presented in Section 2.1 can be expressed as a two-layer KAN. The inner functions constitute a KAN layer with Nin = n and Nout = 2n + 1, while the external func- tions constitute another KAN layer with Nin = 2n + 1 and Nout = 1. Let us define the shape of a KAN by [n1, ..., nL+1], where L denotes the number of layers of the KAN. It is worth not- ing the Kolmogorov-Arnold theorem is defined by a KAN of shape [n, 2n + 1, 1]. A generic deeper KAN can be expressed by the composition L layers: y = KAN(x) = (ΦL ◦ ΦL−1 ◦ . . . ◦ Φ1)x. (4) Notice that all the operations are differentiable. Conse- quently, KANs can be trained with backpropagation. Despite their elegant mathematical foundation, KANs are simply combinations of splines and MLPs, which effectively ex- ploit each other’s strengths while mitigating their respective Context length Prediction length Fig. 1: Example of normalized satellite traffic series data with the conditioning and prediction lengths denoted in blue, and red, respectively. weaknesses. Splines stand out for their accuracy on low- dimensional functions and allow transition between various resolutions. Nevertheless, they suffer from a major dimen- sionality problem due to their inability to effectively exploit compositional structures. In contrast, MLPs experience a lower dimensionality problem, due to their ability to learn features, but exhibit lower accuracy than splines in low di- mensions due to their inability to optimize univariate func- tions effectively. KANs have by their construction 2 levels of degrees of freedom. Consequently, KANs possess the ca- pability not only to acquire features, owing to their external resemblance to MLPs, but also to optimize these acquired features with a high degree of accuracy, facilitated by their internal resemblance to splines. To learn features accurately, KANs can capture compositional structure (external degrees of freedom), but also effectively approximate univariate func- tions (internal degrees of freedom with the splines). It should be noted that by increasing the number of layers L or the dimension of the grid G, we are increasing the number of parameters and, consequently, the complexity of the network. This approach constitutes an alternative to traditional DL models, which are currently relying on MLP architectures and motivates our extension of this work. 2.3. KAN time series forecasting network We frame our traffic forecasting problem as a supervised learning framework consisting of a training dataset with input-output {xt0−c:t0−1, yt0:T } in the condition and predic- tion lengths. We want to find f that approximates yt0:T , i.e., yt0:T ≈ f (xt0−c:t0−1). For ease of notation, we describe our framework as a two-layer (2-depth) KAN [Ni, n, No](note that to comply with the original paper notation, the input layer is not accounted as a layer per se). The output and input layers will be comprised of No, and Ni nodes corresponding to the total amount of time steps in (1) and (2), while the transformation/hidden layer of n nodes. The inner functions constitute a KAN layer with Nin = Ni and Nout = n, while the external functions constitute another KAN layer with Table 1: Model configurations for satellite traffic forecasting Model Configuration Time horizon (h) Spline details Activations MLP (3-depth) [168, 300, 300, 300, 24] Context/Prediction: 168/24 N/A ReLU (fixed) MLP (4-depth) [168, 300, 300, 300, 300, 24] Context/Prediction: 168/24 N/A ReLU (fixed) KAN (3-depth) [168, 40, 40, 24] Context/Prediction: 168/24 Type: B-spline, k = 3, G = 5 learnable KAN (4-depth) [168, 40, 40, 40, 24] Context/Prediction: 168/24 Type: B-spline, k = 3, G = 5 learnable Prediction length.................. Context length............ Network structure ... ... ... ... ... ... ... Fig. 2: Example of the flow of information in the KAN net- work architecture for our traffic forecasting task. Learnable activations are represented inside a square box. Nin = n and Nout = No. Our KAN can be expressed by the composition of 2 layers: y = KAN(x) = (Φ2 ◦ Φ1)x. (5) where the output functions Φ2 generates the No outputs values corresponding to (1) by doing the transformation from the previous layers. The proposed network can be used to forecast future traffic data in the prediction length, based solely on the context length. Fig. 2 shows a generic represen- tation for any arbitrary number of layers L. 3. EXPERIMENTAL SETUP The dataset has been generated within the context of the Euro- pean project 5G-STARDUST. The inputs are obtained from a satellite operator (SO), as a result of processing real informa- tion from a GEO satellite communication system, which pro- visions broadband services. The dataset is a long time series capturing aggregated traffic data. To preserve privacy, anony- mous clients have been defined with more than 500 connected users, and the traffic has been normalized. The measurements are monthly long, and the time granularity is 1 hour. The traffic has been extracted per satellite beam in Megabits per second (Mbps). Although the data has been collected using a GEO satellite communication system, it is expected that user needs could be used to address LEO sys- tems, as well. It is worth emphasizing that the data collected can be used for AI-driven predictive analysis, to forecast traf- fic conditions, which is essential to avoid congestion and to make efficient use of satellite resources. Endowing the net- work with intelligence will be beneficial to meet the different demands of satellite applications. 4. SIMULATION RESULTS This section investigates the forecasting performance of dif- ferent KAN and MLP architectures for predicting satellite traffic over the six beam areas. Concretely, we have a con- text length of 168 hours (one week) and a prediction length of 24 hours (one day). This translates to T = 24, c = 168, where yt0+T = 192 in (1) and (2). Our focus is on evaluating the efficacy of KAN models compared to traditional MLPs. We designed our experiments to compare models with simi- lar depths but varying architectures to analyze their impact on forecasting accuracy and parameter efficiency. Table 1 sum- marizes the parameters selected for this evaluation. We have data for the six beams over one month. We use two weeks + 1 day for training and one week + 1 day for testing for all beams that were not seen by the network. We train all the networks with 500 epochs and Adam optimizer with a learning rate of 0.001. The selected loss function minimizes the mean abso- lute error (MAE) of the values around the prediction length. 4.1. Performance analysis We analyze the forecasting performance in the prediction length. Figures 3a-c depicts the real traffic value used as input (in green) to the networks, the expected output predic- tion length (in blue) and the values predicted values using a KAN (in red) and MLP (in purple) of depth 4 both – see Table 1 for details on model configuration. In general, our results show that the predictions obtained using KANs bet- ter approximates the real traffic values than the predictions obtained using traditional MLPs. This is particularly evident in Figure 3a. Here, KAN ac- curately matches rapid changes in traffic volume, which the MLP models sometimes moderately over/under-predicted, as the last part of the forecast shows. This capability suggests that KANs are better suited to adapt to sudden shifts in traffic conditions, a critical aspect of effective traffic management. Additionally, the responsiveness of KANs is particularly noticeable in Figure 3b during fast changing traffic condi- 0 25 50 75 100 125 150 175 200 Time step [hours] 0.2 0.4 0.6Normalized trafficReal traffic (past) Real traffic (future) KAN (4-depth) MLP (4-depth) 0 5 10 15 20 Time step [hours] 0.2 0.4 0.6Normalized trafficZoomed-in view of prediction horizon Real traffic (future) KAN (4-depth) MLP (4-depth) (a) Forecast over beam 1. 0 25 50 75 100 125 150 175 200 Time step [hours] 0.0 0.1 0.2Normalized trafficReal traffic (past) Real traffic (future) KAN (4-depth) MLP (4-depth) 0 5 10 15 20 Time step [hours] 0.00 0.05 0.10Normalized trafficZoomed-in view of prediction horizon Real traffic (future) KAN (4-depth) MLP (4-depth) (b) Forecast over beam 2. 0 25 50 75 100 125 150 175 200 Time step [hours] 0.0 0.2 0.4 0.6 0.8Normalized trafficReal traffic (past) Real traffic (future) KAN (4-depth) MLP (4-depth) 0 5 10 15 20 Time step [hours] 0.0 0.2 0.4 0.6Normalized trafficZoomed-in view of prediction horizon Real traffic (future) KAN (4-depth) MLP (4-depth) (c) Forecast over beam 3. Fig. 3: Satellite traffic over three different beams with their forecasted values using a 4-depth KAN and a 4-depth MLP. tions. KAN shows a rapid adjustment to its forecast that is closely aligned with the actual traffic pattern. This is partic- ularly noticeable in the last 6 hours of the prediction length where MLP exhibits a lag failing to capture these immediate fluctuations, which shows its worse performance to capture dynamic traffic variations. Further analysis is shown in Fig- ure 3c, where traffic conditions are more variable and intense, Table 2: Results summary Model MSE (×10−3) RMSE (×10−2) MAE (×10−2) MAPE Parameters MLP (3-depth) 6.34 7.96 5.41 0.64 238k MLP (4-depth) 6.12 7.82 5.55 1.05 329k KAN (3-depth) 5.99 7.73 5.51 0.62 93k KAN (4-depth) 5.08 7.12 5.06 0.52 109k demonstrated the robustness of KAN in maintaining high per- formance despite the complexity and higher volume. This ro- bustness suggests that KANs can manage different scales and intensities of traffic data more effectively than MLPs, making them more reliable for deployment in varied traffic scenarios. To further quantify the performance and advantages of using KANs for the satellite traffic forecasting task we show Table 2. It shows a detailed comparison of MLPs and KANs different architectures used for evaluation over all the beams. The table displays the Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and the number of trainable parameters for each model. Analyzing the er- ror metrics, it becomes clear that KANs outperform MLPs, where the KAN (4-depth) is the best in performance. Its lower values in MSE and RMSE indicates its better ability to predict traffic volumes with lower deviation. Similarly, its lower values in MAE and MAPE suggests that KANs not only provides more accurate predictions but also maintains consistency across different traffic volumes, which is crucial for practical traffic forecasting scenarios. Furthermore, the parameter count reveals a significant dif- ference in model complexity. KAN models are notably more parameter-efficient, with KAN (4-depth) utilizing only 109k parameters compared to 329k parameters for MLP (4-depth) or 238k for MLP (3-depth). This reduced complexity sug- gests that KANs can achieve higher or comparable fore- casting accuracy with simpler and potentially faster mod- els. Such efficiency is especially valuable in scenarios where computational resources are limited or where rapid model de- ployment is required. The results also show that with an aug- mentation of 16k parameters in KAN, the performance can be significantly improved, contrary to MLPs which an increment of 91k parameters does not showcase a significant improve- ment. From a technical perspective, KANs leverage a theoret- ical foundation that provides an intrinsic advantage in mod- eling complex, non-linear patterns typical in traffic systems. This capability likely contributes to their flexibility and ac- curacy in traffic forecasting. The consistency in performance across diverse conditions also suggests that KANs have strong generalization capabilities, which is essential for models used in geographically varied locations under different traffic con- ditions. Moreover, besides obtaining lower error rates, our results also suggest that KANs can do so with considerably smaller number of parameters than traditional MLP networks. 0 25 50 75 100 125 150 175 200 Epochs 0.2 0.4 0.6 0.8 1.0Loss KANs: Loss across nodes and grid sizes n=5-G=5 n=5-G=10 n=5-G=20 n=10-G=5 n=10-G=10 n=10-G=20 n=20-G=5 n=20-G=10 n=20-G=20 Fig. 4: Ablation comparison of KAN-specific parameters. 4.2. KANs parameter-specific analysis We provide an insightful analysis of how different config- urations of nodes and grid sizes affect the performance of KANs, particularly in the context of traffic forecasting. For this analysis, we designed 3 KANs (2-depth) [168, n, 24] with n ∈ {5, 10, 20} and varying grids G ∈ {5, 10, 20} for a k = 3 order B-spline. These results are shown during training time. Figure 4 shows a clear trend where increasing the number of nodes generally results in lower loss values. This indi- cates that higher node counts are more effective at capturing the complex patterns in traffic data, thus improving the per- formance. For instance, configurations with n = 20 demon- strate significantly lower losses across all grid sizes compared to those with fewer nodes. Similarly, the grid size within the splines of KANs has a notable impact on model performance. Larger grid sizes, when used with a significant amount of nodes (n ∈ {10, 20}), consistently result in better performance. However, when the amount of nodes is low (n = 5) the extra complexity of the grid size shows the opposite effect. When having a signifi- cant amount of nodes larger grids likely provide a more de- tailed basis for the spline functions, allowing the model to accommodate better variations in the data, which is crucial for capturing complex temporal traffic patterns. The best performance is observed in configurations that combine a high node count with a large grid size, such as the n = 20, and G = 20 setup. This combination likely offers the highest degree of flexibility and learning capacity, mak- ing it particularly effective for modeling the intricate depen- dencies found in traffic data. However, this superior perfor- mance comes at the cost of potentially higher computational demands and longer training times, as more trainable param- eters are included. These findings imply that while increasing nodes and grid sizes can significantly enhance the performance of KANs, these benefits must be weighed against the increased com- putational requirements. For practical applications, particu- larly in real-time traffic management where timely responses are critical, it is essential to strike a balance. An effective approach could involve starting with moderate settings and gradually adjusting the nodes and grid sizes based on perfor- mance assessments and computational constraints. Besides, we want to highlight that for this study continual learning was not assessed, a possibility mentioned in the original paper [8]. 5. CONCLUSION In this paper, we have performed an analysis of KANs and MLPs for satellite traffic forecasting. The results highlighted several benefits of KANs, including superior forecasting per- formance and greater parameter efficiency. In our analysis, we showed that KANs consistently outperformed MLPs in terms of lower error metrics and were able to achieve better results with lower computational resources. Additionally, we explored specific KAN parameters impact on perfor- mance. This study showcases the importance of optimizing node counts and grid sizes to enhance model performance. Given their effectiveness and efficiency, KANs appear to be a reasonable alternative to traditional MLPs in traffic manage- ment. 6. REFERENCES [1] George EP Box and al., Time series analysis: forecast- ing and control, John Wiley & Sons, 2015. [2] Rob J Hyndman and George Athanasopoulos, Forecast- ing: principles and practice, OTexts, 2018. [3] Charles C Holt, “Forecasting seasonals and trends by exponentially weighted moving averages,” Int. journal of forecasting, vol. 20, no. 1, pp. 5–10, 2004. [4] Peter R Winters, “Forecasting sales by exponentially weighted moving averages,” Management science, vol. 6, no. 3, pp. 324–342, 1960. [5] G Peter Zhang et al., “Neural networks for time-series forecasting.,” Handbook of natural computing, vol. 1, pp. 4, 2012. [6] Sepp Hochreiter and J¨urgen Schmidhuber, “Long short- term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997. [7] Anastasia Borovykh and al., “Conditional time series forecasting with convolutional neural networks,” arXiv preprint arXiv:1703.04691, 2017. [8] Ziming Liu and al., “Kan: Kolmogorov-arnold net- works,” arXiv preprint arXiv:2404.19756, 2024. [9] Andre˘ı Nikolaevich Kolmogorov, On the representation of continuous functions of several variables by superpo- sitions of continuous functions of a smaller number of variables, American Mathematical Society, 1961. [10] J¨urgen Braun and Michael Griebel, “On a constructive proof of Kolmogorov’s superposition theorem,” Con- structive approximation, vol. 30, pp. 653–675, 2009.","libVersion":"0.3.2","langs":""}
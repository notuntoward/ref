{"path":"lit/lit_sources/Fujiwara23ContrastiveMultipleCorrespondence.pdf","text":"RESEARCH ARTICLE Contrastive multiple correspondence analysis (cMCA): Using contrastive learning to identify latent subgroups in political parties Takanori FujiwaraID 1☯, Tzu-Ping LiuID 2☯* 1 Linko¨ping University, Norrko¨ ping, Sweden, 2 University of Taipei, Taipei, Taiwan ☯These authors contributed equally to this work. * tpliu@utaipei.edu.tw Abstract Scaling methods have long been utilized to simplify and cluster high-dimensional data. How- ever, the general latent spaces across all predefined groups derived from these methods sometimes do not fall into researchers’ interest regarding specific patterns within groups. To tackle this issue, we adopt an emerging analysis approach called contrastive learning. We contribute to this growing field by extending its ideas to multiple correspondence analysis (MCA) in order to enable an analysis of data often encountered by social scientists—con- taining binary, ordinal, and nominal variables. We demonstrate the utility of contrastive MCA (cMCA) by analyzing two different surveys of voters in the U.S. and U.K. Our results suggest that, first, cMCA can identify substantively important dimensions and divisions among sub- groups that are overlooked by traditional methods; second, for other cases, cMCA can derive latent traits that emphasize subgroups seen moderately in those derived by tradi- tional methods. Introduction Scaling has been a prolific and popular topic throughout political and social science. Scholars have developed and utilized a diverse set of methods to uncover similarities/differences among political actors in a latent space. This has been accomplished by utilizing one of two general ideas: accumulative and unfolding models [1–3]. The widely utilized accumulative model is the classic item response theory (IRT) model, which re-scales the data by arranging a series of questions in order of their level of discrimination regarding the measurement of certain latent traits [1]. On the contrary, unfolding models (e.g., the NOMINATE class of models and Opti- mal Classification (OC)) assume that each individual has an ideal point and this individual’s preference is modeled by a single-peaked function, which translates proximity information into distances in a latent space [2, 4]. While these methods have been most frequently applied to roll-call votes in the United States Congress or other political bodies, scaling methods that do not belong to the above categories such as PCA, MCA, black-box scaling, and ordinal IRT (OIRT) have been increasingly used to analyze a range of new data, especially surveys, to explore underlying political patterns among citizens. PLOS ONE PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 1 / 20 a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 OPEN ACCESS Citation: Fujiwara T, Liu T-P (2023) Contrastive multiple correspondence analysis (cMCA): Using contrastive learning to identify latent subgroups in political parties. PLoS ONE 18(7): e0287180. https://doi.org/10.1371/journal.pone.0287180 Editor: Lorien Shana Jasny, University of Exeter, UNITED KINGDOM Received: October 20, 2022 Accepted: May 31, 2023 Published: July 10, 2023 Peer Review History: PLOS recognizes the benefits of transparency in the peer review process; therefore, we enable the publication of all of the content of peer review and author responses alongside final, published articles. The editorial history of this article is available here: https://doi.org/10.1371/journal.pone.0287180 Copyright: © 2023 Fujiwara, Liu. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability Statement: Data are located at: https://github.com/tzuliu/Contrastive-Multiple- Correspondence-Analysis-cMCA. Funding: This study was supported by the Taiwan National Science and Technology Council in the Ever since the work by Downs [5], the spatial modelers have theoretically and empirically posited the existence of a single-dimensional ideological space [6–8], and the latent left-right dimension of ideology is often recovered as an important principal component (PC) in politi- cal data [9–11]. Indeed, almost all of the above methods recover ideology as their first PC in the derived latent space. These lower-dimensional representations can both help us under- stand the general structure of a given dataset, such as its distribution, and identify underlying patterns, such as clusters and outliers [12, 13]. However, as in the aforementioned ideology PC, the general structure and related patterns may not be particularly interesting or important to researchers. Rather, using alternatives, researchers may be interested in exploring specific structure and patterns within each of groups (e.g., citizens supporting different political parties). A variation that is overlooked across groups may be significant within groups, as seen in substantive applications in political science. Indeed, traditional scaling methods sometimes only uncover certain intraparty divisions that are related to the “mainstream” pattern (e.g., when applying scaling methods only to Democrats or Republicans, the explored subgroups are usually consistent with ideological differences). This demonstrates how traditional methods may merely concentrate on the general patterns among all actors but then necessarily do not identify latent trends within groups and their corresponding causes, such as factors that divide a group into subgroups. It is important to note that we by no means consider existing models to be problematic or inferior—we expect an alternative approach that focuses on variation spe- cific within each group to be especially fruitful in cases where subgroups may lurk in the data and be overlooked by ordinary approaches. In this article, we use contrastive learning to analyze potential differences above and beyond these general patterns. While these mentioned scaling methods analyze the data as a whole, contrastive scaling instead contrasts two groups within the data to utilize their differences to find intra-group patterns. The idea of “contrastive scaling” is straightforward—through com- parison of two groups, contrastive scaling identifies principle directions/components on which the data of one group varies largely but only slightly for the data of the other group [14]. Practically, contrastive scaling first splits data into different groups, usually by predefined clas- ses (e.g., party ID), and then compares the data of the target group against the background group to identify PC(s) on which the target group has relatively high variance and the back- ground group has relatively low variance. Compared to the most prolific contrastive scaling method—contrastive PCA (cPCA) [14] —we contribute to the field of contrastive learning by extending cPCA’s idea from PCA to MCA to develop contrastive MCA (cMCA). cMCA allows researchers to apply contrastive scaling to noncontinuous data—binary, ordinal, and nominal data—that is encountered fre- quently but cannot be analyzed by cPCA properly. With cMCA, researchers can keep the com- pleteness of data as much as possible while analyzing the survey with contrastive scaling (i.e., without dropping noncontinuous data). This is important as listwise deletion (due to variable- type cannot be analyzed) can bias the distribution and further cause false recovery. As a mem- ber of the contrastive scaling family, cMCA first splits the data into predefined groups, such as partisanship or treatment and control groups; second, as in the case with MCA, cMCA applies a one-hot encoder to convert a categorical dataset into a binary-format matrix, called a dis- junctive matrix; finally, cMCA takes one of the groups as the target and then compares this group with the background group to derive PCs on which the positions of members’ ideal points from the target group vary the most but those from the background group vary the least. To demonstrate the utility of cMCA, we analyze two citizen surveys, the 2020 Cooperative Election Survey (CES 2020), which investigates general political attitudes, various PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 2 / 20 form of a grant to T-PL [110-2410-H-845 -030 -MY2] and by The Knut and Alice Wallenberg Foundation in the form of a grant to TF [KAW 2019.0024]. Competing interests: The authors have declared that no competing interests exist. demographic factors, assessment of roll call voting choices, political information, and vote intentions of American voters, and the U.K. module of the 2018 European Social Survey (ESS-UK 2018), which is a biennial cross-national survey of attitudes and behaviors across member countries. The results show that other than the general pattern among all the respon- dents, which is derived through traditional scaling, cMCA objectively detects subgroups along with certain directions that are overlooked by traditional approaches. By objectivity, we mean that instead of picking variables/attributes based on researchers’ own (subjective) prior beliefs regarding the cause of intra-polarization, cMCA (or contrastive learning in general) explores these attributes by recognized statistical properties/criteria, e.g., variables which cause the larg- est variance to the distribution. Such subgroups include those corresponding to the attitudes on Trump’s two recent Supreme Court nominations and on Trump’s job approval among Republicans in the U.S.; the pro- and anti-EU attitudes among supporters from each of the Conservative and Labour parties in the U.K. Regardless of whether traditional scaling finds a clear pattern among the respondents or not, cMCA derives its own salient factors to identify subgroups from a specific group. This article makes an important contribution to scaling meth- ods, unsupervised learning, data mining, and visualization for complex data by enabling researchers to identify latent dimensions that effectively classify and divide observations (vot- ers) even when the left-right scale cannot effectively discriminate between classes/groups (par- tisanship). Note that although cMCA compares predefined groups first, it is still considered unsupervised learning given that the labels which categorize/cluster samples within each pre- defined group remain unknown before conducting the analysis. Simply put, cMCA’s results provide important information for researchers to better understand and make distinctions between underlying subgroups in their data. Finally, to date, almost all of the methods for subgroup analysis, such as class specific MCA (CSA) [15, 16] and subgroup MCA (sMCA) [17] (see Discussion for detailed discussion), require researchers to already know how data should be “subgrouped” in the latent dimensions (i.e., what variables may cause divisions within groups), and therefore, without such prior knowledge, these methods are almost infeasible to use, especially when data is high-dimen- sional. By revealing the overlooked influential factors, cMCA indeed provides a springboard, i.e., feature extraction, for further subgroup analyses. The paper proceeds with a description of cMCA and its application to the two voter surveys; then, through the comparison with current subgroup analysis methods, we conclude the paper with general thoughts about and rules of thumb for using cMCA as well as its application to substantive topics. Contrastive learning and contrastive MCA Contrastive learning is an emerging machine learning approach that analyzes high-dimen- sional data to capture “patterns that are specific to, or enriched in, one data relative to another” [14]. Unlike ordinary scaling methods, which usually define PCs along which data as a whole has the largest variance or best demonstrates the (dis)similarity between observations in the data, the logic behind contrastive scaling is to instead define principal components/directions that better capture the distribution of data within one group (the target group) in contrast to another (the background group). Specifically, contrastive scaling identifies PCs that capture the most variance within the target data and the least variance in the background data. So far, contrastive learning has been applied to several machine learning methods, including PCA [14], latent Dirichlet allocation [18], hidden Markov models [18], regressions [19], and varia- tional autoencoders [20]. We utilize this contrastive learning approach by applying it to MCA, an enhanced version of PCA for nominal and ordinal data analysis [16, 21]. MCA is a valuable tool for exploring principle components of categorical data; however, it has not been widely PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 3 / 20 utilized in political science (see [22–24] for exceptions). The source code of the derived method, contrastive MCA (cMCA), is available on our online repository: https://github.com/ takanori-fujiwara/cmca. Multiple correspondence analysis (MCA)—an alternative to PCA Although PCA has been widely used across disciplines for scaling, dimensional reduction, and data visualization, this approach has major limitations that may cause issues in analytical results. That is, as PCA assumes the relationships between variables are linear [25], PCA does not effectively analyze categorical data [26]—including binary data, nominal data, and ordinal data, where the parallel slopes assumption does not often hold [27]. One of the standard prac- tices to resolve this issue is to create dummy variables for all categorical data, then standardize the whole data, and finally apply PCA to this new dataset—this procedure is commonly recog- nized as a variant of PCA for categorical data and referred to as multiple correspondence analy- sis (MCA) by some scholars [21]. With similar analytical procedures to PCA, as an alternative, MCA can include all types of noncontinuous variables in the analysis to overcome the above issues. Practically, given that researchers frequently transform continuous variables to ordinal variables while analyzing sur- veys, such as age, years spent at school, and so on, MCA does not necessarily exclude continu- ous variables from the analysis. During the analysis, MCA first converts an input noncontinuous dataset, X 2 R p�d (p: the number of data points, d: the number of variables), into what is called a disjunctive matrix, G 2 Rp�K (K: the total number of categories), by apply- ing one-hot encoding to each of d categorical dimensions [28]. For illustration, assume that X consists of two columns/variables, color and shape, and the variables have three (red, green, blue) and two (circle, rectangle) levels, respectively. In this case, the disjunctive matrix G will contain five categories: red, green, blue, circle, and rectangle. For instance, a piece of blue rectangle paper will be represented as a row vector, [0, 0, 1, 0, 1], in G. We can further derive a probability/correspondence matrix, Z, through dividing each value of G by the grand total of G (i.e., Z = N −1G where N is the grand total of G). This probability matrix can now be treated as a typical dataset for use in continuous data analyses. Given that the probability can be treated as a type of data, similar to PCA, we first normalize Z, resulting Zn; then, obtain what is called a Burt matrix, B, with B ¼ def Z > n Zn (B 2 R K�K). This Burt matrix B under MCA corresponds to a variance-covariance matrix under PCA. Thus, as in PCA, to derive principal directions, MCA performs eigenvalue decomposition (EVD) to B to preserve the variance of G. Similar to PCA, without computing B, one could directly apply singular value decomposition (SVD) to Zn to obtain the same principal directions. Extending MCA to cMCA To date, there is only one application of contrastive learning to a scaling method—cPCA [14] and its variants (e.g., online cPCA [29], sparse cPCA [30], and unified linear comparative anal- ysis [31]). Given that the procedure of cPCA adds only one additional step into PCA to com- pare two groups, cPCA and its variants inevitably inherit PCA’s limitations. Such limitations impede the usage of contrastive scaling to survey data. To apply contrastive learning to MCA as an alternative to cPCA, we first split the original dataset, X 2 Rp�d, into a target group, XT 2 Rn�d, and a background group, XB 2 R m�d (where p = n + m), by a certain predefined boundary, such as individuals’ partisanship or whether they were assigned to the treatment or control groups. We then further derive two Burt matrices from the target and background PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 4 / 20 groups, BT and BB, respectively. Importantly, because the contrastive learning procedure uti- lizes matrix subtraction, the Burt matrices must have the same dimensions, i.e., BT 2 R K�K and BB 2 R K�K. Note that this does not mean that the original datasets of two groups must have the same sample size, as expressed as XT 2 R n�d and XB 2 Rm�d (on the other hand, the origi- nal datasets of two groups must have the same set of d variables). Both cPCA and cMCA only compare the variance-covariance or Burt matrices of two original datasets. When two datasets have the same set of variables, their variance-covariance/Burt matrices are guaranteed to have the same dimensions. Let u be any K-dimensional vector. Similar to cPCA, we can derive the variances of the tar- get and background groups along u with s 2 TðuÞ ¼ defu>BTu and s 2 BðuÞ ¼ def u>BBu. In addition, since we want to find a “direction” (i.e., we are not interested in the length of u), we impose a con- straint that sets the derived direction to be a unit vector, i.e., kuk = 1. Let S denote (BT − αBB). To find the direction, u*,on which a target group’s probability matrix, ZT, has a large variance and a background group’s probability matrix, ZB, has a small variance, one needs to solve the optimization problem: u ∗ ¼ arg max u s2 TðuÞ \u0000 as2 BðuÞ ¼ arg max u u>ðBT \u0000 aBBÞu ð1Þ where α (0 � α � 1) is a hyperparameter of cMCA, called the contrast parameter. From Eq 1, because both BT and αBB are symmetric matrices, (BT − αBB) is also symmetric as the trans- pose of BT − αBB is equal to itself, i.e., ðBT \u0000 aBBÞ> ¼ B> T \u0000 aB > B ¼ BT \u0000 aBB. In other words, we can simply apply the analytical procedure of MCA to this matrix, (BT − αBB). Let S denote (BT − αBB), and we now rewrite Eq 1 as follow: u∗ ¼ arg max kuk¼1 u>Su ð2Þ This maximization problem can be solved by using Lagrangian function which is trans- formed from Eq 2 by the method of Lagrange multiplier: Lðu; lÞ ¼ u>Su \u0000 lðu >u \u0000 1Þ ð3Þ Taking the derivative on Eq 3 with respect to u, we have the following equation: ruL ¼ Su \u0000 lu ð4Þ By setting Eq 4 to be equal to 0, we finally derive that S u = λu, which implies that λ and u are the eigenvalue and eigenvector of S, respectively [32]. In other words, u*corresponds to the first eigenvector of the matrix (BT − αBB) and can be derived through performing EVD over (BT − αBB). Unlike MCA, SVD cannot be directly applied to obtain eigenvectors for cMCA because we need to compute the difference between target and background groups’ Burt matrices (i.e., BT − αBB). Note that the same procedure can be applied to derive a set of multiple eigenvectors/directions, U 2 R K�K0 , where K 0 is the number of the derived top eigenvectors. Selection of the contrast parameter The contrast parameter α in Eq 1 controls the trade-off between having high target variance versus low background variance. When α = 0, cMCA only maximizes the variance of a target group, and produces results that are equivalent to applying standard MCA only to the target group. As α increases, cMCA places a greater emphasis on directions that reduce the variance of a background group. As proved by [33], arbitrarily selected α values within 0 � α � 1 can PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 5 / 20 produce different latent spaces, each of which shows a different ratio of high target variance to low background variance. Thus, researchers can manually explore different α values to seek latent spaces that demonstrate clear patterns of data division/clustering for them. In other words, by examining various different α values in cMCA, researchers can investigate multiple potential factors that reveal unique patterns in the target group relative to the background group. In addition to manual selection, we extend the automatic selection of a value of α for cPCA [34] to the one for cMCA, with which researchers can find a value of α that derives a latent space where a target group has the highest variance relative to a background group’s variance. Given that U >BTU and U >BBU are respectively target and background groups’ variance- covariance matrices under a latent space, the sum of the diagonal of each variance-covariance matrix, i.e., tr(U >BTU) and tr(U>BBU), represents the total variance of each group. Thus, this specific α can be found by solving the following trace-ratio problem: max U>U¼I trðU >BTUÞ trðU >BBUÞ ð5Þ Eq 5 finds the highest ratio between the total variances of the target and background groups and treats this ratio as the desired contrast parameter, α, to derive the corresponding eigenvec- tors and latent spaces. Following the work by Dinkelbach [35], we employ an iterative algo- rithm to solve Eq 5 as directly finding a solution is usually difficult. The iterative algorithm consists of two steps: Given eigenvectors, Us, at iteration step s (s � 0 and s 2 Z), we perform Step1: as trðU > s BTUsÞ trðU > s BBUsÞ ð6Þ Step2: Usþ1 arg max U>U¼I trðU >ðBT \u0000 asBBÞUÞ ð7Þ At the beginning of the iteration (i.e., s = 0), since the computed U0 does not exist, we self- define α0 = 0 as the default solution to Step 1. As demonstrated, αs in Eq 6 is an objective value of Eq 5, which is computed with the current Us. The second step (Eq 7) is to derive the eigen- vectors, Us+1, for the next iteration. This step just solves the original cMCA problem based on the current contrast parameter, αs. With this iterative algorithm, αs monotonically increases to a value that satisfies Eq 5 and usually converges quickly (i.e., in less than 10 iterations). For detailed mathematical explanations, please refer to [35]. Note that when BB is nearly singular, αs approaches infinity. One potential solution to avoid this issue is adding a small constant value, ε (e.g., ε = 10 −3 by default), to each diagonal element of BB. However, ε does not have a clear connection to the optimization problem in Eq 5, and consequently, it is hard for researchers to control α’s search space. Therefore, we add εtr(U >BTU) to the denominator of Eq 6, i.e., as trðU> s BT UsÞ trðU> s BBUsÞþε trðU> s BT UsÞ. This allows us to control the search space so that αs reaches at most 1/ε (e.g., when ε = 10 −3, αs may increase up to 1,000). Data-point coordinates, category coordinates, and category loadings As in ordinary MCA, in cMCA, we provide three essential tools to help researchers relate data points and categories/variables to the contrastive latent space: (1) data-point coordinates (also known as coordinates of rows [21] or clouds of individuals [16]), (2) category coordinates (also known as coordinates of columns [21] or clouds of categories [16]), and (3) category loadings. PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 6 / 20 Data-point coordinates provide a lower-dimensional representation of the data, which is standard in most dimensional reduction techniques. On the contrary, category coordinates and loadings provide essential information on how to interpret these data-point coordinates. Similar to data-point coordinates, category coordinates present the position of each category/ level in a lower-dimensional space. Given that the coordinates of each category are on the same contrastive latent space with data-point coordinates (see Eq 10), by comparing these two sets of coordinates, we can better understand the associations between the data points and cat- egories. Respondents who are placed close to the position of some category are highly likely to hold this category for the corresponding variable [16, 21, 36]. On the other hand, category loadings indicate how strongly each category contributes to each derived eigenvector. Similar to MCA, for a target group, XT, whose probability matrix is ZT, cMCA’s data-point coordinates, Y row T 2 R n�K0 (K 0 < K), can be derived as: Yrow T ¼ ZTU ð8Þ where U 2 R K�K0 is the top-K 0 eigenvectors obtained by EVD as demontrated in Eq 1. In other words, Y row T is ZT’s projected positions onto the new space defined with U. We call this new space’s axes (i.e., linear combinations of the original categories and the eigenvectors) contras- tive principal components (cPCs). Although the eigenvectors, U, are sometimes called cPCs in existing literature (e.g., [12, 14]), it is inaccurate according to Jolliffe and Cadima [37]. Simi- larly, we can obtain data-point coordinates, Y row B , of a background group, XB, (its probability matrix is ZB) onto the same low-dimensional space with Y row T through: Y row B ¼ ZBU ð9Þ As discussed in [38], visualizing low-dimensional representations of both target and back- ground groups can help researchers determine whether or not a target group has certain unique patterns relative to a background group. When the scatteredness/shape of plotted data points of Yrow T is greatly larger than Yrow B , we can conclude that Yrow T contains the unique patterns. To derive the target group’s category coordinates, Ycol T , we follow a way taken in MCA: Ycol T ¼ D \u0000 1=2 T U ð10Þ where DT is a diagonal matrix that has the sum of each column of the standardized probability matrix of a target group (this sum is conventionally called column mass [21]), as each diagonal element. One can consider Eq 10 similar to PCA’s standardized variable coordinates—instead of the Euclidean distance used in PCA, MCA or cMCA adopts χ 2 distance under the latent space (see [21, 39] for detailed discussion). Finally, since MCA’s derivation procedure is highly similar to that of PCA, we utilize the concept of normalized PC loadings in PCA to calculate category loadings in cMCA. More pre- cisely, category loadings, L 2 RK�K0 , under cMCA can be derived through: L ¼ UdiagðlÞ1=2 ð11Þ which normalizes U with the corresponding eigenvalues (or inertia), λ. For negative eigenval- ues, L is undefined. However, in practice, we only take a few top eigenvalues for analyses, and usually, we can expect that they are positive. Note that since Eq 11 derives only loadings for each category; thus, to know the influence of each variable (which contains a set of categories) on each contrastive PC, we need to obtain an aggregated measure of the loadings of each vari- able’s categories. As an aggregated measure for each variable, we calculate a value range of PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 7 / 20 each variable’s category loadings since the variable’s influence on the separation of data points in their coordinates highly depends on the difference of the category loadings. For example, in a case where variable A has two categories with loadings of 0 and 2 and variable B has two cate- gories with loadings of 1 and 2, while the total sum of loadings of B is larger than the one of A, A is more influential on making differences in data point coordinates. While we take a value range by default, one can use other statistical values based on their analysis focus (e.g., a vari- ance of loadings when emphasizing more on outliers of the loadings is preferable). Application Based on our substantive expertise, we analyze two national surveys—the CES 2020 and the ESS-UK 2018—to demonstrate the utility of cMCA while using the manual- or auto-selection of the hyperparameter α. As suggested by Le Roux and Rouanet [16], we recoded and restricted the number of cate- gories/levels to deal with the potential issue caused by certain variables’ active but infrequent categories, which could be overly influential in the determination of latent axes. Specifically, we first removed all missing values from each of the datasets; then, for variables that have more than five active levels, we pooled the adjacent two or three levels as one new level (For a more detailed recoding scheme, please refer to S6 Appendix). For instance, the left-right ideo- logical scale in the ESS-UK 2018 is originally an eleven-point scale from zero through ten, we converted this variable as a new five-point scale: the original levels 0–1, 2–3, 4–6, 7–8, 9–10 are recoded as 1, 2, 3, 4, 5, respectively. Due to the focus of this paper being the usage of cMCA, we assume that the missing data is generated at random in our analyses; however, note that when the data is not missing at random, listwise deletion could cause analytical results to be inefficient and biased [40]. Given that the home countries of these surveys (i.e., the U.S. and the U.K.) have different political environments, these surveys provide a wide range of political situations for testing. Also, because the general political systems and cultures of each country are unique, we validate the consistency of the derived cMCA results with the observed, qualitative political realities in each country. Case one: CES 2020—The hidden conflicts regarding Trump and related issues We firstly demonstrate the utility of cMCA by analyzing the 2020 Cooperative Election Study, conducted from September 29, 2020 to November 2, 2020 (the pre-election wave) and from November 8, 2021 to December 18, 2021 (the post-election wave). The principal investigators/ institutions, original data, and survey guide of CES 2020 can be retrieved from https:// dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910/DVN/E9N6PH. For this analysis, we focus on the pre-election wave and select 69 variables all related to national issues, including job approval, roll call votes, and salient policies and issues (e.g., abortion and gun control). For job approval questionnaires, those related to the president are only included in the analysis, given that other institutions are not solely run by a single nation-wise politician, and thus voters may lean more on local, rather than national, perspectives, to make evalua- tions. After deletion, the sample size reduces from 7,700 respondents to 5,616. In addition, to demonstrate the differences between analytical results of cMCA and ordinary scaling, we apply three different scaling methods to this data, each of which represents a different model- ing strategy—MCA (nonparametric approach), blackbox scaling (parametric and frequentist approach), and OIRT (parametric and Bayesian approach)—and present the MCA result (Fig 1) in the main text and the rest in S1 Appendix (Figs 6 and 7) for references. Note that unlike PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 8 / 20 the blackbox function and MCA, OIRT does not estimate normal vectors of issues. Therefore, for the OIRT model results, we set the vector of CC20.320a as the x-axis and the vector of ideo5 as the y-axis to approximate the latent pattern derived by the other two approaches (please refer to S6 Appendix for detailed coding schema). Although the three ordinary scaling methods utilize different approaches for dimensional reduction and point estimation, their results show one similar pattern among American voters. Given that MCA does not derive “explained variance” for each variable but for each category, making a biplot-type figure may confuse readers, unlike the case for blackbox scaling. Instead, the category loadings plot and the category coordinates plot are provided in S2 Appendix. That is, the derived PC1 clearly splits American voters along with partisanship (Dem: Demo- crat or Rep: Republican), with some exceptions. For the sake of simplicity, we will mainly focus on the discussion of PC1 which explains the most of variation in the data in general. Nevertheless, auxiliary information related to PC2 is included for reference. Given that the positions are estimated through voters’ self-reported values, these results demonstrate that the predispositions the U.S. voters heavily rely on are highly associated with their vote choices and party preferences [41, 42]. In general, although self-identified ideology is not the most promi- nent variable comprising PC1 (as seen in S2.1 in S2 Appendix), we find that PC1 is consistent with the liberal-conservative ideology as self-identified ideologies leaning toward liberal (responding 1 or 2 on the 5-point ideological scale), moderate (responding 3), and conserva- tive (responding 4 or 5) are respectively observed on the left, center, and right sides of the fig- ure. In other words, all ordinary scaling results demonstrate the existence of ideological polarization among the U.S. public, which is identified previously in the literature [43–46]. Furthermore, both the theoretical concept of ideology [47–49] and statistical interpretation of PCs [30, 50] are indeed defined as a (linear) combination of multiple issues/attitudes; thus, the scaling results of variable vectors help explore influential issues/variables and their magni- tude of the influence on the composition of each PC. According to Table 1 in S2 Appendix, one can see that variables most contributed to PC1 are related to Trump’s approval (i.e., CC20.320a, CC20.350f, and CC20.350g), border-related policies (CC20.442c), and nomination to the Supreme Court (CC20.356) during this time. As the validity check, in the category coordinates of each variable in Fig 8 in S2 Appendix, one can find that the categories Fig 1. MCA result of CES 2020. https://doi.org/10.1371/journal.pone.0287180.g001 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 9 / 20 corresponding to liberal and conservative views on the aforementioned issues are also respec- tively placed on the left and right sides of the same space with the data-point coordinates. To demonstrate how cMCA (with the manual-selection method) works differently from ordinary scaling, we generate cMCA results (i.e., data-point coordinates) shown in Fig 2a by assigning the Democrats to the target group and the Republicans to the background group. When α is manually set to 2, the Democrats (in blue color) can be divided by the Republicans (in red color) along the first contrastive PC (cPC1) and the second contrastive PC (cPC2) sep- arately. This indicates that cMCA uncovers the directions along which the Democrats have much higher variations relative to the Republicans. When using cMCA, we derive two types of auxiliary information—the category coordinates and category loadings—as heuristics to identify the most influential categories and variables. Please refer to S3 Appendix for this auxiliary information. As discussed earlier, a value range of each variable’s category loadings guides us to select influential variables on the division or distance between respondents’ positions along the latent direction. In addition, given that the category coordinates are in the same space as the data-point/respondent coordinates, a respon- dent who is placed near a certain category is likely to be associated with this category. Note that although one of the scaling method’s main functions is to extract influential variables, this Fig 2. cMCA results of CES 2020. https://doi.org/10.1371/journal.pone.0287180.g002 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 10 / 20 does not necessarily mean that each derived PC simply corresponds to only a few influential variables; rather, each PC usually represents a linear combination of all variables [51]. For the sake of simplicity and clarity, in the following, we will only report and discuss the top few influential variables on cPC1 in the main text and provide a more detailed report in S5 Appendix. Through the auxiliary information listed in S3.1 in S3 Appendix, we identify that the most influential variables/issues on the division of the Democrats on cPC1 are controversies over Trump’s two most recent nominations to the Supreme Court. More precisely, the top-two issues (in order) that separate the Democrats along cPC1 are: Amy Coney Barrett’s confirma- tion (CC20.356) and Brett Kavanaugh’s confirmation (CC20.350c). Overall, the cMCA results indicate that the Democrats who hold liberal attitudes and disapprove of the two nomi- nated Supreme Court judges are distributed to the right side of the space, whereas those who hold conservative views and approve of the two Supreme Court nominees are distributed on the left side of the space in Fig 2a. We further color-encode the Democrats based on their approval of the nomination of the two new Supreme Court judges. Both variables are binary, where 1 refers to approval and 2 refers to disapproval. Please refer to S6 Appendix for more details. More precisely, the Demo- crats who disapprove of both judges are colored in purple (Dem_Pro) and the rest (i.e., those who approve of one or both judges) is colored in teal (Dem_Con) (see Fig 2c). By and large, Fig 2a and 2c conclude that compared with the other issues, these two issues together, the approval of Amy Coney Barrett and of Brett Kavanaugh, dominate the composition of cPC1 (i.e., the main variation), which divides the Democrats into two sub-groups. On the other hand, cMCA also discovers a specific pattern within the Republicans when assigning Democrats to the background group. As presented in Fig 2b, when α is manually set to 32, we find that Republicans (red) are split by Democrats (blue) into two sub-groups along cPC1 and cPC2 separately. According to the auxiliary information in S3.1 in S3 Appendix, we identify that the top-two influential variables that compose cPC1 are all related to the approval of Trump’s performance, i.e., the approval of Trump’s job (CC20. 320a) and whether to remove Trump due to abuse of power (CC20.350f). As seen in Fig 2d, cMCA uncovers a subgroup of Republicans who support and approve of Trump (Rep_Pro in orange), which differs from the one consisting of those who generally oppose him (Rep_Con in pink). By and large, Fig 2 demonstrates that even when the PCs derived from the traditional meth- ods are informative originally, cMCA can still identify influential variables and categories that create division within each of the predefined groups relative to the other. As demonstrated by the above results, although traditional scaling and cMCA may derive conceptually similar principal directions, the derived PCs may not necessarily be comprised of the same set of vari- ables. Intuitively, the defined structure of the ideological scale within the general public (which includes both Republicans and Democrats) and solely within the Democrats or Republicans should be different. Indeed, as demonstrated, Trump’s performance is the defining issue among the Republicans but only a partial issue for the general public. In other words, while there exists a general pattern across groups, cMCA provides an alternative way of exploring hidden patterns within each group that are different from the general pattern. Case two: ESS-UK 2018—The Brexit cleavage in the Labour and Conservative parties The second survey we examine is the British module of the 2018 European Social Survey (ESS-UK 2018). We first manually select 23 variables that are generally related to political/ PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 11 / 20 social issues and recode this data through the aforementioned procedure. For simplicity, we only include three partisans: the Conservative, Labour, and UK Independence Party (UKIP). After deletion, the sample size decreases from 946 respondents to 777. As in the case of CES 2020, we first apply all the aforementioned three ordinary scaling approaches to this survey. For the OIRT model of ESS, we set the vector of lrscale as the x-axis and the vector of atcherp as the y-axis. According to Fig 3 and S1.2 in S1 Appendix (for more detailed infor- mation, please refer to Table 2 and Fig 9 in S2 Appendix), one may find there is only a vague pattern that roughly divides British voters ideologically. These results demonstrate that differ- ent partisans highly overlap around the area surrounding the origin, with some respondents spread wider out. Roughly speaking, from right to left, the general positions of the different partisans in order along PC1 of the MCA result (Fig 3) are the UKIP, Conservative, and Labour supporters. However, there only exists a low level of political division among partisans in the U.K., unlike the case of American voters. To contrast with standard scaling, we apply cMCA with the automatic selection of α to the same survey, and present the results in Figs 4 and 5. As shown in Fig 4, the first pair we exam- ine includes the two major parties, the Conservative and Labour. According to Labour’s posi- tions in Fig 4a and the auxiliary information in S3.2 in S3 Appendix, when we specify the Labour as the target group, one can find that respondents’ self-reported ideological position (lrscale) is the single most influential variable for both the first and second cPCs. Further- more, based on cross-referencing the category and respondent coordinates, we see that Labour supporters could be further divided into several smaller groups along with their ideology from top-right (liberal leaning) toward bottom-left (conservative leaning) within the contrastive space (see Fig 4c). As shown in Fig 4e, we can further categorize these Labour supporters into three subgroups: those who are liberal (responses of 1, 2 to lrscale), moderate (a response of 3 to lrscale), and conservative (responses of 4 and 5 to lrscale). Fig 3. MCA result of ESS-UK 2018. https://doi.org/10.1371/journal.pone.0287180.g003 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 12 / 20 Similar to the above, when we take the Conservatives as the target group, we can see the respondents’ self-reported ideological position is the single most influential variable again. The Conservatives also can be solely divided into several smaller groups along with their own ideol- ogy from the top-right (conservative leaning) towards the bottom-left (liberal leaning) within Fig 4. cMCA results of ESS-UK 2018 (Con versus Lab). https://doi.org/10.1371/journal.pone.0287180.g004 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 13 / 20 the contrastive space (see Fig 4d). The groups can be further categorized into the Conservatives who are conservative, moderate, and liberal, as shown in Fig 4e. By and large, by comparing Conservative and Labour supporters, the cMCA results show that ideology is the main differentiator between these two parties—the Labour (Conservative) supporters hold mostly moderate to liberal (conservative) opinions with some amount of con- trarian outliers who hold views similar to their out-partisans. This result is greatly different than the results when applying traditional scaling to the entire dataset. On the one hand, the traditional scaling results such as Fig 3 and S1.2 in S1 Appendix demonstrate that British parti- sans are vaguely divided by ideology; on the other hand, cMCA explores the existing differ- ences between two parties’ ideological distributions, as depicted in Fig 4. Simply put, cMCA identifies ideology as the most effective variable to produce: 1) a high variance among Labour supporters relative to the Conservatives as liberal ideologies can be seen more among Labour supporters, and 2) a high variance among Conservatives relative to Labour as conservative ide- ologies can be seen more among Conservative supporters. Therefore, we conclude that cMCA finds that, in contrast to the American case, there exists a relatively low level of ideological polarization between Conservative and Labour supporters. This pattern of low-level polariza- tion is obscured when using ordinary scaling because of the large degree of moderates found among Conservative and Labour supporters, about 43% and 45% separately. On the contrary, there are only about 37% and 23% of moderate supporters within the Democratic and Republi- can parties in the U.S. separately. Indeed, this recovered degree of low-level polarization is consistent with the current academic understanding that the U.K. has undergone a period of political depolarization since the second wave of ideological convergence between the elites of the Conservative and Labour parties [52–54]. In contrast, when cMCA contrasts each of the two main parties with UKIP, we can find dif- ferent patterns, as shown in Fig 5. In Fig 5a where α is automatically set to around 1000, we observe that the Labour supporters (red) have much larger variance than the UKIP support- ers (green). As shown in Eqs 5, 6 and 7, the automatic selection can reach a very large α value when there exist cPCs that produce an extremely large variance difference between the target and background groups. cMCA tends to be able to find such cPCs for the ESS-UK 2018 survey as the survey has a relatively large number of categories K (K = 200) when compared to the number of rows/respondents of each group (Con: 383, Lab: 364, UKIP: 30). Conse- quently, the automatic selection sets α close to 1000, which is the upper bound of the search space using a default � value (� = 10 −3). We set � = 10 −3 by default to follow the same upper bound of α suggested in the original cPCA [14]. However, when the number of rows/respon- dents is much larger than the number of categories, smaller � values can be used (e.g., � = 10 −4) to extend the search space, and vice versa. According to S3.3 in S3 Appendix, the responses of 5 to imwbcnt (immigration makes the U.K. an (extremely) better place to live), the responses of 1 to imdfetn (allowing many immigrants of the different race as non-majority), and the responses of 5 to imueclt (the respondent’s life is extremely enriched by immigrants) are the top-three influential categories, which “pull” some Labour supporters away from their co-par- tisans to the left side of cPC1 (all three variables are five-point scales—please refer to S6 Appendix for more details). Furthermore, from the category coordinates, we infer that the respondents who hold extreme attitudes are placed to the left side (e.g., 4 or 5 to imwbcnt). To illustrate the subgroups, as shown in Fig 5c, we color the Labour supporters who hold rela- tively EU-supportive opinions over all the three variables above with yellow (Lab_Pro) and the rest with pink (Lab_Oth). Based on these results, we find that Labour supporters who are on the left of cPC1 in Fig 5a are associated with relatively open views over these three immigration or Brexit-related variables. PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 14 / 20 Analogously, we also find subgroups within the Conservatives by using the UKIP support- ers as the background group. Similar to the previous results, as shown in Fig 5b, with an auto- matically selected α (around 1000), the Conservatives (blue) have much larger variance than the UKIP supporters (green). According to S3.3 in S3 Appendix, we see that the Conserva- tive supporters located on the left side of cPC1 are also highly associated with the response of 5 to imwbcnt, imueclt, and imbgeco (the immigration makes the economy of the U.K. better). Similar to the dimension obtained for the Labour, we speculate that cPC1 represents a similar latent trait, which also divides the Conservative supporters into two subgroups—those who are relatively open on EU-related issues and the rest. In fact, as shown in Fig 5d, we can further divide the Conservative supporters into two sub-groups, where those who hold rela- tively open opinions over all three above variables are colored teal (Con_Pro) and the rest is colored purple (Con_Oth). Overall, given that recent research has linked negative immigration attitudes to anti-EU sentiments [55], we conclude that both Conservative and Labour party supporters are inter- nally divided by Brexit attitudes (with different magnitudes). As we may expect, although the two parties are divided by Brexit attitudes, the size of the pro-EU sub-group is significantly Fig 5. cMCA results of ESS-UK 2018 (Lab versus UKIP, Con versus UKIP). https://doi.org/10.1371/journal.pone.0287180.g005 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 15 / 20 larger among Labour supporters, as compared to Conservative ones: there are about 52% of the Labours who are in the subgroup of Lab_Pro and about 23% of the Conservatives who are in the subgroup of Con_Pro. The results of Figs 4 and 5 demonstrate that although the derived variation from tradi- tional scaling does not align with the boundaries between predefined groups (i.e., partisans), cMCA finds clear trends within those predefined groups. The results demonstrate the intrin- sic differences in how traditional scaling and cMCA treat data, which lead to their divergent analytical outcomes. In addition, the ESS-UK 2018 analysis emphasizes an important com- ponent of cMCA—that is, the selection of the background group is highly influential in deriving results. Given that the basic concept of contrastive learning methods is to explore variation that is significant in the target group but insignificant in the background group, it is intuitive to see that the derived cPCs highly depend on one’s selection. Consequently, one can imagine that if the selected target and background groups are highly similar, cMCA may not capture differences between two groups given the high level of similarity. Nevertheless, such “failure” can be also informative to know two groups’ similarities. Thus, researchers can apply the contrastive learning method to any two groups to examine their level of simi- larity or dissimilarity. Discussion Scaling has been widely used for both pattern recognition and latent-space derivation. Never- theless, given that ordinary scaling methods only explore an overall latent pattern across groups, the derived results sometimes do not satisfy researchers whose interests instead focus on patterns within a group. In this article, we contribute to the scaling, contrastive learning, data mining, and data visualization literature by extending contrastive learning to MCA, enabling researchers to preserve analytical unbiasedness and efficiency as much as possible and to derive contrasted dimensions identifying subgroups hidden in data. So far, while comparing latent patterns between multiple datasets, the main approach is to apply ordinary scaling first and compare the similarities or differences manually. However, this approach does not guarantee that the derived latent pattern is unique in the target group or the latent pattern considers the differences between the two groups. cMCA (or contrastive scaling in general) is designed to serve as a workhorse that simultaneously explores and com- pares multiple unique/different latent patterns. In short, this work demonstrates that cMCA, or contrastive methods in general, might provide novel insights overlooked by traditional methods when analyzing categorical survey data. To demonstrate this point, we apply MCA to only Democrats, Republicans, Labours, and Conservatives and provide category loadings of PC1 of each result in S4 Appendix as comparisons to the cMCA results in S3 Appendix. As one can see, given that the linear combination or the structure of PCs derived by different approaches are not the same (i.e., the sets of top-5 most influential variables derived by differ- ent approaches are not identical), this demonstrates that applying MCA to each of the two sin- gle groups separately does not generate the same results as applying cMCA to the two groups together. The structure of PCs generated by MCA is either totally different from those gener- ated by cMCA (the Conservatives and Labours) or still similar to them but with more noise (the Democrats and Republicans). Note that we are not implying that contrastive scaling is, in general, a replacement for pre- vious methods. Our methodology falls into the category of other statistical and machine learning methods that were developed as new and unique approaches to tackle problems that a researcher may encounter. For example, local regression (e.g., LOESS or LOWESS) was developed specifically for a case where a researcher wants to reduce biases from parametric PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 16 / 20 assumptions. However, if a researcher is interested in having more generalizable results, ordinary least squares may be more useful. Similarly, we consider contrastive scaling as not a superior method for exploring multidimensional data, but rather an alternative that enables researchers to explore aspects of their data that are often missed by previous methods. There- fore, the question as to whether a researcher should adopt ordinary or contrastive scaling is one that solely depends on both the researchers’ research question and own discretion—if researchers consider that there are no inter-group differences or are simply uninterested in exploring intra-group differences, they should utilize ordinary scaling; otherwise, contrastive scaling can be a powerful tool for exploring their data and answering their research questions. It is also important to note that contrastive learning, including cMCA, is different from ordinary subgroup-analysis methods: Almost all standard methods for subgroup-analysis require researchers to have prior knowledge about how data should be “subgrouped,” i.e., what variables/factors may cause differences between factions of an existing group. For exam- ple, as compared with two alternatives of MCA for subgroup analysis, class-specific MCA (CSA) and subgroup MCA (sMCA), cMCA allows researchers to agnostically explore all possi- ble latent traits from different perspectives in the space. In other words, while conducting cMCA, without prior knowledge, researchers need only compare any two groups and apply different values of α, either with manual- or auto-selection, to derive latent dimensions or traits which could subgroup data. In contrast, both CSA and sMCA require researchers to subjectively subset/subgroup the original data along with certain traits first and then compare the derived patterns with the reference group, usually the original complete data. The ideas behind CSA and sMCA are similar: CSA is used to study whether a predefined subset of data points has a different latent pattern or not; sMCA is used to explore whether data points distribute differently after excluding certain subgroup(s) from the original data [16, 17]. Therefore, without any prior knowledge regarding how groups themselves could be divided, it is difficult for researchers to effectively apply CSA and sMCA to objectively explore subgroups. Especially, when data is high-dimensional (as in our two examples), this becomes more difficult because finding effective subgrouping criteria from many variables is not a trivial procedure. This compari- son by no means indicates that cMCA is superior to CSA, sMCA, or other subgroup-analysis methods. Rather, it shows how cMCA is an additional tool for researchers to agnostically explore latent traits. In that vein, we believe that this new approach can assist to extract important features from high-dimensional data to define subgroups and complement exist- ing subgroup-analysis methods. There are several use cases for cMCA. First, cMCA can be used for analyzing covariate-bal- ance between treatment and control groups in experiments. Indeed, cMCA can be utilized to explore the level of similarity of two sets of identical active variables—if cMCA finds a high variance only in one group, variables that contribute to the variance are likely to contain prob- lems regarding the procedure of randomization. Second, cMCA can be applied to substantive topics. Recently, scholars of political polarization found that American voters’ extremely ideo- logical disagreement (i.e., ideological polarization), which usually aligns with partisan lines [56, 57], has formed solid in-group/out-group identities and further reinforced their emotional cleavage (i.e., affective polarization) [58–60]. However, for instance, given that the alignment between salient issue disagreement and partisan lines among British voters is not as clear as among American voters, a derived “issue cleavage” from cMCA, which could cause social dis- tance among citizens and further create group/faction affiliation [61], can be a good source of studying affective polarization as well, in addition to party ID. PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 17 / 20 Supporting information S1 Appendix. Results of ordinary scaling. Blackbox scaling and ordinal item response theory model. (PDF) S2 Appendix. Auxiliary information of MCA. Category loadings and category coordinates. (PDF) S3 Appendix. Auxiliary information of cMCA. Category loadings and category coordinates. (PDF) S4 Appendix. Applying MCA to single groups as comparisons. (PDF) S5 Appendix. Auxiliary information of MCA and cMCA in detail. (PDF) S6 Appendix. Variable coding scheme. (PDF) Author Contributions Writing – original draft: Tzu-Ping Liu. Writing – review & editing: Takanori Fujiwara. References 1. Armstrong D, Bakker R, Carroll R, Hare C, Poole K, Rosenthal H. Analyzing Spatial Models of Choice and Judgment with R. FL: CRC Press.; 2014. 2. Duck-Mayr J, Montgomery, J. Ends Against the Middle: Measuring Latent Traits When Opposites Respond the Same Way for Antithetical Reasons. Political Analysis. 2023. https://doi.org/10.1017/pan. 2022.33. 3. Jacoby WG. Levels of Measurement and Political Research: An Optimistic View. American Journal of Political Science. 1999; 43(1):271–301. https://doi.org/10.2307/2991794 4. Jacoby WG. Data Theory and Dimensional Analysis. Newbury Park, CA: Sage; 1991. 5. Downs A. An Economic Theory of Political Action in a Democracy. Journal of Political Economy. 1957; 65(2):135–150. https://doi.org/10.1086/257897 6. Cahoon L, Hinich, Melvin J, Ordeshook PC. A Statistical Multidimensional Scaling Method Based on the Spatial Theory of Voting. In: Wang PCC (ed.), Graphical Representation of Multivariate Data. New York: Academic Press, INC; 2002. pp:243–278. 7. Hinich MJ, Munger MC. A Spatial Theory of Ideology. Journal of Theoretical Politics. 1992; 4(1):5–30. https://doi.org/10.1177/0951692892004001001 8. Rabinowitz GB. Spatial Models of Electoral Choice: An Empirical Analysis. Ann Arbor, MI: University of Michigan; 1973. 9. Coombs CH. A Theory of Data. New York: Wiley; 1964. 10. Poole K, Rosenthal H. Patterns of Congressional Voting. American Journal of Political Science. 1991; 35(1):228–278. https://doi.org/10.2307/2111445 11. Poole K. Nonparametric Unfolding of Binary Choice Data. Political Analysis. 2000; 3(8):211–237. https://doi.org/10.1093/oxfordjournals.pan.a029814 12. Fujiwara T, Kwon OH, Ma KL. Supporting Analysis of Dimensionality Reduction Results with Contras- tive Learning. IEEE Transactions on Visualization and Computer Graphics. 2020; 26(1):45–55. https:// doi.org/10.1109/TVCG.2019.2934251 PMID: 31425080 13. Wenskovitch J, Crandell I, Ramakrishnan N, House L, North C. Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics. IEEE Transactions on Visualization and Com- puter Graphics. 2018; 24(1):131–141. https://doi.org/10.1109/TVCG.2017.2745258 PMID: 28866581 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 18 / 20 14. Abid A, Zhang, Martin J, Bagaria VK, Zou J. Exploring Patterns Enriched in a Dataset with Contrastive Principal Component Analysis. Nature Communications. 2018; 9(1):1–7. https://doi.org/10.1038/ s41467-018-04608-8 15. Hjellbrekke J, Korsnes O. Field Analysis, MCA and Class Specific Analysis: Analysing Structural Homologies Between, and Variety Within Subfields in the Norwegian Field of Power. In: Jőrg B, Fre´ de´ric L, Le Roux B, Schmitz A (ed.), Empirical Investigations of Social Space. Cham: Springer International Publishing; 1993. 16. Le Roux B, Rouanet H. Quantitative Applications in the Social Sciences: Multiple Correspondence Anal- ysis. CA: SAGE Publications, Inc; 2010. 17. Greenacre M, Pardo R. Subset Correspondence Analysis: Visualizing Relationships Among a Selected Set of Response Categories from a Questionnaire Survey. Sociological Methods & Research. 2006; 35 (2):193–218. https://doi.org/10.1177/0049124106290316 18. Zou JY, Hsu DJ, Parkes DC. Adams RP. Contrastive Learning Using Spectral Methods. Proceedings of the International Conference on Neural Information Processing Systems. 2013; 26:2238–2246. 19. Ge R, Zou J. Rich Component Analysis. Proceedings of the International Conference on Machine Learning. 2016; 48:1502–1510. 20. Severson KA, Ghosh S, Ng K. Unsupervised Learning with Contrastive Latent Variable Models. Pro- ceedings of the AAAI Conference on Artificial Intelligence. 2019; 33:4862–4869. https://doi.org/10. 1609/aaai.v33i01.33014862 21. Greenacre M. Correspondence Analysis in Practice. New York: Chapman & Hall/CRC; 2017. 22. Bonica A. Mapping the Ideological Marketplace. American Journal of Political Science. 2014; 58 (2):367–386. https://doi.org/10.1111/ajps.12062 23. Blasius J, Thiessen V. Methodological Artifacts in Measures of Political Efficacy and Trust: A Multiple Correspondence Analysis. Political Analysis. 2001; 9(1):1–20. https://doi.org/10.1093/oxfordjournals. pan.a004862 24. Gibson T, Hare C. Moral Epistemology and Ideological Conflict in American Political Behavior. Social Science Quarterly. 2016; 97(5):1157–1173. https://doi.org/10.1111/ssqu.12217 25. Linting M, Meulman JJ, Groenen P, van der Koojj AJ. Nonlinear principal components analysis: intro- duction and application. Psychological Methods. 2007; 12(3):336–358. https://doi.org/10.1037/1082- 989X.12.3.336 PMID: 17784798 26. Gauch HG Jr, Qian S, Piepho H-P, Zhou L, Chen R. Consequences of PCA Graphs, SNP Codings, and PCA Variants for Elucidating Population Structure. PLOS ONE. 2019; 14(6): e0218306. https://doi.org/ 10.1371/journal.pone.0218306 PMID: 31211811 27. Long JS. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks: Sage Publications; 1997. 28. Harris D, Harris S. Digital Design and Computer Architecture. CA: Morgan Kaufmann Publishers; 2010. 29. Golkar P, Lipshutz D, Tesileanu, T, Chklovskii DB. An Online Algorithm for Contrastive Principal Com- ponent Analysis. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 2023. 30. Boileau P, Hejazi NS, Dudoit S. Exploring High-Dimensional Biological Data with Sparse Contrastive Principal Component Analysis. Bioinformatics. 2020; 36(11):3422–3430. https://doi.org/10.1093/ bioinformatics/btaa176 PMID: 32176249 31. Fujiwara T, Wei X, Zhao J, Ma KL. Interactive Dimensionality Reduction for Comparative Analysis. IEEE Transactions on Visualization and Computer Graphics. 2022; 28(1):758–768. https://doi.org/10. 1109/TVCG.2021.3114807 PMID: 34591765 32. Strang G. Introduction to Linear Algebra. Wellesley: Wellesley-Cambridge Press; 1993. 33. Abid A, Zou, J. Contrastive Variational Autoencoder Enhances Salient Features. arXiv:1902.04601. 2019. Available from: https://arxiv.org/abs/1902.04601. 34. Fujiwara T, Zhao J, Chen F, Yu Y, Ma KL. Network Comparison with Interpretable Contrastive Network Representation Learning. Journal of Data Science, Statistics, and Visualisation. 2022. https://doi.org/ 10.52933/jdssv.v2i5. 35. Dinkelbach D. On nonlinear fractional programming. Management Science. 1967; 13(7):492–498. https://doi.org/10.1287/mnsc.13.7.492 36. Pagès J. Multiple Factor Analysis by Example Using R. FL: CRC Press; 2014. 37. Jolliffe IT, Cadima J. Principal Component Analysis: A Review and Recent Developments. Philosophi- cal Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 2016; 374 (2065):20150202. https://doi.org/10.1098/rsta.2015.0202 PMID: 26953178 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 19 / 20 38. Fujiwara T, Zhao J, Chen F, Ma KL. A Visual Analytics Framework for Contrastive Network Analysis. Proceedings of IEEE Conference on Visual Analytics Science and Technology. 2020; 48–59. 39. Greenacre M, Blasius J. Multiple Correspondence Analysis and Related Methods. New York: Chap- man and Hall/CRC; 2006. 40. King G, Honaker J, Joseph A, Scheve K. Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation. American Political Science Review. 2001; 95(1):49–69. https://doi. org/10.1017/S0003055401000235 41. Bolsen T, Druckman JN, Cook FL. The Influence of Partisan Motivated Reasoning on Public Opinion. Political Behavior. 2014; 36(2):235–262. https://doi.org/10.1007/s11109-013-9238-0 42. Zaller JR. The Nature and Origins of Mass Opinion. Cambridge: Cambridge University Press; 1992. 43. Fiorina M, Abrams S. Political Polarization in the American Public. Annual Review of Political Science. 2008; 11:563–588. https://doi.org/10.1146/annurev.polisci.11.053106.153836 44. Iyengar S, Westwood SJ. Fear and Loathing Across Party Lines: New Evidence on Group Polarization. American Journal of Political Science. 2015; 59(3):690–707. https://doi.org/10.1111/ajps.12152 45. Lelkes Y. Mass Polarization: Manifestations and Measurements. Public Opinion Quarterly. 2016; 80 (S1):392–410. https://doi.org/10.1093/poq/nfw005 46. Smidt C. Polarization and the Decline of the American Floating Voter. American Journal of Political Sci- ence. 2017; 61(2):365–381. https://doi.org/10.1111/ajps.12218 47. Converse PE. The Nature of Belief Systems in Mass Publics (1964). Critical Review. 2006; 18(1-3):1– 74. https://doi.org/10.1080/08913810608443650 48. Feldman S, Johnston C. Understanding the Determinants of Political Ideology: Implications of Structural Complexity. Political Psychology. 2014; 35(3):337–358. https://doi.org/10.1111/pops.12055 49. Miller WE, Stokes DE. Constituency Influence in Congress. American Political Science Review. 1963; 57(1):45–56. https://doi.org/10.2307/1952717 50. Zou H, Hastie T, Tibshirani R. Sparse Principal Component Analysis Journal of Computational and Graphical Statistics. 2006; 15(2):265–286. https://doi.org/10.1198/106186006X113430 51. Zou H, Xue L. A Selective Overview of Sparse Principal Component Analysis. Proceedings of the IEEE. 2018; 106(8):1311–1320. https://doi.org/10.1109/JPROC.2018.2846588 52. Adams J, Green J, Milazzo C. Has the British Public Depolarized along with Political Elites? An Ameri- can Perspective on British Public Opinion. Comparative Political Studies. 2012; 45(4):507–530. https:// doi.org/10.1177/0010414011421764 53. Adams J, Green J, Milazzo C. Who Moves? Elite and Mass-Level Depolarization in Britain, 1987–2001. Electoral Studies. 2012; 31(4):643–655. https://doi.org/10.1016/j.electstud.2012.07.008 54. Leach R. Political Ideology in Britain. London: Macmillan International Higher Education; 2015. 55. Colantone I, Stanig P. Global Competition and Brexit. American Political Science Review. 2016; 112 (2):201–218. https://doi.org/10.1017/S0003055417000685 56. Baldassarri D, Gelman A. Partisans Without Constraint: Political Polarization and Trends in American Public Opinion. American Journal of Sociology. 2008; 114(2):408–446. https://doi.org/10.2139/ssrn. 1010098 PMID: 24932012 57. Levendusky MS. Americans, Not Partisans: Can Priming American National Identity Reduce Affective Polarization? The Journal of Politics. 2018; 80(1):59–70. https://doi.org/10.1086/693987 58. Bougher LD. The Correlates of Discord: Identity, Issue Alignment, and Political Hostility in Polarized America. Political Behavior. 2017; 39(3):731–762. https://doi.org/10.1007/s11109-016-9377-1 59. Mason L. Ideologues Without Issues: The Polarizing Consequences of Ideological Identities Public Opinion Quarterly. 82(S1):866–887. https://doi.org/10.1093/poq/nfy005 60. Rogowski JC, Sutherland JL. How Ideology Fuels Affective Polarization. Political Behavior. 2016; 38 (2):485–508. https://doi.org/10.1007/s11109-015-9323-7 61. Iyengar S, Lelkes Y, Levendusky M, Malhotra N, Westwood SJ. The Origins and Consequences of Affective Polarization in the United States. Annual Review of Political Science. 2019; 22:129–146. https://doi.org/10.1146/annurev-polisci-051117-073034 PLOS ONE Contrastive multiple correspondence analysis (cMCA) PLOS ONE | https://doi.org/10.1371/journal.pone.0287180 July 10, 2023 20 / 20","libVersion":"0.3.2","langs":""}
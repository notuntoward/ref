{"path":"lit/lit_sources/Nowotarski18elecPriceProbFrcstRvw.pdf","text":"HSC/16/07 HSC Research Report Recent advances in electricity price forecasting: A review of probabilistic forecasting Jakub Nowotarski 1 Rafał Weron1 1 Department of Operations Research, Wrocław University of Technology, Poland Hugo Steinhaus Center Wrocław University of Technology Wyb. Wyspiańskiego 27, 50-370 Wrocław, Poland http://www.im.pwr.wroc.pl/~hugo/ Forthcoming in Renewable & Sustainable Energy Reviews, 2017, DOI: 10.1016/j.rser.2017.05.234 Recent advances in electricity price forecasting: A review of probabilistic forecasting Jakub Nowotarskia, Rafał Werona aDepartment of Operations Research, Wrocław University of Science and Technology, 50-370 Wrocław, Poland Abstract Since the inception of competitive power markets two decades ago, electricity price forecas- ting (EPF) has gradually become a fundamental process for energy companies’ decision making mechanisms. Over the years, the bulk of research has concerned point predictions. However, the recent introduction of smart grids and renewable integration requirements has had the eﬀect of increasing the uncertainty of future supply, demand and prices. Academics and practitioners alike have come to understand that probabilistic electricity price (and load) forecasting is now more important for energy systems planning and operations than ever before. With this paper we oﬀer a tutorial review of probabilistic EPF and present much needed guidelines for the rigorous use of methods, measures and tests, in line with the paradigm of ‘maximizing sharpness subject to reliability’. The paper can be treated as an update and a further extension of the otherwise compre- hensive EPF review of Weron [1] or as a standalone treatment of a fascinating and underdeveloped topic, that has a much broader reach than EPF itself. Keywords: Electricity price forecasting, Probabilistic forecast, Reliability, Sharpness, Day-ahead market, Autoregression, Neural network Email addresses: jakub.nowotarski@pwr.edu.pl (Jakub Nowotarski), rafal.weron@pwr.edu.pl (Rafał Weron) Contents 1 Introduction 3 2 Literature review 4 2.1 Bibliometric survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Winners of the GEFCom2014 price track . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Other notable probabilistic EPF papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3.1 The ﬁrst years . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3.2 Density forecasts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3.3 Bootstrapped PIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.4 Factor models and medium-term forecasts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.5 Spike occurrence and threshold forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3 Constructing probabilistic forecasts 13 3.1 Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Historical simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3 Distribution-based probabilistic forecasts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.4 Bootstrapped PIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.5 Quantile Regression Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4 Evaluation metrics 16 4.1 Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.1.1 Unconditional coverage and the Kupiec test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.1.2 Independence, conditional coverage and the Christoﬀersen test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.1.3 Extensions and alternatives to the Christoﬀersen test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.1.4 Probability Integral Transform (PIT) and the Berkowitz test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.2 Sharpness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2.1 Proper scoring rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2.2 Pinball loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2.3 Winkler score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.2.4 Continuous Ranked Probability Score (CRPS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.2.5 Equal predictive performance and the Diebold-Mariano test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.2.6 Alternatives to the Diebold-Mariano test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.3 Other measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5 Empirical study 25 5.1 The data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2 Individual (point) forecasting models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2.1 Data preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.2 The na¨ıve benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.3 The ARX model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.4 The mARX model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.2.5 The NN model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.3 Empirical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.3.1 Constructing probabilistic forecasts from point predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.3.2 Evaluating reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.3.3 Evaluating sharpness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.4 Final thoughts and recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6 Conclusions 38 2 1. Introduction In their analysis of research in time series forecasting, covering the period 1982-2005 and summarizing over 940 papers, De Gooijer and Hyndman [2] conclude that the use of prediction intervals and densities, or probabilistic forecasting, has become much more common over the years, as ‘practitioners have come to understand the limitations of point forecasts’. Nevertheless, back in 2013, when Weron started writing his review [1], this did not seem to be the case for electricity price forecasting (EPF).1 The article speculated, however, that probabilistic forecasting was one of ﬁve directions that should and would develop over the next decade or so. Somewhat surprisingly, this ‘prophecy’ has already come true. After a decade of limited interest, probabilistic EPF gained momentum with the Global Energy Forecasting Competition (GEFCom2014), which commenced in August 2014 and focused solely on probabilistic energy (load, price, wind and solar) forecasting [3]. The price track attracted 287 contestants worldwide and the best ranking teams were later invited to submit a paper to the 2016 special issue of the International Journal of Forecasting, see Section 2. Altogether, seven price forecasting articles appeared in the issue, marking the beginning of the era of probabilistic EPF. Naturally, the GEFCom2014 competition was not the reason, rather the eﬀect of increased in- terest in probabilistic energy forecasting. The energy industry has been going through a signiﬁcant modernization process. In the last decade, the increased market competition, aging infrastructure, introduction of smart grids and renewable integration requirements have had the eﬀect of proba- bilistic load and price forecasting becoming more and more important to energy systems planning and operations [4–7]. And probabilistic forecasting has a lot to oﬀer, in particular, improved asses- sment of future uncertainty, ability to plan diﬀerent strategies for the range of possible outcomes, increased eﬀectiveness of submitted bids and possibility of more thorough forecast comparisons [1, 8, 9]. However, probabilistic EPF is an underdeveloped topic, with both academics and practitioners not using the correct evaluation or testing procedures (as discussed below). With this paper we oﬀer a much needed tutorial review that explains the complexity of the available solutions, in- cluding notable techniques, statistically sound and less formal evaluation methods and common misunderstandings. The paper can be treated as an update and a further extension of the otherwise comprehensive EPF review of Weron [1] or as a standalone treatment of a fascinating and under- developed topic, that has a much broader reach than EPF itself. In particular, as Raza and Khosravi [10] argue, the electricity price is one of the inﬂuential explanatory variables in load forecasting and PEPFs could be considered as input in probabilistic load forecasting models for smart grids and buildings. We start with a top-down literature review in Section 2. We ﬁrst conduct an extensive biblio- metric study of the Web of Science and Scopus databases. Then, acknowledging the importance of the GEFCom2014 competition, in particular its competitiveness and uniﬁed forecast evaluation, we summarize the methods used by the top four winning teams in the price track (note that we 1To avoid ambiguous and verbose presentation – unless stated otherwise – we use the term price forecasting to refer to electricity price forecasting. We also use EPF as the abbreviation for both electricity price forecasting and electricity price forecast, while PEPF for probabilistic EPF. The plural form, i.e., forecasts, is abbreviated EPFs and PEPFs, respectively. 3 will utilize two of these approaches in the empirical study in Section 5). Finally, in Section 2.3 we review other important PEPF publications. In Section 3 we ﬁrst formulate the probabilistic forecasting problem, then discuss four ap- proaches to constructing probabilistic forecasts: (i) historical simulation (or empirical/sample pre- diction intervals 2, PIs), (ii) distribution-based probabilistic forecasts, (iii) bootstrapped PIs and (iv) Quantile Regression Averaging (QRA). Next, in Section 4, following the paradigm of ‘maximizing sharpness subject to reliability’ [15–17], we ﬁrst present the numerical tools and statistical tests to assess reliability (i.e., the statistical consistency between the distributional forecasts and the observations; also called calibration or unbiasedness), then discuss the techniques for measuring and analyzing the sharpness (i.e., the concentration of the predictive distributions). In the empirical study of Section 5 we employ most of the methods detailed in the preceding two Sections. To provide transparency and replicability, we use a dataset that comes from the price track of the GEFCom2014 competition, which is available as supplementary material accompa- nying Ref. [3]. In the closing paragraphs, in Section 5.4, we put forward recommendations for the evaluation of probabilistic forecasts. Finally, in Section 6 we conclude. 2. Literature review Compared to probabilistic wind power forecasting [18–21], the literature on probabilistic EPF is relatively scarce, even taking into account the 2016 special issue on the GEFCom2014 com- petition [3]. This ‘maturity’ of wind power forecasting is likely due to its close relationship to meteorological forecasting, where probabilistic predictions are well-established and commonly accepted. On the other hand, EPF has not picked up before the deregulation of the 1990s and the establishment of power markets for trading electricity [22]. As Hong et al. [3] argue, electricity prices, and especially price spikes, are inﬂuenced heavily by a wide range of factors other than the electricity demand, such as transmission congestion, generation outages, market participant behaviors, etc. These factors, and the uncertainties associated with them, are hard to incorporate into EPF models. So the ﬁrst wave of models focused on point forecasting, which is generally less demanding and easier to comprehend and implement than PEPF [1, 7]. To put probabilistic EPF in perspective we start with a bibliometric study of EPF itself. 2.1. Bibliometric survey In this section, we report on the bibliometric analysis we performed on 15 March 2017, nearly three years after a similar study of Weron [1]. We use two well-established, constantly expanding and generally acknowledged databases: Web of Science (WoS) and Scopus. We will ﬁrst present general results for both databases, then more specialized queries for Scopus only (its search engine is more user-friendly and allows for more reﬁned queries). Since the collections of publications indexed by WoS and Scopus are not the same, the results do diﬀer quantitatively but the overall picture is similar. 2Some authors have erroneously used the term conﬁdence interval (CI) instead of prediction interval (PI) [11–13]. However, in most EPF applications we are interested in PIs associated with electricity prices yet to be observed, i.e., intervals which contain the true values of future prices with speciﬁed probability, not in CIs quantifying the uncertainty of a parameter estimate. See Hyndman [14] for a discussion. 4 0 15 30 45 60 75 Number of WoS-indexed publications <2000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 Articles Proceedings papers 0 15 30 45 60 75 Number of Scopus-indexed publications <2000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 Articles Books, chapters, editorials Conference papers Figure 1: The number of WoS- (left panel) and Scopus-indexed (right panel) electricity price forecasting (EPF) pu- blications in the years 1992-2016. All publications prior to year 2000 (8 for WoS, 6 for Scopus) have been aggregated into one category ‘<2000’. In Figure 1 we plot the number of WoS- and Scopus-indexed EPF publications in the years 1992-2016.3 The overall number of publications is 559 for WoS and 664 for Scopus. Respectively 285 (51%) and 328 (49%) of these are journal articles. Both databases are constantly being ex- panded to cover more journals and proceedings volumes, but still the indexed publications are not representative of the true number of conference papers. Since the latter are typically also of lower quality than journal articles, like Weron [1], we mostly concentrate on articles. Note, however, that because we have modiﬁed the queries to better ﬁlter out relevant EPF publications, the results are not fully comparable between the two bibliometric studies. Except for a handful of papers, EPF publications have not appeared in the literature before year 2000. The next major breakthrough were the years 2005 and 2006 when the number of publications ﬁrst doubled, then tripled with respect to 2002-2004 ﬁgures, mainly due to conference 3To search publication titles, abstracts and keywords for EPF-related phrases we have used the follo- wing Scopus query: (TITLE(((((\"electric*\" OR \"energy market\" OR \"power price\" OR \"power market\" OR \"power system\" OR pool OR \"market clearing\" OR \"energy clearing\") AND (price OR prices OR pricing)) OR lmp OR \"locational marginal price\") AND (forecast OR forecasts OR forecasting OR prediction OR predicting OR predictability OR \"predictive densit*\")) OR (\"price forecasting\" AND \"smart grid*\")) OR TITLE-ABS(\"electricity price forecasting\" OR \"forecasting electricity price\" OR \"day-ahead price forecasting\" OR \"day-ahead mar* price forecasting\" OR (gefcom2014 AND price) OR ((\"electricity market\" OR \"electric energy market\") AND \"price forecasting\") OR (\"electricity price\" AND (\"prediction interval\" OR \"interval forecast\" OR \"density forecast\" OR \"probabilistic forecast\"))) AND NOT TITLE (\"unit commitment\")) AND (EXCLUDE(AU-ID,\"[No Author ID found]\" undefined)) and the equivalent WoS query. All look-ups have been further reﬁned to exclude non-English language texts or include only speciﬁc document types. 5 0 10 20 30 40 50 Number of Scopus-indexed articles and citations <2000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 Journal articles Citations ( ×50) 0 10 20 30 40 50 Number of Scopus-indexed articles <2000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 Neural network only Neural network & time series Time series only Other methods Probabilistic EPF Figure 2: Left panel: The number of Scopus-indexed EPF journal articles and citations to those articles in the years 1994-2016. Four articles prior to year 2000 have been aggregated into one category ‘<2000’. Right panel: The number of Scopus-indexed EPF journal articles in each of four ‘method’ classes (see text for details). Additionally, the number of probabilistic EPF papers in each year is indicated by a narrow white bar. papers; journal articles followed with a delay. The publication rate rapidly increased until 2009, then dropped to pre-2009 levels, to pick up again in 2012. As of 2016 the topic seems to have regained interest, with the ﬁgures for 2015-2016 being signiﬁcantly higher than the numbers for 2009. This is also visible in the constantly increasing numbers of citations, see the left panel in Fig. 2. As far as probabilistic forecasting4 is concerned, the topic was not present in the EPF literature until Zhang et al. published two conference papers in 2002 (not visible in Fig. 2) and the ﬁrst pro- babilistic EPF article in 2003 in IEEE Transactions on Power Systems [23], see the narrow white bars in the right panel of Fig. 2. Between 2005 and 2009 further eight articles were published, including two papers in a special issue of the International Journal of Forecasting on ‘Energy Forecasting’ [24, 25]. The topic picked up again in 2011 and averaged four articles per year in the period 2012-2015. A big change came in 2016 with the special issue on the GEFCom2014 competition [3], which included 7 papers on probabilistic EPF. As of 15 March 2017, 38 articles (and 9 conference papers) have been indexed by Scopus for the period 2002-2016, with another 4 articles published in 2017. Regarding the methods used, there is no clear temporal pattern, see the right panel in Fig. 2. Overall, the share of ‘neural network’-type (including support vector machines and fuzzy logic) 4To search for probabilistic EPF papers the Scopus query given in footnote 3 was appended in front by: (TITLE((\"probabilistic\" AND \"forecasting\") OR interval OR density) OR TITLE-ABS-KEY( \"probabilistic forecast*\" OR \"interval forecast*\" OR \"density forecast*\" OR \"prediction interval*\")) AND. 6 methods exceeds that of ‘statistical time series’ models. It should be noted, however, that the classiﬁcation was automatic and possibly includes some errors. In particular, the look-up for ‘sta- tistical time series’ methods is more complicated as there are many commonly used keywords and phrases. 5 Out of the 328 articles indexed by Scopus, the search yielded 184 ‘neural network’-type papers and 137 ‘statistical time series’ papers. However, in some articles both types of methods are used, in other none of the tools automatically classiﬁed as coming from one of the two groups. If we consider four disjoint sets: (i) ‘neural network’ papers, (ii) papers where both ‘neural net- work’ and ‘statistical time series’ models are used, (iii) ‘statistical time series’ papers and (iv) papers where neither ‘neural network’ nor ‘statistical time series’ methods are used, then the over- all count is 115, 69, 68 and 76, respectively. Apparently ‘neural network’-type methods are nearly twice as popular as ‘statistical time series’ techniques. Let us now see which are the most popular outlets for EPF articles, see the top panel in Fig. 3. Clearly the number one journal is IEEE Transactions on Power Systems with 35 publications (out of 328 indexed by Scopus). Like for other engineering journals, the share of ‘neural network’-type methods exceeds that of ‘statistical time series’ models. On the other hand, the latter methods are mainly published in non-engineering journals: Energy Economics and International Journal of Forecasting. In particular, not a single article published in Energy Economics involved neural networks, support vector machines or fuzzy logic. As argued by Weron [1], a likely reason for the latter situation is the diﬀerence in educational training of electrical engineers (focused on com- putational intelligence) and econometricians/statisticians (focused on regression and time series models), who constitute the two main groups of authors submitting papers to those two journal classes. Unfortunately, these diﬀerences in educational training have their consequences in the quality of research. Typically ‘electrical engineering’ papers consider sophisticated computati- onal intelligence tools and relatively simple (or not properly applied) statistical models, while ‘econometric’ or ‘statistical’ papers usually show that (advanced) statistical models outperform (simple) computational intelligence techniques. There is deﬁnitely room for improvement and closer cooperation between the two communities. In the bottom panel of Figure 3 we summarize the research output of the 14 most proliﬁc authors. The list is headed by Nima Amjady (Semnan University, Iran), who has (co-)authored 16 EPF articles, including the inﬂuential Day-ahead price forecasting of electricity markets by a new fuzzy neural network [26] (175 citations in Scopus since 2006; w/o self citations). The second on the list is Rafał Weron (Wrocław University of Technology, Poland) with 11 Scopus-indexed articles, including the most comprehensive EPF review to date – Electricity price forecasting: A review of the state-of-the-art with a look into the future [1] (103 citations since 2014); Weron has also authored the ﬁrst monograph devoted to EPF [22] (344 citations in Scopus since 2006; included in Fig. 1, but not in Figs. 2-3). The third on the list is Jo˜ao P.S. Catal˜ao (University of Beira Interior, Portugal), who has published nine EPF articles, including the highly cited Short- term electricity prices forecasting in a competitive market: A neural network approach [27] (154 5To search for ‘neural network’-type papers the Scopus query given in footnote 3 was appended in front by: TITLE-ABS-KEY(\"neural network\" OR \"support vector machine\" OR fuzzy) AND, while for ‘statis- tical time series’ methods by: (TITLE-ABS-KEY (\"AR\" OR \"ARMA\" OR \"ARIMA\" OR \"GARCH\" OR \"VaR\" OR \"regression\" OR \"autoregressive\" OR \"autoregression\") OR ABS(\"time series model\")) AND. 7 0 5 10 15 20 25 30 35 Number of Scopus-indexed articles IEEE Transactions on Smart Grid Neural Computing and Applications Applied Energy Energies International Journal of Forecasting IET Generation Transmission & Distribution Electric Power Systems Research Int. J. Electrical Power & Energy Systems Energy Economics Energy Conversion and Management IEEE Transactions on Power Systems Neural network only Neural network & time series Time series only Other methods Probabilistic EPF 0 2 4 6 8 10 12 14 16 Number of Scopus-indexed articles Zareipour, H. Luh, P.B. Garcia-Martos, C. Aggarwal, S.K. / Kumar, A. / Saini, L.M. Senjyu, T. Nowotarski, J. Dong, Z.Y. Mandal, P. Keynia, F. Catalao, J.P.S. Weron, R. Amjady, N. Neural network only Neural network & time series Time series only Other methods Probabilistic EPF Figure 3: Top panel: The number of Scopus-indexed EPF articles published in the years 1994-2016 in the 11 most popular outlets. Bottom panel: The number of Scopus-indexed EPF articles published in the years 1994-2016 by the 14 most proliﬁc authors (S.K. Aggarwal, A. Kumar and L.M. Saini have jointly written all six of their EPF papers, hence are listed as ‘one’ author). In both panels the papers are subdivided into four ‘method’ classes and additional narrow white bars indicate the number of probabilistic EPF papers in each journal (top panel) and by each author (bottom panel). citations since 2007). Generally, the top publishing authors are not very diversiﬁed in their use of forecasting tools – most specialize in ‘neural network’-type techniques (Amjady, Catal˜ao, Keynia, Mandal, Dong, Senjyu), some in ‘statistical time series’ models (Weron, Nowotarski) and a few in data-mining and dimension reduction procedures (Garcia-Martos, Zareipour). However, three of the listed authors have published only ‘multi-method’/review-type papers (Aggarwal, Kumar and Saini have jointly written all six of their EPF papers, hence are listed as ‘one’ author in Fig. 3). Regarding probabilistic forecasting, only two journals can boast a sizable amount of probabi- listic EPF studies – IEEE Transactions on Power Systems (6 articles) and International Journal of Forecasting (10 articles; largely a result of the 2016 special issue on the GEFCom2014 competi- tion), see the top panel in Fig. 3. Only three of the 14 most proliﬁc EPF authors – Rafał Weron (Wrocław University of Technology, Poland) [1, 12, 24, 28, 29], Z.Y. (Joe) Dong (University of 8 Sydney, Australia) [30–32] and Jakub Nowotarski (Wrocław University of Technology, Poland) [28, 29, 33] – have (co-)authored at least three probabilistic EPF articles, see the bottom panel in Fig. 3. 2.2. Winners of the GEFCom2014 price track The dataset available to participants of the price track consisted of three time series at an hourly resolution: locational marginal prices and day-ahead predictions of zonal and system loads, see Section 5.1 for details. During the competition the information set was being extended on a weekly basis. For the ﬁrst of the 12 competition Tasks (or weeks; there were also three ‘trial’ pre-competition Tasks) almost 2.5 years of historical data was available. The objective was to forecast 99 quantiles 6 (as an approximation of the predictive distribution) of the next day’s 24 hourly prices, i.e., arrays of 99 × 24 values. Given the high participation rate (the price track attracted 287 contestants worldwide) and the uniﬁed forecast evaluation scheme (entries of all participants were ranked using the pinball loss function, see Section 4.2.2), the GEFCom2014 competition provided a unique, large scale test ground for PEPF, something that was missing in EPF studies thus far [1]. A total of 14 teams beat the benchmark and submitted ﬁnal reports [3]. The top four teams submitted papers to the special issue; their methodology is discussed in this Section. Interestingly, three of them used quantile regression [34] as the main tool for obtaining quantiles of the predictive distribution. However, what is particularly worth emphasizing, the best performing models beat dozens of competitors in a fair ‘battle’ and, as such, are recommended for benchmarks in future probabilistic EPF research. Team TOLOLO were the winners of both the load and price tracks. Gaillard, Goude and Ne- dellec [35] used three methods for the more challenging price track. The best on average approach, dubbed quantGAM, utilizes general additive models (GAM) introduced by Hastie and Tibshirani [36] and quantile regression. The former can be viewed as an extension of linear regression – the dependent variable is explained by a sum of smooth functions of the diﬀerent covariates. The se- cond best approach, dubbed quantMixt, is an extension of Quantile Regression Averaging (QRA) introduced by Nowotarski and Weron [28], see Section 3.5 for details, with up to 13 individual point forecasting models (including variants of autoregression, regression, GAM, random forests and gradient boosting) combined using a version of the ML-Poly forecaster. Finally, the third ap- proach, dubbed quantGLM, is a kernel-based quantile regression with a lasso penalty [37]. Team TOLOLO started out by using several versions of quantMixt for Tasks 2-8, then diﬀerent versions of quantGAM for Tasks 9-12 with particularly spiky prices and, ﬁnally, for Tasks 13-15 they used quantGLM, which is designed speciﬁcally for Winter. Like that of other teams, their methodology evolved over the course of the competition. TEAM POLAND, ranked 2nd in the price track, proposed a hybrid approach which consists of four major blocks: point forecasting, pre-ﬁltering, quantile regression modeling and post- 6The qth quantile of random variable X is the value below which a fraction q of observations of this random variable fall, i.e., xq satisﬁes FX(xq) = q, where FX is the cummulative distribution function of X. A sample quantile refers to a value that splits the sample into subsamples of q and (1 − q) observations. For example, the q = 0.1 or 10% quantile is the value below which 10% of the observations may be found. Quantiles q = 0.01, 0.02, ..., 0.99 are also called percentiles. 9 processing. Maciejowska and Nowotarski [33] argue that their approach maintains a proper ba- lance between ﬂexibility and accuracy, and allows the blocks to be developed independently. Two autoregressive models with the same structure (but diﬀerent calibration samples) are used to com- pute point EPFs. They are later used, together with other explanatory variables (hourly, mean daily and ratios of load forecasts, average daily price forecasts and their squares), in a quantile regression setting [34]. As such, that TEAM POLAND’s approach can be viewed as yet another extension of QRA [28]. In the post-processing step, the 99 quantiles are sorted and the quantile curves smoothed. This last step is important, since the neighboring quantiles may be overlapping due to numerical ineﬃciency, a problem that is also known as quantile crossing [34, 38]. Team GMD, ranked 3rd, used a relatively simple neural network for computing EPFs. Dudek’s [39] model is based on a multilayer perceptron (MLP) with ﬁve sigmoid neurons in the hidden layer and one linear neuron in the output layer. To facilitate and accelerate the MLP learning, the input and output variables are preprocessed by mapping them to the interval [−0.9, 0.9]. In the ﬁrst step, the MLP with system and zonal loads (original and squared) as the only input variables is used to obtain point EPFs. The parameters are estimated once for all 24 hours of the next day using 312 hourly loads (i.e., data from 13 previous days). In the second step, the residuals are computed. Since they are assumed to be N(0, σ 2), the computation of quantile forecasts is straightforward, see Section 3.2. The C3 GREEN TEAM, ranked 4th, used machine learning techniques.7 Juban et al. [40] explain that the core part of their model used quantile regression with a regularization term and included a variable selection procedure based on leave-one-out cross validation for linear regres- sion (i.e., for point forecasting). The latter considers predicted loads, historical prices, maximum variation and standard deviation of the previous day’s prices and calendar eﬀects as potential expla- natory variables. In the next step, the input variables are transformed using a radial basis function to incorporate non-linear dependencies. Finally, the quantile regression minimization problem is solved with the help of alternating direction method of multipliers (ADMM) [41]. 2.3. Other notable probabilistic EPF papers 2.3.1. The ﬁrst years In the ﬁrst journal article on probabilistic EPF, Zhang et al. [23] propose an algorithm for obtai- ning the PIs (which they erroneously call ‘conﬁdence intervals’; see footnote 2) from a cascaded neural network model. In a follow-up paper, Zhang and Luh [11] develop a modiﬁed U-D factori- zation method within the decoupled extended Kalman ﬁlter framework. The computational speed and numerical stability of this method are improved signiﬁcantly relative to the earlier method. The new method also provides smaller PIs, though their quality is not formally assessed. Another popular computational intelligence tool, the Support Vector Machine (SVM), has been used for the ﬁrst time in probabilistic EFP by Zhao et al. [30]. They propose a data mining-based approach in order to achieve two major objectives: to forecast electricity spot prices (using the SVM) and to compute the PIs (by introducing a heteroskedastic variance equation to the SVM). Zhao et al. conclude that their method is highly eﬀective relative to existing techniques such as GARCH models. 7Originally ranked 5th, but the team ranked 4th did not submit a valid report. 10 In the ‘statistical time series’ stream of EPF literature, Misiorek et al. [12] and Weron [22] were the ﬁrst to consider probabilistic forecasts. For three expert 8 autoregressive models studied, Misiorek et al. compute the PIs (erroneously called ‘conﬁdence intervals’) by taking the quantiles of a standard normal random variable rescaled by the standard deviation of the residuals in the calibration period (see Section 3.2 for details on the distribution-based PIs). For the Markov Regime-Switching (MRS; see [1, 44]) model they use Monte Carlo simulations to obtain potential future values and, consequently, the empirical PIs. Misiorek et al. evaluate the quality of the PIs only by comparing the nominal coverage of the models to the true coverage, see Section 4.1.1. In the ﬁrst monograph devoted to EPF, Weron [22] does not perform empirical analyses of probabilistic forecasts. However, he does discuss the construction of interval forecasts (historical simulation and distribution-based), which are implemented in the Matlab toolbox accompanying the book. In an article published in the same year, Zhou et al. [45] compute the PIs for SARIMA models ﬁtted to California power market prices, but they use them only as a trigger to stop an iterative SARIMA estimation scheme and do not evaluate nor analyze the PIs. In a study that complements Ref. [12], Weron and Misiorek [24] compare the accuracies of 12 expert time series models, and evaluate their performances in terms of one-step-ahead point and interval forecasts. Two types of PIs are computed: distribution-based and empirical (see Section 3.2). The former are computed as quantiles of the error term density: Gaussian for AR-type mo- dels and kernel estimator-implied for the semiparametric models. The reliability of the PIs is assessed with the Christoﬀersen test [46] for unconditional and conditional coverage, see Sections 4.1.1-4.1.2, which is an innovation in the EPF literature. Weron and Misiorek ﬁnd that the semi- parametric models, and SNARX in particular, generally lead to better PIs than their competitors, and also, more importantly, have the potential to perform well under diverse market conditions. 2.3.2. Density forecasts In the ﬁrst EPF paper that considers density forecasts, Panagiotelis and Smith [25] develop a ﬁrst order vector autoregressive (VAR) model with exogenous eﬀects and skew t distributed innovations within a Bayesian framework. They estimate the model using Markov Chain Monte Carlo and judge the eﬀectiveness of their model by computing the Continuous Ranked Probability Score (CRPS; see Section 4.2.4) obtained from a 30 day forecasting trial. This is probably the ﬁrst PEPF paper where the reliability and sharpness of the predictive densities was jointly evaluated by computing the CRPS, one of the measures recommended in Section 4.2. Serinaldi [13] introduces the class of Generalized Additive Models for Location, Scale and Shape (GAMLSS) and computes the PIs (called ‘conﬁdence intervals’) as the time-varying quanti- les of the density forecasts. The accuracy of the PIs is checked by comparing the nominal coverage with the actual one. Surprisingly, the density forecasts themselves are not analyzed. Huurman et al. [47] consider GARCH-type time-varying volatility models and compute den- sity forecasts. To assess their reliability they use the probability integral transform (PIT) and the Berkowitz [48] test, see Section 4.1.4. Huurman et al. also measure the relative predictive accuracy by applying the Kullback-Leibler Information Criterion (KLIC) [49]. 8We adopt the terminology of Uniejewski et al. [42] and Ziel [43] who refer to such parsimonious structures as expert models, since they are usually built on some prior knowledge of experts. 11 In a more recent paper, Jonsson et al. [50] develop a semi-parametric methodology for gene- rating prediction densities by combining a time-adaptive quantile regression [34] model for the 5%-95% quantiles with an exponential distribution for the tails. They jointly evaluate the reliabi- lity and sharpness of the predictive densities by computing the average CRPS (see Section 4.2.4) and the related Continuous Ranked Probability Skill Score (CRPSS). 2.3.3. Bootstrapped PIs The method of constructing PIs via the bootstrap (see Section 3.4 for details) is very popular in the ‘neural network’ PEPF literature (though, it has also been used in ‘statistical time series’ papers [51]). For instance, Chen et al. [31] combine the extreme learning machine (ELM) with a wild (or external) bootstrap approach, and use them to compute point and interval forecasts of half-hourly spot prices in the Australian electricity market. The uncertainty of data noise is not considered in the construction of the PIs, and the accuracy of the PIs is only checked by comparing the nominal coverage with the actual one. In a follow-up paper, Wan et al. [32] ﬁrst use the ELM to obtain point forecasts of half-hourly Australian spot prices, then use a bootstrap-based ‘neural network’ procedure (involving N + 1 additional neural networks) to compute the PIs. They use the Winkler score (see Section 4.2.3) to evaluate the PIs. Khosravi et al. [52] use a neural network for point forecasts and estimate it with k-fold cross- validation (to determine the number of neurons in each of two hidden layers). In the second step, they apply the ‘delta method’ or the bootstrap to construct 90% PIs. The interval forecasts are evaluated with coverage, interval width and the ﬂawed Coverage Width-based Criterion (CWC; see [53, 54] and the discussion in Section 4.3). In a related article, Khosravi et al. [55] propose a hybrid method for the construction of PIs, which uses moving block bootstrapped neural networks and GARCH models for forecasting electricity prices. Rather than employing the traditional maximum likelihood estimation, the parameters of the GARCH model are adjusted via the minimization of a PI-based cost function. The authors claim that the proposed method generates narrow PIs with a large coverage probability, however, they again use the CWC to evaluate the intervals. More recently, Raﬁei et al. [56] consider a two-layer neural network with the clonal selection algorithm and extreme learning machine. Wavelets are used for pre-processing, to split the original time series into one approximation and three details series. The neural network is ﬁtted to each of them and their model uncertainty is computed with the bootstrap. The data uncertainty is computed afterwards on the aggregated series, again via the bootstrap. The forecasts are evaluated only with descriptive statistics, coverage and mean width. 2.3.4. Factor models and medium-term forecasts In a multivariate context, Garcia-Martos et al. [57] construct PIs based on one-day-ahead forecasts of the common volatility factors in the proposed GARCH-SeaDFA (Seasonal Dynamic Factor Analysis) model, but do not evaluate them. In a related study by the same research team, Alonso et al. [51] construct the PIs via the bootstrap (see Section 3.4), however, the evaluation is limited to just one week and assessed only with the coverage rate. Yet, the authors claim that the SeaDFA model allows to capture seasonality and forecast prices up to one year ahead. Wu et al. [58] propose a recursive dynamic factor analysis (RDFA) algorithm, where the prin- cipal components (PC) are tracked recursively using a subspace tracking algorithm, while the PC 12 scores are tracked further and predicted recursively via the Kalman ﬁlter. The PIs are obtained from the latter and their reliability is checked by comparing the nominal coverage with the ac- tual one (called ‘calibration bias’) and their sharpness by computing the ‘interval score’ (i.e., the Winkler score, see Section 4.2.3). In a recent paper, Bello et al. [59] use scenario generation and a market-equilibrium framework to compute the forecasts. A large number of scenarios is analyzed and transformed into 250 by spatial interpolation techniques. PEPFs are computed directly from them and evaluated with the pinball loss function (see Section 4.2.2) and coverage rate. 2.3.5. Spike occurrence and threshold forecasting Price spikes are a characteristic feature of electricity markets. They may also play a special role in EPF. They can be treated as any other price (as in most of the reviewed above papers), treated as outliers [60] and the input prices pre-ﬁltered to minimize or eliminate spikes [22, 24, 61–63] or they can be treated as the main object of study, as in spike occurrence and threshold forecasting. Spike occurrence forecasting is similar to predicting individual quantiles that separate two regimes – normal and spiky prices. For instance, Christensen et al. [64] treat the time series of spikes as a discrete-time point process and represent it as a nonlinear variant of the autoregressive conditional hazard (ACH) model. They conclude that the ACH model performs better than the benchmark logit model in terms of MAE, RMSE and the log-probability score error (LPSE). Bello et al. [65] compare a number of methods: logistic regression, decision trees, multilayer perceptons as well as a hybrid approach that merges logistic regression with a fundamental market equilibrium model, and evaluate the forecasts with the Brier score [16, 66]. Threshold forecasting is a generalization of spike occurrence forecasting, where the number of regimes is more than two. It could be also considered as a special case of interval forecasting where, instead of constructing a PI around a point forecast, a future price is allocated to one of a few prespeciﬁed price intervals spanning the entire range of attainable prices [1, 7]. A nice example of threshold forecasting is the paper by Zareipour et al. [67], who use two SVM-based models to classify future electricity prices in the Ontario and Alberta markets into three price groups with respect to prespeciﬁed price thresholds. They evaluate the forecasts using the mean percentage classiﬁcation error (MPCE), i.e., a percentage of misclassiﬁcations. 3. Constructing probabilistic forecasts 3.1. Problem statement To deﬁne the probabilistic forecasting problem let us start with a point forecast of the electricity spot price (i.e., the ‘best guess’ or expected value of the spot price 9). Note that the actual price at time t, i.e. Pt, can be expressed as: Pt = ˆPt + εt, (1) 9Note that although commonly used, the term point forecast is not precisely deﬁned. While most often it refers to the mean of a future value, it may as well refer to the median. 13 where ˆPt is the point forecast of the spot price at time t made at an earlier point in time and εt is the corresponding error. In a vast majority of EPF papers the analysis ends at this point, since the authors focus only on point predictions, see [1] and [7] for reviews. The most common extension from point to probabilistic forecasts is to construct prediction intervals (PIs). A number of methods can be used for this purpose, the most popular take into account both the point forecast and the corresponding error [24, 29]: the center of the PI at the (1 − α) conﬁdence level is set equal to ˆPt and its bounds are deﬁned by the α 2 th and (1 − α 2 )th quantiles of the cumulative distribution function (CDF) of εt. For instance, for the commonly used 90% PIs, the 5% and 95% quantiles of the error term are required. We later denote such a PI of the spot price at time t by [ ˆLt, ˆUt], where ˆLt and ˆUt are the lower and upper bounds, respectively. We skip the nominal rate (1 − α) for simplicity. A forecaster may extend their study further and construct multiple PIs. The ﬁnal outcome may be a set of quantiles on many levels, e.g., all 99 percentiles as in the GEFCom2014 competition. Such a set of 99 quantiles (q = 1%, 2%, ..., 99%) is also a reasonable discretization of the price distribution. In general, a density forecast corresponding to Eqn. (1) can be deﬁned as a set of PIs for all α ∈ (0, 1). In other words, computing a probabilistic forecast requires estimation of ˆPt and the distribution of εt. Equivalently, the problem can be formulated in terms of the inverse of the CDF of Pt and of εt: F−1 Pt (q) = ˆPt + F−1 εt (q). (2) Note that splitting the probabilistic forecast into a point forecast and the distribution of the error term is not the only possible approach. The problem may be stated in a more general way. In particular, Gneiting and Katzfuss [17] deﬁne the probabilistic forecast as ‘a forecast in the form of a probability distribution over future quantities or events’ and associate it with a random variable. In our case this means ﬁnding the distribution of the electricity spot price itself, i.e., ˆFPt. The latter approach is utilized in Quantile Regression Averaging (QRA; see [28] and Section 3.5 below). Finally, two important aspects of the problem have to be mentioned at this point. First, in the above discussion we do not mention the probability density function (PDF) of εt. While there are papers in which probabilistic price forecasts are expressed in those terms, see Section 2, other authors argue that such a statement should be avoided. For instance, Ziel and Steinert [68] analyze aggregated supply and demand curves in the EPEX market and ﬁnd that the price distribution is not continuous as it has additional point masses at certain prices. This implies the same conditions for the predicted distribution. Indeed, the authors predicted point probability masses up to around 20% (especially at 0 EUR/MWh). The second point relates to the statistical nature of predicting day-ahead prices. Since 24 hourly predictive distributions have to be constructed at once, their cross-dependencies should be taken into account. However, most studies simplify the framework and predict 24 marginal distributions, and do not discuss their joint distribution. Such a ‘simplistic’ approach is also taken in Section 5. It should be noted, though, that in other areas of energy probabilistic forecasting this problem has already been addressed, see eg. [69]. Two main solutions are available. The ﬁrst one is to model and predict the correlation between the marginal distributions. This, however, has a major drawback – it allows to capture only linear relationships between the hours and a question of proper evaluation arises. The second solution requires simulating 24-hour paths of the day-ahead 14 prices, which then can be treated as vectors from the joint 24-dimensional distribution. 3.2. Historical simulation The method of calculating empirical (or sample) PIs is extremely simple and in the Value-at- Risk literature is known as historical simulation [70]. It is a model-independent approach which consists of computing sample quantiles of the empirical distribution of εt [1]. Later in the text we use the suﬃx -H to denote probabilistic forecasts obtained via historical simulation. EPF studies where PIs are obtained using this approach include [12, 22, 24, 28] among others. 3.3. Distribution-based probabilistic forecasts For time series models driven by Gaussian noise (AR, ARIMA, etc.), the density forecasts can be set equal to the Gaussian distribution approximating the error density and the PIs can be computed analytically as quantiles of this distribution [12]. Later in the text we use the suﬃx -G to denote such probabilistic forecasts. This approach diﬀers from historical simulation in that ﬁrst the standard deviation of the error density, ˆσ, is computed and then the lower and upper bounds of the PI are set equal to selected quantiles of the N(0, ˆσ 2) distribution. The same approach can be used as long as the distribution of the noise term is parametric, for instance, student-t as in [25]. For time series models driven by non-parametric noise, like the IHMAR and SNAR models in [24, 28, 71], the distribution-based lower and upper bounds of the PI can be computed as quantiles of the kernel estimator of the PDF of εt. EPF studies where distribution-based PIs are computed include [12, 24, 25, 29, 30, 39, 72] among others. 3.4. Bootstrapped PIs The third approach, commonly used in neural network EPF studies, is the bootstrap. For one step-ahead forecasts, the method consists of the following steps [73, 74]: 1. Estimate the set of model parameters, ˆΘ, obtain a ﬁt and the corresponding residuals, ˆεt. 2. Generate pseudo-data recursively using ˆΘ and sampled normalized residuals ε ∗ t . • For a model with no autoregression on Pt (like the neural network model of Dudek [39]; see also Section 5.2.5) simply set P∗ t = ˆf (Xt) + ε ∗ t , where ε∗ t is the sampled residual and ˆf (Xt) is an estimated function of exogenous variables Xt. • For an AR(1) model ﬁrst set P∗ 1 = P1 and then recursively put P∗ t = ˆβP∗ t−1 + ε ∗ t for all t ∈ {2, 3, . . . , T }, where T is the time index of the last observation in the calibration window. • For a more general case of an autoregressive model of order r with exogenous variables ﬁrst set P ∗ 1 = P1, ..., P∗ r = Pr and then recursively put P ∗ t = ˆβ1P ∗ t−1+...+ ˆβrP ∗ t−r+ ˆf (Xt)+ε ∗ t for all t ∈ {r + 1, ..., T }. 3. Estimate the model again and compute the bootstrap-implied one step-ahead (point) forecast for time t = T + 1. 4. Repeat steps 2 and 3 B times and obtain the bootstrap sample of the forecasted price, { ˆPi T +1}B i=1. 5. Compute desired quantiles of { ˆPi T +1}B i=1 to obtain PIs. 15 The advantage of the bootstrap over historical simulation or distribution-based PIs is that it takes into account not only historical forecast errors but also parameter uncertainty. The disadvantage is the signiﬁcantly increased computational burden. Later in the text we use the suﬃx -B to denote probabilistic forecasts obtained via the bootstrap. EPF studies where this approach is used to compute PIs include [31, 32, 51, 52, 56, 65] among others. 3.5. Quantile Regression Averaging The fourth method we discuss is Quantile Regression Averaging (QRA), proposed by Nowo- tarski and Weron [28]. Its very good forecasting performance has been veriﬁed by a number of authors [29, 72, 75], not only in the area of EPF [6, 76]. However, its most spectacular success came during the GEFCom2014 competition – the top two winning teams in the price track used variants of QRA [33, 35], see Section 2.2. The method involves applying quantile regression, see [34], to a pool of point forecasts of individual (i.e., not combined) forecasting models. As such, it directly works with the distribution of the electricity spot price, ˆFPt, without the need to split the probabilistic forecast into a point forecast and the distribution of the error term (see Section 3.1 for a discussion). The quantile regression problem can be written as follows: QPt(q|Xt) = Xtβq, (3) where QPt(q|·) is the conditional q-th quantile of the electricity price distribution, Xt are the expla- natory variables (or regressors) and βq is a vector of parameters for quantile q. The parameters are estimated by minimizing the loss function for a particular q-th quantile: min βq   ∑ {t:Pt≥Xtβq} q|Pt − Xtβq| + ∑ {t:Pt<Xtβq} (1 − q)|Pt − Xtβq|   = min βq   ∑ t (q − IPt<Xtβq)(Pt − Xtβq)   . (4) In the ﬁrst papers on QRA [28, 75], the regressors were the point forecasts of m individual mo- dels: Xt = [1, ˆP1,t, ..., ˆPm,t]. The choice of the number of individual models can be made arbitrarily (the best three models, all models, etc.; see the empirical study in Section 5) or, in case of do- zens of competing models, using dimension reduction techniques [29]. During the GEFCom2014 competition the vector of explanatory variables was further expanded to include important exoge- nous variables (hourly, mean daily and ratios of load forecasts, average daily price forecasts and their squares) [33]. There are no limits as to the components of Xt, as long as it includes fore- casts of individual models the method can still be regarded as QRA. The Matlab function qra.m that allows to run QRA on a pool of point forecasts is available from the HSC RePEc repository (https://ideas.repec.org/s/wuu/hscode.html). 4. Evaluation metrics When evaluating a probabilistic forecast, the main challenge is that we never observe the true distribution of the underlying process. In other words, we cannot compare the predictive distri- bution, ˆFPt, or the prediction interval, [ ˆLt, ˆUt], with the actual distribution of the electricity spot price, FPt, only with observed past prices, Pτ, τ < t. 16 Table 1: A comparison of evaluation metrics for probabilistic forecasting. Statistics and tests in italics are discussed in the text, but not illustrated in the empirical study in Section 5. Interval forecasts Density forecasts Statistics Tests Statistics Tests Reliability / calibration / unbiasedness Unconditional coverage [46, 77] Kupiec [77] Probability Integral Transform (PIT) [15, 78] Visual ‘tests’ [15, 17] Tests for uniformity [79, 80] Conditional coverage [46] (CC = UC + Independence) Christoﬀersen [46] (Lagged [81]) Ljung-Box Christoﬀersen [82] Duration-based tests [83, 84] Dynamic Quantile (DQ) [85] VQR [86] Berkowitz CC statistic [48] Berkowitz [48] Sharpness (and reliability) Pinball loss [87, 88] Winkler (interval) score [89] Diebold-Mariano [90, 91] Model conﬁdence set [92] Forecast encompassing [93] Continuous Ranked Probabi- lity Score (CRPS) [16, 94] Logarithmic score [95] Diebold-Mariano [90, 91] Model conﬁdence set [92] Forecast encompassing [93] Over the years, a number of ways have been developed to evaluate probabilistic forecasts. The approach depends on the forecasting target – a quantile forecast requires a diﬀerent evaluation than a predictive distribution, but sometimes it may also depend on the preference of a forecaster. Some methods admit formal statistical tests, while other result in a single number which has a clear interpretation and is easy to compare. We summarize the more popular evaluation metrics in Table 1. Note, however, that the Table does not include measures that can be found in the EPF literature but are not recommended, we discuss some of them in Section 4.3. In a series of papers on probabilistic forecasting, Gneiting et al. [15–17] argue that ‘probabilis- tic forecasting aims to maximize the sharpness of the predictive distributions, subject to reliability’. Reliability (also called calibration or unbiasedness) refers to the statistical consistency between the distributional forecasts and the observations. For instance, if a 90% PI covers 90% of the ob- served prices, then this PI is said to be reliable10 [18, 96], well calibrated11 [15–17] or unbiased [97]. Sharpness, on the other hand, refers to how tightly the predicted distribution covers the ac- tual one, i.e., to the concentration of the predictive distributions. This deﬁnition derives from the idea that reliable predictive distributions of null width would correspond to perfect point predicti- ons [16, 18]. Unlike reliability, which is a joint property of the predictions and the observations, sharpness is a property of the forecasts only. In Section 4.1 we discuss methods for the evaluation of reliability for diﬀerent types of probabilistic forecasts, then do the same for sharpness in Section 10Note that in the electric power industry the term ‘reliability’ is often used to describe the ability of power systems to perform the required functions under stated conditions. For this reason, in their review on probabilistic load forecas- ting, Hong and Fan [5] use the term ‘unconditional coverage’ as a substitute for ‘reliability’. However, our perspective is that ‘reliability’ is a broader concept than ‘unconditional coverage’ and covers ‘independence’ and ‘conditional co- verage’ as well. Since we are not concerned here with power system performance, using the term ‘reliability’ does not lead to ambiguity. 11To avoid ambiguity, in this paper we use the term ‘calibration’ only as a substitute for ‘estimation’, when we refer to the process of estimating the parameters of a model. 17 4.2. Note that there is one more commonly used attribute for probabilistic forecast evaluation, es- pecially in the meteorological or wind power forecasting literature [18, 19]. Resolution refers to how much the predicted density varies over time, stated diﬀerently, to the ability of providing pro- babilistic forecasts (e.g., wind power) conditional to the forecast conditions (e.g., wind direction). As Pinson et al. [18] note, sharpness and resolution are equivalent when probabilistic forecasts have perfect reliability. In view of the ‘maximizing sharpness subject to reliability’ paradigm we advocate, the evaluation of resolution is not critical. Hence, we do not discuss it any further. 4.1. Reliability 4.1.1. Unconditional coverage and the Kupiec test Let us start with prediction intervals. The simplest and the most common approach for as- sessing the quality PIs is the unconditional coverage (UC). By deﬁnition, the empirical coverage should match the nominal rate: P(Pt ∈ [ ˆLt, ˆUt]) = (1 − α). For instance, the 90% PI (i.e., with α = 10%) should yield the nominal coverage of 90%. To obtain the empirical coverage we typi- cally focus on the indicator It series of ‘hits and misses’: It =    1 if Pt ∈ [ ˆLt, ˆUt] → ‘hit’, 0 if Pt < [ ˆLt, ˆUt] → ‘miss’ (or ‘violation’). (5) Note that It may be also considered for individual quantiles, as is common in the risk management (Value-at-Risk) literature [70, 82]. In such a case, the forecaster predicts only the lower or the upper quantile, i.e., in Eqn. (5) either ˆUt is replaced by ∞ or ˆLt by −∞. Some authors simply report the empirical coverage itself (sometimes called the PI coverage probability, PICP), while others subtract it from the nominal level (sometimes called the PI nominal coverage, PINC) to obtain the average coverage error (ACE = PICP − PINC), see e.g. [32, 56]. Either way, the conclusions from the comparison will be the same. Generally, the closer is the empirical coverage to the nominal rate the better. However, if we want to know if ‘close is close enough’ we have to run a formal statistical test. The Kupiec [77] test checks whether P(It = 1) = (1 − α) under the assumption that the violations are independent, which is equivalent to testing that the sequence It is identically and independently distributed (i.i.d.) Bernoulli with mean (1 − α). The test rejects the null hypothesis of an accurate PI if the actual fraction of PI violations is statistically diﬀerent than α. The Kupiec test is carried out in the likelihood ratio (LR) framework. The LR statistics for unconditional coverage: LRUC = −2 log { (1 − c) n0c n1 (1 − π)n0πn1 } (6) is distributed asymptotically as χ 2(1) [46, 77]. Here c = (1 − α) is the nominal coverage rate, π = n1/(n0 + n1) is the percentage of ‘hits’ and n0 and n1 are respectively the number of zeros and ones in the indicator It series. 18 4.1.2. Independence, conditional coverage and the Christoﬀersen test As noted by Christoﬀersen [46], the Kupiec [77] test evaluates the coverage of the PI but it does not have any power against the alternative that the ones and zeros come clustered together in the indicator It series. In other words, in the Kupiec test the order of the PI violations does not matter, only the total number of violations plays a role. To make up for this deﬁciency, Christoﬀersen introduced the independence and conditional coverage (CC) tests; the latter is simply a joint test for independence and UC. Note that some authors use the term ‘Christoﬀersen test’ to refer to all three tests (UC, independence, CC), see [1, 7]. Both tests are carried out in the LR framework. Independence is tested against an explicit ﬁrst-order Markov alternative. Hence, the LR statistics for independence is given by [46]: LRInd = −2 log { (1 − π2) n00+n10π n01+n11 2 (1 − π01)n00π n01 01 (1 − π11)n10πn11 11 } , (7) where π2 = (n01 + n11)/(n00 + n10 + n01 + n11), ni j is the number of observations with value i followed by j and πi j = P(It = j|It−1 = i). Like LRUC, also LRInd is distributed asymptotically as χ 2(1). Furthermore, if we condition on the ﬁrst observation, then the conditional coverage LR test statistics is the sum of the other two, i.e., LRCC = LRUC + LRInd, and is distributed asymptotically as χ2(2). The Matlab function christof.m that allows to run all three tests (i.e., UC, independence and CC) is available from the HSC RePEc repository (https://ideas.repec.org/s/wuu/hscode.html). Note, however, that since the day-ahead electricity price forecasts typically use the same informa- tion set for predicting the next day’s prices and hence are correlated by construction, the tests are usually conducted separately for each of the 24 hours [1, 7, 24, 28, 29, 72]. 4.1.3. Extensions and alternatives to the Christoﬀersen test As Clements and Taylor [81] note, we can conduct the independence test (and consequently the CC test) for any time lag h, in order to capture more than just the ﬁrst-order dependency. The idea of the independence test is based on the Markov chain framework, and relies on investigating transition probabilities π h i j = P(It = j|It−h = i) for h = 1. However, the latter restriction is not crucial. We can relax it and test independence of PI violations for any time lag h. Maciejowska et al. [29] argue that testing independence makes particular sense for h = 1, 2 and 7 days, as these lags are typically the most signiﬁcant when modeling and forecasting electricity spot prices. Note that the mentioned in Section 4.1.2 Matlab function christof.m allows to run the lagged Christoﬀersen test as well. Berkowitz et al. [82] go a step further and suggest to use the Ljung-Box statistics for a joint test of independence for the ﬁrst h lags. Finally, Wallis [79] recasts Christoﬀersen’s tests in the fra- mework of χ 2 statistics, and considers their extension to density forecasts. The use of contingency tables allows for the incorporation of a more informative decomposition of the χ2 goodness-of-ﬁt statistic and the calculation of exact small-sample distributions. The popular in the risk management literature Dynamic Quantile (DQ) test of Engle and Man- ganelli [85] goes in a diﬀerent direction. It is based on a linear regression model of the violations variable on a set of explanatory variables including a constant, the lagged values of the violations 19 variable and any function of the past information set suspected of being informative (for instance, the lower ˆLt and upper ˆUt quantiles themselves). The DQ test rejects the PIs if the intercept is sig- niﬁcantly diﬀerent from (1 − α) or the remaining coeﬃcients are signiﬁcantly diﬀerent from zero. There are also duration-based tests, which check if the duration (i.e., the time interval) between violations of the PI is unpredictable [83]. However, as shown in [82], the DQ test has more power against misspeciﬁed PIs than the duration-based tests and is the preferred option. Gaglianone et al. [86] argue that using only binary variables, such as whether or not there was a violation, sacriﬁces too much information. They propose the VQR (‘Value-at-Risk model based on quantile regressi- ons’) test, which uses more information to reject a misspeciﬁed model and, hence, has more power in ﬁnite samples than the Christoﬀersen or the DQ tests. 4.1.4. Probability Integral Transform (PIT) and the Berkowitz test Testing for the goodness-of-ﬁt of a predictive distribution is, in general, more challenging than evaluating the reliability of a PI. Dawid’s [78] so-called prequential principle states that the pre- dictive distributions need to be assessed on the basis of the forecast-observation pairs ( ˆFPt, Pt) only, regardless of their origins. Indeed, the true distributions, FPt, are unknown, hence stan- dard goodness-of-ﬁt tests cannot be utilized. In this context, Dawid [78] proposed the use of the Probability Integral Transform: PITt = ˆFPt(Pt), (8) which can be traced back at least to the works of Karl Pearson in the 1930s, see [15]. If the distributional forecast, ˆFPt, is perfect (i.e., is the same as the true distribution of the spot price process, FPt), then PITt is independent and uniformly distributed [98]. Although this problem formulation enables us to utilize statistical tests, see e.g. [79, 80], the common approach is to assess the uniformity and independence graphically [17]. The tools to examine it are the histogram (if the forecast is constructed properly, the histogram of PITt shows a uniform distribution) and the plot of the autocorrelation function, respectively. Non-uniformity may lead to quick conclusions how to improve the model. For instance, a histogram with too much probability mass in the center (inverse U-shape) indicates that the predictive distribution has too fat tails. Conversely, a U-shape suggests that the tails of the predictive distribution are not heavy enough. In the risk management literature the following transformation of PIT has been popularized by Berkowitz [48]: νt = Φ −1(PITt) = Φ −1 ( ˆFPt(Pt) ) , (9) where Φ −1(·) is the inverse of the standard normal distribution function. The argument behind it is that in ﬁnite-samples tests based on the Gaussian likelihood are more convenient and ﬂexible than tests of uniformity. Given the transformed sequence νt, we can test the null hypothesis of independence and normality against a ﬁrst-order autoregressive alternative with mean and variance possibly diﬀerent from 0 and 1, respectively. Writing down the ﬁrst-order autoregression: νt − µ = ρ(νt−1 − µ) + εt, (10) the null hypothesis becomes equivalent to µ = 0, σ 2 = Var(εt) = 1 and ρ = 0. 20 Like the Kupiec [77] and Christoﬀersen [46] tests, the Berkowitz test is carried out in the likelihood ratio (LR) framework. The LR statistics for independence: LRBer Ind = −2 {L( ˆµ, ˆσ 2, 0) − L( ˆµ, ˆσ2, ˆρ) } , (11) where L(·, ·, ·) is the standard normal log-likelihood function and the hats denote estimated values, is distributed as χ 2(1). Moreover, the LR statistics for a joint test of independence and normality (or conditional coverage): LRBer CC = −2 {L(0, 1, 0) − L( ˆµ, ˆσ2, ˆρ) } (12) is distributed as χ 2(3) [48]. The implementation of the test is straightforward. The Matlab function berkowitz.m that allows to run the joint test is available as part of the MFE Toolbox from Kevin Sheppard’s webpage (http://www.kevinsheppard.com/MFE Toolbox). 4.2. Sharpness 4.2.1. Proper scoring rules Sharpness, a measure of concentration of the predictive distribution, is closely related to the concept of the so-called proper scoring rules. Recall, that scoring rules provide summary measures for the evaluation of probabilistic forecasts, by assigning a numerical score, S ( ˆFPt, Pt), based on the predictive distribution, ˆFPt, and on the actually observed price, Pt [16, 94]. In fact, scoring rules assess reliability and sharpness simultaneously [17]. A proper scoring rule is designed in such a way that quoting the true distribution as the forecast distribution is an optimal strategy in expectation, i.e., it minimizes the score. More formally, denote by S ( ˆFPt, FPt) the expected value of S ( ˆFPt, Pt) under the true price distribution of Pt. A scoring rule S is proper if S (FPt, FPt) ≤ S ( ˆFPt, FPt) for any probabilistic forecast ˆFPt and any true distribution FPt. The term proper was coined by Winkler and Murphy [99], but the idea dates back at least to Brier [66]. In Sections 4.2.2-4.2.4, we present three proper scoring rules that have seen limited use in probabilistic energy forecasting [3, 5, 6, 19, 29, 32, 50, 58, 72, 100, 101] and deﬁnitely deserve to be recommended. As Gneiting and Raftery [16] emphasize, score propriety is essential in forecast evaluation. They also discuss potential issues that result from the use of intuitively appealing but improper scoring rules. Unfortunately, as the case of the relatively popular, but improper CWC score [52, 102] shows, score propriety has not received enough attention in the probabilistic energy forecasting literature, see Section 4.3. 4.2.2. Pinball loss The pinball loss gained popularity during the GEFCom2014 competition, where it was used as the scoring function of the contestants’ entries [3]. It was chosen over the more popular in probabilistic forecasting, but conceptually more complex Continuous Ranked Probability Score (CRPS; see Section 4.2.4). The pinball loss is a special case of an asymmetric piecewise linear loss function [87, 88, 103]: Pinball ( ˆQPt(q), Pt, q) =    (1 − q) ( ˆQPt(q) − Pt) , for Pt < ˆQPt(q), q (Pt − ˆQPt(q) ) , for Pt ≥ ˆQPt(q), (13) 21 where ˆQPt(q) is the price forecast at the q-th quantile and Pt is the actually observed price. This proper scoring rule is also known in the literature as the linlin, bilinear or newsboy loss [103, 104]; the latter name refers to a newsboy who must order papers when he is uncertain about the demand and unsold papers are worthless to him [94]. Note that pinball loss is the function to be minimized in quantile regression [6, 34] and is similar to Eqn. (4), the loss function minimized in Quantile Regression Averaging [28]. Secondly, the loss function in Eqn. (13) is also the loss function for a regression problem with asymmetric Laplace density assumption for the residuals (instead of Gaussian as in the standard OLS). The target quantile is the asymmetry parameter of the density [105]. The pinball loss, as deﬁned by Eqn. (13), is a measure of ﬁt for one quantile only. It can be averaged across diﬀerent quantiles to provide an aggregate score. Note that in the GEFCom2014 competition it was averaged not only across 99 quantiles (q = 1%, 2%, ..., 99%; i.e., percentiles), but also across the 24 hours of the target day [3]. A lower score indicates a better probabilistic forecast. 4.2.3. Winkler score When faced by multiple PIs with similarly accurate levels of coverage, our preference is to choose the narrowest intervals. Interestingly, reliability and interval width can be assessed jointly using the score function that was proposed by Winkler [89] and is now known as the Winkler or interval score [16]. For a central (1 − α) × 100% prediction interval it is deﬁned as: Winklert =    δt, for Pt ∈ [ ˆLt, ˆUt], δt + 2 α( ˆLt − Pt), for Pt < ˆLt, δt + 2 α(Pt − ˆUt), for Pt > ˆUt, (14) where ˆLt and ˆUt are respectively the lower and upper bounds of the PI, δt = ˆUt − ˆLt is the interval width and Pt is the actual price. The Winkler score gives a penalty if an observation (the actual price) lies outside the constructed interval and rewards a forecaster for a narrow PI; naturally the lower the score the better the PI. Note that the Winkler score, like the pinball score, is a proper scoring rule, which makes it an appealing measure for PI evaluation. 4.2.4. Continuous Ranked Probability Score (CRPS) The logarithmic score [95], also known as predictive deviance or the ignorance score, is a popular proper scoring rule that has many desirable properties, but lacks robustness [16]. It is calculated as the negative of the logarithm of the predictive density evaluated at the observed electricity price, Pt. This restriction to density forecasts can be impractical, however, and the Continuous Ranked Probability Score (CRPS) is deﬁned directly in terms of the predictive CDF, ˆFPt: CRPS ( ˆFPt, Pt) = ∫ ∞ −∞ ( ˆFPt(x) − 1 {Pt≤x})2 dx, (15) where 1 is the indicator function. The idea behind the CRPS can be traced back to the article of Matheson and Winkler [94], but the name itself was probably used for the ﬁrst time by Unger [106]. The CRPS has several appealing properties [107]: (i) its deﬁnition does not require the 22 introduction of a number of predeﬁned classes (e.g., quantiles in the pinball score) on which results may depend, (ii) for a deterministic forecast, it is equal to the well known Mean Absolute Error, and (iii) it can be interpreted as an integral over all possible Brier scores [66]. However, from a practical perspective, the integral in Eqn. (15) poses numerical diﬃculty [16, 106]. Interestingly, the CRPS can be deﬁned equivalently as follows: CRPS ( ˆFPt, Pt) = ∫ 1 0 {Pinball( ˆQPt(q), Pt, q)} dq = (16) = E ˆFPt |Yt − Pt| − 1 2E ˆFPt |Yt − Y ′ t |, (17) where ˆQPt(q) is a q-quantile forecast of the electricity price, and random variables Yt and Y ′ t are two independent copies distributed as ˆFPt. Formula (16) creates a direct link to the pinball loss function (13). Its discretization, e.g., replacing the integral by a sum over quantiles q = 0.01, ..., 0.99, enables us to avoid the complications with the direct use of Eqn. (15); we use this approach in the empirical study in Section 5. Formula (17), on the other hand, is a decomposition of the CRPS into absolute diﬀerences (ﬁrst component; which reduces to the absolute error if ˆFPt is a point forecast) and spread (second component; which measures the lack of sharpness); it has been utilized recently by Taieb et al. [108] in the context of forecasting uncertainty in smart meter data. 4.2.5. Equal predictive performance and the Diebold-Mariano test Quite often we are faced with a situation when we have two (or more) competing forecasting methods and we wish to ﬁnd the best one. We may rank them by their average score over a test set: ˆS = 1 T T∑ t=1 S ( ˆFPt, Pt), (18) where S (·, ·) is a score function and T is the length of the out-of-sample test period. However, we may wish to test if one method signiﬁcantly outperforms the other, more formally, to test the hypothesis that these two methods have equal predictive performance. The extremely simple Diebold-Mariano (DM) [90] test can be used for exactly this purpose; see Diebold [91] for a recent discussion of its uses and abuses. Although the DM test is much more popular in the point forecasting literature, in particular on EPF [71, 109–114], it is readily applicable to probabilistic forecasts. Indeed, Tastu et al. [115] and Baran and Lerch [116], among others, conduct DM tests for probabilistic wind forecasts. However, to the best of our knowledge, our paper is the ﬁrst where the DM test is used for evaluating probabilistic EPFs. The DM test is simply an asymptotic z-test of the hypothesis that the mean of the loss diﬀeren- tial series: dt = S 1( ˆFPt, Pt) − S 2( ˆFPt, Pt) (19) is zero [1, 91], where S i(·, ·) is the score (or loss) of model i. Note that in the point forecasting context this may simply be the squared loss, S 2 i ( ˆPt, Pt) = ε2 t = ( ˆPt − Pt) 2, or the absolute loss, S 1 i ( ˆPt, Pt) = |εt| = | ˆPt − Pt|. In the probabilistic forecasting context the score may be any proper 23 scoring rule, in particular the discussed above Pinball loss, Winkler score or CRPS. Given the loss diﬀerential series, we compute the statistic: DM = √T ˆµdt ˆσdt , (20) where ˆµdt and ˆσdt are the sample mean and standard deviation of dt, respectively, and T is the length of the out-of-sample test period. The key hypothesis of equal predictive accuracy (i.e., equal expected loss) corresponds to E(dt) = 0, in which case, under the assumption of covari- ance stationarity of dt, the DM statistic is asymptotically standard normal, and one- or two-sided asymptotic tail probabilities are readily calculated. Many statistical computing environments, like Matlab or R, nowadays include the DM test in the standard releases or as add-ins. In practice, typically two one-sided DM tests at the 5% signiﬁcance level are conducted: (i) a standard test with the null hypothesis H0 : E(dt) ≤ 0, i.e. the outperformance of the forecasts of model 2 by those of model 1, and (ii) the complementary test with the reverse null HR 0 : E(dt) ≥ 0, i.e., the outperformance of the forecasts of model 1 by those of model 2. To avoid a common mistake, we should remember that the DM test compares forecasts of two models, not the models themselves [91]. In day-ahead electricity markets the predictions for all 24 hours of the next day are usually made at the same time using the same information set and hence forecast errors for a particular day will typically exhibit high serial correlation. Therefore, it is advisable to conduct the DM tests for each load period (e.g., each hour of the day) separately [42, 71, 112, 117]. Even then, we should formally check that the forecasts for consecutive days, hence loss diﬀerentials, are not serially correlated. As reported by Uniejewski et al. [42], this is a generally valid assumption for well performing EPF models. 4.2.6. Alternatives to the Diebold-Mariano test Alternative forecast comparison test procedures to the Diebold-Mariano [90] test include the model conﬁdence set (MCS) approach of Hansen et al. [92] and a test of forecast encompassing (FE) [93]. For two models, the MCS approach is similar to the DM test but estimates the distribu- tion of the test statistic by a bootstrap procedure. In the test of forecast encompassing, on the other hand, the null hypothesis is that model 2 encompasses model 1, i.e., that predictions of model 1 do not contain additional information with respect to those of model 2. In one of the few applications in EPF, Bordignon et al. [112] perform both tests to evaluate combined point forecasts. 4.3. Other measures A number of other evaluation metrics can be found in the probabilistic forecasting literature. One example is the PI width, sometimes normalized by the range of prices. If a PI is constructed properly, with correct coverage rate, it is a good way to assess the concentration of the predictive distribution. However, these two are combined in the Winkler score (see Section 4.2.3) and hence the latter is preferred. Another evaluation metric that has seen widespread use in energy forecasting is the so-called Coverage Width-based Criterion [52, 102]: CWC = ¯δt {1 + 1 (∆b > 0)e η∆b} , (21) 24 where 1 is the indicator function, ∆b is the diﬀerence between nominal and empirical coverage rates, ¯δt is the average width of the PIs and η > 0 is a free parameter that can be set to any positive value. The metric was justly criticized by Pinson and Tastu [53] and Wan et al. [54]. In particular, Pinson and Tastu show that CWC is not a proper scoring rule (see Section 4.2.1) and argue that when using it ‘one can never conclude on the respective quality of the interval forecasts being evaluated’. This critique was rebutted by Khosravi and Nahavandi [102], who proposed a slightly modiﬁed version of the measure: CWCmod = ¯δt + 1 (∆b > 0)e η∆b (22) Surprisingly enough, the example given by Pinson and Tastu [53] shows – contrary to what Khos- ravi and Nahavandi [102] write – that CWCmod is not a proper scoring rule as well. As such, both the original and the modiﬁed CWC measures should be avoided in evaluation of probabilistic forecasts. 5. Empirical study 5.1. The data The dataset used in this empirical study comes from the price track of the Global Energy Forecasting Competition 2014 (GEFCom2014), the largest energy forecasting competition to date [3]. It comprises three time series at an hourly resolution – locational marginal prices (LMP, i.e. zonal prices) and day-ahead predictions of zonal and system loads, see Figure 4. The dataset is now available as supplementary material accompanying Ref. [3], however, during the competition the information set was being extended on a weekly basis to prevent ‘peeking’ into the future. The origin of the data has never been revealed by the organizers. To illustrate the probabilistic EPF and evaluation techniques discussed in this paper, we consi- der a 2.5-year period from 19 June 2011 to 17 December 2013. The ﬁrst 365 days, 19 June 2011 to 18 June 2012, constitute the ﬁrst window for calibrating the individual autoregressive models (ARX and mARX deﬁned in Sections 5.2.3-5.2.4). The neural network model (NN deﬁned in Section 5.2.5) uses a much shorter calibration window of only 312 hourly observations (i.e. 13 past days). When the day-ahead forecasts are made for the 24 hours of 19 June 2012, the 365- and 13-day windows are rolled forward by one day. This procedure is repeated until the predictions of the individual models for the last day in the sample – 17 December 2013 – are made. The second period, initially from 19 June to 17 December 2012 (i.e. 182 days), is utilized for computing the probabilistic forecasts of the na¨ıve, autoregressive and QRA models for 18 December 2012; like in [39], the neural network models use only a 13-day window, initially 5 to 17 December 2012. Then the 182- and 13-days windows are rolled forward by one day, the models are recalibrated and the probabilistic forecasts are computed for 19 December 2012. This procedure is repeated until probabilistic predictions for all 365 days in the out-of-sample test period (18 December 2012 – 17 December 2013) are obtained. 5.2. Individual (point) forecasting models On one hand, our choice of the individual (point) forecasting models is guided by the existing literature on short-term EPF and the results of the GEFCom2014 competition, on the other, by the 25 0 100 200 300 400LMP [USD/MWh] Estimation of individual (point) forecasting Estimation of PEPF Evaluation of probabilistic forecasts 19.06.2011 19.06.2012 18.12.2012 17.12.2013 5 10 15 20 25 30 35Forecasted loads [GWh]Zonal load System load Figure 4: GEFCom2014 hourly locational marginal prices (LMP; top) and hourly day-ahead predictions of the zonal and system loads (bottom) for the period 19 June 2011 to 17 December 2013. The ﬁrst 365 days, 19 June 2011 to 18 June 2012, constitute the ﬁrst window for calibrating the individual (point) forecasting models (see Section 5.2). The vertical dotted lines mark the beginning of the ﬁrst 182-day long window for estimating the probabilistic EPF models (see Section 5.3.1) and the beginning of the 365-day long out-of-sample forecast evaluation period (see Sections 5.3.2-5.3.3). desire to illustrate the probabilistic EPF and evaluation techniques without the need to resort to extremely sophisticated and ﬁne-tuned models. Overall, we consider a simple na¨ıve benchmark and three parsimonious expert (see footnote 8) models: one commonly used in EPF since the study of Misiorek et al. [12] and two that formed the backbone of probabilistic EPF approaches ranked 2nd [33] and 3rd [39] in the price track of GEFCom2014. In the ﬁrst two autoregressive structures (ARX and mARX) the modeling is implemented separately across the hours, leading to 24 sets of parameters for each day, an approach commonly taken in EPF studies [29, 35, 42, 43, 72, 117, 118]. The third expert model is a neural network (NN) which takes into account all observations in the calibration window and leads to one set of parameters for all hours; the forecast for hour 1 is then used as model input to compute the forecast for hour 2, etc. As Ziel [43] interestingly notes, when we compare the forecasting performance of relatively simple models implemented separately across the hours and jointly for all hours, the latter generally perform better for the ﬁrst half of the day, whereas the former are better in the second half of the day. This is probably the reason why the neural network models perform relatively well for the late night and early morning hours and rather poorly for the remainder of 26 the day. 5.2.1. Data preprocessing Like many studies in the EPF literature [1, 7, 98], we use transformations to make the data more symmetric and stabilize the variance. Following Uniejewski et al. [42], the autoregressive models (ARX and mARX) work on centered log-prices, pd,h = log(Pd,h) − 1 365 ∑365 t=1 log(Pt,h), with the centering performed independently for each hour h = 1, ..., 24. Note that from this point onwards we use the more natural for day-ahead markets notation and denote by Pd,h the electricity price for day d and hour h. Clearly, the previously used single time index can be obtained through the relation t = 24d + h. We can apply the logarithmic transformation since the GEFCom2014 price series is positive- valued. If datasets with zero or negative values were considered, we could work with non- transformed prices or apply a diﬀerent transformation (like the area hyperbolic sine, see [119], or the probability integral transform, see [120] and Section 4.1). In the neural network model (NN) we follow Dudek [39] and map the prices (Pd,h → ˜pd,h) and loads (Zzonal d,h → ˜zd,h, Z system d,h → ˜ζd,h) to the interval [−0.9, 0.9] to facilitate and accelerate the learning process, see also Section 5.2.5. 5.2.2. The na¨ıve benchmark The benchmark, most likely introduced by Nogales et al. [121] and dubbed the na¨ıve method, belongs to the class of similar-day techniques (for a taxonomy of EPF approaches see e.g. [1]). It proceeds as follows: the electricity price forecast for hour h on Tuesday, Wednesday, Thursday or Friday is set equal to the price for the same hour on the previous day, i.e., ˆPd,h = Pd−1,h; the forecast for hour h on Saturday, Sunday or Monday is set equal to the price for the same hour a week ago, i.e., ˆPd,h = Pd−7,h. We denote this benchmark by Na¨ıve. 5.2.3. The ARX model The ﬁrst expert model that we consider was originally proposed by Misiorek et al. [12] in one of the ﬁrst probabilistic EPF studies and later used in multiple papers [13, 22, 24, 29, 35, 42, 43, 71, 72, 117, 122, 123]. Within this model the centered log-price on day d and hour h is given by the following formula: pd,h = βh,1 pd−1,h + βh,2 pd−2,h + βh,3 pd−7,h + βh,4 p min d−1 + βh,5zd,h + βh,6DS at + βh,7DS un + βh,8DMon + εd,h, (23) where the lagged log-prices pd−1,h, pd−2,h and pd−7,h account for the autoregressive eﬀects of the previous days (the same hour yesterday, two days ago and one week ago), pmin d−1 ≡ minh=1,...,24{pd−1,h} is the minimum of the previous day’s 24 hourly log-prices, zd,h is the logarithm of the zonal load forecast for day d and hour h, DS at, DS un and DMon are dummy variables that account for the weekly seasonality and the εd,h’s are assumed to be independent and identically distributed (i.i.d.) normal variables. We denote this autoregressive benchmark by ARX to reﬂect the fact that the (zonal) load forecast is used as the exogenous variable in Eqn. (23). 27 5.2.4. The mARX model The second expert model is an extension of ARX, which evolved from it during the successful participation of TEAM POLAND in the GEFCom2014 competition [33]. The rationale for the modiﬁcations stems from the observation that it may be beneﬁcial to use diﬀerent model structures for diﬀerent days of the week, not only diﬀerent parameter sets [117]. The so-called multi-day ARX model or mARX is given by the following formula: pd,h =   ∑ i∈I βh,1,iDi   pd−1,h + βh,2 pd−2,h + βh,3 pd−7,h + βh,4 p min d−1 + βh,5zd,h + βh,6DS at + βh,7DS un + βh,8DMon + βh,11DMon pd−3,h + εd,h, (24) where I ≡ {0, S at, S un, Mon}, D0 ≡ 1 and the term DMon pd−3,h accounts for the autoregressive eﬀect of Friday’s prices on the prices for the same hour on Monday. Note that, to some extent, this structure resembles periodic autoregressive models (i.e. PAR, PARMA), which have seen limited use in EPF [42]. Both autoregressive models (ARX and mARX) are estimated with Least Squares (LS), using Matlab’s regress.m function. 5.2.5. The NN model The third expert model (denoted by NN) is a relatively parsimonious neural network that was used by Dudek [39] in the GEFCom2014 competition. Not only does it use a diﬀerent methodo- logy, more popular among electrical engineers [7, 124], but also does not contain any autoregres- sive terms. The rescaled price (to lie within the interval [−0.9, 0.9]; see Section 5.2.1) is modeled as: ˜pd,h = f (˜zd,h, ˜ζd,h, ˜z2 d,h, ˜ζ2 d,h) + εd,h, (25) where ˜zd,h and ˜ζd,h are the rescaled zonal and system load forecasts, respectively, and f (x) repre- sents a multilayer perceptron with ﬁve sigmoid neurons in the hidden layer and one linear neuron in the output layer. Unlike ARX and mARX, the NN model uses only the past 312 (= 24 hours ×13 days) observations for parameter estimation. Like in [39], the NN model is calibrated using Matlab’s Neural Network Toolbox: ﬁrst, the network is set up using the feedforwardnet.m function, then trained with the train.m function for a maximum of 100 epochs using Bayesian regularization backpropagation (trainFcn=‘trainbr’) and the sum squared error performance function (performFcn=‘sse’). 5.3. Empirical results We are now in a position to illustrate some of the techniques discussed in Sections 3 and 4. We put to work four approaches of calculating probabilistic forecasts: historical simulation, Gaussian PIs, bootstrapping and Quantile Regression Averaging (QRA). Overall we consider nine probabilistic forecasting models built on the point forecasts, ˆPt, of the four individual models deﬁned in Sections 5.2.2-5.2.5 above. Naturally, unless stated otherwise, all evaluation measures and statistics presented in Sections 5.3.2-5.3.3 are averages over all days in the 365-day test period, see Fig. 4 and Section 5.1. 28 5.3.1. Constructing probabilistic forecasts from point predictions Recall from Section 3.2 that historical simulation is a model-independent approach which consists of computing sample quantiles of the empirical distribution of the residuals: εt = Pt − ˆPt. We apply this technique to the na¨ıve benchmark and both autoregressive models; as a result we obtain three PEPF models: Na¨ıve-H, ARX-H and mARX-H. All three use a 182-day rolling window of residuals for constructing the probabilistic forecasts, independently for each hour of the day. We illustrate distribution-based PIs using the neural network model. As Dudek [39], we use a 312-hour (i.e., 13-day) rolling window to compute the standard deviation of the error density, ˆσ, then use it to retrieve quantiles of the Gaussian distribution approximating the error density, see Section 3.3 for details. The resulting probabilistic EPF model is denoted by NN-G. The bootstrap is a more complex and computationally intensive approach, which is based on generating pseudo-data using bootstrapped (i.e., resampled) residuals, see Section 3.4. We apply this technique to the point forecasts of all three expert models and obtain three probabilistic models: ARX-B, mARX-B and NN-B. The former two use a 182-day rolling window of residuals for constructing the probabilistic forecasts (independently for each hour of the day, like ARX-H and mARX-H), the latter uses a 312-hour (i.e., 13-day) rolling window of residuals (like NN-G). Finally, we apply QRA either to point forecasts of the two expert autoregressive models, re- sulting in model QRA(2), or to all three expert models, resulting in model QRA(3). Recall from Section 3.5 that a rolling window of the previous 182 days is used to calibrate QRA and obtain the PIs for the next day. Note that the Na¨ıve benchmark is not used for computing bootstrapped PIs or in QRA. 5.3.2. Evaluating reliability The unconditional coverage (UC) and the related average coverage error (ACE; see Section 4.1.1) of the 50% and 90% PIs for the nine models are shown in Table 2. The results are averages over all 24 hourly load periods. Nearly all models yield a smaller coverage than nominal, i.e., all but one ACE errors are negative. We can observe the worst performance for the two neural network based forecasts. On the other hand, models with the best unconditional coverage (i.e., the closest to nominal) are ARX-B and mARX-B, with the remaining two autoregressive structures, both QRA models and the na¨ıve benchmark trailing closely behind. Surprisingly, the na¨ıve benchmark yields a much better unconditional coverage than the neural networks. Such aggregate measures as the UC and ACE in Table 2, often reported in the probabilistic EPF literature, do not disclose the relevant details. Namely, the coverage is not uniform across the hours. In particular, for the 50% PIs, the neural network models tend to provide a little too wide (or misplaced; see the discussion on sharpness in Section 5.3.3) PIs for late night/early morning hours, with as high as 62% and 54% coverage for hour 3 for NN-G and NN-B, respectively. On the other hand, for the afternoon and evening hours they largely underestimate the variability and yield much too narrow (or signiﬁcantly misplaced) PIs, with as low as 27-28% coverage for hours 19-20. The remaining models provide a more stable performance across the hours. To formally assess coverage we run the Kupiec [77] and Christoﬀersen [46] tests for the 50% and 90% PIs. We conduct the tests separately for each of the 24 hours since the probabilistic forecasts for consecutive hours are correlated by construction. The na¨ıve, autoregressive and QRA 29 Table 2: Unconditional coverage (UC) and average coverage error (ACE, i.e., UC minus nominal coverage; see also Section 4.1.1) of the 50% and 90% two-sided day-ahead PIs by the actual spot price for all nine models. The best results in each row are emphasized in bold, the worst are underlined. Note that the results are averages over all 24 hourly load periods. Na¨ıve-H ARX-H ARX-B mARX-H mARX-B NN-G NN-B QRA(3) QRA(2) 50% prediction intervals UC 46.94% 48.32% 49.38% 47.55% 50.08% 45.50% 40.68% 47.23% 47.66% ACE −3.06% −1.68% −0.62% −2.45% 0.08% −4.50% −9.32% −2.77% −2.34% 90% prediction intervals UC 85.84% 86.59% 87.26% 85.96% 87.44% 80.29% 79.86% 85.14% 85.51% ACE −4.16% −3.41% −2.74% −4.04% −2.56% −9.71% −10.14% −4.86% −4.49% models are estimated 24 times, separately for each hour of the day, using the same information set (prices and load forecasts up to midnight on the day the prediction is made). The neural networks model all hours jointly, but use the same parameter estimates to compute the probabilistic forecasts for all 24 hours of the next day (i.e., using the same information set); it is the load forecasts that make the diﬀerence and diversify the price forecasts for each hour. The results are presented in Fig. 5. For the 50% PIs, the four autoregressive models and QRA(2) yield the best and nearly identical unconditional coverage, see the red circles in the top nine panels. At the 5% level of signiﬁcance between 22 and 24 hours pass the test, at the 1% level all hours pass the test. The QRA(3) model and, as already visible for the aggregate UC results in Table 2, the na¨ıve model follow closely by. The neural network models are deﬁnitely the worst, with acceptable coverage only for 6 to 10 hours; the problems generally arise for the daytime hours (8am-11pm). However, the situation changes when we look at the 90% PIs, see the blue triangles in the top nine panels of Fig. 5. Models ARX-B and mARX-B are now clearly better than the competitors, with as many as 16 hours passing the Kupiec test at the 5% level and 20-23 hours at the 1% level. Next in line is the ARX-H model, then mARX-H and QRA(2). For the 90% PIs, the QRA(3) model is the worst, with only three hours passing the Kupiec test at the 5% signiﬁcance level and ﬁve hours at the 1% level. Now let us look at the results of the Christoﬀersen test for CC. Recall from Section 4.1.1 that LRCC = LRUC + LRInd, if we condition on the ﬁrst observation. Thus the diﬀerences between the UC and CC tests can be explained in terms of dependence or clustering of PI violations. Note that in Fig. 5 we are only looking at ﬁrst-order dependence, however, at almost no cost we can check dependence for any lag, see Clements and Taylor [81] for a general discussion and Maciejowska et al. [29] for an application in EPF. The results for the CC test are deﬁnitely less optimistic than those for the UC test. For the 50% PIs, the bootstrapped mARX-B model performs slightly better than QRA(2) with 4 vs. 2 hours passing the test at 5% signiﬁcance and 9 vs. 7 at 1% signiﬁcance. The remaining models perform much worse, with at most two hours passing the test at 1% signiﬁcance. Finally, let us investigate the reliability of the whole predictive density, not just two selected PIs. Since the methods we consider in this empirical study do not yield predictive densities, only quantile forecasts of any level, we use a set of 99 percentiles, i.e. q = 1%, 2%, ..., 99%, to 30 4 8 12 16 20 24 0 10 20LRUC Naive-H 4 8 12 16 20 24 0 10 20 ARX-H 4 8 12 16 20 24 0 10 20 ARX-B 50% PI 90% PI 4 8 12 16 20 24 0 10 20LRUC mARX-H 4 8 12 16 20 24 0 10 20 mARX-B 4 8 12 16 20 24 0 10 20 NN-G 4 8 12 16 20 24 0 10 20LRUC NN-B 4 8 12 16 20 24 0 10 20 QRA(3) 4 8 12 16 20 24 0 10 20 QRA(2) 4 8 12 16 20 24 0 10 20LRCC Naive-H 4 8 12 16 20 24 0 10 20 ARX-H 4 8 12 16 20 24 0 10 20 ARX-B 4 8 12 16 20 24 0 10 20LRCC mARX-H 4 8 12 16 20 24 0 10 20 mARX-B 4 8 12 16 20 24 0 10 20 NN-G 4 8 12 16 20 24 0 10 20LRCC NN-B 4 8 12 16 20 24 0 10 20 QRA(3) 4 8 12 16 20 24 0 10 20 QRA(2) Figure 5: The unconditional (LRUC; top three rows) and conditional coverage (LRCC; bottom three rows) likelihood ratio statistics for the 50% (◦) and 90% (△) PIs obtained from the nine probabilistic forecasting models considered. Recall from Section 4.1.1 that LRCC = LRUC + LRInd, if we condition on the ﬁrst observation. The solid (dashed) horizontal lines represent the 5% (1%) signiﬁcance level of the appropriate χ 2 distribution. All test values exceeding 20 are set to 20. 31 0 0.25 0.5 0.75 1 0% 5% 10% 15% 20%Frequency Naive-H, 4h 0 0.25 0.5 0.75 1 0% 5% 10% 15% 20%Frequency ARX-B, 4h 0 0.25 0.5 0.75 1 0% 5% 10% 15% 20%Frequency NN-G, 4h 0 0.25 0.5 0.75 1 PIT 0% 5% 10% 15% 20%Frequency QRA(2), 4h 0 0.25 0.5 0.75 1 0% 5% 10% 15% 20%Frequency Naive-H, 19h 0 0.25 0.5 0.75 1 0% 5% 10% 15% 20%Frequency ARX-B, 19h 0 0.25 0.5 0.75 1 0% 5% 10% 15% 20% 25% 30%Frequency NN-G, 19h 0 0.25 0.5 0.75 1 PIT 0% 5% 10% 15% 20%Frequency QRA(2), 19h 0 5 10 15 20 0 0.5 1ACF Naive-H, 4h 0 5 10 15 20 0 0.5 1ACF Naive-H, 19h 0 5 10 15 20 0 0.5 1ACF ARX-B, 4h 0 5 10 15 20 0 0.5 1ACF ARX-B, 19h 0 5 10 15 20 0 0.5 1ACF NN-G, 4h 0 5 10 15 20 0 0.5 1ACF NN-G, 19h 0 5 10 15 20 Lag [days] 0 0.5 1ACF QRA(2), 4h 0 5 10 15 20 Lag [days] 0 0.5 1ACF QRA(2), 19h Figure 6: Probability integral transform (PIT) histograms and sample autocorrelation functions (ACFs) of the PIT values for hours 4 (left) and 19 (right) and four probabilistic models (top to bottom): Na¨ıve-H, ARX-B, NN-G and QRA(2). Note that for the NN-G model and hour 19 the scale on the Y-axis is compressed due to very high bins at both ends of the PIT histogram. approximate the PDFs. In Figure 6 we plot the histograms of the probability integral transform (PIT) histograms and sample autocorrelation functions (ACFs) of the PIT values for two selected hours: 4 (late night trough) and 19 (evening peak). We present results for four selected models representing four methods of constructing probabilistic forecasts: Na¨ıve-H, ARX-B, NN-G and QRA(2). Recall from Section 4.1.4, that a histogram with too much probability mass in the center (inverse U-shape; like for NN-G and hour 4) indicates that the predictive distribution has too fat tails, while a U-shape (like for NN-G and hour 19) suggests that the tails of the predictive distribution are not heavy enough. Some histograms (Na¨ıve-H for hour 19, ARX-B and QRA(2) for hour 4) exhibit skewness, with more mass below the mean than above, and a little too much probability mass at the right end, suggesting that the right tail of the predictive distribution should be heavier. Only QRA(2) for hour 19 and to some extent ARX-B for the same hour yield what seems to be a uniform distribution. The ACF plots generally conﬁrm what can be seen in the PIT 32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Hours 10 0 10 1 10 2 10 3LRCCBer Naive-H ARX-H ARX-B mARX-H NN-G QRA(3) Figure 7: The conditional coverage likelihood ratio statistics (LRBer CC ) for the Berkowitz [48] test of independence and normality of the predictive distributions obtained from all nine probabilistic forecasting models. The solid (dashed) horizontal lines represent the 5% (1%) signiﬁcance level of the χ 2(3) distribution. Note the logarithmic scale on the Y-axis. histograms: QRA(2) and ARX-B perform better than Na¨ıve-H, which in turn is better than NN-G. To formally evaluate the predictive distributions we now conduct the Berkowitz [48] test of independence and normality (i.e., of conditional coverage). The results for all nine probabilistic models and all 24 hourly load periods are presented in Fig. 7. Clearly, none of the models is perfect. All nine have problems with passing the test for the afternoon hours, particularly hours 13 through 17. Yet, if ranked, then both bootstrapped autoregressive models lead the pack with 14-15 hours passing the test at the 5% level and 17-18 hours at the 1% level, and QRA(2) follows closely by, respectively with 12 and 15 hours. At the other end are both neural networks and the na¨ıve model, which do not provide an acceptable conditional coverage for a single hour. Comparing the results of the Christoﬀersen [46] and Berkowitz [48] tests we note that while both generally rank the models in the same order, the latter provides a more holistic picture and points to problems that may not be visible when we test the CC of arbitrarily chosen PIs. As such, the Berkowitz test is the preferred option for an ‘all-in-one’ evaluation procedure. On the other hand, running the Christoﬀersen test for all PIs (from 1% to 99% nominal coverage) may indicate which quantiles are problematic and require attention or a revision of the model. For instance, the CC of the 50% PI for the mARX-B model is acceptable for hours 13 through 17, even at the more restrictive 5% level (see the middle panel in the second row from the bottom in Fig. 5), so it is likely that PIs with higher nominal coverage are responsible for a poor ﬁt of the whole predictive density. Moreover, the Christoﬀersen test can be easily modiﬁed to evaluate more than ﬁrst order dependence, see Section 4.1.3 and Refs. [29, 81], while the Berkowitz test cannot. 33 Table 3: Top: The pinball loss, as deﬁned by Eqn. (13), averaged over all 24 hourly load periods and across 99 percentiles; for an analysis of selected quantiles, see Fig. 8. Middle and bottom: The Winkler (or ‘interval’) score, as deﬁned by Eqn. (14), for the 50% and 90% two-sided day-ahead PIs averaged over all 24 hourly load periods; for an analysis of selected load periods see Fig. 9. The Winkler score is decomposed into PI width, i.e., δt, and a penalty for PI violations, i.e., 2 α ( ˆLt − Pt) or 2 α (Pt − ˆUt). The best results in each row are emphasized in bold, the worst are underlined. Na¨ıve-H ARX-H ARX-B mARX-H mARX-B NN-G NN-B QRA(3) QRA(2) Average score over 99 percentiles Pinball loss 3.927 2.943 2.774 2.971 2.788 3.364 3.305 2.634 2.791 50% prediction intervals Winkler score 34.141 25.505 24.434 25.741 24.556 29.385 29.208 23.108 24.726 PI width 7.636 6.592 8.159 6.556 8.519 10.989 9.638 9.751 10.310 Penalty 26.505 18.914 16.275 19.185 16.037 18.396 19.570 13.357 14.416 90% prediction intervals Winkler score 98.599 74.642 55.575 75.317 55.549 68.975 64.008 50.657 51.017 PI width 39.662 30.723 25.760 30.562 26.287 26.799 27.722 26.076 28.570 Penalty 58.937 43.918 29.815 44.755 29.262 42.177 36.286 24.581 22.447 5.3.3. Evaluating sharpness Following the paradigm of ‘maximizing sharpness subject to reliability’ [15–17] and given that some of the considered models yield reliable forecasts12, we are now in a position to evaluate their sharpness. In Section 4 we have presented in detail three measures: the pinball loss, the Winkler score and the continuous ranked probability score (CRPS). Note, however, that for discretized predictive densities – like the ones considered in this empirical study – the CRPS boils down to the pinball loss, see formula (16). Hence, in what follows, we do not discuss the CRPS itself. Let us ﬁrst look at the results for the pinball loss, the measure used in the GEFCom2014 competition. The aggregate pinball loss presented in Table 3, i.e., Eqn. (13) averaged over all 24 hourly load periods and across 99 percentiles, indicates that the predictions of the QRA(3) model are the sharpest, with two bootstrapped autoregressive models and QRA(2) following closely by. Again the neural networks and the na¨ıve model perform the worst. There are, however, noticeable changes compared to the reliability rankings in Section 5.3.2. Namely, in terms of sharpness measured by the pinball loss, QRA(3) outperforms the autoregressive models, while its predictions have been found to be generally less reliable than those of the autoregressive models, especially ARX-B and mARX-B. On the other hand, while the forecasts of the Na¨ıve benchmark can be regarded as more reliable than those of the neural networks, the latter provide sharper predictions (though still much worse than those of the QRA or autoregressive models). If we decompose the pinball loss and present the contribution of each quantile to the aggregate measure, as in Fig. 8, we can draw two important conclusions. Firstly, the central percentiles contribute more to the aggregate pinball loss than the very low and very high percentiles. In other 12At least for some of the hourly load periods. Note that we have built the empirical study around a simple na¨ıve benchmark and three parsimonious expert models, as the focus of the paper is not on developing a very well performing model, rather on illustrating the construction and evaluation techniques for probabilistic forecasts. 34 0 10 20 30 40 50 60 70 80 90 100 Percentiles 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5Pinball loss Naive-H NN-G Figure 8: The pinball loss for 99 percentiles, as deﬁned by Eqn. (13), averaged over all 24 hourly load periods. Note that the central percentiles contribute more to the aggregate pinball loss presented in Table 3 than the very low and very high percentiles. words, for sharp probabilistic forecasts it is absolutely crucial to get the point forecasts right, as they are an estimate of the mean or median of the predictive distribution. Errors made in the tails of the distribution play a lesser role, though should not be ignored completely. Secondly, the contribution is not symmetric across the percentiles. Pinball loss plots in Fig. 8 are more symmetric for some models (e.g., QRA(3) and Na¨ıve) than for others (particularly ARX-H and mARX-H), but all penalize more for the upper quantiles. Most likely, this is due to the spikiness of electricity spot prices and the inability of the predictive distributions obtained from simple expert models to adequately describe it. Now, let us analyze the Winkler scores for all nine probabilistic models. Recall from Section 4.2.3, that the Winkler score can be decomposed into PI width, i.e., δt, and a penalty for PI violati- ons, i.e., 2 α ( ˆLt − Pt) or 2 α (Pt − ˆUt). The latter component is similar to the pinball loss, but the former provides additional information on the sharpness of the predictive distributions. In Table 3 we pre- sent the Winkler scores for the 50% and 90% two-sided day-ahead PIs averaged over all 24 hourly load periods. Generally, both the pinball loss and the Winkler score lead to similar conclusions. According to both measures, the QRA(3) model yields the sharpest probabilistic forecasts, while the Na¨ıve benchmark the least sharp. The QRA(2) model is sharper than the two bootstrapped autoregressive models for the 90% PIs, but a little less sharp for the 50% PIs. Interestingly, the sharpness of the probabilistic forecasts of QRA(3) and QRA(2) is not due to narrow PIs. Quite the opposite, the QRA models yield relatively wide intervals, especially for the nominal coverage of 50%. They excel, however, in terms of the penalty for PI violations. This is somewhat unexpected since the penalty is partly related to reliability, which is not a strong point of QRA(3). It is also interesting to see the Winkler scores for individual hours, as plotted in Fig. 9. We have selected three load periods for the comparison: hours 4 (late night trough), 12 (midday) and 35 0 10 20 30 40 50 QRA(2) QRA(3) NN-B NN-G mARX-B mARX-H ARX-B ARX-H Naive-H 50% PI, 4h PI width Penalty 0 10 20 30 40 50 QRA(2) QRA(3) NN-B NN-G mARX-B mARX-H ARX-B ARX-H Naive-H 50% PI, 12h 0 10 20 30 40 50 QRA(2) QRA(3) NN-B NN-G mARX-B mARX-H ARX-B ARX-H Naive-H 50% PI, 19h 0 25 50 75 100 125 Winkler score QRA(2) QRA(3) NN-B NN-G mARX-B mARX-H ARX-B ARX-H Naive-H 90% PI, 4h 0 25 50 75 100 125 Winkler score QRA(2) QRA(3) NN-B NN-G mARX-B mARX-H ARX-B ARX-H Naive-H 90% PI, 12h 0 25 50 75 100 125 Winkler score QRA(2) QRA(3) NN-B NN-G mARX-B mARX-H ARX-B ARX-H Naive-H 90% PI, 19h Figure 9: The Winkler (or ‘interval’) score, as deﬁned by Eqn. (14), for the 50% and 90% two-sided day-ahead PIs and three selected load periods: hours 4, 12 and 19. Like in Table 3, the Winkler score is decomposed into PI width, i.e., δt, and a penalty for PI violations, i.e., 2 α ( ˆLt − Pt) or 2 α (Pt − ˆUt). 19 (evening peak). While the general picture is similar to the one from Table 3, in Fig. 9 we can see that for some models the PI widths vary a lot across the hours (particularly for QRA models; i.e., they have a higher resolution across the hourly load periods, see the discussion in the ﬁrst paragraphs of Section 4), while for other they do not (e.g., for neural networks). The latter may be explained to some extent by the diﬀerences in calibration windows – the neural networks use the same 312-hour (i.e., 13-day) rolling window to compute the probabilistic forecasts for all hours (the diﬀerences are a result of diﬀerent load forecasts for the individual hours), while the remaining models use a 182-day rolling window of residuals (independently for each hour of the day). After a descriptive analysis of sharpness, let us now formally evaluate the diﬀerences in pre- dictive performance with the Diebold-Mariano (DM) test [90, 91]. Since predictions for all 24 hours of the next day are made at the same time using the same information set, forecast errors for a particular day will typically exhibit high serial correlation. Therefore, like in [42, 71, 112, 117], we conduct the DM tests for each of the 24 load periods separately. In Figure 10 we summarize the DM results for all three score functions: average Pinball loss over all 99 percentiles, Winkler score for the 50% PIs and Winkler score for the 90% PIs. Like Uniejewski et al. [42], we sum the number of signiﬁcant diﬀerences in forecasting performance across the 24 hours and use a heatmap to indicate the number of hours for which the forecasts of a model on the X-axis are signiﬁcantly better than those of a model on the Y-axis. Two extreme cases – (i) the forecasts of a model on the X-axis are signiﬁcantly better for all 24 hours of the day and (ii) the forecasts of a model on the X-axis are not signiﬁcantly better for any hour – are indicated by white and black squares, respectively. Naturally, the diagonal (white crosses on black squares) should be ignored as it concerns the same model on both axes. Columns with many non- 36 PinballNaive-HARX-HARX-BmARX-HNN-GNN-BQRA(3) Naive-H ARX-H ARX-B mARX-H NN-G NN-B QRA(3) 0 4 8 12 16 20 24 Winkler 50%Naive-HARX-HARX-BmARX-HNN-GNN-BQRA(3) Naive-H ARX-H ARX-B mARX-H NN-G NN-B QRA(3) 0 4 8 12 16 20 24 Winkler 90%Naive-HARX-HARX-BmARX-HNN-GNN-BQRA(3) Naive-H ARX-H ARX-B mARX-H NN-G NN-B QRA(3) 0 4 8 12 16 20 24 Figure 10: Results for conducted one-sided Diebold-Mariano [90] tests at the 5% level for all nine probabilistic forecasting models and three score functions (from left to right): average Pinball score over all 99 percentiles, Winkler score for the 50% PIs and Winkler score for the 90% PIs. We sum the number of signiﬁcant diﬀerences in forecasting performance across the 24 hours and use a heat map to indicate the number of hours for which the forecasts of a model on the X-axis are signiﬁcantly better than those of a model on the Y-axis. A white square indicates that forecasts of a model on the X-axis are better for all 24 hours, while a black square that they are not better for a single hour. black squares (the more green or white the better; this is the case for the QRA(3) model) indicate that the forecasts of a model on the X-axis are signiﬁcantly better than the forecasts of many of its competitors. Conversely, rows with many non-black squares mean that the forecasts of a model on the Y-axis are signiﬁcantly worse than the forecasts of many of its competitors (this is the case for the na¨ıve benchmark and the neural networks to a lesser extent). Generally, the results of the DM tests support earlier formulated conclusions. Firstly, the predictions of the Na¨ıve benchmark are signiﬁcantly outperformed by those of all other models, for (nearly) all hours. The forecasts of the neural networks are outperformed by the predictions of QRA and autoregressive models for a vast majority of hours. This is especially true for the pinball loss and the Winkler score for the 50% PIs. For the 90% PIs the neural networks are relatively better, yet still perform worse than other methods. On the other hand, the best method overall is QRA(3). The forecasts of this model are never signiﬁcantly worse than the predictions of any other model and for some hours they outperform the predictions of the other models, regardless of the evaluation metric used. QRA(2) and bootstrapped autoregressive models perform slightly worse, yet still yield signiﬁcantly better predictions than the remaining models. 5.4. Final thoughts and recommendations This extensive empirical study reﬂects the complexity of the construction and evaluation of probabilistic forecasts, which goes well beyond that of point predictions. In line with the ‘max- imizing sharpness subject to reliability’ paradigm [15–17] we advocate, the ﬁrst crucial step is a thorough evaluation and formal testing of reliability, i.e., the statistical consistency between the distributional forecasts and the observations. A number of techniques have been put to work in Section 5.3.2, several more are available in the literature. Although we strongly suggest to check both the unconditional and conditional coverage, we refrain from recommending one particular procedure. The Kupiec [77] and Christoﬀersen [46] tests can be replaced by the Berkowitz [48] test if the constructed PIs span the whole range of quantiles and constitute a dense grid well ap- proximating the predictive density. However, if more than ﬁrst-order dependence needs to be 37 checked – and this may be the case for electricity price forecasts as argued by Maciejowska et al. [29] – the Christoﬀersen test lends itself readily to an extension which checks independence of PI violations at an arbitrary time lag [81] or all lags at once [82]. The autocorrelation plots of the PITs may be also useful in this context. Once we are done with reliability, and can conclude that the obtained probabilistic forecasts are reliable enough, we should look at sharpness, i.e. the concentration of the predictive distributi- ons. The latter notion is closely related to the so-called proper scoring rules, which simultaneously assess reliability and sharpness, and are minimized when the true distribution is quoted as the fo- recast distribution. We advocate the use of the pinball loss, the Winkler (or ‘interval’) score and/or the continuous ranked probability score (CRPS) to rank the forecasts. Yet, like for evaluating re- liability, several more options are available in the literature. Although similar in spirit, the three scoring rules oﬀer a slightly diﬀerent view on forecast sharpness. The pinball loss lends itself readily to averaging over all considered quantiles, while the Winkler score additionally penalizes for too wide PIs. Moreover, for discretized predictive densities – like the ones considered in this empirical study – the CRPS boils down to the pinball loss, but for continuous predictive densities it is the preferred option. After ranking forecasts using one or more of the mentioned scoring rules, we suggest to formally test for statistically signiﬁcant diﬀerences in the forecasting performance, e.g., using the Diebold-Mariano [90, 91] test. As Weron [1] remarks, this issue has apparently been downplayed in the EPF literature, although it is a standard procedure in econometrics. Sadly, three years later not much has improved. Before we conclude this Section let us brieﬂy comment on the strikingly poor performance of the neural network models. According to most evaluation measures and tests they yield either the worst predictions or only better than those of the na¨ıve benchmark. How is this possible given that one of them (NN-G) was ranked third in the GEFCom2014 competition? The surprising answer is that they perform very well for the 12 competition days (see Table 7 in [3]), but not in general. Namely, the aggregate pinball loss over those 12 days for the NN-G and NN-B models is 3.302 and 3.200, respectively, while for the autoregressive expert models and the na¨ıve benchmark it is in the range 4.454–5.100. The QRA models fare better, with aggregate pinball loss of 4.209 for QRA(2) and only 3.089 for QRA(3). Apparently, the latter model beneﬁts from including point forecasts of the neutral network. On the other hand, this inclusion leads to a generally worse reliability over the whole 365-day out-of-sample test period, when compared to QRA(2), see Section 5.3.2. This qualitatively diﬀerent performance of some models on an arbitrarily, though ex-ante, selected 12-day sample (GEFCom2014) and on a large, 365-day sample (this study) emphasizes the need for long out-of-sample test periods, an issue raised by Weron [1] in his review of EPF and more recently by Hong and Fan [5] in the context of probabilistic load forecasting. 6. Conclusions We have presented guidelines for the rigorous use of methods, measures and tests in proba- bilistic electricity price forecasting. However, the article has a much broader reach. None of the methods for constructing probabilistic forecasts discussed in Section 3 or the evaluation metrics reviewed in Section 4 is restricted to electricity prices. They all are general enough to be used for probabilistic energy forecasting, is it very short-term load forecasting for smart grid applicati- 38 ons or wind and solar power forecasting. With the increasing role of probabilistic predictions in general, we truly hope that this review paper will encourage energy forecasters to develop more eﬃcient, but at the same time statistically sound approaches. We also hope that it will propel those working in other areas of forecasting to move into the exciting and still largely unexplored world of wholesale electricity markets. Acknowledgments This paper has beneﬁted from the discussions with Grzegorz Dudek, Tao Hong, Bartosz Unie- jewski and Florian Ziel, as well as conversations with the participants of the IEEE PES General Meeting (PES-GM2015 Denver), the Energy Finance Christmas Workshops (EFC15 Paris, EFC16 Essen), the International Summer School on Risk Measurement and Control (ISS2016 Rome), the International Symposium on Forecasting (ISF2016 Santander) and the Conference on Energy Fi- nance (EF2016 Paris). The study was partially supported by the National Science Center (NCN, Poland) through grants 2013/11/N/HS4/03649 (to JN) and 2015/17/B/HS4/00334 (to RW). Bibliography [1] R. Weron, “Electricity price forecasting: A review of the state-of-the-art with a look into the future,” Interna- tional Journal of Forecasting, vol. 30, no. 4, pp. 1030–1081, 2014. [2] J. G. De Gooijer and R. J. Hyndman, “25 years of time series forecasting,” International journal of forecasting, vol. 22, no. 3, pp. 443–473, 2006. [3] T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J. Hyndman, “Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond,” International Journal of Forecasting, vol. 32, no. 3, pp. 896–913, 2016. [4] H. Moghimi Ghadikolaei, A. Ahmadi, J. Aghaei, and M. Najaﬁ, “Risk constrained self-scheduling of hydro/wind units for short term electricity markets considering intermittency and uncertainty,” Renewable and Sustainable Energy Reviews, vol. 16, no. 7, pp. 4734–4743, 2012. [5] T. Hong and S. Fan, “Probabilistic electric load forecasting: A tutorial review,” International Journal of Fore- casting, vol. 32, pp. 914–938, 2016. [6] B. Liu, J. Nowotarski, T. Hong, and R. Weron, “Probabilistic load forecasting via Quantile Regression Avera- ging on sister forecasts,” IEEE Transactions on Smart Grid, vol. 8, no. 2, pp. 730–737, 2017. [7] R. Weron and F. Ziel, Forecasting Electricity Prices: A Guide to Robust Modeling. CRC Press, 2018. forthco- ming. [8] C. Chatﬁeld, Time-Series Forecasting. Chapman & Hall/CRC, Boca Raton, Florida, 2000. [9] N. Amjady and M. Hemmati, “Energy price forecasting,” IEEE Power & Energy Magazine, pp. 20–29, March/April 2006. [10] M. Raza and A. Khosravi, “A review on artiﬁcial intelligence based load demand forecasting techniques for smart grid and buildings,” Renewable and Sustainable Energy Reviews, vol. 50, pp. 1352–1372, 2015. [11] L. Zhang and P. Luh, “Neural network-based market clearing price prediction and conﬁdence interval estima- tion with an improved extended Kalman ﬁlter method,” IEEE Transactions on Power Systems, vol. 20, no. 1, pp. 59–66, 2005. [12] A. Misiorek, S. Tr¨uck, and R. Weron, “Point and interval forecasting of spot electricity prices: Linear vs. non-linear time series models,” Studies in Nonlinear Dynamics & Econometrics, vol. 10, no. 3, p. Article 2, 2006. [13] F. Serinaldi, “Distributional modeling and short-term forecasting of electricity prices by Generalized Additive Models for location, scale and shape,” Energy Economics, vol. 33, pp. 1216–1226, 2011. [14] R. Hyndman, “The diﬀerence between prediction intervals and conﬁdence intervals.” Hyndsight Blog (13 March 2013), http://robjhyndman.com/hyndsight/intervals, 2011. 39 [15] T. Gneiting, F. Balabdaoui, and A. Raftery, “Probabilistic forecasts, calibration and sharpness,” Journal of the Royal Statistical Society B, vol. 69, pp. 243–268, 2007. [16] T. Gneiting and A. Raftery, “Strictly proper scoring rules, prediction, and estimation,” Journal of the American Statistical Association, vol. 102, no. 477, pp. 359–378, 2007. [17] T. Gneiting and M. Katzfuss, “Probabilistic forecasting,” The Annual Review of Statistics and Its Application, vol. 1, pp. 125–151, 2014. [18] P. Pinson, H. Nielsen, J. Moller, H. Madsen, and G. Kariniotakis, “Non-parametric probabilistic forecasts of wind power: Required properties and evaluation,” Wind Energy, vol. 10, no. 6, pp. 497–516, 2007. [19] Y. Zhang, J. Wang, and X. Wang, “Review on probabilistic forecasting of wind power generation,” Renewable and Sustainable Energy Reviews, vol. 32, pp. 255–270, 2003. [20] J. Jung and R. P. Broadwater, “Current status and future advances for wind speed and power forecasting,” Renewable and Sustainable Energy Reviews, vol. 31, pp. 762–777, 2014. [21] J. Yan, Y. Liu, S. Han, Y. Wang, and S. Feng, “Reviews on uncertainty analysis of wind power forecasting,” Renewable and Sustainable Energy Reviews, vol. 52, pp. 1322–1330, 2015. [22] R. Weron, Modeling and Forecasting Electricity Loads and Prices: A Statistical Approach. John Wiley & Sons, Chichester, 2006. [23] L. Zhang, P. Luh, and K. Kasiviswanathan, “Energy clearing price prediction and conﬁdence interval estimation with cascaded neural networks,” IEEE Transactions on Power Systems, vol. 18, no. 1, pp. 99–105, 2003. [24] R. Weron and A. Misiorek, “Forecasting spot electricity prices: A comparison of parametric and semiparame- tric time series models,” International Journal of Forecasting, vol. 24, pp. 744–763, 2008. [25] A. Panagiotelis and M. Smith, “Bayesian forecasting of intraday electricity prices using multivariate skew- elliptical distributions,” International Journal of Forecasting, vol. 24, pp. 710–727, 2008. [26] N. Amjady, “Day-ahead price forecasting of electricity markets by a new fuzzy neural network,” IEEE Tran- sactions on Power Systems, vol. 21, no. 2, pp. 887–896, 2006. [27] J. Catal˜ao, S. Mariano, V. Mendes, and L. Ferreira, “Short-term electricity prices forecasting in a competitive market: A neural network approach,” Electric Power Systems Research, vol. 77, no. 10, pp. 1297–1304, 2007. [28] J. Nowotarski and R. Weron, “Computing electricity spot price prediction intervals using quantile regression and forecast averaging,” Computational Statistics, vol. 30, no. 3, pp. 791–803, 2015. [29] K. Maciejowska, J. Nowotarski, and R. Weron, “Probabilistic forecasting of electricity spot prices using Factor Quantile Regression Averaging,” International Journal of Forecasting, vol. 32, no. 3, pp. 957–965, 2016. [30] J. Zhao, Z. Dong, Z. Xu, and K. Wong, “A statistical approach for interval forecasting of the electricity price,” IEEE Transactions on Power Systems, vol. 23, no. 2, pp. 267–276, 2008. [31] X. Chen, Z. Dong, K. Meng, Y. Xu, K. Wong, and H. Ngan, “Electricity price forecasting with extreme learning machine and bootstrapping,” IEEE Transactions on Power Systems, vol. 27, no. 4, pp. 2055–2062, 2012. [32] C. Wan, Z. Xu, Y. Wang, Z. Dong, and K. Wong, “A hybrid approach for probabilistic forecasting of electricity price,” IEEE Transactions on Smart Grid, vol. 5, no. 1, pp. 463–470, 2014. [33] K. Maciejowska and J. Nowotarski, “A hybrid model for GEFCom2014 probabilistic electricity price forecas- ting,” International Journal of Forecasting, vol. 32, no. 3, pp. 1051–1056, 2016. [34] R. W. Koenker, Quantile Regression. Cambridge University Press, 2005. [35] P. Gaillard, Y. Goude, and R. Nedellec, “Additive models and robust aggregation for GEFCom2014 proba- bilistic electric load and electricity price forecasting,” International Journal of Forecasting, vol. 32, no. 3, pp. 1038–1050, 2016. [36] T. J. Hastie and R. J. Tibshirani, Generalized additive models, vol. 43. CRC Press, 1990. [37] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society B, vol. 58, pp. 267–288, 1996. [38] V. Chernozhukov, I. Fernandez-Val, and A. Galichon, “Quantile and probability curves without crossing,” Econometrica, vol. 73, no. 3, pp. 1093–1125, 2010. [39] G. Dudek, “Multilayer perceptron for GEFCom2014 probabilistic electricity price forecasting,” International Journal of Forecasting, vol. 32, pp. 1057–1060, 2016. [40] R. Juban, H. Ohlsson, M. Maasoumy, L. Poirier, and J. Z. Kolter, “A multiple quantile regression approach to the wind, solar, and price tracks of GEFCom2014,” International Journal of Forecasting, vol. 32, no. 3, 40 pp. 1094–1102, 2016. [41] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of multipliers,” Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011. [42] B. Uniejewski, J. Nowotarski, and R. Weron, “Automated variable selection and shrinkage for day-ahead elec- tricity price forecasting,” Energies, vol. 9, p. 621, 2016. [43] F. Ziel, “Forecasting electricity spot prices using LASSO: On capturing the autoregressive intraday structure,” IEEE Transactions on Power Systems, vol. 31, no. 6, pp. 4977–4987, 2016. [44] J. Janczura and R. Weron, “An empirical comparison of alternate regime-switching models for electricity spot prices,” Energy Economics, vol. 32, no. 5, pp. 1059–1073, 2010. [45] M. Zhou, Z. Yan, Y. Ni, G. Li, and Y. Nie, “Electricity price forecasting with conﬁdence-interval estimation through an extended arima approach,” IEE Proceedings: Generation, Transmission and Distribution, vol. 153, no. 2, pp. 187–195, 2006. [46] P. Christoﬀersen, “Evaluating interval forecasts,” International Economic Review, vol. 39, no. 4, pp. 841–862, 1998. [47] C. Huurman, F. Ravazzolo, and C. Zhou, “The power of weather,” Computational Statistics and Data Analysis, vol. 56, no. 11, pp. 3793–3807, 2012. [48] J. Berkowitz, “Testing density forecasts, with applications to risk management,” Journal of Business & Econo- mic Statistics, vol. 19, no. 4, pp. 465–474, 2001. [49] Y. Bao, T.-H. Lee, and B. Salto˘glu, “Comparing density forecast models,” Journal of Forecasting, vol. 26, no. 3, pp. 203–225, 2007. [50] T. Jonsson, P. Pinson, H. Madsen, and H. Nielsen, “Predictive densities for day-ahead electricity prices using time-adaptive quantile regression,” Energies, vol. 7, no. 9, pp. 5523–5547, 2014. [51] A. Alonso, C. Garcia-Martos, J. Rodriguez, and M. Sanchez, “Seasonal dynamic factor analysis and bootstrap inference: Application to electricity market forecasting,” Technometrics, vol. 53, pp. 137–151, 2011. [52] A. Khosravi, S. Nahavandi, and D. Creighton, “Quantifying uncertainties of neural network-based electricity price forecasts,” Applied Energy, vol. 112, pp. 120–129, 2013. [53] P. Pinson and J. Tastu, “Discussion of ‘Prediction intervals for short-term wind farm generation forecasts’ and ‘Combined nonparametric prediction intervals for wind power generation’,” IEEE Transactions on Sustainable Energy, vol. 5, no. 3, pp. 1019–1020, 2014. [54] C. Wan, Z. Xu, J. Østergaard, Z. Y. Dong, and K. P. Wong, “Discussion of ‘Combined nonparametric prediction intervals for wind power generation’,” IEEE Transactions on Sustainable Energy, vol. 5, no. 3, pp. 1021–1021, 2014. [55] A. Khosravi, S. Nahavandi, and D. Creighton, “A neural network-garch-based method for construction of prediction intervals,” Electric Power Systems Research, vol. 96, pp. 185–193, 2013. [56] M. Raﬁei, T. Niknam, and M. Khooban, “Probabilistic electricity price forecasting by improved clonal selection algorithm and wavelet preprocessing,” Neural Computing and Applications, 2016. DOI: 10.1007/s00521-016- 2279-7. [57] C. Garcia-Martos, J. Rodriguez, and M. Sanchez, “Forecasting electricity prices and their volatilities using Unobserved Components,” Energy Economics, vol. 33, no. 6, pp. 1227–1239, 2011. [58] H. C. Wu, S. C. Chan, K. M. Tsui, and Y. Hou, “A new recursive dynamic factor analysis for point and interval forecast of electricity price,” IEEE Transactions on Power Systems, vol. 28, no. 3, pp. 2352–2365, 2013. [59] A. Bello, J. Reneses, A. Mu˜noz, and A. Delgadillo, “Probabilistic forecasting of hourly electricity prices in the medium-term using spatial interpolation techniques,” International Journal of Forecasting, vol. 32, no. 3, pp. 966–980, 2016. [60] J. Janczura, S. Tr¨uck, R. Weron, and R. Wolﬀ, “Identifying spikes and seasonal components in electricity spot price data: A guide to robust modeling,” Energy Economics, vol. 38, pp. 96–110, 2013. [61] M. Shahidehpour, H. Yamin, and Z. Li, Market Operations in Electric Power Systems: Forecasting, Scheduling, and Risk Management. Wiley, 2002. [62] A. J. Conejo, J. Contreras, R. Esp´ınola, and M. A. Plazas, “Forecasting electricity prices for a day-ahead pool-based electric energy market,” International Journal of Forecasting, vol. 21, pp. 435–462, 2005. 41 [63] J. Nowotarski, J. Tomczyk, and R. Weron, “Robust estimation and forecasting of the long-term seasonal com- ponent of electricity spot prices,” Energy Economics, vol. 39, pp. 13–27, 2013. [64] T. Christensen, A. Hurn, and K. Lindsay, “Forecasting spikes in electricity prices,” International Journal of Forecasting, vol. 28, no. 2, pp. 400–411, 2012. [65] A. Bello, J. Reneses, and A. Mu˜noz, “Medium-term probabilistic forecasting of extremely low prices in elec- tricity markets: Application to the Spanish case,” Energies, vol. 9, no. 3, p. 193, 2016. [66] G. W. Brier, “Veriﬁcation of forecasts expressed in terms of probability,” Monthly Weather Review, vol. 78, no. 1, pp. 1–3, 1950. [67] H. Zareipour, A. Janjani, H. Leung, A. Motamedi, and A. Schellenberg, “Classiﬁcation of future electricity market prices,” IEEE Transactions on Power Systems, vol. 26, no. 1, pp. 165–173, 2011. [68] F. Ziel and R. Steinert, “Electricity price forecasting using sale and purchase curves: The X-model,” Energy Economics, vol. 59, pp. 435–454, 2016. [69] P. Pinson, H. Madsen, H. A. Nielsen, G. Papaefthymiou, and B. Kl¨ockl, “From probabilistic forecasts to statis- tical scenarios of short-term wind power production,” Wind Energy, vol. 12, no. 1, pp. 51–62, 2009. [70] C. Alexander, Market Risk Analysis IV: Value at Risk Models. Wiley, 2008. [71] J. Nowotarski, E. Raviv, S. Tr¨uck, and R. Weron, “An empirical comparison of alternate schemes for combining electricity spot price forecasts,” Energy Economics, vol. 46, pp. 395–412, 2014. [72] J. Nowotarski and R. Weron, “To combine or not to combine? Recent trends in electricity price forecasting,” ARGO, vol. 9, pp. 7–14, 2016. [73] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap. Chapman & Hall, New York, 1993. [74] M. P. Clements and J. H. Kim, “Bootstrap prediction intervals for autoregressive time series,” Computational Statistics & Data Analysis, vol. 51, pp. 3580–3594, 2007. [75] J. Nowotarski and R. Weron, “Merging quantile regression with forecast averaging to obtain more accurate interval forecasts of Nord Pool spot prices.” IEEE Conference Proceedings – EEM14, DOI: 10.1109/EEM.2014.6861285, 2014. [76] Y. Zhang, K. Liu, L. Qin, and X. An, “Deterministic and probabilistic interval prediction for short-term wind power generation based on variational mode decomposition and machine learning methods,” Energy Conver- sion and Management, vol. 112, pp. 208–219, 2016. [77] P. H. Kupiec, “Techniques for verifying the accuracy of risk measurement models,” The Journal of Derivatives, vol. 3, no. 2, pp. 73–84, 1995. [78] A. P. Dawid, “Statistical theory: The prequential approach,” Journal of the Royal Statistical Society A, vol. 147, pp. 278–292, 1984. [79] K. F. Wallis, “Chi-squared tests of interval and density forecasts and the Bank of England fan charts,” Interna- tional Journal of Forecasting, vol. 19, pp. 165–175, 2003. [80] V. Corradi and N. R. Swanson, “Predictive density evaluation,” in Handbook of Economic Forecasting (G. El- liott, C. W. J. Granger, and A. Timmermann, eds.), pp. 197–284, Elsevier, 2006. [81] M. P. Clements and T. Taylor, “Evaluating interval forecasts of high-frequency ﬁnancial data,” Journal of Applied Econometrics, vol. 18, no. 4, pp. 445–456, 2003. [82] J. Berkowitz, P. Christoﬀersen, and D. Pelletier, “Evaluating Value-at-Risk models with desk-level data,” Ma- nagement Science, vol. 57, no. 12, pp. 2213–2227, 2011. [83] P. Christoﬀersen and D. Pelletier, “Backtesting Value-at-Risk: A duration-based approach,” Journal of Finan- cial Econometrics, vol. 2, no. 1, pp. 84–108, 2004. [84] A. P. P. Santos and M. F. Alves, “A new class of independence tests for interval forecast evaluation,” Computa- tional Statistics and Data Analysis, vol. 56, no. 11, pp. 3366–3380, 2012. [85] R. F. Engle and S. Manganelli, “CAViaR: Conditional autoregressive Value at Risk by regression quantiles,” Journal of Business & Economic Statistics, vol. 22, no. 4, pp. 367–381, 2004. [86] W. Gaglianone, L. Lima, O. Linton, and D. Smith, “Evaluating Value-at-Risk models via quantile regression,” Journal of Business and Economic Statistics, vol. 29, no. 1, pp. 150–160, 2011. [87] C. W. J. Granger, “Prediction with a generalized cost of error function,” Operational Research Quarterly, vol. 20, pp. 199–207, 1969. [88] T. Gneiting, “Quantiles as optimal point forecasts,” International Journal of Forecasting, vol. 27, no. 2, 42 pp. 197–207, 2011. [89] R. L. Winkler, “A decision-theoretic approach to interval estimation,” Journal of the American Statistical As- sociation, vol. 67, no. 337, pp. 187–191, 1972. [90] F. X. Diebold and R. S. Mariano, “Comparing predictive accuracy,” Journal of Business and Economic Statis- tics, vol. 13, pp. 253–263, 1995. [91] F. X. Diebold, “Comparing predictive accuracy, twenty years later: A personal perspective on the use and abuse of Diebold-Mariano tests,” Journal of Business and Economic Statistics, vol. 33, no. 1, pp. 1–9, 2015. [92] P. R. Hansen, A. Lunde, and J. M. Nason, “The model conﬁdence set,” Econometrica, vol. 79, pp. 453–497, 2011. [93] D. Harvey, S. Leybourne, and P. Newbold, “Tests for forecast encompassing,” Journal of Business and Econo- mic Statistics, vol. 16, pp. 254–259, 1998. [94] J. E. Matheson and R. L. Winkler, “Scoring rules for continuous probability distributions,” Management Science, vol. 22, pp. 1087–1096, 1976. [95] I. J. Good, “Rational decisions,” Journal of the Royal Statistical Society B, vol. 14, pp. 107–114, 1952. [96] P. Pinson and G. Kariniotakis, “Conditional prediction intervals of wind power generation,” IEEE Transactions on Power Systems, vol. 25, no. 4, pp. 1845–1856, 2010. [97] J. W. Taylor, “Evaluating volatility and interval forecasts,” Journal of Forecasting, vol. 18, pp. 111–128, 1999. [98] B. Uniejewski, R. Weron, and F. Ziel, “Variance stabilizing transformations for electricity spot price forecas- ting,” IEEE Transactions on Power Systems, 2017. submitted. [99] R. L. Winkler and A. H. Murphy, “‘Good’ probability assessors,” Journal of Applied Meteorology, vol. 7, pp. 751–758, 1968. [100] J. M. Morales, A. J. Conejo, H. Madsen, P. Pinson, and M. Zugno, Integrating Renewables in Electricity Markets: Operational Problems. Springer, 2014. [101] C. Wan, Z. Xu, P. Pinson, Z. Dong, and K. Wong, “Probabilistic forecasting of wind power generation using extreme learning machine,” IEEE Transactions on Power Systems, vol. 29, no. 3, pp. 1033–1044, 2014. [102] A. Khosravi and S. Nahavandi, “Closure to the discussion of ‘Prediction intervals for short-term wind farm generation forecasts’ and ‘Combined nonparametric prediction intervals for wind power generation’ and the discussion of ‘Combined nonparametric prediction intervals for wind power generation’,” IEEE Transactions on Sustainable Energy, vol. 5, no. 3, pp. 1022–1023, 2014. [103] G. Elliott and A. Timmermann, Economic Forecasting. Princeton University Press, 2016. [104] M. G. Morgan, M. Henrion, and M. Small, Uncertainty: A Guide to Dealing with Uncertainty in Quantitative Risk and Policy Analysis. Cambridge University Press, 1990. [105] F. Paraschiv, D. W. Bunn, and S. Westgaard, “Estimation and application of fully parametric multifactor quan- tile regression with dynamic coeﬃcients,” 2016. University of St. Gallen, School of Finance Research Paper No. 2016/07. Available at SSRN: https://ssrn.com/abstract=2741692. [106] D. A. Unger, “A method to estimate the continuous ranked probability score.” Preprints, 9th Conference on Probability and Statistics in Atmospheric Sciences, Virginia Beach, VA, 206-213, 1985. [107] H. Hersbach, “Decomposition of the continuous ranked probability score for ensemble prediction systems,” Weather and Forecasting, vol. 15, pp. 559–570, 2000. [108] S. B. Taieb, R. Huser, R. J. Hyndman, and M. G. Genton, “Forecasting uncertainty in electricity smart meter data by boosting additive quantile regression,” IEEE Transactions on Smart Grid, vol. 7, no. 5, pp. 2448–2455, 2016. [109] J. C. Cuaresma, J. Hlouskova, S. Kossmeier, and M. Obersteiner, “Forecasting electricity spot-prices using linear univariate time-series models,” Applied Energy, vol. 77, no. 1, pp. 87–106, 2004. [110] A. Gianfreda and L. Grossi, “Forecasting Italian electricity zonal prices with exogenous variables,” Energy Economics, vol. 34, no. 6, pp. 2228–2239, 2012. [111] Y.-Y. Hong and C.-P. Wu, “Day-ahead electricity price forecasting using a hybrid principal component analysis network,” Energies, vol. 5, no. 11, pp. 4711–4725, 2012. [112] S. Bordignon, D. W. Bunn, F. Lisi, and F. Nan, “Combining day-ahead forecasts for British electricity prices,” Energy Economics, vol. 35, pp. 88–103, 2013. [113] K. Maciejowska and R. Weron, “Forecasting of daily electricity prices with factor models: Utilizing intra-day 43 and inter-zone relationships,” Computational Statistics, vol. 30, no. 3, pp. 805–819, 2015. [114] K. Maciejowska and R. Weron, “Short- and mid-term forecasting of baseload electricity prices in the U.K.: The impact of intra-day price relationships and market fundamentals,” IEEE Transactions on Power Systems, vol. 31, no. 2, pp. 994–1005, 2016. [115] J. Tastu, P. Pinson, P.-J. Trombe, and H. Madsen, “Probabilistic forecasts of wind power generation accounting for geographically dispersed information,” IEEE Transactions on Smart Grid, vol. 5, no. 1, pp. 480–489, 2014. [116] S. Baran and S. Lerch, “Mixture EMOS model for calibrating ensemble forecasts of wind speed,” Environme- trics, vol. 27, no. 2, pp. 116–130, 2016. [117] J. Nowotarski and R. Weron, “On the importance of the long-term seasonal component in day-ahead electricity price forecasting,” Energy Economics, vol. 57, pp. 228–235, 2016. [118] N. Karakatsani and D. Bunn, “Forecasting electricity prices: The impact of fundamentals and time-varying coeﬃcients,” International Journal of Forecasting, vol. 24, pp. 764–785, 2008. [119] S. Schneider, “Power spot price models with negative prices,” Journal of Energy Markets, vol. 4, no. 4, pp. 77– 102, 2011. [120] G. Diaz and E. Planas, “A note on the normalization of Spanish electricity spot prices,” IEEE Transactions on Power Systems, vol. 31, no. 3, pp. 2499–2500, 2016. [121] F. J. Nogales, J. Contreras, A. J. Conejo, and R. Espinola, “Forecasting next-day electricity prices by time series models,” IEEE Transactions on Power Systems, vol. 17, pp. 342–348, 2002. [122] A. Misiorek, “Short-term forecasting of electricity prices: Do we need a diﬀerent model for each hour?,” Medium Econometrisch Toepassingen, vol. 16, no. 2, pp. 8–13, 2008. [123] T. Kristiansen, “Forecasting Nord Pool day-ahead prices with an autoregressive model,” Energy Policy, vol. 49, pp. 328–332, 2012. [124] D. Keles, J. Scelle, F. Paraschiv, and W. Fichtner, “Extended forecast methods for day-ahead electricity spot prices applying artiﬁcial neural networks,” Applied Energy, vol. 162, pp. 218–230, 2016. 44 HSC Research Report Series 2016 For a complete list please visit http://ideas.repec.org/s/wuu/wpaper.html 01 To combine or not to combine? Recent trends in electricity price forecasting by Jakub Nowotarski and Rafał Weron 02 The diamond model of social response within an agent-based approach by Paul R. Nail and Katarzyna Sznajd-Weron 03 Linking consumer opinions with reservation prices in an agent-based model of innovation diffusion by Anna Kowalska-Pyzalska, Karolina Ćwik, Arkadiusz Jędrzejewski and Katarzyna Sznajd-Weron 04 Impact of social interactions on demand curves for innovative products by Katarzyna Maciejowska, Arkadiusz Jędrzejewski, Anna Kowalska-Pyzalska and Rafał Weron 05 On the importance of the long-term seasonal component in day-ahead electricity price forecasting by Jakub Nowotarski and Rafał Weron 06 Automated variable selection and shrinkage for day-ahead electricity price forecasting by Bartosz Uniejewski, Jakub Nowotarski and Rafał Weron 07 Recent advances in electricity price forecasting: A review of probabilistic forecasting by Jakub Nowotarski and Rafał Weron","libVersion":"0.3.2","langs":""}
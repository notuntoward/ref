{"path":"lit/lit_sources/LazarMORALCASEUSING.pdf","text":"1 Preprint 1 THE MORAL CASE FOR USING LANGUAGE MODEL AGENTS FOR RECOMMENDATION Seth Lazar (ANU), Luke Thorburn (KCL), Tian Jin (MIT), Luca Belli (Sator Labs)* Machine Intelligence and Normative Theory (MINT) Lab ABSTRACT Our information and communication environment has fallen short of the ideals that networked global communication might have served. Identifying all the causes of its pathologies is difficult, but existing recommender systems very likely play a contributing role. In this paper, which draws on the normative tools of philosophy of computing, informed by empirical and technical insights from natural language processing and recommender systems, we make the moral case for an alternative approach. We argue that existing recommenders incentivise mass surveillance, concentrate power, fall prey to narrow behaviourism, and compromise user agency. Rather than just trying to avoid algorithms entirely, or to make incremental improvements to the current paradigm, researchers and engineers should explore an alternative paradigm: the use of language model (LM) agents to source and curate content that matches users’ preferences and values, expressed in natural language. The use of LM agents for recommendation poses its own challenges, including those related to candidate generation, computational efficiency, preference modelling, and prompt injection. Nonetheless, if implemented successfully LM agents could: guide us through the digital public sphere without relying on mass surveillance; shift power away from platforms towards users; optimise for what matters instead of just for behavioural proxies; and scaffold our agency instead of undermining it. 1. INTRODUCTION The pathologies of our information and communication environment are widely known [21, 40]. As the social media landscape fragments, they are if anything worsening [19]. From interpersonal abuse to epistemic pollution (of many kinds), from manipulation to polarisation, we presently fall far short of the lofty ideals for how the ‘wealth of networks’ might enrich our public sphere [10, 21, 56]. But what should we do? Scholars are divided as to whether the problems with online communication are wholly due to our failures as communicators [8, 85], or whether the technologies that structure online communication are also to blame. We are * Lazar conceived the project and wrote the paper, with contributions (in order of contribution) from Thorburn, Jin and Belli. This work is supported by Australian Research Council grant FT210100724 and Templeton World Charity Foundation grant 2019-20466. This is a preprint of a work in progress, comments welcome to seth.lazar[at]anu.edu.au. 2 Preprint 2 unlikely ever to definitively answer this vexed question. And if we ourselves are really at fault, then there is probably nothing we can do to fix that. Technologies, on the other hand, are somewhat easier to alter. In this paper, we acknowledge that setting out to heal the digital public sphere is likely a fool’s errand. We do not know whether our existing technologies for the allocation of online attention are the source of the problem, or just an epiphenomenon. We can, however, show that these technologies are deeply interwoven with aspects of the digital public sphere that are independently objectionable—irrespective of whether they ultimately contribute to our societally flawed communicative practices. And on that basis, we can argue for an alternative paradigm for attention allocation that need not have the same failings. In short, we argue that our existing algorithmic recommenders rely on mass surveillance, centralise power, are narrowly behaviourist, and compromise user agency. We argue that researchers and engineers should pursue an alternative paradigm, in which we develop recommender systems that leverage advances in the capabilities of language models (LMs), especially their ability to use other software tools to achieve more demanding objectives.1 We claim that using agents based on these LMs—Language Model Agents—for recommendation could help the digital public sphere avoid at least these four underlying pathologies. Of course, whether avoiding these particular sins will have a net positive effect on the broader health of the digital public sphere is an open question. If the digital public sphere is toxic because people are toxic, then however healthy our recommenders that toxicity will probably rise to the surface. And more pointedly, even if Language Model Agents for recommendation could be used to avoid these pathologies, that does not guarantee that they will. As they move beyond early proof of concept stage [47, 48, 73, 74], powerful actors will face strong incentives to intentionally interweave them with existing practices of surveillance, extraction, and obfuscation. However, resolving these pathologies of the digital public sphere through our existing approach to recommendation is possible only if powerful digital gatekeepers choose for it to be so. Research on Language Model Agents could enable an end-run around those gatekeepers. We therefore propose that technical researchers motivated to improve the digital public sphere should direct their attention to this approach—and that in doing so they should take full advantage of the affordances of Language Model Agents, so as not to repeat the mistakes of the current approach. Section 2 briefly surveys the state of online communication. Section 3 argues that our existing recommender systems are implicated in and incentivise four specific undesirable features of our communicative environment. Section 4 considers responses, and introduces LM agents for recommendation, showing in Section 5 how they could potentially resolve the limitations of our existing systems. Section 6 discusses some of the problems LM recommenders would face in their turn. Section 7 1 Note that we are using ‘LM’ here to mean any transformer-based token-sequence predictor, including multimodal foundation models, vision-language models, and others. 3 Preprint 3 reviews related technical work. Section 8 concludes. 2. SLAVES TO THE ALGORITHM? The digital public sphere—the whole environment of online communication, including explicitly political as well as facially non-political content and discussion— can be assessed in two distinct directions. We can focus on the explicit harms that it enables. And we can highlight its failure to realise a substantive positive ideal. The harms caused by online communication fall into roughly three buckets: abuse, epistemic pollution, and manipulation [56]. The first includes direct and indirect abuse, harassment, and silencing [81, 101]. The second includes communicative practices that make forming accurate beliefs harder, such as the production and circulation of misinformation and disinformation, coordinated inauthentic behaviour, obfuscation, ‘flooding the zone’, useless SEO’d slop made with generative AI, and the ‘liar’s dividend’ [37, 44, 46, 100]. The third, manipulation, is the practice of using communication to influence others to make decisions in ways that compromise their agency [11, 88]. This includes both intentional 1:1 manipulation, for example grooming or radicalisation, and sometimes-unintended manipulation of whole populations, as when platforms contribute to affective polarisation [79], or to the rise of body-image anxiety [22]. Much normative analysis of the public sphere stops there. But a healthy public sphere is defined not only by the absence of pathologies, but by the realisation of positive goods. Very roughly, we should aim to fulfil people’s communicative interests (the interests served by communication) in a fair and optimal way. We should be aiming, in other words, for communicative justice [56]. We will not, here, develop a substantive theory of communicative justice; we only note that insofar as we are designing the digital public sphere we need technologies that can advance positive ideals, as well as remediate harms. What explains the pathologies of the digital public sphere? Commentators often blame its technological architecture, in particular the recommender systems that allocate our attention [65, 66, 91]. While implementations vary, on large social media platforms (e.g., Facebook, Instagram, YouTube, X, and TikTok) these algorithms are at a high- level implemented as a pipeline that, in response to a user request to view a feed, will (1) identify candidate items for recommendation using computationally efficient heuristics, (2) rank those candidates according to a more computationally intensive \"value model\", and (3) slightly re-rank that list to achieve secondary outcomes (e.g., to avoid consecutive items being too similar) [91]. The value model, intended to capture some notion of the value of recommending an item in a particular context, is in practice usually a weighted sum of engagement predictions (e.g., the probability the user will dwell on the item as they scroll through their feed), predicted survey responses (e.g., the probability someone would say the item was 'worth their time'), and classifier scores (e.g., the probabilities an item is not clickbait or overly violent) [2, 5-7, 83]. Some recommenders will also include other components—for example, 4 Preprint 4 Spotify and YouTube likely also include logic that aim to give all content producers a chance to be seen or discovered [13, 16]. The pervasive role played by recommenders in the digital public sphere makes them an obvious target for criticism, especially in popular discourse [28, 75]. Given that the alternative is to blame ourselves for our harmful communicative practices, it is not surprising how ready people are to blame ‘the algorithm’. But this critique is in fact hard to empirically substantiate [35, 38, 68, 70]. And on more general grounds, decades of work on technology’s societal impacts invites scepticism about technological determinism—the idea that a technology’s nature exhaustively predetermines its impacts [26]. Every technology ever designed has been used in surprising ways, often contrary to its intended purpose. Technological outcomes depend on our agency as users of technology, as well as on the societal context in which technologies are used, as much as on the nature of the technology itself. And if our communicative vices are what ultimately explains these communicative harms, then we’re likely to express those vices with whatever communicative means are available to us. So we’re not arguing, in this paper, that we should pursue an alternative path for algorithmic recommendation because doing so is likely to fix all the ills of the digital public sphere. We have more modest goals. We will identify four specific pathologies with which our existing approaches to algorithmic recommendation are tightly interwoven: mass surveillance, concentration of power, behaviourism, and the undermining of user agency. And we will argue that Language Model Agents can in principle be designed that perform the same core societal function of filtering and ranking content without depending to the same degree on the same pathologies. And crucially, these Language Model Agents can be developed and deployed without the explicit authorisation of the digital gatekeepers who currently benefit from those aforementioned pathologies. Building the kinds of LM-based recommender systems that we describe may well make for a healthier digital public sphere overall. But even if it did not, there would still be reason to do it, because resisting surveillance, power concentration, and behaviourism, while advocating for user agency, are worth doing for their own sake. 3. WHAT’S WRONG WITH RECOMMENDERS? 3.1. Surveillance The core tasks of any recommendation algorithm are (at least functionally) to understand candidates for the user’s attention, understand their preferences and values, and make inferences about which candidates will best serve those preferences and values. The prevalent paradigm for online recommendation relies on mass surveillance to perform all three tasks: recommenders understand content by observing how people interact with it; they understand users by observing their behaviour and comparing it 5 Preprint 5 to other users; and they use all of this data to make inferences about what content to show to which users, in what order. Without this surveillance, they would not perform remotely as well at their task. This was true of the early-2010s work on recommendation and predicting click-through rates on ads that first shaped, for example, Facebook’s ad placement algorithms and ultimately its News Feed [36, 41]. It remains true of contemporary approaches to recommendation centred on a value model—regardless of the specific learning paradigm (e.g., supervised, unsupervised, or reinforcement learning) and the specific architectures used—because predictive models that feed into the value model are learned from historical behavioural data. We won’t try to substantiate the claim that mass surveillance of this kind is presumptively morally objectionable—a vast literature has already developed that critique at length (e.g., [11, 96, 107]). We note only that clearly it is reasonable to object to such surveillance, and if our current technologies for allocating online attention depend on it, then that gives us reason to reject those technologies, and to seek others that are not dependent in the same way. One might counter that some privacy enhancing technologies might be able to mitigate the downside of mass surveillance for attention allocation. Perhaps by making better use of methods like differential privacy or federated learning on cohorts, we could have our cake (current recommenders) and eat it too (not worry about surveillance). But while these methods provide a certain kind of secrecy or confidentiality, reducing the individual privacy harms associated with mass surveillance, they have been emphatically shown to do nothing to remediate the broader collective and structural harms of mass surveillance [11, 82, 90, 92]. 3.2. Concentrating Power Existing recommender systems rely on behavioural data to understand content, user preferences, and the connection between them. So they work better when they have access to more behavioural data. This behavioural data is available to them only when the content that they are filtering and ranking is centrally distributed—by controlling the distribution of the content, platform owners can ensure that they collect the data necessary for their recommender systems to work effectively (of course, they want that data for other reasons too). Together, these facts entail that the most effective way to develop and deploy existing recommenders is to maximise the scale of your platform—to bring as much content and as many users together as possible. Increases in scale are equivalent to increases in the concentration of power: the larger the platform, the more data is being collected, the more users’ attention is being allocated, the greater the degree of influence over people’s welfare, options, beliefs and desires [55] that sits with those who own the platform and its recommenders. Existing recommender systems both depend on and incentivise the concentration of power. They enable a few people to substantially influence how the attention of billions is allocated: to amplify some signals, and to de-amplify or even disappear others [34]. There are good philosophical reasons to object to the concentration of power over the distribution of attention [52, 56, 89]. But deeper arguments are perhaps unnecessary when recent years have provided so many object lessons in the folly of trusting this power to a few quixotic billionaires. 6 Preprint 6 The thus-far unrealised promise of ‘middleware’ further shows how existing recommender algorithms concentrate power [32]. Middleware advocates noted that recommenders determine the allocation of attention in society, and so give a few people an unacceptable degree of control over our public sphere. They proposed a market in recommenders sitting between the existing platforms and the user, to break that oligopoly on distributing attention. This obviously attractive prospect has as yet failed to take off (notwithstanding incipient efforts on Bluesky), probably due to the dependence of existing recommenders on behavioural data. The platforms still control this data. And they have both self-interested and legal reasons grounded in data protection not to share it with third parties [53]. To truly challenge the concentration of power over the distribution of attention—and to realise the promise of middleware— we need recommender systems that do not depend on that concentration of power. 3.3. Narrow Behaviourism Existing recommenders make their inferences based primarily on behavioural data— mouseovers, pauses when scrolling, clicks, and explicit signals of likes, reposts, and so on [3, 4, 6]. Obviously other signals are also sometimes used (for example, platforms’ perceptions of content quality) [24]. But recommender systems’ reliance on behavioural data leads to their making many narrowly behaviourist inferences. This is a problem, for at least two reasons. First, your behaviour expresses your revealed preferences, but is a poor guide to your actual preferences [65]. The former are your choices given the options available to you. Your actual preferences are the choices you would make if you had good enough information, option set, and the ability to regulate your own behaviour. Our revealed preferences often fail to shed light on our actual preferences because these three conditions are often inadequately fulfilled [78]. For example, many complain about the moral temperature of online communication—the degree of moral outrage and contestation highlighted in [79]—while at the same time showing a revealed preference to engage with just that kind of content [14, 15]. This is in part because on any given occasion we are unaware of the subtle harm we are doing ourselves by engaging with this content, we lack a sufficient range of adequate alternatives, and we are prone to give in to our baser impulses. Second, existing recommenders’ behaviourism leaves inadequate room for building in societal values [12]. These concern what we (as individuals or collectively) would like to see instantiated in the world, over and above fulfilment of our individual preferences. Revealed preferences cannot accurately measure and implement the promotion of these goods, because individuals almost never face choices that can, in their own right, translate societal values into a preference over options. For example, we, the authors, value a healthy public sphere that supports democratic deliberation. But we have arguably never faced a discrete choice that would have turned that value into a preference (we might have opportunities to contribute to democratic deliberation, but that’s different from expressing a preference for a deliberative public sphere). Conversely, behaviourism supports the narrow maximisation of individual self- interest in each choice, but this approach predictably leads to collective action problems, resulting in aggregate outcomes that few of us would prefer if given the 7 Preprint 7 choice. 3.4. Compromising Agency Existing recommenders are designed to satisfy our attentional appetites with as little intervention from us as possible. They passively observe us as we act, in ways that we largely cannot control, learning from our behaviour and exploiting our System I brains [50], serving us content that we mindlessly scroll even when we know we should be doing something better. Because of how they learn, we cannot easily fine-tune or otherwise shape them, but must ‘train’ them over a long period of use [30], or else rely on crude heuristics like muting keywords. Even when controls are made available, at most a few percent of people use them [24, 49]. Because of their complexity, as well as their reliance on inscrutable deep learning models that identify relations within high- dimensional matrices of features that we could never hope to understand, we cannot inspect them or correct them [59]. As a result, they compromise our attentional agency: that is, they limit our intentional, reasons-directed control over the allocation of our attention [77, 98]. Of course, people can still exercise attentional agency by drawing on different sources and different algorithms—nobody is forced to direct their attention only as one recommender system prescribes. We are not making the exaggerated claim that algorithmic recommendation deprives us of any agency at all over our attention (for a version of that implausibly strong claim, see e.g. [42]). We do contend, however, that overreliance on mechanisms for filtering and recommending online content that are inscrutable and largely uncontrollable, and that situate us as passive consumers, is other things equal bad for our attentional agency. This is problematic in itself, but also has secondary effects—the perception that others’ attentional agency is being compromised can easily slip into those exaggerated claims about manipulation and extrinsic marionette-like control. Even if those claims are in fact unfounded, the mere perception that others’ attentional agency is so deeply compromised can contribute to mutual distrust, for example when people default to believing that those they politically disagree with are systematically misled, rather than holding reasonable views in good faith [11]. Existing recommender systems incentivise surveillance, concentrate power, are narrowly behaviourist, and compromise our attentional agency. This does not make them responsible for all the shortcomings of the digital public sphere. But it does imply that if we can develop alternative technologies that do not depend on these communicative vices, we have reason to do so. 4. WHAT PATHS FORWARD? Existing work suggests at least three responses to the shortcomings of algorithmic recommendation [103]. One is to abandon ML-based recommendation entirely. Another keeps the basic architecture of existing approaches, but aims to identify better behavioural signals. And we will introduce the third at the end of this section. 8 Preprint 8 4.1. ‘Algorithms Ruin Everything’ [27] The first approach, characterised primarily in popular discussion on the ‘fediverse’, especially Mastodon, aims to escape the pathologies of online communication by returning to a subscription and network model [17, 69] in which people are exposed to content only through their voluntary agency—either by directly subscribing, or through their social network, seeing content recommended by those they follow [29]. Content is then presented in reverse chronological order—an algorithm of sorts, to be sure, but one that does not use AI in order to predict what people are likely to want to see. In our view, however, rejecting algorithmic recommendation entirely comes at too great a cost. Purely subscription- and network-based social media feeds require too much attentional agency on the part of the individual, and fail to fulfil our communicative interests due to inadequate discovery mechanisms and inadequate ranking. Existing recommender systems make easy work of both tasks; rejecting them entirely forces much of that work onto the user, and permits only crude, reactive measures (for example, disabling a person’s reposts, or else creating multiple different lists to filter one’s feed). And there is surely a more scientific way of saying this, but reverse chronological ordering sucks, especially if you don’t live in the same time zone as the people with whom who you are sharing a conversation. 4.2. Complementing Behaviourism More promising is to find different metrics or methods to direct existing recommender systems towards more positive societal outcomes [24]. This can include creating new forms of explicit feedback mechanism that give people more agency over their recommendations (think of Facebook’s introduction of different reactions instead of just ‘like’). And it can mean finding new behavioural signals that serve as proxies for other valuable properties—as with the Community Notes approach on X, which ranks contributions in part based on whether they are approved by people who are otherwise ideologically distant from one another (on ‘bridging’ algorithms more generally see [71]). We won’t attempt to review all the proposals here, noting only two things. First, many of them are clearly worth pursuing. Second, while they might help to address some of the concerns with existing recommenders—for example, worries about their ‘optimising for engagement’ [91]—as long as they persist in the same basic paradigm of algorithmic recommendation based on deep learning models trained on user behaviour, they will continue to incentivise mass surveillance, concentrate power, have at least some of the flaws of narrow behaviourism, and likely continue to compromise user agency. To fix the shortcomings of existing recommenders, we may need a more radical approach. 4.3. LMs Significant progress in functional natural language and image understanding by LMs, as well as growing reasoning and planning abilities, suggest an alternative approach to 9 Preprint 9 recommendation (see [12, 31, 47, 62, 95]; the underlying approach was earlier explored in [86]). Recent LMs have three capabilities that could be particularly salient. First: natural language and image understanding. Whether LMs truly understand, for some deep meaning of ‘truly’, is unimportant for our purposes [67]. Functional understanding is enough. That is, they can adeptly classify content [105] in line with complex evaluative concepts [47, 54], and they can engage in dialogue with users about their preferences and values, and represent those preferences and values in a distilled way [58]. [74] shows that a recommender based on natural language profiles of user preferences can perform on par with standard approaches based on matrix factorization, while [106] shows how concise natural language preference profiles can be learned using a standard language model architecture. Second, LMs are capable of at least some kinds of reasoning and planning. These skills have limits, especially when LMs face out of distribution settings [60, 94]. However, they are demonstrably capable of the minimal reasoning task of combining the model’s understanding of some piece of content with its understanding of a user’s preferences and values, and ranking the relevance of that content to that user ([12, 47, 48]; for work specifically on agents, see [25, 33]). Third, LMs are capable tool-users [76]. They can identify when it would be useful to make a function call to another piece of software, make the call, evaluate the response and iterate if necessary, and incorporate the successful response into their context window for further processing. This enables them, for example, to process information more efficiently than simply by including everything in their context window, through techniques like Retrieval Augmented Generation [61]. More generally, it means that LMs can operate as part of more complex systems, where the LM is the executive centre but it draws on different software tools to enable it to achieve the tasks it has been set. Because these more complex systems involve LMs undertaking complex sequences of action without supervision, based on its own reasoning capability, they are appropriately described as LM agents. This presents two distinct possibilities for using LMs to support algorithmic recommendation. On the first, the LM is a tool called upon by a program that also integrates other tools; the task of the LM is limited to, for example, classifying content and matching it to a model of the user (see e.g. [12, 18]). On the second, the LM itself exercises executive control, functionally deciding when to call on other pieces of software. Only the second is strictly speaking an LM agent. Early work in the latter vein has shown promising results. For example, [104] modelled web interactions as state space exploration, [64] focused on multi-modal models that interact visually with interfaces, and [39] trained a web agent LM from scratch using web page source code. An LM-based recommender must have at least four distinct elements: First, a model of the user’s preferences and values and of societal values, which is inspectable and corrigible. Second, a representation of the content to which the user has previously been exposed, enabling the LM recommender to take into account holistic features of the user’s feed, including for example diversity of content, recurrence of themes, and so on. Third, some way of identifying candidates for recommendation (we’ll discuss two distinct possibilities below). And of course fourth, the reasoning engine that 10 Preprint 10 infers, given 1, 2, and 3, what content should be recommended to the user, and in what order. Note that different LMs could be used in each of these four tasks, depending on which capabilities are most salient, and of course on practical considerations like efficiency. 5. THE MORAL PROMISE OF LM-AIDED RECOMMENDATION To be clear, Language Model Agents are not especially performant yet. While LMs do a surprisingly good job of simulating reasoning and planning in a wider range of settings than one might expect, they are prone to making bizarre errors, and even the most capable agents cannot perform modestly complex web browsing tasks, or make substantial progress on software engineering benchmarks (however, bear in mind that before 2023 AI agents were vastly inferior at all of these tasks, so the rate of progress is still impressive). However, more capable LMs are clearly on the horizon, and it has already been shown that complementing LMs with state machines while giving them access to software APIs can enable significant boosts in performance. Even sceptics have argued that LMs together with planning algorithms [51] can plan much better, and fast progress on the ARC benchmark is currently being made through integrating LMs with search algorithms [1]. Nobody has shown a definitive theoretical or empirical reason to think that Language Model Agents won’t significantly improve in the next year or two— what’s more, this is not something that is out of our control. If Language Model Agents would be societally beneficial, then more effort will be put into making them work, and that effort might bear fruit (indeed, with the release of OpenAI’s o1- preview substantial progress might be on the horizon). Technologies might not be value-neutral, but nor do they come with particular end- states baked in [99]. Digital technologies are currently researched, developed, and deployed by the most highly-capitalised and most powerful corporations in the world. If Language Model Agents work well for recommendation, they are likely to be developed and deployed by those corporations, and are likely at least to continue mass surveillance and centralising power, since these are in the interests of those corporations. Enhancing user agency in general means shifting power away from the centre, incurring a risk that users will resist surveillance and take control of their own attention. So we can be reasonably confident that Language Model Agents will be bent to support the existing pathologies of the digital public sphere (with the possible exception of narrow behaviourism) if their development is entirely controlled by the world’s biggest tech companies. However, nothing is inevitable, and Language Model Agents for recommendation have some advantages over our existing algorithmic infrastructure that might enable a very different future. First, and most importantly, LM recommenders don’t need mass behavioural surveillance in order to understand candidate pieces of content, the user’s preferences and values, and then make inferences about the relevance of one to the other. Instead, they can understand the content just in virtue of their ability to functionally understand natural language and images; the same capabilities enable them to form a rich understanding of the user’s values; and their functional simulation of reasoning 11 Preprint 11 allows a mapping from one to the other. This much has already been well- demonstrated (e.g. for societal values [12, 47]). Of course, Language Model Agents as attention-allocators would still generate highly valuable behavioural data, which companies would have an incentive to collect and allocate. But the key difference from existing systems is that this is an externality, not directly related to the recommenders’ performance of their core function. This can create space for users to choose (and businesses to offer) an approach to recommendation that does not depend on mass surveillance. Second, because they would not rely on surveillance, LM recommenders need not concentrate power—whether through surveillance itself, or through the network effects that make existing recommenders work better the more people use them. LM recommenders will of course benefit from scale, but their fundamental reasoning capability depends on the performance of the underlying model, not on live observations about behaviour. Of course, training a highly capable LM does at present require considerable resources, with the next generation of models potentially costing as much as billions of dollars. But we already have GPT-4 level models licensed for commercial use (as long as the number of end-users is less than 700m). Some large tech companies clearly have a commercial incentive to make the most capable models available open weights, if only to prevent their competitors from achieving monopolistic dominance in the field. Moreover, the trajectory of the last eighteen months has been that the frontier AI labs invest hundreds of millions in shifting the frontier forward, and then smaller companies use those models to generate synthetic data to train smaller models, or else distil those models into smaller models, focusing on the key areas of performance for a particular use case. And further to this, we obviously are not powerless in this regard either! If Language Model Agents prove to be as societally consequential as they clearly could, then regulators would have an obvious reason to insist that the market not devolve into an oligopoly. Over the last eighteen months, open models have become more performant and smaller [20]; clear algorithmic improvements are being made [43]; the cost of compute is declining rapidly and radically [23]. Over the next eighteen months, GPT-4 level models may be able to operate locally on PCs and mobile devices. If it takes the next generation of models to make Language Model Agents truly functional, then while those models will initially be centrally controlled by a few big corporations, within a short period afterwards we can reasonably aim for them to also be distilled and to function either locally on device, or else through secure (encrypted) cloud compute. The end result is that it should ultimately be perfectly feasible for people to have their own Language Model Agents, working only in their own interests, that aren’t beholden to the big platform companies. This can shift power away from the centre to the edge: since if you can deploy an Language Model Agent to track down content on your behalf, then you don’t need to subject yourself to the pervasive monitoring and influence of the platforms. Of course, the platforms would resist the prospect of Language Model Agents scraping content, but this could simply shift users towards open protocols like the fediverse, the main shortcoming of which at present is their inability to integrate with existing recommender systems. 12 Preprint 12 Third, LM agents would obviously not have to be narrowly behaviourist. Instead, they would understand our preferences and (societal) values in the same way that we do: they would be able to analyse and deploy the concepts that we attach to them. They already display impressive facility at this task [58]; this can be expected to improve as they improve in other dimensions. An LM recommender could talk with the user about what they want to see, both in the abstract and through dialogical engagement about specific recommendations [73]. Training it would be similar to a process of training a human assistant, giving direct instructions from which they immediately learn, so that they can instinctively operate without supervision. This would be a huge benefit to us! Instead of simply propagating our revealed preferences (and often pandering to our worst selves), recommenders could engage in a thoughtful, quasi- empathetic dialogue about how we want to allocate our attention. They can also nudge us towards better choices, and if necessary also incorporate societal values to guard against our individually rational choices having collectively irrational consequences [12]. Fourth, LM agents would scaffold our agency, instead of compromising it. As opposed to relying on the observed behaviour of millions of people to make predictions about what is likely relevant for you, via a complex mathematical model that you have no way of either extracting or understanding, it can make those inferences in a way that is inspectable and corrigible by the user, because there is a natural language representation of the user that can be queried [73]. You should be able to query and correct the user model in natural language, as well as to challenge any particular recommendation. And if the LM recommender is well-designed, then its inferences should be grounded in a Chain of Thought (or equivalent) reasoning process that is straightforwardly interpretable (if not always totally faithful [93]). The very simple idea here is that, when we developed our existing approaches to recommendation, computers could not understand natural language or images very well, and could not explicitly reason in the subtle manner required to recommend content. So we developed complex mathematical proxies for these tasks, identifying workarounds so that our interests and preferences could be more-or-less extracted from our behaviour. And these workarounds had the unfortunate negative externality of disempowering us and empowering the corporations who control them. But these are, fundamentally, just workarounds. It would be much better if we could directly convey to our computational tools what we really care about, and if they could directly aim to meet those preferences, just by understanding the different candidates for our attention. That now seems feasible. And if it is, then it promises to radically alter the attention economy. Instead of being subject to Mark Zuckerberg’s or Elon Musk’s decisions about what you should and shouldn’t see online, you would have your own agent acting out of your own interests and values. This would in turn radically reduce the ‘stickiness’ of the big digital platforms. Instead of having to put up with X/Twitter because that’s the only way to reliably keep track of AI papers and news, you’d be able to have your Language Model Agent track down those papers across different platforms and bring them to you. Instead of tediously tracking through Bluesky, Threads, Mastodon et al, your agent would do that for you—and liberate you from reverse chronological ordering. Language Model Agents for 13 Preprint 13 recommendation could break the key network effects on which the contemporary platform economy relies, enabling a kind of de facto bottom-up interoperability, as a tool-using LM agent browses the web on your behalf, and aggregates content from across different platforms.2 This would obviously conflict with some platforms’ existing terms of service—but their only power here is that people choose to post on those platforms and not on an open protocol; and they only choose to post on those platforms because that’s how they are assured of an audience; but with Language Model Agents allocating attention, the platforms would no longer be able to guarantee an audience, nor would they be necessary to do so. On this model, platforms themselves would be likely to wither away (or else to radically change). And if everyone has their own Language Model Agent advancing their own interests, then centralised control over the distribution of attention would also attenuate. 6. CHALLENGES FOR LM RECOMMENDERS LMs promise to shed some of the harmful affordances of existing recommenders. But they will obviously face their own challenges, both practical and ethical. The best way to address the practical challenges is just to build viable LM recommenders—we cannot show a priori that they can be solved, but by the same token they also cannot be shown to be a priori insurmountable. We note some obvious hurdles: first, LMs as recommenders may prove vastly less efficient than existing systems. Clearly even the fastest LMs on the most efficient hardware will be less efficient than systems optimised over almost two decades. However, algorithmic and hardware improvements suggest this comparison will become more favourable within the near future [43]. Second, of greater concern is that the candidate generation phase might inescapably require some kind of centralised authority (and so centralised power). Certainly a single system that processes viable content and does candidate generation for everyone must be more efficient than every individual having their own agent do it for them. But if we assume that the cost of computation is likely to approach zero over time, then this may not be a hard constraint, especially since less computationally intensive models can be used for a first pass at candidate generation, with more sophisticated models being used for reranking. This would of course require either (unlikely) collaboration from platforms in providing access to content that would not otherwise make it to the feed, or else reliance on alternative infrastructure (such as open protocols in the fediverse). There’s no reason to think that an approach based on open protocols and a ‘clearinghouse for content’ would need to have the same negative externalities as our current digital platforms. Language Model Agents will undoubtedly require novel infrastructure, but we can design that infrastructure with our values in mind (and there is a profound value proposition for the companies that 2 In the EU the Digital Marketplace Act has an interoperability requirement, at present just for messaging apps. It has been agreed to discuss the feasibility of extending the requirement to social media too in future revisions. In the US the proposed ACCESS Act would also require interoperability between platforms. 14 Preprint 14 do so). Relatedly, third, while existing recommenders compromise user agency, they also make few demands of users. Scaffolding their agency might well involve unreasonable expectations of user participation in the allocation of their attention. Preferences for frictionless technologies that just work without direct instruction or customisation might prove a serious obstacle. But this is, again, a user interface challenge—a heavy- handed approach that places too much burden on the user would likely inspire reactance, but we should not just assume that people are too lazy to be self- determining online. Fourth, while LMs demonstrate fluency with evaluative concepts, their ability to reliably model users’ preferences and values over time is as yet unproven [84]. Progress here may require more than just the LM, for example carefully structured generation, and perhaps an external representation that provides hierarchy to the user’s values. For example, the user model might need to represent some near absolute constraints, some much softer, some interacting goals, and so on. This is an open research question, but LMs at least constitute progress along this path, since they enable us to coherently and faithfully render moral and other practical reasons legible to computational systems. This is the previously unattainable goal of the whole field of machine ethics [97]. With these resources, successfully modelling user preferences and values should be tractable. Of course, the more accurate these user models become, the more important it would be to ensure they are tightly controlled by users themselves, given the consequent privacy risk. Fifth, LM recommenders will shift power away from those who currently have it. They will resist. And if LM recommenders work well, platforms will build their own that deliver some of the user benefits without reducing surveillance, or the centralisation of power, or threats to user agency, all of which serve their interest. Although we are set for a fight, as we have argued throughout the platforms’ victory is not a foregone conclusion. Few governments enthusiastically welcome social media platforms’ impact on our information and communication ecosystem [21]. If we had a viable alternative, regulators would clearly have the authority to ensure genuine competition in the allocation of attention, and to support LM recommenders. And more generally, with so many companies aiming to transform personal computing entirely through LM agents [80], the rise of LM recommenders might ultimately be a foregone conclusion. If LM agents become the new operating system, and the universal interface for our digital lives, recommendation will likely just fold into that broader system. We will shift from a paradigm where digital platforms are the key intermediaries controlling our digital lives, to one where LM agents play that role [57]. Of course, this in turn would open up new ethical challenges. It would be extremely important to ensure that these universal intermediaries are genuinely within our power, and are not another vector for more fine-grained control of our lives by tech companies [57]. And if we are to upend the attention economy, then we will need a new approach to funding it also. By decentralising distribution, LM agents may make compensating content creators harder in that respect. However, the existing approach with digital platforms does not work particularly well. And while microtransactions 15 Preprint 15 have yet to catch on, perhaps due to nobody wanting to spend time thinking about whether a given piece of content is worth a fraction of a cent, agents could quite easily handle such decisions on our behalf. In fact it could be liberating: your agent could have a budget of $X a month, enabling you to have a varied informational diet without being locked in to monthly contracts that you will reliably forget to cancel. In other words, shifting away from centralised distribution will have benefits and costs, but it is ultimately a solvable mechanism design problem. The security and safety of LM agents also raise ethical concerns. At present no system involving an LM is secure against prompt injection, which can hijack the model and induce it to perform specific actions [102]. If the LM is powering an agent, then prompt injection could involve taking that agent over. This could be disastrous, especially if the LM agent has built a veridical and detailed model of the user. And on the safety side, LM agents would be likely to win the trust of their users, and so would potentially be extremely dangerous vectors for manipulation and political persuasion. The fact that they are so decentralised might make them much harder to police [58]. This is a difficult challenge, but not obviously worse than the status quo, since manipulation is already a concern online, including by AI companions [45]. We might also fret that LM agents will enable a more acute version of the filter bubble worry from the early days of the web [72, 87]. We suspect, however, that this worry will remain overblown—people clearly can choose to stay within their own narrow epistemic bubble now using existing tools, but most do not [8]. And while one could design an LM recommender to serve a pathologically narrow informational diet, we obviously can design or require them to be more inclusive and societally aligned, using the same natural language means as we use to represent users’ preferences and values [12, 63]. More troubling, the recommender as proxy agent model presupposes a digital public sphere in which we are more consumers of information than communicators. Social media would be even less social than it is now, if it were a ghost town haunted only by proxy agents looking for content to take back to their principals. It would be like remaining at home while your servant heads to the town square to hear what people are saying, then comes back to relay it to you. One of social media’s attractions is that it makes our tastes public, so we can form them socially. We know roughly what interests each other, and this is useful information. We do not consume in isolation. Relying on proxy agents to navigate our communicative environments would make this more challenging. But these are less decisive objections to the LM agent model, than they are design challenges. How can we reap the benefits of LM agents for recommendation, without them making our digital lives less social? When should we leave our agents behind and venture out into the digital public sphere ourselves? We have no reason to think these questions unanswerable. Lastly, some might argue that there’s a moral case to be made against using LMs for recommendation, or indeed for anything: the most capable LMs are trained on data that was not licensed for that purpose; they are environmentally destructive; they are contributing to further concentration of ‘big tech’ power, and so on (see e.g. [9] and the ensuing literature). We won’t attempt an overall moral defence of building products 16 Preprint 16 with LMs; we note, however, that where the moral shortcomings of our existing approaches to recommendation are integral to their successful deployment, the moral shortcomings of LMs are mostly a function of how they are initially trained. These critiques, therefore, bear on whether and how one should train a frontier LM (and we of course favour respecting copyright and avoiding environmental harm), as opposed to whether one should make use of LMs that have been built. 7. CONCLUSION The prospects of the open web for enabling a brave new age of communication have been extolled before, and instead of the wealth of networks we got the poverty and power of platforms [10]. The new paradigm of personal computing enabled by LM agents may prove an equal disappointment. And in a political culture as fraught and divided as ours, perhaps any technological approach, whatever its affordances, will inevitably reinforce the same inequalities of power and resources that rive our current public sphere. But we can only start from where we are, and new tools make new things possible, even if they cannot solve all of our problems. A computing paradigm that enables more understanding and inference at the edge, without requiring pervasive surveillance, or relying on narrow behaviourism, or rendering us passive objects of mathematical systems that we cannot understand, must surely be an improvement on the status quo. Though there are many unsolved problems ahead, the path is clearly worth pursuing. 17 Preprint 17 1. 'ARC Prize - What is ARC-AGI?'. ARC Prize: https://arcprize.org/arc. 2. 'Introducing 22 system cards that explain how AI powers experiences on Facebook and Instagram'. Meta AI: https://ai.meta.com/blog/how-ai-powers-experiences-facebook- instagram-system-cards/. 3. 'On YouTube’s recommendation system'. blog.youtube: https://blog.youtube/inside- youtube/on-youtubes-recommendation-system/. 4. 'Our approach to explaining ranking'. https://transparency.meta.com/features/explaining-ranking. 5. 'Powered by AI: Instagram’s Explore recommender system'. https://ai.meta.com/blog/powered-by-ai-instagrams-explore-recommender-system/. 6. 'twitter/the-algorithm'. 2024: https://github.com/twitter/the-algorithm. 7. 'YouTube Now: Why We Focus on Watch Time'. blog.youtube: https://blog.youtube/news- and-events/youtube-now-why-we-focus-on-watch-time/. 8. Bail, C., Breaking the social media prism : how to make our platforms less polarizing. 2021, Princeton: Princeton University Press. 9. Bender, E.M., et al., On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ?, in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021, Association for Computing Machinery: Virtual Event, Canada. p. 610– 623. 10. Benkler, Y., The Wealth of Networks: How Social Production Transforms Markets and Freedom. 2006: Yale University Press. 11. Benn, C. and S. Lazar, 'What's wrong with Automated Influence'. Canadian Journal of Philosophy, 2021: 1-24. 12. Bernstein, M., et al., 'Embedding Societal Values into Social Media Algorithms'. Journal of Online Trust and Safety, 2023. 2(1). 13. Boutilier, C., M. Mladenov, and G. Tennenholtz, 'Recommender Ecosystems: A Mechanism Design Perspective on Holistic Modeling and Optimization'. Proceedings of the AAAI Conference on Artificial Intelligence, 2024. 38(20): 22575-22583. 14. Brady, W.J., et al., 'Overperception of moral outrage in online social networks inflates beliefs about intergroup hostility'. Nature Human Behaviour, 2023. 7(6): 917-927. 15. Brady, W.J., et al., 'Emotion shapes the diffusion of moralized content in social networks'. Proceedings of the National Academy of Sciences, 2017. 114(28): 7313-7318. 16. Bugliarello, E., et al. 'Mostra: A Flexible Balancing Framework to Trade-off User, Artist and Platform Objectives for Music Sequencing'. 2022. Association for Computing Machinery. 17. Burrell, J., et al., 'When users control the algorithms: values expressed in practices on twitter'. Proceedings of the ACM on human-computer interaction, 2019. 3(CSCW): 1-20. 18. Carraro, D. and D. Bridge, 'Enhancing Recommendation Diversity by Re-ranking with Large Language Models'. arXiv preprint, 2024: https://arxiv.org/abs/2401.11506. 19. Center, P.R., The Role of Alternative Social Media in the News and Information Environment. 2022. 20. Chen, H., et al., 'ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?'. arXiv preprint, 2024: https://arxiv.org/abs/arXiv.2311.16989. 21. Cohen, J. and A. Fung, 'Democracy and the Digital Public Sphere', in Digital Technology and Democratic Theory, L. Bernholz, H. Landemore, and R. Reich, Editors. 2021, The University of Chicago Press: Chicago. p. 23-61. 22. Cohen, R., T. Newton-John, and A. Slater, 'The relationship between Facebook and Instagram appearance-focused activities and body image concerns in young women'. Body image, 2017. 23: 183-187. 23. Cottier, B., 'Trends in the Dollar Training Cost of Machine Learning Systems'. Epoch AI, 2023. 18 Preprint 18 24. Cunningham, T., et al., 'What We Know About Using Non-Engagement Signals in Content Ranking'. arXiv preprint, 2024: https://arXiv.org/abs/2402.06831. 25. Dagan, G., F. Keller, and A. Lascarides, 'Dynamic planning with a llm'. arXiv preprint, 2023: https://arXiv.org/abs/2308.06391. 26. de la Cruz Paragas, F. and T.T. Lin, 'Organizing and reframing technological determinism'. New Media & Society, 2016. 18(8): 1528-1546. 27. DeVito, M.A., D. Gergle, and J. Birnholtz. '\" Algorithms ruin everything\" # RIPTwitter, Folk Theories, and Resistance to Algorithmic Change in Social Media'. in Proceedings of the 2017 CHI conference on human factors in computing systems. 2017. 28. Dwoskin, E. and E. Scott, Obama says tech companies have made democracy more vulnerable, in The Washington Post. 2022: Washington, DC. 29. Easley, D. and J. Kleinberg, Networks, crowds, and markets: reasoning about a highly connected world. 2010, New York: Cambridge University Press. 30. Eslami, M., et al. 'First I\" like\" it, then I hide it: Folk Theories of Social Feeds'. in Proceedings of the 2016 cHI conference on human factors in computing systems. 2016. 31. Friedman, L., et al., 'Leveraging Large Language Models in Conversational Recommender Systems'. arXiv preprint, 2023: https://arxiv.org/abs/2305.07961. 32. Fukuyama, F., 'Making the Internet Safe for Democracy'. Journal of Democracy, 2021. 32(2): 37-44. 33. Gao, Z., et al. 'Clova: A closed-loop visual assistant with tool usage and update'. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 34. Gillespie, T., 'Do Not Recommend? Reduction as a Form of Content Moderation'. Social Media + Society, 2022. 8(3): 1-13. 35. González-Bailón, S., et al., 'Asymmetric ideological segregation in exposure to political news on Facebook'. Science, 2023. 381(6656): 392-398. 36. Graepel, T., et al., 'Web-Scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft's Bing search engine', in Proceedings of the 27th International Conference on Machine Learning (ICML-10), J. Fürnkranz and T. Joachims, Editors. 2010, Omnipress: Haifa, Israel. p. 13-20. 37. Guess, A.M. and B.A. Lyons, 'Misinformation, Disinformation, and Online Propaganda', in Social Media and Democracy: The State of the Field, Prospects for Reform, N. Persily and J.A. Tucker, Editors. 2020, Cambridge University Press: Cambridge. p. 10-33. 38. Guess, A.M., et al., 'How do social media feed algorithms affect attitudes and behavior in an election campaign?'. Science, 2023. 381(6656): 398-404. 39. Gur, I., et al., 'A real-world webagent with planning, long context understanding, and program synthesis'. The Twelfth International Conference on Learning Representations, 2024: https://openreview.net/forum?id=9JQtrumvg8. 40. Haidt, J. and C. Bail, 'Social Media and Political Dysfunction: A Collaborative Review'. Unpublished MS., 2023. 41. Hao, K., How Facebook got addicted to spreading misinformation, in MIT Technology Review. 2021. 42. Hari, J., Stolen focus. 2021, Crown,: New York. p. 1 online resource. 43. Ho, A., et al., 'Algorithmic progress in language models'. arXiv preprint, 2024: https://arxiv.org/abs/2403.05812. 44. Hoel, E., Here lies the internet, murdered by generative AI, in The Intrinsic Perspective. 2024, Substack. 45. Irvine, R., et al., 'Rewarding Chatbots for Real-World Engagement with Millions of Users'. arXiv preprint, 2023: https://arxiv.org/abs/2303.06135. 46. Jasser Jasser, D.E. and I. Garibay, 'Flooding the Zone: a censorship and disinformation strategy that needs attention'. 2021. 47. Jia, C., et al., 'Embedding democratic values into social media AIs via societal objective 19 Preprint 19 functions'. arXiv preprint, 2023: https://arxiv.org/abs/2307.13912. 48. Jin, T. and X. Dong. Recalign. 2023; Available from: https://github.com/recalign/RecAlign. 49. Jin, Y., B.D.L.R.P. Cardoso, and K. Verbert. 'How do different levels of user control affect cognitive load and acceptance of recommendations?'. in IntRS@ RecSys. 2017. 50. Kahneman, D., Thinking, fast and slow. 1st pbk. ed. 2013, New York: Farrar, Straus and Giroux. 51. Kambhampati, S., et al., 'LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks'. arXiv preprint, 2024: https://arxiv.org/abs/2402.01817.pdf. 52. Keller, D., 'Amplification and its Discontents'. Knight First Amendment Institute at Columbia University Occasional Papers, 2021: 1-47. 53. Keller, D., 'The Future of Platform Power: Making Middleware Work'. Journal of Democracy, 2021. 32(3): 168-172. 54. Kwon, J., J. Tenenbaum, and S. Levine. 'Neuro-Symbolic Models of Human Moral Judgment: LLMs as Automatic Feature Extractors'. in Proceedings of the 40th International Conference on Machine Learning. 2023. 55. Lazar, S., 'Automatic Authorities: Power and AI', in Collaborative Intelligence: How Humans and AI are Transforming our World, A. Sethumadhavan and M. Lane, Editors. 2024, MIT Press: Cambridge, MA. p. https://arxiv.org/abs/2404.05990. 56. Lazar, S., 'Communicative Justice and the Distribution of Attention'. Knight First Amendment Institute, 2023: http://knightcolumbia.tierradev.com/content/communicative- justice-and-the-distribution-of-attention. 57. Lazar, S., Connected by Code: Algorithmic Intermediaries and Political Philosophy. Forthcoming, Oxford: Oxford University Press. 58. Lazar, S., 'Frontier AI Ethics'. Aeon, 2024: https://arxiv.org/abs/2404.06750. 59. Lazar, S., 'Legitimacy, Authority, and Democratic Duties of Explanation'. Oxford Studies in Political Philosophy, 2024. 60. Lewis, M. and M. Mitchell, 'Using counterfactual tasks to evaluate the generality of analogical reasoning in large language models'. arXiv preprint, 2024: https://arxiv.org/abs/2402.08955. 61. Li, H., et al., 'A Survey on Retrieval-Augmented Text Generation'. ArXiv preprint, 2022. abs/2202.01110: https://arxiv.org/abs/2202.01110. 62. Lin, J., et al., 'How can recommender systems benefit from large language models: A survey'. arXiv preprint, 2023: https://arXiv.org/abs/2306.05817. 63. Liu, R., et al., 'Training socially aligned language models on simulated social interactions'. The Twelfth International Conference on Learning Representations, 2024: https://openreview.net/forum?id=NddKiWtdUm. 64. Ma, K., et al., 'Laser: Llm agent with state-space exploration for web navigation'. arXiv preprint, 2023: https://arxiv.org/abs/2309.08172. 65. Milli, S., L. Belli, and M. Hardt, From Optimizing Engagement to Measuring Value, in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021, Association for Computing Machinery: Virtual Event, Canada. p. 714–722. 66. Milli, S., et al., 'Engagement, user satisfaction, and the amplification of divisive content on social media'. arXiv preprint, 2023: https://arxiv.org/abs/2305.16941. 67. Moore, J. 'Language Models Understand Us, Poorly'. 2022. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. 68. Munger, K. and J. Phillips, 'Right-Wing YouTube: A Supply and Demand Perspective'. The International Journal of Press/Politics, 2020. 27(1): 186-219. 69. Narayanan, A., 'Understanding Social Media Recommendation Algorithms'. Knight First Amendment Institute, 2023: 1-49. 70. Nyhan, B., et al., 'Like-minded sources on Facebook are prevalent but not polarizing'. Nature, 2023. 620(7972): 137-144. 20 Preprint 20 71. Ovadya, A. and L. Thorburn, 'Bridging Systems: Open Problems for Countering Destructive Divisiveness Across Ranking, Recommenders and Governance'. Knight First Amendment Institute, 2023. 72. Pariser, E., The filter bubble : what the Internet is hiding from you. 2011, London: Viking. 73. Radlinski, F., et al. 'On natural language user profiles for transparent and scrutable recommendation'. in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2022. 74. Ramos, J., et al., 'Natural Language User Profiles for Transparent and Scrutable Recommendations'. arXiv preprint, 2024: https://arxiv.org/abs/2402.05810. 75. Saad, N., Jon Stewart calls ‘overreaction’ to Spotify’s Joe Rogan debacle ‘a mistake’, in LA Times. 2022: Los Angeles. 76. Schick, T., et al. 'Toolformer: Language Models Can Teach Themselves to Use Tools'. in Advances in Neural Information Processing Systems. 2023. Curran Associates, Inc. 77. Schuster, N. and S. Lazar, 'Attention, moral skill, and algorithmic recommendation'. Philosophical Studies, 2024: 1-26. 78. Sen, A., 'Utilitarianism and Welfarism'. The Journal of Philosophy, 1979. 76(9): 463-489. 79. Settle, J.E., Frenemies: How Social Media Polarizes America. 2018: Cambridge University Press. 80. Shavit, Y., et al., Practices for governing agentic AI systems. 2023, OpenAI. 81. Siegel, A.A., 'Online Hate Speech', in Social Media and Democracy: The State of the Field, Prospects for Reform, N. Persily and J.A. Tucker, Editors. 2020, Cambridge University Press: Cambridge. p. 56-88. 82. Smart, M.A., D. Sood, and K. Vaccaro, 'Understanding Risks of Privacy Theater with Differential Privacy'. Proc. ACM Hum.-Comput. Interact., 2022. 6(CSCW2): Article 342. 83. Smith, B., How TikTok Reads Your Mind, in The New York Times. 2021. 84. Sorensen, T., et al., 'A Roadmap to Pluralistic Alignment'. arXiv preprint, 2024: https://arxiv.org/abs/2402.05070. 85. Standage, T., Writing on the wall: social media - the first 2,000 years. 2013, London: Bloomsbury. 86. Sun, Y. and Y. Zhang. 'Conversational Recommender System'. in Annual ACM Conference on Research and Development in Information Retrieval. 2018. ACM. 87. Sunstein, C.R., Republic.com. 2001, Princeton, N.J. ; Oxford: Princeton University Press. 88. Susser, D., B. Roessler, and H. Nissenbaum, 'Online manipulation: Hidden influences in a digital world'. Georgetown Law Technology Review, 2019. 4: 1-45. 89. Susskind, J., The Digital Republic: On freedom and democracy in the 21st century. 2022: Bloomsbury Publishing. 90. Taylor, L., L. Floridi, and B. van der Sloot, eds. Group Privacy: New Challenges of Data Technologies. Philosophical Studies Series, ed. L. Floridi and M. Taddeo. 2017, Springer. 91. Thorburn, L., P. Bengani, and J. Stray, How Platform Recommenders Work, in Medium. 2022. 92. Tisné, M., Collective data rights can stop big tech from obliterating privacy, in MIT Technology Review. 2021, MIT Press: Cambridge, Mass. 93. Turpin, M., et al. 'Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting'. in Advances in Neural Information Processing Systems. 2023. Curran Associates, Inc. 94. Valmeekam, K., et al. 'On the Planning Abilities of Large Language Models - A Critical Investigation'. in Advances in Neural Information Processing Systems. 2023. Curran Associates, Inc. 95. Vats, A., et al., 'Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review'. arXiv preprint arXiv:2402.18590, 2024. 96. Véliz, C., Privacy is power : Why and how you should take back control of your data. 2021, London: Penguin Books. 97. Wallach, W., C. Allen, and Oxford University Press., Moral machines teaching robots right 21 Preprint 21 from wrong. 2009, Oxford University Press,: Oxford. p. 1 online resource (xi, 275 p. 98. Williams, J., Stand out of our light: freedom and resistance in the attention economy. 2018, Cambridge: Cambridge University Press. 99. Winner, L., 'Do Artifacts Have Politics?'. Daedalus, 1980. 109(1): 121-136. 100. Woolley, S.C., 'Bots and Computational Propaganda: Automation for Communication and Control', in Social Media and Democracy: The State of the Field, Prospects for Reform, N. Persily and J.A. Tucker, Editors. 2020, Cambridge University Press: Cambridge. p. 89-110. 101. Wu, T., 'Is the First Amendment Obsolete?'. Michigan law review, 2018(117.3): 547. 102. Zhan, Q., et al., 'InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents'. arXiv preprint, 2024: https://arxiv.org/abs/2403.02691. 103. Zhang, A.X., et al., 'Form-From: A Design Space of Social Media Systems'. arXiv preprint, 2024: https://arxiv.org/abs/2402.05388. 104. Zhang, Z. and A. Zhang, 'You only look at screens: Multimodal chain-of-action agents'. arXiv preprint, 2023. 105. Zheng, B., et al., 'GPT-4V(ision) is a Generalist Web Agent, if Grounded'. arXiv preprint, 2024: https://arxiv.org/abs/2401.01614. 106. Zhou, J., Y. Dai, and T. Joachims, 'Language-Based User Profiles for Recommendation'. arXiv preprint, 2024: https://arxiv.org/abs/2402.15623. 107. Zuboff, S., The Age of Surveillance Capitalism. 2019, New York: Public Affairs.","libVersion":"0.3.2","langs":""}
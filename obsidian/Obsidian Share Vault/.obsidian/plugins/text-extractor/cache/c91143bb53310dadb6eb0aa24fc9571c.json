{"path":"lit/lit_sources/Xu24conformPredMultiDimTS.pdf","text":"Conformal prediction for multi-dimensional time series by ellipsoidal sets Chen Xu∗1, Hanyang Jiang∗1, and Yao Xie†1 1H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology. May 24, 2024 Abstract Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called MultiDimSPCI that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that MultiDimSPCI maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines. 1 Introduction Conformal prediction (CP) has been a popular distribution-free technique to quantify uncertainty in modern machine learning (Volkhonskiy et al., 2017). In building predictive algorithms, CP can enhance trained machine learning estimators to output not just point estimates but also provide uncertainty sets that contain the unobserved ground truth with user-specified high probability. As a result, CP has been applied successfully to many applications, such as anomaly detection (Xu & Xie, 2021a), classification (Angelopoulos et al., 2021; Xu & Xie, 2022), regression (Barber et al., 2021), and so on. In a nutshell, CP methods work as wrappers that take in three components: a black-box predictive model f , an input feature X, and a potential output Y . Then, it designs a so-called “non-conformity” score that measures how non-conforming the potential output is. ∗Equal contribution †Correspondence: yao.xie@isye.gatech.edu 1arXiv:2403.03850v2 [stat.ML] 23 May 2024 Table 1: A 2 × 2 taxonomy of conformal prediction approaches (not an exhaustive list), categorized based on the dimension of the response variable Y (rows) and data assumptions (columns). Exchangeable Non-exchangeable Univariate Y (Volkhonskiy et al., 2017) (Barber et al., 2021; Kim et al., 2020) (Zaffran et al., 2022; Xu & Xie, 2023a) (Xu & Xie, 2023b; Barber et al., 2023) Multivariate Y (Messoudi et al., 2021; Diquigiovanni et al., 2022) (Johnstone & Ndiaye, 2022; Feldman et al., 2023) Ours (Stankeviˇci¯ut˙e et al., 2021; Sun & Yu, 2024) Naturally, the uncertainty set conditioning on the input feature and the predictor model would contain all potential outputs that are conforming to the past. Most successful applications of CP have considered Y as an univariate variable. In the regression setting (Kim et al., 2020), CP methods thus output prediction intervals while in the classification setting (Romano et al., 2020), these methods produce prediction sets. With mild assumptions, such as assuming that data (X, Y ) are exchangeable, these one-dimensional uncertainty sets can theoretically guarantee high coverage probability. Recent works have also extended such guarantees to non-exchangeable observations and either quantify the coverage gap in finite training samples (Barber et al., 2023) or show asymptotic coverage convergence (Xu & Xie, 2023b). Despite the success of CP on scalar outputs Y , effective use of CP on multi-dimensional outputs is considerably less studied, especially when data are non-exchangeable as in the case of multivariate time-series forecasting. Moreover, there can be complex dependence between the multiple dimensions of the time series, making the problem more interesting and important. Specifically, the goal is not just to provide a prediction interval for each dimension of Y but to produce an uncertainty region that captures the correlation within Y and jointly contains all coordinates of Y . While uncertainty quantification methods for this problem have existed outside CP, as in vector auto-regressive models (Salinas et al., 2020), these approaches often have strong modeling or data assumption and lack rigorous theoretical justifications. On the other hand, various multi-dimensional CP methods have been proposed. Yet, they are either repeated use of one-dimensional CP methods (Stankeviˇci¯ut˙e et al., 2021) or fail to work beyond exchangeability (Messoudi et al., 2022). We highlight the differences against copula-based CP methods, which have been developed for multi-dimensional forecasting. The initial approach developed in (Messoudi et al., 2021) assumed exchangeability, which is unsuitable for time series. The recent development by (Sun & Yu, 2024) proposes copula CP for multi-step time-series forecasting. However, their theory assumed that each data sample of an entire time series is drawn i.i.d. from an unknown distribution, hence ignoring temporal dependency. We further introduce copula and its use in CP in Section 3.2, with additional comparison against baselines in Section 5.2. Hence, the central focus of this work is to advance CP in the context of multivariate time-series 2 forecasting. Specifically, we build ellipsoidal prediction sets whose size is adaptively and efficiently calibrated during test time. We also provide rigorous theoretical guarantees and extensive experiments to showcase the utility of the proposed method. In summary, our contributions are • We propose a sequential conformal prediction method that builds ellipsoidal prediction regions for multivariate time series. In particular, sizes of the ellipsoids are sequentially re-estimated during test time to ensure adaptiveness and accuracy. • We provide finite-sample high-probability bounds for the coverage gap of constructed prediction regions, which do not depend on the exchangeability of the observations. • We empirically verify on multivariate time-series (up to dimension 20) that MultiDimSPCI can yield smaller prediction regions than baseline CP and non-CP methods without losing coverage. For clarity, a taxonomy of existing CP methods is in Table 1 to highlight our role within the CP literature. In this paper, we assume that the noise sequence in the data-generating process (see Eq (12)) is stationary and strongly mixing, where the original data sequence does not need to be exchangeable. Meanwhile, we impose some standard assumptions on the tail behavior of the distribution to derive a non-asymptotic bound on the conditional guarantee. We highlight that our guarantees differ from existing multi-dimensional CP works that assume exchangeability (Messoudi et al., 2021, 2022) or i.i.d. data (Sun & Yu, 2024). We adopt the standard notations. For a random process {Xn}n, Xn = op(an) means that |Xn|/an p → 0. For function f (n) and g(n), f (n) = ˜O(g(n)) means that f (n) = O(g(n) log(g(n))k) for some k. Besides, for event A and B, the notation A∣ ∣B means A under the condition B. For vector u, v ∈ Rp, u ⊗ v is the outer product of u and v. 1.1 Literature review CP with exchangeable data. The field of CP started in (Vovk et al., 2005) and has been widely used for uncertainty quantification due to its flexibility and robustness. On a high level, we define a “non-conformity” score and evaluate such scores on a hold-out calibration set. Then, uncertainty sets include all potential observations whose non-conformity scores are within the empirical quantiles of the calibration scores. Assuming nothing but that input data are exchangeable, CP methods have been successfully developed in different applications (Wisniewski et al., 2020; Xu & Xie, 2021a, 2022), in addition to the active research on proper designs of non-conformity scores (Angelopoulos et al., 2021; Huang et al., 2023; Gui et al., 2023). Comprehensive reviews of conformal prediction can be found in (Fontana et al., 2023; Angelopoulos & Bates, 2023). CP for one-dimensional time series. Two general trends of extending beyond exchangeability work well for univariate Y . The first considers adaptively adjusting the significance level α during 3 test time to account for mis-coverage. Such works include (Gibbs & Candes, 2021; Zaffran et al., 2022; Lin et al., 2022). The recent work (Angelopoulos et al., 2024) extends such framework through the lens of control theory to prospectively model non-conformity scores in online settings. The second considers weighing the past non-conformity score non-equally so that scores more similar to the present are given higher weights. Such works include (Tibshirani et al., 2019; Xu & Xie, 2021b, 2023b; Xu et al., 2023; Nair & Janson, 2023), some of which have successfully been applied to univariate time series. The recent work (Barber et al., 2023) also suggests that re-weighting can be a general scheme to account for non-exchangeability. Our MultiDimSPCI is similar to the second line of approaches but works in high dimensions. CP for multi-dimensional data. Numerous works have been on this topic. (Stankeviˇci¯ut˙e et al., 2021) builds coordinate-wise prediction intervals for multi-horizon time-series prediction using Bonferroni correction of the significance level. For multivariate functional data, a similar idea of building prediction bands was studied in (Diquigiovanni et al., 2022), where this idea was further developed for time series (Diquigiovanni et al., 2021). In addition, (Messoudi et al., 2021) develops a principled way to determine the length of coordinate-wise intervals by using copula, resulting in hyper-rectangular prediction regions. The extension of copula for time-series forecasting was later studied in (Sun & Yu, 2024). However, it is important to note that the use of hyper-rectangles can be sub-optimal in many cases, even in the two-dimensional instances when the true conditional distribution Y |X is N (f (X), Σ) with a non-zero off-diagonal entry in Σ. To overcome this, (Messoudi et al., 2022) considers ellipsoidal uncertainty sets that rely on data exchangeability. A more exact quantification of the uncertainty set is studied in (Johnstone & Ndiaye, 2022), which, however, strongly depends on the underlying predictive model of Y . As a result, extending CP for multivariate time-series forecasting beyond using hyper-rectangles still needs to be explored. Uncertainty quantification beyond CP. The task of building uncertainty set for unobserved response has been widely studied beyond CP. There has been a long history of using copula to capture the joint distribution of multivariate response by relating the joint cumulative distribution function (CDF) with each marginal CDF (Sklar, 1959; Elidan, 2013). Meanwhile, (Dobriban & Lin, 2023) uses conditional pivots to construct joint coverage regions for parameters and observations, extending beyond CP. However, its utility beyond exchangeable data remains unclear. On the other hand, there has been extensive development in the probabilistic forecasting literature, popular examples of which include the DeepAR (Salinas et al., 2020) and Temporal Fusion Transformer (Lim et al., 2021). Such approaches optimize (variants of) the pinball loss to estimate quantiles of multivariate responses but typically require extensive hyper-parameter tuning and return hyper- rectangular uncertainty sets. We will show experimentally that their performances are often worse than their CP counterparts. Lastly, (Feldman et al., 2023) uses CP in the representation space 4 learnt by a deep generative model, allowing general prediction sets for multivariate data. Coverage guarantee for exchangeable data is proved, and extension to time series remains unexplored. 2 Problem setup We consider a multi-dimensional time-series regression setup: for time index t = 1, 2, . . ., assume observations Zt = (Xt, Yt) are sequentially revealed, where Yt ∈ Rp are p-dimensional vector variables and Xt ∈ Rd are d-dimensional features. The features Xt may be the history of Yt or contain other variables that help predict Yt. In particular, we allow arbitrarily unknown correlation among the observations Zt. Let the first T samples {Zt}T t=1 be the training data. Our goal is to sequentially construct prediction regions ̂Ct−1(Xt, α) starting from t = T + 1, which depends on past observations, the current feature Xt, and a user-specified significance level α ∈ [0, 1]. In particular, we desire the prediction regions to contain the true observations Yt with a probability at least 1 − α. Mathematically, there are two types of coverage guarantees to be satisfied by ̂Ct−1(Xt, α). The first is the weaker marginal coverage: P(Yt ∈ ̂Ct−1(Xt, α)) ≥ 1 − α, ∀t, (1) while the second is the stronger conditional coverage: P(Yt ∈ ̂Ct−1(Xt, α)|Xt) ≥ 1 − α, ∀t. (2) If ̂Ct−1(Xt, α) satisfies (1) or (2), it is called marginally or conditionally valid, respectively. When ̂Ct−1(Xt, α) satisfies the coverage guarantees, we further construct regions that are as small as possible to quantify uncertainty precisely. 3 Method In this section, we first propose the ellipsoidal uncertainty set that effectively quantifies multi- dimensional prediction. We then discuss several benefits of the proposed approach against alternatives. We finally suggest alternative forms of the uncertainty set beyond using ellipsoids. 3.1 Ellipsoidal uncertainty set We build the prediction regions in the shape of ellipsoids and calibrate the radius of ellipsoids using conformal prediction for univariate time series. Recall that we have access to T training data Zt = (Xt, Yt) for t = 1, . . . , T . Assume we have been given an algorithm ˆf , trained on a separate 5 set I to perform point prediction for Y . Meanwhile, we collect prediction residuals ˆεt ∈ Rp ˆεt = Yt − ˆf (Xt), t = 1, . . . , T. This approach is similar to the split conformal prediction (Volkhonskiy et al., 2017) or leave-one-out techniques (Barber et al., 2021; Xu & Xie, 2023a). To define the ellipsoidal uncertainty set, we first denote the covariance estimator over the prediction residuals as ̂Σ = 1 T − 1 T∑ t=1(ˆεt − ¯ε)(ˆεt − ¯ε)T , (3) where ¯ε = 1 T ∑T t=1 ˆεt is the sample mean vector over the residuals. As the definition of an ellipsoid will rely on the inverse of ̂Σ in (3), which may not be invertible, we consider a low-rank approximation of ̂Σ as follows. Let the singular value decomposition of ̂Σ be ̂Σ = U SV T , where S = diag(λ1, . . . , λp) is the diagonal matrix of singular values satisfying λ1 ≥ . . . ≥ λp ≥ 0, and U and V satisfy U U T = V V T = Ip. Given a small positive threshold ρ > 0, the low-rank approximation ̂Σρ is ̂Σρ = UρSρV T ρ , (4) where Sρ = diag(λ1, . . . , λk) for which λk ≥ ρ, and Uρ and V T ρ contain the first k columns and rows of U and V T , respectively. The pseudo-inverse ̂Σ−1 ρ is thus written as ̂Σ −1 ρ = VρS−1 ρ U T ρ , (5) where S−1 ρ = diag(1/λ1, . . . , 1/λk). Using ̂Σ−1 ρ , which is always well-defined, an ellipsoid with radius r can thus be written as B(r, ¯ε, ̂Σρ) = {x ∈ Rp : (x − ¯ε)T ̂Σ−1 ρ (x − ¯ε) ≤ r}. We then find an appropriate radius r using time-series conformal prediction methods. First, given a new residual ˆε = Y − ˆf (X) and the pseudo-inverse ̂Σ−1 ρ in (5), we define the scalar non-conformity score ˆe(Y ) as ˆe(Y ) = (ˆε − ¯ε) T ̂Σ−1 ρ (ˆε − ¯ε) ∈ R. (6) We then compute the non-conformity scores on the training set (Xt, Yt) for t = 1, . . . , T to obtain the set ET = {ˆe(Yt)}T t=1. 6 Note that non-conformity scores in ET can be sequentially dependent due to the inherent depen- dency among the original data (Xt, Yt). We take this into account by using SPCI (Xu & Xie, 2023b), a sequential conformal inference method for univariate time series. Specifically, rather than directly taking the empirical quantile over ET , we fit a quantile regression estimator ̂Qt on ET , where ̂Qt(α) aims to predict the α-quantile of the unseen non-conformity score. There is no specific restriction on the quantile regression method used here. For example, SPCI(Xu & Xie, 2023b) uses the quantile random forest. We first define the set difference of two sets A and B as A \\ B = {x : x ∈ A and x /∈ B}. Thus, the prediction set ̂Ct−1(Xt, α) ⊂ Rp for a given confidence level α takes the form ̂Ct−1(Xt, α) = {Y : ̂Qt( ˆβ) ≤ ˆe(Y ) ≤ ̂Qt(1 − α + ˆβ)} (7) = ˆf (Xt) + B( √ ̂Qt(1 − α + ˆβ), ¯ε, ̂Σρ) \\ B( √ ̂Qt( ˆβ), ¯ε, ̂Σρ) ˆβ = arg min β∈[0,α] V (̂Σρ, ̂Qt(1 − α + β)) − V (̂Σρ, ̂Qt(β)). (8) In (7), the prediction region contains all Y such that their non-conformity scores ˆe(Y ) are within the respective quantiles of the ellipsoid, which centers at the prediction ˆf (Xt) as shown in the second line of Eq. 7. In (8), V (̂Σρ, r) denotes the volume of the ellipsoid with radius r, and we find ˆβ empirically as the tightest significance level at which the volume of the prediction region is smallest. Note that optimizing β further allows us to consider asymmetry in the distribution of non-conformity scores. When the optimal ˆβ is zero, it reduces to an ellipsoid as follows, which is also shown in Figure 1(c). ̂Ct−1(Xt, α) = {Y : ˆe(Y ) ≤ ̂Qt(1 − α)} = ˆf (Xt) + B (√ ̂Qt(1 − α), ¯ε, ̂Σρ ) We propose MultiDimSPCI in Algorithm 1 as a multi-dimensional generalization of the original SPCI (Xu & Xie, 2023b) method. The main benefits lie in the extension to quantify uncertainty in multi-dimensional prediction. The method we propose is simple and uses an ellipsoid uncertainty set. However, we will show later that this method can achieve conditional coverage. Besides, even though our method only uses the information of the first two moments, it outperforms the copula method, which is the state of the art. Figure 1 illustrates the benefit of MultiDimSPCI over existing methods in yielding smaller uncertainty sets for multi-dimensional UQ time series. 3.2 Comparison with copula We briefly introduce copula and explain how copula has been utilized in multivariate conformal prediction. We then highlight the key differences of the copula-based CP method with our 7 (a) (b) (c) Figure 1: Comparison of multivariate CP method on real two-dimensional wind data (see Section 5.2). Left (a): Empirical copula (Messoudi et al., 2021) which constructs coordinate-wise prediction intervals. Middle (b): Spherical confidence set introduced in (Sun & Yu, 2024). Right (c): our proposed ellipsoidal confidence set via MultiDimSPCI. While all methods yield coverage at least above the target 95% on test data, our method yields the smallest average size. Algorithm 1 Multi-dimensional SPCI (MultiDimSPCI) Require: Training data {(Xt, Yt)}T t=1, prediction algorithm A, significance level α, quantile regression algorithm Q, positive threshold ρ > 0. Ensure: Prediction intervals ̂Ct−1(Xt, α), t > T 1: Obtain ˆf and residuals {ˆεt}T t=1 ⊂ Rp (computed on the holdout set) with A and {(Xt, Yt)}T t=1 2: Compute non-conformity scores ET from {ˆεt}T t=1 and ̂Σρ using (6) 3: for t > T do 4: Use quantile regression to obtain ̂Qt ← Q(ET ) 5: Obtain uncertainty set ̂Ct−1(Xt, α) as in (7) 6: Obtain new residual ˆεt 7: Update residual set {ˆεt}T t=1 by adding ˆεt and removing the oldest one and update ET 8: end for MultiDimSPCI. Let X = (X1, . . . , Xp) be a generic p-dimensional continuous random vector with the joint CDF F and marginal CDFs Fj of Xj for j = 1, . . . , p. We remark that in this subsection, for notation convenience, the subscript j in Xj denotes the j-th component of X rather than the j-th feature vector of the original time series (i.e., Xj in the sequence {(Xt, Yt)}). Define Uj := Fj(Xj), where for u ∈ [0, 1], P(Uj ≤ u) = P(Xj ≤ F −1 j (u)) = Fj(F −1 j (u)) = u. Hence, Uj ∼ Unif[0, 1] is a uniform random variable on [0, 1]. Now, the joint CDF of (U1, . . . , Up) is the copula C of 8 (X1, . . . , Xp): C(u1, . . . , up) = P(U1 ≤ u1, . . . , Up ≤ up) (9) = P(X1 ≤ F −1 1 (u1), . . . , Xp ≤ F −1 p (up)) = F (F −1 1 (u1), . . . , F −1 p (up)). Hence, the copula C links p marginal CDFs {Fj} to the joint CDF F . For instance, consider bivariate Gaussian copula as an example, where we can explicitly write down the copula C. Let (X1, X2) ∼ N (0, Σ) with Σ11 = Σ22 = 1 and Σ12 = Σ21 = κ for κ ∈ [−1, 1]. Then, C(u1, u2) = P(U1 ≤ u1, U2 ≤ u2) = P(X1 ≤ Φ −1(u1), X2 ≤ Φ−1(u2)) = Φ2(Φ −1(u1), Φ −1(u2); κ), (10) where Φ is the CDF of N (0, 1) and Φ2(·; κ) is the joint CDF of N (0, Σ). Note that the bivariate Gaussian copula is parametric, assuming the marginal and joint distributions follow Gaussian distributions. In conformal prediction, copula has been used to calibrate the coordinate-wise quantile of prediction residuals. Let |ˆεtj| be the j-th coordinate of the t-th prediction residual in absolute value, and let Ftj be its marginal distribution. Then, past works (Messoudi et al., 2021) fit a copula Ct to the p-dimensional random vector (|ˆεt1|, . . . , |ˆεtp|). Specifically, they find (ut1, . . . , utp) ∈ [0, 1]p so that P(|ˆεt1| ≤ F −1 t1 (ut1), . . . , |ˆεtp| ≤ F −1 tp (utp)) = Ct(ut1, . . . , utp) = 1 − α, where α is a pre-specified significance level (e.g., α = 0.05). In practice, Ftj is unknown so it is replaced by ˆFtj, the empirical distribution defined using past residuals, and the values (ut1, . . . , utp) are found under special assumptions (e.g., ut1 = . . . = utp (Messoudi et al., 2021)) or searched via stochastic gradient descent (Sun & Yu, 2024). We remark two main differences between copula conformal prediction and our proposed MultiDimSPCI. First, the use of copula CP requires searching for multi-dimensional vectors ut = (ut1, . . . , utp) at each t, whose efficiency and accuracy also highly depends on the choice of copula Ct. How to design copula and search for the best ut remains unclear. In contrast, our MultiDimSPCI requires much less design effort, as it only uses an estimation of the covariance matrix of residuals {ˆεt}. Second, note that copula CP returns hyper-rectangular prediction sets, as the method constructs one prediction interval at each p coordinates. Such hyper-rectangular sets can be too large compared to ellipsoidal sets, as we experimentally find ours are significantly 9 smaller without affecting test coverage (see Section 5.2). 3.3 Benefits of the proposed approach We further discuss the benefits of MultiDimSPCI against other approaches. Against coordinate-wise use of SPCI (Xu & Xie, 2023b): Rather than building ellipsoidal un- certainty sets, a naive but perhaps more intuitive approach is to apply SPCI p times, once per dimension of Y ∈ Rp. The resulting uncertainty sets are hyper-rectangles, which can be unneces- sarily large in many cases. In addition, the significance values for SPCI at different dimensions need to be adjusted appropriately to achieve valid coverage of Y . Computationally, such use of SPCI is also more expensive than MultiDimSPCI because (p − 1)T1 additional quantile regression models are fitted (T1 is the length of the test set). Against copula-based CP methods (Messoudi et al., 2021; Sun & Yu, 2024): Besides the limitation above of returning hyper-rectangular uncertainty sets, these copula-based methods fail to account for the sequential dependency of non-conformity scores when taking the empirical quantile over scores. In contrast, the proposed MultiDimSPCI explicitly takes dependency into account by adaptively re-estimating the quantile of non-conformity scores. Against probabilistic forecasting methods (Salinas et al., 2020; Lim et al., 2021): There are two main benefits of MultiDimSPCI. First, our proposed method is compatible with any user- specified prediction model ˆf of Y . In contrast, such probabilistic forecasting methods often require specifically designed deep neural networks to predict the quantiles of Y directly. Second, we can provide coverage guarantees for the proposed method, whereas those methods often lack sound justifications. 3.4 Improvements using local ellipsoids In practice, constructing the scalar non-conformity scores in (6) based on a global covariance matrix in (3) fails to capture local variation in data. We can improve our MultiDimSPCI through using local ellipsoids proposed in (Messoudi et al., 2022). Specifically, given a test data feature Xt for t > T , we first consider its k nearest neighbors among previous T samples {Xt−1, Xt−2, . . . , Xt−T }. Let the index set of neighbors be Nt with |Nt| = k. Then, we denote ̂Covt as the sample covariance estimator using {ˆεt}t∈Nt (see Eq. (3) for the definition using {ˆεt}T t=1). As a result, given a parameter λ ∈ [0, 1], the local covariance estimator at time t is written as the weighted average ̂Σt = λ ̂Covt + (1 − λ)̂Σ, (11) 10 where ̂Σ is the global empirical covariance matrix in (3). We recommend setting k = 0.1T and λ = 0.95 to capture local variations effectively. Lastly, as ̂Σt in (11) may not be invertible, we use its low-rank approximation in (4) and the corresponding pseudo-inverse in (5), which would be used to compute the non-conformity score (6) at time t. We empirically find that compared to using (3), the use of (11) can lead to up to 25% reduction in the average size of prediction sets. 4 Theoretical Analysis In this section, we will present theoretical results for bounding the conditional coverage of our method. The result is based on the case where the sample covariance matrix is invertible. We first recall and define the notations and then give out the assumptions required, which are general and identifiable. After that, we will present our coverage guarantees when using the empirical quantile function as the quantile regression predictor. The norm ∥ · ∥ used in the paper is the spectral norm (2-norm). The proof details will be in Appendix B. The main idea of the proof consists of two parts. The first part is the convergence of the empirical CDF to the true CDF of the residual. The second part is to control the estimation error of the sample covariance matrix. We assume that Yt ∈ Rp is generated from a true model with unknown additive noise: Yt = f (Xt) + εt, t = 1, 2, . . . , (12) where f is an unknown function and εt represents the process noise, whose marginal distribution is not necessarily Gaussian, the process noise may have temporal dependence. For the simplicity of notation, we assume ¯ε = 0 here so that the non-conformity score simplifies to ˆεT t ̂Σ−1 ˆεt. Besides, without loss of generality, we can assume E[ε] = 0. Otherwise, we can subtract the mean from the noises and add to the function f . We define ˆεt = yt − ˆf (xt) as the vector prediction residual, and the scalar non-conformity score ˆet = ˆεT t ̂Σ−1 ˆεt ∈ R. Moreover, et = εT t Σ−1εt, ∆t = ˆεt − εt. Here εt ∈ Rp is the true noise in model (12) and Σ ∈ Rp×p is the true covariance matrix of ε. Besides, we define the empirical CDF ̂FT +1(x) = 1 T T∑ t=1 1{ˆet ≤ x}, ̃FT +1(x) = 1 T T∑ t=1 1{et ≤ x}. (13) 11 We also use Fe(x) = P{e ≤ x}, to represent the CDF of the nonconformity score. Since we consider the case where the marginal distributions of et are identical, their CDF is the same, and we can define it as Fe(x). In our method, we use the empirical distribution of non-conformity score ˆe to approximate the distribution of e. A new observation YT +1 being covered by the conformal interval with given coverage is equivalent to ˆeT +1 falling in a given quantile in empirical distribution ̂FT +1. From the property of CDF, we know that Fe(eT +1) ∼ Unif[0, 1]. If we can show that ̂FT +1 approximates Fe well, then it will follow that ̂FT +1 approximately covers a region of 1 − α probability. Comparing ˆe and e, we see that ˆet − et = ˆεT t ̂Σ−1 ˆεt − εT t Σ−1εt, (14) where ̂Σ is estimated from {ˆεt}T t=1. We would need ˆεt to be close to εt. Otherwise, this approximation would not hold. Assumption 4.1 (i.i.d. and Lipschitz). Assume {εt}T +1 t=1 are independent and identically dis- tributed (i.i.d.). Meanwhile, Fe(x) (the CDF of the true non-conformity score) is assumed to be Lipschitz continuous with constant LT +1 > 0. Remark 4.2. We first assume that the error process {εt} T +1 t=1 is i.i.d. In fact, this assumption is not necessary, and we will extend this assumption to cases beyond exchangeability. The result for stationary and strong mixing sequences will be presented in Corollary 4.18. Assumption 4.3 (Estimation quality). There exists a sequence {δT }T ≥1 such that 1 T T∑ t=1 ∥∆t∥2 ≤ δ2 T , ∥∆T +1∥ ≤ δT . (15) Remark 4.4. The assumption requires that the square sum of the prediction error be bounded by δ2 T . For many estimators, there exists a sequence {δT }T ≥1 that goes to zero. For example, δT = op(T −1/4) for general neural networks sieve estimators when f is sufficiently smooth (Chen & White, 1999). When f is a sparse high-dimensional linear model, δT = op(T −1/2) for the Lasso estimator and Dantzig selector (Bickel et al., 2009). Assumption 4.5 (Covariance eigenvalue). There exists a λ > 0 s.t. ∥Σ∥ ≥ λ and ∥̂Σ∥ ≥ λ. Remark 4.6. It is a common assumption to require both the covariance matrix and the estimated covariance matrix to be strictly positive definite. The condition holds for the covariance matrix 12 as long as there is no linear dependency between variables, which is true for the errors εt. Besides, the assumption is also satisfied by the sample covariance matrix because our algorithm uses the pseudo-inverse, which ensures positive eigenvalues. Assumption 4.7 (Tail behavior). There exist some constants q > 4, K1, K2 > 0 and L ≥ 1, such that maxt≤T ∥εt∥ ≤ √K1p almost surely and E|⟨ε, x⟩|q ≤ Lq for x ∈ Sp−1. Besides, there exists a constant K2 such that Var[∥ε∥2] ≤ K2p. Remark 4.8. The assumption is required in Theorem 1.2 (Vershynin, 2012) so that the sample covariance matrix converges to the true covariance matrix in the operator norm. Other assumptions in literature ensure a O(T −1/2) convergence. For example, (Koltchinskii & Lounici, 2017) requires random variables ε to be weakly square-integrable, sub-Gaussian, and pre-Gaussian. Our method can use covariance estimators other than the classic sample covariance matrix. The sample covariance matrix is a natural choice, but it is a poor estimator when the dimension is very high unless there are some nice tail behaviors (Lugosi & Mendelson, 2019). There are a lot of results in the literature focusing on covariance estimation under different conditions. The Assumption 4.7 can be easily switched to other requirements if we change the estimator. (Cai et al., 2016) offers an overview of covariance estimators with their optimal rates. The choice and analysis of the covariance matrix is not the main focus of our paper, so we use the sample covariance matrix here for simplicity. With the i.i.d. assumption, we can show that the empirical distribution of et approximates the true CDF well in the following sense. Lemma 4.9 (Convergence of empirical CDF of {εt}T t=1 under i.i.d.). Under Assumption 4.1, for any training size T, there is an event AT which occurs with probability at least 1 − √ log(16T ) T , such that conditioning on AT , sup x | ̃FT +1(x) − Fe(x)| ≤ √ log(16T ) T . (16) Remark 4.10. The i.i.d. assumption is not a must, and we can easily extend it to the case where {et}T t=1 is stationary and strong mixing. We will show a similar result in Corollary B.11. With the assumptions, we can also show that the empirical distribution of ˆe approximates the empirical distribution of e well in the following sense: Lemma 4.11 (Distance between the empirical CDF of {εt}T t=1 and {ˆεt}T t=1 under i.i.d.). Under Assumption 4.1, 4.3, 4.5 and 4.7, with a high probability 1 − δ, sup x | ̂FT +1(x) − ̃FT +1(x)| ≤ (LT +1 + 1)CS + 2 sup x | ̃FT +1(x) − Fe(x)|, (17) 13 where CS = ( δ2 T λ + K3 λ2 [ C ( 1 δ )20/9q log ( 1 δ ) (log log p) 2 p3/2−2/q T 1/2−2/q + 22K3 max { p3 T δ , p 3/2δT }])1/2 = ˜O ( max { p3/4−1/q T 1/4−1/q , p 3/4δ1/2 T }) , (18) and C is a constant that depends only on K1, q, L and K3 = K1 + √3K2 is a constant. Our main theorem is the following Theorem 4.12, which establishes the asymptotic conditional coverage as a result of Lemma 4.9 and 4.11. Theorem 4.12 (Conditional guarantee under i.i.d. assumption). Under Assumption 4.1, 4.3, 4.5 and 4.7, with a high probability 1 − δ, for any training size T and α ∈ (0, 1), we have |P(YT +1 ∈ ̂Cα T +1 | XT +1 = xT +1) − (1 − α)| ≤ 12 √ log(16T ) T + 4(LT +1 + 1)(CS + δT ), (19) where CS is defined in (18). Remark 4.13. The bound is controlled by the sample size T and the coefficient δT . This vanishes when T → ∞ and δT → 0, which means that when the sample size is large enough, and the estimator ˆf is accurate enough, the conditional coverage will converge to 1 − α. Corollary 4.14 (Guarantee with true covariance matrix, and under i.i.d.). If the true covariance matrix Σ is known, we can use ̂Σ = Σ. Under Assumption 4.1, 4.3, 4.5 and 4.7, for any training size T and α ∈ (0, 1), we have |P(YT +1 ∈ ̂Cα T +1 | XT +1 = xT +1) − (1 − α)| ≤ 12 √ log(16T ) T + 4(LT +1 + 1) ( δT√λ + δT ) . (20) Remark 4.15. When the true covariance matrix is known, we have a tighter and simpler bound than Theorem 4.12. Although the true covariance is usually unknown in reality, the same bound can be reached if the sample covariance matrix is estimated from another independent training set. Then, we would present a corollary that extends our guarantee to the case where {εt}T t=1 is a stationary and strong mixing sequence. 14 Definition 4.16. A sequence of random variables {Xn} is said to be strictly stationary if for every k ≥ 1, any integers n1, . . . , nk, and any integer h, the joint distribution of the random variables (Xn1, . . . , Xnk ) is the same as the joint distribution of (Xn1+h, . . . , Xnk+h). Definition 4.17. A sequence of random variables {Xn} is said to be strongly mixing (or α-mixing) if the mixing coefficients αk defined by αk = sup n∈N sup A∈F n 1 ,B∈F ∞ n+k |P (A ∩ B) − P (A)P (B)| tend to zero as k → ∞, where F b a denotes the σ-algebra generated by {Xa, . . . , Xb}. Using a similar technique, we can prove the following result for the case where {εt}T t=1 is a stationary and strong mixing sequence. Here, we assume that the true covariance matrix Σ is known for simplicity, but we will discuss in Remark 4.19 how to extend the result to the case where true covariance is unknown. Corollary 4.18 (Guarantee with true covariance matrix, under stationarity and strong mixing). Assume {εt}T t=1 is a stationary and strong mixing sequence with mixing coefficient 0 < ∑ k>0 αk < M . Under Assumption 4.3, 4.5 and 4.7, for any training size T and α ∈ (0, 1), we have |P(YT +1 ∈ ̂Cα T +1 | XT +1 = xT +1) − (1 − α)| ≤ 12 ( M 2 )1/3(log T )2/3 T 1/3 + 4(LT +1 + 1) ( δT√λ + δT ) . (21) Remark 4.19. The first term in the convergence rate is of order ˜O(T −1/3), which is slighter bigger than the order ˜O(T −1/2) in Theorem 4.12 under i.i.d. case. The second term is the same. The essence of generalizing the outcome to scenarios where the true covariance matrix Σ remains unknown lies in the convergence properties of the sample covariance matrix. There are works directed towards these convergence properties within the context of stationary time series. For instance, Chen et al. (2013) presented an asymptotic convergence result for a threshold sample covariance estimator. Utilizing methodologies akin to those employed in Theorem 4.12, a similar result can be substantiated, and there is also the possibility to apply a variety of estimators. However, the focus of this work is not on the convergence of the covariance matrix estimator; hence, further exploration in this direction is omitted. Moreover, as previously indicated, independent training data can be leveraged to estimate the covariance matrix and achieve the same bound in Corollary 4.18. 15 Table 2: Simulation results by both methods. Target coverage is 90%. Standard deviation is computed over ten independent trials in which training and test data are regenerated. (a) Independent AR(w) p 2 4 8 10 16 20 Method MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI Coverage 90.0% (0.26) 90.0% (0.29) 90.0% (0.25) 89.9% (0.14) 90.0% (0.31) 89.9% (0.30) 89.8% (0.25) 89.8% (0.27) 89.9% (0.24) 89.9% (0.23) 90.0% (0.26) 89.8% (0.30) Size 1.45e+1 (9.34e-2) 1.52e+1 (8.73e-2) 3.00e+2 (2.62e+0) 3.94e+2 (3.38e+0) 1.30e+5 (1.43e+3) 3.68e+5 (6.44e+3) 2.65e+6 (4.79e+4) 1.22e+7 (1.61e+5) 2.23e+10 (5.61e+8) 5.84e+11 (1.39e+10) 9.15e+12 (2.97e+11) 8.67e+14 (2.90e+13) (b) VAR(w) p 2 4 8 10 16 20 Method MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI MultiDim SPCI SPCI Coverage 90.0% (0.26) 92.7% (0.25) 90.2% (0.21) 91.5% (0.22) 90.0% (0.23) 91.6% (0.18) 89.9% (0.23) 90.7% (0.31) 89.9% (0.20) 91.0% (0.19) 90.0% (0.25) 90.9% (0.19) Size 2.73e+0 (1.36e-2) 6.46e+0 (5.84e-2) 3.89e+1 (2.25e-1) 4.94e+2 (7.49e+0) 7.16e+4 (7.25e+2) 9.27e+6 (1.46e+5) 3.63e+7 (4.79e+5) 3.24e+9 (6.09e+7) 8.55e+12 (1.45e+11) 1.91e+17 (5.38e+15) 1.14e+16 (2.11e+14) 7.41e+22 (1.68e+21) 5 Experiments We test Algorithm 1 on both simulated and real-time series to show that MultiDimSPCI reaches valid coverage with smaller prediction regions. In all our experiments, the value of ρ used in (4) is set to be 0.001. For simplicity, we only consider the global covariance matrix in (3) rather than its local variant (11), which would bring further improvements. Code is available at https://github.com/hamrel-cxu/MultiDimSPCI. 5.1 Simulation We simulate two types of stationary time series. The first case considers independent AR(w) sequences, and the second case considers VAR(w) sequences. We want to show that compared to SPCI applied independently to each dimension (equivalent to using independent copula (Messoudi et al., 2021, See Sec. 3.3.1)), MultiDimSPCI yields significantly smaller prediction regions without sacrificing coverage. Data generation. Denote Yt = [Yi1, . . . , Yip]T ∈ Rp for p ≥ 2. We generate Yt as Yt = w∑ l=1 αlYi−l + εt, εt ∼ N (0, Σ). (22) In (22), αl ∈ Rp×p contains the set of coefficients, where we further construct them so that the sequences {Yt} are stationary. In the first case of independent AR(w) sequences, we have Σ = Ip. In the second case of VAR(w) sequences, we design Σ = BBT to be a positive definite covariance matrix, where Bij i.i.d. ∼ Unif[−1, 1]. 16 (a) Wind data (b) Solar data (c) Traffic data Figure 2: Real-data comparison of rolling coverage (target coverage is 95%) and size of prediction sets at p = 8. In each subplot of (a)-(c), the top row plots rolling coverage over prediction time indices (red dashed line is the target coverage) and as boxplots, and the bottom row shows results for rolling sizes. We only visualize the comparison of MultiDimSPCI with selected CP baselines, which have comparable average size of prediction regions in Table 3. Setup. In both cases of AR and VAR time series following (22), we let w = 5 and vary p ∈ {2, 4, 8, 10, 16, 20}. The initial 80K samples {Yt} are training data; the remaining 20K samples are test data. Because SPCI assumes independence across different univariate sequence, we let ˜α = 1 − (1 − α)1/p and apply SPCI on individual sequences with the corrected ˜α. The multivariate linear regression method is used as the point predictor. 17 Table 3: Real-data comparison of test coverage and average prediction set size by different methods. The target coverage is 0.95, and at each p, the smallest size of prediction sets is in bold. Our MultiDimSPCI yields the narrowest confidence sets without sacrificing coverage for two reasons. First, it explicitly captures dependency among coordinates of Yt by forming ellipsoidal prediction sets. Second, it captures temporal dependency among non-conformity scores upon adaptive re-estimation of score quantiles. (a) Wind data Method p = 2 coverage p = 2 size p = 4 coverage p = 4 size p = 8 coverage p = 8 size MultiDimSPCI 0.97 1.60 0.96 7.02 0.96 72.10 CopulaCPTS (Sun & Yu, 2024) 0.98 2.55 0.97 10.23 0.97 252.67 Local ellipsoid (Messoudi et al., 2022) 0.96 3.51 0.97 13.07 0.98 1.09e+3 Copula (Messoudi et al., 2021) 0.98 2.81 0.98 10.32 0.97 1.60e+3 TFT (Lim et al., 2021) 0.94 10.61 0.75 159.39 0.94 2.91e+4 DeepAR (Salinas et al., 2020) 0.96 7.07 0.76 67.97 0.96 1.79e+5 (b) Solar data Method p = 2 coverage p = 2 size p = 4 coverage p = 4 size p = 8 coverage p = 8 size MultiDimSPCI 0.96 1.68 0.96 2.89 0.97 4.97 CopulaCPTS (Sun & Yu, 2024) 0.99 4.36 0.99 37.56 0.99 3.28e+3 Local ellipsoid (Messoudi et al., 2022) 0.97 1.32 0.97 3.20 0.97 43.07 Copula (Messoudi et al., 2021) 0.99 4.11 0.99 27.73 0.99 1.42e+3 TFT (Lim et al., 2021) 0.99 13.68 0.99 71.72 0.93 1.19e+3 DeepAR (Salinas et al., 2020) 0.97 10.76 0.98 157.09 0.74 31.82 (c) Traffic data Method p = 2 coverage p = 2 size p = 4 coverage p = 4 size p = 8 coverage p = 8 size MultiDimSPCI 0.96 1.31 0.96 1.93 0.96 2.98 CopulaCPTS (Sun & Yu, 2024) 0.95 1.70 0.94 3.15 0.95 14.10 Local ellipsoid (Messoudi et al., 2022) 0.95 1.36 0.94 2.08 0.95 4.13 Copula (Messoudi et al., 2021) 0.95 1.44 0.95 3.90 0.94 40.60 TFT (Lim et al., 2021) 0.89 9.07 0.93 87.92 0.88 9.69e+2 DeepAR (Salinas et al., 2020) 0.87 13.53 0.88 57.20 0.82 9.89e+3 Results. Table 2 examines the empirical coverage and average size of prediction regions in Rp by both methods on the two cases of data generation. Both methods can maintain valid coverage around the target 90% in two cases. Nevertheless, it is clear that as dimension p increases, the average size of prediction regions by the proposed MultiDimSPCI is significantly smaller (for several magnitudes) than that by SPCI applied to individual sequences. In Figure A.1, we further visualize the non-critical regions in both cases to demonstrate why MultiDimSPCI provides smaller prediction regions. 5.2 Real-data We now compare MultiDimSPCI with existing methods designed for multivariate uncertainty quantification. The three CP baselines are CopulaCPTS (Sun & Yu, 2024), Local ellipsoid (Messoudi et al., 2022), and Copula (Messoudi et al., 2021). The two probabilistic forecasting baselines are temporal fusion transformers (TFT) (Lim et al., 2021) and DeepAR (Salinas et al., 18 Table 4: Comparison on wind data when dimension d = 25. The setup is identical to that in Table 3. MultiDimSPCI CopulaCPTS Local ellipsoid Copula Coverage 0.95 0.98 0.98 0.94 Size 3.55e+7 1.13e+14 4.93e+16 1.20e+13 2020). We consider three real multivariate time-series datasets. The first wind dataset considers wind speed in meters per second at different wind farms (Zhu et al., 2021), with 764 observations in total. The second solar dataset considers solar radiation in Diffused Horizontal Irradiance (DHI) units at different solar sensors (Zhang et al., 2021), with 8755 observations in total. The third traffic dataset considers traffic flow collected at different traffic sensors (Xu & Xie, 2021a), with 8778 observations in total. On each dataset, we randomly select p ∈ {2, 4, 8} locations (same for all methods) and examine the test coverage and average size on the p-dimensional time series. The first 85% data are used for training, and the remaining 15% are used for testing. Table 3 shows that the test coverage of MultiDimSPCI and two CP methods is always valid by yielding coverage greater than or equal to the target 95%. In contrast, the two probabilistic forecasting baselines may incur severe under-coverage, where TFT coverage is generally better than DeepAR’s. Regarding the average size of prediction regions, we also note that the average size by MultiDimSPCI is consistently smaller than those by baselines (except against Local ellipsoid on solar data when p = 2), demonstrating that our proposed method quantifies prediction uncertainty more precisely. We believe these benefits come from using ellipsoidal rather than hyper-rectangular prediction sets and the adaptive re-estimation of quantiles of non-conformity scores. Additionally, Table 4 shows comparisons on higher-dimensional wind data, on which the benefits of MultiDimSPCI persist. Figure 2 further analyzes the rolling performance of different methods. We see that the rolling coverage of MultiDimSPCI and the CP baselines all center around the target 95% coverage value with reasonably small variations. Meanwhile, MultiDimSPCI (a) Wind data (b) Solar data (c) Traffic data Figure 3: Distribution of estimated correlation and the eigenvalues of corresponding correlation matrices on real-time series. We visualize the results using p-dimensional prediction residuals with p = 8. 19 has a smaller rolling width than the CP baselines, indicating that our proposed method almost always yields smaller prediction regions. Lastly, as seen in Figure 3, the estimated correlation between residuals from two different locations can be as high as 0.92 (see solar data). Thus, it is indeed necessary to consider such correlation when constructing prediction regions to quantify prediction uncertainty effectively. 6 Discussions In this work, we proposed MultiDimSPCI, a general sequential conformal prediction method for multivariate time series. Specifically, MultiDimSPCI extends sequential univariate CP method to construct ellipsoids during test time. Extensions using local ellipsoids that are adaptive in shape are also discussed. Theoretically, we bound the coverage gap in finite samples without assuming data exchangeability. Empirically, on both simulation and real time series, we show MultiDimSPCI yields significantly smaller prediction sets than baselines and maintains coverage. In the future, we will explore constructing prediction regions beyond ellipsoids. One such possibility is using convex hulls, which are irregular in shape but could lead to the tightest fit as prediction sets. We discuss such possibility and preliminary results in Appendix A). We will also further study the theoretical properties of CP in high dimensions, leveraging existing results on multivariate quantile estimation. Acknowledgement The authors would like to thank Jonghyeok Lee, and Bo Dai for their helpful discussions and comments. This work is partially supported by an NSF CAREER CCF-1650913, NSF DMS- 2134037, CMMI-2015787, CMMI-2112533, DMS-1938106, DMS-1830210, and the Coca-Cola Foundation. References Angelopoulos, A., Candes, E., and Tibshirani, R. J. Conformal pid control for time series prediction. Advances in Neural Information Processing Systems, 36, 2024. Angelopoulos, A. N. and Bates, S. Conformal prediction: A gentle introduction. Foundations and Trends® in Machine Learning, 16(4):494–591, 2023. ISSN 1935-8237. doi: 10.1561/2200000101. URL http://dx.doi.org/10.1561/2200000101. Angelopoulos, A. N., Bates, S., Jordan, M., and Malik, J. Uncertainty sets for image classifiers 20 using conformal prediction. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=eNdiU_DbM9. Barber, R. F., Cand`es, E. J., Ramdas, A., and Tibshirani, R. J. Predictive inference with the jackknife+. The Annals of Statistics, 49(1):486 – 507, 2021. doi: 10.1214/20-AOS1965. URL https://doi.org/10.1214/20-AOS1965. Barber, R. F., Candes, E. J., Ramdas, A., and Tibshirani, R. J. Conformal prediction beyond exchangeability. The Annals of Statistics, 51(2):816–845, 2023. Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009. Cai, T. T., Ren, Z., and Zhou, H. H. Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation. Electronic Journal of Statistics, 10: 1–59, 2016. Chen, X. and White, H. Improved rates and asymptotic normality for nonparametric neural network estimators. IEEE Transactions on Information Theory, 45(2):682–691, 1999. Chen, X., Xu, M., and Wu, W. B. Covariance and precision matrix estimation for high-dimensional time series. The Annals of Statistics, 41(6):2994–3021, 2013. Diquigiovanni, J., Fontana, M., and Vantini, S. Distribution-free prediction bands for multivariate functional time series: an application to the italian gas market. arXiv preprint arXiv:2107.00527, 2021. Diquigiovanni, J., Fontana, M., and Vantini, S. Conformal prediction bands for multivariate functional data. Journal of Multivariate Analysis, 189:104879, 2022. Dobriban, E. and Lin, Z. Joint coverage regions: Simultaneous confidence and prediction sets. arXiv preprint arXiv:2303.00203, 2023. Elidan, G. Copulas in machine learning. In Copulae in Mathematical and Quantitative Finance: Proceedings of the Workshop Held in Cracow, 10-11 July 2012, pp. 39–60. Springer, 2013. Feldman, S., Bates, S., and Romano, Y. Calibrated multiple-output quantile regression with representation learning. Journal of Machine Learning Research, 24(24):1–48, 2023. Fontana, M., Zeni, G., and Vantini, S. Conformal prediction: a unified review of theory and new challenges. Bernoulli, 29(1):1–23, 2023. 21 Gibbs, I. and Candes, E. Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems, 34:1660–1672, 2021. Gui, Y., Barber, R., and Ma, C. Conformalized matrix completion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= 6f320HfMeS. Huang, K., Jin, Y., Candes, E., and Leskovec, J. Uncertainty quantification over graph with conformalized graph neural networks. NeurIPS, 2023. Johnstone, C. and Ndiaye, E. Exact and approximate conformal inference in multiple dimensions. arXiv preprint arXiv:2210.17405, 2022. Kim, B., Xu, C., and Barber, R. Predictive inference is free with the jackknife+-after-bootstrap. Advances in Neural Information Processing Systems, 33:4138–4149, 2020. Koltchinskii, V. and Lounici, K. Concentration inequalities and moment bounds for sample covariance operators. Bernoulli, pp. 110–133, 2017. Kosorok, M. R. Introduction to empirical processes and semiparametric inference, volume 61. Springer, 2008. Lim, B., Arık, S. ¨O., Loeff, N., and Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4):1748–1764, 2021. Lin, Z., Trivedi, S., and Sun, J. Conformal prediction with temporal quantile adjustments. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=PM5gVmG2Jj. Lugosi, G. and Mendelson, S. Near-optimal mean estimators with respect to general norms. Probability theory and related fields, 175(3-4):957–973, 2019. Messoudi, S., Destercke, S., and Rousseau, S. Copula-based conformal prediction for multi-target regression. Pattern Recognition, 120:108101, 2021. Messoudi, S., Destercke, S., and Rousseau, S. Ellipsoidal conformal inference for multi-target regression. In Conformal and Probabilistic Prediction with Applications, pp. 294–306. PMLR, 2022. Nair, Y. and Janson, L. Randomization tests for adaptively collected data. arXiv preprint arXiv:2301.05365, 2023. 22 Rio, E. et al. Asymptotic theory of weakly dependent random processes, volume 80. Springer, 2017. Romano, Y., Sesia, M., and Candes, E. Classification with valid and adaptive coverage. Advances in Neural Information Processing Systems, 33:3581–3591, 2020. Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–1191, 2020. Sklar, M. Fonctions de r´epartition `a n dimensions et leurs marges. In Annales de l’ISUP, volume 8, pp. 229–231, 1959. Stankeviˇci¯ut˙e, K., Alaa, A., and van der Schaar, M. Conformal time-series forecasting. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=Rx9dBZaV_ IP. Sun, S. H. and Yu, R. Copula conformal prediction for multi-step time series prediction. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=ojIJZDNIBj. Tibshirani, R. J., Barber, R. F., Candes, E., and Ramdas, A. Conformal prediction under covariate shift. In Advances in Neural Information Processing Systems, pp. 2530–2540, 2019. Vershynin, R. How close is the sample covariance matrix to the actual covariance matrix? Journal of Theoretical Probability, 25(3):655–686, 2012. Volkhonskiy, D., Burnaev, E., Nouretdinov, I., Gammerman, A., and Vovk, V. Inductive conformal martingales for change-point detection. In Conformal and Probabilistic Prediction and Applications, pp. 132–153. PMLR, 2017. Vovk, V., Gammerman, A., and Shafer, G. Algorithmic learning in a random world, volume 29. Springer, 2005. Wisniewski, W., Lindsay, D., and Lindsay, S. Application of conformal prediction interval estimations to market makers’ net positions. In Gammerman, A., Vovk, V., Luo, Z., Smirnov, E., and Cherubin, G. (eds.), Proceedings of the Ninth Symposium on Conformal and Probabilistic Prediction and Applications, volume 128 of Proceedings of Machine Learning Research, pp. 285–301. PMLR, 09–11 Sep 2020. 23 Xu, C. and Xie, Y. Conformal anomaly detection on spatio-temporal observations with missing data. arXiv preprint arXiv:2105.11886, 2021a. ICML 2021 Workshop on Distribution-Free Uncertainty Quantification. Xu, C. and Xie, Y. Conformal prediction interval for dynamic time-series. In International Conference on Machine Learning, pp. 11559–11569. PMLR, 2021b. Xu, C. and Xie, Y. Conformal prediction set for time-series. arXiv preprint arXiv:2206.07851, 2022. ICML 2022 Workshop on Distribution-Free Uncertainty Quantification. Xu, C. and Xie, Y. Conformal prediction for time series. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023a. Xu, C. and Xie, Y. Sequential predictive conformal inference for time series. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 38707–38727. PMLR, 23–29 Jul 2023b. Xu, C., Xie, Y., Vazquez, D. A. Z., Yao, R., and Qiu, F. Spatio-temporal wildfire prediction using multi-modal data. IEEE Journal on Selected Areas in Information Theory, 4:302–313, 2023. doi: 10.1109/JSAIT.2023.3276054. Zaffran, M., Dieuleveut, A., F’eron, O., Goude, Y., and Josse, J. Adaptive conformal predictions for time series. In ICML, 2022. Zhang, M., Xu, C., Sun, A., Qiu, F., and Xie, Y. Solar radiation ramping events modeling using spatio-temporal point processes. arXiv preprint arXiv:2101.11179, 2021. Zhu, S., Zhang, H., Xie, Y., and Van Hentenryck, P. Multi-resolution spatio-temporal prediction with application to wind power generation. arXiv preprint arXiv:2108.13285, 2021. A Additional experiments Comparison of non-critical regions. From Figure A.1, we see that MultiDimSPCI almost always yields smaller prediction sets than the coordinate-wise use of SPCI, whose prediction regions are squares that tend to over-cover the test samples. In contrast, MultiDimSPCI can well capture the dependency within Y to enable accurate uncertainty quantification. 24 Figure A.1: Non-critical regions on independent AR(w) (left) and VAR(w) (right) in R2. The average size of prediction regions is shown in captions. The size of the non-critical region by the proposed method is smaller, especially on VAR(w). As a result, the prediction regions by MultiDimSPCI are smaller than those by SPCI. Results using convex hulls. Currently, the ellipsoid shape is utilized for the prediction region. This method is robust and guarantees coverage accuracy, as demonstrated by experiments. However, considering the distribution shape of the residuals may further enhance its performance. The distribution might not conform to an ellipsoidal shape in high-dimensional cases, potentially being irregular. As a result, the ellipsoidal shape may not be the most optimal or tight fit. What if we could allow our prediction set to adapt to any shape? In doing so, the new region would likely be much tighter in scenarios where the true residuals do not follow an ellipsoidal distribution. In almost all instances, a convex hull can cover a set of points more compactly than an ellipsoid. We could achieve a significantly tighter fit by adopting a convex hull for the prediction region. The primary distinction between the two methods lies in the control parameters: the ellipsoid requires only the radius adjustment, whereas the convex hull necessitates control over all vertices. Ideally, we would select a set of data points that optimally balances coverage and minimizes the region size. However, this becomes computationally infeasible as the dataset size increases. Rather than optimizing the convex hull, we require it to cover exactly all the training data encompassed by the ellipsoid method. The convex hull method selects the points in the training set that are covered by the ellipsoid Table A.1: Accuracy and size of the prediction sets on two independent AR(w) sequences. MultiDimSPCI Copula Convex hull Baseline Coverage 90.2 90.6 90.0 89.8 Size 3.92 3.59 3.64 3.58 25 Table A.2: Accuracy and size of the prediction set for error uniformly spread in [−1, 1]4. MultiDimSPCI Copula Convex hull Baseline Coverage (80000 training samples) 89.6 90.5 85.9 89.9 Size (80000 training samples) 22.2 14.5 14.2 14.4 Coverage (800000 training samples) 90.2 90.5 88.6 89.9 Size (800000 training samples) 22.2 14.5 14.3 14.4 method and uses the convex hull of these points as the prediction regions. It has a smaller region than the ellipsoid method because of how it is constructed. As shown in Table A.1, the convex hull method on time series in R2 reaches valid coverage with a smaller prediction set. However, as shown in Table A.2, getting a region with valid coverage in higher dimensions requires much more training data. The computational cost in higher dimensions becomes unaffordable if we want to reach a reasonable coverage. This aspect will be explored in future research. B Proof Lemma B.1. Fe(eT +1) ∼ Unif[0, 1]. Proof. This holds for random variable e as long as the CDF Fe is continuous and strictly increasing. Lemma B.2. Under Assumption 4.1, for any training size T, there is an event AT which occurs with probability at least 1 − √ log(16T )/T , such that conditioning on AT , sup x | ̃FT +1(x) − Fe(x)| ≤ √ log(16T ) T . (A.1) Proof. The proof follows the proof of Lemma 1 in (Xu & Xie, 2023a). When the error process is i.i.d., the famous Dvoretzky-Kiefer-Wolfowitz inequality (Kosorok, 2008) implies that P (sup x | ̃FT +1(x) − Fe(x)| > sT ) ≤ 2e−2T s2 T . (A.2) Pick sT = √ W (16T )/(2 √T ), where W (T ) is the Lambert W function that satisfies W (T )eW (T ) = T . We see that sT ≤ √ log(16T )/T . Thus, define the event AT on which supx | ̃FT +1(x) − Fe(x)| ≤ 26 √ log(16T )/T , whereby we have sup x | ̃FT +1(x) − Fe(x)| ∣ ∣AT ≤ √ log(16T ) T , P (AT ) > 1 − √ log(16T ) T . (A.3) Lemma B.3 (Theorem 1.2, Vershynin (2012)). Consider a random vector ε ∈ Rp(p ≥ 4) which has zero mean and satisfies moment assumption 4.7 for some q > 4 and some K1, L. Let δ > 0. Then, with probability at least 1 − δ, the covariance matrix Σ of ε can be approximated by the sample covariance matrix 1 T ∑T t=1 εt ⊗ εt as ∥ ∥ ∥ ∥ ∥ Σ − 1 T T∑ t=1 εt ⊗ εt ∥ ∥ ∥ ∥ ∥ ≤ C ( 1 δ )20/9q log ( 1 δ ) (log log p)2 ( p T )1/2−2/q , (A.4) where C is a constant that depends only on parameters q, K1, L. Lemma B.4. Under Assumption 4.1, 4.3, 4.5 and 4.7, with high probability 1 − δ, T∑ t=1 |ˆet−et| ≤ T λ δ2 T + K3T λ2 [ C ( 1 δ )20/9q log ( 1 δ ) (log log p) 2 p3/2−2/q T 1/2−2/q + 22K3 max { p3 T δ , p 3/2δT }] , (A.5) where C is a constant that depends only on parameters q, K1, L and K3 = K1 + √3K2. Proof. For any test conformity score ˆe and the corresponding e, we drop subscript t here for notation simplicity. |ˆe − e| = |ˆεT ̂Σ−1 ˆε − εT Σ −1ε| ≤ |ˆεT ̂Σ−1 ˆε − εT ̂Σ −1ε| + |εT ̂Σ−1ε − εT Σ−1ε| ≤ ∥̂Σ −1∥∥∆∥2 + |εT (̂Σ−1 − Σ −1)ε| ≤ 1 λ ∥∆∥2 + ∥ε∥2∥̂Σ−1 − Σ−1∥ = 1 λ ∥∆∥2 + ∥ε∥2∥̂Σ−1(Σ − ̂Σ)Σ−1∥ ≤ 1 λ ∥∆∥2 + ∥ε∥2∥̂Σ−1∥∥Σ−1∥∥Σ − ̂Σ∥ ≤ 1 λ ∥∆∥2 + 1 λ2 ∥ε∥2∥Σ − ̂Σ∥. (A.6) Then the problem becomes bounding the spectral norm ∥Σ − ̂Σ∥. Recall that ˆεt = εt + ∆t, and we 27 define ¯ε∗ = ( ∑T t=1 ˆεt)/T , ¯∆ = ( ∑T t=1 ∆t)/T . The sample covariance matrix can be represented as ̂Σ = 1 T − 1 T∑ t=1(ˆεt − ¯ε)(ˆεt − ¯ε)T = 1 T − 1 T∑ t=1[(εt + ∆t) − (¯ε∗ + ¯∆)][(εt + ∆t) − (¯ε∗ + ¯∆)] T = 1 T − 1 T∑ t=1[(εt − ¯ε∗) + (∆t − ¯∆)][(εt − ¯ε∗) + (∆t − ¯∆)] T = 1 T − 1 T∑ t=1(εt − ¯ε∗)(εt − ¯ε∗) T ︸ ︷︷ ︸ 1 + 1 T − 1 T∑ t=1(∆t − ¯∆)(∆t − ¯∆) T ︸ ︷︷ ︸ 2 + 1 T − 1 T∑ t=1 [ (εt − ¯ε∗)(∆t − ¯∆) T + (∆t − ¯∆)(εt − ¯ε∗)T ] ︸ ︷︷ ︸ 3 . (A.7) The first term is the sample covariance matrix of εt, which typically converges to the true covariance matrix when the dimension p is negligible compared to T . From the assumption 4.3, the magnitude of the second term and the third term should be bounded by δT , which is the accuracy of prediction. This means that ∥̂Σ − Σ∥ ≤ ∥ 1 − Σ∥ + ∥ 2 ∥ + ∥ 3 ∥. (A.8) We can bound each spectral norm respectively. ∥ 1 − Σ∥ = ∥ ∥ ∥ ∥ ∥ 1 T − 1 ( T∑ t=1 εtεT t − T ¯ε∗ ¯εT ∗ ) − Σ ∥ ∥ ∥ ∥ ∥ = ∥ ∥ ∥ ∥ ∥ ( 1 T T∑ t=1 εtεT t − Σ ) + 1 T (T − 1) ( T∑ t=1 εtεT t − T 2 ¯ε∗ ¯εT ∗ )∥ ∥ ∥ ∥ ∥ ≤ ∥ ∥ ∥ ∥ ∥ 1 T T∑ t=1 εtεT t − Σ ∥ ∥ ∥ ∥ ∥ + 1 T (T − 1) (∥ ∥ ∥ ∥ ∥ T∑ t=1 εtεT t ∥ ∥ ∥ ∥ ∥ + ∥ ∥T 2 ¯ε∗ ¯εT ∗ ∥ ∥ ) (i) ≤ C ( 3 δ ) 20 9q log ( 3 δ ) (log log p)2 ( p T ) 1 2 − 2 q + 1 T (T − 1) ( T∑ t=1 ∥εtεT t ∥ + T 2∥¯ε∗ ¯εT ∗ ∥ ) (ii) ≤ 4C ( 1 δ ) 20 9q log ( 1 δ ) (log log p) 2 ( p T ) 1 2 − 2 q + 1 T (T − 1) T∑ t=1 ∥εt∥ 2 + 2∥¯ε∗∥ 2, 28 where (i) holds with high probability 1 − δ 3 under Assumption 4.7 according to the Lemma B.3 from Theorem 1.2 in Vershynin (2012) and (ii) holds because 320/9q ≤ 2, log(3/δ) ≤ 2 log(1/δ) when δ ≤ 1/3 and T /(T − 1) ≤ 2. We can put the constant 4 into the constant C which simplifies the notation. From now on, we use C to represent the whole constant 4C in the expression. For the other term ∥¯ε∗∥2 = ∥(∑T t=1 εt)/T ∥2, we can bound it with Chebshev’s inequality. Since εt ∈ Rp, we use εti(1 ≤ i ≤ p) to denote the ith entry of random vector εt. ∥¯ε∗∥ 2 = ∥ ∥ ∥ ∥ ∥ ∑T t=1 εt T ∥ ∥ ∥ ∥ ∥ 2 = p∑ i=1 ∣ ∣ ∣ ∣ ∣ ∑T t=1 εti T ∣ ∣ ∣ ∣ ∣ 2 . (A.9) Using Chebshev inequality, we have that P (∣ ∣ ∣ ∣ ∣ ∑T t=1 εti T ∣ ∣ ∣ ∣ ∣ ≥ √ 3pVar(εti) T δ ) ≤ Var(εti) T ( 3pVar(εti) T δ ) = δ 3p (A.10) This means that P ( ∥¯ε∗∥2 ≤ 3p ∑p i=1 Var(εti) T δ ) = P   p∑ i=1 ∣ ∣ ∣ ∣ ∣ ∑T t=1 εti T ∣ ∣ ∣ ∣ ∣ 2 ≤ 3p ∑p i=1 Var(εti) T δ   ≥ P   p⋂ i=1 { ∣ ∣ ∣ ∣ ∣ ∑T t=1 εti T ∣ ∣ ∣ ∣ ∣ 2 ≤ 3pVar(εti) T δ }  ≥ ( 1 − δ 3p )p ≥ 1 − δ 3 . (A.11) Considering that p∑ i=1 Var(εti) = E(∥εt∥2) ≤ K1p. We have that with probability higher than 1 − δ/3, ∥ 1 − Σ∥ ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2 ( p T ) 1 2 − 2 q + 1 T (T − 1) T∑ t=1 ∥εt∥2 + 2∥¯ε∗∥ 2 ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2 ( p T ) 1 2 − 2 q + 1 T (T − 1) T∑ t=1 ∥εt∥2 + 6K1p2 T δ . (A.12) 29 For the second term, we have ∥ 2 ∥ = ∥ ∥ ∥ ∥ ∥ 1 T − 1 ( T∑ t=1 ∆t∆T t − T ¯∆ ¯∆T )∥ ∥ ∥ ∥ ∥ ≤ 1 T − 1 (∥ ∥ ∥ ∥ ∥ T∑ t=1 ∆t∆T t ∥ ∥ ∥ ∥ ∥ + T ∥ ¯∆ ¯∆T ∥ ) ≤ 2 T − 1 T∑ t=1 ∥∆t∥ 2 ≤ 2T δ2 T T − 1 . (A.13) For the third term, we have ∥ 3 ∥ = 1 T − 1 ∥ ∥ ∥ ∥ ∥ T∑ t=1(εt∆ T t + ∆tεT t ) − T (¯ε∗ ¯∆T + ¯∆¯εT ∗ ) ∥ ∥ ∥ ∥ ∥ ≤ 2 T − 1 (∥ ∥ ∥ ∥ ∥ T∑ t=1 εt∆T t ∥ ∥ ∥ ∥ ∥ + T ∥ ¯∆∥∥¯ε∗∥ ) = 2 T − 1   ∥ ∥ ∥ ∥ ∥ T∑ t=1 εt∆T t ∥ ∥ ∥ ∥ ∥ + √ √ √ √ ( ∥ ∑T t=1 εt∥2 T ) ( ∥ ∑T t=1 ∆t∥2 T )  ≤ 2 T − 1   T∑ t=1 ∥εt∥∥∆t∥ + √ √ √ √ ( T∑ t=1 ∥εt∥2) ( T∑ t=1 ∥∆t∥2)  (i) ≤ 4 T − 1 √ √ √ √ ( T∑ t=1 ∥εt∥2) ( T∑ t=1 ∥∆t∥2) ≤ 4δT T − 1 √ √ √ √T T∑ t=1 ∥εt∥2. The inequality (i) holds because of Cauchy-Schwarz inequality (∑T t=1 ∥εt∥2)(∑T t=1 ∥∆t∥2) ≥ ( ∑T t=1 ∥εt∥∥∆t∥)2. From Assumption 4.7, we have E [ T∑ t=1 ∥εt∥ 2] = T E(∥εt∥2) ≤ T K1p. (A.14) 30 Using Chebshev’s inequality we have P {∣ ∣ ∣ ∣ ∣ 1 T T∑ t=1 ∥εt∥2 − E[∥εt∥2] ∣ ∣ ∣ ∣ ∣ ≥ √ 3Var[∥εt∥2] T δ } ≤ Var[∥εt∥2] T ( 3Var[∥εt∥2]) T δ ) = δ 3 , (A.15) which means with probability higher than 1 − δ/3, 1 T T∑ t=1 ∥εt∥ 2 ≤ E[∥εt∥ 2] + √ 3Var[∥εt∥2] T δ ≤ K1p + √ 3K2p T δ ≤ (K1 + √ 3K2)p := K3p. (A.16) The last inequality holds because of Assumption 4.7 and pT δ ≥ 1. Without loss of generality, we can assume K3 ≥ 1. Define ST = 1 T ∑T t=1 ∥εt∥2. Overall, with probability higher than 1 − δ, we have inequality A.16 and the following inequality holds when T ≥ p ∥̂Σ − Σ∥ ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 1 T (T − 1) T∑ t=1 ∥εt∥2 + 6K1p2 T δ + 2T δ2 T T − 1 + 4δT T − 1 √ √ √ √T T∑ t=1 ∥εt∥2 = 2C ( 1 δ ) 20 9q log ( 3 δ ) (log log p)2( p T ) 1 2 − 2 q + 2 T − 1 ST + 6K1p2 T δ + 2T δ2 T T − 1 + 4T δT T − 1 √ST (i) ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 4 T ST + 6K1p2 T δ + 4δ2 T + 8δT √ ST ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 16 max { ST T , δ2 T , δT √ ST } + 6K1p2 T δ ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 16 max { K3p T , δ2 T , √ K3pδT } + 6K3p2 T δ ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 22K3 max { p T , δ2 T , √pδT , p2 T δ } (ii) ≤ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 22K3 max { p2 T δ , √ pδT } . where (i) holds because T T −1 ≤ 2 and (ii) holds because p ≥ δ2 T . Then the following inequality 31 holds with high probability 1 − δ, T∑ t=1 |ˆet − et| ≤ 1 λ T∑ t=1 ∥∆t∥ 2 + 1 λ2 T∑ t=1 ∥εt∥2∥̂Σ − Σ∥ ≤ T λ δ2 T + K3T p λ2 [ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2( p T ) 1 2 − 2 q + 22K3 max { p2 T δ , √pδT }] = T λ δ2 T + K3T λ2 [ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p) 2 p3/2−2/q T 1/2−2/q + 22K3 max { p3 T δ , p 3/2δT }] . (A.17) Corollary B.5. If the true covariance matrix Σ is known and we use ̂Σ = Σ, then T∑ t=1 |ˆet − et| ≤ T λ δ2 T . (A.18) Proof. This is because when we use ˆΣ = Σ, the second term in Equation A.17 is zero. Then the bound is simply the first term. Lemma B.6. Under Assumption 4.1, 4.3, 4.5 and 4.7, with a high probability 1 − δ, sup x | ̂FT +1(x) − ̃FT +1(x)| ≤ (LT +1 + 1)CS + 2 sup x | ̃FT +1(x) − Fe(x)|, (A.19) where CS = ( δ2 T λ + K3 λ2 [ C ( 1 δ ) 20 9q log ( 1 δ ) (log log p)2 p3/2−2/q T 1/2−2/q + 22K3 max { p3 T δ , p 3/2δT }])1/2 and K3 = K1 + √3K2. Proof. Using lemma B.4 and 4.3, we have that with probability 1 − δ T∑ t=1 |et − ˆet| ≤ T λ δ2 T + K3T λ2 [ C ( 1 δ )20/9q log ( 1 δ ) (log log p) 2 p3/2−2/q T 1/2−2/q + 22K3 max { p3 T δ , p 3/2δT }] = T C2 S. (A.20) 32 Let S = {t : |et − ˆet| ≥ CS}. Then |S|CS ≤ T∑ t=1 |et − ˆet| ≤ T C2 S. (A.21) So |S| ≤ T CS. Then | ̂FT +1(x) − ̃FT +1(x)| ≤ 1 T T∑ t=1 |1{ˆet ≤ x} − 1{et ≤ x}| ≤ 1 T ( |S| + ∑ t /∈S |1{ˆet ≤ x} − 1{et ≤ x}| ) (i) ≤ 1 T ( |S| + ∑ t /∈S 1{|et − x| ≤ CS} ) ≤ 1 T ( |S| + T∑ t=1 1{|et − x| ≤ CS} ) ≤ CS + P(|eT +1 − x| ≤ CS)+ sup x ∣ ∣ ∣ ∣ ∣ 1 T T∑ t=1 1{|et − x| ≤ CS} − P(|eT +1 − x| ≤ CS) ∣ ∣ ∣ ∣ ∣ = CS + [Fe(x + CS) − Fe(x − CS)] + sup x ∣ ∣ ∣[ ̃FT +1(x + CS) − ̃FT +1(x − CS)] − [Fe(x + CS) − Fe(x − CS)] ∣ ∣ ∣ (ii) ≤ (LT +1 + 1)CS + 2 sup x | ̃FT +1(x) − Fe(x)|, where (i) is because |1{a ≤ x} − 1{b ≤ x}| ≤ 1{|b − x| ≤ |a − b|} for a, b ∈ R and (ii) is because the Lipschitz continuity of Fe(x). Corollary B.7. If the true covariance matrix Σ is known and we use ̂Σ = Σ. Under Assumption 4.1, 4.3, 4.5 and 4.7, with a high probability 1 − δ, sup x | ̂FT +1(x) − ̃FT +1(x)| ≤ (LT +1 + 1) δT√λ + 2 sup x | ̃FT +1(x) − Fe(x)|. (A.22) Theorem B.8. Under Assumption 4.1, 4.3, 4.5 and 4.7, with a high probability 1 − δ, for any training size T and α ∈ (0, 1), we have |P(YT +1 ∈ ̂Cα T +1 | XT +1 = xT +1) − (1 − α)| ≤ 12 √ log(16T )/T + 4(LT +1 + 1)(CS + δT ). (A.23) 33 Proof. First, we recall the notation here. We define ˆεt = yt − ˆf (xt) as the prediction residual, ˆet = ˆεT t ̂Σ−1 ˆεt as the non-conformity score, et = εT t Σ−1εt and ∆t = ˆεt − εt. Besides, we define the empirical CDF ̂FT +1(x) = 1 T T∑ t=1 1{ˆet ≤ z} ̃FT +1(x) = 1 T T∑ t=1 1{et ≤ z}. (A.24) For any β ∈ [0, α], following the idea in Section 4: ∣ ∣ ∣P (YT +1 ∈ ̂Cα T +1∣ ∣XT +1 = xT +1) − (1 − α) ∣ ∣ ∣ = ∣ ∣ ∣P (ˆeT +1 ∈ [β quantile of ̂FT +1, 1 − α + β quantile of ̂FT +1] ∣ ∣XT +1 = xT +1) − (1 − α) ∣ ∣ ∣ = ∣ ∣ ∣P (β ≤ ̂FT +1 (ˆeT +1) ≤ 1 − α + β) − P (β ≤ Fe (eT +1) ≤ 1 − α + β) ∣ ∣ ∣ . (A.25) The last equality is a result of Lemma B.1. We can further rewrite the equation A.25 as follows: |P(β ≤ ̂FT +1(ˆeT +1) ≤ 1 − α + β) − P(β ≤ Fe(eT +1) ≤ 1 − α + β)| ≤ E|1{β ≤ ̂FT +1(ˆeT +1) ≤ 1 − α + β} − 1{β ≤ Fe(eT +1) ≤ 1 − α + β}| ≤ E(|1{β ≤ ̂FT +1(ˆeT +1)} − 1{β ≤ Fe(eT +1)}|+ |1{ ̂FT +1(ˆeT +1) ≤ 1 − α + β} − 1{Fe(eT +1) ≤ 1 − α + β}|). (A.26) The last inequality is because that for any constants a, b and univariates x, y, |1{a ≤ x ≤ b} − 1{a ≤ y ≤ b}| ≤ |1{a ≤ x} − 1{a ≤ y}| + |1{x ≤ b} − 1{y ≤ b}|. Then we have E|1{β ≤ ̂FT +1(ˆeT +1)} − 1{β ≤ Fe(eT +1)}| ≤ P(|Fe(eT +1) − β| ≤ | ̂FT +1(ˆeT +1) − Fe(eT +1)|) E|1{ ̂FT +1(ˆeT +1) ≤ 1 − α + β} − 1{Fe(eT +1) ≤ 1 − α + β}| ≤ P(|Fe(eT +1) − (1 − α + β)| ≤ | ̂FT +1(ˆeT +1)−Fe(eT +1)|), which holds since |1{a ≤ x} − 1{b ≤ x}| ≤ 1{|b − x| ≤ |a − b|} for any constant a, b and univariate x and E[1{A}] = P(A). Recall in Lemma 4.9, we defined AT as the event on which sup x | ̃FT +1(x) − Fe(x)| ∣ ∣AT ≤ √ log(16T ) T , 34 where P(AT ) > 1 − √ log(16T ) T . Let AC T denote the complement of the event AT . For any γ ∈ [0, 1], we have that P(|Fe(eT +1) − γ| ≤ | ̂FT +1(ˆeT +1) − Fe(eT +1)|) ≤ P(|Fe(eT +1) − γ| ≤ | ̂FT +1(ˆeT +1) − Fe(eT +1)| ∣ ∣AT ) + P(AC T ) ≤ P(|Fe(eT +1) − γ| ≤ | ̂FT +1(ˆeT +1) − Fe(eT +1)| ∣ ∣AT ) + P(AC T ) ≤ P(|Fe(eT +1) − γ| ≤ | ̂FT +1(ˆeT +1) − Fe(ˆeT +1)| + |Fe(ˆeT +1) − Fe(eT +1)| ∣ ∣AT ) + √ log(16T ) T . (A.27) To bound the conditional probability above, we note that with a high probability 1−δ, conditioning on the event AT , | ̂FT +1(ˆeT +1) − Fe(ˆeT +1)| + |Fe(ˆeT +1) − Fe(eT +1)| ∣ ∣AT ≤ sup x | ̂FT +1(x) − Fe(x)| ∣ ∣AT + LT +1|ˆeT +1 − eT +1| ≤ sup x | ̂FT +1(x) − ̃FT +1(x)| ∣ ∣AT + sup x | ̃FT +1(x) − Fe(x)|∣ ∣AT + LT +1|ˆeT +1 − eT +1| ≤ (LT +1 + 1)CS + 3 sup x | ̃FT +1(x) − Fe(x)| ∣ ∣AT + LT +1δT ≤ 3 √ log(16T ) T + (LT +1 + 1)(CS + δT ). (A.28) which follows the result from Lemma 4.9 and 4.11. Therefore, because Fe(eT +1) ∼ Unif[0, 1], we have P (|Fe(eT +1) − γ| ≤ | ̂FT +1(ˆeT +1) − Fe(ˆeT +1)| + |Fe(ˆeT +1) − Fe(eT +1)| ∣ ∣AT ) ≤6 √ log(16T ) T + 2(LT +1 + 1)(CS + δT ). (A.29) As a result, by letting γ = β and 1 − α + β, we have |P(YT +1 ∈ ̂Cα T +1 | XT +1 = xT +1) − (1 − α)| ≤ 12 √ log(16T ) T + 4(LT +1 + 1)(CS + δT ). (A.30) Now we have an asymptotic coverage guarantee for the case where {εt}T t=1 is i.i.d., and we can extend the result to the case where {εt}T t=1 is stationary and strong mixing. Assumption B.9. Assume {εt} T +1 t=1 is stationary and strong mixing with the mixing coefficients 35 ∑ k>0 αk < M . Meanwhile, Fe(x) (the CDF of true non-conformity score) is Lipschitz continuous with constant LT +1 > 0. The properties of stationary and strong mixing can be imparted to the sequence {et}T t=1. Lemma B.10. Under Assumption B.9, {et}T t=1 is stationary and strong mixing with coefficients ∑ k>0 αk({et}t≥1) < M , where αk({et}t≥1) represents the α−mixing coefficients of the random sequence {et}t≥1. Proof. The relationship between et and εt is that et = εT t Σ−1εt. (A.31) Define f (x) = xT Σ−1x. Since f (x) is a Borel-measurable function, we have that f −1(B) is also Borel-measurable for any Borel set B ⊂ R. Thus we have P(en1 ∈ Bn1, · · · , enk ∈ Bnk ) = P(εn1 ∈ f −1(Bn1), · · · , εnk ∈ f −1(Bnk )) = P(εn1+h ∈ f −1(Bn1), · · · , εnk+h ∈ f −1(Bnk )) = P(en1+h ∈ Bn1, · · · , enk+h ∈ Bnk ), (A.32) which shows the stationarity of {et}T t=1. Besides, the σ-algebra generated by f (εt) is contained in the σ-algebra generated by εt; consequently for all I ⊂ Z (possibly infinite), σ(f (εt), t ∈ I) ⊂ σ(εt, t ∈ I). (A.33) Since the definition of the mixing coefficient is the maximum over the sub-sigma algebra generated by the sequence, it follows that for all k, αk({f (εt)}t≥1) ≤ αk. (A.34) As a result, we have that {et}T t=1 is strong mixing with mixing coefficients ∑ k>0 αk({et}t≥1) < M . Lemma B.11. Under Assumption B.9, with a high probability 1 − ( M (log T )2 2T ) 1 3 , sup x | ̃FT +1(x) − Fe(x)| ≤ ( M 2 )1/3(log T )2/3 T 1/3 . (A.35) 36 Proof. The proof follows the proof of Corollary 2 in (Xu & Xie, 2023a). Define vT (x) := √T ( ̃FT +1(x) − Fe(x)). Then, Proposition 7.1 in (Rio et al., 2017) shows that E ( sup x |vT (x)| 2) ≤ ( 1 + 4 T∑ k=0 αk ) ( 3 + log T 2 log 2 )2 , (A.36) where αk is the kth mixing coefficient. Since we assumed that the coefficients are summable with ∑ k≥0 αk < M (for example, αk = O(n−s), s > 1), Markov’s Inequality shows that P(sup x | ̃FT +1(x) − Fe(x)| ≥ sT ) ≤ E(supx |vT (x)|2/T ) s2 T ≤ 1 + 4M T s2 T ( 3 + log T 2 log 2 )2 . (A.37) Thus, we let sT := ( 1 + 4M T ( 3 + log T 2 log 2 )2)1/3 ≈ ( M (log T )2 2T )1/3 , (A.38) and see that P ( sup x | ̃FT +1(x) − Fe(x)| ≤ ( M (log T )2 2T )1/3) ≥ 1 − ( M (log T )2 2T )1/3 . (A.39) Hence, the event AT is chosen so that conditioning on AT , sup x | ̃FT +1(x) − Fe(x)| ≤ ( M 2 )1/3(log T )2/3 T 1/3 . (A.40) Corollary B.12. Assume {εt}T t=1 is a stationary and strong mixing sequence with mixing co- efficient 0 < ∑ k>0 αk < M . Under Assumption 4.3, 4.5 and 4.7, for any training size T and α ∈ (0, 1), we have |P(YT +1 ∈ ̂Cα T +1 | XT +1 = xT +1) − (1 − α)| ≤ 12 ( M 2 )1/3(log T )2/3 T 1/3 + 4(LT +1 + 1) ( δT√λ + δT ) . (A.41) When the true covariance matrix Σ is known, lemma B.5 also holds for the stationary and 37 strong mixing process, and the proof can be directly used. Combining B.5 and B.11 with the same technique in Theorem B.8 yields the bound in Corollary 4.18. When the true covariance matrix Σ is unknown, we only need to prove a similar result in Lemma B.4. The difference is that we require the covariance estimator to converge to the true covariance matrix at a certain speed. As mentioned in Remark 4.19, there is work presenting covariance estimators with guarantee in the stationary case, like Chen et al. (2013). As long as we plug in certain estimators, the proof will follow, and the bound will depend on the guarantee of the estimator. 38","libVersion":"0.3.2","langs":""}
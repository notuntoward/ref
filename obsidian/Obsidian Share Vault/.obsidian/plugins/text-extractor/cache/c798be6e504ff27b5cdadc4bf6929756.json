{"path":"lit/lit_notes_OLD_PARTIAL/Metz24chatGPTphysWorldST.pdf","text":"3/18/24, 1:31 PM How the Al that drives ChatGPT will move into the physical world | The Seattle Times The Seattle Times Business TheSeattleTimes How the Al that drives ChatGPT will move into the physical world March18, 2024 at 7:00 am 3 2 of 15 | Covariant’s Al-powered Robotic Putwall system autonomously sorts items at the company’s headquarters in Emeryville, Calif., on March 8, 2024. Covariant, a robotics start-up, is designing technology that lets robots learn... (Balazs Gardi/ The New York Times) More \\ By Cade Metz The New York Times EMERYVILLE, Calif. — Companies such as OpenAl and Midjourney build chatbots, image generators and other artificial intelligence tools that operate in the digital world. Now, a startup founded by three former OpenAl researchers is using the technology development methods behind chatbots to build Al technology that can navigate the physical world. Covariant, a robotics company headquartered in Emeryville, Calif., is creating ways for robots to pick up, move and sort items as they are shuttled through warehouses and distribution centers. Its goal is to help robots gain an understanding of what is going on around them and decide what they should do next. The technology also gives robots a broad understanding of the English language, letting people chat with them as if they were chatting with ChatGPT. The technology, still under development, is not perfect. But it is a clear sign that the Al systems that drive online chatbots and image generators will also power machines in warehouses, on roadways and in homes. Like chatbots and image generators, this robotics technology learns its skills by analyzing enormous amounts of digital data. That means engineers can improve the technology by feeding it more and more data. Covariant, backed by $222 million in funding, does not build robots. It builds the software that powers robots. The company aims to deploy its new technology with warehouse robots, providing a road map for others to do much the same in manufacturing plants and perhaps even on roadways with driverless cars. The Al systems that drive chatbots and image generators are called neural networks, named for the web of neurons in the brain. By pinpointing patterns in vast amounts of data, these systems can learn to recognize words, sounds and images — or even generate them on their own. This is how OpenAlI built ChatGPT, giving it the power to instantly answer questions, write term papers and generate computer programs. It learned these skills from text culled from across the internet. (Several media outlets, including The New York Times, have sued OpenAl for copyright infringement.) Companies are now building systems that can learn from different kinds of data at the same time. By analyzing both a collection of photos and the captions that describe those photos, for example, a system can grasp the relationships between the two. It can learn that the word “banana” describes a curved yellow fruit. OpenAl employed that system to build Sora, its new video generator. By analyzing thousands of captioned videos, the system learned to generate videos when given a short description of a scene, like “a gorgeously rendered papercraft world of a coral reef, rife with colorful fish and sea creatures.” Covariant, founded by Pieter Abbeel, a professor at the University of California, Berkeley, and three of his former students, Peter Chen, Rocky Duan and Tianhao Zhang, used similar techniques in building a system that drives warehouse robots. The company helps operate sorting robots in warehouses across the globe. It has spent years gathering data — from cameras and other sensors — that shows how these robots operate. “It ingests all kinds of data that matter to robots — that can help them understand the physical world and interact with it,” Chen said. By combining that data with the huge amounts of text used to train chatbots such as ChatGPT, the company has built Al technology that gives its robots a much broader understanding of the world around it. After identifying patterns in this stew of images, sensory data and text, the technology gives a robot the power to handle unexpected situations in the physical world. The robot knows how to pick up a banana, even if it has never seen a banana before. It can also respond to plain English, much like a chatbot. If you tell it to “pick up a banana,” it knows what that means. If you tell it to “pick up a yellow fruit,” it understands that, too. It can even generate videos that predict what is likely to happen as it tries to pick up a banana. These videos have no practical use in a warehouse, but they show the robot’s understanding of what’s around it. “If it can predict the next frames in a video, it can pinpoint the right strategy to follow,” Abbeel said. https://www.seattletimes.com/business/how-the-ai-that-drives-chatgpt-will-move-into-the-physical-world/ 1/2 3/18/24, 1:31 PM How the Al that drives ChatGPT will move into the physical world | The Seattle Times The technology, called RFM, for robotics foundational model, makes mistakes, much like chatbots do. Though it often understands what people ask of it, there is always a chance that it will not. It drops objects from time to time. Gary Marcus, an Al entrepreneur and an emeritus professor of psychology and neural science at New York University, said the technology could be useful in warehouses and other situations where mistakes are acceptable. But he said it wouldbe more difficult and riskier to deploy in manufacturing plants and other potentially dangerous situations. “It comes down to the cost of error,” he said. “If you have a 150-pound robot that can do something harmful, that cost can be high.” As companies train this kind of system on increasingly large and varied collections of data, researchers believe it will rapidly improve. That is very different from the way robots operated in the past. Typically, engineers programmed robots to perform the same precise motion again and again — such as pick up a box of a certain size or attach a rivet in a particular spot on the rear bumper of a car. But robots could not deal with unexpected or random situations. By learning from digital data — hundreds of thousands of examples of what happens in the physical world — robots can begin to handle the unexpected. And when those examples are paired with language, robots can also respond to text and voice suggestions, as a chatbot would. This means that like chatbots and image generators, robots will become more nimble. “What is in the digital data can transfer into the real world,” Chen said. This story was originallypublished at nytimes.com.Read it here. The Seattle Times does not append comment threads to stories from wire services such as the Associated Press, The New York Times, The Washington Post or Bloomberg News. Rather, we focus on discussions related to local stories by our own staff. You can read more about our communitv policies here. Latest in Business - EPA bans asbestos, linked to more than 40,000 U.S. deaths per year The Environmental Protection Agency has announced a comprehensive... 18 minutes ago Reddit’s long, rocky road to an initial public offering Reddit, a throwback to an earlier era of... How the Al that drives ChatGPT will move into the physical world Covariant, a robotics start-up, is designing technology that... These cars come with sleeping kits, drones, kitchens and more The high-tech offerings highlight the risk for Western... A 10-acre slice of Seattle listed for $1.5M. But there's a catch A new listing in Seattle's frenzied real estate... United Airlines CEO tries to reassure customers that the airline is safe despite recent incidents The CEO of United Airlines is trying to... https://www.seattletimes.com/business/how-the-ai-that-drives-chatgpt-will-move-into-the-physical-world/ 2/2","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/Chilinski20nnLikViaCDF.pdf","text":"Neural Likelihoods via Cumulative Distribution Functions Pawel Chilinski University College London pawel.chilinski.14@ucl.ac.uk Ricardo Silva ∗ University College London and The Alan Turing Institute ricardo@stats.ucl.ac.uk Abstract We leverage neural networks as universal ap- proximators of monotonic functions to build a parameterization of conditional cumulative distribution functions (CDFs). By the applica- tion of automatic differentiation with respect to response variables and then to parameters of this CDF representation, we are able to build black box CDF and density estimators. A suite of families is introduced as alternative con- structions for the multivariate case. At one ex- treme, the simplest construction is a compet- itive density estimator against state-of-the-art deep learning methods, although it does not provide an easily computable representation of multivariate CDFs. At the other extreme, we have a ﬂexible construction from which multi- variate CDF evaluations and marginalizations can be obtained by a simple forward pass in a deep neural net, but where the computation of the likelihood scales exponentially with di- mensionality. Alternatives in between the ex- tremes are discussed. We evaluate the differ- ent representations empirically on a variety of tasks involving tail area probabilities, tail de- pendence and (partial) density estimation. 1 CONTRIBUTION We introduce a novel parameterization of multivari- ate cumulative distribution functions (CDFs) using deep neural networks. We explain how training can be done by a straightforward adaptation of standard methods for neural networks. The main motivations behind our work include: a direct evaluation of tail area probabilities; co- herent estimation of low dimensional marginals of a joint ∗Partially supported by EPSRC grant EP/N510129/1. Proceedings of the 36 th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), PMLR volume 124, 2020. distribution without the requirement of ﬁtting a full joint; and supervised/unsupervised density estimation. The ﬁrst two tasks beneﬁt directly from a CDF computed by a forward pass in a neural network, as tail probabil- ities and marginal CDFs can be read-off essentially di- rectly in this representation. The latter has been tack- led by an increasingly large literature on neural density estimators. This dates back at least to Bishop (1994), who used multilayer perceptrons to encode conditional means, variances, and mixture probabilities for a (con- ditional) mixture of Gaussians. Recently, models using transformations of a simple distribution into more com- plex ones were proposed. Dinh et al. (2015),Dinh et al. (2017),Papamakarios et al. (2017), Huang et al. (2018) and De Cao and Titov (2019) are examples of the state of the art for density estimation. They use invertible trans- formations from simple base distributions where the de- terminant of the Jacobian is easy to compute and Monte Carlo approaches for computing gradients become feasi- ble. Depending on the architecture, they are optimized for density estimation or sampling. We show we re- main competitive against these methods while keeping a comparatively simple uniform structure with few hy- perparameters. For instance, compared to the method of Bishop (1994), there is no need to choose the base distri- bution of the mixture nor the number of mixtures. All of the above is predicated on how we construct mul- tivariate CDFs. The most direct extension from uni- variate to multivariate CDFs is conceptually simple, but the calculation of the likelihood grows exponentially in the dimensionality. This is essentially the counterpart to computing a partition function in an undirected graphi- cal model, where here the problem is differentiation as opposed to integration. Compromises are discussed, in- cluding the relationship to Gaussian copula models and other CDF constructions based on small dimensional marginals. One extreme sacriﬁces the ability of repre- senting a CDF by a single forward pass in exchange for scalability to high dimensions, where we can compare it against state-of-the-art neural density estimators. This paper is organized as follows. Section 2 describes our main approach. Related work is described in Sec- tions 3 and S1. Experiments are discussed in Section 4. We show that our models are competitive for density estimation while able to directly tackle some modelling problems where recent neural-based models could not be applied in an obvious manner. 2 THE MONOTONIC NEURAL DENSITY ESTIMATOR We now introduce the Monotonic Neural Density Esti- mator (MONDE), inspired by neural network methods for parameterizing monotonic functions. The primary usage of MONDE is to compute conditional CDFs with a single forward pass in a deep neural network, while allowing for the calculation of the corresponding con- ditional densities by tapping into existing methods for computing derivatives in deep learning. The latter is particularly relevant for likelihood-based ﬁtting meth- ods such as maximum (composite) likelihood. We will focus on the continuous case only, where probability density functions (pdfs) are deﬁned, although extensions to include mixed combinations of discrete and continu- ous variables are straightforward by considering differ- ence operations as opposed to differentiation operations. We start with the simplest but important univariate case, where dependency between variables does not have to be modelled. We progress through more complex con- structions to conclude with the most ﬂexible but com- putationally demanding case, where we deal with mul- tivariate data without assuming any speciﬁc families of distributions for the data generating process. Here we use the following notation. F (y|x): multivariate conditional CDF, where y ∈ RK is the response vector and x ∈ RD is a covariate vector; Fk(yk|x): k-th marginal conditional CDF; f (y|x): multivariate conditional pdf; fk(yk|x): k-th marginal conditional pdf; w: the set of parameters of a neural network, where wl ij represents a particular weight connecting two nodes i and j, with i located at layer l of the network. 2.1 UNIVARIATE CASE The structure of MONDE for univariate responses is sketched in Figure1 as a directed graph1 with two types 1This graph is to be interpreted as a high-level simpliﬁed computation graph as opposed to a graphical model. It does not illustrate a generative process, hence the placement of output random variable y as an input to other variables. x 1 x D. . . . . . . . . . . . y . . . . . . . . . . . . t ∂ F ( y | x ) ∂ y F ( y | x ) f ( y | x ) w ij ∈ R w ij ∈ R + no weights x d co v ariate y response v ariable t σ ( bias + ∑ i w ij input i ) tanh( bias + ∑ i w ij input i ) Figure 1: The graph representing Univariate Mono- tonic Neural Density Estimator computational structure. The last node symbolizes the operation of differentiating parameterized conditional distribution function F (y|x) with respect to the input y. Its output f (y|x) encodes the conditional density function. The legend explains the symbols used. of edges and a layered structure so that two consecu- tive layers are fully connected, with no further edges. The deﬁnition of layer in this case follows immediately from the topological ordering of the graph. Covariates xi, . . . , xD and response variable y are nodes without parents in the graph, with the layer of the covariates de- ﬁned to be layer 1. The response variable y is posi- tioned in some layer 1 < ly < L, where L is the ﬁnal layer. Each intermediate node i at layerl, h l i, returns a non-linear transformation of a weighted sum of all nodes in layer l − 1. Here, as commonly used in the neu- ral network literature and based on preliminary results from our experiments, we use the hyperbolic tangent function such that h l i = tanh (∑ vj ∈Vl−1 wl ijvj + wl i0), where 1 < l < L and Vl is the set of nodes in layer l. The ﬁnal layer L consists of a single node t(y, x) ≡ sigmoid (∑ vj ∈VL−1 wL ijvj + wL i0), repre- senting the probability P (Y ≤ y | X = x) as encoded by the weights of the neural net. In other words, t(y, x) is interpreted as a CDF Fw(y | x) encoded by some w. Assuming w parameterizes a valid CDF, we can use an automatic differentiation method to generate the density function fw(y | x) corresponding to Fw(y | x) by dif- ferentiating t(y, x) with respect to y. The same princi- ple behind backpropagation applies here, and in our im- plementation we use Tensorﬂow2 to construct the com- putation graph that generates t(y, x). Once the pdf is 2https://www.tensorﬂow.org constructed, automatic differentiation can once again be used, now with respect to w, to generate gradients to be plugged into any gradient-based learning algorithm. To guarantee that t(y, x) is a valid CDF, we must en- force three constraints: (i) limy→−∞ t(y, x) = 0; (ii) limy→+∞ t(y, x) = 1; (iii) ∂t(y, x)/∂y ≥ 0. We chose the following design to meet these conditions: for every wl ij where vj ∈ Vl−1 is a descendant of y in the corre- sponding directed graph, enforce wl ij ≥ 0. This means that for all layers l > ly, all weights {wl ij} are con- strained to be non-negative, while {wl i0}, representing bias parameters, are unconstrained. Meanwhile, wl ij ∈ R for l ≤ ly. In Figure 1, the constrained weights are represented as squiggled edges. This guarantees mono- tonicity condition (iii). Due to the range of the logistic function being [0, 1] and t(y, x) being monotonic with respect to y, (i) and (ii) are also guaranteed to some ex- tent: an arbitrary choice of parameter vector w will not imply e.g. that limy→+∞ t(y, x) = 1 since the contribu- tion of y to the ﬁnal layer is bounded by the tanh non- linearity. However, by learning w, this limit is satisﬁed approximately since a likelihood-based ﬁtting method will favour ∑n i=1 F (ymax | x(i))/n ≈ 13, where n is the sample size, i indexes the training sample, and ymax is the maximum observed training value of Y in the sam- ple. There are ways of “normalizing” t(y, x) so that the limit is achieved exactly for all parameter conﬁgurations (see Section 2.4). We have not found it mandatory in or- der to obtain satisfactory empirical results. This happens even though our “unnormalized” implementation is at a theoretical disadvantage, as it can potentially represent densities with the total mass less than 1. This is veriﬁed by the experiments described in Section 4. 2.2 AUTOREGRESSIVE MONDE The ﬁrst variation of the MONDE model, capable of ef- ﬁciently encoding multivariate distributions, is presented in the Figure 2. It uses a similar approach to univari- ate output distributions, as described in Section 2.1, to parameterize each factor according to a fully connected probabilistic directed acyclic graph (DAG) model. That is, we assume a given ordering y1, . . . , yK deﬁning the fully connected DAG model: f (y | x) = K∏ k=1 fk(yk | x, y<k), (1) where y<k is set of response variables with index smaller than k. In theory, the indexing of variables can be chosen 3The expression is an empirical estimate of the marginal F (ymax) which is 1 in the empirical distribution. The claim follows as our estimator is chosen to minimize the KL diver- gence with respect to the empirical distribution. x y 0 1 y 1 1 y 1 M . . . y 2 1 y 2 M . . . y L − 1 1 y L − 1 M . . . . . . y L 1 ∂ F 1 ( y 1 | x ) ∂ y 1 ∂ F K ( y K | x , y <K ) ∂ y K . . . = F 1 ( y 1 | x ) , F 2 ( y 2 | x , y 1 ) . . . F k ( y K | x , y <K ) ∏ f K ( y K | x , y <K )f 1 ( y 1 | x ) f ( y | x ) x = x 0 . . . x D y 0 1 = y 1 . . . y K y l m = y l m, 1 . . . y l m,K y L 1 = F 1 ( y 1 ) , F 2 ( y 2 | y 1 ) . . . F k ( y K | y <K ) Figure 2: Autoregressive Model, MONDE MADE, The Multivariate Monotonic Neural Density Estimator archi- tecture with shared parametrization inspired by MADE. The square nodes of the computational graph contain vectors, to differentiate from the oval nodes representing scalar values. arbitrarily. In this work, we do not try to optimize it. This type of DAG parameterization was called “autoregres- sive” in the neural density estimator of Uria et al. (2013), a nomenclature we use here to emphasize that this is a related method. Our implementation of the autoregres- sive model uses parameter sharing inspired by MADE (Germain et al., 2015). The input to the computational graph is a K-dimensional vector y of response variables and a D-dimensional vec- tor x of covariate variables. These vectors comprise the ﬁrst layer of the network. Each consecutive hidden layer is an afﬁne transformation of the previous layer pro- ceeded by a nonlinear elementwise map transforming its inputs via sigmoid function. Each hidden layer is com- posed of M K-dimensional vectors yl m, where l indexes the layer and m is vector index within layer l. The afﬁne transformation matrix is constrained so that the k-th ele- ment of m-th vector, i.e. yl m,k, depends on a subset of the elements of the previous layer, i.e. yl−1 .,<k, and is mono- tonically non-decreasing with respect to yl−1 .,k . Here, dot . represents all possible indices m ∈ {1, 2, . . . , M }. Monotonicity is preserved using non-negative weights in the respective elements of the transformation matrix. Finally, the L-th layer consists of a single K-dimensional vector yL 1 , with each element representing a CDF factor, F1(y1), . . . , Fk(yk|x, y<k). Each CDF factor is differ- entiated with respect to its respective response variable to obtain its pdf. The product of all pdf factors provide the density function f (y|x). We provide the implemen- tation details in the supplement, Section S2.1. x 1 x D. . . . . . . . . . . . y 1 . . . . . . . . . . . . t ∂ F 1 ( y 1 | x ) ∂ y 1 F 1 ( y 1 | x ) Φ − 1 . . . . . . . . . . . . y K . . . . . . . . . . . . t ∂ F K ( y K | x ) ∂ y K F K ( y K | x ) Φ − 1 . . . . . . . . . . . . ρ ϕ ρ ∏ f 1 ( y 1 | x ) f K ( y K | x ) f ( y | x ) Figure 3: Multivariate Monotonic Neural Density Esti- mator with Gaussian Copula Dependency and Constant Covariance. 2.3 GAUSSIAN COPULA MODELS A standard way of extending univariate models to mul- tivariate models is to use a copula model (Sklar, 1959; Schmidt, 2006). In a nutshell, we can write a multivariate CDF F (y) as F (y) = P (Y ≤ y) = P (F −1(F (Y)) ≤ y) = P (U ≤ F (y)). Here, U is a random vector with uniformly distributed marginals in the unit hyper- cube and F −1(·) is the inverse CDF, applied element- wise to Y, which will be unique for continuous data as targeted in this paper. The induced multivariate distribu- tion with uniform marginals, P (U ≤ u), is called the copula of F (·). Elidan (2013) presents an overview of copulas from a machine learning perspective. This leads to a way of creating new distributions. Start- ing from a multivariate distribution, we extract its cop- ula. We then replace its uniform marginals with any marginals of interest, forming a copula model. In the case of the multivariate Gaussian distribution, the den- sity function f (y) = ϕρ(Φ −1(F1(y1)), . . . , Φ−1(FK(yK))) K∏ k=1fk(yk) (2) is a Gaussian copula model where ϕρ is a Gaussian den- sity function with zero mean and correlation matrix ρ, Φ−1 is the inverse CDF of the standard Gaussian, fk(·) is any arbitrary univariate density function and Fk(·) its respective distribution function. We can show that the k-th marginal of this density is indeed fk(·). We extend the density estimator from the previous sec- tion to handle a K-dimensional multivariate output y by exploiting two (conditional) copula variations. The ﬁrst variation is shown in Figure 3. Weight sharing is done so that all output variables yk are placed in layer ly, with all weights wl′ ij, l′ ≤ ly, producing transformations of the input x that is shared by all conditional marginals Fk(yk | x). From layers ly + 1, . . . , L, the neural net- work is divided into K disjoint blocks, each composed of two partitions: the ﬁrst depending monotonically on its respective yk, and the second depending on shared transformation of x. The ﬁrst partition depends on the second but not vice versa so monotonicity with respect to yk is preserved (all the paths from yk to tk in the com- putational graph use non-negative parameters, as shown in the diagram). Each of the K blocks generates output tk(yk, x) representing an estimate of the corresponding marginal Fk(yk | x). The k-th marginal pdf can be ob- tained by applying backpropagation with respect to yk: fk(yk) = ∂tk(yk, x)/∂yk. Next, the individual marginal distributions evaluated at each training point are trans- formed via standard normal quantile functions. Such quantiles Φ−1(Fk(yk | x)) are standard normal variables, which we use to estimate the correlation matrix for the entire training set. The estimated marginals and correla- tion matrix fully deﬁne our model. Taking the product of the estimated copula and estimated marginal densities (as shown in Equation 2) gives us an estimate of the joint density with a correlation matrix that does not change with x but which is simple to estimate by re-using the univariate MONDE. We call this the Constant Covari- ance Copula Model. The next improvement, achieved at a higher computa- tional cost, consists of parameterizing the correlation matrix using a covariate transformation. The diagram of this model is presented in Figure S2 in the supplement. This time the correlation matrix is parameterized via a low rank factorization of the covariance matrix which is a function of the covariates, allowing for a model with heteroscedasticity in the copula of the output variables. The correlation matrix parameterization is as follows: Σ(x) = u(x) · u(x) T + diag(d(x)) (3) D(x) ≡ √diag(Σ(x)), (4) ρ(x) ≡ D −1(x) · Σ(x) · D−1(x), (5) where Σ(x) is the covariate-parameterized low rank co- variance matrix; u(x) ∈ RK and d(x) ∈ RK + are covariate-parameterized vectors; diag is an operator which extracts a diagonal vector from the square ma- trix or creates a diagonal matrix from a vector (according to context); ρ(x) is the resulting covariate-parameterized correlation matrix. x 1 x D. . . σ σ. . . σ σ. . . . . . h x y 1 σσ . . . σσ . . . σσ . . . . . . h xy 1 y 2 σ σ. . . σ σ. . . σ σ. . . . . . h xy 2 . . . m . . . . . . . . . t ∝ F ( y | x ) σ sig moid ( bias + ∑ j w j input j ) sof tpl us ( bias + ∑ j w j input j ) Scalar multiplication Figure 4: Graph of an “Unnormalized” Distribution Function of PUMONDE, Pure Monotonic Neural Den- sity Estimator. It shows two response variables y1, y2 and covariates x transformed via computational graphs: hx, hxy1, hxy2, m and t. 2.4 PUMONDE: PURE MONOTONIC NEURAL DENSITY ESTIMATOR Our ﬁnal model family is a ﬂexible multivariate CDF pa- rameterization. It can be combined with multivariate dif- ferentiation, with respect to multiple response variables, to provide a likelihood function. The higher order deriva- tive ∂Kt(y, x)/∂y1 . . . ∂yK has to be non-negative so that the model can represent a valid density function4. The graph representing a monotone function with respect to each response variable with no ﬁnite upper bound (to be later “renormalized”) is presented in Figure 4. It is composed of several transformations, each of them represented in the computational graph as dashed rect- angle containing the nodes and edges symbolizing its computations: hx, a transformation the covariates us- ing a standard multilayer network of sigmoid transforma- tions; hxyi, a sequential composition of monotonic non- linear mappings starting with hx and response variable yi; m, the element-wise multiplication ⊙K i=1hxyi assum- ing all hxyi have the same dimensionality; and t, a mono- tonic transformation with respect to all its inputs that re- turns a positive real valued scalar. The last transforma- tion t uses only softplus as it non-linear transformations (softplus(x) ≡ log(1 + exp x)). The function t is non- 4This condition rules out sigmoid as the ﬁnal transforma- tion of the computational graph for the distribution function because ∂2σ(z)/∂2z ∈ R, therefore this version of the CDF estimator uses different approach to map its output to be in the (0, 1) range. decreasing with respect to any response variable on the same premises as previous models. In this model, we replaced tanh with sigmoid and softplus, where a hidden unit uses softplus if it has more than one ancestor in y1, . . . , yK and sigmoid otherwise. This is because non-convex activation functions such as the sigmoid will not guarantee e.g. ∂2t(y, x)/∂y1∂y2 ≥ 0 for units which have more than one target variable as an ancestor. Higher order derivatives with respect to the same response variable can take any real number because of the properties of the computational graph i.e., using products of non-decreasing functions which are always positive and noting the fact that second order derivative with respect to the same response variable transformed by sigmoid can take any real value. The density is then computed from the following trans- formations, here exempliﬁed for a bivariate model: Fw(y1, y2 | x) = t(m(hxy1(y1, hx(x)), hxy2(y2, hx(x)))) t(1) , (6) fw(y1, y2 | x) = ∂2Fw(y1, y2 | x) ∂y1∂y2 . (7) All output elements of m(·) have values in the [0, 1] range because it is element-wise multiplication of vec- tors with component values in [0, 1]. By plugging-in the maximum value 1 as input of the t transformation (as shown in denominator of Equation 6) we normalize the output of the distribution estimator Fw to lie within [0, 1] so to output a valid CDF. The guarantee of non- decreasing monotonicity and positiveness of the Fw with respect to each element of y assures that the range of the proposed estimator of a distribution function is in [0, 1] 5. 2.4.1 Composite Log-likelihood It must be stressed that an unstructured PUMONDE with full connections will in general require an exponential number of steps (as a function of K) for the gradient to be computed, mirroring the problem of computing parti- tion functions in undirected graphical models. Here we explore the alternative with the use of composite likeli- hood (Varin et al., 2011). 5The discussion at the end of Section 2.1, about the univari- ate MONDE not being able strictly attain 0 or 1 is applicable here as well, because of the hxyi(yi, x) transformation using bounded non-linearities. However, we can modify the initial layer at ly to simply monotonically map the real line to [0, 1] (or whatever the support of each Yk is), and do the normaliza- tion with respect to the output of ly having value 1, as opposed to the output of m. We decided to omit this in order to make the description of the model simpler, and due to the lack of early evidence that this pre-processing was useful in practice. We train the PUMONDE model by minimizing the ob- jective composed of the sum of the bivariate negative log-likelihoods (LL) for each pair of response variables (composite likelihood): LL = ∑ i=1..K,j=1..K,i<j log ∂2Fw(yi, yj|x) ∂yi∂yj . (8) We compute estimates of such sums over mini-batches of data sampled from the training set. We update param- eters using stochastic gradient descent as in other meth- ods presented in this work. In the future, we want to check its role in graphical models for CDFs (Huang and Frey, 2008; Silva et al., 2011). For now we will restrict PUMONDE to small dimensional problems. 3 RELATED WORK Our work is inspired by the literature on neural networks applied to monotonic function approximation and to den- sity estimation which is reviewed in the supplement Sec- tion S1. 4 EXPERIMENTS In this section, we describe experiments in which we compare our and baseline models on various datasets and ﬁve success criteria. In what follows, Tasks I, III and IV show how MONDE variations are competitive against the state-of-the-art on modelling dependencies. Given that, Tasks II and V advertise the convenience of a CDF parameterization against other approaches. As baselines, depending on the task, we use the follow- ing models: RNADE (Uria et al., 2013, 2014), MDN (Bishop, 1994), MADE (Germain et al., 2015), MAF (Papamakarios et al., 2017), TAN (Oliva et al., 2018) and NAF (Huang et al., 2018; De Cao and Titov, 2019). More experiments are included in the supplement, Section S4. 4.1 TASK I: DENSITY ESTIMATION In this section, we show results on density estimation us- ing UCI datasets. We use the same experimental setup as in (Papamakarios et al., 2017; Huang et al., 2018; De Cao and Titov, 2019) to compare recently proposed learning algorithms to one introduced in this work. In particu- lar, we evaluate a MONDE MADE variant which is de- scribed in Section 2.2. It is a simple extension of our MONDE model to multivariate response variables using autoregressive factorization. Among our methods, it is the only viable option to be applied to high dimensional and large datasets that does not make use of a paramet- ric component, as in the Gaussian copula variants. Re- sults are presented in Table 1, which contains test log- likelihoods and error bars of 2 standard deviations on ﬁve datasets. MONDE MADE matched the performance of the state of the art NAF model from (Huang et al., 2018) for the POWER dataset, and exceeded the performance of the NAF for the GAS dataset. We achieved slightly worse results on the other UCI datasets but we noticed that our model had a tendency to overﬁt the training data in these cases. We have not applied techniques that could improve generalization like batch normalization which were used in the baseline models. We conclude that our models, by achieving comparable results and having a complementary inductive bias to the baselines, can be used as yet another tool for the beneﬁt of practitioners. 4.2 TASK II: TAIL EVENT CLASSIFICATION (a) ROC Curves (b) Precision/Recall Curves Figure 5: ROC Curves/AUC Scores (Area) and Precision-Recall Curves/Average Precision Scores (Area). RF and Xgb clustered together at a lower TPR and precision - Classiﬁcation Task (better seen in color). We tested the Copula MONDE (Section 2.3) and PUMONDE (Section 2.4 and 2.4.1) models on a prob- lem of detecting events falling at the tail of a distribu- tion which, for a ﬁxed threshold deﬁning the tail, can be compared against standard classiﬁers. We use foreign exchange ﬁnancial data described in section S4.5. Data for the experiment was prepared as follows: 1) Sample one minute negative log returns of 12 ﬁnancial instru- ments. At each time ti, we obtain a 12 element vector r(ti) = log p(ti−1) − log p(ti), where p(ti) is the vec- tor of 12 instruments mid prices at time ti. Each r(ti) represents a vector of 1 minute losses. 2) For each ti, we collect y = r(ti)10,11,12 and x = r(ti)1..9, r(ti−1). This composition of data encodes a 3 dimensional response variable representing 1 minute loss from the last 3 instru- ments at time ti and covariates are 1 minute losses from the rest of the instruments at time ti, combined together with the previous period ti−1 1 minute losses from all the instruments (x is 21 dimensional vector). We train our estimators on such constructed data by maximizing the log-likelihood function. Table 1: Mean Loglikelihoods - Large UCI Datasets. Power Gas Hepmass Miniboone Bsds300 MADE MoG 0.40 ± 0.01 8.47 ± 0.02 −15.15 ± 0.02 −12.27 ± 0.47 153.71 ± 0.28 MAF-afﬁne (5) 0.14 ± 0.01 9.07 ± 0.02 −17.70 ± 0.02 −11.75 ± 0.44 155.69 ± 0.28 MAF-afﬁne (10) 0.24 ± 0.01 10.08 ± 0.02 −17.73 ± 0.02 −12.24 ± 0.45 154.93 ± 0.28 MAF-afﬁne MoG (5) 0.30 ± 0.01 9.59 ± 0.02 −17.39 ± 0.02 −11.68 ± 0.44 156.36 ± 0.28 TAN (various architectures) 0.48 ± 0.01 11.19 ± 0.02 −15.12 ± 0.02 −11.01 ± 0.48 157.03 ± 0.07 NAF 0.62 ± 0.01 11.96 ± 0.33 −15.09 ± 0.40 −8.86 ± 0.15 157.73 ± 0.04 B-NAF 0.61 ± 0.01 12.06 ± 0.09 −14.71 ± 0.38 −8.95 ± 0.07 157.36 ± 0.03 MONDE MADE 0.62 ± 0.01 12.12 ± 0.02 −15.83 ± 0.06 −10.7 ± 0.46 153.17 ± 0.29 We want to assess the models’ ability to correctly rank tail events of any of the 3 assets experiencing loss at least in the 95 percentile of the historical loss in the next minute. To do this, we obtain the 95-th percentile thresh- old for each dimension of the y measured on the training set: y95. We compute the labels on the test partition as: l = 1(y1 > y95 1 ∨ y2 > y95 2 ∨ y3 > y95 1 ) i.e. the la- bel is 1 whenever value at any of the dimensions is larger then its 95-th percentile, otherwise is 0. We compute the test score for the trained estimator by feeding it with test set covariates x and plugging in y95 as the response vector (the same response vector for each test covariate vector). The CDF output from the model is the estimate of the probability P (Y ≤ y95|x) = F (y95|x). We es- timate the tail probability of the label being equal 1 i.e. P (L = 1 | x) = P (Y1 > y95 1 ∨ Y2 > y95 2 ∨ Y3 > y95 1 | x) = 1 − F (y95|x). Such computed ranks and the true labels are used to compute the Receiver Operating Characteristic curve, Area Under Curve score, Precision- Recall curve and Average Precision score on the test par- tition of the dataset. These performance measures are used to compare our estimators to a multilayer percep- tron with sigmoid outputs, Random Forests and Gradient Boosting Trees (Chen and Guestrin, 2016). These dis- criminative methods are trained directly on labels pre- deﬁned before training. Our estimators do not have to use a particular threshold at a test time. It can be changed after training is completed which is not possible for dis- criminative models 6. ROC plots are shown in Figure 5. ROC curves for the XGBoost and Random Forest classiﬁers cluster at the lower level of TPR for small values of FPR. The other models have ROC curves placed slightly higher. We see that results for all models are similar, where the multilayer perceptron and PUMONDE models achieved slightly higher AUC score than the rest of the classiﬁers. PR plots are shown in Figure 5. PR curves and aver- age precision score (labelled “Area” in the legend of the Figure) tell a similar story. In conclusion, we showed ev- idence that our method is competitive in this task against 6This is analogous to a Bayesian Network providing the an- swer to any query, as opposed to a specialized predictor ﬁt to answer a single predeﬁned query. black-boxed models ﬁnely tuned to a particular choice of threshold, but where we can instantaneoulsy re-evaluate classiﬁcations by changing the decision threshold with- out retraining the model. This is not possible with the baseline models, which are also less interpretable as they do not show how the distribution of the original continu- ous measurements changes around the tails. 4.3 TASK III: TAIL DEPENDENCE In this experiment, we assess whether our models can be used to estimate a measure of extreme dependence be- tween two random variables Yi and Yj, tail dependence (Joe, 1997): λL(u) = lim u→0+ P (Yi ≤ F −1 i (u)|Yj ≤ F −1 j (u)) = lim u→0+ P (Yi ≤ F −1 i (u), Yj ≤ F −1 j (u)) P (Yi ≤ F −1 i (u)) λR(u) = lim u→1− P (Yi > F −1 i (u)|Yj > F −1 j (u)) = lim u→1− P (Yi > F −1 i (u), Yj > F −1 j (u)) P (Yi > F −1 i (u)) = lim u→1− 1 − 2u + Fij(F −1 i (u), F −1 j (u)) 1 − u , where λL(u) and λR(u) are lower and upper tail depen- dence indices respectively, Fi is the marginal distribu- tion function for random variable Yi, Fij is the bivariate marginal for random variables Yi and Yj. In our experi- ment we use conditional distributions so distributions de- pend on covariates: Fi(yi | x) and Fij(yi, yj | x). In order to have ground truth and provide some inter- pretability, we generate synthetic data as follows. We sample a Bernoulli random variable C ∈ {0, 1} that indi- cates which of two components generates the covariates X. The components are two Gaussian multivariate dis- tributions with different means and identity covariance matrices. The choice of component also generates re- sponse variables Y. In this case, the two distributions are such that the ﬁrst is normally distributed (no tail depen- dence) and the second is t-distributed with 2 degrees of freedom. We repeat this process independently for each point in the dataset: C ∼Bernouli(0.5) X ∼Xc X0 ∼N ((−2, −3), I) X1 ∼N ((2, 5), I) Y ∼Yc Y0 ∼N ((0, 0, 0), Σ) Y1 ∼t((0, 0, 0), 2, Σ) Σ =σPσ σ =   0.4 0 0 0 0.5 0 0 0 0.8   P =   1.0 0.8 0.1 0.8 1.0 −0.5 0.1 −0.5 1.0   . To illustrate concentration in the tails of the distribu- (a) Gaussian Component (b) T Component Figure 6: Tail Dependence Concentration Plots (better seen in colour). The triangle shaped curve in each plot is the tail dependence concentration plot for isotropic Gaussian (shown as comparison for curves depicting de- pendence and larger kurtosis). The ﬁrst plot shows that all models correctly capture the lack of tail dependence in Gaussian distribution. The second plot shows that only PUMONDE and MAF concentration plots are close to the data concentration plot in the tails (when u tends to 0 or 1) tion, we plot ˆλL(u) for u ∈ (0, 0.5) and ˆλR(u) for u ∈ (0.5, 1) in Figure 6. This includes models presented in this paper and also two baseline models (MAF and MDN). We describe the procedure used to compute these estimators in Section S4.6. We present two concentration plots. The ﬁrst one is for the response variable gener- ated from the multivariate Gaussians i.e. the ﬁrst com- ponent, which does not exhibit tail dependence. This can be observed in the curves which tend to 0 when u gets closer to 0 and 1. The second one which depicts concen- tration plots for mixture component generated from the t-distributed sample with 2 degrees of freedom clearly present tail dependence which can be noticed by their limit converging to 0.6. We can see that Copula and MDN models fail to capture tail dependence (the ﬁrst as expected, but the latter somehow has issues approximat- ing non-Gaussian tails with a mixture of Gaussians). The PUMONDE model trained with composite likelihood (as described in Section 2.4.1) and MAF model are able to capture “fatter” tails in the data. 4.4 TASK IV: MUTUAL INFORMATION Table 2: Mutual Information. Gaussian Component T Component Model I(Y0, Y1) I(Y0, Y2) I(Y1, Y2) I(Y0, Y1) I(Y0, Y2) I(Y1, Y2) Data 0.5108 0.0057 0.1454 0.5108 0.0057 0.1454 MAF 0.5107 (0.0001) 0.0018 (−0.004) 0.1827 (0.0373) 0.5786 (0.0677) 0.0831 (0.0774) 0.199 (0.0536) MDN 0.5172 (0.0064) 0.0359 (0.0301) 0.1718 (0.0264) 0.6112 (0.1004) 0.1143 (0.1085) 0.2356 (0.0901) MONDE Const 0.4826 (−0.0283) 0.0078 (0.0021) 0.1304 (−0.015) 0.5363 (0.0255) 0.0414 (0.0357) 0.1431 (−0.0024) MONDE Param 0.5078 (−0.003) 0.0796 (0.0738) 0.1303 (−0.0151) 0.438 (−0.0728) 0.0849 (0.0792) 0.1268 (−0.0186) PUMONDE 0.4682 (−0.0425) 0.004 (−0.0017) 0.1105 (−0.0349) 0.5307 (0.0199) 0.0573 (0.0515) 0.1621 (0.0167) Pairwise mutual information measures how much infor- mation is shared between two random variables. It cap- tures not only linear dependency, but also more com- plex relations. It is deﬁned as KL-divergence between the joint bivariate marginal distribution and the prod- uct of the corresponding univariate marginals. We now show results concerning estimation of pairwise mutual information. The data generating process is the same as in Section 4.3. We compute mutual information by marginalizing distributions provided by the models using numerical quadrature. We can apply this method because of the low dimensionality of the problem. Mutual information was computed for each pair of vari- ables for the data generating process, two baseline mod- els (MDN and MAF) and two of our models (Gaus- sian copula MONDE, PUMONDE). We compute mutual information for each mixture component separately by conditioning each model on the covariate equal to the mean vector for the given covariate mixture component. The results are presented in Table 2. For each combi- nation of model/pair of response variables/mixture com- ponent, we obtain two values: mutual information score and the absolute difference between mutual information value for the model and value for the data generating pro- cess (the difference is shown in brackets). The small- est absolute value of the difference in a given column is highlighted in bold, indicating which model represents the closest mutual information to the one computed from the data generating process. MONDE models are better in ﬁve out of six cases. We can conclude that models pre- sented in this paper are competitive in encoding bivari- ate dependency with the current state of the art methods. Having this evidence leads us to our ﬁnal Task, where we exploit estimating simultaneously multiple marginals of a common joint. CDF parameterizations are partic- ularly attractive, as marginalization takes the same time as evaluating the joint model (Joe, 1997), unlike some of the methods discussed in this section. 4.5 TASK V: BIVARIATE LIKELIHOOD Table 3: Bivariate Likelihood Model Comparison. MDN MONDE Const MONDE Param PUMONDE MDN N A 0 0 0 MONDE Const 210 N A 27 0 MONDE Param 210 183 N A 0 PUMONDE 210 210 210 N A In many practical problems, we are interested in estimat- ing only particular marginals. Parameters for higher or- der interactions are considered to be nuisance parame- ters. Allowing for partial likelihood speciﬁcation is one of the primary motivations behind composite likelihood (Varin et al., 2011) 7. The problem with partial speciﬁcation is that in general there are no guarantees that the corresponding marginals come from any possible joint distribution. On the other hand, a fully speciﬁed likelihood has nuisance parame- ters. Ideally, we would like a ﬂexible, overparameterized joint model so that parameters are not obviously respon- sible for any marginals a priori, with the objective func- tion regularizing them towards the marginals of inter- est. PUMONDE provides such an alternative. Although high dimensional likelihoods are intractable to compute in PUMONDE, low dimensional marginals are not. In this section, we test the ability of our models to encode coherent bivariate dependence in the data for problems 7This is not to be confused with another motivation, which is to provide a tractable replacement for the likelihood func- tion. In this case, a full likelihood is still speciﬁed and of in- terest. While computational tractability is a more common mo- tivation in machine learning, partial speciﬁcation is one of the main reasons for the development of composite likelihood in the statistics literature. of larger dimensionality. For example, in ﬁnance this can be useful to model second order dependence of returns in the portfolio of instruments as used in computation of the Value at Risk metric (Holton, 2003). We use foreign ex- change ﬁnancial data as described in section S4.5. This data contains a 21 dimensional response variable repre- senting 1 minute losses for ﬁnancial instruments at time ti and a 21 dimensional covariate variable representing 1 minute losses for all the instruments at time ti−1. We train the estimators on such constructed data maximiz- ing the likelihood objective. For PUMONDE, we op- timize the composite likelihood objective comprised of the sum of all combinations of bivariate likelihoods. The only neural density estimator we use is MDN, ﬁt to the 21 dimensional distribution. Marginalization in MDN is easy as it encodes a mixture of Gaussians, while the other baseline models cannot be easily marginalized. To assess model performance, we compute the average log-likelihood for each bivariate combination of response variables on the test partition, giving 210 unique pairs. Each cell of Table 3 contains the number of times the average log-likelihood computed for each bivariate com- bination of response variable was larger for model shown in the row compared to the model which is shown in the column. We can see that the best performing model on this test is the PUMONDE which obtained larger test log likelihoods in all 210 cases when compared to each other model. MONDE with parametrized covariance achieved better results than MDN and MONDE with constant co- variance. The worst results were obtained by MDN. 5 DISCUSSION We proposed a new family of methods for represent- ing probability distributions based on deep networks. Our method stands out from other neural probability es- timators by encoding directly the CDF. This comple- ments other methods for problems where the CDF rep- resentation is particularly helpful, such as computing tail area probabilities and computing small dimensional marginals. As future work, we will exploit its relation- ship to graphical models for CDFs (Huang and Frey, 2008; Silva et al., 2011), using PUMONDE to param- eterize small dimensional factors. Variations on soft- recursive partitioning, such as hierarchical mixture of ex- perts (Jordan and Jacobs, 1994), can also be implemented using tail events to deﬁne the partitioning criteria. An- other interesting and less straightforward venue of future research is to exploit approximations to the likelihood based on the link between differentiation, latent variable models and message passing, as exploited in the con- text of graphical CDF models (Silva, 2015; Huang et al., 2010) and automated differentiation (Minka, 2019). References C. Bishop. Mixture density networks. Technical report, NCRG 4288, Aston University, Birmingham, 1994. T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In KDD, 2016. N. De Cao and I. Titov. Block neural autoregressive ﬂow. In UAI, 2019. L. Dinh, D. Krueger, and Y. Bengio. NICE: Non-linear inde- pendent components estimation. In ICLR, 2015. L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using Real NVP. In ICLR, 2017. G. Elidan. Copulas in machine learning. In Copulae in Mathe- matical and Quantitative Finance. Springer, Berlin, 2013. M. Germain, K. Gregor, I. Murray, and H. Larochelle. MADE: Masked autoencoder for distribution estimation. In ICML, 2015. G.A. Holton. Value-at-risk: Theory and Practice. Academic Press Inc, 2003. C-W. Huang, D. Krueger, A. Lacoste, and A. Courville. Neural autoregressive ﬂows. In ICML, 2018. J. Huang and B. Frey. Cumulative distribution networks and the derivative-sum-product algorithm. In UAI, 2008. J. Huang, N. Jojic, and C. Meek. Exact inference and learning for cumulative distribution func- tions on loopy graphs. In NIPS, 2010. H. Joe. Multivariate models and dependence concepts. Mono- graphs on statistics and applied probability. CRC Press, 1997. M. Jordan and R. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6:181–214, 1994. T. Minka. From automatic differentiation to message pass- ing. Invited talk at the Advances and challenges in Machine Learning Languages workshop (ACMLL 2019), 2019. URL https://tminka.github.io/papers/ acmll2019/. J. Oliva, A. Dubey, M. Zaheer, B. P´oczos, R. Salakhutdinov, E. Xing, and J. Schneider. Transformation autoregressive networks. In ICML, 2018. G. Papamakarios, T. Pavlakou, and I. Murray. Masked autore- gressive ﬂow for density estimation. In NIPS, 2017. T. Schmidt. Coping with copulas. In Copulas – From Theory to Applications in Finance, pages 3–34. Risk Books, 2006. R. Silva. Bayesian inference in cumulative distribution ﬁelds. Interdisciplinary Bayesian Statistics, pages 83–95, 2015. R. Silva, C. Blundell, and Y-W. Teh. Mixed cumulative distri- bution networks. In AISTATS, 2011. A. Sklar. Fonctions de r´epartition `a n dimensions et leurs marges. Publications de l’Institut de Statistique de l’Universit´e de Paris, 8, 1959. B. Uria, I. Murray, and H. Larochelle. RNADE: the real-valued neural autoregressive density-estimator. In NIPS, 2013. B. Uria, I. Murray, and H. Larochelle. A deep and tractable density estimator. In ICML, 2014. C. Varin, N. Reid, and D. Firth. An overview of composite likelihood methods. Statistica Sinica, 21(1), 2011.","libVersion":"0.3.1","langs":""}
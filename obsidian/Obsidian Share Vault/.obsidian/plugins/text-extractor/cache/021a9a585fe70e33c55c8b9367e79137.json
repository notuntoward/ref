{"path":"lit/lit_notes_OLD_PARTIAL/Cahyawijaya24humanValuesVecLLM.pdf","text":"High-Dimension Human Value Representation in Large Language Models Samuel Cahyawijaya∗ HKUST Hong Kong scahyawijaya@connect.ust.hk Delong Chen ∗ HKUST Hong Kong delong.chen@connect.ust.hk Yejin Bang ∗ HKUST Hong Kong yjbang@connect.ust.hk Leila Khalatbari HKUST Hong Kong lkhalatbari@connect.ust.hk Bryan Wilie HKUST Hong Kong bwilie@connect.ust.hk Ziwei Ji HKUST Hong Kong zjiad@connect.ust.hk Etsuko Ishii HKUST Hong Kong eishii@connect.ust.hk Pascale Fung∗ HKUST Hong Kong pascale@ece.ust.hk Abstract The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling. 1 Introduction The remarkable capabilities of Large Language Models (LLMs) have revolutionized general-purpose AI assistants leading to their widespread adoption in many tasks and fields [1, 2, 3, 4]. This newfound power comes with the responsibility of ensuring that these AI assistants align with human values. Numerous efforts have been made to imbue AI systems with ethical principles and moral values, from designing robust frameworks for value alignment [5, 6, 7] to incorporating diverse perspectives into training data [8, 9, 10, 11, 12]. The ability to adhere to ethical and societal values has become a critical factor in developing LLMs as important as the quality and generalization on performing tasks effectively [13, 14]. One of the most important methods to align LLMs with human values is ∗ These authors contributed equally. Preprint. Under review.arXiv:2404.07900v1 [cs.CL] 11 Apr 2024 Reinforcement Learning with Human Feedback (RLHF) [5] where training an AI reward model using human feedback, which is then employed as a reward function to refine policies via reinforcement learning (RL). It has been shown that RLHF is able to inject human preferences into LLMs. Another innovation in this area is to replace the human annotators in RLHF with an AI model. This method, named RLAIF or Constitutional AI [7] uses AI models for making annotated preference data based on a list of principles (constitution) curated by humans. These methods are combined with various safety fine-tuning (SFT) methods to ensure that LLMs are more fair, less toxic, and align with human and societal preferences and values. Human values and preferences can range from (1) high level ethical principles such as those under the \"Univerasl Declaration of Human Rights\" signed by 192 member states of the United Nations, to (2) more culturally specific values found in various moral philosophy schools such as the Enlightenment values in the West, Confucian values in East Asia, Hindu or Islamic values in many countries in the world; to (3) laws and regulations in various jurisdictions such as the lèse-majesté law in Thailand or the GDPR in the EU; to (4) social etiquette and best practices in various human societies and professional settings; to (5) domain-specific human preferences such as \"empathy\" for health assistants and \"helpful\" for customer service agents, etc. These human values and preferences can originate from long philosophical traditions, societal and professional consensus. They form the building blocks of all the AI regulations and guidelines published by different policy bodies today. LLMs, trained from vast amounts of data in different languages, are pre-trained to incorporate the values represented in those data in the first place. RLHF adds a further step in crowd-sourcing values and preferences from human annotators by modifying the outcome of LLMs. However, studies have shown that there are inconsistencies in values provided by human annotators due to the nature of crowd-sourcing and the fact that there are different efforts in RLHF for different languages in the LLMs [15, 16, 17]. Whereas the majority of English language LLMs produced by North American institutions tend to manifest American coastal liberal values [18], and those from Chinese institutions might incorporate additional Chinese values [19, 20, 21, 22], it is not at all clear what values and preferences have been pre-trained in all the LLMs released. Do different models reflect consistent values in a given language and culture? Does a single model embody different values in different languages? Are values transferable between models and languages? Even at release time, the producers of LLMs lack such a representative view of the values in the models they have released and whether their models do indeed align with the desirable values. To better understand the human values of LLMs, one can use surveys of human values to query LLMs [13, 14, 23, 24]. Surveys can be seen as a kind of sampling in the value distribution space of an LLM. However, we argue that survey answers are a limited sampling method as they only cover a limited amount of dimensions. For instance, the dimension of cultural values [25, 26] only captures 6 dimensions to map a vast variability in human cultures, while the theory of basic values [27, 28, 29] and the World Value Survey (WVS) [30, 31, 32], only cover 19 and 10 dimensions of values, respectively. We argue that such a low-dimension semantic representation will likely fail to give us a view of the full picture of human values in an LLM. Instead, what we would like to have is a high dimension representation of human value distribution in LLMs to reflect the complexity of the embedded values in LLMs. This representation needs to be orthogonal to language and model architecture. In this paper, we propose UniVaR - a high dimensional representation of human values in LLMs that is language and model invariant. We show that UniVaR representations show us the distances and similarities between different cultures in terms of human values as reflected in the LLMs. UniVaR offers a systematic and statistical approach to understanding the value systems of LLMs. UniVaR facilitates the exploration of how different LLMs learn and prioritize various values in different languages and is ultimately a powerful tool for more transparent and accountable LLMs. By bridging the gap between the capabilities of LLMs and the imperative of aligning them with human values, UniVaR represents a significant step forward in the quest for ethically sound AI assistants. The significance of our work can be summarized as follows: 1. We are the first to develop a theoretical formulation for understanding values in LLMs using a high-dimensional abstract representation of values 2. We introduce UniVaR, a scalable end-to-end learnable method for understanding values of LLMs in a high-dimensional space allowing a better generalization across different values 3. Using the high-dimensional value representation from UniVaR, we are the first to show a map of human value distributions across different LLMs in different languages and cultures 2 2 Related Work 2.1 Value Alignment in LLMs Aligning LLMs with humans to enhance their service and mitigate risks has become a major focus [33]. There are three major goals [8] to LLMs alignment: 1) Teach the model to follow human instructions enhancing model capability to follow user intent and solve diverse tasks [5]; 2) Align the model with implicit human preferences [34]; 3) Align LLMs to a set of pre-defined principles reflecting the values of human individual and society [7]. Value alignment is commonly done in two phases, i.e., supervised fine-tuning (SFT) and reinforcement learning with human or AI feedback (RLHF/RLAIF). In SFT, the model is fine-tuned by consuming a set of curated conversation data complying with human desired attributes such as human preferences or values [10, 35, 36, 37]. The selection of high- quality, diverse data is substantial in determining the alignment quality [8, 9, 10, 11, 12]. The model can be fine-tuned using the standard language modeling loss or other training paradigms such as contrastive learning [38, 39] and distillation [40]. RLHF [5, 6, 41] is an essential alignment technique applied by the majority of recent LLMs [42, 43] before they are released for public access. RLHF is achieved through reinforcement learning methods such as PPO [44] where models receive feedback from a value-aligned reward model, guiding them to adjust their policies. Recently, DPO [45] is introduced to alleviate the need for a reward model in the loop for human alignment. Unlike RLHF, RLAIF generates feedback based on the model itself, reducing reliance on manual annotation [46, 47, 48, 49, 50]. In RLHF, preferences are implicit as they are elicited from ranking data pairs, making it difficult for LLMs to generalize to explicit principles. Some approaches like Constitutional AI [7] establish explicit principles or ’constitutions’ for AI, enhancing model alignment with human values through self-critique and modification of responses. 2.2 Surveying Human Values in LLMs Early attempts to understand the ethics knowledge of human values from language models includ- ing the ETHICS dataset [51] composed of open-world scenarios about justice, deontology, virtue ethics, utilitarianism, and commonsense moral judgments. As a further step forward, Zhang et. al. (2023) [24] investigated what LLMs “know” their responses belong to a particular value category and what reasons lead to them falling into that category as a whole. To explore how LLM behavior mirrors that of humans, a line of work evaluates to what extent a model correctly exhibits opinions, stances, or positions of different social groups. Durmus et. al. (2023) [13] explores positional alignment of global opinions adopting Pew Global Attitudes Survey (PEW) and the World Values Survey (WVS) [30, 31, 52]. Alkhamissi et. al. (2024) [53] focuses on Egypt and the United States cultural alignment, by also using WVS. Zhang et. al. (2024) [14] adopts a social value orientation (SVO) [54] and SVO slider measures to assess the alignment between the real value mapped with LLMs’ behaviors by SVO with its human-aligned value. This line of work focuses on measuring the alignment of LLMs with a focus on equitable representation of different people’s opinions. Meanwhile, our work focuses on the methodology of capturing the complex human values in higher dimensional space, which will allow an understanding of the value-eliciting spaces of LLMs and ultimately alignment verification as well as transferring. 2.3 High-Dimension Embedding Representation The concept of representing entities through distributed representations, as initially proposed by [55], revolutionized machine learning by facilitating the encoding of input similarities into a high- dimensional vector space. This paradigm shift underpinned subsequent advances in embeddings, enabling algorithms to capture nuanced semantic relationships and significantly enhance generaliza- tion capabilities. Seminal works in NLP laid the groundwork for word embeddings [56, 57, 58, 59]. This progress was further accelerated by [60, 61], who refined methods to generate word vectors, subsequently enriching research on sub-word and sentence-level embeddings [62, 63, 64]. In parallel, computer vision benefited from embedding techniques to capture object representations [65, 66, 67], with recent expansions into sub-object embeddings [68] demonstrating the approach’s versatility. Embeddings have also been applied in healthcare and personalized recommendation systems to model complex behaviors and preferences [69, 70]. Our work further extends the embedding paradigm to explore abstract value representations elicited by Large Language Models (LLMs), advancing our 3 understanding of embedding representations in encapsulating complex data and enhancing machine learning applications in LLM interaction. 3 Our approach: Universal Value Representation (UniVaR) 3.1 Problem Formulation In this paper, we assume that some factors in an LLM contribute towards aligning with certain human values while others towards value-agnostic aspects (e.g., wording, syntax, or style) during text generation. Our goal is to extract the value-pertinent factors from LLMs. By doing so, we can analyze similarities of values from different LLMs, transfer values across multiple LLMs, or align values with human preference without sacrificing the value-agnostic factors. Let an LLM parameterized by θ be fθ. Then, our assumption above can be formalized as θ = ϕ(ϑvalue, ϑother) with some function ϕ, where ϑvalue is the value-decisive factors and ϑother is the value-agnostic factors of the LLM. If we know ϕ and are able to derive the inverse function ϕ−1, we can directly obtain ϑvalue from θ. However, it seems not feasible because ϕ is unknown, and there is no clue how to estimate it. The relationship and behaviors of ϑvalue and ϑother are mostly unknown. Even if we could get ϕ, the order of magnitude of θ may prohibit us from computing ϕ−1. Following the information bottleneck principle for representation learning [71, 72], we consider a surrogate task named value embedding learning [73]. In value embedding learning, we learn a representation Z that contains the value-decisive information in a self-supervised way. Definition 1 (Value embedding learning). The goal of value embedding learning is to extract the information about the value-decisive factors ϑvalue and represent it as a vector Z – the value embedding. The overall objective can be written as: max Z I(ϑvalue; Z) ︸ ︷︷ ︸ maximizing dependency − H(Z) ︸ ︷︷ ︸ minimizing redundancy , (1) where I and H denote mutual information and entropy, respectively. In Eq. 1, the first term is to extract as much information as possible from ϑvalue, and the second term is to regularize Z to discard as much irrelevant, redundant information as possible. 3.2 Value Eliciting Question Answering In the value embedding learning, the core challenge lies in the fact that ϑvalue exists as a latent variable [74, 75]. What we can observe is a query and an output in text from an LLM, not ϑvalue itself. In this work, we propose to apply question answering to gather observations on behaviors about the values of LLMs. Let Q denote a question and A an answer which is generated by an LLM. As we described in Section 3.1, ϑvalue may or may not be involved in generating A, depending on the nature of Q. For example, a question asking for a simple arithmetic operation would be highly dependent on the reasoning capabilities represented by the value-agnostic ϑother, while ϑvalue hardly matters. On the other hand, a question that involves an ethical dilemma such as the trolley problem should depend on ϑvalue. Since our interest lies on values, we consider a set of questions Qvalue that elicit values: Definition 2 (Value eliciting question). Given LLMs f i θ and f j θ where ϑi value ̸= ϑj value, a question Q ∈ Qvalue leads the LLMs to generate different answers: Ai = f i θ(Q) ̸= Aj = f j θ (Q). We call this set of questions Qvalue as value elicit questions. If a QA pair ⟨Q, A⟩ satisfies Q ∈ Qvalue, I(ϑvalue; ⟨Q, A⟩) > 0 holds by Definition 2 2. However, a single pair is not representative enough of the distribution of all the ϑvalue. Indeed, it is impossible to extrapolate the entirety of human values from a single QA. For instance, even a broad question such 2By definition, mutual information I(ϑvalue; ⟨Q, A⟩) = DKL(P (⟨Q, A⟩, ϑvalue)||P (⟨Q, A⟩)P (ϑvalue)), where the KL divergence DKL is always non-negative and is zero if two distributions are identical. Since ⟨Q, A⟩ and ϑvalue are dependent, their joint distribution is different from the product of their marginal distributions, we can know I(ϑvalue; ⟨Q, A⟩) > 0. 4 𝑄𝑄1 𝑄𝑄2 𝑄𝑄𝑚𝑚… 𝐴𝐴1 𝐴𝐴2 𝐴𝐴𝑚𝑚… value eliciting QA set 𝒳𝒳 = 𝑄𝑄𝑗𝑗, 𝐴𝐴𝑗𝑗 𝑗𝑗=1 𝑚𝑚 value eliciting questions 𝒬𝒬value 𝑄𝑄3 𝐴𝐴3𝑓𝑓𝜃𝜃value decisive 𝜗𝜗value value agonistic 𝜗𝜗other parametrize 𝜃𝜃 = 𝜙𝜙(𝜗𝜗value, 𝜗𝜗other) question 𝑄𝑄 answer 𝐴𝐴 LLM 𝑓𝑓𝜃𝜃 Goal: learn a value embedding 𝑍𝑍 s.t. max 𝐼𝐼 𝜗𝜗value; 𝑍𝑍 − 𝐻𝐻(𝑍𝑍) Section 3.1 Problem Formulation Section 3.2 Value Eliciting Question Answering Section 3.3 Value Embedding Learning 𝒳𝒳1 𝒳𝒳2 𝑍𝑍𝒳𝒳1 𝑍𝑍𝒳𝒳2 generate views from 𝒳𝒳 = 𝑄𝑄𝑗𝑗, 𝐴𝐴𝑗𝑗 𝑗𝑗=1 𝑚𝑚 ∈ 𝒳𝒳all maximizing MI max 𝐼𝐼(𝑍𝑍𝒳𝒳1, 𝑍𝑍𝒳𝒳2) 𝑔𝑔 𝑔𝑔share weights Figure 1: UniVaR directly extract LLMs values in a high-dimensional space without the needs of any prior survey knowledge, allowing better scalability for extracting, understanding, and manipulating values of LLMs. Left: our objective is to learn a value embedding Z that represents the latent value-relevant factor ϑvalue in LLM fθ that determine its values. Middle: we elicit LLM values through question answering, such that the hidden value-relevant aspect ϑvalue is expressed by the distribution of its value eliciting QA set X . Right: we apply multi-view self-supervised learning to compress the redundant information in the QA set. By maximizing the mutual information across views, the value-relevant aspect is extracted, while irrelevant information is eliminated. as “What is the meaning of life?” or “What is the ideal society?” can only elicit values that are related to terminal values [76, 77] and cultural values [25, 26], while neglecting other aspects of human values. It is also very difficult to improve the coverage of a single QA, due to the limitation of LLMs’ context length, instruction following ability, and the challenges in prompt engineering – optimizing Q towards maximizing I(ϑvalue; ⟨Q, A⟩) is difficult, since there is no off-the-shelf optimizer that can operate well in the discrete textual space. While adding more QA pairs may elicit different values in LLM, we will eventually reach the point where all the values have been comprehensively elicited, and adding further questions will not contribute any additional information. Let’s denote a set of QA pairs as {⟨Qj, Aj⟩| Qj ∈ Qvalue, Aj = fθ(Qj)} m j=1. We define the sufficient condition of a value elicit QA set as follows: Definition 3 (Sufficient value eliciting QA set). We call {⟨Qj, Aj⟩}m j=1 as a sufficient value eliciting when adding any QA pair ⟨Qm+1, Am+1⟩ ̸∈ {⟨Qj, Aj⟩}m j=1 yields no additional information in terms of LLM values, i.e., I(ϑvalue; {⟨Qj, Aj⟩}m+1 j=1 ) = I(ϑvalue; {⟨Qj, Aj⟩}m j=1). 3.3 Value Embedding Learning In this section, we introduce the value embedding learning process of in Definition 1 with a value eliciting QA set. Let X be a sufficient value eliciting QA set, and Xall = {X1, X2, . . .} be the set of all possible X . Note that there can be multiple combinations of QA pairs that satisfy the sufficiency condition inDefinition 3 3. As discussed in Section 3.2, X will give enough guidance for the first term in Eq. 1 to maximize dependency. However, X may have redundancy because ⟨Q, A⟩ is in natural language and does not explicitly express the certain value of an LLM. Inspired by [73], we apply multi-view self-supervised learning to eliminate the redundancy in X while keeping value-relevant information intact. For each LLM’s values, we first generate its multiple views by selecting different subsets of QA pairs from its sufficient value eliciting QA set (Definition 3), then we adopt a Joint Embedding Predictive Architecture (JEPA) [78], which includes a Siamese network that learns to extract the shared information (the values of one LLM) across different views into a high-dimensional continuous vectors space, and discard redundant non-shared information. To begin with, we first restate the multi-view assumption introduced by previous works [73, 79]. 3In the computer vision field, the views can be images of one person’s face taken from different angle. Intuitively, our view generation process can be interpreted as taking several “pictures” of one LLM’s values. 5 Assumption 1 (Multi-view Assumption). For two views drawn from a sufficient value eliciting QA set X1, X2 ∈ Xall of an LLM parameterized by θ = ϕ(ϑvalue, ϑother), there is a small ϵinfo > 0 such that I(ϑvalue; X2|X1) ≤ ϵinfo, I(ϑvalue; X1|X2) ≤ ϵinfo. We embed X1 and X2 into a vector space, such that the resulting representation maximize the mutual information across two views, i.e., max I(X1, X2) . Specifically, we apply a neural network g that takes X1 and X2 as input and produce representations ZX1 = g(X1) and ZX2 = g(X2). We obtain g by maximizing the following objective: max I(ZX1; ZX2 ). (2) Maximizing mutual information between ZX extracted from multiple views requires capturing underlying the factors whose influence spans multiple views, while excluding non-shared factors. Consider an LLM asked two trolley problem-style questions in different scenarios. While its underlying value judgment, e.g., utilitarianism vs. deontology, would be consistent across the two QA pairs (thus will be captured by g and keep the ZX as an informative value embedding for ϑvalue), the specific phrasing, word choice, sentence structure etc. used in each response would be independent (thus will be discarded by g and make the ZX a compact vector that does not contain irrelevant information). We apply the InfoNCE loss function [80] to maximize the objective function in Eq. 2, but other alternatives can be also used [81, 82, 83, 84, 85, 86]. The InfoNCE loss function encourages the embeddings to be similar for views from the same LLM and to be dissimilar for views from different LLMs. Given a batch of B view pairs from a n of LLMs, the InfoNCE loss for a positive pair Z (i) X1 , Z (i) X2 is: LInfoNCE = − 1 B B∑ i=1 log exp(sim(Z (i) X1 , Z (i) X2 )/τ ) ∑B j=1 exp(sim(Z (i) X1 , Z (j) X2 )/τ ) , (3) where sim(·, ·) is a similarity function, τ is a temperature, and B is the batch size. Minimizing LInfoNCE maximizes a lower bound on the mutual information I(ZX1 ; ZX2): I(ZX1; ZX2) ≥ log(B) − LInfoNCE. (4) This encourages the network g to capture the shared information about the LLM’s values ϑvalue while discarding irrelevant view-specific details. The quality of this approximation improves as the batch size B increases. 4 Experiment Design 4.1 Constructing The Value Eliciting QA Training Set The overview of our value-eliciting QA pipeline is described in Figure 2. In detail, to construct the set of value-eliciting questions Qvalue, we collect reference human values from multiple human value studies including world value survey [30, 87, 31], cultural dimensions theory [25, 88, 89, 26], theory of basic human values [90, 27, 91, 92, 93, 94, 95], the refined theory of values [29] and Rokeach value survey [76, 77, 96, 97]. We gathered a set of 87 human values and use them to generate 50 relevant value-eliciting questions Q ∈ Qvalue using LLMs. We conduct manual verification of the generated questions and remove irrelevant questions. We collected a total of 4296 questions across all 87 reference human values. To increase the robustness, we paraphrase each question 4 times resulting in a total data size of 21480 questions. We translate all the questions into 12 languages that are supported by all the LLMs within our study, to elicit values of LLMs across diverse pre-trained languages. We use NLLB-200 (3.3B) [98] to translate the questions into 12 languages. 4 4https://huggingface.co/facebook/nllb-200-3.3B 6 Performance orientation Humane orientation Individualism Collectivism Masculinity & feminity Gender egalitarianism Human Values Value Eliciting Questions Would you rather work overtime to complete a project or delegate tasks to ensure work-life balance? Would you prioritize your own needs or the needs of your community? Are women in your society encouraged to pursue interests outside of their domestic duties? Would you rather work overtime to complete a project or delegate tasks to ensure work-life balance? 您愿意加班来完成项目还是 委派任务以确保工作与生活 的平衡？ لﺎﻤﻛﻹ ﻲﻓﺎﺿﻹا ﻞﻤﻌﻟا ﻞﻀﻔﺗ ﻞھ نﺎﻤﻀﻟ مﺎﮭﻤﻟا ﺾﯾﻮﻔﺗ وأ عوﺮﺸﻤﻟا ؟ةﺎﯿﺤﻟاو ﻞﻤﻌﻟا ﻦﯿﺑ نزاﻮﺘﻟا I would rather delegating tasks to ensure my work-life balance. 我宁愿加班来完成任 务，因为这是我责任 的一部分。 لﺎﻤﻛﻹ ﻲﻓﺎﺿﻹا ﻞﻤﻌﻟا ﻞّﻀﻓأ ﻦﻣ ءﺰﺟ ﺎﮭﻧﻷ ﺔﻤﮭﻤﻟا ﻲﺘﯿﻟوﺆﺴﻣ. Translated Questions I prefer to delegate tasks to ensure work-life balance. I would rather work overtime to complete the task because it is part of my responsibility. I would rather delegating tasks to ensure my work- life balance. LLaMA-2 (English) ChatGLM (Chinese) Jais (Arabic) LLMs Generated Answers Translated Answers Figure 2: The proposed value eliciting QA generation pipeline. English value-eliciting questions are synthesized using a set of human values and the question diversity is enhanced through paraphrasing. Each resultant question is then translated into multiple languages to elicit values of LLMs in various languages. The questions are fed into LLMs to get the value-eliciting QA pair. All QA pairs are translated back into English to minimize the linguistics variation across multilingual QAs allowing a more precise value extraction. The multilingual value-eliciting questions are fed into LLMs to obtain the corresponding value- eliciting answers. To minimize linguistic variations across different languages, all question-answer pairs from languages other than English are then machine-translated into English. This translation step is to eliminate language ID issues that are not relevant to human values. We perform the English translation using the NLLB-200 (3.3B) model. Overall, we collected ∼1.3M QA pairs from 11 LLMs. We use 8 LLMs for training and leave the other 4 as unseen LLMs for evaluations. 4.2 Model and Language Coverage Model Name Preference Tuned Supported Languages Subset Mixtral Instruct (8x7B) 5 ✓ fra, deu, spa, ita, eng Training Aya 101 (13B) [99, 100] 6 ✓ eng, fra, arb, deu, ita, jpn, hin Training zho, vie, tur, spa, ind SeaLLM (7B) [101] 7 ✓ eng, zho, vie, ind Training BLOOMZ RLHF (7B) [102] 8 ✓ eng, zho, fra, spa, arb, vie, hin, ind Training ChatGLM-3 (6B) [20, 19] 9 ✗ zho, eng Training Nous Hermes Mixtral (8x7B) 10 ✓ fra, deu, spa, ita, eng Training SOLAR Instruct [103] 11 ✓ eng Training Mistral Instruct (7B) 12 ✗ fra, deu, spa, ita, eng Training JAIS Chat (30B) [104] 13 ✓ arb, eng Unseen Yi Chat (34B) [22] 14) ✓ zho, eng Unseen LLaMA2 Chat (13B) [42] 15 ✓ eng Unseen ChatGPT [105] 16 ✓ eng, fra, arb, deu, ita, jpn, hin Unseen zho, vie, tur, spa, ind Table 1: List of LLMs incorporated in our UniVaR experiment. For language codes, we adopt the ISO 639-3 standard, i.e., English (eng), French (fra), German (deu), Spanish (spa), Italian (ita), Arabic (arb), Hindi (hin), Vietnamese (vie), Indonesian (ind), Chinese (zho), Japanese (jpn). For building UniVaR, we incorporate 11 off-the-shelf open-source LLMs that are instruction tuned [106, 102, 107, 108] to ensure their ability in answering the given query instead of per- forming sentence completion. We prioritize LLMs that have undergone human value and prefer- ence tuning such as safety tuning [109, 110, 111], reinforcement learning with human feedback (RLHF) [34, 5, 105], direct preference optimization (DPO) [112], etc. The 11 LLMs support a total of 12 languages which are considered high-resource languages. The list of the LLMs and all the languages supported is shown in Table 1. 7 0.5 1.5 2.5 3.5 0 5000 10000 15000 Steps Training 3.5 3.7 3.9 4.1 4.3 4.5 0 5000 10000 15000 Steps 1 5 4 3 2 ℒInfoNCE Validation 1 5 4 3 2 Figure 3: The effect of varying the number of QA pairs k in UniVaR 4.3 Training Hyperparameters For UniVaR training, we adopted a similar hyperparameter setting used for fine-tuning a pre-trained BERT model [113]. The model was trained using AdamW optimizer [114] for 1 epoch with a learning rate of 1e-5 and a linear warmup scheduler with a warmup step of 1000. During training, we use a batch size of 48 for both training and validation. For the view size for our value embedding learning, we adopted varying degrees of the number of QA per view k from [1 . . . 5] and, as shown in Figure 3, we found out that using k = 5 brings the best convergence rate and the best generalization to unseen QAs. All our experiments are conducted on a single NVIDIA Tesla A800 GPU. 5 Analyzing Human Values in LLMs with UniVaR 5.1 Map of human values across LLMs The refined theory of basic values [90, 91, 93, 95, 28] brings up the map of human values to help understand relations in the beliefs, values and motivations of people throughout the world. 17 Inspired 17https://www.commoncause.com.au/values-map-and-definitions Figure 4: Map of values across LLMs and languages in PVQ-RR. Each point represents the centroid of a certain LLM and the color of a certain language. The contour denotes the distribution of the QAs within a certain language. Language code: English (eng), French (fra), Arabic (arb), Indonesian (ind), Chinese (zho), Japanese (jpn). 8 Figure 5: Map of values across LLMs and languages in ValuePrism. Each point represents the centroid of a certain LLM and the color of a certain language. The contour denotes the distribution of the QAs within a certain language. Language code: English (eng), French (fra), Arabic (arb), Indonesian (ind), Chinese (zho), Japanese (jpn). by this theory and the ever-increasing need of human value alignment in LLMs, we introduce a value map of LLMs to visualize the values embedded in LLMs. We leverage two external sources of value-eliciting questions, i.e., PVQ-RR [29] and ValuePrism [115], to build the value map of LLMs using UniVaR. PVQ-RR is the final version of the Portrait Value Questionnaire (PVQ) [27, 92] that is specifically designed to measure values defined in the refined theory of basic values [28]. ValuePrism is the largest and most comprehensive high-quality value dataset collection consisting of 31k human-written situations, each with relevant human values and their corresponding explanations. The two question sources do not originally provide natural questions for LLMs, hence we employ Mixtral 8x7B to generate questions based on the situations provided in the two sources. We then prompt the 12 LLMs used shown in Table 1 to generate responses to the questions. Subsequently, we encode each QA using UniVaR and we visualize the map of LLM values by projecting the QA representations into a 2D plane using TSNE [116, 117]. The result of the value distributions in PVQ-RR are shown in a \"world map\" in Figure 4. To eliminate the influence of language encoding, the non-English answers are all translated into English for this plot. In general, we observe that value QA pairs in the same language from different models are clustered together, showcasing that the values embedded in LLMs largely come from the culture of the language they are trained in. In this case, language acts as a proxy for culture. 5.2 Relation between LLM values and human cultures There is also a separation of value distribution between LLMs in different languages as shown in Table 2. The distance of values across different languages also signifies the similarity and differences of human values between different cultures. For instance, Chinese-Japanese, English-French, and Indonesian-Arabic are closer in value distribution compared to the other language pairs with a relatively distant culture. German and French share similar European values. Chinese and Japanese share similar Confucian and Buddhist values. Indonesian and Arabic cultures share Islamic values. Interestingly, English is relatively far from French and German, despite originating from countries with Western values. We conjecture that this might happen due to the use of English as a lingua franca in many countries and cultures in the world [118, 119, 120, 121]. This pattern is also consistent with the QA representations of ValuePrism shown in Figure 5, indicating that the value representation in UniVaRis robust to the variability of questions. While the values across LLMs in each language are generally closer from one to another, LLMs that are trained from a huge amount of translated data tend to demonstrate similar values between languages – the UniVaR representations across different 9 Lang1 Lang2 ValuePrism PVQ-RR Avg. arb ind 0.219 0.264 0.242 arb jpn 0.282 0.211 0.247 zho jpn 0.421 0.188 0.304 deu fra - 0.324 0.324 fra ind 0.346 0.395 0.370 arb zho 0.459 0.293 0.376 ind jpn 0.424 0.401 0.413 deu ind - 0.424 0.424 arb fra 0.396 0.463 0.429 arb deu - 0.487 0.487 zho ind 0.563 0.505 0.534 arb eng 0.500 0.667 0.584 fra jpn 0.588 0.624 0.606 deu jpn - 0.620 0.620 eng ind 0.575 0.677 0.626 eng deu - 0.657 0.657 eng fra 0.633 0.688 0.660 deu zho - 0.702 0.702 fra zho 0.716 0.691 0.704 eng jpn 0.646 0.817 0.732 eng zho 0.699 0.849 0.774 Table 2: Distance between language centroid in ValuePrism and PVQ-RR. Bold signifies the shared cultural values between Indonesian and Arabic, Chinese and Japanese, and between German and French. Underline denotes the unexpected distant relationship between world English and other languages with Western values. Language code: English (eng), French (fra), Arabic (arb), Indonesian (ind), Chinese (zho), Japanese (jpn). languages of the models Aya and JAIS tend to be closer to one another, showing relatively similar elicited value answers. 6 Conclusion The widespread adoption of Large Language Models (LLMs) in numerous tasks and fields has led to a pressing need to understand the representation of human values in these models. Our paper proposes UniVaR, a high-dimensional, language and model-invariant representation that allows for the statistical analysis and understanding of the human value aspect in LLMs. Through UniVaR we can explore how different LLMs prioritize various values across different languages and cultures, shedding light on the complex interplay between human values and AI systems. Our approach enables us to statistically analyze and understand the value systems embedded in LLMs, providing transparency and accountability in the development and usage of AI technologies. With UniVaR, we can gain a better understanding of the human values reflected in LLMs and make further strides toward aligning these powerful tools with human preferences. Work is ongoing in using UniVaR to enable controllable transfer of human values between models and languages. 7 Limitations Coverage of Values We used existing value taxonomy as a starting point for the value-eliciting QAs. Human value taxonomy is not a fixed entity and some philosophers think that we can never have a comprehensive human value taxonomy. The research on human values in philosophy, social science, and psychology is ongoing; and there are more crowd-sourcing efforts for collective value datasets. Our approach is agnostic to taxonomy development and can be updated with future taxonomies of human values and preferences. Coverage of LLMs Our work underscores the significant finding that values encoded in LLMs vary across languages, reflecting the similarities and differences in human values between diverse cultures. While our study provides valuable insights, it only encompasses 11 LLMs, with 4 unseen 10 LLMs in 6 languages across 2 question sources. Our current result does not cover enough diversity of LLMs, languages, and question sources. We will release the tool and invite the makers of LLMs to extend the coverage to build a more comprehensive and holistic value coverage across more LLMs, languages, and question sources in future work. Granularity of Analysis Our work presents a comprehensive value map analysis, offering insights into the similarities and differences in values across languages and LLMs. This broad perspective lays the groundwork for further exploration. In future studies, it would be beneficial to delve deeper into the fine-grained aspects of value representation. Specifically, question-level and instance-level analyses could provide a more nuanced understanding of how values are conveyed and interpreted within the context of specific prompts or instances. Such analyses would enhance our comprehension of the variability and consistency of value representations across different LLMs, shedding light on the intricacies of their behavior and potential biases. Comprehensive Evaluation of UniVaR In our work, we demonstrate the impact of varying the number of QA pairs in UniVaR. Our results illustrate the benefits of utilizing a larger number of QA pairs as a view within our multi-view perspective, enhancing the effectiveness of the model. Future work might encompass a more comprehensive evaluation on question types and others. 8 Ethics Statement This paper proposes UniVaR as a tool for inspecting the value distributions in LLMs to compare different models, languages, and cultures. It uses existing value taxonomy in doing so. It is not a benchmark on the adequacy of human value alignment in each LLM. References [1] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023. [2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675–718, 2023. [3] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is ChatGPT a general-purpose natural language processing task solver? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empiri- cal Methods in Natural Language Processing, pages 1339–1384, Singapore, December 2023. Association for Computational Linguistics. [4] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [5] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli 11 Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. [8] Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. From instructions to intrin- sic human values–a survey of alignment goals for big models. arXiv preprint arXiv:2308.12014, 2023. [9] Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023. [10] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. [11] Amelia Glaese, Nat McAleese, Maja Tr˛ebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. [12] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. [13] Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring the representation of subjective global opinions in language models. arXiv preprint arXiv:2306.16388, 2023. [14] Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, and Yaodong Yang. Heterogeneous value alignment evaluation for large language models. In AAAI-2024 Workshop on Public Sector LLMs: Algorithmic and Sociotechnical Design, 2024. [15] Arnav Arora, Lucie-aimée Kaffee, and Isabelle Augenstein. Probing pre-trained language models for cross-cultural differences in values. In Sunipa Dev, Vinodkumar Prabhakaran, David Adelani, Dirk Hovy, and Luciana Benotti, editors, Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 114–130, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. [16] Aida Ramezani and Yang Xu. Knowledge of cultural moral norms in large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 428–446, Toronto, Canada, July 2023. Association for Computational Linguistics. [17] Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. In The Twelfth International Conference on Learning Representations, 2024. [18] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conver- sational ai: Converging evidence on chatgpt’s pro-environmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768, 2023. [19] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335, 2022. [20] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. 12 [21] Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical study of instruction-tuning large language models in chinese. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [22] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. [23] Daniel S Brown, Jordan Schneider, Anca Dragan, and Scott Niekum. Value alignment verification. In International Conference on Machine Learning, pages 1105–1115. PMLR, 2021. [24] Zhaowei Zhang, Fengshuo Bai, Jun Gao, and Yaodong Yang. Measuring value understanding in language models through discriminator-critique gap. arXiv preprint arXiv:2310.00378, 2023. [25] Geert Hofstede. Culture’s consequences: Comparing values, behaviors, institutions and organizations across nations. Sage publications, 2001. [26] Geert Hofstede, Gert Jan Hofstede, and Michael Minkov. Cultures and organizations: Software of the mind, volume 2. Mcgraw-hill New York, 2005. [27] Shalom H Schwartz. A theory of cultural values and some implications for work. Applied psychology: an international review, 1999. [28] Shalom H. Schwartz. The Refined Theory of Basic Values, page 51–72. Springer International Publishing, 2017. [29] Shalom H Schwartz and Jan Cieciuch. Measuring the refined theory of individual values in 49 cultural groups: psychometrics of the revised portrait value questionnaire. Assessment, 29(5):1005–1019, 2022. [30] Ronald Inglehart, Miguel Basanez, Jaime Diez-Medrano, Loek Halman, and Ruud Luijkx. World values surveys and european values surveys, 1981-1984, 1990-1993, and 1995-1997. Ann Arbor-Michigan, Institute for Social Research, ICPSR version, 2000. [31] Ronald Inglehart. Mapping global values. Comparative sociology, 5(2-3):115–136, 2006. [32] Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, Eduard Ponarin, and Bi Puranen. World values survey wave 7 (2017-2022) cross-national data-set, 2022. [33] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models’ alignment. arXiv preprint arXiv:2308.05374, 2023. [34] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Ben- gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [35] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yang- gong, and Junbo Zhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning. arXiv preprint arXiv:2305.09246, 2023. [36] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [37] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv preprint arXiv:2309.15025, 2023. [38] Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. The CRINGE loss: Learning what language not to model. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8854–8874, Toronto, Canada, July 2023. Association for Computational Linguistics. 13 [39] Leila Khalatbari, Yejin Bang, Dan Su, Willy Chung, Saeed Ghadimi, Hossein Sameti, and Pascale Fung. Learn what not to learn: Towards generative safety in chatbots. arXiv preprint arXiv:2304.11220, 2023. [40] Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, and Rui Yan. Cyclealign: Iterative distillation from black-box llm to white-box models for better human alignment. arXiv preprint arXiv:2310.16271, 2023. [41] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020. [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mi- haylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. [43] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [44] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [45] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024. [46] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. [47] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. [48] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. [49] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409–14428, Toronto, Canada, July 2023. Association for Computational Linguistics. [50] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023. [51] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275, 2020. [52] Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, Eduard Ponarin, and Bi Puranen. World values survey time-series (1981-2022) cross-national data-set, 2022. [53] Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, and Mona Diab. Investigating cultural alignment of large language models. arXiv preprint arXiv:2402.13231, 2024. 14 [54] Ryan O Murphy, Kurt A Ackermann, and Michel JJ Handgraaf. Measuring social value orientation. Judgment and Decision making, 6(8):771–781, 2011. [55] Geoffrey E Hinton. Distributed representations. 1984. [56] GE Hinton, JL McClelland, and DE Rumelhart. Distributed representations. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations, pages 77–109. 1986. [57] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986. [58] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990. [59] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre- sentations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. [60] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [61] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014. [62] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. Massive exploration of neural machine translation architectures. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1442–1451, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. [63] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, 2018. [64] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. arXiv preprint arXiv:1908.10084, 2019. [65] Huan Gui, Jialu Liu, Fangbo Tao, Meng Jiang, Brandon Norick, and Jiawei Han. Large-scale embedding learning in heterogeneous event data. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pages 907–912. IEEE, 2016. [66] Pascal Mettes and Cees GM Snoek. Spatial-aware object embeddings for zero-shot localization and classification of actions. In Proceedings of the IEEE international conference on computer vision, pages 4443–4452, 2017. [67] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web, pages 173–182, 2017. [68] Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, and Pascale Fung. Subobject-level image tokenization. arXiv preprint arXiv:2402.14327, 2024. [69] Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, Michael Thompson, James Bost, Javier Tejedor-Sojo, and Jimeng Sun. Multi-layer representation learning for medical concepts. In proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1495–1504, 2016. [70] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommen- dations. In Proceedings of the 10th ACM conference on recommender systems, pages 191–198, 2016. [71] Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep learning. In International Conference on Learning Representations, 2018. [72] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pages 1–5, 2015. 15 [73] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self- supervised learning from a multi-view perspective. In International Conference on Learning Representations, 2021. [74] Julius Von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [75] Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12979–12990. PMLR, 18–24 Jul 2021. [76] Milton Rokeach. A theory of organization and change within value-attitude systems. Journal of social issues, 1968. [77] Milton Rokeach. The nature of human values. Free press, 1973. [78] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022. [79] Ravid Shwartz Ziv and Yann LeCun. To compress or not to compress—self-supervised learning and information theory: A review. Entropy, 26(3), 2024. [80] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019. [81] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self- supervised learning via redundancy reduction. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12310–12320. PMLR, 18–24 Jul 2021. [82] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 21271–21284. Curran Associates, Inc., 2020. [83] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9726–9735, 2020. [84] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020. [85] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. [86] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021. [87] Ronald Inglehart. Human beliefs and values: A cross-cultural sourcebook based on the 1999-2002 values surveys. Siglo XXI, 2004. [88] Robert J House, Paul J Hanges, Mansour Javidan, Peter W Dorfman, and Vipin Gupta. Culture, leadership, and organizations: The GLOBE study of 62 societies. Sage publications, 2004. [89] Geert Hofstede. Dimensionalizing cultures: The hofstede model in context. Online readings in psychology and culture, 2(1):8, 2011. [90] Shalom H Schwartz. Beyond individualism/collectivism: New cultural dimensions of values. 1994. 16 [91] Shalom H Schwartz. Mapping and interpreting cultural differences around the world. In Comparing cultures, pages 43–73. Brill, 2004. [92] Peter Schmidt, Sebastian Bamberg, Eldad Davidov, Johannes Herrmann, and Shalom H Schwartz. Die messung von werten mit dem “portraits value questionnaire”. Zeitschrift für Sozialpsychologie, 38(4):261–275, 2007. [93] Shalom Schwartz. Cultural value orientations: Nature & implications of national differences. Psychology. Journal of Higher School of Economics, 5(2):37–67, 2008. [94] Constanze Beierlein, Eldad Davidov, Peter Schmidt, Shalom H Schwartz, and Beatrice Ramm- stedt. Testing the discriminant validity of schwartz’portrait value questionnaire items–a replication and extension of knoppen and saris (2009). In Survey Research Methods, volume 6, pages 25–36, 2012. [95] Shalom H Schwartz. An overview of the schwartz theory of basic values. Online readings in Psychology and Culture, 2(1):11, 2012. [96] Milton Rokeach. Some unresolved issues in theories of beliefs, attitudes, and values. In Nebraska symposium on motivation. University of Nebraska Press, 1979. [97] Milton Rokeach. Understanding human values. Simon and Schuster, 2008. [98] Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. [99] Ahmet Ustun, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned open-access multilingual language model, 2024. [100] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemi´nski, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning, 2024. [101] Xuan-Phi Nguyen, Wenxuan Zhang, Mahani Aljunied Xin Li, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and Lidong Bing. Seallms - large language models for southeast asia. 2023. [102] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. [103] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeon- woo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling, 2024. [104] Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, and Eric Xing. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models, 2023. [105] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi, editors, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the 17 Association for Computational Linguistics (Volume 1: Long Papers), pages 675–718, Nusa Dua, Bali, November 2023. Association for Computational Linguistics. [106] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. [107] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. [108] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023. [109] Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, and Minlie Huang. InstructSafety: A unified framework for building multidimensional and explainable safety detector through instruction tuning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10421–10436, Singapore, December 2023. Association for Computational Linguistics. [110] Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, and Dilek Hakkani-Tur. Using in-context learning to improve dialogue safety. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11882–11910, Singapore, December 2023. Association for Computational Linguistics. [111] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. In The Twelfth International Conference on Learning Representations, 2024. [112] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [113] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [114] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [115] Taylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19937–19947, 2024. [116] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579–2605, 2008. [117] Laurens van der Maaten. Accelerating t-sne using tree-based algorithms. Journal of Machine Learning Research, 15(93):3221–3245, 2014. [118] David Crystal. English as a global language. Cambridge university press, 2003. [119] C Tardy. The role of english in scientific communication: lingua franca or tyrannosaurus rex? Journal of English for Academic Purposes, 3(3):247–269, July 2004. 18 [120] Vladimir M. Smokotin, Anna S. Alekseyenko, and Galina I. Petrova. The phenomenon of linguistic globalization: English as the global lingua franca (eglf). Procedia - Social and Behavioral Sciences, 154:509–513, October 2014. [121] Ana Cristina Suzina. English as lingua franca. or the sterilisation of scientific work. Media, Culture & Society, 43(1):171–179, September 2020. 19","libVersion":"0.3.2","langs":""}
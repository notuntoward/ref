{"path":"lit/lit_sources.backup/Jain24liveCodeBenchLeaderboard_summary.pdf","text":"4/18/24, 4:48 PM LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code https://livecodebench.github.io 1/6 LiveCodeBench: Holistic and Contamination-Free Evaluation of Large Language Models for Code Naman Jain1, King Han1, Alex Gu2, Wen-Ding Li3, Fanjia Yan1, Tianjun Zhang1, Sida Wang, Armando Solar-Lezama2, Koushik Sen1, Ion Stoica1 1UC Berkeley 2MIT 3Cornell University Paper Code Data Leaderboard LIVECODEBENCH collects problems from periodic contests on LEETCODE, ATCODER, and CODEFORCES pla\u0000orms and uses them for constructing a holistic benchmark for evaluating Code LLMs across variety of code-related scenarios continuously over time. 4/18/24, 4:48 PM LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code https://livecodebench.github.io 2/6 Introduction LiveCodeBench is a holistic and contamination-free evaluation benchmark of LLMs for code that continuously collects new problems over time. Particularly, LiveCodeBench also focuses on broader code- related capabilities, such as self-repair, code execution, and test output prediction, beyond mere code generation. Currently, LiveCodeBench hosts over three hundred high-quality coding problems published between May 2023 and February 2024. We evaluate 29 LLMs on LiveCodeBench scenarios and present novel empirical \u0000ndings not revealed in prior benchmarks. Contamination LIVECODEBENCH annotates problems with release dates, and thus allows evaluating models on problems released during a speci\u0000c time period. Thus, for a newer model with a training-cuto\u0000 date D, we can evaluate it on problems released after D to measure its generalization on unseen problems. The above plots depict the performance of models on code generation and test output prediction scenarios on problems released over di\u0000erent months. We \u0000nd that DEEPSEEK models exhibit a stark drop in performance on LeetCode problems released since September 2023, its release date, indicating that the earlier problems might be 4/18/24, 4:48 PM LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code https://livecodebench.github.io 3/6 contaminated. In contrast, for GPT models, the performance is relatively stable across di\u0000erent months. Holistic Evaluation and Open vs Closed Models LIVECODEBENCH evaluates models on a variety of code-related scenarios, such as code generation, self-repair, test output prediction, and code execution. We \u0000nd that while model performances are correlated across di\u0000erent scenarios, there relative performances and ordering can vary (left \u0000gure). For instance, CLAUDE-3-OPUS overtakes GPT-4-TURBO in the test output prediction scenario, but not in the code generation scenario. Similarly, MISTRAL-LARGE performs considerably better on natural language reasoning tasks like test output prediction and code execution. We compare the performance of open access models with closed api- access models on LIVECODEBENCH and \u0000nd that generally the closed api- access models outperform the open models. Particularly, the only open models that surpass the barrier are \u0000ne-tuned variants of large (30+B parameter) models. Potential Over\u0000tting in HumanEval 4/18/24, 4:48 PM LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code https://livecodebench.github.io 4/6 We also \u0000nd that models that perform well on HUMANEVAL might be over\u0000tting on the benchmark. Particularly, the models are separated into two clusters depicted by the green and red shaded region in the right scatterplot. The models in the green region perform similarly on HUMANEVAL and LCB-EASY, while the models in the red region perform well on HUMANEVAL but lag behind on LCB-EASY. For instance, DS-INS-1.3B model outperforms GEMINI-PRO and CLAUDE-INS-1 but performs considerably worse on LCB-EASY. Interestingly, the models in the red region are mostly \u0000ne- tuned variants of open access models. On the other hand, base models and most of the closed api-access models lie in the green region. This highlights a potential lack of diverse \u0000ne-tuning data being employed by the open source community and the need for optimizing models for a broader set of code-related tasks. Model Comparisions 4/18/24, 4:48 PM LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code https://livecodebench.github.io 5/6 The above plots depict the performance of models on di\u0000erent scenarios considered in LIVECODEBENCH. We \u0000nd that GPT-4-TURBO and CLAUDE-3-OPUS models perform best across di\u0000erent scenarios. Among open source models, DS-INS-33B and PHIND-34B perform the best. Submitting Custom Models To submit models to the leaderboard, you can run the evaluation using the evaluation scripts in GitHub. Once you have the results, you can \u0000ll out this form. You will need to \u0000ll out model details and provide the generated evaluation \u0000le with model generations and pass@1 scores. We will review the submission and add the model to the leaderboard accordingly. BibTeX 4/18/24, 4:48 PM LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code https://livecodebench.github.io 6/6 @article{jain2024livecodebench, title={LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language M author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and journal={arXiv preprint arXiv:2403.07974}, year={2024}} Please reach out to naman_jain@berkeley.edu for questions or feedback on LiveCodeBench. We are also open to collaborations and suggestions for new scenarios to add to the benchmark. Finally, LiveCodeBench provides one axis of LLM coding evaluations and we recommend the following leaderboards for measuring code LM ability on various coding tasks, such as EvalPlus Leaderboard, CruxEval Leaderboard, Chatbot Arena Leaderboard, BigCode Models Leaderboard, In\u0000Coder-Eval, and TabbyML Leaderboard. The source code from this website is borrowed from this template!","libVersion":"0.3.2","langs":""}
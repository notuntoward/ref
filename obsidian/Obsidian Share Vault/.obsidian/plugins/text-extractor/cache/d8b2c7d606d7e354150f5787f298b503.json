{"path":"lit/lit_sources/Ma11MutualInformationCopula.pdf","text":"TSINGHUA SCIENCE AND TECHNOLOGY IS SN ll 1007 - 0214 ll 08/17 ll pp51- 54 Volume 16, Number 1, February 2011 Mutual Information Is Copula Entropy MA Jian (马 健), SUN Zengqi (孙增圻) ** State Key Laboratory on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China Abstract: Mutual information (MI) is a basic concept in information theory. Therefore, estimates of the MI are fundamentally important in most information theory applications. This paper provides a new way of under- standing and estimating the MI using the copula function. First, the entropy of the copula, named the copula entropy, is defined as a measure of the dependence uncertainty represented by the copula function and then the MI is shown to be equivalent to the negative copula entropy. With this equivalence, the MI can be estimated by first estimating the empirical copula and then estimating the entropy of the empirical copula. Thus, the MI estimate is an estimation of the entropy, which reduces the complexity and computational re- quirements. Tests show that the method is more effective than the traditional method. Key words: copula entropy; mutual information; estimation; empirical copula Introduction Since the seminal paper by Shannon[1], information theory has been widly used in many branches of sci- ence. In information theory, entropy and mutual infor- mation (MI) are two different fundamental concepts [2]. Entropy is defined as the measure of uncertainty asso- ciated with random variables and with multivariate MI as a measure of the mutual dependence of random variables defined as follows: 1 () ( ) () N i i IH x H = =−∑xx (1) where 1[, , ] N Nxx=∈x R are random variables, H is the entropy, and I is the MI. Unlike traditional statis- tical measures of dependence, such as correlation (i.e. second-order statistics) and higher-order statistics [3], MI is a measure of all orders of dependence. Another branch of study, copula theory [4,5], concerns the representation of dependence between random variables. Theoretically, copula is a function that represents the dependence between random vari- ables [4,5]. Such a representation is guaranteed by the Sklar theorem [6]. Let P be a joint distribution func- tion with margins {, 1, , }iFi N= . The Sklar theorem states that P can be represented as copula function C with F as its arguments: 11( ) ( ( ), , ( ))NNP CF x F x=x (2) If the random valuables F are separated with margins from the joint distribution function, the copula has all the dependence information of the random variables. This paper investigates the relation between the measure of dependence, MI, and the representation of the dependence, the copula function. De la Peña et al. [7] studied the properties of MI and other measures of de- pendence with the copula represented as U-statistics. Davy and Doucet [8] studied the relation between copula and Cohen-Posch theory, but with little mention of bivariate copula and MI. Jenison and Reale [9] discussed how to model neural populations with copula. This paper shows that MI is actually a kind of entropy, de- fined as the copula entropy. A method developed to estimate the MI is then easier to use than the previous methods. Received: 2009-03-30; revised: 2010-12-10 ** To whom correspondence should be addressed. E-mail: szq-dcs@tsinghua.edu.cn; Tel: 86-10-62788939 Tsinghua Science and Technology, February 2011, 16(1): 51-54 52 1 MI and Copula Entropy Before analyzing the relation between MI and copula, a definition of the copula entropy is introduced. Definition 1 Copula entropy Let N∈x R be random variables with marginal functions 1[, , ]NFF=u and copula density ( )c u . Copula en- tropy of x is defined as: c () ( ) log ( ) d u Hc c=−∫xu u u (3) where 12 d( ) () dd d N N C c uu u = u u . As contrast, the margin entropy is defined as follows. Definition 2 Margin entropy Let N∈x R be random variables with marginal functions 1[, , ]Npp . The margin entropy is defined as () ( )log ( ) d i ii i i i i x H x px px x=−∫ (4) where 1, , .iN= Theorem 1 The mutual information of random variables is equivalent to their negative copula entropy: c() ()IH=−xx (5) Proof c () () () log d () () ( ) log ( ) d () log ( ) d ( ). ii i ii i p Ip px cp x c cc H == = =− ∫ ∏ ∏∫ ∫ x xx x xx x x x xx x uu x uu u x Theorem 1 can be interpreted as MI exactly meas- uring the dependence uncertainty of random variables. This provides a new way of understanding MI as shown in Fig. 1. As defined in Eq. (1), MI used to be understood as the intersection of margin entropies. However, in Eq. (5), MI as copula entropy has no in- tersection with the margin entropy of each random variable because the independence between copula and margins means that (1) the variations of random vari- ables can have different margin entropies that have no effect on the copula entropy and that (2) the same ran- dom variables can have the same margin entropy, but different types of copula functions, i.e., different types of dependent relationships, hence, different copula entropies. The following corollary shows the relationship between the entropy of x, the margin entropy, and the corresponding copula entropy. Fig. 1 A new way of understanding mutual informa- tion. Left: the traditional way of understanding MI [2]. Right: the representation of MI as copula entropy with the copula entropy at the center surrounded by margin entropies. Corollary 1 The entropy of joint random variables is composed of margin entropies and the copula entropy. c 1 () ( ) ( ) N i i HH x H = =+∑xx (6) Proof c ( ) ( ) () () () ( ) ( ) ( ). i i ii ii IH x H H Hx I Hx H = −⇔ = −= + ∑ ∑∑ xx x xx Corollary 1 indicates that the entropy of random variables is composed of N margin entropies and the copula entropy, i.e., entropy of their interrelations. This relationship is illustrated in Fig. 2 which gives insight into the inner relationship between MI and copula as a connection between information theory and copula theory. Fig. 2 The relationship between the entropy of random variables, margin entropies, and the copula entropy 2 Estimating Mutual Information via Copula MI estimates are a common problem in many scientific fields. All traditional methods are based on Eq. (1) which leads to analyses of both margin entropy and the entropy of all the variables [10,11]. Theorem 1 shows that the entropy estimation and the MI will be the same if the copula can be estimated. Such estimates can be achieved with empirical copula. MA Jian (马 健) et al.：Mutual Information Is Copula Entropy 53 Given independent identical distribution samples 1{, , }TTXX=X generated from an N dimensional N∈x R , the MI, ( )I x , can be estimated as: Input: 1{,, }TTX X=X Output: ()I x Step 1 Determine the empirical copula density ˆ TU from .TX Step 2 Estimate the entropy of ˆ TU as ()I x . The empirical copula density, ˆ TU , can be deter- mined using the empirical functions: {} 1 1 1 ii tt T i t Xx t U T = = ∑ (7) where 1, ,iN= and 1 is an indicator function. Step 1 can use other methods, such as analytical or kernel methods [12]. The empirical method is the easiest option because it is intuitive and easily calculated, since it can be solved as sorting problem. The selection of an estimation method depends on many factors, such as the background information, accuracy re- quirements, or personal preferences, etc. There have been many methods developed for entropy estimates, so the entropy of ˆ TU in Step 2 can be estimated using many well-established methods [10,11,13]. The present method is non-parametric so all previ- ous methods for estimating the MI can be transformed to this method which only needs the entropy estimate. Thus, the MI estimates can be achieved by simply es- timating one entropy instead of many as in Eq. (1). This improvement is very significant since MI estimates are both widely-used and computationally burdensome. 3 Simulations The effectiveness of the estimation method was evalu- ated using a typical case with two correlated standard Gaussian variables with covariance ρ , for which the MI can be calculated as 21 log(1 ) 2 ρ−− . The tests used ρ from 0.0 to 0.9 with steps of 0.1 with a 1000 sam- ple set generated for each value. The copula entropy was estimated using the KNN methods [10]. The MI es- timation method in Kraskov et al. [10] was also used on the sample sets. The results in Fig. 3 show that both methods give very good estimates. The present method provides a competitive estimate of MI that is much less computationally intensive. Fig. 3 Mutual information estimates. The solid line represents the analytical value of MI between two Gaus- sians, the dotted line represents the methods in Ref. [10], and the dashed line represents the current method. 4 Conclusions This paper proves that MI is essentially a kind of en- tropy, named the copula entropy, to provide a new way of understanding MI with the theoretical relationship between information theory and copula theory. A non- parametric method is then developed to estimate the MI, which is simple and less computationally burden- some than all the previous methods based on the clas- sical understanding of MI. This simple MI method is very useful since MI is a core concept in information theory and estimates of the MI are of fundamental im- portance in many applications of information theory. References [1] Shannon C. A mathematical theory of communication. Bell System Technical Journal, 1948, 27: 379-423. [2] Cover T M, Thomas J A. Elements of Information Theory. New York: Wiley, 1991. [3] Kendall M, Stuart A. The Advanced Theory of Statistics. Ch. Griffin, 1969. [4] Nelsen R B. An Introduction to Copulas. New York: Springer, 1999. [5] Joe H. Multivariate Models and Dependence Concepts. London: Chapmann & Hall, 1997. [6] Sklar A. Fonctions de repartition à n dimensions et leurs marges. Publications de l'Institut de Statistique de l'Uni- versité de Paris, 1959, 8: 229-231. [7] De la Peña V, Ibragimov R, Sharakhmetov S. Characteriza- tions of joint distributions, copulas, information, depend- ence and decoupling, with applications to time series. Lec- ture Notes-Monograph Series, Institute of Mathematical Statistics, USA, 2006: 183-209. Tsinghua Science and Technology, February 2011, 16(1): 51-54 54 [8] Davy M, Doucet A. Copulas: A new insight into positive time-frequency distributions. IEEE Signal Processing Let- ters, 2003, 10: 215-218. [9] Jenison R, Reale R. The shape of neural dependence. Neu- ral Computation, 2004, 16: 665-672. [10] Kraskov A, Stögbauer H, Grassberger P. Estimating mutual information. Physical Review E, 2004, 69: 066138. [11] Paninski L. Estimation of entropy and mutual information. Neural Computation, 2003, 15: 1191-1253. [12] Silverman B. Density Estimation for Statistics and Data Analysis. Chapman & Hall/CRC, 1986. [13] Beirlant J, Dudewicz E, Gyoerfi L, et al. Nonparametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 1997, 6: 17-40.","libVersion":"0.3.1","langs":""}
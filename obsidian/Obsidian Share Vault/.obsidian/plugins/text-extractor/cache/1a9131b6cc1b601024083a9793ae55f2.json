{"path":"lit/lit_sources.backup/Sousa24improvedConformQR.pdf","text":"IMPROVED CONFORMALIZED QUANTILE REGRESSION ∗ Martim Sousa IEETA/DETI, University of Aveiro martimsousa@ua.pt Ana Maria Tomé IEETA/DETI, University of Aveiro ana@ua.pt José Moreira IEETA/DETI, University of Aveiro jose.moreira@ua.pt ABSTRACT Conformalized quantile regression is a procedure that inherits the advantages of conformal prediction and quantile regression. That is, we use quantile regression to estimate the true conditional quantile and then apply a conformal step on a calibration set to ensure marginal coverage. In this way, we get adaptive prediction intervals that account for heteroscedasticity. However, the aforementioned conformal step lacks adaptiveness as described in (Romano et al., 2019). To overcome this limitation, instead of applying a single conformal step after estimating conditional quantiles with quantile regres- sion, we propose to cluster the explanatory variables weighted by their permutation importance with an optimized K-means and apply k conformal steps. To show that this improved version outperforms the classic version of conformalized quantile regression and is more adaptive to heteroscedasticity, we extensively compare the prediction intervals of both in open datasets. Keywords Conformal prediction · Conformalized quantile regression · Conditional coverage · Group-balanced conformal prediction · Permutation importance 1 Introduction Conformal prediction (CP) is a set of distribution-free and model agnostic algorithms devised to predict with a user- deﬁned conﬁdence with coverage guarantee, see Theorem (1.2). CP is key in high risk decision-making settings where the true output must be within the prediction interval (PI) with high probability. Unlike bayesian models [1, 2, 3] that only ensure asymptotic coverage guarantees and bootstrap uncertainty estimations methods [4, 5], CP provides valid coverage in ﬁnite samples that work for any model, any distribution, any level α ∈ (0, 1) and any non-conformity score function s, under the mild assumption of exchangeability (see Deﬁnition (1)). However, despite being true regardless of the non-conformity score function s, it has a direct impact in prediction intervals (PIs) width and adaptiveness to heteroscedasticity. Therefore, we must clerverly choose the non-conformity score function s to have short as possible adaptive PIs. PIs are said to be adaptive when they take in consideration the uncertainty of the conditional distribution Y |X. In plain words, this means that PIs must vary on the input and be wide whenever the model is highly uncertain or the input is indeed hard to predict and narrow if the model has minimal uncertainty on the given input. Unfortunately, conditional coverage, see Eq.(3), is a stronger property that CP does not ensure; however, there are heuristic ways to approximate it. CP has two main forms: (i) inductive conformal prediction (ICP) and (ii) full conformal prediction [6]. In a nutshell, the former requires data splitting and therefore is more scalable, whereas the latter does not require data splitting at the ∗Remark: This is a preprint whose ﬁnal form is not yet published.arXiv:2207.02808v8 [stat.ML] 6 Nov 2022 cost of reﬁtting the model multiple times, thus being computationally onerous. Henceforth, in the interest of scalability, we will solely dedicate to ICP, also referred to as split conformal prediction. Throughout this paper, we use the notation depicted in Table (1). Notation Meaning α Miscoverage rate α ˆC1−α(xn) 1 − α prediction interval on input xn x(k) k-th rank statistic of (x1, ..., xn) ϵ set of non-conformity scores Quantile(ϵ; 1 − α) 1 − α order quantile of ϵ x a scalar x a vector X a matrix or a random variable (easily distinguished) X a tensor or a random vector (easily distinguished) (xtrain, xcal, xval) train, calibration and test data, respectively Table 1: Notation used throughout this paper. Paper outline The remainder of this Section does a brief recap on ICP and quantile regression (QR) since the contribution is grounded on it. Section 2 summarizes conformalized quantile regression (CQR) as described in (Romano et al., 2019) [7]. Section 3 introduces our improved version of CQR for tabular data named improved conformalized quantile regression (ICQR). Section 4 extensively compare PIs width and coverage of CQR and ICQR on open datasets and discusses the results. Finally, Section 5 draws the main points and ﬁndings of this research, and addresses shortcomings. 1.1 Inductive conformal prediction The general outline of ICP is as follows: 1. Train a model ˆf on a training dataset or use a pre-trained off-the-shelf model; 2. Deﬁne a symmetric heuristic notion of uncertainty denoted as s : X × Y → A ⊆ R usually referred to as the non-conformity score function, where larger scores encode worse agreement between pairs (x, y) ∈ X × Y; 3. Compute ϵ1, ..., ϵn = s(x1, y1), ..., s(xn, yn) non-conformity scores on a calibration dataset, not seen by the model during training, using the trained model ˆf , and the non-conformity score function s applied on pairs of a calibration dataset Dcal := {(xi, yi)} n i=1; 4. Compute ˆq = Quantile (ϵ; ⌈(n+1)(1−α)⌉ n ); 5. Deploy PIs as ˆC1−α(xn+1) = {y ∈ Y : s(xn+1, y) ≤ ˆq}. Deﬁnition 1 (Exchangeability). A sequence of random variables Z1, Z2, ..., Zn ∈ Z are exchangeable if and only if for any permutation π : {1, 2, ..., n} → {1, 2, ..., n} and every measurable set E ⊆ Z n, we have P{(Z1, Z2, ..., Zn) ∈ E} = P{(Zπ(1), Zπ(2), ..., Zπ(n)) ∈ E} (1) Lemma 1.1. Let Z1, ..., Zn ∈ Z be exchangeable random variables with no ties almost surely, then their ranks are uniformly distributed on {1,...,n}. Proof. Let Ri = Rank(Zi). Since (Z1, ..., Zn) are exchangeable, we have a total of n! possible permutations between them all being equally probable. Additionally, given that Z1, ..., Zn have no ties almost surely, then P(Ri ̸= Rj) = 1, ∀i ̸= j ∈ {1, ..., n}. For any i ∈ {1, ..., n}, if we ﬁx Ri on rank j ∈ {1, ..., n}, there are (n-1)! possible permutations among the other n-1 random variables. Hence, P(Ri = j) = (n−1)! n! = 1 n and thus P(Ri = 1) = P(Ri = 2) = ... = P(Ri = n) = 1 n , ∀i ∈ {1, ..., n}. 2 Theorem 1.2 (Marginal coverage guarantee). Let (X1, Y1), ..., (Xn, Yn) be exchangeable random vectors with no ties almost surely drawn from a distribution P, additionally if for a new pair (Xn+1, Yn+1), (X1, Y1), ..., (Xn+1, Yn+1) are still exchangeable, then by constructing C1−α(Xn+1) using ICP, the following inequality holds for any non- conformity score function s : X × Y → A ⊆ R and any α ∈ ( 1 n+1 , 1) 1 − α ≤ P{Yn+1 ∈ C1−α(Xn+1)} ≤ 1 − α + 1 n + 1 . (2) Proof. Since Z1 = (X1, Y1), ..., Zn = (Xn, Yn) are exchangeable random vectors with no ties almost surely, the corresponding non-conformity scores ϵ = {(ϵi := s(Zi))}n i=1 ⊆ A are also exchangeable (see Theorem 3 of [8]) with no ties almost surely. Since the non-conformity scores ϵ1, ..., ϵn, ϵn+1 are exchangeable, their ranks are uniformly distributed by Lemma (1.1). Therefore, Rank(ϵn+1) ∼ U{1, n + 1}. That is, P{ϵn+1 ≤ ϵ(k)} = P{Rank(ϵn+1) ≤ k} = k n + 1 , k ∈ {1, ..., n + 1}. By deﬁnition, since ˆq = Quantile(ϵ; ⌈(n+1)(1−α)⌉ n ) = ϵ(⌈(n+1)(1−α)⌉), follows that P{Yn+1 ∈ C1−α(Xn+1)} = P{y ∈ Y : s(Xn+1, y) ≤ ˆq} = P{ϵn+1 ≤ ϵ(⌈(n+1)(1−α)⌉)} = ⌈(n + 1)(1 − α)⌉ n + 1 . It is easy to note that 1 − α ≤ ⌈(n+1)(1−α)⌉ n+1 ≤ (n+1)(1−α)+1 n+1 = 1 − α + 1 n+1 . As a consequence of Theorem (1.2), ICP provides asymptotic exact coverage since limn→+∞(1 − α + 1 n+1 ) = 1 − α =⇒ P{Yn+1 ∈ C1−α(Xn+1)} = 1 − α. In practice, Quantile(ϵ; ⌈(n+1)(1−α)⌉ n ), is essentially a very minor ﬁnite sample correction on the 1 − α quantile. However, henceforth, for simplicity, we will use Quantile(ϵ; 1 − α), but bear in mind that Quantile(ϵ; ⌈(n+1)(1−α)⌉ n ) is the formally correct form. Deﬁnition 2 (Conditional coverage). An ICP procedure guarantees conditional coverage if P(Yval ∈ C1−α(Xval)|X = xval) ≥ 1 − α. (3) 1.2 Naive method The most basic and widely used non-conformity score function for regression problems is the absolute error given by s(x, y) = |y − fθ(x)|, (4) where fθ(x) is the model’s forecast with respect to input x. After calculating ˆq on the calibration set, PIs come as ˆC1−α(xval) = [fθ(xval) − ˆq, fθ(xval) + ˆq]. (5) As proven above, this naive method guarantees marginal coverage. However, PIs length are always equal to 2ˆq regardless of the input, hence not adaptive. In fact, this naive method is known to overcover easy inputs and to undercover hard inputs. As an example, we call the reader’s attention to Figs. (1) and (2). It is straightforward to see that for α = 0.1 this method would cover the lowest 90% residuals in Figure (2) and fail the greatest 10%. This has great implications in practice since the greatest 10% non-conformity scores might represent a group that is being ignored. In turn, evaluating PIs by simply looking to their mean amplitude or marginal coverage is not enough nor adequate. A ﬁner analysis must look to other statistics as standard deviation and quantiles. PIs with high standard deviation are not necessarily a bad sign, it may sign adaptiveness to heteroscedasticity. Figure 1: Homocedastic residuals. Figure 2: Heteroscedastic residuals. 3 1.3 Quantile regression Usually, in a regression problem, we attempt to ﬁnd the parameters θ of a model fθ : RD → R via the minimization of the sum of squared residuals on the training set Dtrain as min θ ntrain∑ i=1 (fθ(xi) − yi)2 + Ω(θ), (6) where Ω(θ) is a potential regularizer. QR [9, 10], however, is a non-parametric method that attempts to approximate the true τ ∈ (0, 1) conditional quantile Qτ (x) given by Qτ (x) = inf {y ∈ Y : FY |X (y|X = x) ≥ τ }, (7) of the true conditional distribution Y |X by minimizing the following objective min θ ntrain∑ i=1 ρτ (yi, fθ(xi)) + Ω(θ), (8) where ρα is the quantile loss, also known as pinball loss due to its resemblance to a pinball ball movement, see Fig.(3). This loss function can be mathematically expressed as ρτ (y, fθ(x)) = max (τ (y − fθ(x)), (τ − 1)(y − fθ(x)) . (9) Figure 3: Visualization of the pinball loss where ϵ = y − fθ(x) for different values of α. Consequently, it is suggested to train a QR model fθ(x) for τ = α/2 and τ = 1 − α/2 on Dtrain to obtain 1 − α conditional coverage via [ ˆQ α 2 (xval), ˆQ1− α 2 (xval)], (10) where ˆQ α 2 (xval) and ˆQ1− α 2 (xval) are conditional quantile estimations of Q α 2 (xval) and Q1− α 2 (xval), respectively. Note that, unlike naive PIs, QR PIs are adaptive (they depend on the input). Another great virtue of QR is that it can be applied on top of any model by just changing the loss function to a pinball loss. Although the estimation ˆQα(x) yielded by QR of the unknown true conditional quantile Qα(x) is known to be asymptotically consistent under certain conditions [7, 11, 12], it rarely provides 1 − α coverage in ﬁnite samples. To overcome this limitation, (Romano et al., 2019) [7] drawn several ideas from CP and devised the so-called CQR that we introduce in the next section. 2 Conformalized quantile regression CQR grounds on correcting QR intervals with ICP techniques on a calibration set Dcal to ensure marginal coverage, hence inheriting the advantages of both, i.e., adaptive intervals with marginal coverage guarantee. Speciﬁcally, if QR bounds are constantly undercovering, then PIs must get wider. On the contrary, in case of QR PIs cover in a 4 ratio superior to 1 − α, they must be shortened. For this purpose, (Romano et al., 2019) [7] proposed the following non-conformity score function s(x, y) = max { ˆQ α 2 (x) − y, y − ˆQ1− α 2 (x)}. (11) Subsequently, after calculating ˆq = Quantile(ϵ; 1 − α), adaptive PIs with 1 − α marginal coverage guarantee are yielded as [ ˆQ α 2 (xval) − ˆq, ˆQ1− α 2 (xval) + ˆq] (12) Built on the same idea, a different non-conformity score function was proposed in [13]; however, it has not proved to outperform the non-conformity score function in Eq.(11) [14]. Note that, a ˆq > 0 (most cases) is a result of a QR model that did not ensure 1 − α coverage and therefore PIs must get wider. In the case of ˆq < 0, it signiﬁes that QR bounds are overcovering and thus we shoul narrow the bounds, i.e., the lower bound increases ˆq units and the upper bound decreases the same amount, while still ensuring 1 − α marginal coverage. CQR seems appealing, and it is in fact, yet the calibration step lacks adaptiveness. The role of ˆq is to simply shift the bounds ˆq units. Also, it does not depend on the input at hand in any form. Our contribution, presented in the next section, will focus on improving the calibration step to make it more adaptive and dependent on the input. However, this improvement is only possible in tabular data. 3 Contribution Our idea to improve CQR is to do k conformal steps instead, one per each group, as illustrated in Fig.(4). This is based on the idea that x1 ≈ x2 =⇒ fθ(x1) ≈ fθ(x2). To attain such goal, we could compute the euclidean distance between observations and cluster them using K-means [15]; however, this is not a good heuristic to approximate the conditional distribution Y |X. We need two additional conditions: (1) features must be scaled, since euclidean distance is scale-dependent; and (2) each feature has a different predictive power upon the response variable y. To accommodate condition (2), whenever clustering the observations, we must weigh each feature by the respective feature importance. A simple way of calculating feature importance that work for any model is permutation importance [16]. Algorithms (1), (2), and (3) comprise every step to successfully perform K-means, permutation importance, and our improved CQR version, respectively. In a nutshell, these are the main steps of our improved version: (1) normalize the training data, and apply the same normalization object on the calibration and validation set; (2) train fθ : RD → R with pinball loss for τ = α 2 and τ = 1 − α 2 ; (3) get feature importance by means of permutation importance algorithm using the calibration set for evaluation; (4) create a copy of the training set, calibration set, and validation set weighted by feature importance, henceforth referred to as clustering training set, clustering calibration set, and clustering validation set, respectively; (5) select the best k of K-means on the clustering calibration set and store k centroids to represent each cluster; (6) assign each observation of the clustering calibration set to the nearest cluster/centroid; (7) knowing which elements belong to each of the k clusters given by the previous step, compute a different ˆq for each cluster using Eq.(11) as the non-conformity score function on the calibration set; (8) given a new observation xval, ﬁnd the nearest centroid ci of x∗ val (the matching observation of xval weighted by the respective permutation importance.) and use the respective ˆq(i) to produce PIs as [ ˆQ α 2 (xval) − ˆq(i), ˆQ1− α 2 (xval) + ˆq(i)]. Figure 4: Group-balanced conformal prediction. Image inspired by [17]. 5 K-means, as shown in Algorithm (1) is a clustering algorithm that attempts to minimize the following objective L = k∑ i=1 ∑ x∈Si ||x − ci|| 2, (13) where k is the number of clusters; Si denotes the cluster set i; ci the centroid of cluster i; and ||.|| the euclidean norm. Since K-means relies upon the hyperparameter k to deﬁne the number of clusters beforehand, we must select a criteria to ﬁnd the potential best k. For the purpose at hand, the fraction of variance explained by the centroids is a good choice, which is mathematically expressed as σk = ∑k i=1 ni(ci − µ) T (ci − µ) ∑N i=1(xi − µ)T (xi − µ) , (14) where ni is the number of examples in cluster i and µ denotes the feature mean. Remarks • The selection of k for K-means is not limited to our approach, different criteria regarding the selection of k for K-means are acceptable, e.g., silhouete score and elbow method [18, 19, 20]; • Different clustering algorithms beyond K-means might also be adequate as long as they take in consideration the essential point, which is cluster based on similarity between observations and feature importance. Algorithm 1 K-means algorithm Input: c(1) 1 , c(1) 2 , ..., c(1) k initial centroids efﬁciently randomly assigned as in [21]. Output: k clusters given by their centroids that minimize Eq.(13) 1: Converged ← False 2: t ← 1 3: while Converged is False do 4: Assign each observation to the cluster with the nearest centroid i given by 5: S(t) i = {x : ||x − c(t) i || ≤ ||x − c(t) j || ∀1 ≤ j ≤ k} 6: t ← t + 1 7: Update centroids 8: for i ← 1 to k do 9: c(t+1) i ← 1 |S(t) i | ∑ x∈S(t) i x 10: end for 11: Stop if all clusters are the same as from previous iteration. 12: if c(t+1) i = c(t) i , ∀i ∈ {1, 2, ..., k} then 13: Converged ← True 14: end if 15: end while Algorithm 2 Permutation importance Input: A dataset D={(xi, yi)} N i=1, a model fθ : RD → R, a performance measure M, and R repetitions. Output: D feature importance’s I1, I2, ..., ID 1: Split D in two mutually exclusive sets, a training set Dtrain = (Xtrain, ytrain) and a validation set Dval = (Xval, yval) 2: Train fθ on Dtrain, and compute the baseline error score Eb on Dval using the performance measure M 3: for j ← 1 to D do 4: for i ← 1 to R do 5: Permutate column j on the validation X set Xval and compute the permutated error score E(i) πj using performance measure M 6: ∆(i) j ← |Eb − E(i) πj | 7: end for 8: Ij ← 1 R ∑R i=1 ∆(i) j 9: end for 6 Algorithm 3 Improved conformalized quantile regression (ICQR) Input: Three normalized sets: Dtrain = (Xtrain, ytrain), Dcal = (Xcal, ycal), and Dval = (Xval, yval), miscov- erage error rate α, desired explained variance σ, maximum number of clusters K, R repetitions, and a model fθ : RD → R Output: Adaptive intervals with 1 − α coverage 1: Part 1: Estimate conditional quantiles with quantile regression. 2: 3: Use QR to estimate ˆQ α 2 (x) and ˆQ1− α 2 (x) on Dtrain. 4: 5: Part 2: Weigh the X sets by the feature importance of each feature I1, I2, ..., ID. 6: 7: Get a list of feature importances I1, I2, ..., ID given by Algorithm (2) using Dcal for evaluation. 8: X ∗ cal, X ∗ val ← 0ncal×D, 0nval×D 9: for i ← 1 to D do 10: coli(X ∗ cal) ←coli(Xcal) × Ii 11: coli(X ∗ val) ←coli(Xval) × Ii 12: end for 13: 14: Part 3: Selection of k for K-means. 15: 16: k ← 2 17: bestk ← False 18: while k ≤ K and bestk is False do 19: Apply K-means as in Algorithm (1) on X ∗ cal. 20: Compute σk as in Eq.(14). 21: if σk > σ then 22: bestk ← True 23: Store the cluster centroids c1, c2, ..., ck 24: else 25: k ← k + 1 26: end if 27: end while 28: 29: Part 4: Compute ˆq(1), ˆq(2), ..., ˆq(k) on the calibration set, one per cluster. 30: 31: ϵ(1), ϵ(2), ..., ϵ(k) ← {}, {}, ...., {} 32: for (x ∗, x, y) in (X ∗ cal, Xcal, ycal) do 33: i ← argminj∈{1,2,...,k} ||x∗ − cj|| 34: ϵ(i) ← ϵ(i) ∪ { max ( ˆQ α 2 (x) − y, y − ˆQ1− α 2 (x) )} 35: end for 36: ˆq(i) ← Quantile (ϵ(i); 1 − α) , ∀i ∈ {1, ..., k} 37: 38: Part 5: Deploy PIs on new data. Exempliﬁed as (Xval, yval). 39: 40: for (x ∗, x, y) in (X ∗ val, Xval, yval) do 41: i ← argminj∈{1,2,...,k} ||x∗ − cj|| 42: Return ˆC1−α(x) ← [ ˆQ α 2 (x) − ˆq(i), ˆQ1− α 2 (x) + ˆq(i)] 43: end for 4 Experiments In this section we apply Algorithm (3) and compare it against CQR, QR and Naive on the datasets shown in Table (2). We use a FFNN (feedforward neural network) [22] with two output neurons to estimate Q α 2 (x), and Q1− α 2 (x), respectively. This is easily achieved by having a pinball loss function with τ = α/2 for the ﬁrst output neuron and τ = 1 − α/2 for the second. ˆQ α 2 (x) and ˆQ1− α 2 (x) represent the estimated lower bound, upper bound, with respect to x, respectively. Due to the stochastic behaviour of FFNN, the model is trained T=100 times to reduce the associated 7 variance of the random initialization. On top of these 100 trained models each of the aforementioned methods is applied. Each dataset in Table (2) is divided in three mutually exclusive datasets, a training set containing 50% of the data, a calibration set with 25%, and a evaluation set with the last 25%. Thereafter, each method is assessed on the evaluation set considering summary statistics of PIs width and coverage for α = 0.1 and a threshold explained variance of σk = 0.9. All the data and code, which is written in Python can be found here. Dataset N D Source Blogfeedback 60.000 280 [23] Boston house prices 506 13 [24] Bike sharing 17379 17 [25] Table 2: Datasets description. Blogfeedback (a) (b) Figure 5: (Blogfeedback) Feature importance (panel left). Number of clusters for a threshold fraction variance of 0.9 (panel right). Method min max mean std Q1 median Q3 IQR Naive 14.943970 20.984375 17.479034 0.976794 16.812687 17.417236 18.004509 1.191822 QR 0.000002 1546.067383 5.822205 17.578970 1.105402 2.070409 4.677099 3.571697 CQR 0.054895 1552.663086 12.865065 17.589760 8.126973 9.508371 12.118934 3.991961 ICQR 0.000061 1595.305754 23.701899 47.737405 2.431001 5.260328 35.063214 32.632213 Table 3: (Blogfeedback) PI width summary statistics. Method min max mean std Q1 median Q3 IQR Naive 0.899440 0.908237 0.903714 0.001837 0.902656 0.903872 0.904788 0.002132 QR 0.332600 0.828469 0.747061 0.092558 0.745585 0.778722 0.792366 0.046781 CQR 0.897908 0.906837 0.902400 0.002016 0.900756 0.902306 0.904105 0.003349 ICQR 0.894775 0.906371 0.901395 0.002302 0.899907 0.901706 0.902972 0.003065 Table 4: (Blogfeedback) Coverage summary statistics. 8 Boston house prices (a) (b) Figure 6: (Boston house prices) Feature importance (panel left). Number of clusters for a threshold fraction variance of 0.9 (panel right). Method min max mean std Q1 median Q3 IQR Naive 9.530071 12.689045 10.898064 0.602834 10.478755 10.807868 11.260028 0.781272 QR 1.324307 32.966770 8.323204 2.736615 6.490421 7.689121 9.650486 3.160065 CQR 2.978894 36.200993 10.921992 2.769641 9.070092 10.472181 12.346348 3.276256 ICQR 1.812310 44.346356 12.020735 4.937976 8.044880 10.293163 15.649653 7.604772 Table 5: (Boston house prices) PI width summary statistics. Method min max mean std Q1 median Q3 IQR Naive 0.889764 0.976378 0.940472 0.018104 0.929134 0.944882 0.952756 0.023622 QR 0.771654 0.889764 0.837795 0.025148 0.818898 0.834646 0.858268 0.039370 CQR 0.866142 0.984252 0.938583 0.020290 0.929134 0.937008 0.952756 0.023622 ICQR 0.889764 0.984252 0.943228 0.019489 0.929134 0.944882 0.960630 0.031496 Table 6: (Boston house prices) Coverage summary statistics. Bike sharing (a) (b) Figure 7: (Bike sharing) Feature importance (panel left). Number of clusters for a threshold fraction variance of 0.9 (panel right). 9 Method min max mean std Q1 median Q3 IQR Naive 0.427124 7.491333 2.336785 1.362940 1.327354 2.135223 3.189590 1.862236 QR 0.001831 35.566101 3.324998 2.115591 2.047175 2.682456 3.896203 1.849028 CQR 0.000229 36.159119 2.763314 2.186563 1.385670 2.202379 3.450058 2.064388 ICQR 0.000165 25.988984 2.249955 1.470346 1.267076 1.949806 2.848970 1.581894 Table 7: (Bike sharing) PI width summary statistics. Method min max mean std Q1 median Q3 IQR Naive 0.891139 0.915535 0.900700 0.004449 0.897583 0.900575 0.903625 0.006041 QR 0.084695 1.000000 0.897445 0.193131 0.887687 0.987457 0.995224 0.107537 CQR 0.887227 0.910702 0.900242 0.005158 0.897756 0.900230 0.904028 0.006272 ICQR 0.889068 0.916686 0.902656 0.006272 0.897986 0.902762 0.906847 0.008861 Table 8: (Bike sharing) coverage summary statistics. Results discussion In the two ﬁrst datasets, QR is constantly undercovering; hence, QR bounds are in need of ICP to ensure 1 − α coverage. ICQR is clearly more adaptive to heteroscedasticity in comparison to CQR since it has generally higher std and IQR PI width, but lower median PI width, while still ensuring the same coverage, as seen in Table (3-6). In the last dataset (bike sharing), we have the opposite case, QR is overcovering as seen in Table (8). Consequently, we can reduce the bounds up to 1 − α coverage with ICP. Remarks • Naive bounds are not dependent on the input in any form nor adaptive. The interval amplitude is always equal to 2ˆq. The small deviation seen in the above tables is a result of training the model 100 times to reduce the variance associated with FFNN due to the random initialization process. • Evaluating PIs by just looking to their mean value is not adequate. Most times, naive method has the lowest mean value; however, it achieves 1 − α coverage not in a group-balanced way, overcovering easy inputs, and PIs have always the same amplitude, ignoring heteroscedasticity. Therefore, analyzing PIs width by quantile gives a better perspective regarding conditional coverage. Adaptive methods generally have higher mean PI width because they are not optimized for heteroscedastic residuals. Nevertheless, the median PI width is usually lower in adaptive/heteroscedastic methods. 5 Conclusion In this paper, we have proposed an improved version of CQR. Results demonstrate that our version is further adaptive to heteroscedasticity, hence ICQR is one step ahead towards conditional coverage. The major shortcoming of ICQR in comparison to CQR are the two additional steps to calculate permutation importance and perform K-means. Despite this minor disadvantage, ICQR offers eye-catching adaptive PIs in comparison to the classic CQR, which convey us to strongly endorse its use across any high-stakes tabular regression problem. Acknowledgments This work has been supported by COMPETE: POCI-01-0247-FEDER-039719 and FCT - Fundação para a Ciência e Tecnologia within the Project Scope: UIDB/00127/2020. References [1] Ethan Goan and Clinton Fookes. Bayesian neural networks: An introduction and survey. In Case Studies in Applied Bayesian Data Science, pages 45–87. Springer International Publishing, 2020. 10 [2] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859–877, apr 2017. [3] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1613–1622, Lille, France, 07–09 Jul 2015. PMLR. [4] Tom Heskes. Practical conﬁdence and prediction intervals. In Proceedings of the 9th International Conference on Neural Information Processing Systems, NIPS’96, page 176–182, Cambridge, MA, USA, 1996. MIT Press. [5] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050–1059. PMLR, 2016. [6] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. 2005. [7] Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [8] Arun Kumar Kuchibhotla. Exchangeability, conformal prediction, and rank tests, 2020. [9] Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives, 15(4):143–156, 2001. [10] Roger W Koenker and Gilbert Bassett. Regression quantiles. Econometrica, 46(1):33–50, 1978. [11] Ichiro Takeuchi, Quoc Le, Timothy Sears, and Alexander Smola. Nonparametric quantile estimation. Journal of Machine Learning Research, 7:1231–1264, 12 2006. [12] Ingo Steinwart and Andreas Christmann. Estimating conditional quantiles with the help of the pinball loss. Bernoulli, 17(1), feb 2011. [13] Danijel Kivaranovic, Kory D. Johnson, and Hannes Leeb. Adaptive, distribution-free prediction intervals for deep networks, 2019. [14] Matteo Sesia and Emmanuel J. Candè s. A comparison of some conformal quantile regression methods. Stat, 9(1), jan 2020. [15] Kardi Teknomo. K-means clustering tutorial. Medicine, 100(4):3, 2006. [16] André Altmann, Laura Tolo¸si, Oliver Sander, and Thomas Lengauer. Permutation importance: a corrected feature importance measure. Bioinformatics, 26(10):1340–1347, 2010. [17] Anastasios N. Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantiﬁcation, 2021. [18] Fei Wang, Hector-Hugo Franco-Penya, John D Kelleher, John Pugh, and Robert Ross. An analysis of the application of simpliﬁed silhouette to the evaluation of k-means clustering validity. In International Conference on Machine Learning and Data Mining in Pattern Recognition, pages 291–305. Springer, 2017. [19] Purnima Bholowalia and Arvind Kumar. Ebk-means: A clustering technique based on elbow method and k-means in wsn. International Journal of Computer Applications, 105(9), 2014. [20] D. Pham, Stefan Dimov, and Cuong Nguyen. Selection of k in k -means clustering. Proceedings of The Institution of Mechanical Engineers Part C-journal of Mechanical Engineering Science - PROC INST MECH ENG C-J MECH E, 219:103–119, 01 2005. [21] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical report, Stanford, 2006. [22] Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. Introduction to multi-layer feed-forward neural networks. Chemometrics and Intelligent Laboratory Systems, 39(1):43–62, 1997. [23] Blogfeedback dataset. https://archive.ics.uci.edu/ml/datasets/BlogFeedback. Accessed: 2022-07- 5. [24] Boston house prices dataset. https://www.kaggle.com/datasets/vikrishnan/boston-house-prices. Accessed: 2022-07-5. [25] Bike sharing dataset. https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset. Accessed: 2022-07-5. 11","libVersion":"0.3.2","langs":""}
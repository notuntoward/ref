{"path":"lit/sources/papers_added/papers/Mohammadi20EmulatingComputerModels.pdf","text":"Emulating computer models with step-discontinuous outputs using Gaussian processes Hossein Mohammadi ∗1, 2, Peter Challenor1, 2, Marc Goodfellow 1, 2, and Daniel Williamson 1 1College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, UK 2EPSRC Centre for Predictive Modelling in Healthcare, University of Exeter, Exeter, UK Abstract In many real-world applications we are interested in approximating costly functions that are analytically unknown, e.g. complex computer codes. An emulator provides a “fast” approximation of such functions relying on a limited number of evaluations. Gaussian processes (GPs) are commonplace emulators due to their statistical prop- erties such as the ability to estimate their own uncertainty. GPs are essentially de- veloped to ﬁt smooth, continuous functions. However, the assumptions of continuity and smoothness is unwarranted in many situations. For example, in computer mod- els where bifurcations or tipping points occur, the outputs can be discontinuous. This work examines the capacity of GPs for emulating step-discontinuous functions. Several approaches are proposed for this purpose. Two “special” covariance functions/kernels are adapted with the ability to model discontinuities. They are the neural network and Gibbs kernels whose properties are demonstrated using several examples. Another approach, which is called warping, is to transform the input space into a new space where a GP with a standard kernel, such as the Mat´ern family, is able to predict the function well. The transformation is performed by a parametric map whose parameters are estimated by maximum likelihood. The results show that the proposed approaches have superior performance to GPs with standard kernels in capturing sharp jumps in the “true” function. Keywords: Covariance kernel, Discontinuity, Emulator, Gaussian processes, Warping. 1 Introduction Computer models (or simulators) are widely used in many applications ranging from modelling the ocean and atmosphere [1, 6] to healthcare [2, 7]. By simulating real- ∗Corresponding Author: h.mohammadi@exeter.ac.uk 1arXiv:1903.02071v4 [stat.ME] 30 Sep 2020 world phenomena, computer models allow us to better understand/analyse them as a complement to conducting physical experiments. However, on the one hand, the simulators are often “black box” since they are available as commercial packages and we do not have access to their internal procedures. On the other hand, they are computationally expensive due to the fact that each simulation outcome is actually the solution of some complex mathematical equations, such as partial diﬀerential equations. One of the main purposes of using a computer model is to perform prediction. However, the accuracy of the prediction is questionable because simulators are simpli- ﬁcations of physical phenomena. In addition, due to factors such as lack of knowledge or measurement error, the inputs to the model are subject to uncertainty which yield uncertain outputs. Under this condition, decision makers need to know how good the prediction is. In other words, they need an estimation of the uncertainty propagated through the model [27]. This entails running the simulator very many times which is impractical in the context of time-consuming simulators. To overcome this compu- tational complexity, one can replace the simulator with an emulator which is fast to run. Emulation is a statistical approach for representing unknown functions by approx- imating the input/output relationship based on evaluations at a ﬁnite set of points. Gaussian process (GP) models (also known as kriging) are widely used to predict the outputs of a complex model and are regarded as an important class of emulators [28]. GPs are nonparametric probabilistic models that provide not only a mean predictor but also a quantiﬁcation of the associated uncertainty. They have become a standard tool for the design and analysis of computer experiments over the last two decades. This includes uncertainty propagation [26, 21], model calibration [20, 17], design of experiments [34, 31], optimisation [19, 3] and sensitivity analysis [27, 18]. GPs can be applied to ﬁt any smooth, continuous function [24]. The basic assump- tion when using a GP emulator is that the unknown function depends smoothly on its input parameters. However, there are many situations where the model outputs are not continuous. It is very common in computer models that at some regions of the input space, a minor change in the input parameters leads to a sharp jump in the out- put. For example, models described by nonlinear diﬀerential equations often exhibit diﬀerent modes (phases). Shifting from one mode to another relies on a diﬀerent set of equations which raises a discontinuity in the model output. To our knowledge, there are only a few studies that investigate the applicability of GPs in modelling discontinuities. The reason may be due to the fact that they are essentially developed to model smooth and continuous surface forms. However, a natural way of emulating discontinuous functions is to partition the input space by ﬁnding discontinuities and then ﬁt separate GP models within each partition. In [4], for example, a simulator with tipping point behaviour is emulated such that the boundary of the regions with discontinuity is found ﬁrst and the simulator output is emulated separately in each region. It is reported that ﬁnding the discontinuous regions is a time-consuming operation. The treed Gaussian process (TGP) [14] is a popular model introduced by Gramacy and Lee. The TGP makes binary splits (parallel to the input axes) on the value of a single variable recursively such that each partition (leaf of the tree) is a subregion of the previous section. Then, an independent stationary GP emulator is applied within each 2 section. The disadvantage of the TGP is that it requires many simulation runs which is not aﬀordable in the context of computationally expensive simulators. A similar approach is presented in [30] where Voronoi tessellation is applied to partition the input space. The procedure uses the reversible jump Markov chain Monte Carlo [15] that is time-consuming. In [12] a two-step method is proposed for emulating cardiac electrophysiology models with discontinuous outputs. First a GP classiﬁer is employed to detect boundaries of discontinuities and then the GP emulator is built subject to these boundaries. Here we provide an alternative perspective in which a single kernel is used to cap- ture discontinuities. The advantage is that there is no need to detect discontinuous boundaries separately which is burdensome. The proposed methods include two non- stationary covariance functions, namely the neural network (NN) [40, 32] and Gibbs kernels [13, 29], and the idea of warping the input space [5]. The NN kernel was ﬁrst derived by Williams [40] and relies on the correspondence between GPs and single-layer neural networks with inﬁnite number of hidden units (neurons) and random weight pa- rameters [24]. As a result, the NN kernel is more expressive than standard kernels in modelling complex data structures. In the Gibbs kernel the parameter that regu- lates the correlation between observations is a function of the inputs which makes that kernel more ﬂexible than the classical covariance functions. The warping technique has been already proven to be successful in modelling nonstationary functions, see e.g. [36, 39, 23]. A warped kernel is obtained by applying a deterministic non-linear transformation to its inputs. This can be regarded as a special case of “Deep Gaussian processes” which is a functional composition of multiple GPs [8, 35]. In this work we show how these techniques coming from machine learning can be employed in the ﬁeld of computer experiments to emulate models that present very steep variations. 2 Overview of Gaussian process emulators The random (or stochastic) process Z = (Z(x))x∈D, i.e. a collection of random vari- ables indexed by the set D, is a Gaussian process if and only if ∀ N ∈ N, ∀ xj ∈ D, (Z(x1), . . . , Z(xN ))⊤ has a multivariate normal distribution on RN [33]. Let (Ω, B, P), where Ω is a sample space, B is a sigma-algebra and P is a probability measure, be the probability space on which Z(x) is deﬁned: Z : (x, ω) 7→ Z(x, ω) , (x, ω) ∈ D × (Ω, B, P) . For a given ωo ∈ Ω, Z(·, ωo) is called a sample path (or realisation) and for a given xo ∈ D, Z(xo, ·) is a Gaussian random variable. In this framework, GPs can be regarded as the probability distribution over functions such that the function being approximated is considered as a particular realisation of the distribution. Herein, f : D 7→ F denotes the unknown function that maps the input space D ⊂ Rd to the output space F. In this work, F = R. A GP is fully determined by its mean function µ(·) and covariance kernel k(·, ·) which are deﬁned as: 3 Z ∼ GP (µ(·), k(·, ·)) ; µ : D 7→ R , µ(x) = E [Z(x)] k : D × D 7→ R , k(x, x′) = Cov (Z(x), Z(x′)). While µ could be any function, k needs to be symmetric positive semideﬁnite. The function µ captures the global trend and k controls the structure of sample paths such as diﬀerentiability, symmetry, periodicity, etc. In this work, the GP mean is assumed to be an unknown constant which is estimated from data, see Equation (6). The notation µ is (slightly abusively) used to denote the value of this constant. Generally, covariance functions are divided into two groups: stationary and nonsta- tionary. Stationary kernels depend only on the separation vector x − x′. As a result, they are translation invariant in the input space: k(x, x′) = k(x + τ , x′ + τ ) , τ ∈ Rd. (1) One of the most common covariance functions is the squared exponential (SE) kernel whose (separable) form is given by kSE(x, x′) = σ2 d∏ i=1 exp (− |xi − x′ i| 2 2l2 i ) . (2) Here, the parameters σ2 and li are called process variance and correlation length- scale along the i-th coordinate, respectively. The former determines the scale of the amplitude of sample paths and the latter regulates how quickly the spatial correlation decays. In this paper, these parameters are estimated via maximum likelihood (ML) [33, 19], see Appendix A. Figure 1 shows the shape of the SE kernel and two sample paths with diﬀerent length-scales. Another important class of stationary kernels is the Mat´ern covariance function [33]. Nonstationary kernels are applied to model functions that do not have uniform smoothness within the input space and change signiﬁcantly in some regions compared to others [41]. In Sections 3 and 4 two nonstationary covariance functions, namely the neural network and Gibbs kernels, are studied. The GP prediction of f is obtained by conditioning Z on function evaluations. Let X = (x1, . . . , xn)⊤ denote n sample locations in the input space and y = (f (x 1), . . . , f (x n))⊤ represent the corresponding outputs (observations). Together, the set A = {X, y} is called the training set. The conditional distribution of Z on A is again a GP Z|A ∼ GP (m(·), c(·, ·)) , (3) speciﬁed by m(x) = ˆµ + k(x)⊤K−1 (y − ˆµ1) (4) c(x, x′) = k(x, x′) − k(x) ⊤K −1k(x′) + (1 − 1⊤K −1k(x′) )2 (1⊤K−11) . (5) Here, ˆµ is the ML estimate of µ obtained by [33] ˆµ = 1 ⊤K −1y 1⊤K−11 . (6) 4 x -4 -2 0 2 4 x′ -4 -2 0 2 4 0.0 0.2 0.4 0.6 0.8 1.0 -4 -2 0 2 4-2-10123 x kSE(x,x′;l =0.1) kSE(x,x′;l =1) Figure 1: Left: shape of the squared exponential kernel. Right: sample paths corresponding to the SE kernel with l = 0.1 (solid) and l = 1 (red dashed). In both cases σ2 = 1 Also, k(x) = (k(x, x1), . . . , k(x, x n))⊤, K is an n×n covariance matrix whose elements are: Kij = k(xi, xj) , ∀i, j ; 1 ≤ i, j ≤ n and 1 is a n × 1 vector of ones. We call m(x) and s 2(x) = c(x, x) the GP mean and variance which reﬂect the prediction and the associated uncertainty at x, respectively. It can be shown that in the classic covariance functions such as Mat´ern kernel where k(x, x; σ2 = 1) = 1, the predictive mean expressed by Equation (4) interpolates the points in the training set. Also, the prediction uncertainty (Equation (5)) vanishes there. To clarify, we obtain the prediction and the associated uncertainty at x = xj, the j-th training point. In this case, k(xj) is equivalent to the j-th column of the covariance matrix K. Because K is a positive deﬁnite matrix, the term k(xj) ⊤K −1 yields vector ej = (0, . . . , 0, 1, 0, . . . , 0) whose elements are zero except the j-th element which is one. As a result m(xj) = ˆµ + ej z }| { k(xj) ⊤K −1(y − ˆµ1) = f (xj), (7) s 2(xj) = k(xj, x j) − k(xj) ⊤K −1k(xj) + (1 − 1⊤K −1k(xj))2 1⊤K−11 = 0, (8) since k(xj) = (k(x j, x1), . . . , σ2 z }| { k(xj, xj), . . . , k(xj, xn))⊤. 5 3 Neural network kernel In this section we ﬁrst show how the NN kernel is derived from a single-layer neural network with inﬁnite number of hidden units. Let ˜f (x) be a neural network with Nh units that maps inputs to outputs according to ˜f (x) = b + Nh∑ j=1 vjh(x; u j), (9) where b is the intercept, vjs are weights to the units, h(·) represents the transfer (activa- tion) function in which u j represents the weight assigned to the input x. Suppose b and every vj have zero mean Gaussian distribution with variances σ2 b and σ2 v/Nh, respec- tively. If u js have independent and identical distribution, then the mean and covariance of ˜f (x) are (w represents all weights together, i.e. w = [b, v1, . . . vNh , u 1, . . . , u Nh ]) Ew[ ˜f (x) ] = 0 , (10) Cov ( ˜f (x), ˜f (x′)) = Ew[ ( ˜f (x) − 0)( ˜f (x ′) − 0)] = σ2 b + 1 Nh Nh∑ j=1 σ2 vEu[ h(x; u j)h(x ′; u j) ] = σ2 b + σ2 vEu[h(x; u)h(x′; u) ] . (11) Since ˜f (x) is the sum of independent random variables, it tends towards a normal distribution as Nh → ∞ according to the central limit theorem. In this situation, any collection { ˜f (x1), . . . , ˜f (xN )| ∀N ∈ N} has a joint normal distribution and ˜f (x) becomes a zero mean Gaussian process with a covariance function speciﬁed in Equation (11). The neural network kernel is a particular case of the covariance structure expressed by Equation (11) such that h(x; u) = erf (u0 + ∑d i=1 uixi) [40]. Here, erf(·) is the error function: erf(x) = 2√π ∫ x 0 exp(−t2)dt and u ∼ N (0, Σ) in which Σ is a diagonal matrix with elements σ2 0, σ2 1, . . . , σ2 d as the variances of u0, u1, . . . , ud. This choice of the activation function leads to the neural network kernel given by kN N (x, x′) = 2σ2 π arcsin ( 2˜x ⊤Σ˜x ′ √(1 + 2˜x⊤Σ˜x)(1 + 2˜x′⊤Σ˜x′) ) . (12) where ˜x = (1, x1, . . . , xd)⊤ is the augmented input vector. The length-scale of the i-th coordinate is of order 1/σi; the larger σi, the sample functions vary more quickly in the i-th coordinate [22, 33]. This is illustrated in Figure 2 where the shapes of the NN kernel for two diﬀerent values of σ1 and the corresponding sample paths are plotted. The NN kernel is nonstationary, see Figure 2 and also Equation (12) which does not depend on x − x′. It can take negative values contrary to classic covariance functions such as the SE kernel depicted in Figure 1. In this kernel due to the superposition of the function erf(u0 + u1x), sample paths tend to constant values for large positive or 6 x -4 -2 0 2 4 x ′ -4 -2 0 2 4 -0.5 0.0 0.5 x -4 -2 0 2 4 x ′ -4 -2 0 2 4 -0.5 0.0 0.5 -4 -2 0 2 4-1.0-0.50.00.51.0 x -4 -2 0 2 4-1.0-0.50.00.51.0 x Figure 2: Top: The shapes of the NN kernel for two diﬀerent values of σ1: 1 (left) and 50 (right). Bottom: Two sample paths corresponding to the kernels on top. The kernel with σ1 = 50 is a more suitable choice for modelling discontinuities. Here, σ = σ0 = 1. 7 negative x [33]. Also, the correlation at zero distance is not one: Corr(Z(x), Z(x)) = kN N (x, x; σ2 = 1) = 2 π arcsin ( 2˜x⊤Σ˜x 1 + 2˜x⊤Σ˜x ) < 1, (13) since arcsin ( 2˜x ⊤Σ˜x 1+2˜x⊤Σ˜x ) < π/2. Thus, the mean predictor obtained by the NN kernel does not interpolate the points in the training data and the prediction variances are greater than zero there. Figure 3 compares the Mat´ern 3/2 and NN kernels in modelling a step-function deﬁned as f (x) = {−1 x1 ≤ 0 1 x1 > 0 . (14) As can be seen, the NN kernel has superior performance to Mat´ern 3/2 in both 1D and 2D cases. The predictive mean of the GP with Mat´ern 3/2 neither captures the discontinuity nor performs well in the ﬂat regions. In the NN kernel, the ML estimation of the parameter that controls the horizontal scale of ﬂuctuation, i.e. σ1, takes its maximum possible value which is 103. Figure 4 illustrates a function whose step-discontinuity is located at x = 0.5. As can be seen from the picture on the left of Figure 4, the NN kernel is not able to model f well. This problem can be solved if the NN kernel is modiﬁed as follows k(x, x ′) = 2σ2 π arcsin   2˜x ⊤ τ Σ−1 ˜x ′ τ√(1 + 2˜x⊤ τ Σ −1 ˜xτ )(1 + 2˜x′⊤ τ Σ−1 ˜x′ τ )   , (15) where ˜xτ = (1, x − τ )⊤ and τ is estimated together with other parameters using ML. In this case, ˆτ = 0.457 which is an estimation for the location of the discontinuity. 4 Gibbs kernel Mark Gibbs [13] in his PhD thesis derived the following covariance function: kGib(x, x′) = σ2 d∏ i=1 ( 2li(x)li(x′) li(x)2 + li(x′)2 )1/2 exp ( − d∑ i=1 (xi − x′ i)2 li(x)2 + li(x′)2 ) , (16) where li(·) is a length-scale function in the i-th input dimension. These length-scales can be any arbitrary positive functions of x. This allows the kernel to model sudden variations in the observations: a process with Gibbs kernel is smooth at regions of the input space where the length-scales are relatively high and it changes rapidly where the length-scales reduce. Note that the correlation is one when x = x ′, i.e. kGib(x, x) = 1. In this work, we use the same length-scale functions for all dimensions: kGib(x, x′) = σ2 ( 2l(x)l(x′) l2(x) + l2(x′) )d/2 exp ( − ∑d i=1(xi − x′ i) 2 l2(x) + l2(x′) ) . (17) 8 -2-10 1 2-2.0-1.00.00.51.01.5 xf f Prediction, m(x) m(x)±2s(x) Training data -2-10 1 2-1.0-0.50.00.51.0 xf f Prediction, m(x) m(x)±2s(x) Training datax1 -2 -1 0 1 2x2 -2 -1 0 1 2 -1.0 -0.5 0.0 0.5 1.0x1 -2 -1 0 1 2x2 -2 -1 0 1 2 -1.0 -0.5 0.0 0.5 1.0 Figure 3: Emulation of the step-function f deﬁned in (14) with the Mat´ern 3/2 (left panel) and NN (right panel) kernels in 1D (top row) and 2D (bottom row). Red points are the training data. In the NN kernel, 1D: ˆσ1 = 103 which is the upper bound in the likelihood optimisation. 2D: ˆσ1 = 103 (corresponding to x1) and ˆσ1 = 10−2 (corresponding to x2) which is the upper bound in the likelihood optimisation. 9 -2-10 1 2-6-4-202 xf f Prediction, m(x) m(x)±2s(x) Training data -2-10 1 2-1.0-0.50.00.51.0 xf f Prediction, m(x) m(x)±2s(x) Training data Figure 4: Left: The NN kernel given by Equation (12) is not able to well-approximate f (dashed red) whose discontinuity is at 0.5. Right: The modiﬁed NN kernel based on Equation (15) can well-model f . The ML estimation of τ is 0.457 which is the estimated location of the discontinuity. Figure 5 shows the shapes of the Gibbs kernel for three diﬀerent length-scale func- tions and corresponding sample paths. As can be seen, it is possible to model both nonstationary and discontinuous functional forms with the Gibbs kernel if a suitable length-scale function is chosen. For example, the nonstationary function depicted in Figure 6 varies more quickly in the region x ∈ [0, 0.3] than in the region [0.3, 1]. Thus, a suitable length-scale function should have “small” values when x ∈ [0, 0.3] and larger values when x ∈ [0.3, 1]. The length-scale used for the Gibbs kernel (right picture) is of the form l(x) = c1x2 + c2 whose unknown parameters c1 and c2 are estimated by ML. This choice of the length-scale allows the GP to predict f with a higher accuracy in comparison to the GP with the Mat´ern 3/2 kernel (left picture). The estimated parameters of the length-scale are ˆc1 ≈ 45.63 and ˆc2 ≈ 0.11 which are in line with the nonstationarity of f . In order to model discontinuities, one can employ the Gibbs kernel with a sigmoid shaped length-scale. The length-scale functions we use in our experiments (see Section 6) are all sigmoid functions, speciﬁcally: (i) Error function: erf(c1ejx) + c2; c2 > 1 (ii) Logistic function: 1 1+exp(c1ej x) + c2; c2 > 0 (iii) Hyperbolic tangent: tanh(c1ejx) + c2; c2 > 1 (iv) Arctangent: arctan(c1ejx) + c2; c2 > π/2 which have all been modiﬁed slightly by adding a constant c2 > 0 to make l(x) strictly 10 -4-2024024681012 xl(x) l(x)=0.5x2+0.1 x -4-2 0 2 4 x ′ -4 -20 24 0.2 0.4 0.6 0.8 1.0 -4-2024-2-1012 x-4-20241.01.21.41.61.82.02.2 xl(x) l(x)= 11+exp(30x)+1 x -4-2 0 2 4 x ′ -4-2024 0.2 0.4 0.6 0.8 1.0 -4-2024-2.5-1.5-0.50.5 x-4-202401234 xl(x) l(x)={0.4,x≤0 4,x>0 x -4-2024 x ′ -4 -2 0 240.00.2 0.4 0.6 0.8 1.0 -4-2024-2-1012 x Figure 5: Left panel: three diﬀerent length-scale functions. Middle panel: shapes of the Gibbs kernel based on the corresponding length-scale functions. Right panel: two GP sample paths with the Gibbs kernel on the left. With the Gibbs kernel, one can model both nonstationary (ﬁrst row) and discontinuous (second and third rows) functions. 11 0.00.20.40.60.81.0-0.50.00.5 xf f Prediction (Matern 3/2) 0.00.20.40.60.81.0-0.6-0.4-0.20.00.20.4 xf f Prediction (Gibbs) Figure 6: GP prediction (solid blue) of a nonstationary function (dashed) with the Mat´ern 3/2 (left) and Gibbs (right) kernels. The function is deﬁned as f (x) = sin (30(x − 0.9)4) cos (2(x − 0.9)) + (x−0.9) 2 [41] which varies more quickly in the region x ∈ [0, 0.3] than in the region [0.3, 1]. The length-scale function used in the Gibbs kernel is l(x) = c1x2 + c2 whose parameters are estimated by ML: ˆc1 ≈ 45.63 and ˆc2 ≈ 0.11. The shaded area represents the prediction uncertainty and the red points are the training data. 12 positive. The parameter c1 controls the slope of the transition in the sigmoid function. Both c1 and c2 are estimated by ML. All components of the vector ej are zero except the j-th one which is 1. This vector determines the j-th axis in which the function is discontinuous. 5 Transformation of the input space (warping) In this section, warping or embedding is studied as an alternative approach to emulate functions with discontinuities. The method ﬁrst uses a non-linear parametric function to map the input space into a feature space. Then, a GP with a standard kernel is applied to approximate the map from the feature space to the output space [22, 5]. A similar idea is used in [38] where the transformation is performed on the output space to model non-Gaussian processes. In warping, we assume that f is a composition of two functions f = G ◦ M : M : D 7→ D′ , G : D′ 7→ F, (18) where M is the transformation function and D′ represents the feature space. The function G is approximated by a GP relying on the training set { ˜X, Y} in which ˜X = (M (x 1), . . . , M (xn) )⊤. Notice that D and D′ need not have the same dimensionality [22]. For example, if the squared exponential kernel kSE : R × R 7→ R is composed with M (x) = [ cos( 2πx T ), sin( 2πx T )]⊤ ∈ R2, the result is a periodic kernel with period T [37, 16]. In practice, a parametric family of M is selected and its parameters are estimated together with the kernel parameters via ML. Such modelling is equivalent to emulate f with a GP whose covariance function ˜k is ˜k(x, x′) = k (M (x), M (x′)) . (19) Note that ˜k is generally nonstationary even if k is a stationary kernel, see Figure 7. The prediction (conditional mean) and the associated uncertainty (conditional variance) of a warped GP are calculated in the same way as Equations (4) and (5). According to Figure 7 (second row), a sigmoid transformation is a suitable choice for modelling step-discontinuities. The sigmoid functions given in Section 4 are used as the transformation mappings in our experiments in the next section. The unknown parameter of the map, i.e. c1, is estimated by the ML method together with other kernel parameters such as the length-scales and process variance. 6 Numerical examples In this section, the performance of the proposed methods in modelling step-discontinuities is compared with the standard kernels, i.e. Mat´ern 3/2 and squared exponential. The step-function given by Equation (14) is used as the test function in dimensions 2 and 5. The input space is D = [−2, 2]d. Four sigmoid functions are employed as the length- scales of the Gibbs kernel, l(x), and the transformation maps, M (x), in the warping method. The sigmoid functions are: error, logistic, hyperbolic tangent and arctangent 13 -4-2024-1.0-0.50.00.51.0 xM(x) M(x)=sin(1.5x)x -4 -2 0 2 4x′ -4-2024 0.20.40.60.81.0 -4-2024-1.0-0.50.00.51.0 x-4-20240.00.20.40.60.81.01.2 xM(x) M(x)= 11+exp(10x) x -4-20 2 4 x ′ -4 -2 0 24 0.70.80.91.0 -4-2024-1.0-0.50.0 x Figure 7: Left panel: two diﬀerent transformation functions, M (x). Middle panel: shapes of the warped kernel ˜k (x, x′) = k (M (x), M (x′)) in which k is the squared exponential ker- nel. Right panel: two sample paths of a GP with the covariance function ˜k. As can be seen, a sigmoid transformation function is a suitable choice for modelling step-discontinuities. 14 whose analytical expressions are given in Section 4. The covariance kernel, k, in the warping approach is squared exponential. The accuracy of the prediction is measured by the root mean square error (RMSE) criterion deﬁned as RM SE = v u u √ 1 nt nt∑ t=1 (f (xt) − ˆf (xt) )2, (20) where xt and nt represent a test point and the size of test set, respectively. In our experiments nt = 1000. There are 20 diﬀerent training sets and for each set, each method produces one prediction. All training sets are of size 10d and “space-ﬁlling”, meaning that the sample points are uniformly spread over the input space. They are obtained by the maximinESE LHS function implemented in the R package DiceDesign [9]. The results are shown in Figure 8. As can be seen, the squared exponential (SquarExp) and Mat´ern 3/2 (Mat32) kernels have the worst prediction performances. The neural network kernel (NeurNet) can model the step-function well and has one of the best RMSEs in our experiments. Generally, the GP model with the Gibbs kernel outperforms the warping technique. The arctangent function is a suitable choice as the length-scale of the Gibbs kernel and the transformation map in the warping approach. In both cases, the RMSE associated with the logistic function is (on average) the largest in comparison to other sigmoid functions. 7 Conclusions Gaussian processes are mainly used to predict smooth, continuous functions. However, there are many situations in which the assumptions of continuity and smoothness do not hold. In computer experiments, it is common that the output of a complex computer code has discontinuity, e.g. when bifurcations or tipping points occur. This paper deals with the problem of emulating step-discontinuous functions using GPs. Several methods, including two covariance kernels and the idea of transforming the input space (warping), are proposed to tackle this problem. The two covariance functions are the neural network and Gibbs kernels whose properties are demonstrated using several examples. In warping, a suitable transformation function is applied to map the input space into a new space where a standard kernel, e.g. Mat´ern family of kernels, is able to predict the discontinuous function well. Our experiments show that these techniques have superior performance to GPs with standard kernels in modelling step- discontinuities. Acknowledgements The authors gratefully acknowledge the ﬁnancial support of the EPSRC via grant EP/N014391/1. 15 Gibs_arctan Gibs_erf Gibs_logistic Gibs_tanh Mat32 NeurNet SquarExp Warp_arctan Warp_erf Warp_logistic Warp_tanh0.00.10.20.30.4 RMSE Gibs_arctan Gibs_erf Gibs_logistic Gibs_tanh Mat32 NeurNet SquarExp Warp_arctan Warp_erf Warp_logistic Warp_tanh0.00.10.20.30.4 RMSE Figure 8: Box-plot of RMSEs associated with the prediction of the step-function (Equation (14)) in 2D (left) and 5D (right) using the methods presented in this work: neural net- work kernel, Gibbs kernel and warping. Four sigmoid functions (error, logistic, hyperbolic tangent and arctangent) are considered as the length-scale of the Gibbs kernel and the transformation function in the warping approach. The two standard kernels, i.e. squared exponential (SquarExp) and Mat´ern 3/2 (Mat32), have the worst prediction performances. The best results are obtained by the neural network kernel, Gibbs kernel (with arctangent) and warping (with arctangent). 16 References [1] A. Adcroft, C. Hill, J-M. Campin, J. Marshall, and P. Heimbach. Overview of the formulation and numerics of the MIT GCM. In Seminar on recent developments in numerical methods for atmospheric and ocean modelling, pages 139–150, Shinﬁeld Park, Reading, September 2004. [2] P.J. Birrell, G. Ketsetzis, N.J. Gay, B.S. Cooper, A.M. Presanis, R.J. Harris, A. Charlett, X-S. Zhang, P.J. White, R.G. Pebody, and Angelis D. De. Bayesian modeling to unmask and predict inﬂuenza A/H1N1 pdm dynamics in London. Proceedings of the national academy of sciences of The United States of America, 108:18238–18243, 2011. [3] Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian opti- mization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. CoRR, abs/1012.2599, 2010. [4] C.C.S. Caiado and M. Goldstein. Bayesian uncertainty analysis for complex phys- ical systems modelled by computer simulators with applications to tipping points. Communications in Nonlinear Science and Numerical Simulation, 26(1):123 – 136, 2015. [5] R. Calandra, J. Peters, C. E. Rasmussen, and M. P. Deisenroth. Manifold Gaus- sian processes for regression. In 2016 International Joint Conference on Neural Networks (IJCNN), pages 3338–3345, 2016. [6] Peter Challenor. The probability of rapid climate change. Signiﬁcance, 1(4):155– 158, 2004. [7] Proctor CJ., Boche D., Gray DA., and Nicoll JAR. Investigating interventions in alzheimers disease with computer simulation models. PLoS ONE, 8(9):e73631, 2013. [8] Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In Proceedings of the Sixteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 207–215, Scotts- dale, Arizona, USA, 29 Apr–01 May 2013. PMLR. [9] Delphine Dupuy, C´eline Helbert, and Jessica Franco. DiceDesign and DiceEval: two R packages for design and analysis of computer experiments. Journal of Statistical Software, 65(11):1–38, 2015. [10] David Duvenaud. Automatic model construction with Gaussian processes. PhD thesis, Computational and Biological Learning Laboratory, University of Cam- bridge, 2014. [11] Alexander I. J. Forrester, Andras Sobester, and Andy J. Keane. Engineering design via surrogate modelling - A practical guide. Wiley, 2008. [12] Sanmitra Ghosh, David J. Gavaghan, and Gary R. Mirams. Gaussian process emulation for discontinuous response surfaces with applications for cardiac elec- trophysiology models. arXiv e-prints, page arXiv:1805.10020, 2018. [13] Mark N. Gibbs. Bayesian Gaussian processes for regression and classiﬁcation. PhD thesis, Department of Physics, University of Cambridge, 1997. 17 [14] Robert B. Gramacy and Herbert K. H. Lee. Bayesian treed Gaussian process mod- els with an application to computer modeling. Journal of the American Statistical Association, 103(483):1119–1130, 2008. [15] Peter J. Green. Reversible jump markov chain monte carlo computation and Bayesian model determination. Biometrika, 82:711–732, 1995. [16] N. Haji-Ghassemi and M.P. Deisenroth. Approximate inference for long-term forecasting with periodic Gaussian processes. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2014. [17] Dave Higdon, James Gattiker, Brian Williams, and Maria Rightley. Computer model calibration using high-dimensional output. Journal of the American Sta- tistical Association, 103(482):570–583, 2008. [18] Bertrand Iooss and Paul Lemaˆıtre. A review on global sensitivity analysis meth- ods. In Uncertainty Management in Simulation-Optimization of Complex Systems: Algorithms and Applications. Springer, 2015. [19] Donald R. Jones, Matthias Schonlau, and William J. Welch. Eﬃcient global optimization of expensive black-box functions. Journal of Global Optimization, 13(4):455–492, 1998. [20] Marc C. Kennedy and Anthony O’Hagan. Bayesian calibration of computer mod- els. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3):425–464, 2001. [21] Brian A. Lockwood and Mihai Anitescu. Gradient-enhanced universal kriging for uncertainty propagation. Nuclear Science and Engineering, 170(2):168–195, 2012. [22] D. J. C. MacKay. Introduction to Gaussian processes. In C. M. Bishop, editor, Neural Networks and Machine Learning, pages 133–166. Springer-Verlag, 1998. [23] Sbastien Marmin, David Ginsbourger, Jean Baccou, and Jacques Liandrat. Warped Gaussian processes and derivative-based sequential designs for functions with heterogeneous variations. SIAM/ASA Journal on Uncertainty Quantiﬁca- tion, 6(3):991–1018, 2018. [24] Radford M. Neal. Regression and classiﬁcation using Gaussian process priors. pages 475–501. Bayesian Statistics 6, Oxford University Press, 1998. [25] H. B. Nielsen, S. N. Lophaven, and J. Søndergaard. DACE – A Matlab krig- ing toolbox – version 2.0. Informatics and Mathematical Modelling, Technical University of Denmark, DTU, 2002. [26] Jeremy Oakley. Estimating percentiles of uncertain computer code outputs. Jour- nal of the Royal Statistical Society: Series C (Applied Statistics), 53(1):83–93, 2004. [27] Jeremy E. Oakley and Anthony O’Hagan. Probabilistic sensitivity analysis of complex models: a Bayesian approach. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 66(3):751–769, 2004. [28] A. O’Hagan. Bayesian analysis of computer code outputs: A tutorial. Reliability Engineering & System Safety, 91(10-11):1290–1300, 2006. 18 [29] Christopher J. Paciorek and Mark J. Schervish. Nonstationary covariance func- tions for Gaussian process regression. In S. Thrun, L. K. Saul, and B. Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16, pages 273–280. MIT Press, 2004. [30] C. A. Pope, J. P. Gosling, S. Barber, J. Johnson, T. Yamaguchi, G. Feingold, and P. Blackwell. Modelling spatial heterogeneity and discontinuities using Voronoi tessellations. ArXiv e-prints, 2018. [31] Luc Pronzato and Werner G. M¨uller. Design of computer experiments: space ﬁlling and beyond. Statistics and Computing, 22(3):681–701, 2012. [32] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial diﬀerential equations. Journal of Computational Physics, 357:125 – 141, 2018. [33] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning (adaptive computation and machine learning). The MIT Press, 2005. [34] Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and analysis of computer experiments. Statistical Science, 4(4):409–423, 1989. [35] Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In Advances in Neural Information Processing Systems 30, pages 4588–4599. Curran Associates, Inc., 2017. [36] Paul D. Sampson and Peter Guttorp. Nonparametric estimation of nonstation- ary spatial covariance structure. Journal of the American Statistical Association, 87(417):pp. 108–119, 1992. [37] Matthias Seeger. Bayesian Gaussian process models: PAC-Bayesian generalisa- tion error bounds and sparse approximations. PhD thesis, University of Edinburgh, 2003. [38] Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian processes. In Advances in Neural Information Processing Systems (NIPS), pages 337–344. MIT Press, 2004. [39] Jasper Snoek, Kevin Swersky, Richard Zemel, and Ryan P. Adams. Input warp- ing for Bayesian optimization of non-stationary functions. In Proceedings of the 31st International Conference on International Conference on Machine Learning, volume 32 of ICML14, pages 1674–1682. JMLR.org, 2014. [40] Christopher K. I. Williams. Computing with inﬁnite networks. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems 9, pages 295–301. MIT Press, 1997. [41] Ying Xiong, Wei Chen, Daniel Apley, and Xuru Ding. A non-stationary covariance-based kriging method for metamodelling in engineering design. In- ternational Journal for Numerical Methods in Engineering, 71(6):733–756, 2007. 19 Appendix A Covariance functions/kernels Covariance kernels are positive deﬁnite (PD) functions. The symmetric function k : D × D 7→ R is PD if N∑ i=1 N∑ j=1 αiαjk(xi, x j) ≥ 0 for any N ∈ N points x 1, . . . , x N ∈ D and α = [α1, . . . , αN ] ⊤ ∈ RN . If k is a PD function, then the N × N matrix K whose elements are Kij = k(xi, x j) is a positive semideﬁnite matrix because ∑N i=1 ∑N j=1 αiαjKij ≥ 0. Checking the positive deﬁniteness of a function is not easy. One can combine the existing kernels to make a new one. For example, if k1 and k2 are two kernels, the function k obtained by the following operations is a valid covariance kernel: k(x, x ′) = k1(x, x′) + k2(x, x ′) k(x, x ′) = k1(x, x′) × k2(x, x ′) k(x, x ′) = ck1(x, x′), c ∈ R+ k(x, x ′) = k1(x, x′) + c, c ∈ R+ k(x, x ′) = g(x)k1(x, x ′)g(x ′) for any function g(.). We refer the reader to [33, 10] for a detailed discussion about the composition of covariance functions. It is also possible to compose kernels with a function as explained in Section 5. Usually, a covariance function depends on some parameters p which are unknown and need to be estimated from data. In practice, a parametric family of k is chosen ﬁrst. Then the parameters are estimated via maximum likelihood (ML), cross-validation or (full) Bayesian approaches [33]. In the sequel, we describe the ML approach as is used in this paper. The likelihood function measures the adequacy between a probability distribution and the data; a higher likelihood function means that observations are more consistent with the assumed distribution. In the GP framework, as observations are presumed to have the normal distribution, the likelihood function is p (y|X, p, µ) = 1 (2π)n/2|K|1/2 exp ( − (y − µ1)⊤ K −1 (y − µ1) 2 ) , (21) where |K| is the determinant of the covariance matrix. In the above equation, if µ is unknown, it is replaced with its estimate given by Equation (6). Usually for optimisation, it is more convenient to work with the natural logarithm of the likelihood (log-likelihood) function which is ln p (y|X, p, µ) = − n 2 ln(2π) − 1 2 ln |K| − (y − µ1)⊤ K −1 (y − µ1) 2 . (22) Maximising (22) is a challenging task as the log-likelihood function is often nonconvex with multiple maxima. To do so, numerical optimisation algorithms are often applied. We refer the reader to [25, 11] for further information. 20","libVersion":"0.3.1","langs":""}
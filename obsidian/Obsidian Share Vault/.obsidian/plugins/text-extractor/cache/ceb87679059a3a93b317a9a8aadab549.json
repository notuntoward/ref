{"path":"lit/sources/papers_to_add/Papers I'm Reviewing Right Now/dependent_data.pdf","text":"Author(s): Kerby Shedden, Ph.D., 2010 License: Unless otherwise noted, this material is made available under the terms of the Creative Commons Attribution Share Alike 3.0 License: http://creativecommons.org/licenses/by-sa/3.0/ We have reviewed this material in accordance with U.S. Copyright Law and have tried to maximize your ability to use, share, and adapt it. The citation key on the following slide provides information about how you may share and adapt this material. Copyright holders of content included in this material should contact open.michigan@umich.edu with any questions, corrections, or clarification regarding the use of content. For more information about how to cite these materials visit http://open.umich.edu/privacy-and-terms-use. Any medical information in this material is intended to inform and educate and is not a tool for self-diagnosis or a replacement for medical evaluation, advice, diagnosis or treatment by a healthcare professional. Please speak to your physician if you have questions about your medical condition. Viewer discretion is advised: Some medical content is graphic and may not be suitable for all viewers. 1 / 1 Regression analysis with dependent data Kerby Shedden Department of Statistics, University of Michigan November 20, 2013 2 / 1 Clustered data Clustered data are sampled from a population that can be viewed as the union of a number of related subpopulations. Write the data as Yij ∈ R, Xij ∈ R p, where i indexes the subpopulation and j indexes the individuals in the sample belonging to the i th subpopulation. 3 / 1 Clustered data It may happen that units from the same subpopulation are more alike than units from diﬀerent subpopulations, i.e. cor(Yij , Yij ′) > cor(Yij , Yi ′j ′). Part of the within-cluster similarity may be explained by X , i.e. units from the same subpopulation may have similar X values, which leads to similar Y values. In this case, cor(Yij , Yij ′) > cor(Yij , Yij ′|Xij , Xij ′). Even after accounting for measured covariates, units in a cluster may still resemble each other more than units in diﬀerent clusters: cor(Yij , Yij ′|X ) > cor(Yij , Yi ′j ′|Xij , Xi ′j ′). 4 / 1 Clustered data A simple working model to account for this dependence is ˆYij = ˆθi + ˆβ′Xij . The idea here is that β′Xij explains the variation in Y that is related to the measured covariates, and the θi explain variation in Y that is related to the clustering. This working model would be correct if there were q omitted variables Zijℓ, ℓ = 1, . . . , q, that were constant for all units in the same cluster (i.e. Zijℓ depends on ℓ and i, but not on j). In that case, ˆθi would stand in for the value of ∑ ℓ ˆψℓZijℓ that we would have obtained if the Zijℓ were observed. 5 / 1 Clustered data As an alternate notation, we can vectorize the data to express Y ∈ Rn and X ∈ Rn×p+1, then write ˆY = ∑ j ˆθj Ij + X ˆβ, where Ij ∈ Rn is the indicator of which subjects belong to cluster j. If the observed covariates in X are related to the clustering (i.e. if the columns of X and the Ij are not orthogonal), then OLS apportions the overlapping variance between ˆβ and ˆθ. 6 / 1 Test score example Suppose Yij is a reading test score for the j th student in the i th classroom, out of a large number of classrooms that are considered. Suppose Xij is the income of a student’s family. We might postulate as a population model Yij = θi + β′Xij + ϵij , which can be ﬁt as ˆYij = ˆθi + ˆβ′Xij . 7 / 1 Test score example Ideally we would want the parameter estimates to reﬂect sources of variation as follows: ▶ “Direct eﬀects” of parent income such as access to books, life experiences, good health care, etc. should go entirely to ˆβ. ▶ Attributes of classrooms that are not related to parent income, for example, the eﬀect of an exceptionally good or bad teacher, should go entirely to the ˆθi . ▶ Attributes of classrooms that are correlated with parent income, such as teacher salary, training, and resources, will be apportioned by OLS between ˆθi and ˆβ. ▶ Unique events aﬀecting particular individuals, such as the severe illness of the student or a family member, should go entirely to ϵ. 8 / 1 Other examples of clustered data ▶ Treatment outcomes for patients treated in various hospitals. ▶ Crime rates in police precincts distributed over a number of large cities (the precincts are the units and the cities are the clusters). ▶ Prices of stocks belonging to various business sectors. ▶ Surveys in which the data are collected following a cluster sampling approach. 9 / 1 What if we ignore the θi ? The θi are usually not of primary interest, but we should be concerned that by failing to take account of the clustering, we may incorrectly assess the relationship between Y and X . If the θi are nonzero, but we fail to include them in the model, the working model is misspeciﬁed. Let X be the design matrix without intercept, and let Q be the matrix of cluster indicators (which includes the intercept): Y =         Y11 Y12 Y21 Y22 Y23 · · ·  | | | | | |  Q =         1 0 · · · 1 0 · · · 0 1 · · · 0 1 · · · 0 1 · · · · · · · · · · · ·  | | | | | |  X =         X111 X112 · · · X121 X122 · · · X211 X212 · · · X221 X222 · · · X231 X232 · · · · · · · · · · · ·  | | | | | |  10 / 1 What if we ignore the θi ? Let ˜X = [1n|X ]. The estimate that results from regressing Y on ˜X is E ˆβ∗ = ( ˜X ′ ˜X ) −1 ˜X ′EY = ( ˜X ′ ˜X ) −1 ˜X ′(X β + Qθ). where ˆβ∗ = ( ˆβ0, ˆβ′)′. For the bias of ˆβ for β to be zero, we need E ˆβ∗ − ˜β = ( ˜X ′ ˜X ) −1 ˜X ′(X β + Qθ − ˜X ˜β) = 0, where β0 = E ˆβ0 and ˜β = (β0, β′)′. Thus we need 0 = ˜X ′(X β + Qθ − ˜X ˜β) = ˜X ′(Qθ − β0). 11 / 1 What if we ignore the θi ? Let S = Qθ be the vector of cluster eﬀects. Since the ﬁrst column of ˜X consists of 1’s, we have that ¯S = β0. For the other covariates, we have that X ′ j S = β0X ′ j 1n, which implies that Xj and S have zero sample covariance. This is unlikely in many studies, where people tend to cluster (in schools, hospitals, etc.) with other people having similar covariate levels. 12 / 1 Fixed eﬀects analysis In a ﬁxed eﬀects approach, we model the θi as regression parameters, by including additional columns in the design matrix whose covariate levels are the cluster indicators. As the sample size grows, in most applications the cluster size will stay constant (e.g. a primary school classroom might have up to 30 students). Thus the number of clusters must grow, so the dimension of the parameter vector θ grows. This is not typical in standard regression modeling. 13 / 1 Cluster eﬀect examples What does the inclusion of ﬁxed eﬀects do to the parameter estimates ˆβ, which are often of primary interest? The following slides show scatterplots of an outcome Y against a scalar covariate X , in a setting where there are three clusters (indicated by color). Above each plot are the coeﬃcient estimates, Z-scores, and r 2 values for ﬁts in which the cluster eﬀects are either included (top line) or excluded (second line). These estimates are obtained from the following two working models: ˆY = ˆα + ˆβX + ∑ ˆθj Ij ˆY = ˆα + ˆβX . The Z-scores are ˆβ/SD( ˆβ) and the r 2 values are cor( ˆY , Y )2. 14 / 1 Cluster eﬀect examples Left: No eﬀect of clusters or X ; X and clusters are independent; ˆβ ≈ 0 with and without cluster eﬀects in model. Right: An eﬀect of X , but no cluster eﬀects; X and clusters are independent; ˆβ, Z , and r 2 are similar with and without cluster eﬀects in model. XY ˆβ1 =−0.07 Z =−0.49 r2 =0.02 ˆβ1s =−0.06 Zs =−0.44 r 2 s =0.00 XY ˆβ1 =1.04 Z =7.75 r2 =0.57 ˆβ1s =1.01 Zs =7.81 r 2 s =0.56 15 / 1 Cluster eﬀect examples Left: Cluster eﬀects, but no eﬀect of X ; X and clusters are independent; ˆβ ≈ 0 with and without cluster eﬀects in model. Right: An eﬀect of clusters but no eﬀect of X ; X and clusters are dependent; when cluster eﬀects are not modeled their eﬀect is picked up by X . XY ˆβ1 =−0.03 Z =−0.85 r2 =0.94 ˆβ1s =0.00 Zs =0.02 r 2 s =0.00 XY ˆβ1 =0.03 Z =0.19 r2 =0.94 ˆβ1s =0.92 Zs =20.72 r 2 s =0.90 16 / 1 Cluster eﬀect examples Left: Eﬀects for clusters and for X ; X and clusters are dependent. Right: Eﬀects for clusters but not for X ; X and clusters are dependent but the net cluster eﬀect is not linearly related to X . XY ˆβ1 =0.90 Z =5.97 r2 =0.95 ˆβ1s =0.96 Zs =29.26 r 2 s =0.95 XY ˆβ1 =−0.04 Z =−0.31 r2 =0.84 ˆβ1s =0.00 Zs =0.02 r 2 s =0.00 17 / 1 Cluster eﬀect examples Left: Eﬀects for clusters and for X ; X and clusters are dependent; the X eﬀect has the opposite sign as the net cluster eﬀect. Right: Eﬀects for clusters and for X ; X and clusters are dependent; the signs and magnitudes of the X eﬀects are cluster-speciﬁc. XY ˆβ1 =−1.93 Z =−12.68 r2 =0.99 ˆβ1s =2.72 Zs =16.81 r 2 s =0.85 XY ˆβ1 =−0.52 Z =−1.70 r2 =0.98 ˆβ1s =2.92 Zs =8.20 r 2 s =0.58 18 / 1 Implications of ﬁxed eﬀects analysis for observational data A stable confounder is a confounding factor that is approximately constant within clusters. A stable confounder will become part of the net cluster eﬀect. If a stable confounder is correlated with an observed covariate X , then this will create non-orthogonality between the cluster eﬀects and the eﬀects of the observed covariates. 19 / 1 Implications of ﬁxed eﬀects analysis for experimental data Experiments are often carried out on “batches” of objects (specimens, parts, etc.) in which uncontrolled factors cause elements of the same batch to be more similar than elements of diﬀerent batches. If treatments are assigned randomly within each batch, there are no stable confounders (in general there are no confounders in experiments). Therefore the overall OLS estimate of β is unbiased as long as the standard linear model assumptions hold. 20 / 1 Implications of ﬁxed eﬀects analysis for experimental data Suppose the design is balanced (e.g. exactly half of each batch is treated and half is untreated). This is an orthogonal design, so the estimate based on the working model ˆYij = ˆβXij and the estimate based on the working model ˆYij = ˆθi + ˆβ∗Xij are identical ( ˆβ = ˆβ∗). Thus they have the same variance. But the estimated variance of ˆβ will be greater than the estimated variance of ˆβ∗ (since the corresponding estimate of σ2 is greater), so it will have lower power and wider conﬁdence intervals. 21 / 1 Random cluster eﬀects As we have seen, the cluster eﬀects θi can be treated as unobserved constants, and estimated as we would estimate any other regression coeﬃcient. An alternative way to handle cluster eﬀects is to view the θi as unobserved (latent) random variables. Now we have two random variables in the model: θi and ϵij . These are usually taken to be independent of each other. We can combine the cluster eﬀects with the error terms to get a single random variable per observation: ϵ ∗ ij = θi + ϵij . 22 / 1 Random cluster eﬀects Let Yi• = (Yi1, . . . , Yini )′ denote the vector of responses in the i th cluster, let Xi• denote the matrix of predictor variables for the i th cluster, and let ϵi• denote the vector of errors (ϵ∗ ij ) for the i th cluster. Thus we have the model Yi• = Xi•β + ϵi•, for clusters i = 1, . . . , m. The ϵi• are taken to be uncorrelated between clusters, i.e. cov(ϵi•, ϵi ′•) = 0 for i ̸= i ′. 23 / 1 Random cluster eﬀects The structure of the covariance matrix Si ≡ cov(ϵi•|Xi•) is Si =       σ2 + τ 2 τ 2 · · · · · · · · · τ 2 σ2 + τ 2 τ 2 · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · τ 2 τ 2 · · · · · · σ2 + τ 2  | | | |  , where var(ϵij ) = σ2 and var(θi ) = τ 2. 24 / 1 Generalized Least Squares Suppose we have a linear model with mean structure E [Y |X ] = X β for Y ∈ Rn, X ∈ Rn×p+1, and β ∈ Rp+1, and variance structure var(Y |X ) ∝ Σ, where Σ is a given n × n matrix. We can factor the covariance matrix as Σ = GG ′, and consider the transformed model G −1Y = G −1X β + G −1ϵ. Then letting η ≡ G −1ϵ, it follows that cov(η) = I , and note that the slope vector β of the transformed model is identical to the slope vector of the original model. 25 / 1 Generalized Least Squares We can use GLS to analyze clustered data with random cluster eﬀects. Let Y ∗ i• = S −1/2 i Yi•, ϵ∗ i• = S −1/2 i ϵi•, and X ∗ i• = S −1/2 i Xi•. Let Y ∗, X ∗, and ϵ∗ be Y ∗ i•, X ∗ i•, and ϵ∗ i•, respectively, stacked over i. 26 / 1 Generalized Least Squares Since cov(ϵ∗ i•) ∝ I , the OLS estimate of β for the model Y ∗ = X ∗β + ϵ∗ is the best estimate of β that is linear in Y ∗ (by the Gauss-Markov theorem). Since the set of linear estimates based on Y ∗ is the same as the set of linear estimates based on Y , it follows that the GLS estimate of β based on Y is the best unbiased estimate of β that is linear in Y . 27 / 1 Generalized Least Squares What if the “working covariance” S ∝ cov(ϵ) is incorrect? Since E (ϵ ∗|X ∗) = E (ϵ ∗|X ) = 0, the estimate ˆβ remains unbiased. However it has two problems: it may not be the BLUE (i.e. it may not have the least variance among unbiased estimates), and the usual linear model inference procedures will be wrong. 28 / 1 Generalized Least Squares The sampling covariance when the error structure is mis-speciﬁed is given by the “sandwich expression:” cov( ˆβ) = (X ∗′X ∗) −1X ∗′Σϵ∗X ∗(X ∗′X ∗) −1 where Σϵ∗ = cov(ϵ ∗|X ∗). 29 / 1 Generalized Least Squares In some cases we can reﬁne our estimate of Σϵ∗ based on the residuals of our initial ﬁt. For example, suppose that our working covariance has the form Si (j, j) = ν Si (j, k) = r ν(j ̸= k). Starting with an initial value r = 0, we use OLS to produce an unbiased estimate ˆβ. The usual regression MSE can be used to update our initial estimate of ν, and the intraclass correlation coeﬃcient (ICC) of the residuals can be used to estimate r . Note that in terms of the random eﬀects model deﬁned earlier, ν = σ2 + τ 2, and r ν = τ 2. At this point, we can use the sandwich expression with the updated Σϵ∗ to do inference on β. 30 / 1 Generalized Least Squares and stable confounders For GLS to give unbiased estimates of β (that also have minimum variance due to the Gauss-Markov theorem), we must have E [ϵ ∗ ij |X ] = E [θi + ϵij |X ] = 0. Since E [ϵij |X ] = 0, this is equivalent to requiring that E [θi |X ] = 0. Thus if the X covariates have distinct within-cluster mean values, and the within-cluster mean values of X are correlated with the θi , then the GLS estimate of β will be biased. 31 / 1 Likelihood inference for the random intercepts model The random intercepts model Yij = θi + β′Xij + ϵij , can be analyzed using a likelihood approach, using: ▶ A random intercepts density ϕ(θ|X ) ▶ A density for the data given the random intercept: f (Y |X , θ) 32 / 1 Likelihood inference for the random intercepts model The marginal model for the observed data is obtained by integrating out the random intercepts f (Y |X ) = ∫ f (Y |X , θ)ϕ(θ|X )dθ. The parameters in f (Y |X ) are all parameters in f (Y |X , θ), and all parameters in ϕ(θ|X ). Typically, f (Y |X , θ) is parameterized in terms of β and σ2, and ϕ(θ|X ) is parameterized in terms of a scale parameter τ . 33 / 1 Likelihood inference for the random intercepts model The ﬁrst two moments of Yij |X do not depend on the likelihood. Let µθ ≡ E (θ|X ) and σ2 θ = var(θ|X ). The moments are E (Yij |X ) = µθ + β′Xij and var(Yij |X ) = σ2 θ + σ2, and also cov(Yij1, Yij2|X ) = σ2 θ when j1 ̸= j2. 34 / 1 Likelihood inference for the random intercepts model Suppose ϵ|X ∼ N(0, σ2) θ|X ∼ N(µθ, σ2 θ ). In this case Y |X is Gaussian, with mean and variance as given above. Thus the random intercept model can be equivalently written in marginal form as Yij = µθ + β′Xij + ϵ ∗ ij where ϵ∗ ij = θi − µθ + ϵij . It follows that E (ϵ∗ ij |X ) = 0, var(ϵ∗ ij |X ) = σ2 θ + σ2, and the ϵ∗ ij values have correlation coeﬃcient σtheta2/(σ2 + σ2 θ ) within clusters. 35 / 1 Likelihood computation Maximum likelihood estimates for the model can be calculated using iteratively reweighted least squares, or alternatively with the EM algorithm, or a gradient-based optimization procedure. Asymptotic standard errors can be obtained from the inverse of Fisher information, and likelihood ratio tests can be used to compare nested models. 36 / 1 Predicting the random intercepts In a random intercepts model, the primary emphasis is usually the β coeﬃcients, the distribution of θ is generally of secondary interest. In some cases there is an interest in the individual θi values, in which case we can predict them. The best linear unbiased predictor (BLUP) is E ˆβ,ˆσ2,ˆσ2 θ,ˆµθ (θ|Y , X ) = ˆµθ + dcov(θ, Y )dcov(Y ) −1(Y − EY ) 37 / 1 Predicting the random intercepts The BLUP is a linear function of the data. For the i th cluster, E ˆβ,ˆσ2,ˆσ2 θ,ˆµθ (θi |Y , X ) = ˆµθ + rM −1(Y − EY ), where M = 0 B B B B @ 1 r r · · · r r 1 r · · · r · · · · · · r r r · · · 1 1 C C C C A , and r = σ2 θ /(σ2 θ + σ2) is the ICC. Note that the uncertainty in estimating β, σ2, σ2 θ , µθ is not considered in forming the prediction. 38 / 1 Random slopes Suppose we are interested in individual responses to changes in a measured covariate X . We might start with the model Y = θ + X β + ϵ, so if X is a treatment indicator (1 for treated subjects, 0 for untreated subjects), then β is the average change in response associated with treatment (the population treatment eﬀect). In many cases, it is reasonable to consider the possibility that diﬀerent individuals may respond diﬀerently to the treatment – that is, diﬀerent subjects have diﬀerent β values. In this case we can let βi be the response for subject i. How can we handle estimation and inference for these values? 39 / 1 Random slopes Suppose we model the random slopes βi as being Gaussian (given X ) with expected value β0 and covariance matrix Σβ. The marginal model is E (Yi |Xi ) = X ′ i β0 and cov(Yi |Xi ) = X ′ i ΣβXi + σ2I . As with the random intercepts model, Y |X is Gaussian. In this case there is no natural IRLS algorithm, but the EM and other optimization procedures can be used to calculate maximum likelihood estimates for the model parameters. 40 / 1","libVersion":"0.3.1","langs":""}
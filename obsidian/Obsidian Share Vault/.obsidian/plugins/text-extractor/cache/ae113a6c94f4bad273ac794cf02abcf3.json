{"path":"lit/lit_sources/Yang19distribOptSurvey.pdf","text":"ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Annual Reviews in Control xxx (xxxx) xxx Contents lists available at ScienceDirect Annual Reviews in Control journal homepage: www.elsevier.com/locate/arcontrol Review A survey of distributed optimization ✩ Tao Yang a , Xinlei Yi b , Junfeng Wu c , Ye Yuan d , ∗, Di Wu e , Ziyang Meng f , Yiguang Hong g , Hong Wang h , Zongli Lin i , Karl H. Johansson b a Department of Electrical Engineering, University of North Texas, Denton, TX 76203, USA b Division of Decision and Control Systems, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, 100 44, Sweden c College of Control Science and Engineering, Zhejiang University, Hangzhou, 310027, China d School of Artiﬁcial Intelligence and Automation and State Key Lab of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan 430074, China e Paciﬁc Northwest National Laboratory, Richland, WA 99352, USA f Department of Precision Instrument and the State Key Laboratory of Precision Measurement Technology and Instruments, Tsinghua University, Beijing 10 0 084, China g Key Laboratory of Systems and Control, Institute of Systems Science, Chinese Academy of Science, Beijing 100190, China h Oak Ridge Laboratory, Oak Ridge, TN 37932, USA i Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA 22904, USA a r t i c l e i n f o Article history: Received 14 January 2019 Revised 10 April 2019 Accepted 13 May 2019 Available online xxx Keywords: Distributed optimization Coordination of distributed energy resources a b s t r a c t In distributed optimization of multi-agent systems, agents cooperate to minimize a global function which is a sum of local objective functions. Motivated by applications including power systems, sensor net- works, smart buildings, and smart manufacturing, various distributed optimization algorithms have been developed. In these algorithms, each agent performs local computation based on its own information and information received from its neighboring agents through the underlying communication network, so that the optimization problem can be solved in a distributed manner. This survey paper aims to offer a de- tailed overview of existing distributed optimization algorithms and their applications in power systems. More speciﬁcally, we ﬁrst review discrete-time and continuous-time distributed optimization algorithms for undirected graphs. We then discuss how to extend these algorithms in various directions to handle more realistic scenarios. Finally, we focus on the application of distributed optimization in the optimal coordination of distributed energy resources. © 2019 Elsevier Ltd. All rights reserved. Contents 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1. Distributed optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2. Coordination of distributed energy resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3. Contributions and outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2. Preliminaries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.1. Graph theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 ✩ This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a nonexclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan ( http://energy.gov/downloads/doe- public- access- plan ). ∗ Corresponding author . E-mail addresses: Tao.Yang@unt.edu (T. Yang), xinleiy@kth.se (X. Yi), jfwu@zju.edu.cn (J. Wu), yye@hust.edu.cn (Y. Yuan), di.wu@pnnl.gov (D. Wu), ziyangmeng@mail.tsinghua.edu.cn (Z. Meng), yghong@iss.ac.cn (Y. Hong), wangh6@ornl.gov (H. Wang), zl5y@virginia.edu (Z. Lin), kallej@kth.se (K.H. Johansson). https://doi.org/10.1016/j.arcontrol.2019.05.006 1367-5788/© 2019 Elsevier Ltd. All rights reserved. Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 2 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] 2.2. Convex analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3. Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3. Basic distributed optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.1. Discrete-time Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.1.1. Algorithms with diminishing step-sizes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.1.2. Algorithms with a ﬁxed step-size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2. Continuous-time algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2.1. First-order gradient-based algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2.2. Second-order algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4. Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.1. Directed graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.1.1. Discrete-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.1.2. Continuous-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.2. Complex agent dynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.3. Constrained optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.3.1. Local constraint sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.3.2. Global constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.4. Time delays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.4.1. Discrete-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.4.2. Continuous-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.5. Random graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.6. Event-triggered communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.7. Finite-time convergence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.7.1. Discrete-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.7.2. Continuous-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 5. Application to coordination of distributed energy resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 5.1. Problem formulation of DER coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.2. Undirected graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.3. Directed graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.3.1. Discrete-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.3.2. Continuous-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.4. Communication imperfections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.5. Event-triggered communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.6. Finite-time convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6. Summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6.1. Current state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6.2. Future research directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Conﬂict of interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 1. Introduction In recent years, rapid developments in digital systems, com- munication and sensing technologies have led to the emergence of networked systems. These networked systems consist of a large number of interconnected subsystems (agents), which are required to cooperate in order to achieve a desirable global objective. Ap- plications for such networked systems include but are not limited to power systems, sensor networks, smart buildings, and smart manufacturing ( Bullo, 2017; Dong, Hua, Zhou, Ren, & Zhong, 2018; Dörﬂer, Simpson-Porco, & Bullo, 2016; Gao, Gao, Ozbay, & Jiang, 2019; Lamnabhi-Lagarrigue et al., 2017; Meng & Moore, 2016; Ren & Cao, 2011; Yuan, Zhang, Wu, Zhu, & Ding, 2017; Zhu & Martínez, 2015 ). Many problems in networked systems can be posed in the framework of convex optimization ( Boyd, Parikh, Chu, Peleato, & Eckstein, 2011 ). Due to the distributed nature of networked sys- tems, the traditional centralized strategies are not suitable to solve these optimization problems. Moreover, the centralized framework is subject to performance limitations, such as a single point of failure, high communication requirement, substantial computation burden, and limited ﬂexibility and scalability. All of these have made imperative the use of distributed approaches to solve these optimization problems ( Boyd et al., 2011; Nedi ´c, 2015; Sayed, 2014a ). 1.1. Distributed optimization In a networked system of N agents, each of which has a local private convex objective function f i ( x ), where x ∈ R n is the opti- mization variable. The objective of distributed optimization is to minimize a global objective function, which is a sum of the objec- tive functions of all agents: min x ∈ R n N ∑ i =1 f i (x ) , (1) in a distributed manner by local computation and communication. The distributed optimization problem has been studied for a long time and can be traced back to the seminal works ( Bertsekas & Tsitsiklis, 1989; Tsitsiklis, 1984; Tsitsiklis, Bertsekas, & Athans, 1986 ) in the context of parallel and distributed computation. It has gained growing renewed interest over the last decade due to its various applications in power systems, communication networks, machine learning, and sensor networks, just to name a few ( Cao, Yu, Ren, & Chen, 2013; Nedi ´c, 2015; Nedi ´c, Olshevsky, & Rabbat, 2018; Sayed, 2014a ). Various distributed algorithms have been pro- posed in the literature. For the recent review and progress, please refer to the surveys ( Boyd et al., 2011; Nedi ´c, 2015; Nedi ´c & Liu, 2018; Nedi ´c et al., 2018; Sayed, 2014a; Yang & Johansson, 2010 ) Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 3 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] and the books ( Bullo, 2017; Giselsson, 2018; Ren & Cao, 2011; Zhu & Martínez, 2015 ). Most existing distributed algorithms are consensus based and can be divided into two categories depending on whether an al- gorithm is in discrete-time or in continuous-time. In these algo- rithms, each agent holds a dynamic state, which is the estimate of the optimization variable, and updates its value based on its own information and the information received from its neighbors through the underlying communication network. 1.2. Coordination of distributed energy resources Although distributed optimization ﬁnds various applications, in this paper, we focus on the optimal coordination of distributed en- ergy resources (DERs) in power systems (see, e.g., Bidram, Lewis, & Davoudi, 2014; Hadjicostis, Domínguez-García, & Charalambous, 2018; Kraning, Chu, Lavaei, & Boyd, 2014; Molzahn et al., 2017; Nedi ´c & Liu, 2018; Qin, Ma, Shi, & Wang, 2017 ). In the past decades, the power system has been undergoing a transition from a system with conventional generation power plants and inﬂexible loads to a system with a large number of dis- tributed generators, energy storages, and ﬂexible loads, often re- ferred to as distributed energy resources (DERs) ( Lasseter et al., 2003; Pedrasa, Spooner, & MacGill, 2010; Rahimi & Ipakchi, 2010 ). These resources are small and highly ﬂexible compared with con- ventional generators and can be aggregated to provide power nec- essary to meet the regular demand. As the electricity grid contin- ues to modernize, DERs can facilitate the transition to a smarter grid. In order to achieve an effective and eﬃcient deployment among DERs, one needs to properly design the coordination among them. The objective of the optimal DER coordination problem is to min- imize the total production cost while meeting the total demand and satisfying the individual generator output limits. One ap- proach is through a completely centralized control strategy, which suffers from the limitations as pointed out before. To overcome these limitations, recently, by using the results developed in the ﬁeld of distributed optimization, various distributed strategies have been proposed for solving the optimal DER coordination problem, see, e.g., Dominguez-Garcia, Cady, and Hadjicostis (2012) ; Kar and Hug (2012) ; Rahbari-Asr, Ojha, Zhang, and Chow (2014) ; Xing, Mou, Fu, and Lin (2015) ; Yang, Tan, and Xu (2013) ; Yang, Wu, Sun, and Lian (2016) ; Zhang and Chow (2012) ; Zhao, He, Cheng, and Chen (2017) and Hadjicostis et al. (2018) . 1.3. Contributions and outline Recent survey papers on distributed optimization ( Nedi ´c, 2015; Nedi ´c & Liu, 2018; Nedi ´c et al., 2018 ) mainly focused on discrete- time algorithms with diminishing step-sizes. This paper aims to provide a detailed and comprehensive overview of discrete-time algorithms with both diminishing step-sizes and ﬁxed step-sizes as well as continuous-time algorithms. In addition, we also discuss their applications to the optimal DER coordination problem, and offer some future research directions. In Section 2 , we provide some background on graph theory, convex analysis, and convergence analysis. In Section 3 , we review various basic discrete-time optimiza- tion algorithms as well as continuous-time optimization algorithms for undirected graphs. Regarding discrete-time algorithms, we fo- cus on the recently developed algorithms with ﬁxed step-sizes for the case where the local objective functions are strongly convex and smooth (have Lispchitz continuous gradients). Compared to al- gorithms with diminishing step-sizes, these algorithms have faster convergence, which is desirable in practical time critical applica- tions, such as the DER coordination ( Tang, Hill, & Liu, 2018 ). More- over, due to the faster convergence, their computational costs and communication overheads are much smaller compared to the algo- rithms with diminishing step-sizes. With these discrete-time and continuous-time algorithms in hand, in Section 4 , we discuss various extensions of these algo- rithms to handle more realistic scenarios. These extensions are not independent but actually may overlap to some extent. In Section 5 , we focus on the application of distributed opti- mization to DER coordination, which has received substantial at- tention in both control and power system ﬁelds in recent years (see, e.g., Bidram et al., 2014; Hadjicostis et al., 2018; Kraning et al., 2014; Molzahn et al., 2017; Nedi ´c & Liu, 2018; Qin et al., 2017 ). The speciﬁc requirements and structures for power systems application provide more challenges and opportunities for the development of distributed optimization algorithms. As such, various general dis- tributed optimization algorithms reviewed in Sections 3 and 4 have been applied and/or adapted to solve the DER coordination prob- lem in power systems. Finally, Section 6 concludes the paper with some discussion on future research directions. Notations: Given a matrix A, A ⊤ denotes its transpose and A −1 denotes its inverse. We denote by A \u0002B the Kronecker product be- tween matrices A and B. I n denotes the identity matrix of dimen- sion n × n . 1 n and 0 n denote the column vector with each entry being 1 and 0, respectively. 2. Preliminaries This section introduces some background on graph theory, con- vex analysis, and convergence analysis. 2.1. Graph theory We ﬁrst recall some basic concepts in graph theory ( Godsi & Royle, 2001 ). Let G = (V, E ) denote a directed graph (digraph) with the set of nodes (agents) V = { 1 , . . . , N} and the set of edges E ⊆ V × V. A directed edge from node i to node j is denoted by (i, j) ∈ E. A digraph is undirected if and only (i, j) ∈ E implies ( j, i ) ∈ E. For notational simpliﬁcation, we assume that the digraph does not have any self loop, i.e., (i, i ) / ∈ E for all i ∈ V although each node i has an access to its own information. A directed path from node i 1 to node i k is a sequence of nodes { i 1 , . . . , i k } such that (i j , i j+1 ) ∈ E for j = 1 , . . . , k − 1 . If there exists a directed path from node i to node j , then node j is said to be reachable from node i . A digraph G is strongly connected if every node is reachable from every other node. Let A = [ a ij ] ∈ R N×N be adjacency matrix associated with the digraph G, where a ij > 0 is the weight for edge ( j, i ) ∈ V and a ij = 0 otherwise. A digraph is weigh-balanced if ∑ N j=1 a ij = ∑ N j=1 a ji for all i ∈ V. A digraph is detailed balanced if there exist some real num- bers ω i > 0, i = 1 , 2 , . . . , N, such that the coupling weights of the graph satisfy ω i a ij = ω j a ji for all i, j = 1 , 2 , . . . , N. The underlying communication network may also be modeled as a time-varying directed graph G(k ) = (V, E(k )) , where the edge set changes over time due to unexpected loss of communica- tion links. All nodes which transmit information to node i di- rectly at time t are said to be its in-neighbors and belong to the set N in i (k ) = { j ∈ V | ( j, i ) ∈ E(k ) } . All nodes which receive in- formation from agent i at time k belong to the set of its out- neighbors, denoted by N out i (k ) = { j ∈ V | (i, j) ∈ E(k ) } . The joint graph of G(k ) in the time interval [ k 1 , k 2 ) with k 1 < k 2 ≤∞ is deﬁned as G([ k 1 , k 2 )) = ∪ k ∈ [ k 1 ,k 2 ) G(k ) = (V, ∪ k ∈ [ k 1 ,k 2 ) E(k )) . A time- varying directed graph G(k ) is said to be uniformly jointly strongly connected if there exists a constant B > 0 such that G([ k 0 , k 0 + B )) is strongly connected for any k 0 ≥ 0. Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 4 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] 2.2. Convex analysis Next, we provide some background on basic convex analysis ( Boyd & Vandenberghe, 2004; Nesterov, 2004 ). A set X ⊂ R n is con- vex if for all x, y ∈ X , and for all θ ∈ [0, 1], we have θ x + (1 − θ ) y ∈ X. A function f : R n → R is convex if f (θ x + (1 − θ ) y ) ≤ θ f (x ) + (1 − θ ) f (y ) for all x, y ∈ R n and for all θ ∈ (0, 1). A function f : R n → R is strictly convex if f (θ x + (1 − θ ) y ) < θ f (x ) + (1 − θ ) f (y ) for all x ̸ = y ∈ R n and for all θ ∈ (0, 1). A continuously differentiable function f : R n → R is strongly convex with convexity parameter μ if (∇ f (y ) − ∇ f (x )) ⊤ (y − x ) ≥ μ∥ y − x ∥ 2 for all x, y ∈ R n . A contin- uously differentiable function f : R n → R is restricted strongly con- vex with respect to a point x ∈ R n if there exists a constant μ > 0 such that (∇ f (y ) − ∇ f (x )) ⊤ (y − x ) ≥ μ∥ y − x ∥ 2 for all y ∈ R n . A continuously differentiable function f : R n → R is smooth if it has a globally Lipschitz continuous gradient, i.e., there exists a constant L > 0 such that ∥∇ f (y ) − ∇ f (x ) ∥ ≤ L ∥ y − x ∥ for all x, y ∈ R n . 2.3. Convergence analysis Finally, we recall the deﬁnition of linear (exponential in the lan- guage of control theory) convergence of a sequence from Nedi ´c, Ol- shevsky, and Shi (2017a) . Suppose that a sequence { x ( k )} converges to x ∗ in some norm ∥ · ∥ . Then the convergence is (i) Q-linear if there exists a constant 0 < ρ < 1 such that ∥ x (k +1) −x ∗∥ ∥ x (k ) −x ∗∥ ≤ ρ for all k ; (ii) R-linear if there exist some constants C > 0 and 0 < ρ < 1 such that ∥ x (k ) − x ∗∥ ≤ Cρk for all k . Both of these rates are exponential or geometric. The difference is that Q-linear implies a monotonic decrease of ∥ x (k ) − x ∗∥ , while the R-linear does not. 3. Basic distributed optimization algorithms In this section, we provide an overview of some existing discrete-time and continuous-time distributed algorithms for solv- ing the distributed unconstrained optimization problem over undi- rected and ﬁxed communication networks. Various extensions of these algorithms will be discussed in Section 4 . 3.1. Discrete-time Algorithm Most existing distributed algorithms are in the discrete-time setting (see Nedi ´c, 2015; Nedi ´c and Liu, 2018; Nedi ´c et al., 2018; Sayed, 2014a , and references therein). Among these algorithms, the ﬁrst-order algorithms based on the consensus theory and the (sub)gradient method have received much attention since they are simple, amenable to implementation and easily generalizable. The existing distributed ﬁrst-order discrete-time algorithms can be cat- egorized into two classes depending on whether the step-sizes are diminishing or ﬁxed. 3.1.1. Algorithms with diminishing step-sizes We begin by reviewing discrete-time algorithms with dimin- ishing step-sizes. A seminal work in this direction is Nedi ´c and Ozdaglar (2009) , where the authors proposed a simple distributed ﬁrst-order (sub)gradient descent algorithm. In the proposed algo- rithm, each agent performs a consensus step and then a descent step along the local (sub)gradient direction of its own convex ob- jective function. In particular, at time instant (step) k , each agent i runs the following update: x i (k + 1) = N ∑ j=1 w ij (k ) x j (k ) − α(k ) s i (k ) , (2) where x i (k ) ∈ R n is agent i ’s estimate of the optimal solution at time instant k, w ij ( k ) is the edge weight of communication link ( j, i ) at time instant k such that W (k ) = [ w ij (k )] ∈ R N×N is dou- bly stochastic for all k, s i ( k ) is the (sub)gradient of the local objec- tive function f i ( x ) which is convex and possibly non-differentiable at x = x i (k ) , and α( k ) > 0 is the diminishing step-size satisfying the following conditions: ∞ ∑ k =0 α(k ) = ∞ , ∞ ∑ k =0 α2 (k ) < ∞ , α(k ) ≤ α(s ) for all k > s ≥ 0 . (3) Under the assumption that subgradients are bounded, it is shown in Nedi ´c and Ozdaglar (2009) that the distributed (sub)gradient descent (DGD) algorithm with diminishing step-sizes asymptotically converges to one of the optimal solutions, if the ﬁxed undirected graph is connected or the time-varying undirected graph is uniformly jointly connected. The authors of Johansson, Ke- viczky, Johansson, and Johansson (2008) modiﬁed of the DGD algo- rithm (2) by changing the order in which the consensus-step and the subgradient descent step are executed. Although the simple DGD algorithm (2) is applicable to nons- mooth convex functions and has been extended in several direc- tions to handle more realistic scenarios, which will be reviewed in details in the next section, the convergence is rather slow due to the diminishing step-sizes. For the case where local objec- tive functions are smooth, the authors of Jakoveti ´c, Xavier, and Moura (2014b) developed a fast distributed algorithm based on the centralized Nesterov gradient method. They showed that the pro- posed algorithm with diminishing step-sizes has a faster conver- gence rate compared to the DGD algorithm and the dual averaging algorithm ( Duchi, Agarwal, & Wainwright, 2012 ), but is still slower than the centralized gradient descent algorithm. With a ﬁxed step- size, it is shown in Matei and Baras (2011) and Yuan, Ling, and Yin (2015) that the DGD algorithm only converges to a point in the neighborhood of an optimal solution. In order to achieve the convergence rate that matches the cen- tralized gradient descent algorithm and to reduce communication overheads, recent studies focused on developing distributed accel- erated algorithms with ﬁxed step-sizes for the case where local objective functions are strongly convex and smooth. Both discrete- time and continuous-time distributed algorithms have been pro- posed and will be reviewed in Section 3.1.2 and Section 3.2 , re- spectively. 3.1.2. Algorithms with a ﬁxed step-size In this subsection, we present several recently developed discrete-time distributed algorithms with ﬁxed step-sizes which linearly (exponentially in the language of control theory) converge to an optimal solution. A common strategy of these proposed algo- rithms is to use some sort of historical information. EXTRA. The earliest work is perhaps ( Shi, Ling, Wu, & Yin, 2015a ), where the authors developed an ex act ﬁrs t -orde r a lgorithm (abbreviated as EXTRA). More speciﬁcally, the EXTRA contains two steps. In the ﬁrst step, agent i performs the following update: x i (1) = N ∑ j=1 w ij x j (0) − α∇ f i (x i (0)) , (4) where α > 0 is a ﬁxed step-size, the weight mixing matrix W = [ w ij ] ∈ R N×N is doubly stochastic and ∇f i ( · ) is the gradient of the local objective function f i ( · ). In the second step, agent i performs the following update: x i (k + 2) = x i (k + 1) + N ∑ j=1 w ij x j (k + 1) − N ∑ j=1 ˜ w ij x j (k ) − α( ∇ f i (x i (k + 1)) − ∇ f i (x i (k )) ) , k = 0 , 1 , ... , (5) Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 5 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] where the weight mixing matrix ˜ W = [ ˜ w ij ] ∈ R N×N is also doubly stochastic. Compared to the DGD algorithm (2) , which only uses the estimate of the optimal solution and the gradient at the pre- vious iteration, the EXTRA uses the estimates of the optimal solu- tion and the gradients at the previous two iterations. It is shown in Shi et al. (2015a) that the EXTRA can be viewed as the DGD with a cumulative correction term to correct the error caused by the DGD with a ﬁxed step-size. For an undirected connected network, under certain conditions on the doubly stochastic weight mixing matrices, the linear con- vergence of the EXTRA with the step-size less than a certain crit- ical value has been established if the global objective function is restricted strongly convex with respect to the global minimizer and local convex objective functions are smooth. The primal-dual interpretation of the EXTRA has been offered in Mokhtari and Ribeiro (2016) and Mokhtari, Shi, Ling, and Ribeiro (2016) based on the augmented Lagrangian function. The EXTRA has also been extended to composite convex problems where the local objective functions have the smooth and nonsmooth composite convex form. In particular, the authors of Shi, Ling, Wu, and Yin (2015b) devel- oped a proximal gradient exact ﬁrst-order algorithm (PG-EXTRA) by using proximal operations ( Parikh & Boyd, 2014 ). Other proximal algorithms for composite optimization are also available in the literature, see, e.g., Aybat, Wang, and Iyen- gar (2015) ; Aybat, Wang, Lin, and Ma (2018) ; Dhingra, Khong, and Jovanovi ´c (2019) and Xu, Zhu, Sohy, and Xie (2018b) . In particu- lar, the authors of Aybat et al. (2015) developed a distributed aug- mented Lagrangian algorithm with a double-loop structure. The authors of Aybat et al. (2018) proposed a distributed proximal gra- dient algorithm and its stochastic variant with noisy gradients. The proposed algorithms only contain a single-loop and therefore they are easy to be implemented. The authors of Dhingra et al. (2019) developed a distributed proximal augmented Lagrangian method for the distributed com- posite convex optimization. The authors of Xu et al. (2018b) de- veloped a distributed algorithm based on the Bregman method and operator splitting, referred to as Distributed Forward-Backward Bregman Splitting (D-FBBS). The proposed distributed algorithm provides a uniﬁed framework which recovers most existing dis- tributed algorithms for ﬁxed graphs, such as EXTRA ( Shi et al., 2015a ) and P-EXTRA ( Shi et al., 2015b ). Moreover, the proposed al- gorithm allows agents to communicate asynchronously, and thus is applicable to stochastic networks. DIGing. Next, we review another class of distributed algo- rithms with ﬁxed step-sizes, based on the combination of the d istributed i nexact g radient method and the gradient track ing technique (abbreviated as DIGing), developed independently in Nedi ´c et al. (2017a) ; Qu and Li (2018) ; Xu, Zhu, Soh, and Xie (2015, 2018a) and Nedi ´c, Olshevsky, Shi, and Uribe (2017b) . In these al- gorithms, in addition to x i ( k ), which is agent i ’s estimate of the optimal solution at time step k , each agent i also holds another state y i ( k ), which is the estimate of the average gradient at time step k . In algorithms proposed in Qu and Li (2018) and Nedi ´c et al. (2017a) , these states are updated as follows: x i (k + 1) = N ∑ j=1 w ij x j (k ) − αy i (k ) , (6a) y i (k + 1) = N ∑ j=1 w ij y j (k ) + ∇ f i (x i (k + 1)) − ∇ f i (x i (k )) , (6b) where the weight mixing matrix W = [ w ij ] ∈ R N×N is doubly stochastic and α > 0 is a ﬁxed step-size. The algorithm is initial- ized with any x i (0) and y i (0) = ∇ f i (x i (0)) . The algorithm is based on the combination of the distributed inexact gradient method and the gradient tracking technique. More speciﬁcally, the update (6a) is a distributed inexact gradient method, where the variable y i ( k ) is used instead of the average gradient, and in the update (6b) , y i ( k ) tracks the average gradient by employing dynamic av- erage consensus ( Zhu & Martínez, 2010 ). Under the assumption that local objective functions are strongly convex and smooth, the linear convergence of the DIGing algo- rithm has been established for undirected connected networks if the ﬁxed step-size is chosen properly ( Nedi ´c et al., 2017a; Qu & Li, 2018 ). For ﬁxed undirected networks, it is shown in Nedi ´c et al. (2017a) that the DIGing algorithm is equivalent to the EXTRA by properly choosing the two mixing matrices in the EXTRA. More- over, Nedi ´c et al. (2017a) also provided a primal-dual interpretation for the DIGing algorithm based on the augmented Lagrangian func- tion. Note that the algorithms proposed in Qu and Li (2018) and Nedi ´c et al. (2017a) require an identical step-size for all agents. A few algorithms with different (uncoordinated) step-sizes have been proposed, see, e.g., Aug-DGM (augmented distributed gradi- ent methods) ( Xu et al., 2015 ), AsynDGM (asynchronous distributed gradient method) ( Xu, Zhu, Soh, & Xie, 2018a ), and ATC-DIGing (adapt-then-combine distributed inexact gradient tracking) ( Nedi ´c et al., 2017b ). More speciﬁcally, each agent i performs the following update rule 1 : x i (k + 1) = N ∑ j=1 w ij (x j (k ) − α j y j (k )) , (7a) y j (k + 1) = N ∑ j=1 w ij (y j (k ) + ∇ f j (x j (k + 1)) − ∇ f j (x j (k )) ), (7b) where αj > 0 is the step-size of agent j . Compared with the DIGing algorithm (6), which employs a combine-then-adapt (CTA) structure ( Sayed, 2014b ), where the states are combined with neighboring agents’ states and then adapted, algorithm (7) utilizes an adapt-then-combine (ATC) struc- ture ( Sayed, 2014b ), in which the states are ﬁrst adapted and then combined with the adapted states of neighboring agents. As shown in Nedi ´c et al. (2017b) ; Xu et al. (2015, 2018a) , the ATC structure is capable of employing uncoordinated step-sizes. Another nice fea- ture of the ATC structure is that the algorithms with ATC sturcture enjoy faster convergence ( Nedi ´c et al., 2017a; Nedi ´c et al., 2017b ). Recently, the authors of Jakoveti ´c (2019) provided a uniﬁed primal-dual analysis for both the EXTRA ( Shi et al., 2015a ) and the DIGing ( Nedi ´c et al., 2017a; Qu & Li, 2018 ) and revealed that a major difference between these two methods is on the effect of the primal error on the dual error. They then general- ized these methods by deriving a new method which reduces the negative effect of primal error on the dual error. The authors of Sundararajan, Hu, and Lessard (2017) and Sundararajan, Scoy, and Lessard (2018) proposed a uniﬁed framework for analyzing the EX- TRA, the DIGing, the algorithms proposed in Jakoveti ´c (2019) , and other existing ﬁrst-order distributed algorithms for strongly con- vex and smooth objective functions by formulating a semideﬁnite program (SDP) which can be eﬃciently solved to provide a numeri- cal certiﬁcate of the linear convergence. These works ( Sundararajan et al., 2017; Sundararajan et al., 2018 ) can be viewed as an ex- tension of their earlier work ( Lessard, Recht, & Packard, 2016 ) for optimization, where the authors developed a framework to ana- lyze and design iterative optimization algorithms by using integral quadratic constraints (IQC) from robust control theory ( Megretski 1 Note that the update for the variable y i in AsynDGM proposed in Xu et al. (2018a) is slighly different, i.e., (7b) is replaced by y j (k + 1) = ∑ N j=1 w ij y j (k ) + ∇ f i (x i (k + 1)) − ∇ f i (x i (k )) . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 6 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] & Rantzer, 1997 ), to distributed multi-agent optimization. By using the IQC and dissipativity theory, the author of Han (2019) provided a computational proof for the convergence analysis of the dis- tributed algorithm for undirected ﬁxed graphs proposed in Qu and Li (2018) when local objective functions are smooth and convex but not strongly convex. Distributed PI algorithm. To correct the error caused by the distributed gradient based algorithms with ﬁxed step-sizes, dis- tributed algorithms based on the proportional-integral (PI) con- trol strategy have been developed in Lei, Chen, and Fang (2016) ; Yao, Yuan, Sundaram, and Yang (2018) and Yang, Wan, Wang, and Lin (2018) . These algorithms are discrete-time counterparts of the continuous-time distributed PI algorithms proposed in Gharesifard and Cortés (2014) ; Kia, Cortés, and Martínez (2015a) ; Wang and Elia (2010) and Xie and Lin (2017) , which will be re- viewed in Section 3.2 . In particular, in these algorithms, each agent performs the following update: x i (k + 1) = x i (k ) − v i (k ) − α∇ f i (x i (k )) − β ∑ j∈N i a ij (x i (k ) − x j (k )) , (8a) v i (k + 1) = v i (k ) + αβ ∑ j∈N i a ij (x i (k ) − x j (k )) , (8b) where x i (k ) ∈ R n is the local estimate of the global minimizer x ∗ of agent i at time step k, α, β > 0 are gain parameters, and N i = { j ∈ V | ( j, i ) ∈ E} is the set of neighbors for agent i . The algorithm is initialized with any x i (0) and v i (0) such that ∑ N i =1 v i (0) = 0 n . It is shown in Yao et al. (2018) that the distributed PI algorithm (8) is equivalent to the EXTRA by properly choosing the two mix- ing matrices in the EXTRA. Therefore, the convergence analysis fol- lows from the proof in Shi et al. (2015a) . For the case where local objective functions are quadratic, a less conservative convergence condition was also established in Yao et al. (2018) via the Lyapunov stability analysis. Distributed Newton–Raphson algorithm. Although we focus on ﬁrst-order gradient-based algorithms, it is worthy to men- tion that a few second-order distributed algorithms based on the Newton method have been proposed, see, e.g., Varagnolo, Zanella, Cenedese, Pillonetto, and Schenato (2016) ; Wei, Ozdaglar, and Jadbabaie (2013a,b) ; Zanella, Varagnolo, Cenedese, Pillonetto, and Schenato (2011) . In particular, the distributed Newton– Raphson algorithm proposed in Zanella et al. (2011) and Varagnolo et al. (2016) is based on the average consensus algo- rithm and the separation of time-scale idea. Intuitively, each agent computes and sequentially updates an approximated Newton– Raphson direction by means of suitable average consensus ra- tios. Although the algorithm proposed in Zanella et al. (2011) and Varagnolo et al. (2016) is actually a discrete-time algorithm, in or- der to establish its convergence, the algorithm is considered as a forward-Euler discretization of a continuous dynamics. The authors established the exponential stability of the continuous dynamics, which in turn implies the stability of the Euler discretization pro- vided that the Euler discretization step-size is suﬃciently small. 3.2. Continuous-time algorithms Although classical distributed optimization algorithms are in the discrete-time setting, with the development of cyber-physical systems, much attention has been paid to the continuous-time setting, mainly because many practical systems such as robots and unmanned vehicles operate in continuous-time and the well-developed continuous-time control techniques (in particu- lar Lyapunov stability theory) may facilitate the analysis. Various continuous-time distributed algorithms have been developed and can be categorized into two groups depending on whether an algo- rithm uses the ﬁrst-order gradient information or the second-order Hessian information, which will be reviewed in Section 3.2.1 and Section 3.2.2 , respectively. Similar to the discrete-time case, we fo- cus on the distributed algorithms for ﬁxed undirected graphs in this subsection and will review various extensions in Section 4 . 3.2.1. First-order gradient-based algorithms Distributed PI algorithm. By using the PI control strategy, var- ious continuous-time distributed algorithms have been developed in the literature. The earliest work is perhaps ( Wang & Elia, 2010 ), where the authors proposed the following algorithm: ˙ x i (t) = N ∑ j=1 a ij (x j (t) − x i (t)) + N ∑ j=1 a ij (v j (t) − v i (t)) − ∇ f i (x i (t)) , (9a) ˙ v i (t) = N ∑ j=1 a ij (x i (t) − x j (t)) , (9b) where a ij > 0 is the edge weight for edge ( j, i ) ∈ E and a ij = 0 oth- erwise. The algorithm is motivated by a feedback mechanism and is a proportional-integral control. More speciﬁcally, in (9a) , the term −∇ f i (x i (t)) ensures that each agent follows its local gradient de- scent while the term ∑ N j=1 a ij (x j (t) − x i (t)) ensures that consensus is achieved among agents. However, if the dynamics just contains these two terms, the agents’ states would not converge since the local gradients are not the same in general. Thus, to correct the er- ror, the additional integral feedback term ∑ N j=1 a ij (v j (t) − v i (t)) is included, where the dynamics of v i ( t ) is governed by (9b) . By deﬁning x (t) = [ x ⊤ 1 (t) , x ⊤ 2 (t) , . . . , x ⊤ N (t)] ⊤ and v (t) = [ v ⊤ 1 (t) , v ⊤ 2 (t) , . . . , v ⊤ N (t)] ⊤ , algorithm (9) can be rewritten in a more compact form: ˙ x (t) = −(L \u0002 I n ) x (t) − (L \u0002 I n ) v (t) − ∇ f (x (t)) , (10a) ˙ v (t) = (L \u0002 I n ) x (t) , (10b) where L = [ ℓ ij ] ∈ R N×N , with ℓ ii = ∑ N j=1 a ij and ℓ ij = −a ij for j ̸ = i , is the Laplacian matrix associated with the underlying communication graph, f (x (t)) = ∑ N i =1 f i (x i (t )) and ∇ f (x (t )) = [ ∇ f ⊤ 1 (x 1 (t)) , ... , ∇ f ⊤ N (x N (t))] ⊤ . Note that algorithm (10) can also be interpreted as a primal- dual algorithm as shown in Gharesifard and Cortés (2014) ; Wang and Elia (2011) and Dörﬂer (2019) . More speciﬁcally, as shown in Gharesifard and Cortés (2014 , Lemma 3.1), the dis- tributed optimization problem (1) is equivalent to the following optimization problem: min x ∈ R Nn N ∑ i =1 f i (x i ) (11a) s . t . (L \u0002 I n ) x = 0 Nn . (11b) For the above optimization problem (11), consider the aug- mented Lagrangian function L (x, v ) = f (x ) + v ⊤ (L \u0002 I n ) x + 1 2 x ⊤ (L \u0002 I n ) x. Then algorithm (10) is the associated saddle-point ﬂow, which is also the continuous-time gradient ﬂow algorithm developed in Arrow, Huwicz, and Uzawa (1958) . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 7 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] For undirected connected graphs, it is shown in Wang and Elia (2010) that algorithm (10) solves the distributed optimization problem asymptotically, that is, lim t→∞ x (t) = 1 N \u0002 x ∗. Moreover, lim t→∞ v (t) = v ∗, where (L \u0002 I n ) v ∗ = ∇ f (1 N \u0002 x ∗) . 3.2.2. Second-order algorithms In this subsection, we review the existing second-order dis- tributed algorithms based on the second-order Hessian informa- tion. Compared to the ﬁrst-order distributed algorithms, these al- gorithms result in a faster convergence rate by exploring the Hes- sian information. Zero-Gradient-Sum Algorithm. The authors of Lu and Tang (2012) developed the following algorithm: ˙ x i (t) = γ ( ∇ 2 f i ( x i (t) ) )−1 ∑ j∈N i a ij (x j (t) − x i (t)) , (12) where γ > 0 is a gain parameter. The algorithm is initialized with x i (0) = x ∗ i , where x ∗ i is the minimizer of the local objective func- tion f i ( x ). Note that it is shown that ∑ i ∈V ∇ f i (x i (t)) = 0 for all t ≥ 0 if the undirected network is connected. For this reason, the dis- tributed algorithm (12) is called Zero-Gradient-Sum (ZGS) algo- rithm. For undirected connected graphs, the authors of Lu and Tang (2012) showed that algorithm (12) exponentially converges to the global minimizer via Lyapunov stability analysis if local objec- tive functions are twice continuously differentiable, strongly con- vex, and have locally Lipschitz Hessians. 4. Extensions In Section 3 , we have reviewed various existing discrete-time and continuous-time distributed algorithms for solving the un- constrained optimization problem over undirected graphs. In this section, we discuss how these algorithms have been extended in various directions to handle more realistic scenarios. In particu- lar, Section 4.1 describes how these algorithms can be extended to directed communication networks, which may be time-varying. Section 4.2 reviews distributed optimization algorithms for the case where agent dynamics are more complicated. Section 4.3 dis- cusses how to handle the distributed optimization problem with constraints. We then focus on the case where the communication networks may be subject to communication constraints, such as time-delays in Section 4.4 and random graphs in Section 4.5 . In Section 4.6 , we describe how to design event-triggered communi- cation strategies for distributed optimization algorithms to avoid continuous communication and to reduce communication over- heads. Section 4.7 reviews distributed ﬁnite-time optimization al- gorithms. In these subsections for various extensions, we ﬁrst provide an overview of both discrete-time and continuous-time optimization algorithms, if both exist in the literature. Although we only focus on one extension in each of following subsections, these extensions are not independent but actually may overlap one another to some extent. We will also brieﬂy discuss the future research directions at the end of each subsection. 4.1. Directed graphs In this subsection, we focus on the case where the commu- nication networks are directed and possibly time-varying, since in practice, the information exchange may be unidirectional due to nonuniform communication powers, and the network topology may vary due to unexpected loss of communication links. Various discrete-time and continuous-time distributed optimization algo- rithms have been proposed for directed networks, which will be discussed in Section 4.1.1 and Section 4.1.2 , respectively. 4.1.1. Discrete-time Existing discrete-time distributed algorithms can be divided into two classes depending on the method an algorithm uses to handle directed communication networks. Distributed Push-Sum Based Algorithms. Most existing discrete-time distributed algorithms for directed graphs are based on the push-sum method ( Bénézit, Blondel, Thiran, Tsitsiklis, & Vetterli, 2010; Kempe, Dobra, & Gehrke, 2003; Nedi ´c & Olshevsky, 2015 ) or ratio consensus ( Charalambous et al., 2015; Domínguez-García & Hadjicostis, 2011; 2015 ; Hadjicostis & Charalambous, 2014 ), which relaxes the require- ment of doubly stochastic mixing matrices in the distributed algorithms reviewed in Section 3.1 for undirected graphs to col- umn stochastic matrices. In the push-sum method, each agent runs two linear iterations simultaneously with a column stochastic matrix, where the initial conditions of these two iterations are the initial estimations for the optimal solution and the vector of all ones, respectively, and the ratio of the two states converges to the initial average for strongly connected directed graphs which are not necessarily weight-balanced. By using this method together with the (sub)gradient method, various distributed algorithms have been developed. The earlier studies focused on developing distributed algo- rithms with diminishing step-sizes for the case where the local convex objective functions are not necessarily differentiable but with bounded subgradients. The authors of Tsianos, Lawlor, and Rabbat (2012) developed a distributed algorithm based on the push-sum method and dual averaging ( Duchi et al., 2012 ) for ﬁxed directed graphs which are strongly connected. Later, the authors of Nedi ´c and Olshevsky (2015) studied general directed time-varying networks which are uniformly jointly strongly connected. The con- vergences of these proposed algorithms are rather slow due to the diminishing step-sizes. To accelerate the convergence process, distributed algorithms with ﬁxed step-sizes by using the push-sum method have been proposed in Nedi ´c et al. (2017a , 2017b) ; Xi and Khan (2017) ; Zeng and Yin (2017) . In particular, the authors of Zeng and Yin (2017) developed an algorithm, termed ExtraPush, for the case where the local objective functions are quasi-strongly convex and smooth. the authors of Xi and Khan (2017) developed an algo- rithm, termed DEXTRA (Directed-EXTRA), for the case where the local objective functions are restricted strongly convex with respect to the global minimizer and smooth. The step-sizes of both Ex- traPush and DEXTRA are restrictive in the sense that their lower bounds are strictly greater than zero. the authors of Xi, Xin, and Khan (2018) proposed an algorithm, termed ADD-OPT (Accelerated Distributed Directed Optimization), in which the lower bound of the step-size is zero, and thus supports a wider range of step-sizes. The linear convergence was established for the case where the lo- cal objective functions are strongly convex and smooth. It is shown in Nedi ´c et al. (2017a) that the ATC variant of the DIGing algo- rithm with the push-sum method is applicable to time-varying di- rected graphs which are uniformly jointly strongly connected. This algorithm was extended with uncoordinated step-sizes in Nedi ´c et al. (2017b) and the linear convergence was established for con- nected undirected graphs even if the step-sizes are not identical. Distributed Push–Pull Based Algorithms. The second class of algorithms is based on the push–pull method (see, e.g., Du, Yao et al., 2018; Pu, Shi, Xu, & Nedi ´c, 2018; Xin & Khan, 2018; Yang et al., 2013 ). In Yang et al. (2013) , the authors focused on the case where local objective functions are quadratic and de- veloped a distributed algorithm based on the surplus idea ( Cai & Ishii, 2012 ). The asymptotic convergence was established for strongly connected directed graphs if the learning gain parameter is suﬃciently small. Recently, to extend the DIGing algorithm (6) to directed networks and general non-quadratic objective functions, Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 8 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] distributed algorithms developed in Du, Yao et al. (2018) ; Xin and Khan (2018) and Pu et al. (2018) use a row stochastic matrix for the mixing of estimates of the optimal point, while they employ a column stochastic matrix for tracking the average gradient. From the viewpoint of an agent, the information about the estimates of the optimal point is pushed to the neighbors, while the infor- mation about the gradients is pulled (collected) from the neigh- bors, hence giving the name push–pull gradient method ( Pu et al., 2018 ). For the case where local objective functions are strongly convex and smooth, the linear convergence has been established for these push–pull based algorithms over strongly connected di- rected graphs ( Du, Yao et al., 2018; Pu et al., 2018; Xin & Khan, 2018 ). Compared with push-sum based algorithms, distributed push– pull based algorithms require less computation and communica- tion, since in push-sum based algorithms, the division operators are used, which causes additional computation ( Pu et al., 2018 ). Recently, the authors of Zhang, Yi, George, and Yang (2019) pre- sented a uniﬁed framework based on IQC for analyzing the lin- ear convergence of various push–pull based algorithms proposed in Du, Yao et al. (2018) ; Xin and Khan (2018) and Pu et al. (2018) and the push–pull variant of the algorithm proposed in ( Xu et al., 2018a ) when local objective functions are strongly convex and smooth. The work can be viewed as an extension of the IQC frame- work recently proposed in ( Sundararajan et al., 2017 ) for undi- rected graphs to directed graphs. the authors of Saadatniaki, Xin, and Khan (2018) developed a push–pull algorithm for time varying directed graphs which are uniformly jointly strongly connected. To implement both distributed push-sum based and push–pull based algorithms, each agent is required to know its out-degree in order to construct a column stochastic matrix. This may be imprac- tical when agents use broadcast-based communication ( Hendrickx & Tsitsiklis, 2015 ). On the other hand, the construction of a row stochastic matrix is much easier since each agent could assign the edge weight to its in-neighbors. As shown in Hendrickx and Tsit- siklis (2015) , in order to compute the optimal solution for the dis- tributed optimization of a sum of local functions in the context of broadcast-based algorithms, each agent needs to know either its out-degree or its unique identiﬁer. Recently, by assuming that each agent knows its unique identiﬁer, the authors of Xi, Mai, Xin, Abed, and Khan (2018) developed a distributed algorithm which only uses a row stochastic matrix. To cancel the imbalance caused by employing only the row-stochastic matrix, each agent holds an additional variable that converges asymptotically to the left eigen- vector corresponding to the eigenvalue at 1 of the row-stochastic matrix. The gradient is then divided by this additional variable to cancel the imbalance. The linear convergence was established for strongly connected directed graphs if the step-size is less than a certain value. This algorithm was later extended to uncoordinated step-sizes in Xin, Xi, and Khan (2019) and the linear convergence was established even if the step-sizes are not identical as long as the largest step-size is suﬃciently small. 4.1.2. Continuous-time In this subsection, we discuss how to extend continuous-time algorithms reviewed in Section 3.2 for undirected graphs to di- rected graphs. Algorithm (10) originally proposed in Wang and Elia (2010) for undirected graphs was extended to directed graphs in Gharesifard and Cortés (2014) . In particular, the authors developed the following algorithm: ˙ x (t) = −α(L \u0002 I n ) x (t) − L v (t) − ∇ f (x (t)) , (13a) ˙ v (t) = (L \u0002 I n ) x (t) , (13b) where α > 0 is a design parameter. Under the assumptions that the directed graph is strongly connected and weight-balanced and the local convex objective functions are smooth, it is shown that algo- rithm (13) asymptotically converges to the global minimizer if the gain parameter α is appropriately chosen. Note that both algorithm (10) and algorithm (13) require com- munication for both variables x and v . In order to reduce the over- all communication, the authors of Kia et al. (2015a) developed the following algorithm: ˙ x (t) = −β(L \u0002 I n ) x (t) − v (t) − α∇ f (x (t)) , (14a) ˙ v (t) = αβ(L \u0002 I n ) x (t) , (1 ⊤ N \u0002 I n ) v (0) = 0 n , (14b) where α, β > 0 are gain parameters. Compared with algorithms (10) and (13), algorithm (14) only needs the communication for the variable x , but not for the vari- able v . On the other hand, in order to remove the communica- tion among the variable v , it requires a special initialization for v (0) as given in (14b) . Under assumptions that the directed graph is strongly connected and weight-balanced, and the local objec- tive functions are strongly convex and smooth, the authors of Kia et al. (2015a) established the exponential convergence of al- gorithm (14) with properly chosen gain parameters. Note that gain parameters in most existing algorithms often depend on some global information, such as the Lipschitz con- stant and the network connectivity. To remove such a requirement, several distributed adaptive algorithms have been developed (see, e.g., Li, Ding, Sun, & Li, 2018; Lin, Ren, & Farrell, 2017; Zhao, Liu, Wen, & Chen, 2017 ). Continuous-time distributed algorithms dis- cussed so far are based on the PI control strategy and are ﬁrst- order algorithms based on the gradient information. On the other hand, the ZGS algorithm (12) which uses the second-order Hes- sian information proposed in Lu and Tang (2012) has also been ex- tended to strongly connected and weight-balanced directed graphs in Chen and Ren (2016) and Guo and Chen (2018) . In summary, various discrete-time and continuous-time dis- tributed optimization algorithms have been extended to directed graphs. In the discrete-time setting, common techniques for the convergence analysis are linear systems of inequalities and the small-gain theory. These techniques have been extended in Nedi ´c et al. (2017a , 2017b) to time-varying directed graphs which are uniformly jointly strongly connected but not necessarily weight- balanced. In the continuous-time setting, a common technique used for the convergence analysis is the Lyapunov stability the- ory. For ﬁxed directed graphs which are strongly connected and weight-balanced, various Lyapunov functions have been developed in the existing studies. However, it is diﬃcult to construct a Lya- punov function for general unbalanced directed graphs. Moreover, it is also challenging to construct a common or multiple Lyapunov function for time-varying graphs ( Liberzon & Morse, 1999 ). There- fore, the Lyapunov analysis has been only extended to time-varying directed graphs for certain special cases. For example, for the case where all local objective functions have a common minimizer, the common Lyapunov function has been constructed in Shi, Johans- son, and Hong (2013) . An interesting research direction is to extend the Lyapunov analysis to general unbalanced ﬁxed directed graphs and time-varying directed graphs. 4.2. Complex agent dynamics Most existing distributed optimization algorithms discussed so far can be viewed as distributed optimal coordination algorithms for multi-agent systems with single integrator agent dynamics. However, in many practical applications, the agent dynamics may Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 9 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] be more complicated, such as those of double integrators, high- order systems, and Euler–Lagrangian (EL) systems. For example, EL systems have been used to describe many mechanical sys- tems, such as mobile robots, rigid bodies, and autonomous vehi- cles ( Lynch & Park, 2017; Spong, Hutchinson, & Vidyasagar, 2006 ). Moreover, any EL system with exact knowledge of nonlinearities can be transformed into a double-integrator system. Recently, focusing on undirected connected graphs or strongly connected and weight-balanced directed graphs, distributed op- timization algorithms for multi-agent systems with more com- plicated agent dynamics have been developed. Most studies fo- cused on continuous-time agent dynamics. The research is pro- gressing with an increasing complexity of agent dynamics. The double integrator agent dynamics were considered in Liu and Wang (2015) ; Yi, Yao, Yang, George, and Johansson (2018) ; Zhang and Hong (2014) and Wang, Gupta, and Wang (2018) . In particular, the authors of Zhang and Hong (2014) developed a distributed algorithm and showed that the proposed algorithm asymptotically converges to the optimal solution if the local objec- tive functions are strongly convex and smooth. Note that the pa- rameters of the algorithm proposed in Zhang and Hong (2014) de- pend on some global information. To relax such dependence, the authors of Yi et al. (2018) developed an alternative distributed al- gorithm where no global information is needed to be known in ad- vance. Moreover, the exponential convergence was established for the case where each local objective function is locally smooth and the global objective function is restricted strongly convex with re- spect to the global minimizer, which is more general compared to the strong convexity required in Zhang and Hong (2014) . There are a few works which studied the case where the agent dynamics are high-order systems. For agents described by chains of integrators, the authors of Zhang and Hong (2015) de- veloped a continuous-time distributed algorithm based on the gradient method and the integral feedback idea. For the case where agents are general linear-time invariant systems and the local objective functions are quadratic-like, two distributed adap- tive algorithms based on dynamic coupling gains have been de- veloped in Zhao, Liu et al. (2017) . For the case where the agent dynamics are EL systems, continuous-time distributed algorithms have been developed in Meng et al. (2017) ; Qiu, Hong, and Xie (2016) and Zhang, Deng, and Hong (2017) . In particular, the authors of Meng et al. (2017) considered a special case where the intersection of local optimal convex sets is non-empty, while the general case was considered in Zhang et al. (2017) . the authors of Zhang et al. (2017) developed two continuous-time distributed algorithms for the case without parametric uncertainties and the case with parametric uncertainties, respectively. The exponential convergence was established for the case without parametric un- certainties, while the asymptotic convergence was established for the case with parametric uncertainties. Apart from more complex agent dynamics, another important issue is the physical constraints of the actuator since every ac- tuator is subject to saturation due to its physical limitations. Al- though various distributed control laws (algorithms) have been de- veloped, these proposed control laws designed in the absence of actuator saturation may fail to solve the distributed optimization problem in the presence of actuator saturation. Note that global consensus without the optimality concern has been widely studied (see, e.g., Li, Xiang, & Wei, 2011; Meng, Zhao, & Lin, 2013; Yang, Meng, Dimarogonas, & Johansson, 2014; Yi, Yang, Wu, & Johans- son, 2019a; Zhao & Lin, 2016 ). However only a limited number of results are available on global optimal consensus (or equivalently distributed optimization) for multi-agent systems in the presence of actuator saturation. the authors of Xie and Lin (2017) devel- oped distributed protocols for single integrator agents and dou- ble integrator agents, and showed that global optimal consensus is achieved in the presence of actuator saturation when the under- lying network is strongly connected and detailed balanced. The de- sign was later extended to high-order integrator agents in Xie and Lin (2019) . For EL systems, the authors ( Qiu, Hong et al., 2016 ) de- veloped a distributed protocol to achieve global optimal consensus under given constraints on velocity with the requirement that the control input is bounded. In summary, most studies in this direction assume that the graph is either undirected and connected or directed strongly con- nected and weight-balanced. Some of these results have been ex- tended to the case with local constraints in Liu and Wang (2015) ; Qiu, Hong et al. (2016) ; Qiu, Liu, and Xie (2016) and Qiu, Liu, and Xie (2018) . To avoid the continuous communication and to reduce the communication overheads, several event-triggered algorithms for the case where the agent dynamics are second-order have de- veloped in Yi et al. (2018) and Wang, Gupta et al. (2018) . These two extensions (local constraints and event-triggered communica- tion schemes) will be discussed in Section 4.3 and Section 4.6 , re- spectively. There are a few other future research directions: i) extend the commonly used Lyapunov stability theory in these existing studies for high-order multi-agent systems to unbalanced directed graphs. Compared with the Lyapunov analysis for ﬁrst-order multi-agent systems, the complex agent dynamic makes such an extension more challenging; ii) investigate the robustness properties of these existing distributed algorithms, since various disturbances, arising from either environment or communication, are ubiquitous in re- ality. Some preliminary results are available in Mateos-Núñez and Cortés (2016) ; Wang, Hong, and Ji (2016) . In particular, the au- thors of Mateos-Núñez and Cortés (2016) studied the distributed PI algorithms for single integrator agents with persistent Gaus- sian white noise, and showed that the resulting stochastic dynam- ics is noise-to-state exponentially stable in the second moment, while the authors of Wang, Hong et al. (2016) considered the case where the agent dynamics are heterogeneous nonlinear high-order systems perturbed by external disturbances, and developed a dis- tributed protocol based on the internal model principle; iii) study the multi-agent systems with discrete-time agent dynamics as such models are relevant for many practical sampled-data systems. Some studies are available in this direction but mainly focused on discrete-time single integrator agents. For example, the authors of Yang et al. (2018) extended the results of Xie and Lin (2017) for the continuous-time single-integrator agents to the discrete-time set- ting, while the authors of Qiu et al. (2018) developed a distributed bounded control protocol with time-varying gain parameters based on the local subgradient descent and the projection method for solving the distributed constrained optimization problem. 4.3. Constrained optimization So far, we have only focused on the distributed unconstrained optimization problem. However, in physical applications, there may exist various constraints such as local constraints, global inequality and equality constraints. Such a constrained optimization problem can be formulated as follows: min x ∈ R n N ∑ i =1 f i (x ) (15a) s . t . x ∈ ∩ N i =1 \ti , (15b) g(x ) ≤ 0 m , (15c) h (x ) = 0 p , (15d) Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 10 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] where the function f i : R n → R is the local convex objective func- tion for agent i , and \ti is the local constraint set known to agent i only and is assumed to be closed and convex. The function g : R n → R m with each component g ℓ , ℓ ∈ { 1 , 2 , . . . , m } , being con- vex. The inequality g ( x ) ≤ 0 m is understood to be component-wise, i.e., g ℓ ( x ) ≤ 0 for all ℓ ∈ { 1 , 2 , . . . , m } , and represents a global in- equality constraint. The function h : R n → R p with each component h ℓ , ℓ ∈ { 1 , 2 , . . . , p} , being convex, represents a global equality con- straint. 4.3.1. Local constraint sets We begin with a simple case where each node is subject to a local constraint set and there is no global inequality and equality constraints, that is, constraints (15c) and (15d) in (15) are nonexis- tent. The constrained optimization problem (15) then reduces to: min x ∈ R n N ∑ i =1 f i (x ) (16a) s . t . x ∈ ∩ N i =1 \ti . (16b) The earlier studies focused on the special case when the con- straint sets are identical (common), i.e., \ti for i = 1 , 2 , . . . , N, are the same. In the discrete-time setting, the authors of Nedi ´c, Ozdaglar, and Parrilo (2010) developed a distributed projected (sub)gradient algorithm with diminishing step-sizes, in which each agent performs distributed algorithm (2) and takes the projection of this vector on the common constraint set. In the continuous- time setting, the authors of Qiu, Liu et al. (2016) developed a dis- tributed protocol containing three terms: local averaging, local pro- jection, and local subgradient with a diminishing but persistent gain. Both algorithms were shown to converge to an optimal point if the undirected graph is uniformly jointly connected. For the general case when the constraint sets are different, the convergence of the projected subgradient algorithm was only es- tablished for ﬁxed fully connected graphs in Nedi ´c et al. (2010) . Such a result was extended to time-varying directed graphs in Zhu and Martínez (2012) and Lin, Ren, and Song (2016) , where the authors established the convergence results when time-varying di- rected graph is uniformly jointly strongly connected and weight- balanced at all times. Recently, the authors of Wang, Lin, Ren, and Song (2018) established the convergence under the same condition without the square summable requirement of the step-sizes in (3) . Note that these existing methods require each node to compute a projection on the local constraint set, which may not be easy. the authors of Aybat and Hamedani (2016) and Hamedani and Ay- bat (2017) developed distributed primal-dual algorithms for the case where the objective functions are composite convex and the constraint sets are conic. Although the aforementioned optimization algorithms are ap- plicable to time-varying graphs, their convergence is rather slow due to the required diminishing step-sizes. To accelerate the con- vergence rate, a few distributed algorithms have been developed for ﬁxed undirected graphs. In the discrete-time setting, the au- thors of Lei et al. (2016) developed a distributed primal-dual al- gorithm with a ﬁxed step-size. The proposed algorithm can be viewed as an extension of the EXTRA ( Shi et al., 2015a ) and dis- tributed PI algorithm ( Yao et al., 2018 ) to the constrained case. In the continuous-time setting, a few distributed algorithms are also available. the authors of Liu and Wang (2015) extended the distributed PI algorithm originally proposed in Gharesifard and Cortés (2014) to the constrained case. Note that the auxiliary vari- ables of the proposed algorithm may be asymptotically unbounded, which makes the proposed algorithm hard to implement in prac- tice. Such an issue was removed later in Zeng, Yi, and Hong (2017) , where the authors developed a distributed algorithm which has bounded states while seeking the optimal solutions. These pro- posed algorithms in Lei et al. (2016) ; Liu and Wang (2015) and Zeng et al. (2017) are applicable to nonsmooth local convex ob- jective functions and the asymptotic convergence result was es- tablished for connected undirected graphs. For the special case where local objective functions are strongly convex and locally smooth on the constraint set, the authors of Wang, Wang, Sun, and Wang (2018) proposed a distributed algorithm based on the inte- gral control strategy and the projection method, and established its exponential convergence if the parameters are properly chosen. 4.3.2. Global constraints In Section 4.3.1 , we have discussed the distributed constrained optimization problem when each agent is subject to a local con- straint set. In this subsection, we consider the general case when global inequality and equality constraints are also present in addi- tion to the local constraints. We ﬁrst consider the case where there are only global inequal- ity constraints, that is, constraints (15d) in (15) are nonexistent. The constrained optimization problem (15) then reduces to: min x ∈ R n N ∑ i =1 f i (x ) (17a) s . t . x ∈ ∩ N i =1 \ti , (17b) g(x ) ≤ 0 m , (17c) By assuming that the global inequality constraints are known to all the agents, a few discrete-time distributed algorithms with diminishing step-sizes have been proposed. In particular, the au- thors of Zhu and Martínez (2012) developed a primal-dual pro- jected subgradient algorithm with diminishing step-sizes based on the characterization of saddle points of the augmented Lagrangian function. It is shown that the algorithm asymptotically converges to an optimal solution for time-varying directed graphs which are weight-balanced and uniformly jointly strongly connected. For the case where local constraints set are identical, the au- thors of Yuan, Xu, and Zhao (2011) proposed a distributed primal- dual subgradient algorithm with a ﬁxed step-size and multiple up- dates for each consensus step. For connected undirected graphs, it is shown that the algorithm converges to the optimal point within the error level depending on the number of consensus updates. Note that aforementioned algorithms in this subsection require each agent projecting its primal-dual estimate onto some convex sets at every iteration, which results in high computational cost. To reduce such a cost, a distributed regularized primal-dual subgradi- ent algorithm with a ﬁxed step-size was developed in Yuan, Ho, and Xu (2016) for the case without local constraints, whose imple- mentation requires only one projection at the last iteration. More- over, the explicit convergence rate for the objective error was also established for time-varying directed graphs which are uniformly jointly strongly connected. Next, we consider the case where there are only global linear equality constraints, that is, constraints (15c) in (15) are nonexis- tent. The constrained optimization problem (15) then reduces to: min x ∈ R n N ∑ i =1 f i (x ) (18a) s . t . x ∈ ∩ N i =1 \ti , (18b) h (x ) = 0 p , (18c) Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 11 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] where the function h : R n → R p is deﬁned as h (x ) = Bx − b with B ∈ R p×n . For the case where local constraint sets are identical, the au- thors of Zhu and Martínez (2012) developed a distributed penalty primal-dual subgradient algorithm with a diminishing step-size. It is shown that the algorithm asymptotically converges to an op- timal solution for time-varying directed graphs which are uni- formly jointly strongly connected and weight-balanced at all times. For the general case where local constraint sets are different, the authors of Liu, Yang, and Hong (2017) developed discrete- time distributed algorithm with a ﬁxed step-size. Asymptotic con- vergence to the global minimizer was established for undirected connected graphs if the step-size is less than an estimable upper bound. In the continuous-time setting, the authors ( Yang, Liu, & Wang, 2017b; Zhu, Yu, Wen, Chen, & Ren, 2018 ) developed distributed algorithms based on the PI control strategy and the subgradient method, respectively. By using the nonsmooth analysis and the Lyapunov stability theory, the asymptotic convergence results were established for undirected connected graphs. In summary, most existing discrete-time and continuous-time distributed algorithms for solving the distributed constrained op- timization problem are only applicable to undirected graphs or weight-balanced digraphs. One future research direction is to de- velop distributed algorithms for unbalanced digraphs. An interest- ing result in this direction has been obtained by Xie, You, Tempo, Song, and Wu (2018) , where the authors considered a distributed optimization problem with a local inequality constraint and de- veloped a discrete-time distributed algorithm with a diminishing step-size using the epigraph form of the constrained optimization. It is shown that the proposed algorithm asymptotically converges to the common optimal point even for time-varying unbalanced di- graphs which are uniformly jointly strongly connected. It is worthy to investigate if such a result can be extended to the distributed al- gorithms with ﬁxed step-sizes. Another interesting future direction is to consider the opti- mization problem with a general set of equality and inequal- ity constraints that couple all the agents’ decision variables. In the presence of a coupled constraint, the feasible region of one agent’s decision variable is inﬂuenced by some other agents’ de- cision variables. If such a constraint is known by all agents, var- ious algorithms have been proposed (see, e.g., primal-dual al- gorithms proposed in Cherukuri, Mallada, & Cortés, 2016; Fei- jer & Paganini, 2010; Qu & Li, 2019 ). However, in practice, such a coupled constraint may not be known to each agent. To meet this challenge, a few distributed algorithms have been developed, among which discrete-time algorithms are given in Chang, Nedi ´c, and Scaglione (2014) ; Chatzipanagiotis, Dentcheva, and Zavlanos (2015) ; Chatzipanagiotis and Zavlanos (2016) ; Falsone, Margellos, Garatti, and Prandini (2017) ; Margellos, Falsone, Garatti, and Prandini (2018) ; Nedi ´c, Olshevsky, and Shi (2018) and continuous-time algorithms are presented in Cherukuri and Cortés (2015, 2016) ; Deng, Liang, and Yu (2018) ; Liang, Zeng, and Hong (2018) ; Yi, Hong, and Liu (2016) and Deng, Liang, and Hong (2018) . Also note that most of these existing studies do not con- sider the communication network imperfections, such as vary- ing topologies, time-delays, and packet drops. It is worthy to study the effects of these network imperfections to the exist- ing algorithms and to develop distributed algorithms for solv- ing the distributed constrained optimization problem even in the presence of these communication imperfections. In the next two subsections, we provide an overview of the existing distributed optimization algorithms that deal with communication network imperfections. 4.4. Time delays In this subsection and the next subsection, we respectively dis- cuss the effects of two communication imperfections, time-delays and random graphs, on the existing distributed optimization algo- rithms. Time delays are ubiquitous in practical systems especially multi-agent systems, due to extra time required to get information, limited communication speed, computation time and execution time required to implement the control input ( Aström & Kumar, 2014; Cao et al., 2013; Hespanha, Naghshtabrizi, & Xu, 2007 ). It is known that for a single system time delays might degrade the sys- tem performance or even destroy the stability ( Richard, 2003; Zhu, Qi, Ma, & Chen, 2018 ). The effects of time delays on various ex- isting discrete-time and continuous-time distributed optimization have been investigated, and will be reviewed in Section 4.4.1 and Section 4.4.2 , respectively. 4.4.1. Discrete-time In the discrete-time setting, the effects of both constant de- lays and time-varying bounded delays have been studied for ex- isting distributed optimization algorithms. To model the bounded communication time delays, a common approach for convergence analysis is to use the augmented graph idea originated from Tsitsiklis (1984) , which has also been used in the literature on dis- tributed consensus ( Cao, Morse, & Anderson, 2008; Charalambous et al., 2015; Hadjicostis & Charalambous, 2014; Nedi ´c & Ozdaglar, 2010 ). More speciﬁcally, for each node in the original graph, ex- tra virtual nodes are introduced to capture the effect of delays on various communication links. The distributed optimization prob- lem with time delays in the original communication network is then reduced to the problem without delays in the augmented graph. The earlier studies focused on distributed algorithms with di- minishing step-sizes. For ﬁxed directed networks, it is shown in Tsianos et al. (2012) ; Tsianos and Rabbat (2011) that the distributed algorithm based on dual averaging proposed in Duchi et al. (2012) can be extended to case with both ﬁxed con- stant delays and random bounded delays, for undirected ﬁxed graphs and strongly connected directed graphs, respectively. For time-varying directed networks, the weight-balanced case and the unbalanced case have been considered in Lin et al. (2016) and Yang, Lu et al. (2017) , respectively. In particular, the authors of Lin et al. (2016) extended the dis- tributed algorithm based on the subgradient projection originally proposed in Nedi ´c et al. (2010) for the distributed constrained optimization problem to handle communication delays and non- identical constraint sets. It is shown that the proposed subgradient projection algorithm solves the distributed constrained optimiza- tion problem if the time-varying directed network is uniformly jointly strongly connected and weight-balanced at all times, even in the presence of arbitrarily large bounded time-varying delays. the authors of Yang, Lu et al. (2017) showed that the distributed algorithm based on the push-sum method originally proposed in Nedi ´c and Olshevsky (2015) still converges to the optimal solution if the time-varying directed network is uniformly jointly strongly connected but not necessarily weight-balanced, even in the presence of arbitrarily large bounded time-varying delays. Note that the above distributed algorithms use diminishing step-sizes, which results in slow convergence. Recently, the ef- fects of constant time delays to the distributed algorithm with a ﬁxed learning gain (step-size) proposed in Yang et al. (2013) for ﬁxed directed strongly connected graphs have been investigated in Yang, Wu, Sun, and Lian (2015) ; Zhao, Duan, and Shi (2019) . It is shown in Zhao, Duan et al. (2019) that the algorithm still Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 12 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] converges to the global minimizer even in the presence of nonuni- form constant time delays provided that the ﬁxed step-size is suf- ﬁciently small. The explicit upper bound on the step-size to en- sure the convergence was also established for uniform constant delays. 4.4.2. Continuous-time The effects of communication time delays on the existing distributed continuous-time optimization algorithms have also been studied, and the ﬁndings are reviewed in this subsection. the authors of Yang, Liu, and Wang (2017a) investigated the effects of delays on the distributed PI algorithm proposed in Kia et al. (2015a) for strongly connected and weight-balanced directed graphs. By using a Lyapunov–Krasovskii functional for time-delay systems, the authors established the delay-dependent and delay-independent suﬃcient conditions in the form of lin- ear matrix inequalities (LMIs) for both slow varying delays and fast varying delays. The results were later extended to the case where the agent dynamics are double integrators in Tran, Wang, Liu, and Xiao (2017) and when each agent is subject to a local bounded constraint set in Wang, Wang, Chen, and Wang (2018) . in Hatanaka, Chopra, Ishizaki, and Li (2018) , the authors provided a passivity-based perspective for the distributed PI algorithm pro- posed in Wang and Elia (2010) . By using the Lyapunov analysis and passivity, the authors showed that the algorithm is capable of han- dling arbitrarily large unknown constant time delays. The authors of Guo and Chen (2018) studied the performance of the distributed zero-gradient-sum algorithm proposed in Lu and Tang (2012) in the face of time-varying communication delays. By constructing a Lyapunov–Krasovskii functionals, they established explicit suﬃcient conditions for the maximum admissible time de- lay to guarantee the convergence for ﬁxed undirected connected graphs. The extension to time-varying undirected graphs that are connected at all times was also studied by using a common Lya- punov function. For connected undirected graphs, the authors of Doan, Beck, and Srikant (2017) studied the effects of uniform constant com- munication time delays on the continuous-time version of the DGD algorithm proposed in Nedi ´c et al. (2010) . By constructing a Lyapunov–Razumikhin function, the authors provided an explicit analysis of the convergence rate of the algorithm as a function of the network size, topology, and time delay. In summary, various discrete-time and continuous-time algo- rithms have been extended to solve the distributed optimization problem in the presence of communication time delays. In the discrete-time setting, a common approach for the convergence analysis is to use the augmented graph idea ( Tsitsiklis, 1984 ) to convert the distributed optimization problem with bounded time delays in the original graph to the distributed optimiza- tion problem without time delays in the augmented graph. In the continuous-time setting, a common technique for the con- vergence analysis is to use the Lyapunov analysis based on Lyapunov–Krasovskii functionals or Lyapunov–Razumikhin func- tions ( Fridman, 2014 ). Most existing studies focused on the case where the agent dynamics are single-integrator. It is an interest- ing future research direction to study the effects of communication time delays on the existing distributed algorithms for multi-agent systems with high-order agent dynamics. The objective here is to explicitly characterize the delay margin for high-order multi-agent systems subject to unknown communication time delays, which can be reviewed as a non-trivial extension of the recent studies ( Ma & Chen, 2019 ) for a single agent and ( Ma, Tian, Zulﬁqar, Chen, & Chai, 2019 ) for distributed consensus to distributed optimization. This in turn provides an upper bound on the time delays under which the convergence of the distributed continuous-time algo- rithms is still guaranteed albeit slower. 4.5. Random graphs In addition to communication time delays, there exist other un- certainties in communication networks, such as packet drops and link failures. Although time delays can be used to model unreliable communication networks with these uncertainties, a more realis- tic approach is to model such communication networks as random graphs ( Schenato, Sinopoli, Franceschetti, Poolla, & Sastry, 2007 ). Thus, it is important to investigate the performance of the existing distributed optimization algorithms in the face of these communi- cation uncertainties. The earlier studies focused on discrete-time distributed al- gorithms for undirected graphs. the authors of Lobel and Ozdaglar (2011) considered the case where communication links fail according to a given stochastic process and showed that the DGD algorithm with a diminishing step-size proposed in Nedi ´c and Ozdaglar (2009) almost surely converges to an optimal solu- tion if the link failures are independent and identically distributed (i.i.d.) over time and the expected graph is connected. With a ﬁxed step-size, the authors of Matei and Baras (2011) showed that for twice continuously differentiable strongly convex objective func- tions with bounded Hessians, the algorithm converges in the mean square sense to the global minimizer with a guaranteed distance, which can be made arbitrarily small by an appropriate choice of the step-size. Recently, a few algorithms have been developed for solving the distributed optimization problem over directed random networks resulting from packet-dropping communication links. the authors of Carli, Notarstefano, Schenato, and Varagnolo (2015) developed a distributed algorithm based on the Newton–Raphson consen- sus algorithm ( Varagnolo et al., 2016 ), the push-sum method ( Bénézit et al., 2010; Charalambous et al., 2015; Hadjicostis & Char- alambous, 2014; Kempe et al., 2003 ), and the robustiﬁed strat- egy ( Dominguez-Garcia, Hadjicostis, & Vaidya, 2012; Hadjicostis, Vaidya, & Dominguez-Garcia, 2016 ). It is shown that the algo- rithm converges to the global minimizer almost surely if the Eu- ler discretization step-size is suﬃciently small. Note that the above algorithm uses the second-order Hessian information. When the Hessian information is not available, the authors of Wu, Yang, Wu, Kalsi, and Johansson (2017) developed a distributed algo- rithm by integrating the distributed algorithm based on the push sum method ( Nedi ´c & Olshevsky, 2015; Yang, Lu et al., 2017 ) and the robustiﬁed strategy proposed in Dominguez-Garcia, Had- jicostis et al. (2012) ; Hadjicostis et al. (2016) . The almost sure convergence was established with diminishing step-sizes. Although the above algorithms are resilient to link failures, the drawback is that their convergence rates are rather small since ( Carli et al., 2015 ) requires the Euler discretization step-size to be suﬃciently small and ( Wu, Yang, Wu et al., 2017 ) uses diminishing step- sizes. In order to accelerate the convergence rate, the authors of Jakoveti ´c, Xavier, and Moura (2014a) modiﬁed the two dis- tributed Nesterov-like gradient methods proposed in their earlier work ( Jakoveti ´c et al., 2014b ) for ﬁxed networks to handle random graphs and established their convergence rates in terms of the ex- pected optimality gap in the cost function at an arbitrary node. The aforementioned studies assumed that the subgradient is perfectly measured. However, in certain cases, the subgradient may only be measured with noises. Recently, the effects of both random graphs and noisy stochastic subgradients were studied simultane- ously by focusing on undirected graphs (see, e.g., Jakoveti ´c, Bajovic, Sahu, & Kar, 2018; Lei, Chen, & Fang, 2018; Srivastava & Nedi ´c, 2011 ). In particular, the authors of Srivastava and Nedi ´c (2011) and Jakoveti ´c et al. (2018) developed distributed algorithms with two sets of diminishing step-sizes, one for the consensus part and the other one for the noisy subgradient part. The convergence results were established if the step-sizes for the subgradeint part decays Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 13 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] to zero at a faster rate than those for the consensus part. Later, the authors of Lei et al. (2018) considered the distributed constrained optimization problem with local constraint sets and developed a distributed primal-dual algorithm which only uses the same di- minishing step-sizes for both the consensus part and the subgra- dient part. The almost sure convergence was established by using the stochastic approximation theory. In summary, the existing distributed algorithms for random graphs either use the gradient information (see, e.g., Jakoveti ´c et al., 2014a; Lobel & Ozdaglar, 2011; Wu, Yang, Wu et al., 2017 ) and diminishing step-sizes or ﬁxed step-sizes and the Hessian in- formation (see, e.g., Carli et al., 2015; Matei & Baras, 2011 ). It is an interesting future research direction to develop accelerated gradient-based distributed algorithms with ﬁxed step-sizes for ran- dom networks. It is worthy to mention that some effort s have been devoted to this direction. the authors of Xu et al. (2018a) de- veloped a distributed algorithm based on the gradient tracking method and the dynamic average consensus technique for undi- rected random graphs. In contrast to most existing studies with di- minishing step-sizes, the algorithm uses a constant uncoordinated step-size and allows for asynchronous implementation. Moreover, the authors showed that the algorithm converges almost surely to the optimal point if the largest step-size is less than a certain critical value. We believe that this algorithm can be extended to directed random graphs by using the push-sum method ( Bénézit et al., 2010; Charalambous et al., 2015; Hadjicostis & Charalambous, 2014; Kempe et al., 2003; Nedi ´c & Olshevsky, 2015 ) and the push– pull method ( Du, Yao et al., 2018; Pu et al., 2018; Xin & Khan, 2018; Yang et al., 2013 ). Also existing studies that investigated the effects of both ran- dom graphs and noisy stochastic subgradients are restricted to dis- tributed algorithms for undirected graphs. It is also worthy to in- vestigate both of these effects simultaneously on the existing push- sum based and push–pull based distributed algorithms for directed graphs. 4.6. Event-triggered communication Most existing continuous-time distributed optimization algo- rithms require continuous information exchange among agents, which may be impractical in physical applications. Moreover, dis- tributed networks are usually resources constrained and commu- nication is energy-consuming. In order to avoid continuous com- munication and to reduce communication overheads, the idea of event-triggered communication and control has been proposed. The early works focused on a single system ( Aström & Bern- hardsson, 2002; Girard, 2015; Tabuada, 2007 ) and have been ex- tended to multi-agent systems ( Heemels, Johansson, & Tabuada, 2012; Wan & Lemmon, 2009; Wang & Lemmon, 2011; Zhang, Li, Sun, & He, 2019; Zhang, Sun, Liang, & Li, 2019; Zhong & Cassan- dras, 2010 ) and distributed consensus ( Dimarogonas, Frazzoli, & Johansson, 2012; Meng, Xie, Soh, Nowzari, & Pappas, 2015; Sey- both, Dimarogonas, & Johansson, 2013; Yi et al., 2019a ) Zhong and He (2019) . For more details, please refer to Ding, Han, Ge, and Zhang (2018) ; Nowzari, Cortés, and Pappas (2018) ; Nowzari, Gar- cia, and Cortés (2019) ; Yi (2017 , and references therein). Recently, the event-triggered communication strategies have been extended to distributed optimization. Several studies devel- oped event-triggered communication mechanisms for implement- ing distributed continuous-time algorithms over connected undi- rected graphs or strongly connected and weight-balanced directed graphs. The key challenge is to design the event-trigger commu- nication strategies, such that the event-triggered algorithm con- verges to the optimal solution and is free of Zeno behavior, an inﬁnite number of triggered events in a ﬁnite period of time ( Johansson, Egerstedt, Lygeros, & Sastry, 1999 ). Most studies focused on the ﬁrst-order multi-agent systems with single integrator agent dynamics. in Kia et al. (2015a) , the authors developed a distributed PI algorithm with an event- triggered communication strategy and established its exponen- tial convergence to a neighborhood of an optimal point. To achieve the exact convergence, the distributed ZGS algorithm pro- posed in Lu and Tang (2012) has been extended with a pe- riodical time-triggered communication mechanism in Chen and Ren (2016) and an event-triggered scheme in Liu and Chen (2016) . Inspired by the distributed dynamic event-triggered strategy pro- posed in Girard (2015) ; Yi (2017) , the authors of Du, Yi, George, Johansson, and Yang (2018) extended the distributed ZGS algo- rithm with a dynamic event-triggered communication strategy, which is more eﬃcient compared to the time-triggered strategy and the static event-triggered strategy. the authors of Du, Yi, Zhang, George, and Yang (2019) equipped distributed PI algo- rithms proposed in Gharesifard and Cortés (2014) ; Wang and Elia (2010) and Kia et al. (2015a) with static event-triggered com- munication schemes. For the second-order multi-agent systems with double inte- grator agent dynamics, the authors of Tran, Wang, Liu, Xiao, and Lei (2019) modiﬁed the distributed algorithm proposed in Zhang and Hong (2014) and equipped it with an event-triggered communication scheme. Note that the existing distributed opti- mization algorithms for second-order multi-agent systems are not fully distributed since the gain parameters of the algorithms de- pend on some global parameters, such as the eigenvalues of the graph Laplacian matrix. the authors of Yi et al. (2018) developed a fully distributed algorithm for second-order multi-agent systems where no global information is needed and extended it with a dy- namic event-triggered communication mechanism. For the high-order multi-agent systems, the authors of Wang, Wang, and Wang (2016) modiﬁed the distributed optimiza- tion algorithm proposed in Zhang and Hong (2015) and extended it with an event-triggered communication scheme. Recently, the design of event-triggered communication strate- gies has taken communication effects, such as time-varying topolo- gies, time delays, packet drops, and limited bandwidth, into con- sideration. in Liu, Chen, and Dai (2019) , the authors developed an event-triggered algorithms based on the ZGS algorithm pro- posed in Lu and Tang (2012) for undirected time-varying net- works. For ﬁrst-order multi-agent systems, the authors of Liu, Xie, and Quevedo (2018) considered both the event-triggered commu- nication scheme and the limited communication bandwidth, and developed a distributed event-triggered optimization algorithm with dynamic encoder and decoder pairs. The proposed algorithm is based on the algorithm proposed in Shi et al. (2013) which uses diminishing step-sizes and is applicable only to a spe- cial case where all local objective functions have a common minimizer. So far most existing studies focused on developing event- triggered communication schemes for information exchange among agents. Recently, both the event-triggered communica- tion strategy and the event-triggered gradient measurement strat- egy have been developed for distributed gradient based algo- rithms for ﬁrst-order multi-agent systems. in Tran, Wang, Liu, and Xiao (2018) , the authors developed identical periodical time- triggered strategies for both communication and gradient infor- mation. Based on the internal model principle, the authors of Deng, Wang, and Hong (2017) proposed a distributed algorithm with both the event-triggered communication strategy and the event-triggered gradient measurement strategy and established the convergence in the presence of the external disturbances. It is in- teresting to develop both the event-triggered communication strat- egy and the event-triggered gradient measurement strategy for high-order multi-agent systems. Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 14 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Most existing distributed event-triggered optimization algo- rithms are in the continuous-time setting. Recently, a limited number of studies developed distributed event-triggered optimiza- tion algorithms in the discrete-time setting. An event-triggered algorithm based on the discretization of the ZGS algorithm ( Lu & Tang, 2012 ) was proposed in Chen and Ren (2016) . The hy- brid event-time-driven optimization algorithm was developed in Hu, Guan, Chen, and Shen (2019) . In order to ensure the effec- tiveness of the proposed event-triggered communication strate- gies, it is desirable to show that the inter event-times are at least lower bounded by a non-trivial bound, which has been largely ig- nored in the literature even for distributed consensus. Most event- triggered schemes for discrete-time distributed consensus and op- timization algorithms only admitted a trivial lower bound of inter- event times, that is, the lower bound is one. We are only aware of one work ( Meng, Xie, & Soh, 2016 ) in the context of the network utility maximization, which ensures a non-trivial lower bound of two. In the future, it is worthy to develop event-triggered com- munication strategies for distributed discrete-time optimization al- gorithms with ﬁxed step-sizes, which admit a non-trivial lower bound of inter-event times. 4.7. Finite-time convergence All the distributed optimization algorithms discussed so far converge to an optimal solution either asymptotically or exponen- tially. However, in some time-critical applications, such as DER co- ordination to be considered in Section 5 , it is highly desirable to develop distributed optimization algorithms which solve the prob- lem in a ﬁnite-time. An added advantage of the ﬁnite-time con- vergence is that the overall computation and communication over- heads can be greatly reduced. In the literature, several distributed ﬁnite-time optimiza- tion algorithms have been developed in both discrete-time and continuous-time settings. 4.7.1. Discrete-time In the discrete-time setting, only a limited number of results are available ( Mai & Abed, 2018; Yao et al., 2018 ). Both studies used the ﬁnite-time computation technique for distributed con- sensus originally proposed in Sundaram and Hadjicostis (2007) ; Yuan, Stan, Shi, Barahona, and Goncalves (2013) and Yuan (2012) , which enables an arbitrarily chosen agent to compute the ﬁ- nal consensus value within a ﬁnite number of time steps, by using its local successive states. In particular, the authors of Yao et al. (2018) studied the case where the communication graph is undirected. The authors ﬁrst proposed a distributed discrete- time PI algorithm and established its linear convergence to the global minimizer for quadratic local objective functions. The pro- posed distributed PI algorithm was then equipped with the ﬁnite- time consensus technique to enable agents to compute the global minimizer in a ﬁnite number of time steps. Independently, for quadratic local objective functions, a dif- ferent distributed algorithm based on the ﬁnite-time consen- sus technique ( Sundaram & Hadjicostis, 2007; Yuan, 2012; Yuan et al., 2013 ) and the ratio consensus algorithm proposed in Charalambous et al. (2015) ; Domínguez-García and Hadji- costis (2011) ; Hadjicostis and Charalambous (2014) was developed for directed graphs in Mai and Abed (2018) by exchanging the parameters of the objective functions. For general local objective functions, the ﬁnite-time consensus technique was used in Mai and Abed (2018) to periodically reset the distributed subgradient algo- rithm with a ﬁxed step-size. It is shown that all agents reach con- sensus on an identical estimate of an optimal solution in a ﬁnite- time if the global objective function is strongly convex. The algo- rithm then behaves like the centralized subgradient method, i.e., all local estimates continue to agree and approach the global min- imizer together. 4.7.2. Continuous-time In the continuous-time setting, several distributed ﬁnite-time optimization algorithms have also been developed (see, e.g., Chen & Li, 2018; Feng & Hu, 2017; Lin et al., 2017; Pilloni, Pisano, Franceschelli, & Usai, 2016; Song & Chen, 2016 ). These algo- rithms are motivated by the discontinuous ﬁnite-time consen- sus protocols. In order to establish the convergence, the ﬁnite- time Lyapunov analysis ( Bhat & Bernstein, 20 0 0 ) is commonly used. the authors of Lin et al. (2017) developed a distributed ﬁnite-time algorithm based on a distributed tracking algorithm and a dynamic averaging estimator for the distributed con- strained optimization problem where the local constraint sets are identical. For quadratic local objective functions, the authors of Pilloni et al. (2016) proposed a discontinuous signum-function based algorithm by modifying the distributed PI algorithms orig- inally developed in Gharesifard and Cortés (2014) ; Wang and Elia (2010) and Kia et al. (2015a) . Motivated by the ﬁnite-time consensus protocol ( Yu & Long, 2015 ), the authors of Feng and Hu (2017) developed a distributed ﬁnite-time optimization algo- rithm to solve the distributed optimization problem with com- munication and computation uncertainties. Inspired by the ﬁnite- time consensus protocol ( Wang & Xiao, 2010 ) and the distributed ZGS algorithm ( Lu & Tang, 2012 ), the authors of Song and Chen (2016) developed a distributed ﬁnite-time ZGS algorithm. The settling time in the existing continuous-time ﬁnite-time optimization algorithms depends on the initial conditions, which may be hard to preassign off-line. Recently, to overcome this lim- itation, the authors of Li, Yu, Zhou, and Ren (2017) and Chen and Li (2018) developed distributed algorithms which converge to the optimal solution within a ﬁxed time independent of the initial condition, by using the ﬁxed-time stability theory ( Polyakov, 2018; Polyakov, Eﬁmov, & Perruquetti, 2015a; 2015b ). In summary, distributed ﬁnite-time optimization algorithms in both discrete-time and continuous-time settings have been de- veloped by using different techniques. In the discrete-time set- ting, the existing algorithms are based on the decentralized ﬁnite- time computation mechanism. In the continuous-time setting, the proposed algorithms are based on discontinuous ﬁnite-time con- sensus protocols. Note that the existing distributed ﬁnite-time al- gorithms in the continuous-time setting are only applicable to undirected networks. However, directed communication networks are more realistic due to nonuniform communication powers. Thus, it is worthy to develop distributed ﬁnite-time continuous- time optimization algorithms for directed networks in the future. 5. Application to coordination of distributed energy resources In this section, we focus on the application of distributed opti- mization algorithms to solve the optimal coordination problem of distributed energy resources (DERs), which has received substan- tial attention in both control and power system ﬁelds in recent years (see, e.g., Bidram et al., 2014; Hadjicostis et al., 2018; Kran- ing et al., 2014; Molzahn et al., 2017; Nedi ´c & Liu, 2018; Qin et al., 2017 ). In particular, Section 5.1 presents the problem formulation of DER coordination. In Section 5.2 , we review existing works on distributed DER coordination for undirected graphs. The extension to directed graphs is discussed in Section 5.3 . The communication constraints, such as time delays and packet drops, on the exist- ing DER coordination algorithms are discussed in Section 5.4 . Dis- tributed DER coordination algorithms with event-triggered com- munication mechanisms and the ﬁnite-time convergence are re- viewed in Section 5.5 and Section 5.6 , respectively. Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 15 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] 5.1. Problem formulation of DER coordination The objective of the optimal DER coordination problem is to minimize the total production cost while meeting the total de- mand and simultaneously satisfying the individual generator out- put limits. Mathematically, it is formulated as the following opti- mization problem: min x i ∈ X i N ∑ i =1 C i (x i ) (19a) s . t . N ∑ i =1 x i = D, (19b) x i ∈ X i := [ x min i , x max i ] , i = 1 , 2 , . . . , N, (19c) where N is the number of distributed generators (DGs), x i is the power generation of DG i, D is the total demand, and C i (·) : R + → R + is the cost function of DG i , R + is the set of positive real num- bers. Constraint (19b) is the power balance constraint, and con- straints (19c) are generators’ capacity constraints, where x min i and x max i are respectively the lower and upper bounds for DG i . The cost function C i ( x i ) is assumed to be strictly convex and twice con- tinuously differentiable over X i . We note that there are various power system applications where distributed control and optimization play an important role, such as distributed algorithms for optimal power ﬂow (OPF), distributed optimal frequency control, distributed optimal volt- age control, and optimal wide-area control. We refer the read- ers to the recent survey Molzahn et al. (2017) , which provides a comprehensive review for these problems, Lam, Zhang, and Tse (2012) ; Low (2014) and Madani, Sojoudi, and Lavaei (2015) for the semideﬁnite program (SDP) relaxation of OPF, Zheng, Wu, Zhang, Sun, and Liu (2016) and Peng and Low (2018) for the sec- ond order cone program (SOCP) relaxation of OPF, and Kim and Baldick (1997) ; Sun, Phan, and Ghosh (2013) and Guo, Hug, and Tonguz (2017) for the non-convex OPF. The problem formulation in (19) though simple, is very repre- sentative. For example, as pointed out in Nedi ´c and Liu (2018) , a similar problem formulation has been applied to other power sys- tem applications, including economic dispatch for microgrids ( Hug, Kar, & Wu, 2015; Tang et al., 2018; Zhang & Chow, 2012 ), opti- mal load control ( Mallada, Zhao, & Low, 2017; Zhao, Topcu, & Low, 2013 ), optimal load sharing ( Yi, Hong, & Liu, 2015 ), and distributed energy management ( Du, Yao et al., 2018; He, Yu, Huang, & Li, 2019; Zhang, Xu, Liu, Zang, & Yu, 2015; Zhao, He, Cheng et al., 2017 ). Due to the wide representation of the problem formulation in (19), a few survey papers on distributed consensus and opti- mization also brieﬂy discussed the DER coordination problem, see, e.g., Qin et al. (2017) and Nedi ´c and Liu (2018) . The purpose of this section is to review distributed DER coordination algorithms in de- tails. One approach to solving the optimal DER coordination problem is through a completely centralized control strategy, which requires a single control center that accesses the entire network’s informa- tion, computes the optimal generations, and sends the informa- tion back to generators. This centralized approach may be subject to performance limitations, such as a single point of failure, high communication requirement, substantial computation burden, and limited ﬂexibility and scalability. To overcome these limitations, by using the results developed in the ﬁeld of distributed optimiza- tion, various distributed algorithms have been proposed recently to solve the optimal DER coordination problem. Before providing a detailed review for these distributed algo- rithms, we provide a brief discussion on how to solve the optimal DER coordination problem in (19) by distributed optimization al- gorithms developed for the problem of the form (1) . Please refer to Yang, Lu et al. (2017) and Nedi ´c and Liu (2018) for details. Since i) each cost function C i ( · ) is convex, ii) constraint (19b) is aﬃne, and iii) the set X 1 × \u0002\u0002\u0002 × X N is a polyhedral set, if we dualize problem (19) with respect to constraint (19b) , there is zero duality gap. Moreover, the dual optimal set is nonempty ( Bertsekas, Nedi ´c, & Ozdaglar, 2003 ). Consequently, solutions of the optimal DER co- ordination problem can be obtained by solving its equivalent dual problem. For convenience, let x = [ x 1 , . . . , x N ] ⊤ ∈ R N + . Then, deﬁne the La- grangian function L (x , λ) = N ∑ i =1 C i (x i ) − λ ( N ∑ i =1 x i − D ) , (20) where λ is the dual variable (incremental cost). The corresponding Lagrangian dual problem can be written as max λ∈ R N ∑ i =1 \u000bi (λ) , (21) where \u000bi (λ) = min x i ∈ X i C i (x i ) − λ(x i − D i ) , (22) and D i is a virtual local demand at each bus such that ∑ N i =1 D i = D . Therefore, the optimal DER coordination problem (19), which is a constrained optimization problem, can be solved by solving its equivalent dual problem (21) , which has the same form as the distributed unconstrained optimization problem (1) . The only dif- ference is that the dual problem (21) is a maximization problem for concave functions while problem (1) is a minimization problem for convex functions. Therefore, distributed algorithms reviewed in Sections 3 and 4 for the distributed optimization problem (1) can be readily applied. We are now ready to review existing distributed algorithms for solving the optimal DER coordination problem. 5.2. Undirected graphs The earlier studies focused on the case where the communi- cation network is modeled as an undirected graph and the gen- eration cost functions are quadratic. the authors of Zhang and Chow (2012) proposed a leader–follower consensus based algo- rithm where the leader collects the mismatch between the de- mand and the total generation, and then leads the updates of the incremental cost. Through various case studies, it is shown that this distributed algorithm converges to the optimal generations for connected undirected graphs if the feedback gain parameter for the mismatch term is less than a certain critical value. An alternative distributed algorithm with two sets of dimin- ishing step-sizes based on the consensus and innovation approach ( Kar & Moura, 2013 ) was developed in Kar and Hug (2012) , where the consensus part guarantees that consensus is achieved, and the innovation term ensures the balance between the total generation and the total demand. The convergence result was established if the consensus term dominates the innovation term, i.e., if the di- minishing step-size for the consensus term decays to zero at a faster rate compared to that of the innovation term. Motivated by the EXTRA ( Shi et al., 2015a ) the authors of Tang et al. (2018) de- veloped a distributed DER coordination algorithm with a ﬁxed step-size which leads to a fast convergence speed. In summary, all the aforementioned distributed algorithms are only applicable to undirected communication networks. In the lit- erature, various distributed algorithms for DER coordination have Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 16 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] been developed by taking more realistic scenarios into considera- tion and will be reviewed in the following subsections. 5.3. Directed graphs In this subsection, we review the existing discrete-time and continuous-time distributed algorithms for solving the optimal DER coordination problem over directed graphs which may be time-varying. 5.3.1. Discrete-time In the discrete-time setting, the earlier studies focused on the case where the digraph is ﬁxed and the cost functions are quadratic. The authors of Dominguez-Garcia, Cady et al. (2012) de- veloped a distributed algorithm based on ratio consensus ( Bénézit et al., 2010; Domínguez-García & Hadjicostis, 2011; Kempe et al., 2003 ). It is shown that the ratio consensus-based algorithm asymptotically converges to the optimal generations for strongly connected digraphs. A distributed algorithm with a ﬁxed learning gain (step-size) based on the surplus idea ( Cai & Ishii, 2012 ) was proposed in Yang et al. (2013) . The algorithm asymptotically con- verges to the optimal generations for strongly connected digraphs if the step-size is suﬃciently small. Recently, several distributed DER coordination algorithms have been developed for general non-quadratic convex cost functions. In particular, the authors of Xing et al. (2015) proposed a dis- tributed bisection method based on a consensus-like iteration. The asymptotic convergence was established for the case where the cost functions are strictly convex and twice continuously differen- tiable and the directed graph is strongly connected. The authors ( Yang, Lu et al., 2017 ) developed a distributed algorithm based on the push-sum method ( Nedi ´c & Olshevsky, 2015 ) for time-varying directed graphs. It is shown that the algorithm converges to the optimal generations if the time-varying directed graph is uniformly jointly strongly connected and the step-sizes are diminishing. A nonnegative surplus-based distributed algorithm was developed in Xu et al. (2017) . The asymptotic convergence was established if the time-varying directed graph is uniformly jointly strongly con- nected and the time-varying parameters are chosen appropriately. Recently, focusing on undirected graphs, the strictly convexity as- sumption on the cost functions has recently been relaxed to only convexity in Doan and Beck (2017, 2018) . In particular, the au- thors of Doan and Beck (2017) developed a distributed Lagrangian algorithm with a diminishing step-size for connected undirected graphs and established the asymptotic convergence, while the au- thors of Doan and Beck (2018) focused on time-varying loads and uniformly jointly connected time-varying undirected graphs and established the almost sure convergence. The aforementioned DER coordination algorithms for time- varying directed graphs require diminishing step-sizes, which results in rather slow convergence. Recently, the authors of Du, Yao et al. (2018) developed a distributed push–pull based al- gorithm with a ﬁxed step-size for solving the DER coordination problem over both ﬁxed strongly connected directed graphs and time-varying uniformly jointly strongly connected directed graphs. The proposed distributed push–pull based algorithm with a ﬁxed step-size can be viewed as an extension of the distributed algo- rithm proposed in Yang et al. (2013) for ﬁxed graphs and quadratic cost functions to time-varying graphs and general non-quadratic strictly convex cost functions. 5.3.2. Continuous-time In the continuous-time setting, most studies focused on ﬁxed graphs. the authors of Cherukuri and Cortés (2015) developed a distributed algorithm based on the Laplacian nonsmooth gradi- ent dynamics for solving the DER coordination problem over ﬁxed directed graphs. It is shown that the algorithm asymptotically converges to the optimal generations for strongly connected and weight-balanced directed graphs. In order to ensure the convergence of the above algorithm, the initial conditions need to satisfy the load constraints. Such an initialization requirement was later removed in Cherukuri and Cortés (2016) , where the authors developed a distributed algorithm based on the Laplacian nonsmooth gradient dynamics and dynamic average consensus ( Freeman, Yang, & Lynch, 2006; Kia, Cortés, & Martínez, 2015b ). In particular, each generator estimates the av- erage mismatch between the demand and the total generation by employing the dynamic average consensus algorithm, which was then fedback to the Laplacian nonsmooth gradient dynamics. It is shown that the algorithm asymptotically converges to the optimal generations for any initial condition. The proposed distributed algorithms in Cherukuri and Cortés (2015, 2016) require each agent to share its own gradient infor- mation with its neighboring agents, which may be private infor- mation that cannot be shared. To overcome this issue, the au- thors of Yi et al. (2016) developed two distributed initialization- free algorithms based on either projection or differentiated projec- tion to solve the DER coordination problem over connected undi- rected graphs, which can be easily extended to strongly connected and weight-balanced directed graphs. Moreover, the proposed al- gorithms are also capable of handling more general local convex constraints other than box constraints. By using an augmented Lagrangian function with the generation capacity constraints, the authors of Bai, Ye, Sun, and Hu (2019) de- veloped an alternative initialization-free distributed DER coordi- nation algorithm. The proposed algorithm is based on the saddle point dynamics, dynamic average consensus and leader–follower consensus, and is applicable to solve the DER coordination prob- lem with transmission line constraints. 5.4. Communication imperfections Since communication imperfections, such as time delays and packet drops, are ubiquitous in communication networks ( Aström & Kumar, 2014; Cao et al., 2013; Hespanha et al., 2007 ), it is de- sirable to investigate the potential effects of these communication imperfections on the existing distributed DER coordination algo- rithms, and to develop distributed algorithms that are robust to these imperfections. First, we consider the effects of communication time delays on both discrete-time and continuous-time distributed DER coordina- tion algorithms. In the discrete-time setting, the authors of Zhang, Chow, and Chakrabortty (2012) and Yang et al. (2015) respectively investi- gated the performance of the algorithms proposed in Zhang, Ying, and Chow (2011) and Yang et al. (2013) in the presence of uni- form constant time delays via numerical simulations. Both stud- ies found that there exists a critical value for delays, below which these DER coordination algorithms with given gain parameters still converge, and above which these algorithms fail to converge. It is shown in Zhao, Duan et al. (2019) that the algorithm proposed in Yang et al. (2013) still converges to the optimal generations even in the presence of nonuniform constant time delays provided that the learning gain parameter is suﬃciently small. The explicit up- per bound on the learning gain parameter to ensure the conver- gence was established for uniform constant delays. Note that the above studies considered constant time delays. In the case of time- varying delays, the authors of Yang, Lu et al. (2017) developed a distributed push-sum based algorithm with a diminishing step-size ( Nedi ´c & Olshevsky, 2015 ) and showed that the proposed algorithm converges to the optimal generations for time-varying uniformly Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 17 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] jointly strongly connected digraphs, even in the presence of arbi- trarily large bounded time-varying delays. In the continuous-time setting, the authors of Zhu, Yu, and Wen (2016) developed a distributed DER coordination algorithm and studied the effects of uniform constant time delays. It is shown that the proposed algorithm still converges to the optimal genera- tions if uniform constant delays are less than some threshold. The authors of Chen and Zhao (2018) developed a distributed DER co- ordination algorithm and investigated its performance in the pres- ence of nonuniform constant time delays. The maximum allow- able delay bound was obtained by the Generalized Nyquist Crite- rion. The above studies focused on the case where the communi- cation graph is ﬁxed. In the case of time-varying graphs, the au- thors of Somarakis and Baras (2015) and Somarakis, Maity, and Baras (2016) developed a distributed DER coordination algorithm and investigated its performance in the presence of time-varying delays. Suﬃcient conditions under which the proposed algorithm still solves the DER coordination problem were established. Next, we consider another common communication imperfec- tion in the communication networks –packet drops. Although time-varying communication networks may be used to model packet drops, a more realistic modeling approach is based on the probability framework, i.e., the communication link fails with a certain probability. In such a probability setting, most existing DER coordination algorithms are not able to handle packet drops. the authors of Wu, Yang, Wu et al. (2017) devel- oped a robustiﬁed extension of the distributed algorithm pro- posed in Yang, Lu et al. (2017) by using the robustiﬁed strat- egy proposed in Dominguez-Garcia, Hadjicostis et al. (2012) and Hadjicostis et al. (2016) . Under the assumption that the underly- ing communication network is strongly connected with a positive probability and the packet drops are i.i.d., it is shown that the ro- bustiﬁed distributed algorithm solves the DER coordination prob- lem almost surely even in the presence of packet drops. 5.5. Event-triggered communication Most distributed algorithms for solving the optimal DER coor- dination problem discussed so far require the continuous informa- tion exchange among DERs. To avoid such continuous communi- cation and thus to reduce the communication burden, distributed event-triggered DER coordination algorithms have been developed. The authors of Li, Yu, Yu, Huang, and Liu (2016) ﬁrst de- veloped a distributed DER coordination algorithm based on θ - logarithmic barrier-based method and then equipped it with an event-triggered communication scheme. It is shown that the event-triggered DER coordination algorithm converges to the op- timal generations if the undirected graph is connected. An event- triggered algorithm was proposed in Ding, Wang, Yin, Zheng, and Han (2019) to solve the DER coordination problem including both distributed generators and demand response. Recently, the authors of Zhao, Li, and Ding (2019) developed an event-triggered algo- rithm to solve the DER coordination problem with transmission losses. It is shown that the event-triggered DER coordination algo- rithm is free of Zeno behavior and converges to the optimal gen- erations for undirected connected graphs. For the case where the communication network is modeled as a directed graph, the au- thors of Shi, Wang, Song, and Yan (2018) developed a distributed algorithm with a diminishing step-size and an event-triggered scheme to solve the DER coordination problem over strongly con- nected directed graphs. 5.6. Finite-time convergence All the distributed algorithms discussed so far only solve the DER coordination problem asymptotically. However, power and en- ergy systems require time-critical and fast response when new en- ergy needs are demanded ( Yu & Xue, 2016 ). Thus, it is highly de- sirable to develop distributed algorithms which solve the DER co- ordination problem in a ﬁnite-time. In the literature, a few dis- tributed ﬁnite-time algorithms have been developed in both the continuous-time setting and the discrete-time setting. Most exist- ing studies focused on quadratic generator cost functions. In the discrete-time setting, motivated by the decentralized ﬁnite-time computation technique proposed in Sundaram and Had- jicostis (2007) and Yuan et al. (2013) , the authors of Yang, Wu, Sun et al. (2016) developed a decentralized algorithm which en- ables each distributed generator to compute its optimal generation in a minimum number of time steps, by using its local successive states obtained from the underlying DER coordination algorithm proposed in Yang et al. (2013) for the case where the generation cost functions are quadratic and the directed graph is strongly con- nected. In the continuous-time setting, based on distributed ﬁnite- time consensus protocols, several distributed DER coordina- tion algorithms with ﬁnite-time convergence have been pro- posed. Motivated by discontinuous consensus protocol proposed in Chen, Lewis, and Xie (2011) , the authors of Chen, Ren, and Feng (2017) developed a distributed DER coordination algorithm and established its ﬁnite-time convergence to the optimal gener- ations for connected undirected graphs. Note that uncertain infor- mation commonly exists in the communication network and the computation process. the authors of Feng and Hu (2017) developed a distributed ﬁnite-time DER coordination algorithm and estab- lished its ﬁnite-time convergence for connected undirected graphs and general convex cost functions, even in the presence of com- munication and computation uncertainties. Although these continuous-time DER coordination algorithms converge to the optimal generations in a ﬁnite-time, the set- tling time depends on the initial condition, which may be diﬃ- cult to preassign off-line. To overcome this limitation, the authors of Li et al. (2017) and Chen and Li (2018) have recently devel- oped distributed algorithms for solving the DER coordination prob- lem over connected undirected graphs, without generation capacity constraints and with generation capacity constraints, respectively. It is shown that these algorithms converge to the optimal genera- tions within a ﬁxed time independent of the initial conditions. 6. Summary In this paper, we have provided a comprehensive survey of ex- isting discrete-time and continuous-time distributed optimization algorithms. Moreover, we have discussed how these distributed al- gorithms are applied/or adapted to solve the optimal DER coordi- nation problem in power systems. 6.1. Current state Tables 1–3 show comprehensive lists of existing distributed al- gorithms reviewed in Sections 3 and 4 . In particular, Table 1 and Table 2 compare the existing discrete-time algorithms with dimin- ishing step-sizes and ﬁxed step-sizes, respectively, while the exist- ing continuous-time algorithms are summarized in Table 3 . Table 4 provides a list of existing distributed algorithms for solving the op- timal DER coordination problem. 6.2. Future research directions Although we have pointed out some future research directions for distributed optimization in each subsections of Section 4 , there are still other important and yet challenging future research direc- Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 18 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Table 1 Discrete-time distributed algorithms with diminishing step-sizes. Study Approach Constraints type Cost function Communication graph Convergence ( Nedi ´c & Ozdaglar, 2009 ) distributed (sub)graident method unconstrained convex but nonsmooth uniformly jointly connected O ( ln k/ √ k ) ( Duchi et al., 2012 ) distributed dual averaging common convex constraint set convex but nonsmooth connected undirected O ( ln k/ √ k ) ( Jakoveti ´c et al., 2014b ) distributed augmented Lagrangian with multiple consensus step in the inner loop unconstrained convex and smooth with bounded gradient connected undirected O (1/ k 2 ) ( Nedi ´c et al., 2010 ) distributed projected (sub)gradient method common convex constraint set convex but nonsmooth uniformly jointly connected asymptotic different convex constraint sets fully connected undirected ( Tsianos & Rabbat, 2011 ) dual averaging method ( Duchi et al., 2012 ) with delays common convex constraint set convex but nonsmooth connected undirected graphs with ﬁxed and random bounded delays O (1 / √ k ) ( Tsianos et al., 2012 ) a combination of dual averaging and the push-sum method common convex constraint set convex but nonsmooth strongly connected digraphs with ﬁxed and random bounded delays O (1 / √ k ) ( Nedi ´c & Olshevsky, 2015 ) a combination of the distributed subgradient method and the push-sum method unconstrained convex but nonsmooth uniformly jointly strongly connected O ( ln k/ √ k ) ( Zhu & Martínez, 2012 ) primal-dual projected subgradient method different local constraint sets and global inequality constraint convex but nonsmooth weight-balanced and uniformly jointly strongly connected asymptotic identical local constraint sets and global equality constraint ( Lin et al., 2016 ) projected subgradient method different local constraint sets convex but nonsmooth weight-balanced and uniformly jointly strongly connected digraphs without/with arbitrarily bounded delays asymptotic ( Wang, Lin et al., 2018 ) projected subgradient method with step-sizes which are not square summable unconstrained convex but nonsmooth weight-balanced and uniformly jointly strongly connected O (1 / √ k ) different convex constraint sets ( Lobel & Ozdaglar, 2011 ) distributed subgradient method unconstrained convex but nonsmooth undirected graphs with random link failures almost sure ( Srivastava & Nedi ´c, 2011 ) distributed stochastic gradient method different local constraint sets convex but nonsmooth undirected random graphs almost sure ( Jakoveti ´c et al., 2018 ) distributed stochastic subgradient method unconstrained strongly convex and bounded Hessians undirected random graphs with noisy communications O (1/ k ) in mean square ( Lei et al., 2018 ) stochastic approximation-based distributed primal-dual algorithm different local constraint sets convex and smooth undirected random graphs with noisy communications almost sure Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 19 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Table 2 Discrete-time distributed algorithms with ﬁxed step-sizes. Study Approach Step-size Constraints type Cost function Communication graph Convergence ( Shi et al., 2015a ) distributed PI identical unconstrained convex and smooth connected undirected O (1/ k ) restricted strongly convex and smooth linear ( Qu & Li, 2018 ) a combination of the distributed inexact method and the gradient tracking identical unconstrained convex and smooth connected undirected O (1/ k ) strongly convex and smooth linear ( Jakoveti ´c, 2019 ) a uniﬁed framework for ( Qu & Li, 2018; Shi et al., 2015a ) identical unconstrained strongly convex and smooth connected undirected linear ( Yao et al., 2018 ) distributed PI and ﬁnite-time consensus ( Sundaram & Hadjicostis, 2007; Yuan et al., 2013 ) identical unconstrained quadratic connected undirected ﬁnite-time computation ( Mai & Abed, 2018 ) diffusion the coeﬃcients of objective functions and ﬁnite-time consensus computation ( Sundaram & Hadjicostis, 2007; Yuan et al., 2013 ) identical unconstrained quadratic strongly connected ﬁnite-time computation ( Zeng & Yin, 2017 ) identical unconstrained quasi-strong convex and smooth strongly connected linear ( Xi & Khan, 2017 ) EXTRA and push-sum based method restricted strongly convex and smooth ( Xi, Xin et al., 2018 ) strongly convex and smooth ( Pu et al., 2018; Xin & Khan, 2018 ) push–pull based method identical unconstrained strongly convex and smooth strongly connected linear ( Nedi ´c et al., 2017a ) DIGing identical unconstrained strong convex and smooth uniformly jointly connected linear DIGing and push-sum method uniformly jointly strongly connected ( Xu et al., 2018b ) Bregman splitting method identical unconstrained strongly convex and smooth undirected stochastic O (1/ k ) ( Aybat et al., 2018 ) distributed proximal method adaptive unconstrained composite convex connected undirected O (1/ k ) ( Lei et al., 2016 ) a primal-dual algorithm with the projection method identical different constraint sets convex and locally smooth connected undirected asymptotic ( Yuan et al., 2011 ) a primal-dual subgradient algorithm with a ﬁxed step-size identical global inequality constraint convex but nonsmooth connected undirected –( Yuan et al., 2016 ) O (k −1 / 4 ) ( Liu et al., 2017 ) a projection algorithm with a ﬁxed step-size identical global equality constraint convex but nonsmooth connected undirected asymptotic ( Xu et al., 2015 ) augmented distributed gradient method uncoordinated unconstrained convex and smooth connected undirected asymptotic ( Xu et al., 2018a ) asynchronous distributed gradient method uncoordinated unconstrained convex and smooth undirected stochastic graphs with random failures almost sure strongly convex and smooth linear ( Nedi ´c et al., 2017b ) DIGing-ATC uncoordinated unconstrained strongly convex and smooth connected undirected linear ( Saadatniaki et al., 2018 ) push–pull based method uncoordinated unconstrained strongly convex and smooth uniformly jointly strongly connected linear Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 20 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Table 3 Continuous-time distributed algorithms. Study Approach Arbitrary initialization Constraints type Cost function Communication graph Triggering Convergence ( Wang & Elia, 2010 ) distributed PI yes unconstrained convex but nonsmooth connected undirected –asymptotic ( Gharesifard & Cortés, 2014 ) distributed PI yes unconstrained convex but nonsmooth connected undirected –asymptotic convex and smooth strongly connected and weight-balanced ( Kia et al., 2015a ) distributed PI partial unconstrained convex local functions and a strictly convex global function connected undirected –asymptotic strongly convex and smooth –exponential strongly convex and smooth static event-triggered exponential to a neigh- borhood strongly convex and smooth strongly connected and weight-balanced –exponential strongly convex and smooth strongly connected and weight-balanced at all times –exponential ( Du et al., 2019 ) distributed PI ( Gharesifard & Cortés, 2014; Wang & Elia, 2010 ) yes unconstrained convex and continuously differentiable connected undirected asymptotic restricted strongly convex and locally smooth static event-triggered exponential distributed PI ( Kia et al., 2015a ) partial convex and continuously differentiable asymptotic restricted strongly convex and locally smooth exponential ( Lu & Tang, 2012 ) zero gradient sum method no unconstrained strongly convex with locally Lipschitz Hessian connected and unidrected –exponential ( Liu & Wang, 2015 ) two-layer projection neural network yes different local constraint sets convex but nonsmooth connected and unidrected –asymptotic ( Qiu, Liu et al., 2016 ) consensus, subgraient, and projection yes identical local constraint set convex uniformly jointly connected –asymptotic ( Yang et al., 2017b ) distributed PI yes different local constraint sets and global equality and inequality constraints convex on locally bounded feasible region connected and unidrected –asymptotic ( Zhu, Yu et al., 2018 ) distributed subgradient method yes different local constraint sets and global equality and inequality constraints convex and nonsmooth local functions and a strictly convex global functions connected and unidrected –asymptotic ( Yang et al., 2017a ) ( Kia et al., 2015a ) in the presence of delays partial unconstrained strongly convex and smooth strongly connected and weight-balanced digraphs with time-varying delays –asymptotic ( Hatanaka et al., 2018 ) distributed PI and passivity yes unconstrained strongly convex and smooth connected undirected graphs with constant delays –asymptotic ( continued on next page ) Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 21 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Table 3 ( continued ) Study Approach Arbitrary initialization Constraints type Cost function Communication graph Triggering Convergence ( Guo & Chen, 2018 ) ( Lu & Tang, 2012 ) under delays no unconstrained strongly convex with locally Lipschitz Hessians undirected connected graphs/strongly connected and weight-balanced digraphs at all times with time-varying delays –asymptotic ( Doan et al., 2017 ) distributed gradient method under delays yes different local constraint sets convex and differentiable undirected connected graphs with uniform constant delays –O ( ln k/ √ k ) ( Chen & Ren, 2016 ) ( Lu & Tang, 2012 ) with ETM no unconstrained strongly connected and periodic triggered exponential ( Liu & Chen, 2016 ) strongly convex with locally Lipschitz Hessians weight-balanced static event-triggered ( Du, Yi et al., 2018 ) connected undirected dynamic event-triggered ( Lin et al., 2017 ) a distributed algorithm with nonuniform state-dependent gradient gains no identical local constraint sets convex and differentiable connected undirected at all times –asymptotic a combination of a distributed tracking algorithm and a dynamic averaging estimator no convex and twice differentiable –ﬁnite-time ( Pilloni et al., 2016 ) distributed PI-like no unconstrained quadratic connected undirected –ﬁnite-time ( Feng & Hu, 2017 ) ﬁnite-time consensus protocol in Yu and Long (2015) yes unconstrained quadratic connected undirected graphs with communication uncertainties –ﬁnite-time Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 22 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Table 4 Distributed algorithms for the optimal DER coordination problem. Study Algorithm Type Approach Cost function Communication graph Communication imperfection Triggering Convergence ( Zhang & Chow, 2012 ) discrete-time ﬁxed step-size leader–follower consensus-based quadratic connected undirected ––asymptotic ( Kar & Hug, 2012 ) discrete-time diminishing step-size consensus and innovation approach ( Kar & Moura, 2013 ) quadratic connected undirected ––asymptotic ( Tang et al., 2018 ) discrete-time ﬁxed step-size EXTRA ( Shi et al., 2015a ) quadratic connected undirected ––asymptotic ( Dominguez-Garcia, Cady et al., 2012 ) discrete-time ﬁxed step-size ratio consensus ( Domínguez-García & Hadjicostis, 2011 ) quadratic strongly connected ––asymptotic ( Yang et al., 2013 ) discrete-time ﬁxed step-size surplus ( Cai & Ishii, 2012 ) quadratic strongly connected ––asymptotic ( Xing et al., 2015 ) discrete-time ﬁxed step-size bisection method and consensus-like strictly convex strongly connected ––asymptotic ( Yang, Lu et al., 2017 ) discrete-time diminishing step-size push-sum based algorithm ( Nedi ´c & Olshevsky, 2015 ) strictly convex uniformly jointly strongly connected bounded time-varying delays –asymptotic ( Du, Yao et al., 2018 ) discrete-time ﬁxed step-size push–pull method strictly convex strongly connected ––asymptotic uniformly jointly strongly connected ( Doan & Beck, 2017 ) discrete-time diminishing step-size distributed Lagrangian method convex but nonsmooth connected undirected ––asymptotic Doan and Beck (2018) uniformly jointly connected ( Cherukuri & Cortés, 2015; 2016 ) continuous-time Laplacian nonsmooth gradient dynamics convex, continuous, and locally Lipschitz strongly connected and weight-balanced ––asymptotic ( Yi et al., 2016 ) continuous-time projected dynamics strictly convex and smooth connected undirected ––asymptotic strongly convex exponential ( Bai et al., 2019 ) continuous-time saddle point dynamics quadratic connected undirected ––ﬁnite-timetoa neighborhood Zhao, Duan et al. (2019) discrete-time ﬁxed step-size the algorithm proposed in Yang et al. (2013) quadratic strongly connected uniform constant delays –asymptotic ( Zhu et al., 2016 ) continuous-time consensus-based quadratic connected undirected uniform constant delays –asymptotic ( Chen & Zhao, 2018 ) continuous-time consensus-based quadratic connected undirected nonuniform constant delays –asymptotic ( Somarakis & Baras, 2015; Somarakis et al., 2016 ) continuous-time consensus-based quadratic uniformly jointly connected time-varying bounded delays –asymptotic ( Wu, Yang, Wu et al., 2017 ) discrete-time diminishing step-size push sum method ( Nedi ´c & Olshevsky, 2015 ) and running sum method ( Dominguez-Garcia, Hadjicostis et al., 2012; Hadjicostis et al., 2016 ) quadratic strongly connected with a positive probability packet-drops –almost sure ( Li et al., 2016 ) discrete-time θ-logarithmic barrier-based quadratic connected undirected –yes asymptotic ( Zhao, Li et al., 2019 ) continuous-time saddle-point dynamics strongly convex connected undirected –yes asymptotic ( Yang, Wu, Sun et al., 2016 ) discrete-time ﬁnite-time consensus computation ( Sundaram & Hadjicostis, 2007; Yuan et al., 2013 ) quadratic strongly connected ––ﬁnite-time ( Chen et al., 2017 ) continuous-time distributed ﬁnite-time algorithm based on ( Chen et al., 2011 ) quadratic connected undirected ––asymptotic ( Chen & Li, 2018; Li et al., 2017 ) continuous-time sliding mode control quadratic connected undirected ––ﬁxed-time Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 23 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] tions which have received much attention recently. Some of them are brieﬂy summarized as follows: • Distributed non-convex optimization. Most existing studies fo- cused on the distributed convex optimization problem. How- ever, in many physical applications, the optimization problem is non-convex. Thus, it is important to develop distributed al- gorithms to solve the non-convex optimization problem. This challenging yet important problem has drawn attention re- cently from various communities, such as control, signal pro- cessing, and machine learning (see, e.g., Chatzipanagiotis & Za- vlanos, 2017; Hong, Hajinezhad, & Zhao, 2017; Lorenzo & Scu- tari, 2016; Matei & Baras, 2017; Tatarenko & Touri, 2017; Tian, Sun, Du, & Scutari, 2018; Wai, Lafond, Scaglione, & Moulines, 2017; Zeng & Yin, 2018; Zhu & Martínez, 2013 ). In these stud- ies, distributed algorithms have been developed to solve un- constrained and constrained non-convex optimization problems over either ﬁxed or time-varying graphs, either undirected or directed. However, this direction is far from being complete. For example, it is interesting to investigate the performance of these existing algorithms in case of noisy gradients. Another in- teresting direction is to develop event-trigger communication schemes for these existing algorithms to reduce the commu- nication overheads. It is also worthy to apply these existing distributed algorithms to the non-convex optimization problem with theoretical guarantees to solve the non-convex optimal power ﬂow problem. • Distributed resilient optimization. The common assumption in the existing distributed optimization literature is that all agents cooperate to learn the optimal soultion in a collaborative manner. However, in networked cyber-physical systems, some agents may become adversarial due to failures or malicious at- tacks. Therefore, it is important to investigate the performance of the existing distributed optimization algorithms in the pres- ence of adversarial agents. Although distributed resilient con- sensus has been well studied (see, e.g., LeBlanc, Zhang, Kout- soukos, & Sundaram, 2013; Pasqualetti, Bicchi, & Bullo, 2012; Sundaram & Hadjicostis, 2011 ), distributed resilient optimiza- tion with adversarial agents is less studied and there are only a limited number of studies (see, e.g., Su & Vaidya, 2016; Sun- daram & Gharesifard, 2019; Zhao, He, & Wang, 2017 ). These re- sults established suﬃcient and/or necessary conditions under which the proposed distributed algorithms ensure that the non- adversarial agents converge to the convex hull of the local min- imizers even in the presence of adversarial agents. However, the results focused on distributed algorithms with diminishing step-sizes. It is interesting to develop distributed resilient opti- mization algorithms with ﬁxed step-sizes. • Distributed online convex optimization. Another common as- sumption in the existing distributed optimization literature is that every agent knows its local private convex objective func- tion in advance. However, in many applications, there is no prior knowledge of the objective functions since the informa- tion is highly uncertain and unpredictable. For example, in a microgrid with a high penetration of DERs such as wind gen- erators and solar panels, there is high uncertainty of power generation. Thus, the uncertain and unpredictable features of DERs need to be taken into account to design a more ac- curate energy management system for microgrids ( Ma, Wang, Gupta, & Chen, 2018 ). This issue can be addressed within the framework of distributed online convex optimization (DOCO). Discrete-time DOCO with the time-invariant constraint set and inequality constraints has been studied in Koppel, Jakubiec, and Ribeiro (2015) ; Lee and Zavlanos (2016) ; Li, Yi, and Xie (2018) ; Tsianos and Rabbat (2012) and Yi, Yang, Wu, and Johans- son (2019b) and continuous-time DOCO without constraints has been considered in Zhang et al. (2017) . It is an important future research direction to consider DOCO with time-varying con- straint sets, which is more challenging and practical. • Coordination of multi-type of DERs. Most existing studies for DER coordination only considered coordination of distributed generators. It is desirable to consider other types of resources, such as energy storages and demand response. In fact, some re- sults are available in this direction. For example, the authors of Cherukuri and Cortés (2018) ; Hug et al. (2015) ; Wu, Yang, Stoor- vogel, and Stoustrup (2017) ; Yang, Wu, Stoorvogel, and Stous- trup (2016) and He et al. (2019) studied the coordination be- tween distributed generators and energy storages, while the au- thors of Li, Chen, and Low (2011) ; Wu, Lian, Sun, Yang, and Hansen (2017) and Qin, Wan, Yu, Li, and Li (2019) studied the coordination between distributed generators and demand re- sponse. Most of these existing studies assumed reliable com- munication networks. However, the communication network in the distribution network is still under-deployed and has lim- ited capabilities compared to that for the bulk power trans- mission network ( Magnússon, Fischione, & Li, 2017; Zhang, Shi, Zhu, Dall’Anese, & Ba ¸s ar, 2018 ). Such a communication net- work could suffer from communication imperfections, such as switching topologies, time delays, and packet drops. An inter- esting future research direction is to develop distributed algo- rithms to optimally coordinate various distributed energy re- sources in the presence of these communication imperfections. Conﬂict of interest None. References Arrow, K. , Huwicz, L. , & Uzawa, H. (1958). Studies in linear and non-linear program- ming . Stanford University Press . Aström, K. , & Kumar, P. R. (2014). Control: A perspective. Automatica, 50 (1), 3–43 . Aström, K. J. , & Bernhardsson, B. M. (2002). Comparison of Riemann and Lebesgue sampling for ﬁrst order stochastic systems. In Proceedings of the 41st IEEE con- ference on decision and control (pp. 2011–2016) . Aybat, N. , Wang, Z. , & Iyengar, G. (2015). An asynchronous distributed proximal gra- dient method for composite convex optimization. In Proceedings of the 32nd in- ternational conference on machine learning (ICML) (pp. 2454–2462) . Aybat, N. S. , & Hamedani, E. Y. (2016). A primal-dual method for conic constrained distributed optimization problems. In Proceedings of the 30th international con- ference on neural information processing systems (pp. 5056–5064) . Aybat, N. S. , Wang, Z. , Lin, T. , & Ma, S. (2018). Distributed linearized alternating direction method of multipliers for composite convex consensus optimization. IEEE Transactions on Automatic Control, 63 (1), 5–20 . Bai, L. , Ye, M. , Sun, C. , & Hu, G. (2019). Distributed economic dispatch control via saddle point dynamics and consensus algorithms. IEEE Transactions on Control Systems Technology, 27 (2), 898–905 . Bénézit, F. , Blondel, V. , Thiran, P. , Tsitsiklis, J. N. , & Vetterli, M. (2010). Weighted gos- sip: Distributed averaging using non-doubly stochastic matrices. In Proceedings of the IEEE international symposium on information theory (pp. 1753–1757) . Bertsekas, D. , & Tsitsiklis, J. (1989). Parallel and distributed computation: Numerical methods . Prentice-Hall . Bertsekas, D. P. , Nedi ´c, A. , & Ozdaglar, A. (2003). Convex analysis and optimization . Belmont, MA: Athena Scientiﬁc . Bhat, S. P. , & Bernstein, D. S. (20 0 0). Finite-time stability of continuous autonomous systems. SIAM Journal on Control and Optimization, 38 (3), 751–766 . Bidram, A. , Lewis, F. L. , & Davoudi, A. (2014). Distributed control systems for small- -scale power networks: Using multiagent cooperative control theory. IEEE Con- trol Systems Magazine, 34 (6), 56–77 . Boyd, S. , Parikh, N. , Chu, E. , Peleato, B. , & Eckstein, J. (2011). Distributed optimiza- tion and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3 (1), 1–122 . Boyd, S. , & Vandenberghe, L. (2004). Convex optimization . Cambridge University Press . Bullo, F. (2017). Lectures on network systems, version 0.95 . with contributions by Cortes, J., Dörﬂer, F., Martinez, S. Cai, K. , & Ishii, H. (2012). Average consensus on general strongly connected digraphs. Automatica, 48 (11), 2750–2761 . Cao, M. , Morse, A. S. , & Anderson, B. D. O. (2008). Reaching a consensus in a dy- namically changing environment: convergence rates, measurement delays, and asynchronous events. SIAM Journal of Control and Optimization, 47 (2), 601–623 . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 24 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Cao, Y. , Yu, W. , Ren, W. , & Chen, G. (2013). An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial In- formatics, 9 (1), 427–438 . Carli, R. , Notarstefano, G. , Schenato, L. , & Varagnolo, D. (2015). Analysis of Newton-Raphson consensus for multi-agent convex optimization under asyn- chronous and lossy communication. In Proceedings of the 54th IEEE conference on decision and control (CDC) (pp. 418–424) . Chang, T. H. , Nedi ´c, A. , & Scaglione, A. (2014). Distributed constrained optimization by consensus-based primal-dual perturbation method. IEEE Transactions on Au- tomatic Control, 59 (6), 1524–1538 . Charalambous, T. , Yuan, Y. , Yang, T. , Pan, W. , Hadjicostis, C. N. , & Johans- son, M. (2015). Distributed ﬁnite-time average consensus in digraphs in the presence of time-delays. IEEE Transactions on Control of Network Systems, 2 (4), 370–381 . Chatzipanagiotis, N. , Dentcheva, D. , & Zavlanos, M. M. (2015). An augmented Lagrangian method for distributed optimization. Mathematical Programming, 152 (1), 405–434 . Chatzipanagiotis, N. , & Zavlanos, M. M. (2016). A distributed algorithm for convex constrained optimization under noise. IEEE Transactions on Automatic Control, 61 (9), 2496–2511 . Chatzipanagiotis, N. , & Zavlanos, M. M. (2017). On the convergence of a distributed Augmented Lagrangian method for non-convex optimization. IEEE Transactions on Automatic Control, 62 (9), 4 405–4 420 . Chen, G. , Lewis, F. L. , & Xie, L. (2011). Finite-time distributed consensus via binary control protocols. Automatica, 47 (9), 1962–1968 . Chen, G. , & Li, Z. (2018). A ﬁxed-time convergent algorithm for distributed convex optimization in multi-agent systems. Automatica, 95 , 539–543 . Chen, G. , Ren, J. , & Feng, E. N. (2017). Distributed ﬁnite-time economic dispatch of a network of energy resources. IEEE Transactions on Smart Grid, 8 (2), 822– 832 . Chen, G. , & Zhao, Z. (2018). Delay effects on consensus-based distributed economic dispatch algorithm in microgrid. IEEE Transactions on Power Systems, 33 (1), 602–612 . Chen, W. , & Ren, W. (2016). Event-triggered zero-gradient-sum distributed consen- sus optimization over directed networks. Automatica, 65 , 90–97 . Cherukuri, A. , & Cortés, J. (2015). Distributed generator coordination for initializa- tion and anytime optimization in economic dispatch. IEEE Transactions on Con- trol of Network Systems, 2 (3), 226–237 . Cherukuri, A. , & Cortés, J. (2016). Initialization-free distributed coordination for eco- nomic dispatch under varying loads and generator commitment. Automatica, 74 , 183–193 . Cherukuri, A. , & Cortés, J. (2018). Distributed coordination of DERs with storage for dynamic economic dispatch. IEEE Transactions on Automatic Control, 63 (3), 835–842 . Cherukuri, A. , Mallada, E. , & Cortés, J. (2016). Asymptotic convergence of constrained primal-dual dynamics. Systems & Control Letters, 96 , 110–117 . Deng, Z. , Liang, S. , & Hong, Y. (2018). Distributed continuous-time algorithms for resource allocation problems over weight-balanced digraphs. IEEE Transactions on Cybernetics, 48 (11), 3116–3125 . Deng, Z. , Liang, S. , & Yu, W. (2018). Distributed optimal resource allocation of sec- ond-order multiagent systems. International Journal of Robust and Nonlinear Con- trol, 28 (14), 4246–4260 . Deng, Z. , Wang, X. , & Hong, Y. (2017). Distributed optimisation design with triggers for disturbed continuous-time multi-agent systems. IET Control Theory & Appli- cations, 11 (2), 282–290 . Dhingra, N. K., Khong, S. Z., & Jovanovi ´c, M. R. (2019). The proximal augmented La- grangian method for nonsmooth composite optimization. IEEE Transactions on Automatic Control . https://ieeexplore.ieee.org/abstract/document/8449971 . Dimarogonas, D. V. , Frazzoli, E. , & Johansson, K. H. (2012). Distributed event-trig- gered control for multi-agent systems. IEEE Transactions on Automatic Control, 57 (5), 1291–1297 . Ding, L. , Han, Q. , Ge, X. , & Zhang, X. (2018). An overview of recent advances in event-triggered consensus of multiagent systems. IEEE Transactions on Cybernet- ics, 48 (4), 1110–1123 . Ding, L. , Wang, L. Y. , Yin, G. , Zheng, W. X. , & Han, Q. (2019). Distributed energy management for smart grids with an event-triggered communication scheme. IEEE Transactions on Control Systems Technology . to appear Doan, T. T., & Beck, C. L. (2018). Distributed resource allocation over dynamic net- works with uncertainty. arXiv: 1708.03543 . Doan, T. T. , & Beck, C. L. (2017). Distributed Lagrangian methods for network re- source allocation. In Proceedings of the IEEE conference on control technology and applications (CCTA) (pp. 650–655) . Doan, T. T. , Beck, C. L. , & Srikant, R. (2017). On the convergence rate of distributed gradient methods for ﬁnite-sum optimization under communication delays. In Proceedings of the ACM on Measurement and Analysis of Computing Systems - SIG- METRICS: 1 (pp. 37:1–37:27) . Dominguez-Garcia, A. , Cady, S. , & Hadjicostis, C. N. (2012). Decentralized optimal dispatch of distributed energy resources. In Proceedings of the 51st IEEE confer- ence decision and control (CDC) (pp. 3688–3693) . Domínguez-García, A. D. , & Hadjicostis, C. N. (2011). Distributed strategies for aver- age consensus in directed graphs. In Proc. IEEE decision control european control conf. (pp. 2124–2129) . Domínguez-García, A. D. , & Hadjicostis, C. N. (2015). Distributed resource coordina- tion in networked systems described by digraphs. Systems & Control Letters, 82 , 33–39 . Dominguez-Garcia, A. D. , Hadjicostis, C. N. , & Vaidya, N. H. (2012). Resilient net- worked control of distributed energy resources. IEEE Journal on Selected Areas in Communications, 30 (6), 1137–1148 . Dong, X. , Hua, Y. , Zhou, Y. , Ren, Z. , & Zhong, Y. (2018). Theory and experiment on formation-containment control of multiple multirotor unmanned aerial ve- hicle systems. IEEE Transactions on Automation Science and Engineering, 16 (1), 229–240 . Dörﬂer, F. (2019). Distributed consensus-based optimization. Lecture note on ad- vanced topics in control 2017: Distributed systems & control . http://people. ee.ethz.ch/ ∼ﬂoriand/docs/Teaching/ATIC _ 2018/Optimization _ Lecture.pdf (last ac- cessed: April 4) Dörﬂer, F. , Simpson-Porco, J. , & Bullo, F. (2016). Breaking the hierarchy: distributed control and economic optimality in microgrids. IEEE Transactions on Control of Network Systems, 3 (3), 241–253 . Du, W. , Yao, L. , Wu, D. , Li, X. , Liu, G. , & Yang, T. (2018). Accelerated distributed en- ergy management for microgrids. In Proceedings of the IEEE power and energy society general meeting . Du, W. , Yi, X. , George, J. , Johansson, K. H. , & Yang, T. (2018). Distributed optimiza- tion with dynamic event-triggered mechanisms. In Proceedings of the 57th IEEE conference on decision and control (CDC) (pp. 969–974) . Du, W. , Yi, X. , Zhang, S. , George, J. , & Yang, T. (2019). Event-triggered proportion- al-integral algorithms for distributed optimization. submitted to the 58th IEEE conference on decision and control (CDC) . Duchi, J. C. , Agarwal, A. , & Wainwright, M. J. (2012). Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic Control, 57 (3), 592–606 . Falsone, A. , Margellos, K. , Garatti, S. , & Prandini, M. (2017). Dual decomposition for multi-agent distributed optimization with coupling constraints. Automatica, 84 , 149–158 . Feijer, D. , & Paganini, F. (2010). Stability of primal-dual gradient dynamicsand appli- cations to network optimization. Automatica, 46 (12), 1974–1981 . Feng, Z. , & Hu, G. (2017). Finite-time distributed optimization with quadratic ob- jective functions under uncertain information. In Proceedings of the 56th IEEE conference on decision and control (CDC) (pp. 208–213) . Freeman, R. A. , Yang, P. , & Lynch, K. M. (2006). Stability and convergence proper- ties of dynamic average consensus estimators. In Proceedings of the 45th IEEE conference on decision and control (CDC) (pp. 338–343) . Fridman, E. (2014). Tutorial on Lyapunov-based methods for time-delay systems. Eu- ropean Journal of Control, 20 (6), 271–283 . Gao, W. , Gao, J. , Ozbay, K. , & Jiang, Z. (2019). Reinforcement-learning-based cooper- ative adaptive cruise control of buses in the lincoln tunnel corridor with time– varying topology. IEEE Transactions on Intelligent Transportation Systems . to ap- pear Gharesifard, B. , & Cortés, J. (2014). Distributed continuous-time convex optimiza- tion on weight-balanced digraphs. IEEE Transactions on Automatic Control, 59 (3), 781–786 . Girard, A. (2015). Dynamic triggering mechanisms for event-triggered control. IEEE Transactions on Automatic Control, 60 (7), 1992–1997 . Giselsson, P. (2018). Large-scale and distributed optimization . Springer International Publishing . Godsi, C. , & Royle, G. F. (2001). Algebraic graph theory : 207. New York: Springer-Ver- lag . Guo, J. , Hug, G. , & Tonguz, O. K. (2017). A case for nonconvex distributed optimiza- tion in large-scale power systems. IEEE Transactions on Power Systems, 32 (5), 3842–3851 . Guo, Z. , & Chen, G. (2018). Distributed zero-gradient-sum algorithm for convex opti- mization with time-varying communication delays and switching networks. In- ternational Journal of Robust and Nonlinear Control, 28 (16), 4 900–4 915 . Hadjicostis, C. N. , & Charalambous, T. (2014). Average consensus in the presence of delays in directed graph topologies. IEEE Transactions on Automatic Control, 59 (3), 763–768 . Hadjicostis, C. N. , Domínguez-García, A. D. , & Charalambous, T. (2018). Distributed averaging and balancing in network systems with applications to coordina- tion and control. Foundations and Trends in Systems and Control, 5 (2–3), 99– 292 . Hadjicostis, C. N. , Vaidya, N. , & Dominguez-Garcia, A. (2016). Robust distributed av- erage consensus via exchange of running sums. IEEE Transactions on Automatic Control, 61 (6), 1492–1507 . Hamedani, E. Y. , & Aybat, N. S. (2017). Multi-agent constrained optimization of a strongly convex function over time-varying directed networks. In Proceedings of the 55th annual Allerton conference on communication control, and computing (Allerton) (pp. 518–525) . Han, S. (2019). Computational convergence analysis of distributed gradient descent for smooth convex objective function. In Proceedings of the IEEE american control conference . To appear Hatanaka, T. , Chopra, N. , Ishizaki, T. , & Li, N. (2018). Passivity-based distributed opti- mization with communication delays using PI consensus algorithm. IEEE Trans- actions on Automatic Control, 63 (12), 4 421–4 428 . He, X., Yu, J., Huang, T., & Li, C. (2019). Distributed power management for dy- namic economic dispatch in the multimicrogrids environment. IEEE Transactions on Control Systems Technology . https://ieeexplore.ieee.org/abstract/document/ 8329211 . Heemels, W. P. M. H. , Johansson, K. H. , & Tabuada, P. (2012). An introduction to event-triggered and self-triggered control. In Proceedings of the 51st IEEE confer- ence on decision and control (pp. 3270–3285) . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 25 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Hendrickx, J. M. , & Tsitsiklis, J. N. (2015). Fundamental limitations for anonymous distributed systems with broadcast communications. In Proceedings of the 53rd annual Allerton conference on communication control, and computing (Allerton) (pp. 9–16) . Hespanha, J. , Naghshtabrizi, P. , & Xu, Y. (2007). A survey of recent results in net- worked control systems. Proceedings of the IEEE, 95 (1), 138–162 . Hong, M. , Hajinezhad, D. , & Zhao, M. M. (2017). Prox-PDA: The proximal primal– dual algorithm for fast distributed nonconvex optimization and learning over networks. In Proceedings of the 34th international conference on machine learning (ICML) (pp. 1529–1538) . Hu, B. , Guan, Z. , Chen, G. R. , & Shen, X. S. (2019). A distributed hybrid event-time– driven scheme for optimization over sensor networks. IEEE Transactions on In- dustrial Electronics, 66 (9), 7199–7208 . Hug, G. , Kar, S. , & Wu, C. (2015). Consensus + innovations approach for distributed multiagent coordination in a microgrid. IEEE Transactions on Smart Grid, 6 (4), 1893–1903 . Jakoveti ´c, D. (2019). A uniﬁcation and generalization of exact distributed ﬁrst order methods. IEEE Transactions on Signal and Information Processing over Networks, 5 (1), 31–46 . Jakoveti ´c, D. , Bajovic, D. , Sahu, A. , & Kar, S. (2018). Convergence rates for distributed stochastic optimization over random networks. In Proceedings of the 57th IEEE conference on decision and control (pp. 4238–4245) . Jakoveti ´c, D. , Xavier, J. , & Moura, J. M. F. (2014a). Convergence rates of distributed Nesterov-like gradient methods on random networks. IEEE Transactions on Signal Processing, 62 (4), 868–882 . Jakoveti ´c, D. , Xavier, J. , & Moura, J. M. F. (2014b). Fast distributed gradient methods. IEEE Transactions on Automatic Control, 59 (5), 1131–1146 . Johansson, B. , Keviczky, T. , Johansson, M. , & Johansson, K. H. (2008). Subgradi- ent methods and consensus algorithms for solving convex optimization prob- lems. In Proceedings of the 47th IEEE conference on decision and control (CDC) (pp. 4185–4190) . Johansson, K. H. , Egerstedt, M. , Lygeros, J. , & Sastry, S. (1999). On the regularization of Zeno hybrid automata. Systems & Control Letters, 38 (3), 141–150 . Kar, S. , & Hug, G. (2012). Distributed robust economic dispatch in power systems: A consensus+innovations approach. In Proceedings of the IEEE power and energy society general meeting . Kar, S. , & Moura, J. (2013). Consensus + innovations distributed inference over net- works. IEEE Signal Processing Magazine, 30 (3), 99–109 . Kempe, D. , Dobra, A. , & Gehrke, J. (2003). Gossip-based computation of aggregate information. In Proceedings of the 44th annual IEEE symposium on foundations of computer science (pp. 4 82–4 91) . Kia, S. S. , Cortés, J. , & Martínez, S. (2015a). Distributed convex optimization via con- tinuous-time coordination algorithms with discrete-time communication. Auto- matica, 55 , 254–264 . Kia, S. S. , Cortés, J. , & Martínez, S. (2015b). Dynamic average consensus under lim- ited control authority and privacy requirements. International Journal of Robust and Nonlinear Control, 23 (13), 1941–1966 . Kim, B. H. , & Baldick, R. (1997). Coarse-grained distributed optimal power ﬂow. IEEE Transactions on Power Systems, 12 (2), 932–939 . Koppel, A. , Jakubiec, F. Y. , & Ribeiro, A. (2015). A saddle point algorithm for networked online convex optimization. IEEE Transactions on Signal Processing, 63 (19), 5149–5164 . Kraning, M. , Chu, E. , Lavaei, J. , & Boyd, S. (2014). Dynamic network energy man- agement via proximal message passing. Foundations and Trends in Optimization, 1 (2), 70–122 . Lam, A. , Zhang, B. , & Tse, D. (2012). Distributed algorithms for optimal power ﬂow problem. In Proceedings of the IEEE conference on decision and control (pp. 430–437) . Lamnabhi-Lagarrigue, F. , Annaswamy, A. , Engell, S. , Isaksson, A. , Khargonekar, P. , Murray, R. M. , et al. (2017). Systems & control for the future of humanity, re- search agenda: Current and future roles, impact and grand challenges. Annual Reviews in Control, 43 , 1–64 . Lasseter, R. , Akhil, A. , Marnay, C. , Stephens, J. , Dagle, J. , Guttromson, R. , et al. (2003). Integration of distributed energy resources: The CERTS Microgrid concept. Lawrence Berkeley National Laboratory. . LeBlanc, H. J. , Zhang, H. , Koutsoukos, X. , & Sundaram, S. (2013). Resilient asymptotic consensus in robust networks. IEEE Journal on Selected Areas in Communications, 31 (4), 766–781 . Lee, S. , & Zavlanos, M. M. (2016). Distributed primal-dual methods for online con- strained optimization. In Proceedings of the IEEE american control conference (pp. 7171–7176) . Lei, J. , Chen, H. F. , & Fang, H. T. (2016). Primal-dual algorithm for distributed con- strained optimization. Systems & Control Letters, 96 , 110–117 . Lei, J. , Chen, H. F. , & Fang, H. T. (2018). Asymptotic properties of primal-dual al- gorithm for distributed stochastic optimization over random networks with imperfect communications. SIAM Journal of Control and Optimization, 56 (3), 2159–2188 . Lessard, L. , Recht, B. , & Packard, A. (2016). Analysis and design of optimization al- gorithms via integral quadratic constraints. SIAM Journal on Optimization, 26 (1), 57–95 . Li, C. , Yu, X. , Yu, W. , Huang, T. , & Liu, Z. W. (2016). Distributed event-triggered scheme for economic dispatch in smart grids. IEEE Transactions on Industrial In- formatics, 12 (5), 1775–1785 . Li, C. , Yu, X. , Zhou, X. , & Ren, W. (2017). A ﬁxed time distributed optimization: A sliding mode perspective. In Proceedings of the 43rd annual conference of the IEEE industrial electronics society (pp. 8201–8207) . Li, N. , Chen, L. , & Low, S. H. (2011). Optimal demand response based on utility max- imization in power networks. In Proceedings of the IEEE power and energy society general meeting . Li, X., Yi, X., & Xie, L. (2018) Distributed online optimization for multi-agent net- works with coupled inequality constraints. arXiv: 1805.05573 . Li, Y. , Xiang, J. , & Wei, W. (2011). Consensus problems for linear time-invariant mul- ti-agent systems with saturation constraints. IET Control Theory & Applications, 5 (6), 823–829 . Li, Z. , Ding, Z. , Sun, J. , & Li, Z. (2018). Distributed adaptive convex optimization on directed graphs via continuous-time algorithms. IEEE Transactions on Automatic Control, 63 (5), 1434–1441 . Liang, S. , Zeng, X. , & Hong, Y. (2018). Distributed nonsmooth optimization with cou- pled inequality constraints via modiﬁed Lagrangian function. IEEE Transactions on Automatic Control, 63 (6), 1753–1759 . Liberzon, D. , & Morse, A. S. (1999). Basic problems in stability and design of switched systems. IEEE Control Systems Magazine, 19 (5), 59–70 . Lin, P. , Ren, W. , & Farrell, J. A. (2017). Distributed continuous-time optimization: Nonuniform gradient gains, ﬁnite-time convergence, and convex constraint set. IEEE Transactions on Automatic Control, 62 (5), 2239–2253 . Lin, P. , Ren, W. , & Song, Y. (2016). Distributed multi-agent optimization subject to nonidentical constraints and communication delays. Automatica, 65 , 120– 131 . Liu, J. , & Chen, W. (2016). Distributed convex optimisation with event-triggered communication in networked systems. International Journal of Systems Science, 47 (16), 3876–3887 . Liu, J., Chen, W., & Dai, H. (2019). Event-triggered zero-gradient-sum distributed convex optimization over networks with time-varying topologies. International Journal of Control . doi: 10.1080/00207179.2018.1460693 . Liu, Q. , & Wang, J. (2015). A second-order multi-agent network for bound-con- strained distributed optimization. IEEE Transactions on Automatic Control, 62 (7), 3461–3467 . Liu, Q. , Yang, S. , & Hong, Y. (2017). Constrained consensus algorithms with ﬁxed step size for distributed convex optimization over multiagent networks. IEEE Trans- actions on Automatic Control, 62 (8), 4259–4265 . Liu, S. , Xie, L. , & Quevedo, D. E. (2018). Event-triggered quantized communica- tion-based distributed convex optimization. IEEE Transactions on Control of Net- work Systems, 5 (1), 167–178 . Lobel, I. , & Ozdaglar, A. (2011). Distributed subgradient methods for convex opti- mization over random networks. IEEE Transactions on Automatic Control, 56 (6), 1291–1306 . Lorenzo, P. D. , & Scutari, G. (2016). NEXT: In-network nonconvex optimization. IEEE Transactions on Signal and Information Processing over Networks, 2 (2), 120–136 . Low, S. (2014). Convex relaxation of optimal power ﬂow part I: Formulations and equivalence. IEEE Transactions on Control of Network Systems, 1 (1), 15–27 . Lu, J. , & Tang, C. Y. (2012). Zero-gradient-sum algorithms for distributed convex op- timization: The continuous-time case. IEEE Transactions on Automatic Control, 57 (9), 2348–2354 . Lynch, K. M. , & Park, F. C. (2017). Modern robotics: Mechanics, planning, and control . Cambridge University Press . Ma, D. , & Chen, J. (2019). Delay margin of low-order systems achievable by PID con- trollers. IEEE Transactions on Automatic Control, 64 (5), 1958–1973 . Ma, D., Tian, R., Zulﬁqar, A., Chen, J., & Chai, T. (2019). Bounds on delay consensus margin of second-order multi-agent systems with robust position and veloc- ity feedback protocol. IEEE Transactions on Automatic Control . https://ieeexplore. ieee.org/abstract/document/8552432 . Ma, W. J. , Wang, J. , Gupta, V. , & Chen, C. (2018). Distributed energy management for networked microgrids using online alternating direction method of multipliers with regret. IEEE Transactions on Smart Grid, 9 (2), 847–856 . Madani, R. , Sojoudi, S. , & Lavaei, J. (2015). Convex relaxation for optimal power ﬂow problem: Mesh networks. IEEE Transactions on Power Systems, 30 (1), 199–211 . Magnússon, S. , Fischione, C. , & Li, N. (2017). Voltage control using limited commu- nication. IFAC-PapersOnLine, 50 (1), 1–6 . 20th IFAC World Congress Mai, V. S. , & Abed, E. H. (2018). Local prediction for enhanced convergence of dis- tributed optimization algorithms. IEEE Transactions on Control of Network Sys- tems, 5 , 1962–1975 . Mallada, E. , Zhao, C. , & Low, S. (2017). Optimal load-side control for frequency reg- ulation in smart grids. IEEE Transactions on Automatic Control, 62 (12), 6294– 6309 . Margellos, K. , Falsone, A. , Garatti, S. , & Prandini, M. (2018). Distributed constrained optimization and consensus in uncertain networks via proximal minimization. IEEE Transactions on Automatic Control, 63 (5), 1372–1387 . Matei, I., & Baras, J. S. (2017) Nonlinear programming methods for distributed opti- mization. arXiv: 1707.04598 . Matei, I. , & Baras, J. S. (2011). Performance evaluation of the consensus-based dis- tributed subgradient method under random communication topologies. IEEE Journal of Selected Topics in Signal Processing, 5 (4), 754–771 . Mateos-Núñez, D. , & Cortés, J. (2016). Noise-to-state exponentially stable distributed convex optimization on weight-balanced digraphs. SIAM Journal on Control and Optimization, 54 (1), 266–290 . Megretski, A. , & Rantzer, A. (1997). System analysis via integral quadratic con- straints. IEEE Transactions on Automatic Control, 42 (6), 819–830 . Meng, D. , & Moore, K. L. (2016). Learning to cooperate: Networks of formation agents with switching topologies. Automatica, 64 , 278–293 . Meng, X. , Xie, L. , & Soh, Y. C. (2016). Distributed event driven optimization for net- work utility maximization. In Proceedings of the 55th IEEE conference on decision and control (CDC) (pp. 2221–2226) . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 26 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Meng, X. , Xie, L. , Soh, Y. C. , Nowzari, C. , & Pappas, G. J. (2015). Periodic event-trig- gered average consensus over directed graphs. In Proceedings of the 54th IEEE conference on decision and control (CDC) (pp. 2055–2060) . Meng, Z. , Yang, T. , Shi, G. , Dimarogonas, D. V. , Hong, Y. , & Johansson, K. H. (2017). Targeted agreement of multiple Lagrangian systems. Automatica, 84 , 109–116 . Meng, Z. , Zhao, Z. , & Lin, Z. (2013). On global leader-following consensus of identical linear dynamic systems subject to actuator saturation. Systems & Control Letters, 62 , 132–142 . Mokhtari, A. , & Ribeiro, A. (2016). DSA: Decentralized double stochastic averaging gradient algorithm. Journal of Machine Learning Research, 17 , 1–35 . Mokhtari, A. , Shi, W. , Ling, Q. , & Ribeiro, A. (2016). DQM: Decentralized quadratically approximated alternating direction method of multipliers. IEEE Transactions on Signal Processing, 64 (19), 5158–5173 . Molzahn, D. K. , Dörﬂer, F. , Sandberg, H. , Low, S. H. , Chakrabarti, S. , Baldick, R. , & Lavaei, J. (2017). A survey of distributed optimization and control algorithms for electric power systems. IEEE Transactions on Smart Grid, 8 (6), 2941–2962 . Nedi ´c, A. (2015). Convergence rate of distributed averaging dynamics and optimiza- tion in networks. Foundations and Trends in Systems and Control, 2 (1), 1–100 . Nedi ´c, A. , & Liu, J. (2018). Distributed optimization for control. Annual Review of Control, Robotics, and Autonomous Systems, 1 , 77–103 . Nedi ´c, A. , & Olshevsky, A. (2015). Distributed optimization over time-varying di- rected graphs. IEEE Transactions on Automatic Control, 60 (3), 601–615 . Nedi ´c, A. , Olshevsky, A. , & Rabbat, M. G. (2018). Network topology and communi- cation-computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106 (5), 953–976 . Nedi ´c, A. , Olshevsky, A. , & Shi, W. (2017a). Achieving geometric convergence for dis- tributed optimization over time-varying graphs. SIAM Journal on Optimization, 27 (4), 2597–2633 . Nedi ´c, A. , Olshevsky, A. , Shi, W. , & Uribe, C. A. (2017b). Geometrically convergent distributed optimization with uncoordinated step-sizes. In Proceedings of the IEEE American control conference (pp. 3950–3955) . Nedi ´c, A . , Olshevsky, A . , & Shi, W. (2018). Improved convergence rates for dis- tributed resource allocation. In Proceedings of 57th IEEE 2018 IEEE conference on decision and control (CDC) (pp. 172–177) . Nedi ´c, A. , & Ozdaglar, A. (2009). Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54 (1), 48–61 . Nedi ´c, A. , & Ozdaglar, A. (2010). Convergence rate for consensus with delays. Journal of Global Optimization, 47 (3), 437–456 . Nedi ´c, A. , Ozdaglar, A. , & Parrilo, P. A. (2010). Constrained consensus and opti- mization in multi-agent networks. IEEE Transactions on Automatic Control, 55 (4), 922–938 . Nesterov, Y. (2004). Introductory lectures on convex optimization: A basic course . Kluwer Academic Publishers . Nowzari, C. , Cortés, J. , & Pappas, G. J. (2018). Event-triggered control for multi-agent average consensus (pp. 177–208). Wiley . Ch. 7 Nowzari, C. , Garcia, E. , & Cortés, J. (2019). Event-triggered communication and con- trol of network systems for multi-agent consensus. Automatica, 105 , 1–27 . Parikh, N. , & Boyd, S. (2014). Proximal algorithms. Foundations and Trends in Opti- mization, 1 (3), 123–231 . Pasqualetti, F. , Bicchi, A. , & Bullo, F. (2012). Consensus computation in unreliable networks: A system theoretic approach. IEEE Transactions on Automatic Control, 57 (1), 90–104 . Pedrasa, M. A. A. , Spooner, T. D. , & MacGill, I. F. (2010). Coordinated scheduling of residential distributed energy resources to optimize smart home energy ser- vices. IEEE Transactions on Smart Grid, 1 (2), 134–143 . Peng, Q. , & Low, S. H. (2018). Distributed optimal power ﬂow algorithm for radial networks, I: Balanced single phase case. IEEE Transactions on Smart Grid, 9 (1), 111–121 . Pilloni, A. , Pisano, A. , Franceschelli, M. , & Usai, E. (2016). A discontinuous algo- rithm for distributed convex optimization. In Proceedings of the 14th interna- tional workshop on variable structure systems (VSS) (pp. 22–27) . Polyakov, A. (2018). Nonlinear feedback design for ﬁxed-time stabilization of linear control system. IEEE Transactions on Automatic Control, 57 (8), 2106–2109 . Polyakov, A. , Eﬁmov, D. , & Perruquetti, W. (2015a). Finite-time and ﬁxed-time stabi- lization: Implicit Lyapunov function approach. Automatica, 51 , 332–340 . Polyakov, A. , Eﬁmov, D. , & Perruquetti, W. (2015b). Nonsingular ﬁxed-time consen- sus tracking for second-order multi-agent networks. Automatica, 54 , 305–309 . Pu, S. , Shi, W. , Xu, J. , & Nedi ´c, A. (2018). A push–pull gradient method for distributed optimization in networks. In Proceedings of the 57th IEEE conference on decision and control (CDC) (pp. 3385–3390) . Qin, J. , Ma, Q. , Shi, Y. , & Wang, L. (2017). Recent advances in consensus of multi- -agent systems: A brief survey. IEEE Transactions on Industrial Electronics, 64 (6), 4 972–4 983 . Qin, J., Wan, Y., Yu, X., Li, F., & Li, C. (2019). Consensus-based distributed coordi- nation between economic dispatch and demand response. IEEE Transactions on Smart Grid . https://ieeexplore.ieee.org/abstract/document/8356102 . Qiu, Z. , Hong, Y. , & Xie, L. (2016). Optimal consensus of Euler-Lagrangian systems with kinematic constraints. IFAC-papersonline, 49 (22), 327–332 . 6th IFAC Work- shop on Distributed Estimation and Control in Networked Systems NECSYS 2016 Qiu, Z. , Liu, S. , & Xie, L. (2016). Distributed constrained optimal consensus of multi- -agent systems. Automatica, 68 , 209–215 . Qiu, Z. , Liu, S. , & Xie, L. (2018). Necessary and suﬃcient conditions for distributed constrained optimal consensus under bounded input. International Journal of Ro- bust and Nonlinear Control, 28 (6), 2619–2635 . Qu, G. , & Li, N. (2018). Harnessing smoothness to accelerate distributed optimiza- tion. IEEE Transactions on Control of Network Systems, 5 (3), 1245–1260 . Qu, G. , & Li, N. (2019). On the exponential stability of primal-dual gradient dynam- ics. IEEE Control Systems Letters, 3 (1), 43–48 . Rahbari-Asr, N. , Ojha, U. , Zhang, Z. , & Chow, M. Y. (2014). Incremental welfare con- sensus algorithm for cooperative distributed generation/demand response in smart grid. IEEE Transactions on Smart Grid, 6 (5), 2836–2845 . Rahimi, F. , & Ipakchi, A. (2010). Demand response as a market resource under the smart grid paradigm. IEEE Transactions on Smart Grid, 1 (1), 82–88 . Ren, W. , & Cao, Y. (2011). Distributed coordination of multi-agent networks. Com- munications and control engineering series . London: Springer Verlag . Richard, J. P. (2003). Time-delay systems:an overview of some recent advances and open problems. Automatica, 39 (10), 1667–1694 . Saadatniaki, F., Xin, R., & Khan, U. A. (2018). Optimization over time-varying directed graphs withrow and column-stochastic matrices. arXiv: 1810.07393 . Sayed, A. (2014a). Adaptation, learning, and optimization over networks. Foundations and Trends in Machine Learning, 7 (4–5), 311–801 . Sayed, A. H. (2014b). Diffusion adaptation over networks. In Academic press li- brary in signal processing, vol. 3 of academic press library in signal processing (pp. 323–453). Elsevier . Schenato, L. , Sinopoli, B. , Franceschetti, M. , Poolla, K. , & Sastry, S. S. (2007). Foun- dations of control and estimation over lossy networks. Proceedings of the IEEE, 95 (1), 163–187 . Seyboth, G. S. , Dimarogonas, D. V. , & Johansson, K. H. (2013). Event-based broad- casting for multi-agent average consensus. Automatica, 49 (1), 245–252 . Shi, G. , Johansson, K. H. , & Hong, Y. (2013). Reaching an optimal consensus: Dy- namical systems that compute intersections of convex sets. IEEE Transactions on Automatic Control, 58 (3), 610–622 . Shi, W. , Ling, Q. , Wu, G. , & Yin, W. (2015a). EXTRA: An exact ﬁrst-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25 (2), 944–966 . Shi, W. , Ling, Q. , Wu, G. , & Yin, W. (2015b). A proximal gradient algorithm for decen- tralized composite optimization. IEEE Transactions on Signal Processing, 63 (22), 6013–6023 . Shi, X. , Wang, Y. , Song, S. , & Yan, G. (2018). Distributed optimisation for resource allocation with event-triggered communication over general directed topology. International Journal of Systems Science, 49 (6), 1119–1130 . Somarakis, C. , & Baras, J. S. (2015). Distributed solution of the economic dispatch problem in smart grid power systems framework with delays. In Proceedings of the 54th IEEE conference on decision and control (CDC) (pp. 6577–6582) . Somarakis, C. , Maity, D. , & Baras, J. S. (2016). The effect of delays in the economic dispatch problem for smart grid architectures. In Proceedings of the IEEE Ameri- can control conference (pp. 3533–3538) . Song, Y. , & Chen, W. (2016). Finite-time convergent distributed consensus optimisa- tion over networks. IET Control Theory and Applications, 10 (11), 1314–1318 . Spong, M. W. , Hutchinson, S. , & Vidyasagar, M. (2006). Robot dynamics and control . John Wiley & Sons, Inc. . Srivastava, K. , & Nedi ´c, A. (2011). Distributed asynchronous constrained stochas- tic optimization. IEEE Journal of Selected Topics in Signal Processing, 5 (4), 772– 790 . Su, L. , & Vaidya, N. (2016). Multi-agent optimization in the presence of byzantine adversaries: Fundamental limits. In Proceedings of the IEEE American control con- ference (pp. 7183–7188) . Sun, A. , Phan, D. , & Ghosh, S. (2013). Fully decentralized ac optimal power ﬂow algorithms. In Proceedings of the IEEE power and energy society general meeting . Sundaram, S. , & Gharesifard, B. (2019). Distributed optimization under adversarial nodes. IEEE Transactions on Automatic Control, 64 (3), 1063–1076 . Sundaram, S. , & Hadjicostis, C. N. (2007). Finite-time distributed consensus in graphs with time-invariant topologies. In Proceedings of the IEEE American con- trol conference (pp. 711–716) . Sundaram, S. , & Hadjicostis, C. N. (2011). Distributed function calculation via lin- ear iterative strategies in the presence of malicious agents. IEEE Transactions on Automatic Control, 56 (7), 1495–1508 . Sundararajan, A. , Hu, B. , & Lessard, L. (2017). Robust convergence analysis of dis- tributed optimization algorithms. In Proceedings of the 55th annual Allerton con- ference on communication control, and computing (Allerton) (pp. 1206–1212) . Sundararajan, A., Scoy, B. V., & Lessard, L. 2018. A canonical form for ﬁrst-order distributed optimization algorithms. arXiv: 1809.08709 . Tabuada, P. (2007). Event-triggered real-time scheduling of stabilizing control tasks. IEEE Transactions on Automatic Control, 52 (9), 1680–1685 . Tang, Z. , Hill, D. J. , & Liu, T. (2018). A novel consensus-based economic dispatch for microgrids. IEEE Transactions on Smart Grid, 9 (4), 3920–3922 . Tatarenko, T. , & Touri, B. (2017). Non-convex distributed optimization. IEEE Transac- tions on Automatic Control, 62 (8), 3744–3757 . Tian, Y., Sun, Y., Du, B., & Scutari, G. 2018. ASY-SONATA: Achieving geometric con- vergence for distributed asynchronous optimization. arXiv: 1803.10359 . Tran, N. T. , Wang, Y. W. , Liu, X. K. , & Xiao, J. W. (2017). Distributed optimisation of second-order multi-agent systems by control algorithm using position-only interaction with time-varying delay. IET Control Theory & Applications, 11 (15), 2549–2558 . Tran, N. T. , Wang, Y. W. , Liu, X. K. , & Xiao, J. W. (2018). Event-triggered gradien- t-based distributed optimisation for multi-agent systems with state consensus constraint. IET Control Theory & Applications, 12 (10), 1515–1519 . Tran, N. T., Wang, Y. W., Liu, X. K., Xiao, J. W., & Lei, Y. (2019). Distributed opti- mization problem for second-order multi-agent systems with event-triggered and time-triggered communication. Journal of the Franklin Institute . doi: 10.1016/ j.jfranklin.2018.02.009 . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx 27 ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Tsianos, K. I. , Lawlor, S. , & Rabbat, M. G. (2012). Push-sum distributed dual averaging for convex optimization. In Proceedings of the 51st IEEE conference on decision and control (CDC (pp. 5453–5458) . Tsianos, K. I. , & Rabbat, M. G. (2011). Distributed consensus and optimization under communication delays. In Proceedings of the 49th annual Allerton conference on communication control, and computing (Allerton) (pp. 974–982) . Tsianos, K. I. , & Rabbat, M. G. (2012). Distributed strongly convex optimization. In Proceedings of the 50th annual Allerton conference on communication control, and computing (Allerton) (pp. 593–600) . Tsitsiklis, J. N. (1984). Problems in decentralized decision making and computation . MIT, Cambridge, MA Ph.d. thesis . Tsitsiklis, J. N. , Bertsekas, D. P. , & Athans, M. (1986). Distributed asynchronous de- terministic and stochastic gradient optimization algorithms. IEEE Transactions on Automatic Control, 31 (9), 803–812 . Varagnolo, D. , Zanella, F. , Cenedese, A. , Pillonetto, G. , & Schenato, L. (2016). New- ton-Raphson consensus for distributed convex optimization. IEEE Transactions on Automatic Control, 61 (4), 994–1009 . Wai, H. , Lafond, J. , Scaglione, A. , & Moulines, E. (2017). Decentralized Frank-Wolfe algorithm for convex and nonconvex problems. IEEE Transactions on Automatic Control, 62 (11), 5522–5537 . Wan, P. , & Lemmon, M. D. (2009). Event-triggered distributed optimization in sensor networks. In Proceedings of the international conference on information processing in sensor networks (pp. 49–60) . Wang, D. , Gupta, V. , & Wang, W. (2018). An event-triggered protocol for distributed optimal coordination of double-integrator multi-agent systems. Neurocomputing, 319 , 34–41 . Wang, D. , Wang, D. , & Wang, W. (2016). An optimal algorithm for high-order mul- ti-agent systems with event-triggered communication. In 31st youth academic annual conference of Chinese association of automation (YAC) (pp. 235–240) . Wang, D. , Wang, Z. , Chen, M. , & Wang, W. (2018). Distributed optimization for mul- ti-agent systems with constraints set and communication time-delay over a di- rected graph. Information Sciences, 438 , 1–14 . Wang, J. , & Elia, N. (2010). Control approach to distributed optimization. In Proceed- ings of the 48th annual Allerton conference on communication control, and com- puting (Allerton) (pp. 557–561) . Wang, J. , & Elia, N. (2011). A control perspective for centralized and distributed con- vex optimization. In Proceedings of the 50th IEEE conference on decision and con- trol and european control conference (pp. 3800–3805) . Wang, L. , & Xiao, F. (2010). Finite-time consensus problems for networks of dynamic agents. IEEE Transactions on Automatic Control, 55 (4), 950–955 . Wang, P. , Lin, P. , Ren, W. , & Song, Y. (2018). Distributed subgradient-based multia- gent optimization with more general step sizes. IEEE Transactions on Automatic Control, 63 (7), 2295–2302 . Wang, X. , Hong, Y. , & Ji, H. (2016). Distributed optimization for a class of nonlinear multiagent systems with disturbance rejection. IEEE Transactions on Cybernetics, 46 (7), 1655–1666 . Wang, X. , & Lemmon, M. D. (2011). Event-triggering in distributed networked con- trol systems. IEEE Transactions on Automatic Control, 56 (3), 586–601 . Wang, Z. , Wang, D. , Sun, J. , & Wang, W. (2018). Exponential convergence rate of distributed optimisation for multi-agent systems with constraints set over a di- rected graph. IET Control Theory & Applications, 12 (9), 1201–1207 . Wei, E. , Ozdaglar, A. , & Jadbabaie, A. (2013a). A distributed Newton method for net- work utility maximization-I: Algorithm. IEEE Transactions on Automatic Control, 58 (9), 2162–2175 . Wei, E. , Ozdaglar, A. , & Jadbabaie, A. (2013b). A distributed Newton method for net- work utility maximization-II: Convergence. IEEE Transactions on Automatic Con- trol, 58 (9), 2176–2188 . Wu, D. , Lian, J. , Sun, Y. , Yang, T. , & Hansen, J. (2017). Hierarchical control framework for integrated coordination between distributed energy resources and demand response. Electric Power Systems Research, 150 , 45–54 . Wu, D. , Yang, T. , Stoorvogel, A. A. , & Stoustrup, J. (2017). Distributed optimal coordi- nation for distributed energy resources in power systems. IEEE Transactions on Automation Science and Engineering, 14 (2), 414–424 . Wu, J. , Yang, T. , Wu, D. , Kalsi, K. , & Johansson, K. H. (2017). Distributed optimal dispatch of distributed energy resources over lossy communication networks. IEEE Transactions on Smart Grid, 8 (6), 3125–3137 . Xi, C. , & Khan, U. A. (2017). DEXTRA: A fast algorithm for optimization over directed graphs. IEEE Transactions on Automatic Control, 62 (10), 4 980–4 993 . Xi, C. , Mai, V. S. , Xin, R. , Abed, E. , & Khan, U. A. (2018). Linear convergence in op- timization over directed graphs with row-stochastic matrices. IEEE Transactions on Automatic Control, 63 (10), 3558–3565 . Xi, C. , Xin, R. , & Khan, U. A. (2018). ADD-OPT: Accelerated distributed directed opti- mization. IEEE Transactions on Automatic Control, 63 (5), 1329–1339 . Xie, P. , You, K. , Tempo, R. , Song, S. , & Wu, C. (2018). Distributed convex optimization with inequality constraints over time-varying unbalanced digraphs. IEEE Trans- actions on Automatic Control, 63 (12), 4331–4337 . Xie, Y. , & Lin, Z. (2017). Global optimal consensus of multi-agent systems with bounded controls. Systems & Control Letters, 102 , 104–111 . Xie, Y. , & Lin, Z. (2019). Global optimal consensus for higher-order multi-agent sys- tems with bounded controls. Automatica, 99 , 301–307 . Xin, R. , & Khan, U. A. (2018). A linear algorithm for optimization over directed graphs with geometric convergence. IEEE Control Systems Letters, 2 (3), 325–330 . Xin, R. , Xi, C. , & Khan, U. A. (2019). FROST–fast row-stochastic optimization with uncoordinated step-sizes. EURASIP Journal on Advances in Signal Processing, 1 , 1–14 . Xing, H. , Mou, Y. , Fu, M. , & Lin, Z. (2015). Distributed bisection method for eco- nomic power dispatch in smart grid. IEEE Transactions on Power Systems, 30 (6), 3024–3035 . Xu, J. , Zhu, S. , Soh, Y. C. , & Xie, L. (2015). Augmented distributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes. In Pro- ceedings of 54th IEEE conference on decision and control (CDC)pp. 2055–2060 . Xu, J. , Zhu, S. , Soh, Y. C. , & Xie, L. (2018a). Convergence of asynchronous distributed gradient methods over stochastic networks. IEEE Transactions on Automatic Con- trol, 63 (2), 434–448 . Xu, J. , Zhu, S. , Soh, Y. C. , & Xie, L. (2018b). A Bregman splitting scheme for dis- tributed optimization over networks. IEEE Transactions on Automatic Control, 63 (11), 3809–3824 . Xu, Y. , Han, T. , Cai, K. , Lin, Z. , Yan, G. , & Fu, M. (2017). A distributed algorithm for re- source allocation over dynamic digraphs. IEEE Transactions on Signal Processing, 65 (10), 2600–2612 . Yang, B. , & Johansson, M. (2010). Distributed optimization and games: A tutorial overview (pp. 109–148). London: Springer . Yang, S. , Liu, Q. , & Wang, J. (2017a). Distributed optimization based on a multiagent system in the presence of communication delays. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 47 (5), 717–728 . Yang, S. , Liu, Q. , & Wang, J. (2017b). A multi-agent system with a proportional-inte- gral protocol for distributed constrained optimization. IEEE Transactions on Au- tomatic Control, 60 (12), 3310–3315 . Yang, S. , Tan, S. , & Xu, J. X. (2013). Consensus based approach for economic dispatch problem in a smart grid. IEEE Transactions on Power Systems, 28 (4), 4 416–4 426 . Yang, T. , Lu, J. , Wu, D. , Wu, J. , Shi, G. , Meng, Z. , et al. (2017). A distributed algorithm for economic dispatch over time-varying directed networks with delays. IEEE Transactions on Industrial Electronics, 64 (6), 5095–5106 . Yang, T. , Meng, Z. , Dimarogonas, D. V. , & Johansson, K. H. (2014). Global consensus for discrete-time multi-agent systems with input saturation constraints. Auto- matica, 50 (2), 499–506 . Yang, T. , Wan, Y. , Wang, H. , & Lin, Z. (2018). Global optimal consensus for discrete– time multi-agent systems with bounded controls. Automatica, 102 , 104–111 . Yang, T. , Wu, D. , Stoorvogel, A. , & Stoustrup, J. (2016). Distributed coordination of energy storage with distributed generators. In Proceedings of the IEEE power and energy society general meeting . Yang, T. , Wu, D. , Sun, Y. , & Lian, J. (2015). Impacts of time delays on distributed algorithms for economic dispatch. In Proceedings of the IEEE power and energy society general meeting . Yang, T. , Wu, D. , Sun, Y. , & Lian, J. (2016). Minimum-time consensus based approach for power system applications. IEEE Transactions on Industrial Electronics, 63 (2), 1318–1328 . Yao, L. , Yuan, Y. , Sundaram, S. , & Yang, T. (2018). Distributed ﬁnite-time optimiza- tion. In Proceedings of the 14th IEEE international conference on control and au- tomation (ICCA) (pp. 147–154) . Yi, P. , Hong, Y. , & Liu, F. (2015). Distributed gradient algorithm for constrained opti- mization with application to load sharing in power systems. Systems & Control Letters, 83 , 45–52 . Yi, P. , Hong, Y. , & Liu, F. (2016). Initialization-free distributed algorithms for opti- mal resource allocation with feasibility constraints and application to economic dispatch of power systems. Automatica, 74 , 259–269 . Yi, X. (2017). Resource-constrained multi-agent control systems: Dynamic event-trigger- ing, input saturation, and connectivity preservation . Royal Institute of Technology, Sweden Licentiate thesis . Yi, X. , Yang, T. , Wu, J. , & Johansson, K. H. (2019a). Distributed event-triggered control for global consensus of multi-agent systems with input saturation. Automatica, 100 , 1–9 . Yi, X. , Yang, T. , Wu, J. , & Johansson, K. H. (2019b). Distributed online convex opti- mization with long term coupled constraints. In Proceedings of the 38th Chinese control conference . Yi, X. , Yao, L. , Yang, T. , George, J. , & Johansson, K. H. (2018). Distributed optimization for second-order multi-agent systems with dynamic event-triggered communi- cation. In Proceedings of the 57th IEEE conference on decision and control (CDC) (pp. 3397–3402) . Yu, S. , & Long, X. (2015). Finite-time consensus for second-order multi-agent sys- tems with disturbances by integral sliding mode. Automatica, 54 , 158–165 . Yu, X. , & Xue, Y. (2016). Smart grids: A cyber-physical systems perspective. Proceed- ings of the IEEE, 104 (5), 1058–1070 . Yuan, D. , Ho, D. W. C. , & Xu, S. (2016). Regularized primal-dual subgradient method for distributed constrained optimization. IEEE Transactions on Cybernetics, 46 (9), 2109–2118 . Yuan, D. , Xu, S. , & Zhao, H. (2011). Distributed primal-dual subgradient method for multiagent optimization via consensus algorithms. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 41 (6), 1715–1724 . Yuan, K. , Ling, Q. , & Yin, W. (2015). On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26 (3), 1835–1854 . Yuan, Y. (2012). Decentralised network prediction and reconstruction algorithms . Uni- versity of Cambridge Ph.d. thesis . Yuan, Y. , Stan, G. B. , Shi, L. , Barahona, M. , & Goncalves, J. (2013). Decentralised min- imum-time consensus. Automatica, 49 (5), 1227–1235 . Yuan, Y. , Zhang, H. , Wu, Y. , Zhu, T. , & Ding, H. (2017). Bayesian learning-based mod- el-predictive vibration control for thin-walled workpiece machining processes. IEEE/ASME Transactions on Mechatronics, 22 (1), 509–520 . Zanella, F. , Varagnolo, D. , Cenedese, A. , Pillonetto, G. , & Schenato, L. (2011). New- ton-Raphson consensus for distributed convex optimization. In Proceedings of the 50th IEEE conference on decision and control and European control conference (pp. 5917–5922) . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006 28 T. Yang, X. Yi and J. Wu et al. / Annual Reviews in Control xxx (xxxx) xxx ARTICLE IN PRESS JID: JARAP [m5G; May 22, 2019;12:34 ] Zeng, J. , & Yin, W. (2017). Extrapush for convex smooth decentralized optimization over directed networks. Journal of Computational Mathematics, 35 (4), 381–394 . Zeng, J. , & Yin, W. (2018). On nonconvex decentralized gradient descent. IEEE Trans- actions on Signal Processing, 66 (11), 2834–2848 . Zeng, X. , Yi, P. , & Hong, Y. (2017). Distributed continuous-time algorithm for con- strained convex optimizations via nonsmooth analysis approach. IEEE Transac- tions on Automatic Control, 62 (10), 5227–5233 . Zhang, K. , Shi, W. , Zhu, H. , Dall’Anese, E. , & Ba ¸s ar, T. (2018). Dynamic power distri- bution system management with a locally connected communication network. IEEE Journal of Selected Topics in Signal Processing, 12 (4), 673–687 . Zhang, S. , Yi, X. , George, J. , & Yang, T. (2019). Computational convergence analysis of distributed optimization algorithms for directed graphs. In Proceedings of the 15th IEEE international conference on control and automation (ICCA) . Zhang, W. , Xu, Y. , Liu, W. , Zang, C. , & Yu, H. (2015). Distributed online optimal en- ergy management for smart grids. IEEE Transactions on Industrial Informatics, 11 (3), 717–727 . Zhang, Y. , Deng, Z. , & Hong, Y. (2017). Distributed optimal coordination for multiple heterogeneous Euler–Lagrangian systems. Automatica, 79 , 207–213 . Zhang, Y. , & Hong, Y. (2014). Distributed optimization design for second-order multi-agent systems. In Proceedings of the 33rd Chinese control conference (pp. 1755–1760) . Zhang, Y. , & Hong, Y. (2015). Distributed optimization design for high-order multi-agent systems. In Proceedings of the 34th Chinese control conference (pp. 7251–7256) . Zhang, Y., Li, H., Sun, J., & He, W. (2019). Cooperative adaptive event-triggered con- trol for multiagent systems with actuator failures. IEEE Transactions on Systems, Man, and Cybernetics: Systems . https://ieeexplore.ieee.org/abstract/document/ 8587139 . Zhang, Y., Sun, J., Liang, H., & Li, H. (2019). Event-triggered adaptive tracking control for multiagent systems with unknown disturbances. IEEE Transactions on Cyber- netics . https://ieeexplore.ieee.org/abstract/document/8474310 . Zhang, Z. , & Chow, M. Y. (2012). Convergence analysis of the incremental cost con- sensus algorithm under different communication network topologies in a smart grid. IEEE Transactions on Power Systems, 27 (4), 1761–1768 . Zhang, Z. , Chow, M. Y. , & Chakrabortty, A. (2012). The inﬂuence of time delays on decentralized economic dispatch by using incremental cost consensus algo- rithm. In M. D. Ili ´c (Ed.), Control and optimization methods for electric smart grids, vol. 3 of power electronics and power systems (pp. 313–326). New York: Springer . Zhang, Z. , Ying, X. , & Chow, M. Y. (2011). Decentralizing the economic dispatch prob- lem using a two-level incremental cost consensus algorithm in a smart grid en- vironment. In Proceedings of the North American power symposium (NAPS) . Zhao, C., Duan, X., & Shi, Y. (2019). Analysis of consensus-based economic dispatch algorithm under time delays. IEEE Transactions on Systems, Man, and Cybernetics: Systems . https://ieeexplore.ieee.org/abstract/document/8387460 . Zhao, C. , He, J. , Cheng, P. , & Chen, J. (2017). Consensus-based energy management in smart grid with transmission losses and directed communication. IEEE Trans- actions on Smart Grid, 8 (5), 2049–2061 . Zhao, C. , He, J. , & Wang, Q. (2017). Resilient distributed optimization algorithm against adversary attacks. In Proceedings of 13th IEEE international conference on control automation (ICCA) (pp. 473–478) . Zhao, C. , Topcu, U. , & Low, S. (2013). Optimal load control via frequency measure- ment and neighborhood area communication. IEEE Transactions on Power Sys- tems, 28 (4), 3576–3587 . Zhao, T., Li, Z., & Ding, Z. (2019). Consensus-based distributed optimal energy man- agement with less communication in a microgrid. IEEE Transactions on Industrial Informatics . https://ieeexplore.ieee.org/abstract/document/8469107 . Zhao, Y. , Liu, Y. , Wen, G. , & Chen, G. (2017). Distributed optimization for linear mul- tiagent systems: Edge- and node-based adaptive designs. IEEE Transactions on Automatic Control, 62 (7), 3602–3609 . Zhao, Z. , & Lin, Z. (2016). Global leader-following consensus of a group of general linear systems using bounded controls. Automatica, 68 , 294–304 . Zheng, W. , Wu, W. , Zhang, B. , Sun, H. , & Liu, Y. (2016). A fully distributed reactive power optimization and control method for active distribution networks. IEEE Transactions on Smart Grid, 7 (2), 1021–1033 . Zhong, M. , & Cassandras, C. G. (2010). Asynchronous distributed optimization with event-driven communication. IEEE Transactions on Automatic Control, 55 (12), 2735–2750 . Zhong, X., & He, H. (2019). GrHDP solution for optimal consensus control of multi- agent discrete-time systems. IEEE Transactions on Systems, Man, and Cybernetics: Systems . https://ieeexplore.ieee.org/abstract/document/8330750 . Zhu, J. , Qi, T. , Ma, D. , & Chen, J. (2018). Limits of stability and stabilization of time-de- lay systems: A small-gain approach . Springer International Publishing . Zhu, M. , & Martínez, S. (2010). Discrete-time dynamic average consensus. Automat- ica, 46 (2), 322–329 . Zhu, M. , & Martínez, S. (2012). On distributed convex optimization under inequality and equality constraints. IEEE Transactions on Automatic Control, 57 (1), 151–164 . Zhu, M. , & Martínez, S. (2013). An approximate dual subgradient algorithm for mul- ti-agent non-convex optimization. IEEE Transactions on Automatic Control, 58 (6), 1534–1539 . Zhu, M. , & Martínez, S. (2015). Distributed optimization-based control of multi-agent networks in complex environments . Springer . Zhu, Y. , Yu, W. , & Wen, G. (2016). Distributed consensus strategy for economic power dispatch in a smart grid with communication time delays. In Proceed- ings of the 2016 IEEE international conference on industrial technology (ICIT) (pp. 1384–1389) . Zhu, Y. , Yu, W. , Wen, G. , Chen, G. , & Ren, W. (2018). Continuous-time distributed subgradient algorithm for convex optimization with general constraints. IEEE Transactions on Automatic Control, 64 (4), 1694–1701 . Please cite this article as: T. Yang, X. Yi and J. Wu et al., A survey of distributed optimization, Annual Reviews in Control, https://doi.org/ 10.1016/j.arcontrol.2019.05.006","libVersion":"0.3.2","langs":""}
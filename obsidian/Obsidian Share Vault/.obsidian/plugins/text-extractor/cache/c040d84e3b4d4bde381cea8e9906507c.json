{"path":"lit/lit_notes_OLD_PARTIAL/Idan19showFriendTellVote.pdf","text":"Show me your friends, and I will tell you whom you vote for: Predicting voting behavior in social networks Lihi Idan Department of Computer Science Yale University lihi.idan@yale.edu Joan Feigenbaum Department of Computer Science Yale University joan.feigenbaum@yale.edu Abstract—Increasing use of social media in campaigns raises the question of whether one can predict the voting behavior of social-network users who do not disclose their political preferences in their online proﬁles. Prior work on this task only considered users who generate politically oriented content or voluntarily disclose their political preferences online. We avoid this bias by using a novel Bayesian-network model that combines demographic, behavioral, and social features; we apply this novel approach to the 2016 U.S. Presidential election. Our model is highly extensible and facilitates the use of incomplete datasets. Furthermore, our work is the ﬁrst to apply a semi-supervised approach for this task: Using the EM algorithm, we combine labeled survey data with unlabeled Facebook data, thus obtaining larger datasets as well as addressing self-selection bias. Index Terms—Bayesian Networks, Social Media, U.S. Elections I. INTRODUCTION Political choices and voting decisions are considered by many people to be highly sensitive and private information that they are reluctant to reveal. Political campaigns, on the other hand, are investing heavily in voter targeting, focusing intently on social network platforms because of the micro-targeted advertising capabilities they provide. In this paper, we explore the following question: Can we predict the voting behavior of Facebook users from their public Facebook proﬁles? We present a novel approach for predicting the voting behavior of Facebook users using a Bayesian-network model that combines demographic, behavioral and social features. Although we focus on the 2016 U.S. Presidential election, our approach can be extended to any two-part system. This paper is the ﬁrst to use a Bayesian network model for political attribute inference. Furthermore, it is the ﬁrst to not only address, but also offer concrete solutions to the selection bias problem, combining a representative and heterogeneous datasets of both politically active and passive users, a semi-supervised training methodology and a diverse set of demographic, behavioral and social features. Finally, our model is trained to predict not only to whom the user will vote, but also whether she will vote at all, a task that has not been performed by any existing work on vote prediction. A. Related work There are several streams of research that investigate the predictive power of social media for political purposes. One stream of research concentrates on predicting the polit- ical orientation of an individual from various components of her social network proﬁle: Tweets’ content [21]; retweet graph [8]; following behavior [3]; degree of tweets and retweets [5], [25]; and “liking” politically oriented pages [4]. A more general approach, taken by [19], [26]–[28] aims at building a generic framework for latent attribute inference of social media users. Those works do not focus on political ori- entation as a stand-alone trait, but rather use it to demonstrate the functionality of their generic classiﬁcation system. Another stream of research focuses on election prediction from social network data. Such works usually take either a volume based approach or a sentiment analysis based approach and use it to predict the outcomes of various election systems in both two-party and multiparty settings [6], [15], [22], [23]. Although closely related to the lines of research discussed above, individual voting behavior differs in several respects. Individual voting behavior prediction aims at predicting an individual choice (unlike election prediction, that aims at capturing an aggregated measure) for a speciﬁc event. This event has a well-deﬁned end date and a well-deﬁned set of choices (labels) that represent the possible voting choices the individual has in a given election. In contrast, political orientation reﬂects a generic and subjective measure that does not have well deﬁned time boundaries; furthermore, the multitude of scales used for measuring political orientation, combined with the fact that any point on such scale may have different meanings to different individuals results in the lack of a well-deﬁned set of labels. In contrast to the work on political orientation and election prediction, relatively little work has focused on individual voting behavior prediction: Gayo-Avello [9] tried to infer the votes of Twitter users in the 2008 U.S. elections by applying multiple sentiment analysis methods. Bachhuber et al. [2] examined several approaches aimed at ﬁnding distinct clusters Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. ASONAM ’19, August 27-30, 2019, Vancouver, Canada © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6868-1/19/08/$15.00 https://doi.org/10.1145/3341161.3343676 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 816 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. of Twitter users based on linguistic properties of their tweets, creating language proﬁles for supporter groups in the 2016 US elections. Kristensen et al. [14] used Facebook to predict voter intentions in the 2015 Danish election, and examined users’ political like history to predict which party will they vote for. B. Shortcomings of prior research A signiﬁcant shortcoming of prior research is the use of biased datasets. Those datasets are composed of politically active social-network users, a minority which does not repre- sent the ordinary user population. Classiﬁers trained on such datasets will thus experience limited predictive accuracy when applied to ordinary users [7], who are less politically engaged yet constitute the majority of social media users [18]. We recognize three potential sources of bias: Platform: Twitter is the social network most commonly used for politically oriented data-mining research. However, Twitter is one of the least representative social networks [1], [17]: First, the usage statistics among American adults are quite low. Second, Twitter users are not demographically rep- resentative of the population; the resulting demographic bias is often ignored in research concerning political orientation inference and voting behavior prediction. Third, Twitter is con- sidered the most “political” social network, attracting a user population with unusually high political awareness. Twitter users’ datasets are therefore highly likely to be politically and demographically unrepresentative of the general population. Features: The vast majority of prior work relies solely on the analysis of user-generated content or politically oriented activities in social networks. This approach introduces sub- stantial selection bias because the only social network users who appear in such datasets are those who engage in politically overt activities, and only a minority of all users do so [7], [14]. In other words, those approaches yield high-accuracy results, but those results are limited to a small fraction of social media users. Indeed, Table II shows that less than 27% of the users in our dataset performed a public activity related to Trump or Clinton before the 2016 election date; that is, more than 73% of the users for whom solely relying on user-generated content would yield results which are essentially no better than those we would have received using a random classiﬁer. Labels: Previous works have used several methodologies for extracting labels from social-network accounts, relying on online behaviors such as explicitly stating political orientation online [9], [19], [27]; including politically related content in tweets [2], [8], [25]; supporting partisan causes [21] or following candidates [24]. Subsequently, only users for whom such label exists are included in the datasets, leading to the creation of biased datasets, composed entirely of individuals who voluntarily disclose their political preference online. This methodology introduces self-selection bias into the ﬁnal re- sults, as users who choose to disclose their political afﬁliations constitute a minority of social media users [20]. C. Our contribution The goal of the models presented in this paper is the following: given a Facebook user, predict the individual voting behavior of that user based on the public portions of the user’s Facebook proﬁle. Our main contributions are the following: Addressing sources of potential bias: we address platform- based bias by using Facebook as our social network platform. Facebook is known to be much more representative of the general population than Twitter and supports a richer proﬁle representation; we address features-based bias by going be- yond active content analysis and combining multiple types of information about the user: static (demographic attributes of the user), dynamic (activities performed by the user and their frequency patterns) and social (information we can learn about the user from her social network links). Finally, we address label-based bias by applying a semi-supervised approach that uses a combination of labeled data, where labels are obtained from surveys, and unlabeled data, composed of users who didn’t participate in the survey though were offered to. Incorporating non-voters: Existing works implicitly as- sume that all users indeed vote. That is, the only users that are included in the datasets are users who voted for one of the candidates in a given election. This assumption is not only an unrealistic oversimpliﬁcation of election systems but also prevents us from identifying important subpopulations, whose speciﬁc vote is inconsistent with their general partisanship such as strong partisans who decide not to vote in a given election. Our key assumption is that a voting decision is inﬂuenced by two types of components: a “static” component that is determined by the general party identiﬁcation of the individual, and “dynamic” components that are determined by speciﬁc characteristics of the candidates and include both factual policy-related material and subjective perception of the candidate. Each component can be learnt from different elements in a social-network proﬁle, and its inﬂuence on the individual’s voting behavior is combined with the rest of the components using a Bayesian network model. Using a novel Bayesian network model: Bayesian-network (BN) classiﬁers offer important advantages that speciﬁcally ﬁt the nature of our problem as well as the nature of our data. One advantage of BNs is their ability to support the com- bination of data and prior knowledge about the problem’s domain. This allows us to use existing research and statistics for encoding some of the model’s parameters; these include interactions between demographic attributes, and interactions between demographic attributes and party identiﬁcation. An- other advantage of BNs is that they handle missing data very well within both training and evidence data. This is particularly important when dealing with social network datasets, which are often incomplete. Indeed, datasets used in this work contain a large number of missing values, which correspond to attributes that users have chosen not to include in their public Facebook proﬁle. Due to our use of BN, missing values in the evidence data need not be imputed but rather can be fed directly to the model. Furthermore, the probabilistic represen- tation combines naturally with the Expectation-Maximization (EM) algorithm, enabling our training data to include both missing values and latent variables. It is this trait of BNs that facilitates the use of both labeled and unlabeled training data. 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 817 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. II. METHODOLOGY AND DATASETS We designed and distributed a comprehensive survey which adopted purposeful sampling of eligible voters in the 2016 U.S. elections that are also Facebook users. We used the Qualtrics survey platform to create and host the survey. Survey construction: The survey included questions about the user’s demographics, her Facebook activity, her Facebook friends’ demographics and activity, and her political opinions including her party identiﬁcation and her vote in the 2016 presidential election. All survey data was anonymized after collection. We informed participants that their responses would be used for academic research. We implemented several methods for identifying and ex- cluding data from participants who answered unreliably. First, we eliminated responses from participants who took the survey more than once. We took a conservative approach and dis- carded responses that came from the same IP address. Second, to ensure the eligibility of participants, they had to complete a screening questionnaire before taking the survey. The ques- tionnaire was carefully designed, in order to prevent respon- dents from inferring the qualiﬁcations we were looking for and taking the survey without being an eligible participant. We avoided yes/no questions regarding the qualiﬁcation needed and used multiway questions instead, as yes/no questions tend to insinuate the “correct” answer in order to pass the screening questionnaire. In addition, we disguised the real screening questions among other dummy questions. For example: instead of directly asking: “do you have a Facebook account”? we asked a series of identical multiway questions about news consumption habits from each of the media platforms. For example, the question “how often do you consume news via TV?” had the following ﬁve answers: More than 5 times a day; 3 times a day; once daily; Never; I do not have a TV. In the same question that dealt with Facebook, the last answer was replaced by “I do not have a Facebook account”. Third, we included control questions to ensure that the respondents were providing reliable data; those were fairly straightforward questions which asked the same question multiple times, in different parts of the survey, and using a slightly different terminology. We excluded participants who failed in one or more of the control questions. Lastly, the survey’s name did not include words such as ’politics’ or ’social media’ so as to not expose the qualiﬁcations needed, as well as not to oversample from the more politically engaged population. Datasets: In this paper, we make use of three datasets: D1 consists of 1638 survey responses collected via Ama- zon’s Mechanical Turk (MTurk). Labels for this dataset are obtained through the survey. D2 consists of 841 Facebook proﬁles and corresponding survey responses collected via both Facebook and Qualtrics. Labels for this dataset are obtained through the survey. D3 consists of 500 Facebook proﬁles collected via Face- book. Those individuals did not participate in our survey, though were offered to. The only information that was col- lected on this dataset is public — details that the users have TABLE I: Descriptive statistics for D1 and D2. D1 was stratiﬁed by Party Iden- tiﬁcation, resulting in a balanced representation of Republicans, Democrats, and Independents within the training set. Attribute Metric D2 D1 Census Gender Female 48.6% 50.6% 50.8% Male 51.4% 49.4% 49.2% Education Level Post graduate degree 52% 54.5% 62.6% College degree 35% 31.2% 26.2% No college degree 13% 14.3% 11% Marital Status Married 48.9% 49.5% 48.3% Unmarried 51.1% 50.5% 51.7% Race Caucasian 66.6% 71.4% 62.8% African American 10.7% 9.77% 12.2% Hispanic 15.6% 12.1% 16.9% Other 7.13% 6.72% 8.1% State Of Residence Solid republican 28.3% 23.6% 24% Lean republican 8.56% 10.5% 11.7% Competitive 38% 39.2% 40.5% Lean democratic 13.1% 14.6% 14.8% Solid democratic 12% 11.9% 9% Age 20-33 years 38.9% 53.7% 19.7% 34-49 30.2% 26% 20.5% 50+ years 30.9% 20.3% 44% Income <$25k 36.9% 38.5% 38.9% $25k-$60k 47.6% 46.8% 42.2% $60k+ 15.5% 14.8% 18.9% Party Identiﬁcation Republican 33.9% 33.3% Democrat 40% 33.3% Independent 26.2% 33.3% Vote Trump 30.8% 32.1% Clinton 36.7% 37.3% Other 32.5% 30.6% published in their public Facebook proﬁle. For that reason, D3 is unlabeled and contains a large number of missing features. The use of D1, D2, D3 was motivated by two considerations: Obtaining larger datasets Recruiting a large number of respondents via MTurk (D1) was a relatively quick and easy task in comparison to a direct Facebook recruitment process (D2). D1, however, was not fully representative of our target population, as it is limited to individuals who are registered to MTurk. Furthermore, tasks published on MTurk cannot ask for personal data, such as name or Facebook proﬁle. In order to account for those limitations, we used D1 as our training dataset and D2 as our test dataset. A second strategy for obtaining larger datasets is the use of unlabeled data, D3, for augmenting our training data. Such data can be easily collected as the inclusion of a Facebook account in an unlabeled dataset does not require the account holder to participate in the survey. Addressing selection bias we consider three types of bias: Visibility bias: Datasets used in prior research were often created by systematically excluding users who did not publish a certain attribute — one that is crucial to the model’s per- formance, such as politically oriented activity or self-reported label — in their public social network proﬁle. Acknowledging that such approach yield datasets that are not representative of the Facebook user population, our datasets were constructed so that the absence of a feature or a label from a user’s public proﬁle does not affect the user’s chance of being selected; as is evident from Table II, neither feature was publicly disclosed by more than 46% of the users in D2, and only 4.4% of them published their vote in the 2016 presidential election. Self-selection bias: D1 represents a population of individuals 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 818 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. Fig. 1: The BASIC model who have actively picked our survey from the list of tasks published on MTurk. D2 represent a population that was invited by us to participate in the survey and agreed to do so. Those two groups selected to participate in our survey. In order to address this selection bias, we created a third dataset, D3, composed of Facebook users who, though were asked to, did not participate in our survey. While we could not use D3 for testing, as it is unlabeled, we did combine it in our training set in order to further generalize our model. Demographic bias: one advantage of the use of MTurk for data collection is the ability to obtain data from demographi- cally diverse groups; although MTurk’s population is not fully representative of the US population, the ease of data collection via MTurk enabled us to reach crowds that are diverse across the primary demographic dimensions used in this work. Table I presents descriptive statistics of D1 and D2 and compares a subset of their demographics to the 2014 US census. As can be seen, each group is well represented in the datasets. External software: Throughout this paper we refer to speciﬁc subtasks that were performed using existing, readily available software. Examples include Genderize.io 1 and ethni- colr 2 for gender and race inference based on the user’s name; Python NLTK for sentiment analysis tasks; GeNle and pysmile for our BNs’ creation, parameter learning, and inference. III. THE BAYESIAN NETWORK The target node in our BN, “vote”, represents the voting behavior of an individual user. It may take one of three values: vote for candidate 0 (“c0”); vote for candidate 1 (“c1”); not vote at all or vote for a third party candidate (“Other”). In our setting, “candidate 0” represents a vote for Donald Trump; “candidate 1” represents a vote for Hillary Clinton; “Other” represents non voters or a vote for a third party candidate in the 2016 U.S. elections. The basic idea behind our BN models is to treat the vote node as both a cause and an effect. As such, it is inﬂuenced by a set of causes and causes a set of effects: Causes include the party identiﬁcation (PID) of the user and the voting behavior of the user’s network neighborhood (NET). Effects include the user’s politically oriented activities on Facebook (ACT). Each of the causes and effects belongs to a subnetwork that includes 1https://genderize.io/ 2https://ethnicolr.readthedocs.io/ a different type of observable variables: demographic attributes of the user (“static subnetwork”); types and patterns of Face- book activity performed by the user (“dynamic subnetwork”); and activities performed by the user’s network neighborhood (“social subnetwork”). Due to the BN’s structure, we are able to elicit different priors with varying levels of conﬁdence to the different subnetworks. Formally, given a target Facebook user ut, Let D = {D1 t ,D2 t , ...Dn t } be n demographic attributes of ut, A = {A 1 t ,A 2 t , ...A k t } be k attributes describing Facebook activities performed by ut and N = {N 1 t ,N 2 t , ...N j t } be j attributes describing ut’s network neighborhood. Given our target node, vote, we are interested in the label c that maximizes the following posterior probability: c =argmax v∈{c0,c1,Other} P (vote = v | P ID, ACT, N ET ) (1) Assuming that {D1 t ,D2 t , ...Dn t }, {A 1 t ,A 2 t , ...A k t } and {N 1 t ,N 2 t , ...N j t } are observable attributes, and using a bayesian approach, P (ACT |A 1 t ,A 2 t , ...A k t ),P (NET |N 1 t ,N 2 t , ...N j t ) are obtained from our datasets and a uniform prior, and P (PID|D1 t ,D2 t , ...Dn t ) is obtained from our datasets and survey-based, or census- based priors. Using the EM algorithm, we employed a two-stage learning methodology: ﬁrst, we trained a BN using the available labeled data (D1) and probabilistically labeled the unlabeled data; then, we trained a second BN using both the labeled data and a subset of the unlabeled data (D3) about which the ﬁrst BN was the most conﬁdent. This allowed us to both learn the parameters of the model’s latent variables and diversify our training set with a more representative, but unlabeled, data. Inference was done using a Junction tree based algorithm. Complexity-accuracy trade-off: the models presented in this paper were carefully designed so as to create compact models without damaging performance; if a certain edge did not substantially contribute to the model’s predictive performance, we did not include it in the network. For example, the income node in our SELF INFER model is, theoretically, inﬂuenced by many demographic attributes. However, we have found that the only edges that signiﬁcantly contribute to the overall accuracy were those connecting income with occupation, race and gender, and hence included only those edges in the network. BASIC: Two nodes, “party identiﬁcation” and “activity” are directly linked to the “vote” node. “Party identiﬁcation” covers the inﬂuence of an individual’s general party identiﬁcation on her actual vote. It is considered an unobservable attribute, and thus inferred from the user’s demographic attributes. The attributes which are included in the model met two criteria: attributes which are highly indicative of party identiﬁcation; observable attributes, in the sense that they are either offered by Facebook as an optional ﬁeld in a proﬁle or can be inferred from other public information, such as name. The following 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 819 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. attributes were included in our model: gender, age, race, state of residence, education level, marital status. We enhance the static subnetwork using prior information about the magnitude of the interactions between each demo- graphic attribute and the PID. Priors are based on multiple surveys conducted by Pew research center 3 and on statistics provided by the U.S. Census Bureau; priors refer to data that was collected at least six months before the elections and were incorporated into the model’s parameters using a Bayesian approach and a Dirichlet prior with varying conﬁdence factors based on the available prior information on a given parameter. As can be seen in Fig. 1, the static subnetwork is built in a conceptually “layered” fashion: The ﬁrst level represents as- cribed attributes; the second level represents acquired attributes; the ﬁnal level contains the PID node. In order to avoid a large CPT for the PID node, we used a parent-divorcing technique, introducing intermediate nodes (colored orange in Fig. 1,2) that serve as the “accumulators” of the PID given the subset of demographic attributes to which they are linked. The second node which is directly linked to the target node is the “activity” node, which addresses the more dynamic indicators of the user’s voting behavior and is inﬂuenced by the public Facebook activities that the user performs. In BASIC, we included three types of activities: writing a post; sharing an item; and “liking” a page that is positively or negatively associated with one of the candidates. Each of those activities is represented in the model using a separate observable node. The “activity” node aggregates the different combinations of activities into three states that represent the user’s overall activity: activity associated with supporting can- didate 0, activity associated with supporting candidate 1 and activity that is not associated with supporting either candidate (such as writing a positive post about both candidates). REVISED: Our second model, the REVISED model, reﬁnes the dynamic subnetwork (Fig. 2) in order to reﬂect several in- sights we obtained from reviewing the explanations respondents gave to different questions in the survey: Positive activity towards a candidate and negative ac- tivity towards her opponent are not equivalent: Although positive activities were almost always associated with a vote for the candidate who was the subject of the activity, negative activities towards a candidate were associated either with a vote for the candidate’s opponent, or with non voters. Passive users versus politically passive users: The respon- dents who answered that they did not perform any political activities on Facebook were asked to explain why they didn’t. A common explanation was an unwillingness to reveal political opinions on Facebook. It is important to note that those answers differed from the answers of respondents who reported that they rarely use Facebook, use Facebook without performing activities, or are simply not interested in politics. An inter- esting observation was that most users who said they did not want to expose their political opinions on Facebook identiﬁed themselves as either voted for Trump or did not vote at all. 3https://www.pewresearch.org/ TABLE II: Fraction of users (D2) whose public proﬁle contains the following attributes Attribute Fraction revealed Age 18.2% Educational attainment 34.8% Marital status 36% Occupation 32.7% State of residence 45.5% Wall content (full) 57.7% Wall content (partial) 72.5% Friends list 53.6% Activity (positive or negative) associated with either Clinton or Trump 26.6% Vote in the 2016 Presidential election 4.4% Considering other Facebook activities: Respondents pointed out that, although they have not performed an activity associated with one of the main candidates, they did perform an activity that was associated with a third-party candidate. Based on our observations, we created the REVISED model: First, the “activity” node was replaced with two nodes: “positive activity” and “negative activity”. In addition, the observed “post”, “share”, and “like” nodes are each replaced by two nodes, one for the positive form of the activity and one for the negative form of the activity. Note, that unlike the BASIC model in which there was no distinction between a positive activity towards candidate x and a negative activity towards her opponent (as both were mapped to the same “pro candidate x” state), here we distinguish between positive and negative activities, each separately inﬂuences the vote node. Second, we added a new binary node, labeled “Other activ- ity”, that represents various online activities that may indicate a vote for a third-party candidate. In the REVISED model, it is set to one only if there is a positive activity associated with a third party candidate in the user’s proﬁle. Third, we added a binary node, “activity level”, that rep- resents the general activity level of the user on Facebook. We wanted to distinguish those users who are not active on Facebook from those who are not politically active on Facebook; our second observation suggested that while the ﬁrst group is not particularly important for the inference process, the second group is strongly associated with users who voted for Trump (more generally, with the candidate who is considered less “socially acceptable”). We added four questions to the survey: one asked for the subjective estimation, on a ten- point scale, of the respondent’s activity volume on Facebook. The remaining three asked about the number of posts, shares, and likes that the user has recently performed. We aggregated those four measurements into one node, “activity level”, by discretizing each variable with a median split and setting the “activity-level” value to the four variables’ logical AND. IV. HANDLING INCOMPLETE DATA The models discussed thus far has considered a simpliﬁed setting, in which no evidence data is missing. While social- networking services can potentially obtain such complete in- formation about their users (by accessing proﬁle items in all visibility levels or collecting demographic data when the user creates an account), third-party services can not, because they are limited to the public portion of social-network proﬁles that 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 820 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. Fig. 2: Additions to the BASIC model of Figure 1 needed to obtain the REVISED (right) and SELF-INFER (left) models often lacks some of the attributes that comprise our models’ features. Thus, it is essential to understand how well does the model perform in the presence of missing data. MISSING: In order to understand to what extent our model’s performance is inﬂuenced by missing data, we tested the REVISED model on another dataset, D′2, which is identical to D2 except for the following case: if the value of an attribute was not publicly shared in a user’s Facebook proﬁle, we deleted the attribute’s value from the user’s record in D′2 and treated it as “missing evidence”. In other words, D′2 includes only those attributes that the users in D2 have chosen to publish under a “visible to everyone” setting. For attributes that do not have their own ﬁeld in a Facebook proﬁle, such as race, we used external software and inferred them from other public information such as name; if the software misclassiﬁed the attribute (compared to the real label that was obtained from the survey), we deleted it from D′2. Results for the MISSING model suggested a signiﬁcant decrease in the overall accuracy, resulting in the need to design models that are speciﬁcally suited for missing evidence. The following models demonstrate several strategies that were used to handle the challenge of incomplete datasets. SELF INFER: This model enriches REVISED by exploring various correlations between the input variables for the purpose of reducing the inﬂuence of missing data on the overall results. Interactions among demographic attributes: Instead of treating demographic attributes uniformly, we partition them into two groups: ascribed attributes and acquired attributes. We can then make use of the possible dependency relations between the two groups. This idea is incorporated in the model by creating new edges between the two “acquired” attributes, education level and marital status, and the “ascribed” attributes which are known to be highly indicative of them: race, gender for the education level, and age, gender for the marital status. An important advantage of this approach is the ability to combine priors in the CPD of the demographic child nodes, which represent the known “inﬂuence” of the node’s parents on their descendant; the priors for this purpose were taken from census statistics and were incorporated into the model’s parameters using a Bayesian approach with a Dirichlet prior. Hidden demographic attributes: Demographic attributes that were included in the former models were both inﬂuential on the PID and observable, i.e., can be directly extracted from a Facebook proﬁle. A question that may arise is whether both of those traits must exist in a single attribute. The key idea is that while some observable attributes are not highly inﬂuential on the PID, they are inﬂuential on other unobservable attributes which do have a high inﬂuence on the PID. Using the chain of observable attribute → unobservable attribute → PID we can combine such unobservable nodes in our model as well. We consider one such pair: occupation and personal income. While income is not a “potentially observable” attribute on Facebook, it is highly indicative of the individual’s PID. Oc- cupation, on the other hand, is not considered very indicative of PID but can be considered as an observable attribute. In the model shown in Fig. 2, we incorporate this idea, introducing two new nodes: an observed occupation node and a hidden income node. The latter is fed by three observed nodes: occupation, race and gender, and feeding the PID node. The occupation node’s states represents the twenty main occupation categories according to the standard SOC (Standard Occupa- tional Classiﬁcation) system, to which we added two additional states: “student” and “retired”. V. COMBINING LINK INFORMATION:THE FULL MODEL Homophily is the tendency of individuals to link to others who are similar to them. In a social network setting, this princi- ple implies that the network neighborhood of an individual may reveal a signiﬁcant amount of information about the individual. In the FULL model, we adopt a ﬁne-grained interpretation of the homophily principle in order to both enrich the static subnetwork and create the social subnetwork. A common BN representation of social network ties [12], [13] relies on a user-based granularity where each node corre- sponds to a user within the target user’s (ut) neighborhood; in such representation, dependency relations (and, consequently, similarity relations) can only be established between users. Thus, such representation implicitly assumes that if two users share a tie, all their traits are similar. As demonstrated by [16], this assumption is false: It is known that some attributes experience a more homophilous nature than others. For some, similarity does not induce homophily at all. In order to facilitate a more realistic representation of social network ties, our FULL model relies on an attribute-based granularity, thus reﬂecting a more “ﬁne-grained” homophily which may vary across different attributes. To achieve that, we decompose a user into the different attributes that constitute her social network proﬁle by associating nodes with pairs of <user, attribute>, and dependency relations with links between attributes of users. Using this ﬁne-grained representation, de- pendency relations can be established between a single attribute of ut and the same attribute of each of her neighbors only if the attribute is known to be highly homophilous. The above idea is implemented in the model using a small, ﬁxed number of “aggregator” nodes, thus avoiding the overhead of dynamically allocating a separate node for each neighbor of ut. For a given attribute, those nodes aggregate the magnitude of each of the attribute’s states within ut’s neighborhood: let at be an attribute of ut that we want to infer from ut’s neighbors, and has q(at) states, a1 t ..aq(at) t . In the BN, we allocate q(at) 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 821 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. TABLE III: Comparison of the overall classiﬁcation accuracy using different training conﬁgurations. TEST → D2 D′2 TRAIN ↓ BASELINE BASIC REVISED MISSING SELF INFER FULL 10-fold CV .382 .68 .745 .661 .718 .825 D1 .382 .726 .76 .67 .712 — D1+D3 .382 .738 .782 .695 .755 — aggregator nodes, parents of at, where the ith node represent the magnitude of the subset of ut’s neighbors for whom the value of at equals the state ai t. A. Enriching the static subnetwork We use the homophily principle to infer missing demo- graphic traits of ut from the Facebook proﬁle of ut’s neighbors. FULL: In the FULL model, we focused on a particular subset of ties within ut’s network neighborhood. The subset is composed of neighbors who are ”close friends” of ut, where a close friend is deﬁned as a friend who has reacted to a recent post on ut’s wall. Using this approach, we only consider ties that are both intense and recent, as the stronger the tie connecting two individuals, the more similar they are [11], [16]. Furthermore, such activity based metric is more predictive of tie strength than metrics based on network structure or social distance, incorporating both intimacy and intensity factors [10], [11]. The major advantage of this approach is its simplicity and practicality, as it does not require access to the full list of friends, but only to ut’s wall. Furthermore, such ties can be identiﬁed and accessed directly from ut’s proﬁle. We examined two attributes that are known to be highly homophilous: state of residence (SOR) and age. We elaborate on the CPD design of the state of residence node (VSOR). The CPD of the age node was built in a similar process. The inﬂuence of ut’s neighbors’ SOR on ut’s own SOR is modeled using ﬁve aggregator nodes that represent the ﬁve states that the SOR variable can take in the BN (see Table I), {SORi}1≤i≤5. The jth aggregator node quantiﬁes the portion of ut’s close friends for whom VSOR=SORj. Each of the aggregator nodes was discretized to reﬂect the rank of the state it represents among VSOR’s states and was linked to VSOR as its parent using a memory-efﬁcient multinomial logistic CPD. B. Creating the social subnetwork The inﬂuence of the target user’s neighborhood on her voting behavior has been considered thus far only indirectly, through the static subnetwork. A natural extension is to treat the vote node as a stand-alone trait; then, the homophily principle can be used directly to infer the voting behavior of ut from the voting behavior of her neighborhood. However, because the voting behavior cannot be directly extracted from a Facebook proﬁle ﬁeld we cannot use exact counts as we did for demographic attributes. Instead, we use an approximation of the overall voting behavior of ut’s neighborhood, obtained via the following procedure: First, we mark the subset of ut’s neighborhood whose voting behavior can be estimated with high conﬁdence; these are users who have performed a social network activity that is associated with one of the candidates. Second, we determine the inﬂuence of ut’s neighborhood on her own voting behavior using only this marked subset, as well as the magnitude of this subset relative to ut’s neighborhood. FULL: A social subnetwork, built under the same principles used to enhance the static subnetwork, is added to the FULL model. The subnetwork contains four aggregator nodes, repre- senting the portion of ut’s close friends who performed either a positive or negative Facebook activity associated with candidate 0 or 1, and a “noise” node, representing the portion of ut’s close friends who did not perform any public Facebook activity associated with a candidate. In order to avoid continuous-valued nodes, the portions represented by each node were discretized into ﬁve intervals using equal-frequency discretization. All four nodes were linked into one of two intermediate nodes (’Positive neighborhood activity’ and ’Negative neighborhood activity’), while the ’noise’ node was linked to both. Those intermediate nodes aggregate the inﬂuence of the user’s neighborhood on her voting behavior and feed directly to the vote node. VI. EXPERIMENTAL RESULTS The BASELINE, BASIC and REVISED models were eval- uated on D2, which includes attributes in all visibility levels. The REVISED model was also evaluated on D′2, which only includes attributes from D2 published under a “visible to everyone” setting. SELF INFER and FULL, designed to handle missing evidence, were evaluated on D′2. Table IV provides a detailed summary of results. We report overall accuracy, Precision, Recall, and AUC for each class. For FULL, we report the 10-fold cross-validation results on D′2, because D1 does not include social features. However, in order to take advantage of the information in D1, we use the CPDs obtained for the static and dynamic subnetwork in previous models as high-conﬁdence priors for FULL. Inspired by [19], we employ a simple baseline system (BASELINE) that classiﬁes all the users explicitly mentioning their vote in the 2016 election within one of their public posts. All other users are considered misses for the given class. Table III compares the overall accuracy of each model using different training conﬁgurations. The cross-validation results allow adequate comparison between the results of FULL in Table IV and the rest of the results. As can be seen, augmenting the labeled dataset, D1, with unlabeled data, D3, indeed improves the classiﬁcation accuracy. Furthermore, this increase becomes more signiﬁcant as the number of missing values within the test set increases. BASELINE’s low overall accuracy probably results from the fact that very few users have publicly disclosed their voting intention. Results for the REVISED model demonstrate an improvement over the BASIC model, especially for the “other” class. This improvement suggests that the ﬁner-grained model- ing of the dynamic subnetwork allows the classiﬁer to capture additional information which uniquely characterizes the “other” class, thus better separating it from the other two classes. Results also show that the MISSING model underperforms the REVISED model. However, the differences are not as sharp as we expected considering the large number of missing values. 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 822 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. FULL was the best model, as evidenced by its high accuracy score (82.5%). Its use of social features boosts both precision and recall, but the SELF INFER model, which only includes demographic and behavioral features, achieves a decent score (75.5%) as well. This demonstrates the fact that using carefully designed models combined with complementary sources of information about the user yields solid predictions even when both training and testing datasets are incomplete. Most determinant features: We performed a sensitiv- ity analysis to assess the impact of the various models’ features on our target variable, vote. We elaborate here on the two most determinant features for each state of the vote node. The most inﬂuential feature for vote=Trump was race=African American, which decreased the pos- terior probability of vote=Trump by 19%, followed by positive neighborhood activity Trump=very high, which in- creased the posterior probability of vote=Trump by 13%. For vote=Clinton, the two most inﬂuential features were posi- tive post=Clinton and education level=post graduate degree; they increased the posterior probability of vote=Clinton by 22% and 15%, respectively. The two most inﬂuential features on vote=Other were negative activity=both candidates and noise=very high. They increased the posterior probability of vote=Other by 15% and 9%, respectively. Comparison with Existing Results: Unlike political ori- entation, prediction of individual voting behavior has not been intensively studied. Exceptions include [2], [9], and [14], achieving an overall accuracy of 63%, 78.8%, 70.8% respectively. However, datasets used in all three works were artiﬁcially limited to include only politically active users: users who performed politically oriented tweets [2], hashtags [9] or likes [14]. The last is highlighted because the accuracy of political-orientation classiﬁers is heavily dependent on users’ political-engagement level. For example, [7] showed that meth- ods for inferring the political orientation of social-network users previously claimed to achieve greater than 90% accuracy on politically active users, actually achieve barely 65% accuracy when applied to “politically modest” users. The prior work most relevant to ours is that of Kristensen et al. [14], which considers ﬁve models. We could not use all ﬁve as baselines for our work, because some use features that can not be directly extracted from a Facebook proﬁle (such as the user’s opinion towards politically dividing issues, information which [14] obtains from surveys). In contrast, our main principle in this work is to only use features that can be extracted from a Facebook proﬁle directly, without requiring the user to participate actively in the data-collection process (our survey is used for validation purposes only). We were able to apply model 2, which uses the user’s single most recent like that is associated with a party or politician’s page; model 3, which uses the number of such likes over the past two years; and their ﬁnal model, which combines the features from model 3 with all the features from the survey that served as their baseline model. We selected from their ﬁnal model only those features that can be extracted from a Facebook proﬁle directly: gender, age, geography and education. All models were implemented TABLE IV: Detailed results for the Bayesian network models presented in this paper Model Class Acc. Prec. Rec. AUC BASIC Trump .77 .76 .91 .738 Clinton .72 .89 .91 Other .72 .53 .78 REVISED Trump .73 .85 .94 .782 Clinton .79 .83 .92 Other .83 .64 .84 MISSING Trump .63 .84 .88 .695 Clinton .75 .7 .85 Other .72 .53 .78 SELF INFER Trump .69 .87 .91 .755 Clinton .81 .73 .88 Other .76 .65 .82 FULL Trump .78 .88 .94 .825 Clinton .83 .86 .93 Other .85 .72 .91 using Python’s scikit-learn library. Applying model 2 to our dataset (Using D1 and D2as training and test sets) achieved an overall accuracy of 38.6%, a low score both compared to the result achieved in [14] (43.9%) and to our BN models’ results. Applying model 3 to our dataset achieved an overall accuracy of 40.4%. It is interesting to see that there is not much improvement in the accuracy compared to model 2, while the difference between the two models in [14] is more signiﬁcant, with model 3 achieving 60.9%. Finally, applying their ﬁnal model to our dataset achieved an overall accuracy of 60%; we can see a clear increase in the accuracy compared to models 2 and 3; however, it still underperforms [14]’s ﬁnal results (70.8%). This results from the fact that [14] both artiﬁcially limit their datasets to users who performed a “political like” (only 19% from our dataset) and use features that can not be directly extracted from a user’s proﬁle such as opinion about politically dividing issues. As evidenced by Table IV, [14]’s ﬁnal model signiﬁcantly underperforms all our BN models when applied to our datasets. This highlights the added value that BNs have for this speciﬁc task compared to a regression model, and the importance of the other features that are not included in [14]’s ﬁnal model but are included in our BN models. Apart from [14], no other existing work could serve as a proper baseline for our work, primarily because their objective is different from ours; Unlike works such as [15], [22], we are not trying to forecast the general outcome of an election. Unlike works such as [7], [8], [19], [27], we are not trying to predict a general political orientation of an individual. Therefore, a direct comparison of our results and theirs is meaningless. For election forecasting, such comparison is impossible since the evaluation metrics are different: while we use an overall accuracy score, election prediction papers use MSE and com- pare their forecasting results to national surveys. Furthermore, the vast majority of political orientation inference papers have dealt with Twitter; thus, their models heavily rely on Twitter- speciﬁc features, making it impossible to apply those models 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 823 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply. directly to our datasets. However, in order to gain further insights about how our BN compares to other models used in previous work, we chose two other classiﬁers commonly used in previous work on political orientation inference: Support Vector Machines (SVM) and Boosted Decision Trees (BDT), as well as simpliﬁed Multinomial Naive Bayes (MNB) classiﬁer, and tested the performance of each classiﬁer when applied to our datasets and fed with the features of each of our BN models. Hyperparameters (cost and γ for the RBF SVM; number of trees and learning rate for BDT) were chosen using a random- ized grid search with 5-fold cross-validation. All three models were implemented using Python’s scikit-learn library. As seen in Fig. 3, both MNB and SVM considerably underperform the BN on all ﬁve conﬁgurations. On the other hand, the BDT classiﬁer performs on par with the BN; it slightly outperforms the BN on the REVISED model but underperforms the BN on the MISSING, SELF-INFER and FULL models; that is, when the test set contains missing evidence. VII. CONCLUSION In this paper, we presented a novel approach to predicting the voting behavior of Facebook users based on a Bayesian- network model that combines diverse yet complementary types of information about the user. In contrast to previous works, we made use of datasets of ordinary Facebook users, thus avoiding the bias entailed in cherry-picked datasets, limited to politically active users. Using a semi-supervised methodology, we applied our model to the case of the 2016 U.S. elections, achieving promising results despite large amounts of missing data. Interesting avenues for future research include augmenting the model with additional behavioral and interest based traits, combining more complex measures of the network neighbor- hood, and adding temporal features that capture how various attributes change over time. The latter is particularly useful for politically-oriented applications such as identifying swing voters by examining whether the political opinions of a user are consistent over time, or measuring the extent to which a speciﬁc event inﬂuenced the user’s voting intentions. ACKNOWLEDGMENT The ﬁrst author was supported in part by US National Science Foundation grants CNS-1407454 and CNS-1409599 and William and Flora Hewlett Foundation grant 2016-3834. The second author was supported in part by US National Science Foundation grants CNS-1407454 and CNS-1409599 and US Ofﬁce of Naval Research grant N00014-18-1-2743. REFERENCES [1] “https://www.pewinternet.org/2018/03/01/social-media-use-in-2018/.” [2] J. Bachhuber et al., “A linguistic analysis of US twitter users,” in Designing Networks for Innovation and Improvisation, 2016. [3] P. Barber´a, “Birds of the same feather tweet together: Bayesian ideal point estimation using twitter data,” Political Analysis, vol. 23, 2015. [4] R. Bond and S. Messing, “Quantifying social media’s political space: Estimating ideology from public revealed preferences on facebook,” American Political Science Review, vol. 109, pp. 62–78, 02 2015. [5] A. Boutet, H. Kim, and E. Yoneki, “What’s in your tweets? I know who you supported in the UK 2010 general election,” in ICWSM, 2012. BASIC REVISED MISSING SELF INFER FULL 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 SVM MNB BN BDT Fig. 3: Overall classiﬁcation accuracy among different classiﬁers [6] M. C. MacWilliams, “Forecasting congressional elections using face- book data,” PS: Political Science & Politics, vol. 48, pp. 579–583, 2015. [7] R. Cohen and D. Ruths, “Classifying political orientation on twitter: It’s not easy!” in ICWSM, 2013. [8] M. Conover, B. Gonc¸alves, J. Ratkiewicz, and A. Flammini, “Predicting the political alignment of twitter users,” in IEEE SocialCom, 2011. [9] D. Gayo-Avello, “Don’t turn social media into another ’literary digest’ poll,” Commun. ACM, vol. 54, no. 10, pp. 121–128, 2011. [10] E. Gilbert and K. Karahalios, “Predicting tie strength with social media,” in CHI, 2009. [11] M. Granovetter, “The strength of weak ties: A network theory revisited,” Sociological Theory, vol. 1, pp. 201–233, 1983. [12] J. He, W. W. Chu, and Z. Liu, “Inferring privacy information from social networks,” in ISI, 2006. [13] J. Jia, B. Wang, and L. Zhang, “Attriinfer: Inferring user attributes in online social networks using markov random ﬁelds,” in WWW, 2017. [14] J. Kristensen et al., “Parsimonious data: How a single facebook like predicts voting behavior in multiparty systems,” PLOS ONE, 2017. [15] D. A. Linzer, “Dynamic bayesian forecasting of presidential elections in the states,” Journal of the American Statistical Association, vol. 108, pp. 124–134, 2013. [16] M. McPherson, L. Smith-Lovin, and J. M. Cook, “Birds of a feather: Homophily in social networks,” Annual Review of Sociology, 2001. [17] A. Mislove, S. Lehmann, Y. Ahn, J. Onnela, and J. N. Rosenquist, “Understanding the demographics of twitter users,” in ICWSM 2011. [18] E. Mustafaraj, S. Finn, C. Whitlock, and P. T. Metaxas, “Vocal minority versus silent majority: Discovering the opionions of the long tail,” in IEEE SocialCom, 2011. [19] M. Pennacchiotti and A. Popescu, “A machine learning approach to twitter user classiﬁcation,” in ICWSM, 2011. [20] A. Priante, D. Hiemstra, T. van den Broek, and A. Need, “# whoami in 160 characters? classifying social identities based on twitter proﬁle descriptions,” in NLP+CSS, 2016. [21] D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta, “Classifying latent user attributes in twitter,” in SMUC, 2010. [22] E. A. Stoltenberg, “Bayesian forecasting of election results in multiparty systems,” Master’s thesis, 2013. [23] A. Tumasjan, T. Sprenger, P. Sandner, and I. Welpe, “Predicting elections with twitter: What 140 characters reveal about political sentiment,” in ICWSM, 2010. [24] S. Volkova, G. Coppersmith, and B. V. Durme, “Inferring user political preferences from streaming communications,” in ACL, 2014. [25] F. M. F. Wong, C. W. Tan, S. Sen, and M. Chiang, “Quantifying political leaning from tweets and retweets,” in ICWSM, 2013. [26] W. Youyou, M. Kosinski, and D. Stillwell, “Computer-based personality judgments are more accurate than those made by humans,” Proceedings of the National Academy of Sciences, vol. 112, pp. 1036–1040, 2015. [27] F. Zamal, W. Liu, and D. Ruths, “Homophily and latent attribute inference: Inferring latent attributes of twitter users from neighbors,” in ICWSM, 2012. [28] E. Zheleva and L. Getoor, “To join or not to join: the illusion of privacy in social networks with mixed public and private proﬁles,” WWW, 2009. 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 824 Authorized licensed use limited to: Cornell University Library. Downloaded on August 23,2020 at 20:04:30 UTC from IEEE Xplore. Restrictions apply.","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources.backup/papers_to_add/Papers I'm Reviewing Right Now/SplineRgrsn/other spline papers/Cornillon08forcstPrincComp.pdf","text":"Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 www.elsevier.com/locate/csda Forecasting time series using principal component analysis with respect to instrumental variables P.-A. Cornillon a, W. Imam b, E. Matzner-LZber a,∗ aÉquipe de Statistique, IRMAR UMR 6625, Université Rennes 2, Av. G. Berger CS24307, Haute Bretagne, 35043 Rennes, France bHigher Institute for Demographic Studies and Researches, Damascus, Syria Received 16 September 2005; received in revised form 31 January 2007; accepted 11 June 2007 Available online 29 June 2007 Abstract Two new forecasting methods of time series are introduced. They are both based on a factorial analysis method called spline principal component analysis with respect to instrumental variables (spline PCAIV). The ﬁrst method is a straightforward application of spline PCAIV while the second one is an adaptation of spline PCAIV. In the modiﬁed version, the used criteria according to the unknown value that need to be predicted are differentiated. Those two forecasting methods are shown to be well adapted to time series. © 2007 Elsevier B.V. All rights reserved. Keywords: Additive spline; Forecasting; PCAIV; Time series 1. Introduction Forecasting future observations is probably the most important task of time series analysis and this fascinates statisticians and economists. Among recent papers, see for instance Heij et al. (2007), Koopman and Ooms (2006). The purpose of this paper is to introduce two new forecasting methods, both based on a factorial analysis method called spline principal component analysis with respect to instrumental variables (spline PCAIV). Spline PCAIV is a multivariate method introduced by Durand (1993). A classical approach for forecasting time series is postulating parametric models, estimating a few coefﬁcients and computing the forecasts. Let {Zt }1 ⩽ t ⩽ T denote a univariate time series process, the most popular model is the autoregressive model of order p Zt = p∑ i=1 \u0002iZt−i + \u0003t , (1) ∗ Corresponding author. E-mail address: eml@uhb.fr (E. Matzner-LZber). 0167-9473/$ - see front matter © 2007 Elsevier B.V. All rights reserved. doi:10.1016/j.csda.2007.06.017 1270 P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 which is a particular model of the well-known autoregressive moving average (ARMA) models. Model (1) represents the current state of Zt through its immediate p past values. This is done in a linear form where the intercept is excluded. We suppose E(Zt ) = 0 and, in practice the sample mean from the data is subtracted before ﬁtting. The limitation of that type of linear models is well known and inﬁnitely many forms of nonlinear models have been explored since the eighties. The development in nonparametric regression provides techniques for modelling time series through the following model: Zt = g(Zt−1,...,Zt−p) + \u0003t . However, when p is large, the nonparametric approach suffers from the “curse of dimensionality” and a natural sim- pliﬁcation is the nonlinear additive autoregressive models Zt = f1(Zt−1) + ··· + fp(Zt−p) + \u0003t . (2) Additive models are very useful for approximating the high-dimensional autoregressive function g(.) given above. They and their extensions have become one of the widely used nonparametric techniques. The functions fi could be estimated by several nonparametric methods: spline, kernel... . An excellent survey is given in Fan and Yao (2003). Note, however, that one can add a constant to a component and subtract that constant from another, thus the functions f1,...,fp are not identiﬁable. To prevent ambiguity, usual conditions are imposed E(fi(Z)) = 0,i = 1,...,p. with this constraints, E(Zt ) = 0. Therefore, we assume that the mean has already been removed from the series or we add an intercept to the model. Recall that following the seminal paper of Rao (1964), the goal of linear principal component analysis with respect to instrumental variables (PCAIV) is to explain a few response Y1,...,Yq by a linear model based on explanatory variables X1,...,Xp. The key idea is to use the same linear combinations of explanatory variables for each response variables. These linear combinations are chosen to be 2 × 2 orthogonal and to be ordered from the most explanatory to the least one. This classical presentation can be viewed as PCA of the projection of Y on the vector space spanned by X1,...,Xp. Another classical presentation of PCAIV, which leads to the same linear combination of explanatory variables, is given in Section 2.1. This deﬁnition of PCAIV is used by Durand (1993) to propose a nonlinear PCAIV using B-splines. A presentation of this nonlinear multivariate tool is given in Section 2.2. In 1999, Imam et al. used spline PCAIV to estimate the order p of a nonlinear autoregressive model and the functions fi in model (2). Following their work, two methods derived from spline PCAIV are proposed to forecast time series. The ﬁrst one is classical, the functions fi are estimated then the forecasts are computed. The second one is completely new, as we optimize a criteria according to the unknown value we want to forecast. Section 3 is devoted to the presentation of these forecasting methods. In Section 4, the forecasting methods are applied to two well-known real time series and the results of the forecast are compared with other classical forecasting methods. 2. Presentation of spline PCAIV 2.1. Linear PCAIV The purpose of PCAIV is to analyze linear relations between two sets of variables X and Y. Let X be a n × p matrix, representing p explanatory variables X1,...,Xp measured on the same n individuals. Let Y be a n × q matrix, representing q response variables Y1,...,Yq measured on the same n objects. The approach proposed by Escouﬁer (1987) introduces metrics to compute distances between, respectively, objects and variables. The variable space Rn is equipped with the metric D = diag(...,di,...) corresponding to the individual weight, in general D = 1 n In. The individual space corresponding to Y (respectively, to X)is R q (resp. R p) and is equipped with the metric Q (resp. R which is unknown). A ﬁrst triple (Y,Q,D) has to be explained by a second one (X,R,D). From now on, we consider that D = 1 n In and Q = Iq . Let us summarize the procedure in its most classical presentation. P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 1271 Deﬁnition 1. Let (Y, Iq , 1 n In) be a triple to be explained and (X, R, 1 n In) an explanatory triple with unknown R. PCAIV consists of two steps: (1) Find a metric ˆR on the objects space of X such as the discrepancy between YY ′ and XRX′ is minimum according to the Hilbert Schmidt norm, norm on the symmetric matrix space. ˆR = arg min R ∥YY ′ − XRX′∥2. (2) Perform principal component analysis (PCA) of the triple (X, ˆR, 1 n In). A formal solution of this minimization problem is given by ˆR =[X′X]+X′YY ′X[X′X]+, where A+ denotes the Moore–Penrose generalized inverse, see Escouﬁer (1987) for further details. Remark. Linear PCAIV leads to the same representation of observations as the PCA of (PXY, Iq , 1 n In) where PX is the orthogonal projector on the linear subspace of R n spanned by the columns of X. 2.2. Spline PCAIV The idea of spline PCAIV (Durand, 1993) is PCAIV of Y with respect to X(s) where X(s) is a transformation of X via B-splines. For each explanatory variable Xj , the user deﬁnes the degree of the spline and the position and number of knots leading to rj basis functions. For sake of simplicity, rj are chosen to be equal to r for j = 1,...,p.In practice, as in Durand (1993), we use three equally space knots and degree 2 spline. As these p basis are evaluated on the design, each element xij of the j th explanatory variable Xj , the corresponding spline basis is written as (Bj 1 (xij ), Bj 2 (xij ),...,Bj r (xij )). The nonlinear spline transformation of Xj is given by Xj (sj ) = ⎡ ⎢ ⎢ ⎣ ∑r l=1Bj l (x1j )sj l ... ∑r l=1Bj l (xnj )sj l ⎤ ⎥ ⎥ ⎦ =[Bj 1 | ... |Bj r ] ⎡ ⎢ ⎢ ⎣ sj l ... sj r ⎤ ⎥ ⎥ ⎦ = Bj (n×r)sj (r×1), Bj is a coding matrix of Xj and sj is the vector of spline coefﬁcients. In matrix notation, the transformation of X is X(n×p)(s) = B(n×pr)S(pr×p), where B =[B1| ... |Bp] and S = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ s1 1 0 .. 0 ... ... ... ... ... s1 r 0 .. 0 ... ... ... ... ... 00 ..sp 1 ... ... ... ... ... 00 ..sp r ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ . The matrix S is the unknown spline coefﬁcient matrix and s is the unknown spline coefﬁcient vector s = (s1′,...,sp′)′. With those notations, spline PCAIV can be deﬁned as follows: 1272 P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 Deﬁnition 2. Spline PCAIV consists of two steps: (1) Find a metric ˆR and a transformation ˆs such as the representation of the triple to be explained (Y, Iq , 1 n In) is the closest to the representation of the explanatory triple (X(s), R, 1 n In) ( ˆR, ˆs) = arg min (R,s) ∥YY ′ − X(s)RX′(s)∥2 = arg min (R,s) tr{[YY ′ − X(s)RX′(s)]2}. (3) (2) Perform PCA of the triple (X(ˆs), ˆR, 1 n In). A solution to this minimization problem (3), is obtained by setting the partial derivatives equal to zero, leading to the so-called “normal” equations. If s is ﬁxed, an explicit solution for R is given by ˆR =[X′(s)X(s)]+X′(s)Y Y ′X(s)[X′(s)X(s)]+. (4) If R is ﬁxed, no explicit solution for s is available, then minimization algorithms have to be used. Once s is computed, R is recomputed and so on. This iterative procedure was proposed by Durand (1993) (steepest descent on s) and modiﬁed by Imam and Durand (1997) (steepest descent on each sj alternatively). In both algorithms, the starting values s0 for s are the nodal values being the value such that X(s0) = X. Roughly speaking nodal values are computed as mean of knots. For an exact expression, see for example Schumaker (1981) or Durand (1993). The ﬁrst step of spline PCAIV is linear PCAIV since X(s0) equal X. We obtain the same representation of the individuals by performing the PCA of (X(ˆs), ˆR, 1 n In) or the PCA of (PX(ˆs)Y, Q, 1 n In) where PX(ˆs)Y is the projection of Y on the space spanned by the columns of X(ˆs). This is the second step of spline PCAIV and it can be written as the PCA of ( ˆYspl.acpvi,Iq , 1 n In) where ˆYspl.acpvi = PX(ˆs)Y = B ˆS( ˆS ′B′B ˆS)+ ˆS′B′Y . (5) This formula could be compared to the one obtained by spline regression on the same basis function ˆYspl.reg = PB Y = B(B′B)+B′Y . (6) It can be proved that ˆYspl.reg = ˆYspl.acpvi (Cornillon and Matzner-LZber, 2007). As a closed form exists for (6), one could wonder why using algorithms for solving (5)? The use of such algorithms allows to circumvent the curse of dimensionality inherent to the use of B-splines. However, in the same spirit of spline PCAIV, one can think about other algorithmic procedures. 3. Spline PCAIV and time series analysis In time series context, let Y be (ZT ,...,Zp+1) and let X be the p-lagged values of Zt ,so Y (respectively, X)isan n × 1 (respectively, n × p) matrix. In PCAIV words, the instrumental variables are the lagged variables. As Y is a vector, there is no need of doing the second step of spline PCAIV. Expanding (5) gives ˆyi = p∑ l=1 ˆ\u0004l ( r∑ k=1 ˆsl kBl k(xik) ) = ˆf1(xi1) + ··· + ˆfp(xip). (7) P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 1273 Applying this formula to the last p-observed values of the time series leads to the ﬁrst forecasting method ˆZT +1 = ˆf1(ZT ) + ··· + ˆfp(ZT −p+1). (8) From now on, this method is refereed to Direct. As we use nodal values as the starting point of the algorithm, the ﬁrst step of spline PCAIV is the linear PCAIV and in the context of time series gives an estimation of a linear autoregressive model. Then, step by step, the estimations become more and more nonlinear but still in an additive form. Let us imagine that the value ZT +1 is available (which it is not in reality), Y and X would have n + 1 lines: ̃Y = ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ ZT +1 ZT ... Zp+1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ , ̃X = ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ ZT ... ZT −p+1 ZT −1 ZT −p ... ... ... Zp ... Z1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ . We could conduct a spline PCAIV analysis of ̃Y with respect to ̃X to obtain estimates of (2) based on T − p + 1 observations instead of T − p. Unfortunately ZT +1 is unknown, but the spline PCAIV criterion could be written as ( ˆR, ˆs, ̂ZT +1) = arg min (R,s,ZT +1) tr((̃Y ̃Y ′ − ̃X(s)R ̃X′(s))(̃Y ̃Y ′ − ̃X(s)R ̃X′ (s))′) (9) Obviously the minimization is conducted over (R, s) and also the unknown value ZT +1. The partial derivative of (9) with respect to R gives the expression of the optimal metric, being a function of both ZT +1 and s R = (̃X′(s)̃X(s))+(̃X′(s)̃Y ̃Y ′ ̃X(s))(̃X′(s)̃X(s))+. Deriving the objective function (Eq. (9)) with respect to the spline coefﬁcient s, as in spline PCAIV, does not lead to a separable expression for s, thus the need of using an iterative algorithm. Finally setting the partial derivative of (9) with respect to ̂ZT +1 equal to 0 leads to an exact formula for ̂ZT +1. Proposition 1. The minimal value of the objective function g(R, s, ZT +1) =∥̃Y ̃Y ′ − ̃X(s)R ̃X′ (s)∥2, when (R, s) remain ﬁxed, this is equal to one real root of the polynomial of degree 3 p(y) = y3 + (Y ′Y − a11)y − Y ′A′ 12, (10) where a11 ∈ R and A′ 12 ∈ R T −p are such as: A = ̃X(s)R ̃X′ = ( a11 A12 A′ 12 A22 ) . 1274 P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 Using this proposition we can deﬁne the forecasting PCAIV denoted Fpcaiv via the following algorithm: Proposition 2. Indirect spline PCAIV forecast. Forecasting PCAIV Z(0) T +1 ⇐ median(Z1,...,ZT ) s(0) such that ̃X(s(0)) = ̃X i ⇐ 1 g(R(0),s(0),Z(0) T +1) ⇐−∞ g(R(1),s(1),Z(1) T +1) ⇐ 0 while |g(R(i),s(i),Z(i−1) T +1 ) − g(R(i−1),s(i−1),Z(i−1) T +1 )| > \u0005 or i< N do R(i) ⇐ (̃X′(s(i−1))̃X(s(i−1)))+(̃X′(s(i−1))̃Y (i−1)̃Y (i−1) ̃X(s(i−1))) (̃X′(s(i−1))̃X(s(i−1)))+ Z(i) T +1 ⇐ y ∈ R |p(y) = 0(see proposition 1) s(i) ⇐ s(i−1) − \u0006 \u0002g \u0002s (s(i−1)) (gradient descent see Durand (1993) or Imam and Durand (1997)) ̃Y (i)′ ⇐ (Y ′...Z(i) T +1) g(R(i),s(i),Z(i−1) T +1 ) ⇐∥̃Y (i)̃Y (i)′ − ̃X(s(i))R(i) ̃X′(s(i))∥2 i ⇐ i + 1 end do Remark. If at stage (i) three real roots are found for the polynomial p(y) = y3 + (Y ′Y − a11)y − Y ′A′ 12, it sufﬁces to choose the one which is closer to Z(i−1) T +1 . But usually because n is large, there is only one real root as shown hereafter. The polynomial p(y) has only one real root if (Y ′Y −a11) is positive. Remember that spline PCAIV tries to approximate the scalar product YY ′ by X(s)RX(s)′. Thus a11 is close to the squared norm of YT and Y ′Y is the sum of n terms of the same order as a11 that is a quantity of order n. The probability of Y ′Y − a11 in order to be positive increases with n and also with the stage (i). 4. Examples In the following section, those methods are applied to two real time series, both classical but with different features. The ﬁrst one is short and regular and the second one have much more observations, is irregular and have explanatory variables. In the two following examples, we use three equally space knots and degree 2 spline as done in Durand (1993). We use time series directly without subtracting the sample mean. As presented in the introduction our model includes an intercept. 4.1. Choice of the model As shown in Imam et al. (1999), spline PCAIV is used to estimate the order p of the nonlinear autoregressive model by proceeding as follows: (1) Choose P big enough to incorporate possible seasonal component. (2) Build a n × 1 matrix Y and a n × P matrix X. (3) Perform a spline PCAIV. (4) Compute the residual variance of order j, for all j less than P ˆ\u00072 1,...,j = 1 n − 1 T∑ t=P +1 (Zt − ˆf1(Zt−1) − ··· − ˆfj (Zt−j ))2. (5) Plot ˆ\u00072 1,...,j against j and choose p as the value for which ˆ\u00072 1,...,j j> p stabilizes. P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 1275 0 20 40 60 80 100 120 1.5 2 2.5 3 3.5 4 Fig. 1. Time plot of the log10 lynx data (114 observations). At that point, the last p lagged values have been selected and we have two matrices: a n × 1 matrix Y and a n × p matrix X. If the aim is to select the most explicative variable in order to get a smaller model, do (1) Compute the residual variance for the p lagged variables ˆ\u00072 j = 1 n − 1 T∑ t=p+1 (Zt − ˆfj (Zt−j ))2 and choose the most explicative variable denoted i1. (2) Compute then ˆ\u0007 2 i1,j = 1 n − 1 T∑ t=p+1(Zt − ˆfi1(Zt−i1 ) − ˆfj (Zt−j )) 2. (3) Stop when the estimated variance stabilizes with the selected variables i1,... with a maximum number of variable being ˆp ⩽p. 4.2. Canadian lynx data We study the classic Canadian lynx data consisting of the annual record of the numbers of Canadian lynx trapped in the Mackenzie River district of North west Canada for the period 1821–1934. This time series has attained the status of “benchmark data set”. Tong (1990, chapter 7), exposed in great details the historical background of the series and then presenting a complete analysis of the data. Recently, Lin and Pourahmadi (1998) conﬁne their attention to the class of additive and projection pursuit regression model and rely on the estimated prediction error variance to compare the prediction performance of various (non-)linear models. The series consists in 114 observations. The last 14 observations are put aside and are used to evaluate the forecasting performance of the methods proposed in the previous section. As Lin and Pourahmadi (1998) we will compute one step ahead forecast and multi-step ahead forecast. We use the common log10-transformation and denote zt this transformation. Fig. 1 gives the plot of the series. In a ﬁrst step, we select P = 15 and conduct spline PCAIV. We select the two ﬁrst variables assuming the model zt = f1(zt−1) + f2(zt−2) + \u0003t . A new spline PCAIV is conducted and estimate the two functions via Eq. (7). The estimations are plotted in Fig. 2. The relative performance of the predictors are assessed by computing s1 = (S1/14)1/2, where S1 is the sum of squares of the one step and multi-step prediction errors for t = 101,..., 114 and s2 = S2/14, where S2 is the sum of the absolute percentage error of the one step and multi-step prediction errors for t = 101,..., 114. The forecasts for the lynx data set are given in Table 1. Our predictive methods fairly compare with the results obtained by Lin and Pourahmadi (1998) for their best model (PAD(1,2)). As shown in Table 1, the direct and the 1276 P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 2 2.5 3 3.5 3 4 5 2 2.5 3 3.5 -2.2 -1.8 -1.4 -1 Fig. 2. Plots of functions: (a) ̂f1(zt−1) and (b) ̂f2(zt−2) ﬁtted to the log10 lynx data using spline PCAIV. Table 1 One-step and multi-step-ahead forecasts for the lynx data set Observed Direct Direct Fpcaiv Fpcaiv PAD(1,2) PAD(1,2) One-step Multi-step One-step Multi-step One-step Multi-step Z101 = 2.36 2.35 2.35 2.34 2.43 2.33 2.33 Z102 = 2.60 2.70 2.69 2.67 2.66 2.73 2.68 Z103 = 3.05 2.85 2.98 2.84 2.92 2.91 3.04 Z104 = 3.39 3.37 3.24 3.35 3.16 3.39 3.32 Z105 = 3.55 3.57 3.43 3.55 3.35 3.51 3.43 Z106 = 3.47 3.40 3.42 3.38 3.38 3.44 3.33 Z107 = 3.19 3.05 3.10 3.03 3.15 3.15 3.09 Z108 = 2.72 2.74 2.69 2.73 2.78 2.85 2.86 Z109 = 2.69 2.52 2.58 2.50 2.63 2.49 2.76 Z110 = 2.82 2.85 2.72 2.84 2.74 2.81 2.82 Z111 = 3.00 3.04 2.93 3.02 2.93 3.02 2.96 Z112 = 3.20 3.22 3.17 3.20 3.14 3.17 3.12 Z113 = 3.42 3.36 3.38 3.35 3.33 3.30 3.22 Z114 = 3.53 3.46 3.43 3.44 3.36 3.43 3.23 s1 0.091 0.084 0.098 0.114 0.093 0.125 s2 2.27 2.45 2.35 3.02 2.47 3.13 forecasting PCAIV have both good sums of the absolute percentage error (s2), better than the PAD(1,2) model. For the standard error of forecast, s1, direct PCAIV is still better than the other methods. Forecasting PCAIV is penalized by a bad forecast for the third highest value (Z106) and because s1 does not take into account the percentage of error but perform well for the multi-step forecasting. Finally for this short and very regular time series either direct PCAIV or forecasting PCAIV improves the quality of forecast and their implementation, as nonparametric procedures, are easy. We do not try to optimize our methods by tuning important parameters such as the number and location of knots and the degree of spline because the aim of the paper is to show that with default values (three equally spaced knots and degree 2) our methods work very well. The next example is the River Jökulsá Eystri of Iceland which has explanatory variables. 4.3. River Jökulsá Eystri of Iceland The data consist of daily riverﬂow (yt ), precipitation (zt ) and temperature (xt ) from January 1, 1972, to December 31, 1974. There are 1096 observations. An important hydrological feature of this river is the presence of a glacier on the drainage area. For further information, see Tong (1990, chapter 7). Using only data of the ﬁrst two years and keeping the last year for out-of-sample prediction, we applied a ﬁrst spline PCAIV with X being the 10 lagged valued of the P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 1277 200 400 600 800 1000 20 40 60 80 100 120 140 200 400 600 800 1000 0 10 20 30 40 50 60 70 80 200 400 600 800 1000 -25 -20 -15 -10 -5 0 5 10 15 Fig. 3. Time plots of river Jökulsá Eystri riverﬂow data: (a) daily riverﬂow yt (m3/s); (b) daily precipitation zt (mm/day); (c) daily temperature xt (◦C). 40 120 40 120 40 120 -30 -10 40 120 10 25 40 120 -20 -10 0 50 0 4 -20 10 -2 4 -20 10 0 10 -20 10 -3 1 -20 10 -1 3 -20 10 -4 2 Fig. 4. Plots of functions ﬁtted to the Jökulsá Eystri riverﬂow data using spline PCAIV. (yt−1): ̂f1(yt−1) versus yt−1,...,(xt−5): ̂g6(xt−5) versus xt−5. Threshold value of Tong (temperature of −2◦C) is ﬁgured by dotted vertical line. riverﬂow, precipitation and temperature (Fig. 3). We select the different orders to be (4, 1, 5) and after a new spline PCAIV we select the following model: yt = f1(yt−1) + f2(yt−2) + f3(yt−3) + f4(yt−4) + g1(zt ) + g2(xt ) + g3(xt−1) + g4(xt−2) + g5(xt−3) + g6(xt−5) + \u0003t . The estimated functions are drawn in Fig. 4. Our model is very similar to the model of Chen and Tsay (1993) and Tong (1990) models. We refer the reader to these references for a complete discussion of hydrological features. For instance, the second line of Fig. 4 (ﬁtted functions for temperature) indicates the presence of a threshold around xt =−2asin the threshold model of Tong. In terms of residual variance (for the 3 years) our model (27.26) performs better than the Chen and Tsay model (35.32) and the Tong model (31.77). We calculate the out-of-sample predictive power of these two last models and our two predictive procedures, keeping the observations of the last year to perform prediction errors. We compute the mean 1278 P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 Table 2 Multi-step-ahead mean square errors of forecasts for the Jökulsá data set Model’s name Forecasts for the following lead times 1 234567 Tong 66.67 153.39 196.37 231.85 258.92 283.16 300.27 Chen and Tsay 65.52 142.01 175.88 203.03 230.02 256.89 279.32 Direct 58.05 123.87 157.83 183.94 215.12 259.68 304.75 Fpcaiv 57.01 126.42 160.82 185.59 209.35 245.07 286.33 square errors of forecasts of the four models. We focus our interest on one day ahead to one week ahead predictions. The results are given in Table 2. From Table 2, it is clear that our methods produce the best forecast among the four models. As in lynx data example, direct forecasting and forecasting PCAIV (Fpcaiv) leads to close forecast values. 5. Conclusion In recent years, due to availability of low-cost computing power, there has been a great proliferation of classes of nonlinear models for ﬁtting and predicting time series. Two new forecasting methods both based on the same criterion are presented. As presented in Section 2, this criterion tries to minimize the discrepancy between two scalar products. This objective function seems to be well suited for time series, both for model building (Imam et al., 1999) and forecasting. These methods use iterative algorithms which start from linear autoregressive models by using nodal values. Then iteration by iteration, we estimate more accurately the nonlinearity of the components of the time series. The price to be paid for nonlinearity, while circumventing the curse of dimensionality is the use of an iterative algorithm. However, with nowadays computer, the computational times are small. For example, it took 10 s on a personal computer to compute the 14 forecasts of the lynx data. In the paper, we do not try to optimize our methods by tuning important parameters such as the number and location of knots and the degree of spline. Instead, we show that with default values (three equally spaced knots and degree 2) our methods work very well. By optimizing these parameters, we could improve the results but it is not the scope of the paper to ﬁne tune parameters for each time series. To distinguish between the two procedures, direct PCAIV, to our knowledge, seems to perform better in the context of short and regular time series. On the other hand, for long-time series or irregular ones, the forecasting PCAIV allows more ﬂexibility leading to better forecasts. Acknowledgment The authors express their gratitude to the anonymous referee and Co-Editor whose comments greatly improved this paper. Appendix A. Proof of Proposition 1 Recall that the objective function g is deﬁned by g(R, s, YT ) = tr(̃Y ̃Y ′ − ̃X(s)R ̃X′(s))2 = tr(̃Y ̃Y ′ ̃Y ̃Y ′) ︸ ︷︷ ︸ (A.1) +tr(̃X(s)R ̃X′(s))2 −2tr(̃Y ̃Y ′ ̃X(s)R ̃X′(s)) ︸ ︷︷ ︸ (A.2) . We refer to Magnus and Neudecker (1988) for differentiation with respect to a vector. Differentiation with respect to ̃Y of (A.1) leads to d{tr(̃Y ̃Y ′ ̃Y ̃Y)}′ = tr(d̃Y ̃Y ′ ̃Y ̃Y ′) + tr(̃Y d̃Y ′ ̃Y ̃Y ′ ) + tr(̃Y ̃Y ′d̃Y ̃Y ′) + tr(̃Y ̃Y ′ ̃Y d̃Y ′) = 4̃Y ′ ̃Y ̃Y ′d̃Y . P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 1279 From the last equation we easily get the partial derivative of (A.1) with respect to YT , which is the ﬁrst coordinate of ̃Y : \u0002tr(̃Y ̃Y ′ ̃Y ̃Y ′ ) \u0002YT = 4YT ̃Y ′ ̃Y . Differentiating (A.2) with respect to ̃Y leads to: d{−2tr(̃Y ̃Y ′ ̃X(s)R ̃X′(s))}= − 2tr(d̃Y ̃Y ′ ̃X(s)R ̃X′(s)) − 2tr(̃Y d̃Y ′ ̃X(s)R ̃X′(s)) =− 4tr(̃Y ′ ̃X(s)R ̃X′(s)d̃Y) =− 4̃Y ′Ad̃Y . Recall that we are only interested in the derivative with respect to YT which is the ﬁrst coordinate. Thus we apply d̃Y = (10 ... 1) to the last equation to get the derivative. Recall that we partition the matrix A: A = X(s)RX(s)′ = ( a11 A12 A′ 12 A22 ) , and we can use the following partition ̃Y ′ = (YT Y ′). Using these equations we easily obtain the partial derivative of g with respect to YT : \u0002g \u0002YT = 4Y 3 T + 4(Y ′Y − a11)YT − 4Y ′A′ 12 = p(YT ). We want a real root of this degree 3 polynomial. The derivative of p is p′(YT ) = 12Y 2 T + 4(Y ′Y − a11). If (Y ′Y − a11) is positive then p(YT ) has only one real root. Spline PCAIV tries to approximate the scalar product YY ′ by X(s)RX(s)′. Thus a11 is close to the squared norm of YT and is a quantity of order 1. On the other hand, Y ′Y is the sum of n terms of the same order as a11, thus Y ′Y − a11 is positive. References Chen, R., Tsay, R.S., 1993. Nonlinear additive arx models. J. Amer. Statist. Assoc. 88, 955–967. Cornillon, P.A., Matzner-LZber, E., 2007. Relation between spline PCAIV and spline regression. in: Proceedings of the 12th ASMDA conference. Durand, J.F., 1993. Generalized principal component analysis with respect to instrumental variables via univariate spline transformations. Comput. Statist. Data Anal. 16, 423–440. Escouﬁer, Y., 1987. Principal components analysis with respect to instrumental variables. European Courses in Advanced Statistics, University of Napoli, pp. 285–299. Fan, J., Yao, Q., 2003. Nonlinear Time Series: Nonparametric and Parametric Methods. Springer, New York. Heij, C., Groenen, P.J.F., van Dicjk, D., 2007. Forecast comparison of principal component regression and principal covariate regression. Comput. Statist. Data Anal. 51, 3612–3625. Imam, W., Durand, J.F., 1997. Une extension spline additive de l’analyse en composantes principales sur variables instrumentales. in: 29e Journées de Statistique ASU, pp. 468–471. Imam, W., Matzner-LZber, E., Aifoute, A.S., 1999. Choix de l’ordre des modèles autorégressifs fonctionnels additifs. Rev. Statist. Appl. 47, 63–80. Koopman, S.J., Ooms, M., 2006. Forecasting daily time series using periodic unobserved components time series models. Comput. Statist. Data Anal. 51, 885–903. 1280 P.-A. Cornillon et al. / Computational Statistics & Data Analysis 52 (2008) 1269 – 1280 Lin, T., Pourahmadi, M., 1998. Nonparametric and non-linear models and data mining in time series: a case study in the Canadian lynx data. Appl. Statist. 47, 187–201. Magnus, J.R., Neudecker, H., 1988. Matrix Differential Calculus with Applications in Statistics and Econometrics. Wiley, New York. Rao, C.R., 1964. The use and interpretation of principal component analysis in applied research. Sankhya, A 26, 329–358. Schumaker, L.L., 1981. Spline Functions: Basic Theory. Wiley, New York. Tong, H., 1990. Nonlinear Time Series: A Dynamical System Approach. Clarendon Press, Oxford.","libVersion":"0.3.2","langs":""}
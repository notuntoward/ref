{"path":"lit/lit_notes_OLD_PARTIAL/Khalil23predictThenOptimize.pdf","text":"Predict-then-Optimize: a tour of the state-of-the-art using Elias B. Khalil Department of Mechanical & Industrial Engineering SCALE AI Research Chair in Data-Driven Algorithms for Modern Supply Chains Bo Tang For complete references to related work, please check our arXiv manuscript linked in the GitHub. Optimization with a linear objective 2 min w {câŠºw : w âˆˆ ğ’®} Decision variables Linear cost function {{{Feasible set Solution: use appropriate algorithm depending on type of feasible set (MILP, MIQCP, CP, custom algorithmsâ€¦) 3 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®} min w {câŠº nw : w âˆˆ ğ’®} The cost vectors could be completely unrelated nâ€¦ 4 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®} min w {câŠº nw : w âˆˆ ğ’®}â€¦ x1 âˆˆ â„ p x2â€¦ xn Instance feature vector (Observed) 5 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®}â€¦ x1 âˆˆ â„ p x2â€¦ xn ci = g(xi; Î¸) âˆˆ â„ d min w {câŠº nw : w âˆˆ ğ’®} Instance feature vector (Observed) Function is parametrized by a vector .g Î¸ 6 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®}â€¦ x1 âˆˆ â„ p x2â€¦ xn ci = g(xi; Î¸) âˆˆ â„ d min w {câŠº nw : w âˆˆ ğ’®} Instance feature vector (Observed) If you know then you can evaluate to obtain and optimize with your favorite method Î¸ g(xi; Î¸) ci 7 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®}â€¦ x1 âˆˆ â„ p x2â€¦ xn Ì‚ci = g(xi; Î¸) âˆˆ â„ d Ì‚c Ì‚c min w {câŠº nw : w âˆˆ ğ’®}Ì‚c Instance feature vector (Observed) If you know then you can evaluate to obtain and optimize with your favorite method Î¸ g(xi; Î¸) ci What if you donâ€™t know and you only observe ? Î¸ xi 8 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®}â€¦ x1 âˆˆ â„ p x2â€¦ xn Ì‚ci = g(xi; Î¸) âˆˆ â„ d Ì‚c Ì‚c min w {câŠº nw : w âˆˆ ğ’®}Ì‚c Instance feature vector (Observed) Assume these instances are from the past, i.e., â€œtrainingâ€ instances n True cost vector (Observed for training instances only) c1 âˆˆ â„ d c2 cnâ€¦ 9 min w {câŠº 1 w : w âˆˆ ğ’®} Same Decision variables{{Same Feasible set min w {câŠº 2 w : w âˆˆ ğ’®}â€¦ x1 âˆˆ â„ p x2â€¦ xn Ì‚ci = g(xi; Î¸) âˆˆ â„ d Ì‚c Ì‚c min w {câŠº nw : w âˆˆ ğ’®}Ì‚c Instance feature vector (Observed) Assume these instances are from the past, i.e., â€œtrainingâ€ instances n Goal: given training set , learn that approximates groundtruth vectors â€œwellâ€ ğ’Ÿ = {(xi, ci)} n i=1 Î¸ ci True cost vector (Observed for training instances only) c1 âˆˆ â„ d c2 cnâ€¦ Predict-then-Optimize Training 10 Predict-then-Optimize Training and Test-time inference 11 Predict-then-Optimize s-t shortest path: training data Google Maps, MontrÃ©al, QuebÃ©c, Canada 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 12 Predict-then-Optimize s-t shortest path: full pipeline 13 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 Travel time prediction model (regression) for each road segment Predicted travel times <=> s-t shortest path cost vector Predict-then-Optimize s-t shortest path: full pipeline 14 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 Travel time prediction model (regression) for each road segment Predicted travel times <=> s-t shortest path cost vector ?? Predict-then-Optimize s-t shortest path: full pipeline 15 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 Travel time prediction model (regression) for each road segment Predicted travel times <=> s-t shortest path cost vector ?? Squared error between and ci Ì‚ci Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 16 Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 17 Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs Edge 2 optimal Edge 1 optimal Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 18 Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs Edge 2 optimal Edge 1 optimal !! Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 19 Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs â€œSmart Predict then Optimizeâ€ Edge 2 optimal Edge 1 optimal Edge 2 optimal Edge 1 optimal End-to-End Predict-then-Optimize Training 20 Decision error 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice.{Optimal solution if you optimize with Ì‚c Optimal value with true costs c{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Easy: gradient of predicted costs to model parameters!{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Easy: gradient of predicted costs to model parameters!{Trickier: how does the decision loss vary with the predicted costs?{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Trickier: how does the decision loss vary with the predicted costs?{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Trickier: how does the decision loss vary with the predicted costs?{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Crux: how does the optimum change with the predicted costs? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. SPO+: a principled and effective method Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. 28 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. is not differentiable w.r.t. Ì‚c Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 11 Figure 1 Geometric Illustration of SPO Loss (a) Polyhedral feasible region (b) Elliptic feasible region Note. In these two ï¬gures, we consider a two-dimensional polyhedron and ellipse for the feasible region S.We plot the (negative) of the true cost vector c,as wellas two candidate predictions Ë†cA and Ë†cB that are equidistant from c and thus have equivalent LS loss. One can see that the optimal decision for Ë†cA coincides with that of c,since wâˆ—(Ë†cA)= wâˆ—(c).In the polyhedron example,any predicted cost vector whose negative is not in the gray region will result in a wrong decision, where as in the ellipse example anypredicted cost vector that is not exactlyparallel with c results in a wrong decision. direction and parallel to c. Deï¬nition 1 formalizes this true SPO loss associated with making the prediction Ë†c when the actual cost vector is c,given a particular oracle wâˆ—(Â·) for P (Â·). Definition 1 (SPO Loss). Given a cost vector prediction Ë†c and a realized cost vector c,the true SPO loss â„“wâˆ— SPO(Ë†c, c) w.r.t. optimization oracle wâˆ—(Â·) is deï¬ned as â„“wâˆ— SPO(Ë†c, c):= c T wâˆ—(Ë†c) âˆ’ zâˆ—(c) . Note that there is an unfortunate deï¬ciency in Deï¬nition 1, which is the dependence on the particular oracle wâˆ—(Â·) used to solve (2). Practically speaking, this deï¬ciency is not a major issue since we should usually expect wâˆ—(Ë†c) to be a unique optimal solution, i.e., we should expect W âˆ—(Ë†c) to be a singleton. Note that if any solution from W âˆ—(Ë†c) may be used by the loss function, then the loss function essentially becomes minwâˆˆW âˆ—(Ë†c) c T w âˆ’ zâˆ—(c).Thus, a prediction model would then be incentivized to always make the degenerate prediction Ë†c =0 since W âˆ—(0) = S.This would then imply that the SPO loss is 0. In any case, if one wishes to address the dependence on the particular oracle wâˆ—(Â·) in Deï¬nition 1, then it is most natural to â€œbreak tiesâ€ by presuming that the implemented decision has worst-case behavior with respect to c.Deï¬nition 2 is an alternative SPO loss function that does not depend on the particular choice of the optimization oracle wâˆ—(Â·). 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. SPO+: a principled and effective method Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. 29 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. is not differentiable w.r.t. Ì‚c Title Suppressed Due to Excessive Length 11 Fig. 3: As shown for the learning curves of the training of and on the shortest path, regret and unambiguous regret in the various tasks overlap almost exactly. Besides regret, decision error can also be deï¬ned as the diâ†µerence between the true solution and its prediction, such as Hamming distance of solutions [35] and squared error of the solutions [6]. In addition, Dalle et al. [10] also considered treating the objective value c|wâ‡¤ Ë†c itself as a loss. 3.4 Methodologies 3.4.1 Smart Predict-then-Optimize [16] To make the decision error diâ†µerentiable, Elmachtoub and Grigas [16] proposed , a convex upper bound on the regret: lSP O+(Ë†c, c)= \u0000min w2S{(2Ë†c \u0000 c)|w} +2Ë†c|wâ‡¤(c) \u0000 zâ‡¤(c). (5) One proposed subgradient for this loss writes as follows: 2(wâ‡¤(c) \u0000 wâ‡¤(2Ë†c \u0000 c)) 2 @lSPO+(Ë†c, c) @ Ë†c (6) Thus, we can use Algorithm 1 to directly minimize lSPO+(Ë†c, c) with gradi- ent descent. This algorithm with requires solving minw2S(2Ë†c \u0000 c)|w for each training iteration. To accelerate the training, Mandi et al. [27] employed relaxations ( ) and warm starting ( ) to speed-up the optimization. The idea of is to use the continuous relaxation of the integer program during training. This simpliï¬cation greatly reduces the training time at the expense of model performance. Compared to , the improvement of in training e\u0000ciency is not negligible. For example, linear programming can be solved in polynomial time while integer programming is worst-case exponential. In Section 6, we will further discuss this performance-e\u0000ciency tradeoâ†µ. For , Mandi et al. [27] suggested using previous solutions as a is a convex upper bound on regret and has subgradients 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. SPO+: a principled and effective method Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. 30 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. is not differentiable w.r.t. Ì‚c Title Suppressed Due to Excessive Length 11 Fig. 3: As shown for the learning curves of the training of and on the shortest path, regret and unambiguous regret in the various tasks overlap almost exactly. Besides regret, decision error can also be deï¬ned as the diâ†µerence between the true solution and its prediction, such as Hamming distance of solutions [35] and squared error of the solutions [6]. In addition, Dalle et al. [10] also considered treating the objective value c|wâ‡¤ Ë†c itself as a loss. 3.4 Methodologies 3.4.1 Smart Predict-then-Optimize [16] To make the decision error diâ†µerentiable, Elmachtoub and Grigas [16] proposed , a convex upper bound on the regret: lSP O+(Ë†c, c)= \u0000min w2S{(2Ë†c \u0000 c)|w} +2Ë†c|wâ‡¤(c) \u0000 zâ‡¤(c). (5) One proposed subgradient for this loss writes as follows: 2(wâ‡¤(c) \u0000 wâ‡¤(2Ë†c \u0000 c)) 2 @lSPO+(Ë†c, c) @ Ë†c (6) Thus, we can use Algorithm 1 to directly minimize lSPO+(Ë†c, c) with gradi- ent descent. This algorithm with requires solving minw2S(2Ë†c \u0000 c)|w for each training iteration. To accelerate the training, Mandi et al. [27] employed relaxations ( ) and warm starting ( ) to speed-up the optimization. The idea of is to use the continuous relaxation of the integer program during training. This simpliï¬cation greatly reduces the training time at the expense of model performance. Compared to , the improvement of in training e\u0000ciency is not negligible. For example, linear programming can be solved in polynomial time while integer programming is worst-case exponential. In Section 6, we will further discuss this performance-e\u0000ciency tradeoâ†µ. For , Mandi et al. [27] suggested using previous solutions as a is a convex upper bound on regret and has subgradients Computational overhead: To compute SPO+ loss, we need to solve an optimization problem in the forward pass; this is shared with other methods. Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 31 Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Elmachtoub and Grigas: Smart â€œPredict, then Optimizeâ€ 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ï¬t and predictions, while green lines and points correspond to the SPO ï¬t and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ï¬nding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ï¬rst observe that for any Î± âˆˆ R,the SPO loss can be written as â„“SPO(Ë†c, c)= max wâˆˆW âˆ—(Ë†c) { c T w âˆ’ Î±Ë†c T w} + Î±zâˆ—(Ë†c) âˆ’ zâˆ—(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart â€œpredict, then optimizeâ€.\"Â Management ScienceÂ 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs SPO+ Edge 2 optimal Edge 1 optimal Edge 2 optimal Edge 1 optimal Predict-then-Optimize s-t shortest path: training data Google Maps, MontrÃ©al, QuebÃ©c, Canada 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 32 Predict-then-Optimize s-t shortest path: training data Google Maps, MontrÃ©al, QuebÃ©c, Canada 8 am 10 am 12 pm 4 pm w* 1 w* 2 x1 x2 x3 x4 33 What if the historical cost vectors were unavailable, but the optimal solutions were? ğ’Ÿ = {(xi, w* i )}n i=1 w* 3 w* 4 PFYL: Perturbed Fenchel-Young Loss Berthet, Quentin, et al. \"Learning with diï¬€erentiable perturbed optimizers.\"Â NeurIPS 2020. 34 Recall the required gradient: 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Title Suppressed Due to Excessive Length 13 constant function wâ‡¤(Ë†c), Eâ‡ [wâ‡¤(Ë†c + \u0000â‡ )] varies the proportions in response to the change of Ë†c, providing a nonzero gradient of Ë†c: @ Eâ‡ [wâ‡¤(Ë†c + \u0000â‡ )] @ Ë†c â‡¡ 1 K KX ï£¿ wâ‡¤(Ë†c + \u0000â‡ ï£¿)â‡ ï£¿. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: Ë†c, K, \u0000 1: for sample ï£¿ 2 {1,...,K} do 2: Generate Gaussian noise â‡ ï£¿ 3: Solve: wâ‡  ï£¿ := wâ‡¤(Ë†c + \u0000â‡ ï£¿) 4: Save wâ‡  ï£¿ and â‡ ï£¿ for backward pass 5: end for 6: return 1 K PK ï£¿ wâ‡  ï£¿ Algorithm 5 DPO Backward Pass Require: @l(Â·) @ Eâ‡ [wâ‡¤] , K 1: Load wâ‡  ï£¿ and â‡ ï£¿ from forward pass 2: Compute @ Eâ‡ [wâ‡¤] @ Ë†c := 1 K PK ï£¿ wâ‡  ï£¿â‡ ï£¿ 3: Compute l(Â·) @ Ë†c := @l(Â·) @ Eâ‡ [wâ‡¤] @ Eâ‡ [wâ‡¤] @ Ë†c 4: return l(Â·) @ Ë†c 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(Ë†c, wâ‡¤(c)) and gradient @lFY(Ë†c,wâ‡¤(c)) @ Ë†c . Compared to , avoids the ine\u0000cient calcula- tion of the Jacobian matrix rT wâ‡¤(Ë†c) and includes a theoretically sound loss function. The loss of is based on Fenchel duality: The expectation of the per- turbed minimizer is deï¬ned as F (c)= Eâ‡ [min w2S{(c + \u0000â‡ )|w}], and the dual of F (c), denoted by âŒ¦(wâ‡¤(c)), is utilized to deï¬ne the Fenchel-Young loss: lFY(Ë†c, wâ‡¤(c)) = Ë†c|wâ‡¤(c) \u0000 F (Ë†c) \u0000 âŒ¦(wâ‡¤(c)), then the gradient of the loss is @lFY(Ë†c, wâ‡¤(c)) @ Ë†c = wâ‡¤(c) \u0000 @F (Ë†c) @ Ë†c = wâ‡¤(c) \u0000 E â‡  [argmin w2S {(Ë†c + \u0000â‡ )|w}]. Similar to , we can estimate the well-deï¬ned gradient as @lFY(Ë†c, wâ‡¤(c)) @ Ë†c â‡¡ wâ‡¤(c) \u0000 1 K KX ï£¿ argmin w2S {(Ë†c + \u0000â‡ ï£¿)|w} 4 Implementation and Modeling The core module of PyEPO is an â€œautogradâ€ function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero Average optimal solution over randomly perturbed costsK{ A random perturbation of predicted costs (ÏƒÎ¾k) Ì‚c{ PFYL: Perturbed Fenchel-Young Loss Berthet, Quentin, et al. \"Learning with diï¬€erentiable perturbed optimizers.\"Â NeurIPS 2020. 35 Recall the required gradient: 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Title Suppressed Due to Excessive Length 13 constant function wâ‡¤(Ë†c), Eâ‡ [wâ‡¤(Ë†c + \u0000â‡ )] varies the proportions in response to the change of Ë†c, providing a nonzero gradient of Ë†c: @ Eâ‡ [wâ‡¤(Ë†c + \u0000â‡ )] @ Ë†c â‡¡ 1 K KX ï£¿ wâ‡¤(Ë†c + \u0000â‡ ï£¿)â‡ ï£¿. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: Ë†c, K, \u0000 1: for sample ï£¿ 2 {1,...,K} do 2: Generate Gaussian noise â‡ ï£¿ 3: Solve: wâ‡  ï£¿ := wâ‡¤(Ë†c + \u0000â‡ ï£¿) 4: Save wâ‡  ï£¿ and â‡ ï£¿ for backward pass 5: end for 6: return 1 K PK ï£¿ wâ‡  ï£¿ Algorithm 5 DPO Backward Pass Require: @l(Â·) @ Eâ‡ [wâ‡¤] , K 1: Load wâ‡  ï£¿ and â‡ ï£¿ from forward pass 2: Compute @ Eâ‡ [wâ‡¤] @ Ë†c := 1 K PK ï£¿ wâ‡  ï£¿â‡ ï£¿ 3: Compute l(Â·) @ Ë†c := @l(Â·) @ Eâ‡ [wâ‡¤] @ Eâ‡ [wâ‡¤] @ Ë†c 4: return l(Â·) @ Ë†c 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(Ë†c, wâ‡¤(c)) and gradient @lFY(Ë†c,wâ‡¤(c)) @ Ë†c . Compared to , avoids the ine\u0000cient calcula- tion of the Jacobian matrix rT wâ‡¤(Ë†c) and includes a theoretically sound loss function. The loss of is based on Fenchel duality: The expectation of the per- turbed minimizer is deï¬ned as F (c)= Eâ‡ [min w2S{(c + \u0000â‡ )|w}], and the dual of F (c), denoted by âŒ¦(wâ‡¤(c)), is utilized to deï¬ne the Fenchel-Young loss: lFY(Ë†c, wâ‡¤(c)) = Ë†c|wâ‡¤(c) \u0000 F (Ë†c) \u0000 âŒ¦(wâ‡¤(c)), then the gradient of the loss is @lFY(Ë†c, wâ‡¤(c)) @ Ë†c = wâ‡¤(c) \u0000 @F (Ë†c) @ Ë†c = wâ‡¤(c) \u0000 E â‡  [argmin w2S {(Ë†c + \u0000â‡ )|w}]. Similar to , we can estimate the well-deï¬ned gradient as @lFY(Ë†c, wâ‡¤(c)) @ Ë†c â‡¡ wâ‡¤(c) \u0000 1 K KX ï£¿ argmin w2S {(Ë†c + \u0000â‡ ï£¿)|w} 4 Implementation and Modeling The core module of PyEPO is an â€œautogradâ€ function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero Average optimal solution over randomly perturbed costsK{ A random perturbation of predicted costs (ÏƒÎ¾k) Ì‚c{ Notice the true costs do not appear here.c PFYL: Perturbed Fenchel-Young Loss Berthet, Quentin, et al. \"Learning with diï¬€erentiable perturbed optimizers.\"Â NeurIPS 2020. 36 Recall the required gradient: 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters âœ“ for predictor g(x; âœ“) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor Ë†c := g(x; âœ“) 6: Forward pass to compute optimal solution wâ‡¤(Ë†c):=argminw2S Ë†c|w 7: Forward pass to compute decision loss l(Ë†c, Â·) 8: Backward pass from loss l(Ë†c, Â·)toupdateparameters âœ“ with gradient 9: end for 10: end for @l(Ë†c, Â·) @âœ“ = @l(Ë†c, Â·) @ Ë†c @ Ë†c @âœ“ = @l(Ë†c, Â·) @wâ‡¤(Ë†c) @wâ‡¤(Ë†c) @ Ë†c @ Ë†c @âœ“ Note: @ Ë†c @âœ“ = @g(x; âœ“) @âœ“ (3) The last term @ Ë†c @âœ“ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of diâ†µerentiable optimizer @l(Ë†c,Â·) @ Ë†c or the direct decision loss function @wâ‡¤(Ë†c) @ Ë†c . Because the optimal solution wâ‡¤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector wâ‡¤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(Ë†c,Â·) @ Ë†c , measuring decision errors with respect to speciï¬c losses, while and , as diâ†µerentiable optimizers, compute approximate @wâ‡¤(Ë†c) @ Ë†c , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deï¬ned as the diâ†µerence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(Ë†c, c)= c|wâ‡¤(Ë†c) \u0000 zâ‡¤(c). (4) Given a cost vector Ë†c, there may be multiple optimal solutions to minw2S Ë†c|w. Therefore, Elmachtoub and Grigas [16] devised the â€œunambiguousâ€ regret (also called unambiguous SPO Loss): lURegret(Ë†c, c) = maxw2W â‡¤(c)w|c\u0000zâ‡¤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Title Suppressed Due to Excessive Length 13 constant function wâ‡¤(Ë†c), Eâ‡ [wâ‡¤(Ë†c + \u0000â‡ )] varies the proportions in response to the change of Ë†c, providing a nonzero gradient of Ë†c: @ Eâ‡ [wâ‡¤(Ë†c + \u0000â‡ )] @ Ë†c â‡¡ 1 K KX ï£¿ wâ‡¤(Ë†c + \u0000â‡ ï£¿)â‡ ï£¿. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: Ë†c, K, \u0000 1: for sample ï£¿ 2 {1,...,K} do 2: Generate Gaussian noise â‡ ï£¿ 3: Solve: wâ‡  ï£¿ := wâ‡¤(Ë†c + \u0000â‡ ï£¿) 4: Save wâ‡  ï£¿ and â‡ ï£¿ for backward pass 5: end for 6: return 1 K PK ï£¿ wâ‡  ï£¿ Algorithm 5 DPO Backward Pass Require: @l(Â·) @ Eâ‡ [wâ‡¤] , K 1: Load wâ‡  ï£¿ and â‡ ï£¿ from forward pass 2: Compute @ Eâ‡ [wâ‡¤] @ Ë†c := 1 K PK ï£¿ wâ‡  ï£¿â‡ ï£¿ 3: Compute l(Â·) @ Ë†c := @l(Â·) @ Eâ‡ [wâ‡¤] @ Eâ‡ [wâ‡¤] @ Ë†c 4: return l(Â·) @ Ë†c 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(Ë†c, wâ‡¤(c)) and gradient @lFY(Ë†c,wâ‡¤(c)) @ Ë†c . Compared to , avoids the ine\u0000cient calcula- tion of the Jacobian matrix rT wâ‡¤(Ë†c) and includes a theoretically sound loss function. The loss of is based on Fenchel duality: The expectation of the per- turbed minimizer is deï¬ned as F (c)= Eâ‡ [min w2S{(c + \u0000â‡ )|w}], and the dual of F (c), denoted by âŒ¦(wâ‡¤(c)), is utilized to deï¬ne the Fenchel-Young loss: lFY(Ë†c, wâ‡¤(c)) = Ë†c|wâ‡¤(c) \u0000 F (Ë†c) \u0000 âŒ¦(wâ‡¤(c)), then the gradient of the loss is @lFY(Ë†c, wâ‡¤(c)) @ Ë†c = wâ‡¤(c) \u0000 @F (Ë†c) @ Ë†c = wâ‡¤(c) \u0000 E â‡  [argmin w2S {(Ë†c + \u0000â‡ )|w}]. Similar to , we can estimate the well-deï¬ned gradient as @lFY(Ë†c, wâ‡¤(c)) @ Ë†c â‡¡ wâ‡¤(c) \u0000 1 K KX ï£¿ argmin w2S {(Ë†c + \u0000â‡ ï£¿)|w} 4 Implementation and Modeling The core module of PyEPO is an â€œautogradâ€ function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero Average optimal solution over randomly perturbed costsK{ A random perturbation of predicted costs (ÏƒÎ¾k) Ì‚c{ Notice the true costs do not appear here.c Computational overhead: To compute the PFYL gradient, we need to solve optimization problems. K Avoiding solver calls by Learning to Rank Mandi, Jayanta, et al. \"Decision-focused learning: through the lens of learning to rank.\"Â ICML, 2022. Mulamba, Maxime, et al. \"Contrastive Losses and Solution Caching for Predict-and-Optimize.\"Â IJCAI, 2021. 37 Recall that both SPO+ and PFYL made one or more calls to the solver in each forward pass! w1 w2 w3 Objective using prediction A Objective using prediction B True Objective Sol 1 0 1 0 1 3 1 Sol 2 1 0 0 3 2 2 Sol 3 1 1 1 2 1 3 Avoiding solver calls by Learning to Rank Mandi, Jayanta, et al. \"Decision-focused learning: through the lens of learning to rank.\"Â ICML, 2022. Mulamba, Maxime, et al. \"Contrastive Losses and Solution Caching for Predict-and-Optimize.\"Â IJCAI, 2021. 38 w1 w2 w3 Objective using prediction A Objective using prediction B True Objective Sol 1 0 1 0 1 3 1 Sol 2 1 0 0 3 2 2 Sol 3 1 1 1 2 1 3{Terrible solution ranking!{Somewhat betterâ€¦ Avoiding solver calls by Learning to Rank Mandi, Jayanta, et al. \"Decision-focused learning: through the lens of learning to rank.\"Â ICML, 2022. Mulamba, Maxime, et al. \"Contrastive Losses and Solution Caching for Predict-and-Optimize.\"Â IJCAI, 2021. 39 w1 w2 w3 Objective using prediction A Objective using prediction B True Objective Sol 1 0 1 0 1 3 1 Sol 2 1 0 0 3 2 2 Sol 3 1 1 1 2 1 3{Terrible solution ranking!{Somewhat betterâ€¦ Key Idea: Learn to predict coefficients that lead to good rankings of a set of collected solutions! 40 Predict-then-Optimize: a tour of the state-of-the-art using Bo Tang Manuscript describing PyEPO, the literature, and extensive experiments: https://arxiv.org/abs/2206.14234 4142 Benchmarks Optimization modeling ML modeling Training algorithms Shortest path Knapsack TSP Gurobi Any custom solver SPO+ SPO+ with relaxations PFYL/DPO DBB (Pogancic et al. [2019]) Benchmark generation Based on Elmachtoub & Grigas 43 s t cij = Î±((â„¬xi)j + 3)deg â‹… Ïµi,j xi âˆˆ â„ p i-th instanceâ€™s feature vector (Observed) j deg Integer in [1,6] regulating how non- linear the mapping is â„¬ âˆˆ {0,1} dÃ—n â„¬p,q âˆ¼ Bernoulli(0.5) Ïµi,j Uniform random noise Warcraft Shortest-Path Benchmark Based on PoganÄiÄ‡, Marin Vlastelica, et al. \"Diï¬€erentiation of blackbox combinatorial solvers.\"Â ICLR 2020. 44 24 Bo Tang, Elias B. Khalil Feature 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Solution Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k â‡¥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deï¬ned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diâ†µerent predictors and / / with a linear prediction model g(x; âœ“). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ï¬‚exibility in the loss could be useful for diâ†µerent problems. In the original paper, PoganË‡ciÂ´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diâ†µerence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciï¬cally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diâ†µerent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width Â¯âœ 2 {0.0, 0.5}. We then conduct Image inputs (RGB features) True costs Optimal solution (NW -> SE) Optimization model (in Gurobi) 45 Title Suppressed Due to Excessive Length 17 4.1.2 Optimization Model with Gurobi On the other hand, we provide optGrbModel to create an optimization model with GurobiPy. Unlike optModel, optGrbModel is more lightweight but less ï¬‚exible for users. Let us use the following optimization model (7) as an exam- ple, where ci is an unknown cost coe\u0000cient: max x 4X i=0 cixi s.t. 3x0 +4x1 +3x2 +6x3 +4x4 ï£¿ 12 4x0 +5x1 +2x2 +3x3 +5x4 ï£¿ 10 5x0 +4x1 +6x2 +2x3 +3x4 ï£¿ 15 8xi 2 {0, 1} (7) Inheriting optGrbModel is the convenient way to use Gurobi with PyEPO. The only implementation required is to override getModel and return a Gurobi model and the corresponding decision variables. In addition, there is no need to assign a value to the attribute modelSense in optGrbModel manually. An example for Model (7) is as follows: 1 import gurobipy as gp 2 from gurobipy import GRB 3 from pyepo . model . grb import optGrbModel 4 5 class myModel ( optGrbModel ) : 6 def _getModel ( self ) : 7 # ceate a model 8 m= gp . Model () 9 # varibles 10 x= m . addVars (5 , name = \" x \" ,vtype = GRB . BINARY ) 11 # sense ( must be minimize ) 12 m. modelSense = GRB . MAXIMIZE 13 # constraints 14 m. addConstr (3* x [0]+4* x [1]+3* x [2]+6* x [3]+4* x [4] <=12) 15 m. addConstr (4* x [0]+5* x [1]+2* x [2]+3* x [3]+5* x [4] <=10) 16 m. addConstr (5* x [0]+4* x [1]+6* x [2]+2* x [3]+3* x [4] <=15) 17 return m, x 18 19 optmodel = myModel () 4.1.3 Optimization Model with Pyomo Similarly, optOmoModel allows modeling mathematical programs with Py- omo. In contrast to optGrbModel, optOmoModel requires an explicit object attribute modelSense.Since Pyomo supports multiple solvers, instantiating an optOmoModel requires a parameter solver to specify the solver. The fol- lowing is an implementation of problem 7: 1 from pyomo import environ as pe 2 from pyepo import EPO Title Suppressed Due to Excessive Length 17 4.1.2 Optimization Model with Gurobi On the other hand, we provide optGrbModel to create an optimization model with GurobiPy. Unlike optModel, optGrbModel is more lightweight but less ï¬‚exible for users. Let us use the following optimization model (7) as an exam- ple, where ci is an unknown cost coe\u0000cient: max x 4X i=0 cixi s.t. 3x0 +4x1 +3x2 +6x3 +4x4 ï£¿ 12 4x0 +5x1 +2x2 +3x3 +5x4 ï£¿ 10 5x0 +4x1 +6x2 +2x3 +3x4 ï£¿ 15 8xi 2 {0, 1} (7) Inheriting optGrbModel is the convenient way to use Gurobi with PyEPO. The only implementation required is to override getModel and return a Gurobi model and the corresponding decision variables. In addition, there is no need to assign a value to the attribute modelSense in optGrbModel manually. An example for Model (7) is as follows: 1 import gurobipy as gp 2 from gurobipy import GRB 3 from pyepo . model . grb import optGrbModel 4 5 class myModel ( optGrbModel ) : 6 def _getModel ( self ) : 7 # ceate a model 8 m= gp . Model () 9 # varibles 10 x= m . addVars (5 , name = \" x \" ,vtype = GRB . BINARY ) 11 # sense ( must be minimize ) 12 m. modelSense = GRB . MAXIMIZE 13 # constraints 14 m. addConstr (3* x [0]+4* x [1]+3* x [2]+6* x [3]+4* x [4] <=12) 15 m. addConstr (4* x [0]+5* x [1]+2* x [2]+3* x [3]+5* x [4] <=10) 16 m. addConstr (5* x [0]+4* x [1]+6* x [2]+2* x [3]+3* x [4] <=15) 17 return m, x 18 19 optmodel = myModel () 4.1.3 Optimization Model with Pyomo Similarly, optOmoModel allows modeling mathematical programs with Py- omo. In contrast to optGrbModel, optOmoModel requires an explicit object attribute modelSense.Since Pyomo supports multiple solvers, instantiating an optOmoModel requires a parameter solver to specify the solver. The fol- lowing is an implementation of problem 7: 1 from pyomo import environ as pe 2 from pyepo import EPO Creating a dataset based on features and true costs 46 20 Bo Tang, Elias B. Khalil 1 import pyepo 2 # init Fenchel - Young loss 3 pfyl = pyepo . func . p e r t u r b e d F e n c h e l Y o u n g ( optmodel , n_samples =3 , sigma =1.0 , processes =8) The below code block illustrates the calculation of Fenchel-Young loss: 1 # calculate Fenchel - Young loss 2 loss = pfyl ( pred_cost , true_sol ) 4.3 The optDataset Class for Managing Data The utilization of decision losses, such as and , necessitates the availability of true optimal solutions. Therefore, to facilitate convenience in PyEPO training and testing, an auxiliary optDataset has been introduced, which is not strictly indispensable. optDataset stores the features and their associated costs of the objective function and solves optimization problems to get optimal solutions and corresponding objective values. optDataset is extended from PyTorch Dataset. In order to obtain optimal solutions, optDataset requires the corresponding optModel (see in Section 4.1). The parameters for the optDataset are as follows: â€“ model: an instance of optModel; â€“ feats: data features; â€“ costs: corresponding costs of objective function; Then, as the following example, PyTorch DataLoader receives an optDataset and wraps the data samples and acts as a sampler that provides an iterable over the given dataset. It is required to provide the batch size which is the number of training samples that will be used in each update of the model parameters. 1 import pyepo 2 from torch . utils . data import DataLoader 3 4 # build dataset 5 dataset = pyepo . data . dataset . optDataset ( optmodel , feats , costs ) 6 7 # get data loader 8 dataloader = DataLoader ( dataset , batch_size =32 , shuffle = True ) By iterating over the DataLoader, we can obtain a batch of features, true costs, optimal solutions, and corresponding objective values: 1 for x, c, w, z in dataloader : 2 # a batch of features 3 print (x) 4 # a batch of true costs 5 print (c) 6 # a batch of true optimal solutions 7 print (w) 8 # a batch of true optimal objective values 9 print (z) 20 Bo Tang, Elias B. Khalil 1 import pyepo 2 # init Fenchel - Young loss 3 pfyl = pyepo . func . p e r t u r b e d F e n c h e l Y o u n g ( optmodel , n_samples =3 , sigma =1.0 , processes =8) The below code block illustrates the calculation of Fenchel-Young loss: 1 # calculate Fenchel - Young loss 2 loss = pfyl ( pred_cost , true_sol ) 4.3 The optDataset Class for Managing Data The utilization of decision losses, such as and , necessitates the availability of true optimal solutions. Therefore, to facilitate convenience in PyEPO training and testing, an auxiliary optDataset has been introduced, which is not strictly indispensable. optDataset stores the features and their associated costs of the objective function and solves optimization problems to get optimal solutions and corresponding objective values. optDataset is extended from PyTorch Dataset. In order to obtain optimal solutions, optDataset requires the corresponding optModel (see in Section 4.1). The parameters for the optDataset are as follows: â€“ model: an instance of optModel; â€“ feats: data features; â€“ costs: corresponding costs of objective function; Then, as the following example, PyTorch DataLoader receives an optDataset and wraps the data samples and acts as a sampler that provides an iterable over the given dataset. It is required to provide the batch size which is the number of training samples that will be used in each update of the model parameters. 1 import pyepo 2 from torch . utils . data import DataLoader 3 4 # build dataset 5 dataset = pyepo . data . dataset . optDataset ( optmodel , feats , costs ) 6 7 # get data loader 8 dataloader = DataLoader ( dataset , batch_size =32 , shuffle = True ) By iterating over the DataLoader, we can obtain a batch of features, true costs, optimal solutions, and corresponding objective values: 1 for x, c, w, z in dataloader : 2 # a batch of features 3 print (x) 4 # a batch of true costs 5 print (c) 6 # a batch of true optimal solutions 7 print (w) 8 # a batch of true optimal objective values 9 print (z) Creating an ML model with PyTorch 47 Title Suppressed Due to Excessive Length 21 4.4 End-to-End Training The core capability of PyEPO is to build an optimization model, and then embed the optimization model into a PyTorch neural network for the end-to- end training. Here, we build a simple linear regression model in PyTorch as an example: 1 from torch import nn 2 3 # construct linear model 4 class LinearRegression ( nn . Module ) : 5 def __init__ ( self ) : 6 super (LinearRegression , self ) . __init__ () 7 # size of input and output is the size of feature and cost 8 self . linear = nn . Linear ( num_feat , len_cost ) 9 def forward ( self , x ) : 10 out = self . linear ( x ) 11 return out 12 13 # init model 14 predmodel = L i ne a r R e g r e s s i o n () Then, we can train the prediction model with SPO+ loss to predict un- known cost coe\u0000cients, make decisions, and compute decision errors. The training of the prediction model is performed using a stochastic gradient de- scent (SGD) optimizer. By utilizing PyTorch automatic diâ†µerentiation capa- bilities, the gradients of the loss with respect to the model parameters are computed and used to update the model parameters during training. 1 import torch 2 3 # set SGD optimizer 4 optimizer = torch . optim . SGD ( predmodel . parameters () , lr =1 e -3) 5 6 # training 7 for epoch in range (num_epochs) : 8 # iterare features , costs , solutions , and objective values 9 for x, c, w, z in dataloader : 10 # forward pass 11 cp = predmodel ( x ) # predict costs 12 loss = spop ( cp , c , w , z ) . mean () # calculate SPO + loss 13 # backward pass 14 optimizer . zero_grad () # reset gradients to 0 15 loss . backward () # compute gradients 16 optimizer . step () # update model parameters 4.5 Metrics PyEPO provides evaluation functions to measure model performance, in par- ticular the two metrics mentioned in Section 3.3.1: regret and unambiguous regret. We further deï¬ne the normalized (unambiguous) regret by Pntest i=1 lRegret(Ë†ci, ci) Pntest i=1 |zâ‡¤(ci)| . End-to-end training! 48 Title Suppressed Due to Excessive Length 21 4.4 End-to-End Training The core capability of PyEPO is to build an optimization model, and then embed the optimization model into a PyTorch neural network for the end-to- end training. Here, we build a simple linear regression model in PyTorch as an example: 1 from torch import nn 2 3 # construct linear model 4 class LinearRegression ( nn . Module ) : 5 def __init__ ( self ) : 6 super (LinearRegression , self ) . __init__ () 7 # size of input and output is the size of feature and cost 8 self . linear = nn . Linear ( num_feat , len_cost ) 9 def forward ( self , x ) : 10 out = self . linear ( x ) 11 return out 12 13 # init model 14 predmodel = L i ne a r R e g r e s s i o n () Then, we can train the prediction model with SPO+ loss to predict un- known cost coe\u0000cients, make decisions, and compute decision errors. The training of the prediction model is performed using a stochastic gradient de- scent (SGD) optimizer. By utilizing PyTorch automatic diâ†µerentiation capa- bilities, the gradients of the loss with respect to the model parameters are computed and used to update the model parameters during training. 1 import torch 2 3 # set SGD optimizer 4 optimizer = torch . optim . SGD ( predmodel . parameters () , lr =1 e -3) 5 6 # training 7 for epoch in range (num_epochs) : 8 # iterare features , costs , solutions , and objective values 9 for x, c, w, z in dataloader : 10 # forward pass 11 cp = predmodel ( x ) # predict costs 12 loss = spop ( cp , c , w , z ) . mean () # calculate SPO + loss 13 # backward pass 14 optimizer . zero_grad () # reset gradients to 0 15 loss . backward () # compute gradients 16 optimizer . step () # update model parameters 4.5 Metrics PyEPO provides evaluation functions to measure model performance, in par- ticular the two metrics mentioned in Section 3.3.1: regret and unambiguous regret. We further deï¬ne the normalized (unambiguous) regret by Pntest i=1 lRegret(Ë†ci, ci) Pntest i=1 |zâ‡¤(ci)| . Experiments with TSP20 Vertical axis: average regret w.r.t. true OPT on unseen test instances 49 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as , , and . The normalized regret is visualized under diâ†µerent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of â€œ2-stage Autoâ€ is the mean squared error of the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ï¬ve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. 2-stage methods: regress on true costs (no end-to-end training) Harder learning task Lower is better Experiments with TSP20 Vertical axis: average regret w.r.t. true OPT on unseen test instances 50 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as , , and . The normalized regret is visualized under diâ†µerent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of â€œ2-stage Autoâ€ is the mean squared error of the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ï¬ve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 5000, Noise Half\u0000width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 5000, Noise Half\u0000width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as , , and . The normalized regret is visualized under diâ†µerent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of â€œ2-stage Autoâ€ is the mean squared error of the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ï¬ve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. SPO+ and PFYL, both using a linear model perform the best. With more data, 2-stage Random Forest is competitive! 2-stage methods: regress on true costs (no end-to-end training) n=1000 training instances n=5000 training instances Regret-accuracy tradeoff 51 SPO+ and PFYL, both using a linear model perform the best. Title Suppressed Due to Excessive Length 33 0 20 40 60 80 MSE 0.050 0.075 0.100 0.125 0.150 0.175 0.200NormalizedRegret TSP Training Set Size = 1000, Polynomial degree = 4, Noise Half\u0000width = 0.5 Fig. 13: MSE v.s. Regret: The result covers diâ†µerent two-stage methods, , , and their regularization. is omitted because it is far away from others. The size of the circles is proportional to the training time (Sec), so the smaller is better. Finding #5 Generally, and can achieve good decisions at the cost of pre- diction accuracy. If one is seeking a balanced tradeoâ†µ between decision quality and prediction accuracy, an end-to-end method with prediction regularization may be preferable. 7 Empirical Evaluation for Image-Based Shortest Path Following PoganË‡ciÂ´c et al. [35] and Berthet et al. [6], we employ a truncated ResNet18 convolutional neural network (CNN) architecture consisting of the ï¬rst ï¬ve layers on Warcraft terrain images (refer to Section 5.2). As Table 5 shows, the methods we compare include a two-stage method, , , , and with truncated ResNet18. We train the CNN over 50 epochs with batches of size 70. The learning rate is set to 0.0005 decaying at the epochs 30 and 40, and the hyperparameters n =1, \u0000 = 1 for and , \u0000 = 10 for . We use the Hamming distance for and the squared error of solutions for , which are the loss functions used in the original papers. The sample size of the test set ntest is 1000. To evaluate our methods, we compute the relative regret c|wâ‡¤(Ë†c)\u0000zâ‡¤(c) zâ‡¤(c) and path accuracy Pd j=1 (zâ‡¤(c)j =zâ‡¤(Ë†c)j ) d per instance on the test set; the latter is simply the fraction of edges in the â€œpredictedâ€ solution that are also in the optimal solution. As shown in Figure 14, the two-stage method, and achieve com- parable levels of performance in predicting the shortest path on the Warcraft Vertical axis: average regret w.r.t. true OPT on unseen test instances Horizontal axis: Mean- Squared Error on true costs 30 Bo Tang, Elias B. Khalil Fig. 10: Normalized regret for the 2D knapsack (at the top) and TSP (at the bottom) on the test set: The methods in the experiment include , and w/o relaxation. Then, we visualize the normalized regret under diâ†µerent sample sizes and polynomial degrees to investigate the impact of the relaxation method. For normalized regret, lower is better. a tighter bound does reduce the regret, and shows advantages over . Overall, using relaxations achieves fairly good performance with improved computational e\u0000ciency. Moreover, formulations with tighter linear relaxation lead to better performance. Finding #3 End-to-end predict-then-optimize with relaxation has excellent poten- tial to improve computation e\u0000ciency at a slight degradation in perfor- mance, particularly when the true cost-generating function is not very non-linear. 6.4 Prediction Regularization As proposed in Elmachtoub and Grigas [16], the mean absolute error lMAE(Ë†c, c)= 1 n Pn i kË†ci \u0000 cik1 or mean squared error lMSE(Ë†c, c)= 1 2n Pn i kË†ci \u0000 cik 2 2 of the predicted cost vector w.r.t. true cost vector can be added to the decision loss as l1 or l2 regularizers. When using regularization, we set either the l1 regular- ization parameter \u00001 and the l2 regularization parameter \u00002 from 0.001 to 10 logarithmically. For the experiments, we still use the same instances, model, and hyperparameters as before, while the number of training samples n,the noise half-width Â¯âœ and, the polynomial degree deg are ï¬xed at 1000, 0.5 and 4. Warcraft Shortest-Path Benchmark Based on PoganÄiÄ‡, Marin Vlastelica, et al. \"Diï¬€erentiation of blackbox combinatorial solvers.\"Â ICLR 2020. 52 24 Bo Tang, Elias B. Khalil Feature 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Solution Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k â‡¥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deï¬ned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diâ†µerent predictors and / / with a linear prediction model g(x; âœ“). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ï¬‚exibility in the loss could be useful for diâ†µerent problems. In the original paper, PoganË‡ciÂ´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diâ†µerence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciï¬cally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diâ†µerent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width Â¯âœ 2 {0.0, 0.5}. We then conduct Image inputs (RGB features) True costs Optimal solution (NW -> SE) Warcraft Shortest-Path Benchmark Based on PoganÄiÄ‡, Marin Vlastelica, et al. \"Diï¬€erentiation of blackbox combinatorial solvers.\"Â ICLR 2020. 53 ResNet-18 24 Bo Tang, Elias B. Khalil Feature Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k â‡¥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deï¬ned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diâ†µerent predictors and / / with a linear prediction model g(x; âœ“). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ï¬‚exibility in the loss could be useful for diâ†µerent problems. In the original paper, PoganË‡ciÂ´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diâ†µerence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciï¬cally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diâ†µerent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width Â¯âœ 2 {0.0, 0.5}. We then conduct 24 Bo Tang, Elias B. Khalil 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k â‡¥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deï¬ned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diâ†µerent predictors and / / with a linear prediction model g(x; âœ“). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ï¬‚exibility in the loss could be useful for diâ†µerent problems. In the original paper, PoganË‡ciÂ´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diâ†µerence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciï¬cally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diâ†µerent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width Â¯âœ 2 {0.0, 0.5}. We then conduct Image inputs Cost predictions Warcraft Shortest-Path Benchmark Based on PoganÄiÄ‡, Marin Vlastelica, et al. \"Diï¬€erentiation of blackbox combinatorial solvers.\"Â ICLR-20. 54 ResNet-18 24 Bo Tang, Elias B. Khalil Feature Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k â‡¥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deï¬ned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diâ†µerent predictors and / / with a linear prediction model g(x; âœ“). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ï¬‚exibility in the loss could be useful for diâ†µerent problems. In the original paper, PoganË‡ciÂ´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diâ†µerence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciï¬cally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diâ†µerent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width Â¯âœ 2 {0.0, 0.5}. We then conduct 24 Bo Tang, Elias B. Khalil 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k â‡¥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deï¬ned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with diâ†µerent predictors and / / with a linear prediction model g(x; âœ“). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ï¬‚exibility in the loss could be useful for diâ†µerent problems. In the original paper, PoganË‡ciÂ´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared diâ†µerence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciï¬cally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Diâ†µerent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width Â¯âœ 2 {0.0, 0.5}. We then conduct Image inputs Cost predictions 34 Bo Tang, Elias B. Khalil Method Description 2S Two-stage method where the predictor is a truncated ResNet18 Truncated ResNet18 with SPO+ loss [16] Truncated ResNet18 with diâ†µerentiable black-box optimizer and Hamming distance loss [35] Truncated ResNet18 with diâ†µerentiable perturbed optimizer and squared error loss [6] Truncated ResNet18 with perturbed Fenchel-Young loss [6] Table 5: Methods compared in the experiments. terrain, while obtains solutions that agree the most with the optima (i.e., highest Path Accuracy). It seems that the Warcraft shortest path problem may not require end-to-end learning. However, it is noteworthy that ,despite lacking knowledge of the true costs, yields a competitive result, encouraging researchers to broaden the applications for end-to-end learning. 0 10 20 30 40 50 Epochs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8NormalizedRegretLearning Curve on Test Set 2S SPO+ PFYL DBB DPO 2S SPO+ PFYL DBB DPO Methods 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75RelativeRegret Relative Regret for each Instance on Test Set Fig. 14: Learning curve, relative regret, and path accuracy for the shortest path problem on the test set: The methods in the experiment include a two- stage neural network, , , , and . The learning curve shows relative regret on the test set, and the box plot demonstrates the distribution of relative regret on the test set. For relative regret, lower is better. Finding #6 End-to-end learning is eâ†µective in rich contextual features such as im- ages. Moreover, the study highlights that can achieve impressive performance levels, even without knowing the costs during training. 8 Conclusion Because of the lack of easy-to-use generic tools, the potential power of the end- to-end predict-then-optimize has been underestimated or even overlooked in various applications. Our PyEPO package aims to alleviate barriers between the theory and practice of the end-to-end approach. PyEPO,the PyTorch-based end-to-end predict-then-optimize tool, is specif- ically designed for linear objective functions, including linear programming and 34 Bo Tang, Elias B. Khalil Method Description 2S Two-stage method where the predictor is a truncated ResNet18 Truncated ResNet18 with SPO+ loss [16] Truncated ResNet18 with diâ†µerentiable black-box optimizer and Hamming distance loss [35] Truncated ResNet18 with diâ†µerentiable perturbed optimizer and squared error loss [6] Truncated ResNet18 with perturbed Fenchel-Young loss [6] Table 5: Methods compared in the experiments. terrain, while obtains solutions that agree the most with the optima (i.e., highest Path Accuracy). It seems that the Warcraft shortest path problem may not require end-to-end learning. However, it is noteworthy that ,despite lacking knowledge of the true costs, yields a competitive result, encouraging researchers to broaden the applications for end-to-end learning. 2S SPO+ PFYL DBB DPO Methods 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00PathAccuracy Path Accuracy for each Instance on Test Set Fig. 14: Learning curve, relative regret, and path accuracy for the shortest path problem on the test set: The methods in the experiment include a two- stage neural network, , , , and . The learning curve shows relative regret on the test set, and the box plot demonstrates the distribution of relative regret on the test set. For relative regret, lower is better. Finding #6 End-to-end learning is eâ†µective in rich contextual features such as im- ages. Moreover, the study highlights that can achieve impressive performance levels, even without knowing the costs during training. 8 Conclusion Because of the lack of easy-to-use generic tools, the potential power of the end- to-end predict-then-optimize has been underestimated or even overlooked in various applications. Our PyEPO package aims to alleviate barriers between the theory and practice of the end-to-end approach. PyEPO,the PyTorch-based end-to-end predict-then-optimize tool, is specif- ically designed for linear objective functions, including linear programming and 2-Stage, SPO+, and PFYL, all using a truncated ResNet-18, perform the best. DBB, originally benchmarked on this dataset, is far worse. SPO+ and PFYL match true optimal paths better than other methods, including 2-Stage. Summary â€¢ Predict-then-Optimize is a highly practical paradigm. â€¢ SPO+ and PFYL are very effective end-to-end learning methods. â€¢ The â€œnaiveâ€ 2-stage approach is sufficient training set is large. â€¢ Open questions: â€¢ Predictions in the constraints; see Hu, Xinyi, Jasper CH Lee, and Jimmy HM Lee. \"Branch & Learn with Post-hoc Correction for Predict+ Optimize with Unknown Parameters in Constraints.\"Â CPAIOR 2023. â€¢ Reducing training time; see work by Tias Guns and collaborators. â€¢ More applications; see work by B. Dilkina, M. Tambe, B. Wilder, H. Bastani Bo Tang Manuscript: https://arxiv.org/abs/2206.14234 Machine Learning for Integer Programming 56 Mixed-Integer Linear Programming SL + GNN [TMLR-22] [AAAI-22] [NeurIPS-20] SL + Simple models [IJCAI-17] [AAAI-16] Custom ML [AAAI-22] [NeurIPS-21] Branch Schedule heuristics Select nodes Detect backdoors Warmstart solver SL: Supervised Learning RL: Reinforcement Learning GNN: Graph Neural Networks Machine Learning for Discrete Optimization 57 MILP Column Generation Stochastic Programming Graph Optimization Multiobjective optimization RL + GNN [NeurIPS-22] [TMLR-22] [NeurIPS-17] SL + GNN [TMLR-22] [AAAI-22] [NeurIPS-20] SL + Simple models [IJCAI-17] [AAAI-16] [NeurIPS-22] https://arxiv.org/ abs/2307.03171 Custom ML [AAAI-22] [NeurIPS-21] Survey on GNN for CombOpt [JMLR 2023]https://tinyurl.com/ACP23-PredictAndOptimize Lab Colab: PyEPO Github/Docs: https://github.com/khalil-research/PyEPO","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_notes_OLD_PARTIAL/Khalil23predictThenOptimize.pdf","text":"Predict-then-Optimize: a tour of the state-of-the-art using Elias B. Khalil Department of Mechanical & Industrial Engineering SCALE AI Research Chair in Data-Driven Algorithms for Modern Supply Chains Bo Tang For complete references to related work, please check our arXiv manuscript linked in the GitHub. Optimization with a linear objective 2 min w {c⊺w : w ∈ 𝒮} Decision variables Linear cost function {{{Feasible set Solution: use appropriate algorithm depending on type of feasible set (MILP, MIQCP, CP, custom algorithms…) 3 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮} min w {c⊺ nw : w ∈ 𝒮} The cost vectors could be completely unrelated n… 4 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮} min w {c⊺ nw : w ∈ 𝒮}… x1 ∈ ℝ p x2… xn Instance feature vector (Observed) 5 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮}… x1 ∈ ℝ p x2… xn ci = g(xi; θ) ∈ ℝ d min w {c⊺ nw : w ∈ 𝒮} Instance feature vector (Observed) Function is parametrized by a vector .g θ 6 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮}… x1 ∈ ℝ p x2… xn ci = g(xi; θ) ∈ ℝ d min w {c⊺ nw : w ∈ 𝒮} Instance feature vector (Observed) If you know then you can evaluate to obtain and optimize with your favorite method θ g(xi; θ) ci 7 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮}… x1 ∈ ℝ p x2… xn ̂ci = g(xi; θ) ∈ ℝ d ̂c ̂c min w {c⊺ nw : w ∈ 𝒮}̂c Instance feature vector (Observed) If you know then you can evaluate to obtain and optimize with your favorite method θ g(xi; θ) ci What if you don’t know and you only observe ? θ xi 8 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮}… x1 ∈ ℝ p x2… xn ̂ci = g(xi; θ) ∈ ℝ d ̂c ̂c min w {c⊺ nw : w ∈ 𝒮}̂c Instance feature vector (Observed) Assume these instances are from the past, i.e., “training” instances n True cost vector (Observed for training instances only) c1 ∈ ℝ d c2 cn… 9 min w {c⊺ 1 w : w ∈ 𝒮} Same Decision variables{{Same Feasible set min w {c⊺ 2 w : w ∈ 𝒮}… x1 ∈ ℝ p x2… xn ̂ci = g(xi; θ) ∈ ℝ d ̂c ̂c min w {c⊺ nw : w ∈ 𝒮}̂c Instance feature vector (Observed) Assume these instances are from the past, i.e., “training” instances n Goal: given training set , learn that approximates groundtruth vectors “well” 𝒟 = {(xi, ci)} n i=1 θ ci True cost vector (Observed for training instances only) c1 ∈ ℝ d c2 cn… Predict-then-Optimize Training 10 Predict-then-Optimize Training and Test-time inference 11 Predict-then-Optimize s-t shortest path: training data Google Maps, Montréal, Quebéc, Canada 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 12 Predict-then-Optimize s-t shortest path: full pipeline 13 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 Travel time prediction model (regression) for each road segment Predicted travel times <=> s-t shortest path cost vector Predict-then-Optimize s-t shortest path: full pipeline 14 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 Travel time prediction model (regression) for each road segment Predicted travel times <=> s-t shortest path cost vector ?? Predict-then-Optimize s-t shortest path: full pipeline 15 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 Travel time prediction model (regression) for each road segment Predicted travel times <=> s-t shortest path cost vector ?? Squared error between and ci ̂ci Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 16 Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 17 Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs Edge 2 optimal Edge 1 optimal Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 18 Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs Edge 2 optimal Edge 1 optimal !! Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 19 Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs “Smart Predict then Optimize” Edge 2 optimal Edge 1 optimal Edge 2 optimal Edge 1 optimal End-to-End Predict-then-Optimize Training 20 Decision error 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice.{Optimal solution if you optimize with ̂c Optimal value with true costs c{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Easy: gradient of predicted costs to model parameters!{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Easy: gradient of predicted costs to model parameters!{Trickier: how does the decision loss vary with the predicted costs?{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. ?? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Trickier: how does the decision loss vary with the predicted costs?{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Trickier: how does the decision loss vary with the predicted costs?{ 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Crux: how does the optimum change with the predicted costs? 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. SPO+: a principled and effective method Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. 28 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. is not differentiable w.r.t. ̂c Elmachtoub and Grigas: Smart “Predict, then Optimize” 11 Figure 1 Geometric Illustration of SPO Loss (a) Polyhedral feasible region (b) Elliptic feasible region Note. In these two ﬁgures, we consider a two-dimensional polyhedron and ellipse for the feasible region S.We plot the (negative) of the true cost vector c,as wellas two candidate predictions ˆcA and ˆcB that are equidistant from c and thus have equivalent LS loss. One can see that the optimal decision for ˆcA coincides with that of c,since w∗(ˆcA)= w∗(c).In the polyhedron example,any predicted cost vector whose negative is not in the gray region will result in a wrong decision, where as in the ellipse example anypredicted cost vector that is not exactlyparallel with c results in a wrong decision. direction and parallel to c. Deﬁnition 1 formalizes this true SPO loss associated with making the prediction ˆc when the actual cost vector is c,given a particular oracle w∗(·) for P (·). Definition 1 (SPO Loss). Given a cost vector prediction ˆc and a realized cost vector c,the true SPO loss ℓw∗ SPO(ˆc, c) w.r.t. optimization oracle w∗(·) is deﬁned as ℓw∗ SPO(ˆc, c):= c T w∗(ˆc) − z∗(c) . Note that there is an unfortunate deﬁciency in Deﬁnition 1, which is the dependence on the particular oracle w∗(·) used to solve (2). Practically speaking, this deﬁciency is not a major issue since we should usually expect w∗(ˆc) to be a unique optimal solution, i.e., we should expect W ∗(ˆc) to be a singleton. Note that if any solution from W ∗(ˆc) may be used by the loss function, then the loss function essentially becomes minw∈W ∗(ˆc) c T w − z∗(c).Thus, a prediction model would then be incentivized to always make the degenerate prediction ˆc =0 since W ∗(0) = S.This would then imply that the SPO loss is 0. In any case, if one wishes to address the dependence on the particular oracle w∗(·) in Deﬁnition 1, then it is most natural to “break ties” by presuming that the implemented decision has worst-case behavior with respect to c.Deﬁnition 2 is an alternative SPO loss function that does not depend on the particular choice of the optimization oracle w∗(·). 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. SPO+: a principled and effective method Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. 29 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. is not differentiable w.r.t. ̂c Title Suppressed Due to Excessive Length 11 Fig. 3: As shown for the learning curves of the training of and on the shortest path, regret and unambiguous regret in the various tasks overlap almost exactly. Besides regret, decision error can also be deﬁned as the di↵erence between the true solution and its prediction, such as Hamming distance of solutions [35] and squared error of the solutions [6]. In addition, Dalle et al. [10] also considered treating the objective value c|w⇤ ˆc itself as a loss. 3.4 Methodologies 3.4.1 Smart Predict-then-Optimize [16] To make the decision error di↵erentiable, Elmachtoub and Grigas [16] proposed , a convex upper bound on the regret: lSP O+(ˆc, c)= \u0000min w2S{(2ˆc \u0000 c)|w} +2ˆc|w⇤(c) \u0000 z⇤(c). (5) One proposed subgradient for this loss writes as follows: 2(w⇤(c) \u0000 w⇤(2ˆc \u0000 c)) 2 @lSPO+(ˆc, c) @ ˆc (6) Thus, we can use Algorithm 1 to directly minimize lSPO+(ˆc, c) with gradi- ent descent. This algorithm with requires solving minw2S(2ˆc \u0000 c)|w for each training iteration. To accelerate the training, Mandi et al. [27] employed relaxations ( ) and warm starting ( ) to speed-up the optimization. The idea of is to use the continuous relaxation of the integer program during training. This simpliﬁcation greatly reduces the training time at the expense of model performance. Compared to , the improvement of in training e\u0000ciency is not negligible. For example, linear programming can be solved in polynomial time while integer programming is worst-case exponential. In Section 6, we will further discuss this performance-e\u0000ciency tradeo↵. For , Mandi et al. [27] suggested using previous solutions as a is a convex upper bound on regret and has subgradients 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. SPO+: a principled and effective method Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. 30 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. is not differentiable w.r.t. ̂c Title Suppressed Due to Excessive Length 11 Fig. 3: As shown for the learning curves of the training of and on the shortest path, regret and unambiguous regret in the various tasks overlap almost exactly. Besides regret, decision error can also be deﬁned as the di↵erence between the true solution and its prediction, such as Hamming distance of solutions [35] and squared error of the solutions [6]. In addition, Dalle et al. [10] also considered treating the objective value c|w⇤ ˆc itself as a loss. 3.4 Methodologies 3.4.1 Smart Predict-then-Optimize [16] To make the decision error di↵erentiable, Elmachtoub and Grigas [16] proposed , a convex upper bound on the regret: lSP O+(ˆc, c)= \u0000min w2S{(2ˆc \u0000 c)|w} +2ˆc|w⇤(c) \u0000 z⇤(c). (5) One proposed subgradient for this loss writes as follows: 2(w⇤(c) \u0000 w⇤(2ˆc \u0000 c)) 2 @lSPO+(ˆc, c) @ ˆc (6) Thus, we can use Algorithm 1 to directly minimize lSPO+(ˆc, c) with gradi- ent descent. This algorithm with requires solving minw2S(2ˆc \u0000 c)|w for each training iteration. To accelerate the training, Mandi et al. [27] employed relaxations ( ) and warm starting ( ) to speed-up the optimization. The idea of is to use the continuous relaxation of the integer program during training. This simpliﬁcation greatly reduces the training time at the expense of model performance. Compared to , the improvement of in training e\u0000ciency is not negligible. For example, linear programming can be solved in polynomial time while integer programming is worst-case exponential. In Section 6, we will further discuss this performance-e\u0000ciency tradeo↵. For , Mandi et al. [27] suggested using previous solutions as a is a convex upper bound on regret and has subgradients Computational overhead: To compute SPO+ loss, we need to solve an optimization problem in the forward pass; this is shared with other methods. Why regression on cost coefficients may fail Vertical axis: edge cost. Horizontal axis: feature x 31 Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Elmachtoub and Grigas: Smart “Predict, then Optimize” 15 Figure 3 Illustrative Example. Note. The circles correspond to edge 1 costs and the squares correspond to edge 2 costs. Red lines and points correspond to the least squares ﬁt and predictions, while green lines and points correspond to the SPO ﬁt and predictions. The vertical dotted lines correspond to the decision boundaries under the true and prediction models. The SPO+ decision boundary in this stylized example coincides with the SPO decision boundary. ﬁnding the prediction model that minimizes the empirical risk using the SPO+ loss, this prediction model will also approximately minimize (4), the empirical risk using the SPO loss. To begin the derivation of the SPO+ loss, we ﬁrst observe that for any α ∈ R,the SPO loss can be written as ℓSPO(ˆc, c)= max w∈W ∗(ˆc) { c T w − αˆc T w} + αz∗(ˆc) − z∗(c) (5) Figures from Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science 68.1 (2022): 9-26. s t Edge 1 Edge 2 Edge 1 optimal Edge 2 optimal Least-squares regression on dataset of (x, c) pairs SPO+ Edge 2 optimal Edge 1 optimal Edge 2 optimal Edge 1 optimal Predict-then-Optimize s-t shortest path: training data Google Maps, Montréal, Quebéc, Canada 8 am 10 am 12 pm 4 pm c1,k c2,k c3,k c4,k x1 x2 x3 x4 32 Predict-then-Optimize s-t shortest path: training data Google Maps, Montréal, Quebéc, Canada 8 am 10 am 12 pm 4 pm w* 1 w* 2 x1 x2 x3 x4 33 What if the historical cost vectors were unavailable, but the optimal solutions were? 𝒟 = {(xi, w* i )}n i=1 w* 3 w* 4 PFYL: Perturbed Fenchel-Young Loss Berthet, Quentin, et al. \"Learning with diﬀerentiable perturbed optimizers.\" NeurIPS 2020. 34 Recall the required gradient: 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Title Suppressed Due to Excessive Length 13 constant function w⇤(ˆc), E⇠[w⇤(ˆc + \u0000⇠)] varies the proportions in response to the change of ˆc, providing a nonzero gradient of ˆc: @ E⇠[w⇤(ˆc + \u0000⇠)] @ ˆc ⇡ 1 K KX  w⇤(ˆc + \u0000⇠)⇠. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: ˆc, K, \u0000 1: for sample  2 {1,...,K} do 2: Generate Gaussian noise ⇠ 3: Solve: w⇠  := w⇤(ˆc + \u0000⇠) 4: Save w⇠  and ⇠ for backward pass 5: end for 6: return 1 K PK  w⇠  Algorithm 5 DPO Backward Pass Require: @l(·) @ E⇠[w⇤] , K 1: Load w⇠  and ⇠ from forward pass 2: Compute @ E⇠[w⇤] @ ˆc := 1 K PK  w⇠ ⇠ 3: Compute l(·) @ ˆc := @l(·) @ E⇠[w⇤] @ E⇠[w⇤] @ ˆc 4: return l(·) @ ˆc 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(ˆc, w⇤(c)) and gradient @lFY(ˆc,w⇤(c)) @ ˆc . Compared to , avoids the ine\u0000cient calcula- tion of the Jacobian matrix rT w⇤(ˆc) and includes a theoretically sound loss function. The loss of is based on Fenchel duality: The expectation of the per- turbed minimizer is deﬁned as F (c)= E⇠[min w2S{(c + \u0000⇠)|w}], and the dual of F (c), denoted by ⌦(w⇤(c)), is utilized to deﬁne the Fenchel-Young loss: lFY(ˆc, w⇤(c)) = ˆc|w⇤(c) \u0000 F (ˆc) \u0000 ⌦(w⇤(c)), then the gradient of the loss is @lFY(ˆc, w⇤(c)) @ ˆc = w⇤(c) \u0000 @F (ˆc) @ ˆc = w⇤(c) \u0000 E ⇠ [argmin w2S {(ˆc + \u0000⇠)|w}]. Similar to , we can estimate the well-deﬁned gradient as @lFY(ˆc, w⇤(c)) @ ˆc ⇡ w⇤(c) \u0000 1 K KX  argmin w2S {(ˆc + \u0000⇠)|w} 4 Implementation and Modeling The core module of PyEPO is an “autograd” function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero Average optimal solution over randomly perturbed costsK{ A random perturbation of predicted costs (σξk) ̂c{ PFYL: Perturbed Fenchel-Young Loss Berthet, Quentin, et al. \"Learning with diﬀerentiable perturbed optimizers.\" NeurIPS 2020. 35 Recall the required gradient: 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Title Suppressed Due to Excessive Length 13 constant function w⇤(ˆc), E⇠[w⇤(ˆc + \u0000⇠)] varies the proportions in response to the change of ˆc, providing a nonzero gradient of ˆc: @ E⇠[w⇤(ˆc + \u0000⇠)] @ ˆc ⇡ 1 K KX  w⇤(ˆc + \u0000⇠)⇠. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: ˆc, K, \u0000 1: for sample  2 {1,...,K} do 2: Generate Gaussian noise ⇠ 3: Solve: w⇠  := w⇤(ˆc + \u0000⇠) 4: Save w⇠  and ⇠ for backward pass 5: end for 6: return 1 K PK  w⇠  Algorithm 5 DPO Backward Pass Require: @l(·) @ E⇠[w⇤] , K 1: Load w⇠  and ⇠ from forward pass 2: Compute @ E⇠[w⇤] @ ˆc := 1 K PK  w⇠ ⇠ 3: Compute l(·) @ ˆc := @l(·) @ E⇠[w⇤] @ E⇠[w⇤] @ ˆc 4: return l(·) @ ˆc 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(ˆc, w⇤(c)) and gradient @lFY(ˆc,w⇤(c)) @ ˆc . Compared to , avoids the ine\u0000cient calcula- tion of the Jacobian matrix rT w⇤(ˆc) and includes a theoretically sound loss function. The loss of is based on Fenchel duality: The expectation of the per- turbed minimizer is deﬁned as F (c)= E⇠[min w2S{(c + \u0000⇠)|w}], and the dual of F (c), denoted by ⌦(w⇤(c)), is utilized to deﬁne the Fenchel-Young loss: lFY(ˆc, w⇤(c)) = ˆc|w⇤(c) \u0000 F (ˆc) \u0000 ⌦(w⇤(c)), then the gradient of the loss is @lFY(ˆc, w⇤(c)) @ ˆc = w⇤(c) \u0000 @F (ˆc) @ ˆc = w⇤(c) \u0000 E ⇠ [argmin w2S {(ˆc + \u0000⇠)|w}]. Similar to , we can estimate the well-deﬁned gradient as @lFY(ˆc, w⇤(c)) @ ˆc ⇡ w⇤(c) \u0000 1 K KX  argmin w2S {(ˆc + \u0000⇠)|w} 4 Implementation and Modeling The core module of PyEPO is an “autograd” function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero Average optimal solution over randomly perturbed costsK{ A random perturbation of predicted costs (σξk) ̂c{ Notice the true costs do not appear here.c PFYL: Perturbed Fenchel-Young Loss Berthet, Quentin, et al. \"Learning with diﬀerentiable perturbed optimizers.\" NeurIPS 2020. 36 Recall the required gradient: 10 Bo Tang, Elias B. Khalil Algorithm 1 End-to-end Gradient Descent Require: coe\u0000cient matrix A,right-handside b,data D 1: Initialize predictor parameters ✓ for predictor g(x; ✓) 2: for epochs do 3: for each batch of training data (x, c) do 4: Sample batch of the cost vectors c with the corresponding features x 5: Predict cost using predictor ˆc := g(x; ✓) 6: Forward pass to compute optimal solution w⇤(ˆc):=argminw2S ˆc|w 7: Forward pass to compute decision loss l(ˆc, ·) 8: Backward pass from loss l(ˆc, ·)toupdateparameters ✓ with gradient 9: end for 10: end for @l(ˆc, ·) @✓ = @l(ˆc, ·) @ ˆc @ ˆc @✓ = @l(ˆc, ·) @w⇤(ˆc) @w⇤(ˆc) @ ˆc @ ˆc @✓ Note: @ ˆc @✓ = @g(x; ✓) @✓ (3) The last term @ ˆc @✓ is the gradient of the predictions w.r.t. the model param- eters, which is trivial to calculate in modern deep learning frameworks. The challenging part is to compute the gradient of di↵erentiable optimizer @l(ˆc,·) @ ˆc or the direct decision loss function @w⇤(ˆc) @ ˆc . Because the optimal solution w⇤(c) for linear and integer programming is a piecewise constant function from cost vec- tor c to solution vector w⇤, the predictor parameters cannot be updated with gradient descent. Thus, and , as direct decision loss functions, de- rive surrogate @l(ˆc,·) @ ˆc , measuring decision errors with respect to speciﬁc losses, while and , as di↵erentiable optimizers, compute approximate @w⇤(ˆc) @ ˆc , allowing customized loss functions. 3.3.1 Decision Loss To measure the error in decision-making, the notion of regret (also called SPO Loss [16]) has been proposed and is deﬁned as the di↵erence in objective value between an optimal solution (using the true but unknown cost vector) and one obtained using the predicted cost vector: lRegret(ˆc, c)= c|w⇤(ˆc) \u0000 z⇤(c). (4) Given a cost vector ˆc, there may be multiple optimal solutions to minw2S ˆc|w. Therefore, Elmachtoub and Grigas [16] devised the “unambiguous” regret (also called unambiguous SPO Loss): lURegret(ˆc, c) = maxw2W ⇤(c)w|c\u0000z⇤(c). This loss considers the worst case among all optimal solutions w.r.t. the predicted cost vector. PyEPO provides an evaluation module (Section 4.5) that includes both the regret and the unambiguous regret. However, as Figure 3 shows, the regret and the unambiguous regret are almost the same in all training proce- dures. Therefore, although unambiguous regret is more theoretically rigorous, it is not necessary to consider it in practice. Title Suppressed Due to Excessive Length 13 constant function w⇤(ˆc), E⇠[w⇤(ˆc + \u0000⇠)] varies the proportions in response to the change of ˆc, providing a nonzero gradient of ˆc: @ E⇠[w⇤(ˆc + \u0000⇠)] @ ˆc ⇡ 1 K KX  w⇤(ˆc + \u0000⇠)⇠. The forward pass and backward pass are as follows: Algorithm 4 DPO Forward Pass Require: ˆc, K, \u0000 1: for sample  2 {1,...,K} do 2: Generate Gaussian noise ⇠ 3: Solve: w⇠  := w⇤(ˆc + \u0000⇠) 4: Save w⇠  and ⇠ for backward pass 5: end for 6: return 1 K PK  w⇠  Algorithm 5 DPO Backward Pass Require: @l(·) @ E⇠[w⇤] , K 1: Load w⇠  and ⇠ from forward pass 2: Compute @ E⇠[w⇤] @ ˆc := 1 K PK  w⇠ ⇠ 3: Compute l(·) @ ˆc := @l(·) @ E⇠[w⇤] @ E⇠[w⇤] @ ˆc 4: return l(·) @ ˆc 3.5 Perturbed Fenchel-Young Loss [6] Instead of an arbitrary loss for , Berthet et al. [6] further construct the Fenchel-Young loss [7] to directly compute the decision error lFY(ˆc, w⇤(c)) and gradient @lFY(ˆc,w⇤(c)) @ ˆc . Compared to , avoids the ine\u0000cient calcula- tion of the Jacobian matrix rT w⇤(ˆc) and includes a theoretically sound loss function. The loss of is based on Fenchel duality: The expectation of the per- turbed minimizer is deﬁned as F (c)= E⇠[min w2S{(c + \u0000⇠)|w}], and the dual of F (c), denoted by ⌦(w⇤(c)), is utilized to deﬁne the Fenchel-Young loss: lFY(ˆc, w⇤(c)) = ˆc|w⇤(c) \u0000 F (ˆc) \u0000 ⌦(w⇤(c)), then the gradient of the loss is @lFY(ˆc, w⇤(c)) @ ˆc = w⇤(c) \u0000 @F (ˆc) @ ˆc = w⇤(c) \u0000 E ⇠ [argmin w2S {(ˆc + \u0000⇠)|w}]. Similar to , we can estimate the well-deﬁned gradient as @lFY(ˆc, w⇤(c)) @ ˆc ⇡ w⇤(c) \u0000 1 K KX  argmin w2S {(ˆc + \u0000⇠)|w} 4 Implementation and Modeling The core module of PyEPO is an “autograd” function which is inherited from PyTorch [32]. Such functions implement a forward pass that yields optimal solutions or decision losses directly and a backward pass to obtain non-zero Average optimal solution over randomly perturbed costsK{ A random perturbation of predicted costs (σξk) ̂c{ Notice the true costs do not appear here.c Computational overhead: To compute the PFYL gradient, we need to solve optimization problems. K Avoiding solver calls by Learning to Rank Mandi, Jayanta, et al. \"Decision-focused learning: through the lens of learning to rank.\" ICML, 2022. Mulamba, Maxime, et al. \"Contrastive Losses and Solution Caching for Predict-and-Optimize.\" IJCAI, 2021. 37 Recall that both SPO+ and PFYL made one or more calls to the solver in each forward pass! w1 w2 w3 Objective using prediction A Objective using prediction B True Objective Sol 1 0 1 0 1 3 1 Sol 2 1 0 0 3 2 2 Sol 3 1 1 1 2 1 3 Avoiding solver calls by Learning to Rank Mandi, Jayanta, et al. \"Decision-focused learning: through the lens of learning to rank.\" ICML, 2022. Mulamba, Maxime, et al. \"Contrastive Losses and Solution Caching for Predict-and-Optimize.\" IJCAI, 2021. 38 w1 w2 w3 Objective using prediction A Objective using prediction B True Objective Sol 1 0 1 0 1 3 1 Sol 2 1 0 0 3 2 2 Sol 3 1 1 1 2 1 3{Terrible solution ranking!{Somewhat better… Avoiding solver calls by Learning to Rank Mandi, Jayanta, et al. \"Decision-focused learning: through the lens of learning to rank.\" ICML, 2022. Mulamba, Maxime, et al. \"Contrastive Losses and Solution Caching for Predict-and-Optimize.\" IJCAI, 2021. 39 w1 w2 w3 Objective using prediction A Objective using prediction B True Objective Sol 1 0 1 0 1 3 1 Sol 2 1 0 0 3 2 2 Sol 3 1 1 1 2 1 3{Terrible solution ranking!{Somewhat better… Key Idea: Learn to predict coefficients that lead to good rankings of a set of collected solutions! 40 Predict-then-Optimize: a tour of the state-of-the-art using Bo Tang Manuscript describing PyEPO, the literature, and extensive experiments: https://arxiv.org/abs/2206.14234 4142 Benchmarks Optimization modeling ML modeling Training algorithms Shortest path Knapsack TSP Gurobi Any custom solver SPO+ SPO+ with relaxations PFYL/DPO DBB (Pogancic et al. [2019]) Benchmark generation Based on Elmachtoub & Grigas 43 s t cij = α((ℬxi)j + 3)deg ⋅ ϵi,j xi ∈ ℝ p i-th instance’s feature vector (Observed) j deg Integer in [1,6] regulating how non- linear the mapping is ℬ ∈ {0,1} d×n ℬp,q ∼ Bernoulli(0.5) ϵi,j Uniform random noise Warcraft Shortest-Path Benchmark Based on Pogančić, Marin Vlastelica, et al. \"Diﬀerentiation of blackbox combinatorial solvers.\" ICLR 2020. 44 24 Bo Tang, Elias B. Khalil Feature 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Solution Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k ⇥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with di↵erent predictors and / / with a linear prediction model g(x; ✓). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for di↵erent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared di↵erence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Di↵erent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width ¯✏ 2 {0.0, 0.5}. We then conduct Image inputs (RGB features) True costs Optimal solution (NW -> SE) Optimization model (in Gurobi) 45 Title Suppressed Due to Excessive Length 17 4.1.2 Optimization Model with Gurobi On the other hand, we provide optGrbModel to create an optimization model with GurobiPy. Unlike optModel, optGrbModel is more lightweight but less ﬂexible for users. Let us use the following optimization model (7) as an exam- ple, where ci is an unknown cost coe\u0000cient: max x 4X i=0 cixi s.t. 3x0 +4x1 +3x2 +6x3 +4x4  12 4x0 +5x1 +2x2 +3x3 +5x4  10 5x0 +4x1 +6x2 +2x3 +3x4  15 8xi 2 {0, 1} (7) Inheriting optGrbModel is the convenient way to use Gurobi with PyEPO. The only implementation required is to override getModel and return a Gurobi model and the corresponding decision variables. In addition, there is no need to assign a value to the attribute modelSense in optGrbModel manually. An example for Model (7) is as follows: 1 import gurobipy as gp 2 from gurobipy import GRB 3 from pyepo . model . grb import optGrbModel 4 5 class myModel ( optGrbModel ) : 6 def _getModel ( self ) : 7 # ceate a model 8 m= gp . Model () 9 # varibles 10 x= m . addVars (5 , name = \" x \" ,vtype = GRB . BINARY ) 11 # sense ( must be minimize ) 12 m. modelSense = GRB . MAXIMIZE 13 # constraints 14 m. addConstr (3* x [0]+4* x [1]+3* x [2]+6* x [3]+4* x [4] <=12) 15 m. addConstr (4* x [0]+5* x [1]+2* x [2]+3* x [3]+5* x [4] <=10) 16 m. addConstr (5* x [0]+4* x [1]+6* x [2]+2* x [3]+3* x [4] <=15) 17 return m, x 18 19 optmodel = myModel () 4.1.3 Optimization Model with Pyomo Similarly, optOmoModel allows modeling mathematical programs with Py- omo. In contrast to optGrbModel, optOmoModel requires an explicit object attribute modelSense.Since Pyomo supports multiple solvers, instantiating an optOmoModel requires a parameter solver to specify the solver. The fol- lowing is an implementation of problem 7: 1 from pyomo import environ as pe 2 from pyepo import EPO Title Suppressed Due to Excessive Length 17 4.1.2 Optimization Model with Gurobi On the other hand, we provide optGrbModel to create an optimization model with GurobiPy. Unlike optModel, optGrbModel is more lightweight but less ﬂexible for users. Let us use the following optimization model (7) as an exam- ple, where ci is an unknown cost coe\u0000cient: max x 4X i=0 cixi s.t. 3x0 +4x1 +3x2 +6x3 +4x4  12 4x0 +5x1 +2x2 +3x3 +5x4  10 5x0 +4x1 +6x2 +2x3 +3x4  15 8xi 2 {0, 1} (7) Inheriting optGrbModel is the convenient way to use Gurobi with PyEPO. The only implementation required is to override getModel and return a Gurobi model and the corresponding decision variables. In addition, there is no need to assign a value to the attribute modelSense in optGrbModel manually. An example for Model (7) is as follows: 1 import gurobipy as gp 2 from gurobipy import GRB 3 from pyepo . model . grb import optGrbModel 4 5 class myModel ( optGrbModel ) : 6 def _getModel ( self ) : 7 # ceate a model 8 m= gp . Model () 9 # varibles 10 x= m . addVars (5 , name = \" x \" ,vtype = GRB . BINARY ) 11 # sense ( must be minimize ) 12 m. modelSense = GRB . MAXIMIZE 13 # constraints 14 m. addConstr (3* x [0]+4* x [1]+3* x [2]+6* x [3]+4* x [4] <=12) 15 m. addConstr (4* x [0]+5* x [1]+2* x [2]+3* x [3]+5* x [4] <=10) 16 m. addConstr (5* x [0]+4* x [1]+6* x [2]+2* x [3]+3* x [4] <=15) 17 return m, x 18 19 optmodel = myModel () 4.1.3 Optimization Model with Pyomo Similarly, optOmoModel allows modeling mathematical programs with Py- omo. In contrast to optGrbModel, optOmoModel requires an explicit object attribute modelSense.Since Pyomo supports multiple solvers, instantiating an optOmoModel requires a parameter solver to specify the solver. The fol- lowing is an implementation of problem 7: 1 from pyomo import environ as pe 2 from pyepo import EPO Creating a dataset based on features and true costs 46 20 Bo Tang, Elias B. Khalil 1 import pyepo 2 # init Fenchel - Young loss 3 pfyl = pyepo . func . p e r t u r b e d F e n c h e l Y o u n g ( optmodel , n_samples =3 , sigma =1.0 , processes =8) The below code block illustrates the calculation of Fenchel-Young loss: 1 # calculate Fenchel - Young loss 2 loss = pfyl ( pred_cost , true_sol ) 4.3 The optDataset Class for Managing Data The utilization of decision losses, such as and , necessitates the availability of true optimal solutions. Therefore, to facilitate convenience in PyEPO training and testing, an auxiliary optDataset has been introduced, which is not strictly indispensable. optDataset stores the features and their associated costs of the objective function and solves optimization problems to get optimal solutions and corresponding objective values. optDataset is extended from PyTorch Dataset. In order to obtain optimal solutions, optDataset requires the corresponding optModel (see in Section 4.1). The parameters for the optDataset are as follows: – model: an instance of optModel; – feats: data features; – costs: corresponding costs of objective function; Then, as the following example, PyTorch DataLoader receives an optDataset and wraps the data samples and acts as a sampler that provides an iterable over the given dataset. It is required to provide the batch size which is the number of training samples that will be used in each update of the model parameters. 1 import pyepo 2 from torch . utils . data import DataLoader 3 4 # build dataset 5 dataset = pyepo . data . dataset . optDataset ( optmodel , feats , costs ) 6 7 # get data loader 8 dataloader = DataLoader ( dataset , batch_size =32 , shuffle = True ) By iterating over the DataLoader, we can obtain a batch of features, true costs, optimal solutions, and corresponding objective values: 1 for x, c, w, z in dataloader : 2 # a batch of features 3 print (x) 4 # a batch of true costs 5 print (c) 6 # a batch of true optimal solutions 7 print (w) 8 # a batch of true optimal objective values 9 print (z) 20 Bo Tang, Elias B. Khalil 1 import pyepo 2 # init Fenchel - Young loss 3 pfyl = pyepo . func . p e r t u r b e d F e n c h e l Y o u n g ( optmodel , n_samples =3 , sigma =1.0 , processes =8) The below code block illustrates the calculation of Fenchel-Young loss: 1 # calculate Fenchel - Young loss 2 loss = pfyl ( pred_cost , true_sol ) 4.3 The optDataset Class for Managing Data The utilization of decision losses, such as and , necessitates the availability of true optimal solutions. Therefore, to facilitate convenience in PyEPO training and testing, an auxiliary optDataset has been introduced, which is not strictly indispensable. optDataset stores the features and their associated costs of the objective function and solves optimization problems to get optimal solutions and corresponding objective values. optDataset is extended from PyTorch Dataset. In order to obtain optimal solutions, optDataset requires the corresponding optModel (see in Section 4.1). The parameters for the optDataset are as follows: – model: an instance of optModel; – feats: data features; – costs: corresponding costs of objective function; Then, as the following example, PyTorch DataLoader receives an optDataset and wraps the data samples and acts as a sampler that provides an iterable over the given dataset. It is required to provide the batch size which is the number of training samples that will be used in each update of the model parameters. 1 import pyepo 2 from torch . utils . data import DataLoader 3 4 # build dataset 5 dataset = pyepo . data . dataset . optDataset ( optmodel , feats , costs ) 6 7 # get data loader 8 dataloader = DataLoader ( dataset , batch_size =32 , shuffle = True ) By iterating over the DataLoader, we can obtain a batch of features, true costs, optimal solutions, and corresponding objective values: 1 for x, c, w, z in dataloader : 2 # a batch of features 3 print (x) 4 # a batch of true costs 5 print (c) 6 # a batch of true optimal solutions 7 print (w) 8 # a batch of true optimal objective values 9 print (z) Creating an ML model with PyTorch 47 Title Suppressed Due to Excessive Length 21 4.4 End-to-End Training The core capability of PyEPO is to build an optimization model, and then embed the optimization model into a PyTorch neural network for the end-to- end training. Here, we build a simple linear regression model in PyTorch as an example: 1 from torch import nn 2 3 # construct linear model 4 class LinearRegression ( nn . Module ) : 5 def __init__ ( self ) : 6 super (LinearRegression , self ) . __init__ () 7 # size of input and output is the size of feature and cost 8 self . linear = nn . Linear ( num_feat , len_cost ) 9 def forward ( self , x ) : 10 out = self . linear ( x ) 11 return out 12 13 # init model 14 predmodel = L i ne a r R e g r e s s i o n () Then, we can train the prediction model with SPO+ loss to predict un- known cost coe\u0000cients, make decisions, and compute decision errors. The training of the prediction model is performed using a stochastic gradient de- scent (SGD) optimizer. By utilizing PyTorch automatic di↵erentiation capa- bilities, the gradients of the loss with respect to the model parameters are computed and used to update the model parameters during training. 1 import torch 2 3 # set SGD optimizer 4 optimizer = torch . optim . SGD ( predmodel . parameters () , lr =1 e -3) 5 6 # training 7 for epoch in range (num_epochs) : 8 # iterare features , costs , solutions , and objective values 9 for x, c, w, z in dataloader : 10 # forward pass 11 cp = predmodel ( x ) # predict costs 12 loss = spop ( cp , c , w , z ) . mean () # calculate SPO + loss 13 # backward pass 14 optimizer . zero_grad () # reset gradients to 0 15 loss . backward () # compute gradients 16 optimizer . step () # update model parameters 4.5 Metrics PyEPO provides evaluation functions to measure model performance, in par- ticular the two metrics mentioned in Section 3.3.1: regret and unambiguous regret. We further deﬁne the normalized (unambiguous) regret by Pntest i=1 lRegret(ˆci, ci) Pntest i=1 |z⇤(ci)| . End-to-end training! 48 Title Suppressed Due to Excessive Length 21 4.4 End-to-End Training The core capability of PyEPO is to build an optimization model, and then embed the optimization model into a PyTorch neural network for the end-to- end training. Here, we build a simple linear regression model in PyTorch as an example: 1 from torch import nn 2 3 # construct linear model 4 class LinearRegression ( nn . Module ) : 5 def __init__ ( self ) : 6 super (LinearRegression , self ) . __init__ () 7 # size of input and output is the size of feature and cost 8 self . linear = nn . Linear ( num_feat , len_cost ) 9 def forward ( self , x ) : 10 out = self . linear ( x ) 11 return out 12 13 # init model 14 predmodel = L i ne a r R e g r e s s i o n () Then, we can train the prediction model with SPO+ loss to predict un- known cost coe\u0000cients, make decisions, and compute decision errors. The training of the prediction model is performed using a stochastic gradient de- scent (SGD) optimizer. By utilizing PyTorch automatic di↵erentiation capa- bilities, the gradients of the loss with respect to the model parameters are computed and used to update the model parameters during training. 1 import torch 2 3 # set SGD optimizer 4 optimizer = torch . optim . SGD ( predmodel . parameters () , lr =1 e -3) 5 6 # training 7 for epoch in range (num_epochs) : 8 # iterare features , costs , solutions , and objective values 9 for x, c, w, z in dataloader : 10 # forward pass 11 cp = predmodel ( x ) # predict costs 12 loss = spop ( cp , c , w , z ) . mean () # calculate SPO + loss 13 # backward pass 14 optimizer . zero_grad () # reset gradients to 0 15 loss . backward () # compute gradients 16 optimizer . step () # update model parameters 4.5 Metrics PyEPO provides evaluation functions to measure model performance, in par- ticular the two metrics mentioned in Section 3.3.1: regret and unambiguous regret. We further deﬁne the normalized (unambiguous) regret by Pntest i=1 lRegret(ˆci, ci) Pntest i=1 |z⇤(ci)| . Experiments with TSP20 Vertical axis: average regret w.r.t. true OPT on unseen test instances 49 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as , , and . The normalized regret is visualized under di↵erent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of “2-stage Auto” is the mean squared error of the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ﬁve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. 2-stage methods: regress on true costs (no end-to-end training) Harder learning task Lower is better Experiments with TSP20 Vertical axis: average regret w.r.t. true OPT on unseen test instances 50 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 1000, Noise Half\u0000width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as , , and . The normalized regret is visualized under di↵erent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of “2-stage Auto” is the mean squared error of the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ﬁve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. 28 Bo Tang, Elias B. Khalil 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 5000, Noise Half\u0000width = 0.0 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB 1 2 4 6 Polynomial Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40NormalizedRegret TSP Training Set Size = 5000, Noise Half\u0000width = 0.5 2-stage LR 2-stage RF 2-stage Auto SPO+ PFYL DBB Fig. 8: Normalized regret for the TSP problem on the test set: There are 20 nodes to visit. The methods in the experiment include two-stage approaches with linear regression, random forest and Auto-Sklearn and end-to-end learn- ing such as , , and . The normalized regret is visualized under di↵erent sample sizes, noise half-width, and polynomial degrees. For normal- ized regret, lower is better. 6.2 Two-stage Method with Automated Hyperparameter Tuning This method leverages the sophisticated Auto-Sklearn [18] tool that uses bayesian optimization methods for automated hyperparameter tuning of Scikit-Learn regression models. The metric of “2-stage Auto” is the mean squared error of the predicted costs, which does not reduce decision error directly. Because of the limitation of multioutput regression in Auto-Sklearn v0.14.6, the choices of the predictor in 2-stage Auto only include ﬁve models: k-nearest neighbor (KNN), decision tree, random forest, extra-trees, and Gaussian process. Even with these limitations, Auto-Sklearn can achieve a low regret. Although the training of 2-stage Auto is time-consuming, it is still a competitive method. SPO+ and PFYL, both using a linear model perform the best. With more data, 2-stage Random Forest is competitive! 2-stage methods: regress on true costs (no end-to-end training) n=1000 training instances n=5000 training instances Regret-accuracy tradeoff 51 SPO+ and PFYL, both using a linear model perform the best. Title Suppressed Due to Excessive Length 33 0 20 40 60 80 MSE 0.050 0.075 0.100 0.125 0.150 0.175 0.200NormalizedRegret TSP Training Set Size = 1000, Polynomial degree = 4, Noise Half\u0000width = 0.5 Fig. 13: MSE v.s. Regret: The result covers di↵erent two-stage methods, , , and their regularization. is omitted because it is far away from others. The size of the circles is proportional to the training time (Sec), so the smaller is better. Finding #5 Generally, and can achieve good decisions at the cost of pre- diction accuracy. If one is seeking a balanced tradeo↵ between decision quality and prediction accuracy, an end-to-end method with prediction regularization may be preferable. 7 Empirical Evaluation for Image-Based Shortest Path Following Poganˇci´c et al. [35] and Berthet et al. [6], we employ a truncated ResNet18 convolutional neural network (CNN) architecture consisting of the ﬁrst ﬁve layers on Warcraft terrain images (refer to Section 5.2). As Table 5 shows, the methods we compare include a two-stage method, , , , and with truncated ResNet18. We train the CNN over 50 epochs with batches of size 70. The learning rate is set to 0.0005 decaying at the epochs 30 and 40, and the hyperparameters n =1, \u0000 = 1 for and , \u0000 = 10 for . We use the Hamming distance for and the squared error of solutions for , which are the loss functions used in the original papers. The sample size of the test set ntest is 1000. To evaluate our methods, we compute the relative regret c|w⇤(ˆc)\u0000z⇤(c) z⇤(c) and path accuracy Pd j=1 (z⇤(c)j =z⇤(ˆc)j ) d per instance on the test set; the latter is simply the fraction of edges in the “predicted” solution that are also in the optimal solution. As shown in Figure 14, the two-stage method, and achieve com- parable levels of performance in predicting the shortest path on the Warcraft Vertical axis: average regret w.r.t. true OPT on unseen test instances Horizontal axis: Mean- Squared Error on true costs 30 Bo Tang, Elias B. Khalil Fig. 10: Normalized regret for the 2D knapsack (at the top) and TSP (at the bottom) on the test set: The methods in the experiment include , and w/o relaxation. Then, we visualize the normalized regret under di↵erent sample sizes and polynomial degrees to investigate the impact of the relaxation method. For normalized regret, lower is better. a tighter bound does reduce the regret, and shows advantages over . Overall, using relaxations achieves fairly good performance with improved computational e\u0000ciency. Moreover, formulations with tighter linear relaxation lead to better performance. Finding #3 End-to-end predict-then-optimize with relaxation has excellent poten- tial to improve computation e\u0000ciency at a slight degradation in perfor- mance, particularly when the true cost-generating function is not very non-linear. 6.4 Prediction Regularization As proposed in Elmachtoub and Grigas [16], the mean absolute error lMAE(ˆc, c)= 1 n Pn i kˆci \u0000 cik1 or mean squared error lMSE(ˆc, c)= 1 2n Pn i kˆci \u0000 cik 2 2 of the predicted cost vector w.r.t. true cost vector can be added to the decision loss as l1 or l2 regularizers. When using regularization, we set either the l1 regular- ization parameter \u00001 and the l2 regularization parameter \u00002 from 0.001 to 10 logarithmically. For the experiments, we still use the same instances, model, and hyperparameters as before, while the number of training samples n,the noise half-width ¯✏ and, the polynomial degree deg are ﬁxed at 1000, 0.5 and 4. Warcraft Shortest-Path Benchmark Based on Pogančić, Marin Vlastelica, et al. \"Diﬀerentiation of blackbox combinatorial solvers.\" ICLR 2020. 52 24 Bo Tang, Elias B. Khalil Feature 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Solution Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k ⇥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with di↵erent predictors and / / with a linear prediction model g(x; ✓). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for di↵erent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared di↵erence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Di↵erent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width ¯✏ 2 {0.0, 0.5}. We then conduct Image inputs (RGB features) True costs Optimal solution (NW -> SE) Warcraft Shortest-Path Benchmark Based on Pogančić, Marin Vlastelica, et al. \"Diﬀerentiation of blackbox combinatorial solvers.\" ICLR 2020. 53 ResNet-18 24 Bo Tang, Elias B. Khalil Feature Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k ⇥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with di↵erent predictors and / / with a linear prediction model g(x; ✓). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for di↵erent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared di↵erence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Di↵erent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width ¯✏ 2 {0.0, 0.5}. We then conduct 24 Bo Tang, Elias B. Khalil 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k ⇥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with di↵erent predictors and / / with a linear prediction model g(x; ✓). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for di↵erent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared di↵erence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Di↵erent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width ¯✏ 2 {0.0, 0.5}. We then conduct Image inputs Cost predictions Warcraft Shortest-Path Benchmark Based on Pogančić, Marin Vlastelica, et al. \"Diﬀerentiation of blackbox combinatorial solvers.\" ICLR-20. 54 ResNet-18 24 Bo Tang, Elias B. Khalil Feature Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k ⇥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with di↵erent predictors and / / with a linear prediction model g(x; ✓). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for di↵erent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared di↵erence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Di↵erent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width ¯✏ 2 {0.0, 0.5}. We then conduct 24 Bo Tang, Elias B. Khalil 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 0.8 0.8 0.8 0.8 0.8 0.8 1.2 1.2 1.2 1.2 9.2 1.2 1.2 0.8 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 0.8 0.8 1.2 1.2 0.8 1.2 1.2 7.7 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 1.2 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 0.8 0.8 0.8 0.8 1.2 7.7 1.2 1.2 1.2 0.8 1.2 1.2 1.2 0.8 0.8 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 1.2 0.8 0.8 1.2 0.8 0.8 1.2 7.7 7.7 0.8 0.8 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 7.7 7.7 7.7 0.8 0.8 1.2 9.2 9.2 9.2 9.2 1.2 1.2 1.2 7.7 7.7 Cost Fig. 5: Warcraft terrain shortest path dataset: (Left) Each input feature is a k ⇥ k terrain map image as a grid of tiles; (Middle) the respective weights is a matrix indicating traveling costs; (Right) the corresponding binary matrix represents the shortest path from top left to bottom right. 6 Empirical Evaluation for PyEPO Datasets In this section, we present experimental results for the benchmark datasets of Section 5.1. The experiments aimed to investigate the training time and normalized regret (as deﬁned in Section 4.5) on a test set of size ntest = 1000. As Table 3 shows, the methods we compare include the two-stage approach with di↵erent predictors and / / with a linear prediction model g(x; ✓). Notably, was not shown due to its overall subpar performance. Unlike direct decision loss functions and , and allow the use of arbitrary loss functions, and the ﬂexibility in the loss could be useful for di↵erent problems. In the original paper, Poganˇci´c et al. [35] used the Hamming distance between the true optimum and the predicted solution, while Berthet et al. [6] employed the squared di↵erence between solutions. However, in our experiments, compared to the regret, using the Hamming distance is only sensible for the shortest path problem but leads to much worse decisions in knapsack and TSP. For the sake of consistency, we only use regret (4) as the loss for . All the numerical experiments were conducted in Python v3.7.9 with 32 Intel E5-2683 v4 Broadwell CPUs and 32GB memory. Speciﬁcally, we used Py- Torch [33] v1.10.0 for training end-to-end models, and Scikit-Learn [34] v0.24.2 and Auto-Sklearn [18] v0.14.6 for the predictors of the two-stage method. Gurobi [22] v9.1.2 was the optimization solver used throughout. 6.1 Performance Comparison between Di↵erent Methods We compare the performance between two-stage methods, , , and with varying training data size n 2 {100, 1000, 5000}, polynomial de- gree deg 2 {1, 2, 4, 6}, and noise half-width ¯✏ 2 {0.0, 0.5}. We then conduct Image inputs Cost predictions 34 Bo Tang, Elias B. Khalil Method Description 2S Two-stage method where the predictor is a truncated ResNet18 Truncated ResNet18 with SPO+ loss [16] Truncated ResNet18 with di↵erentiable black-box optimizer and Hamming distance loss [35] Truncated ResNet18 with di↵erentiable perturbed optimizer and squared error loss [6] Truncated ResNet18 with perturbed Fenchel-Young loss [6] Table 5: Methods compared in the experiments. terrain, while obtains solutions that agree the most with the optima (i.e., highest Path Accuracy). It seems that the Warcraft shortest path problem may not require end-to-end learning. However, it is noteworthy that ,despite lacking knowledge of the true costs, yields a competitive result, encouraging researchers to broaden the applications for end-to-end learning. 0 10 20 30 40 50 Epochs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8NormalizedRegretLearning Curve on Test Set 2S SPO+ PFYL DBB DPO 2S SPO+ PFYL DBB DPO Methods 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75RelativeRegret Relative Regret for each Instance on Test Set Fig. 14: Learning curve, relative regret, and path accuracy for the shortest path problem on the test set: The methods in the experiment include a two- stage neural network, , , , and . The learning curve shows relative regret on the test set, and the box plot demonstrates the distribution of relative regret on the test set. For relative regret, lower is better. Finding #6 End-to-end learning is e↵ective in rich contextual features such as im- ages. Moreover, the study highlights that can achieve impressive performance levels, even without knowing the costs during training. 8 Conclusion Because of the lack of easy-to-use generic tools, the potential power of the end- to-end predict-then-optimize has been underestimated or even overlooked in various applications. Our PyEPO package aims to alleviate barriers between the theory and practice of the end-to-end approach. PyEPO,the PyTorch-based end-to-end predict-then-optimize tool, is specif- ically designed for linear objective functions, including linear programming and 34 Bo Tang, Elias B. Khalil Method Description 2S Two-stage method where the predictor is a truncated ResNet18 Truncated ResNet18 with SPO+ loss [16] Truncated ResNet18 with di↵erentiable black-box optimizer and Hamming distance loss [35] Truncated ResNet18 with di↵erentiable perturbed optimizer and squared error loss [6] Truncated ResNet18 with perturbed Fenchel-Young loss [6] Table 5: Methods compared in the experiments. terrain, while obtains solutions that agree the most with the optima (i.e., highest Path Accuracy). It seems that the Warcraft shortest path problem may not require end-to-end learning. However, it is noteworthy that ,despite lacking knowledge of the true costs, yields a competitive result, encouraging researchers to broaden the applications for end-to-end learning. 2S SPO+ PFYL DBB DPO Methods 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00PathAccuracy Path Accuracy for each Instance on Test Set Fig. 14: Learning curve, relative regret, and path accuracy for the shortest path problem on the test set: The methods in the experiment include a two- stage neural network, , , , and . The learning curve shows relative regret on the test set, and the box plot demonstrates the distribution of relative regret on the test set. For relative regret, lower is better. Finding #6 End-to-end learning is e↵ective in rich contextual features such as im- ages. Moreover, the study highlights that can achieve impressive performance levels, even without knowing the costs during training. 8 Conclusion Because of the lack of easy-to-use generic tools, the potential power of the end- to-end predict-then-optimize has been underestimated or even overlooked in various applications. Our PyEPO package aims to alleviate barriers between the theory and practice of the end-to-end approach. PyEPO,the PyTorch-based end-to-end predict-then-optimize tool, is specif- ically designed for linear objective functions, including linear programming and 2-Stage, SPO+, and PFYL, all using a truncated ResNet-18, perform the best. DBB, originally benchmarked on this dataset, is far worse. SPO+ and PFYL match true optimal paths better than other methods, including 2-Stage. Summary • Predict-then-Optimize is a highly practical paradigm. • SPO+ and PFYL are very effective end-to-end learning methods. • The “naive” 2-stage approach is sufficient training set is large. • Open questions: • Predictions in the constraints; see Hu, Xinyi, Jasper CH Lee, and Jimmy HM Lee. \"Branch & Learn with Post-hoc Correction for Predict+ Optimize with Unknown Parameters in Constraints.\" CPAIOR 2023. • Reducing training time; see work by Tias Guns and collaborators. • More applications; see work by B. Dilkina, M. Tambe, B. Wilder, H. Bastani Bo Tang Manuscript: https://arxiv.org/abs/2206.14234 Machine Learning for Integer Programming 56 Mixed-Integer Linear Programming SL + GNN [TMLR-22] [AAAI-22] [NeurIPS-20] SL + Simple models [IJCAI-17] [AAAI-16] Custom ML [AAAI-22] [NeurIPS-21] Branch Schedule heuristics Select nodes Detect backdoors Warmstart solver SL: Supervised Learning RL: Reinforcement Learning GNN: Graph Neural Networks Machine Learning for Discrete Optimization 57 MILP Column Generation Stochastic Programming Graph Optimization Multiobjective optimization RL + GNN [NeurIPS-22] [TMLR-22] [NeurIPS-17] SL + GNN [TMLR-22] [AAAI-22] [NeurIPS-20] SL + Simple models [IJCAI-17] [AAAI-16] [NeurIPS-22] https://arxiv.org/ abs/2307.03171 Custom ML [AAAI-22] [NeurIPS-21] Survey on GNN for CombOpt [JMLR 2023]https://tinyurl.com/ACP23-PredictAndOptimize Lab Colab: PyEPO Github/Docs: https://github.com/khalil-research/PyEPO","libVersion":"0.3.2","langs":""}
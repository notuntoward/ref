{"path":"lit/lit_sources.backup/Regenwetter22deepGenMdlsEngrRvw.pdf","text":"Deep Generative Models in Engineering Design: A Review Lyle Regenwetter Dept. of Mechanical Engineering Massachusetts Institute of Technology Cambridge, MA 02139 Email: regenwet@mit.edu Amin Heyrani Nobari Dept. of Mechanical Engineering Massachusetts Institute of Technology Cambridge, MA 02139 Email: ahnobari@mit.edu Faez Ahmed Dept. of Mechanical Engineering Massachusetts Institute of Technology Cambridge, MA 02139 Email: faez@mit.edu Automated design synthesis has the potential to revolu- tionize the modern engineering design process and improve access to highly optimized and customized products across countless industries. Successfully adapting generative Ma- chine Learning to design engineering may enable such au- tomated design synthesis and is a research subject of great importance. We present a review and analysis of Deep Generative Machine Learning models in engineering de- sign. Deep Generative Models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs. Recently, DGMs such as feedforward Neural Networks (NNs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and certain Deep Rein- forcement Learning (DRL) frameworks have shown promis- ing results in design applications like structural optimiza- tion, materials design, and shape synthesis. The prevalence of DGMs in engineering design has skyrocketed since 2016. Anticipating continued growth, we conduct a review of re- cent advances to beneﬁt researchers interested in DGMs for design. We structure our review as an exposition of the algorithms, datasets, representation methods, and applica- tions commonly used in the current literature. In particu- lar, we discuss key works that have introduced new tech- niques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the devel- opment of DGMs through datasets or auxiliary methods. We further identify key challenges and limitations currently seen in DGMs across design ﬁelds, such as design creativity, han- dling constraints and objectives, and modeling both form and functional performance simultaneously. In our discus- sion, we identify possible solution pathways as key areas on which to target future work. 1 Introduction The human design process is a ubiquitous element of modern society, playing a critical role in the technologies producing the food we eat, the products we use, and the spaces in which we live. Accelerating the design process through automation can reduce cost and increase industrial productivity, which would be immensely desirable for global productivity and prosperity. Integrating AI into the design process can alleviate dependence on human experts and rev- olutionize user customizability, providing specialized prod- ucts for individual users without the prohibitive cost of man- ual design. Driven by the widespread potential to advance global equity and prosperity through design automation, methods such as “generative design” have recently emerged alongside advanced computing and automation technologies. “Generative design” is the process in which algorithms directly synthesize designs either via explicit programming or implicit learning. Early generative design methods leaned heavily on explicit programming of human design exper- tise through manually-deﬁned design representation methods like grammars [1]. While practical for explicitly encoding design constraints and objectives, these rule-based frame- works ignored opportunities for implicit leaning on infor- mation and knowledge encoded in the vast expanse of exist- ing designs. As the availability of computational resources increased over the past decade, data-intensive methods like deep learning opened doors to successfully automate com- plex human tasks such as image processing and natural lan- guage processing. In deep learning, data is propagated through sequential layers to learn progressively higher-level meaning, an ar- chitecture generally known as an Artiﬁcial Neural Network (ANN) or just Neural Network (NN) [2]. Most of the deep learning-based approaches pioneered during the 2010s lever- 1 Copyright © by ASMEarXiv:2110.10863v4 [cs.LG] 16 Mar 2022 aged extensive quantities of data to avoid explicit feature engineering. This trend is mirrored in engineering design with algorithms learning data distributions instead of requir- ing them to be predeﬁned. Among these algorithms are Deep Generative Models (DGMs) — deep learning models that can approximate complicated, high-dimensional probability distributions using a large dataset. In this review paper, we speciﬁcally deﬁne “Deep Generative Models” as algorithms that are capable of generating new samples using deep learn- ing. Generative Adversarial Networks (GANs) and Varia- tional Autoencoders (VAEs) are two classes of DGMs that have demonstrated compelling synthesis of images, text, and tabular data in numerous domains. Considering that images, text, and tabular data are all common representation methods for design, one might assume that DGMs should be capable of synthesizing full designs as well with relative ease. How- ever, several unique properties of the generative design task pose particular challenges for DGMs. Many of these chal- lenges are so fundamental that the future success of DGMs in engineering design is largely contingent on the ability to overcome them. We list four of these challenges below: 1. Modeling design performance: Real-world functional performance is critical in many engineering design tasks. Developing performance-aware DGMs capable of synthesizing designs for a given set of target require- ments (a process termed as inverse design) is a challeng- ing task that is exacerbated by the computational cost of numerical simulation and the even greater difﬁculty of real-world evaluation. 2. Data sparsity: Compared to other research ﬁelds like Computer Vision, which have massive publicly avail- able datasets, the availability of large, well-annotated, public datasets in engineering is severely lacking. Fur- thermore, even when data is available, the distribution of the data often does not cover the design space evenly, with much sparsity often observed in the data. 3. The creativity gap: In conventional DGM applications, the overarching goal is to mimic the training data and emulate existing designs. In engineering design, emula- tion of existing products is often undesirable. Designers typically aim to introduce products with novel features to target new market segments. 4. Usability and feasibility: For synthesized designs to be physically fabricated, they must be physically feasible. Furthermore, designs must be encoded in a data repre- sentation that contains enough parametric detail to be converted into a representation usable for fabrication. Over the past few years, the design community has made substantial progress in using generative machine learning (ML) models to create new designs. DGMs have been ap- plied to a broad range of design tasks such as structural opti- mization, materials design, and shape synthesis. Over time, researchers have introduced increasingly advanced methods, which have begun to address some of the above challenges. For example, many works have proposed approaches to in- corporate design performance and optimization into DGM training. Other works have explored incorporating novelty and creativity into DGMs. Despite these advancements, DGMs for engineering design are still in their infancy and will require further efforts to effectively overcome these fun- damental challenges. Our primary goal in this work is to help build cohe- sion between the countless active researchers in the design ﬁeld working with DGMs and furthermore provide a start- ing point for researchers entering the ﬁeld. In particular, we seek to provide researchers with a reference guide in plan- ning projects in the data-driven generative design space. To this end, we provide an overview of common methods and tools (Sec. 2), a discussion of different data parameterization methods (Sec. 3), a review of potentially relevant research across various design domains (Sec. 5), an overview of rele- vant datasets (Sec. 6), and an analysis of common challenges in the ﬁeld (Sec. 7). Figure 1 provides an overview of the standard process to apply DGMs in engineering design. 2 Overview of Deep Generative Models Deep Generative Machine Learning approaches share the goal of high-quality synthesis but signiﬁcantly vary in methodology. In practice, we identify four common ap- proaches to generate designs: direct generation using deep neural networks (DNN), adversarial generation with Gener- ative Adversarial Networks (GAN), generation from embed- ding vectors using Variational Autoencoders (VAE), and se- quential generation using Reinforcement learning (RL). In the design community, we observe that GANs, VAEs, and RL are most commonly used for design synthesis. While DNNs as well as extensions like recurrent neural networks (RNNs) are occasionally used for direct design synthesis, they are more frequently used for non-generative tasks. In this section, we brieﬂy discuss the background and method- ology of GANs, VAEs, and RL. 2.1 Generative Adversarial Networks Originally introduced in 2014, Generative Adversarial Networks (GANs) [3] found initial success with convincing image synthesis performance [4, 5, 6, 7]. We provide an in- troduction of GANs but refer the reader to [8] for a detailed overview. A generative adversarial network [3] consists of two models — a generator and a discriminator. The genera- tor G maps an arbitrary noise distribution to the data distribu- tion, in our case the distribution of designs, and can thus gen- erate new data; simultaneously, the discriminator D learns to distinguish between real and generated data. Both models are usually built with deep neural networks. As D improves, G also improves as it learns to generate data that fools D. Common challenges in training GAN models: GANs are often considered difﬁcult to train, suffering from train- ing instability stemming from several sources [9, 10]. One common issue in GAN training occurs when the discrimi- nator overpowers the generator, easily distinguishing gener- ated samples, and causing the gradient to the generator to vanish, effectively halting the generator’s training. This is- 2 Copyright © by ASME Fig. 1: This ﬁgure outlines the typical components of design synthesis problems using Deep Generative Models. Some design problems are more suitable for speciﬁc design representation methods, which also inﬂuences the type of deep generative model architectures required. sue has been addressed by many researchers. For example, the WGAN replaces the discriminator with a critic, modiﬁes the GAN’s loss function to estimate the Wasserstein (Earth Mover’s) distance between the original data and generated data distributions, and modiﬁes the training process [11, 12]. Another problem that GANs face is the issue of “mode collapse,” where the generator fails to encompass all modes in the data distribution or even generates only a handful of unique samples that are capable of fooling the discrimina- tor. To overcome these issues, researchers have developed novel algorithmic techniques [13, 14, 15] to reward diversity in samples. GAN conditioning: In the design domain, we often have design constraints, requirements, and objectives that any generated design should satisfy. To use DGMs for such prob- lems, these requirements should be imposed on them. For example, we may seek to train a DGM to generate bikes, but depending on our user, we may want to constrain it to gener- ate only roadbikes or mountain bikes without retraining for each generation task. Model conditioning is one method to do this. Several proposed approaches add conditioning to the GAN using a condition vector which is intended to be interpretable. Typically GANs are discretely conditioned by feeding the condition vector into both the generator and dis- 3 Copyright © by ASME criminator, in a conﬁguration known as a Conditional GAN (cGAN) [16]. Instead of feeding the condition vector into the discriminator, an auxiliary network and cross entropy loss can instead be used to reconstruct the condition vec- tor from the generated samples in a conﬁguration known as an Information Maximizing GAN (InfoGAN) [17]. Con- ditioning is also essential in design applications where in- verse design is being done on performance metrics, which often exist in continuous spaces (e.g., stiffness, lift coef- ﬁcient, drag coefﬁcient, density, etc.). Researchers have come up with continuous conditioning solutions for GANs such as the Regressional GAN [18], continuous conditional GAN (CcGAN) [19] and performance conditioned diverse GAN (PcDGAN) [20]. 2.2 Variational Autoencoders Introduced in 2013, Variational Autoencoders found sig- niﬁcant success in many machine learning applications. Au- toencoders are unsupervised embedding algorithms consist- ing of an encoder that maps an input design into a (typi- cally) lower-dimensional latent space and a decoder that re- constructs the design as accurately as possible from the la- tent space. The encoder and decoder are conventionally im- plemented using deep neural networks. To generate new samples, latent vectors are sampled from the latent space and fed through the decoder. Typically, the distribution of the real data mapped to the latent space of an autoencoder is sparse, meaning that sampling a realistic latent vector is difﬁcult. This limitation is addressed with the introduc- tion of the Variational Autoencoder (VAE), ﬁrst proposed by Kingma et al. [21]. The Variational Autoencoder adds in a probabilistic sampling in the latent space that regularizes the latent distribution. In practice, the VAEs’s encoder outputs n means and n variances, from which n-dimensional latent vec- tors are sampled before decoding. To maintain a predictable latent space distribution, The VAE adds a Kullback-Liebler (KL) divergence [22] loss between the distribution of the la- tent space and a standard Gaussian. Interested readers are encouraged to refer to the literature [23] for a more detailed overview. Conditional VAEs: Just as we do for GANs, we may also seek to condition VAE training on design constraints or user preferences. The VAE has a natural advantage over the GAN in that its latent space is typically already structured. Since this structure may be fairly weak and difﬁcult to interpret, ex- plicitly conditioning VAEs may still be desirable. The Con- ditional VAE (cVAE) [24] extends on the conventional VAE by adding a conditioning vector as an input to both the en- coder and decoder and helps achieve this goal. 2.3 Reinforcement Learning Reinforcement Learning fundamentally differs from the other DGMs discussed in that it learns without a dataset in an unsupervised fashion through a large set of trial and error in- teractions between an actor and an environment [25]. This is typically done through some reward signal being sent to the actor after taking actions, based on the effects of said actions on the environment. In this scenario, the actor’s goal is to maximize the rewards it receives by making decisions (i.e., taking actions) such that the total reward is maximized. From this point of view, reinforcement learning can be thought of as an approach similar to optimization, where an objective (maximizing the reward) is being optimized. One of the ﬁrst attempts at introducing deep learning to the reinforcement learning approach was done in 2013 by Mnih et al., when they introduced deep learning to a rein- forcement learning process known as Q-Learning [26]. Q- Learning refers to learning the state-action value function or Q-function, which is a progressively updated estimate of the expected reward to be received from taking a particular action in a particular state. Mnih et al., attempted to learn the Q-function using convolutional neural networks (CNN). Many Deep RL techniques have been introduced since this ﬁrst work by Mnih et al.. Further exploration of the details of these approaches is left to the reader. In practice, when applying RL to design applications, the design process is usually broken down into a sequential process of building a design or altering existing designs in steps (i.e., actions taken to alter or expand the current state of a design being generated) and the reward is measured by the quality or performance of the resulting design (i.e., the environment). While RL requires no dataset, this advantage is balanced by dependence on meaningful and reliable re- ward signals, which may often require a high-ﬁdelity simu- lation environment. One major beneﬁt of RL over GANs and VAEs is the fact that the reward function can be set based on any objective which does not need to be differentiable. In contrast, any objective added to the loss function of a GAN or VAE must be differentiable since GANs and VAEs are trained using the gradient-based optimization [15]. 3 Overview of Design Representation Methods In this section, we discuss common design representa- tion methods seen in DGMs for engineering design which are visualized in Section 2 of Figure 1. We include a deﬁni- tion and discuss the pros and cons of each method. 3.1 Images Design data often comes in the form of images (e.g. microstructure scans) or can be represented in image form (e.g. Topology Optimization). An image consists of a rect- angular grid of pixels, each of which contains a color pa- rameter. They are commonly represented by third-order ten- sors (height × width × channels). Common color schemes are black-and-white (boolean color channel), grayscale (in- teger color channel), and color (3-4 integer color chan- nels). Pros: The image is an information-rich represen- tation and can capture many details of a design. The use of convolution/convolution-transpose ﬁlters in deep learn- ing provides a convenient tool for learning/generation of both high-level and low-level features as well as upsam- pling/downsampling. Many cutting-edge ML techniques are 4 Copyright © by ASME pioneered in the computer vision domain and are often di- rectly applicable to images. Cons: Representing designs us- ing pixels means that the generated design images can be infeasible for downstream tasks. Accurately fabricating de- signs based on images can be difﬁcult or impossible. Even performance evaluation using conventional simulation tools like FEA or CFD can require an intermediate conversion from an image to a 3D model. The poor usability of im- ages is exacerbated by the prevalence of artifacts in many applications (hanging pixels, disconnected geometry, etc.). Artifacts are especially common when training on (typically) small datasets in the design domain since training is often ter- minated early due to over-ﬁtting concerns. All in all, images can be considered surrogate representations of engineering designs and may lack domain knowledge and information on the physical realization of the design. Therefore, DGMs us- ing images as representations often have a gap between the generated images and the actual design they are representing. 3.2 Voxelizations Voxels are 3D grid points that are effectively the 3D equivalent of pixels. As such, voxelizations share many char- acteristics with images. In practice, voxels aren’t conducive to ‘color’ parameterization and are typically represented as booleans (space vs. object). This effectively makes them third-order tensors with dimension (height × width × depth). Pros: Voxelizations support 3D convolution which can learn high-level and low-level features in 3D. Cons: Compared to images and other representations, the curse of dimensional- ity is especially pronounced with voxels, with the number of parameters scaling with the cube of spatial resolution. Voxelizations share the same issues as images. Their us- ability is limited in downstream tasks and artifacts are very prevalent. Like images, voxelizations serve as surrogate rep- resentations (often representing CAD models which origi- nate from parametric representations or 3D shapes which originate from meshes). Like many other representations such as point clouds and Signed Distance Fields, voxeliza- tions often require conversion before they can be used in downstream tasks. For example, they are often converted to Boundary Representation (BRep) or polygonal represen- tations, which are often the native parameterizations of ren- dering and graphics software, Finite Element Analysis, and Computational Fluid Dynamics simulation. 3.3 Point Clouds Point Clouds are simple collections of points, often in 3D space, which are deﬁned to be within some object. Pros: Point Clouds can represent arbitrarily complex geometry with a ﬁnite number of points, though ﬁdelity may vary. Point Clouds are often the native output of 3D scanning soft- ware, making them relatively easy to create [27]. Cons: Like Voxelizations and Signed Distance Functions, Point Clouds often require conversion to BRep or polygonal representa- tions such as meshes [28] for downstream tasks. 3.4 Meshes Meshes are a common method to represent objects in 3D space. Triangular meshes are by far the most commonly used form. Triangular meshes are the native representation used in many computer graphics algorithms and software, as well as many Finite Element tools. Pros: Meshes can be directly vi- sualized and simulated in many FEA or CFD tools, enabling easy pipelines for performance evaluation using numerical methods. A mesh can be considered a specialized type of graph and can leverage graph operators like graph convolu- tional operators. Cons: In contrast to other representations like voxelizations and point clouds, meshes are more chal- lenging to directly generate using Machine Learning meth- ods, despite recent advances in algorithms that directly gen- erate meshes [29, 30, 31]. 3.5 Signed Distance Functions The Signed Distance Function/Field (SDF) is a repre- sentation method that consists of a (typically 3D) functional map from a coordinate point to an SDF value. The magni- tude of this value indicates the distance to the nearest point on the surface of the object and the sign indicates whether the point is inside or outside the object. SDFs themselves can be represented in many ways, for example, as a rasterized grid in which each ‘voxel’ contains a continuous numerical value denoting the SDF value at that point. Pros: SDFs can serve as a convenient intermediate parameterization for many learning tasks. Cons: Like point clouds or voxels, SDFs are difﬁcult to use in downstream tasks without ﬁrst converting to BRep or polygonal representations. 3.6 Parameterizations We use the term “parametric” data to encompass any de- sign representation consisting of a collection of design pa- rameters where any spatial or temporal signiﬁcance of pa- rameters is unknown. Most parametric data can be orga- nized in tabular form with each row being a collection of parameters representing a single design and each column de- scribing a design parameter. Tabular design data often con- sists of a collection of mixed-datatype parameters where re- lations between these parameters may be unclear or nonex- istent. Since parametric data may come in many varieties, the pros and cons discussed may not apply to every case. Pros: Quality parametric data is typically very information- dense (i.e. requiring fewer parameters to encode the same level of geometric detail). Whereas spatially-organized rep- resentations such as pixels or voxels encode designs with uniform information density, parametric data can contain more detail in design-critical areas without the need for up- sampling the entire representation. This information density often comes with a lower dimensionality which can make optimization of parametrically represented designs signiﬁ- cantly easier. Parametric data may also be more support- ive of downstream tasks, especially if design parameters are human-interpretable. For example, a detailed enough design parameterization may allow generated designs to be directly fabricated using conventional (non-additive) manufacturing 5 Copyright © by ASME techniques. Human-interpretable parameterizations can also give human designers a tractable method to interact with gen- erative methods to allow for human-in-loop design. Finally, design parameters can sometimes be directly linked to the latent space of a generative method, as demonstrated in nu- merous works [32, 33], creating a pipeline to directly condi- tion design generation on high-level design goals. Linking design parameters with latent variables has several potential advantages, such as enabling more effective optimization or inverse design using generative methods. Cons: Learning parametric data can be particularly challenging. Parametric data commonly uses mixed datatypes and inherits the train- ing challenges of the constituent components. Multimodal distributions, skewed categories, non-Gaussian distributions, data sparsity, and poor data scaling additionally make the ap- plication of DGMs and training very difﬁcult. Since methods that are robust to all of the mentioned challenges are difﬁcult to come by, successfully applying existing methods to the parametric data domain can be hard. Finally, since paramet- ric data may be nontrivial to convert to 3D models, generated parametric designs may be challenging to evaluate using nu- merical simulations or through qualitative visualization. 3.7 Grammars Grammars are representation methods consisting of variables, terminal symbols, nonterminal symbols, and a set of rules. Rules describe how non-terminal symbols can ex- pand into other terminal and nonterminal symbols. The most prevalent grammars in engineering design are graph and spa- tial grammars [1], and they have been applied in a wide variety of applications, such as the design of satellites and electro-mechanical systems. Grammars can be especially useful to dictate feasible assembly hierarchies of design com- ponents as in [34]. Pros: Grammars can explicitly con- strain design spaces to feasible or desirable regions by nature of their construction, thereby encoding domain knowledge. Cons: Grammars are challenging to implicitly learn and of- ten must be manually deﬁned. Grammars can also restrict the exploration of the design space. A survey on grammar-based design synthesis approaches is provided in [1]. 3.8 Graphs Graphs are a highly ﬂexible representation method con- sisting of nodes and edges, which can be directed or undi- rected. Graph-based representations have proven success- ful for many different aspects of design generation and opti- mization and provided avenues for describing complex sys- tems efﬁciently. Pros: Graphs are highly adaptable and are capable of representing many different kinds of complex sys- tems and designs [35, 36, 37, 38]. Graphs also provide repre- sentations for design processes and modeling complex inter- actions in systems [39,40] which may enable methods for au- tomating the design of systems or modeling inter-part depen- dencies. Graph neural networks (GNNs) [41, 42, 43, 44, 45] provide an excellent tool for machine learning on graphs. Cons: Despite the developments of graph neural networks (GNNs) in the computer science community [46, 47, 48, 49] and their success in molecular graph generation [50, 51], there is less usage of graph-based DGMs in the design community, possibly due to the lack of graph-based design datasets. 4 Literature Review Methodology Sec. 5 discusses speciﬁc works that apply Deep Genera- tive Models to engineering design or make advancements to existing generative ML methods in the context of engineer- ing design. We consider works based on a predeﬁned scope, with each work we discuss meeting the following selection criteria. Note that these criteria only apply to the engineer- ing design papers presented and that the works we cite to add context to or substantiate the discussion of fundamentals, ap- plications, and datasets need not adhere to these rules. 1. We limit our consideration speciﬁcally to papers involv- ing Deep Generative Models, focusing on Variational Autoencoders, Generative Adversarial Networks, and Reinforcement Learning in particular. Works must uti- lize deep learning. Works only considering design opti- mization are excluded. 2. We only consider work speciﬁcally relevant to engineer- ing design and not other domains (such as computer sci- ence). 3. We consider only work published between Jan. 2014 and Sep. 2021, when we conclude our review, as many piv- otal works in deep learning (CNNs, VAEs, GANs) were introduced in this period. To identify works, we speciﬁcally searched for “Generative Adversarial Network,” “Variational Autoencoder,” and “Re- inforcement Learning” in Google Scholar. We initially con- ﬁned our search to a set of known design venues, speciﬁ- cally the Journal of Mechanical Design, the Proceedings of the International Design Engineering Technical Conferences, Computer-Aided Design Journal, International Conference on Engineering Design, and Artiﬁcial Intelligence for Engi- neering Design, Analysis, and Manufacturing Journal. This helped us identify an initial set of seed papers. From all search results identiﬁed through these methods, 41 papers were deemed to be relevant and included. The relevance was decided by independent assessment by two raters, who are also authors of this paper. For papers where there was a disagreement, all authors discussed them and mutually de- cided on their classiﬁcation. Next, papers cited in these seed papers were considered and added to this paper. 22 papers from other venues were deemed to be relevant and included as well. Of the 63 papers, 48 were published between Jan. 2019 and Aug. 2021, while a mere 15 were published be- tween Jan. 2016 and Dec. 2018, indicating strong growth in the ﬁeld in recent years. 5 Application Domains in Engineering Design Engineering design encompasses a wide variety of ap- plications, ranging from designing aircraft models to small- scale metamaterials. To structure different types of applica- 6 Copyright © by ASME Table 1: Characterization of application studies by domain and architecture. Some works use multiple architectures or ﬁt into multiple application domains and are listed more than once. We additionally classify works by datatype used: Imagei, Point Cloudc, Voxelelizationv, Meshm, Signed Distance Fields, Grammarg, Graphh, and Parametricp (other parameterization) Topology Optimization Materials 2D Shape Synthesis 3D shape synthesis Other Domains Plain NN [52]i [53]vi [54]p∗ [55]i [56]p [57]i [58]i GAN [59]i [60]i [61]i [62]i [63] [64]i [65]vi [66]v [67]i [68]vi [69]c GAN+Conditioning [70]i [71]i [72]i [73]i [74]p [75]p [76]p [33]ph New GAN-based [72]i [15]p [77]v [78]i AE/VAE [79]i [80]i [81]i [82]i [32]i [83]i [84]v [85]p, [86]p, [87]p cVAE [88]i † [89]i [85]p [90]i New VAE-based [91]i [92]s [93]ip RL-based [94]p [95]p [96]ip [34]g [97]ip [98]p [99]p Other Method [100]v [101]i [102]i [103]i [67]i [104]i [105]i [106]i [95]p [100]v [107]p [34]g [108]ip [109]ip [110]g +Style Transfer [79]i [64]i [65]vi [80]i [57]i [104]i +Genetic Alg. [82]i [105]i [92]s +Bayesian Opt. [64]i [83]i [76]p *Model-based reconstruction using parameters in a lower-dimensional space † Technically not images, but use an image-like parameterization structure tions using DGMs, we grouped them into a few categories of application areas, which are discussed in this section and are reported in Table 1. 5.1 Deep Generative Models in Topology Optimization Fig. 2: Sample topologies generated by 3D Topology Opti- mization Topology Optimization (TO) is a research ﬁeld with a long history of research and methods. TO searches a de- sign space to ﬁnd an ideal spatial distribution of material to optimize some predeﬁned objective. Common areas of ap- plication include solid mechanics [111, 112], ﬂuid dynam- ics [113, 114], additive manufacturing [115, 116], and heat transfer [117, 118]. While Topology Optimization is con- sidered a method for generative design, standard TO does not leverage deep learning and is not considered a DGM. In recent years, however, several papers have proposed meth- ods to use TO and DGMs together, in many cases to address the computational cost of TO on large amounts of data. De- spite differences between architectures, we found that many DGMs for TO share certain characteristics. In particular, due to the predominant use of voxelized or pixelized rep- resentations, many hybrid TO-Generative ML models use methods originally developed for computer vision appli- cations, such as convolutional neural networks and super- resolution [59, 53]. Supervised generation of optimized topologies: To avoid the computational cost of Topology Optimization, DGMs have been used to predict ﬁnal optimized topologies di- rectly. This approach can also be used as an initialization technique for conventional TO, which allows conventional TO algorithms to rapidly converge, saving computational cost. We consider a baseline for DGMs in TO, in which an existing generative architecture is applied to a dataset of TO-generated designs and the network is trained to gener- ate samples mimicking the training data. A few papers fall within this category of methods: Rawat and Chen [60] train a WGAN on a dataset generated through TO and train an auxiliary network to additionally predict performance met- rics. Sharpe and Seepersad [70] expand on this baseline with a cGAN conditioned on volume fraction and load location. Guo et al. [79] train a VAE with an additional style trans- fer [119,120] loss on a TO-generated dataset for heat transfer and propose iterative strategies for targeted design optimiza- tion using the VAE’s latent space. Style transfer is discussed further in Sec. 5.2. Iterative DGM training, ﬁltering, and human guidance in TO: Oh et al. [62, 61] propose a method that expands on the baseline generative-network-ﬁtting by iteratively syn- thesizing new designs, optimizing them in TO, then drop- ping designs that are too similar from the full collection of designs. The authors use a modiﬁed Boundary Equilibrium GAN (BEGAN) [121], which extends on the WGAN and be- gin training on a collection of existing TO-generated topolo- gies. However, unlike the previously discussed generative- network-ﬁtting approaches used in literature [60, 70, 79], the retraining and re-optimization allows the framework to gen- erate, optimize, and explore new areas of the design space. The proposed method is applied to the problem of wheel de- sign, with the key motivation being to ﬁnd a trade-off be- tween the aesthetics of real designs and the structural per- formance of designs found through TO. The authors present strong empirical results on this wheel design problem. The issue of gaps in the design space of topolo- gies is also addressed by Fujita et al. [101], who pro- pose an approach leveraging a Variational Deep Embedding (VaDE) [122]. An initial dataset is generated using TO. The 7 Copyright © by ASME proposed method sequentially identiﬁes voids in the design space using the VaDE, decodes designs from the void, opti- mizes them using TO, then adds them to the training dataset. Although the above works have proposed methods with automated retraining steps, human input can also be injected into the training process. For example, Valdez et al. [73] propose a cGAN-based human-in-loop topology design gen- eration framework in which the designer iteratively selects design clusters to gradually hone in on preferable designs. DGMs for TO that utilize super-resolution: Since one of the key limitations of Topology Optimization is computa- tional cost, which scales with the resolution, a major focus in DGMs for TO has focused on attaining high-resolution TO-like results. A common approach uses super-resolution, which is a technique used in computer vision to convert low- resolution images to high-resolution ones. Several works expand on the baseline DGM-in-TO framework by learn- ing from a low-resolution TO-generated dataset, then per- forming super-resolution on synthesized topologies, such as Yu et al. [72], who add a cGAN for upscaling. A similar approach proposed by Li et al. [59] uses a Super- Resolution GAN (SRGAN) [123] to generate high-resolution optimal topologies for heat transfer problems after generat- ing low-dimensional topologies on a different GAN. Other researchers have proposed transfer learning for the super- resolution task instead of using GANs or VAEs for super- resolution. For example, Behzadi et al. [53], train a feedfor- ward NN-based model to predict optimized topologies di- rectly without any discriminator and using the MSE loss. Once they have trained their model on the low-resolution data, they lock the model weights and add a few layers to the model to increase the output resolution and only train said layers to obtain high-resolution samples. This avoids the need for a large quantity of high-resolution data or the extra time required to train a high-resolution model from scratch. Many papers that have applied super-resolution techniques demonstrate that after initial training, their framework con- sistently generates near-optimal topologies many times faster than classic TO [72, 59]. Improving DGM-based topology generation using physi- cal properties: Several papers have succeeded in improv- ing the baseline performance of DGMs for topology gen- eration through the use of physical properties of the de- sign domain. For example, Nie et al. [71] propose to adapt the cGAN architecture Pix2Pix [124] to generate synthetic topologies based on spatial ﬁelds of various physical param- eters (displacement, strain energy density, and Von Mises stress) as input to the generator. Topologies generated by TO are taken as ground truth for training. The authors also propose a new generator architecture combining the Squeeze and Excitation ResNet [125,126] with U-Net [127]. Cang et al. [55] use a neural network to generate optimized topologies from loading conditions. They propose an ap- proach to rapidly evaluate the deviation of proposed solu- tions from the problem’s optimality conditions. They then progressively augment their dataset during training by recal- culating optimal solutions (using TO) for proposed solutions that are in greatest violation of these conditions. Other DGM approaches in topology generation: Sos- novik and Oseledets [52] propose to use gradient informa- tion from a topology distribution to inform estimates of a ﬁnal topology through a DNN. Their DNN takes both the density distribution and gradient of this density distribution from some intermediate step of a TO process to generate an estimate of the ﬁnal topology without waiting for TO con- vergence. The authors demonstrate binary accuracy of 98% when predicting the ﬁnal topology after only ﬁve iterations of TO, when the full TO process would take 100 iterations to complete. In a different kind of approach, Keshavarz- zadeh et al. [54], address the generalization problem of training for different scales and different domains. They parametrically represent shapes in both 2D and 3D using their proposed “Disjunctive Normal Shape Model” (DNSM) [54]. Using this DNSM, they create a platform for resolution-independent shape reconstruction. They then train an NN model to generate optimal topologies given boundary conditions and other problem-speciﬁc information in the DNSM space, which then can be reconstructed at any resolution using the DNSM. They demonstrate that their DNSM method overcomes the super-resolution problem across a variety of problems and domains. 5.2 Microstructure, Nanostructure, and Metamaterials Fig. 3: Deep Generative Models are often employed to gen- erate 3D Metamaterial Unit cells Many design applications not only require the need to design the topology or shape of the artifact, but also the ma- terial properties within it. Inverse design of materials is one of the key elements of Computational Materials Science. The most common approach to developing inverse materials de- sign frameworks is the development of Process-Structure- Property (PSP) links, i.e. understanding how a particular material processing approach impacts its microstructure and how the corresponding microstructure impacts its physical properties [128]. Designing material microstructures for di- rect use is difﬁcult, as there must then be some fabrication process to generate a material with the target microstructure. A common goal in computational materials science is mi- crostructure image “reconstruction,” in other words, generat- ing microstructure images that exhibit certain characteristics. Bostanabad et al. [128] give an overview of the Microstruc- ture Characterization and Reconstruction (MCR) ﬁeld. Re- construction can accelerate downstream tasks like augment- ing the training data of networks that attempt to model PSP links. Better modeling PSP links can in turn create more ac- curate generative material design pipelines. Many classes of 8 Copyright © by ASME DGMs have been applied to this reconstruction task, includ- ing GANs [64], VAEs [80] and Convolutional Deep Belief Networks (CDBNs) [129] [103, 102, 106]. Other studies at- tempt to bridge the gap between microstructure and proper- ties in generative tasks using trained black-box surrogates, such as Tan et al.’s work [63]. Most research focuses on 2D microstructure images, though several also consider 3D vox- elizations [66, 68]. Since reconstruction often involves mimicking exist- ing microstructures, several papers have applied style trans- fer [119, 120] to the problem as part of a DGM architecture. Style transfer is in essence a loss between two images that attempts to capture the difference in “style” between the im- ages. It is typically calculated by comparing the interme- diate layers of an auxiliary style convolutional neural net- work. Cang et al. [80], for example, propose a VAE with style transfer which is targeted at applications where only a small set of training data is available. Li et al. [57] also uti- lize style transfer in their proposed transfer learning recon- struction framework. In another approach using style trans- fer, this time using a vanilla GAN, Yang et al. [64] expand on the reconstruction task by attempting inverse design on the structure-property link. After training their GAN with style transfer loss and an additional loss to penalize mode collapse, they treat the noise vectors as input variables and optimize them with Bayesian optimization, using the gener- ator to translate between design vectors and microstructure images. The authors choose to optimize microstructures for energy absorption which can be evaluated from images us- ing coupled-wave analysis [130], providing them a conve- nient structure-property link that would be more challenging if optimizing for other objectives. More recently, Fokina et al. [104] adapt StyleGAN [7] to the microstructure image synthesis domain. Other stud- ies impose the “style” of generated microstructure images through enforcement of physical properties. For example, Chen et al. [89] propose an approach to generate Random Heterogeneous Material (RHM) microstructure images using a cGAN conditioned on target images. Their cGAN uses an augmented loss based on the matching of perimeter, volume, and Euler characteristics. DGM studies in microstructure design have also used other advanced methods introduced in computer vision such as super-resolution [58] and image translation [67, 4, 67, 124]. Although 2D images are a widely used means to rep- resent microstructures, Zhang et al. [65] expand the mi- crostructure reconstruction task to the 3D domain. Their ScaffoldGAN method generates scaffold materials to mimic real-world examples of human bone scaffolds as well as foam metal scaffolds in both 3D and 2D data. They employ the conventional GAN approach with style loss for better vi- sual similarity between generated scaffolds and real ones. However, the authors additionally introduce a novel “struc- tural loss” term to speciﬁcally address spatial coherence, a limitation of GANs in emulating scaffolds. This additional loss helps them achieve better results that mimic the features of the data more realistically. Other studies also consider DGMs on 3D microstructures, such as Mosser et al. [66] and Liu et al. [68]. Photonics and phononics: Within microstructure design, the design of materials with photonic or phononic proper- ties is an area of interest in numerous industries including sensing, communications, and display technology. The work by Yang et al. [64] is an example of a subﬁeld of genera- tive microstructure design targeting the development of mi- crostructures with particular photonic or phononic proper- ties. Molesky et al. [131] also provide a review of inverse design in nanophotonics. In the photonics and phononics subﬁeld, performance evaluations such as the one used in [64] are frequently used to incorporate performance into DGMs. For exam- ple, the technique of optimization using learned mappings from the latent space of a trained autoencoder to the property space has been employed in several papers. Li et al. [81], Liu et al. [82], and Wang et al. [105] train an autoencoder, VAE, and Gaussian Mixture VAE [132] respectively on im- ages of microstructures. They map the latent variables to the property space using a DNN, CNN, and Gaussian Pro- cess Regressor, respectively. Li et al. [81] directly optimize the properties using the DNN, while Liu et al. [82], and Wang et al. [105] optimize using a Genetic Algorithm. Though optimizing latent variables is a common ap- proach, several other DGMs have been proposed for pho- tonics/phononics design that optimize performance in other ways. For instance, Malkiel et al. [56] uniquely use a para- metric representation and a bidirectional DNN to simulta- neously learn a bidirectional mapping between nanostruc- tures and the property space. Ma et al. [91] propose a novel VAE-based framework consisting of feature extraction, pre- diction, recognition, and generation networks. The proposed method is capable of both forward prediction of properties based on metamaterial structure as well as the inverse (gen- erative) prediction task based on properties. Furthermore, the framework supports self-supervised learning where the model trains on unlabeled metamaterial pattern images with- out corresponding property labels. The authors also demon- strate transfer learning to other microstructure shape classes as well as the inverse design of multiple microstructures forming a meta-mirror with desired properties. DGMs have also been applied to nano-scale photonic devices, such as an optic broadband power splitter as in Tang et al. [88]. Unit-cell-based metamaterials: Research in generative design for microstructures has also focused on the devel- opment of unit cell structures for metamaterials. For in- stance, Wang et al. [32] ﬁt a VAE to metamaterial unit cells and demonstrate that latent space parameters can re- ﬂect the physical properties of the unit cells. They then demonstrate several downstream tasks using their VAE, such as diverse subset selection and generation, targeted genera- tion to match desired stiffness matrix values, and metama- terial family design. The authors also demonstrate several 2D macro-optimization runs to design arrays of unit cells to match macro-level deﬂection targets in an approach similar to classic TO. Xue et al. [83] train a VAE to generate unit 9 Copyright © by ASME cells, then perform Bayesian Optimization in the latent space of the VAE to attain desired macroscopic elastic properties. 5.3 2D Shape Synthesis Fig. 4: Airfoil synthesis using DGMs usually entails employ- ing 2D shape generation approaches. Designing features with critical geometric considera- tions is a task that appears in several engineering ﬁelds such as aerospace and automobile design. DGMs have been widely applied to these problems. Within aerospace, the de- sign of an airfoil, which is the cross-sectional shape of a wing, is of particular interest to many researchers. Airfoils have a wide variety of uses in engineering domains such as propeller, rotor, and turbine blade design. Since airfoil per- formance parameters are of key interest, most of the research focuses on performance-aware conditional generation. For example, Yilmaz and German [74] apply a cGAN to the airfoil design problem, conditioning their network on vari- ous stall parameters. While the overwhelming majority of research in this domain uses DGMs trained to learn shape parameterization based on spline interpolation of Cartesian points, equation-based parameterization is also sometimes used. For example, Li et al. [94] propose a performance- aware RL framework where the RL agent learns the opti- mal equation coefﬁcients using Proximal Policy Optimiza- tion (PPO) [133]. Other works generalize their applications to general 2D shape synthesis. For example, Chen and Fuge [75] propose B´ezier GAN, a framework that learns shape representations through B´ezier curves, featuring InfoGAN style condition- ing [17]. Chen et al. [76] expand on this work with Bayesian Optimization to maximize lift/drag ratio. Classic test data for 2D shape synthesis methods are the UIUC airfoil dataset as well as artiﬁcially-designed shapes like superformulas [134]. While the emphasis of certain papers is on shapes or airfoils themselves, several papers primarily propose methodology advancements that they choose to demonstrate on the 2D shape synthesis domain. For example, Chen and Fuge [33] address the challenging problem of multi-component design generation using a GAN to synthesize parts using inter-part dependencies. The authors assume inter-part dependencies in a design are known and propose modeling them using di- rected acyclic graphs. They propose a hierarchical adapta- tion of the InfoGAN using a single discriminator for the en- tire design and one generator/auxiliary network pair for each part in the design, which they term the hierarchical GAN (HGAN). The method is tested on a medley of synthetic datasets generated using B´ezierGAN [75] and demonstrated to form meaningful and interpretable latent spaces. Although the paper assumes that part dependency graphs for the ob- ject class are well deﬁned, this paper takes a signiﬁcant step towards multi-component design synthesis and interpretable GANs. Several other papers use the 2D shape synthesis do- main to address the challenges of performance evaluation in DGMs. Dering et al. [95], for example, apply an iter- ative retraining approach to boat sketches using an adapta- tion of the Long Short-Term Memory (LSTM)-based Sketch- RNN [135] as the generator. To evaluate candidate designs, the performance is scored using a simulated environment in a game engine, within which the “behavior” (motion) of the design is learned. A recent work by Chen and Ahmed [15] addresses both performance evaluation and design novelty through their pro- posed Performance Augmented Diverse GAN (PaDGAN) framework. Conventional GANs are trained to mimic the design space they are trained in and as a consequence, are penalized for generating novel designs. PaDGAN ex- pands on the conventional GAN architecture by modeling design performance and design diversity using a Determi- nantal Point Process (DPP) kernel [136]. By promoting di- versity, PaDGAN directly addresses mode collapse in GANs too (see Sec. 2.1). The authors demonstrated that PaDGAN is capable of generating high-quality and previously unseen novel designs for the UIUC airfoil database, using the lift to drag ratio as the quality metric. [137] extends this work to multi-objective generation. 5.4 3D Shape Synthesis Fig. 5: DGM models, such as Range-GAN [77], can be used to generate 3D models of aircrafts with constraints. 3D object generation using deep learning is an active ﬁeld in computer science, with signiﬁcant research efforts being dedicated to generating realistic-looking shapes and objects. 3D shapes and objects are typically represented by voxels, point clouds, or meshes. Advancements in 3D shape synthesis in the computer graphics community have leaned heavily on GANs or Autoencoders, as well as other machine learning advancements like Recurrent Neural Net- works (RNNs), Transformers, and Graph Neural Networks (GNNs). This research is tangentially relevant to engineer- ing design but is typically focused more on visual appear- ance and aesthetics rather than design considerations like functional performance derived from simulations or manu- facturability. For this reason, we do not discuss these meth- ods in detail and opt to list a few of the many noteworthy papers in this ﬁeld from 2016 to 2021 for the reader to fur- ther explore at their discretion: [138,139,140,141,142,143]. It is important to note that many of these proposed DGM architectures could be adapted to the design domain too. De- spite the prominence of 3D object generation papers in the computer graphics community, a few have arisen within the engineering design community as well. In an early work, for example, Brock et al. [84] implement a 3D cVAE to re- construct and interpolate between voxelized models from the ModelNet-10 Dataset. The majority of 3D shape synthesis work in engineering design, however, has implemented some kind of design per- 10 Copyright © by ASME formance consideration. Zhang et al. [92] propose the use of a Genetic Algorithm (GA) to optimize latent space design embeddings of a trained VAE variant called the Variational Shape Learner (VSL) [144], which they demonstrate on the optimization of 3D aircraft models. Shu et al. [69] take an iterative retraining approach by retraining a GAN on high- performance models evaluated using Computational Fluid Dynamics (CFD) evaluation. The authors apply the pro- posed method to point cloud aircraft models from ShapeNet and use minimization of aerodynamic drag as the perfor- mance objective of choice. The authors use a standard GAN loss and use a discriminator architecture from [145]. No- bari et al. [77] introduce another self-augmentation approach to train on skewed or sparse datasets. They couple it with a “range loss” to encourage designs to comply with design constraints related to parameter bounds and apply their ap- proach to generate 3D aircraft models. 5.5 Other Applications In this section, we discuss several application domains for DGMs with only a few relevant papers identiﬁed in our search. Manufacturability: Synthesizing manufacturable designs is an important, albeit less-explored area in DGMs. Greminger [100] proposes approaching the problem of manufacturability using an MSG-GAN architecture [146] adapted to the 3D domain. The author synthetically gen- erates topologies that are manufacturable by a 3-axis mill, trains the GAN to generate similar topologies, then optimizes generated topologies using TO. A key challenge in GAN- based synthesis of manufacturable designs is a lack of anno- tated datasets with design and manufacturing process details. Fluid ﬂow shapes: DGMs have also been used to gener- ate ﬂow shapes. For example, Lee et al. [96] apply a Dou- ble Deep Q-Network (DoubleDQN) [147] RL framework to learn the geometry of a microﬂuidic channel to generate tar- get ﬂow shapes. The authors also propose methods in which a human designer can participate in the design process along- side the RL agent to impart design knowledge or constraints. Other approaches that integrate humans into the design gen- eration process are discussed next. Integrating humans into the generation process: One el- ement of design that is often less emphasized in DGMs for design is human interpretation and design input during the generative process. Though datasets are often created us- ing some human input, humans are not directly involved dur- ing the training process of most DGMs. Burnap et al. [90] note that numerical performance measures often do not cor- respond to a human’s perception of design quality, and study this phenomenon on automobile images generated through a conditional VAE. Supporting the involvement of humans during the training process is a potential approach to ﬁnd the middle ground between the expertise and intuition of hu- mans and the detailed information of datasets, which can en- able different tasks. For example, Wang et al. [78] look at the problem of generating samples that are visually pleasing for any given human subject. To do this, they train a modi- ﬁed version of the Auxiliary Classiﬁer GAN (ACGAN) [148] which classiﬁes the type of image being generated at the same time as learning to generate images. The authors, how- ever, do not opt for a cGAN architecture. Instead, they condi- tion the GAN using Electroencephalography (EEG) signals from human subjects. They train an encoder that takes EEG signals and transforms them into a set of input features for the GAN. By exposing human subjects to the images as they are being generated, the encoder introduces the human re- sponse to the design into the generation process. This ef- fectively allows for a human subject to extract more visually pleasing samples from the GAN model. Several other stud- ies such as [73, 85, 96] propose frameworks that incorporate humans into the design process. Kinematic synthesis: In machine design, generating a mechanism to generate speciﬁc motion curves or transmit power is a common design task. Kinematic synthesis in- volves selecting the size, type, and conﬁguration of mech- anism components to achieve such a goal. Several works have applied DGMs to various kinematic synthesis prob- lems. Deshpande and Purwar [85], for example, propose a VAE-based method for planar linkage synthesis. They de- terministically parameterize both coupler paths and linkages, then generate sample coupler paths from a variety of linkages (four-bar, slider-crank, Stephenson-type six-bar). In the ﬁrst of their two studies, the authors train a VAE to reconstruct trajectories. They then propose a method in which humans can interact with the trained VAE to customize linkage paths to their design goals by visualizing the effect of various latent space perturbations and selecting one that matches their de- sign vision. The authors then expand on their ﬁrst study with a cVAE architecture, feeding in the linkage as the data sam- ples and the coupler curve as the condition vector. In a sim- ilar work, Sharma and Purwar [86] explore spatial linkage synthesis of 5-SS mechanisms using VAEs. In other papers, the same authors take a look at paths themselves as guid- ing mediums for linkage mechanism design by ﬁrst training models to encode paths, then using said models to search within a dataset of mechanisms to obtain solutions for the speciﬁc paths [93]. Other studies have approached the kinematic synthesis task through Reinforcement Learning. Vermeer et al. [107] propose using Deep Q-Learning to synthesize planar link- age mechanisms to obtain mechanisms that are capable of producing straight lines, showing that RL can be effective in generating linkage mechanisms using machine learning. Truss design: Raina et al. [108] propose an RL-based ap- proach to truss design. In their approach, they learn from a dataset of sequential human design decisions [149]. They ﬁrst train an Autoencoder to map trusses to and from a design embedding. Sequential design embeddings are then used to predict a heatmap of possible next design states using a su- pervised transition network. Interestingly, the authors choose 11 Copyright © by ASME to convert to and from images from their parametric repre- sentation when training the autoencoder and using decoded results. Finally, a rule-based agent selects from possible next design steps. Even without knowledge of design metrics and performance, the RL agents were found to generate compet- itive designs compared to humans. Puentes et al. [109] ex- pand on this work by learning action-sequence heuristics in- stead of individual design actions. Raina et al. [97] further expand on this work with a goal-based reinforcement learn- ing agent. Hierarchical product design synthesis: Some works have applied DGMs to multi-component products, typically with a well-known hierarchical structure. Stump et al. [34], for example, propose a unique approach that simultaneously combines RL, Recurrent Neural Networks (RNNs), gram- mars, and physics-based simulation. In their approach, the RNN learns to generate modular sailing crafts by sampling discrete selections from a predeﬁned shape grammar. The training of the RNN is detailed in [110]. The control policy of the craft is then optimized using RL in a physics-based simulated sailing environment. Regenwetter et al. [87, 150] explore the full synthesis of bicycles, a diverse class of prod- ucts typically consisting of numerous hierarchical compo- nents. The authors demonstrate full generative synthesis of bikes using VAEs for both image and parametric represen- tations. The authors note that their detailed design parame- terization allows AI-generated designs to be physically fab- ricated. Procedural content generation: Lopez et al., explore pro- cedural content generation in a speciﬁc context for virtual reality. In their work, they introduce a reinforcement learn- ing model that can learn to generate 3D virtual reality (VR) content which users can explore in a VR environment [98]. In their approach, they speciﬁcally work towards generat- ing manufacturing environments that are physically valid and feasible. In an extension of this work, Cunningham et al., ex- tend the method to generate content for multiple contexts in- stead of just one at a time [99], which enables the integration of user-speciﬁc parameters. 6 Datasets In this section, we present commonly-used datasets that have been or have the potential to be used to train DGMs for data-driven design tasks. We provide a more detailed list on- line1. While the datasets listed are not comprehensive, we aim to note key types of datasets that have been commonly used in engineering design applications. We hope that re- searchers can create larger well-annotated datasets and make them public for the community to use. 6.1 Topology Optimization Datasets Most papers on DGMs for Topology Optimization gen- erate their own datasets, which are often not publicly avail- 1https://decode.mit.edu/datasets/ able. The most common method for dataset generation is Solid Isotropic Material with Penalisation (SIMP), which has several publicly available software implementations. A few TO datasets, however, are open source. Sosnovik and Os- eledets [52] generate a dataset of 10,000 artiﬁcially gener- ated topologies providing both ﬁnal and intermediate topolo- gies from the optimization process2. Each included topology contains one 40x40 image of the topology generated at ev- ery stage of 100 steps of Topology Optimization. Hence, a total of one million images are provided. Nie et al. [71] provide the dataset used to train their TopologyGAN frame- work3. Unlike the aforementioned, this dataset consists only of ﬁnal topologies but contains 49078 generated topologies at a resolution of 64x128, generated from 42 unique bound- ary conditions. Both datasets discussed use the ToPy [151] implementation of SIMP. 6.2 Microstructure Datasets Well-established technologies such as optical mi- croscopy (OM) and scanning electron microscopy (SEM) are often used to visualize material microstructures. As such, numerous datasets of microstructure scan images are pub- licly available, such as [152], [153], and [154]. Numer- ous datasets are also available for the design of composite materials of various types, such as the NanoMine nanopoly- mer composite database4, which contains over 20,000 data- points [155]. Compiled lists of materials science datasets5 and synthetic microstructure datasets are also available. For example, Yang et al. [64] provide a trained GAN model which generates synthetic microstructure images6. 6.3 Design Geometry Datasets The UIUC airfoil database7 has been used as a case study in several generative design research frameworks [76, 15, 75]. The database details nearly 1,600 real-world airfoil designs using coordinates of points on the surface. Since the original data provides inconsistent numbers of coordinates along the top and bottom surfaces, Chen et al. [76] propose a method to standardize the data with B-spline interpolation over the airfoil which is also used in other works [33, 15]. 6.4 3D Object Datasets ShapeNet8 [156] is one of the most commonly-used 3D model datasets, consisting of over 51,300 3D models of 55 object categories. PartNet [157] expands on ShapeNet with ﬁne-grained hierarchical semantic annotations for compo- nent parts of ShapeNet objects. Princeton ModelNet9 [158] 2https://github.com/ISosnovik/top 3https://github.com/zhenguonie/2020_TopologyGAN 4https://github.com/tetherless-world/ nanomine-ontology 5https://github.com/sedaoturak/ data-resources-for-materials-science 6https://github.com/zyz293/GAN_Materials_Design 7https://m-selig.ae.illinois.edu/ads/coord_ database.html 8https://shapenet.org/ 9https://modelnet.cs.princeton.edu/12 Copyright © by ASME is another commonly-used 3D model dataset consisting of 127,915 voxel-based 3D models of 662 object categories. Numerous works discussed in this review use ShapeNet or ModelNet models [69, 77, 92, 84]. Sangpil et al. [159] introduce a dataset of 58,696 models of mechanical components from 68 classes called the Me- chanical Components Benchmark (MCB)10. The MCB con- tains a hierarchical label tree grouping components into sub- classes of different levels, such as Components → Fasteners → Nuts → Wingnuts, for example. Models are represented as point clouds, voxels, and 2D views. 6.5 CAD and CAD-based Datasets Willis et al. [160] introduce two datasets of Autodesk Fusion models11. One dataset, intended for reconstruction tasks, contains 8,625 models and the other, intended for segmentation, contains 35,680 models. The reconstruction dataset is particularly interesting for generative tasks, as it contains information governing the sequential CAD opera- tion steps taken to generate a part. Regenwetter et al. [87, 150] introduce a dataset called BIKED12 consisting of mixed data extracted from 4,512 bi- cycle CAD models. The dataset includes bicycle assembly images, segmented subcomponent images, as well as “para- metric” data. The “parametric” data consists of 2,395 mixed- type design parameters describing both high-level and low- level characteristics. BIKED provides an advantage over conventional 3D object datasets for generative tasks in that synthesized designs contain the necessary parametric infor- mation to physically fabricate designs. BIKED’s paramet- ric data is used in [87] for bicycle synthesis and its image data in [161] for generating novel designs. The FRAMED dataset [162] expands on BIKED with structural perfor- mance data, such as weight, safety factors, and deﬂections under various loads for all 4512 models as well as artiﬁcially generated bicycle frames13. 6.6 Metamaterials Datasets Wang et al. [32] introduce a dataset of 248,396 2D unit cells represented by 50x50 pixelated matrices. The unit cells have associated stiffness tensor components provided and can also be used for TO research. Chan et al. [163] introduce a dataset of 3,000 3D isosurface unit cells sampled from 30 level-set functions, along with corresponding 3D elastic ten- sor components. Wang et al. [164] introduce a dataset of 795 unit cells generated from 10 lattice models. Associated stiff- ness tensors are provided. The three datasets can be found here14. 10https://bit.ly/3ne4gwv 11https://github.com/AutodeskAILab/ Fusion360GalleryDataset 12https://decode.mit.edu/projects/biked/ 13https://decode.mit.edu/projects/framed/ 14https://ideal.mech.northwestern.edu/research/ software/ 6.7 Sketch Datasets QuickDraw [165] is a sketch dataset of 50 million doo- dles from 345 categories15. The doodles were collected by Google from user-drawn sketches in an interactive sketch- ing game. QuickDraw data is used in several works dis- cussed [166, 95]. Toh & Miller [167] introduce a dataset of 934 innovative milk frother design sketches with associated text descriptions that can potentially be used to train DGMs, perhaps factoring in Natural Language Processing (NLP)16. 6.8 Sequential Human Design Datasets McComb et al. [149] provide a tabular truss design dataset taken from a truss design activity executed by six- teen human teams17. Sequential design operations such as joint and member placement are recorded using geometric parameters such as joint coordinates, member size, etc.. Per- formance metrics such as safety factors and weights are also included. While this dataset can be used as a truss design dataset, it is primarily intended as a resource to study or mimic the human design process. We note that the Autodesk Fusion reconstruction dataset mentioned previously [160] can also be used for learning sequential design tasks. 7 Discussion, Challenges and Future Work When applying DGMs to engineering design, the ‘stan- dard’ objective of mimicking the training data is often insuf- ﬁcient, or even counterproductive. Instead, real designs are governed by speciﬁc objectives and constraints, often includ- ing novelty or creativity. Thus, different objectives, such as real-world performance metrics, novelty, and adherence to constraints may make for better training objectives. We dis- cuss the challenges with performance evaluation, constraint violation, and incorporation of novelty in the following sec- tions, as well as pathways to potential solutions. We further discuss some general challenges for DGMs in engineering design, such as limited availability of data, lack of bench- mark problems and metrics, and delayed adoption of cutting edge methods from different research communities. Our dis- cussion is supported by observations from our literature re- view. 7.1 Design Performance Evaluation Incorporating performance evaluation into DGMs is one of the key stepping stones toward practical applications of ML in design. Three major challenges in performance eval- uation are ﬁdelity, cost, and differentiability. 1. Evaluation methods lacking ﬁdelity may result in DGM- generated designs that do not meet speciﬁcations. 2. Computational cost precludes compatibility with meth- ods that heavily sample performance values. 15https://github.com/googlecreativelab/ quickdraw-dataset 16https://sites.psu.edu/creativitymetrics/2018/ 07/18/milkfrother/ 17https://www.sciencedirect.com/science/article/ pii/S2352340918302014?via%3Dihub 13 Copyright © by ASME 3. Lack of differentiability makes implementation into the training process difﬁcult in any machine learning mod- els which rely on gradient-based optimization. Physical evaluation, which means building a product and testing its performance in the real-world, typically has the highest ﬁdelity but is rarely adopted in DGMs due to its prohibitive cost. Qualitative human evaluation of designs has also been investigated in several papers [85, 73, 96]], although the evaluation is typically not focused on perfor- mance. Medium-ﬁdelity evaluations, such as numerical sim- ulations often deliver satisfactory ﬁdelity, but can be costly and are rarely differentiable. Methods that do incorporate medium-ﬁdelity performance evaluation like [69, 62] do not rely on performance score gradients, and instead alternate (re)training and performance evaluation, typically doing this retraining a handful of times. Low-cost evaluation methods like surrogate models can be worked into the training objective and are by far the most common performance evaluation method seen in DGMs [15, 20, 77, 18, 70, 79, 60]. Unfortunately, surrogate models can be brittle and generalize poorly to designs that differ from the data the surrogate was trained on. This is a particular concern when training data for surrogates is unevenly dis- tributed across the performance space [77] and when nov- elty and performance are incorporated into training objec- tives for DGMs. Many different approaches have attempted to improve the performance of low-ﬁdelity surrogate models. For example, self-supervised data augmentation [77] uses the DGM itself to generate samples in sparse regions (through optimization [137] or conditioning [77]) which then can be used to train the low-ﬁdelity models to perform more accu- rately and in turn improve methods that rely on them for de- sign generation. Multi-ﬁdelity modeling is another promis- ing approach, which involves generating surrogate models that augment a few costly high-ﬁdelity samples with low- ﬁdelity samples to attain higher ﬁdelity surrogates with min- imal expense. Advancements have also been made in using machine learning to improve and accelerate medium-ﬁdelity physics simulations such as Finite Element Analysis [168, 169, 170, 171,172] and Computational Fluid Dynamics [173,174,175, 172]. These works, although not yet robust enough to be easily applied as an alternative to high-ﬁdelity simulations, provide a proof of concept for machine learning-based accel- eration and even replacement of higher-ﬁdelity simulations. Numerous studies have also proposed crowdsourcing methods to evaluate synthesized data from generative ma- chine learning methods [90, 176, 166] instead of physical or physics-based evaluation. Though the ﬁdelity of crowd- sourced evaluation is highly task- and crowd-expertise- dependent, the high evaluation cost and lack of differentia- bility are fairly universal. In summary, current DGMs are largely constrained in performance evaluation by an inherent ﬁdelity vs. cost trade- off, however, several directions show promise in enabling faster and more accurate evaluation methods that may escape this limitation. 7.2 Feasibility, Constraints, and Manufacturability One key component of design performance is obeying explicit design constraints, as well as implicit constraints such as physical feasibility and manufacturability. DGMs are difﬁcult to rely upon for explicit design constraints since they are generally probabilistic and may generate completely invalid designs. This issue is a concern that many researchers have pointed to [177,33,103]. One potential solution is to de- velop inexpensive and reliable validation methods, however, this is a challenging task that may require signiﬁcant human input. Another underlying issue lies in design representation, which is discussed in detail in Sec. 3. Representations such as images often have no clear translation to representations with practical uses. Other representations, such as 3D mod- els of various types can only be realistically fabricated us- ing additive manufacturing, which precludes many design domains. For ML-generated designs to be physically fabri- cated and used in practical applications, the feasibility across this ‘domain gap’ must be overcome [103, 87]. Though a few works have investigated manufacturability [100], we ob- served that modeling it is not considered in most of the pa- pers we reviewed. Other works have attempted to apply DGMs to parameterizations that encode similar parametric design data to what would be used in manufacturing draw- ings. However, the same works identify that parametric data of this sort is challenging to learn and generate [87]. 7.3 Creativity and Novelty Whereas creativity and novelty are essential aspects of the classic design process, DGMs rarely explicitly con- sider either. Most DGMs learn to mimic the data cover- ing the existing and already explored portions of the design space. While this emulative behavior is helpful for main- taining realism and ensuring sample quality, it incentivizes DGMs against generating creative or novel designs [178]. For DGMs to progress towards more human-like design, ad- vancements must be made in modeling creativity as well as in developing architectures that promote creativity. Several recent works [137,15,161] have proposed meth- ods to encourage creativity and novelty in DGMs. In their framework, CreativeGAN, Nobari et al., focus on identify- ing novelty and guiding DGMs towards such behavior by di- rectly introducing novel features into typical designs, thereby expanding the design space and novelty of the DGM’s data. Creativity in machine learning has also been explored out- side of the design community. For example, Creative Ad- versarial Networks (CAN) [178] introduce entropy into the training to encourage the generation of surprising images. Readers are directed to Franceschelli et al.’s survey on the topic for more detail [179]. 7.4 Evaluating Model Performance & Benchmark Problems Readers may have noted that many of the works dis- cussed have nearly identical methodologies, though they are applied to a speciﬁc dataset and only compare their work 14 Copyright © by ASME in this respect with a few other baselines. This may be at- tributed to a large variety of design applications compared to other domains such as computer vision and a tendency of de- sign researchers to ﬁnd solutions to their particular problem, instead of ﬁnding generalizable solutions to many. How- ever, this makes any measure of ‘state-of-the-art’ algorithm dependent on speciﬁc applications. Without a good under- standing of state-of-the-art methods that are broadly applica- ble across engineering domains, practitioners will struggle to select models for practical deployment. While some works in the design community have speciﬁcally introduced bench- mark problems and datasets [87, 162], there is still a need for larger, higher-quality, and more numerous datasets and benchmarks. The difﬁculty of establishing a state-of-the-art lies in the lack of benchmark problems and performance metrics within the DGM ﬁeld and within design automation as a whole. Design datasets tend to be small, restricted in domain, and sparse in distribution as we discuss in Section 7.5. Further- more, many methods are developed on proprietary datasets. Finally, the hugely different design representation methods across the ﬁeld make establishing standardized model perfor- mance benchmarks difﬁcult, even if there were good datasets upon which to do so. These problems are noted by many au- thors in the ﬁeld as well, who ﬁnd it difﬁcult to compare their approach with existing ones [60, 177]. 7.5 Data Limitations and Quality Data sparsity is one of the greatest challenges facing the data-driven design community and is a particular concern for researchers developing data-hungry DGMs. Broadly, there are three problems when it comes to the data. The ﬁrst of these is a general lack of data in many design domains. Although datasets continue to cover more and more design ﬁelds, there still are many that lack publicly available data. There are also data representations that are underrepresented in the current datasets, notably graphs as discussed in Sec. 3. The second key data-related problem that designers face in applying DGMs is the insufﬁcient size of current datasets. Many of the latest breakthroughs in deep learning, notably in computer vision and natural language processing, have owed their success to very large models which are notori- ously data-hungry, requiring millions or billions of training examples, and cannot be properly utilized with datasets of smaller size. Focusing resources on the generation of very large publicly available datasets or effective data augmenta- tion methods would open doors for the design community to leverage larger deep models. The last major limitation in existing data is sparsity and bias. These problems are com- mon in many design datasets. How the data is distributed in the performance space can cause some challenges for many DGMs, especially in inverse design problems [77]. Diver- sity is important to ensure the data covers the design space as evenly as possible to avoid bias in the data. Keeping di- versity and bias avoidance in mind will help researchers gen- erate better datasets. We highly encourage researchers to develop diverse datasets, publicly release these datasets, and establish bench- mark problems for future works. We subsequently encourage researchers to test their methods on other researchers’ data, publicly release their benchmarking results, and acknowl- edge state-of-the-art methods when possible. 7.6 Other DGMs So far our discussion has been focused on the VAE, GAN, and RL-based approaches, which have dominated the ﬁeld of DGMs in engineering design. Transformer-based se- quential models were initially developed for natural language processing and generation or translation of text, however, researchers have shown that the scalability of these mod- els allows for them to be used for the generation of com- plex data in a sequential manner [180, 181, 182, 183]. Fur- thermore, these models have an unprecedented ability to be mixed with language processing. This may enable human language-based conditioning and control over the generation process of data [181, 183], enabling deployment for a much wider user base and a much wider set of tasks. The applica- tions of transformer-based models as well as their scalability may appeal to researchers in the design community. Despite this, it is important to note that these models are particularly large and therefore require very large datasets, further high- lighting the need for better datasets. 8 Conclusion In this review, we discussed the applications of Deep Generative Models (DGMs) across engineering design ﬁelds. To give readers a sense of the tools and methods available, we began our review with an overview of DGMs typically used for engineering design problems, emphasizing Gen- erative Adversarial Networks (GANs), Variational Autoen- coders (VAEs), and Reinforcement Learning (RL). To help weigh different design representation methods, we then dis- cussed the strengths and weaknesses of common parameter- ization methods. To inform readers about existing work out- side of their subdomain, we then collected and reviewed 63 papers from a variety of different engineering design sub- disciplines which directly propose DGMs. To make read- ers aware of the datasets available on which to develop and test data-driven design methods, we review commonly-used datasets in the ﬁeld. Finally, to provide inspiration, we dis- cuss key challenges and limitations currently seen across the ﬁeld and highlight possible solution pathways. References [1] Chakrabarti, A., Shea, K., Stone, R., Cagan, J., Camp- bell, M., Hernandez, N. V., and Wood, K. L., 2011. “Computer-Based Design Synthesis Research: An Overview”. Journal of Computing and Information Science in Engineering, 11(2), 06. 021003. [2] Deng, L., and Yu, D., 2014. “Deep learning: Meth- ods and applications”. Found. Trends Signal Process., 7(3–4), jun, p. 197–387. 15 Copyright © by ASME [3] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y., 2014. “Generative adversarial nets”. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS’14, MIT Press, p. 2672–2680. [4] Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A., 2017. “Unpaired image-to-image translation using cycle- consistent adversarial networks”. In 2017 IEEE In- ternational Conference on Computer Vision (ICCV), pp. 2242–2251. [5] Choi, Y., Uh, Y., Yoo, J., and Ha, J.-W., 2020. “Star- gan v2: Diverse image synthesis for multiple do- mains”. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8185– 8194. [6] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehti- nen, J., and Aila, T., 2020. “Analyzing and improving the image quality of stylegan”. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion (CVPR), pp. 8107–8116. [7] Karras, T., Laine, S., and Aila, T., 2019. “A style- based generator architecture for generative adversar- ial networks”. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4396–4405. [8] Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B., and Bharath, A. A., 2018. “Genera- tive adversarial networks: An overview”. IEEE Signal Processing Magazine, 35(1), pp. 53–65. [9] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X., 2016. “Improved tech- niques for training gans”. In Proceedings of the 30th International Conference on Neural Information Pro- cessing Systems, NIPS’16, Curran Associates Inc., p. 2234–2242. [10] Arjovsky, M., and Bottou, L., 2017. “Towards princi- pled methods for training generative adversarial net- works”. arXiv preprint arXiv:1701.04862. [11] Arjovsky, M., Chintala, S., and Bottou, L., 2017. “Wasserstein generative adversarial networks”. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, JMLR.org, p. 214–223. [12] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A., 2017. “Improved training of wasserstein gans”. In Proceedings of the 31st In- ternational Conference on Neural Information Pro- cessing Systems, NIPS’17, Curran Associates Inc., p. 5769–5779. [13] Srivastava, A., Valkov, L., Russell, C., Gutmann, M. U., and Sutton, C., 2017. “Veegan: Reducing mode collapse in gans using implicit variational learning”. In Advances in Neural Information Processing Sys- tems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wal- lach, R. Fergus, S. Vishwanathan, and R. Garnett, eds., Vol. 30, Curran Associates, Inc., pp. 3308–3318. [14] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X., and Chen, X., 2016. “Im- proved techniques for training gans”. In Advances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, eds., Vol. 29, Curran Associates, Inc., pp. 2234–2242. [15] Chen, W., and Ahmed, F., 2021. “Padgan: Learning to generate high-quality novel designs”. Journal of Mechanical Design, 143(3), p. 031703. [16] Mirza, M., and Osindero, S., 2014. “Condi- tional generative adversarial nets”. arXiv preprint arXiv:1411.1784. [17] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P., 2016. “Infogan: Inter- pretable representation learning by information max- imizing generative adversarial nets”. In Proceedings of the 30th International Conference on Neural Infor- mation Processing Systems, pp. 2180–2188. [18] Dong, Y., Li, D., Zhang, C., Wu, C., Wang, H., Xin, M., Cheng, J., and Lin, J., 2020. “Inverse design of two-dimensional graphene/h-bn hybrids by a regres- sional and conditional gan”. Carbon, 169, pp. 9–16. [19] Ding, X., Wang, Y., Xu, Z., Welch, W. J., and Wang, Z. J., 2021. “Ccgan: Continuous conditional gener- ative adversarial networks for image generation”. In 9th International Conference on Learning Represen- tations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, OpenReview.net. [20] Heyrani Nobari, A., Chen, W., and Ahmed, F., 2021. “Pcdgan: A continuous conditional diverse generative adversarial network for inverse design”. Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, Aug. [21] Kingma, D. P., and Welling, M., 2013. “Auto- encoding variational bayes”. arXiv preprint arXiv:1312.6114. [22] Kullback, S., and Leibler, R. A., 1951. “On infor- mation and sufﬁciency”. The annals of mathematical statistics, 22(1), pp. 79–86. [23] Kingma, D. P., and Welling, M., 2019. “An introduc- tion to variational autoencoders”. Foundations and Trends in Machine Learning, 12(4), pp. 307–392. [24] Sohn, K., Yan, X., and Lee, H., 2015. “Learn- ing structured output representation using deep con- ditional generative models”. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS’15, MIT Press, p. 3483–3491. [25] Kaelbling, L. P., Littman, M. L., and Moore, A. W., 1996. “Reinforcement learning: A survey”. J. Artif. Int. Res., 4(1), May, p. 237–285. [26] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Ried- miller, M., Fidjeland, A. K., Ostrovski, G., et al., 2015. “Human-level control through deep reinforce- ment learning”. nature, 518(7540), pp. 529–533. [27] Daneshmand, M., Helmi, A., Avots, E., Noroozi, F., Alisinanoglu, F., Arslan, H. S., Gorbova, J., Haamer, R. E., Ozcinar, C., and Anbarjafari, G., 2018. “3d 16 Copyright © by ASME scanning: A comprehensive survey”. arXiv preprint arXiv:1801.08863. [28] Remondino, F., 2003. “From point cloud to surface: the modeling and visualization problem”. Interna- tional Archives of the Photogrammetry, Remote Sens- ing and Spatial Information Sciences, 34. [29] Ranjan, A., Bolkart, T., Sanyal, S., and Black, M. J., 2018. “Generating 3d faces using convolutional mesh autoencoders”. In Proceedings of the European Con- ference on Computer Vision (ECCV), pp. 704–720. [30] Cheng, S., Bronstein, M., Zhou, Y., Kotsia, I., Pan- tic, M., and Zafeiriou, S., 2019. “Meshgan: Non- linear 3d morphable models of faces”. arXiv preprint arXiv:1903.10384. [31] Zhang, Z., Wang, Y., Jimack, P. K., and Wang, H., 2020. “Meshingnet: a new mesh generation method based on deep learning”. In International Conference on Computational Science, Springer, pp. 186–198. [32] Wang, L., Chan, Y.-C., Ahmed, F., Liu, Z., Zhu, P., and Chen, W., 2020. “Deep generative modeling for mechanistic-based learning and design of metamate- rial systems”. Computer Methods in Applied Mechan- ics and Engineering, 372, p. 113377. [33] Chen, W., and Fuge, M., 2019. “Synthesizing designs with interpart dependencies using hierarchical gener- ative adversarial networks”. Journal of Mechanical Design, 141(11), p. 111403. [34] Stump, G. M., Miller, S. W., Yukish, M. A., Simp- son, T. W., and Tucker, C., 2019. “Spatial grammar- based recurrent neural network for design form and behavior optimization”. Journal of Mechanical De- sign, 141(12). [35] Cao, W., Robinson, T., Hua, Y., Boussuge, F., Col- ligan, A. R., and Pan, W., 2020. “Graph Repre- sentation of 3D CAD Models for Machining Feature Recognition With Deep Learning”. Vol. Volume 11A: 46th Design Automation Conference (DAC) of Inter- national Design Engineering Technical Conferences and Computers and Information in Engineering Con- ference. V11AT11A003. [36] Yang, W., Ding, H., Zi, B., and Zhang, D., 2017. “New Graph Representation for Planetary Gear Trains”. Journal of Mechanical Design, 140(1), 11. 012303. [37] Hsu, C.-H., and Lam, K.-T., 1992. “A New Graph Representation for the Automatic Kinematic Analysis of Planetary Spur-Gear Trains”. Journal of Mechani- cal Design, 114(1), 03, pp. 196–200. [38] Lee, J. Y., and Kim, K., 1996. “Geometric rea- soning for knowledge-based parametric design us- ing graph representation”. Computer-Aided Design, 28(10), pp. 831–841. [39] Coatan´ea, E., Nonsiri, S., Christophe, F., and Mokam- mel, F., 2014. “Graph Based Representation and Analyses for Conceptual Stages”. Vol. Volume 1A: 34th Computers and Information in Engineering Con- ference of International Design Engineering Techni- cal Conferences and Computers and Information in Engineering Conference. V01AT02A071. [40] Patalano, S., Vitolo, F., and Lanzotti, A., 2013. “A graph-based software tool for the cad modeling of me- chanical assemblies”. In GRAPP/IVAPP. [41] Henaff, M., Bruna, J., and LeCun, Y., 2015. Deep convolutional networks on graph-structured data. [42] Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J., 2019. “Graph transformer networks”. In Advances in Neural Information Processing Systems, H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, eds., Vol. 32, Curran Asso- ciates, Inc. [43] Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y., 2018. “Graph attention networks”. In International Conference on Learning Representations. [44] Vashishth, S., Sanyal, S., Nitin, V., and Talukdar, P., 2019. “Composition-based multi-relational graph convolutional networks”. In International Conference on Learning Representations. [45] Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R., 2017. Gated graph sequence neural networks. [46] Liao, R., Li, Y., Song, Y., Wang, S., Hamilton, W., Duvenaud, D. K., Urtasun, R., and Zemel, R., 2019. “Efﬁcient graph generation with graph recurrent at- tention networks”. In Advances in Neural Informa- tion Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, eds., Vol. 32, Curran Associates, Inc. [47] Bojchevski, A., Shchur, O., Z¨ugner, D., and G¨unnemann, S., 2018. “Netgan: Generating graphs via random walks”. In International Conference on Machine Learning, pp. 609–618. [48] You, J., Ying, R., Ren, X., Hamilton, W., and Leskovec, J., 2018. “Graphrnn: Generating realis- tic graphs with deep auto-regressive models”. In In- ternational conference on machine learning, PMLR, pp. 5708–5717. [49] Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P., 2018. Learning deep generative models of graphs. [50] Cao, N. D., and Kipf, T., 2018. Molgan: An implicit generative model for small molecular graphs. [51] You, J., Liu, B., Ying, R., Pande, V., and Leskovec, J., 2018. “Graph convolutional policy network for goal- directed molecular graph generation”. In Proceedings of the 32nd International Conference on Neural Infor- mation Processing Systems, pp. 6412–6422. [52] Sosnovik, I., and Oseledets, I., 2019. “Neural net- works for topology optimization”. Russian Journal of Numerical Analysis and Mathematical Modelling, 34(4), pp. 215–223. [53] Behzadi, M. M., and Ilies¸, H. T., 2021. “Real-time topology optimization in 3d via deep transfer learn- ing”. Computer-Aided Design, 135, p. 103014. [54] Keshavarzzadeh, V., Alirezaei, M., Tasdizen, T., and Kirby, R. M., 2021. “Image-based multiresolution topology optimization using deep disjunctive nor- 17 Copyright © by ASME mal shape model”. Computer-Aided Design, 130, p. 102947. [55] Cang, R., Yao, H., and Ren, Y., 2019. “One-shot generation of near-optimal topology through theory- driven machine learning”. Computer-Aided Design, 109, pp. 12–21. [56] Malkiel, I., Mrejen, M., Nagler, A., Arieli, U., Wolf, L., and Suchowski, H., 2018. “Plasmonic nanostruc- ture design and characterization via deep learning”. Light: Science & Applications, 7(1), pp. 1–8. [57] Li, X., Zhang, Y., Zhao, H., Burkhart, C., Brin- son, L. C., and Chen, W., 2018. “A transfer learn- ing approach for microstructure reconstruction and structure-property predictions”. Scientiﬁc reports, 8(1), pp. 1–13. [58] Jung, J., Na, J., Park, H. K., Park, J. M., Kim, G., Lee, S., and Kim, H. S., 2021. “Super-resolving ma- terial microstructure image via deep learning for mi- crostructure characterization and mechanical behavior analysis”. npj Computational Materials, 7(1), pp. 1– 11. [59] Li, B., Huang, C., Li, X., Zheng, S., and Hong, J., 2019. “Non-iterative structural topology optimization using deep learning”. Computer-Aided Design, 115, pp. 172–180. [60] Rawat, S., and Shen, M. H., 2019. Application of adversarial networks for 3d structural topology opti- mization. Tech. rep., SAE Technical Paper. [61] Oh, S., Jung, Y., Lee, I., and Kang, N., 2018. “De- sign automation by integrating generative adversarial networks and topology optimization”. In International Design Engineering Technical Conferences and Com- puters and Information in Engineering Conference, Vol. 51753, American Society of Mechanical Engi- neers, p. V02AT03A008. [62] Oh, S., Jung, Y., Kim, S., Lee, I., and Kang, N., 2019. “Deep generative design: Integration of topology opti- mization and generative models”. Journal of Mechan- ical Design, 141(11). [63] Tan, R. K., Zhang, N. L., and Ye, W., 2020. “A deep learning–based method for the design of microstruc- tural materials”. Structural and Multidisciplinary Op- timization, 61(4), pp. 1417–1438. [64] Yang, Z., Li, X., Catherine Brinson, L., Choudhary, A. N., Chen, W., and Agrawal, A., 2018. “Mi- crostructural materials design via deep adversarial learning methodology”. Journal of Mechanical De- sign, 140(11). [65] Zhang, H., Yang, L., Li, C., Wu, B., and Wang, W., 2021. “Scaffoldgan: Synthesis of scaffold materials based on generative adversarial networks”. Computer- Aided Design, 138, p. 103041. [66] Mosser, L., Dubrule, O., and Blunt, M. J., 2017. “Re- construction of three-dimensional porous media using generative adversarial neural networks”. Physical Re- view E, 96(4), p. 043309. [67] Lee, J.-W., Goo, N. H., Park, W. B., Pyo, M., and Sohn, K.-S., 2021. “Virtual microstructure design for steels using generative adversarial networks”. Engi- neering Reports, 3(1), p. e12274. [68] Liu, S., Zhong, Z., Takbiri-Borujeni, A., Kazemi, M., Fu, Q., and Yang, Y., 2019. “A case study on ho- mogeneous and heterogeneous reservoir porous me- dia reconstruction by using generative adversarial net- works”. Energy Procedia, 158, pp. 6164–6169. [69] Shu, D., Cunningham, J., Stump, G., Miller, S. W., Yukish, M. A., Simpson, T. W., and Tucker, C. S., 2020. “3d design using generative adversarial net- works and physics-based validation”. Journal of Me- chanical Design, 142(7), p. 071701. [70] Sharpe, C., and Seepersad, C. C., 2019. “Topology design with conditional generative adversarial net- works”. In International Design Engineering Tech- nical Conferences and Computers and Information in Engineering Conference, Vol. 59186, American Soci- ety of Mechanical Engineers, p. V02AT03A062. [71] Nie, Z., Lin, T., Jiang, H., and Kara, L. B., 2021. “Topologygan: Topology optimization using genera- tive adversarial networks based on physical ﬁelds over the initial domain”. Journal of Mechanical Design, 143(3), p. 031715. [72] Yu, Y., Hur, T., Jung, J., and Jang, I. G., 2019. “Deep learning for determining a near-optimal topological design without any iteration”. Structural and Multi- disciplinary Optimization, 59(3), pp. 787–799. [73] Valdez, S., Seepersad, C., and Kambampati, S., 2021. “A framework for interactive structural design explo- ration”. In International Design Engineering Tech- nical Conferences and Computers and Information in Engineering Conference, IDETC-21, ASME. [74] Yilmaz, E., and German, B., 2020. “Conditional gen- erative adversarial network framework for airfoil in- verse design”. In AIAA aviation 2020 forum, p. 3185. [75] Chen, W., and Fuge, M., 2018. “B´eziergan: Au- tomatic generation of smooth curves from inter- pretable low-dimensional parameters”. arXiv preprint arXiv:1808.08871. [76] Chen, W., Chiu, K., and Fuge, M., 2019. “Aero- dynamic design optimization and shape exploration using generative adversarial networks”. In AIAA Scitech 2019 Forum, p. 2351. [77] Heyrani Nobari, A., Chen, W. W., and Ahmed, F., 2021. “RANGE-GAN: Design Synthesis Under Constraints Using Conditional Generative Adversarial Networks”. Journal of Mechanical Design, 09, pp. 1– 16. [78] Wang, P., Peng, D., Li, L., Chen, L., Wu, C., Wang, X., Childs, P., and Guo, Y., 2019. “Human-in-the- loop design with machine learning”. Proceedings of the Design Society: International Conference on En- gineering Design, 1(1), p. 2577–2586. [79] Guo, T., Lohan, D. J., Cang, R., Ren, M. Y., and Alli- son, J. T., 2018. “An indirect design representation for topology optimization using variational autoencoder and style transfer”. In 2018 AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Con- 18 Copyright © by ASME ference, p. 0804. [80] Cang, R., Li, H., Yao, H., Jiao, Y., and Ren, Y., 2018. “Improving direct physical properties prediction of heterogeneous materials from imaging data via con- volutional neural network and a morphology-aware generative model”. Computational Materials Science, 150, pp. 212–221. [81] Li, X., Ning, S., Liu, Z., Yan, Z., Luo, C., and Zhuang, Z., 2020. “Designing phononic crystal with antici- pated band gap through a deep learning based data- driven method”. Computer Methods in Applied Me- chanics and Engineering, 361, p. 112737. [82] Liu, Z., Raju, L., Zhu, D., and Cai, W., 2020. “A hy- brid strategy for the discovery and design of photonic structures”. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 10(1), pp. 126–135. [83] Xue, T., Wallin, T. J., Menguc, Y., Adriaenssens, S., and Chiaramonte, M., 2020. “Machine learning gen- erative models for automatic design of multi-material 3d printed composite solids”. Extreme Mechanics Let- ters, 41, p. 100992. [84] Brock, A., Lim, T., Ritchie, J. M., and We- ston, N., 2016. “Context-aware content genera- tion for virtual environments”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, Vol. 50084, American Society of Mechanical Engi- neers, p. V01BT02A045. [85] Deshpande, S., and Purwar, A., 2019. “Computational creativity via assisted variational synthesis of mecha- nisms using deep generative models”. Journal of Me- chanical Design, 141(12). [86] Sharma, S., and Purwar, A., 2020. “Path synthesis of defect-free spatial 5-ss mechanisms using machine learning”. In International Design Engineering Tech- nical Conferences and Computers and Information in Engineering Conference, Vol. 83990, American Soci- ety of Mechanical Engineers, p. V010T10A034. [87] Regenwetter, L., Curry, B., and Ahmed, F., 2021. “BIKED: A dataset and machine learning benchmarks for data-driven bicycle design”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, IDETC-21, ASME. [88] Tang, Y., Kojima, K., Koike-Akino, T., Wang, Y., Wu, P., Tahersima, M., Jha, D., Parsons, K., and Qi, M., 2020. “Generative deep learning model for a multi- level nano-optic broadband power splitter”. In 2020 Optical Fiber Communications Conference and Exhi- bition (OFC), IEEE, pp. 1–3. [89] Chen, H., and Liu, X., 2021. “Geometry enhanced generative adversarial networks for random heteroge- neous material representation”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, IDETC-21, ASME. [90] Burnap, A., Liu, Y., Pan, Y., Lee, H., Gonza- lez, R., and Papalambros, P. Y., 2016. “Estimat- ing and exploring the product form design space us- ing deep generative models”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, Vol. 50107, American Society of Mechanical Engi- neers, p. V02AT03A013. [91] Ma, W., Cheng, F., Xu, Y., Wen, Q., and Liu, Y., 2019. “Probabilistic representation and inverse design of metamaterials based on a deep generative model with semi-supervised learning strategy”. Advanced Materials, 31(35), p. 1901111. [92] Zhang, W., Yang, Z., Jiang, H., Nigam, S., Yamakawa, S., Furuhata, T., Shimada, K., and Kara, L. B., 2019. “3d shape synthesis for conceptual design and opti- mization using variational autoencoders”. In Inter- national Design Engineering Technical Conferences and Computers and Information in Engineering Con- ference, Vol. 59186, American Society of Mechanical Engineers, p. V02AT03A017. [93] Deshpande, S., and Purwar, A., 2020. “An Image- Based Approach to Variational Path Synthesis of Linkages”. Journal of Computing and Information Science in Engineering, 21(2), 10. 021005. [94] Li, R., Zhang, Y., and Chen, H., 2021. “Learning the aerodynamic design of supercritical airfoils through deep reinforcement learning”. AIAA Journal, pp. 1– 14. [95] Dering, M., Cunningham, J., Desai, R., Yukish, M. A., Simpson, T. W., and Tucker, C. S., 2018. “A physics- based virtual environment for enhancing the quality of deep generative designs”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, Vol. 51753, American Society of Mechanical Engi- neers, p. V02AT03A015. [96] Lee, X. Y., Balu, A., Stoecklein, D., Ganapathysub- ramanian, B., and Sarkar, S., 2019. “A case study of deep reinforcement learning for engineering de- sign: Application to microﬂuidic devices for ﬂow sculpting”. Journal of Mechanical Design, 141(11), p. 111401. [97] Raina, A., Puentes, L., Cagan, J., and McComb, C., 2021. “Goal-directed design agents: Integrating vi- sual imitation with one-step lookahead optimization for generative design”. Journal of Mechanical Design, 143(12), p. 124501. [98] Lopez, C. E., Ashour, O., and Tucker, C. S., 2019. “Reinforcement Learning Content Generation for Vir- tual Reality Applications”. Vol. Volume 1: 39th Com- puters and Information in Engineering Conference of International Design Engineering Technical Confer- ences and Computers and Information in Engineering Conference. V001T02A009. [99] Cunningham, J., Lopez, C., Ashour, O., and Tucker, C. S., 2020. “Multi-Context Generation in Virtual Re- ality Environments Using Deep Reinforcement Learn- ing”. Vol. Volume 9: 40th Computers and Informa- tion in Engineering Conference (CIE) of International 19 Copyright © by ASME Design Engineering Technical Conferences and Com- puters and Information in Engineering Conference. V009T09A072. [100] Greminger, M., 2020. “Generative adversarial net- works with synthetic training data for enforcing man- ufacturing constraints on topology optimization”. In International Design Engineering Technical Confer- ences and Computers and Information in Engineer- ing Conference, Vol. 84003, American Society of Me- chanical Engineers, p. V11AT11A005. [101] Fujita, K., Minowa, K., Nomaguchi, Y., Yamasaki, S., and Yaji, K., 2021. “Design concept generation with variational deep embedding over comprehensive opti- mization”. In International Design Engineering Tech- nical Conferences and Computers and Information in Engineering Conference, IDETC-21, ASME. [102] Cang, R., Vipradas, A., and Ren, Y., 2017. “Scalable microstructure reconstruction with multi- scale pattern preservation”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, Vol. 58134, American Society of Mechanical Engi- neers, p. V02BT03A010. [103] Cang, R., Xu, Y., Chen, S., Liu, Y., Jiao, Y., and Yi Ren, M., 2017. “Microstructure representation and reconstruction of heterogeneous materials via deep belief network for computational material design”. Journal of Mechanical Design, 139(7), p. 071404. [104] Fokina, D., Muravleva, E., Ovchinnikov, G., and Os- eledets, I., 2020. “Microstructure synthesis using style-based generative adversarial networks”. Phys- ical Review E, 101(4), p. 043308. [105] Wang, Z., Xian, W., Baccouche, M. R., Lanzerath, H., Li, Y., and Xu, H., 2021. “A gaussian mixture variational autoencoder-based approach for designing phononic bandgap metamaterials”. In International Design Engineering Technical Conferences and Com- puters and Information in Engineering Conference, IDETC-21, ASME. [106] Cang, R., and Ren, M. Y., 2016. “Deep Network- Based Feature Extraction and Reconstruction of Com- plex Material Microstructures”. Vol. Volume 2B: 42nd Design Automation Conference of International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference. V02BT03A008. [107] Vermeer, K., Kuppens, R., and Herder, J., 2018. “Kinematic Synthesis Using Reinforcement Learn- ing”. Vol. Volume 2A: 44th Design Automation Con- ference of International Design Engineering Techni- cal Conferences and Computers and Information in Engineering Conference. V02AT03A009. [108] Raina, A., McComb, C., and Cagan, J., 2019. “Learn- ing to design from humans: Imitating human design- ers through deep learning”. Journal of Mechanical Design, 141(11). [109] Puentes, L., Raina, A., Cagan, J., and McComb, C., 2020. “Modeling a strategic human engineering design process: Human-inspired heuristic guidance through learned visual design agents”. In Proceedings of the Design Society: DESIGN Conference, Vol. 1, Cambridge University Press, pp. 355–364. [110] Yukish, M. A., Stump, G. M., and Miller, S. W., 2020. “Using recurrent neural networks to model spa- tial grammars for design creation”. Journal of Me- chanical Design, 142(10), p. 104501. [111] Zhu, J.-H., Zhang, W.-H., and Xia, L., 2016. “Topol- ogy optimization in aircraft and aerospace structures design”. Archives of Computational Methods in Engi- neering, 23(4), pp. 595–622. [112] Xia, L., and Breitkopf, P., 2017. “Recent advances on topology optimization of multiscale nonlinear struc- tures”. Archives of Computational Methods in Engi- neering, 24(2), pp. 227–249. [113] Borrvall, T., and Petersson, J., 2003. “Topology opti- mization of ﬂuids in stokes ﬂow”. International jour- nal for numerical methods in ﬂuids, 41(1), pp. 77– 107. [114] Zhou, S., and Li, Q., 2008. “A variational level set method for the topology optimization of steady- state navier–stokes ﬂow”. Journal of Computational Physics, 227(24), pp. 10178–10195. [115] Zegard, T., and Paulino, G. H., 2016. “Bridging topology optimization and additive manufacturing”. Structural and Multidisciplinary Optimization, 53(1), pp. 175–192. [116] Langelaar, M., 2016. “Topology optimization of 3d self-supporting structures for additive manufactur- ing”. Additive Manufacturing, 12, pp. 60–70. [117] Dbouk, T., 2017. “A review about the engineering de- sign of optimal heat transfer systems using topology optimization”. Applied Thermal Engineering, 112, pp. 841–854. [118] Koga, A. A., Lopes, E. C. C., Nova, H. F. V., De Lima, C. R., and Silva, E. C. N., 2013. “Development of heat sink device by using topology optimization”. In- ternational Journal of Heat and Mass Transfer, 64, pp. 759–772. [119] Gatys, L., Ecker, A. S., and Bethge, M., 2015. “Tex- ture synthesis using convolutional neural networks”. Advances in neural information processing systems, 28, pp. 262–270. [120] Gatys, L. A., Ecker, A. S., and Bethge, M., 2016. “Image style transfer using convolutional neural net- works”. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414– 2423. [121] Berthelot, D., Schumm, T., and Metz, L., 2017. “Be- gan: Boundary equilibrium generative adversarial net- works”. arXiv preprint arXiv:1703.10717. [122] Jiang, Z., Zheng, Y., Tan, H., Tang, B., and Zhou, H., 2017. “Variational deep embedding: an unsupervised and generative approach to clustering”. In Proceed- ings of the 26th International Joint Conference on Ar- tiﬁcial Intelligence, pp. 1965–1972. [123] Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cun- 20 Copyright © by ASME ningham, A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al., 2017. “Photo-realistic single image super-resolution using a generative adversarial net- work”. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, pp. 105–114. [124] Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A., 2017. “Image-to-image translation with conditional adver- sarial networks”. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp. 1125–1134. [125] He, K., Zhang, X., Ren, S., and Sun, J., 2016. “Deep residual learning for image recognition”. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pp. 770–778. [126] Hu, J., Shen, L., and Sun, G., 2018. “Squeeze-and- excitation networks”. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp. 7132–7141. [127] Long, J., Shelhamer, E., and Darrell, T., 2015. “Fully convolutional networks for semantic segmentation”. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431–3440. [128] Bostanabad, R., Zhang, Y., Li, X., Kearney, T., Brin- son, L. C., Apley, D. W., Liu, W. K., and Chen, W., 2018. “Computational microstructure characteriza- tion and reconstruction: Review of the state-of-the-art techniques”. Progress in Materials Science, 95, pp. 1– 41. [129] Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y., 2009. “Convolutional deep belief networks for scal- able unsupervised learning of hierarchical representa- tions”. In Proceedings of the 26th annual international conference on machine learning, pp. 609–616. [130] Yu, S., Zhang, Y., Wang, C., Lee, W.-k., Dong, B., Odom, T. W., Sun, C., and Chen, W., 2017. “Characterization and design of functional quasi- random nanostructured materials using spectral den- sity function”. Journal of Mechanical Design, 139(7), p. 071401. [131] Molesky, S., Lin, Z., Piggott, A. Y., Jin, W., Vuckovi´c, J., and Rodriguez, A. W., 2018. “Inverse design in nanophotonics”. Nature Photonics, 12(11), pp. 659– 670. [132] Dilokthanakul, N., Mediano, P. A., Garnelo, M., Lee, M. C., Salimbeni, H., Arulkumaran, K., and Shana- han, M., 2016. “Deep unsupervised clustering with gaussian mixture variational autoencoders”. arXiv preprint arXiv:1611.02648. [133] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O., 2017. “Proximal policy optimization algorithms”. arXiv preprint arXiv:1707.06347. [134] Gielis, J., 2003. “A generic geometric transforma- tion that uniﬁes a wide range of natural and abstract shapes”. American journal of botany, 90(3), pp. 333– 338. [135] Ha, D., and Eck, D., 2018. “A neural representation of sketch drawings”. In International Conference on Learning Representations. [136] Kulesza, A., Taskar, B., et al., 2012. “Determinantal point processes for machine learning”. Foundations and Trends® in Machine Learning, 5(2–3), pp. 123– 286. [137] Chen, W., and Ahmed, F., 2021. “Mo-padgan: Reparameterizing engineering designs for augmented multi-objective optimization”. Applied Soft Comput- ing, 113, p. 107909. [138] Li, J., Xu, K., Chaudhuri, S., Yumer, E., Zhang, H., and Guibas, L., 2017. “Grass: Generative recursive autoencoders for shape structures”. ACM Transac- tions on Graphics (TOG), 36(4), pp. 1–14. [139] Zou, C., Yumer, E., Yang, J., Ceylan, D., and Hoiem, D., 2017. “3d-prnn: Generating shape primitives with recurrent neural networks”. In Proceedings of the IEEE International Conference on Computer Vision, pp. 900–909. [140] Groueix, T., Fisher, M., Kim, V. G., Russell, B. C., and Aubry, M., 2018. “A papier-mˆach´e approach to learning 3d surface generation”. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 216–224. [141] Gao, L., Yang, J., Wu, T., Yuan, Y.-J., Fu, H., Lai, Y.-K., and Zhang, H., 2019. “Sdm-net: Deep gener- ative network for structured deformable mesh”. ACM Transactions on Graphics (TOG), 38(6), pp. 1–15. [142] Mo, K., Guerrero, P., Yi, L., Su, H., Wonka, P., Mitra, N. J., and Guibas, L. J., 2019. “Structurenet: hierar- chical graph networks for 3d shape generation”. ACM Transactions on Graphics, 38(6), pp. 1–19. [143] Park, J. J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S., 2019. “Deepsdf: Learning continuous signed distance functions for shape representation”. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pp. 165–174. [144] Liu, S., Giles, L., and Ororbia, A., 2018. “Learning a hierarchical latent-variable model of 3d shapes”. In 2018 International Conference on 3D Vision (3DV), IEEE, pp. 542–551. [145] Qi, C. R., Su, H., Mo, K., and Guibas, L. J., 2017. “Pointnet: Deep learning on point sets for 3d classiﬁ- cation and segmentation”. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pp. 652–660. [146] Karnewar, A., and Wang, O., 2020. “Msg-gan: Multi- scale gradients for generative adversarial networks”. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pp. 7799–7808. [147] van Hasselt, H., Guez, A., and Silver, D., 2016. “Deep reinforcement learning with double q-learning”. Pro- ceedings of the AAAI Conference on Artiﬁcial Intelli- gence, 30(1), Mar. [148] Odena, A., Olah, C., and Shlens, J., 2017. “Condi- tional image synthesis with auxiliary classiﬁer gans”. In International conference on machine learning, PMLR, pp. 2642–2651. [149] McComb, C., Cagan, J., and Kotovsky, K., 2018. “Data on the design of truss structures by teams of en- 21 Copyright © by ASME gineering students”. Data in brief, 18, pp. 160–163. [150] Regenwetter, L., Curry, B., and Ahmed, F., 2022. “Biked: A dataset for computational bicycle design with machine learning benchmarks”. Journal of Me- chanical Design, 144(3). [151] Hunter, W., et al., 2017. Topy-topology optimization with python. [152] Iren, D., Ackermann, M., Gorfer, J., Pujar, G., Wesselmecking, S., Krupp, U., and Bromuri, S., 2021. “Aachen-heerlen annotated steel microstructure dataset”. Scientiﬁc Data, 8(1), pp. 1–9. [153] Larmuseau, M., Sluydts, M., Theuwissen, K., Duprez, L., Dhaene, T., and Cottenier, S., 2020. “Compact representations of microstructure images using triplet networks”. npj Computational Materials, 6(1), pp. 1– 11. [154] DeCost, B. L., Hecht, M. D., Francis, T., Webler, B. A., Picard, Y. N., and Holm, E. A., 2017. “Uhcsdb: ultrahigh carbon steel micrograph database”. Integrat- ing Materials and Manufacturing Innovation, 6(2), pp. 197–205. [155] Zhao, H., Wang, Y., Lin, A., Hu, B., Yan, R., Mc- Cusker, J., Chen, W., McGuinness, D. L., Schadler, L., and Brinson, L. C., 2018. “Nanomine schema: An extensible data representation for polymer nanocom- posites”. APL Materials, 6(11), p. 111108. [156] Chang, A. X., Funkhouser, T., Guibas, L., Hanra- han, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al., 2015. “Shapenet: An information-rich 3d model repository”. arXiv preprint arXiv:1512.03012. [157] Mo, K., Zhu, S., Chang, A. X., Yi, L., Tripathi, S., Guibas, L. J., and Su, H., 2019. “Partnet: A large-scale benchmark for ﬁne-grained and hierarchi- cal part-level 3d object understanding”. In Proceed- ings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pp. 909–918. [158] Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J., 2015. “3d shapenets: A deep rep- resentation for volumetric shapes”. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912–1920. [159] Kim, S., Chi, H.-g., Hu, X., Huang, Q., and Ra- mani, K., 2020. “A large-scale annotated mechani- cal components benchmark for classiﬁcation and re- trieval tasks with deep neural networks”. In Proceed- ings of 16th European Conference on Computer Vi- sion (ECCV). [160] Willis, K. D., Pu, Y., Luo, J., Chu, H., Du, T., Lam- bourne, J. G., Solar-Lezama, A., and Matusik, W., 2021. “Fusion 360 gallery: A dataset and environ- ment for programmatic cad construction from human design sequences”. ACM Transactions on Graphics (TOG), 40(4), pp. 1–24. [161] Nobari, A. H., Rashad, M. F., and Ahmed, F., 2021. “Creativegan: Editing generative adversarial networks for creative design synthesis”. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC- 21, ASME. [162] Regenwetter, L., Weaver, C., and Ahmed, F., 2022. Framed: Data-driven structural performance analysis of community-designed bicycle frames. [163] Chan, Y.-C., Ahmed, F., Wang, L., and Chen, W., 2021. “Metaset: Exploring shape and property spaces for data-driven metamaterials design”. Journal of Me- chanical Design, 143(3), p. 031707. [164] Wang, L., van Beek, A., Da, D., Chan, Y.-C., Zhu, P., and Chen, W., 2021. “Data-driven multiscale design of cellular composites with multiclass microstructures for natural frequency maximization”. arXiv preprint arXiv:2106.06478. [165] Jongejan, J., Rowley, H., Kawashima, T., Kim, J., and Fox-Gieg, N., 2016. “The quick, draw!-ai experi- ment”. Mount View, CA, accessed Feb, 17(2018), p. 4. [166] Lopez, C., Miller, S. R., and Tucker, C. S., 2018. “Human validation of computer vs human generated design sketches”. In International De- sign Engineering Technical Conferences and Com- puters and Information in Engineering Conference, Vol. 51845, American Society of Mechanical Engi- neers, p. V007T06A015. [167] Toh, C. A., and Miller, S. R., 2013. “Exploring the utility of product dissection for early-phase idea gen- eration”. In International Design Engineering Tech- nical Conferences and Computers and Information in Engineering Conference, Vol. 55928, American Soci- ety of Mechanical Engineers, p. V005T06A034. [168] Liang, L., Liu, M., Martin, C., and Sun, W., 2018. “A deep learning approach to estimate stress distribution: a fast and accurate surrogate of ﬁnite-element analy- sis”. Journal of The Royal Society Interface, 15(138), p. 20170844. [169] Jiang, H., Nie, Z., Yeo, R., Farimani, A. B., and Kara, L. B., 2020. “StressGAN: A Generative Deep Learning Model for 2D Stress Distribution Predic- tion”. Vol. Volume 11B: 46th Design Automation Conference (DAC) of International Design Engineer- ing Technical Conferences and Computers and Infor- mation in Engineering Conference. V11BT11A023. [170] Nie, Z., Jiang, H., and Kara, L. B., 2019. “Stress Field Prediction in Cantilevered Structures Using Convolu- tional Neural Networks”. Vol. Volume 1: 39th Com- puters and Information in Engineering Conference of International Design Engineering Technical Confer- ences and Computers and Information in Engineering Conference. V001T02A011. [171] Nie, Z., Jiang, H., and Kara, L. B., 2019. “Stress ﬁeld prediction in cantilevered structures using con- volutional neural networks”. Journal of Computing and Information Science in Engineering, 20(1), Sep. [172] Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and Battaglia, P., 2020. “Learning mesh-based simulation with graph networks”. In International Conference on Learning Representations. [173] Kochkov, D., Smith, J. A., Alieva, A., Wang, Q., 22 Copyright © by ASME Brenner, M. P., and Hoyer, S., 2021. “Machine learning–accelerated computational ﬂuid dynamics”. Proceedings of the National Academy of Sciences, 118(21). [174] Duraisamy, K., Iaccarino, G., and Xiao, H., 2019. “Turbulence modeling in the age of data”. Annual Re- view of Fluid Mechanics, 51(1), pp. 357–377. [175] Kim, B., Azevedo, V. C., Thuerey, N., Kim, T., Gross, M., and Solenthaler, B., 2019. “Deep ﬂuids: A gen- erative network for parameterized ﬂuid simulations”. Computer Graphics Forum, 38(2), May, p. 59–70. [176] Dering, M. L., and Tucker, C. S., 2017. “Generative adversarial networks for increasing the veracity of big data”. In 2017 IEEE International Conference on Big Data (Big Data), IEEE, pp. 2595–2602. [177] Panchal, J. H., Fuge, M., Liu, Y., Missoum, S., and Tucker, C., 2019. “Special Issue: Machine Learning for Engineering Design”. Journal of Mechanical De- sign, 141(11), 10. 110301. [178] Elgammal, A., Liu, B., Elhoseiny, M., and Mazzone, M., 2017. “Can: Creative adversarial networks gen- erating “art” by learning about styles and deviating from style norms”. In 8th International Conference on Computational Creativity, ICCC 2017, Georgia In- stitute of Technology. [179] Franceschelli, G., and Musolesi, M., 2021. Creativity and machine learning: A survey. [180] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I., 2020. “Generative pre- training from pixels”. In Proceedings of the 37th In- ternational Conference on Machine Learning, H. D. III and A. Singh, eds., Vol. 119 of Proceedings of Ma- chine Learning Research, PMLR, pp. 1691–1703. [181] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I., 2021. Zero- shot text-to-image generation. [182] Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I., 2020. Jukebox: A generative model for music. [183] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I., 2021. Learning transferable visual models from natural lan- guage supervision. 23 Copyright © by ASME","libVersion":"0.3.2","langs":""}
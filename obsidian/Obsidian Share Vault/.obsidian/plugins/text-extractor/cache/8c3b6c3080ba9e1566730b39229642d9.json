{"path":"lit/lit_notes_OLD_PARTIAL/Hur20optFlowDpAge.pdf","text":"Optical Flow Estimation in the Deep Learning Age Junhwa Hur and Stefan Roth Department of Computer Science, TU Darmstadt, Germany {junhwa.hur, stefan.roth}@visinf.tu-darmstadt.de Abstract. Akin to many subareas of computer vision, the recent ad- vances in deep learning have also significantly influenced the literature on optical flow. Previously, the literature had been dominated by clas- sical energy-based models, which formulate optical flow estimation as an energy minimization problem. However, as the practical benefits of Convolutional Neural Networks (CNNs) over conventional methods have become apparent in numerous areas of computer vision and beyond, they have also seen increased adoption in the context of motion estimation to the point where the current state of the art in terms of accuracy is set by CNN approaches. We first review this transition as well as the devel- opments from early work to the current state of CNNs for optical flow estimation. Alongside, we discuss some of their technical details and compare them to recapitulate which technical contribution led to the most significant accuracy improvements. Then we provide an overview of the various optical flow approaches introduced in the deep learning age, including those based on alternative learning paradigms (e.g., un- supervised and semi-supervised methods) as well as the extension to the multi-frame case, which is able to yield further accuracy improvements. 1 Emergence and Advances of Deep Learning-based Optical Flow Estimation The recent advances in deep learning have significantly influenced the literature on optical flow estimation and fueled a transition from classical energy-based formulations, which were mostly hand defined, to end-to-end trained models. We first review how this transition proceeded by recapitulating early work that started to utilize deep learning, typically as one of several components. Then, we summarize several canonical end-to-end approaches that have successfully adopted CNNs for optical flow estimation and have highly influenced the main- stream of research, including other subareas of vision in which optical flow serves as an input. 1.1 From classical energy-based approaches to CNNs For more than three decades, research on optical flow estimation has been heav- ily influenced by the variational approach of Horn and Schunck [20]. Their ba- sic energy minimization formulation consists of a data term, which encourages To appear as a book chapter in Modelling Human Motion, N. Noceti, A. Sciutti and F. Rea, Eds., Springer, 2020. The final publication will be available through Springer.arXiv:2004.02853v1 [cs.CV] 6 Apr 2020 2 Junhwa Hur and Stefan Roth brightness constancy between temporally corresponding pixels, and a spatial smoothness term, which regularizes neighboring pixels to have similar motion in order to overcome the aperture problem. The spatially continuous optical flow field u = (ux, uy) is obtained by minimizing E(u) = ∫ ((Ixux + Iyuy + It) 2 + α2(∥∇ux∥2 + ∥∇uy∥ 2)) dx dy, (1) where Ix, Iy, It are the partial derivatives of the image intensity I with respect to x, y, and t (Fig. 1a). To minimize Eq. (1) in practice, spatial discretization is necessary. In such a spatially discrete form, the Horn and Schunck model [20] can also be re-written in the framework of standard pairwise Markov random fields (MRFs) [7, 31] through a combination of a unary data term D(·) and a pairwise smoothness term S(·, ·), E(u) = ∑ p∈I D(up) + ∑ p,q∈N S(up, uq), (2) where I is the set of image pixels and the set N denotes spatially neighboring pixels. Starting from this basic formulation, much research has focused on de- signing better energy models that more accurately describe the flow estimation problem (see [11, 49] for reviews of such methods). Concurrently with pursing better energy models, the establishment of public benchmark datasets for optical flow, such as the Middlebury [4], MPI Sintel [8], and KITTI Optical Flow benchmarks [14, 37], has kept revealing the challenges and limitations of existing methods. These include large displacements, severe illumination changes, and occlusions. Besides allowing for the fair comparison between existing methods on the same provided data, these public benchmarks have moreover stimulated research on more faithful energy models that address some of the specific challenges mentioned above. Meanwhile, the relatively recent success of applying Convolutional Neural Networks (CNNs) with backpropagation on a large-scale image classification task [29] paved the way for applying CNNs to various other computer vision problems, including optical flow as well. Early work that applied CNNs to opti- cal flow used them as an advanced feature extractor [2, 3, 12, 16], as sketched in Fig. 1b. The main idea behind this is to substitute the data term (e.g., in Eqs. (1) and (2)) in classical energy-based formulations with a CNN-based fea- ture matching term. Instead of using image intensities, image gradients, or other hand-crafted features as before, CNNs enable learning feature extractors such that each pixel can be represented with a high-dimensional feature vector that combines a suitable amount of distinctiveness and invariance, for example to appearance changes. The putative similarity between regions is given by the fea- ture distance. The remaining pipeline, including using the smoothness term as well as the optimization strategies, remain the same. As we will review in more detail below, several methods [2, 3, 12, 16] demonstrated an accuracy benefit of such CNN-based feature extractors. At the same time, another line of research investigated regression-based CNN architectures that can directly estimate optical flow from a pair of input images Optical Flow Estimation in the Deep Learning Age 3 (a) Classical energy-based approach (b) Using CNNs as a feature extractor (c) CNN regression architecture Fig. 1: Transition from (a) classical energy-based approaches to (b) CNN-based ap- proaches that use CNNs as a feature extractor or to (c) end-to-end trainable CNN regression architectures. and can be trained end-to-end, as sketched in Fig. 1c. Unlike methods that com- bine CNN feature extractors with classical regularizers and energy minimization, such regression frameworks employ CNNs for the entire pipeline by virtue of their ability to act as a function approximator, which effectively learns the relationship between the input images and the desired flow output given the labeled training dataset. FlowNet [10] is the first work that demonstrated an end-to-end CNN regression approach for estimating optical flow based on an encoder-decoder ar- chitecture. Owing to the difficulty of obtaining dense ground truth optical flow in real-world images, Dosovitskiy et al. [10] generated a synthetic dataset from CAD models of chairs, which move in front of a static background. Pairs of images with ground truth optical flow serve to train the network. FlowNet [10] demonstrated that a CNN-based regression architecture is able to predict optical flow directly, yet the accuracy remained behind that of state-of-the-art energy- based methods at the time [44, 53]. Unlike in other areas of computer vision, this left it initially unclear whether end-to-end CNN architectures can compete with classical energy-based methods in terms of accuracy. However, later research cleared up this question by developing better end-to- end architectures that eventually outperformed classical energy-based methods, reaching new accuracy levels on public benchmarks [8, 14, 37]. These advances mainly stem from discovering new architecture designs, for example, by stacking multiple networks to refine previous estimates [25] or constructing a CNN pyra- 4 Junhwa Hur and Stefan Roth mid to estimate flow in a coarse-to-fine fashion [23, 41, 48], as had been done in classical methods before. Unlike energy-based models, CNN regressors run in real time on GPUs combined with much better accuracy. In other words, end-to- end CNN regressors have established themselves by now as dominant paradigm in the current literature on optical flow estimation. Yet, they have not remained without limitations, hence much research continues to be carried out. For ex- ample, recent work aims to overcome the reliance on large amounts of labeled data as well as accuracy drops on unseen domains and datasets, for example by pursuing unsupervised or semi-supervised learning paradigms. In the following, we will give a detailed overview of the two major CNN paradigms in optical flow estimation and survey other recent trends. 1.2 CNNs as feature extractor Not restricted to the problem domain of optical flow estimation but rather cor- respondence estimation more generally, several early works [18, 46, 51, 61] em- ployed CNNs for matching descriptors or patches. In most cases, the underlying network uses a so-called Siamese architecture that extracts a learned feature descriptor separately for each of two input image patches, followed by a shallow joint network that computes a matching score between the two feature repre- sentations. The name Siamese alludes to the fact that the two feature extractor sub-networks are identical including their weights. Inspired by these successes, significant amounts of earlier work that adopted deep learning for optical flow es- timation focused on utilizing CNNs as a feature extractor on top of conventional energy-based formulations such as MRFs. Their main idea is to utilize CNNs as a powerful tool for extracting discriminative features and then use well-proven conventional energy-based frameworks for regularization. Gadot and Wolf [12] proposed a method called PatchBatch, which was among the first flow approaches to adopt CNNs for feature extraction. Patch- Batch [12] is based on a Siamese CNN feature extractor that is fed 51 × 51 input patches and outputs a 512-dimensional feature vector using a shallow 5-layer CNN. Then, PatchBatch [12] adopts Generalized PatchMatch [5] as an Approxi- mate Nearest Neighbor (ANN) algorithm for correspondence search, i.e., match- ing the extracted features between two images. The method constructs its train- ing set by collecting positive corresponding patch examples given ground-truth flow and negative non-matching examples by randomly shifting the image patch in the vicinity of where the ground-truth flow directs. The intuition of collecting negative examples in such a way is to train CNNs to be able to separate non- trivial cases and extract more discriminative features. The shallow CNNs are trained using a variant of the DrLIM [17] loss, which minimizes the squared L2 distance between positive patch pairs and maximizes the squared L2 distance between negative pairs above a certain margin. In a similar line of work, Bailer et al. [3] proposed to use the thresholded hinge embedding loss for training the feature extractor network. The hinge embedding loss based on the L2 loss function has been commonly used to minimize the Optical Flow Estimation in the Deep Learning Age 5 feature distance between two matching patches and to maximize the feature distance above m between non-matching patches: lhinge(P1, P2) = { L2(P1, P2), (P1, P2) ∈ M + max(0, m − L2(P1, P2) ), (P1, P2) ∈ M − (3) L2(P1, P2) = ∥ ∥F (P1) − F (P2)∥ ∥ 2, (4) where F (P1) and F (P2) are the extracted descriptors from CNNs applied to P1 in the first image and P2 in the second image, respectively, L2(P1, P2) calculates the L2 loss between the two descriptors, and M + and M − are collected sets of positive and negative samples, respectively. However, minimizing the L2 loss of some challenging positive examples (e.g., with appearance difference or illumination changes) can move the decision bound- ary into an undesired direction and lead to misclassification near the decision boundary. Thus, Bailer et al. [3] proposed to put a threshold t on the hinge embedding loss in order to prevent the network from minimizing the L2 distance too aggressively: lt-hinge(P1, P2) = { max(0, L2(P1, P2) − t ), (P1, P2) ∈ M + max(0, m − (L2(P1, P2) − t) ), (P1, P2) ∈ M −. (5) Compared to standard losses, such as the hinge embedding loss in Eq. (3) or the DrLIM loss [17], this has led to more accurate flow estimates. Meanwhile, G¨uney and Geiger [16] demonstrated successfully combining a CNN feature matching module with a discrete MAP estimation approach based on a pairwise Markov random field (MRF) model. The proposed CNN module outputs per-pixel descriptors, from which a cost volume is constructed by cal- culating feature distances between sample matches. This is input to a discrete MAP estimation approach [38] to infer the optical flow. To keep training efficient, G¨uney and Geiger [16] followed a piece-wise setting that first trains the CNN module alone and only then trains the joint CNN-MRF module together. Bai et al. [2] followed a similar setup overall, but utilized semi-global block matching (SGM) [19] to regress the output optical flow from the cost volume, which is constructed by calculating a distance between features from CNNs. Taken together, these approaches have successfully demonstrated that the benefits of the representational power of CNNs can be combined with well-proven classical energy-based models. Specifically, they demonstrated more accurate es- timates on inliers and more precise estimates on object boundaries than previous baselines with hand-constructed features. 1.3 End-to-end regression architectures for optical flow estimation Concurrently with the development of feature extraction-based networks, active research also started on developing end-to-end CNN architectures for optical flow estimation based on regression. Unlike methods that use CNNs only for feature extraction as addressed above, such regression methods exploit CNNs for the 6 Junhwa Hur and Stefan Roth entire pipeline and directly output optical flow from a pair of input images. By substituting classical regularizers and avoiding energy minimization, these CNN- based methods combine the advantages of end-to-end trainability and runtime efficiency. Dosovitskiy et al. proposed the first end-to-end CNN architecture for esti- mating optical flow, called FlowNet [10], which has two main architectural lines, FlowNetS and FlowNetC. The two models are fundamentally based on an hourglass-shaped neural network architecture that consists of an encoder and a decoder, and differs only in the encoder part. In FlowNetS, a pair of input images is simply concatenated and then input to the hourglass-shaped network that directly outputs optical flow. On the other hand, FlowNetC has a shared encoder for both images, which extracts a feature map for each input image, and a cost volume is constructed by measuring patch-level similarity between the two feature maps with a correlation operation. The result is fed into the subsequent network layers. To train the networks in a supervised way, a training dataset with a large number of image pairs and their ground truth flow are required, but at the time only datasets with few hundreds of images or even fewer were available [4, 14, 37]; the challenge of obtaining dense optical flow ground truth in real-world images remains until today. In order to overcome the shortage of suitable training data, Dosovitskiy et al. [10] established a synthetic dataset, called FlyingChairs, by layering natural images with rendered CAD models of chairs; their parameterized affine motion is designed to follow the motion statistics of existing real-world datasets. However, due to the intrinsic differences between synthetic and real- world images, unfortunately FlowNet trained on the synthetic dataset alone did not generalize well to real images. In fact, even after fine-tuning on real-world images, the accuracy initially remained behind that of classical energy-based models at the time. This left the question whether such a generic CNN regression architecture can actually outperform classical energy-based methods, or why it did not (yet). Importantly, however, FlowNet [10] demonstrated the possibility of employing an end-to-end regression architecture for optical flow estimation. Moreover, FlowNet established several standard practices for training optical flow networks such as learning rate schedules, basic network architectures, data augmentation schemes, and the necessity of pre-training on synthetic datasets, which have substantially impacted follow-up research. Ranjan et al. proposed SPyNet [41], which incorporates the classical “coarse- to-fine” concept (please refer to Fig. 2a for an illustration) into a CNN model and updates the residual flow over multiple pyramid levels. SPyNet consists of 5 pyramid levels, and each pyramid level consists of a shallow CNN that estimates flow between a source image and a target image, which is warped by the cur- rent flow estimate (see Fig. 2b). This estimate is updated so that the network can residually refine optical flow through a spatial pyramid and possibly handle large displacements. Compared to FlowNet, SPyNet significantly reduces the number of model parameters by 96% by using a pyramid-shaped architecture, while achieving comparable and sometimes even better results than FlowNet. Optical Flow Estimation in the Deep Learning Age 7 II1 I2 Coarse Fine (a) Coarse-to-fine estimation Source image Target image Warped image 𝑝𝑝1 𝑝𝑝2 (b) Backward warping Fig. 2: (a) The classical coarse-to-fine concept proceeds by estimating optical flow using a multi-scale image pyramid, starting from the coarsest level to the finest level. By gradually estimating and refining optical flow through the pyramid levels, this approach can handle large displacements better and improve accuracy. (b) Backward warping is commonly used in optical flow estimation. For each pixel p1 in the source image, the warped image obtains the intensity from (sub)pixel location p2, which is obtained from the estimated flow. Bilinear interpolation is often used to obtain the pixel intensity at the non-integer coordinate. Although SPyNet [41] is still outperformed by classical energy-based methods, it demonstrates a promising way of designing flow architectures by integrating classical principles into deep learning. Meanwhile, Ilg et al. [25] proposed FlowNet2, which significantly improves the flow accuracy over their previous FlowNet architecture and started to out- perform classical energy-based approaches. The main limitations of FlowNet are blurry outputs from the CNN decoder and lower accuracy compared to classi- cal approaches. To overcome these limitations, Ilg et al. proposed the key idea that by stacking multiple FlowNet-style networks, one can sequentially refine the output from the previous network modules. Despite of the conceptual simplic- ity, stacking multiple networks is very powerful and significantly improves the flow accuracy by more than 50% over FlowNet. Additionally, Ilg et al. revealed several important practices for training their networks, including the necessity of pre-training and fine-tuning on synthetic datasets, the effectiveness of using a correlation layer, and the guidance of proper learning rate schedules, followed by in-depth empirical analyses. In practice, Ilg et al. [25] suggest to pre-train their networks on a less challenging synthetic dataset first (i.e., the FlyingChairs dataset [10]) and then further train on a more challenging synthetic dataset with 3D motion and photometric effects (i.e., the FlyingThings3D dataset [35]). Their empirical study revealed a more than 20% accuracy difference depending on the usage of the proper pre-training dataset (see Table 1 in [25]). The underlying conjecture is that making the network first learn the general concept of motion estimation with a simpler dataset is more important than learning to handle various challenging examples from the start. Also, the proposed learning rate schedules for pre-training and fine-tuning have become a standard and guidance for follow-up research. 8 Junhwa Hur and Stefan Roth After the successful demonstration of FlowNet2 [25] that end-to-end regres- sion architectures can outperform energy-based approaches, further investiga- tions on finding better network architectures have continued. Sun et al. proposed an advanced architecture called PWC-Net [48] by exploiting well-known design principles from classical approaches. PWC-Net relies on three main design prin- ciples: (i) pyramid, (ii) warping, and (iii) cost volume. Similar to SPyNet [41], PWC-Net estimates optical flow in a coarse-to-fine way with several pyramid lev- els, but PWC-Net constructs a feature pyramid by using CNNs, while SPyNet constructs an image pyramid by simply downsampling images. Next, PWC-Net constructs a cost volume with a feature map from the source image and the warped feature map from the target image based on the current flow. Then, the subsequent CNN modules act as a decoder that outputs optical flow from the cost volume. In terms of both accuracy and practicality, PWC-Net [48] set a new state of the art with its light-weight architecture allowing for shorter training times, faster inference, and more importantly, clearly improved accuracy. Com- paring to FlowNet2 [25], PWC-Net is 17 times smaller in model size and twice as fast during inference while being more accurate. Similar to SPyNet, the com- putational efficiency stems from using coarse-to-fine estimation, but PWC-Net crucially demonstrates that constructing and warping feature maps instead of using downsampled warped images yields much better accuracy. As a concurrent work and similar to PWC-Net [48], LiteFlowNet [23] also demonstrated utilizing a multi-level pyramid architecture that estimates flow in a coarse-to-fine manner, proposing another light-weight regression architecture for optical flow. The major technical differences to PWC-Net are that Lite- FlowNet residually updates optical flow estimates over the pyramid levels and proposes a flow regularization module. The proposed flow regularization module creates per-pixel local filters using CNNs and applies the filters to each pixel so that customized filters refine flow fields by considering neighboring estimates. The regularization module is given the optical flow, feature maps, and occlusion probability maps as inputs to take motion boundary information and occluded areas into account in creating per-pixel local filters. The experimental results demonstrate clear benefits, especially from using the regularization module that smoothes the flow fields while effectively sharpening motion boundaries, which reduces the error by more than 13% on the training domain. Afterwards, Hur and Roth [24] proposed an iterative estimation scheme with weight sharing entitled iterative residual refinement (IRR), which can be applied to several backbone architectures and improves the accuracy further. Its main idea is to take the output from a previous pass through the network as input and iteratively refine it by only using a single network block with shared weights; this allows the network to residually refine the previous estimate. The IRR scheme can be used on top of various flow architectures, for example FlowNet [10] and PWC-Net [48]. For FlowNet [10], the whole hourglass shape network is iteratively re-used to keep refining its previous estimate and, in contrast to FlowNet2 [25], increases the accuracy without adding any parameters. For PWC-Net [48], a repetitive but separate flow decoder module at each pyramid level is replaced Optical Flow Estimation in the Deep Learning Age 9 Table 1: Overview of the main technical design principles of end-to-end optical flow architectures. MethodsFlowNetS[10]FlowNetC[10]SPyNet[41]FlowNet2[25]PWC-Net[48]LiteFlowNet[23]HD3[58]VCN[56] Pyramid – 3-level feature 5-level image 3-level feature 6-level feature 6-level feature 5-level feature 6-level feature Warping – – Image Image Feature Feature Feature Feature Cost volume – 3D – 3D 3D 3D 3D 4D Network stacking – – – 5 – – – – Flow inference Direct Direct Residual Direct Direct Residual Residual Hypothesis selection Parameters (M) 38.67 39.17 1.20 162.49 8.75 5.37 39.6 6.20 with only one common decoder for all levels, and then iteratively refines the estimation through the pyramid levels. Applying the scheme on top of PWC- Net [48] is more interesting as it makes an already lean model even more com- pact by removing repetitive modules that perform the same functionality. Yet, the accuracy is improved, especially on unseen datasets (i.e. allowing better gen- eralization). Furthermore, Hur and Roth [24] also demonstrated an extension to joint occlusion and bi-directional flow estimation that leads to further flow ac- curacy improvements of up to 17.7% while reducing the number of parameters by 26.4% in case of PWC-Net; this model is termed IRR-PWC [24]. Yin et al. [58] proposed a general probabilistic framework termed HD 3 for dense pixel correspondence estimation, exploiting the concept of the so-called match density, which enables the joint estimation of optical flow and its un- certainty. Mainly following the architectural design of PWC-Net (i.e., using a multi-scale pyramid, warping, and a cost volume), the method estimates the full match density in a hierarchical and computationally efficient manner. The estimated spatially discretized match density can then be converted into optical flow vectors while providing an uncertainty assessment at the same time. This output representation of estimating the match density is rather different from all previous works above, which directly regress optical flow with CNNs. On established benchmarks datasets, their experimental results demonstrate clear advantages, achieving state-of-the-art accuracy regarding both optical flow and uncertainly measures. While the cost volume has been commonly used in backbone architectures [10, 23, 25, 48, 58], its representation is mainly based on a heuristic design. Instead of representing the matching costs between all pixels (x, y) with their possible 2D displacements (u, v) into a 4D tensor (x, y, u, v), the conventional design is based on a 3D cost volume – a 2D array (x, y) augmented with a uv channel, which is computationally efficient but often yields limited accuracy and 10 Junhwa Hur and Stefan Roth Table 2: Quantitative comparison on public benchmarks: MPI Sintel [8] and KITTI [14, 37]. Methods MPI Sintel a KITTI b Clean Final 2012 2015 FlowNetS [10] 6.158 7.218 37.05 % – FlowNetC [10] 6.081 7.883 – – SPyNet [41] 6.640 8.360 12.31 % 35.07 % FlowNet2 [25] 3.959 6.016 4.82 % 10.41 % PWC-Net [48] 4.386 5.042 4.22 % 9.60 % LiteFlowNet [23] 3.449 5.381 3.27 % 9.38 % IRR-PWC [24] 3.844 4.579 3.21 % 7.65 % HD 3 [58] 4.788 4.666 2.26 % 6.55 % VCN [56] 2.808 4.404 – 6.30 % a Evaluation metric: end point error (EPE). b Evaluation metric: outlier rate (i.e. less than 3 pixel or 5% error is considered an inlier) overfitting. To overcome this limitation, Yang et al. [56] proposed Volumetric Correspondence Networks (VCN), which are based on true 4D volumetric pro- cessing: constructing a proper 4D cost volume and processing with 4D convolu- tion kernels. For reducing the computational cost and memory of 4D processing, Yang et al. [56] used separable 4D convolutions, which approximate the 4D con- volution operation with two 2D convolutions, reducing the complexity by N 2 (please refer the original paper for technical details). Through proper 4D volu- metric processing with computationally cheaper operations, the method further pushes both accuracy and practicality on widely used public benchmarks, im- proving generalization and demonstrating faster training convergence – requiring 7 times fewer training iterations than its competitors. Table 1 summarizes the main differences in technical design of the various end-to-end optical flow architectures discussed above. Starting from FlowNetS [10], the methods are listed in chronological order. We omit the IRR scheme as it can be applied on top of several backbone architectures. Table 2 compares the quantitative results of each method on the MPI Sintel [8] and KITTI bench- marks [14, 37]. Each method is pre-trained on synthetic datasets first and then fine-tuned on each benchmark. Looking at the two tables, we can gain some first insights into which design choices lead to the observed accuracy improvements. First, having a pyramid structure by adopting a “coarse-to-fine” strategy makes networks more compact and improves the flow estimation accuracy (e.g., from FlowNet [10] to SPyNet [41], PWC-Net [48], and LiteFlowNet [23]). Second, stacking networks can also improve the flow accuracy while linearly increasing the number of parameters (e.g., from FlowNet [10] to FlowNet2 [25]). Third, constructing a cost volume by calculating a patch-wise correlation between two feature maps has become a standard approach and is more beneficial than not using it (e.g., FlowNetS vs. FlowNetC, according to a study from [25]). Fourth, even if based on similar conceptual designs, subtle design differences or addi- tional modules can further lead to accuracy improvements (e.g., LiteFlowNet [23] vs. PWC-Net [48]). Fifth, the iterative residual refinement scheme IRR [24] Optical Flow Estimation in the Deep Learning Age 11 Ground truth FlowNetS [10] FlowNetC [10] SPyNet [41] EPE: 7.873 EPE: 8.152 EPE: 8.881 Overlayed image FlowNet2 [25] PWC-Net [48] LiteFlowNet [23] EPE: 5.448 EPE: 5.150 EPE: 5.073 IRR-PWC [24] HD 3 [58] VCN [56] EPE: 4.717 EPE: 4.247 EPE: 4.395 Fig. 3: Qualitative comparison of end-to-end architectures: Example from Sintel Final Test [8]. The first column shows the ground-truth flow and the overlayed input images. In the further columns, we show the color-coded flow visualization of each method, overlayed with the end point error (EPE) and their error maps (the brighter a pixel, the higher its error). can further boost the accuracy of existing backbone architectures (e.g., from PWC-Net [48] to IRR-PWC [24]). Lastly, investigating better fundamental de- signs such as the output representation (e.g., the match density [58]) or the cost volume representation (e.g., 4D cost volume [56]) can lead to further improve- ment, sometimes quite significantly so. Fig. 3 shows a qualitative comparison of each method on an example from the Sintel Final Test set [8]. The optical flow visualizations and the error maps demonstrate how significantly end-to-end methods have been improved over the past few years, especially near motion boundaries and in non-textured areas. 2 Approaches with Alternative Learning Paradigms Aside from the question of how to design deep network architectures for optical flow estimation, another problem dimension has grown into prominence recently – how to train such CNNs for optical flow especially in the context of the lim- ited quantities of ground-truth data available in practice. Most (early) CNN approaches are based on standard supervised learning and directly train the 12 Junhwa Hur and Stefan Roth network on labeled data. However, real-world labeled data is available only in comparatively small quantities and often constrained to certain settings, which turns out to have the limitation that the accuracy can drop significantly on unseen data. To overcome this, a number of alternative approaches based on unsupervised or semi-supervised learning have been proposed to lighten the ne- cessity of and reliance on large amounts of labeled data. In this section, we review and categorize CNN approaches in terms of their underlying learning paradigm: supervised learning, unsupervised or self-supervised learning, and finally semi- supervised learning. 2.1 Challenges of supervised learning Based on the end-to-end trainability of CNNs, the most straightforward way to train CNNs for optical flow estimation is in a supervised fashion using a labeled dataset. In the supervised learning setting – but not only there – the dataset plays an important role, and details such as the size and design of the dataset, the type of loss function, and training schedules become critical factors in achieving high accuracy. Approaches that are based on CNNs as feature extractor [2, 3, 12, 16], as already discussed above, collect positive matching samples and negative non- matching samples as a training set and train the CNNs by applying a loss func- tion at the final output of the network. Different types of loss functions has been investigated to obtain discriminative features that are invariant to common ap- pearance and illumination changes (please refer to Sec. 1.2 for further details). When training CNNs in general, having a large labeled dataset is crucial to avoid overfitting on the training dataset and enable the network to generalize to unseen data. As the networks tend to be comparatively lean and do not have to (and in fact cannot) learn something about plausible motions, but rather only classify when patches match in terms of their appearance, the issue of overfitting is less prominent than in end-to-end regression approaches. For training end-to-end optical flow architectures in a supervised fashion, on the other hand, we need to have a training dataset with many temporally consecutive image pairs with dense ground-truth flow, representing the range of possible optical flow fields. The entire flow map with per-pixel labels is used to train the network by minimizing the per-pixel Euclidean distance between the ground truth flow and the output from the network. However, collecting such a dataset with real-world images has been challenging due to the difficulty of mea- suring the true motion for all pixels [4]. Establishing synthetic datasets instead is a viable alternative (e.g., the FlyingChairs [10], Sintel [8], and FlyingThings3D [35] datasets), as it is much easier to generate a large amount of synthesized images with accurate ground-truth flow. Yet, using a synthetic dataset for training flow networks still does not com- pletely solve the issue of dataset suitability. The generalization to an unseen setting remains a challenge. According to the empirical studies of [25] and [48], the flow accuracy significantly depends on the dataset used for training and on Optical Flow Estimation in the Deep Learning Age 13 how close the test-time domain is to the training domain. Consequently, overfit- ting on the training dataset domain is a problem. As a solution, FlowNet2 [25] is accompanied with a training dataset schedule that leads to a better local param- eter optimum so that the trained networks can perform reasonably on unseen data: pre-training on synthetic datasets before fine-tuning on the target domain dataset in the end (please refer to Sec. 1.3 for further details). Both FlowNet2 [25] and PWC-Net [48] empirically demonstrated that training networks with this schedule allows for better generalization to an unseen target domain. In fact, pre-training on a synthetic dataset followed by fine-tuning on the target domain yields much better accuracy than directly training on the target domain, even on the target domain itself. All regression architectures mentioned above have multi-scale intermediate optical flow outputs along the decoder (e.g., FlowNet [10] and FlowNet2 [25]) or at each pyramid level (e.g., PWC-Net [48], SPyNet [41], and LiteFlowNet [23]). For all intermediate outputs, an L2 loss between the output and the downscaled ground truth is applied per pixel so that the network learns to estimate optical flow in a coarse-to-fine manner and achieves better accuracy at the final output resolution. The final training loss becomes the weighted sum of all intermediate losses. 2.2 Unsupervised or self-supervised learning While synthetic datasets enable training CNNs with a large amount of labeled data, the networks only trained on synthetic datasets perform relatively poorly on real-world datasets due to the domain mismatch between the training domain and the target domain. As just discussed, supervised approaches thus require fine-tuning on the target domain for better accuracy. However, this can be prob- lematic if there is no ground truth optical flow available for the target domain. To resolve this issue, unsupervised learning approaches have been proposed to directly train CNNs on the target domain without having access to any ground truth flow. Such methods are also called self-supervised, as the supervisory sig- nal comes from the input images themselves. In this section, we will overview existing unsupervised or self-supervised learning methods and discuss how they have progressed to achieve results that are competitive with many supervised methods. Ahmadi and Patras [1] pioneered unsupervised learning-based optical flow using CNNs. Inspired by the classical Horn and Schunck [20] method, Ahmadi and Patras used the classical optical flow constraint equation as a loss function for training the network. By minimizing this unsupervised loss function, the network learns to predict optical flow fields that satisfy the optical flow constraint equation on the input images, i.e., the brightness constancy assumption. [1] further combines this with classical coarse-to-fine estimation so that the flow field improves through multi-scale estimation. By demonstrating that the flow accuracy is close to the best supervised method at the time, i.e. FlowNet [10], Ahmadi and Patras [1] suggest that unsupervised learning of networks for optical 14 Junhwa Hur and Stefan Roth flow estimation is possible and can overcome some of the limitations of supervised learning approaches. Concurrently, Yu et al. [59] and Ren et al. [43] proposed to use a proxy unsupervised loss that is inspired by a standard MRF formulation. Following classical concepts, the proposed unsupervised proxy loss consists of a data term and a smoothness term as in Eq. (2). The data term directly minimizes the intensity difference between the first image and the warped second image from estimated optical flow, and the smoothness term penalizes flow differences be- tween neighboring pixels. Both methods demonstrate that directly training on a target domain (e.g., the KITTI datasets [14]) in an unsupervised manner per- forms competitive to or sometimes even outperforms the same network that is trained on a different domain (e.g., the FlyingChairs dataset [10]) in a supervised manner. This observation suggests that unsupervised learning approaches can be a viable alternative to supervised learning, if labeled data for training is not available in the target domain. In a follow-up work, Zhu et al. [63] showed that the backbone network can be improved by using a dense connectivity. They built on DenseNet [22], which uses dense connections with skip connections between all convolutional layers to improve the accuracy over the previous state of the art for image classification. Inspired by DenseNet, Zhu et al. [63] adopted the such dense connections in an hourglass-shaped architecture by using dense blocks before every downsampling and upsampling step; each dense block has four convolutional layers with dense skip connections between each other. [63] improves the flow accuracy by more than 10% on public benchmark datasets over [59] on average, which uses FlowNet [10] as a backbone network, indicating the importance of choosing the right backbone network in the unsupervised learning setting as well. Zhu et al. [62] also proposed a different direction of unsupervised learning, combining an unsupervised proxy loss and a guided supervision loss using proxy ground truth obtained from an off-the-shelf classical energy-based method. As in [43, 59], the unsupervised proxy loss makes the network learn to estimate optical flow to satisfy the brightness constancy assumption while the guided loss helps the network perform close to off-the-shelf classical energy-based method. In the circumstance that learning with the unsupervised proxy loss is outperformed by the classical energy-based method, the guided loss can help and even achieve better accuracy than either of the two losses alone. Unsupervised or self-supervised learning of optical flow relies on minimizing a proxy loss rather than estimating optical flow close to some ground truth. Thus, designing a faithful proxy loss is critical to its success. Meister et al. [36] pro- posed a proxy loss function that additionally considers occlusions, demonstrates better accuracy than previous unsupervised methods, and outperforms the su- pervised backbone network (i.e., FlowNet [10]). Further, bi-directional flow is estimated from the same network by only switching the order of input images and occlusions are detected using a bi-directional consistency check. The proxy loss is applied only to non-occluded regions as the brightness constancy assump- tion does not hold for occluded pixels. In addition, Meister et al. [36] suggested Optical Flow Estimation in the Deep Learning Age 15 to use a higher-order smoothness term and a ternary census loss [47, 60] to obtain a data term that is robust to brightness changes. This advanced proxy loss significantly improves the accuracy by halving the error compared to pre- vious unsupervised learning approaches. [36] resulting in better accuracy than supervised approaches pre-trained on synthetic data alone (assuming the same backbone), which suggests that directly training on the target domain in an unsupervised manner can be a good alternative to supervised pre-training with synthetic data. Wang et al. [52] also introduced an advanced proxy loss that takes occlusion into account and is applied only to non-occluded regions. Similar to [36], Wang et al. [52] estimate bi-directional optical flow and then obtain an occlusion mask for the forward motion by directly calculating disocclusion from the backward flow. They exploit the fact that occlusion from the forward motion is the in- verse of disocclusion from the backward motion. Disocclusions can be obtained by forward-warping the given flow and detecting the holes to which no pixels have been mapped. In addition to occlusion handling, their approach contains other innovations such as a modified architecture and pre-processing. Accord- ing to their ablation study, the accuracy is improved overall by 25% on public benchmark datasets compared to the unsupervised approach of Yu et al. [59]. In addition, the method demonstrates good occlusion estimation results, close to those of classical energy-based approaches. Janai et al. [26] extended unsupervised learning of optical flow to a multi- frame setting, taking in three consecutive frames and jointly estimating an occlu- sion map. Based on the PWC-Net [48] architecture, they estimate bi-directional flow from the reference frame and occlusion maps for both directions as well. After the cost volume of PWC-Net, Janai et al. use three different decoders: (i) a future frame decoder that estimates flow from the reference frame to the future frame, (ii) a past flow decoder, and (iii) an occlusion decoder. A basic unsu- pervised loss consisting of photometric and smoothness terms is applied only on non-occluded regions for estimating flow, and a constant velocity constraint is also used, which encourages the magnitude of forward flow and backward flow to be similar but going in opposite directions. Their experimental results demon- strate the benefits of using multiple frames, outperforming all two-frame based methods. Furthermore, the accuracy of occlusion estimation is competitive with classical energy-based methods. Liu et al. [32, 33] demonstrated another direction for unsupervised (or self- supervised) learning by using a data distillation framework with student-teacher networks. Their two methods, DDFlow [32] and its extension SelFlow [33], distill reliable predictions from a teacher network, which is trained in an unsupervised manner [36], and use them as pseudo ground truth for training the student net- work, which is used at inference time. The accuracy of this framework depends on how to best distill the knowledge for the student network. For better accu- racy especially in occluded regions, the two methods focus on how to provide more reliable labels for occluded pixels to the student network. DDFlow [32] proposes to randomly crop the predicted flow map from the teacher network as 16 Junhwa Hur and Stefan Roth well as the input images. Then in the cropped images, some of the non-occluded pixels near the image boundaries become out-of-bounds pixels (i.e., occluded pixels), and its reliably predicted optical flow from the non-occluded pixels in the teacher network can work as reliable pseudo ground truth for occluded pixels in the student network. In the experiments, DDFlow [32] showed data distilla- tion to significantly improve the accuracy on average up to 34.7% on public benchmark datasets, achieving the best accuracy among existing unsupervised learning-based approaches. SelFlow [33] suggests a better data distillation strategy by exploiting su- perpixel knowledge and hallucinating occlusions in non-occluded regions. Given the prediction from the teacher network, SelFlow [33] superpixelizes the target frame and perturbs random superpixels by injecting random noise as if non- occluded pixels in the target images were occluded by randomly looking super- pixels. Then likewise, those non-occluded pixels with reliable predictions from the teacher network become occluded pixels when training the student network, guiding to estimate reliable optical flow in occluded areas. In addition, SelFlow [33] further demonstrates multi-frame extensions using 3 frames as input for improving the accuracy by exploiting temporal coherence. Evaluating on public benchmark datasets, SelFlow [33] further improves the accuracy over DDFlow [32], demonstrating the importance of having a better data distillation strategy and suggesting a promising direction for self-supervised learning. 2.3 Semi-supervised learning Complementary to supervised and unsupervised learning methods, semi-supervised learning approaches have been also proposed recently. Lai et al. [30] utilized Gen- erative Adversarial Networks (GANs) [15] and proposed an adversarial loss that captures the structural pattern of the flow warp error, allowing to train a net- work in a semi-supervised way. First, a generator network produces optical flow from the two given input images. Next, the flow warp error map is obtained by calculating the image intensity difference between the first image and the warped second image using the flow output. Then, a discriminator network tries to dis- tinguish whether the warp error map is created by the generator or is the ground truth. The generator aims to fool the discriminator network by producing optical flow whose warp error patterns look close to those of the ground truth. Mean- while, the discriminator keeps trying to correctly distinguish whether the flow warp error pattern is from the generated flow or the ground truth flow, challeng- ing the generator. To train the networks, a combination of labeled and unlabeled data has been used, equally distributed in each mini-batch. For labeled data in each mini-batch, the standard L2 loss is applied to the output of the generator to ensure closeness of the flow estimate to the ground truth. The adversarial loss is applied to the output of the discriminator to both labeled and unlabeled data. The experiments demonstrate benefits over purely supervised and purely unsupervised methods: the results are more accurate than when training with a synthetic dataset only in a supervised way and they also outperform training with unlabeled real data in the target domain only in an unsupervised way. Optical Flow Estimation in the Deep Learning Age 17 Yang et al. [57] proposed another semi-supervised approach by learning a conditional prior for predicting optical flow. They posit that current learning- based approaches to optical flow do not rely on any explicit regularizer (which refers to any prior, model, or assumption that adds any restrictions to the solu- tion space), which results in a risk of overfitting on the training domain, relating to the domain mismatch problem regarding the testing domain. To address the issue, they propose a network that contains prior information of possible opti- cal flows that an input image can give rise to and then use the network as a regularizer for training a standard off-the-shelf optical flow network. They first train the conditional prior network in a supervised manner to learn prior knowl- edge on the possible optical flows of an input image, and then train FlowNet [10] in an unsupervised manner with a regularization loss from the trained con- ditional prior network. The experiments demonstrate that the conditional prior network enables the same network trained on the same dataset (i) to outperform typical unsupervised training and (ii) to give results that are competitive with the usual supervised training, yet showing better generalization across different dataset domains. This observation suggests that semi-supervised learning can benefit domain generalization without labeled data by leveraging the available ground truth from another domain. 3 Multi-frame Optical Flow Estimation In the literature of classical optical flow methods, utilizing multiple frames has a long history (e.g., [39]). When additional temporally consecutive frames are available, different kinds of assumptions and strategies can be exploited. One basic and straightforward way is to utilize the temporal coherence assumption that optical flow smoothly changes over time [6, 27, 28, 50, 54]. This property is sometimes also referred to as constant velocity or acceleration assumption. Another way is to parameterize and model the trajectories of motion, which allows to exploit higher-level motion information instead of simply enforcing temporal smoothness on optical flow [9, 13, 45] in 2D. Recently, there has been initial work on adopting these proven ideas in the context of deep learning to improve the flow accuracy. Ren et al. [42] proposed a multi-frame optical flow network by extending the two-frame, state-of-the-art PWC-Net [48]. Given three temporally consecutive frames, It−1, It, and It+1, the proposed method fuses the two optical flows from It−1 to It and from It to It+1 to exploit the temporal coherence between the three frames. Each optical flow is obtained using PWC-Net. In order to fuse the two optical flows, the method also estimates the flow from It to It−1 to backwardwarp the flow from It−1 to It to match the spatial coordinates of corresponding pixels. When fusing the two flows, Ren et al. use an extra network that inputs the flows with their brightness error and outputs the refined final flow. The underlying idea of inputting the brightness error together is to guide regions to refine to where optical flow may be inaccurate. In their experiments, Ren et al. [42] demonstrated that utilizing two adjacent optical flows and fusing 18 Junhwa Hur and Stefan Roth them improves the flow accuracy especially in occluded areas and out-of-bound areas. Maurer et al. [34] also proposed a multi-frame optical flow method that ex- ploits the temporal coherence but in a different direction by learning to predict forward flow from the backward flow in an online manner. Similarly given three temporally consecutive frames, It−1, It, and It+1, the proposed method first esti- mates the forward flow (i.e., from It to It+1) and the backward flow (i.e., from It to It−1) using an off-the-shelf energy-based approach [21]. Next, the method finds inliers for each flow by estimating the opposite directions of each flow (i.e., from It+1 to It and from It−1 to It) and performing a consistency check. Given the inlier flow for both directions as ground truth data, the method then trains shal- low 3-layer CNNs that predict the forward flow (i.e., from It to It+1) from the input backward flow (i.e., from It to It−1). The idea to predict the forward flow from the backward flow is to exploit the valuable motion information from the previous time step including in occluded regions, which the current step is not able to properly handle but that are visible in the previous time step. This train- ing is done in an online manner so that the network can be trained adaptively to input samples while exploiting temporal coherence. Finally, the method fuses the predicted forward flow and the estimated forward flow to obtain a refined forward flow. On major benchmark datasets, the method demonstrates the ad- vantages of exploiting temporal coherence by improving the accuracy especially in occluded regions by up to 27% overall over a baseline model that does not use temporal coherence. Finally, Neoral et al. [40] proposed an extended version of PWC-Net [48] in the multi-frame setting, jointly estimating optical flow and occlusion. Given a temporal sequence of frames, Neoral et al. proposed to improve the flow and occlusion accuracy by leveraging each other in a recursive manner in the tem- poral domain. First, they propose a sequential estimation of optical flow and occlusion: estimating occlusion first and then estimating optical flow, feeding the estimated occlusion as one of inputs into the flow decoder. They found that providing the estimated occlusion as an additional input improves the flow accu- racy by more than 25%. Second, they input the estimated flow from the previous time step into the occlusion and flow decoders as well, which yields additional accuracy improvements for both tasks, especially improving the flow accuracy by more than 12% on public benchmark datasets. Similar to other multi-frame based methods above, the flow accuracy improvement is especially prominent in occluded areas and also near motion boundaries. 4 Conclusion The recent advances in deep learning have significantly influenced the tran- sition from classical energy-based formulations to CNN-based approaches for optical flow estimation. We reviewed this transition here. Two main families of CNN approaches to optical flow have been pursued: (i) using CNNs as a feature extractor on top of conventional energy-based formulations and (ii) end- Optical Flow Estimation in the Deep Learning Age 19 to-end trainable, regression-based CNN architectures. While methods proposed in the initial stages of this transition were outperformed by classical energy- based formulations at the time, steady research progress, e.g. discovering bet- ter backbone architectures, synthetic training datasets, and learning strategies eventually led CNN-based methods to yield the most accurate results today and to dominate the current literature. To overcome the (domain) overfitting ten- dency of supervised learning, unsupervised or self-supervised methods, as well as semi-supervised learning methods have been recently investigated as alter- natives. Finally, multi-frame CNN approaches, exploiting temporal smoothness or coherency, have demonstrated the potential of improving the flow estimation accuracy even further. Despite the significant progress, a number of limitations of current approaches remain including, e.g., (i) the domain overfitting tendency, i.e. trained models do not generalize well to unseen domains yet, and (ii) the necessity of complex training schemes, which require pre-training on synthetic datasets first before fine-tuning on the target domain and can make training models complicated in practice. These and other challenges leave significant room for future work on deep learning methods for optical flow. References 1. Ahmadi, A., Patras, I.: Unsupervised convolutional neural networks for motion estimation. In: Proceedings of the IEEE International Conference on Image Pro- cessing, pp. 1629–1633 (2016) 2. Bai, M., Luo, W., Kundu, K., Urtasun, R.: Exploiting semantic information and deep matching for optical flow. In: B. Leibe, J. Matas, N. Sebe, M. Welling (eds.) Proceedings of the 14th European Conference on Computer Vision, Lecture Notes in Computer Science, vol. 9908, pp. 154–170. Springer (2016) 3. Bailer, C., Varanasi, K., Stricker, D.: CNN-based patch matching for optical flow with thresholded hinge embedding loss. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3250–3259. Honolulu, Hawaii (2017) 4. Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.: A database and evaluation methodology for optical flow. International Journal of Computer Vision 92(1), 1–31 (2011) 5. Barnes, C., Shechtman, E., Goldman, D.B., Finkelstein, A.: The generalized Patch- Match correspondence algorithm. In: K. Daniilidis, P. Maragos, N. Paragios (eds.) Proceedings of the 11th European Conference on Computer Vision, Lecture Notes in Computer Science, vol. 6313, pp. 29–43. Springer (2010) 6. Black, M.J., Anandan, P.: Robust dynamic motion estimation over time. In: Pro- ceedings of the IEEE Computer Society Conference on Computer Vision and Pat- tern Recognition, pp. 296–302. Lahaina, Maui, Hawaii (1991) 7. Boykov, Y., Veksler, O., Zabih, R.: Markov random fields with efficient approxi- mations. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 648–655. Santa Barabara, California (1998) 8. Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie for optical flow evaluation. In: A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, 20 Junhwa Hur and Stefan Roth C. Schmid (eds.) Proceedings of the 12th European Conference on Computer Vi- sion, Lecture Notes in Computer Science, vol. 7577, pp. 611–625. Springer (2012) 9. Chaudhury, K., Mehrotra, R.: A trajectory-based computational model for optical flow estimation. IEEE Transactions on Robotics and Automation 11(5), 733–741 (1995) 10. Dosovitskiy, A., Fischer, P., Ilg, E., H¨ausser, P., Hazırba¸s, C., Golkov, V., v. d. Smagt, P., Cremers, D., Brox, T.: FlowNet: Learning optical flow with convolu- tional networks. In: Proceedings of the Fifteenth IEEE International Conference on Computer Vision, pp. 2758–2766. Santiago, Chile (2015) 11. Fortun, D., Bouthemy, P., Kervrann, C.: Optical flow modeling and computation: A survey. Computer Vision and Image Understanding 134(1), 1–21 (2015) 12. Gadot, D., Wolf, L.: PatchBatch: A batch augmented loss for optical flow. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 4236–4245. Las Vegas, Nevada (2016) 13. Garg, R., Roussos, A., Agapito, L.: A variational approach to video registration with subspace constraints. International Journal of Computer Vision 104(3), 286– 314 (2013) 14. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? The KITTI vision benchmark suite. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3354–3361. Provi- dence, Rhode Island (2012) 15. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in Neural Information Processing Systems, pp. 2672–2680 (2014) 16. G¨uney, F., Geiger, A.: Deep discrete flow. In: S.H. Lai, V. Lepetit, K. Nishino, Y. Sato (eds.) Proceedings of the Thirteenth Asian Conference on Computer Vi- sion, Lecture Notes in Computer Science, vol. 10115, pp. 207–224. Springer (2016) 17. Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality reduction by learning an in- variant mapping. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1735–1742. New York, New York (2006) 18. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.: MatchNet: Unifying fea- ture and metric learning for patch-based matching. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3279–3286. Boston, Massachusetts (2015) 19. Hirschm¨uller, H.: Stereo processing by semiglobal matching and mutual informa- tion. IEEE Transactions on Pattern Analysis and Machine Intelligence 30(2), 328–341 (2008) 20. Horn, B.K.P., Schunck, B.G.: Determining optical flow. Artificial Intelligence 17(1– 3), 185–203 (1981) 21. Hu, Y., Song, R., Li, Y.: Efficient Coarse-to-Fine PatchMatch for large displace- ment optical flow. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 5704–5712. Las Vegas, Nevada (2016) 22. Huang, G., Liu, Z., v. d. Maaten, L., Weinberger, K.Q.: Densely connected convo- lutional networks. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2261–2269 (2017) 23. Hui, T.W., Tang, X., Loy, C.C.: LiteFlowNet: A lightweight convolutional neural network for optical flow estimation. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 8981–8989. Salt Lake City, Utah (2018) Optical Flow Estimation in the Deep Learning Age 21 24. Hur, J., Roth, S.: Iterative residual refinement for joint optical flow and occlu- sion estimation. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 5754–5763 (2019) 25. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: FlowNet 2.0: Evolution of optical flow estimation with deep networks. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1647–1655. Honolulu, Hawaii (2017) 26. Janai, J., G¨uney, F., Ranjan, A., Black, M.J., Geiger, A.: Unsupervised learning of multi-frame optical flow with occlusions. In: V. Ferrari, M. Hebert, C. Sminchis- escu, Y. Weiss (eds.) Proceedings of the 15th European Conference on Computer Vision, Lecture Notes in Computer Science, pp. 713–731. Springer (2018) 27. Janai, J., Guney, F., Wulff, J., Black, M.J., Geiger, A.: Slow Flow: Exploiting high- speed cameras for accurate and diverse optical flow reference data. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recog- nition, pp. 1406–1416 (2017) 28. Kennedy, R., Taylor, C.J.: Optical flow with geometric occlusion estimation and fusion of multiple frames. In: X.C. Tai, E. Bae, T. Chan, M. Lysaker (eds.) Pro- ceedings of the 10th International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition, Lecture Notes in Computer Science, pp. 364–377 (2015) 29. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classification with deep con- volutional neural networks. In: Advances in Neural Information Processing Sys- tems, vol. 25, pp. 1097–1105 (2012) 30. Lai, W.S., Huang, J.B., Yang, M.H.: Semi-supervised learning for optical flow with generative adversarial networks. In: Advances in Neural Information Processing Systems, pp. 354–364 (2017) 31. Li, S.Z.: Markov random field models in computer vision. In: J.O. Eklundh (ed.) Proceedings of the Third European Conference on Computer Vision, Lecture Notes in Computer Science, pp. 361–370. Springer (1994) 32. Liu, P., King, I., Lyu, M.R., Xu, J.: DDFlow: Learning optical flow with unla- beled data distillation. In: Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, pp. 8770–8777. Honolulu, Hawaii (2019) 33. Liu, P., Lyu, M., King, I., Xu, J.: SelFlow: Self-supervised learning of optical flow. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 4571–4580. Long Beach, California (2019) 34. Maurer, D., Bruhn, A.: ProFlow: Learning to predict optical flow. In: Proceedings of the British Machine Vision Conference. Newcastle, UK (2018) 35. Mayer, N., Ilg, E., H¨ausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox, T.: A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 4040–4048. Las Vegas, Nevada (2016) 36. Meister, S., Hur, J., Roth, S.: UnFlow: Unsupervised learning of optical flow with a bidirectional census loss. In: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence. New Orleans, Louisiana (2018) 37. Menze, M., Geiger, A.: Object scene flow for autonomous vehicles. In: Proceed- ings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3061–3070. Boston, Massachusetts (2015) 38. Menze, M., Heipke, C., Geiger, A.: Discrete optimization for optical flow. In: Proceedings of the 37th German Conference on Pattern Recognition, Lecture Notes in Computer Science, pp. 16–28. Springer (2015) 22 Junhwa Hur and Stefan Roth 39. Nagel, H.H.: Extending the ‘oriented smoothness constraint’ into the temporal domain and the estimation of derivatives of optical flow. In: O.D. Faugeras (ed.) Proceedings of the First European Conference on Computer Vision, Lecture Notes in Computer Science, vol. 427, pp. 139–148. Springer (1990) 40. Neoral, M., ˇSochman, J., Matas, J.: Continual occlusions and optical flow esti- mation. In: C. Jawahar, H. Li, G. Mori, K. Schindler (eds.) Proceedings of the Fourtheenth Asian Conference on Computer Vision, Lecture Notes in Computer Science. Springer (2018) 41. Ranjan, A., Black, M.J.: Optical flow estimation using a spatial pyramid network. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2720–2729. Honolulu, Hawaii (2017) 42. Ren, Z., Gallo, O., Sun, D., Yang, M.H., Sudderth, E.B., Kautz, J.: A fusion approach for multi-frame optical flow estimation. In: IEEE Winter Conference on Applications of Computer Vision, pp. 2077–2086. Waikoloa Village, HI (2019) 43. Ren, Z., Yan, J., Ni, B., Liu, B., Yang, X., Zha, H.: Unsupervised deep learning for optical flow estimation. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pp. 1495–1501. San Francisco, California (2017) 44. Revaud, J., Weinzaepfel, P., Harchaoui, Z., Schmid, C.: EpicFlow: Edge-preserving interpolation of correspondences for optical flow. In: Proceedings of the Fifteenth IEEE International Conference on Computer Vision, pp. 1164–1172. Santiago, Chile (2015) 45. Ricco, S., Tomasi, C.: Dense lagrangian motion estimation with occlusions. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1800–1807. Providence, Rhode Island (2012) 46. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.: Discriminative learning of deep convolutional feature point descriptors. In: Pro- ceedings of the Fifteenth IEEE International Conference on Computer Vision, pp. 118–126. Santiago, Chile (2015) 47. Stein, F.: Efficient computation of optical flow using the census transform. In: C. Rasmussen, H. B¨ulthoff, B. Sch¨olkopf, M. Giese (eds.) Pattern Recognition, Proceedings of the 26th DAGM-Symposium, Lecture Notes in Computer Science, vol. 3175, pp. 79–86. Springer (2004) 48. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 8934–8943. Salt Lake City, Utah (2018) 49. Tu, Z., Xie, W., Zhang, D., Poppe, R., Veltkamp, R.C., Li, B., Yuan, J.: A survey of variational and CNN-based optical flow techniques. Signal Processing: Image Communication 72, 9–24 (2019) 50. Volz, S., Bruhn, A., Valgaerts, L., Zimmer, H.: Modeling temporal coherence for optical flow. In: Proceedings of the Thirteenth IEEE International Conference on Computer Vision, pp. 1116–1123. Barcelona, Spain (2011) 51. ˇZbontar, J., LeCun, Y.: Computing the stereo matching cost with a convolutional neural network. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1592–1599. Boston, Massachusetts (2015) 52. Wang, Y., Yang, Y., Yang, Z., Zhao, L., Wang, P., Xu, W.: Occlusion aware unsu- pervised learning of optical flow. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 4884–4893 (2018) Optical Flow Estimation in the Deep Learning Age 23 53. Weinzaepfel, P., Revaud, J., Harchaoui, Z., Schmid, C.: DeepFlow: Large displace- ment optical flow with deep matching. In: Proceedings of the Fourteenth IEEE International Conference on Computer Vision, pp. 1385–1392. Sydney, Australia (2013) 54. Werlberger, M., Trobin, W., Pock, T., Wedel, A., Cremers, D., Bischof, H.: Anisotropic huber-L1 optical flow. In: Proceedings of the British Machine Vision Conference. London, UK (2009) 55. Xu, J., Ranftl, R., Koltun, V.: Accurate optical flow via direct cost volume pro- cessing. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1289–1297. Honolulu, Hawaii (2017) 56. Yang, G., Ramanan, D.: Volumetric correspondence networks for optical flow. In: Advances in Neural Information Processing Systems, pp. 793–803 (2019) 57. Yang, Y., Soatto, S.: Conditional prior networks for optical flow. In: V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (eds.) Proceedings of the 15th European Conference on Computer Vision, Lecture Notes in Computer Science, pp. 271–287. Springer (2018) 58. Yin, Z., Darrell, T., Yu, F.: Hierarchical discrete distribution decomposition for match density estimation. In: Proceedings of the IEEE Computer Society Confer- ence on Computer Vision and Pattern Recognition, pp. 6044–6053. Long Beach, California (2019) 59. Yu, J.J., Harley, A.W., Derpanis, K.G.: Back to Basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. In: Proceedings of the 14th European Conference on Computer Vision Workshops, Lecture Notes in Computer Science, pp. 3–10. Springer (2016) 60. Zabih, R., Woodfill, J.: Non-parametric local transforms for computing visual cor- respondence. In: J.O. Eklundh (ed.) Proceedings of the Third European Conference on Computer Vision, Lecture Notes in Computer Science, vol. 801, pp. 151–158. Springer (1994) 61. Zagoruyko, S., Komodakis, N.: Learning to compare image patches via convolu- tional neural networks. In: Proceedings of the IEEE Computer Society Confer- ence on Computer Vision and Pattern Recognition, pp. 4353–4361. Boston, Mas- sachusetts (2015) 62. Zhu, Y., Lan, Z., Newsam, S., Hauptmann, A.G.: Guided optical flow learning. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2017) 63. Zhu, Y., Newsam, S.: DenseNet for dense flow. In: Proceedings of the IEEE Inter- national Conference on Image Processing, pp. 790–794 (2017)","libVersion":"0.3.2","langs":""}
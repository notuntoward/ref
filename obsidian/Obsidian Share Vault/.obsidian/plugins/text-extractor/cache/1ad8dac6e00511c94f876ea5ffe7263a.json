{"path":"lit/lit_sources.backup/Guimaraes24whatPretrainedLLMknow.pdf","text":"FO CUS A RTICL E Pre-trained language models: What do they know? Nuno Guimar˜aes 1,2 | Ricardo Campos 1,3,4 | Alípio Jorge 1,2 1LIAAD-INESCTEC, Porto, Portugal 2University of Porto, Porto, Portugal 3University of Beira Interior, Covilh˜a, Portugal 4Ci2 - Smart Cities Research Centre (Polytechnic Institute of Tomar), Tomar, Portugal Correspondence Nuno Guimar˜aes, LIAAD-INESCTEC, Porto, Portugal. Email: nuno.r.guimaraes@inesctec.pt Funding information FCT—Fundaç˜ao para a Ciência e a Tecnologia, Grant/Award Number: 2022.09312.PTDC; Component 5—Capitalization and Business Innovation, Grant/Award Number: 41; European Union (EU); Next Generation EU Edited by: Elisa Bertino, Associate Editor and Witold Pedrycz, Editor-in-Chief Abstract Large language models (LLMs) have substantially pushed artificial intelligence (AI) research and applications in the last few years. They are currently able to achieve high effectiveness in different natural language processing (NLP) tasks, such as machine translation, named entity recognition, text classification, question answering, or text summarization. Recently, significant attention has been drawn to OpenAI's GPT models' capabilities and extremely accessible inter- face. LLMs are nowadays routinely used and studied for downstream tasks and specific applications with great success, pushing forward the state of the art in almost all of them. However, they also exhibit impressive inference capabilities when used off the shelf without further training. In this paper, we aim to study the behavior of pre-trained language models (PLMs) in some inference tasks they were not initially trained for. Therefore, we focus our attention on very recent research works related to the inference capabilities of PLMs in some selected taskssuchasfactual probingand common-sense reasoning. We highlight relevant achievements made by these models, as well as some of their current limitations that open opportunities for further research. This article is categorized under: Fundamental Concepts of Data and Knowledge > Key Design Issues in Data Mining Technologies > Artificial Intelligence KEYWOR DS large language models, natural language, pretrained language models, processing 1 | INTRODUCTION The introduction of the Transformers architecture (Vaswani et al., 2017) had a major impact on the artificial intelligence (AI) community. This architecture revolutionized the way models were trained and proved to be a major leap from previous machine learning models such as decision trees, support vector machines, and previously existing neural networks. Soon after Transformers, LLMs like BERT start to outperform the state of the art (SOTA) in more than 10 different natural language processing (NLP) tasks (Devlin et al., 2019). Other models followed this one with a higher number of parameters, data, and consequently, training time. In the majority of the cases, these characteristics translate into a boost in performance, with GPT-3 (trained over 499 billion tokens and encompassing 175 billion parameters) boosting, even more, the SOTA effectiveness in several different NLP tasks (Brown et al., 2020). More recently, OpenAI's ChatGPT1 gained mainstream attention outside of the AI field due to the extraordinary accessibility of the model via a chat-based prompt as well as its encyclopedic and conversational capabilities. Received: 4 July 2023 Revised: 29 August 2023 Accepted: 30 August 2023 DOI: 10.1002/widm.1518 WIREs Data Mining Knowl Discov. 2024;14:e1518. wires.wiley.com/dmkd © 2023 Wiley Periodicals LLC. 1of10 https://doi.org/10.1002/widm.1518 In an era where PLMs are more relevant than ever and with the latest release of GPT-4 (OpenAI, 2023), it is essential to understand what they do know, how they perform, and what are their current limitations in different tasks where humans excel by default. Thus, in this manuscript, we aim to understand some of these common-sense inference capabilities displayed by PLMs in an off-the-shelf scenario. For that, we overview some very recent work focused on PLMs and highlight their achievements in real-world settings. In the next section, we give a brief introduction to LLMs. In Section 3 we explore the different commonsense infer- ence capabilities displayed by PLMs in recent research. Next, in Section 4, we discuss some of the milestones achieved by PLMs in real-world settings. Finally, we discuss the current limitations and possible research paths to explore in future work. 2 | A BRIEF INTRODUCTION TO LLMS For the sake of interpretation and discussion in the following sections, we begin by describing what is an LLM and how it learns and acquires knowledge. There are two main types of language models: probabilistic language models and the more recent neural network-based language models. Probabilistic language models, such as Hidden Markov Models (HMM) (Baum & Petrie, 1966) or conditional random fields (CRF) (Lafferty et al., 2001), are based on the conditional probability of the last word of an n-gram, based on the previous n \u0001 1 grams. In other words, probabilistic models try to predict the next word/token based on the conditional probability of the n \u0001 1 sequence of words. This approach is very limited since the next word prediction is solely based on the n \u0001 1 previous words. This affects the accuracy of the model since the context of the n \u0001 1 previous words is insufficient for many common tasks. In addition, increasing n results in an exponential growth of the memory required due to the number of word permutations computed. Neural network-based language models or Neural language models (NLMs) for short, are more commonly used and improve on probabilistic models since they do not require the storing of all possible permutations to create long-term dependencies. Instead, NLMs use neural networks to estimate probabilities. By relying on approximation, they can model dependencies between tokens in the text that are far apart (long-term dependencies). Part of the success is due to the vectorial representation of the tokens, which is automatically learned in order to optimize the estimation of the probabilities of the words in a sequence. The learning process naturally leads to semantically related words having similar vectors representing them. These vectors are usually called word embeddings. Word2Vec is one early and rele- vant example of this architecture, where each word is represented as a 300-size numerical vector in the pre-trained model presented in Mikolov et al. (2013). A first major advance using this approach was ELMo (Peters et al., 2018), a language model where each token is represented taking into account the context of the entire sentence where the token appears. This means that, unlike Word2vec, similar words may have different embeddings depending on the context where they appear. To this purpose, ELMo uses two bidirectional long short-term memory (LSTM) layers (forward and backward) to learn and produce contextual embeddings from the context given by the words at the right and from the words at the left of the token. However, as previously stated, Transformers changed the way language models were developed. Instead of focusing on Recurrent Neural Networks where updating and forgetting mechanisms are learned by the network, Transformers showed that with a sufficiently large training corpus, attention mechanisms are sufficient to achieve high effectiveness in several NLP tasks. From this point on, several LLMs were developed based on Transformers with the first advance being the BERT model (Devlin et al., 2019) which uses two tasks for pre-training: masked-language modeling and next sentence prediction task. The first is used to create token-level representations and the second to teach the model long- term dependencies between sentences. More recently, Generative pre-trained transformer (GPT) models (Redford et al., 2018) showed a remarkable progress in the generation of text and, unlike BERT, they are auto-regressive, mean- ing that the prediction of the next word is solely based on the previous words. While the first version of GPT models was trained in the bookCorpus, a dataset containing approximately 11 thousand unpublished books (Zhu et al., 2015), subsequent iterations of this model (Brown et al., 2020; OpenAI, 2023) led to a significant increase in the size of the data, including a wide variety of sources such as books, websites, and other textual content available on the internet.2 After this brief introduction to LLMs, we now focus on understanding what the pre-trained language models are capable of beyond what they have been trained for, especially in terms of common-sense inference capabilities. To that regard, in the following section, we analyze recent research that tests and evaluates different capabilities in PLMs (such as factual probing and common sense) in an off-the-shelf manner (i.e., without additional fine-tuning for specific tasks). 2of10 GUIMARÃES ET AL. 3 | W HAT DO PRE-TR AINED LARGE LANG UAGE M ODELS K NOW In this section, we explore what PLMs are capable of without additional tuning. In other words, what do PLMs know, and what they are able to accomplish in human-like tasks relying on inference and common sense. To assess these questions, we first need to introduce the concept of prompts. A prompt is a non-formal textual input, freely structured, that instructs the model to provide the answer we want. For example, in a task where an input x is used to predict a label y, a prompt is the transformation of the input x into a fragment of text to be understood by the language model. This input is then fed to the PLM with the goal of predicting y (Liu et al., 2022). A good example is the machine translation task where x is a sentence in English and y is the sentence translated into Portuguese. Given x ¼ “I like this movie” and y = “Eu gosto deste filme,” a possible prompt could be “English: [x] Portuguese: z” where z is the slot to be filled by the PLM (and consequently mapped to y). Given the complexity of PLMs, the way prompts are formulated has a strong impact in the model's response. There- fore, characterizing the structure of the prompts is a very important step in understanding what PLMs are capable of. In fact, prompt engineering is a rapidly growing field, with different prompt architectures being studied and devel- oped and with empirical results showing that performance largely depends on the prompts provided (Jiang et al., 2021). A detailed explanation of the different prompt architectures is out of the scope of this work. A comprehensive review on the subject is presented in Liu et al. (2022) for the interested reader. 3.1 | Factual probing The first task we aim to analyze is whether PLMs are able to store knowledge in the form of facts. Factual probing con- sists of extracting facts from PLMs. The authors in Petroni et al. (2019) showed that PLMs such as BERT achieve com- petitive results compared with a knowledge-based approach in open-domain question and answering (QA). To prove this, the authors gather several datasets consisting of fact triplets (subject-relation-object) or QA pairs and convert them into prompts. For example, the triplet (ravens,capableOf, fly) can be converted to “Ravens can [z]” where y = “fly” and z must be equal to y for a correct prediction. The authors also refer to the importance of prompt engineering in improv- ing the performance of PLMs. Another study using similar data (Jiang et al., 2020) shows the use of paraphrasing and mining-based techniques (i.e., prompts generated under the assumption that the words between the subject and object are frequently indicative of their relation) to build different prompts and maximize the chances of successfully probing PLMs for the correct answers. The paper proposes different ensembles of prompts (i.e., combining the results from dif- ferent prompts based on their accuracy in a training dataset), with the evaluation showing that these present a signifi- cant improvement in factual knowledge retrieval over manually designed prompts. Nevertheless, there are some limitations to the facts that can be extracted from LLMs. One particular limitation occurs with negation. Taking the previous example “Ravens can [z],” the knowledge stored in PLMs can infer that the most suitable answer for [z] is “fly”. However, as the authors in Kassner and Schütze (2020) showed, LLMs are often less accurate when the prompt comes in a negated form. For example, in the prompt “Ravens cannot [z],” several values for [z] can be factually correct (e.g., “Ravens cannot vote,”“Ravens cannot talk,”…). Yet, it seems that large pre- trained models are unable to distinguish between positive and negative prompts in most cases. This is most likely due to the overabundance of positive examples in the training data. In addition, the authors also experiment with the con- cept of misprime where an incorrect word in the form of a question is placed at the beginning of the prompt (e.g., “Talk? Ravens can [z]”). With this prompt template, the authors experimented different prompts' versions, ranging the incorrect word from completely random (e.g., “Dinosaurs? Munich is located in [z]”) to being in a similar context to the answer (e.g., “Prussia? Munich is located in [z]”). The results show that although PLMs seem to be robust to mis- leading random words, the same cannot be said when the first word is semantically close to the expected answer. This is thus an example that these models can be distracted by a single incorrect word consequently misguiding the truthful- ness of their knowledge. 3.2 | Commonsense reasoning A more human capability studied in PLMs is commonsense reasoning. This topic tackles the capability of LLMs to per- ceive and demonstrate practical knowledge and judgment intrinsic to the majority of humans. One example of that is GUIMARÃES ET AL. 3of10 stereotypic tacit assumptions (STAs). STAs are sufficiently large occurrences of a certain property in a generic concept that make it generally accepted (Weir et al., 2020). For example, the statement “Peter feeds his dog” does not require addi- tional context such as “Humans have dogs,”“Peter is a Human,” or “Dogs need to be fed.” The difference between STAs and facts is that these properties cannot be applied to all cases (for example, not all humans have dogs). Weir et al. (2020) demonstrate that PLMs (specifically RoBERTa-large) achieve high performance on extracting concepts. For example, given the prompt with the associated properties presented as an STA example in Table 1, the model responds “person.” Another way to evaluate commonsense reasoning in PLMs is by assessing the plausibility of sentences using presup- position. As an example, let us consider the following sentences: •“There, a barman talked to John for a while” •“There, the barman talked to John for a while”. The main difference is that the use of “The” presupposes the existence of a barman in this context, the use of “A” assumes that the existence of a barman in this context is new information. Thus, “The” is a presupposition trigger and directly affects the plausibility of sentences. For example, using the same example with the presupposition trigger, the following two texts: 1. “John went to the bar. There, the barman talked to John for a while” 2. “John went to the school. There, the barman talked to John for a while”. have different degrees of plausibility (Singh et al., 2016). In fact, commonsense guides us to believe that the first example is more plausible than the second. Driven by this principle, the authors in Cong (2022) evaluate how PLMs deal with the plau- sibility of sentences using presupposition triggers by masking the noun word (in the previous example “bar” and “school”) and evaluating the plausibility of the responses. In other words, the PLM was considered successful if the word was in the top 5 tokens in the plausible examples and was not in the top 5 in the implausible examples. Results show an accuracy of 75% using the pre-trained DistilBERT model, which curiously surpasses the same model fine-tuned for the task. The authors also explore the concept of scalar implicature in PLMs. Scalar implicature refers to the use of certain quantifiers to imply that there is a reason for not using a larger term. For example, the sentence “I drank some of the juice” implies that there is some juice left, and not all juice was drunk. Once again, the use of scalar implicature gener- ates more or less plausible sentences whose degree can be assessed using commonsense reasoning. For example, the sentence “Some people have lungs” is less plausible than the sentence “Some people have pets.” Having this into account, the authors evaluate the capability of DistilBERT to infer the critical word in different plausible sentences (for example “some” in the prompt “__ office buildings have plants” and “all” in the prompt “__ office buildings have desks and can become dusty”). The model is considered successful if the correct critical word is in the top 5 token list. The result shows that this model achieves an accuracy of approximately 65% in this task, with the results improving signifi- cantly (>95%) with fine-tuning. The last inference task we present is related to the theory of mind concept. Theory of mind is a central ability humans have in social communications or interactions. It allows an observer to predict or explain people's behavior based on what the observer believes they know, believe, or desire. In other words, an observer may input mental states TABLE 1 Concepts and prompt examples. Concept Prompt example Facts Cristiano Ronaldo was born in [z] Negation Dogs cannot [z] Misprime Italy? Vilnius is located in [z] STAs A [z] has parents, siblings, relatives, a home, a pet, a car, a spouse, a job. Presupposition John went to the [z]. There, the barman talked to John for a while. Scalar Implicature [z] people have pets, which require good care. Numeric Commonsense Cats have [z] legs. Math Word Problems John had 4 chocolates. He gave two to Alice. How many chocolates does John have left? [z] 4of10 GUIMARÃES ET AL. to others based on these principles (Premack & Woodruff, 1978; Frith & Frith, 2005). For example, let us consider the following scenario: • John left the cake inside the fridge. Then, John left the house to go to work. While he was gone, Mary took the cake from the fridge and left it on the table. When John returned where did he look for the cake? The ability to infer that John will look for the cake in the fridge is a theory of mind principle. The observer is capa- ble of imputing a mental state to John. Since John was outside the house when Mary took the cake from the fridge, it is reasonable to assume that John will look for it in the place where he left it and not on the table. With this principle in mind, the author in Kosinski (2023) found out that theory of mind capabilities might have naturally emerged in PLMs. Theory of mind capabilities of different PLMs were evaluated using two tasks: • Unexpected transfer task: where an observant understands that a participant observes a certain state of the world and leaves. Then, during its absence, an unexpected change happens. An observant with theory of mind capabilities must realize that although a change occurs, the participant still falsely believes that the state of the world remains the same. For example, the observant knows that John believes that the cake is in the fridge. • Unexpected content task: where an observant understands that the participant holds a belief that the observant knows it's false. As an example, let us consider the following scenario: There is a box of cookies on the table at John's house. However, the box does not contain cookies but fruit. John sees the box for the first time and cannot see what is inside the box. John reads the label. An observant with theory of mind capabilities predicts that the participant wrongly assumes that the box is full of cookies. Results show that ChatGPT is able to outperform other LLMs and solve 92% of the two types of tasks presented by the author. This corresponds to 20 unexpected contents tasks and 20 unexpected transfer tasks where each task is evalu- ated using 6 different prompts: two aimed at the models' understanding of the content or location of an object, and four aimed at the model's understanding of the participant's belief. In addition, the authors also show the model's theory of mind capabilities in different stages of the story. Using the John and Mary example, the authors query the model using the following prompts: “The cake is in the __” and “When John comes back home, he will look for the cake in the __” to understand if the model can infer the cake location and where did John believe the cake was, before and after Mary changed its place. 3.3 | PLMs do not like numbers So far, it seems that PLMs have a good performance in commonsense reasoning and factual probing. However, in their pre-trained form, they tend to show weaker results in numerical commonsense knowledge and numerical reasoning. To assess the numerical commonsense knowledge of PLMs such as GPT2, BERT, and RoBERTa-based models, Lin et al. (2020) explored several categories of numeric commonsense reasoning such as Objects (e.g., “Bicycles have [z] tires”) and Geometry (e.g., “Squares have [z] sides”). The authors concluded that pre-trained BERT and RoBERTa models have low performance in this task and are largely outperformed by humans. However, more recently, Ning et al. (Bian et al., 2023) evaluated ChatGPT using the same data and achieved an accuracy of 79%, demonstrating the improvement achieved in this domain by more recent GPT models. BERT and RoBERTa were also evaluated on math word problems (MWPs). MWPs are mathematical problems described in text format and in the form of a narrative with characters, entities, items, and quantifiers (Lu, Qiu, et al., 2023). An example of an MWP is presented in Table 1. The authors in Stolfo et al. (2023) evaluate 11 PLMs using MWPs. Results show that GPT-3-Davinci largely surpasses other GPT variants achieving approximately 0.8 Preci- sion@10 and 0.2 Precision@1. Achieving similar effectiveness in more complex mathematical problems such as theorem proving and geometry problem solving using PLMs without additional knowledge or tuning is a difficult task. First, because mathematical data is scarcer for training LLMs. Second, current PLMs are not specifically trained in mathematical data. In fact, depending on the architecture of the model and the format of the numbers in the data, tokenization of the same num- ber can lead to different results which severely impact the performance of these models in numerical tasks (Lu, Qiu, et al., 2023). However, recent research with ChatGPT code interpreter3(which improves to provide logical natural GUIMARÃES ET AL. 5of10 language reasoning and code snippets) and carefully designed prompts for self-verification of the provided output has shown promising results in the effectiveness of PLMs in this domain (Zhou et al., 2023). In summary, PLMs already display several common-sense inference capabilities, although with limitations in areas such as negation and mathematical reasoning. In the next section, we explore different tasks, designed to be done by humans, where PLMs use these capabilities to achieve competitive and human-like results. 4 | RELEVANT ACHIEVEMENTS IN REAL-WORLD SCENARIOS The commonsense inference capabilities acquired by PLMs are currently being explored in different domain-specific tasks. Due to the large volume of manuscripts being published in the area, we are prioritizing the most recent studies (including some that have not yet been peer-reviewed). For example, in the health domain, PLMs such as GPT-3, have been evaluated in the correct triage and diagnosis of patients (Levine et al., 2023). When compared with physicians and lay individuals with Internet access, GPT-3 capabilities surpass the latter in the majority of cases and even perform slightly better than physicians in specific triage cases. The application of PLMs in the health domain is not limited to the correct diagnosis of patients. In fact, studies have shown promising results on the use of PLMs as assistants in differ- ent health-related tasks such as the translation of radiology reports to a clear language (e.g., to be read by patients) (Lyu et al., 2023) and de-identification of medical texts (Liu et al., 2023). In addition, researchers are also assessing the possi- bilities of using GPT-models for assistance in intensive care (Lu, Wu, et al., 2023) and text-based mental health classifi- cation (Lamichhane, 2023) to name a few. Several studies are also probing PLMs to understand their capabilities in passing exams designed to evaluate the knowledge of students in the health domain. The authors in Nori et al. (2023) show that GPT-4 is capable of overcoming the passing grade in the United States Medical Licensing Examination (USMLE) which is designed to evaluate students in clinical scenarios. It is also interesting to notice that even without additional media, GPT-4 is able to achieve a 70%–80% accuracy in exams where question-based images are included. Finally, it is important to highlight that GPT-4 is often capable of explaining the answer it provided. In addition, it demonstrates the theory of mind in educational situations (for example when asked to explain to the student why the answer he/she pro- vided is not correct). The previous GPT model was only near the passing threshold (Kung et al., 2023)showing the leap in the performance from ChatGPT to GPT-4. The results in the 2023 Japanese Nursing Examination were sim- ilar. While GPT-3.5 failed the passing standards, GPT-4 passed with an overall accuracy of 79.7% (Kaneda et al., 2023). A similar study evaluates ChatGPT and GPT-4 in the Japanese Medical License Examinations (Kasai et al., 2023). Once again, GPT-4 is able to pass the exam. Nevertheless, the selection of prohibited choices (i.e,. clin- ical practices prohibited in Japan such as euthanasia) is still a problem since these options are prohibited by law in the country. This highlights the current limitations on training these models in large corpus and applying them where country-related policies are enforced. In other words, concerning health practices, the knowledge acquired globally can be inadequate in country-specific settings. Nevertheless, the studies on the capabilities of PLMs in exams are not limited to the health domain. Nunes et al. (2023) evaluated ChatGPT and GPT-4 using ENEM, a multidisciplinary Brazilian university admission exam. The authors tested different types of prompts (zero, few-shot, and few-shot with chain of thought). Results show high overall performance in the 2022 exam with zero-shot performance. However, as discussed before, in specific areas with mathe- matical/numerical reasoning, the results are lower. In these cases, chain-of-thought prompts achieve a better perfor- mance since they force the model to reason over the result of the output. GPT models were also evaluated in the exams from other domains such as programming. The authors in Savelka, Agarwal, Bogart, Song, and Sakr (2023) evaluate the capabilities of GPT-3.5 in programming courses where the questions were a combination of multiple-choice questions (MCQ) and practical coding exercises. GPT- 3.5 answers correctly to 64.3% of the MCQ. However, since the requirement for passing the courses was 70% correct answers in each test, GPT-3.5 failed the overall courses. Nevertheless, GPT-3.5 was capable of passing some of the modules of the courses with an accuracy above 70%. An important highlight from the authors in this and a similar paper (Savelka, Agarwal, Bogart, & Sakr, 2023) is the lack of ability of GPT-3.5 to deal with code embedding in MCQ since there is a difference of approximately 12% accuracy between no-code and code-embedded MCQ. How- ever, with the release of GPT-4, some of these limitations have been overcome. As a matter of fact, GPT-4 achieves a passing grade in all modules of the course, surpassing GPT-3.5 on average by 20% accuracy (Savelka, Agarwal, An, Bogart, & Sakr, 2023). Yet, the authors also state that GPT-3.5 and 4 struggle with more complex practical 6of10 GUIMARÃES ET AL. coding exercises. Although GPT-4 performs better in these exercises, it seems to struggle in the same ones as GPT-3.5. Practical exercises regarding debugging, refactoring, testing and packaging, and data analysis seem to be the most challenging, with GPT-4 not able to achieve 50% of the points in a first iteration. Therefore, there are still some limitations that affected previous iterations of GPT models in this task that still hold to a certain degree in GPT-4. PLMs are also being explored in other programming-related scenarios like code explanation (MacNeil et al., 2022) and bug-fixing (Sobania et al., 2023). Another scenario where PLMs are able to achieve impressive results is in the legal domain (Ganguly et al., 2023). First, GPT-4 was able to pass the Uniform Bar Exam, a precondition exam in the United States to practice law. GPT-4 was able to achieve a better performance than previous models (like ChatGPT) achieving an accuracy of 75.7% in MCQ, an overall score of 4.2 in 6 in open questions (essays), and 4.2 in 6 in two questions concerning complex family, criminal, and ethical laws (Katz et al., 2023). Another study (Savelka, 2023) evaluates the capability of ChatGPT in the semantic annotation of legal documents. The authors concluded that zero-shot prompting in ChatGPT is compet- itive when compared with LLMS fine-tuned for the task, which may be useful in scenarios where there is a lack of annotations available. Being a recent technology and with new and improved LLMs and PLMs released on a monthly basis, the possible applications are still hard to quantify since a large number of researchers are still studying how can PLMs like GPT-4 and ChatGPT assist and reduce the workload associated with some domain-specific tasks. Additional examples include chemistry-related tasks such as molecule captioning and reagent recommendation (Guo et al., 2023), data analysis tasks (Cheng et al., 2023), hateful content (Wang et al., 2023) and misinformation (Pelrine et al., 2023) detection, grammatical error correction (Loem et al., 2023), and post-editing translation (i.e., rectifying errors in neural machine translation; Raunak et al., 2023). 5 | DISCUSSION AND FINAL REMARKS The quick rise of PLMs and their remarkable effectiveness in several NLP tasks, has led the way for the continuous development of new LLMs in the domain of Computer Science and Artificial Intelligence. However, the introduction of ChatGPT to mainstream audiences has sparked interest in this technology in several other areas, exponentially increas- ing the number of studies on the possible uses and capabilities of PLMs in different domains. The capabilities (in an off-the-shelf scenario) of these are worth discussing in order to understand their achievements in real-world cases such as their ability to pass multiple exams and be a useful assistant in several domain tasks. For example, factual knowledge and commonsense reasoning are two of the most important capabilities to ensure that PLMs are able to pass several exams in different domains. Nevertheless, the fact that these models have limitations in numerical reasoning has an impact on their effectiveness, namely in more complex mathematical problems or in practical programming exercises. When PLMs are evaluated as tutors/professors, they are capable of providing explanations of the correct answer, aiding students to understand why the answer they selected is incorrect. In this context, we can see the influence of the theory of mind capabilities of PLMs since they must be capable of putting themselves in the role of the student and understand why they choose the incorrect answer (Nori et al., 2023). In summary, these individual capabilities in PLMs make them excel in different real-world tasks. Thus, the more these capabilities are improved, the more PLMs would achieve better results in these scenarios. However, the development of PLMs has been majorly focusing on increasing the training cor- pus and not necessarily on increasing commonsense capabilities. These often emerge spontaneously and are later dis- covered (Kosinski, 2023). It is unclear at this point whether the constant increase in the data will improve further the capabilities of PLMs in, for example, complex mathematical reasoning and theory of mind. In addition, the most recent GPT model,4 GPT-4 still suffers from hallucinations (i.e., creates a confident and assertive, but wrong answer) and rea- soning errors (OpenAI, 2023). Thus, although PLMs have already had a tremendous impact worldwide and have been thoroughly evaluated by the scientific community using different benchmarks (Hendrycks et al., 2021; Lin et al., 2022; Zellers et al., 2019; Clark et al., 2018; Gao et al., 2021), they still have limitations that require human validation in the majority of real-world applications. Therefore, there are still research opportunities for the development of methods to improve commonsense capabilities in LLMs beyond increasing the training data and the number of parameters (for example, by combining LLMs with symbolic reasoning layers (Garcez et al., 2019, Zhang et al., 2022). Thus, it is our belief that the current limitations and new methods for the integration of commonsense inference in LLMs should be an important discussion point in the development and evaluation of future releases of intelligent systems based on pre- trained language models. GUIMARÃES ET AL. 7of10 AUTHOR C O NTRIBUTIO NS Nuno Guimar˜aes: Conceptualization (supporting); investigation (lead); writing – original draft (lead); writing – review and editing (supporting). Ricardo Campos: Supervision (supporting); writing – review and editing (equal). Alípio Jorge: Conceptualization (lead); supervision (lead); writing – review and editing (equal). ACKNO WLEDGMENTS This work is financed by National Funds through the FCT—Fundaç˜ao para a Ciência e a Tecnologia, I.P. (Portuguese Foundation for Science and Technology) within the project StorySense, with reference 2022.09312.PTDC. This work is co-financed by Component 5—Capitalization and Business Innovation, integrated in the Resilience Dimension of the Recovery and Resilience Plan within the scope of the Recovery and Resilience Mechanism (MRR) of the European Union (EU), framed in the Next Generation EU, for the period 2021–2026, within project HfPT, with reference 41. CONFLICT OF INTEREST STATEMENT The authors declare no conflicts of interest. DATA AVAILABILITY STATEM ENT Data sharing is not applicable to this article as no new data were created or analyzed in this study. ORCID Nuno Guimar˜aes https://orcid.org/0000-0003-2854-2891 R EL A TE D WIR Es AR TI CL ES Text-based question answering from information retrieval and deep neural network perspectives: A survey A review on multimodal zero-shot learning ENDNOTES 1 https://openai.com/blog/chatgpt/ 2 Exact details of the specific corpora and datasets used for training these recent models have not been disclosed 3 https://openai.com/blog/chatgpt-plugins 4 Up to the date of writing REFERE NCES Baum, L. E., & Petrie, T. (1966). Statistical inference for probabilistic functions of finite state Markov chains. The Annals of Mathematical Statistics, 37(6), 1554–1563. Bian, N., Han, X., Sun, L., Lin, H., Lu, Y., & He, B. (2023). ChatGPT is a knowledgeable but inexperienced solver: An investigation of com- monsense problem in large language models. http://arxiv.org/abs/2303.16421 Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language models are few-shot learners. https://arxiv.org/abs/2005.14165 Cheng, L., Li, X., & Bing, L. (2023). Is GPT-4 a good data analyst? http://arxiv.org/abs/2305.15038 Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., & Tafjord, O. (2018). Think you have solved question answering? Try ARC, the AI2 reasoning challenge. https://arxiv.org/abs/1803.05457 Cong, Y. (2022). Psycholinguistic diagnosis of language models' commonsense reasoning. In Proceedings of the first workshop on common- sense representation and reasoning (CSRR 2022) (pp. 17–22). Association for Computational Linguistics. Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding. NAACL HLT 2019–2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, 1(Mlm):4171–4186. Frith, C., & Frith, U. (2005). Theory of mind. Current Biology, 15(17), R644–R645. Ganguly, D., Conrad, J. G., Ghosh, K., Ghosh, S., Goyal, P., Bhattacharya, P., Nigam, S. K., & Paul, S. (2023). Legal ir and nlp: The history, challenges, and state-of-the-art. In J. Kamps, L. Goeuriot, F. Crestani, M. Maistro, H. Joho, B. Davis, C. Gurrin, U. Kruschwitz, & A. Caputo (Eds.), Advances in information retrieval (pp. 331–340). Springer Nature Switzerland. Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., & Zou, A. (2021). A framework for few-shot language model evaluation. https://doi.org/10.5281/ zenodo.5371629 8of10 GUIMARÃES ET AL. Garcez, A. d., Gori, M., Lamb, L. C., Serafini, L., Spranger, M., & Tran, S. N. (2019). Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. http://arxiv.org/abs/1905.06088 Guo, T., Guo, K., Nan, B., Liang, Z., Guo, Z., Chawla, N. V., Wiest, O., & Zhang, X. (2023). What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. http://arxiv.org/abs/2305.18365 Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). Measuring massive multitask language under- standing. Proceedings of the International Conference on Learning Representations (ICLR). International Conference on Learning Representations. Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2020). How can we know when language models know? CoRR, abs/2012.00955. https://arxiv.org/ abs/2012.00955 Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). How can we know when language models know? On the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9, 962–977. Kaneda, Y., Takahashi, R., Kaneda, U., Akashima, S., Okita, H., Misaki, S., Yamashiro, A., Ozaki, A., & Tanimoto, T. (2023). Assessing the performance of GPT-3.5 and GPT-4 on the 2023 Japanese nursing examination. Cureus, 15(8), e42924 Publisher: Cureus, Inc. Kasai, J., Kasai, Y., Sakaguchi, K., Yamada, Y., & Radev, D. (2023). Evaluating GPT-4 and ChatGPT on Japanese medical licensing examina- tions. http://arxiv.org/abs/2303.18027 Kassner, N., & Schütze, H. (2020). Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceed- ings of the 58th annual meeting of the Association for Computational Linguistics (pp. 7811–7818). Computational Linguistics. Katz, D. M., Bommarito, M. J., Gao, S., & David Arredondo, P. (2023). GPT-4 Passes the Bar Exam. https://doi.org/10.2139/ssrn.4389233 Kosinski, M. (2023). Theory of mind may have spontaneously emerged in large language models. https://arxiv.org/abs/2302.02083 Kung, T. H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., Madriaga, M., Aggabao, R., Diaz-Candido, G., Maningo, J., & Tseng, V. (2023). Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLoS Digital Health, 2(2), e0000198. Lafferty,J.D., McCallum,A., & Pereira,F.C. N.(2001). Conditionalrandom fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the eighteenth international conference on machine learning, ICML ‘01 (pp. 282–289). Morgan Kaufmann Publishers Inc. Lamichhane, B. (2023). Evaluation of ChatGPT for NLP-based mental health applications. http://arxiv.org/abs/2303.15727 Levine, D. M., Tuwani, R., Kompa, B., Varma, A., Finlayson, S. G., Mehrotra, A., & Beam, A. (2023). The diagnostic and triage accuracy of the gpt-3 artificial intelligence model. medRxiv. https://www.medrxiv.org/content/10.1101/2023.01.30.23285067v1 Lin, B. Y., Lee, S., Khanna, R., & Ren, X. (2020). Birds have four legs?! NumerSense: Probing numerical commonsense knowledge of pre- trained language models. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP) (pp. 6862– 6868, Online). Association for Computational Linguistics. Lin, S., Hilton, J., & Evans, O. (2022). TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th annual meet- ing of the Association for Computational Linguistics (volume 1: Long papers) (pp. 3214–3252). Ireland. Association for Computational Linguistics. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2022). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys. Liu, Z., Yu, X., Zhang, L., Wu, Z., Cao, C., Dai, H., Zhao, L., Liu, W., Shen, D., Li, Q., Liu, T., Zhu, D., & Li, X. (2023). DeID-GPT: Zero-shot medical text de-identification by GPT-4. http://arxiv.org/abs/2303.11032 Loem, M., Kaneko, M., Takase, S., & Okazaki, N. (2023). Exploring effectiveness of GPT-3 in grammatical error correction: A study on performance and controllability in prompt-based methods. In Proceedings of the 18th workshop on innovative use of NLP for building educational applications (BEA 2023) (pp. 205–219). Toronto, Canada. Lu, P., Qiu, L., Yu, W., Welleck, S., & Chang, K.-W. (2023). A survey of deep learning for mathematical reasoning. In Proceedings of the 61st annual meeting of the Association for Computational Linguistics (volume 1: Long papers) (pp. 14605–14631). Association for Computa- tional Linguistics. Lu, Y., Wu, H., Qi, S., & Cheng, K. (2023). Artificial intelligence in intensive care medicine: Toward a ChatGPT/GPT-4 way? In Annals of bio- medical engineering. Springer. Lyu, Q., Tan, J., Zapadka, M. E., Ponnatapura, J., Niu, C., Myers, K. J., Wang, G., & Whitlow, C. T. (2023). Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Results, limitations, and potential. Visual Computing for Industry, Biomedi- cine, and Art, 6(1), 9. MacNeil, S., Tran, A., Mogil, D., Bernstein, S., Ross, E., & Huang, Z. (2022). Generating diverse code explanations using the GPT-3 large language model. In ICER 2022 - proceedings of the 2022 ACM conference on international computing education research (Vol. 2, pp. 37–39). Association for Computing Machinery, Inc. Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. 1st international confer- ence on learning representations, ICLR 2013, Scottsdale, Arizona, USA, May 2–4, 2013, Workshop Track Proceedings. Nori, H., King, N., McKinney, S. M., Carignan, D., & Horvitz, E. (2023). Capabilities of GPT-4 on medical challenge problems. http://arxiv. org/abs/2303.13375 Nunes, D., Primi, R., Pires, R., Lotufo, R., & Nogueira, R. (2023). Evaluating GPT-3.5 and GPT-4 models on Brazilian university admission exams. http://arxiv.org/abs/2303.17003 OpenAI. (2023). GPT-4 technical report. Technical report, OpenAI. http://arxiv.org/abs/2303.08774 GUIMARÃES ET AL. 9of10 Pelrine, K., Reksoprodjo, M., Gupta, C., Christoph, J., & Rabbany, R. (2023). Towards reliable misinformation mitigation: Generalization, uncertainty, and GPT-4. http://arxiv.org/abs/2305.14928 Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. In Proceedings of the 2018 conference of the north American chapter of the Association for Computational Linguistics: Human language technologies, volume 1 (long papers) (pp. 2227–2237). Association for Computational Linguistics. Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., & Miller, A. (2019). Language models as knowledge bases? In Proceed- ings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) (pp. 2463–2473). Association for Computational Linguistics. Premack, D., & Woodruff, G. (1978). Does the chimpanzee have a theory of mind? Behavioral and Brain Sciences, 1(4), 515–526. Raunak, V., Sharaf, A., Awadallah, H. H., & Menezes, A. (2023). Leveraging GPT-4 for automatic translation post-editing. http://arxiv.org/ abs/2305.14878 Redford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. technical report, OpenAI. Savelka, J. (2023). Unlocking practical applications in legal domain: Evaluation of GPT for zero-shot semantic annotation of legal texts. http://arxiv.org/abs/2305.04417 Savelka, J., Agarwal, A., An, M., Bogart, C., & Sakr, M. (2023). Thrilled by your progress! Large language models (GPT-4) No longer struggle to pass assessments in higher education programming courses. https://arxiv.org/abs/2306.10073 Savelka, J., Agarwal, A., Bogart, C., & Sakr, M. (2023). Large language models (GPT) struggle to answer multiple-choice questions about code. http://arxiv.org/abs/2303.08033 Savelka, J., Agarwal, A., Bogart, C., Song, Y., & Sakr, M. (2023). Can generative pre-trained transformers (gpt) pass assessments in higher education programming courses? In Proceedings of the 2023 conference on innovation and Technology in Computer Science Education V. 1, ITiCSE 2023 (pp. 117–123). Association for Computing Machinery. Singh, R., Fedorenko, E., Mahowald, K., & Gibson, E. (2016). Accommodating presuppositions is inappropriate in implausible contexts. Cognitive Science, 40(3), 607–634. Sobania, D., Briesch, M., Hanna, C., & Petke, J. (2023). An analysis of the automatic bug fixing performance of chatgpt. In 2023 IEEE/ACM international workshop on automated program repair (APR) (pp. 23–30). IEEE Computer Society. Stolfo, A., Jin, Z., Shridhar, K., Schölkopf, B., & Sachan, M. (2023). A causal framework to quantify the robustness of mathematical reasoning with language models. In A. Rogers, J. L. Boyd-Graber, & N. Okazaki (Eds.), Proceedings of the 61st annual meeting of the Association for Computational Linguistics (volume 1: Long papers), ACL 2023, Toronto, Canada, July 9–14, 2023 (pp. 545–561). Association for Computa- tional Linguistics. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in neural information processing systems, volume 30. Curran Associates, Inc. Wang, H., Hee, M. S., Awal, M. R., Choo, K. T. W., & Lee, R. K.-W. (2023). Evaluating gpt-3 generated explanations for hateful content moderation. In Proceedings of the international joint conference on artificial intelligence (IJCAI). International Joint Conferences on Artifi- cial Intelligence. Weir, N., Poliak, A., & Van Durme, B. (2020). Probing neural language models for human tacit assumptions. In 42nd annual virtual meeting of the cognitive. Science Society (CogSci). Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., & Choi, Y. (2019). Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Zhang, H., Li, L. H., Meng, T., Chang, K.-W., & Broeck, G. V. d. (2022). On the paradox of learning to reason from data. Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence IJCAI-23. International Joint Conferences on Artificial Intelligence. Zhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song, L., Zhan, M., & Li, H. (2023). Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. https://arxiv.org/abs/2308.07921 Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations. Watching Movies and Reading Books http://arxiv.org/abs/1506.06724 How to cite this article: Guimar˜aes, N., Campos, R., & Jorge, A. (2024). Pre-trained language models: What do they know? WIREs Data Mining and Knowledge Discovery, 14(1), e1518. https://doi.org/10.1002/widm.1518 10 of 10 GUIMARÃES ET AL.","libVersion":"0.3.2","langs":""}
{"path":"lit/sources/Beinert23PowerFlowForecasts.pdf","text":"Journal Pre-proof Power flow forecasts at transmission grid nodes using Graph Neural Networks Dominik Beinert, Clara Holzhüter, Josephine M. Thomas, Stephan Vogt PII: S2666-5468(23)00034-4 DOI: https://doi.org/10.1016/j.egyai.2023.100262 Reference: EGYAI 100262 To appear in: Energy and AI Received date : 27 December 2022 Revised date : 29 March 2023 Accepted date : 5 April 2023 Please cite this article as: D. Beinert, C. Holzhüter, J.M. Thomas et al., Power flow forecasts at transmission grid nodes using Graph Neural Networks. Energy and AI (2023), doi: https://doi.org/10.1016/j.egyai.2023.100262. This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. © 2023 Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).JournalPre-proof Journal Pre-proof Power Flow Forecasts at Transmission Grid Nodes Using Graph Neural Networks Dominik Beinerta,∗, Clara Holzh¨uter a,b,∗, Josephine M. Thomas b, Stephan Vogtb aFraunhofer Institute for Energy Economics and Energy System Technology IEE, Joseph-Beuys-Straße 8, Kassel, 34117, Germany bIntelligent Embedded Systems Lab, University of Kassel, Wilhelmsh¨oher Allee 73, Kassel, 34121, Germany Abstract The increasing share of renewable energy in the electricity grid and pro- gressing changes in power consumption have led to fluctuating, and weather- dependent power flows. To ensure grid stability, grid operators rely on power forecasts which are crucial for grid calculations and planning. In this paper, a Multi-Task Learning approach is combined with a Graph Neural Network (GNN) to predict vertical power flows at transformers connecting high and extra-high voltage levels. The proposed method accounts for local differences in power flow characteristics by using an Embedding Multi-Task Learning ap- proach. The use of a Bayesian embedding to capture the latent node charac- teristics allows to share the weights across all transformers in the subsequent node-invariant GNN while still allowing the individual behavioral patterns of the transformers to be distinguished. At the same time, dependencies be- tween transformers are considered by the GNN architecture which can learn relationships between different transformers and thus take into account that power flows in an electricity network are not independent from each other. The effectiveness of the proposed method is demonstrated through evaluation on two real-world data sets provided by two of four German Transmission System Operators, comprising large portions of the operated German trans- ∗Both authors contributed equally. Dominik Beinert mainly contributed the Multi-Task Approach and Clara Holzh¨uter mainly contributed the GNN Approach Email addresses: dominik.beinert@iee.fraunhofer.de (Dominik Beinert), clara.juliane.holzhueter@iee.fraunhofer.de (Clara Holzh¨uter) Preprint submitted to Elsevier March 29, 2023 Manuscript Click here to view linked ReferencesJournalPre-proof Journal Pre-proof mission grid. The results show that the proposed Multi-Task Graph Neural Network is a suitable representation learner for electricity networks with a clear advantage provided by the preceding embedding layer. It is able to cap- ture interconnections between correlated transformers and indeed improves the performance in power flow prediction compared to standard Neural Net- works. A sign test shows that the proposed model reduces the test RMSE on both datasets compared to the benchmark models significantly. Keywords: Power Flow Forecasting, Graph Neural Network, Graph Convolutional Network, Embedding Multi-Task Learning 1. Introduction The expansion of renewable energies and an increasing number of new consumers, such as electric vehicles, contribute to an increased volatility in power grids. This poses challenges to grid operators because power flows are becoming harder to predict. In the past, power production used to be planned according to the expected consumption needs, but nowadays, it is also largely dependent on local weather and thus heavily fluctuating. At the same time, power consumption is changing due to the growth of e-mobility, batteries, and heat pumps, as well as trends in individual consumption behavior like the increasing occurrence of home office. 1.1. Problem Statement Vertical power flow denotes the power being transferred between two volt- age levels in the electric grid measured at a transformer station. For grid operators, they are an essential input to grid calculations, which indicate grid congestion, allowing the operator to initiate countermeasures and thus maintain grid stability. Forecasts of vertical power flows are required for better planning and earlier identification of possible grid congestion. While separate machine learning models can be trained for each trans- former to predict its vertical power flow, the use case poses further challenges. The vertical power flows at transformers are not independent of each other but are interconnected due to the meshed power grid such that a dynamic balancing behavior of power flows can be observed. In contrast to standard 2JournalPre-proof Journal Pre-proof Neural Networks, Graph Neural Networks (GNNs) can consider the intercon- nection between network components and leverage them to make predictions. Therefore, GNNs are particularly suitable for the task at hand. 1.2. Contribution For accurate predictions of vertical power flows, a global model like a GNN is required to model the relations of power flows at different transformers in the power grid. By interpreting transformers as nodes in a graph, GNNs can take into account dependencies and balancing of power flows through message passing between nodes via the edges. The main contribution of this paper is the novel combination of a Bayesian Multi-Task Embedding with a GNN architecture for power flow prediction that combines the following three aspects: • Vertical Power flow at different transformers with individual latent characteristics, such as a specific share of renewable power plants and load characteristic, can be predicted by only one model using an Em- bedding Multi-Task Learning approach. • The model generates forecasts of the vertical power flow at transformers taking into account relevant information such as weather conditions at neighboring transformers. • Changes of how power flows at different transformers influence each other can be handled by an attention mechanism. The paper aims to examine the proposed method for day-ahead forecasts of vertical power flows on real data sets provided by two German Transmis- sion System Operators (TSO) and evaluate it against a local model bench- mark. Here, local refers to a model that makes predictions for each trans- former based only on local features of that single transformer, such as the wind speed at its location. This is done by, e.g., standard Neural Networks. The research questions to be answered are: 1. Can the combined architecture based on GNNs and Multi-Task Learn- ing improve the prediction performance of vertical power flow forecasts compared to benchmark models based on only one of these approaches? 2. Can the proposed method learn relations between transformers in a dy- namically changing power grid so that it correctly predicts the power flow for nodes that are influenced by the behavior of nearby transform- ers? 3JournalPre-proof Journal Pre-proof 1.3. Related Work Power Flow approximation is required for a secure operation of the power grid in order to avoid blackouts. Security analysis typically relies on load flow solvers. Such methods estimate the power flow of transmission lines considering the physical constraints of the power grid. They are typically probabilistic Monte-Carlo methods using Newton-Raphson optimization to minimize the mismatch between the in- and outgoing power of grid nodes [1]. Since such solvers are computationally expensive, several machine learning- based approaches have been proposed to speed up computations. Duchesne et al. [2], for example, combine Neural Networks with variance reduction techniques to accelerate a Monte Carlo approach. Other Neural Network approaches are presented in, e.g., [3] or [4]. More traditional machine learning approaches have also been applied by, e.g. Yu et al. [5], who use support vector regression to learn the relationships of variables in the power flow equations. In contrast to the approach proposed in this paper, these methods do not take the grid structure and thus the interaction of network components into account. A similar use case, i.e., vertical power flow prediction, is proposed in [6]. The authors Brauns et al. use a Long Short Term Memory (LSTM) approach to incorporate time series data, which comes with additional computational complexity. However the focus of the approach differs significantly from the approach presented in this paper, since the authors perform daily retraining of their local models to account for dynamic changes in time series charac- teristics. This is an alternative solution to the problem at hand. When using local models at each transformer, events at other transformers cannot be used directly to adjust model predictions. Instead, the models are retrained on new data which is affected by the mentioned events. In contrast, the ap- proach proposed in this paper is a global model that is able to include events at other locations. Thus, a retraining of a global model is not necessarily required. Further differences in the approach by Brauns et al. are the longer forecast horizon of 48 hours and a difference of the data set, which contains power flows between the medium and high voltage grid. To consider that components in the power grid are not independent but highly correlated, several GNN approaches for power flow calculation have been proposed. Some of them focus on specific areas of power forecast- ing such as wind or solar power prediction ([7], [8], [9], [10]). Accordingly, these approaches tackle a related use case of renewable energies, but at an- other level, i.e.,they directly estimate the production and not the power flow. 4JournalPre-proof Journal Pre-proof General power flow approximation has been approached using GNNs as well. However, only a few works have been proposed so far. Bolz et al. [11], for example, apply a spatial Graph Convolutional Network (GCN) to approxi- mate the loadings of lines based on reactive and active power flow. A similar use case is presented by Wang et al. [12] who use Cheb-Net to learn the distribution characteristics of power flow. Kundacina et al. [13] propose a similar approach that approximates the voltage angle and magnitude using a spectral GNN based on current measurements. All of these methods out- perform the compared traditional methods [14]. Compared to our approach, these methods rather focus on the physical aspects and constraints of power flow approximation. Most of them take only technical variables such as re- active and active power flow into account. However, they show the potential of GNNs for such tasks since they are less computationally expensive than traditional methods, such as linear state estimators and solvers, while be- ing more flexible than standard Neural Networks [14]. Unlike the approach presented in this paper, the input data for these approaches consider buses, loads and generators explicitly. The above mentioned methods are to some extent related to physics in- spired methods that use physical contraints such as Kirchhoffs law of the power grid as training loss for a neural network to learn the prediction of power flows. Such methods have been combined with GNNs as well ([15, 16, 17]). Although GNNs have proven their applicability to the task of power flow estimation, these approaches are not comparable to the proposed approach, since they optimize a completely different target. For the related use case of grid voltage prediction in the distribution grid, Fusco et al. [18] use a Gaussian graphical model expressed by a GNN, which incorporates domain knowledge as well a physical constraints. It focuses more on grid congestion and market bidding. To the best of the authors’ knowledge, no other work proposed a Multi- Task GNN approach that embeds latent attributes of individual transformer stations and further processes the input data using a GNN to estimate the corresponding load flow based only on weather, price and consumption fore- cast and calendar information. 1.4. Outline The paper is organized as follows: Section 2 provides details on the be- havior of power flows in the electric grid. The subsequent section 3 covers 5JournalPre-proof Journal Pre-proof the proposed model consisting of a GNN method combined with task embed- dings. In section 4, the data set and experimental setup, including model and training parameters, are described. Section 5 presents the obtained results and discusses them. Finally, a short conclusion is given in section 6. 2. Power Flows in the Electric Grid The increasing amount of renewable energy sources has drastically changed the power flows in the electric grid. While in the past, most of the power generation took place at the extra-high voltage level, nowadays, much energy is fed into the distribution grid at lower voltage levels. In this section, the particularities of power flows at transformers are discussed as well as the implications for a suitable model. 2.1. Interactions between transformers The power grid has a meshed structure, meaning that there are redun- dant lines and, thus, multiple paths for electricity to flow, making the grid more flexible as electricity can be redirected. Due to this meshed structure, the path the power takes when traveling between its point of generation or consumption and the extra-high voltage level can change dynamically, which results in changing characteristics of the vertical power flow. Typical events triggering such changes can be power grid switching or maintenance of sin- gle transformers. This behavior can particularly be observed in transformers situated at the same substation. Fig. 1 shows the vertical power flows at two such transformers between the high and extra-high voltage grid. When one transformer is inactive, i.e., its power flow measurements are zero, the other takes over. Consequently, its power flow differs visibly from periods of simultaneous activity. While this kind of interaction can be seen clearly in time series plots, the balancing behavior of power flows in the electric grid is typically less obvious. Especially in the lower voltage levels, events can be hard to detect and even harder to model, as detailed knowledge about the grid topology is required. 6JournalPre-proof Journal Pre-proof 2019-05 2019-06 2019-07 2019-08 2019-09 2019-10 2019-11 2019-12 0.6 0.4 0.2 0.0power [normalized] measurement node 157 measurement node 158 Figure 1: Vertical power flows of two transformers at one substation with visible interaction after switching node 158 into operation in august 2019. The positive sign denotes power transmitting from the high into the extra-high voltage grid, whereas the negative sign refers to power transmitting from the extra-high into high voltage grid. 2.2. Transformer-specific behavior The topology of the distribution grid and the location and attributes of power plants and consumers contribute to the specific power flow character- istics of different transformers. Vertical power flows can be understood as the sum of generated and consumed power in the lower voltage levels. As seen in Fig. 2, the specific mix of production and consumption can be esti- mated from the power flow time series. While one vertical power flow shows a typical behavior of power consumption, which follows a periodical diurnal cycle with negative consumption peaks during mornings and evenings, the other is strongly influenced by power production, i.e. most of the time power is fed from the high voltage level into the extra-high voltage level, which is denoted by a positive sign. The energy sources contributing to this power flow are not known, but likely include wind power due to the high volatility of the power flow. 2.3. Implications for a forecast model A physical model approach for power flow forecasts is usually unfeasible or limited by strong simplifications, as it requires local modeling of all possible power generation and consumption types at any time, as well as complete knowledge of the transmission and distribution grid topology, including all connected consumers and generators. For a machine learning approach, this means that it is not sufficient to treat all transformers’ behavior and attributes identically. Instead, the indi- 7JournalPre-proof Journal Pre-proof 2019-06-29 2019-07-01 2019-07-03 2019-07-05 2019-07-07 2019-07-09 2019-07-11 0.25 0.00 0.25 0.50power [normalized] measurement node 146 measurement node 147 Figure 2: Vertical power flows with a consumption characteristic at transformer node 146 and a mixed characteristic with mainly power generation at transformer node 147. Both transformers are situated at the same location. The positive sign denotes power transmitting from the high into the extra-high voltage grid, whereas the negative sign refers to power transmitting from the extra-high into high voltage grid. vidual, local properties of each transformer must be considered, for example, by means of an individual model per transformer. Another option is a multi- task learning model to allow for differences while leveraging similarities. However, to include the interactive behavior of the electric grid in power flow forecasts, the model needs to consider neighboring node interactions. In this paper, a combination of a multi-task learning model and a GNN is proposed. The multi-task approach accounts for the individual behavior of the transformer, whereas the GNN takes the information of neighboring transformers into account to model the dependencies between transformers. Furthermore, the goal is a mainly data-driven method to be independent of grid topology models as an input. For TSOs, this can pose an advantage, as it does not require other grid operators to provide grid topology and switching state information in lower voltage levels. In summary, the interactive behavior displayed in Fig. 1 demands for a global GNN model approach where information of neighboring nodes is made available. On the other hand, the different specific time series characteristics as seen in Fig. 2 require the model to distinguish between nodes, which in this paper is achieved by the Bayesian Task Embedding. This additional aspect has to be considered as the graph in this use case does not contain a complete replica of the real electric grid with every single power plant, consumer and power line. Instead, the model must be able to predict different types of time series behavior from local input features. In fact, the two nodes in Fig. 2 are 8JournalPre-proof Journal Pre-proof located at exactly the same location at the same substation, which means that the same input values will be provided to the model for both nodes. While it is still possible to consider varying node behaviour in a GNN by message passing, i.e., if the nodes have different connections, relying on it will most likely not yield good results. In the example of node 147 in Fig. 2, wind power is generated by near-by wind farms, so the local wind speed will likely be higher correlated to the target time series than the wind speeds at other nodes that are available due to message passing. By integrating a Bayesian Task Embedding approach, the model is allowed to use local input variables differently depending on the specific node. 3. Graph Neural Networks With Multi-Task Embedding The method proposed in this paper combines a GNN with a Bayesian Em- bedding Multi-Task Learning approach, enabling the model to learn both in- teractions between nodes and node-specific time series characteristics. Given inputs Xt ∈ Rd×n with d features at n nodes for a specific timestamp t, the model will predict the vertical power flows yt ∈ R 1×n. For training, a data set (X, y) = (Xt, yt)t∈T is provided, where T denotes the given time range of training data. 3.1. Graph Convolutional Networks (GCNs) Compared to standard Neural Networks, the usage of GNNs to predict power flow at nodes in the electricity network has a theoretical advantage: the information exchange between connected nodes. Note that the mentioned information exchange is invariant of the absolute node position in the graph. For standard Neural Networks, the input samples are treated as independent samples. For the given use case and data set, this means that the transform- ers would be assumed to be independent of each other. However, this is not the case since the power flow depends on several features that can be corre- lated across different transformers. On one hand, features such as wind speed or solar irradiance are locally correlated, i.e., transformers at close locations will have a similar weather forecast. On the other hand, the transformers are directly correlated in the sense that they show compensatory behavior, e.g., if a transformer close by is inactive due to maintenance. GNNs are designed for such data and enable the propagation of information through the graph via the edges. Therefore, they can model relationships between nodes in the 9JournalPre-proof Journal Pre-proof graph. More specifically, GNNs operate on graphs G = (E, V ) where E denotes the edges and V the nodes. The attributes of node v are represented as a feature vector. Accordingly, the set of all nodes is represented as a feature matrix of shape d × n where n is the number of nodes, and d is the dimension of the features. The edges are represented as an adjacency matrix A of shape n × n in which Au,v = 1 if node u and v are connected and Au,v = 0 otherwise. In the case of a weighted graph, Au,v is set to the weight associated with the edge between node u and v. Due to the great success of Convolutional Neural Networks (CNNs), Graph Convolutional Networks (GCNs) have been developed to generalize convolu- tions to graph data. In the proposed approach, spatial graph convolutions are used, which resemble the standard convolution such that in each layer, each node v aggregates the features of its neighbors, i.e., adjacent nodes. Similar to standard CNNs in which filters are learned, GCNs aggregate neighboring features using learnable weight matrices as well. Since each node aggregates information from direct neighbors in each layer, the receptive field size of the network increases by one with each layer [19]. This way, the GNN can learn a high-level node representations which cover the interaction with a larger portion of the underlying electrical grid. The following exemplary basic spatial graph convolution as defined in, e.g., [20] is given by: h (l+1) u = σ(W1h (l) u + W2 ∑ v∈N (u) h (l) v ). (1) h (l) u ∈ R d(l) denotes the d-dimensional node representation of node u in layer l and N (u) describes the neighborhood of node u. W1 and W2 are shared learnable weight matrices and σ is an activation function such as ReLU. In the first layer, hu is the input features xu of node u. In this graph convo- lution, every neighbor is weighted equally in the neighborhood aggregation. Using the adjacency matrix and notation H (l) = (h (l) 1 , ..., h(l) n ), Eq. (1) can be written in matrix form as multiplication with the adjacency matrix A described above: H (l+1) = σ(W1H (l) + W2H (l)A) (2) Note that A does not contain self-loops which means that the diagonal is set to zero. The features of the target node itself are transformed using matrix W1. To learn individual weights for each neighbor, basic graph convolutions can be equipped with attention mechanisms as presented in [21]. The new 10JournalPre-proof Journal Pre-proof node representation for a node i changes Eq. (1) as follows: h (l+1) u = σ(W1h (l) u + ∑ v∈N (u) αu,vW2h (l) v ). (3) In contrast to the standard convolution from Eq. (1), every neighbor v is weighted by attention coefficient αu,v, which is computed individually for each pair of nodes. The coefficient, which represents the importance of one node to another, is computed as follows: αu,v = softmax ( (W3hu) T W4hv√dl ) (4) where dl is the number of outputs for the hidden layer l, i.e., the hidden dimension and ∑ v∈V αu,v = 1. Although α is usually different for each node and neighbor, the weights used to compute α are shared across all nodes. To include edge features in this attention mechanism, the equation changes as follows: h (l+1) u = σ(W1h (l) u + ∑ v∈N (u) αu,v(W2h (l) v + W5eu,v)). (5) eu,v denotes the weight associated with the edge between node u and v and W5 corresponds to a learnable weights matrix also used to compute α with edge weights included: αu,v = softmax ((W3hu)T (W4hvW5eu,v) √dl ) . (6) To learn different relations between nodes in one layer, multi-head at- tention can be used. For this purpose, the attention mechanism is applied multiple times, each learning its individual weights. The resulting feature maps, called attention heads, are then concatenated afterwards [21]. The concept is similar to producing multiple feature maps in standard CNNs. Stacking multiple convolutional layers builds a GCN that outputs a graph with new node representations that can be used to perform specific prediction tasks such as node regression or classification. For this purpose, the node embeddings can be used directly by applying a suitable activation function, or they are further transformed by a suitable readout network. Typically, such a network consists of a set of fully connected layers that transform each node separately. 11JournalPre-proof Journal Pre-proof 3.2. Embedding Multi-Task Learning Multi-Task Learning (MTL) refers to a machine learning setting in which a model is trained to solve multiple similar problems, also referred to as tasks. The goal is to use the combined knowledge of all tasks during training such that all tasks benefit from each other. In contrast to GNNs, an MTL model does not exchange current information between tasks when making predictions. One possible way to share knowledge during training is the concept of hard parameter sharing, where a certain part of model parameters is shared between all tasks while other parameters are trained specifically for each task. In the Bayesian Embedding Multi-Task Learning Multi-Layer Percep- tron (BEMTL) model as proposed in [22], all weights in an Artificial Neural Network (ANN) are shared, and only a Bayesian task embedding is used to differentiate between tasks. A task embedding can be understood as a vector encoding a specific task by placing it in a low-dimensional embed- ding space. It is given into the network’s first hidden layer concatenated to the task’s input variables and can be trained similarly to ANN weights by back-propagation. A Bayesian approach enforces a stable embedding train- ing that maps similar or even indistinguishable tasks close to each other in the embedding space. Thus, instead of specific vectors wu ∈ R m in the em- bedding space, multidimensional probability distributions q(wu) are used to represent tasks u ∈ {1, ..., n}. To simplify calculations, independent multi- variate normal distributions are chosen, and an identical, independent prior normal distribution p(w) is introduced as regularization for all tasks. The posterior probability distribution can be trained by using Bayes By Backprop as described in [23] and used in [22]. The results are independent multivari- ate normal distributions q(wu|µu, σu) = N (µu, diag (σ2 u)) with µu, σu ∈ Rm for tasks u ∈ {1, ..., n}. With w = (w1, ..., wn), the probability distribution parameters µ = (µ1, ..., µn) and σ = (σ1, ..., σn) minimize the cost function arg min µ,σ F(X, y, µ, σ) = arg min µ,σ {KL [q(w|µ, σ)||p(w)] − Eq(w|µ,σ) [ln p(y|X, w)]}, (7) where the first term is the Kullback-Leibler (KL) divergence between the approximation q(w|µ, σ) and the chosen prior p(w), and the second term represents the negative log-likelihood cost function. The KL divergence is a measure of the difference between two probability distributions. Thus, it 12JournalPre-proof Journal Pre-proof Task IDs Embedding Layer for µ Embedding Layer for σ Encoded Task IDs from Embedding Space Node Features Xt c o n c a t e n a t e Adjacency Matrix A Transformer Conv Layer ×2 Dense Layer ×3 yt Figure 3: Proposed model architecture BEMTL-GNN. serves as a penalty for diverging too far from the given prior and can, in practice, be weighted by a hyperparameter λbayes for appropriate regulariza- tion. 3.3. Proposed Architecture In the given use case, transformers are represented by nodes in a graph. Since different transformers display different characteristic time series behav- ior, as explained in section 2, they can be seen as different but also similar tasks in a Multi-Task Learning setting. A standard GNN, which can be interpreted as a particular type of hard parameter-sharing ANN, cannot dis- tinguish between the graph nodes. Therefore, a Bayesian task embedding is used as an additional node input into the GNN by introducing two embed- ding layers for µ and σ. The proposed architecture BEMTL-GNN with the novel combination of GNN with a Bayesian task embedding for node distinc- tion is shown in Fig. 3. For n nodes and d input features, Xt is a d×n matrix containing inputs for one timestamp, while µ and σ are m × n matrices with m being the dimension of the embedding space. With wu ∼ N (µu, diag (σ2 u)) a sample for node u can be chosen from the embedding probability distri- bution, and w = (w1, ..., wn) for nodes {1, ..., n}. It should be noted that samples are drawn from the distribution only during the training process and w = µ when making predictions. The encoded task representations w from embedding space are then concatenated with Xt to create the initial (d + m) × n node representations H (0) = (h (0) 1 , ..., h(0) n ). 13JournalPre-proof Journal Pre-proof These representations are passed to the GNN, which applies the attention convolution defined in Eq. (3). As a readout function the convolutional layers are followed by a set of fully connected layers to produce the final 1×n output yt, i.e., one value for each transformer for the given timestamp t. 4. Experiments An experiment was carried out to test the approach on a real-world data set and evaluate it against models without GNN architectures and a standard GNN without the embedding layer of the proposed model. 4.1. Dataset This study has been conducted on two real-world data sets containing ver- tical power flow measurements at transformers of the German Transmission grid, respectively. The overall problem size considered in the dataset corre- sponding to TSO1 (TenneT) includes 176 transformers with 37 features each and 175 transformers with 28 features for TSO2 (50Hertz). Together, the data sets cover about half of the transmission grid in Germany which make them a real-world use case at scale of the real system. The transformers con- nect the high voltage distribution grid to the extra-high voltage transmission Figure 4: Location of transformers for TSO 1 TenneT (left) and TSO 2 50Hertz (right). Multiple transformers at the same location are possible. 14JournalPre-proof Journal Pre-proof grid. Additionally, the dataset includes metadata comprising coordinates for each transformer. Apart from that, no exact information about the actual grid topology is available for this experiment; nonetheless, it can be assumed that the existence of an overhead line correlates strongly with geographic proximity. Fig. 4 shows the location of the given transformers for both data sets. The measurement time series are given in a 15 min resolution with available data from Jan 5, 2018 to Dec 31, 2019 for the TenneT data set and from Jan 2, 2020 to Feb 2, 2022 for 50Hertz. Power flows from the distri- bution grid to the transmission grid are interpreted as positive values and power flows from the transmission to the distribution grid as negative values. The measurements are normalized by each transformer’s estimated capacity rating, receiving target values between -1 and 1. Furthermore, gaps in the measurements with a duration of up to one hour are interpolated linearly. As local inputs at each node, a mix of numerical weather predictions (NWP) and other features are used to include explanatory variables for each possible type of power generation and consumption. Tab. 1 shows the input features used in both data sets. As NWP inputs, the IFS weather model by ECMWF [24] with day-ahead forecast horizons of +24 to +48 hours and at transformer locations is used. All input features get standardized before training. 4.2. Experimental Setup In this section the experimental setup is discussed. First, the graph con- struction is explained, followed by an introduction of benchmark models that are compared to the novel approach. Lastly, the choice of model parameters as well as the process of hyperparameter tuning are elaborated. 4.2.1. Graph Construction In this use case, the graph represents the power grid, in which the nodes correspond to the individual transformers between high and extra-high volt- age levels. The edges that connect the transformers are defined by the dis- tance between them, since the true wiring is not given in the data set. Trans- mission lines, bus bars or other grid components are not considered in the graph representation. Using a graph in which each node is connected to every other node would increase the computational complexity significantly. At the same time, the mutual influence of two transformers is expected to decrease with larger geographical distances, such that edges between transformers that 15JournalPre-proof Journal Pre-proof Table 1: Input Features TSO 1 TSO 2 Wind speed at 10 m and 100 m, with +/- 1 hour time lag x x Wind direction at 100 m x x Temperature at 2 m level x x Dew point temperature at 2 m level x x Air pressure at surface level x x Global horizontal irradiation with +/- 1 hour time lag x x Precipitation as a moving mean, as well as accumulated precipi- tation of the last 1, 2, 3 and 4 days x Solar height and solar azimuth x x A night / day logical with value 1 from 10pm to 6am and 0 oth- erwise to accommodate noise protection regulations x x Calendar information (hour of the day, day of the week, day of the year; logicals for holidays, sundays and saturdays) x x Consumption day-ahead forecasts for Germany x x and all four German TSO regions [25] x Price day-ahead forecasts for Germany [26] x x Active / inactive logical with value 1 for a detected active trans- former state and 0 for inactive state. Inactivity is assumed for five or more consecutive zeros or missing values in the vertical power flow measurements x x are far apart from each other are likely less beneficial for the prediction per- formance. To affirm this hypothesis and test the effect of edge density in the graph, the experiment is performed on two differently constructed graphs. In the first step, the benefit of the proposed approach shall be shown in a proof of concept. For this purpose, a very sparsely connected graph with edges only between transformers situated at the same substation is chosen. Accordingly, the distance between that are connected by edges is not allowed to exceed 0 km. This experiment is performed on both data sets resulting in 102 edges for TSO1 and 171 edges for TSO2. In the second step, an experiment is run on a more densely connected graph for data set 1 with edges between all transformers with a maximum geographical distance of 50 km. This results in a graph with 769 edges. No topological grid information is used to define edges. Additionally, for the more densely connected graph, the edge weights are set to the exponential inverse of the distance between the two transformers connected by an edge. 16JournalPre-proof Journal Pre-proof 4.2.2. Benchmark Models Four different models are compared in total, of which two models are GCNs. The other two models serve as benchmark models. The first bench- mark model is a Bayesian Embedding Multi-Task Learning approach (BE- MTL) as described in section 3.2, in which a set of standard fully connected layers is applied to the input features of the transformers after the Bayesian embedding layer. This model is equivalent to the proposed BEMTL-GNN model applied to the transformer graph without considering the edges. Ac- cordingly, no information is exchanged across nodes. The other non-GNN benchmark model is a single-task model, which trains a fully connected Neu- ral Network (STL ANN) for each transformer separately. Hence, each model learns individual weights optimized for the corresponding transformer. Both GNN models apply the convolution from Eq. (3) with one attention head per layer. The standard GNN, which serves as a benchmark, does not in- clude a Bayesian embedding layer. Accordingly, the model allows information exchange between different transformers, including individual attention co- efficients, but it cannot distinguish between different transformers. The pro- posed architecture BEMTL-GNN includes both multi-task embedding and information exchange across nodes. Although a temporal approach might be beneficial for the predictive performance, a time series-based approach was not included as benchmark, since it comes with an additional increase of complexity. Instead, this paper focuses on the aspect of combing MTL Learning with GNN for power flow prediction. To analyse the impact of this approach, a temporal model is not needed. However, for future work and fur- ther model improvement, both the benchmark and the BEMTL-GNN model can be expanded by temporal layers. BEMTL-GNN BEMTL St. GNN STL Embedding d = 8, λ = 1e−10 d = 8, λ = 1e−10 - - Conv. Layers 2 - 2 - Lin. Layers 3 5 3 5 Neurons per layer 100 Activation function ReLU Batch size 128 L2-regularization 1e-8 Learning rate 0.001 Table 2: Parameters of the compared models. 17JournalPre-proof Journal Pre-proof 4.2.3. Model and Training Parameters The exact parameters of all models, such as the number of layers and neurons, are shown in Tab. 2. All non-GNN architectures and the model training are implemented using pytorch [27]. All graph-related methods, in- cluding the GNN architectures, and the graph data handling are implemented using PyTorch Geometric [28], which provides the applied GNN layers. Preceding the experiment, a small range of hand-picked hyperparameters were investigated. These include different network depths and numbers of attention heads (1-3 layers/heads), and several different l2-regularization val- ues to fine-tune the model broadly. As part of the hyperparameter tuning, two different architectures of GCNs were applied, one based on the layer de- fined in Eq. (1) and the other one using the attention-based convolution from [21] defined in Eq. (3). Results showed an advantage of the attention-based convolution used for further experiments. Other aspects that were considered in a small study comprise the embedding regularization parameter λbayes, the convolutional layers’ aggregation function, and the order of convolutional and standard dense layers. Of course, the hyperparameter optimization has been performed exclusively on the training set, defined as described below. The data described in section 4.1 was split into a training and test set containing data of individually consecutive periods. Furthermore, a 2-month period at the end of the training period is used as a validation set for hy- perparameter tuning. Tab. 3 shows the exact data split for both data sets. Table 3: Split of training, validation and test periods training validation test TSO 1 5 Jan - 31 Dec 2018 1 Jan - 28 Feb 2019 1 Mar - 31 Dec 2019 TSO 2 2 Jan - 31 Dec 2020 1 Jan - 28 Feb 2021 1 Mar 2021 - 7 Feb 2022 Using consecutive periods for each split is necessary to avoid including data points from inside the test period in the training set. Sampling randomly from the entire period would lead to a higher correlation between the sets, which would reduce the reliability of the results on the test set. Additionally, the power production is highly dependent on the weather and, thus, on the seasons. Therefore results are most representative when an entire year can be included in the test set. In the experiments, all models were trained over 20 epochs using the Adam optimizer. By tracking the validation error while training, the epoch 18JournalPre-proof Journal Pre-proof with lowest validation error was identified and the corresponding model weights chosen as final model. As a loss function, the mean squared er- ror has been used. For all models which include the Bayesian embedding, an additional loss term was introduced to measure the quality of the embed- dings. For this purpose, the KL loss has been computed as described in Eq. 7 and weighted by hyperparameter λbayes. The exact training parameters, such as learning rate, are shown in Tab. 2. To evaluate the performance of each model, the root mean squared error (RMSE) on the test set averaged over five runs was compared. 5. Results and discussion In this section, the proposed approach is evaluated and compared to benchmark models described in section 4.2. For evaluation the models’ test RMSE on all transformers are compared and significance of observed RMSE differences between models are asserted by a sign test. As error values alone do not give any insight in how the model uses the additional informa- tion provided by the graph, forecast time series of the proposed model and the strongest benchmark are compared and discussed. The results for the sparsely and densely connected graph setup are examined separately. 0.0 0.1 0.2 0.3 0.4 0.5 root mean squared error STL ANN Standard GNN BEMTL BEMTL-GNN Figure 5: Test RMSE of normalized predictions on TSO 1 data set for benchmark models STL ANN, Standard GNN and BEMTL and proposed model BEMTL-GNN. One box represents 176 error samples, where each sample is the test error for one transformer averaged over 5 runs. The red line indicates the median over all samples and the black circle indicates the mean. 5.1. Sparsely connected graph For each method, five experimental runs were performed to reduce ran- dom effects in the evaluation. Thus, in each run and for each method one 19JournalPre-proof Journal Pre-proof RMSE value per transformer is calculated on the test set. By averaging over the five runs, for each method a mean RMSE per transformer is obtained. Fig. 5 shows those averaged test RMSEs of data set 1 as boxplots, i.e. each method’s boxplot depicts 176 values. The results show that BEMTL-GNN achieves the lowest median and mean RMSE but only with a slight advantage compared to the pure Multi-Task Model BEMTL. Noticeably, the standard GNN performs poorly with the highest average and median RMSE, indi- cating that in the given use case it is essential for the model to be able to distinguish between nodes. These results correspond to the authors’ expec- tations that the differing behavior of the transformers requires a multi-task approach that captures individual characteristics. A comparison between the two non-GNN models, STL ANN and BE- MTL, points towards an advantage of the BEMTL model. However, note that hyperparameters were tuned only for BEMTL-GNN. This hyperparam- eter tuning translates better to a Standard GNN and the Multi-Task model BEMTL since those models are trained on the same amount of training data given by all transformers, while STL ANNs are trained separately for each transformer and thus are provided with fewer training samples. Accordingly, a lower error might be achieved by STL ANN when performing a separate hyperparameter optimization. However, in an experiment on wind power forecasting, [22] showed that BEMTL can match or even outperform STL ANNs with individual hyperparameter optimization for each STL model, such that focusing on BEMTL as the strongest benchmark seems justified. The results show that test errors vary widely across different transformers. Thus, a better comparison of models can be made by looking at RMSE differences at each transformer. In Fig. 6 the test error differences between BEMTL and BEMTL-GNN are shown, with both median and mean being greater than zero, indicating that BEMTL-GNN outperforms BEMTL on the majority of transformers. To be exact, on data set 1, the proposed approach achieves lower RMSEs at 130 of 176 (74%) of transformers. A sign test performed on these results testifies that the null hypothesis that BEMTL achieves lower errors than BEMTL-GNN can be dismissed with a p-value of 8.88e-11. This attests that the result of the proposed model being more likely to outperform the benchmark is significant. Furthermore, the median is greater than zero with a confidence interval of 95%, as indicated by the notches. This shows that the proposed model achieves higher performance on a large fraction of the transformers. The results on data set 2 are less clear but point in the same direction, 20JournalPre-proof Journal Pre-proof 0.08 0.06 0.04 0.02 0.00 0.02 0.04 root mean squared error [BEMTL] - [BEMTL-GNN] 0.01 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 root mean squared error [BEMTL] - [BEMTL-GNN] Figure 6: Top: TSO 1 data set, bottom: TSO 2 data set. Test RMSE differences between normalized predictions for benchmark model BEMTL and proposed model BEMTL-GNN. One box represents 176 and 175 error samples, respectively, where each sample is the test error difference for one transformer averaged over 5 runs. Note that axes are not scaled and clipped equally. with BEMTL-GNN being the better choice of model for 109 of 175 (62%) of transformers. Here, the null hypothesis that BEMTL achieves lower errors than BEMTL-GNN can be dismissed with a p-value of 7.14e-4. However, evaluating error values does not give any insight into the model’s ability to learn relations between transformers. For this purpose, forecasts of BEMTL and BEMTL-GNN at example transformers situated at the same substation are compared. As can be seen in Fig. 7, forecasts by both models accurately predict zero values during times of inactivity, but only BEMTL- GNN forecasts show a clear impact of one transformer’s inactivity on the other. It can be observed that BEMTL-GNN indeed learns the relation that these two transformers strongly influence each other. This shows that the proposed BEMTL-GNN model has successfully leveraged the ability of GNNs to capture such relationships. Since the BEMTL model lacks infor- mation flow between transformers, such effects cannot be explored, resulting in the model ignoring the inactivity of correlated transformers. In this exam- ple, node 158 was inactive for 4 months in the training set, while node 157 only had short periods of inactivity. Thus, BEMTL forecasts of node 157 are biased towards behavior that assumes the inactivity of node 158. The pre- 21JournalPre-proof Journal Pre-proof 0.5 0.0power [normalized]measurement node 157 measurement node 158 0.5 0.0power [normalized]measurement node 157 BEMTL prediction node 157 measurement node 158 BEMTL prediction node 158 2019-10-08 2019-10-15 2019-10-22 2019-11-01 2019-11-08 2019-11-15 2019-11-22 0.5 0.0power [normalized] measurement node 157 BEMTL-GNN prediction node 157 measurement node 158 BEMTL-GNN prediction node 158 Figure 7: Vertical power flow predictions during test period by benchmark BEMTL (center plot) and proposed model BEMTL-GNN (bottom plot) at two transformers by TSO 1. dictions of BEMTL-GNN appear to be better than the ones of BEMTL, even when both transformers are active. This could be due to the fact that BE- MTL has to average over the possible activity states to some extent to make good predictions during times of activity and inactivity of partner nodes. While this example portrays the positive effect of the convolutional layers provided by the GNN, the advantage of including the Bayesian Embedding approach in the model is best illustrated by Fig. 8. Here the center plot shows forecasts of the benchmark Standard GNN at two transformer nodes located at the same substation. Being situated at the same location, input values and edges are identical for both nodes. Therefore, the standard GNN model without a Multi-Task Embedding produces the same output for both transformers. As it can be observed from the true measurements, the fore- casts of the standard GNN are not a good match for either node. However, the proposed BEMTL-GNN model shown at the bottom of the figure is able to make two different predictions using the same graph and input features, which results in more accurate power flow forecasts for both nodes. This 22JournalPre-proof Journal Pre-proof 0.0 0.5power [normalized]measurement node 146 measurement node 147 0.0 0.5power [normalized]measurement node 146 Standard GNN prediction node 146 measurement node 147 Standard GNN prediction node 147 2019-07-01 2019-07-05 2019-07-09 2019-07-13 2019-07-17 2019-07-21 2019-07-25 2019-07-29 0.0 0.5power [normalized] measurement node 146 BEMTL-GNN prediction node 146 measurement node 147 BEMTL-GNN prediction node 147 Figure 8: Vertical power flow predictions during test period by the benchmark Standard GNN (center plot) and proposed model BEMTL-GNN (bottom plot) at two transformers at the same substation by TSO 1. Note that in the center plot, the blue line of Standard GNN prediction for node 146 is overlayed by the orange line for node 147. is due to the task embeddings that were optimized during the training pro- cess and resulted in dissimilar embedding values for both nodes, providing the necessary variance of input. Consequently, including the Bayesian task embedding is a valuable addition to the model in this use case. 5.2. Densely connected graph The second experiment was performed on a more densely connected graph in which two nodes are connected if their distance does not exceed 50 km. Again, five experimental runs were performed and the error values averaged as described in 5.1 The results show a reduction of the test RMSE compared to the benchmark BEMTL, as can be observed in Fig. 9. The plot shows the error differences between BEMTL and BEMTL-GNN operating on the dense graph. As before on the sparse graph, BEMTL-GNN performed better on 130 of 176 (74%) transformers. However, in this experiment, forecasts by 23JournalPre-proof Journal Pre-proof 0.02 0.00 0.02 0.04 0.06 root mean squared error [BEMTL] - [BEMTL-GNN_dense] Figure 9: Test RMSE differences between normalized predictions for benchmark model BEMTL and model BEMTL-GNN operating on a densely connected graph on TSO 1 data set. The box represents 176 error samples, each sample being the test error difference for one transformer averaged over 5 runs. BEMTL-GNN do not clearly show the expected balancing behavior that the model was able to learn on the sparse graph. Since closer transformers show more adaptive behavior towards each other, a more densely connected graph probably includes too many factors in the neighborhood aggregation of the graph convolution. Hence, the adaptive behavior is harder to recognize for the model. To be able to predict such relationships also on densely connected graphs, further research on edge attributes or attention mechanisms could be of interest. 5.3. General remarks on efficiency and scale Regarding efficiency, the following observations can be made: Firstly, STL has by far the most trainable parameters since it includes one model per transformer, whereas the other approaches learn only one model in to- tal. With approximately 62,000 trainable parameters, BEMTL is the least complex model, followed by the standard GNN, and BEMTL-GNN with ap- proximately 86,000 and 92, 000 parameters, respectively. Each STL model contains about 24,000 parameters multiplied by the number of nodes in the graph. Accordingly, the model in total is about 125 times larger than the BEMTL model. Regarding the inference and train times, one STL model is roughly three times faster than the BEMTL model, which, in turn, requires about half the inference and train time of the GNN models. Conclusively, the superior performance of the proposed BEMTL-GNN model comes with an increased training and inference time compared to the STL and BEMTL models. The 24JournalPre-proof Journal Pre-proof reason for this is the higher number of parameters, which reside mainly from the attention convolutions. The inference time of the proposed model increases linearly with the number of nodes since for each node a separate node representation has to be computed. However, the size of the weight matrices in the proposed model does not depend on the number of nodes, but only on the number of node features. A more important factor regarding the runtime is connectivity of the input graph. The reason is that for the computation of a node embedding, the embeddings of all neighbours of that node are involved. Accordingly, the more edges are present in the graph, the more computationally expensive is the GNN layer computation. This holds especially for the applied attention convolution, since it computes an attention score for each edge (c.f. eq. 4) For the experiments on the densely connected graph including 769 edges, which is more than 7 times more edges than in the sparsely connected graph, the inference time increases to 1.75 of the sparse graph. When considering graphs with edges based on 100 kms (˜2400 edges) and 150 kms (˜4100 edges) distance between two transformers, an increase of complexity to approxi- mately 2.8 and 3.7 compared to the sparse graph can be observed. Accord- ingly, in this range of connectivity, the inference time increases linearly with the number of edges. Since a higher connectivity, i.e. more edges, did not enhance the performance in the experiments, a larger distance than 150 is not suitable for the conducted experiments. 6. Conclusion and outlook This paper proposes BEMTL-GNN, a GNN with a node-specific embed- ding layer and an attention mechanism to forecast vertical power flows at transformers. The model successfully learns individual latent characteristics of the transformers, while also taking into account information from close-by transformers. It has been shown that a preceding embedding layer enhances the per-node predictions of the applied GNN significantly, since it can cap- ture the characteristic behavior of the individual transformers. Compared to an equivalent multi-task network without a GNN architecture, the novel approach indeed improves the prediction performance of vertical power flow forecasts. More specifically, the proposed model reduces the test loss at 74% of transformers from a real-world data set. This includes situations in which transformers are directly influenced by nearby transformers. A sign 25JournalPre-proof Journal Pre-proof test asserts the significance of the result that BEMTL-GNN is more likely to outperform the pure multi-task model. However, a visible balancing rela- tionship between forecasts at different transformers can only be retraced in time series plots when the model is trained on a sparse graph with few edges. In summary, the proposed method delivers a clear advantage by providing both the ability to use message passing of GNN layers to include transformer interactions in forecasts, and the ability to distinguish between nodes to al- low a characteristic usage of input features at different transformers. This is achieved by the novel combination of GNN layers and a Bayesian task embedding. On the other hand, the approach does not provide a solution for defining relevant graph edges yet. Further research is also necessary to extend the desired behavior seen in the experiment with a sparse graph to densely connected graphs. To enable the model to learn the more complex relations of several trans- formers in a neighborhood, more than one attention head might be necessary. Further research could also focus on reducing irrelevant edges, as a large num- ber of edges between transformers with little impact on each other hampers model training. For this, a two-step approach might be helpful, where the existence of edges is learned in the first step before applying the model pro- posed in this paper in the second step. Another approach could address the problem of unbalanced data sets. Generally, periods in the training set from which specific relations between nodes could be learned are relatively short. This is because some switching states of the grid occur in excep- tional situations like maintenance or special weather conditions only. Thus, data augmentation with a more frequent appearance of different grid situa- tions could improve model training but requires detection and distinction of grid states in the first place, which is not trivial. Lastly, including real grid topology information where possible might help the model to learn unknown relations. While the above suggestions for improvement can pave the way for a more exhaustive, data-driven graph representation of the power grid, including more system components and their interactions, the proposed method can already enhance vertical power flow forecasts without requiring additional information about grid topology. This is a valuable step for grid operators towards a more accurate grid calculation for increasingly fluctuating electric- ity networks. 26JournalPre-proof Journal Pre-proof Acknowledgement This publication was supported by the Hessian Ministry of Higher Edu- cation, Research, Science and the Arts, Germany, through the Competence Center for Cognitive Energy Systems of the Fraunhofer IEE with reference number 511/17.001. Additionally, this work was supported by TRANSFER (grant number 01IS20020A) funded by the German Federal Ministry of Edu- cation and Research. Further, the authors would like to thank their partners TenneT TSO GmbH and 50Hertz Transmission GmbH for their support, as well as the GAIN research group at University Kassel funded by the Min- istry of Education and Research Germany (BMBF), under the funding code 01IS20047A. References [1] B. Donon, B. Donnot, I. Guyon, A. Marot, Graph Neural Solver for Power Systems, in: 2019 International Joint Conference on Neural Networks (IJCNN), 2019, pp. 1–8, iSSN: 2161-4407. doi:10.1109/ IJCNN.2019.8851855. [2] L. Duchesne, E. Karangelos, L. Wehenkel, Using Machine Learning to Enable Probabilistic Reliability Assessment in Operation Planning, in: 2018 Power Systems Computation Conference (PSCC), 2018, pp. 1–8. doi:10.23919/PSCC.2018.8442566. [3] M. Cai, R. Chen, L. Kong, Hyper-Chaotic Neural Network Based on Newton Iterative Method and Its Application in Solving Load Flow Equations of Power System, in: 2009 International Conference on Mea- suring Technology and Mechatronics Automation, Vol. 3, 2009, pp. 226– 229, iSSN: 2157-1481. doi:10.1109/ICMTMA.2009.531. [4] A. Karami, M. S. Mohammadi, Radial basis function neural network for power system load-flow, International Journal of Electrical Power & En- ergy Systems 30 (1) (2008) 60–66. doi:10.1016/j.ijepes.2007.10.004. URL https://www.sciencedirect.com/science/article/pii/ S0142061507001135 [5] J. Yu, Y. Weng, R. Rajagopal, Robust mapping rule estimation for power flow analysis in distribution grids, in: 2017 North American Power Symposium (NAPS), IEEE, Morgantown, WV, 2017, pp. 1–6. doi: 27JournalPre-proof Journal Pre-proof 10.1109/NAPS.2017.8107397. URL http://ieeexplore.ieee.org/document/8107397/ [6] K. Brauns, C. Scholz, A. Schultz, A. Baier, D. Jost, Vertical power flow forecast with LSTMs using regular training up- date strategies, Energy and AI 8 (2022) 100143. doi:https: //doi.org/10.1016/j.egyai.2022.100143. URL https://www.sciencedirect.com/science/article/pii/ S2666546822000064 [7] W. Bai, X. Zhu, K. Y. Lee, Dynamic Optimal Power Flow Based on a Spatio-Temporal Wind Speed Forecast Model, in: 2021 IEEE Congress on Evolutionary Computation (CEC), 2021, pp. 136–143. doi:10.1109/ CEC45853.2021.9504847. [8] M. Khodayar, J. Wang, Spatio-Temporal Graph Deep Neural Network for Short-Term Wind Speed Forecasting, IEEE Transactions on Sustain- able Energy 10 (2) (2019) 670–681, conference Name: IEEE Transac- tions on Sustainable Energy. doi:10.1109/TSTE.2018.2844102. [9] J. Park, J. Park, Physics-induced graph neural network: An appli- cation to wind-farm power estimation, Energy 187 (2019) 115883. doi:https://doi.org/10.1016/j.energy.2019.115883. URL https://www.sciencedirect.com/science/article/pii/ S0360544219315555 [10] R. Chen, J. Liu, F. Wang, H. Ren, Z. Zhen, Graph Neural Network- Based Wind Farm Cluster Speed Prediction, in: 2020 IEEE 3rd Student Conference on Electrical Machines and Systems (SCEMS), 2020, pp. 982–987. doi:10.1109/SCEMS48876.2020.9352310. [11] V. Bolz, J. Rueß, A. Zell, Power Flow Approximation Based on Graph Convolutional Networks, in: 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), 2019, pp. 1679–1686. doi:10.1109/ICMLA.2019.00274. [12] D. Wang, K. Zheng, Q. Chen, G. Luo, X. Zhang, Probabilistic Power Flow Solution with Graph Convolutional Network, in: 2020 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe), 2020, pp. 650–654. doi:10.1109/ISGT-Europe47291.2020.9248786. 28JournalPre-proof Journal Pre-proof [13] O. Kundacina, M. Cosovic, D. Vukobratovic, State Estimation in Electric Power Systems Leveraging Graph Neural Networks, arXiv:2201.04056 [cs, eess, math] (Apr. 2022). URL http://arxiv.org/abs/2201.04056 [14] W. Liao, B. Bak-Jensen, J. R. Pillai, Y. Wang, Y. Wang, A Review of Graph Neural Networks and Their Applications in Power Systems, Journal of Modern Power Systems and Clean Energy 10 (2) (2022) 345– 360, conference Name: Journal of Modern Power Systems and Clean Energy. doi:10.35833/MPCE.2021.000058. [15] A. B. Jeddi, A. Shafieezadeh, A Physics-Informed Graph Attention- based Approach for Power Flow Analysis, in: 2021 20th IEEE Inter- national Conference on Machine Learning and Applications (ICMLA), 2021, pp. 1634–1640. doi:10.1109/ICMLA52953.2021.00261. [16] T. B. Lopez-Garcia, J. A. Dom´ınguez-Navarro, Power flow analysis via typed graph neural networks, Engineering Applications of Artificial Intelligence 117 (2023) 105567. doi:10.1016/j.engappai.2022.105567. URL https://www.sciencedirect.com/science/article/pii/ S0952197622005577 [17] B. Donon, R. Cl´ement, B. Donnot, A. Marot, I. Guyon, M. Schoenauer, Neural networks for power flow: Graph neural solver, Electric Power Sys- tems Research 189 (2020) 106547. doi:10.1016/j.epsr.2020.106547. URL https://linkinghub.elsevier.com/retrieve/pii/ S0378779620303515 [18] F. Fusco, B. Eck, R. Gormally, M. Purcell, S. Tirupathi, Knowledge- and Data-driven Services for Energy Systems using Graph Neural Networks, in: 2020 IEEE International Conference on Big Data (Big Data), 2020, pp. 1301–1308. doi:10.1109/BigData50022.2020.9377845. [19] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, P. S. Yu, A Comprehen- sive Survey on Graph Neural Networks, IEEE Transactions on Neu- ral Networks and Learning Systems 32 (1) (2021) 4–24, conference Name: IEEE Transactions on Neural Networks and Learning Systems. doi:10.1109/TNNLS.2020.2978386. 29JournalPre-proof Journal Pre-proof [20] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rat- tan, M. Grohe, Weisfeiler and leman go neural: Higher-order graph neural networks, in: Proceedings of the AAAI conference on artificial intelligence, Vol. 33, 2019, pp. 4602–4609. [21] Y. Shi, Z. Huang, S. Feng, H. Zhong, W. Wang, Y. Sun, Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Clas- sification, in: Proceedings of the Thirtieth International Joint Confer- ence on Artificial Intelligence, International Joint Conferences on Artifi- cial Intelligence Organization, Montreal, Canada, 2021, pp. 1548–1554. doi:10.24963/ijcai.2021/214. URL https://www.ijcai.org/proceedings/2021/214 [22] S. Vogt, A. Braun, J. Dobschinski, B. Sick, Wind power forecasting based on deep neural networks and transfer learning, in: Wind Integra- tion Workshop, Dublin, Ireland, 2019. [23] C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra, Weight uncer- tainty in neural networks (2015). doi:10.48550/ARXIV.1505.05424. URL https://arxiv.org/abs/1505.05424 [24] R. G. Owens, T. Hewson, Ecmwf forecast user guide (05 2018). doi: 10.21957/m1cs7h. URL https://www.ecmwf.int/node/16559 [25] Entsoe transparency platform, https://transparency.entsoe.eu, last accessed: 2022-02-03. [26] Volue insight, https://www.volue.com/insight, last accessed: 2022- 05-27. [27] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, Pytorch: An imperative style, high- performance deep learning library, in: Advances in Neural Information Processing Systems 32, Curran Associates, Inc., 2019, pp. 8024–8035. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library.pdf 30JournalPre-proof Journal Pre-proof [28] M. Fey, J. E. Lenssen, Fast graph representation learning with PyTorch Geometric, in: ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. 31JournalPre-proof Journal Pre-proof Highlights  Combinaton of a Graph oeural oewoor and Multi-Tas Learnin wo phredictw w e vertctal phooer foo aw wransformers  Pooer Floo Forectasw pherformed on woo reali-oorld dawa sews oiw oeaw er ctonditonss ctalendar informatons and phricte forectasw as inphuw feawures for a sew of wransformers  Bayesian multi-was embeddin ctaphwures individual ct aractweristcts of w e wransformers  Graph oeural oewoor arct iwectwure ctonsiders informaton from ctlosei-by wransformers  Supherior pherformancte ctomphared wo benct mar modelss phartctularly in w e ctase of inweracttn wransformersJournalPre-proof Journal Pre-proof Beinert D., Holzhüter C., Thomas J. M., Vogt S. Funded by Power Flow Forecasts at Transmission Grid Nodes Using Graph Neural Networks Graph BEMTL-GNN Transformers in power grid with • mutual dependencies GNN Power flow forecasts at transformers Transformer encoding in embedding space Combining GNN and Bayesian Embedding Multi-Task Learning to predict vertical power flows at transformers between high and extra high voltage levels • Improved prediction performance Results • Model captures transformer interactions • characteristic power flow behaviorJournalPre-proof Journal Pre-proof Declaratio if ioterettt The authors declare that they have no known competng fnancial interests or personal relatonships ☒ that could have appeared to infuence the work reported in this paper. The authors declare the following fnancial interests/personal relatonships which may be considered ☐ as potental competng interests:","libVersion":"0.3.1","langs":""}
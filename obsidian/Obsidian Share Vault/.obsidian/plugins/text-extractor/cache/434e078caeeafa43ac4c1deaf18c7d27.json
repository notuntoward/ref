{"path":"lit/lit_sources.backup/LeCun24animalSmarterLessDat.pdf","text":"Yann LeCun VP & Chief AI Scientist at Meta Follow View full profile Follow Reactions Like Yann LeCun • 2nd VP & Chief AI Scientist at Meta 5mo • Edited • Animals and humans get very smart very quickly with vastly smaller amounts of training data than current AI systems. Current LLMs are trained on text data that would take 20,000 years for a human to read. And still, they haven't learned that if A is the same as B, then B is the same as A. Humans get a lot smarter than that with comparatively little training data. Even corvids, parrots, dogs, and octopuses get smarter than that very, very quickly, with only 2 billion neurons and a few trillion \"parameters.\" My money is on new architectures that would learn as efficiently as animals and humans. Using more text data (synthetic or not) is a temporary stopgap made necessary by the limitations of our current approaches. The salvation is in using sensory data, e.g. video, which has higher bandwidth and more internal structure. The total amount of visual data seen by a 2 year-old is larger than the amount of data used to train LLMs, but still pretty reasonable. 2 years = 2x365x12x3600 or roughly 32 million seconds. We have 2 million optical nerve fibers, carrying roughly ten bytes per second each. That's a total of 6E14 bytes. The volume of data for LLM training is typically 1E13 tokens, which is about 2E13 bytes. It's a factor of 30. Importantly, there is more to learn from video than from text because it is more redundant. It tells you a lot about the structure of the world. 13,811 721 comments· 941 reposts … Comment Repost Send Add a comment… Most relevant Jiangwei Pan • 3rd+ Recommendation Algorithms at Netflix Like · 35 Reply · 9 Replies Load previous replies 5mo Is it possible that humans are born pretrained, and our learning is just for fine-tuning? Home My Network Jobs Messaging Notifications Me For Business Get Premiu month, 1 1 23 5/5/24, 9:14 AM (25) Post | LinkedIn https://www.linkedin.com/posts/yann-lecun_animals-and-humans-get-very-smart-very-quickly-activity-7133567569684238336-szrF/ 1/3 Qazi Fazli Azeem, MFA • 3rd+ Designer | Educator | Artist | Accessibility Advocate Like Reply 5mo(edited) I see the logic here. Implications for biological evolution. Looks like someone reverse engineered the theory of carcinogenesis and why mutation happens and applied it to computational AI evolution. Imagine what happens when you apply genetic algorithms and shift to bio-computing. More likely that using generative AI on quantum…see more Fiona Yuan Tian • 3rd+ Software Engineer at Meta Like · 20 Reply · 3 Replies Load previous replies 5mo All our chip today (GPU, CPU, etc) “calculates” with a clock (this is called synchronous computation): every clock cycle all “neurons” need to be updated to a new value - this is essentially how we are realizing these matrix calculations with today’s AI algorithm/model in silicon level. However, this is not how human brain works: human brain works more in an asynchronous…see more Mahdi Shariatzadeh • 3rd+ Executive Deputy of the AI Headquarter, Vice Presidency for Sci & Tech Like Reply 2mo This is not possible in current tech products. In an H100 card there are some (~16k) processors each of which mimicking a number (~8,16) of neurons and quickly switch to mimic another neurons. Kay Rottmann, PhD (He/Him) • 2nd GenAI and LLM expert Like · 9 Reply · 2 Replies Load previous replies 5mo(edited) I agree on the need for new architectures and importance of multimodality. But it is also interesting that we can still learn and fully function even if a modality is missing at birth (blindness from birth on etc.). So what would be the minimum number of modalities needed to work out, and can we really rule out immediately that just language would not work? …see more Mudit S. James (He/Him) • 2nd Principal Engineer @ Bosch | Data Analytics, Distributed Computing & Stream Processing Like · 1 Reply 5mo Kay\u0000 Rottmann Some kind of transfer learning to mimic genetic traits.. maybe? John F. Sciacca, MIT ACE - Quantum Physics • 3rd+ Chief Innovation Officer | Certified AI Research Analyst | Solutions Architect | Quantum Physics | Aspiring Author | Future of Work Like · 8 Reply · 8 Replies Load previous replies 5mo Good pondering. What is this architecture where a machine can learn, grow, adapt and transform like a 2-year into an adolescent? This technology architecture does not exist despite the lipogram nepo propagating \"AGI\" hype. …see more John F. Sciacca, MIT ACE - Quantum Physics • 3rd+ Chief Innovation Officer | Certified AI Research Analyst | Solutions Architect | Quantum Physics | Aspiring Author | Future of Work Like · 2 Reply 5mo Cassio Cristani — That is the Valiant hypothesis — anything is [theoretically] possible. However, we both know that a new ontological cyborg has not been created from a non-peer reviewed paper reflecting fundamental physics of thermodynamics. We can write a paper, wrap new jargon in it with ____(fill_in_blank) AI — hire an overzealous CEO…see more Hector Barrio • 3rd+ Technology Director - Machine Learning & Artificial Intelligence 5mo 5/5/24, 9:14 AM (25) Post | LinkedIn https://www.linkedin.com/posts/yann-lecun_animals-and-humans-get-very-smart-very-quickly-activity-7133567569684238336-szrF/ 2/3 Load more comments Like · 8 Reply · 2 Replies Load previous replies DNA apparently creates a \"pretrained\" brain in animals. James Halladay • 3rd+ SDE @ Amazon | Former Network Traffic Researcher @ Colorado Mesa University Like Reply 5mo I would compare DNAs role in intelligence to the hyper parameters used when training models. Things like learning rate, architecture, information flow, initialization, how you route feedback to learn, and self contained modules that are biased towards learning certain types of data or certain patterns more effectively. …see more Andreas Kraemer • 3rd+ Computational Systems Biologist at Qiagen Like · 9 Reply · 4 Replies Load previous replies 5mo I guess the question is how much of that \"training\" already happened during evolution (so is encoded in DNA), i.e. to what extend a brain is pre-trained before being exposed to any external information. Irene Gabashvili • 2nd Entrepreneur, Innovator, and Educator Like · 2 Reply 5mo Andreas Kraemer Our brain's ability to assign different weighs to different parts of the input is fascinating, indeed (Eg, focusing on a conversation in a noisy room). We do basic processing of all signals at first, then pre-filter, use neurotransmitters to prioritize, employ neural connections in the prefrontal cortex and parietal lobes and previous…see more Romain Janil • 3rd+ 3D Synthetic Environment Production Designer | industrial designer Like · 6 Reply 5mo Redundancy is probably the trap Mubaraq Onipede • 3rd+ Data Analytics & Insights Lead @ AIESEC in UAE | Data Scientist Intern @ NCAIR Like · 6 Reply 5mo Very interesting write up Andrew Eisenhawer • 3rd+ I ask great questions, and sometimes find answers. Like · 10 Reply 5mo The comparison is mute. Organisms don't learn anything from a blank slate. We have 500 million years accumulation of evolutionary selection of neural architectures that are particularly useful in a physical world, and maybe 300 million years worth modeling social interactions. The architectures are, from infancy, already at least partially prepared to model complex notions such as…see more José Luis Prado Seoane • 3rd+ #AI #DataScience #DataArchitectures #DevOps #MLOps #DataOps #NeurIPSLover (opiniones personales) Like · 6 Reply 5mo Atomize the information.. About Accessibility Help Center Ad Choices Advertising Get the LinkedIn app More LinkedIn Corporation © 2024 Privacy & Terms Business Services 5/5/24, 9:14 AM (25) Post | LinkedIn https://www.linkedin.com/posts/yann-lecun_animals-and-humans-get-very-smart-very-quickly-activity-7133567569684238336-szrF/ 3/3","libVersion":"0.3.2","langs":""}
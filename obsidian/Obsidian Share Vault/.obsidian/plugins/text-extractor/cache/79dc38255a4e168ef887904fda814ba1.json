{"path":"lit/lit_sources.backup/Liu23EndtoendLearningUser.pdf","text":"End-to-End Learning of User Equilibrium with Implicit Neural Networks Zhichen Liu1, Yafeng Yin *1, Fan Bai2, and Donald K Grimm2 1Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, MI, 48109, United States 2General Motors Research and Development, Warren, MI, 48092, United States February 25, 2023 Abstract This paper intends to transform the transportation network equilibrium modeling paradigm via an ”end-to-end” framework that directly learns travel choice preferences and the equilib- rium state from multi-day link flow observations. The centerpiece of the proposed framework is to use deep neural networks to represent travelers’ route choice preferences and then encap- sulate the neural networks in a variational inequality that prescribes the user equilibrium flow distribution. The proposed neural network architecture ensures the existence of equilibrium and accommodates future changes in road network topology. The variational inequality is then embedded as an implicit layer in a learning framework, which takes the context features (e.g., road network and traveler characteristics) as input and outputs the user equilibrium flow distribution. By comparing computed equilibrium flows with observed flows, the neural networks can be trained. The proposed end-to-end framework is demonstrated and validated using synthesized data for the Sioux Falls network. Keywords: Network equilibrium, neural-network-based variational inequality, end-to-end learning, and implicit layer 1 Introduction Transportation network equilibrium modeling paradigm plays an important role in the plan- ning and operations of transportation networks. It has been widely used to compare different improvement designs or operation plans and aid the decision-making in selecting a better one for implementation. The paradigm was initiated by Beckmann et al. (1956) for modeling route choices in a static and deterministic network. Over the past 66 years, it has been extended to model other travel choices (e.g., destination and mode), better represent travel behaviors (e.g., bounded rationality) and capture traffic dynamics (within-day or day-to-day). This modeling paradigm has been established via a ”bottom-up” approach. Specifically, the process starts with adopting a particular assumption on how travelers make their travel choices *Corresponding author. yafeng@umich.edu. 1 © 2023 published by Elsevier. This manuscript is made available under the Elsevier user license https://www.elsevier.com/open-access/userlicense/1.0/ Version of Record: https://www.sciencedirect.com/science/article/pii/S0968090X23000748 Manuscript_04397f84e41b830f872777c3a7b11e13 (trip generation, destination, mode, route, and/or departure time) over a congestible network. By viewing the interactions among travelers as a non-cooperative non-atomic game, modelers describe the outcome of these interactions, i.e., the traffic flow distribution, as Wardropian user equilibrium (Nash equilibrium with infinitely many players) where no traveler would be better off by unilaterally changing their travel choice (Wardrop, 1952). This equilibrium condition is then mathematically defined and subsequently formulated as an equivalent mathematical pro- gram or variational inequality (alternatively fixed point or nonlinear complementarity problem). Lastly, the formulation is solved to obtain the equilibrium flow distribution, from which various performance measures can be quantified. A constant battle modelers have been fighting over the past half a century when developing a network equilibrium model is to strike a balance between behavioral realism and mathemat- ical tractability. Informed by findings from travel behavior research, modelers understand that travelers are boundedly rational (Simon, 1955), and their travel choice behaviors are much more complicated than what has been assumed in existing equilibrium models. However, when in- corporating a better behavioral consideration as per a more advanced theory, e.g., the prospect theory (Tversky and Kahneman, 1992), the resulting model quickly becomes computationally intractable for large-scale networks (Xu et al., 2011). Therefore, despite numerous efforts for enhancing behavioral realism in network models, those with simplified behavioral rules, e.g., travelers choosing the fastest path, still dominate in the planning practice. In this ”bottom-up” approach, the selection of the behavior model is divorced from the end goal of the model building, i.e., prescribing an equilibrium flow distribution that matches obser- vations as closely as possible. To construct a network equilibrium model, we would pre-select a behavior model, e.g., adopting multinomial logit for modeling route choice, calibrate the asso- ciated parameters with stated or revealed preference route-choice data and then proceed with the model building. This process is justified when the behavior model is perfect (or near so) for modeling route choice, which is certainly far from being true (Chen et al., 2016). The selection of the multinomial logit model reflects our belief or judgment rather than being the outcome of a calibration process against empirical flow data. Recall that a different behavior model yields different equilibrium conditions that would produce a different traffic flow distribution. There- fore, observed flows should play a role in selecting the behavior model. However, we have never done so, due to the lack of a selection methodology and empirical data. This model pre-selection bias is actually beyond the behavioral side, as the equilibrium model involves the supply-side components that also need to be learned. In contrast to the traditional ”bottom-up” approach for building a network equilibrium model, this study aims to transform the modeling paradigm via an end-to-end framework that directly learns travel choice preferences and the equilibrium state from data. More specifically, we consider a routing game where the context information such as route features and traveler characteristics is known and origin-destination (OD) demand observations and partial link flow data are available for an extended period of time. The centerpiece of the proposed end-to-end framework is to use deep neural networks to represent travelers’ route choice preferences. The neural networks will then be embedded in a variational inequality (VI) that prescribes the user equilibrium flow distribution (see Figure 1). We will treat this VI as an implicit layer in a learning framework, which takes the context features and demands as input and outputs the user equi- 2 librium flow distribution. By comparing computed equilibrium flows with observed flows, the neural networks can be trained. Figure 1: An illustrative example of the proposed end-to-end learning framework, where de- mands and link flow observations are available over M = {1, 2, 3} days; h denotes path flows, and travelers’ route choice preferences are represented by the cost function Cθ(h, x) parameter- ized by neural networks θ. The implicit layer takes the demands (q[1], q[2], q[3]) and context x as input and outputs the equilibrium link flows (v⋆[1], v⋆[2], v⋆[3]), where superscript [m] associates a variable with observation m ∈ M. One major advantage of the proposed end-to-end framework is that it does not pre-select a particular behavioral rule or theory to model travelers’ choices. Instead, it represents travelers’ route choice preferences with neural networks and directly learns them from observations. Deep neural networks can achieve a much richer representation of travelers’ choice preferences with comparable interpretability as the classical discrete choice models (Sifringer et al., 2020). More importantly, the end-to-end framework learns the equilibrium state of the network. Because real systems never settle into equilibrium, observed flows are indeed not equilibrium flows. The training process essentially yields an equilibrium state that matches all the observations as closely as possible (measured by a distance/loss function). The learned equilibrium state will then serve as a consistent benchmark or reference point against which improvement plans can be designed and compared. The resulting equilibria are ”perturbed” from the learned equilibrium state and will help decision-makers differentiate various plans. In this sense, the proposed framework melds the data-decision pipeline by integrating learning and decision/optimization into a single end-to-end system. To the best of our knowledge, our work is the first to integrate the learning of travel choice preferences into an end-to-end learning framework, with neural networks automatically discov- ering a good specification of route choice preferences from empirical data. This paper presents our first attempt to overcome the modeling and algorithmic challenges for enabling such an end-to-end learning. We identify a novel neural network architecture that guarantees the exis- tence of an equilibrium solution and accommodates the changes in the road network topology that may arise in subsequent ”what-if” planning analysis. For training, we adopt recent devel- opments in operator-splitting methods to enable scalable solution algorithms for a batch of VI 3 problems in forward propagation. In backpropagation, implicit differentiation techniques enable the proposed framework to efficiently differentiate through the implicit layer. The rest of this paper is organized as follows. Section 2 presents a summary of the relevant literature to better position this paper. Section 3 models the user equilibrium as a VI parameter- ized by neural networks and demonstrates the neural network architecture. Section 4 presents the end-to-end framework and elaborates on the forward and backward propagations. Numerical experiments are conducted on the Sioux Falls network to demonstrate the proposed framework in Section 5. Finally, Section 6 concludes the paper. 2 Relevant Literature 2.1 Traffic flow prediction from observations In the traditional ”bottom-up” network modeling paradigm, one can calibrate behavioral parame- ters in an equilibrium model from flow observations and then predict traffic flows at Wardropian equilibrium. The calibration process is usually formulated as a bi-level program or a math- ematical program with equilibrium constraints. For example, Yang et al. (2001) considered a logit-based stochastic user equilibrium and formulated a bi-level program to calibrate the disper- sion parameter in the logit model and OD demands from link flow observations. Later studies extend such a calibration framework to accommodate more complex model structures. Wang et al. (2016) considered a dynamic dispersion parameter and performed experiments using real- world data gathered from a small network in Seattle, WA. Guarda and Qian (2022) considered a multi-criteria linear cost function. They analyzed the pseudo-convexity property of the bi-level program and developed a hypothesis test framework to examine the statistical properties of cali- brated parameters. The proposed framework in this paper differs from these previous studies in that it does not pre-select a behavioral model to represent the route choice preferences. Another stream of studies uses deep neural networks, ranging from Long Short-Term Mem- ory to Spatial-Temporal Graph Convolution Neural Network (see, e.g, Yao et al., 2019), to predict short-term traffic flows. These models can capture complex spatiotemporal correlations of traf- fic flows from multi-source data and show satisfactory accuracy. However, fundamentally, these models assume future flows will be generated by the same process that generated historical flows and then learn a direct mapping from input features to traffic flows. As such, the models would likely fail in an ”out-of-distribution” test where the underlying process changes. For example, in ”what-if” analysis, a planning agency may update the road network topology, thereby changing the process of generating traffic flows. More recently, some used supervised learning to learn a mapping from demands to equilibrium flows (e.g., Rahman and Hasan, 2022; Spana and Du, 2022). Such models suffer from the same limitation in ”out-of-distribution” tests. Moreover, they don’t use empirical data to learn the equilibrium state or travel choice preferences. By contrast, the proposed framework directly learns travel choice preferences from data and captures equilib- rium conditions with a neural-network-based VI. The learned equilibrium state will then serve as a consistent benchmark to help decision makers differentiate various plans. Although the preferences may evolve over time (but can also be learned over time), it is reasonable to assume the same choice preferences when conducting ”what-if” planning analysis. 4 2.2 Neural-network-based discrete choice model Recent studies have explored integrating neural networks with discrete choice models for more accurate prediction and more tractable parameter estimation. They show that neural networks, when carefully designed, are not ”black-box” and can be interpreted for use in choice analysis. One notable example is Sifringer et al. (2020), who decomposed the systematic part of the utility function into a knowledge-driven part from classical discrete choice models and a data-driven part from neural networks. By maintaining the independence of elasticities from two parts, their framework benefits from the predictive power of neural networks while keeping some key parameters interpretable. Other similar attempts include Wang et al. (2020), who encoded the irrelevant alternative constraints with alternative-specific connectivity. Their domain-knowledge- regularized neural network architecture better captures the substitution patterns of travel mode choices. Different types of neural networks, such as residual networks (Wong and Farooq, 2021), are synergized with discrete choice models to allow for similar interpretability as a Multinomial Logit model. These interpretable neural-network-based discrete choice models offer a good foun- dation for us to design the neural network architectures in the proposed end-to-end framework, particularly when behavior interpretability is desired. 2.3 Learning-based transportation network modeling Computational graphs along with automated differentiation provide powerful tools to calculate gradients in end-to-end learning, where the entire framework connecting the input features to the desired output is learned from data. In the field of network modeling, Wu et al. (2018) made an interesting attempt to encode trip generation, trip distribution, and path-based traffic loading with layered computational graphs and estimated hierarchical travel demands from multi-source data. Later studies extend the static framework to estimate multi-class dynamic OD demands (Ma et al., 2020). Apart from pre-selecting a behavior model before calibration, these studies ignore the flow-dependent congestion effect and only consider a one-step network loading encoded with standard or explicit neural networks. By contrast, the proposed framework models the interactions among travelers as a routing game and captures the equilibrium conditions with a implicit layer in end-to-end learning. The output of the implicit layer solves a fixed point problem. It is called implicit because the output of the layer is defined implicitly–there is no analytical formula for it—and cannot be obtained via explicit computation rules, as the computational graph in standard or explicit neural net- works (Travacca et al., 2020). The implicit layer was first proposed by Bai et al. (2019) and has been applied to various fields such as power flow prediction (Fioretto et al., 2020) and auction mechanism design (Feng et al., 2018). Of the most relevant to our study are Li et al. (2020) and Heaton et al. (2021), who explored learning the equilibrium states of routing games with implicit layers. Specifically, Li et al. (2020) cast the equilibria as an implicit layer and calibrated cost parameters in an end-to-end fashion. Their study, however, still follows the traditional ”bottom-up” approach and pre-selects a behav- ior model before calibration. They only used computational graphs as a tool to enhance com- putational efficiency rather than exploring the representation power of neural networks. Heaton et al. (2021) approximated the weather-dependent link performance functions with fully con- 5 nected layers, which take the link flows and weather as input and output link travel time. They reformulated the weather-dependent equilibrium conditions as the fixed point of a decoupled projection operator and encapsulated the fixed point problem in the implicit layer. They then trained the neural network with link flow observations. Our work advances these previous stud- ies by integrating the selection or learning of behavior rules into an end-to-end framework, with neural networks automatically discovering a good specification of travel choice preferences from empirical data. In addition, we propose a novel neural network architecture that ensures the existence of equilibria and accommodates changes in the road network topology to facilitate ”what-if” planning analysis. 3 Neural-network-based VI Formulation of UE 3.1 Model formulation We consider a case where all OD demands and partial or all link flows at peak periods are observable for a long period of time. This case is particularly relevant for a tolled freeway network for which the OD demand data are available from the electronic tolling collection system and link flow data are available from detectors, or a subway network where the OD demands are from smart card data and link flows are available from automatic passenger counting system. Suppose that a planning agency is interested in developing a static network equilibrium model to analyze the network for peak periods. The learning task is to approximate the cost function with neural networks and learn travelers’ route choice preferences from multi-day observations. Mathematically, consider a network G = (N , A), where N and A are the set of nodes and links. Let R denote the set of OD pairs. Each OD pair r ∈ R is connected by paths that form a finite and nonempty feasible path set Pr. P represents the set of feasible paths for all OD pairs. OD demands and a subset of link flows are observed over a number of days (sample set) M. Let q[m] ∈ Q and v[m] ∈ V be the OD demands and link flows observed on day (sample) m ∈ M, where Q = {q ∈ R|R| : q ≥ 0} and V = {v ∈ R|A| : v ≥ 0} respectively. Hereinafter, superscript [m] associates a variable with a sample m. All vectors are assumed to be column vectors unless otherwise specified. We distinguish ”feature” from ”attribute” to avoid ambiguity: features refer to the input data of neural networks whereas attributes refer to the learned outputs of neural networks. Appendix A summarizes the notations used throughout this paper. We propose Attribute Net to learn the (path) attributes considered by travelers in their route choice decision process. As shown in Figure 2, attributes π[m] depend on path flows h[m] and road network features xG. Attribute Net Gθ learns a continuous mapping from path flows h[m] ∈ H[m] and road network features xG ∈ X G to attributes π[m] ∈ Π, defined as: Gθ : H[m] × X G → Π, where θ is the learnable parameters; the feasible path flow set H[m] = {h ∈ R|P | : h ≥ 0, Σ⊤h = q[m]} requires the feasible path flows h[m] to be nonnegative and satisfy flow conservation; Σ ∈ R|P |×|R| is the path-OD incidence matrix, in which Σpr equals one if path p ∈ Pr and zero otherwise; X G is the feasible set of road network features; Π ⊂ R|P |×|S| is the feasible attribute set and |S| is the number of attributes considered by travelers. 6 Figure 2: Illustration of the end-to-end learning framework. The superscripts for a sample m are omitted. Weight Net is proposed to capture traveler heterogeneities and learn the OD-specific prefer- ences over learned attributes (see Figure 2). We treat all travelers between the same OD pair as a single class that shares the same preferences. It is straightforward to further classify travelers between one OD pair to be multiple classes to reflect the preference heterogeneity among them. Weight Net Fθ learns a mapping from traveler characteristics xR ∈ X R to OD-specific weights w ∈ W, defined as: Fθ : X R → W, where X R ⊂ R|R|×|I R| is the feasible set of traveler characteristics; |I R| is the number of charac- teristics attached to each OD; W ⊂ R|R|×|S| is the feasible weight set. The parameters of Weight Net and Attribute Net are learned simultaneously and thus are both represented as θ. Subsequently, we assume that travelers choose routes to minimize their perceived path costs c[m] ∈ C ⊂ R|P |, which are represented as a weighted sum of attributes: c[m] = Σ w ⊙ π[m] 1, (1) where ⊙ represents the Hadamard (element-wise) product and 1 ∈ R|S| is a column vector of 7 ones to calculate the sum over the rows. Equivalently, let context features x ∈ X include traveler characteristics xR and road network features xG. The cost function Cθ : H[m] × X → C maps path flows and context features to path costs, defined as: Cθ(h[m], x) = Σ Fθ(xR) ⊙ Gθ(h[m], xG ) 1, (2) is approximated by neural networks. The multi-class User Equilibrium (UE) with inelastic demand for sample m is formulated as a parameterized VI in Eq.(3), the solution to which is the equilibrium flow h⋆[m], i.e., h⋆[m] ∈ VI(Cθ(h, x), H[m]). 〈Cθ(h⋆[m], x), h[m] − h⋆[m]〉 ≥ 0, ∀h[m] ∈ H[m]. (3) The VI problem in Eq.(3) requires the knowledge of feasible path set P. This is a common assumption for path-based UE formulation (Guarda and Qian, 2022; Bekhor and Toledo, 2005) with well-developed methods for generating the feasible path set in the literature (Frejinger et al., 2009). If one believes that path costs are link additive, the following link-based UE formulation can be used instead: 〈Tθ(v⋆[m], x), v[m] − v⋆[m]〉 ≥ 0, ∀v[m] ∈ V [m], (4) where v[m] ∈ V [m] and Tθ(v⋆[m], x) are link flows and link cost functions respectively; V [m] = {v ∈ R|A| : v ≥ 0, Σ⊤Λv = q[m]} is the set of feasible link flows and Λ ∈ R|P |×|A| denotes the path- link incidence matrix, Λpa equals one if link a ∈ A belongs to path p ∈ P, and zero otherwise. In this paper, we consider that path cost may not be link additive and thus use the path-based formulation. Theorem 1 (Existence of equilibrium) There exists at least one solution to the multi-class user equi- librium problem in Eq.(3). Proof 1 Path flow h[m] is a solution to VI (Cθ(h, x), H[m]) if and only if it is the fixed point of the projection operator PH[m] (·) for any α > 0, defined as: h⋆[m] ∈ VI (Cθ(h, x), H[m]) ⇐⇒ h⋆[m] = PH[m] (h⋆[m] − α Cθ(h⋆[m], x)) , (5) where the projection operator is defined as PH[m] (y) = argminy′∈H[m] ∥y′ − y∥. The cost function Cθ(h[m], x) is approximated by neural networks and thus is continuous. The fixed point operator PH[m] (·) is a projection operator that is continuous. Because the feasible flow set H[m] is convex and compact, as per Brouwer’s fixed point theorem, there exists at least one solution to the fixed point problem in Eq.(5). 3.2 Neural network architecture This section discusses the design of the neural network architecture in the proposed end-to-end learning framework. The architecture needs to accommodate the changes in the road network topology to facilitate ”what-if” analysis. Moreover, it can be designed to ensure that the cost function possesses desired properties to enable efficient training. Hereinafter, we highlight that features/attributes are the concentation of single features/attributes for all elements within one set. For example, the link feature is xA = {xA 1 , · · · , xA a , · · · , xA |A|} = {xA a }a∈A. 8 3.2.1 Attribute Net One may construct the Attribut Net Gθ with fully connected layers and learn a global mapping from link flows to link costs (e.g. Heaton et al. (2021)). In this case, the input and output dimensions of fully connected layers depend on the number of links in the road network. However, in ”what-if” analysis, a planning agency may change the road network topology by adding or removing links. The fully connected layers — by definition with fixed size input and output — are incapable of accommodating the change in the number of links. Inspired by the ”kernel” concept in Convolution Neural Networks, we propose to learn the local attributes on the link, node, and path levels with three parallel, fully connected layers. As shown in Figure 3, the fully connected layers that learn link, node, and path attributes are called link, node, and path block respectively. The parameters of each block are shared among all elements of the same level to capture repeated patterns. Each block’s input and output dimensions are independent of road network topology, allowing for changeable input sizes. To facilitate the presentation, the superscripts for a sample m are omitted for the rest of this section. We use the superscript A, N , and P to distinguish the notations related to link, node, and path block. Figure 3: Illustration of Attribute Net. The feature/attribute subscripts for enumerating the elements within a set and the superscripts for a sample m are omitted to facilitate presentation. The detailed constructions of link, node, and path block are similar. Hence, we take the link block as an example. As opposed to accepting multiple links as input, the link block gA θ takes the single link flow va ∈ R+ and single link features xA a ∈ X A a of one link a ∈ A as input and 9 outputs the corresponding link attributes πA a ∈ ΠA a , defined as: gA θ : R+ × X A a → ΠA a , where X A a ⊂ R|I A| is the feasible set of single link features; |I A| is the number of features associated with one link; ΠA a ⊂ R|S A| is the feasible set of single link attributes and |S A| is the number of link attributes considered by travelers. Note the input and output dimensions of the link block are independent of link numbers. Example 1 further illustrates how the proposed neural network architecture deals with changeable size inputs. The link attributes πA ∈ R|A|×|S A| are the concatenation of single link attributes, defined as: πA = { gA θ (va, xA a )⊤ } a∈A . Example 1 (Accommodate changeable input sizes) Consider a road network with a single OD pair connected by two parallel paths or links (i.e., link 1 and link 2). As shown in Figure 4, the link block takes the link flow, capacity, and free-flow time of link 1 as input and outputs the link travel time on link 1 (highlighted with red boxes). The input dimension is (1 + |I A| = 3) and the output dimension (|S A| = 1) are independent of the number of links in the road network. When a new link is added to the original road network, one dimension is added to path flow h (denoted as the slash box in Figure 4) whereas the input and output dimensions of the link block remain the same. Figure 4: Illustrations of link, node, and path blocks. Slash boxes denote the change in variables when another parallel link is added to the road network. Similarly, let node flow un be the sum of link flows from all approaches at node n ∈ N . To capture the interactions among link flows, node block gN θ : R+ × X N n → ΠN n maps the single 10 node flow un ∈ R+ and single node features xN n ∈ X N n ⊂ R|I N | of one node n ∈ N to its local node attributes πN n ∈ ΠN n ⊂ R|S N |. The node attributes πN ∈ R|N |×|S N | are the concatenation of single node attributes, defined as: πN = { gN θ (un, xN n )⊤ } n∈N . And the path block gP θ : R+ × X P p → ΠP p maps the single path flows hp ∈ R+ and single path features xP p ∈ X P p ⊂ R|I P | of one path p ∈ P to its path attributes πP p ∈ ΠP p ⊂ R|S P |. The path attributes are: πP = { gP θ (hp, xP p )⊤ } p∈P . Finally, the attributes π are the concatenation of link attributes πA, node attributes πN and path attributes πP , defined as: π = {ΛπA, ΓπN , πP } , (6) where Γ ∈ R|P |×|N | is the path-node incidence matrix. To facilitate training and enhance model performance, we can fully or partially replace each block with a pre-calibrated function, if available. For instance, we can replace the link block with the link performance functions calibrated by a planning agency. In addition, our future study will explore the use of convolution layers to accommodate changeable input sizes. The challenge will be to ensure the desired properties of the learned cost function. 3.2.2 Weight Net OD pairs can be added or removed in ”what-if” analysis thus Weight Net also needs to ac- commodate the change in the number of OD pairs. Weight Net learns a function fθ that maps the single traveler characteristics xR r ∈ X R r ⊂ R|I R| of one OD pair to its OD-specific weights wr ∈ Wr ⊂ R|S|, defined as: fθ(xR r ) : X R r → Wr, where |I R| is the number of traveler characteristics. The weights w ∈ W is represented as: w = Fθ(xR) = { fθ(xR r )⊤ } r∈R . The parameters of neural network fθ are shared among all OD-pairs to capture the repeated patterns in weights. Recent developments in interpretable neural-network-based discrete choice modeling, as discussed in Section 2.2, can be incorporated into the proposed framework and guide the design of neural network architectures, particularly when behavior interpretability is desired. 3.2.3 Regularization of cost function As shown in Theorem 1, the continuity of cost function Cθ(h, x) ensures the existence of equilib- ria. However, stronger properties of the cost function may be desired to ensure the uniqueness of equilibrium or enable an efficient solution algorithm. In this section, we seek to entail the 11 cost function with monotonicity and Lipschitz continuity via neural network regularization tech- niques. Both monotonicity, which suggests the path cost is non-decreasing as more travelers use this path, and Lipschitz continuity, which suggests a finite change in path flows results in a finite change in path costs, are mild assumptions but will largely enhance computational traceability (see Section 4). Theorem 2 shows sufficient conditions to entail the cost function with monotonicity and Lipschitz continuity. The proof is shown in Appendix B. Note that the context features x are independent of samples and only path flows are treated as variables in this case. Theorem 2 (Monotonicity and Lipschitz continuity of cost function) The cost function Cθ(h, x) defined in Eq.(2) is maximal monotone and Lipschitz continuous with respect to path flows h if weight w is positive and link block gA θ , node block gN θ and path block gP θ are column-wise monotone and Lipschitz continuous. Recall that each block is composed of fully connected layers. Let y(l−1) and σ(l) represent the input and activation function of the l-th layer respectively. The output of the l-th layer is calculated as y(l) = σ(l)(W(l)y(l−1) + b(l)), where W(l) and b(l) are learnable parameters of linear layers. We constrain the sign of weights w as strict positive by using SoftPlus 1 as the last layer of Weight Net Fθ. The column-wise monotonicity and Lipschitz continuity of attribute blocks, however, are more challenging to obtain. Most activation layers, such as ReLU and SoftPlus, are monotone and Lipschitz (Bibi et al., 2019) and both monotonicity and Lipschitz continuity are preserved via operator composition. Therefore, we only need to regularize the linear layer to entail the block with desired properties. Without loss of generality, we design a monotonic and Lipschitz continuous architecture that explicitly constrains the weights of the linear layers. More specifically, the weight of each linear layer is constrained to be positive to maintain monotonicity. The linear layer can be parameterized as W(l) = B⊤B + ιI with ι > 0 if strict monotonicity or strong monotonicity are desired. The spectral normalization as proposed by Miyato et al. (2018) is applied to constrain the spectral norm of each W(l) and maintain Lipschitz continuity. This explicit method is reliable, easy to implement, and shows satisfactory performances in our numerical experiments. Other regularization methods, such as adding heuristic penalty terms to the loss function or solving integral problems in forward propagation (Wehenkel and Louppe, 2019; Gouk et al., 2021) are open for exploration in our future study. 4 End-to-end Learning with an Implicit Layer We are now ready to discuss how to learn the parameters θ associated with the cost function Cθ(h, x) in the VI problem defined in Eq.(3). To train the neural networks, the VI is encapsulated as an implicit layer in a learning framework, which takes the context features x and demands (demand q[m] encapsulated in feasible set H[m]) as input and outputs user equilibrium flows. Then the neural networks are trained by comparing the computed equilibrium flows with observed flows. Consider a smooth loss function L : V × V → R that measures the distance between predicted equilibrium flows and real-world observations, the end-to-end learning problem can be formulated as the following Mathematical Program with Equilibrium Constraints (MPEC). 1σ(y(l)) = log(1 + exp y(l)) 12 min θ Eq∼Q [L(v⋆[m], v[m])] s.t. h⋆[m] ∈ VI (Cθ(h, x), H[m]) , ∀m ∈ M v⋆[m] = Λ⊤h⋆[m], ∀m ∈ M (7) where the demand q is assumed to follow a distribution Q; h⋆[m] and v⋆[m] are the equilibrium path and link flows on sample m respectively. This MPEC is nonconvex in general (Lawphong- panich and Hearn, 2004). Remark 1 If the cost function is independent of input feature x and equals the sum of link travel times and an entropy term (i.e., c(h) = Λt(h) + ln(h)), the learning problem defined in Eq.(7) will reduce to the logit dispersion parameter calibration problem investigated by Yang et al. (2001). If the equilibrium constraints are removed, the learning problem would directly learn a mapping from the context features x to link flows v. In this case, the problem reduces to neural-network-based short-term traffic flow prediction investigated in the literature (e.g., Yao et al. (2019)). The remainder of the section deals with two computational challenges to implementing im- plicit layers in the proposed framework. First, it requires efficiently solving a batch of VI prob- lems in the forward propagation, as previous methods for solving VI may not necessarily be suit- able for batch operations. Second, because solving VIs usually entails many iterations, explicit backpropagation through each iteration can be computationally expensive. Efficient differentia- tion through the implicit layer, i.e., the VI, is needed. 4.1 Forward: solving a batch of constrained VIs Batched operation is essential for training neural networks with massive empirical data. Instead of solving one constrained VI problem, the forward propagation in the proposed framework requires simultaneously solving a batch of VIs. Previous methods for solving VIs require re- peatedly calling external optimization libraries to project onto the polyhedron constraint set of feasible path flows, and thus may not necessarily be suitable for batch operations (Li et al., 2020). To efficiently solve a batch of constrained VIs, we adopt the recent developments in operator- splitting methods and reformulate the original VI problem as an auxiliary fixed point problem, which can be iteratively solved with closed-form update rules. More specifically, we decompose the polyhedron constraint set H[m] into two simpler sets H1 = {h ∈ R|P | : h ⪰ 0} and H[m] 2 = {h ∈ R|P | : Σ⊤h = q[m]}. Projection operators onto H1 and H[m] 2 have closed-form solutions 2 3that can be encoded with computational graphs and efficiently implemented in a batched manner (Heaton et al., 2021). We consider an auxiliary variable z[m] ∈ Z ⊂ R|P | and auxiliary fixed point operator Tθ(z[m], x) for any α > 0, defined as: Tθ(z[m], x) ≜ z[m] − PH1(z[m]) + PH[m] 2 (2PH1(z[m]) − z[m] − α Cθ(PH1(z[m]), x)) . 2PH1 (z[m]) = [z[m]]+ 3PH[m] 2 (z[m]) = z[m] − UE−1V⊤(Σz[m] − q[m]), where Σ = UEV is the compact singular value decomposition of Σ such that U and V have orthonormal columns and E is invertible. 13 The VI problem in Eq.(3) can be equivalently formulated as an auxiliary fixed point problem: z⋆[m] = Tθ(z⋆[m], x), and the equilibrium path flows are calculated as h⋆[m] = PH1(z⋆[m]). It is equivalent to say: h⋆[m] ∈ VI (Cθ(h, x), H[m]) ⇐⇒ z⋆[m] = Tθ(z⋆[m], x), h⋆[m] = PH1(z⋆[m]). (8) The equivalence can be established when the cost function Cθ(h, x) is maximal monotone (Heaton et al., 2021), which holds, as we have proved in Theorem 2, if the weight w is positive and all three blocks are column-wise monotone. Appendix C presents a brief proof of the equivalence between the VI problem and the auxiliary fixed point problems in Eq.(8). Interested readers can refer to Ryu and Yin (2021) and Heaton et al. (2021) for more details. We explore two iterative methods, fixed point iteration and root-finding, to solve the auxiliary fixed point problem. Two criteria are considered to measure the convergence at iteration k. To facilitate understanding, the superscripts for a sample m are omitted for the rest of this section. Definition 1 (Residual and relative residual) The residual ϕ measures the absolute change of the aux- iliary variable between two consecutive iterations, defined as: ϕ(k) ≜ ∥z(k+1) − z(k)∥. The relative residual φ measures the relative absolute change of the auxiliary variable between two consec- utive iterations, defined as: φ(k) ≜ ∥z(k+1) − z(k)∥ ∥z(k)∥ . 4.1.1 Fixed point iteration Starting with an initial point ˜z, the fixed point iteration repeats z(k+1) = Tθ(z(k), x) for each step until the relative residual is smaller than a threshold ε1 or the iteration step k exceeds the maximum number of iterations κ1. The algorithm heuristically adjusts the step size α1 when the current step size fails to reduce the relative residual. Algorithm 1 shows the algorithm details. The initial point ˜z is not necessarily feasible and will be projected onto the feasible region during the iteration. z(k+1/2) denotes auxiliary path flows that only satisfy nonnegativity constraints. The threshold and rule to heuristically adjust step sizes are represented as β1 and γ1 respectively. Proposition 1 Supposing that the cost function Cθ(h, x) is L-Lipschitz continuous and step size α1 ∈ (0, 2/L), the fixed point iteration z(k+1) = Tθ(z(k), x) converges linearly to the fixed point if one exists. The proof of Proposition 1 is shown in Appendix D. The selection of step size is vital. If the step size is too large, the fixed point iteration may diverge; if too small, the convergence can be extremely slow. The optimal step size depends on an unknown Lipschitz constant L, the exact computation of which is NP-hard (Virmaux and Scaman, 2018). We thus explore two variants of fixed point iteration to adjust the step sizes and speed up the convergence: Anderson mixing (Walker and Ni, 2011) and weighted ergodic iteration (Davis and Yin, 2017). 14 Algorithm 1 Fixed point iteration Input: x, q, ˜z Output: h⋆ 1: z(1) ← ˜z, z(0) ← ˜z, k ← 1, φ(0) = ε1 + 1 ▷ Initialization 2: while φ(k) > ε1 and k < κ1 do 3: z(k+1/2) ← PH1(z(k)) ▷ Update auxiliary path flow 4: y(k+1) ← PH2 (2z(k+1/2) − z(k) − α(k) 1 Cθ(z(k+1/2), x)) ▷ Projection 5: z(k+1) ← z(k) − z(k+1/2) + y(k+1) ▷ Update auxiliary variable 6: φ(k+1) ← φ(z(k), z(k+1)) ▷ Update relative residual 7: if φ(k+1)/φ(k) ≥ β1 then ▷ If fails to decrease the relative residual 8: α(k+1) 1 ← γ1 α(k) 1 ▷ Adjust step size 9: end if 10: k ← k + 1 ▷ Update iteration step 11: end while 12: h⋆ ← PH1(z(k)) ▷ Output equilibrium path flow Anderson mixing updates z(k+1) as an optimal linear combination of τ previous iterations, i.e., z(k+1) = ∑τ i=1 α(i) Tθ (z(k−i+1), x). The optimal step size α(i) solves a quadratic program min ∑τ i=1 α(i)=1 τ ∑ i=1 (ϕ(k−i+1)α(i))2 , where the objective function is to minimize the sum of residual norms over τ iterations. Another variant, weighted ergodic iteration, heuristically adjusts the step size at each iteration and updates z(k) as a linear combination of previous steps: z(k) = 2 (k + 1)(k + 2) k ∑ i=0(i + 1)z(i). Weighted ergodic iteration improves the convergence rate of fixed point iteration from O ( 1√k+1 ) to O ( 1 k+1 ) when the Jacobian matrix of Cθ(h, x) is symmetric (Davis and Yin, 2017), which holds, as shown in Appendix B, if the Attribute Net is constructed following Eq.(6). 4.1.2 Root-finding Solving for the auxiliary fixed point is equivalent to finding the root of z⋆ − Tθ(z⋆, x) = 0 via a root-finding method. The projection operator PH1(·) is non-differentiable at the boundary of a set and thus Newton’s method may diverge. Therefore, we use Broyden’s method, a quasi-Netown’s method that does not require differentiability. Broyden’s method approximates Newton’s direc- tion and updates the point as z(k+1) = z(k) − s(k). Let the initial guess be s(0) = −I and the direction is updated as: s(k+1) = s(k) + ∆z(k+1) − s(k)∆ϕ(k+1) ∆z(k+1)⊤s(k)∆ϕ(k+1) ∆z(k+1)⊤s(k), (9) 15 where ∆z(k+1) = z(k+1) − z(k) and ∆ϕ(k+1) = ϕ(k+1) − ϕ(k). The details of the root-finding method are shown in Algorithm 2, where the step size α2 is heuristically adjusted when the current step size fails to reduce the relative residual. Algorithm 2 Root-finding Input: x, q, ˜z Output: h 1: z(1) ← ˜z, z(0) ← ˜z, k ← 1, s(0) ← −I, φ(0) = ε2 + 1 ▷ Initialization 2: while φ(k) < ε2 and k < κ2 do 3: s(k) ← s(k−1) + ∆z(k)−s(k)∆ϕ(k) ∆z(k)⊤s(k−1)∆ϕ(k) ∆z(k)⊤s(k−1) ▷ Update direction 4: z(k+1) ← z(k) − α(k) 2 s(k) ▷ Update auxiliary variable 5: φ(k+1) ← φ(z(k), z(k+1)). ▷ Update relative residual 6: if φ(k+1)/φ(k) ≥ β2 then ▷ If fails to decrease the relative residual 7: α(k+1) 2 ← γ2 α(k) 2 ▷ Adjust step size 8: end if 9: k ← k + 1 ▷ Update iteration step 10: end while 11: h⋆ = PH1(z(k)) ▷ Output equilibrium path flow 4.2 Backward: differentiate through the fixed point solution In this section, we consider a multivariate function f (x, y) : Rp × Rm → Rn and denote the differentiation with respect to one argument x, as ∂ f ∂x ≜ ∂g(x)where g(x) = f (x, y). Note that ∂ f ∂x ∈ Rn×p is a matrix consistent with the input and output dimensions. For backpropagation, we need to efficiently differentiate through the fixed point solution z⋆ and adjust the parameter θ with ∂L ∂θ : ∂L ∂θ = ∂L ∂z⋆ ∂z⋆ ∂θ . Solving the auxiliary fixed point problem usually entails many iterations and thus explicitly backpropagating through each iteration to calculate ∂z⋆/∂θ can be computationally expensive and prone to vanishing or exploding gradients. Using the implicit function theorem, z⋆ is a continuous function of θ near the fixed point and ∂z⋆/∂θ can be expressed as follows Bai et al. (2019): ∂z⋆ ∂θ = (I − ∂Tθ(z⋆, x) ∂z⋆ )−1 ∂Tθ(z⋆, x) ∂θ , (10) where ∂Tθ(z⋆, x)/∂z⋆ and ∂Tθ(z⋆, x)/∂θ can be computed with automated differentiation. We further reduce the computational difficulty by approximating the matrix inversion in Eq.(10). Three methods are explored. First, the Jacobian-free backpropagation replaces (I − ∂Tθ (z⋆,x) ∂z⋆ )−1 with one identity matrix. This method can be viewed as a preconditioned gradient and only re- quires backpropagating through the final forward step (Fung et al., 2021). Second, an inverse matrix can be approximated with truncated Neumann series 4, reducing the computational cost 4A−1 ≈ ∑τ−1 i=0 (I − A)i 16 from matrix inversion to matrix-matrix multiplications. Third, the gradient of our interest can be reformulated and solved with another fixed point iteration, on which we elaborate in Appendix E. Interested readers can refer to Bai et al. (2019) for more details. Remark 2 Calculating the gradients of equilibrium flows h⋆ with respect to demand or supply-side per- turbations has been studied as equilibrium flow sensitivity analysis in the transportation literature. Tobin and Friesz (1988) showed that the Jacobian exists if the utilized path set remains the same with a small perturbation in parameters. Patriksson (2004) further suggested that the Jacobian exists if all unused paths remain unused with the perturbation. Li et al. (2020) pointed out the Jacobian exists if the cost function is strongly monotone in a neighborhood of h⋆. These conditions may not hold in a general setting. However, the aforementioned numerical methods work well in our numerical experiments. 5 Numerical Experiments In this section, we conduct a proof-of-concept of the proposed framework in the Sioux Falls network with |A| = 76 links, |N | = 28 nodes, and |R| = 528 OD pairs. The prediction accuracy and robustness of the proposed framework are tested in different scenarios. 5.1 Experiment setting 5.1.1 Training set generation Each OD pair r ∈ R is assumed to have one continuous feature x1 r denoting income and one binary feature x2 r denoting travel purpose, which equals 1 if the destination of OD pair r is a commercial area and equals 0 otherwise. We assume the path travel time includes two parts: link travel times and node delays. The link travel time on link a ∈ A follows the BPR function, i.e., ta(va) = ta (1 + 0.15 (va/ca)4), where ta and ca represent free-flow time and link capacity respectively. The node delay on node n ∈ N follows an exponential form as proposed by Jeihani et al. (2006), i.e., dn(un) = dn (un/cn)βn + γn, where dn, cn, βn, and γn are parameters depending on intersection layout. Moreover, pavement surface conditions, such as roughness, are the main feature that decides user comfort (Hawas, 2004; Yin et al., 2008). We classify the links as good and bad pavement conditions and assume travelers experience a non-link-additive discomfort ep on bad-condition links. Let 0 ≤ xp ≤ 1 denote the proportion of bad-condition link length to the total path length. The discomfort follows the exponential form and increases with the bad-condition link proportion, i.e., ep = exp(αxp) + β. We set α = 2 and β = −1 so that the discomfort is zero if path p only includes good-condition links. The ”ground-truth” cost for travelers of OD pair r to use path p ∈ Pr is a weighted sum of link travel times, node delays, and a discomfort constant: cp = ∑ a∈p ta(va) + wd r ∑ n∈p dn(un) + we r ep, where the class-specific weights for the node delays and the discomfort constant are wd r = (5.5 + x1 r ) · exp (1 − x2 r ) and we r = 10(x1 r ·x2 r ), respectively. This suggests that travelers with higher incomes 17 have higher weights on both node delays and discomfort. Travelers traveling to commercial areas have higher weights on discomfort yet lower weights on node delays. The feasible path set Pr includes the top three paths with the shortest free-flow time. If one OD pair has fewer than three feasible paths, its path flows are padded to a dimension of three and the padded path flows are nullified with the mask trick during training. Three demand levels are considered: (i) base scenario q0, (ii) uncongested scenario with base demand q0 reduced by 50%, and (iii) congested scenario with base demand q0 increased by 50%. For each scenario, we randomly sample travel demands from a uniform distribution between 0.5 q0 and 1.5 q0. The equilibrium flow is solved for each sampled demand given the ground-truth cost and one training sample is ((x, q[m]), v[m]). The training and test sets include 1, 536 and 512 samples respectively. So far all links are assumed to be observable. 5.1.2 Neural network architecture The link block is replaced with pre-calibrated BPR functions. Weight Net, node block, and path block are composed of three fully connected layers with four neurons and with LeaklyReLu as the activation function. Normalization layers are added to enhance training stability. The input of the node block includes node flows and intersection parameters. The proportion of bad-condition links is the input of the path block. The input and output dimensions are as follows: |I N | = 4, |I P | = 1, |I R| = 2, |S N | = |S P | = 1, and |S| = 3. Weighted ergodic iteration and fixed point approximation are used as the default forward and backward methods respectively. The model is trained with Adam optimizer with Mean Square Error as the loss function under the learning rate of 0.1. Early stop is enabled if no loss descent is observed in five consecutive epochs. To illustrate the feasibility and importance of learning route choice preferences, we bench- mark our model with three well-established network equilibrium models. First, the cost function is assumed to be link travel time and travelers choose the paths with minimum travel time, yield- ing conventional Deterministic User Equilibrium (denoted as DUE). The second behavior model assumes travelers’ path choices follow a logit model and thus results in a Stochastic User Equi- librium (denoted as SUE). In this case, the dispersion parameter is calibrated, similar to Yang et al. (2001). The third model keeps the same path choice model but assumes the cost function is a linear combination of link travel time and the proportion of bad-condition links (denoted as SUE-2). Two linear coefficients are calibrated in this case, similar to Guarda and Qian (2022). We compare the efficiency and robustness of different forward algorithms. The first type in- cludes fixed point iteration (F) and its accelerated variant: Anderson mixing (FA) and weighted ergodic iteration (FW). The second type is Broyden’s method (R). We also explore the combina- tions of two types (denoted as F-R, FA-R, FW-R), which use fixed point iterations initially and switch to the root-finding when the relative residential is sufficiently small. Details of algorithm hyperparameters are shown in Table 1. The stopping thresholds are ε1 = ε2 = 1e − 5. 5.1.3 Performance measurement We consider two types of tests: in-distribution and out-of-distribution. In in-distribution tests, the model is trained on observations from the Sioux Falls network G and tested on the same road network G. By contrast, in out-of-distribution tests, the trained model is tested on a partially 18 Name κ1 α1 β1 γ1 κ2 α2 β2 γ2 F 1e3 1e-3 1 0.9 - - - - FA 1e3 1e-3 1 0.9 - - - - FW 1e3 1e-3 1 0.9 - - - - R - - - - 1e2 1e-3 1 0.9 F-R 1e3 1e-3 1 0.9 1e2 1e-3 1 0.9 FA-R 10 1e-3 1 0.9 10 1e-3 1 0.9 FW-R 1e3 1e-3 1 0.9 1e2 1e-3 1 0.9 Table 1: Forward algorithm hyperparameters changed road network G ′. In our experiments, four links are added to the original Sioux Falls network and 25% links are randomly selected to increase or decrease their capacities by 50%. Decreasing the capacities under congested demand generates unreasonable training sets and is excluded in later analysis. Hereinafter, Mean Absolute Percentage Error (MAPE) is used to measure the percentage differences in link flow predictions. MAPE of one sample is: η ≜ 1 |A| ∑ a∈A |v⋆ a − va| va × 100%. 5.2 Performance comparisons Table 2 compares the MAPE of different network equilibrium models. The proposed end-to-end learning framework is denoted as ”Implicit” in Table 2. We use DUE as the baseline and denote its MAPE as η0. The change in MAPE of other models is denoted as ∆η = η − η0. Note that the behavioral assumptions of SUE are different from the ground truth. Although SUE can reduce the in-distribution MAPE by 18.2%, it shows inferior performance in out-of-distribution tests, increasing the MAPE by 9.2%. This suggests inaccurately assuming an SUE behavior model can cause bias in parameter estimation, misleading the flow prediction in subsequent ”what-if” analysis. Similar results have been shown in Torres et al. (2011) and Van Der Pol et al. (2014). In comparison, SUE-2 performs better, because it happens to capture the impact of discomfort from the bad-condition links. The performance of SUE-2 is still less satisfactory compared with the end-to-end framework because the former learns linear combinations by assumption whereas the latter can deal with nonlinear patterns. Since neural networks include more parameters than baseline models and offer greater flexibility to recover the complicated ground truth cost function, the proposed framework has the best performance in both in-distribution and out-of- distribution tests as expected, reducing the benchmark MAPE by 61.5% and 55.1% respectively. Figure 5 shows the training processes of different forward algorithms. R converges the fastest within nine epochs whereas FW converges the slowest after 27 epochs. Combining F or FA with R slows down the convergence and hurts the training performance. As shown in Table 3, FW and FW-R achieve the smallest MAPE of 5.7% in in-distribution tests whereas FW-R slightly outperforms FW by 1% in out-of-distribution tests. Forward algorithms involving Anderson mixing, such as FA and FA-R, can be the most unstable. By contrast, forward 19 In-distribution test Demand Capacity DUE η0 SUE ∆η SUE-2 ∆η Implicit ∆η Base Default 20.6 -4.7 -11.8 -15.0 Uncongested Default 12.5 -3.1 -0.2 -3.4 Congested Default 13.41 -0.6 -4.4 -10.2 Mean 15.5 -2.8 ( - 18.2%) -5.4 ( - 35.1%) -9.5 ( - 61.5%) Out-of-distribution test Demand Capacity DUE η0 SUE ∆η SUE-2 ∆η Implicit ∆η Base Default 22.3 -7.3 -14.4 -16.6 -50% 11.3 +13.4 -1.6 -7.9 +50% 8.1 +4.8 -1.0 -1.3 Uncongested Default 23.4 -8.5 -15.6 -14.9 -50% 12.1 +12.6 -2.4 -4.1 +50% 10.4 +2.5 -3.3 -1.1 Congested Default 13.8 -3.5 -6.3 -10.1 +50% 11.9 -3.5 -5.2 -6.4 Mean 14.2 +1.3 ( +9.2%) -6.2 ( - 44.0%) -7.8 ( -55.1%) Table 2: MAPE of different network equilibrium models. MAPEs are shown in percentage. Figure 5: Training process of different forward algorithms. algorithms involving weighted ergodic iteration, such as FW and FW-R, are more stable as it consistently shrinks the step size during iterations. Figure 6 compares the performance of three backpropagation methods: Jacobian-Free (JF) ap- proximation, Newman Approximation (NA), and Fixed point Approximation (FA) under differ- ent demand levels. FA has the best performance among the three proposed backward methods. JF significantly hurts the learning process. Similar results have been found by Huang et al. (2021). The effects of spectral normalization are shown in Figure 7. Although requiring additional computation, the spectral normalization constrains the Lipshitz constant of the cost function within a reasonable range and speeds up the convergence by three to four times under all de- mand levels. 20 In-distribution test Demand Capacity F FA FW R F-R FA-R FW-R Base Default 8.4 5.6⋆ 5.7 8.0 8.7 6.2 6.0 Uncongested Default 9.5 9.1 8.1 9.5 8.3 8.5 8.0⋆ Congested Default 6.1 3.2 3.2 6.2 11.0 4.5 3.1⋆ Mean 8.0 6.0 5.7⋆ 7.9 9.3 6.4 5.7⋆ Std 1.8 3.0 2.4 1.7 1.4 2.0 2.5 Out-of-distribution test Scenario Capacity F FA FW R F-R FA-R FW-R Base Default 7.5 5.7⋆ 5.8 7.2 7.7 6.4 6.0 - 50% 4.5 3.4⋆ 3.4⋆ 5.0 4.8 3.6 3.4⋆ +50% 9.1 6.9⋆ 7.0 10.2 9.3 8.8 7.4 Uncongested Default 8.3 8.5 7.6 8.1 8.0 8.2 7.5⋆ - 50% 9.5 8.0 7.3 9.9 7.6 14.4 6.9⋆ +50% 8.4 9.4 7.9⋆ 8.4 8.2 7.9⋆ 7.9⋆ Congested Default 5.7 3.6⋆ 3.8 5.1 10.2 4.3 3.6⋆ +50% 8.8 5.5⋆ 5.7 6.2 11.9 6.1 5.5⋆ Mean 7.7 6.4 6.1 7.5 8.5 7.5 6.0⋆ Std 1.8 2.2 1.7 2.0 2.1 3.4 1.7 Table 3: MAPE of proposed forward algorithms. MAPEs are shown in percentage and superscript ⋆ denotes the best performance of each scenario. (a) (b) (c) Figure 6: Performances of different backpropagation methods under (a) base, (b) uncongested, and (c) congested demand. We conclude this section by sharing some tips on training the proposed framework. The weighted ergodic iteration and fixed point approximations are recommended as default forward and backward methods. The step size turns out to be the most important hyperparameter and one should always start by fine-tuning it. Spectral normalization is recommended to constrain the Lipshitz constant and speed up the convergence. 21 (a) (b) (c) Figure 7: Effects of spectral normalization under (a) base, (b) uncongested, and (c) congested demand. ”w” suggests ”with spectral normalization” and ”w/o” suggests ”without spectral normalization”. 5.3 Robustness analysis In this section, we examine the robustness of the proposed framework by relaxing model assump- tions. FW, R, and FW-R have the best performance among fixed point iterations, root-finding, and combined methods and are thus selected. Since in-distribution and out-of-distribution per- formances have similar trends, all the following analyses are based on in-distribution tests. All links are assumed to be observable in previous analyses. We relax this assumption by randomly observing a proportion of links. FW-R is the most stable when only a proportion of links are equipped with sensors (see Figure 8). For example, Figure 8b shows the MAPE of FW-R slightly increases from 8.0% to 11.5% when the proportion of observable links decreases from 100% to 20% under uncongested demand. Since approximation errors can accumulate in both forward propagation, where iterations terminate with residuals, and backward propagation, where the gradients are approximated, the training of the proposed framework can stop at local optimums. Previous studies have shown the training process and final performances of models involving implicit layers can be relatively noisy and require more hyperparameter tuning (Huang et al., 2021; Li et al., 2020). Usually, there are no direct observations of OD demands in urban road networks. OD de- mands need to be estimated and thus prone to estimation errors. We examine the model per- formances when the input OD demands are different from the ground truth. More specifically, random observation noises, which are proportional to the ground-truth, are added to all de- mands. As shown in Figure 9, FW is the most stable in the case of demand noises. Given a noise scale of 100%, the increase in its MAPE ranges from 12.5% to 22.2% under different demand lev- els. Note that if we consider an elastic demand user equilibrium, the travel demand function can also be approximated with another neural network and learned with the proposed framework. The simultaneous learning of route choice preferences and demand functions will be explored in our future study. The selection of feasible path sets can be tricky when no information about path choices is available. We examine the model performances when the selection of feasible paths is different 22 (a) (b) (c) Figure 8: Model performances with different sensor coverage rates under (a) base, (b) uncon- gested, and (c) congested demand. (a) (b) (c) Figure 9: Model performances with demand noises under (a) base, (b) uncongested, and (c) congested demand. from travelers’ actual path choices. There are 1,587 paths in the ground-truth path set and we consider two scenarios: one with an incomplete path set of 1,058 paths and the other with a redundant path set of 2,645 paths. FW-R has the best performance when the selection of feasible paths is inaccurate (see Figure 10). As shown in Figure 10a, an incomplete path set increases the MAPE by 8.0% under base demand, compared with an increase of 2.9% induced by a redundant path set. Since an incomplete path set yields more negative effects, one can start with a large feasible set with sufficient feasible paths and gradually reduces it during training. To sum up, the proposed framework is robust to incomplete observations and input noises. More specifically, the combined method (i.e., FW-R) is more robust when only a proportion of links are equipped with sensors or no information about path choice is available. The fixed-point iteration method (i.e., FW) is preferred when the input OD demands are poorly estimated. 23 (a) (b) (c) Figure 10: Effects of inaccurate feasible path sets under (a) base, (b) uncongested, and (c) con- gested demand. 6 Conclusion This study has proposed an end-to-end framework for transportation network equilibrium anal- ysis, which directly learns travel choice preferences and the equilibrium state from multi-day link flow observations. Travelers’ route choice preferences are represented with deep neural net- works and embedded in a VI that prescribes the user equilibrium flow distribution. The neural network architecture is designed to simultaneously entail the cost function with monotonicity and Lipschitz continuity and accommodate the change of road network topology in the subse- quent ”what-if” planning analysis. To enable efficient batch operations in forward propagation, the VI is reformulated as an auxiliary fixed point problem and is then embedded as an implicit layer in a learning framework. Two iterative algorithms, i.e., fixed point iteration and root-finding method, are explored to solve the auxiliary fixed point problem. The proposed end-to-end frame- work is tested in the Sioux Falls network. Our numerical experiments show that the framework achieves 94.0% accuracy in link flow prediction when the road network topology changes and is robust to incomplete observations and input noises. The proposed framework is flexible and can be applied to model various travel choices and learn supply-side components. We plan to extend the proposed framework to enable learning of another travel choice (e.g., trip generation, which implies learning travel demand functions), feasible path set, or combined choices (e.g., simultaneous choice of destination and route), by considering the availability of multi-source data (e.g., trajectories and observations of travel time, link flow, OD demand or trip productions/attractions). For each scenario, we will investigate appropriate architectures to facilitate end-to-end learning, and tailor efficient training algorithms for each learning problem. We also plan to leverage the established end-to-end learning framework to prescribe improve- ment schemes, such as capacity expansion and congestion pricing. Consider that policymakers attempt to perturb the equilibrium flow distribution by changing certain continuous decision variables that would affect travelers’ route choice. These decision variables can be encoded as additional learnable parameters in Attribute Net. By maximizing the expected social welfare, the proposed end-to-end framework can be trained to update the decision variables and output op- 24 timal decisions. Additionally, the proposed framework has been tested on a synthesized dataset. We plan to validate the proposed framework with real-world datasets in the next step. 7 Acknowledgement The work described in this paper was partly supported by research grants from General Motors, the USDOT Center for Connected and Automated Transportation and National Science Founda- tion. References Bai, S., Kolter, J.Z., Koltun, V., 2019. Deep equilibrium models. Advances in Neural Information Processing Systems 32. Beckmann, M., McGuire, C.B., Winsten, C.B., 1956. Studies in the Economics of Transportation. Technical Report. Bekhor, S., Toledo, T., 2005. Investigating path-based solution algorithms to the stochastic user equilibrium problem. Transportation Research Part B: Methodological 39, 279–295. Bibi, A., Ghanem, B., Koltun, V., Ranftl, R., 2019. Deep layers as stochastic solvers . Chen, C., Ma, J., Susilo, Y., Liu, Y., Wang, M., 2016. The promises of big data and small data for travel behavior (aka human mobility) analysis. Transportation research part C: emerging technologies 68, 285–299. Davis, D., Yin, W., 2017. A three-operator splitting scheme and its optimization applications. Set-valued and variational analysis 25, 829–858. Emberton, J., 2008. An elucidation of vector calculus through differential forms . Feng, Z., Narasimhan, H., Parkes, D.C., 2018. Deep learning for revenue-optimal auctions with budgets, in: Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems, pp. 354–362. Fioretto, F., Mak, T.W., Van Hentenryck, P., 2020. Predicting ac optimal power flows: Combining deep learning and lagrangian dual methods, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 630–637. Frejinger, E., Bierlaire, M., Ben-Akiva, M., 2009. Sampling of alternatives for route choice model- ing. Transportation Research Part B: Methodological 43, 984–994. Fung, S.W., Heaton, H., Li, Q., McKenzie, D., Osher, S.J., Yin, W., 2021. Fixed point networks: Implicit depth models with jacobian-free backprop . Gouk, H., Frank, E., Pfahringer, B., Cree, M.J., 2021. Regularisation of neural networks by enforc- ing lipschitz continuity. Machine Learning 110, 393–416. 25 Guarda, P., Qian, S., 2022. Statistical inference of travelers’ route choice preferences with system- level data. arXiv preprint arXiv:2204.10964 . Hawas, Y.E., 2004. Development and calibration of route choice utility models: factorial experi- mental design approach. Journal of transportation engineering 130, 159–170. Heaton, H., McKenzie, D., Li, Q., Fung, S.W., Osher, S., Yin, W., 2021. Learn to predict equilibria via fixed point networks. arXiv preprint arXiv:2106.00906 . Huang, Z., Bai, S., Kolter, J.Z., 2021. Implicit layers for implicit representations. Advances in Neural Information Processing Systems 34, 9639–9650. Jeihani, M., Lawe, S., Connolly, J., 2006. Improving traffic assignment model using intersection delay function. Technical Report. Lawphongpanich, S., Hearn, D.W., 2004. An mpec approach to second-best toll pricing. Mathe- matical programming 101, 33–55. Li, J., Yu, J., Nie, Y., Wang, Z., 2020. End-to-end learning and intervention in games. Advances in Neural Information Processing Systems 33, 16653–16665. Ma, W., Pi, X., Qian, S., 2020. Estimating multi-class dynamic origin-destination demand through a forward-backward algorithm on computational graphs. Transportation Research Part C: Emerging Technologies 119, 102747. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y., 2018. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957 . Patriksson, M., 2004. Sensitivity analysis of traffic equilibria. Transportation Science 38, 258–281. Rahman, R., Hasan, S., 2022. Data-driven traffic assignment: A novel approach for learning traffic flow patterns using a graph convolutional neural network. arXiv preprint arXiv:2202.10508 . Ryu, E., Yin, W., 2021. Large-scale convex optimization via monotone operators (2020). URL https://large-scale-book. mathopt. com/LSCOMO. pdf.(visited on 03/2021) . Sifringer, B., Lurkin, V., Alahi, A., 2020. Enhancing discrete choice models with representation learning. Transportation Research Part B: Methodological 140, 236–261. Simon, H.A., 1955. A behavioral model of rational choice. The quarterly journal of economics 69, 99–118. Spana, S., Du, L., 2022. Optimal information perturbation for traffic congestion mitigation: Gaus- sian process regression and optimization. Transportation Research Part C: Emerging Technolo- gies 138, 103647. Tobin, R.L., Friesz, T.L., 1988. Sensitivity analysis for equilibrium network flow. Transportation Science 22, 242–250. 26 Torres, C., Hanley, N., Riera, A., 2011. How wrong can you be? implications of incorrect utility function specification for welfare measurement in choice experiments. Journal of Environmen- tal Economics and Management 62, 111–121. Travacca, B., El Ghaoui, L., Moura, S., 2020. Implicit optimization: Models and methods, in: 2020 59th IEEE Conference on Decision and Control (CDC), IEEE. pp. 408–415. Tversky, A., Kahneman, D., 1992. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty 5, 297–323. Van Der Pol, M., Currie, G., Kromm, S., Ryan, M., 2014. Specification of the utility function in discrete choice experiments. Value in Health 17, 297–301. Virmaux, A., Scaman, K., 2018. Lipschitz regularity of deep neural networks: analysis and efficient estimation. Advances in Neural Information Processing Systems 31. Walker, H.F., Ni, P., 2011. Anderson acceleration for fixed-point iterations. SIAM Journal on Numerical Analysis 49, 1715–1735. Wang, S., Mo, B., Zhao, J., 2020. Deep neural networks for choice analysis: Architecture design with alternative-specific utility functions. Transportation Research Part C: Emerging Technolo- gies 112, 234–251. Wang, Y., Ma, X., Liu, Y., Gong, K., Henricakson, K.C., Xu, M., Wang, Y., 2016. A two-stage al- gorithm for origin-destination matrices estimation considering dynamic dispersion parameter for route choice. PloS one 11, e0146850. Wardrop, J.G., 1952. Road paper. some theoretical aspects of road traffic research. Proceedings of the institution of civil engineers 1, 325–362. Wehenkel, A., Louppe, G., 2019. Unconstrained monotonic neural networks. Advances in neural information processing systems 32. Wong, M., Farooq, B., 2021. Reslogit: A residual neural network logit model for data-driven choice modelling. Transportation Research Part C: Emerging Technologies 126, 103050. Wu, X., Guo, J., Xian, K., Zhou, X., 2018. Hierarchical travel demand estimation using multi- ple data sources: A forward and backward propagation algorithmic framework on a layered computational graph. Transportation Research Part C: Emerging Technologies 96, 321–346. Xu, H., Lou, Y., Yin, Y., Zhou, J., 2011. A prospect-based user equilibrium model with endogenous reference points and its application in congestion pricing. Transportation Research Part B: Methodological 45, 311–328. Yang, H., Meng, Q., Bell, M.G., 2001. Simultaneous estimation of the origin-destination matrices and travel-cost coefficient for congested networks in a stochastic user equilibrium. Transporta- tion science 35, 107–123. 27 Yao, H., Tang, X., Wei, H., Zheng, G., Li, Z., 2019. Revisiting spatial-temporal similarity: A deep learning framework for traffic prediction, in: Proceedings of the AAAI conference on artificial intelligence, pp. 5668–5675. Yin, Y., Lawphongpanich, S., Lou, Y., 2008. Estimating investment requirement for maintaining and improving highway systems. Transportation Research Part C: Emerging Technologies 16, 199–211. 28 A Notations Name Notation Description Sets node set N link set A OD pair set R path set P path set for OD pair r Pr sample set M feasible demand set Q Q = {q ∈ R|R| : q ≥ 0} link flow observation set V V = {v ∈ R|A| : v ≥ 0} traveler characteristic set X R X R ⊂ R|R|×|I R| characteristic set of a single trav- eler X R r X R r ⊂ R|I R| feature set of a single link X A a X A a ⊂ R|I A| feature set of a single node X N n X N n ⊂ R|I N | feature set of a single path X P p X P p ⊂ R|I P | attribute set Π Π ⊂ R|P |×|S| attributes set of a single link ΠA a ΠA a ⊂ R|S A| attribute set of a single node ΠN n ΠN n ⊂ R|S A| attribute set of a single path ΠP p ΠP p ⊂ R|S A| feasible link flow set V [m] V [m] = {v ∈ R|A| : v ≥ 0, Σ⊤Λv = q[m]} feasible node flow set U [m] U [m] = {u ∈ R|N | : u ≥ 0, Σ⊤Γu = q[m]} feasible path flow set H[m] H[m] = {h ∈ R|P | : h ≥ 0, Σ⊤h = q[m]} feasible auxiliary variable set Z Z ⊂ R|P | feasible cost set C C = {c ∈ R|P | : c ≥ 0} Parameters path-link incidence matrix Λ Λ ∈ R|P |×|A|, Λpa = 1 if link a is on path p path-node incidence matrix Γ Γ ∈ R|P |×|N |, Γpn = 1 if node n is on path p path-OD incidence matrix Σ Σ ∈ R|P |×|R|, Σpr = 1 if path p connects OD pair r OD demands q[m] q[m] ∈ Q context features x x ∈ X traveler characteristics xR xR ∈ X R characteristics of a single traveler xR r xR r ∈ X R r road network features xG xG ∈ X G features of a single link xA a xA a ∈ X A a features of a single node xN n xN n ∈ X N n features of a single path xP p xP p ∈ X P p cost function parameters θ Variables path flows h[m] h[m] ∈ H[m] 29 link flows v[m] v[m] ∈ V [m] node flows u[m] u[m] ∈ U [m] auxiliary variables z[m] z[m] ∈ Z costs c[m] c[m] ∈ C weights w w ∈ W attributes π[m] π[m] ∈ Π link attributes πA[m] πA[m] ∈ ΠA node attributes πN [m] πN [m] ∈ ΠN path attributes πP [m] πP [m] ∈ ΠP attributes of a single link πA[m] a πA[m] a ∈ ΠA a attributes of a single node πN [m] n πN [m] n ∈ ΠN n attributes of a single path πP [m] p πP [m] p ∈ ΠP p residual ϕ relative residual φ Functions cost function Cθ Cθ : H[m] × X → C Weight Net Fθ Fθ : X R → W Attribute Net Gθ Gθ : H[m] × X G → Π link block gA θ gA θ : R+ × X A a → ΠA a node block gN θ gN θ : R+ × X N n → ΠN n path block gP θ gP θ : R+ × X P p → ΠP p loss function L L : V × V → R Forward algorithm hyperparameters maximum iterations κ κ1 (κ2) for fixed point iteration (root-finding method) stopping threshold ε > 0 ε1 (ε2) for fixed point iteration (root-finding method) step size α > 0 α1 (α2) for fixed point iteration (root-finding method) step size adjust threshold β > 0 β1 (β2) for fixed point iteration (root-finding method) step size adjust factor γ > 0 γ1 (γ2) for fixed point iteration (root-finding method) B Proof of Theorem 2 To facilitate understanding, this section omits the superscript for a sample m and the dependence upon both context features x and neural network parameters θ. ∥A∥ = supx̸=0 ∥Ax∥ ∥x∥ represents the spectral norm of matrix A. The Jacobian matrix of a vector-to-vector function F(x) : Rn → Rm is denoted as JF(x) ∈ Rm×n. We first give the formal definition of monotonicity and Lipschitz continuity of a vector-to- vector function F(x) and equivalent conditions when the function is a self-mapping and differ- entiable everywhere on its domain. The equivalent conditions are more tractable and used in proving the monotonicity and Lipschitz continuity of the cost function. Definition 2 (Monotonicity) A function F(x) : Rn → Rm is monotone if ⟨F(x) − F(y), x − y⟩ ≥ 0, ∀x, y ∈ Rn. 30 A differentiable function F(x) : Rn → Rn is monotone if and only if its Jacobian matrix JF(x) ∈ Rn×n is positive-semidefinite for all x ∈ Rn. Definition 3 (Lipschitz Continuity) A function F(x) : Rn → Rn is L-Lipschitz continuous if there exists L > 0, such that ∥F(x) − F(y)∥ ≤ L ∥x − y∥ , ∀x, y ∈ Rn. A differentiable function F(x) : Rn → Rn is L-Lipschitz continuous if and only if its Jacobian matrix JF(x) ∈ Rn×n has finite spectral norms for all x ∈ Rn. To begin with, consider a special one-column scenario where Attribute Net has only one link block and the output of the link block has one column, i.e., gA(va) : R+ → R. By assumption, gA(va) is monotone and Lipschitz continuous with respect to va, i.e., 0 ≤ dgA dva ≤ L with L > 0. Let GA(v) : R|A| → R|A| denote the mapping from link flows v to link attributes πA, defined as GA(v) = {gA(va)}a∈A. Its Jacobian matrix, JGA (v) = Diag ({ dgA dva } a∈A ) , is a diagonal matrix with nonnegative and finite elements. It is straightforward to show that JGA (v) is positive-semidefinite with finite spectral norm ∥ JGA (v)∥ ≤ maxa∈A { dgA dva } = L. Recall that the attributes π are the product of path-link incidence matrix Λ and link attributes. The Attribute Net G(h) : R|P | → R|P | is now defined as a self-mapping with respect to path flows, i.e., G(h) = Λ GA(Λ⊤h). It follows that the Jacobian matrix of G(h), JG(h) = Λ JGA (v) Λ⊤, is symmetric and positive-semidefinite. Path-link incidence matrix Λ is a 0-1 matrix with bounded spectral norm. As per Cauchy–Schwarz inequality, the spectral norm ∥ JG(h)∥ ≤ L∥Λ∥2. The cost function C(h) : R|P | → R|P | is formulated as C(h) = Σ w ⊙ G(h). Suppose the weights are positive (w > 0), the Jacobian matrix of the cost function, JC(h) = Diag(Σ w) JG(h), is the product of two symmetric positive-semidefinite matrices and thus symmetric positive- semidefinite with spectral norm bounded by ∥ Diag(Σ w)∥ ∥ JG(h)∥. It is equivalent to say the cost function C(h) is monotone and Lipschitz continuous with respect to the path flows h. This proof can be adapted to node block and path block by replacing the path-link incidence matrix Λ with the path-node incidence matrix Γ or an identity matrix. Now we consider a general case. Let wi denote the i-th column of weights and Gi denote the i-th column of attributes. The cost function C(h) is: C(h) = Σ w ⊙ G(h) 1 = |S| ∑ i=1 Σ wi ⊙ Gi(h). Suppose each block is column-wise monotone and Lipschitz continuity, wi ⊙ Gi(h) is monotone and Lipschitz continuous following previous proof for one-column scenarios. Monotonicity and 31 Lipschitz continuity are preserved under summation, its follows that the cost function C(h) is monotone and Lipschitz continuous with respect to the path flows h. Additionally, it is straightforward to show that the Jacobian matrix of the cost function JC(h) is the sum of |S| symmetric matrix and thus is symmetric. Suppose JC(h) is real everywhere, there exists a scalar function y(h) : R|P | → R such that C(h) is the gradient of y (Emberton, 2008). Under mild assumptions that y is closed and proper, the monotonicity of C(h) is equivalent to maximal monotonicity (Ryu and Yin, 2021). This completes the proof. C Proof sketch of the auxiliary fixed point reformulation To begin with, we formally state three relevant definitions. Definition 4 (Resolvent and reflected resolvent of a monotone operator) Consider a maximal op- erator A : Rn → Rn and α > 0, the resolvent of αA is: JαA ≜ (I + αA)−1, and the reflected resolvent of αA is: RαA ≜ 2JαA − I. Definition 5 (Indicator function) For Ω ⊂ Rk, the indicator function is δΩ(u) = {0 if u ∈ Ω ∞ otherwise The resolvent of the subgradient of indicator fucntion ∂δΩ is just the projection operator i.e., J∂δΩ = PΩ. Definition 6 (Cocoercivity) A single-valued operator A : Rn → Rn is β-cocoercive if β > 0 and ⟨Ax − Ay, x − y⟩ ≥ β∥x − y∥2 ∀x, y ∈ Rn. Cocoercivity is the dual property of strong monotonicity. When A is β-cocoercive, A is (1/β)-Lipschitz continuous. The converse is not necessarily true. Two theorems from Ryu and Yin (2021) are cited without proof. Theorem 3 (Three operator splitting) Consider three maximal monotone operators A, B, and C, with C single-valued. Then for any α > 0, h is a solution to the operator inclusion problem if and only if there is a z is the fixed point of operator ( 1 2 I + 1 2 T) and h = JαBz. 0 ∈ (A + B + C)h ⇔ ( 1 2 I + 1 2 S) z = z, h = JαBz (11) where S = RαA (RαB − αCJαB) − αCJαB. 32 Theorem 4 (Convergence of average operator) Suppose C is β-cocoercive and α ∈ (0, 2β), the fixed point iteration defined as, z(k+1/2) = JαB(z(k)) y(k+1) = JαA (2z(k+1/2) − z(k) − αCz(k+1/2)) z(k+1) = z(k) + y(k+1) − z(k+1/2) converges to a fixed point if one exists. Let ∂δH(h) denote the subgradient of the indicator function on the feasible path flow set H. Solving a constrained VI problem is equivalent to solving an operator inclusion problem (Heaton et al., 2021). h⋆ ∈ VI(Cθ(h, x), H) ⇐⇒ 0 ∈ Cθ(h, x) + ∂δH(h) It is straightforward to show that δH(u) = δH1(u) + δH2(u) for H = H1 ∩ H2. And in our case, it follows: h⋆ ∈ VI(Cθ(h, x), H1 ∩ H2) ⇐⇒ 0 ∈ Cθ(h, x) + ∂δH1(h) + ∂δH2(h) Let A = ∂δH2, B = ∂δH1, and C = Cθ(h, x) which is maximal monotone and 1/L-co-coercivity as shown in Appendix B and D. Apply it to Theorem 3, it follows: T = 1 2 I + 1 2 S = I − JαB + JαA (RαB − αCJαB) = I − Jα∂δH1 + Jα∂δH2 (Rα∂δH1 − α Cθ ( Jα∂δH1 )) = I − PH1 + PH2 (2PH1 − I − α Cθ ( PH1)) D Proof of Proposition 1 Appendix B shows the cost function C(h) is the gradient of a differentiable convex function y(h). In this case, its L-Lipschitz continuity is equivalent to 1/L-co-coercivity (Ryu and Yin, 2021). According to Theorem 4, given Cθ(h, x) is 1/L-co-coercive and α ∈ (0, 2 L ), 1 2 I + 1 2 S is an average operator. Starting with any point z0 ∈ Rn, the following fixed point iteration converges to the fixed point of operator T. z(k+1/2) = PH1(z(k)) y(k+1) = P2 (2z(k+1/2) − z(k) − α Cθ (z(k+1/2))) z(k+1) = z(k) − z(k+1/2) + y(k+1) And the equilibrium flow h⋆ = JαBz⋆ = PH1(z⋆). E Discussion about fixed point approximation for implicit differenti- ation 33 In backpropagation, the gradient of interest is: ( ∂z⋆ ∂θ )⊤ ( ∂L ∂z⋆ ) = ( ∂Tθ(z⋆, x) ∂θ )⊤ (I − ∂Tθ(z⋆, x) ∂z⋆ )−⊤ ( ∂L ∂z⋆ ) ︸ ︷︷ ︸ g , (12) where ∂L ∂z⋆ denote the input gradient of the implicit layer. It turns out that g can be rearranged as the solution of another fixed point problem defined as: g = ( ∂Tθ(z⋆, x) ∂z⋆ )⊤ g + ( ∂L ∂z⋆ ). After solving for g with fixed point iteration, the gradient of interest in Eq. (12) can be calculated with via typical automatic differentiation. The fixed point iteration converges if the Jacobian ∂Tθ (z⋆,x) ∂θ is a stable matrix with maximum eigenvalue that has a magnitude less than one. Previous studies show that these iterations typically are convergent in practice (Bai et al., 2019). 34","libVersion":"0.3.2","langs":""}
{"path":"lit/sources/Urban23ChatGPTImprovesCreative.pdf","text":"CHATGPT AND CREATIVE PROBLEM-SOLVING 1 ChatGPT Improves Creative Problem-Solving Performance in University Students: An Experimental Study Marek Urban a*, Filip Děchtěrenko a,b, Jiří Lukavský a,b, Veronika Hrabalová b, Filip Svacha c, Cyril Brom d, Kamila Urban e a Institute of Psychology, The Czech Academy of Sciences, Hybernska 8, 110 00 Prague, Czech Republic b Faculty of Arts, Charles University, nam. J. Palacha 1/2, 116 38 Prague, Czech Republic c Faculty of Humanities, Charles University, Patkova 2137/5, 182 00 Prague, Czech Republic d Faculty of Mathematics and Physics, Charles University, V Holesovickach 747/2, Prague, Czech Republic e Institute for Research in Social Communication, Slovak Academy of Sciences, Dubravska cesta 9, 845 11 Bratislava, Slovakia * First and corresponding author: Marek Urban, marek.m.urban@gmail.com, ORCID 0000-0003-2772-138 Co-authors: Filip Děchtěrenko, ORCID 0000-0003-0472-915X Jiří Lukavský, ORCID 0000-0002-1082-229X Veronika Hrabalová, ORCID 0000-0001-7234-2243 Filip Svacha, ORCID 0000-0001-8593-8943 Cyril Brom, ORCID 0000-0001-5945-0514 Kamila Urban, ORCID 0000-0003-4547-9804 August 29th, 2023. This preprint (version 2.0) is currently undergoing peer review. Contents may change prior to publication. CHATGPT AND CREATIVE PROBLEM-SOLVING 2 ChatGPT Improves Creative Problem-Solving Performance in University Students: An Experimental Study Abstract: University students often employ generative artificial intelligence tools such as ChatGPT in resolution of ill-defined problem-solving tasks. However, the experimental evidence about effects of ChatGPT on complex problem-solving performance is still missing. In this preregistered experiment, the impact of ChatGPT on performance in a complex creative problem-solving task was investigated in 77 university students solving a task with ChatGPT in comparison to 68 students solving a task without it. ChatGPT use significantly improved self-efficacy for task resolution (d = 0.65) and enhanced the quality (d = 0.69), elaboration (d = 0.61), and originality (d = 0.55) of solutions. Moreover, participants with ChatGPT assistance perceived task as easier (d = 0.56) and requiring less mental effort (d = 0.58). However, use of ChatGPT did not make task resolution more interesting (d = 0.08), and the impact of ChatGPT on metacognitive monitoring accuracy was unclear. Although there were no significant differences in absolute accuracy between students solving the task with and without the assistance of ChatGPT, the absence of correlation between self- evaluation judgments and performance suggests that participants struggled to calibrate their self-evaluations when using ChatGPT. Notably, the perceived usefulness of ChatGPT appeared to inform self-evaluation judgments, resulting in higher inaccuracy. The implications for hybrid human-AI regulation (HHAIR) theory are discussed. To regulate effectively, students using AI tools should focus on valid metacognitive cues and ignore cues that could lead to inaccurate outcomes. Keywords: generative AI, ChatGPT, creativity, metacognitive monitoring, metacognitive experiences, ill-defined problem-solving task CHATGPT AND CREATIVE PROBLEM-SOLVING 3 ChatGPT Improves Creative Problem-Solving Performance in University Students: An Experimental Study Introduction Ill-defined problem-solving tasks, such as essay writing, case study analysis, or decision- making dilemmas (Jonassen, 2011), are widely employed in STEM (science, technology, engineering, and mathematics) education (van Gog et al., 2020), where fostering creativity and innovation is an underlying goal (DeHaan, 2009; DeHaan & Narayan, 2008). Students who solve ill-defined problem-solving tasks in class are better at transferring knowledge from one task to another, applying academic knowledge outside academia, and generating novel insights and innovations (Walker & Leary, 2009). Ill-defined problem-solving tasks foster student critical-thinking skills (Liu & Pasztor, 2022) and motivation to learn (Demirel & Dagyar, 2016). In ill-defined problems, the goals, procedure, and evaluation criteria are unknown. Students therefore need to activate their prior knowledge and generate unique representations of the problem, set goals, monitor and regulate their progress, generate a variety of prototypical solutions, and converge on the most original and useful final outcome (Treffinger et al., 2008). In other words, ill-defined problem-solving requires a combination of cognitive and metacognitive skills and divergent thinking (Mumford et al., 1991; 2019). However, the use of generative artificial intelligence (AI) technologies like ChatGPT may cause a large disruption in the problem-solving process of ill-defined problems (Terwiesch, 2023; Zhai, 2022, December 27). ChatGPT is capable of producing a variety of representations of the problem, providing information that may help solve the problem, and most importantly, generating prototypical versions of the solution (Zhai, 2022, December 27). Consequently, educational researchers are very concerned about the future use of ill- defined problem-solving tasks in higher education (Dwivedi et al., 2023; Lim et al. 2023). There is, however, very little scientific evidence about the impact of ChatGPT on actual problem-solving performance (Noy & Zhang, 2023, March 2). Therefore, the goal of the present study is to examine how ChatGPT use impacts actual problem-solving performance (i.e., quality, elaboration, and originality of solutions). More importantly, drawing from the hybrid human-AI regulation theory (Molenaar, 2022a; 2022b), the research will target five additional components that are required for efficient use of generative artificial intelligence: motivation to perform the task (i.e., self-efficacy for solving complex ill-defined problems), CHATGPT AND CREATIVE PROBLEM-SOLVING 4 metacognitive monitoring accuracy and heuristic cues for metacognitive monitoring (i.e., metacognitive experiences like perceived task interest and perceived task difficulty), and invested mental effort. ChatGPT Although various domain specific and goal oriented chatbots are used for improvement of reading (Liu et al., 2022), mathematics (Lee & Yeo, 2022) or argumentation (Guo et al., 2023), ChatGPT as a non-goal oriented tool offers broader range of applications in educational settings (Jeon et al., 2023). ChatGPT is an extension of a large language model (LLM) called GPT-3.5 or GPT-4 in its plus version (Manning, 2022). GPT-3.5 was the most complex LLM containing 175 billion parameters until it was surpassed by GPT-4 (but GPT- 4’s exact technical specifications are unknown; OpenAI, 2023). ChatGPT has an extensive number of parameters that enable it to provide answers that closely align with user expectations. However, the large number of parameters may present a problem as it means that the model can produce outcomes that are satisfactory but not necessarily factually correct (Lin et al., 2022). The reasons for this are twofold. First, the training data for GTP-3.5 consisted of potentially controversial resources (Brown et al., 2020): common internet texts (60%), reddit posts (22%), books (16%), and Wikipedia pages (3%). The second reason is that ChatGPT is trained using so-called reinforcement learning from human feedback (RLHF). In RLHF, hired human participants (typically paid volunteers via services such as Amazon Mechanical Turk) reward or punish the AI in line with goal achievement. Although the goals employed in ChatGPT training are unknown, AI is generally trained to (1) provide clear, helpful, authoritative-sounding answers that satisfy human readers, (2) give correct information, and (3) avoid offending marginalized groups (see Alexander, 2022, December 12, for a broader discussion). The problem arises when these goals conflict. For example, the answer “I don’t know” may be truthful, but it is not helpful and fails to achieve reader satisfaction. ChatGPT therefore tends to produce answers consisting of partially true statements that may deceive the reader: the answers seem credible enough for it to be rewarded for the first goal (the reader is satisfied, the reward is granted), and AI escapes punishment for not achieving the second goal (the reader does not notice the answer is incorrect so there is no punishment; Bang et al., 2023). For these reasons, the use of ChatGPT in educational settings could prove highly problematic (Lim et al. 2023; Peters et al., 2023), CHATGPT AND CREATIVE PROBLEM-SOLVING 5 as highlighted by researchers (see the joint statement by 73 researchers in Dwivedi et al., 2023). Yet the potential benefits of the ChatGPT are also excessive. Zhai (2022, December 27) in his case study reported that essay completion was much quicker and required less cognitive load when ChatGPT was used. Noy & Zhang (2023, March 2), in their experiment, found that ChatGPT improved self-efficacy for task resolution and that ChatGPT users spent less time on the generative phase of the process but still produced higher quality solutions. Moreover, ChatGPT can be used for generating various passages for fostering literacy (Li et al., 2023). However, the fact that ChatGPT may generate answers consisting of partially true statements to satisfy its primary goal of appearing credible and avoiding punishment means that students have to actively monitor and evaluate the information received from ChatGPT (Bezirhan & von Davier, 2023). According to hybrid human-AI regulation theory (Molenaar, 2022a; 2022b), students have to assess whether the individual parts of the response fit within the overall context of the problem-solving task. That requires metacognitive regulation, including processes like selecting relevant information, reiterating and clarifying ideas, debugging errors, and adjusting their approach to ensure coherence and meaningfulness. Finally, students need to perform accurate self-evaluation, to assesses the quality and originality of the final outcome, which contains ideas generated by the student and ChatGPT. In other words, with the emergence of generative AI, metacognition may come to play an increasingly important role in problem-solving (Joksimovic et al., 2023; Rafner et al., 2021). The Role of Metacognition in Solving Ill-defined Tasks In tackling ill-defined tasks, individuals employ both divergent thinking and metacognitive skills when setting unique goals (“What should my outcome look like?”) and develop their own unique problem-solving strategy (“What should I do to create the outcome I desire?”; Greene et al., 2018; Lubart, 2001). Divergent thinking is used to generate various prototypical ideas that are metacognitively evaluated in order to select the most promising ones for further elaboration (Mumford et al., 2019). At the end, individuals self-evaluate their results (“Is this the outcome I planned at the beginning?”, “Is it useful enough?”, “Is it distinctive enough?”) with accurate self-evaluation being a necessary component of highly creative problem-solving performances (Authors, 20xxc). Solving ill-defined problems is CHATGPT AND CREATIVE PROBLEM-SOLVING 6 therefore inherently creative as it harnesses both divergent and convergent thinking in pursuit of original and useful solutions (Authors, 20xxa). As such, solving ill-defined problems is often called creative problem-solving (Authors, 20xxa, Mumford et al., 2019). A variety of metacognitive experiences (such as feeling of difficulty or perceived task interest) provide cues to guide the problem-solving process (Puente-Díaz et al., 2021) and the allocation and regulation of cognitive resources (such as invested mental effort; Ackerman, 2019; van Gog et al., 2020). Perceived task difficulty refers to how individuals subjectively assess and interpret how challenging or complex a particular task is. Perceived difficulty can vary based on individual factors, such as prior knowledge, skills, and self-efficacy (Winne, 2017). Mental effort refers to the cognitive resources and energy individuals allocate to task performance. That requires concentration, focus, and the use of problem-solving strategies to effectively tackle the demands of the task (Paas & Van Merriënboer, 1994). As long as the task is not too easy or too difficult, perceived task difficulty correlates strongly with invested mental effort, but individuals who perceive the task to be too easy or too difficult may invest less mental effort in it (Paas et al., 2005; Scheiter et al., 2020). Self-Efficacy and Creative Problem-Solving Metacognitive experiences and self-efficacy are mutually intertwined factors that influence task outcomes. Self-efficacy is conceptualized as the belief in one's ability to master or complete a task (Bandura, 1999). Creative self-efficacy is characterized by an individual's confidence in their capacity to generate novel and useful ideas (Tierney & Farmer, 2002; 2011), to creatively solve problems, find alternative solutions, and effectively implement them (Karwowski, 2011; Li et al., 2020). In experts, creative self-efficacy can influence confidence in generating groundbreaking inventions and patentable ideas. By believing in their own capacity to generate innovative solutions, inventors with higher creative self- efficacy may be more motivated and persistent in pursuing novel and valuable inventions. In creative problem-solving, there is a weak-to-moderate relationship between creative self- efficacy and creative performance (Haase et al., 2018; Puente-Díaz & Cavazos-Arroyo, 2017). If individuals believe that they are capable of mastering a difficult task, they invest more mental effort in the task, resulting in a better task performance (Zimmerman, 2000). Effort regulation, therefore, mediates the relationship between self-efficacy and performance (Komarraju & Nadler, 2013). However, it is worth noting that high self-efficacy can have CHATGPT AND CREATIVE PROBLEM-SOLVING 7 drawbacks. Vancouver and Kendall (2006) pointed that when self-efficacy is too high, the absence of self-doubt about one’s abilities can weaken commitment (i.e., reduce motivation to perform) which negatively affects performance and final product quality. Present Study The present study employs a complex ill-defined problem-solving task (Puente-Díaz et al., 2021) in order to explore the differences between university students who use ChatGPT during task resolution and those who do not. As discussed above, ChatGPT can help create clear problem representations, provide potentially useful information, and generate various prototypical versions of the solution. Noy & Zhang (2023, March 2) have demonstrated that ChatGPT can improve the quality of the solution and Yilmaz & Yilmaz (2023) showed that ChatGPT improved a learning performance in programming class. However, none of the previous studies have explored whether the answers are better elaborated (i.e., more detailed) and original (i.e., more unique or distinctive) than those obtained without the use of ChatGPT. As Todd et al. (2019) note, quality, elaboration, and originality are the three the most important dimensions when assessing performance on ill-defined problem-solving tasks. The present study therefore hypothesizes that H1: “Participants who use ChatGPT create solutions with higher quality (H1a) that are more elaborated (H1b) and more original (H1c) than the control group solving the same task without using ChatGPT.” The use of ChatGPT as a resource can boost individuals’ self-efficacy by providing support and a feeling of competence (Yilmaz & Yilmaz, 2023). By providing the assistance in the generative phase of the problem-solving process, ChatGPT can make the task seem more manageable, strengthening participants’ belief in their ability to successfully complete the task and generate high-quality solutions (Noy & Zhang, 2023, March 2; Zhai, 2022, December 27). The present study therefore hypothesizes that H2: “Participants who use ChatGPT have higher on-task self-efficacy for task resolution.” According to hybrid human-AI regulation theory (Molenaar, 2022a; 2022b), using ChatGPT in problem-solving requires strong metacognitive skills, since individuals do not only work with their own ideas, but also need to select relevant information provided by ChatGPT, reiterate and clarify ideas, fix errors, and adapt ChatGPT’s ideas to produce functional solutions. They must accurately evaluate the quality and originality of the final outcome, which contains a combination of student- and ChatGPT-generated ideas (Joksimovic et al., 2023). Because working with AI is particularly demanding in terms of CHATGPT AND CREATIVE PROBLEM-SOLVING 8 metacognitive skills (Rafner et al., 2021), the present study hypothesizes that H3: “Participants who use ChatGPT overestimate their performance more than the control group.” The research on effect of domain specific chatbots showed that students exhibited higher levels of task interest in reading (Liu et al., 2022) and argumentation (Guo et al., 2023). Therefore, the novelty of real-time interaction with an advanced AI system and its capacity to expand and diversify the problem-solving process underpins hypothesis H4: “Participants who use ChatGPT to solve the task find the task more interesting than those who do not.” As ChatGPT can provide help and a clear structure during the planning (i.e., generating problem representations, supplying relevant information) and generative phases (i.e., producing various prototypical solutions) of the problem-solving process, the task may appear less challenging (Jonassen, 2011). Sawyer (2018) found that students solving ill- defined problems often ask teachers for clarification to help them cope with uncertainty. ChatGPT can perform this function in an engaging and interactive manner. The present study therefore hypothesizes that H5: “Participants who use ChatGPT to solve the task perceive the task as easier than those who do not.” Finally, ChatGPT assistance in the generative phase of the process may reduce cognitive demands and as a consequence task resolution may require less mental effort (Iku- Silan et al., 2023). Moreover, since the assumption is that those using ChatGPT will consider the problem-solving easier, and task difficulty is a cue for effort allocation (Ackerman, 2019; Hoch et al., 2023), the present study hypothesizes that H6: “Participants who use ChatGPT invest less mental effort in completing the task.” The experimental design and research hypotheses were preregistered at Open Science Framework (osf.io/q6wtu). Methods Experimental Procedure The experiment took place in a laboratory at the first author’s institution one month before the end of the academic semester. The pool of volunteers (approx. 8000 university students) were sent an email announcing the general title of the research (“Creative Thinking”). Students who participated in the study received a small portion of a course credit for participation. CHATGPT AND CREATIVE PROBLEM-SOLVING 9 Male and female students who volunteered for the experiment were randomly assigned to either the control condition (task resolution without ChatGPT) or the experimental one (task resolution with ChatGPT) prior to the experiment. All the participants passed the attention checks and completed all the required tasks. Therefore, no participants were excluded. The experiment was held in a quiet room with a maximum of five participants at one time. The control group and experimental groups had separate sessions. The participants worked on a personal computer in a separated work area and could not see each other when working. The personal computers had exactly the same hardware and software settings. The informed consent form was administered electronically before task resolution. The experimental tasks, along with the measures of self-efficacy, metacognitive judgments, and metacognitive experiences, were completed in a single open tab in the web browser. In the control condition, there were no other open tabs. In the experimental condition, the web browser had a second tab open with ChatGPT-3.5 (Plus). Although the students in both conditions were free to use the web browser to access additional tools (e.g., to search for more information), no one chose to do so. Participants in both conditions solved two tasks (see Measures section for a detailed description). First, the Unusual Uses Task was administered without ChatGPT in both conditions to establish baseline originality for both groups. The control group solved the second, complex ill-defined problem-solving task (Product Improvement Task; PIT) without the use of ChatGPT, and the experimental group did so with ChatGPT. Before solving the PIT, the procedure in experimental condition contained a brief description of ChatGPT with example prompts and responses. Participants were informed that (1) ChatGPT can respond to prompts, (2) that the prompts need to be elaborated and contain sufficient detail for ChatGPT to be able to create accurate responses, and (3) that they could provide additional context or make a more specific request and ask ChatGPT to re- generate its responses. The whole experiment took Mtime = 26 (SD = 8) minutes in the control condition and Mtime = 36 (SD = 9) minutes in the experimental condition. Participants For the group comparison, an a priori sample size calculation was performed in G*Power 3.1.9.6 with α = .05, β = .80 and expected effect Cohen’s d = .53. The expected effect size was calculated from the between-group differences in Noy & Zhang (2023, March 2). The CHATGPT AND CREATIVE PROBLEM-SOLVING 10 minimum expected number of participants was 57 for the control group and 57 for the experimental one. The control group for the present study contained 68 university students (22 men and 46 women) and the experimental group contained 77 participants (24 men and 53 women). Mean age was Mage = 22.4 years (SD = 4.5), and there were no significant between-group differences in age, t(143) = 0.06, p = .955. Most of the participants were studying social science and humanities (75.2%), followed by life sciences (17.9%) and technical subjects (6.9%). The distribution of the study fields was the same for both groups, χ 2(2) = 3.75, p = .154. Regarding degree level, 74.5% of the research participants were undergraduate (BA) students, and 25.5% were graduate (MA) students. The distribution of degree level was the same for both groups, χ 2(1) = 2.76, p = .097. With regard to prior ChatGPT experience measured on a 7-point scale, 46.2% of participants had no prior experience of using ChatGPT but may have heard about it, 45.5% had occasionally used it, and 8.3% of participants considered themselves regular users using ChatGPT weekly to daily. There were no significant differences in participant prior experience between the experimental and control conditions, χ2(6) = 4.04, p = .670. The sample was homogenous in race and ethnicity. Ethics approval was obtained from the Ethical Committee of the first author’s institution in conformity with the APA ethical principles. Measures Baseline originality. Prior to the experiment, an Unusual Uses Task (UUT; Torrance, 2008) was used to measure the baseline originality in both conditions. UUT is the most common divergent thinking task. Participants have to generate the largest possible number of most original ideas about different uses of a common object (e.g., paperclip, brick, can). The instructions in the present study explicitly emphasized that the solutions had to be original: “A paperclip can be used in many different ways. Some of these are quite common, while some can be considered original. Your task is to create as many original ideas as possible about how a paperclip can be used.” The originality of the answers was independently evaluated by two trained experts on a 5-point scale from 1 (no originality) to 5 (high originality). Answers were presented in randomized order, and the experts were blind to the experimental condition. The inter-rater agreement was substantial (weighted κoriginality = .80; CHATGPT AND CREATIVE PROBLEM-SOLVING 11 Landis & Koch, 1977), therefore one originality score was calculated as the mean of both expert judgments. Creative problem-solving. To measure creative problem-solving performance, a complex ill-defined problem-solving task (Product Improvement Task; PIT) was adapted from Puente- Díaz et al. (2021). The original PIT (Torrance, 2008) asks participants to think of ways of improving a product (e.g., common toy, chair, table). The adapted version featured a more complex scenario [square brackets indicate additional instructions for the experimental condition]: Mattel is an American toy manufacturer. In terms of sales, it is the second largest toy manufacturer in the world, right after the Lego Group. However, Mattel’s goal for this year is to become the largest toy manufacturer in the world. Imagine you have been hired by Mattel as a consultant. Your first task is to come up with three ideas to improve an ordinary stuffed bunny, about 30 cm in size, to make it more fun to play with. How can the bunny be improved so Mattel’s sales are higher than the Lego Group? [With the assistance of ChatGPT,] you are asked to create three solutions that are both as original as possible and as useful as possible to help Mattel achieve higher sales than the Lego Group. Two experts, blind to the experimental condition, independently evaluated three dimensions of the answers, presented in randomized order: quality, elaboration, and originality (Todd et al., 2019). Quality refers to the extent to which the answers match the goals provided (i.e., to improve the bunny and to increase sales), elaboration is measured by the amount of detail, and originality represents the uniqueness of the ideas (see Appendix 1 for the evaluation matrix). Each dimension was evaluated on a scale ranging from 1 (worst) to 5 (best). The inter-rater agreement was perfect for each evaluated component (weighted κquality = .88, κelaboration = .93, κoriginality = .85; Landis & Koch, 1977). Self-efficacy. To measure self-efficacy for the problem-solving task, participants first read the task instructions (see the description of PIT above). Prior to task resolution, participants in the control condition expressed agreement with four statements (e.g., “I am confident that I can come up with three original and useful ideas to improve the stuffed bunny”; Puente-Díaz CHATGPT AND CREATIVE PROBLEM-SOLVING 12 et al., 2021) on a 100-point scale from 1 (absolutely disagree) to 100 (absolutely agree). In the experimental condition, a total of eight statements was used, four relating to self-efficacy for solving the task without ChatGPT (e.g., “Even without ChatGPT, I am confident that I can come up with three original and useful ideas to improve the stuffed bunny”) and four with ChatGPT (e.g., “With the help of ChatGPT, I am confident that I can come up with three original and useful ideas to improve the stuffed bunny”). The reliability of the statements was excellent, McDonald’s ωcontrol = .93, ωno-ChatGPT = .95, ωwith-ChatGPT = .95. Self-evaluation. After PIT resolution, all participants evaluated the quality and originality of their solutions using two self-evaluation judgments (Authors, 20xxb; Rominger et al., 2022). Aligned with Beghetto & Karwowski (2017) and Karwowski et al. (2019), the instructions for self-evaluation of quality were as follows: “On a scale of 1 to 100, indicate how useful you think your list of improvements is for increasing sales”. The instructions for self- evaluating originality were as follows: “On a scale of 1 to 100, please indicate how original you think your list of improvements is”. Accuracy of self-evaluation (bias index). Accuracy of self-evaluation judgments was represented by two bias indices (Authors, 20xxc; Schraw, 2009): one of the bias indices related to the self-evaluation of quality and the other self-evaluation of originality. The bias index is calculated as the difference between self-evaluation judgment and performance (i.e., dimensions of quality and originality). Since the scales for performance and judgments were different, performance was transformed to a range from 1 to 100. The final bias index was then divided by 100 to produce a range of -1 to 1. Negative values represent underestimation of performance, values close to zero represent accurate self-evaluation, and positive values represent overestimation of performance. Perceived task interest. After task resolution, all participants were asked to evaluate task interest (Schiefele, 2009; Silvia, 2005) by answering the question “How interesting did you find solving this task?” on a scale ranging from 1 (not interesting at all) to 100 (very interesting). Perceived task difficulty. After task resolution, all participants were asked to evaluate task difficulty (Efklides, 2006) by answering the question “How difficult was it to solve this task?” on a scale ranging from 1 (very easy) to 100 (very difficult). CHATGPT AND CREATIVE PROBLEM-SOLVING 13 Perceived mental effort. After task resolution, all participants were asked to evaluate the mental effort invested in the task (Paas, 1992) by answering the question “How much mental effort did you invest in solving this task?” on a scale ranging from 1 (no effort at all) to 100 (all my effort). Perceived usefulness of ChatGPT. In the experimental condition, the final question related to the usefulness of ChatGPT: “How useful was ChatGPT for you when solving this task?” Participants answered on a scale ranging from 1 (not useful at all) to 100 (very useful). Results The Results section investigates whether ChatGPT improves the quality (H1a), elaboration (H1b), and originality (H1c) of the solutions to the creative problem-solving task; whether ChatGPT boosts self-efficacy for task resolution (H2); whether ChatGPT affects the accuracy of the self-evaluations (H3); and whether using ChatGPT makes the task resolution more interesting (H4), easier (H5), and requiring less mental effort (H6). Table 1 Descriptive statistics and basic comparison of individual variables for the control and experimental groups Control Experimental M SD M SD t(143) p d 1. UUT (orig.) 1.74 0.99 1.71 0.81 0.18 .855 0.03 2. Self-efficacy (no ChatGPT) 51.96 20.65 55.96 24.92 1.04 .299 0.17 3. Self-efficacy (with ChatGPT) — — 66.05 22.28 — — — 4. PIT (quality) 2.99 0.86 3.60 0.89 4.16 < .001 0.69 5. PIT (elaboration) 2.21 0.74 2.79 1.11 3.65 < .001 0.61 6. PIT (originality) 2.21 0.91 2.74 1.02 3.32 < .001 0.55 7. Self-evaluation (quality) 53.74 26.95 66.84 19.30 3.40 < .001 0.57 8. Self-evaluation (originality) 44.00 23.53 52.55 23.01 2.21 .029 0.37 9. Bias (quality) 0.04 0.32 0.02 0.29 0.39 .694 0.07 10. Bias (originality) 0.14 0.29 0.09 0.35 0.90 .371 0.15 11. Interest 59.41 26.32 61.51 25.86 0.48 .630 0.08 12. Difficulty 50.22 23.23 37.08 24.09 3.33 < .001 0.56 13. Effort 54.07 24.72 40.08 23.79 3.47 < .001 0.58 14. Usefulness of ChatGPT — — 71.20 26.96 — — — Note. UUT stands for Unusual Uses Task; PIT stands for Product Improvement Task. CHATGPT AND CREATIVE PROBLEM-SOLVING 14 The descriptive statistics for the control and experimental groups are listed in Table 1, together with the independent-samples t-tests. For the subsequent hypothesis testing, ANCOVAs were used with the pre-experiment ChatGPT experience as a covariate. This allowed to explore the impact of ChatGPT regardless of prior user experience. ChatGPT improves the quality, elaboration, and originality of task solutions (H1) Use of ChatGPT had a strong impact on the quality of the solutions, F(1,142) = 19.07, p <.001, η 2p = .12, a moderate impact on elaboration, F(1,142) = 14.87, p = <.001, η2 p = .10, and a moderate impact on originality, F(1,142) = 12.56, p < .001, η 2p = .08. Prior experience of ChatGPT had a small effect on the quality, F(1,142) = 4.00, p = .047, η 2p = .03, elaboration, F(1,142) = 3.95, p = .049, η2p = .03, and originality of the solutions, F(1,142) = 4.44, p = .037, η 2p = .03. Figure 1 Originality of solutions in pre-test and experimental task CHATGPT AND CREATIVE PROBLEM-SOLVING 15 Note. The pre-test task was an Unusual Uses Task (solved without ChatGPT in both conditions); the experimental task was a Product Improvement Task (solved with ChatGPT in the experimental group and without ChatGPT in the control group). Furthermore, Figure 1 shows no difference in originality in the pre-test, F(1,142) = 0.01, p = .928, η 2p = .00, where both groups solved the task without using ChatGPT. However, the significant interaction term, F(1,142) = 7.79, p = .006, η2 p = .05, indicated a moderate improvement in originality scores when ChatGPT was used. These findings point to both between-subject differences (the group of participants using ChatGPT achieved greater originality) and within-subject differences (individual participants created more original solutions with ChatGPT than they did without it). CHATGPT AND CREATIVE PROBLEM-SOLVING 16 Table 2 Correlations among variables in the control (upper-triangle) group and experimental (lower-triangle) group 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1. Prior ChatGPT experience — .14 .08 n/a .04 -.06 .04 .06 -.08 .03 -.10 .01 -.01 .09 Pre-task 2. UUT (originality) .37 *** — .16 n/a .22 .18 .29 * .09 .23 -.07 -.04 -.05 -.14 -.15 3. Self-efficacy (no ChatGPT) .13 .11 — n/a .11 .12 .01 .58 *** .69 *** .41 *** .56 *** .37 *** -.14 .09 4. Self-efficacy (with ChatGPT) .12 .08 .60 *** — n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a Problem-solving task 5. PIT (quality) .28 ** .21 -.02 .04 — .77 *** .71 *** .12 .22 -.56 *** -.38 *** .13 .07 .16 6. PIT (elaboration) .31 ** .25 * .16 .11 .74 *** — .58 *** .19 .17 -.36 ** -.31 ** .20 .00 .16 7. PIT (originality) .28 ** .19 .01 -.01 .73 *** .68 *** — .05 .21 -.43 *** -.61 *** .11 .17 .12 Post-task 8. Self-evaluation (quality) .25 * .16 .37 *** .38 *** .02 .17 .03 — .62 *** .75 *** .47 *** .34 ** -.12 .23 9. Self-evaluation (originality) .09 -.15 .31 ** .43 *** -.01 .08 -.03 .58 *** — .37 *** .65 *** .34 ** -.04 .15 10. Bias (quality) -.05 -.05 .26 * .22 -.75 *** -.46 *** -.53 *** .65 *** .39 *** — .64 *** .20 -.15 .08 11. Bias (originality) -.15 -.23 * .20 .29 * -.53 *** -.45 *** -.75 *** .36 ** .69 *** .65 *** — .19 -.16 .03 12. Interest .06 -.12 .18 .38 *** .20 .20 .15 .15 .41 *** -.06 .16 — .03 .41 *** 13. Difficulty -.24 * -.23 * -.21 -.15 .19 .05 .08 -.37 ** -.11 -.39 *** -.13 .11 — .55 *** 14. Effort -.13 -.30 ** .08 .10 .16 .07 .10 -.17 .16 -.24 * .03 .39 *** .73 *** — 15. Usefulness of ChatGPT .07 .12 -.05 .27 * -.01 .07 -.16 .37 *** .15 .26 * .22 .08 -.10 -.11 Note. UUT stands for Unusual Uses Task; PIT stands for Product Improvement Task. *** p < .001. ** p < .01. * p < .05 CHATGPT AND CREATIVE PROBLEM-SOLVING 17 ChatGPT boosts self-efficacy for task resolution (H2) Figure 2 illustrates participant self-efficacy for creating useful and original ideas in the experimental task. Participant assessments of self-efficacy for task resolution without ChatGPT indicated no significant differences between the control group and experimental group, F(1,142) = 1.35, p = .247, η 2p = .01. However, participant assessments of self-efficacy for task resolution with ChatGPT showed moderately higher self-efficacy in comparison to the control group, which did not use ChatGPT, F(1,142) = 16.28, p < .001, η 2p = .10. Furthermore, participants in the experimental condition were asked to assess their self-efficacy for task resolution with and without ChatGPT. The within-subject differences were very strong, F(1,75) = 17.57, p < .001, η 2p = .19, indicating that individual participants were more confident when solving the task with ChatGPT than without it. Interestingly, prior experience of ChatGPT was unrelated to self-efficacy for task resolution, F(1,75) = 1.51, p = .222, η 2p = .00. Figure 2 Self-efficacy to resolve the problem-solving task without and with ChatGPT CHATGPT AND CREATIVE PROBLEM-SOLVING 18 From an exploratory perspective, it is important to note that self-efficacy and actual problem- solving performance (see Table 2) had only a very small correlation in the control group (median r = .11) and a negligible one in the experimental group (median rno-ChatGPT = .01, median rwith-ChatGPT = .04). However, higher self-efficacy was associated with greater overestimation of one’s performance in both the control group (median r = .49) and experimental group (median rno-ChatGPT = .23, median rwith-ChatGPT = .26). To conclude, the findings suggest that ChatGPT boosts self-efficacy, but the benefits of higher self-efficacy for actual performance may be limited. ChatGPT has no influence on the absolute accuracy of self-evaluations (H3) Interestingly, there were no differences between the control and experimental group in self- evaluation accuracy, nor in self-assessment of quality, F(1,142) = 0.16, p = .687, η2p = .00, or originality, F(1,142) = 1.08, p = .301, η 2p = .01. Prior ChatGPT experience had no relationship with self-assessment accuracy of quality, F(1,142) = 0.02, p = .892, η 2 p = .00, or originality, F(1,142) = 2.32, p = .130, η 2 p = .02. However, an exploratory examination of the correlation matrix in Table 2 offers one important insight. With the use of ChatGPT, there is no correlation between self-evaluation judgments and quality (r = .02) or originality (r = -.03) of solutions, suggesting that participants at the group level were unable to calibrate their judgments according to their performance. However, it is important to keep in mind that absence of evidence is not evidence of absence. Further exploration of the correlations in Table 2 also shows that the perceived usefulness of ChatGPT is moderately associated with overestimation of performance (median r = .24). This may prove crucial because perceived usefulness of ChatGPT does not correlate with problem-solving performance (median r = -.01). In other words, the more useful the participants found ChatGPT, the more they overestimated the quality and originality of their answers. But as suggested above, these results are exploratory and they require further investigation. CHATGPT AND CREATIVE PROBLEM-SOLVING 19 Figure 3 Perceived task interest, difficulty, and mental effort invested in task resolution ChatGPT use does not make the task resolution more interesting (H4) Another rather surprising finding is that participants in both conditions considered the task resolution similarly interesting. As illustrated in Figure 3, there were no significant differences between the control and experimental groups in task interest, F(1,142) = 0.27, p = .602, η 2p = .00. Moreover, there was no relationship between prior ChatGPT experience and perceived task interest, F(1,142) = 0.22, p = .642, η 2p = .00. Task resolution is easier with ChatGPT (H5) However, as further illustrated in Figure 3, task resolution with ChatGPT was perceived as moderately easier than task resolution without it, F(1,142) = 12.16, p = <.001, η2p = .08. Overall, prior ChatGPT experience was unrelated to perceived difficulty, F(1,142) = 2.62, p = .108, η 2p = .02. CHATGPT AND CREATIVE PROBLEM-SOLVING 20 However, the exploration of correlation in Table 2 found moderate negative correlation between perceived difficulty and prior ChatGPT experience when working with ChatGPT, r = -.24, indicating that the task resolution was perceived easier with greater ChatGPT experience. Again, this finding is exploratory and requires further testing. Task resolution with ChatGPT requires less mental effort (H6) And finally, Figure 3 shows that task resolution with ChatGPT required moderately less mental effort than task resolution without, F(1,142) = 12.07, p < .001, η 2p = .08. Prior ChatGPT experience was unrelated to invested mental effort, F(1,142) = 0.09, p = .760, η 2p = .00. Discussion The goal of the present study was to investigate creative problem-solving performance in university students with and without ChatGPT assistance. The Product Improvement Task employed in the present study was a complex ill-defined problem-solving task in which participants were asked to create three product improvements that were both original and useful for achieving two overarching objectives (Puente-Díaz et al., 2021). In line with expectations (Noy & Zhang; 2023, March 2; Yilmaz & Yilmaz, 2023), the present study found that students who used ChatGPT created solutions that were more original, elaborated and better reflected the task goals. Task resolution was perceived as easier (Sawyer, 2018) and requiring less mental effort (Iku-Silan et al., 2023). Moreover, students reported higher task self-efficacy compared to those not using ChatGPT (Yilmaz & Yilmaz, 2023). Surprisingly, ChatGPT assistance did not make task resolution more interesting (contrasting the findings of Guo et al., 2023; Liu et al., 2022). Furthermore, the effect of ChatGPT on self- evaluation accuracy was unclear. Although there were no differences in the absolute accuracy of self-evaluations between control and experimental condition, the negligible correlation between self-evaluation judgments and performance with the assistance of ChatGPT suggests that participants at the group level had serious problems calibrating when using ChatGPT. In this context, it is notable that the perceived usefulness of ChatGPT was moderately associated with self-evaluation accuracy of performance and so could serve as a heuristic cue for self- evaluation of the answers (see Ackerman, 2019). CHATGPT AND CREATIVE PROBLEM-SOLVING 21 Quality, Elaboration, and Originality of Solutions Co-Created with ChatGPT The present study found that the solutions of participants using ChatGPT were of higher quality (i.e., they more closely reflected the general objectives of the task). Moreover, these solutions were better elaborated and more original than the solutions of peers working on their own. Interestingly, these findings contradict the views of Dwiwedi et al. (2023) and Peters et al. (2023). In their position papers, they claim that ChatGPT could “kill creativity”, that ChatGPT solutions “lack originality” and that ChatGPT “just did not bear a chance against human creativity”. Although ChatGPT can produce solutions that are not original, it may be important to keep in mind that the goal of generative AI should not be to replace users but to help users develop their own ideas. This is corroborated by Hitsuwari et al. (2023), who found that AI-generated haikus – only when further elaborated by human creators – were rated the most beautiful. Based on the present study, it may be possible to propose several explanations of why ChatGPT improved all three dimensions: quality, elaboration, and originality (Todd et al., 2019). As suggested by Woo et al. (2023), ChatGPT may enhance divergent thinking processes by exposing students to a wide range of possible solutions. This exposure then encourages students to consider a larger pool of solutions, allowing them to select more promising prototypical solutions (Mumford et al., 1991; 2019). Furthermore, ChatGPT can provide support to students in the iterative development of their ideas, investigating these ideas in a greater detail, enabling them to explore the problem space in more depth, and thus develop more elaborate solutions. Finally, interacting with ChatGPT may spark novel combinations of concepts, contributing to more original final solutions. As will be discussed further, this may be especially true of students with greater prior experience of ChatGPT. The Role of Experience in Using ChatGPT to Solve Problems Zamfirescu-Pereira et al. (2023) in their analysis identified difficulties non-expert users had in prompting chatbot to provide solutions that could help them to achieve their assigned goals. Participants with no prior experience did not know how to initiate interaction with the chatbot and their prompts were too vague to guide the conversation. The subsequent frustration led them to abandon the task prematurely, and they were ineffective at seeking help. Moreover, they did not systematically test their approach and although they all eventually reached their goals (with the help of the interviewer), they were unable to CHATGPT AND CREATIVE PROBLEM-SOLVING 22 accurately evaluate the quality of their prompts or explain why their prompts worked. In other words, these findings suggest that novice participants lack the metacognitive skills of monitoring and regulation (Smy et al., 2016). In light of these findings, one can see why in the present study prior experience was moderately associated with the quality, elaboration, and originality of the solutions created with ChatGPT and why participants with greater prior experience thought task resolution was easier. Therefore, future research should target the temporal aspects of the problem-solving process in participants with different levels of experience, in addition to measuring the outcomes. It is necessary to explore how participants with different levels of expertise approach prompt engineering and how the use of different kinds of prompts affects performance on ill-defined problem-solving tasks (Dang et al., 2022, September 3). The Use of ChatGPT Boosts Self-efficacy In academic settings, students who possess strong self-efficacy in a particular subject tend to approach complex assignments with confidence and overcome problems because they perceive the task as manageable and within their capabilities. As a result, they perform better (Zimmerman, 2000). The present study found that self-efficacy for task resolution with ChatGPT was considerably higher than self-efficacy for task resolution without ChatGPT assistance. Participants with higher self-efficacy for resolving the task with ChatGPT assistance thought task resolution was both easier and more interesting and that ChatGPT was more useful. This finding suggests that people with greater confidence in their abilities to solve the task using ChatGPT considered ChatGPT more useful. However, it is important to note that there was no association between higher self- efficacy and actual task performance. The higher self-efficacy was on the other hand associated with overestimation of one’s performance. In other words, although ChatGPT strongly enhanced self-efficacy, students with higher self-efficacy did not perform better, although they believed that they did. This could be explained by Vancouver & Kendall’s (2006) findings that higher levels of self-efficacy were associated with better performance when aggregated for the group of participants, but that higher self-efficacy at the within- subject level was associated with a negative performance. This means that if individuals believe they will perform better on some tasks than on others, that belief may in fact hinder their performance on tasks they believe they will excel at. Since ChatGPT is a novel technology that enhances self-efficacy at both the between- and within-subject levels, it may CHATGPT AND CREATIVE PROBLEM-SOLVING 23 be that this boost in self-efficacy has yet to be fully taken into account when performing a task and self-evaluating the outcomes. In other words, the boost in self-efficacy from ChatGPT might be making people overconfident in their abilities, which could negatively impact their performance. Accuracy of Metacognitive Monitoring with ChatGPT Although the hybrid human-AI regulation theory (Molenaar, 2022a; 2022b) highlights the importance of metacognitive monitoring in human-AI collaboration, there was no study conducting a research on effects of generative AI on accuracy of metacognitive monitoring (Joksimovic et al., 2023). Self-evaluation plays a crucial role in student learning and problem-solving (Brown & Harris, 2013). It requires students to actively monitor their own problem-solving process and, by taking responsibility for assessing their progress, students become actively engaged in evaluating their understanding and making any necessary adjustments. When students self-evaluate their own work, they are encouraged to reflect on the overall quality of their work as well on the strategies they have employed (Panadero et al., 2017). However, Zamfirescu-Pereira et al. (2023) found that novice users are generally inaccurate at metacognitive monitoring when working with a chatbot. The present study’s findings are therefore rather ambiguous. Firstly, there was no difference in the absolute accuracy of self-evaluations between the groups of participants. In absolute terms, participants in both groups were similarly accurate when evaluating quality and similarly inaccurate when evaluating originality. However, the negligible correlation between self- evaluation judgments and performance in experimental group may suggest that aggregated data may hide the fact that ChatGPT use may have a different impact on different participants. Previous studies have shown that participants can be broken down into those who are skilled and aware, those who are unskilled but aware, those who are skilled but underestimate their performance, and those who are unskilled and overestimate their performance (Authors, 20xxb; Dörrenbächer-Ulrich & Perels, 2023). In other words, this finding may be related to the clustered nature of self-evaluative skills. Perceived Usefulness of ChatGPT as a Heuristic Cue in Metacognitive Monitoring CHATGPT AND CREATIVE PROBLEM-SOLVING 24 When individuals metacognitively monitor their performance, self-evaluation judgments rely on various heuristic cues rooted in self-perceptions, task characteristics and metacognitive experiences (Ackerman, 2019; Hoch et al., 2023). The question is which heuristic cues were participants using ChatGPT relying on in the situations where their self-evaluation judgments did not correlate with performance. The explorations in the present study offer two suggestions. First, the perceived usefulness of ChatGPT was moderately associated with overestimation of the performance. The more useful the participants found ChatGPT, the more they overestimated the quality and originality of their answers. Second, perceived task difficulty was negatively associated with self-evaluation accuracy. That means that participants who thought the task easier also overestimated their performance more. This finding is crucial because students using ChatGPT thought the task was moderately less difficult than students working on their own. In the other words, ChatGPT use decreased the perceived difficulty, but with a lower difficulty, students may have felt that their solutions were better and therefore invested less mental effort in the task resolution. The existing research suggests that subjective effort appraisals reflect more than just the effort invested in the task, as they are influenced by factors unrelated to the tasks themselves (Scheiter et al., 2020). That could mean that in their effort appraisals learners also make use of cues that are invalid in terms of effort expenditure. Students using ChatGPT reported moderately less effort required to successfully complete the task. However, lower effort was associated with higher overestimation of one’s performance. These findings suggest that students using ChatGPT in problem-solving may have difficulty using valid heuristic cues in self-evaluation because task resolution with ChatGPT is seen as easier and requiring less mental effort. Limitations The limitations of the present study mainly relate to its experimental nature. The study used a complex ill-defined problem-solving task (product improvement task) that is often employed in creative problem-solving research to explore maximum achievable performance in individuals (Authors, 20xxa; Torrance, 2008). The task is not contextualized in classroom practice and so is ideal for experimental designs because participants have no prior beliefs, experiences, or motivations that might hinder performance. On the other hand, the findings may have lower ecological validity, as is often the case with laboratory experiments. Future quasi-experimental research should therefore target problem-solving of ill-defined tasks in CHATGPT AND CREATIVE PROBLEM-SOLVING 25 real classroom conditions (such as writing essays for their academic course; see Jonassen, 2011). Moreover, future studies should focus specifically on the temporal aspects of a problem-solving process in addition to measuring outcomes, exploring the trace data and various on-task indices (such as monitoring judgments during problem-solving; Winne, 2010). For example, it is worth noting that the students using ChatGPT took longer to solve the tasks despite them reporting lower difficulty and less mental effort. Finally, the metacognitive experiences in the present study were measured using a single judgment. There may be other, more accurate measuring tools (such as the enjoyment subscale from the Intrinsic Motivation Inventory that measures on-task interest; Ryan et al., 1983). Conclusions Dellermann et al. (2019) argued that human-AI hybrid intelligence is characterized as the capacity to accomplish complex objectives by blending human and artificial intelligence, resulting in outcomes superior to those that can be achieved independently and allowing continuous improvement as they learn from one another. In an idealized scenario, humans and AI share agency (Molenaar, 2022a; 2022b), each providing a unique competence and together achieving optimal results (Järvelä et al., 2023; Rafner et al., 2021). The present study showed that student co-creation with ChatGPT considerably improved creative problem- solving performance together with their on-task self-efficacy. The task resolution was considered easier and requiring less mental effort. In the learning context, this translates into the ideal conditions for effective learning (Pass & Van Merriënboer, 1994). Yet the present study also identified potential pedagogical problems that educators and students will face: they will have to learn to deliberately use valid monitoring cues during task resolution and evaluation, and be aware that ChatGPT’s usefulness and perceived ease of task resolution does not automatically result in more useful and original solutions (Baars et al., 2020). In other words, ChatGPT may enhance divergent thinking skills, but require greater use of metacognitive skills. This will be especially important for students choosing to use ChatGPT for their actual learning assignments (Guo, 2022). These findings may prove essential for future advancements of hybrid human-AI regulation theory (Molenaar, 2022a; 2022b), as integrating metacognitive prompts or various types of feedback (Kuklick et al., 2023) may prove crucial for production of problem-solving outcomes that are both novel and factually accurate. CHATGPT AND CREATIVE PROBLEM-SOLVING 26 References Ackerman, R. (2019). Heuristic Cues for Meta-Reasoning Judgments: Review and Methodology. Psychological Topics, 28(1), 1-20. https://doi.org/10.31820/pt.28.1.1 Alexander, S. (2022, December 12). Perhaps It Is A Bad Thing That The World's Leading AI Companies Cannot Control Their AIs. Astral Codex Ten. https://astralcodexten.substack.com/p/perhaps-it-is-a-bad-thing-that-the Authors (20xxa). Authors (20xxb). Authors (20xxc). Baars, M., Wijnia, L., de Bruin, A., & Paas, F. (2020). The Relation Between Students’ Effort and Monitoring Judgments During Learning: A Meta-analysis. Educational Psychology Review, 32, 979–1002. https://doi.org/10.1007/s10648-020-09569-3 Bandura, A. (1999). Self-efficacy: Toward a unifying theory of behavioral change. In R. F. Baumeister (Ed.), The self in social psychology (pp. 285–298). Psychology Press. Bang, Y. et al. (2023). Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. ArXiv. https://doi.org/10.48550/arXiv.2302.04023 Beghetto, R. A., & Karwowski, M. (2017). Toward Untangling Creative Self-Beliefs. In M. Karwowski & J.C. Kaufman (Eds.), The creative self (pp. 3–22). Elsevier. https://doi.org/10.1016/B978-0-12-809790-8.00001-7 Bezirhan, U., & von Davier, M. (2023). Automated reading passage generation with OpenAI’s large language model. Computers and Education: Artificial Intelligence, 5, article number 100161. https://doi.org/10.1016/j.caeai.2023.100161 CHATGPT AND CREATIVE PROBLEM-SOLVING 27 Brown, G. T. L., & Harris, L. R. (2013). Student Self-Assessment. In J. H. McMillan (Ed.), The SAGE Handbook of Research on Classroom Assessment (pp. 367-393). Thousand Oaks. https://doi.org/10.4135/9781452218649.n21 Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., … Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv. https://doi.org/10.48550/arXiv.2005.14165 Dang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022, September 3). How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models. arXiv. https://doi.org/10.48550/arXiv.2209.01390 DeHaan R. L. (2009). Teaching creativity and inventive problem solving in science. CBE- Life Sciences Education, 8(3), 172–181. https://doi.org/10.1187/cbe.08-12-0081 DeHaan, R. L., & Narayan, K. M. V. (2008). Education for Innovation: A Tri-National Overview. In R. L. DeHaan & K. M. V. Narayan (Eds.), Education for Innovation: Implications for India, China and America (pp. 1-16). Brill. https://doi.org/10.1163/9789087902858_002 Dellermann, D., Ebel, P., Söllner, M., & Leimeister, J. M. (2019). Hybrid Intelligence. Business & Information Systems Engineering, 61, 637–643. https://doi.org/10.1007/s12599-019-00595-2 Demirel, M., & Dagyar, M. (2016). Effects of Problem-Based Learning on Attitude: A Meta- analysis Study. Eurasia Journal of Mathematics, Science & Technology Education, 12(8), 2115-2137. https://doi.org/10.12973/eurasia.2016.1293a Dörrenbächer-Ulrich, L., & Perels, F. (2023). Metacognitive Judgment Skills and the Metacognitive Component of Self-Regulated Learning. Zeitschrift für Entwicklungspsychologie und Pädagogische Psychologie. https://doi.org/10.1026/0049- 8637/a000274 CHATGPT AND CREATIVE PROBLEM-SOLVING 28 Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. A., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A. ... Wright, R. (2023). “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, article number 102642. https://doi.org/10.1016/j.ijinfomgt.2023.102642 Efklides, A. (2006). Metacognition and affect: What can metacognitive experiences tell us about the learning process? Educational Research Review, 1, 3-14. https://doi.org/10.1016/j.edurev.2005.11.001 Greene, J. A., Freed, R., & Sawyer, R. K. (2018). Fostering creative performance in art and design education via self-regulated learning. Instructional Science, 47, 127–149. https://doi.org/10.1007/s11251-018-9479-8 Guo, K., Zhong, Y., Li, D., & Chu, S. K. W. (2023). Effects of chatbot-assisted in-class debates on students’ argumentation skills and task motivation. Computers and Education, 203, article number 104862. https://doi.org/10.1016/j.compedu.2023.104862 Guo, L. (2022). Using metacognitive prompts to enhance self-regulated learning and learning outcomes: A meta-analysis of experimental studies in computer-based learning environments. Journal of Computer Assisted Learning, 38(3), 811-832. https://doi.org/10.1111/jcal.12650 Haase, J., Hoff, E. V., Hanel, P. H. P., & Innes-Ker, Å. (2018). A meta-analysis of the relation between creative self-efficacy and different creativity measurements. Creativity Research Journal, 30(1), 1–16. https://doi.org/10.1080/10400419.2018.1411436 Hitsuwari, J., Ueda, Y., Yun, W., & Nomura, M. (2023). Does human–AI collaboration lead to more creative art? Aesthetic evaluation of human-made and AI-generated haiku poetry. Computers in Human Behavior, 139, article number 107502. https://doi.org/10.1016/j.chb.2022.107502 CHATGPT AND CREATIVE PROBLEM-SOLVING 29 Hoch, E., Sidi, Y., Ackerman, R., Hoogerheide, V., & Scheiter, K. (2023). Comparing Mental Effort, Difficulty, and Confidence Appraisals in Problem-Solving: A Metacognitive Perspective. Educational Psychology Review, 35, article number 61. https://doi.org/10.1007/s10648-023-09779-5 Iku-Silan, A., Hwang, G., & Chen, C. (2023). Decision-guided chatbots and cognitive styles in interdisciplinary learning. Computers & Education, 201, article number 104812. https://doi.org/10.1016/j.compedu.2023.104812 Järvelä, S., Nguyen, A., & Hadwin, A. (2023). Human and artificial intelligence collaboration for socially shared regulation in learning. British Journal of Educational Technology. https://doi.org/10.1111/bjet.13325 Jeon, J., Lee, S., & Choe, H. (2023). Beyond ChatGPT: A conceptual framework and systematic review of speech-recognition chatbots for language learning. Computers & Education, advanced online publication. https://doi.org/10.1016/j.compedu.2023.104898 Joksimovic, S., Ifenthaler, D., Marrone, R., De Laat, M., & Siemens, G. (2023). Opportunities of artificial intelligence for supporting complex problem-solving: Findings from a scoping review. Computers & Education: Artificial Intelligence, 4, article number 100138. https://doi.org/10.1016/j.caeai.2023.100138 Jonassen, D. H. (2011). Learning to Solve Problems. Routledge. Karwowski, M. (2011). It doesn't hurt to ask…But sometimes it hurts to believe: Polish students' creative self-efficacy and its predictors. Psychology of Aesthetics, Creativity, and the Arts, 5(2), 154–164. https://doi.org/10.1037/a0021427 Karwowski, M. (2012). Did curiosity kill the cat? Relationship between trait curiosity, creative self-efficacy and creative personal identity. Europe’s Journal of Psychology, 8, 547–558. https://doi.org/10.5964/ejop.v8i4.513 CHATGPT AND CREATIVE PROBLEM-SOLVING 30 Karwowski, M., Lebuda, I., & Beghetto, R. A. (2019). Creative self-beliefs. In J. C. Kaufman & R. J. Sternberg (Eds.), The Cambridge handbook of creativity (pp. 396–418). Cambridge University Press. Komarraju, M., & Nadler, D. (2013). Self-efficacy and academic achievement: Why do implicit beliefs, goals, and effort regulation matter? Learning and Individual Differences, 25, 67–72. https://doi.org/10.1016/j.lindif.2013.01.005 Kuklick, L., Greiff, S., & Lindner, M. A. (2023). Computer-based performance feedback: Effects of error message complexity on cognitive, metacognitive, and motivational outcomes. Computers & Education, 200, article number 104785. https://doi.org/10.1016/j.compedu.2023.104785 Landis, J. R., & Koch, G. G. (1977). The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1), 159. https://doi.org/10.2307/2529310 Li, Y., Sha, L., Yan, L., Lin, J., Rakovic, M., Galbraith, K., Lyons, K., Gašević, D., & Chen, G. (2023). Can large language models write reflectively. Computers & Education: Artificial Intelligence, 4, article number 100140. https://doi.org/10.1016/j.caeai.2023.100140 Lee, D., & Yeo, S. (2022). Developing an AI-based chatbot for practicing responsive teaching in mathematics. Computers and Education, 191, article number 104646. https://doi.org/10.1016/j.compedu.2022.104646 Li, C., Murad, M., Shahzad, F., Khan, M. A. S., Ashraf, S. F., & Dogbe, C. S. K. (2020). Entrepreneurial Passion to Entrepreneurial Behavior: Role of Entrepreneurial Alertness, Entrepreneurial Self-Efficacy and Proactive Personality. Frontiers in psychology, 11, article number 1611. https://doi.org/10.3389/fpsyg.2020.01611 Lim, W. M., Gunasekara, A., Leigh Pallant, J., Pallant, J. A., & Pechenkina, E. (2023). Generative AI and the future of education: Ragnarök or reformation? A paradoxical perspective from management educators. The International Journal of Management Education, 21(2), article number 100790. https://doi.org/10.1016/j.ijme.2023.100790 CHATGPT AND CREATIVE PROBLEM-SOLVING 31 Lin, S. et al. (2022). TruthfulQA: Measuring How Models Mimic Human Falsehoods. ArXiV. https://doi.org/10.48550/arXiv.2109.07958 Liu, C.-C., Liao, M.-G., Chang, C.-H., & Lin, H.-M. (2022). An analysis of children’ interaction with an AI chatbot and its impact on their interest in reading. Computers & Education, 189, article number 104576. https://doi.org/10.1016/j.compedu.2022.104576 Liu, Y., & Pasztor, A. (2022). Effects of problem-based learning instructional intervention on critical thinking in higher education: A meta-analysis. Thinking Skills and Creativity, 45, article number 101069. https://doi.org/10.1016/j.tsc.2022.101069 Lubart, T. I. (2001). Models of the creative process: Past, present and future. Creativity Research Journal, 13(3-4), 295–308. https://doi.org/10.1207/S15326934CRJ1334_07 Molenaar, I. (2022a). The concept of hybrid human-AI regulation: Exemplifying how to support young learners’ self-regulated learning. Computers and Education: Artificial Intelligence, 3, article number 100070. https://doi.org/10.1016/j.caeai.2022.100070 Molenaar, I. (2022b). Towards hybrid human-AI learning technologies. European Journal of Education, 57, 632–645. https://doi.org/10.1111/ejed.12527 Mumford, M. D., Martin, R., Elliott, S. N. (2019). Creative Thinking Processes: Managing Innovative Efforts. In Oxford Research Encyclopedias. Oxford University Press. https://doi.org/10.1093/acrefore/9780190224851.013.172 Mumford, M. D., Mobley, M. I., Reiter‐Palmon, R., Uhlman, C. E., & Doares, L. M. (1991). Process analytic models of creative capacities. Creativity Research Journal, 4(2), 91– 122. https://doi.org/10.1080/10400419109534380 Noy, S., & Zhang, W. (2023, March 2). Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence. http://dx.doi.org/10.2139/ssrn.4375283 CHATGPT AND CREATIVE PROBLEM-SOLVING 32 OpenAI. (2023). GPT-4 Technical Report. arXiv. https://doi.org/10.48550/arXiv.2303.08774 Paas, F. G. (1992). Training strategies for attaining transfer of problem-solving skill in statistics: A cognitive-load approach. Journal of Educational Psychology, 84(4), 429– 434. https://doi.org/10.1037/0022-0663.84.4.429 Paas, F. G., & Van Merriënboer, J. J. (1994). Variability of worked examples and transfer of geometrical problem-solving skills: A cognitive-load approach. Journal of educational psychology, 86(1), 122. https://doi.org/10.1037/0022-0663.86.1.122 Paas, F., Tuovinen, J. E., van Merriënboer, J. J. G., & Darabi, A. A. (2005). A Motivational Perspective on the Relation Between Mental Effort and Performance: Optimizing Learner Involvement in Instruction. Educational Technology Research and Development, 53(3), 25–34. https://doi.org/10.1007/BF02504795 Panadero, E., Jonsson, A., & Botella, J. (2017). Effects of self-assessment on self-regulated learning and self-efficacy: Four meta-analyses. Educational Research Review, 22, 74– 98. https://doi.org/10.1016/j.edurev.2017.08.004 Peters, M. A., Jackson, L., Papastephanou, M., Jandrić, P., Lazaroiu, G. … & Fuller, S. (2023). AI and the future of humanity: ChatGPT-4, philosophy and education – Critical responses. Educational Philosophy and Theory. https://doi.org/10.1080/00131857.2023.2213437 Puente-Díaz, R., & Cavazos-Arroyo, J. (2017). Creative Self-Efficacy: The Influence of Affective States and Social Persuasion as Antecedents and Imagination and Divergent Thinking as Consequences. Creativity Research Journal, 29(3), 304-312. https://doi.org/10.1080/10400419.2017.1360067 Puente-Díaz, R., Cavazos-Arroyo, J., & Puerta-Sierra, L. (2021). Idea generation, selection and evaluation: A metacognitive approach. The Journal of Creative Behavior. https://doi.org/10.1002/jocb.505 CHATGPT AND CREATIVE PROBLEM-SOLVING 33 Rafner, J., Dellermann, D., Hjorth, A., Verasztó, D., Kampf, C., Mackay, W., & Sherson, J. (2021). Deskilling, Upskilling, and Reskilling: a Case for Hybrid Intelligence. Morals & Machines, 1(2), 24-39. https://doi.org/10.5771/2747-5174-2021-2-24 Rominger, C., Benedek, M., Lebuda, I., Perchtold-Stefan, C. M., Schwerdtfeger, A. R., Papousek, I., & Fink, A. (2022). Functional brain activation patterns of creative metacognitive monitoring. Neuropsychologia, 177, article number 108416. https://doi.org/10.1016/j.neuropsychologia.2022.108416 Ryan, R. M., Mims‚ V.‚ & Koestner‚ R. (1983). Relation of reward contingency and interpersonal context to intrinsic motivation: A review and test using cognitive evaluation theory. Journal of Personality and Social Psychology‚ 45‚ 736-750. https://doi.org/10.1037/0022-3514.45.4.736 Sawyer, R. K. (2018). The role of failure in learning how to create in art and design. Thinking Skills and Creativity, 33, article number 100527. https://doi.org/10.1016/j.tsc.2018.08.002 Scheiter, K., Ackerman, R., & Hoogerheide, V. (2020). Looking at mental effort appraisals through a metacognitive lens: Are they biased? Educational Psychology Review, 32(4), 1003–1027. https://doi.org/10.1007/s10648-020-09555-9 Schiefele, U. (2009). Situational and individual interest. In K.R. Wentzel & A. Wigfield (Eds.), Handbook of motivation in school (pp. 197-223). Taylor Francis. Schraw, G. (2009). A conceptual analysis of five measures of metacognitive monitoring. Metacognition and Learning, 4, 33-45. https://doi.org10.1007/s11409-008-9031-3 Silvia, P. J. (2005). What Is Interesting? Exploring the Appraisal Structure of Interest. Emotion, 5(1), 89–102. doi:10.1037/1528-3542.5.1.89 Smy, V., Cahillane, M., & MacLean, P. (2016). Sensemaking and metacognitive prompting in ill-structured problems. International Journal of Information and Learning Technology, 33(3), 186-199. https://doi.org/10.1108/IJILT-10-2015-0027 CHATGPT AND CREATIVE PROBLEM-SOLVING 34 Terwiesch, C. (2023). Would Chat GPT3 Get a Wharton MBA? A Prediction Based on Its Performance in the Operations Management Course. Mack Institute for Innovation Management at the Wharton School. Tierney, P., & Farmer, S. M. (2002). Creative self-efficacy: Its potential antecedents and relationship to creative performance. Academy of Management journal, 45(6), 1137- 1148. https://doi.org/10.2307/3069429 Tierney, P., & Farmer, S. M. (2011). Creative self-efficacy development and creative performance over time. Journal of Applied Psychology, 96(2), 277–293. https://doi.org/10.1037/a0020952 Todd, E. M., Higgs, C. A., & Mumford, M. D. (2019). Bias and Bias Remediation in Creative Problem-Solving: Managing Biases through Forecasting. Creativity Research Journal, 31(1), 1-14. https://doi.org/10.1080/10400419.2018.1532268 Torrance, E. P. (2008). Torrance Tests of Creative Thinking: Norms-technical manual, verbal forms A and B. Scholastic Testing Service. Treffinger, D. J., Selby, E. C., & Isaksen, S. G. (2008). Understanding individual problem- solving style: A key to learning and applying creative problem-solving. Learning and Individual Differences, 18(4), 390–401. https://doi.org/10.1016/j.lindif.2007.11.007 van Gog, T., Hoogerheide, V., & van Harsel, M. (2020). The role of mental effort in fostering self-regulated learning with problem-solving tasks. Educational Psychology Review, 32(4), 1055–1072. https://doi.org/10.1007/s10648-020-09544-y Vancouver, J. B., & Kendall, L. N. (2006). When self-efficacy negatively relates to motivation and performance in a learning context. Journal of Applied Psychology, 91(5), 1146–1153. https://doi.org/10.1037/0021-9010.91.5.1146 CHATGPT AND CREATIVE PROBLEM-SOLVING 35 Walker, A., & Leary, H. (2009). A Problem Based Learning Meta Analysis. The Interdisciplinary Journal of Problem-based Learning, 3(1), 12-43. https://doi.org/10.7771/1541-5015.1061 Winne, P.H. (2010). Improving Measurements of Self-Regulated Learning. Educational Psychologist, 45(4), 267-276. https://doi.org/10.1080/00461520.2010.517150 Winne, P. H. (2017). Cognition and metacognition within self-regulated learning. In D. H. Schunk & J. A. Greene (Eds.), Handbook of self-regulation of learning and performance (pp. 36–48). Routledge. Woo, D. J., Wang, Y., Susanto, H., & Guo, K. (2023). Understanding English as a Foreign Language Students’ Idea Generation Strategies for Creative Writing With Natural Language Generation Tools. Journal of Educational Computing Research. https://doi.org/10.1177/07356331231175999 Yılmaz, F. G. K., & Yılmaz, R. (2023). The effect of generative artificial intelligence (AI)- based tool use on students’ computational thinking skills, programming self-efficacy and motivation. Computers & Education: Artificial Intelligence, 4, article number 100147. https://doi.org/10.1016/j.caeai.2023.100147 Zamfrescu-Pereira, J. D., Wong, R., Hartmann, B., & Yang, Q. (2023). Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581388 Zhai, X. (2022, December 27). ChatGPT User Experience: Implications for Education. http://dx.doi.org/10.2139/ssrn.4312418 Zimmerman, B. J. (2000). Self-efficacy: An essential motive to learn. Contemporary Educational Psychology, 25(1), 82–91. https://doi.org/10.1006/ceps.1999.1016 CHATGPT AND CREATIVE PROBLEM-SOLVING 36 Appendix 1: Evaluation Matrix for a Product Improvement Task Quality Elaboration Originality Examples (from the present study) usefulness for goals amount of detail, elegance uniqueness of ideas 1 point non-specific ideas no detail most common ideas color; larger eyes 2 points minor improvements some detail ideas that are slightly different possibility of dressing it in more personalized clothes 3 points major improvements moderate detail unusual ideas reflecting a general trend The bunny could have built-in LED lights or light strips. Children could use the buttons on the bunny's body to activate different light effects. This would be more fun when playing in the dark or at night. 4 points some alignment with both goals (improve and increase sales) substantial detail rare ideas The bunny could be equipped with a reader and speaker, and sold with books about different topics. The bunny could read the books with the children and teach them interesting facts and knowledge about the world around us. It could answer additional questions about nature, animals, history or other topics and provide fun details. 5 points strong alignment with both goals all three solutions are connected in a single coherent narrative unique ideas that are surprising (e.g., offer novel insights or build on knowledge from various domains) I would add an interactive application, a microphone and a speaker, and sensors for interaction with the child. Mattel could develop a complementary mobile app that syncs with the stuffed bunny. Children could use the app to play games, control the bunny in a simulated environment, or learn new songs and rhymes with it. The way it would work is that when you open the app, you see the bunny's house, where it lives, and when you tap on different places, new information, places, tasks appear... For example, on one of the chairs in the bunny’s house, the children could talk to the bunny. Using the button, the children can ask a question and the bunny answers via the internal speaker. The added value is that the app contains tasks for the children to do at home (this could be with additional components that parents can buy and register via QR code in the app): for example, take a bunny and build it a house at his nursery. That encourages the children to be creative and to do so on their own and also increases sales (buying additional products). The additional sensors monitor what’s going on and check everything is proceeding as it should (the bunny functions as a baby monitor connected to an app in the parent’s phone; the parents can see how their child develops). The stuffed bunny combined with the interactive app offer children fun, stimulation, and learning experience of the kind associated with Lego, and has the additional benefit of life sensors that are a popular part of wearables. Mattel could even promote it: a stuffed bunny, the best wearable for your child (with a photograph of a child “wearing” a bunny). Note. Evaluation criteria are adapted from Todd et al. (2019).","libVersion":"0.3.1","langs":""}
{"path":"lit/lit_sources.backup/Steck24isCosSimEmbedSim.pdf","text":"Is Cosine-Similarity of Embeddings Really About Similarity? Harald Steck hsteck@netflix.com Netflix Inc. Los Gatos, CA, USA Chaitanya Ekanadham cekanadham@netflix.com Netflix Inc. Los Angeles, CA, USA Nathan Kallus nkallus@netflix.com Netflix Inc. & Cornell University New York, NY, USA March 11, 2024 Abstract Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the un- normalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless ‘similarities.’ For some linear models the simi- larities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine- similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives. 1 Introduction Discrete entities are often embedded via a learned mapping to dense real-valued vectors in a variety of domains. For instance, words are embedded based on their surrounding context in a large language model (LLM), while recommender 1arXiv:2403.05440v1 [cs.IR] 8 Mar 2024 systems often learn an embedding of items (and users) based on how they are consumed by users. The benefits of such embeddings are manifold. In particular, they can be used directly as (frozen or fine-tuned) inputs to other models, and/or they can provide a data-driven notion of (semantic) similarity between entities that were previously atomic and discrete. While similarity in ’cosine similarity’ refers to the fact that larger values (as opposed to smaller values in distance metrics) indicate closer proximity, it has, however, also become a very popular measure of semantic similarity be- tween the entities of interest, the motivation being that the norm of the learned embedding-vectors is not as important as the directional alignment between the embedding-vectors. While there are countless papers that report the successful use of cosine similarity in practical applications, it was, however, also found to not work as well as other approaches, like the (unnormalized) dot-product between the learned embeddings, e.g., see [3, 4, 8]. In this paper, we try to shed light on these inconsistent empirical observa- tions. We show that cosine similarity of the learned embeddings can in fact yield arbitrary results. We find that the underlying reason is not cosine simi- larity itself, but the fact that the learned embeddings have a degree of freedom that can render arbitrary cosine-similarities even though their (unnormalized) dot-products are well-defined and unique. To obtain insights that hold more generally, we derive analytical solutions, which is possible for linear Matrix Fac- torization (MF) models–this is outlined in detail in the next Section. In Section 3, we propose possible remedies. The experiments in Section 4 illustrate our findings derived in this paper. 2 Matrix Factorization Models In this paper, we focus on linear models as they allow for closed-form solutions, and hence a theoretical understanding of the limitations of the cosine-similarity metric applied to learned embeddings. We are given a matrix X ∈ Rn×p con- taining n data points and p features (e.g., users and items, respectively, in case of recommender systems). The goal in matrix-factorization (MF) models, or equiv- alently in linear autoencoders, is to estimate a low-rank matrix AB⊤ ∈ Rp×p, where A, B ∈ Rp×k with k ≤ p, such that the product XABT is a good approx- imation of X: 1 X ≈ XAB⊤. When the given X is a user-item matrix, the rows ⃗bi of B are typically referred to as the (k-dimensional) item-embeddings, while the rows of XA, denoted by ⃗xu · A, can be interpreted as the user-embeddings, where the embedding of user u is the sum of the item-embeddings ⃗aj that the user has consumed. Note that this model is defined in terms of the (unnormalized) dot-product 1Note that we omitted bias-terms (constant offsets) here for clarity of notation–they can simply be introduced in the preprocessing step by subtracting them from each column or row of X. Given that such bias terms can reduce the popularity-bias of the learned embeddings to some degree, they can have some impact regarding the learned similarities, but it is ultimately limited. 2 between the user and item embeddings (XAB⊤)u,i = ⟨ ⃗xu · A, ⃗bi⟩. Nevertheless, once the embeddings have been learned, it is common practice to also consider their cosine-similarity, between two items cosSim(⃗bi, ⃗bi′), two users cosSim( ⃗xu · A, ⃗xu′ · A), or a user and an item cosSim( ⃗xu · A, ⃗bi). In the following, we show that this can lead to arbitrary results, and they may not even be unique. 2.1 Training A key factor affecting the utility of cosine-similarity metric is the regularization employed when learning the embeddings in A, B, as outlined in the following. Consider the following two, commonly used, regularization schemes (which both have closed-form solutions, see Sections 2.2 and 2.3: min A,B ||X − XAB⊤|| 2 F + λ||AB⊤|| 2 F (1) min A,B ||X − XAB⊤|| 2 F + λ(||XA|| 2 F + ||B|| 2 F ) (2) The two training objectives obviously differ in their L2-norm regularization: • In the first objective, ||AB⊤|| 2 F applies to their product. In linear mod- els, this kind of L2-norm regularization can be shown to be equivalent to learning with denoising, i.e., drop-out in the input layer, e.g., see [6]. Moreover, the resulting prediction accuracy on held-out test-data was ex- perimentally found to be superior to the one of the second objective [2]. Not only in MF models, but also in deep learning it is often observed that denoising or drop-out (this objective) leads to better results on held-out test-data than weight decay (second objective) does. • The second objective is equivalent to the usual matrix factorization ob- jective minW ||X − P Q ⊤|| 2 F + λ(||P || 2 F + ||Q|| 2 F ), where X is factorized as P Q ⊤, and P = XA and Q = B. This equivalence is outlined, e.g., in [2]. Here, the key is that each matrix P and Q is regularized separately, similar to weight decay in deep learning. If ˆA and ˆB are solutions to either objective, it is well known that then also ˆAR and ˆBR with an arbitrary rotation matrix R ∈ Rk×k, are solutions as well. While cosine similarity is invariant under such rotations R, one of the key insights in this paper is that the first (but not the second) objective is also invariant to rescalings of the columns of A and B (i.e., the different latent dimensions of the embeddings): if ˆA ˆB⊤ is a solution of the first objective, so is ˆADD−1 ˆB⊤ where D ∈ Rk×k is an arbitrary diagonal matrix. We can hence define a new solution (as a function of D) as follows: ˆA(D) := ˆAD and ˆB(D) := ˆBD−1. (3) 3 In turn, this diagonal matrix D affects the normalization of the learned user and item embeddings (i.e., rows): (X ˆA(D))(normalized) = ΩAX ˆA(D) = ΩAX ˆAD and ˆB(D) (normalized) = ΩB ˆB(D) = ΩB ˆBD−1, (4) where ΩA and ΩB are appropriate diagonal matrices to normalize each learned embedding (row) to unit Euclidean norm. Note that in general the matrices do not commute, and hence a different choice for D cannot (exactly) be compen- sated by the normalizing matrices ΩA and ΩB. As they depend on D, we make this explicit by ΩA(D) and ΩB(D). Hence, also the cosine similarities of the embeddings depend on this arbitrary matrix D. As one may consider the cosine-similarity between two items, two users, or a user and an item, the three combinations read • item – item: cosSim ( ˆB(D), ˆB(D)) = ΩB(D) · ˆB · D−2 · ˆB⊤ · ΩB(D) • user – user: cosSim (X ˆA(D), X ˆA(D)) = ΩA(D) · X ˆA · D2 · (X ˆA) ⊤ · ΩA(D) • user – item: cosSim (X ˆA(D), ˆB(D)) = ΩA(D) · X ˆA · ˆB⊤ · ΩB(D) It is apparent that the cosine-similarity in all three combinations depends on the arbitrary diagonal matrix D: while they all indirectly depend on D due to its effect on the normalizing matrices ΩA(D) and ΩB(D), note that the (particularly popular) item-item cosine-similarity (first line) in addition depends directly on D (and so does the user-user cosine-similarity, see second item). 2.2 Details on First Objective (Eq. 1) The closed-form solution of the training objective in Eq. 1 was derived in [2] and reads ˆA(1) ˆB⊤ (1) = Vk · dMat(..., 1 1+λ/σ2 i , ...)k · V ⊤ k , where X =: U ΣV ⊤ is the singular value decomposition (SVD) of the given data matrix X, where Σ = dMat(..., σi, ...) denotes the diagonal matrix of singular values, while U, V contain the left and right singular vectors, respectively. Regarding the k largest eigenvalues σi, we denote the truncated matrices of rank k as Uk, Vk and (...)k. We may define 2 ˆA(1) = ˆB(1) := Vk · dMat(..., 1 1 + λ/σ2 i , ...) 1 2 k . (5) 2As D is arbitrary, we chose to assign dMat(..., 1 1+λ/σ2 i , ...) 1 2 k to each of ˆA, ˆB without loss of generality. 4 The arbitrariness of cosine-similarity becomes especially striking here when we consider the special case of a full-rank MF model, i.e., when k = p. This is illustrated by the following two cases: • if we choose D = dMat(..., 1 1+λ/σ2 i , ...) 1 2 , then we have ˆA(D) (1) = ˆA(1) · D = V · dMat(..., 1 1+λ/σ2 i , ...) and ˆB(D) (1) = ˆB(1) · D−1 = V . Given that the full- rank matrix of singular vectors V is already normalized (regarding both columns and rows), the normalization ΩB = I hence equals the identity matrix I. We thus obtain regarding the item-item cosine-similarities: cosSim ( ˆB(D) (1) , ˆB(D) (1) ) = V V ⊤ = I, which is quite a bizarre result, as it says that the cosine-similarity between any pair of (different) item-embeddings is zero, i.e., an item is only similar to itself, but not to any other item! Another remarkable result is obtained for the user-item cosine-similarity: cosSim (X ˆA(D) (1) , ˆB(D) (1) ) = ΩA · X · V · dMat(..., 1 1 + λ/σ2 i , ...) · V ⊤ = ΩA · X · ˆA(1) ˆB⊤ (1), as the only difference to the (unnormalized) dot-product is due to the ma- trix ΩA, which normalizes the rows—hence, when we consider the ranking of the items for a given user based on the predicted scores, cosine-similarity and (unnormalized) dot-product result in exactly the same ranking of the items as the row-normalization is only an irrelevant constant in this case. • if we choose D = dMat(..., 1 1+λ/σ2 i , ...)− 1 2 , then we have analogously to the previous case: ˆB(D) (1) = V · dMat(..., 1 1+λ/σ2 i , ...), and ˆA(D) (1) = V is orthonormal. We now obtain regarding the user-user cosine-similarities: cosSim ( X ˆA(D) (1) , X ˆA(D) (1) ) = ΩA · X · X ⊤ · ΩA, i.e., now the user-similarities are simply based on the raw data-matrix X, i.e., without any smoothing due to the learned embeddings. Concerning the user-item cosine-similarities, we now obtain cosSim (X ˆA(D) (1) , ˆB(D) (1) ) = ΩA · X · ˆA(1) · ˆB⊤ (1) · ΩB, i.e., now ΩB normalizes the rows of B, which we did not have in the previous choice of D. Similarly, the item-item cosine-similarities cosSim ( ˆB(D) (1) , ˆB(D) (1) ) = ΩB · V · dMat(..., 1 1 + λ/σ2 i , ...) 2 · V ⊤ · ΩB are very different from the bizarre result we obtained in the previous choice of D. 5 Overall, these two cases show that different choices for D result in different cosine-similarities, even though the learned model ˆA(D) (1) ˆB(D)⊤ (1) = ˆA(1) ˆB⊤ (1) is invariant to D. In other words, the results of cosine-similarity are arbitray and not unique for this model. 2.3 Details on Second Objective (Eq. 2) The solution of the training objective in Eq. 2 was derived in [7] and reads ˆA(2) = Vk · dMat(..., √ 1 σi · (1 − λ σi )+, ...)k and ˆB(2) = Vk · dMat(..., √ σi · (1 − λ σi )+, ...)k (6) where (y)+ = max(0, y), and again X =: U ΣV ⊤ is the SVD of the train- ing data X, and Σ = dMat(..., σi, ...). Note that, if we use the usual no- tation of MF where P = XA and Q = B, we obtain ˆP = X ˆA(2) = Uk · dMat(..., √ σi · (1 − λ σi )+, ...)k, where we can see that here the diagonal ma- trix dMat(..., √σi · (1 − λ σi )+, ...)k is the same for the user-embeddings and the item-embeddings in Eq. 6, as expected due to the symmetry in the L2-norm regularization ||P || 2 F + ||Q|| 2 F in the training objective in Eq. 2. The key difference to the first training objective (see Eq. 1) is that here the L2-norm regularization ||P || 2 F + ||Q|| 2 F is applied to each matrix individually, so that this solution is unique (up to irrelevant rotations, as mentioned above), i.e., in this case there is no way to introduce an arbitrary diagonal matrix D into the solution of the second objective. Hence, the cosine-similarity applied to the learned embeddings of this MF variant yields unique results. While this solution is unique, it remains an open question if this unique diagonal matrix dMat(..., √σi · (1 − λ σi )+, ...)k regarding the user and item em- beddings yields the best possible semantic similarities in practice. If we believe, however, that this regularization makes the cosine-similarity useful concerning semantic similarity, we could compare the forms of the diagonal matrices in both variants, i.e., comparing Eq. 6 with Eq. 5 suggests that the arbitrary diagonal matrix D in the first variant (see section above) analogously may be chosen as D = dMat(..., √1/σi, ...)k. 3 Remedies and Alternatives to Cosine-Similarity As we showed analytically above, when a model is trained w.r.t. the dot- product, its effect on cosine-similarity can be opaque and sometimes not even unique. One solution obviously is to train the model w.r.t. to cosine similarity, which layer normalization [1] may facilitate. Another approach is to avoid the embedding space, which caused the problems outlined above in the first place, 6 Figure 1: Illustration of the large variability of item-item cosine similarities cosSim(B, B) on the same data due to different modeling choices. Left: ground- truth clusters (items are sorted by cluster assignment, and within each cluster by descending baseline popularity). After training w.r.t. Eq. 1, which allows for arbitrary re-scaling of the singular vectors in Vk, the center three plots show three particular choices of re-scaling, as indicated above each plot. Right: based on (unique) B obtained when training w.r.t. Eq. 2. and project it back into the original space, where the cosine-similarity can then be applied. For instance, using the models above, and given the raw data X, one may view X ˆA ˆB⊤ as its smoothed version, and the rows of X ˆA ˆB⊤ as the users’ embeddings in the original space, where cosine-similarity may then be applied. Apart from that, it is also important to note that, in cosine-similarity, nor- malization is applied only after the embeddings have been learned. This can noticeably reduce the resulting (semantic) similarities compared to applying some normalization, or reduction of popularity-bias, before or during learning. This can be done in several ways. For instance, a default approach in statis- tics is to standardize the data X (so that each column has zero mean and unit variance). Common approaches in deep learning include the use of negative sampling or inverse propensity scaling (IPS) as to account for the different item popularities (and user activity-levels). For instance, in word2vec [5], a matrix factorization model was trained by sampling negatives with a probability pro- portional to their frequency (popularity) in the training data taken to the power of β = 3/4, which resulted in impressive word-similarities at that time. 4 Experiments While we discussed the full-rank model above, as it was amenable to analytical insights, we now illustrate these findings experimentally for low-rank embed- dings. We are not aware of a good metric for semantic similarity, which moti- vated us to conduct experiments on simulated data, so that the ground-truth semantic similarites are known. To this end, we simulated data where items are grouped into clusters, and users interact with items based on their cluster preferences. We then examined to what extent cosine similarities applied to learned embeddings can recover the item cluster structure. In detail, we generated interactions between n = 20, 000 users and p = 7 1, 000 items that were randomly assigned to C = 5 clusters with probabilities pc for c = 1, ..., C. Then we sampled the powerlaw-exponent for each cluster c, βc ∼ Unif(β(item) min , β(item) max ) where we chose β(item) min = 0.25 and β(item) max = 1.5, and then assigned a baseline popularity to each item i according to the powerlaw pi = PowerLaw(βc). Then we generated the items that each user u had interacted with: first, we randomly sampled user-cluster preferences puc, and then computed the user-item probabilities: pui = puci pi∑ i puci pi . We sampled the number of items for this user, ku ∼ PowerLaw(β(user)), where we used β(user) = 0.5, and then sampled ku items (without replacement) using probabilities pui. We then learned the matrices A, B according to Eq. 1 and also Eq. 2 (with λ = 10, 000 and λ = 100, respectively) from the simulated data. We used a low-rank constraint k = 50 ≪ p = 1, 000 to complement the analytical results for the full-rank case above. Fig. 1 shows the ”true” item-item-similarities as defined by the item clusters on the left hand side, while the remaining four plots show the item-item cosine similarities obtained for the following four scenarios: after training w.r.t. Eq. 1, which allows for arbitrary re-scaling of the singular vectors in Vk (as outlined in Section 2.2), the center three cosine-similarities are obtained for three choices of re-scaling. The last plot in this row is obtained from training w.r.t. Eq. 2, which results in a unique solution for the cosine-similarities. Again, the main purpose here is to illustrate how vastly different the resulting cosine-similarities can be even for reasonable choices of re-scaling when training w.r.t. Eq. 1 (note that we did not use any extreme choice for the re-scaling here, like anti-correlated with the singular values, even though this would also be permitted), and also for the unique solution when training w.r.t. Eq. 2. Conclusions It is common practice to use cosine-similarity between learned user and/or item embeddings as a measure of semantic similarity between these entities. We study cosine similarities in the context of linear matrix factorization models, which allow for analytical derivations, and show that cosine similarities are heavily dependent on the method and regularization technique, and in some cases can be rendered even meaningless. Our analytical derivations are comple- mented experimentally by qualitatively examining the output of these models applied simulated data where we have ground truth item-item similarity. Based on these insights, we caution against blindly using cosine-similarity, and pro- posed a couple of approaches to mitigate this issue. While this short paper is limited to linear models that allow for insights based on analytical derivations, we expect cosine-similarity of the learned embeddings in deep models to be plagued by similar problems, if not larger ones, as a combination of various reg- ularization methods is typically applied there, and different layers in the model may be subject to different regularization—which implicitly determines a par- ticular scaling (analogous to matrix D above) of the different latent dimensions in the learned embeddings in each layer of the deep model, and hence its effect 8 on the resulting cosine similarities may become even more opaque there. References [1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization, 2016. arXiv:1607.06450. [2] R. Jin, D. Li, J. Gao, Z. Liu, L. Chen, and Y. Zhou. Towards a better understanding of linear models for recommendation. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2021. [3] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W. Yih. Dense passage retrieval for open-domain question answering, 2020. arXiv:2004.04906v3. [4] O. Khattab and M. Zaharia. ColBERT: Efficient and effective passage search via contextualized late interaction over BERT, 2020. arXiv:2004.12832v2. [5] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space, 2013. [6] H. Steck. Autoencoders that don’t overfit towards the identity. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [7] S. Zheng, C. Ding, and F. Nie. Regularized singular value decomposition and application to recommender system, 2018. arXiv:1804.05090. [8] K. Zhou, K. Ethayarajh, D. Card, and D. Jurafsky. Problems with cosine as a measure of embedding similarity for high frequency words. In 60th Annual Meeting of the Association for Computational Linguistics, 2022. 9","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_notes_OLD_PARTIAL/Chen24misinfoCombatLLM.pdf","text":"Received: 10 October 2023 Revised: 12 February 2024 Accepted: 9 April 2024 DOI: 10.1002/aaai.12188 H I GH LIGH T Combating misinformation in the age of LLMs: Opportunities and challenges Canyu Chen Kai Shu Illinois Institute of Technology, Chicago, Illinois, USA Correspondence Kai Shu, Illinois Institute of Technology, Chicago, IL, USA. Email: kshu@iit.edu More resources on “LLMs Meet Misinformation” are on the website: https://llm-misinformation.github.io/ Funding information U.S. Department of Homeland Security, Grant/Award Number: 17STQAC00001-07-04; Office of the Director of National Intelligence; Intelligence Advanced Research Projects Activity; HIATUS, Grant/Award Number: #2022-22072200001; NSF, Grant/Award Numbers: SaTC-2241068, IIS-2339198, POSE-2346158; Cisco Research Award; Microsoft Accelerate Foundation Models Research Award Abstract Misinformation such as fake news and rumors is a serious threat for infor- mation ecosystems and public trust. The emergence of large language models (LLMs) has great potential to reshape the landscape of combating misinforma- tion. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emerging question is: can we utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to gener- ate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions, respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation. INTRODUCTION Misinformation has been a longstanding serious concern in the contemporary digital age (Shu et al. 2017). With the proliferation of social media platforms and online news outlets, the barriers to generate and share content have sig- nificantly diminished, which also expedites the production and dissemination of various kinds of misinformation (e.g., fake news, rumors). As the consequence of the prevalent misinformation, the public’s belief in truth and authen- ticity can be under threat. Thus, it is pressing to combat misinformation to safeguard information ecosystems and uphold public trust, especially in high-stakes fields such This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non-commercial and no modifications or adaptations are made. © 2024 The Author(s). AI Magazine published by John Wiley & Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence. as healthcare (Chen et al. 2022) and finance (Rangapur, Wang, and Shu 2023). The advent of Large Language Models (LLMs) (e.g., ChatGPT and Llama2; Touvron et al. 2023) has started to make a transformative impact on the landscape of combat- ing misinformation. In general, LLMs are a double-edged sword in the fight against misinformation, indicating that LLMs have brought both emerging opportunities and challenges. On the one hand, the profound world knowl- edge and strong reasoning abilities of LLMs suggest their potential to revolutionize the conventional paradigms of misinformation detection, intervention,and attribu- tion. In addition, LLMs can be augmented with external 354 wileyonlinelibrary.com/journal/aaai AI Magazine. 2024;45:354–368. AI MAGAZINE 355 knowledge, tools, and multimodal information to further enhance their power. On the other hand, the capaci- ties of LLMs on generating human-like content, which possibly contains hallucinated information, and following humans’ instructions indicate that LLMs can be easily uti- lized to generate misinformation in an unintentional or intentional way. More seriously, recent research (Chen and Shu 2024) has found that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same seman- tics, which implies that the misinformation generated by LLMs can have more deceptive styles and potentially cause more harm in the real world. In this paper, we first provide a timely and systematic review of the history of combating misinformation before the rise of LLMs with a focus on the detection aspect. Then we delve into both the opportunities and challenges of combating misinformation in the age of LLMs. As for the opportunities, we will illustrate how to utilize LLMs to combat misinformation? We will present the motivation of adopting LLMs in the fight, the current efforts of utiliz- ing LLMs for combating misinformation, which are mainly around the detection aspect, and an outlook embracing the intervention and attribution aspects. As for the chal- lenges, we will discuss how to combat LLM-generated misinformation? We will dive into the characterization, emergent threats, and countermeasures of LLM-generated misinformation. Looking ahead, we also point out the potential devastating risks of LLM-generated misinforma- tion in the near future, which may not exhibit yet, and the desired interdisciplinary measures. Through this survey, we aim to facilitate the adoption of LLMs in com- bating misinformation and call for collective efforts from stakeholders in different backgrounds to fight misinformation generated by LLMs. HISTORYOFCOMBATING MISINFORMATION In this section, we conduct a systematic review of the techniques for detecting online misinformation before the emergence of LLMs to provide an overview of the his- tory of combating misinformation in terms of the efforts on detection. Capturing linguistic features Numerous linguistic features have been studied for dif- ferentiating misinformation from true information and can be roughly categorized as stylistic features, complexity features, and psychological features (Aich, Bhattacharya, and Parde 2022). As for stylistic features, prior research has found that misleading tweets are usually longer, use more limited vocabulary and often have negative senti- ment (Antypas et al. 2021). As for complexity features, misinformation is likely to be linguistically less com- plex and more redundant (Antypas et al. 2021). The typical psychological features are based on word counts correlated with different psychological process and basic sentiment analysis (Tausczik and Pennebaker 2010), which are shown to have strong association with the possibility of being misleading (Mahbub, Pardede, and Kayes 2022). Based on the linguistic patterns, multiple detection mod- els are proposed (Aich, Bhattacharya, and Parde 2022; Mahyoob, Al-Garaady, and Alrahaili 2020). For example, Mahyoob et al. proposed to leverage 16 linguistic attributes, which include lexical, grammatical, and syntactic fea- tures, to identify the nuance between fake and factual news (Mahyoob, Al-Garaady, and Alrahaili 2020). Leveraging neural models With the development of deep learning in natural lan- guage processing, more recent works utilize neural models such as long short-term memory (LSTM) and convolu- tional neural network (CNN) for feature extraction and prediction instead of manually extracting linguistic pat- terns (Chen et al. 2019;Maetal. 2016). For example, Chen et al. built an attention-residual network combined with CNN for rumor detection (Chen et al. 2019). Notably, as the burgeoning of pretrained language models (PLMs), more advanced neural models such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2019) are also adopted for misinformation detection (Kali- yar, Goswami, and Narang 2021; Beltagy, Lo, and Cohan 2019). For example, FakeBERT combines single-layer CNNs and BERT as the detector and outperforms conven- tional machine learning-based models (Kaliyar, Goswami, and Narang 2021). Exploiting social context Considering social media has been one of the major chan- nels for misinformation production and dissemination, it is essential to incorporate the social context for effectively detecting misinformation and protect the online infor- mation space. Generally, social context can be divided into social engagements and social networks. Specifically, social engagements refer to the users’ interactions with content on social media including tweeting, retweeting, commenting, clicking, liking, and disliking. It is found that the user–news interactions are different for fake and 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 356 AI MAGAZINE authentic news (Shu, Wang, and Liu 2019). Thus, a series of works has explored adopting social engagements as useful auxiliary information for detecting misinforma- tion (Shu et al. 2019; Sheng et al. 2022). For example, Shu et al. proposed a sentence-comment co-attention subnet- work to jointly model news content and users’ comments for fake news detection (Shu et al. 2019). Another line of works aims to leverage social networks, which encompass multiple concepts such as propagation trajectories, user– user and user–post networks, to enhance the detection performance. Since the structure of social networks can be captured and represented as graphs, a majority of works focus on developing graph neural network (GNN) based models to detect various kinds of misinformation (Wu and Hooi 2023;Suetal. 2023). For example, Wu et al. designed a new graph structure learning approach to leverage the distinctive degree patterns of misinformation on social networks (Wu and Hooi 2023). Besides GNNs, there are also some works modeling social context information with mixture marked Hawkes model (Naumzik and Feuer- riegel 2022), Markov random field (Nguyen et al. 2019), or dual-propagation model (Lao, Shi, and Yang 2021). Incorporating external knowledge There are generally two types of widely used external knowledge embracing knowledge graphs and evidential texts for assisting misinformation detection. Knowledge graphs are usually constructed by domain experts and con- tain a large number of entities and their relations, which is helpful for checking the veracity of articles (Hu et al. 2021; Mayank, Sharma, and Sharma 2021). For example, Hu et al. proposed an end-to-end GNN to compare the document graph with external knowledge graphs for fake news detection (Hu et al. 2021). Evidential texts refer to tex- tual facts that can be used for examining the authenticity of articles. Multiple works have investigated evidence-based reasoning strategies for misinformation detection (Jin et al. 2022; Popat et al. 2018). For example, Jin et al. designed a fine-grained graph-based reasoning framework to incorpo- rate multiple groups of external evidence in the detection process (Jin et al. 2022). Enhancing generalization ability In the real world, misinformation can emerge and evolve quickly, indicating that the distributions for misinforma- tion data are likely to keep changing. Thus, a line of research works aims to enhance the generalization ability of misinformation detectors under domain shift (Mosal- lanezhad et al. 2022;Silva et al. 2021)and temporal shift (Hu et al. 2023; Zuo, Zhu, and Cai 2022). As for domain shift, for example, Mosallanezhad et al. built a reinforcement learning-based domain adaptation framework to adapt trained fake news detectors from source domains to tar- get domains (Mosallanezhad et al. 2022). As for temporal shift, one example is that Hu et al. proposed to use the fore- casted temporal distribution patterns of news data to guide the detector (Hu et al. 2023). Minimizing supervision cost Another major challenge for misinformation detection in practices is the lack of supervision labels due to the hard- ness of checking articles’ factuality and the intention to detect misinformation in the early stage of dissemina- tion. Previous works have explored various approaches to address this challenge including data augmentation (He et al. 2021), active learning (Farinneya et al. 2021), prompt-based learning (Wu et al. 2023), adversarial con- trastive learning (Lin et al. 2022), and meta learning (Yue et al. 2023). In particular, multiple works have studied the problem of early misinformation detection (Huang et al. 2022; Xia, Xuan, and Yu 2020;Songetal. 2021). For example, Huang et al. designed a social bot-aware GNN to capture bot behaviors for early rumor detec- tion (Huang et al. 2022). In addition, there are some works exploiting the weak supervision signals, which can be weak labels or constraints from heuristic rules, for misinformation detection (Wang et al. 2020;Yangetal. 2022). Fusing multilingual and multimodality Recently, it has attracted increasing attention to fuse multilingual and multimodal information for misinfor- mation detection. As for multilingual detection, previous research aims to leverage the high-resource language to help low-resource language (De et al. 2021; Dementieva and Panchenko 2021) or build a universal misinforma- tion detector across multiple languages (Hammouchi and Ghogho 2022; Nielsen and McConville 2022). The multi- modal detection generally covers various combinations of different modalities including text, images, audio, video, networking, and temporal information (Sun et al. 2021; Zhangetal. 2019) and has multiple modality fusion strategies including early-fusion, late-fusion, and hybrid- fusion (Alam et al. 2022). For example, Sun et al. proposed to model the cross-modal and content-knowledge incon- sistencies in a unified framework for multimedia misin- formation detection (Sun et al. 2021). In particular, video misinformation has started to cause growing harm due to 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License AI MAGAZINE 357 Opportuni : LLMs for Comba ng Misinforma n Challenges:Comba ng LLM-Generated Misinforma n Fake News Rumors Clickbait Propaganda Hallucina on Uninten onal Genera on Inten onal Genera on Comba ng Misinforma on Detec on A ribu onInterven on Intrinsic Ab es Augmented Abil es World Knowledge Reasoning Ability Retrieval Augmenta on Tool Use Mul modality Agent FIGURE 1 Opportunities and challenges of combating misinformation in the age of LLMs. LLM, large language model. the proliferation of video-sharing platforms such as Tiktok and YouTube (Bu et al. 2023). LLMS FOR COMBATING MISINFORMATION In this section, we aim to illustrate the opportunities of combating misinformation in the age of LLMs, that is, can we utilize LLMs to combat misinformation? First, we will introduce the motivation of adopting LLMs for combating misinformation. Then, we will delve into the booming works on leveraging LLMs for misinforma- tion detection. Finally, we will provide an outlook on leveraging LLMs for combating misinformation in the future. Why adopting LLMs? With regards to the realm of combating misinformation, the advent of LLMs has started to revolutionize the previ- ous paradigms of misinformation detection, intervention, and attribution. As shown in Figure 1, we summarize the reasons from three perspectives: ∙ First, LLMs contain a significant amount of world knowledge. Since LLMs are usually pretrained on a large corpus such as Wikipedia and have billions of parameters, they can store much more knowledge than a single knowledge graph, which is shown in previ- ous benchmarks (Sun et al. 2023). Thus, LLMs have the potential to detect factual errors in misleading texts. One example is shown in Figure 2.Evenif “Mercury” and “Aluminum” are medical terminolo- gies, ChatGPT has an accurate understanding of these terms, reflecting that LLMs have a wide range of world knowledge. ∙ Second, LLMs have strong reasoning abilities, espe- cially in a zero-shot way. Previous research has shown that LLMs have powerful capacities in arith- metic reasoning, commonsense reasoning, and sym- bolic reasoning (Huang and Chang 2022), and can also decompose the problem and reason based on ratio- nales with prompts such as “Let’s think step by step” (Kojima et al. 2022). Thus, LLMs can potentially reason based on their intrinsic knowledge to determine the authenticity of articles. The example in Figure 2 shows that LLMs such as ChatGPT can reason and explain why a piece of misinformation is misleading. In addition, LLMs’ strong zero-shot reasoning ability also largely solves the challenges of distribution shifts and lack of supervision labels. ∙ Third, LLMs can be augmented with external knowl- edge, tools, and multimodal information. One major limitation of LLMs is that they can potentially gener- ate hallucinations, which refer to the LLM-generated texts containing nonfactual information due to intrinsic limitations. One of the main reasons for hallucina- tions is that LLMs cannot get access to up-to-date information and may have insufficient knowledge in specialized domains such as healthcare (Zhang et al. 2023). Recent research has shown that LLMs’ hallu- cinations can be mitigated with the augmentation of retrieved external knowledge (Li et al. 2022) or tools (e.g., real-time search engines such as Google) to get 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 358 AI MAGAZINE FIGURE 2 An example of leveraging ChatGPT to detect misinformation and give explanations. access to up-to-date information (Gou 2023). Further- more, LLMs can be tuned to reason based on mul- timodal information including images, code, tables, and audio (Zhao et al. 2023), which indicates that LLMs are applicable to combating multimodal misin- formation. LLMs have also been shown to own the capacities of serving as autonomous agents in various tasks (Li, Al Kader Hammoud, et al. 2023), imply- ing the great potential to adopt LLMs for autonomiz- ing the process of fact checking and misinformation detection. LLMs for misinformation detection Recently, it has already witnessed increasing efforts explor- ing how to utilize LLMs for misinformation detection. Initially, some works have investigated directly prompt- ing GPT-3 (Buchholz 2023), ChatGPT (Chen and Shu 2024), and GPT-4 (Pelrine et al. 2023) for misinforma- tion detection. For example, Chen et al. (Chen and Shu 2024) have studied ChatGPT-3.5 and GPT-4 with the standard prompting (“No CoT”) strategy and zero-shot chain-of-thought (“CoT”) prompting strategy for detecting both human-written misinformation and LLM-generated misinformation. The extensive experiments show that “CoT” strategy mostly outperforms “No CoT” strategy. Also, a few recent works start to leverage LLMs for detecting multimodal misinformation. One example is that Wu et al. used GPT-3.5 as the feature extrac- tor to help out-of-context image detection (Wu et al. 2023). Besides directly prompting LLMs, Pavlyshenko et al. (Pavlyshenko 2023) fine-tuned an open-source LLM for multiple tasks including fact checking and fake news detection. Since the knowledge contained in LLMs may not be up-to-date or sufficient in detecting factual errors, some works have explored augmenting LLMs with external knowledge (Cheung and Lam 2023) or tools (Chern et al. 2023) for misinformation detection. Specifically, Cheung et al. combined the retrieved knowledge from a search engine and the reasoning ability of Llama to predict the veracity of claims (Cheung and Lam 2023). Chern et al. proposed a fact-checking framework integrated with multiple tools (e.g., Google Search, Google Scholar, code interpreters, Python) to detect factual errors of the texts generated by LLMs (Chern et al. 2023). In addition, some works studied utilizing LLMs to assist conven- tional supervisedly trained detectors via generating weak labels (Leite et al. 2023), rationales (Hu et al. 2024), or instances (Huang et al. 2022). For example, Leite et al. employed a weakly supervised learning framework to 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License AI MAGAZINE 359 leverage LLM-generated supervision signals for training misinformation detectors (Leite et al. 2023). Outlook In this subsection, we provide an outlook of combating misinformation with the assistance of LLMs. In particu- lar, although existing works mainly focus on the detection of misinformation, we will discuss LLMs’ potential to be adopted in misinformation intervention and attribution. Trustworthy misinformation detection Though previous misinformation detectors have achieved relatively high performance, it is under exploration how to ensure the trustworthiness in detection process including robustness, explainability, fairness, privacy, and trans- parency, which are essential for gaining the public trust. Some previous works have explored the robustness (Lyu et al. 2023;Wangetal. 2023) and explainability (Shu et al. 2019) of misinformation detectors. However, all these works are based on conventional supervisedly trained detectors, the emergence of LLMs has brought new oppor- tunities for building trustworthy detectors. For example, as shown in Figure 2, LLMs such as ChatGPT can gen- erate fluent natural language-based explanations for the given misinformation while predicting the authenticity, which are more human-friendly than previous extraction- based explanation methods (Shu et al. 2019). The other aspects of trustworthiness for LLM-based detectors are still under study. Harnessing multilingual multimodal LLMs It has been demonstrated that LLMs can be naturally extended to multilingual (Chen et al. 2023)andmulti- modalities (Yin et al. 2023). First, multilingual LLMs have shown strong generalization ability across different lan- guages including many low-resource ones. For example, one LLM named Phoenix (Chen et al. 2023)can generalize to both Latin (e.g., Deutsch) and non-Latin languages (e.g., Arabic). Thus, multilingual LLMs can largely alleviate the low-resource challenges in cross-lingual misinformation detection. Second, recent studies have demonstrated the impressive multisensory skills of multimodal LLMs (Yin et al. 2023). In particular, GPT-4V (Yang et al. 2023)has manifested surprising capacities of visual-language under- standing and reasoning, indicating the great potential in combating multimodal misinformation. LLMs for misinformation intervention Different from misinformation detection methods that mainly focus on checking the veracity of given texts, mis- information intervention approaches go beyond the pure algorithmic solutions and aim to exert a direct influence on users (Hartwig, Doell, and Reuter 2024), which is also a critical component of the lifecycle of combating misin- formation. Generally, there are two lines of intervention measures. The most standard intervention measures fol- low the pipeline of fact-checking and debunking after humans are exposed to the misinformation. The potential usage of LLMs is to improve the convincingness and per- suasive power of the debunking responses. For example, He et al. (He, Ahamad, and Kumar 2023) proposed to com- bine reinforcement learning and GPT-2 to generate polite and factual counter-misinformation responses. However, one drawback of these post hoc intervention methods is that they may cause “backfiring effect,” suggesting that humans end up with believing more in the original mis- information (Swire-Thompson, DeGutis, and Lazer 2020). Thus, another line of intervention methods aim to lever- age inoculation theories to immunize the public against misinformation (van der Linden 2022). Karinshak et al. pointed out the potential of employing LLMs to gener- ate persuasive antimisinformation messages in advance to enhance the public’s immunity against vaccination misinformation (Karinshak et al. 2023). LLMs for misinformation attribution Misinformation attribution refers to the task of identify- ing the author or source of given misinformation, based on the assumption that the texts written by different authors are likely to have distinct stylometric features and these features will be preserved in different texts for the same author (Uchendu, Le, and Lee 2023). Misinformation attri- bution plays a vital role in combating misinformation because it can be leveraged to trace the origin of pro- paganda or conspiracy theories and hold the publishers accountable. Although there are still no works adopting LLMs in misinformation attribution, LLMs have already exhibited great power in identifying (Huang, Chen, and Shu 2024; Patel, Rao, and Callison-Burch 2023) and manip- ulating (Saakyan and Muresan 2023) stylometric features, indicating the promise of tracing the authorship of mis- information. For example, Huang et al. (Huang, Chen, and Shu 2024) conducted a comprehensive evaluation of LLMs in authorship attribution and verification tasks and demonstrated the advantage of LLMs compared to BERT- based models in a zero-shot setting. Patel et al. (Patel, 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 360 AI MAGAZINE Rao, and Callison-Burch 2023) performed stylometric anal- ysis on a large number of texts via prompting GPT-3 and created a human-interpretable stylometry dataset, which shows that LLMs can have a deep understanding of the stylometric features. Human–LLM collaboration The research in the realm of human–AI collaboration and teaming aims to leverage the strengths of both humans and AI (Amershi et al. 2014). First, human guidance helps steer the development of AI to maximize AI’s benefit to humans and ensure AI will not cause unintended harm, especially for minority groups. Second, AI can boost humans’ ana- lytic and decision-making abilities by providing useful aux- iliary information. There are already some works studying the adoption of human–AI collaboration in combating misinformation (Mendes et al. 2023; Uchendu et al. 2023). For example, Mendes et al. proposed a human-in-the-loop evaluation framework for early detection of COVID-19 mis- information, which combines both modern NLP methods and experts’ involvement (Mendes et al. 2023). In the age of LLMs, we call for more research to leverage the best of both LLMs and humans in fighting misinformation. LLM-GENERATED MISINFORMATION In this section, we will delve into the emerging chal- lenges in the age of LLMs. that is, how to combat LLM-generated misinformation? First, we will provide a characterization of misinformation generated by LLMs. Then, we will illustrate the new threats brought by LLM- generated misinformation in different fields, and the coun- termeasures. Finally, looking ahead, we will discuss the potential risks in the near future and the desired measures. Characterization In general, LLM-generated misinformation can be divided into unintentional generation and intentional generation by different intents (Chen and Shu 2024). As shown in Figure 1, the misinformation generated via uninten- tional generation methods mainly refer to hallucinations, that is, the nonfactual texts generated by LLMs owing to intrinsic limitations. Since hallucinations can occur in any generation process of LLMs due to the intrinsic properties of auto-regressive generation and lack of up- to-date information (Zhang et al. 2023), users without malicious intents may also generate nonfactual content when prompting LLMs. An example is shown in Figure 3. The intentional generation method suggests that malicious users can knowingly prompt LLMs to generate various kinds of misinformation including fake news, rumors, conspiracy theories, clickbait, misleading claims, or propa- ganda. One example is shown in Figure 4. Notably, recent research (Chen and Shu 2024) has discovered that the mis- information generated by LLMs can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, indicating that LLM-generated misinformation can have more deceptive styles and can potentially cause more harm. Emerging threats LLM-generated misinformation has already posed serious threats in the real world. In this subsection, we will discuss the immediate risks of the LLM-generated misinforma- tion on a variety of fields including journalism, healthcare, finance, and politics considering its characteristics of high deceptiveness and easy production. Journalism Journalism may be one of the fields that LLM-generated misinformation has the most substantial impacts on. For example, in April 2023, NewsGuard has identified 49 LLM-powered news websites in seven languages includ- ing English, Chinese, Czech, French, Portuguese, Taga- log, and Thai (newsguardtech.com 2023). These websites can possibly produce hundreds of clickbait articles a day to optimize the advertisement revenue, which cause vast amounts of pollution to online information ecosys- tems. Since LLM-generated misinformation can have more deceptive styles than human-written misinformation with the same semantics (Chen and Shu 2024), it is challeng- ing for readers, fact-checkers, and detection algorithms to effectively discern truth from the misleading informa- tion. In the long run, as the line between human-written news and LLM-generated news blurs, the public trust in legitimate news sources could be undermined and the jour- nalistic ethos—centered on accuracy, accountability, and transparency—might be put to the test. Healthcare Recent works have pointed out the rise of adoption of LLMs in healthcare applications (Sallam 2023; Karabacak and Margetis 2023), however, they can also inadvertently be a tool for the generation and propagation of health mis- information (De Angelis et al. 2023). For example, it is 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License AI MAGAZINE 361 FIGURE 3 An example of unintentionally prompting ChatGPT to generate misinformation (i.e., hallucination). FIGURE 4 An example of intentionally prompting ChatGPT to generate misinformation. found that LLMs such as GPT-3 can be used to generate totally fabricated health articles that appear remarkably authentic (Májovsk `yetal. 2023). Compared with the info- demics driven by human-written misinformation (Chen et al. 2022), it can be even more tough to combat LLM- driven infodemics for the following reasons. First,itis hard for unsuspecting users, who may lack the nuanced understanding of clinical context and medical research, to distinguish LLM-generated hallucinated health con- tent and authentic medical information. If they rely on LLMs to seek health advice, it may lead to potential misinterpretations and adverse health outcomes. Second, malicious actors can manipulate LLMs to craft plausibly sound yet erroneous medical content, promoting alterna- tive healthcare treatments or disproven theories for profit. This will not only undermine the credibility of genuine health information but also pose significant risks to the public health. Finance Previous research has shown that human-written finan- cial misinformation can cause various detrimental conse- quences such as disrupting markets, misleading investors, and amplifying economic instability (Rangapur, Wang, 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 362 AI MAGAZINE and Shu 2023). In the age of LLMs, the financial sector faces an even more escalating threat from LLM-generated misinformation, because bad actors, who are potentially motivated by profit, sabotage, or other malicious intents, can easily leverage LLMs to spread disinformation cam- paigns, create counterfeit financial statements, or even impersonate legitimate financial analysis. Furthermore, considering the prevalence of high-frequency trading and algorithm-driven investment decisions, even short-lived misinformation may trigger automated trades misled by the fabricated content. For example, it is reported that the stock price of an artificial intelligence company iFlytek has a deep drop due to a piece of chatbot-generated online misinformation (scmp.com 2023). Politics Misinformation has a longstanding grave impact on the political spheres (Abilov et al. 2021; Matatov, Naaman, and Amir 2022). The advent of LLMs can potentially usher in a new age of misinformation and disinformation in the realm of politics. The reasons can be summarized as the following two points. The first threat of LLM-generated misinformation is distorting democracy. LLMs can be easily weaponized to generate deceptive narratives about candidates, policies, or events at scale. When people are exposed to such content, their perception of election can- didates might be altered, leading then to vote differently. More seriously, the flooding LLM-generated misinforma- tion can possibly weaken the citizens’ trust on the whole democratic process and eventually erode the foundations of democratic systems. The second threat is amplifying polarization. Bad actors may leverage LLMs to craft per- sonalized misinformation tailored to individual biases and beliefs, which may resonate with specific audiences and increase the likelihood of spreading among targeted com- munities. This can result in exacerbating echo chambers and confirmation biases, driving wedges between different groups and making consensus-building even harder in the political spheres. Countermeasures In this subsection, we will discuss four major countermea- sures against LLM-generated misinformation including alleviating hallucination of LLMs, improving safety of LLMs, detecting LLM-generated misinformation and pub- lic education, through which we hope to inspire more future works on combating misinformation generated by LLMs. Alleviating hallucination of LLMs Hallucination is the main source of unintentional LLM- generated misinformation. Recently, increasing works start to design approaches for evaluating (Du et al. 2023; Lin, Hilton, and Evans 2022) or mitigating hallucina- tion (Li, Cheng, et al. 2023;Lietal. 2022). In general, there are two lines of works. In the training stage, previous research has explored training data curation or knowl- edge grounding methods to incorporate more knowledge into language models (Hu et al. 2023;Yuetal. 2020). In the inference stage, recent works have investigated confi- dence estimation (or uncertainty estimation) (Xiong et al. 2023) and knowledge retrieval (Li et al. 2022) methods to mitigate hallucination. Improving safety of LLMs The safety guard of LLMs aims to prevent malicious users exploiting LLMs to generate harmful content including various types of misinformation. A line of works has been built to evaluate the safety of various LLMs (Huang et al. 2024;Vidgenetal. 2024). Generally, the safety of LLMs can also be strengthened in both training and inference stages. In the training stage, previous works focus on designing alignment training approaches such as rein- forcement learning from human feedback (RLHF) to align LLMs with humans’ values (Yao et al. 2023). In the infer- ence stage, existing research has studied red teaming methods to find LLMs’ flaws (Mehrabi et al. 2023), jail- breaking methods to probe LLMs’ safety risks (Qiu et al. 2023), and defense approaches for the evolving jailbreaking methods (Helbling et al. 2023). Detecting LLM-generated misinformation Misinformation detection is an important measure for plat- forms to prevent its dissemination. Previously, there are a large number of works on detecting human-written misin- formation including fake news (Shu et al. 2019; Sheng et al. 2022), rumor (Li et al. 2019), and propaganda (Martino et al. 2020). Recently, more research focuses on neu- ral misinformation or machine-generated misinformation, suggesting that it is generated by neural models, such as (Aich, Bhattacharya, and Parde 2022; Zellers et al. 2019) and its detection methods (Tan, Plummer, and Saenko 2020; Pagnoni, Graciarena, and Tsvetkov 2022). In the age of LLMs, initial efforts are beginning to explore LLM- generated misinformation detection (Chen and Shu 2024; Jiang et al. 2024), but more research is strongly desired. 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License AI MAGAZINE 363 It is worth noting that detecting LLM-generated misin- formation holds close connection with the techniques in detecting LLM-generated texts, which can be directly adopted in detecting LLM-generated misinformation or take effect via notifying the readers of the potential inau- thenticity. The problem of detecting LLM-generated texts has attracted increasing attention (Muñoz-Ortiz Gómez- Rodrí-guez, Vilares 2023; Weber-Wulff et al. 2023)aswell as the watermarking technique (Kirchenbauer et al. 2023). Public education The goal of public education is two-fold. First, pub- lic education can potentially reduce the risk of normal people abusing LLMs and generating profound hallu- cinated information unintentionally. The general public should be educated about the capacities and limitations of LLMs, which can include the understanding that while LLMs can produce coherent and plausibly sound texts, the LLM-generated content may contain nonfactual infor- mation. Second, it is imperative to enhance the public’s digital literacy and the immunity against LLM-generated misinformation. For example, the characteristics of LLM- generated misinformation and the identification methods should be taught in different communities, especially the minority groups who are found to be more susceptible to misinformation (Pan, Liu, and Fang 2021). Looking ahead In this subsection, we will discuss the potential risks of misinformation generated by LLMs as well as other large generative AI models in the near future, which may not explicitly exhibit yet, and the needed measures. AI-generated multimodal misinformation As the development of generative AI, we have witnessed an exponential increase of various tools to create content in multimodalities, which include not only texts but also audio, images, and video. For example, users can create high-resolution images with open-source (e.g., stable diffusion (Rombach et al. 2021)) or closed-source (e.g., Midjourney) text-to-image generation tools. Also, multimodal LLMs (e.g., GPT-4V Yang et al. 2023)have demonstrated surprisingly strong capacities on visual understanding and image-to-text generation. In real- ity, malicious actors can easily combine these tools to craft hyper-realistic yet entirely fabricated multimodal misinformation, which may bring more challenges for FIGURE 5 A real-world example of AI-generated multimodal misinformation. normal people and even digital experts. A real-world example of AI-generated multimodal fake news is shown in Figure 5, which contains both a piece of misleading text “Reports of an explosion near the Pentagon in Washington DC” and a synthetic fake image1.Wecan also see the extent of potential impact from the number of “views” and “likes.” Autonomous misinformation agents Recent advances of LLM agents have shown that LLMs can finish a wide range of complex tasks automatically which require multiple human-level abilities including planning, reasoning, executing, reflecting, and collaborating (Li, Al Kader Hammoud, et al. 2023). In the future, we can envi- sion a society where humans and agents live together. However, it is also shown that the current safety guard of LLMs can be easily broken via fine-tuning. For example, GPT-3.5’s safety guardrails can be subverted by fine-tuning on only 10 adversarially designed examples (Qi et al. 2023). Thus, the bad actors can possibly create malicious autonomous misinformation agents and deploy them in 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 364 AI MAGAZINE online information ecosystems. The potential danger is that these misinformation agents may operate without the need of humans’ detailed instructions, tirelessly generate vast amounts of misleading content, adapt to conversa- tional contexts in real-time, and adjust their messages to cater to specific targeted audiences, which will make a dev- astating impact on public trust and online safety. To ensure that humans and agents live in harmony in the future, more efforts are still desired. Cognitive security and AI-manipulation The ultimate goal of AI technologies including LLMs should be to maximize the benefits for humans. How- ever, in the future, LLM-generated misinformation could be weaponized to serve as an emerging type of AI- powered cognitive attacks, which can be defined as the cyber-physical-human processes that manipulate humans’ behaviors for malicious purposes by exploiting their cog- nitive vulnerabilities, which pose serious concerns to humans’ cognitive security (Huang and Zhu 2023). The recent evidence has shown that LLMs can be leveraged to infer the cognitive properties (e.g., personalities) of humans from social media posts (Peters and Matz 2023). It is possible that bad actors or LLM-powered autonomous misinformation agents may exploit humans’ cognitive vul- nerabilities for maximizing the impact, which is especially concerning for the minority communities. Furthermore, LLM-generated misinformation can also be regarded a new kind of AI-manipulation (Carroll et al. 2023). It is under-explored how to protect humans against the impact of LLM-generated misinformation from a cognitive per- spective. Interdisciplinary countering efforts In the long run, combating LLM-generated misinforma- tion needs the efforts from different disciplines including technology, sociology, psychology, education, and policy making. From the technology perspective, first, the fac- tuality and safety aspects of LLMs should be further strengthened. Second, more effective detection meth- ods are strongly in need. From the sociology perspec- tive, understanding the patterns of the dissemination of LLM-generated misinformation or the behavior of LLM- powered misinformation agents can help mitigate its spread. From the psychology perspective, recognizing the cognitive weaknesses that make individuals susceptible to misinformation, which may be exploited by bad actors or LLM agents, can lead to more effective intervention measures. From the education perspective, courses on digital literacy and critical thinking can enhance the pub- lic’ discernment skills on LLM-generated misinformation. From the policy making perspective, it is pressing to enact regulations to mandate transparency and account- ability in the development and deployment of LLMs. In addition, we also need to involve the general public in the fight against LLM-generated misinformation and foster constructive discussions on the implications of LLM- generated misinformation on free speech, privacy, and other fundamental rights. By harnessing the efforts from multiple disciplines and different stakeholders, we can form a multipronged defense framework to combat LLM- generated misinformation and safeguard information ecosystems. CONCLUSION The advent of LLMs can potentially usher in a new era of combating misinformation, indicating emerging oppor- tunities and challenges. This survey paper first provides a systematic review of the history of combating misin- formation before the rise of LLMs. Then, we have an in-depth discussion on the existing efforts and a future outlook around the opportunities and challenges for com- bating misinformation in the age of LLMs: can we utilize LLMs to combat misinformation and how to combat LLM- generated misinformation. For the former question, since LLMs have profound world knowledge and strong reason- ing abilities, they show significant promise to be adopted in misinformation detection, intervention, and attribution. For the latter question, the countermeasures include allevi- ating LLMs’ hallucination, improving LLMs’ safety guard, and developing better detection algorithms. Overall, LLMs have great potential to be used in the fight against misinfor- mation and more efforts are needed to minimize the risks of LLM-generated misinformation. A C KNO WLEDGMENTS This material is based upon work supported by the U.S. Department of Homeland Security under Grant Award Number 17STQAC00001-07-04, and the Office of the Direc- tor of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Pro- gram contract #2022-22072200001, NSF (SaTC-2241068, IIS-2339198, POSE-2346158), a Cisco Research Award, a Microsoft Accelerate Foundation Models Research Award. The views and conclusions contained in this document are those of the authors and should not be interpreted as nec- essarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security, ODNI, IARPA, or the U.S. Government. The U.S. Govern- ment is authorized to reproduce and distribute reprints 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License AI MAGAZINE 365 for governmental purposes notwithstanding any copyright annotation therein. C ONFLIC T O F I NTERES T S T A TEMENT The authors declare that there is no conflict. OR CID Canyu Chen https://orcid.org/0000-0003-0937-1046 Kai Shu https://orcid.org/0000-0002-6043-1764 ENDNO T E 1 https://www.boomlive.in/fact-check/fact-check-pentagon- explosion-united-states-fake-image-ai-mainstream-media-22040 REFERENCES Abilov, A., Y. Hua, H. Matatov, O. Amir, and M. Naaman. 2021. “VoterFraud2020: A Multi-Modal Dataset of Election Fraud Claims on Twitter.” In International Conference on Web and Social Media. Aich, A., S. Bhattacharya, and N. Parde. 2022. “Demystifying Neu- ral Fake News Via Linguistic Feature-Based Interpretation.” In Proceedings of the COLING. Alam, F., S. Cresci, T. Chakraborty, F. Silvestri, D. Dimitrov, G. D. S. Martino, S. Shaar, H. Firooz, and P. Nakov. 2022. “A Survey on Multimodal Disinformation Detection.” In Proceedings of the COLING. Amershi, S., M. Cakmak, W. B. Knox, and T. Kulesza. 2014. “Power to the People: The Role of Humans in Interactive Machine Learning.” AI Magazine 35: 105–20. Antypas, D., J. Camacho-Collados, A. Preece, and D. Rogers. 2021. “COVID-19 and Misinformation: A Large-Scale Lexical Analysis on Twitter.” In Proceedings of ACL. Beltagy, I., K. Lo, and A. Cohan. 2019. “SciBERT: A Pretrained Language Model for Scientific Text.” In Proceedings of EMNLP. Bu, Y., Q. Sheng, J. Cao, P. Qi, D. Wang, and J. Li. 2023. “Combating Online Misinformation Videos: Characterization, Detection, and Future Directions.” In Proceedings of the 31st ACM International Conference on Multimedia, 8770–80. Buchholz, M. G. 2023. “Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the Liar Dataset.” arXiv preprint arXiv: 2306.08190. Carroll, M., A. Chan, H. Ashton, and D. Krueger. 2023. “Character- izing Manipulation From AI Systems.” In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization,1–13. Chen, C. and K. Shu. 2024. “Can LLM-Generated Misinformation Be Detected?” In The Twelfth International Conference on Learning Representations. Chen, C., H. Wang, M. Shapiro, Y. Xiao, F. Wang, and K. Shu. 2022. “Combating Health Misinformation in Social Media: Characteri- zation, Detection, Intervention, and Open Issues.” arXiv preprint arXiv:2211.05289. Chen, Y., J. Sui, L. Hu, and W. Gong. 2019. “Attention-Residual Network With CNN for Rumor Detection.” In Proceedings of the CIKM. Chen, Z., F. Jiang, J. Chen, T. Wang, F. Yu, G. Chen, H. Zhang, et al. 2023. “Phoenix: Democratizing Chatgpt Across Languages.” arXiv preprint arXiv: 2304.10453. Chern, I.-C., S. Chern, S. Chen, W. Yuan, K. Feng, C. Zhou, J. He, G. Neubig, and P. Liu. 2023. “FacTool: Factuality Detection in Gen- erative AI – a Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios.” arXiv preprint arXiv: 2307.13528. Cheung, T. H., and K. M. Lam. 2023. “FactLLaMA: Optimizing Instruction-Following Language Models With External Knowl- edge for Automated Fact-Checking.” In 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Con- ference (APSIPA ASC), 846–53. IEEE. De, A., D. Bandyopadhyay, B. Gain, and A. Ekbal. 2021. “A Transformer-Based Approach to Multilingual Fake News Detec- tion in Low-Resource Languages.” ACM Transactions on Asian and Low-Resource Language Information Processing 21: 1–20. De Angelis, L., F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E. Tozzi, and C. Rizzo. 2023. “ChatGPT and the Rise of Large Lan- guage Models: The New AI-Driven Infodemic Threat in Public Health.” Frontiers in Public Health 11: 1166120. Dementieva, D., and A. Panchenko. 2021. “Cross-Lingual Evidence Improves Monolingual Fake News Detection.” In Proceedings of the ACL. Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the NAACL. Du, L., Y. Wang, X. Xing, Y. Ya, X. Li, X. Jiang, and X. Fang. 2023. “Quantifying and Attributing the Hallucination of Large Lan- guage Models Via Association Analysis.” arXiv preprint arXiv: 2309.05217. Farinneya, P., M. M. Abdollah Pour, S. Hamidian, and M. Diab. 2021. “Active Learning for Rumor Identification on Social Media.” In Proceedings of the EMNLP Findings. Gou, Z., Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. 2023. “ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving.” arXiv preprint arXiv: 2309. 17452. Hammouchi, H., and M. Ghogho. 2022. “Evidence-Aware Multilin- gual Fake News Detection.” IEEE Access 10: 116808–18. Hartwig, K., F. Doell, and C. Reuter. 2024. “The Landscape of User- Centered Misinformation Interventions – A Systematic Literature Review.” ACM Computing Surveys 56(11): 1–36. He, B., M. Ahamad, and S. Kumar. 2023. “Reinforcement Learning- Based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation.” In Proceedings of the ACM Web Conference 2023. He, Z., C. Li, F. Zhou, and Y. Yang. 2021. “Rumor Detection on Social Media with Event Augmentations.” In Proceedings of the SIGIR. Helbling, A., M. Phute, M. Hull, and D. H. Chau. 2023. “LLM Self Defense: By Self Examination, LLMs Know They are Being Tricked.” arXiv preprint arXiv: 2308.07308. Hu, B., Q. Sheng, J. Cao, Y. Shi, Y. Li, D. Wang, and P. Qi. 2024. “Bad Actor, Good Advisor: Exploring the Role of Large Language Mod- els in Fake News Detection.” Proceedings of the AAAI Conference on Artificial Intelligence 38(20): 22105–13. Hu, B., Q. Sheng, J. Cao, Y. Zhu, D. Wang, Z. Wang, and Z. Jin. 2023. “Learn Over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection.” In Proceedings of the ACL. Hu, L., Z. Liu, Z. Zhao, L. Hou, L. Nie, and J. Li. 2023. “A Sur- vey of Knowledge Enhanced Pre-Trained Language Models.” IEEE Transactions on Knowledge and Data Engineering 36: 1413–30. Hu, L., T. Yang, L. Zhang, W. Zhong, D. Tang, C. Shi, N. Duan, and M. Zhou. 2021. “Compare to the Knowledge: Graph Neural Fake 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 366 AI MAGAZINE News Detection With External Knowledge.” In Proceedings of the ACL. Huang, B., C. Chen, and K. Shu. 2024. “Can Large Language Models Identify Authorship?” arXiv preprint arXiv:2403.08213. Huang, J. and K. Chang. 2022. “Towards Reasoning in Large Lan- guage Models: A Survey.” In Annual Meeting of the Association for Computational Linguistics. Huang, K.-H., K. McKeown, P. Nakov, Y. Choi, and H. Ji. 2022. “Faking Fake News for Real Fake News Detection: Propaganda- Loaded Training Data Generation.” arXiv preprint arXiv: Arxiv- 2203.05386. Huang, L. and Q. Zhu. “An Introduction of System-Scientific Approaches to Cognitive Security.” arXiv preprint arXiv: 2301.05920. Huang, Y., L. Sun, et al. 2024. “TrustLLM: Trustworthiness in Large Language Models.” In Forty-first International Conference on Machine Learning. Huang, Z., Z. Lv, X. Han, B. Li, M. Lu, and D. Li. 2022. “Social Bot- Aware Graph Neural Network for Early Rumor Detection.” In Proceedings of the COLING. Jiang, B., Z. Tan, A. Nirmal, and H. Liu. 2024. “Disinformation Detec- tion:AnEvolvingChallengeintheAgeofLLMs.”In Proceedings of the 2024 SIAM International Conference on Data Mining (SDM), 427–35. Society for Industrial and Applied Mathematics. Jin, Y., X. Wang, R. Yang, Y. Sun, W. Wang, H. Liao, and X. Xie. 2022. “Towards Fine-Grained Reasoning for Fake News Detection.” In Proceedings of the AAAI. Kaliyar, R. K., A. Goswami, and P. Narang. 2021. “FakeBERT: Fake News Detection in Social Media With a Bert-Based Deep Learn- ing Approach.” Multimedia Tools and Applications 80: 11765– 88. Karabacak, M., and K. Margetis. 2023. “Embracing Large Language Models for Medical Applications: Opportunities and Challenges.” Cureus 15: e39305. Karinshak, E., S. X. Liu, J. S. Park, and J. T. Hancock. 2023. “Working with ai to Persuade: Examining a Large Language Model’s Ability to Generate Pro-Vaccination Messages.” Proceedings of the ACM on Human-Computer Interaction 7: 1–29. Kirchenbauer, J., J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein. 2023. “A Watermark for Large Language Models.” In International Conference on Machine Learning, 17061–84. PMLR. Kojima, T., S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. “Large Language Models are Zero-Shot Reasoners.” In Proceedings of the NeurIPS. Lao, A., C. Shi, and Y. Yang. 2021. “Rumor Detection with Field of Linear and Non-Linear Propagation.” In Proceedings of the WWW. Leite, J. A., O. Razuvayevskaya, K. Bontcheva, and C. Scarton. 2023. “Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision.” arXiv preprint arXiv: 2309.07601. Li, G., H. A. Al Kader Hammoud, H. Itani, D. Khizbullin, and B. Ghanem. 2023. “CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society.” Advances in Neural Information Processing Systems 36: 51991–2008. Li, H., Y. Su, D. Cai, Y. Wang, and L. Liu. 2022. “A Survey on Retrieval- Augmented Text Generation.” arXiv preprint arXiv: 2202.01110. Li, J., X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen. 2023. “HaluE- val: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.” arXiv preprint arXiv: 2305.11747. Li, Q., Q. Zhang, L. Si, and Y. Liu. 2019. “Rumor Detection on Social Media: Datasets, Methods and Opportunities.” In Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda. Lin, H., J. Ma, L. Chen, Z. Yang, M. Cheng, and C. Guang. 2022. “Detect Rumors in Microblog Posts for Low-Resource Domains Via Adversarial Contrastive Learning.” In Proceedings of the ACL Findings. Lin, S., J. Hilton, and O. Evans. 2022. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” In Proceedings of the ACL. Lyu, Y., X. Yang, J. Liu, S. Xie, P. Yu, and X. Zhang. 2023. “Inter- pretable and Effective Reinforcement Learning for Attacking Against Graph-Based Rumor Detection.” In Proceedings of the IJCNN. Ma, J., W. Gao, P. Mitra, S. Kwon, B. J. Jansen, K. Wong, and M. Cha. 2016. “Detecting Rumors from Microblogs with Recurrent Neural Networks.” In Proceedings of the IJCAI. Mahbub, S., E. Pardede, and A. Kayes. 2022. “COVID-19 Rumor Detection Using Psycho-Linguistic Features.” IEEE Access 10: 117530–43. Mahyoob, M., J. Al-Garaady, and M. Alrahaili. 2020. “Linguistic- Based Detection of Fake News in Social Media.” International Journal of English Linguistics Forthcoming. Májovsk `y, M., M. Čern `y, M. Kasal, M. Komarc, and D. Netuka. 2023. “Artificial Intelligence Can Generate Fraudulent but Authentic- Looking Scientific Medical Articles: Pandora’s Box Has Been Opened.” Journal of Medical Internet Research 25: e46924. Martino, G. D. S., S. Cresci, A. Barrón-Cedeño, S. Yu, R. D. Pietro, and P. Nakov. “A Survey on Computational Propaganda Detection.” In Proceedings of the IJCAI. Matatov, H., M. Naaman, and O. Amir. 2022. “Stop the [image] Steal: The Role and Dynamics of Visual Content in the 2020 U.S. Election Misinformation Campaign.” Proceedings of the ACM on Human- Computer Interaction 6(CSCW2): 1–24. Mayank, M., S. Sharma, and R. Sharma. 2021. “Deap-Faked: Knowl- edge Graph Based Approach for Fake News Detection.” In Inter- national Conference on Advances in Social Networks Analysis and Mining. Mehrabi, N., P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta. 2023. “Flirt: Feedback Loop in- Context Red Teaming.” arXiv preprint arXiv: 2308.04265. Mendes, E., Y. Chen, W. Xu, and A. Ritter. 2023. “Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments.” In Proceedings of the ACL. Mosallanezhad, A., M. Karami, K. Shu, M. V. Mancenido, and H. Liu. 2022. “Domain Adaptive Fake News Detection Via Reinforcement Learning.” In Proceedings of the ACM Web Conference 2022. Muñoz-Ortiz, A., C. Gómez-Rodrí-guez, and D. Vilares. 2023. “Con- trasting Linguistic Patterns in Human and LLM-Generated Text.” arXiv preprint arXiv: 2308.09067. Naumzik, C., and S. Feuerriegel. 2022. “Detecting False Rumors from Retweet Dynamics on Social Media.” In Proceedings of the ACM WebConference2022. newsguardtech.com. 2023. “Rise of the Newsbots: AI-Generated News Websites Proliferating Online.” Nguyen, D. M., T. H. Do, R. Calderbank, and N. Deligiannis. “Fake News Detection Using Deep Markov Random Fields.” In Proceedings of the NAACL. 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License AI MAGAZINE 367 Nielsen, D. S. and R. McConville. 2022. “MuMiN: A Large-Scale Mul- tilingual Multimodal Fact-Checked Misinformation Social Net- work Dataset.” In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Pagnoni, A., M. Graciarena, and Y. Tsvetkov. 2022. “Threat Scenarios and Best Practices to Detect Neural Fake News.” In Proceedings of the COLING. Pan, W., D. Liu, and J. Fang. 2021. “An Examination of Factors Con- tributing to the Acceptance of Online Health Misinformation.” Frontiers in Psychology 12: 630268. Patel, A., D. Rao, and C. Callison-Burch. “Learning Interpretable Style Embeddings Via Prompting LLMs.” arXiv preprint arXiv: 2305.12696. Pavlyshenko, B. M. 2023. “Analysis of Disinformation and Fake News Detection using Fine-Tuned Large Language Model.” arXiv preprint arXiv: 2309.04704. Pelrine, K., M. Reksoprodjo, C. Gupta, J. Christoph, and R. Rabbany. 2023. “Towards Reliable Misinformation Mitigation: Generaliza- tion, Uncertainty, and GPT-4.” arXiv preprint arXiv: 2305.14928. Peters, H. and S. Matz. 2023. “Large Language Models Can Infer Psychological Dispositions of Social Media Users.” arXiv preprint arXiv: 2309.08631. Popat, K., S. Mukherjee, A. Yates, and G. Weikum. 2018. “DeClarE: Debunking Fake News and False Claims Using Evidence-Aware Deep Learning.” In Proceedings of the EMNLP. Qi, X., Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson. 2023. “Fine-Tuning Aligned Language Models Com- promises Safety, Even When Users Do Not Intend to!.” arXiv preprint arXiv: 2310.03693. Qiu, H., S. Zhang, A. Li, H. He, and Z. Lan. 2023. “Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models.” arXiv preprint arXiv: 2307.08487. Rangapur, A., H. Wang, and K. Shu. 2023. “Investigating Online Financial Misinformation and Its Consequences: A Computa- tional Perspective.” arXiv preprint arXiv: 2309.12363. Rombach, R., A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. 2021. “High-Resolution Image Synthesis With Latent Diffusion Models.” In Computer Vision and Pattern Recognition. Saakyan, A. and S. Muresan. “ICLEF: In-Context Learning With Expert Feedback for Explainable Style Transfer.” arXiv preprint arXiv: 2309.08583. Sallam, M. 2023. “The Utility of Chatgpt as an Example of Large Language Models in Healthcare Education, Research and Prac- tice: Systematic Review on the Future Perspectives and Potential Limitations.” medRxiv. scmp.com. 2023. “Chinese Artificial Intelligence Firm iFlytek Blames Chatbot-Generated Article for Sudden Share Price Swing on Shenzhen Bourse.” Sheng, Q., J. Cao, X. Zhang, R. Li, D. Wang, and Y. Zhu. 2022. “Zoom Out and Observe: News Environment Perception for Fake News Detection.” In Proceedings of the ACL. Shu, K., L. Cui, S. Wang, D. Lee, and H. Liu. 2019. “Defend: Explainable Fake News Detection.” In Proceedings of the KDD. Shu, K., A. Sliva, S. Wang, J. Tang, and H. Liu. 2017. “Fake News Detection on Social Media: A Data Mining Perspective.” ACM SIGKDD Explorations Newsletter 19: 22–36. Shu, K., S. Wang, and H. Liu. 2019. “Beyond News Contents: The Role of Social Context for Fake News Detection.” In Proceedings of the WSDM. Silva, A., L. Luo, S. Karunasekera, and C. Leckie. “Embracing Domain Differences in Fake News: Cross-Domain Fake News Detection Using Multi-Modal Data.” In Proceedings of the AAAI. Song, C., C. Yang, H. Chen, C. Tu, Z. Liu, and M. Sun. 2021. “CED: Credible Early Detection of Social Media Rumors.” IEEE Transactions on Knowledge and Data Engineering 33: 3035– 47. Su,X., J. Yang,J.Wu, andZ.Qiu.2023. “Hy-DeFake:Hyper- graph Neural Networks for Detecting Fake News in Online Social Networks.” arXiv preprint arXiv: 2309.02692. Sun, K., Y. E. Xu, H. Zha, Y. Liu, and X. L. Dong. 2023. “Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? a.k.a. will LLMs Replace Knowledge Graphs?” arXiv preprint arXiv: 2308.10168. Sun, M., X. Zhang, J. Ma, and Y. Liu. 2021. “Inconsistency Matters: A Knowledge-Guided Dual-Inconsistency Network for Multi-Modal Rumor Detection.” In Proceedings of the EMNLP Findings. Swire-Thompson, B., J. DeGutis, and D. Lazer. 2020. “Searching for the Backfire Effect: Measurement and Design Considerations.” Journal of Applied Research in Memory and Cognition 9: 286– 99. Tan, R., B. Plummer, and K. Saenko. 2020. “Detecting Cross- Modal Inconsistency to Defend Against Neural Fake News.” In Proceedings of the EMNLP. Tausczik, Y. R., and J. W. Pennebaker. 2010. “The Psychologi- cal Meaning of Words: LIWC and Computerized Text Analysis Methods.” Journal of Language and Social Psychology 29: 24–54. Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine- Tuned Chat Models.” arXiv preprint arXiv: 2307.09288. Uchendu, A., T., Le, and D. Lee. 2023. “Attribution and Obfusca- tion of Neural Text Authorship: A Data Mining Perspective.” ACM SIGKDD Explorations Newsletter 25: 1–18. Uchendu, A., J. Lee, H. Shen, T. Le, T.-H. K. Huang, and D. Lee. 2023. “Does Human Collaboration Enhance the Accuracy of Iden- tifying LLM-Generated Deepfake Texts?” Proceedings of the AAAI Conference on Human Computation and Crowdsourcing 11(1): 163–74. van der Linden, S. 2022. “Misinformation: Susceptibility, Spread, and Interventions to Immunize the Public.” Nature Medicine 28: 460– 67. Vidgen, B., A. Agrawal, A. M. Ahmed, V. Akinwande, N. Al- Nuaimi, N. Alfaraj, E. Alhajjar, et al. 2024. “Introducing v0.5 of the AI Safety Benchmark from MLCommons.” arXiv preprint arXiv:2404.12241. Wang,H., Y. Dou, C. Chen,L.Sun,P.S.Yu, andK.Shu.2023. “Attacking Fake News Detectors Via Manipulating News Social Engagement.” In The Web Conference. Wang, Y., W. Yang, F. Ma, J. Xu, B. Zhong, Q. Deng, and J. Gao. 2020. “Weak Supervision for Fake News Detection Via Reinforcement Learning.” In Proceedings of the AAAI. Weber-Wulff, D., A. Anohina-Naumeca, S. Bjelobaba, T. Foltýnek, J. Guerrero-Dib, O. Popoola, P. Šigut, and L. Waddington. 2023. “Testing of Detection Tools for AI-Generated Text.” arXiv preprint arXiv: 2306.15666. Wu,G., W. Wu,X.Liu,K.Xu, T. Wan, andW.Wang. 2023.“Cheap- Fake Detection with LLM Using Prompt Engineering.” In IEEE International Conference on Multimedia and Expo Workshops (ICMEW). 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 368 AI MAGAZINE Wu, J., and B. Hooi. 2023. “DECOR: Degree-Corrected Social Graph Refinement for Fake News Detection.” In Proceedings of the KDD. Wu, J., S. Li, A. Deng, M. Xiong, and B. Hooi. 2023. “Prompt-and- Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection.” arXiv preprint arXiv: 2309.16424. Xia, R., K. Xuan, and J. Yu. 2020. “A State-Independent and Time- Evolving Network for Early Rumor Detection in Social Media.” In Proceedings of the EMNLP. Xiong, M., Z. Hu, X. Lu, Y. Li, J. Fu, J. He, and B. Hooi. “Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.” arXiv preprint arXiv: 2306. 13063. Yang, R., J. Ma, H. Lin, and W. Gao. 2022. “A Weakly Supervised Propagation Model for Rumor Verification and Stance Detec- tion With Multiple Instance Learning.” In Proceedings of the SIGIR. Yang, Z., L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang. 2023. “The Dawn of lMMs: Preliminary Explorations with GPT- 4V(ision).” arXiv preprint arXiv: 2309.17421. Yao, J., X. Yi, X. Wang, J. Wang, and X. Xie. 2023. “From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models.” arXiv preprint arXiv: 2308.12014. Yin, S., C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. 2023. “A Sur- vey on Multimodal Large Language Models.” arXiv preprint arXiv: 2306.13549. Yu, W., C. Zhu, Z. Li, Z. Hu, Q. Wang, H. Ji, and M. Jiang. 2020. “A Survey of Knowledge-Enhanced Text Generation.” arXiv preprint arXiv: 2010.04389. Yue, Z., H. Zeng, Y. Zhang, L. Shang, and D. Wang. 2023. “MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection Via Meta Learning.” In Proceedings of the ACL. Zellers, R., A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y. Choi. 2019. “Defending Against Neural Fake News.” In Proceedings of the NeurIPS. Zhang, H., Q. Fang, S. Qian, and C. Xu. 2019. “Multi-Modal Knowledge-Aware Event Memory Network for Social Media Rumor Detection.” In Proceedings of the ACM MM. Zhang, Y., Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, et al. 2023. “Siren’s Song in the AI Ocean: A Survey on Hallucina- tion in Large Language Models.” arXiv preprint arXiv: 2309. 01219. Zhao, R., H. Chen, W. Wang, F. Jiao, X. L. Do, C. Qin, B. Ding, et al. 2023. “Retrieving Multimodal Information for Augmented Generation: A Survey.” arXiv preprint arXiv: 2303.10868. Zuo, Y., W. Zhu, and G. G. Cai. “Continually Detection, Rapidly React: Unseen Rumors Detection Based on Continual Prompt- Tuning.” In Proceedings of the COLING. How to cite this article: Chen, C., and K. Shu. 2024. “Combating misinformation in the age of LLMs: Opportunities and challenges.” AI Magazine 45: 354–68. https://doi.org/10.1002/aaai.12188 A UTH OR BIOGRAPH IES Canyu Chen (homepage: https://canyuchen.com)is a Ph.D. student in the Department of Computer Sci- ence at Illinois Institute of Technology. He focuses on truthful, safe, and responsible Large Language Mod- els with the applications in social computing and healthcare. He has started and currently leads an initiative “LLMs Meet Misinformation” (https://llm- misinformation.github.io), aiming to combat misinfor- mation in the age of LLMs. He has publications in top-tier conferences including ICLR, EACL, WWW, and NeurIPS. He won multiple awards such as Sigma Xi Student Research Award 2024, the Didactic Paper Award in the workshop ICBINB@NeurIPS 2023, Spot- light Research Award in the symposium AGI Leap Summit 2024. Kai Shu is a Gladwin Development Chair Assistant Professor in the Department of Computer Science at Illinois Institute of Technology. His research lies in machine learning, data mining, social computing with applications such as disinformation, education, and healthcare. He has published well-cited papers in major scientific venues, including TKDE, ICLR, KDD, WWW, SIGIR, CIKM, WSDM, IJCAI, AAAI, and EMNLP. He is the recipient of 2024 NSF CAREER Award, 2024 IIT College of Computing Dean’s Excellence in Research Award, 2023 Microsoft Accelerate Foundation Models Research Award, 2023 DARPA AI Forward Scholarship Award, 2023 AAAI New Faculty Highlights, 2023/2022 Cisco Faculty Research Awards, and 2020 ASU Engi- neering Dean’s Dissertation Award. More can be found at: http://www.cs.iit.edu/∼kshu. 23719621, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/aaai.12188, Wiley Online Library on [10/01/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License","libVersion":"0.3.2","langs":""}
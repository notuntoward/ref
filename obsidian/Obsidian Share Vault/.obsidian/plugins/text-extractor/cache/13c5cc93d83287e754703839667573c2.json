{"path":"lit/lit_notes_OLD_PARTIAL/Lehna22ReinfLrnTradeDEwind.pdf","text":"Energy and AI 8 (2022) 100139 Available online 5 February 2022 2666-5468/Â© 2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/). Contents lists available at ScienceDirect Energy and AI journal homepage: www.elsevier.com/locate/egyai A Reinforcement Learning approach for the continuous electricity market of Germany: Trading from the perspective of a wind park operator Malte Lehna âˆ—, BjÃ¶rn Hoppmann, Christoph Scholz, RenÃ© Heinrich Fraunhofer Institute for Energy Economics and Energy System Technology (IEE), KÃ¶nigstor 59, 34119 Kassel, Germany A R T I C L E I N F O Keywords: Deep Reinforcement Learning German intraday electricity trading Deep neural networks Markov Decision Process Proximal Policy Optimization Electricity price forecast A B S T R A C T With the rising extension of renewable energies, the intraday electricity markets have recorded a growing popularity amongst traders as well as electric utilities to cope with the induced volatility of the energy supply. Through their short trading horizon and continuous nature, the intraday markets offer the ability to adjust trading decisions from the day-ahead market or reduce trading risk in a short-term notice. Producers of renewable energies utilize the intraday market to lower their forecast risk, by modifying their provided capacities based on current forecasts. However, the market dynamics are complex due to the fact that the power grids have to remain stable and electricity is only partly storable. Consequently, robust and intelligent trading strategies are required that are capable to operate in the intraday market. In this work, we propose a novel autonomous trading approach based on Deep Reinforcement Learning (DRL) algorithms as a possible solution. For this purpose, we model the intraday trade as a Markov Decision Process (MDP) and employ the Proximal Policy Optimization (PPO) algorithm as our DRL approach. A simulation framework is introduced that enables the trading of the continuous intraday price in a resolution of one minute steps. We test our framework in a case study from the perspective of a wind park operator. We include next to general trade information both price and wind forecasts. On a test scenario of German intraday trading results from 2018, we are able to outperform multiple baselines with at least 45.24% improvement, showing the advantage of the DRL algorithm. However, we also discuss limitations and enhancements of the DRL agent, in order to increase the performance in future works. 1. Introduction 1.1. Overview In past decades, the day-ahead electricity markets were the ma- jor trading location to auction electricity capacities between market participants. However, with the European Green Deal [1] and the rising proportion of Renewable Energy (RE) production [2], there is a need for a higher flexibility in the electricity trade. Both electricity providers and prosumers require short-term solutions to respond to market changes and reduce their overall trading risk [3,4]. One possible solution is the ID market that enable trading with less than one day prior to the delivery. Here, especially RE producers can anticipate changes in their production forecast and modify their orders from the day-ahead market to ensure an accurate delivery. Consequently, the ID electricity markets have an increasing popularity amongst traders, resulting in record trading volumes across all European ID electricity âˆ— Corresponding author. E-mail addresses: malte.lehna@iee.fraunhofer.de (M. Lehna), bjoernhoppmann@web.de (B. Hoppmann), christoph.scholz@iee.fraunhofer.de (C. Scholz), rene.heinrich@iee.fraunhofer.de (R. Heinrich). spot markets in 2020 [5]. Similarly, in the research community there has been a rising interest in the analysis of the ID trade. The ID trade was examined from various perspectives, given that both the expansion of RE as well as the design of the ID markets differ across nations. In many cases the ID markets have been analyzed from a structural point of view, e.g., for the identification of influencing factors [6]. However, actual trading strategies have not been part of many analysis, considering that ID trading is not comparable to other commodity markets. With its short-term perspective, the strong influence of the RE, the volatility and the stochastic nature of the ID market, traditional trading methods are often not applicable. Consequently, alternative methods have to be investigated that can cope with these difficulties. In our work, we want to contribute to this open question, by proposing an autonomous trading agent based on Deep Reinforcement Learning (DRL). While the autonomous approaches is primarily de- signed to compensate forecast errors on the ID market, it also contribute to more system stability. Especially in combination with flexible energy https://doi.org/10.1016/j.egyai.2022.100139 Received 8 November 2021; Received in revised form 19 January 2022; Accepted 23 January 2022 Energy and AI 8 (2022) 100139 2 M. Lehna et al. providers, autonomous agents could be deployed in the long run to counteract against potential surpluses and thus reduce the risk of shortfalls in the electricity distribution. In order to ensure a realistic setting, the autonomous agent is deployed from the perspective of an electricity provider, specifically a wind park operator. Given that wind forecasts always contain some uncertainty, wind park operators have to incorporate this uncertainty in their production plan. Hence, they cannot exactly determine their produced volume a priori. The autonomous agent has to adjust the traded volumes on the ID markets based on recent changes in the forecasts. 1.2. Related work In terms of the research community, various topics regarding the ID market have been publicized. Many of them can be categorized either in the field of structural analysis, price forecasting or in the field of ID trading. In the first category, papers have been published that examined and analyzed different influencing factors on the ID markets [3,6â€“9]. Naturally, most of the papers reviewed the variability of the ID trade and were sometimes combined with an analysis of the RE or their fore- casting errors [6]. However, these analysis focused in most cases on a longer time horizon, thus are not always suited for a direct application. In contrast, the second topic of electricity price forecasting considers the short-term relationship between prices and their influencing factors, as seen in [10â€“14]. However, next to differences in the markets and the forecast models, the research often differed in the frequency of the forecast target. As example, [10] predicted the ID3 index of the German ID market, while [11,14] predicted the price movement of individual products. As a consequence, different influencing factors were of importance for the different researchers. Lastly, the third research subject is the trading of electricity and its automation on the ID market, where for example Machine Learning (ML) approaches were frequently published by different researchers, e.g., [15,16]. In recent years, a new ML approach was proposed that is especially suited to cope with the volatile price progression and the stochastic nature of the ID market. This respective approach is DRL, which can model complex relations and at the same time does not necessarily require expert knowledge of the trade relations. Accord- ingly, there have been first publications for both the Day-Ahead (DA) and ID market that addressed the challenges of the electricity trade in combination with DRL. In terms of the DA, the researchers utilized the DRL benefits to automatize and optimize the bidding process, sometimes in combination with the ID market. These bidding processes could be from the perspective of one [17] or multiple [18] producers, a load serving entity [19] or a electric storage unit [20]. For the ID market, especially the papers of [21â€“23] are noteworthy, which proposed trading strategies for the German continuous ID market. All three papers based their strategy on a Markov Decision Process (MDP), however differed in their algorithmic implementation and their research objective. In the paper of [21], Reinforcement Learning (RL) was used to implement a threshold policy for accepting price bids in the ID market. To determine the optimal policy, they defined an one- stage MDP and used the REINFORCE algorithm to solve the problem. A different use-case was proposed by [22,23], who both analyzed the trading under the consideration that they need to optimize the usage of an energy or pump storage unit. Through the assumption that the electricity can be stored, they were able to formulate the trading in a multi-stage MDP. In terms of [22], a Deep-Q network was implemented as DRL method, while [23] again applied the REINFORCE algorithm. Even though [23] showed the most advanced modeling of the ID trade, they still needed to impose some simplifications to reduce the dimen- sionality of the optimization problem. Accordingly, they incorporated price thresholds to limit the number of available orders and traded in 24 time steps for each day. By reducing the number of observations per episode, [23] were able to implement their DRL approach with satisfying results. However, under consideration that the ID trade is indeed traded in milliseconds, there is still a need to improve the DRL to make it applicable for the ID trade. In our work, we address this problem and offer the next step in the development of an autonomous trading agent. 1.3. Research contribution As outlined in the related work section, there are still various unan- swered questions concerning the ID electricity trade. With its rising importance and the increasing uncertainty of the RE, there is a practical need to analyze the capabilities of autonomous trading on the respec- tive market. The continuous ID trade with its high trading frequency offers the possibility to realize shortest-term trading decisions through the usage of DRL. In this paper, we propose a first DRL approach in this direction and model the continuous ID trade by simulating minutely trading steps. While there has been some research in this direction, to the best of our knowledge, the high trading frequency has not been part of any analysis. Therefore, it might offer new insights to the electricity trade, both from the research perspective as well as from the perspective of electricity traders. The contributions of this paper can be outlined as follows: 1. We model the ID market as a (first-order) MDP and propose DRL approach to simulate the continuous ID trade. 2. We develop a DRL environment based on the OpenAI library [24] that enables trading of hourly ID products in one minute resolution. 3. We validate our proposed DRL problem from the perspective of an electricity producer that trades the capacities of his wind farm on the German ID market. 4. Within our case study, we are able to show that the proposed DRL agent is able to trade volatile wind volumes in the ID simulation. 5. The DRL agent outperforms the best baseline agent with 45.24% more net profit in our case study. With the outlined research contribution, the remaining paper is struc- tured as follows. In Section 2, the methodology of the underlying DRL is presented. In Section 3 we transfer the framework to the scenario of the ID spot market. Thereafter, we elaborate our case study in Section 4 and present and discuss the results in Section 5. Lastly, in Section 6 we give an overall conclusion. 2. Methodology 2.1. Reinforcement learning as Markov decision process From the area of machine learning methods, the Reinforcement Learning (RL) is an approach first introduced by [25]. It is based on the concept that a model can solve complex problems without outlining the exact relations but instead by learning the correct behavior through incentives in the training process. In the RL approach, an agent is trained in an environment S that incorporates the structure of the real-life problem. The environment iterates over consecutive steps, as seen in Fig. 1, until an episode is complete. 1 For each step of the episode, the agent can take different actions ğ‘ âˆˆ A, with A denoting the space of possible actions. 2 The purpose of the environment is the communication with the RL agent, by supplying the information on its current state ğ‘ ğ‘¡ âˆˆ S at time ğ‘¡ and returning a feedback on the actions ğ‘ğ‘¡ in form of a reward ğ‘Ÿ âˆˆ R with ğ‘Ÿğ‘¡ âˆ¶= ğ‘…(ğ‘ğ‘¡, ğ‘ ğ‘¡), based on the reward 1 Depending on the design of the environment, the length of an episode is either fixed or is subject to the actions of the agent. 2 Most common are either a discrete action space, with ğ‘ consisting of specific actions, e.g., in [26], or continuous action space, where ğ‘ consist of a continuous interval, e.g., in [27]. Energy and AI 8 (2022) 100139 3 M. Lehna et al. Fig. 1. Visualization of the DRL-Framework with environment and agent. Based on the state ğ‘ ğ‘¡ of the environment, the agent computes the action ğ‘ğ‘¡. The action is returned, the reward ğ‘Ÿğ‘¡ calculated and saved for the optimization. Thereafter, the environment transitions to the new state ğ‘ ğ‘¡+1. Fig. 2. Exemplary neural network of the DRL approach. The hidden layers (green) pass the state ğ‘ ğ‘¡ information to the policy layer (light blue). Thereafter, an action ğ‘ğ‘¡ is chosen based on the policy. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) function ğ‘… âˆ¶ A Ã— S â†’ R. Within the episode of length ğ‘‡ , the target of the RL algorithm is to maximize the cumulative reward Ìƒğ‘…ğ‘ğ‘¢ğ‘š âˆ¶= âˆ‘ğ‘‡ ğ‘¡=1 ğ‘Ÿğ‘¡. In each step, the agent consequently chooses the action, which promises the best possible payout, considering all future actions. To correctly define the impact of each action, all the future reward arising from the action has to be considered. For this purpose, the RL problem is assumed to be a MDP, i.e., a (time-discrete) stochastic process {ğ‘€}ğ‘¡âˆˆğ‘‡ on a probability space (ğ›º, îˆ­, P) with ğ‘¡ denoting the set of time steps {0, 1, â€¦ , ğ‘‡ }, fulfilling the first-order Markov property for all ğ‘¡ âˆˆ ğ‘‡ , and all stateâ€“action pairs (ğ‘ğ‘¡, ğ‘ ğ‘¡) âˆˆ A Ã— S: P(ğ‘ ğ‘¡+1|ğ‘ğ‘¡, ğ‘ ğ‘¡) = P(ğ‘ ğ‘¡+1|ğ‘ğ‘¡, ğ‘ ğ‘¡, â€¦ , ğ‘0, ğ‘ 0). (1) Here, P is the transition probability to reach state ğ‘ ğ‘¡+1 from state ğ‘ ğ‘¡ taking action ğ‘ğ‘¡. In a first-order MDP, not all past information about the process are required to assess the probability, but instead the current state ğ‘ ğ‘¡ and the action ğ‘ğ‘¡ are sufficient enough. The consequence of assuming a MDP for the RL approach is that the RL algorithms need to approximate the underlying transition probability P and selects actions, which result in states leading to a higher reward. In RL terms, policies ğœ‹ğœƒ are probability distributions in each state ğ‘ ğ‘¡ over all the possible actions ğ‘ğ‘¡ âˆˆ A. Each probability ğœ‹(â‹…|ğ‘ ğ‘¡) denotes the likelihood of the respective action to yield the maximum reward over the whole episode. Note that next to the current state ğ‘ ğ‘¡, the policies depend on an underlying parameter ğœƒ. In the Deep Reinforcement Learning (DRL) approach, first introduced by [26], these parameters are the weights of a deep neural network, representing the policy and thereby the action selection behavior. Through optimizing the weights of the network, DRL algorithms aim to approximate the optimal policy ğœ‹âˆ— âˆ¶= ğœ‹ğœƒâˆ— offering the best decision for each state ğ‘ ğ‘¡ âˆˆ S. With regard to each step, displayed in Fig. 2, the network receives the current state ğ‘ ğ‘¡, as well as the previous reward ğ‘Ÿğ‘¡ for the policy optimization. The state information is forwarded to compute the policies ğœ‹ğœƒ(ğ‘ğ‘– ğ‘¡|ğ‘ ğ‘¡) of each available action ğ‘ğ‘– ğ‘¡. Based on the best policy, an action ğ‘âˆ— ğ‘¡ is chosen, the reward ğ‘Ÿğ‘¡+1 = ğ‘…(ğ‘âˆ— ğ‘¡ , ğ‘ ğ‘¡) is computed and the next iteration step is started. 2.2. Optimization algorithm There are various DRL algorithms available to update the policy ğœ‹(ğœƒ) that mainly differ in their update strategy and the underlying relationship between policy ğœ‹(ğœƒ) and parameters ğœƒ. In our work, we chose the PPO algorithm because it enables the usage of an continuous action space. It was firstly introduced in [28] and is an enhancement of the Asynchronous Advantage Actorâ€“Critic (A3C) method [29]. In the PPO method, two neural networks are utilized to optimize the policy iteratively. The Actor model is deployed in the environment to generate observations by computing actions and obtaining rewards. The second model, i.e., the Critic, forecasts based on the observations the probable future rewards given a specific action. The difference between the forecasted and actual reward is then evaluated in an advantage function Ì‚ğ´ğ‘¡(ğœƒ). Actions leading to a positive advantage (so higher reward gained by Actor than forecasted by Critic) are reinforced while actions leading to negative advantage are discarded. This optimization problem is formulated as a loss function ğ¿(ğœƒ), with the objective to minimize the loss based on the expected return and the parameters ğœƒ of the DRL model: ğ¿ğ‘¡(ğœƒ) = Ì‚ğ¸ğ‘¡ [ log ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) Ì‚ğ´ğ‘¡(ğœƒ) ] . (2) To improve the weights of the actor, the loss function is optimized through a SGD method, which in case of the PPO is the Adam al- gorithm [30]. However, previous algorithms prior to the PPO had often instabilities in their learning process, due to large updates in their parameters ğœƒ. As a consequence, [28] address this problem by introducing ğœ–-clipping in their loss function to limit the update of the optimization: ğ¿ğ¶ğ¿ğ¼ğ‘ƒ ğ‘¡ (ğœƒ) = Ì‚ğ¸ğ‘¡ [min(ğœğ‘¡(ğœƒ) Ì‚ğ´ğ‘¡(ğœƒ), ğ›¤ğ‘ğ‘™ğ‘–ğ‘) ] , with ğœğ‘¡(ğœƒ) = ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡) ğœ‹ğœƒğ‘œğ‘™ğ‘‘ (ğ‘ğ‘¡|ğ‘ ğ‘¡) , and ğ›¤ğ‘ğ‘™ğ‘–ğ‘ =ğ‘ğ‘™ğ‘–ğ‘(ğœğ‘¡(ğœƒ), 1 âˆ’ ğœ–, 1 + ğœ–) Ì‚ğ´ğ‘¡(ğœƒ). (3) Energy and AI 8 (2022) 100139 4 M. Lehna et al. As one can see, the loss function is the estimated expectation of the minimum value of either ğœğ‘¡(ğœƒ) Ì‚ğ´ğ‘¡(ğœƒ) or the clipped value ğ›¤ğ‘ğ‘™ğ‘–ğ‘. 3 Very large positive updates in policy are clipped while negative updates are still considered valid. Thus, it is ensured that the updates are not running to a local maximum. Next to the optimization of the loss function, [28] further include other algorithmic advantages that enhance the optimization from a computational perspective. These are the parallel computation of multiple Actors, the introduction of mini- batches and the multiple runs of SGD on the same data. All of the advances increase learning speed and exploration of the environment as well as learning stability, thus making the PPO our preferred algorithm. 2.3. Hyperparameter optimization The PPO algorithm has various hyperparameters, which have con- siderable influence on the training performance. Therefore, we included a hyperparameter optimization in our analysis. Under consideration of the DRL characteristics, we decided to use the PBT hyperparameter search algorithm, based on the paper of [31]. The PBT is constructed with an evolutionary concept, where multiple variations of the al- gorithm are trained simultaneously and the most fitting candidate is selected. For this purpose, the hyperparameters of multiple DRL agents are randomly sampled. After a specific training period, the performance of the agents is evaluated and the better performing candidates can continue their training. For the underachieving agents, the hyperpa- rameters are re-sampled and in addition the model states of the better candidates are copied. Therefore, no training progress is lost, while simultaneously different options are tested. 3. Research design 3.1. Overview over the research design In Section 3, we elaborate the structure of our research framework. For this reason, we divided the Section into multiple Subsections that build on each other. After describing the general research setting in Section 3.2, we analyze the German intraday market in Section 3.3 and derive restrictions for our environment in Section 3.4. Thereafter, we provide a detailed explanation of our environment in Section 3.5, with different paragraphs covering the episodes, action and observation space as well as the reward of the environment. Lastly, in Section 3.6, we cover briefly the models for the external forecasts that are required within the environment. 3.2. Trading perspective The trading environment is of huge importance for the overall success of the DRL algorithm and varies in the literature depending on the application of the agent. In our work, we construct research design from the perspective of a wind park operator, as stated in Section 1.1. As a consequence, the task of the trading agent is to react to the changes in the wind forecast of the producer and automatically conduct correction processes on the ID market. For this purpose, we combine our trading environment with external wind forecasts to train the agent. We evaluate our research on the German continuous ID electricity market, given that this market is frequently used in the literature, thus ensures comparability. Consequently, we shortly describe the structure of the continuous German ID market, derive restrictions and thereafter define our trading environment. 3 The ğœğ‘¡(ğœƒ) is the ratio between the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and the current updated one ğœ‹ğœƒ. 3.3. The German continuous intraday electricity market The German ID short-term electricity trade is primarily conducted on the EPEX SPOT exchange, where different electricity products, e.g., quarterly and hourly products, are traded by the market partic- ipants in a continuous manner. In this research we solely focus on the hourly products, which we hereinafter just referred to as products. 4 The ID trade is based on the M7 trading system, thus the trade is executed through an individual Limit Order Book (LOB) for each prod- uct, see [32]. In each LOB, open buy/sell orders are collected, sorted according to their highest/lowest price and matched if a new order with a lower sell/higher buy price is submitted. The matched orders are then recorded in the transaction data.5 For the hourly ID products, the trading interval starts at 3.00 pm of the previous day and closes 5 min p.t.d, as seen in Fig. 3. While the first hours of trade are open for both national and cross-border trade (trade on the XBID exchange), the trading is gradually restricted in conjunction with the distance to the delivery time. At one hour p.t.d only national trades are allowed, while in the last 30 min until 5 min p.t.d only trading in the control area is allowed. As a consequence, the trading behavior differs, depending on the delivery time and the respective market participants. 3.4. Restriction for the trading environment As a consequence of the ID trade structure, it was necessary to construct an environment for the DRL agent capturing both the es- sential information of the LOB, while simultaneously reducing their complexity. Thus, multiple restrictions were necessary to model an adequate simulation of the trades. First, we decided to build our environment only with the transaction data of the ID market, which is in most cases close to the actual LOB. This has the advantage that the transaction data consists only of traded contracts, hence excludes the various cases of open orders, trade limita- tions, over the counter contracts and other redundant information. One consequence arising with this restriction is that we establish our agent as a price taker with no direct influence on the ID price. Under the assumption that the agent is only trading small amounts of energy, this seems reasonable. If one wants to trade a larger amount of electricity, multiple orders from the LOB have to be considered, which might result in a disparity between the traded LOB price and the transaction prices. Second, while the electricity trade is continuous, i.e., traded in milliseconds, we limited our data to minutely frequency by aggregating one minute volume weighted averages of the price. Third, we restricted the trading time of the agent to the last four hours until the last 30 min p.t.d. This step was justified, under the consideration that most of the trades were executed in the last hours p.t.d, as seen in Fig. 4. In addition, we had not sufficient information on the trade in the control areas, thus we excluded the last 30 min p.t.d as well. Lastly, the fourth restriction is solely for the training data, because we excluded products that consisted of extreme outliers. This step was necessary to ensure stable training performance, however for the testing scenario no exclusion was conducted. 3.5. Detailed description of the environment With the outlined restrictions, the trading environment S was con- structed based on the methodology of Section 2.1. Hereby, the primary goal was to provide a trading environment that was close to the actual ID trade and incorporated most features. We followed the widely accepted Gym format of OpenAI [24] and again refer to Fig. 1 for a better overview. 4 In future work, we plan to include the quarterly hour products in our analysis, especially because they are highly correlated to the hourly products. 5 Note that the trading process is heavily simplified, given that there are various order types and restrictions (iceberg orders, fill or kill orders, etc.). Further, there is also a chronological order to execute similar orders. For a detailed analysis, we refer to [32] for more information. Energy and AI 8 (2022) 100139 5 M. Lehna et al. Fig. 3. Overview over the trading stages of an ID product with delivery at day ğ‘‘ and time ğ‘¡. The trading starts at 3.00 pm on the previous day ğ‘‘ âˆ’ 1 and consist of national and cross-border trading. One hour p.t.d the cross-border trading stops, leaving national trading, and half an hour only trading in the control area is allowed. Fig. 4. Visualization of the average number of trades per product, aggregated to minutely values, c.f. [14]. The left axis depicts the aggregated volume in MWh, while the right axis displays the number of trades in the minute. Regarding the case study, the left vertical line (black) shows the beginning of our trading interval, while the right vertical line (red) shows the end of the trading interval. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Episodes. To begin with, each hourly product was considered an in- dividual episode in the environment. Depending on the scenario, the products were either randomly sampled (training) or processed sequen- tial (testing). In the episode, the one minute volume weighted averages of the transaction price was computed. However, at some minutes no new observations were available, i.e., no new trade occurred, resulting in a forward passing of the previous price in order to ensure minutely coverage. As explained in the previous section, we fixed the number of trading steps to a length of 3.5 h, thus each episode had a total of 211 time steps. Note that the agent had no influence on the completion of the episode, which differs from other DRL environments. Action space. For the action space A of the environment, we established a continuous action space, which should reflect the traded volume of the wind farm on the electricity market. We restricted the operation margin, by setting lower and upper limits of MWh to our trading interval ğ‘ğ‘¡ âˆˆ [0, 1] for all ğ‘¡ âˆˆ ğ‘‡ . At each time step ğ‘¡, the agent adjusted through its action ğ‘ğ‘¡ the traded volume and the difference ğ›¥ğ‘£ğ‘¡ = ğ‘ğ‘¡ âˆ’ğ‘ğ‘¡âˆ’1 thereafter was traded with the respective electricity price.6 With regard to the trading interval ğ‘ğ‘¡, we decided to restrict the action for the following reasons. First, through only trading a maximum of one MWh, we could ensure that the price of the transaction was relatively similar to the LOB, as outlined in the previous section. Second, the interval simplified the interpretation of the results. Third, the forecasted wind 6 As example, the current traded portfolio of wind power consist of 0.5MWh for a specific product at time step ğ‘¡ = 3. The DRL agent proposes the action ğ‘4 = 0.6, thus the traded volume has to be increased by ğ‘£4 = 0.1, i.e. 0.1MWh have to be sold on the ID market. volumes were also normalized within the interval [0,1], therefore the limit of A ensured a straightforward implementation. Observation space. The observation space of the DRL agent, i.e., the representation of the current state ğ‘ ğ‘¡, consisted of twelve features. We list the full table of our observation variables in Table A.2 in the Appendix A. Next to the current price of the ID market ğ‘ğ‘¡ and the forecast of the production volume ğœ‚ğ‘¡ (Section 3.6), ten additional features were included to increase the performance of the agent. These features can be differentiated in price variables, volume variables and episode variables and the most important ones are described below. The first important feature is the 5min-forecast of the electricity price Ì‚ğ‘5ğ‘šğ‘–ğ‘› ğ‘¡ (Section 3.6), which we introduce to indicate possible future trends for the agent. Further, we calculated the value of the portfolio by averaging the selling price in relation to the respective volume, denoted as Ìƒğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ ğ‘¡ . With regard to the volume related variables, we included the current volume sold to the market, i.e., the previous action ğ‘ğ‘¡âˆ’1 to ensure that the agent knows the available quantity for trading. In addition, we also included a difference between the current volume and the predicted production of the wind forecast, ğ‘£ğ‘œğ‘™ğ‘‘ğ‘–ğ‘“ ğ‘“ = ğœ‚ğ‘¡ âˆ’ ğ‘ğ‘¡âˆ’1. Lastly, we also added a distance measure that denotes the time until the trade ends ğ‘¡ğ‘¡ğ‘’, given that the variance of the ID price increases in the last minutes of the trade. Reward. From the producer point of view, an increase in the trade volume would correspond to selling more capacity on the market and respectively a decrease in volume would correspond to buying back some electricity. Accordingly, we reflected this relation in the reward ğ‘Ÿğ‘¡ of the agent, which we defined for each time step ğ‘¡ as follows: ğ‘Ÿğ‘¡ = ğ‘Ÿğ‘£ğ‘œğ‘™ ğ‘¡ + ğ‘Ÿğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘’ ğ‘¡ . (4) Energy and AI 8 (2022) 100139 6 M. Lehna et al. As one can see, the reward was separated into two parts, with the first part being the trade reward ğ‘Ÿğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘’ ğ‘¡ and the second the volume reward ğ‘Ÿğ‘£ğ‘œğ‘™ ğ‘¡ . The trade reward is valid for all time steps ğ‘¡ of the episode and is defined as follows: ğ‘Ÿğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘’ ğ‘¡ = ğ‘ğ‘¡ğ›¥ğ‘£ğ‘¡ âˆ’ ğ‘ğ‘“ ğ‘’ğ‘’ âˆ£ ğ›¥ğ‘£ğ‘¡ âˆ£, with ğ›¥ğ‘£ğ‘¡ = ğ‘ğ‘¡ âˆ’ ğ‘ğ‘¡âˆ’1 (5) As one can see, ğ‘Ÿğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘’ ğ‘¡ the denotes the selling/buying of the action difference ğ›¥ğ‘£ğ‘¡ to the current price ğ‘ğ‘¡. Further, we also included a penalty for trading which reflects the transaction cost ğ‘ğ‘“ ğ‘’ğ‘’ and was set to 0.2e/MWh after consultation with industrial experts Thus, if ğ‘ğ‘¡ > 0.2 an increase in volume (sold MWh) corresponds to a positive trade reward, while a decrease in volume (buy MWh) corresponds to a negative trade reward. Regarding the second part of the reward, the ğ‘Ÿğ‘£ğ‘œğ‘™ ğ‘¡ is only computed in the last step of the episode ğ‘‡ and is defined as follows: ğ‘Ÿğ‘£ğ‘œğ‘™ ğ‘¡ = {0 for ğ‘¡ < ğ‘‡ âˆ’0.1ğ›¥ğ‘£2 ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ for ğ‘¡ = ğ‘‡ , with ğ‘£ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ = ğœ‚ğ‘‡ âˆ’ ğ‘ğ‘‡ . (6) The ğ‘Ÿğ‘£ğ‘œğ‘™ ğ‘¡ is a penalty reward that measures the distance between the last action of the agent ğ‘ğ‘‡ and the last provided wind forecast ğœ‚ğ‘‡ . Through the quadratic term, the volume reward penalized large deviations, thus ensuring adaption at the end of the episode. 3.6. External forecasts In order to ensure a realistic trade behavior of the agent, we pro- vided two forecasts in the observation space. The forecasts in question were on the one hand the wind power forecast of the wind farm ğœ‚ğ‘¡ and on the other hand a forecast of the next five minute price average Ì‚ğ‘5ğ‘šğ‘–ğ‘› ğ‘¡ . The motivation to include wind power forecast was to indicate the expected production capacity for trading on the electricity market. For the price forecast, we wanted to capture possible price signals and trading trends for the DRL agent. In the following, we outline their structure shortly. 7 Wind forecast. In terms of the wind power forecast, we wanted to ensure a realistic setting by using a current state-of-the-art wind power prediction model. Therefore, we implemented an encoderâ€“decoder Re- current Neural Network (RNN) [33] for a multi-step-ahead forecast of the power generated by a single wind farm. Encoderâ€“decoder RNN consist of an encoder network that processes an input sequence and encodes it into a latent representation. The decoder network thereafter translates the encoded information to sequentially generate the output sequence, with each previously generated element influencing the next. Encoderâ€“decoder RNN have been used several times in the literature for wind and solar power prediction [34]. In case of our analysis, we used an encoder consisting of an Long Short-Term Memory (LSTM) layer with 50 neurons. As a decoder, we used an LSTM layer with 50 neurons, followed by a dense layer with one neuron and a leaky ReLU activation function. The encoder received wind speed forecasts and wind power measurements from the past 28 h as input. The decoder was initialized with the encoderâ€™s hidden states. Further it received the wind power prediction of the previous prediction step such as the wind speed forecast of the current prediction step as input. The wind speed forecasts for the location of the wind farm were extracted from the weather forecasts of the ICON-EU model, provided by the German Meteorological Service [35]. 7 The hyperparameter of the forecast models are in the Appendix C. Intraday price forecast. The ID price forecast was constructed similar to the paper of [14], where both a LSTM and XGBoost model were used to predict future prices. Contrary to cite [14], we decided to forecast the weighted ID price of the next 5 min interval, instead of multiple 15 min intervals. This provided the advantage that short term changes are anticipated faster, at the cost of losing mid-range information. With regard to the model, we found that both LSTM and XGBoost model had very similar results, with slightly better values of the XGBoost model in the 5 min forecast. Under consideration that the model is also fast in computation and returns more stable values, we decided to use the XGBoost model. Consequently, we implemented the respective model similar to [14]. In order to predict a specific month, the previous three months are considered as training interval. Note that we retrained the models for each new month to embed seasonal differences in the models. With regard to the input data, we included the transaction and LOB data of the EPEX SPOT market as well as the national wind forecast error of Germany. Note that these national forecast errors are not in direct relation with the previous described wind power forecasts of a single wind farm. 4. Case study 4.1. Electricity trading on the German ID market of 2018 In this case study, we analyze the behavior of the DRL trading agent based on historic trading data from the German ID market. As primary data set the EPEX SPOT ID transaction data of 2018 was used for the trading environment, because the data was already available to the institute. With regard to trading regulations, we want to point out two market changes that happened in 2018. First, in June 2018 the XBID trading went live, which enables ID cross border trading [36]. Second, the German-Austrian bidding zone was separated in October 2018 into two different market areas [37]. Considering the impact of the first change, [8] found that the introduction of the XBID had no direct influence on the trading behavior, thus could be disregarded for market modeling. 8 However, in terms of the market split we could not find similar analysis, thus decided to only selected the summer months of 2018 as our data set. In order to ensure impartial results, we proposed 10% of our data as out-of-sample test data, resulting in a training period from the 01.05.2018 till the 14.09.2018, with the remaining data 15.09â€“30.09.2018 as independent testing interval. The test interval represents the overall ID price of 2018 sufficiently and incorporate both products with average price distributions as well as outliers in the ID price. We present a more detailed analysis and comparison in the Appendix B. Further, to give a better overview, we display for each product the average ID price as well as the minimal/maximal outlier in Fig. 5, with the testing interval underlined in red. As one can see, there are some extreme outliers in the data that might result in unstable training behavior. Consequently, training products with values above 150e or below âˆ’50e were discarded, which totals in 33 unregarded products. For the testing interval no products were filtered, given that these observations would not be known beforehand. Overall, we have in total 3.255 training and 384 testing products, which translates into 686.805 training and 81 024 testing observation in our analysis. With regard to the optimization of the agent, two independent training trials were computed to ensure different selections in the PBT and stable training. In each trial, the best 20 checkpoints of the training runs were saved. Under consideration that both trials had similar optimal values in their mean reward and no distinct difference was visible, we 8 One consequence that derives from the introduction of the XBID is that until one hour p.t.d international orders were excepted for the products. Thus, it is possible that international market actors participate and influence the German ID trade to some extend. To keep the data as original as possible, we did not undertake any measurements to remove this possible impact. Energy and AI 8 (2022) 100139 7 M. Lehna et al. Fig. 5. Plot of the 2018 German electricity ID price. Each product is visualized with its average, maximum and minimum value of the ID price. The training set is from May 2018 till mid of September, while the testing set is the remaining September (red). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) compared the performance of all checkpoints with a validation data set. The validation data set consisted of the last 28 days from the training products.9 The checkpoint with the best overall mean performance was then selected as the final PPO agent. 4.2. Baseline agents As part of the case study, we want the agent to perform against different baseline agents to have an objective benchmark and assess the performance of the DRL agent correctly. However, some ground rules are necessary to ensure equal chances. First, all agents, including the DRL agent, had to start with a trading volume of 0 and could only trade in between the interval of [0,1] MWh. Further, to ensure physical delivery all agents were forced to end their trading on the last value of the wind forecast, which we corrected automatically at the end of each episode. Through fixing the start and end volume of the trading interval, we ensured a fair comparison, given that the performance was only depending on the movements within the trading interval. With respect to the individual baseline agents, different approaches were chosen that should reflect different perspectives of the energy trading. The first and easiest agent ğµğ¿First sold in the first step the volume according to the first wind forecasted volume ğœ‚0 at time ğ‘¡0 without any other considerations. Then in the last step, it corrected the volume to the last predicted value of ğœ‚ğ‘‡ . In conceptual terms, the second agent ğµğ¿WF was constructed similar, because it strictly followed the wind volume forecast and traded every change (every 15 min) immediately. This agent therefore corresponds to a very conservative rule-based trading approach. The remaining agents are more sophis- ticated to reflect more complex approaches. The third agent ğµğ¿PF is based on the ID price forecast values Ì‚ğ‘5ğ‘šğ‘–ğ‘› ğ‘¡ and should reflect a more traditional rule based approach in combination with an ML forecasting approach. The aim of the agent was to capture potential arbitrage possibilities between the actual price and the expected future price. If for two sequential periods the difference between the price and the forecast indicated a smaller/larger energy price, the agents sold/bought a fixed proportion of its volume (0.1 MWh) on the energy market. Our reasoning for proportional trading is that a higher proportion also induces higher transaction cost. Further, regarding the two sequential 9 The validation set was the interval from 2018-08-17 17:00:00 till 2018-09-14 23:00:00, i.e., 28 days each with 24 h. periods, we argue that the forecast Ì‚ğ‘5ğ‘šğ‘–ğ‘› ğ‘¡ is an average forecast for the next five minutes and hence captures a trend that might occur in later time steps. These constrains were experimentally found and showed better results than simply trading all capacities whenever the difference between forecast and price appeared. The last agent ğµğ¿Random is a stochastic approach that should simulate a random behavior on the ID market. With a probability of 25%, the baseline samples a new trading volume from a normal distribution. Thereby, the mean was the current wind forecast for the product, while the standard deviation was the standard deviation of the training data set. The 25% probability ensures that the agents performance is not to much influenced by the transaction costs of 0.2 e/MWh . 4.3. Experiment metrics For the comparison of the agents, we use multiple experiment metrics which we shortly introduce. First, we calculate the overall profit, which is the cumulative return across all test products, subtracted by the transaction costs (0.2e/MWh). Second, we derive a percentage performance in comparison to cumulative return of the ğµğ¿WF baseline. Third, we present distributional properties with the mean, the standard deviation as well as the 10% and 90%Quantiles to give a better under- standing of the allocation of the trading results. In addition, we were also interested in the best trade result, which the respective agent had in comparison to its peers. Therefore we return a relative measure in % in the results. Lastly, we conduct the average number of actions per product for the respective agent. 5. Results 5.1. Trading results Based on the specifications of the case study, the results are shown in the following section. To begin with, we first present an exemplary product and thereafter analyze the overall results of the agents. Exemplary product. We selected an exemplary product 18.09.2018 at 23:00 UTC to visualize the actions of both agent and the baseline methods.10 In Fig. 6 the respective actions are plotted, while in Fig. 6b 10 For the selection, we sampled from the test products with the condition that at all agents executed at least one trade movement. Energy and AI 8 (2022) 100139 8 M. Lehna et al. Fig. 6. Visualization of agent and baseline performance on the exemplary product 2018-09-18 at 23.00. we plotted the price history of the product and compare it to the agent and ğµğ¿PF action. Beginning with the baselines, one can see that both the ğµğ¿First and ğµğ¿WF were only trading twice, due to the minimal change in the wind forecast. Similarly, the ğµğ¿Random had a relative narrow range in its movement, resulting in only a small deviation from the forecast. Consequently, all three agents had similar trade results. In contrast, the DRL-agent and ğµğ¿PF had large changes in their trading action and use the full potential of their trading range. Both agents first sold all their available capacity in the first 30 min, re-bought when the price dropped (at around 20.45) and then again sold their capacity when the price was rising again. However, after 21.00 the ğµğ¿PF started to sell parts of their capacity, while the agent seemed to anticipate future price trends and only moved after 22.05. Consequently, when looking at Fig. 6b one can see that the agent was exploiting the price drops at a lower price level than the ğµğ¿PF. Further, we want to draw the attention to the spike of the agent at 22:11, where the agent exploited the sudden plunge of the ID price. As a result, the agent was able to achieve a total return of 20.41e, whereas the ğµğ¿PF only managed a total of 10.36e. Overall performance. While the visualization of a single product offers a first idea on the typical action range, we are naturally more interested in the overall trading results. Therefore, we visualize the performances across all products through a boxplot in Fig. 7. The first thing that becomes apparent is that both the DRL agent as well as the ğµğ¿PF had a larger spread in their performance in comparison to the other more narrowly constructed baselines. This spread does not only correspond to higher trading values but also includes more negative and positive trading results. On the one hand, the DRL agent traded twice with a negative reward of âˆ’39.86e and âˆ’45.27e. On the other hand, the agent was also able to achieve extraordinary returns with a maximum returns of 186.69e and 114.00e. Given that the one cannot fully distinct between the median and quantiles of the boxplot, we report the performance metrics in the Table 1. Here, we can see that the DRL agent was in both mean and median better than the baselines, with the ğµğ¿PF at second place, fol- lowed by the ğµğ¿WF, ğµğ¿First. The worst performance was the ğµğ¿Random, which was not surprising and shows that random behavior does return sufficient results on the ID trade. Note that regarding the medians, the median of ğµğ¿PF and the agent were quite similar. In addition, we are Energy and AI 8 (2022) 100139 9 M. Lehna et al. Fig. 7. Box-plot of the performance from DRL agent and the baselines. The ğ‘¦-axis denotes the total return from each product of the test sample. Table 1 Trade results of the DRL agent and the baseline agents based on the performance metrics of Section 4.3. Agent ğµğ¿First ğµğ¿WF ğµğ¿PF ğµğ¿Random Mean 9.72e 6.23e 6.15e 6.69e 5.50e Median 6.86e 5.24e 5.26e 6.84e 4.59e Standard deviation 16.53e 6.42e 6.18e 10.47e 6.42e 10% Quantile âˆ’1.01e 0.00e 0.00e âˆ’4.73e âˆ’0.79e 90% Quantile 23.88e 13.63e 13.45e 16.45e 12.65e Total net profit 3731.69e 2390.85e 2360.56e 2569.22e 2111.08e % Improvement to ğµğ¿ğ‘Š ğ¹ 58.08% 1.28% 0.00% 8.84% âˆ’10.57% Best performance in % 40.24% 15.48% 11.90% 28.57% 3.81% Steps 27.95 1.08 1.34 77.89 26.59 able to confirm the assumption that the agent and ğµğ¿PF had a larger variance than the remaining baselines, which is both visible in the standard deviation as well as the quantiles. Naturally, when analyzing trading methods one is primarily interested in the total net profit, where the agent was able to achieve the highest return with 3731.69e and outperformed the ğµğ¿WF by 58.08%. With comparison to the best baseline (ğµğ¿PF) this results to a surplus of 1162,47e. Considering the best performance per product, we see that the agent was able to achieve in 40.24% of times the best performance in direct comparison to the other baselines. Interestingly, the ğµğ¿PF was following relatively close with 28.57%, while the remaining agents were clearly exceeded. Lastly, we consider the average number of steps per product, where the agent did an average of 27.95. In comparison to the 77.89 (ğµğ¿PF) and 26.59 (ğµğ¿Random) steps, these results conform the visual examination of the example product, where the agent had both some active trading intervals but also some times, where the action did not change. 5.2. Discussion and future outlook Under consideration that this paper proposed a first attempt at a shortest-term trading DRL agent, we were able to show that it was indeed possible to apply DRL on the ID electricity trade in a minutely resolution. Even though the ID price emits a very high variance, the agent was able to identify patterns and create a sufficient trading strategy. This was primarily achieved through the development of the DRL environment. By excluding extreme outliers and implementing the necessary restrictions, we were able to create a training environment, which returned sufficient performance. The overall results clearly in- dicate an advantage through the DRL approach, considering that the agent outperformed all baselines, with best performances in 40.24% of the products. However, in comparison to the simple forecast of the ğµğ¿ğ‘Š ğ¹ and ğµğ¿ğ‘“ ğ‘–ğ‘Ÿğ‘ ğ‘¡, we have to point out the higher variance in the trade results of the DRL agent. When looking at the highest negative return of the agent, i.e., the 2018-09-20 20:00:00 product with a âˆ’45.27e loss, we observed that this product had large price increase up to 176.61e. The agent was not able to detect the price increase, which might partly be explained by the exclusion of extreme outliers from the training data. This might be a drawback for investors that are risk avers and do not want to anticipate negative results. Therefore, in future work one has to find a solution to cope with extreme outliers to ensure more stable results. Another deduction we have to make, is the role of the short-term price forecast. Its inclusion showed a clear increase in the agents performance and also returned better results in the ğµğ¿ğ‘ƒ ğ¹ baseline. Regarding the median results of the agent and ğµğ¿ğ‘ƒ ğ¹ , there might be an indication that the average products can indeed be traded with the price forecast. However, for more volatile products the DRL agent provided a better trading strategy, resulting in overall better performance. Nevertheless, it will be interesting in future analysis, whether a more complex rule-based approach is able to achieve similar results than the agent. Lastly, given the limitations of our data, we want to address the fact that we evaluated the agent only on test data from one time period. On the one hand, the sample was a relative sufficient representation of the ID price from 2018, given that products with various price distributions were included, again see Appendix B. On the other hand, a two week period might not fully reflect possible long term influence, e.g., seasonal effects, that have an impact on the ID price. Therefore, to ensure reliable results, it will be necessary in future work to test the agent on large test samples as well as newer data. In this context, it might be interesting to enhance the performance through the inclusion of additional variables. One possible candidate could be the order book data, both as input variable as well as in the price mechanism. This would increase the complexity of the environment, considering that the bidding with orders and taking completed orders of the market needs to be included in the environment. 6. Conclusion In this work, trading on the ID electricity market was formulated as a MDP in order to apply the DRL algorithms for autonomous trading. The novelty of the approach was the shortest-term resolution of one minute steps to depict the continuous trade. We offer multiple advance- ment to create a RL environment in which an DRL agent can train, while simultaneously remaining a realistic setting. The DRL environ- ment was tested in a case study from the perspective of an electricity provider to manage a wind park with data from 2018. Our experiments show sufficient results with the agent outperforming the baselines by Energy and AI 8 (2022) 100139 10 M. Lehna et al. Table A.2 Overview over the observation space variables. All variables are normalized in order to ensure stable training results. Note that the day ahead variable ğ‘ƒğ‘‘ğ‘ğ‘¦ğ‘ â„ğ‘’ğ‘ğ‘‘ is constant within each period and therefore has no time index. Variable group Variables Abbr. and Equation Price variables Current and previous volume weighted electricity price ğ‘ğ‘¡, ğ‘ğ‘¡âˆ’1 Day-ahead price ğ‘ƒğ‘‘ğ‘ğ‘¦ğ‘ â„ğ‘’ğ‘ğ‘‘ Forecast of electricity price Ì‚ğ‘5ğ‘šğ‘–ğ‘› ğ‘¡ Current and previous difference between price and forecast ğ‘‘ğ‘¡, ğ‘‘ğ‘¡âˆ’1 with ğ‘‘ğ‘¡ = ğ‘ğ‘¡ âˆ’ Ì‚ğ‘ 5ğ‘šğ‘–ğ‘› ğ‘¡ Portfolio price of the current volume Ìƒğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ ğ‘¡ Price marker ğ‘š = â§ âª â¨ âª â© 1 for ğ‘‘ğ‘¡ > 0andğ‘‘ğ‘¡âˆ’1 > 0 âˆ’1 for ğ‘‘ğ‘¡ < 0andğ‘‘ğ‘¡âˆ’1 < 0 0 ğ‘’ğ‘™ğ‘ ğ‘’ Volume variables Forecast of the production volume from wind park ğœ‚ğ‘¡ Current volume of the trading agent ğ‘ğ‘¡âˆ’1 Difference of current volume and production volume ğ‘£ğ‘œğ‘™ğ‘‘ğ‘–ğ‘“ ğ‘“ Episode variable Time to end ğ‘¡ğ‘¡ğ‘’ = 1 âˆ’ ğ‘¡ ğ‘‡ 45.24%. We discuss the implication of the results as well as limitations of our first approach and further offer possible enhancements for future research. Declaration of competing interest The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment This publication was supported by the Hessian Ministry of Higher Education, Research, Science and the Arts, Germany through the Com- petence Center for Cognitive Energy Systems of the Fraunhofer IEE with reference number: 511/17.001. Appendix A. Input variables of observation space In this section of the appendix, we present the underlying variables of the observation space to more detail. Appendix B. Testsample As a test sample, 10% of the overall data was selected to vali- date the performance of the agents. We decided to use the products from 15.09.2018â€“30.09.2018 under consideration that these products represent the overall data sufficiently. In the test sample, the product 25.09.2018 records with âˆ’224.43e the lowest ID price of 2018. Addi- tionally, the products 2018-09-20 20:00:00 and 2018-09-29 19:00:00 are with their maximum values of 200e and 249e within the top 20 of the highest products of 2018. Therefore, they might offer some insight in the outlier behavior of the model. The appropriateness of the test data set can also be seen in Fig. B.8, where we visualize the overall ID prices of 2018 in comparison to the test interval. Generally, one can see that the test sample represents the product adequately, however small negative skewness in the tails is visible. Note that it is planned for future work to consider further training and testing periods to investigate the quality of the models. Appendix C. Hyperparameter selection In this section of the appendix, the final hyperparameters of the PPO algorithm after the optimization with the PBT are presented. Thereafter, the hyperparameters of the wind and price forecast are displayed. Table C.3 Specifications and hyperparameter of PPO algorithm. The hyperparameters clip param- eter, entropy coefficient, gamma, learning rate, SGD per iteration and value function loss coefficient were selected by the PBT algorithm. Specification Parameter Value Hyperparameter PPO Clip parameter 0.432 Entropy coefficient 0.001433 Gamma 0 KL coefficient 0.5 Learning rate 0.0001 SGD per iteration 7 SGD minibatch size 422 Train batch size 2532 Value function clip parameter 10 Value function loss coefficient 0.984103 Shared value function layers Yes Network specification First hidden layer units 64 Second hidden layer units 64 Third hidden layer units 32 Activation function Tanh Table C.4 Forecast setting and hyperparameters of the XGBoost for the price forecast. The hyperparameters were selected through cross-validation of the training data. Specification Parameter Value Hyperparameter Col-sample by tree 0.817699 Gamma 0.097991 Learning rate 0.043568 Max depth 9 N-estimators 184 Sub sample 0.755471 Forecast horizon Next 15 min 3 Ã— 5 min Forecast update Recalculation of the forecast Every 5 min C.1. Hyperparameter PPO See Table C.3. C.2. Hyperparameter price forecast See Table C.4 C.3. Hyperparameter wind forecast See Table C.5 Energy and AI 8 (2022) 100139 11 M. Lehna et al. Fig. B.8. Histogram of the 2018 products in comparison to the products of the test sample. The overlap in the distribution can be seen as the dark green area in the histogram. Note that the different ğ‘¦-axis correspond to the frequency in the histogram-bins. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Table C.5 Forecast settings and hyperparameters of the wind forecast model. Specification Parameter Value Model architecture Layer size encoder 1 Layer size decoder 1 Hidden layer size encoder 50 Hidden layer size encoder 50 Training parameter Learning rate 1eâˆ’4 alpha 1eâˆ’5 ğ‘™1ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ 0.1 L1 regularization alpha âˆ— ğ‘™1ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ L2 regularization alpha âˆ— (1 âˆ’ ğ‘™1ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ) âˆ— 0.5 Early stopping (patience) 5 Mixed teacher forcing (dynamic reduction) 0.5-0.0 Training specification Training data Prev. 2 months Validation data Last 12.5% of training data Test data Current month Features U-Component of wind 10 m above the ground U-Component of wind 100 m above the ground V-Component of wind 10 m above the ground V-Component of wind 100 m above the ground Forecast horizon 4 h 16 Ã— 15 min Forecast update Recalculation of forecast Every 15 min References [1] European Comission. The European green deal. 2021, https://eur-lex.europa. eu/legal-content/EN/TXT/PDF/?uri=CELEX:52019DC0640&from=EN (Accessed: 2021-10-27). [2] IEA. Global energy review 2021. 2021, https://www.iea.org/reports/global- energy-review-2021 (Accessed: 2021-10-27). [3] Kiesel R, Paraschiv F. Econometric analysis of 15-minute intraday electricity prices. Energy Econ 2017;64:77â€“90. http://dx.doi.org/10.1016/j.eneco.2017.03. 002. [4] Shinde P, Amelin M. A literature review of intraday electricity markets and prices. In: 2019 IEEE milan PowerTech. 2019, p. 1â€“6. http://dx.doi.org/10.1109/ PTC.2019.8810752. [5] EPEX SPOT. New record volume traded on EPEX spot in 2020. 2021, https: //www.epexspot.com/en/news/new-record-volume-traded-epex-spot-2020 (Ac- cessed: 2021-10-27). [6] Ziel F. Modeling the impact of wind and solar power forecasting errors on intraday electricity prices. In: 2017 14th international conference on the Euro- pean energy market (EEM). 2017, p. 1â€“5. http://dx.doi.org/10.1109/EEM.2017. 7981900. [7] Koch C, Hirth L. Short-term electricity trading for system balancing: An empirical analysis of the role of intraday trading in balancing Germanyâ€™s electricity system. Renew Sustain Energy Rev 2019;113:109275. [8] Kath C. Modeling intraday markets under the new advances of the cross-border intraday project (XBID): Evidence from the German intraday market. Energies 2019;12:4339. http://dx.doi.org/10.3390/en12224339. [9] Pape C, Hagemann S, Weber C. Are fundamentals enough? Explaining price variations in the German day-ahead and intraday power market. Energy Econ 2016;54:376â€“87. http://dx.doi.org/10.1016/j.eneco.2015.12.013. [10] Narajewski M, Ziel F. Econometric modelling and forecasting of intraday elec- tricity prices. J Commod Mark 2019;100107. http://dx.doi.org/10.1016/j.jcomm. 2019.100107. [11] Narajewski M, Ziel F. Ensemble forecasting for intraday electricity prices: Simulating trajectories. Appl Energy 2020;279:115801. [12] Uniejewski B, Marcjasz G, Weron R. Understanding intraday electricity markets: Variable selection and very short-term price forecasting using LASSO. Int J Forecast 2019;35:1533â€“47. http://dx.doi.org/10.1016/j.ijforecast.2019.02.001. [13] Janke T, Steinke F. Forecasting the price distribution of continuous intraday electricity trading. Energies 2019;12(22):4262. Energy and AI 8 (2022) 100139 12 M. Lehna et al. [14] Scholz C, Lehna M, Brauns K, Baier A. Towards the prediction of electricity prices at the intraday market using shallow and deep-learning methods. In: Workshop on mining data for financial applications. Springer; 2020, p. 101â€“18. [15] Wang W, Yu N. A machine learning framework for algorithmic trading with virtual bids in electricity markets. In: 2019 IEEE power & energy society general meeting (PESGM). IEEE; 2019, p. 1â€“5. [16] Baltaoglu S, Tong L, Zhao Q. Algorithmic bidding for virtual trading in electricity markets. IEEE Trans Power Syst 2018;34(1):535â€“43. [17] Ye Y, Qiu D, Sun M, Papadaskalopoulos D, Strbac G. Deep reinforcement learning for strategic bidding in electricity markets. IEEE Trans Smart Grid 2019;11(2):1343â€“55. [18] Rashedi N, Tajeddini MA, Kebriaei H. Markov game approach for multi-agent competitive bidding strategies in electricity market. IET Gener Transm Distrib 2016;10(15):3756â€“63. [19] Xu H, Sun H, Nikovski D, Kitamura S, Mori K, Hashimoto H. Deep reinforcement learning for joint bidding and pricing of load serving entity. IEEE Trans Smart Grid 2019;10(6):6366â€“75. [20] Wang H, Zhang B. Energy storage arbitrage in real-time markets via reinforce- ment learning. In: 2018 IEEE power & energy society general meeting (PESGM). IEEE; 2018, p. 1â€“5. [21] Bertrand G, Papavasiliou A. An analysis of threshold policies for trading in continuous intraday electricity markets. In: 2018 15th international conference on the European energy market (EEM). IEEE; 2018, p. 1â€“5. [22] Boukas I, Ernst D, Papavasiliou A, CornÃ©lusse B. Intra-day bidding strategies for storage devices using deep reinforcement learning. In: International conference on the European energy market, ÅÃ³dÅº 27-29 June 2018. 2018, p. 6. [23] Bertrand G, Papavasiliou A. Reinforcement-learning based threshold policies for continuous intraday electricity market trading. In: 2019 IEEE power & energy society general meeting (PESGM). IEEE; 2019, p. 1â€“5. [24] Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, Zaremba W. Openai gym. 2016, arXiv:arXiv:1606.01540. [25] Sutton RS, McAllester DA, Singh SP, Mansour Y, et al. Policy gradient methods for reinforcement learning with function approximation. In: NIPs, Vol. 99. Citeseer; 1999, p. 1057â€“63. [26] Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, et al. Human-level control through deep reinforcement learning. Nature 2015;518(7540):529â€“33. [27] Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, Silver D, Wierstra D. Continuous control with deep reinforcement learning. 2015, arXiv preprint arXiv: 1509.02971. [28] Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy optimization algorithms. 2017, arXiv preprint arXiv:1707.06347. [29] Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D, Kavukcuoglu K. Asynchronous methods for deep reinforcement learning. In: International conference on machine learning. PMLR; 2016, p. 1928â€“37. [30] Kingma DP, Ba J. Adam: A method for stochastic optimization. 2014, arXiv preprint arXiv:1412.6980. [31] Jaderberg M, Dalibard V, Osindero S, Czarnecki WM, Donahue J, Razavi A, Vinyals O, Green T, Dunning I, Simonyan K, et al. Population based training of neural networks. 2017, arXiv preprint arXiv:1711.09846. [32] Martin H, Otterson S. German intraday electricity market analysis and modeling based on the limit order book. In: 2018 15th international conference on the European energy market (EEM). IEEE; 2018, p. 1â€“6. [33] Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks. In: Advances in neural information processing systems. 2014, p. 3104â€“12. [34] Alkhayat G, Mehmood R. A review and taxonomy of wind and solar energy forecasting methods based on deep learning. Energy AI 2021;100060. [35] Deutscher Wetterdienst. ICON database reference manual. 2021, https: //www.dwd.de/SharedDocs/downloads/DE/modelldokumentationen/nwv/ icon/icon_dbbeschr_aktuell.pdf (Accessed: 2021-09-29). [36] EPEX SPOT. EPEX SPOT annual market review 2018. 2021, https:// www.epexspot.com/en/news/traded-volumes-soar-all-time-high-2018 (Accessed: 2021-11-02). [37] EPEX SPOT. EPEX spot to publish separate prices and volumes for the Austrian and German day-ahead markets. 2021, https://www.epexspot.com/en/news/ epex-spot-publish-separate-prices-and-volumes-austrian-and-german-day-ahead- markets (Accessed: 2021-11-02).","libVersion":"0.3.2","langs":""}
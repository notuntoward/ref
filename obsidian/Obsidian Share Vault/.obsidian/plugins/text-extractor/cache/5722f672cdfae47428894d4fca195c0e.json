{"path":"lit/lit_notes_OLD_PARTIAL/Marco24tsFrcstCheatSheet.pdf","text":"Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco ACF plot The autocorrelation function (ACF) plot shows the autocorrelation coefficients as a function of the lag. ‚Ä¢ Use it to determine the order q of a stationary MA(q) process ‚Ä¢ A stationary MA(q) process has significant coefficients up until lag q Output for a MA(2) process (i.e., q = 2): Time series analysis PACF plot The partial autocorrelation function (PACF) plot shows the partial autocorrelation coefficients as a function of the lag. ‚Ä¢ Use it to determine the order p of a stationary AR(p) process ‚Ä¢ A stationary AR(p) process has significant coefficients up until lag p Output for an AR(2) process (i.e., p = 2): Time series decomposition Separate the series into 3 components: trend, seasonality, and residuals ‚Ä¢ Trend: long-term changes in the series ‚Ä¢ Seasonality: periodical variations in the series ‚Ä¢ Residuals: what is not explained by trend and seasonality Note: m is the frequency of data (i.e., how many observations per season) Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco ADF test ‚Äì Test for stationarity A series is stationary it its mean, variance, and autocorrelation are constant over time. Test for stationarity with augmented Dickey-Fuller (ADF) test. ‚Ä¢ Null hypothesis: a unit root is present (i.e., the series is not stationary) ‚Ä¢ We want a p-value < 0.05 Note: to make a series stationary, use differencing. ‚Ä¢ n = 1: difference between consecutive timesteps ‚Ä¢ n = 4: difference between values 4 timesteps apart Differencing removes n data points. Statistical tests Ljung-Box test ‚Äì Residuals analysis Used to determine if the autocorrelation of a group of data is significantly different from 0. Use it on the residuals to check if they are independent. ‚Ä¢ Null hypothesis: the data is independently distributed (i.e., there is no autocorrelation) ‚Ä¢ We want a p-value > 0.05 Note: print the p-values up to h lags, where h is the length of your forecast horizon Granger causality ‚Äì Multivariate forecasting Determine if one time series is useful in predicting the other one. Use to validate the VAR model. If Granger causality test fails, then the VAR model is invalid. ‚Ä¢ Null hypothesis: ùë¶2,ùë° does not Granger-causes ùë¶1,ùë° ‚Ä¢ Works for predictive causality ‚Ä¢ Tests causality in one direction only (i.e., must run the test twice) ‚Ä¢ We want a p-value < 0.05 Note: see how we run the test twice to test causality in both directions Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco Moving average model ‚Äì MA(q) The moving average model: the current value depends on the mean of the series, the current error term, and past error terms. ‚Ä¢ Denoted as MA(q) where q is the order ‚Ä¢ Use ACF plot to find q ‚Ä¢ Assumes stationarity. Use only on stationary data Equation ùë¶ùë° = ùúá + ùúñùë° + ùúÉ1ùúñùë°‚àí1 + ùúÉ2ùúñùë°‚àí2 + ‚ãØ + ùúÉùëûùúñùë°‚àíùëû Forecasting ‚Äì Statistical models Autoregressive model ‚Äì AR(p) The autoregressive model is a regression against itself. This means that the present value depends on past values. ‚Ä¢ Denoted as AR(p) where p is the order ‚Ä¢ Use PACF to find p ‚Ä¢ Assumes stationarity. Use only on stationary data Equation ùë¶ùë° = ùê∂ + ùúô1ùë¶ùë°‚àí1 + ùúô2ùë¶ùë°‚àí2 + ‚ãØ + ùúôùëùùë¶ùë°‚àíùëù + ùúñùë° ARMA(p,q) The autoregressive moving average model (ARMA) is the combination of the autoregressive model AR(p), and the moving average model MA(q). ‚Ä¢ Denoted as ARMA(p,q) where p is the order of the autoregressive portion, and q is the order of the moving average portion ‚Ä¢ Cannot use ACF or PACF to find the order p, and q. Must try different (p,q) value and select the model with the lowest AIC (Akaike‚Äôs Information Criterion) ‚Ä¢ Assumes stationarity. Use only on stationary data. Equation ùë¶ùë° = ùê∂ + ùúô1ùë¶ùë°‚àí1 + ‚ãØ ùúôùëùùë¶ùë°‚àíùëù + ùúÉ1ùúñùë°‚àí1 + ‚ãØ ùúÉùëûùúñùë°‚àíùëû + ùúñùë° Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco ARIMA(p,d,q) The autoregressive integrated moving average (ARIMA) model is the combination of the autoregressive model AR(p), and the moving average model MA(q), but in terms of the differenced series. ‚Ä¢ Denoted as ARMA(p,d,q), where p is the order of the autoregressive portion, d is the order of integration, and q is the order of the moving average portion ‚Ä¢ Can use on non-stationary data Equation ùë¶‚Ä≤ùë° = ùê∂ + ùúô1ùë¶‚Ä≤ùë°‚àí1 + ‚ãØ ùúôùëùùë¶‚Ä≤ùë°‚àíùëù + ùúÉ1ùúñùë°‚àí1 + ‚ãØ ùúÉùëûùúñùë°‚àíùëû + ùúñùë° Note: the order of integration d is simply the number of time a series was differenced to become stationary. Forecasting ‚Äì Statistical models SARIMA(p,d,q)(P,D,Q)m The seasonal autoregressive integrated moving average (SARIMA) model includes a seasonal component on top of the ARIMA model. ‚Ä¢ Denoted as SARIMA(p,d,q)(P,D,Q)m. Here, p, d, and q have the same meaning as in the ARIMA model. ‚Ä¢ P is the seasonal order of the autoregressive portion ‚Ä¢ D is the seasonal order of integration ‚Ä¢ Q is the seasonal order of the moving average portion ‚Ä¢ m is the frequency of the data (i.e., the number of data points in one season) SARIMAX SARIMAX is the most general model. It combines seasonality, a moving average portion, an autoregressive portion, and exogenous variables. ‚Ä¢ Can use external variables to forecast a series Caveat: SARIMAX predicts the next timestep. If your horizon is longer than one timestep, then you must forecast your exogenous variables too, which can amplify the error in your model Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco VARMAX The vector autoregressive moving average with exogenous variables (VARMAX) model is used for multivariate forecasting (i.e., predicting two time series at the same time) ‚Ä¢ Assumes Granger-causality. Must use the Granger-causality test. If the test fails, the VARMAX model cannot be used. Forecasting ‚Äì Statistical models BATS and TBATS BATS and TBATS are used when the series has more than one seasonal period. This can happen when we have high frequency data, such as daily data. ‚Ä¢ When there is more than one seasonal period, SARIMA cannot be used. Use BATS or TBATS. ‚Ä¢ BATS: Box-Cox transformation, ARMA errors, Trend and Seasonal components ‚Ä¢ TBATS: Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and Seasonal components. Exponential smoothing Exponential smoothing uses past values to predict the future, but the weights decay exponentially as the values go further back in time. ‚Ä¢ Simple exponential smoothing: returns flat forecasts ‚Ä¢ Double exponential smoothing: adds a trend component. Forecasts are a straight line (increasing or decreasing) ‚Ä¢ Triple exponential smoothing: adds a seasonal component ‚Ä¢ Trend can be ‚Äúadditive‚Äù or ‚Äúexponential‚Äù ‚Ä¢ Seasonality can be ‚Äúadditive‚Äù or ‚Äúmultiplicative‚Äù Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco Deep neural network (DNN) A deep neural network stacks fully connected layers and can model non-linear relationship in the time series if the activation function is non-linear. ‚Ä¢ Start with a simple model with few hidden layers. Experiment training for more epochs before adding layers Forecasting ‚Äì Deep learning models Long short-term memory - LSTM An LSTM is great at processing sequences of data, such as text and time series. Its architecture allows for past information to still be used for later predictions ‚Ä¢ You can stack many LSTM layers in your model ‚Ä¢ You can try combining an LSTM with a CNN ‚Ä¢ An LSTM is longer to train since the data is processed in sequence Convolutional neural network - CNN A CNN can act as a filter for our time series, due to the convolution operation which reduces the feature space. ‚Ä¢ A CNN trains faster than an LSTM ‚Ä¢ Can be combined with an LSTM. Place the CNN layer before the LSTM Time Series Forecasting with Python ‚Äì Cheat Sheet Data Science with Marco Autoregressive deep learning model An autoregressive deep learning model feeds its predictions back into the model to make further predictions. That way, we generate a sequence of predictions, one forecast at a time. ‚Ä¢ Can be used with any architecture, whether it‚Äôs a DNN, LSTM or CNN ‚Ä¢ If early predictions are bad, the errors will magnify as more predictions are made. Forecasting ‚Äì Deep learning models Autoregressive deep learning model (cont.)","libVersion":"0.3.2","langs":""}
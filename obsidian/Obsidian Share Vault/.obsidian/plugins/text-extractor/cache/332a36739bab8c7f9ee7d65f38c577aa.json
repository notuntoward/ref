{"path":"lit/lit_notes_OLD_PARTIAL/Bocharov13tseriesClustGeom.pdf","text":"Aalborg Universitet Segmentation of Nonstationary Time Series with Geometric Clustering Bocharov, Alexei; Thiesson, Bo Published in: Pattern Recognition - Applications and Methods DOI (link to publication from Publisher): 10.1007/978-3-642-36530-0_8 Publication date: 2013 Document Version Publisher's PDF, also known as Version of record Link to publication from Aalborg University Citation for published version (APA): Bocharov, A., & Thiesson, B. (2013). Segmentation of Nonstationary Time Series with Geometric Clustering. In P. L. Carmona, J. S. Sanchez, & A. L. N. Fred (Eds.), Pattern Recognition - Applications and Methods (Vol. 204, pp. 93-107). Springer Publishing Company. Advances in Intelligent Systems and Computing https://doi.org/10.1007/978-3-642-36530-0_8 General rights Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. - Users may download and print one copy of any publication from the public portal for the purpose of private study or research. - You may not further distribute the material or use it for any profit-making activity or commercial gain - You may freely distribute the URL identifying the publication in the public portal - Take down policy If you believe that this document breaches copyright please contact us at vbn@aub.aau.dk providing details, and we will remove access to the work immediately and investigate your claim. Downloaded from vbn.aau.dk on: August 07, 2023 Segmentation of Nonstationary Time Series with Geometric Clustering Alexei Bocharov and Bo Thiesson Microsoft Research, One Microsoft Way, Redmond, WA 98052, U.S.A. {alexeib,thiesson}@microsoft.com Abstract. We introduce a non-parametric method for segmentation in regime- switching time-series models. The approach is based on spectral clustering of target-regressor tuples and derives a switching regression tree, where regime switches are modeled by oblique splits. Such models can be learned efﬁciently from data, where clustering is used to propose one single split candidate at each split level. We use the class of ART time series models to serve as illustration, but because of the non-parametric nature of our segmentation approach, it readily generalizes to a wide range of time-series models that go beyond the Gaussian error assumption in ART models. Experimental results on S&P 1500 ﬁnancial trading data demonstrates dramatically improved predictive accuracy for the ex- emplifying ART models. Keywords: Regime-switching time series, Spectral clustering, Regression tree, Oblique split, Financial markets. 1 Introduction The analysis of time-series data is an important area of research with applications in areas such as natural sciences, economics, and ﬁnance to mention a few. Many time series exhibit nonstationarity due to regime switching. Proper detection and modeling of this switching is a major challenge in time-series analysis. In regime- switching models, different time series regimes are described by submodels with differ- ent sets of parameters. A particular submodel may apply to multiple time ranges when the underlying time series repeatedly falls into a certain regime. For example, volatil- ity of equity returns may change when affected by events such as earnings releases or analysts’ reports, and we may see similar volatility patterns around similar events. The intuition in this paper is to match proposed regimes with modes of the joint distribution of target-regressor tuples, which is a particular kind of mixture modeling. Prior research offers quite a variety of mixture modeling approaches to the analysis of nonstationary time series. In Markov-switching models (see, e.g., [12,13]) a Markov evolving hidden state indirectly partitions the time-series data to ﬁt local auto-regressive models in the mixture components. Another large body of work (see, e.g., [27,28]) have adapted the hierarchical mixtures of experts in [15] to time series. In these models–also denoted as gated experts–the hierarchical gates explicitly operate on the data in order to deﬁne a partition into local regimes. In both the Markov-switching and the gated expert P.L. Carmona et al. (Eds.): Pattern Recognition – Applications and Methods, AISC 204, pp. 93–107. DOI: 10.1007/978-3-642-36530-0_8 c⃝ Springer-Verlag Berlin Heidelberg 2013 94 A. Bocharov and B. Thiesson models, the determination of the partition and the local regimes are tightly integrated in the learning algorithm and demands an iterative approach, such as the EM algorithm. We focus on a conceptually simple direction that lends itself easier to explanatory analysis. The resulting design differs from the above work in at least three aspects: 1) we propose a modular separation of the regime partitioning and the regime learning, which makes it easy to experiment independently with different types of regime models and different separation methods, 2) in particular, this modularity allows for non-parametric as well as parametric regime models, or a mixture thereof, 3) the regime-switching conditions depend deterministically on data and are easy to interpret. We model the actual switching conditions in a regime-switching model in the form of a regression tree and call it the switching tree. Typically, the construction of a re- gression tree is a stagewise process that involves three ingredients: 1) a split proposer that creates split candidates to consider for a given (leaf) node in the tree, 2) one or more scoring criteria for evaluating the beneﬁt of a split candidate, and 3) a search strategy that decides which nodes to consider and which scoring criterion to apply at any state during the construction of the tree. Since the seminal paper [2] popularized the classic classiﬁcation and regression tree (CART) algorithm, the research community has given a lot of attention to both types of decision trees. Many different algorithms have been proposed in the literature by varying speciﬁcs for the three ingredients in the construction process mentioned above. Although there has been much research on learning regression trees, we know of only one setting, where these models have been used as switching trees in regime-switching time series models–namely the class of auto-regressive tree (ART) models in [18]. The ART models generalize classical auto-regressive (AR) models (e.g., [11]) by having a regression tree deﬁne the switching between the different AR models in the leafs. As such, the well-known threshold auto-regressive (TAR) models [25,24] can also be considered as a specialization of an ART model with the regression tree limited to a single split variable. The layout of our algorithms is strongly inﬂuenced by [18] (which we repeatedly refer to for comparison), but our premises and approach is very different. In particular, we propose a different way to create the candidate splits during the switching tree construction. A split deﬁnes a predicate, which, given the values of re- gressor variables, decides on which side of the split a data case should belong.1 A predicate may be as simple as checking if the value of a particular single regressor is below some threshold or not. We will refer to this kind of split as an axial split,and it is in fact the only type of splits allowed in the ART models. We make use of general multi-regressor split predicates, which in this paper we approximate with linear predi- cates called oblique splits. Importantly, we show evidence that for a broad class of time series, the best split is not likely to be axial. It may sometimes be possible to consider and evaluate the efﬁcacy of all feasible axial splits for the data associated with a node in the tree, but for combinatorial reasons, oblique splitting rarely enjoys this luxury. We therefore need a split proposer, which is more careful about the candidate splits it proposes. In fact, our approach is extreme in that respect by only proposing a single oblique split to be considered for any given node 1 For clarity of presentation, we will focus on binary splits only. It is a trivial exercise to extend our proposed method to allow for n-ary splits. Segmentation of Nonstationary Time Series with Geometric Clustering 95 during the construction of the tree. Our oblique split proposer involves a simple two step procedure. In the ﬁrst step, we use a spectral clustering method to separate the data in a node into two classes. Having separated the data, the second step now proceeds as a simple classiﬁcation problem, by using a linear discriminant method to create the best separating hyperplane for the two data classes. Any discriminant method can be used, and there is in principle no restriction on it being linear, if more complicated splits are sought. Oblique splitting has enjoyed signiﬁcant attention for the classiﬁcation tree setting. See, e.g., [2,20,3,10,14]. Less attention has been given to the regression tree setting, but still a number of methods has come out of the statistics and machine learning com- munities. See, e.g., [7,16,4] to mention a few. Setting aside the time-series context for our switching trees, the work in [7] is in style the most similar to the oblique splitting approach that we propose in this paper. In [7], the EM algorithm for Gaussian mixtures is used to cluster the data. Having committed to Gaussian clusters it now makes sense to determine a separating hyperplane via a quadratic discriminant analysis for a projection of the data onto a vector that ensures maximum separation of the Gaussians. This vector is found by minimizing Fisher’s separability criterion. Our approach to proposing oblique split candidates is agnostic to any speciﬁc para- metric assumptions on the noise distribution and therefore accommodates without change non-Gaussian or even correlated errors (thus our method is more general than ART, which relies on univariate Gaussian quantiles as split candidates). This approach allows us to use spectral clustering - a non-parametric segmentation tool, which has been shown to often outperform parametric clustering tools (see, e.g., [26]). Spectral clustering dates back to the work in [8,9] that suggest to use the method for graph partitionings. Variations of spectral clustering have later been popularized in the machine learning community [23,19,21], and, importantly, very good progress has been made in improving an otherwise computationally expensive eigenvector computation for these methods [29]. We use a simple variation of the method in [21] to create a spectral clustering for the time series data in a node. Given this clustering, we then use a simple perceptron learning algorithm (see, e.g., [1]) to ﬁnd a hyperplane that deﬁnes a good oblique split predicate for the autoregressors in the model. Let us now turn to the possibility of splitting on the time feature in a time series. Due to the special nature of time, it does not make sense to involve this feature as an extra dimension in the spectral clustering; it would not add any discriminating power to the method. Instead, we propose a procedure for time splits, which uses the cluster- ing in another way. The procedure identiﬁes speciﬁc points in time, where succeeding data elements in the series cross the cluster boundary, and proposes time splits at those points. Our split proposer will in this way use the spectral clustering to produce both the oblique split candidate for the regressors, and a few very targeted (axial) split candidates for the time dimension. The rest of the paper is organized as follows. In Section 2, we brieﬂy review the ART models that we use as a baseline, and we deﬁne and motivate the extension that allows for oblique splits. Section 3 reviews the general learning framework for ART models. Section 4 contains the details for both aspects of our proposed spectral splitting method– the oblique splitting and the time splitting. In Sections 5 and 6 we describe experiments 96 A. Bocharov and B. Thiesson and provide experimental evidence demonstrating that our proposed spectral splitting method dramatically improves the quality of the learned ART models over the current approach. We will conclude in Section 7. 2 Standard and Oblique ART Models We begin by introducing some notation. We denote a temporal sequence of variables by X =(X1,X2,...,XT ), and we denote a sub-sequence consisting of the i’th through the j’th element by X j i =(Xi,Xi+1,...,Xj), i< j. Time-series data is a sequence of values for these variables denoted by x =(x1,x2,... ,xT ). We assume continuous values, obtained at discrete, equispaced intervals of time. An autoregressive (AR) model of length p, is simply a p-order Markov model that imposes a linear regression for the current value of the time series given the immediate past of p previous values. That is, p(xt|x t−1 1 )= p(xt|x t−1 t−p) ∼N (m + p∑ j=1 bjxt−j ,σ2) where N (μ, σ2) is a conditional normal distribution with mean μ and variance σ2,and θ =(m, b1,... ,bp,σ2) are the model parameters (e.g., [6, page 55]). The ART models is a regime-switching generalization of the AR models, where a switching regression tree determines which AR model to apply at each time step. The autoregressors therefore have two purposes: as input for a classiﬁcation that determines a particular regime, and as predictor variables in the linear regression for the speciﬁc AR model in that regime. As a second generalization2, ART models may allow exogenous variables, such as past observations from related time series, as regressors in the model. Time (or time- step) is a special exogenous variable, only allowed in a split condition, and is therefore only used for modeling change points in the series. 2.1 Axial and Oblique Splits Different types of switching regression trees can be characterized by the kind of pred- icates they allow for splits in the tree. The ART models allow only a simple form of binary splits, where a predicate tests the value of a single regressor. The models handle continuous variables, and a split predicate is therefore of the form Xi ≤ c where c is a constant value and Xi is any one of the regressors in the model or a variable representing time. A simple split of this type is also called axial, because the predicate that splits the data at a node can be considered as a hyperplane that is orthogonal to the axis for one of the regressor variables or the time variable. 2 The class of ART models with exogenous variables has not been documented in any paper. We have learned about this generalization from communications with the authors of [18]. Segmentation of Nonstationary Time Series with Geometric Clustering 97 The best split for a node in the tree can be learned by considering all possible par- titionings of the data according to each of the individual regressors in the model, and then picking the highest scoring split for these candidates according to some criterion. It can, however, be computationally demanding to evaluate scores for that many split can- didates, and for that reason, [5] investigated a Gaussian quantile approach that proposes only 15 split points for each regressor. They found that this approach is competitive to the more exhaustive approach. A commercial implementation for ART models uses the Gaussian quantile approach and we will compare our alternative to this approach. We propose a solution, which will only produce a single split candidate to be consid- ered for the entire set of regressors. In this solution we extend the class of ART models to allow for a more general split predicate of the form ∑ i aiXi ≤ c (1) where the sum is over all the regressors in the model and ai are corresponding coefﬁ- cients. Splits of this type are in [20] called oblique due to the fact that a hyperplane that splits data according to the linear predicate is oblique with respect to the regressor axes. We will in Section 4 describe the details behind the method that we use to produce an oblique split candidate. 2.2 Motivation for Oblique Splits There are general statistical reasons why, in many situations, oblique splits are prefer- able over axial splits. In fact, for a broad class of time series, the best splitting hyper- plane turns out to be approximately orthogonal to the principal diagonal d =( 1√ p ,..., 1√ p ). To qualify this fact, consider two pre-deﬁned classes of segments x (c),c =1, 2 for the time-series data x.Let μ(c) and Σ(c) denote the mean vector and covariance matrix for the sample joint distribution of X t−1 t−p , computed for observations on p regressors for targets xt ∈ x (c). Let us deﬁne the moving average At = 1 p ∑p i=1 Xt−i. We show in the Appendix that in the context where Xt−i − At is weakly correlated with At, while its variance is comparable with that of At, the angle between the principal diagonal and one of the principal axes of Σ(c),c =1, 2 is small. This would certainly be the case with a broad range of ﬁnancial data, where increments in price curves have notoriously low correla- tions with price values [22,17], while seldom overwhelming the averages in magnitude. With one of the principal axes being approximately aligned with the principal diagonal d for both Σ(1) and Σ(2) it is unlikely that a cut orthogonal to either of the coordinate axes Xt−1,... ,Xt−p can provide optimal separation of the two classes. 3 The Learning Procedure An ART model is typically learned in a stagewise fashion. The learning process starts from the trivial model without any regressors and then greedily evaluates regressors one at a time and adds the ones that improve a chosen scoring criterion to model, while scoring criterion keeps improving. 98 A. Bocharov and B. Thiesson The task of learning a speciﬁc autoregressive model considered at any stage in this process can be cast into a standard task of learning a linear regression tree. It is done by a trivial transformation of the time-series data into multivariate data cases for the regressor and target variables in the model. For example, when learning an ART model of length p with an exogenous regressor, say zt−q, from a related time series, the transformation creates the set of T − max(p, q) cases of the type (x t t−p,zt−q),where max(p, q)+ 1 <t ≤ T . We will in the following denote this transformation as the phase view, due to a vague analogy to the phase trajectory in the theory of dynamical systems. Most regression tree learning algorithms construct a tree in two stages (see, e.g., [2]): First, in a growing stage, the learning algorithm will maximize a scoring criterion by recursively trying to replace leaf nodes by better scoring splits. A least-squares devia- tion criterion is often used for scoring splits in a regression tree. Typically the chosen criterion will cause the selection of an overly large tree with poor generalization. In a pruning stage, the tree is therefore pruned back by greedily eliminating leaves using a second criterion–such as the holdout score on a validation data set–with the goal of minimizing the error on unseen data. In contrast, [18] suggests a learning algorithm that uses a Bayesian scoring criterion, described in detail in that paper. This criterion avoids over-ﬁtting by penalizing for the complexity of the model, and consequently, the pruning stage is not needed. We use this Bayesian criterion in our experimental section. In the next section, we describe the details of the algorithm we propose for producing the candidate splits that are considered during the recursive construction of a regression tree. Going from axial to oblique splits adds complexity to the proposal of candidate splits. However, our split proposer dramatically reduces the number of proposed split candidates for the nodes evaluated during the construction of the tree, and by virtue of that fact spends much less time evaluating scores of the candidates. 4 Spectral Splitting This section will start with a brief description of spectral clustering, followed by details about how we apply this method to produce candidate splits for an ART time-series model. A good tutorial treatment and an extensive list of references for spectral cluster- ing can be found in [26]. The spectral splitting method that we propose constructs two types of split candidates– oblique and time–both relying on spectral clustering. Based on this clustering, the method applies two different views on the data–phase and trace–according to the type of splits we want to identify. The algorithm will only propose a single oblique split candidate and possibly a few time split candidates for any node evaluated during the construction of the regression tree. 4.1 Spectral Clustering Given a set of n multi-dimensional data points (x1,...,xn),we let aij = a(xi,xj) denote the afﬁnity between the i’th and j’th data point, according to some symmet- ric and non-negative measure. The corresponding afﬁnity matrix is denoted byA = Segmentation of Nonstationary Time Series with Geometric Clustering 99 (aij )i,j=1,...,n,and we let D denote the diagonal matrix with values ∑n j=1 aij, i = 1,... ,n on the diagonal. Spectral clustering is a non-parametric clustering method that uses the pairwise prox- imity between data points as a basis of the criterion that the clustering must optimize. The trick in spectral clustering is to enhance the cluster properties in the data by chang- ing the representation of the multi-dimensional data into a (possibly one-dimensional) representation based on eigenvalues for the so-called Laplacian. L = D − A Two different normalizations for the Laplacian have been proposed in [23] and [21], leading to two slightly different spectral clustering algorithms. We will follow a sim- pliﬁed version of the latter. Let I denote the identity matrix. We will cluster the data according to the second smallest eigenvector–the so-called Fiedler vector [9]–of the normalized Laplacian Lnorm = D−1/2LD−1/2 = I − D−1/2AD−1/2 The algorithm is illustrated in Figure 1. Notice that we replace Lnorm with L′ norm = I − Lnorm which changes eigenvalues from λi to 1 − λi and leaves eigenvectors unchanged. We therefore ﬁnd the eigenvector for the second-largest and not the second-smallest eigen- vector. We prefer this interpretation of the algorithm for reasons that become clear when we discuss iterative methods for ﬁnding eigenvalues in Section 4.2. 1. Construct the matrix L ′ norm. 2. Find the second-largest eigenvector e =(e1,.. .,en) of L ′ norm. 3. Cluster the elements in the eigenvector (e.g. by the largest gap in values). 4. Assign the original data point xi to the cluster assigned to ei. Fig. 1. Simple normalized spectral clustering algorithm Readers familiar with the original algorithm in [21] may notice the following sim- pliﬁcations: First, we only consider a binary clustering problem, and second, we only use the two largest eigenvectors for the clustering, and not the k largest eigenvectors in their algorithm. (The elements in the ﬁrst eigenvector always have the same value and will therefore not contribute to the clustering.) Due to the second simpliﬁcation, the step in their algorithm that normalizes rows of stacked eigenvectors can be avoided, be- cause the constant nature of the ﬁrst eigenvector leaves the transformation of the second eigenvector monotone. 4.2 Oblique Splits Oblique splits are based on a particular view of the time series data that we call the phase view, as deﬁned in Section 3. Importantly, a data case in the phase view involves 100 A. Bocharov and B. Thiesson Fig. 2. Oblique split candidate for ART model with two autoregressors. (a) The original time series. (b) The spectral clustering of phase-view data. The polygon separating the upper and lower parts is a segment of a separating hyperplane for the spectral clusters (c) The phase view projection to regressor plane and the separating hyperplane learned by the perceptron algorithm. (d) The effect of the oblique split on the original time series: a regime consisting of the slightly less upward trending and more volatile ﬁrst and third data segments is separated from the regime with more upward trending and less volatile second and fourth segments. values for both the target and regressors, which imply that our oblique split proposals may capture regression structures that show up in the data–as opposed to many standard methods for axial splits that are ignorant to the target when determining split candidates for the regressors. It should also be noted that because the phase view has no notion of time, similar patterns from entirely different segments of time may end up on the same side of an oblique split. This property can at times result in a great advantage over splitting the time series into chronological segments. First of all, splitting on time imposes a severe constraint on predictions, because splits in time restrict the prediction model to infor- mation from the segment latest in time. Information from similar segments earlier in the time series are not integrated into the prediction model in this case. Second, we may need multiple time splits to mimic the segments of one oblique split, which may not be obtainable due to the degradation of the statistical power from the smaller segments of data. Figure 2(d) shows an example, where a single oblique split separates the regime with the less upward trending and slightly more volatile ﬁrst and third data segments of the time series from the regime consisting of the less volatile and more upward trend- ing second and fourth segments. In contrast, we would have needed three time splits to properly divide the segments and these splits would therefore have resulted in four different regimes. Our split proposer produces a single oblique split candidate in a two step procedure. In the ﬁrst step, we strive to separate two modes that relates the target and regressors for the model in the best possible way. To accomplish this task, we apply the afﬁnity based Segmentation of Nonstationary Time Series with Geometric Clustering 101 spectral clustering algorithm, described in Section 4.1, to the phase view of the time series data. For the experiments reported later in this paper, we use an afﬁnity measure proportional to 1 1+ ||p1 − p2||2 where ||p1 − p2||2 is the L2-norm between two phases. We do not consider exogenous regressors from related time series in these experiments. All variables in the phase view are therefore on the same scale, making the inverse distance a good measure of prox- imity. With exogenous regressors, more care should be taken with respect to the scaling of variables in the proximity measure, or the time series should be standardized. Fig- ure 2(b) demonstrates the spectral clustering for the phase view of the time-series data in Figure 2(a), where this phase view has been constructed for an ART model with two autoregressors. The oblique split predicate in (1) deﬁnes an inequality that only involves the regres- sors in the model. The second step of the oblique split proposer therefore projects the clustering of the phase view data to the space of the regressors, where the hyperplane separating the clusters is now constructed. While this can be done with a variety of linear discrimination methods, we decided to use a simple single-layer perceptron op- timizing the total misclassiﬁcation count. Such perceptron will be relatively insensitive to outliers, compared to, for example, Fisher’s linear discriminant. The computational complexity of an oblique split proposal is dominated by the cost of computing the full afﬁnity matrix, the second largest eigenvector for the normalized Laplacian, and ﬁnding the separating hyperplane for the spectral clusters. Recall that n denotes the number of cases in the phase view of the data. The cost of computing the full afﬁnity matrix is therefore O(n2) afﬁnity computations. Direct methods for computing the second largest eigenvector is O(n3). A complexity of O(n3) may be prohibitive for series of substantial length. Fortunately, there are approximate iterative methods, which in practice are much faster with tolerant error. For example, the Implicitly Restarted Lanczos Method (IRLM) has complexity O(mh + nh),where m is the number of non- zero afﬁnities in the afﬁnity matrix and h is the number of iterations required until convergence [29]. With a full afﬁnity matrix m = n2, but a signiﬁcant speedup can be accomplished by only recording afﬁnities above a certain threshold in the afﬁnity matrix. Finally, the perceptron algorithm has complexity O(nh). 4.3 Time Splits A simple but computationally expensive way of determining a good time split is to let the split proposer nominate all possible splits in time for the further evaluation. The commercial implementation of the ART models relies on an approximation to this approach that proposes a smaller set of equispaced points on the time axis. We suggest a data driven approximation, which will more precisely target the change points in the time series. Our approach is based on another view of the time series data that we call the trace view. In the trace view we use the additional time information to label the phase view data in the spectral clustering. The trace view, now traces the clustered data through time and proposes a split point each time the trace jumps across clusters. The rationale behind our approach is that data in the same cluster will behave 102 A. Bocharov and B. Thiesson in a similar way, and we can therefore signiﬁcantly reduce the number of time-split pro- posals by only proposing the cluster jumps. As an example, the thin lines orthogonal to the time axis in Figure 2(d) shows the few time splits proposed by our approach. Get- ting close to a good approximation for the equispaced approach would have demanded far more proposed split points. Turning now to the computational complexity. Assuming that spectral clustering has already been performed for the oblique split proposal, the additional overhead for the trace through data is O(n). 5 Evaluation In this section, we provide an empirical evaluation for our spectral splitting methods. We use a large collection of ﬁnancial trading data. The collection contains the daily closing prices for 1495 stocks from Standard & Poor’s 1500 index3 as of January 1, 2008. Each time series spans across approximately 150 trading days ending on February 1, 2008. (Rotation of stocks in the S&P 1500 lead to the exclusion of 5 stocks with insufﬁent data.) The historic price data is available from Yahoo!, and can be downloaded with queries of format http://ﬁnance.yahoo.com/q/hp?s=SYMBOL, where SYMBOL is the symbol for the stock in the index. We divide each data set into a training set, used as input to the learning method, and a holdout set, used to evaluate the models. We use the last ﬁve observations as the holdout set, knowing that the data are daily with trading weeks of ﬁve days. In our experiments, we learn ART models with an arbitrary number of autoregressors and we allow time as an exogenous split variable. We do not complicate the experiments with the use of exogenous regressors from related time series, as this complication is irrelevant to the objective for this paper. For all the models that we learn, we use the same Bayesian scoring criterion, the same greedy search strategy for ﬁnding the number of autoregressors, and the same method for constructing a regression tree – except that different alternative split candidates are considered for the different splitting algorithms that we consider. We evaluate two different types of splitting with respect to the autoregressors in the model: AxialGaussian and ObliqueSpectral. The AxialGaussian method is the standard method used to propose multiple axial candidates for each split in an ART model, as described in Section 2.1. The ObliqueSpectral method is our proposed method, which for a split considers only a single oblique candidate involving all regressors. In combi- nation with the two split proposer methods for autoregressors, we also evaluate three types of time splitting: NoSplit, Fixed,and TimeSpectral. The NoSplit method does not allow any time splits. The Fixed method is the simple standard method for learning splits on time in an ART model, as described in Section 4.3. The TimeSpectral method is our spectral clustering-based alternative. In order to provide context for the numbers in the evaluation of these methods, we will also evaluate a very weak baseline, namely the method not allowing any splits. We call this method the Baseline method. We evaluate the quality of a learned model by computing the sequential predictive score for the holdout data set corresponding to the training data from which the model 3 standardandpoors.com Segmentation of Nonstationary Time Series with Geometric Clustering 103 was learned. The sequential predictive score for a model is simply the average log- likelihood obtained by a one-step forecast for each of the observations in the holdout set. To evaluate the quality of a learning method, we compute the average of the sequential predictive scores obtained for each of the time series in the collection. Note that the use of the log-likelihood to measure performance simultaneously evaluates both the accuracy of the estimate and the accuracy of the uncertainty of the estimate. Finally, we use a (one-sided) sign test to evaluate if one method is signiﬁcantly better than another. To form the sign test, we count the number of times one method improves the predictive score over the other for each individual time series in the collection. Excluding ties, we seek to reject the hypothesis of equality, where the test statistic for the sign test follows a binomial distribution with probability parameter 0.5. 6Results To make sure that the results reported here are not an artifact of sub-optimal axial split- ting for the AxialGaussian method, we ﬁrst veriﬁed the claim from [5] that the Gaussian quantiles is a sufﬁcient substitute for the exhaustive set of possible axial splits. We com- pared the sequential predictive scores on 10% of the time series in our collection and did not ﬁnd a signiﬁcant difference. Table 1 shows the average sequential predictive scores across the series in our collec- tion for each combination of autoregressor and time-split proposer methods. First of all, for splits on autoregressors, we see a large improvement in score with our ObliqueSpec- tral method over the standard AxialGaussian method. Even with the weak baseline– namely the method not allowing any splits–the relative improvement from AxialGaus- sian to ObliqueSpectral over the improvement from the baseline to AxialGaussian is still above 20%, which is quite impressive. The fractions in Table 2 report the number of times one method has higher score than another method for all the time series in our collection. Notice that the numbers in a frac- tion do not necessarily sum to 1495, because we are not counting ties. We particularly Table 1. Average sequential predictive scores for each combination of autoregressor and time split proposer methods Regressor splits Time splits Ave. score Baseline Baseline -3.07 AxialGaussian NoSplit -1.73 AxialGaussian Fixed -1.72 AxialGaussian TimeSpectral -1.74 ObliqueSpectral NoSplit -1.45 ObliqueSpectral Fixed -1.46 ObliqueSpectral TimeSpectral -1.44 104 A. Bocharov and B. Thiesson Table 2. Pairwise comparisons of sequential predictive scores. The fractions show the number of time series, where one method has higher score than the other. The column labels denote the autoregressor split proposers being compared. Baseline / Baseline / AxialGaussian / AxialGaussian ObliqueSpectral ObliqueSpectral NoSplit 118 / 959 74 / 1168 462 / 615 Fixed 114 / 990 79 / 1182 226 / 418 SpectralTime 122 / 955 71 / 1171 473 / 604 notice that the ObliqueSpectral method is signiﬁcantly better than the standard Axial- Gaussian method for all three combinations with time-split proposer methods. In fact, the sign test rejects the hypothesis of equality at a signiﬁcance level < 10−5 in all cases. Combining the results from Tables 1 and 2, we can conclude that the large im- provement in the sequential predictive scores for our ObliqueSpectral method over the standard AxialGaussian method is due to a general trend in scores across individual time series, and not just a few outliers. We now turn to the surprising observation that adding time-split proposals to either of the AxialGaussian and the ObliqueSpectral autoregressor proposals does not im- prove the quality over models learned without time splits–neither for the Fixed nor the TimeSpectral method. Apparently, the axial and oblique splitting on autoregressors are ﬂexible enough to cover the time splits in our analysis. We do not necessarily expect this ﬁnding to generalize beyond series that behave like stock data, due to the fact that it is a relatively easy exercise to construct an artiﬁcial example that will challenge this ﬁnding. Finally, the oblique splits proposed by our method involve all regressors in a model, and therefore rely on our spectral splitting method to be smart enough to ignore noise that might be introduced by irrelevant regressors. Although efﬁcient, such parsimonious split proposal may appear overly restrictive compared to the possibility of proposing split candidates for all possible subsets of regressors. However, an additional set of experiments have shown that the exhaustive approach in general only leads to insignif- icant improvements in predictive scores. We conjecture that the stagewise inclusion of regressors in the overall learning procedure for an ART model (see Section 3) is a main reason for irrelevant regressors to not pose much of a problem for our approach. 7 Conclusions and Future Work We have presented a method for building regime-switching trees for nonstationary time series. The method is based on geometric clustering. More speciﬁcally, spectral clus- tering has been used in this paper. As such, our method does not rely on any parametric assumptions with regards to the distributions that best describe individual regimes. The clustering-based split proposer is used to propose a single oblique split candidate at each node level in the switching tree, which makes the method computationally efﬁcient. Segmentation of Nonstationary Time Series with Geometric Clustering 105 In the evaluation part of the paper we limited ourselves to an extension of ART mod- els that are built under the assumption of uncorrelated Gaussian error. The joint target- regressor distribution for a regime-switching time series can be modeled as a mixture of Gaussians in this case, and we were able to motivate and then prove empirically that oblique splits are better at learning the mixtures than combinations of axial splits. In fact, the experimental evidence we have collected shows that our approach when used to extend the ART models, dramatically improves predictive accuracy over the current approach. We still experimented under the assumption of Gaussianity. An important future experiment should allow non-Gaussian models in the oblique switching trees. The focus in this paper has been on learning regime-switching time-series mod- els that will easily lend themselves to explanatory analysis and interpretation. In fu- ture experiments we also plan to evaluate the potential tradeoff in modularity, inter- pretability, and computational efﬁciency with forecast precision for our simple learning approach compared to more complicated approaches that integrates learning of soft regime switching and the local regimes in the models, such as the learning of Markov- switching (e.g., [12,13]) and gated experts (e.g., [27,28]) models. References 1. Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford University Press, Oxford (1995) 2. Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.: Classiﬁcation and Regression Trees. Wadsworth International Group, Belmont, California (1984) 3. Brodley, C.E., Utgoff, P.E.: Multivariate decision trees. Machine Learning 19(1), 45–77 (1995) 4. Chaudhuri, P., Huang, M., Loh, W.Y., Yao, R.: Piecewise polynomial regression trees. Sta- tistica Sinica 4, 143–167 (1994) 5. Chickering, D., Meek, C., Rounthwaite, R.: Efﬁcient determination of dynamic split points in a decision tree. In: Proc. of the 2001 IEEE International Conference on Data Mining, pp. 91–98. IEEE Computer Society (November 2001) 6. DeGroot, M.: Optimal Statistical Decisions. McGraw-Hill, New York (1970) 7. Dobra, A., Gehrke, J.: Secret: A scalable linear regression tree algorithm. In: Proc. of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 481–487. ACM Press (2002) 8. Donath, W.E., Hoffman, A.J.: Lower bounds for the partitioning of graphs. IBM Journal of Research and Development 17, 420–425 (1973) 9. Fiedler, M.: Algebraic connectivity of graphs. Czechoslovak Mathematical Journal 23, 298– 305 (1973) 10. Gama, J.: Oblique linear tree. In: Proc. of the Second International Symposium on Intelligent Data Analysis, pp. 187–198 (1997) 11. Hamilton, J.D.: Time Series Analysis. Princeton University Press, Princeton (1994) 12. Hamilton, J.D.: A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica 57(2), 357–384 (1989) 13. Hamilton, J.D.: Analysis of time series subject to changes in regime. Journal of Economet- rics 45, 39–70 (1990) 14. Iyengar, V.S.: Hot: Heuristics for oblique trees. In: Proc. of the 11th IEEE International Conference on Tools with Artiﬁcial Intelligence, pp. 91–98. IEEE Computer Society, Wash- ington, DC (1999) 106 A. Bocharov and B. Thiesson 15. Jordan, M.I., Jacobs, R.A.: Hierarchical mixtures of experts and the EM algorithm. Neural Computation 6, 181–214 (1994) 16. Li, K.C., Lue, H.H., Chen, C.H.: Interactive tree-structured regression via principal Hessian directions. Journal of the American Statistical Association 95, 547–560 (2000) 17. Mandelbrot, B.: Forecasts of future prices, unbiased markets, and martingale models. Journal of Business 39, 242–255 (1966) 18. Meek, C., Chickering, D.M., Heckerman, D.: Autoregressive tree models for time-series analysis. In: Proc. of the Second International SIAM Conference on Data Mining, pp. 229– 244. SIAM (April 2002) 19. Meil˘a, M., Shi, J.: Learning segmentation by random walks. In: Advances in Neural Infor- mation Processing Systems 13, pp. 873–879. MIT Press (2001) 20. Murthy, S.K., Kasif, S., Salzberg, S.: A system for induction of oblique decision trees. Jour- nal of Artiﬁcial Intelligence Research 2, 1–32 (1994) 21. Ng, A.Y., Jordan, M.I., Weiss, Y.: On spectral clustering: Analysis and an algorithm. In: Advances in Neural Information Processing Systems 14, pp. 849–856. MIT Press (2002) 22. Samuelson, P.: Proof that properly anticipated prices ﬂuctuate randomly. Industrial Manage- ment Review 6, 41–49 (1965) 23. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(8), 888–905 (2000) 24. Tong, H.: Threshold models in non-linear time series analysis. Lecture Notes in Statistics, vol. 21. Springer (1983) 25. Tong, H., Lim, K.S.: Threshold autoregression, limit cycles and cyclical data- with discus- sion. Journal of the Royal Statistical Society, Series B 42(3), 245–292 (1980) 26. von Luxburg, U.: A tutorial on spectral clustering. Statistics and Computing 17(4), 395–416 (2007) 27. Waterhouse, S., Robinson, A.: Non-linear prediction of acoustic vectors using hierarchical mixtures of experts. In: Advances in Neural Information Processing Systems 7, pp. 835–842. MIT Press (1995) 28. Weigend, A.S., Mangeas, M., Srivastava, A.N.: Nonlinear gated experts for time series: Dis- covering regimes and avoiding overﬁtting. International Journal of Neural Systems 6(4), 373–399 (1995) 29. White, S., Smyth, P.: A spectral clustering approach to ﬁnding communities in graphs. In: Proc. of the 5th SIAM International Conference on Data Mining. SIAM (2005) Appendix Lemma 1. Let Σ be a non-singular sample auto-covariance matrix for X t−1 t−p deﬁned on the p-dimensional space with principal diagonal direction d =( 1√p ,..., 1√p ), and let At = 1 p ∑p i=1 Xt−i.Then sin 2(Σd, d)= ∑p i=1 cov(Xt−i − At,At) 2 ∑p i=1 cov(Xt−i,At)2 . (2) Proof. Introduce St = ∑p i=1 Xt−i. As per bi-linear property of covariance, (Σd)i = 1√ p cov(Xt−i,St),i =1,... ,p and (Σd)d = 1 p ∑p i=1 cov(Xt−i,St)= cov(At,St). Non-singularity of Σ implies that the vector Σd ̸=0. Hence, |Σd| 2 ̸=0 and Segmentation of Nonstationary Time Series with Geometric Clustering 107 cos 2(Σd, d)= ((Σd)d)2 |(Σd)|2 = pcov(At,St) 2 ∑p i=1 cov(Xt−i,St)2 . It follows that sin 2(Σd, d) =1 − cos 2(Σd, d) = p ( 1 p ∑p i=1 cov(Xt−i,St) 2 − cov(At,St) 2) ∑p i=1 cov(Xt−i,St)2 = ∑p i=1 cov(Xt−i − At,St) 2 ∑p i=1 cov(Xt−i,St)2 Dividing the numerator and denominator of the last fraction by p2 amounts to replacing St by At, which concludes the proof. □ Corollary 1. When Xt−i − At and At are weakly correlated, and the variance of Xt−i − At is comparable to that of At, i =1,... ,p,then sin 2(Σd, d) is small. Speciﬁcally, let σ and ρ denote respectively standard deviation and correlation, and introduce Δi = cov(Xt−i−At,At) σ(At) = ρ(Xt−i − At,At)σ(Xt−i − At). We quantify both assumptions in Corollary 1 by positing that |Δi| <ϵσ(At),i =1,... ,p,where 0 <ϵ ≪ 1. Easy algebra on Equation (2) yields sin 2(Σd, d)= ΣΔ 2 i Σ(σ(At)+ Δi)2 < pϵ2σ(At) 2 p(1 − ϵ)2σ(At)2 = ϵ2 (1 − ϵ)2 (3) Under the assumptions of Corollary 1, we can now show that d is geometrically close to an eigenvector of Σ. Indeed, by inserting (3) into the Pythagorean identity we derive that | cos(Σd, d)| > √1−2ϵ 1−ϵ and close to 1. Now, given a vector v for which |v| = 1, | cos(Σv, v)| reaches the maximum of 1 iff v is an eigenvector of Σ. When the eigenvalues of Σ are distinct, d must therefore be at a small angle with one of the p principal axes for Σ.","libVersion":"0.3.2","langs":""}
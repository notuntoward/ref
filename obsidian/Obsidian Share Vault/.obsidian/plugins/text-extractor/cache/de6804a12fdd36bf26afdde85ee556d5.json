{"path":"lit/lit_sources.backup/Donti21robustCtrlNeural.pdf","text":"Published as a conference paper at ICLR 2021 ENFORCING ROBUST CONTROL GUARANTEES WITHIN NEURAL NETWORK POLICIES Priya L. Donti 1, Melrose Roderick1, Mahyar Fazlyab 2, J. Zico Kolter 1,3 1Carnegie Mellon University, 2Johns Hopkins University, 3Bosch Center for AI {pdonti, mroderick}@cmu.edu, mahyarfazlyab@jhu.edu, zkolter@cs.cmu.edu ABSTRACT When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the aver- age (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that com- bines the strengths of these two approaches: constructing a generic nonlinear con- trol policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Speciﬁcally, our approach en- tails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several do- mains, improving in average-case performance over existing robust control meth- ods and in worst-case stability over (non-robust) deep RL methods. 1 INTRODUCTION The ﬁeld of robust control, dating back many decades, has been able to provide rigorous guarantees on when controllers will succeed or fail in controlling a system of interest. In particular, if the uncer- tainties in the underlying dynamics can be bounded in speciﬁc ways, these techniques can produce controllers that are provably robust even under worst-case conditions. However, as the resulting policies tend to be simple (i.e., often linear), this can limit their performance in typical (rather than worst-case) scenarios. In contrast, recent high-proﬁle advances in deep reinforcement learning have yielded state-of-the-art performance on many control tasks, due to their ability to capture complex, nonlinear policies. However, due to a lack of robustness guarantees, these techniques have still found limited application in safety-critical domains where an incorrect action (either during training or at runtime) can substantially impact the controlled system. In this paper, we propose a method that combines the guarantees of robust control with the ﬂexibility of deep reinforcement learning (RL). Speciﬁcally, we consider the setting of nonlinear, time-varying systems with unknown dynamics, but where (as common in robust control) the uncertainty on these dynamics can be bounded in ways amenable to obtaining provable performance guarantees. Building upon speciﬁcations provided by traditional robust control methods in these settings, we construct a new class of nonlinear policies that are parameterized by neural networks, but that are nonetheless provably robust. In particular, we project the outputs of a nominal (deep neural network-based) controller onto a space of stabilizing actions characterized by the robust control speciﬁcations. The resulting nonlinear control policies are trainable using standard approaches in deep RL, yet are guaranteed to be stable under the same worst-case conditions as the original robust controller. We describe our proposed deep nonlinear control policy class and derive efﬁcient, differentiable projections for this class under various models of system uncertainty common in robust control. We demonstrate our approach on several different domains, including synthetic linear differential inclusion (LDI) settings, the cart-pole task, a quadrotor domain, and a microgrid domain. Although these domains are simple by modern RL standards, we show that purely RL-based methods often produce unstable policies in the presence of system disturbances, both during and after training. In contrast, we show that our method remains stable even when worst-case disturbances are present, while improving upon the performance of traditional robust control methods. 1arXiv:2011.08105v2 [cs.LG] 28 Jan 2021 Published as a conference paper at ICLR 2021 2 RELATED WORK We employ techniques from robust control, (deep) RL, and differentiable optimization to learn prov- ably robust nonlinear controllers. We discuss these areas of work in connection to our approach. Robust control. Robust control is concerned with the design of feedback controllers for dynamical systems with modeling uncertainties and/or external disturbances (Zhou and Doyle, 1998; Ba¸sar and Bernhard, 2008), speciﬁcally controllers with guaranteed performance under worst-case conditions. Many classes of robust control problems in both the time and frequency domains can be formulated using linear matrix inequalities (LMIs) (Boyd et al., 1994; Kothare et al., 1996); for reasonably-sized problems, these LMIs can be solved using off-the-shelf numerical solvers based on interior-point or ﬁrst-order (gradient-based) methods. However, providing stability guarantees often requires the use of simple (linear) controllers, which greatly limits average-case performance. Our work seeks to improve performance via nonlinear controllers that nonetheless retain the same stability guarantees. Reinforcement learning (RL). In contrast, RL (and speciﬁcally, deep RL) is not restricted to simple controllers or problems with uncertainty bounds on the dynamics. Instead, deep RL seeks to learn an optimal control policy, represented by a neural network, by directly interacting with an unknown environment. These methods have shown impressive results in a variety of complex control tasks (e.g., Mnih et al. (2015); Akkaya et al. (2019)); see Bu¸soniu et al. (2018) for a survey. However, due to its lack of safety guarantees, deep RL has been predominantly applied to simulated environments or highly-controlled real-world problems, where system failures are either not costly or not possible. Efforts to address the lack of safety and stability in RL fall into several main categories. The ﬁrst tries to combine control-theoretic ideas, predominantly robust control, with the nonlinear control policy beneﬁts of RL (e.g., Morimoto and Doya (2005); Abu-Khalaf et al. (2006); Feng et al. (2009); Liu et al. (2013); Wu and Luo (2013); Luo et al. (2014); Friedrich and Buss (2017); Pinto et al. (2017); Jin and Lavaei (2018); Chang et al. (2019); Han et al. (2019); Zhang et al. (2020)). For example, RL has been used to address stochastic stability in H∞ control synthesis settings by jointly learning Lyapunov functions and policies in these settings (Han et al., 2019). As another example, RL has been used to address H∞ control for continuous-time systems via min-max differential games, in which the controller and disturbance are the “minimizer” and “maximizer” (Morimoto and Doya, 2005). We view our approach as thematically aligned with this previous work, though our method is able to capture not only H∞ settings, but also a much broader class of robust control settings. Another category of methods addressing this challenge is safe RL, which aims to learn control poli- cies while maintaining some notion of safety during or after learning. Typically, these methods attempt to restrict the RL algorithm to a safe region of the state space by making strong assump- tions about the smoothness of the underlying dynamics, e.g., that the dynamics can be modeled as a Gaussian process (GP) (Turchetta et al., 2016; Akametalu et al., 2014) or are Lipschitz continu- ous (Berkenkamp et al., 2017; Wachi et al., 2018). This framework is in theory more general than our approach, which requires using stringent uncertainty bounds (e.g. state-control norm bounds) from robust control. However, there are two key beneﬁts to our approach. First, norm bounds or polytopic uncertainty can accommodate sharp discontinuities in the continuous-time dynamics. Second, con- vex projections (as used in our method) scale polynomially with the state-action size, whereas GPs in particular scale exponentially (and are therefore difﬁcult to extend to high-dimensional problems). A third category of methods uses Constrained Markov Decision Processes (C-MDPs). These meth- ods seek to maximize a discounted reward while bounding some discounted cost function (Altman, 1999; Achiam et al., 2017; Taleghan and Dietterich, 2018; Yang et al., 2020). While these methods do not require knowledge of the cost functions a-priori, they only guarantee the cost constraints hold during test time. Additionally, using C-MDPs can yield other complications, such as optimal policies being stochastic and the constraints only holding for a subset of states. Differentiable optimization layers. A great deal of recent work has studied differentiable op- timization layers for neural networks: e.g., layers for quadratic programming (Amos and Kolter, 2017), SAT solving (Wang et al., 2019), submodular optimization (Djolonga and Krause, 2017; Tschiatschek et al., 2018), cone programs (Agrawal et al., 2019), and other classes of optimization problems (Gould et al., 2019). These layers can be used to construct neural networks with useful inductive bias for particular domains or to enforce that networks obey hard constraints dictated by the settings in which they are used. We create fast, custom differentiable optimization layers for the latter purpose, namely, to project neural network outputs into a set of certiﬁably stabilizing actions. 2 Published as a conference paper at ICLR 2021 3 BACKGROUND ON LQR AND ROBUST CONTROL SPECIFICATIONS In this paper, our aim is to control nonlinear (continuous-time) dynamical systems of the form ˙x(t) ∈ A(t)x(t) + B(t)u(t) + G(t)w(t), (1) where x(t) ∈ Rs denotes the state at time t; u(t) ∈ Ra is the control input; w(t) ∈ Rd captures both external (possibly stochastic) disturbances and any modeling discrepancies; ˙x(t) denotes the time derivative of the state x at time t; and A(t) ∈ Rs×s, B(t) ∈ Rs×a, G(t) ∈ Rs×d. This class of models is referred to as linear differential inclusions (LDIs); however, we note that despite the name, this class does indeed characterize nonlinear systems, as, e.g., w(t) can depend arbitrarily on x(t) and u(t) (though we omit this dependence in the notation for brevity). Within this class of models, it is often possible to construct robust control speciﬁcations certifying system stability. Given such speciﬁcations, our proposal is to learn nonlinear (deep neural network-based) policies that provably satisfy these speciﬁcations while optimizing some objective of interest. We start by giving background on the robust control speciﬁcations and objectives considered in this work. 3.1 ROBUST CONTROL SPECIFICATIONS In the continuous-time, inﬁnite-horizon settings we consider here, the goal of robust control is often to construct a time-invariant control policy u(t) = π(x(t)), alongside some certiﬁcation that guar- antees that the controlled system will be stable (i.e., that trajectories of the system will converge to an equilibrium state, usually x = 0 by convention; see Haddad and Chellaboina (2011) for a more formal deﬁnition). For many classes of systems, 1 this certiﬁcation is typically in the form of a positive deﬁnite Lyapunov function V : Rs → R, with V (0) = 0 and V (x) > 0 for all x ̸= 0, such that the function is decreasing along trajectories – for instance, ˙V (x(t)) ≤ −αV (x(t)) (2) for some design parameter α > 0. (This particular condition implies exponential stability with a rate of convergence α.2) For certain classes of bounded dynamical systems, time-invariant linear control policies u(t) = Kx(t), and quadratic Lyapunov functions V (x) = xT P x, it is possible to construct such guarantees using semideﬁnite programming. For instance, consider the class of norm-bounded LDIs (NLDIs) ˙x = Ax(t) + Bu(t) + Gw(t), ∥w(t)∥2 ≤ ∥Cx(t) + Du(t)∥2, (3) where A ∈ Rs×s, B ∈ Rs×a, G ∈ Rs×d, C ∈ Rk×s, and D ∈ Rk×a are time-invariant and known, and the disturbance w(t) is arbitrary (and unknown) but obeys the norm bounds above.3 For these systems, it is possible to specify a set of stabilizing policies via a set of linear matrix inequalities (LMIs, Boyd et al. (1994)): [ AS + SA T + µGG T + BY + Y T BT + αS SC T + Y T DT CS + DY −µI ] ≼ 0, S ≻ 0, µ > 0, (4) where S ∈ Rs×s and Y ∈ Ra×s. For matrices S and Y satisfying (4), K = Y S−1 and P = S−1 are then a stabilizing linear controller gain and Lyapunov matrix, respectively. While the LMI above is speciﬁc to NLDI systems, this general paradigm of constructing stability speciﬁcations using LMIs applies to many settings commonly considered in robust control (e.g., settings with norm- bounded disturbances or polytopic uncertainty, or H∞ control settings). More details about these types of formulations are given in, e.g., Boyd et al. (1994); in addition, we provide the relevant LMI constraints for the settings we consider in this work in Appendix A. 1In this work, we consider sub-classes of system (1) that may indeed be stochastic (e.g., due to a stochastic external disturbance w(t)), but that can be bounded so as to be amenable to deterministic stability analysis. However, other settings may require stochastic stability analysis; please see Astrom (1971). 2See, e.g., Haddad and Chellaboina (2011) for a more rigorous deﬁnition of (local and global) exponential stability. Condition (2) comes from Lyapunov’s Theorem, which characterizes various notions of stability using Lyapunov functions. 3A slightly more complex formulation involves an additional term in the norm bound, i.e., Cx(t)+Du(t)+ Hw(t), which creates a quadratic inequality in w. The mechanics of obtaining robustness speciﬁcations in this setting are largely the same as presented here, though with some additional terms in the equations. As such, as is often done, we assume that H = 0 for simplicity. 3 Published as a conference paper at ICLR 2021 3.2 LQR CONTROL OBJECTIVES In addition to designing for stability, it is often desirable to optimize some objective characterizing controller performance. While our method can optimize performance with respect to any arbitrary cost or reward function, to make comparisons with existing methods easier, for this paper we con- sider the well-known inﬁnite-horizon “linear-quadratic regulator” (LQR) cost, deﬁned as ∫ ∞ 0 (x(t)T Qx(t) + u(t) T Ru(t) ) dt, (5) for some Q ∈ S s×s ≽ 0 and R ∈ S a×a ≻ 0. If the control policy is assumed to be time-invariant and linear as described above (i.e., u(t) = Kx(t)), minimizing the LQR cost subject to stability con- straints can be cast as an SDP (see, e.g., Yao et al. (2001)) and solved using off-the-shelf numerical solvers – a fact that we exploit in our work. For example, to obtain an optimal linear time-invariant controller for the NLDI systems described above, we can solve minimize S,Y tr(QS) + tr(R1/2Y S−1Y T R1/2) s. t. Equation (4) holds. (6) 4 ENFORCING ROBUST CONTROL GUARANTEES WITHIN NEURAL NETWORKS We now present the main contribution of our paper: A class of nonlinear control policies, poten- tially parameterized by deep neural networks, that is guaranteed to obey the same stability condi- tions enforced by the robustness speciﬁcations described above. The key insight of our approach is as follows: While it is difﬁcult to derive speciﬁcations that globally characterize the stability of a generic nonlinear controller, if we are given known robustness speciﬁcations, we can create a suf- ﬁcient condition for stability by simply enforcing that our policy satisﬁes these speciﬁcations at all t. For instance, given a known Lyapunov function, we can enforce exponential stability by ensuring that our policy sufﬁciently decreases this function (e.g., satisﬁes Equation (2)) at any given x(t). In the following sections, we present our nonlinear policy class, as well as our general framework for learning provably robust policies using this policy class. We then derive the instantiation of this framework for various settings of interest. In particular, this involves constructing (custom) differentiable projections that can be used to adjust the output of a nominal neural network to satisfy desired robustness criteria. For simplicity of notation, we will often suppress the t-dependence of x, u, and w, but we note that these are continuous-time quantities as before. 4.1 A PROVABLY ROBUST NONLINEAR POLICY CLASS Given a dynamical system of the form (1) and a quadratic Lyapunov function V (x) = xT P x, let C(x) := {u ∈ Ra | ˙V (x) ≤ −αV (x) ∀ ˙x ∈ A(t)x + B(t)u + G(t)w} (7) denote a set of actions that, for a ﬁxed state x ∈ Rs, are guaranteed to satisfy the exponential stability condition (2) (even under worst-case realizations of the disturbance w). We note that this “safe” set is non-empty if P satisﬁes the relevant LMI constraints (e.g., system (4) for NLDIs) characterizing robust linear time-invariant controllers, as there is then some K corresponding to P such that Kx ∈ C(x) for all states x. Using this set of actions, we then construct a robust nonlinear policy class that projects the output of some neural network onto this set. More formally, consider an arbitrary nonlinear (neural network- based) policy class ˆπθ : Rs → Ra parameterized by θ, and let P(·) denote the projection operator for some set (·). We then deﬁne our robust policy class as πθ : Rs → Ra, where πθ(x) = PC(x)(ˆπθ(x)). (8) We note that this policy class is differentiable if the projections can be implemented in a differen- tiable manner (e.g., using convex optimization layers (Agrawal et al., 2019), though we construct efﬁcient custom solvers for our purposes). Importantly, as all policies in this class satisfy the stabil- ity condition (2) for all states x and at all times t, these policies are certiﬁably robust under the same conditions as the original (linear) controller for which the Lyapunov function V (x) was constructed. Given this policy class and some performance objective ℓ (e.g., LQR cost), our goal is to then ﬁnd parameters θ such that the corresponding policy optimizes this objective – i.e., to solve minimize θ ∫ ∞ 0 ℓ ( x, πθ(x) ) dt s. t. ˙x ∈ A(t)x + B(t)πθ(x) + G(t)w. (9) 4 Published as a conference paper at ICLR 2021 Algorithm 1 Learning provably robust controllers with deep RL 1: input performance objective ℓ // e.g., LQR cost 2: input stability requirement // e.g., ˙V (x) ≤ −αV (x) 3: input policy optimizer A // e.g., a planning or RL algorithm 4: compute P , K satisfying LMI constraints // e.g., by optimizing (6) 5: construct speciﬁcations C(x) using P // as deﬁned in Equation (7) 6: construct robust policy class πθ using C // as deﬁned in Equation (8) 7: train πθ via A to optimize Equation (9) 8: return πθ Since πθ is differentiable, we can solve this problem via a variety of approaches, e.g., a model- based planning algorithm if the true dynamics are known, or virtually any (deep) RL algorithm if the dynamics are unknown.4 This general procedure for constructing stabilizing controllers is summarized in Algorithm 1. While seemingly simple, this formulation presents a powerful paradigm: by simply transforming the output of a neural network, we can employ an expressive policy class to optimize an objective of interest while ensuring the resultant policy will stabilize the system during both training and testing. We instantiate our framework by constructing “safe” sets C(x) and their associated (differentiable) projections PC(x) for three settings of interest: NLDIs, polytopic linear differential inclusions (PLDIs), and H∞ control settings. As an example, we describe this procedure below for NLDIs, and refer readers to Appendix B for corresponding formulations for the additional settings we consider. 4.2 EXAMPLE: NLDIS In order to apply our framework to the NLDI setting (3), we ﬁrst compute a quadratic Lyapunov function V (x) = xT P x by solving the optimization problem (6) for the given system via semidef- inite programming. We then use the resultant Lyapunov function to compute the system-speciﬁc “safe” set C(x), and then create a fast, custom differentiable solver to project onto this set. 4.2.1 COMPUTING SETS OF STABILIZING ACTIONS Given P , we compute CNLDI(x) as the set of actions u ∈ Ra that, for each state x ∈ Rs, satisfy the stability condition (2) at that state under even a worst-case realization of the dynamics (i.e., in this case, even under a worst-case disturbance w). The form of the resultant set is given below. Theorem 1. Consider the NLDI system (3), some stability parameter α > 0, and a Lyapunov function V (x) = xT P x with P satisfying Equation (4). Assuming P exists, deﬁne CNLDI(x) := {u ∈ Ra | ∥Cx + Du∥2 ≤ −xT P B ∥GT P x∥2 u − xT (2P A + αP )x 2∥GT P x∥2 } for all states x ∈ Rs. For all x, CNLDI(x) is a non-empty set of actions that satisfy the exponential stability condition (2). Further, CNLDI(x) is a convex set in u. Proof. We seek to ﬁnd a set of actions such that the condition (2) is satisﬁed along all possible trajectories of (3). A set of actions satisfying this condition at a given x is given by CNLDI(x) := { u ∈ Ra | sup w:∥w∥2≤∥Cx+Du∥2 ˙V (x) ≤ −αV (x) } . Let S := {w : ∥w∥2 ≤ ∥Cx + Du∥2}. We can then rewrite the left side of the above inequality as sup w∈S ˙V (x) = sup w∈S ˙xT P x + xT P ˙x = 2xT P (Ax + Bu) + sup w∈S 2xT P Gw = 2x T P (Ax + Bu) + 2∥GT P x∥2∥Cx + Du∥2, by the deﬁnition of the NLDI dynamics and the closed-form minimization of a linear term over an L2 ball. Rearranging yields an inequality of the desired form. We note that by deﬁnition of the 4While this problem is inﬁnite-horizon and continuous in time, in practice, one would optimize it in discrete time over a large ﬁnite time horizon. 5 Published as a conference paper at ICLR 2021 speciﬁcations (4), there is some K corresponding to P such that the policy u = Kx satisﬁes the exponential stability condition (2); thus, Kx ∈ CNLDI, and CNLDI is non-empty. Further, as the above inequality represents a second-order cone constraint in u, this set is convex in u. We further consider the special case where D = 0, i.e., the norm bound on w does not depend on the control action. This form of NLDI arises in many common settings (e.g., where w characterizes linearization error in a nonlinear system but the dynamics depend only linearly on the action), and is one for which we can compute the relevant projection in closed form (as described shortly). Corollary 1.1. Consider the NLDI system (3) with D = 0, some stability parameter α > 0, and Lyapunov function V (x) = xT P x with P satisfying Equation (4). Assuming P exists, deﬁne CNLDI-0(x) := { u ∈ Ra | 2xT P Bu ≤ −xT (2P A + αP )x − 2∥GT P x∥2∥Cx∥2} for all states x ∈ Rs. For all x, CNLDI-0(x) is a non-empty set of actions that satisfy the exponential stability condition (2). Further, CNLDI-0(x) is a convex set in u. Proof. The result follows by setting D = 0 in Theorem 1 and rearranging terms. As the above inequality represents a linear constraint in u, this set is convex in u. 4.2.2 DERIVING EFFICIENT, DIFFERENTIABLE PROJECTIONS For the general NLDI setting (3), we note that the relevant projection PCNLDI(x) (see Theorem 1) represents a projection onto a second-order cone constraint. As this projection does not necessarily have a closed form, we must implement it using a differentiable optimization solver (e.g., Agrawal et al. (2019)). For computational efﬁciency purposes, we implement a custom solver that employs an accelerated projected dual gradient method for the forward pass, and employs implicit differentiation through the ﬁxed point equations of this solution method to compute relevant gradients for the backward pass. Derivations and additional details are provided in Appendix C. In the case where D = 0 (see Corollary 1.1), we note that the projection operation PCNLDI-0(x) does have a closed form, and can in fact be implemented via a single ReLU operation. Speciﬁcally, deﬁning ηT := 2xT P B and ζ := −x T (2P A + αP )x − 2∥G T P x∥2∥Cx∥2, we see that PCNLDI-0(x) (ˆπ(x)) = {ˆπ(x) if ηT ˆπ(x) ≤ ζ ˆπ(x) − ηT ˆπ(x)−ζ ηT η η otherwise = ˆπ(x) − ReLU ( ηT ˆπ(x) − ζ ηT η ) η. (10) 5 EXPERIMENTS Having instantiated our general framework, we demonstrate the power of our approach on a variety of simulated control domains.5 In particular, we evaluate performance on the following metrics: • Average-case performance: How well does the method optimize the performance objec- tive (i.e., LQR cost) under average (non-worst case) dynamics? • Worst-case stability: Does the method remain stable even when subjected to adversarial (worst-case) dynamics? In all cases, we show that our method is able to improve performance over traditional robust con- trollers under average conditions, while still guaranteeing stability under worst-case conditions. 5.1 DESCRIPTION OF DYNAMICS SETTINGS We evaluate our approach on ﬁve NLDI settings: two synthetic NLDI domains, the cart-pole task, a quadrotor domain, and a microgrid domain. (Additional experiments for PLDI and H∞ control settings are described in Appendix I.) For each setting, we choose a time discretization based on the speed at which the system evolves, and run each episode for 200 steps over this discretization. In all cases except the microgrid setting, we use a randomly generated LQR objective where the matrices Q 1/2 and R1/2 are drawn i.i.d. from a standard normal distribution. 5Code for all experiments is available at https://github.com/locuslab/ robust-nn-control 6 Published as a conference paper at ICLR 2021 101 105LossNLDI(D = 0) Non-robust Methods Robust Methods 101 105LossNLDI(D 0) 100 103LossCartpole 100 103LossQuadrotor 0 250 500 750 1000 Training epochs 10 1 102LossMicrogrid 0 250 500 750 1000 Training epochs Setting: MBP PPO RARL Robust MBP * Robust PPO * Original Adversarial Figure 1: Test performance over training epochs for all learning methods employed in our experi- ments. For each training epoch (10 updates for the MBP model and 18 for PPO), we report average quadratic loss over 50 episodes, and use “X” to indicate cases where the relevant method became unstable. (Lower loss is better.) Our robust methods (denoted by ∗), unlike the non-robust methods and RARL, remain stable under adversarial dynamics throughout training. Synthetic NLDI settings. We generate NLDIs of the form (3) with s = 5, a = 3, and d = k = 2 by generating matrices A, B, G, C and D i.i.d. from normal distributions, and producing the disturbance w(t) using a randomly-initialized neural network (with its output scaled to satisfy the norm-bound on the disturbance). We investigate settings both where D ̸= 0 and where D = 0. In both cases, episodes are run for 2 seconds at a discretization of 0.01 seconds. Cart-pole. In the cart-pole task, our goal is to balance an inverted pendulum resting on top of a cart by exerting horizontal forces on the cart. For our experiments, we linearize this system as an NLDI with D ̸= 0 (see Appendix D), and add a small additional randomized disturbance satisfying the NLDI bounds. Episodes are run for 10 seconds at a discretization of 0.05 seconds. Planar quadrotor. In this setting, our goal is to stabilize a quadcopter in the two-dimensional plane by controlling the amount of force provided by the quadcopter’s right and left thrusters. We linearize this system as an NLDI with D = 0 (see Appendix E), and add a small disturbance as in the cart-pole setting. Episodes are run for 4 seconds at a discretization of 0.02 seconds. Microgrid. In this ﬁnal setting, we aim to stabilize a microgrid by controlling a storage device and a solar inverter. We augment the system given in Lam et al. (2016) with LQR matrices and NLDI bounds (see Appendix F). Episodes are run for 2 seconds at a discretization of 0.01 seconds. 7 Published as a conference paper at ICLR 2021 5.2 EXPERIMENTAL SETUP We demonstrate our approach by constructing a robust policy class (8) for each of these settings, and optimizing this policy class via different approaches. Speciﬁcally, we construct a nominal nonlinear control policy class as ˆπθ(x) = Kx + ˜πθ(x), where K is obtained via robust LQR optimization (6), and where ˜πθ(x) is a feedforward neural network. To construct the projections PC, we employ the value of P obtained when solving for K. For the purposes of demonstration, we then optimize our robust policy class πθ(x) = PC(ˆπθ(x)) using two different methods: • Robust MBP (ours): A model-based planner that assumes the true dynamics are known. • Robust PPO (ours): An RL approach based on PPO (Schulman et al., 2017) that does not assume known dynamics (beyond the bounds used to construct the robust policy class). Robust MBP is optimized using gradient descent for 1,000 updates, where each update samples 20 roll-outs. Robust PPO is trained for 50,000 updates, where each update samples 8 roll-outs; we choose the model that performs best on a hold-out set of initial conditions during training. We note that while we use PPO for our demonstration, our approach is agnostic to the particular method of training, and can be deployed with many different (deep) RL paradigms. We compare our robust neural network-based method against the following baselines: • Robust LQR: Robust (linear) LQR controller obtained via Equation (6). • Robust MPC: A robust model-predictive control algorithm (Kothare et al., 1996) based on state-dependent LMIs. (As the relevant LMIs are not always guaranteed to solve, our implementation temporarily reverts to the Robust LQR policy when that occurs.) • RARL: The robust adversarial reinforcement learning algorithm (Pinto et al., 2017), which trains an RL agent in the presence of an adversary. (We note that unlike the other robust methods considered here, this method is not provably robust.) • LQR: A standard non-robust (linear) LQR controller. • MBP and PPO: The non-robust neural network policy class ˆπθ(x) optimized via a model- based planner and the PPO algorithm, respectively. In order to evaluate performance, we train all methods on the dynamical settings described in Sec- tion 5.1, and evaluate them on two different variations of the dynamics: • Original dynamics: The dynamical settings described above (“average case”). • Adversarial dynamics: Modiﬁed dynamics with an adversarial test-time disturbance w(t) generated to maximize loss (“worst case”). We generate this disturbance separately for each method described above (see Appendix G for more details). Initialization states are randomly generated for all experiments. For the synthetic NLDI and mi- crogrid settings, these are generated from a standard normal distribution. For both cart-pole and quadrotor, because our NLDI bounds model linearization error, we must generate initial points within a region where this linearization holds. In particular, the linearization bounds only hold for a speciﬁed L∞ ball, BNLDI, around the equilibrium. We use a simple heuristic to construct this ball and jointly ﬁnd a smaller L∞ ball, Binit, such that there exists a level set L of the Robust LQR Lyapunov function with Binit ⊆ L ⊆ BNLDI (details in Appendix H). Since Robust LQR (and by extension our methods) are guaranteed to decrease the relevant Lyapunov function, this guarantees that these methods will never leave BNLDI when initialized starting from any point inside Binit – i.e., that our NLDI bounds will always hold throughout the trajectories produced by these methods. 5.3 RESULTS Table 1 shows the performance of the above methods. We report the integral of the quadratic loss over the prescribed time horizon on a test set of states, or indicate cases where the relevant method became unstable (i.e., the loss became orders of magnitude larger than for other approaches). (Sam- ple trajectories for these methods are also provided in Appendix H.) These results illustrate the basic advantage of our approach. In particular, both our Robust MBP and Robust PPO methods show improved “average-case” performance over the other provably robust methods (namely, Robust LQR and Robust MPC). As expected, however, the non-robust 8 Published as a conference paper at ICLR 2021 Environment LQR MBP PPO Robust LQR Robust MPC RARL Robust MBP ∗ Robust PPO∗ Generic NLDI (D = 0) O 373 16 21 253 253 27 69 33 A ——— unstable ——— 1009 873 unstable 1111 2321 Generic NLDI (D ̸= 0) O 278 15 82 199 199 147 69 80 A ——— unstable ——— 1900 1667 unstable 1855 1669 Cart-pole O 36.3 3.6 7.2 10.2 10.2 8.3 9.7 8.4 A — unstable — 172.1 42.2 47.8 41.2 50.0 16.3 Quadrotor O 5.4 2.5 7.7 13.8 13.8 12.2 11.0 8.3 A unstable 545.7 137.6 64.8 unstable † 63.1 25.7 26.5 Microgrid O 4.59 0.60 0.61 0.73 0.73 0.67 0.61 0.61 A ——— unstable ——— 0.99 0.92 2.17 7.68 8.91 Table 1: Performance of various approaches, both robust (right) and non-robust (left). We report average quadratic loss over 50 episodes under the original dynamics (O) and under an adversarial disturbance (A). For the original dynamics (O), the best performance for both non-robust methods and robust methods is in bold (lower loss is better). Under the adversarial dynamics (A), we seek to observe whether or not methods remain stable; we use “unstable” to indicate cases where the relevant method becomes unstable (and † to denote any instabilities due to numerical, rather than theoretical, issues). Our robust methods (denoted by ∗) improve performance over Robust LQR and Robust MPC in the average case while remaining stable under adversarial dynamics, whereas the non-robust methods and RARL either go unstable or receive much larger losses. LQR, MBP, and PPO methods often perform better within the original nominal dynamics, as they are optimizing for expected performance but do not need to consider robustness under worst-case scenarios. However, when we apply allowable adversarial perturbations (that still respect our dis- turbance bounds), the non-robust LQR, MBP, and PPO approaches diverge or perform very poorly. Similarly, the RARL agent performs well under the original dynamics, but diverges under adversar- ial perturbations in the generic NLDI settings. In contrast, both of our provably robust approaches (as well as Robust LQR) remain stable under even “worst-case” adversarial dynamics. (We note that the baseline Robust MPC method goes unstable in one instance, though this is due to numerical instability issues, rather than issues with theoretical guarantees.) Figure 1 additionally shows the performance of all neural network-based methods on the test set over training epochs. While the robust and non-robust MBP and PPO approaches both converge quickly to their ﬁnal performance levels, both non-robust versions become unstable under the ad- versarial dynamics very early in the process. The RARL method also frequently destabilizes during training. Our Robust MBP and PPO policies, on the other hand, remain stable throughout the entire optimization process, i.e., do not destabilize during either training or testing. Overall, these results show that our method is able to learn policies that are more expressive than traditional robust methods, while guaranteeing these policies will be stable under the same conditions as Robust LQR. 6 CONCLUSION In this paper, we have presented a class of nonlinear control policies that combines the expressive- ness of neural networks with the provable stability guarantees of traditional robust control. This policy class entails projecting the output of a neural network onto a set of stabilizing actions, param- eterized via robustness speciﬁcations from the robust control literature, and can be optimized using a model-based planning algorithm if the dynamics are known or virtually any RL algorithm if the dynamics are unknown. We instantiate our general framework for dynamical systems characterized by several classes of linear differential inclusions that capture many common robust control settings. In particular, this entails deriving efﬁcient, differentiable projections for each setting, via implicit differentiation techniques. We show over a variety of simulated domains that our method improves upon traditional robust LQR techniques while, unlike non-robust LQR and neural network methods, remaining stable even under worst-case allowable perturbations of the underlying dynamics. We believe that our approach highlights the possible connections between traditional control meth- ods and (deep) RL methods. Speciﬁcally, by enforcing more structure in the classes of deep net- works we consider, it is possible to produce networks that provably satisfy many of the constraints that have typically been thought of as outside the realm of RL. We hope that this work paves the way for future approaches that can combine more structured uncertainty or robustness guarantees with RL, in order to improve performance in settings traditionally dominated by classical robust control. 9 Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This work was supported by the Department of Energy Computational Science Graduate Fellowship (DE-FG02-97ER25308), the Center for Climate and Energy Decision Making through a cooper- ative agreement between the National Science Foundation and Carnegie Mellon University (SES- 00949710), the Computational Sustainability Network, and the Bosch Center for AI. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1745016. Any opinions, ﬁndings, and conclusions or recommenda- tions expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation. We thank Vaishnavh Nagarajan, Filipe de Avila Belbute Peres, Anit Sahu, Asher Trockman, Eric Wong, and anonymous reviewers for their feedback on this work. REFERENCES Kemin Zhou and John Comstock Doyle. Essentials of Robust Control, volume 104. Prentice hall Upper Saddle River, NJ, 1998. Tamer Ba¸sar and Pierre Bernhard. H∞-Optimal Control and Related Minimax Design Problems: A Dynamic Game Approach. Springer Science & Business Media, 2008. Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear Matrix Inequalities in System and Control Theory, volume 15. Siam, 1994. Mayuresh V Kothare, Venkataramanan Balakrishnan, and Manfred Morari. Robust constrained model predictive control using linear matrix inequalities. Automatica, 32(10):1361–1379, 1996. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle- mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving Rubik’s Cube with a Robot Hand. arXiv preprint arXiv:1910.07113, 2019. Lucian Bu¸soniu, Tim de Bruin, Domagoj Toli´c, Jens Kober, and Ivana Palunko. Reinforcement learning for control: Performance, stability, and deep approximators. Annual Reviews in Control, 46:8–28, 2018. Jun Morimoto and Kenji Doya. Robust Reinforcement Learning. Neural Computation, 17(2):335– 359, 2005. Murad Abu-Khalaf, Frank L Lewis, and Jie Huang. Policy Iterations on the Hamilton–Jacobi–Isaacs Equation for H∞ State Feedback Control With Input Saturation. IEEE Transactions on Automatic Control, 51(12):1989–1995, 2006. Yantao Feng, Brian DO Anderson, and Michael Rotkowitz. A game theoretic algorithm to compute local stabilizing solutions to HJBI equations in nonlinear H∞ control. Automatica, 45(4):881– 888, 2009. Derong Liu, Hongliang Li, and Ding Wang. Neural-network-based zero-sum game for discrete-time nonlinear systems via iterative adaptive dynamic programming algorithm. Neurocomputing, 110: 92–100, 2013. Huai-Ning Wu and Biao Luo. Simultaneous policy update algorithms for learning the solution of linear continuous-time H∞ state feedback control. Information Sciences, 222:472–485, 2013. Biao Luo, Huai-Ning Wu, and Tingwen Huang. Off-Policy Reinforcement Learning for H∞ Control Design. IEEE Transactions on Cybernetics, 45(1):65–76, 2014. Stefan R Friedrich and Martin Buss. A robust stability approach to robot reinforcement learning based on a parameterization of stabilizing controllers. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 3365–3372. IEEE, 2017. 10 Published as a conference paper at ICLR 2021 Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust Adversarial Rein- forcement Learning. In Proceedings of the 34th International Conference on Machine Learning, pages 2817–2826. JMLR. org, 2017. Ming Jin and Javad Lavaei. Stability-certiﬁed reinforcement learning: A control-theoretic perspec- tive. arXiv preprint arXiv:1810.11505, 2018. Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural Lyapunov Control. In Advances in Neural Information Processing Systems, pages 3245–3254, 2019. Minghao Han, Yuan Tian, Lixian Zhang, Jun Wang, and Wei Pan. H∞ Model-free Reinforcement Learning with Robust Stability Guarantee. CoRR, 2019. Kaiqing Zhang, Bin Hu, and Tamer Basar. Policy Optimization for H2 Linear Control with H∞ Ro- bustness Guarantee: Implicit Regularization and Global Convergence. In Learning for Dynamics and Control, pages 179–190. PMLR, 2020. Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe Exploration in Finite Markov Decision Processes with Gaussian Processes. In Advances in Neural Information Processing Systems, 2016. Anayo K. Akametalu, Shahab Kaynama, Jaime F. Fisac, Melanie Nicole Zeilinger, Jeremy H. Gillula, and Claire J. Tomlin. Reachability-based safe learning with Gaussian processes. In 53rd IEEE Conference on Decision and Control, CDC 2014, 2014. Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe Model-based Reinforcement Learning with Stability Guarantees. In Advances in Neural Information Process- ing Systems, 2017. Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe Exploration and Optimization of Constrained MDPs Using Gaussian Processes. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. Eitan Altman. Constrained Markov Decision Processes, volume 7. CRC Press, 1999. Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained Policy Optimization. In Proceedings of the 34th International Conference on Machine Learning, 2017. Majid Alkaee Taleghan and Thomas G. Dietterich. Efﬁcient Exploration for Constrained MDPs. In 2018 AAAI Spring Symposia, 2018. Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-Based Constrained Policy Optimization. In International Conference on Learning Representations, 2020. Brandon Amos and J Zico Kolter. OptNet: Differentiable Optimization as a Layer in Neural Net- works. In Proceedings of the 34th International Conference on Machine Learning, pages 136– 145, 2017. Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. SATNet: Bridging deep learning and logical reasoning using a differentiable satisﬁability solver. In Proceedings of the 36th Interna- tional Conference on Machine Learning, pages 6545–6554, 2019. Josip Djolonga and Andreas Krause. Differentiable Learning of Submodular Models. In Advances in Neural Information Processing Systems, pages 1013–1023, 2017. Sebastian Tschiatschek, Aytunc Sahin, and Andreas Krause. Differentiable Submodular Maximiza- tion. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, pages 2731–2738, 2018. Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable Convex Optimization Layers. In Advances in Neural Information Processing Sys- tems, pages 9558–9570, 2019. 11 Published as a conference paper at ICLR 2021 Stephen Gould, Richard Hartley, and Dylan Campbell. Deep Declarative Networks: A New Hope. arXiv preprint arXiv:1909.04866, 2019. Wassim M Haddad and VijaySekhar Chellaboina. Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach. Princeton University Press, 2011. Karl J Astrom. Introduction to Stochastic Control Theory. Elsevier, 1971. David D Yao, Shuzhong Zhang, and Xun Yu Zhou. A primal-dual semi-deﬁnite programming approach to linear quadratic control. IEEE Transactions on Automatic Control, 46(9):1442–1447, 2001. Quang Linh Lam, Antoneta Iuliana Bratcu, and Delphine Riu. Frequency Robust Control in Stand- alone Microgrids with PV Sources: Design and Sensitivity Analysis. 2016. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017. Hassan K Khalil and Jessy W Grizzle. Nonlinear Systems, volume 3. Prentice Hall Upper Saddle River, NJ, 2002. Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer Science & Business Media, 2013. Heinz H Bauschke. Projection algorithms and monotone operators. PhD thesis, Dept. of Mathe- matics and Statistics, Simon Fraser University, 1996. Russ Tedrake. Underactuated robotics: Learning, planning, and control for efﬁcient and agile ma- chines course notes for MIT 6.832. Working draft edition, 3, 2009. Sumeet Singh, Spencer M Richards, Vikas Sindhwani, Jean-Jacques E Slotine, and Marco Pavone. Learning stabilizable nonlinear dynamics with contraction-based regularization. The Interna- tional Journal of Robotics Research, page 0278364920949931, 2020. Roman Kuiava, Rodrigo A Ramos, and Hemanshu R Pota. A New Method to Design Robust Power Oscillation Dampers for Distributed Synchronous Generation Systems. Journal of Dynamic Sys- tems, Measurement, and Control, 135(3), 2013. 12 Published as a conference paper at ICLR 2021 A DETAILS ON ROBUST CONTROL SPECIFICATIONS As described in Section 3.1, for many dynamical systems of the form (1), it is possible to specify a set of linear, time-invariant policies guaranteeing inﬁnite-horizon exponential stability via a set of LMIs. Here, we derive the LMI (4) provided in the main text for the NLDI system (3), and additionally describe relevant LMI systems for systems characterized by polytopic linear differential inclusions (PLDIs) and for H∞ control settings. A.1 EXPONENTIAL STABILITY IN NLDIS Consider the general NLDI system (3). We seek to design a time-invariant control policy u(t) = Kx(t) and a quadratic Lyapunov function V (x) = xT P x with P ≻ 0 for this system that satisfy the exponential stability criterion ˙V (x) ≤ −αV (x), ∀t. We derive an LMI characterizing such a controller and Lyapunov function, closely following and expanding upon the derivation provided in Boyd et al. (1994). Speciﬁcally, consider the NLDI system (3), reproduced below: ˙x = Ax + Bu + Gw, ∥w∥2 ≤ ∥Cx + Du∥2. (A.1) The time derivative of this Lyapunov function along the trajectories of the closed-loop system is ˙V (x) = ˙xT P x + xT P ˙x = (Ax + Bu + Gw) T P x + xT P (Ax + Bu + Gw) = ((A + BK)x + Gw) T P x + xT P ((A + BK)x + Gw) = [x w ]T [ (A + BK) T P + P (A + BK) P G G T P 0 ] [x w ] . (A.2) The exponential stability condition ˙V (x) ≤ −αV (x) is thus implied by inequality [x w ]T M1 [x w ] := [x w ]T [ (A + BK)T P + P (A + BK) + αP P G G T P 0 ] [x w ] ≤ 0. (A.3) Additionally, the norm bound on w can be equivalently expressed as [x w ]T M2 [x w ] := [x w ]T [ (C + DK) T (C + DK) 0 0 −I ] [x w ] ≥ 0. (A.4) Using the S-procedure, it follows that for some λ ≥ 0, the following matrix inequality is a sufﬁcient condition for exponential stability: M1 + λM2 ≼ 0. (A.5) Using Schur Complements, this matrix inequality is equivalent to (A + BK)T P + P (A + BK) + αP + λ(C + DK) T (C + DK) + 1 λ P GG T P ≼ 0. (A.6) Left- and right-multiplying both sides by P −1, and making the change of variables S = P −1, Y = KS, and µ = 1/λ, we obtain SA T + AS + Y T BT + BY + αS + 1 µ (SC T + Y T DT ) (CS + DY ) + µGG T ≼ 0. (A.7) Using Schur Complements again on this inequality, we obtain our ﬁnal system of linear matrix inequalities as [ AS + SA T + µGG T + BY + Y T BT + αS SC T + Y T DT CS + DY −µI ] ≼ 0, S ≻ 0, µ > 0, (A.8) where then K = Y S−1 and P = S−1. Note that the ﬁrst matrix inequality is homogeneous; we can therefore assume µ = 1 (and therefore, λ = 1), without loss of generality. 13 Published as a conference paper at ICLR 2021 A.2 EXPONENTIAL STABILITY IN PLDIS Consider the setting of polytopic linear differential inclusions (PLDIs), where the dynamics are of the form ˙x(t) = A(t)x(t) + B(t)u(t), (A(t), B(t)) ∈ Conv{(A1, B1), . . . , (AL, BL)}. (A.9) Here, A(t) ∈ Rs×s and B(t) ∈ Rs×a can vary arbitrarily over time, as long as they lie in the convex hull (denoted Conv) of the set of points above, where Ai ∈ Rs×s, Bi ∈ Rs×a for i = 1, . . . , L. We seek to design a time-invariant control policy u(t) = Kx(t) and quadratic Lyapunov function V (x) = xT P x with P ≻ 0 for this system that satisfy the exponential stability criterion ˙V (x) ≤ −αV (x), ∀t. Such a controller and Lyapunov function exist if there exist S ∈ Rs×s ≻ 0 and Y ∈ Ra×s such that AiS + BiY + SA T i + Y T BT i + αS ≼ 0, ∀i = 1, . . . , L, (A.10) where then K = Y S−1 and P = S−1. The derivation of this LMI follows similarly to that for exponential stability in NLDIs, and is well-described in Boyd et al. (1994). A.3 H∞ CONTROL Consider the following H∞ control setting with linear time-invariant dynamics ˙x(t) = Ax(t) + Bu(t) + Gw(t), w ∈ L2, (A.11) where A, B, and G are time-invariant as for the NLDI case, and where we deﬁne L2 as the set of time-dependent signals with ﬁnite L2 norm.6 In cases such as these with larger or more unstructured disturbances, it may not be possible to guarantee asymptotic convergence to an equilibrium. In these cases, our goal is to construct a robust controller with bounds on the extent to which disturbances affect some performance output (e.g., LQR cost), as characterized by the L2 gain of the disturbance-to-output map. Speciﬁcally, we consider the stability requirement that this L2 gain be bounded by some parameter γ > 0 when disturbances are present, and that the system be exponentially stable in the disturbance-free case. This requirement can be characterized via the condition that for all t and some σ ≥ 0, E(x, ˙x, u) := ˙V (x) + αV (x) + σ (xT Qx + u T Ru − γ2∥w∥ 2 2) ≤ 0. (A.12) We note that when E(x(t), ˙x(t), u(t)) ≤ 0 for all t, both of our stability criteria are met. To see this, note that integrating both sides of (A.12) from 0 to ∞ and ignoring the non-negative terms on the left hand side after integration yields ∫ ∞ 0 (x(t) T Qx(t)+u(t)T Ru(t))dt ≤ γ2 ∫ ∞ 0 ∥w(t)∥ 2 2dt + (1/σ)V (x(0)). (A.13) This is precisely the desired bound on the L2 gain of the disturbance-to-output map (see Khalil and Grizzle (2002)). We also note that in the disturbance-free case, substituting w = 0 into (A.12) yields ˙V (x) ≤ −αV (x) − σ (xT Qx + u T Ru ) ≤ −αV (x), (A.14) where the last inequality follows from the non-negativity of the LQR cost; this is precisely our condition for exponential stability. We now seek to design a time-invariant control policy u(t) = Kx(t) and quadratic Lyapunov func- tion V (x) = xT P x with P ≻ 0 that satisﬁes the above condition. In particular, we can write E (x(t), (A + BK)x(t) + Gw(t), Kx(t)) = [x(t) w(t) ]T M1 [x(t) w(t) ] , (A.15) where M1 := [(A + BK) T P + P (A + BK) + αP + σ(Q + K T RK) P G G T P −γ2σI ] . (A.16) 6The L2 norm of a time-dependent signal w(t) : [0, ∞) → Rd is deﬁned as √∫ ∞ 0 ∥w(t)∥ 2 2dt. 14 Published as a conference paper at ICLR 2021 Therefore, we seek to ﬁnd a P ∈ Rs×s ≻ 0 and K ∈ Rs×a that satisfy M1 ≼ 0, for some design parameters α > 0 and σ > 0. Using Schur complements, the matrix inequality M1 ≼ 0 is equivalent to (A + BK) T P + P (A + BK) + αP + σ(Q + K T RK) + P GG T P/(γ2σ) ≼ 0. (A.17) As in Appendix A.1, we left- and right-multiply both sides by P −1, and make the change of variables S = P −1, Y = KS, and µ = 1/σ to obtain SA T +AS +Y T BT +BY +αS + 1 µ ((SQ 1/2)(Q 1/2S) + (Y T R1/2)(R1/2Y ) )+µGG T /γ2 ≼ 0. Using Schur Complements again, we obtain the LMI   SA T + AS + Y T BT + BY + αS + µGG T /γ2 [ SQ 1/2 Y T R1/2] [Q 1/2S R1/2Y ] −µI   ≼ 0, S ≻ 0, µ > 0, (A.18) where then K = Y S−1, P = S−1, and σ = 1/µ. B DERIVATION OF SETS OF STABILIZING POLICIES AND ASSOCIATED PROJECTIONS We describe the construction of the set of actions C(x), deﬁned in Equation (7), for PLDI sys- tems (A.9) and H∞ control settings (A.11). (The relevant formulations for the NLDI system (3) are described in the main text.) B.1 EXPONENTIAL STABILITY IN PLDIS For the general PLDI system (A.9), relevant sets of exponentially stabilizing actions CPLDI are given by the following theorem. Theorem B.1. Consider the PLDI system (A.9), some stability parameter α > 0, and a Lyapunov function V (x) = xT P x with P satisfying (A.10). Assuming P exists, deﬁne CPLDI(x) :=    u ∈ Ra |      2x T P B1 2x T P B2 ... 2xT P BL      u ≤ −      xT (αP + 2P A1)x xT (αP + 2P A2)x ... x T (αP + 2P AL)x         for all states x ∈ Rs. For all x, CPLDI(x) is a non-empty set of actions that satisfy the exponential stability condition (2). Further, CPLDI(x) is a convex set in u. Proof. We seek to ﬁnd a set of actions such that the condition (2) is satisﬁed along all possible trajectories of (A.9), i.e., for any allowable instantiation of (A(t), B(t)). A set of actions satisfying this condition at a given x is given by CPLDI(x) := {u ∈ Ra | ˙V (x) ≤ −αV (x) ∀(A(t), B(t)) ∈ Conv{(A1, B1), . . . , (AL, BL)}. Expanding the left side of the inequality above, we see that for some coefﬁcients γi ∈ R ≥ 0, i = 1, . . . , L satisfying ∑L i=1 γi(t) = 1, ˙V (x) = ˙xT P x + xT P ˙x = 2xT P (A(t)x + B(t)u) = 2xT P ( L∑ i=1 γi(t)Aix + γi(t)Biu ) = L∑ i=1 γi (2xT P (Aix + Biu) ) by deﬁnition of the PLDI dynamics and of the convex hull. Thus, if we can ensure 2xT P (Aix + Biu) ≤ −αV (x) = −αx T P x, ∀i = 1, . . . , L, then we can ensure that exponential stability holds. Rearranging this condition and writing it in matrix form yields an inequality of the desired form. We note that by deﬁnition of the speciﬁca- tions (A.10), there is some K corresponding to P such that the policy u = Kx satisﬁes all of the above inequalities; thus, Kx ∈ CPLDI(x), and CPLDI(x) is non-empty. Further, as the above inequality represents a linear constraint in u, this set is convex in u. 15 Published as a conference paper at ICLR 2021 We note that the relevant projection PCPLDI(x) represents a projection onto an intersection of halfs- paces, and can thus be implemented via differentiable quadratic programming (Amos and Kolter, 2017). B.2 H∞ CONTROL For the H∞ control system (A.11), relevant sets of actions satisfying the condition (A.12) are given by the following theorem. Theorem B.2. Consider the system (A.11), some stability parameter α > 0, and a Lyapunov func- tion V (x) = xT P x with P satisfying Equation (A.18). Assuming P exists, deﬁne CH∞ (x) := {u ∈ Ra | u T Ru + (2BT P x)T u + xT (P A+AT P +αP +Q+γ−2P GG T P ) x ≤ 0 } for all states x ∈ Rs. For all x, CH∞ (x) is a non-empty set of actions that guarantee condi- tion (A.12), i.e., that the L2 gain of the disturbance-to-output map is bounded by γ and that the system is exponentially stable in the disturbance-free case. Further, CH∞(x) is convex in u. Proof. We seek to ﬁnd a set of actions such that the condition E(x, ˙x, u) ≤ 0 is satisﬁed along all possible trajectories of (A.11), where E is deﬁned as in (A.12). A set of actions satisfying this condition at a given x is given by CH∞ (x) := {u ∈ Ra | sup w∈L2 E(x, ˙x, u) ≤ 0, ˙x = Ax + Bu + Gw}. To begin, we note that E(x, Ax + Bu + Gw, u) = xT P (Ax + Bu + Gw) + (Ax + Bu + Gw) T P x + αx T P x + σ (xT Qx + uT Ru − γ2∥w∥2 2) We then maximize E over w: w⋆ = arg max w E(x, Ax + Bu + Gw, u) = G T P x/(σγ2). (B.1) Therefore, CH∞ (x) = {u | E(x, Ax + Bu + Gw⋆, u, w⋆) ≤ 0}. (B.2) Expanding and rearranging terms, this becomes CH∞(x) = {u | u T (σR)u + (2BT P x) T u + xT (P A+AT P +αP +σQ+P GG T P/(σγ2)) x ≤ 0}. (B.3) We note that by deﬁnition of the speciﬁcations (A.18), there is some K corresponding to P such that the policy u = Kx satisiﬁes the conditions above (see (A.17)); thus, Kx ∈ CH∞ , and CH∞ is non-empty. We note further that CH∞ is an ellipsoid in the control action space, and is thus convex in u. We rewrite the set CH∞ (x) such that the projection PCH∞ (x) can be viewed as a second-order cone projection, in order to leverage our fast custom solver (Appendix C). In particular, deﬁning ˜P = σR, ˜q = BT P x, and ˜r = xT (P A+AT P +αP +σQ+P GG T P/(σγ2)) x, we can rewrite the ellipsoid above as CH∞ (x) = {u | u ⊤ ˜P u + 2˜q⊤u + ˜r ≤ 0}. (B.4) We note that as ˜P ≻ 0 and ˜r − ˜q⊤ ˜P −1 ˜q < 0, this ellipsoid is non-empty (see, e.g., section B.1 in Boyd and Vandenberghe (2004)). We can then rewrite the ellipsoid as CH∞(x) = {u | ∥ ˜Au + ˜b∥2 ≤ 1} (B.5) where ˜A = √ ˜P ˜q⊤ ˜P −1 ˜q−˜r and ˜b = √ P q⊤P −1q−r P −1q. The constraint ∥ ˜Au + ˜b∥2 ≤ 1 is then a second-order cone constraint in u. 16 Published as a conference paper at ICLR 2021 C A FAST, DIFFERENTIABLE SOLVER FOR SECOND-ORDER CONE PROJECTION In order to construct the robust policy class described in Section 4 for the general NLDI system (3) and the H∞ setting (A.11), we must project a nominal (neural network-based) policy onto the second-order cone constraints described in Theorem 1 and Appendix B.2, respectively. As this projection operation does not necessarily have a closed form, we implement it via a custom differ- entiable optimization solver. More generally, consider a set of the form C = {x ∈ Rn | ∥Ax + b∥2 ≤ c T x + d} (C.1) for some A ∈ Rm×n, b ∈ Rm, c ∈ Rn, and d ∈ R. Given some input y ∈ Rn, we seek to compute the second-order cone projection PC(y) by solving the problem minimize x∈Rn 1 2 ∥x − y∥2 2 subject to ∥Ax + b∥2 ≤ c T x + d. (C.2) Let F denote the ℓ2 norm cone, i.e., F := {(w, t) | ∥w∥2 ≤ t}. Introducing the auxiliary variable z ∈ Rm+1, we can then rewrite the above optimization problem equivalently as minimize x∈Rn, z∈Rm+1 1 2 ∥x − y∥2 2 + 1F (z) subject to z = [ Ax + b c T x + d ] =: Gx + h, (C.3) where for brevity we deﬁne G = [ A cT ] and h = [b d ] , and where 1F denotes the indicator function for membership in the set F. We describe our fast solution technique for computing this projection, as well as our method for obtaining gradients through the solution. C.1 COMPUTING THE PROJECTION We construct a fast solver for problem (C.3) using an accelerated projected dual gradient method. Speciﬁcally, deﬁne µ = Rm+1 as the dual variable on the equality constraint in Equation (C.3). The Lagrangian for this problem can then be written as L (x, z, µ) = 1 2 ∥x − y∥ 2 2 + 1F (z) + µ T (z − Gx − h), (C.4) and the dual problem is given by maxµ minx,z L (x, z, µ). To form the dual problem, we minimize the Lagrangian with respect to x and z as inf x,z L (x, z, µ) = inf x 1 2 {∥x − y∥2 2 − µ T Gx } + inf z {µ T z + 1F (z)} − µ T h. (C.5) We note that the ﬁrst term on the right side is minimized at x ⋆(µ) = y + G T µ. Thus, we see that inf x 1 2 {∥x − y∥ 2 2 − µ T Gx} = − 1 2 µ T GG T µ − µ T Gy. (C.6) For the second term, denote µ = (˜µ, s) and z = (˜z, t). We can then rewrite this term as inf z {µ T z + 1F (z)}} = inf t≥0 inf ˜z {t · s + ˜µ T ˜z | ∥˜z∥2 ≤ t}. (C.7) For a ﬁxed t ≥ 0, the above objective is minimized at ˜z = −t˜µ/∥˜µ∥2. (The problem is infeasible for t < 0.) Substituting this minimizer into (C.7) and minimizing the result over t ≥ 0 yields inf z {µ T z + 1F (z)} = inf t≥0 t(s − ∥˜µ∥2) = −1F (µ) (C.8) 17 Published as a conference paper at ICLR 2021 where the last identity follows from deﬁnition of the second-order cone F. Hence the negative dual problem becomes minimize µ 1 2 µ T GG T µ + µ T (Gy + h) + 1F (µ). (C.9) We now solve this problem via Nesterov’s accelerated projected dual gradient method (Nesterov, 2013). For notational brevity, deﬁne f (µ) := 1 2 µ T GG T µ + µ T (Gy + h). Then, starting from arbitrary µ (−1), µ (0) ∈ Rm+1 we perform the iterative updates ν(k) = µ (k) + β(k)(µ (k) − µ (k−1)) µ (k+1) = PF (ν(k) − 1 Lf ∇f (ν(k))) , (C.10) where Lf = λmax(GG T ) is the Lipschitz constant of f , and PF is the projection operator onto F (which has a closed form solution; see Bauschke (1996)). Letting mf = λmin(GG T ) denote the strong convexity constant of f , the momentum parameter is then scheduled as (Nesterov, 2013) βk =    k − 1 k + 2 if mf = 0 √Lf − √mf √Lf + √mf if mf > 0. (C.11) After computing the optimal dual variable µ ⋆, i.e., the ﬁxed point of (C.10), the optimal primal variable can be recovered via the equation x⋆ = y + G T µ ⋆ (as can be observed from the ﬁrst-order conditions of the Lagrangian (C.4)). C.2 OBTAINING GRADIENTS In order to incorporate the above projection into our neural network, we need to compute the gradi- ents of all problem variables (i.e., G, h, and y) through the solution x⋆. In particular, we note that x⋆ has a direct dependence on both G and y, and an indirect dependence on all of G, h, and y through µ ⋆. To compute the relevant gradients through µ ⋆, we apply the implicit function theorem to the ﬁxed point of the update equations (C.10). Speciﬁcally, as these updates imply that µ ⋆ = ν⋆, their ﬁxed point can be written as µ ⋆ = PF (µ ⋆ − 1 Lf ∇f (µ ⋆) ) . (C.12) Deﬁne M := ∂PF (·) ∂(·) ∣ ∣(·)=µ⋆− 1 Lf ∇f (µ⋆), and note that ∇f (µ ⋆) = GG T µ ⋆ +Gy +h. The differential of the above ﬁxed-point equation is then given by dµ ⋆ = M × (dµ ⋆ − 1 Lf (dGG T µ ⋆ + GdG T µ ⋆ + GG T dµ ⋆ + dGy + Gdy + dh )) . (C.13) Rearranging terms to separate the differentials of problem outputs from problem variables, we see that (I − M + 1 Lf M GG T ) dµ ⋆ = − 1 Lf M (dGG T µ ⋆ + GdG T µ ⋆ + dGy + Gdy + dh ) , (C.14) where I is the identity matrix of appropriate size. As described in e.g. Amos and Kolter (2017), we can then use these equations to form the Jacobian of µ ⋆ with respect to any of the problem variables by setting the differential of the relevant problem variable to I and of all other problem variables to 0; solving the resulting equation for dµ ⋆ then yields the value of the desired Jacobian. However, as these Jacobians can be large depending on problem size, we rarely want to form them explicitly. Instead, given some backward pass vector ∂ℓ ∂µ⋆ ∈ R1×(m+1) with respect to the optimal dual variable, we want to directly compute the gradient 18 Published as a conference paper at ICLR 2021 of the loss with respect to the problem variables: e.g., for y, we want to directly form the result of the product ∂ℓ ∂µ⋆ ∂µ⋆ ∂y ∈ R1×n. We do this via a similar method as presented in Amos and Kolter (2017), and refer the reader there for a more in-depth explanation of the method described below. Deﬁne J := I − M + 1 Lf M GGT to represent the coefﬁcient of dµ ⋆ on the left side of Equa- tion (C.14). Given ∂ℓ ∂µ⋆ , we then compute the intermediate term dµ := −J −T ( ∂ℓ ∂µ⋆ )T . (C.15) We can then form the relevant gradient terms directly as ( ∂ℓ ∂µ⋆ ∂µ ⋆ ∂G )T = 1 Lf M (dµ(G T µ ⋆)T + µ ⋆(G T dµ) T + dµyT ) ( ∂ℓ ∂µ⋆ ∂µ ⋆ ∂h )T = 1 Lf M dµ ( ∂ℓ ∂µ⋆ ∂µ ⋆ ∂y )T = 1 Lf G T M dµ. (C.16) In these computations, we note that as our solver returns x⋆, the backward pass vector we are given is actually ∂ℓ ∂x⋆ ∈ R1×n; thus, we compute ∂ℓ ∂µ⋆ = ∂ℓ ∂x⋆ ∂x⋆ ∂µ⋆ = ∂ℓ ∂x⋆ GT for use in Equation (C.15). Accounting additionally for the direct dependence of some of the problem variables on x ⋆ (recalling that x⋆ = y + G T u⋆), the desired gradients are then given by ( ∂ℓ ∂G )T = ( ∂ℓ ∂x⋆ ∂x ⋆ ∂G + ∂ℓ ∂x⋆ ∂x ⋆ ∂u⋆ ∂u ⋆ ∂G )T = µ ⋆ ∂ℓ ∂x⋆ + 1 Lf M (dµ(G T µ ⋆) T + µ ⋆(G T dµ)T + dµyT ) ( ∂ℓ ∂h )T =   \b \b\b\b\b* 0 ∂ℓ ∂x⋆ ∂x ⋆ ∂h + ∂ℓ ∂x⋆ ∂x ⋆ ∂u⋆ ∂u ⋆ ∂h   T = 1 Lf M dµ ( ∂ℓ ∂y )T = ( ∂ℓ ∂x⋆ ∂x ⋆ ∂y + ∂ℓ ∂x⋆ ∂x ⋆ ∂u⋆ ∂u ⋆ ∂y )T = ( ∂ℓ ∂x⋆ )T + 1 Lf G T M dµ. (C.17) D WRITING THE CART-POLE PROBLEM AS AN NLDI In the cart-pole task, our goal is to balance an inverted pendulum resting on top of a cart by exerting horizontal forces on the cart. Speciﬁcally, the state of this system is deﬁned as x = [px, ˙px, ϕ, ˙ϕ] T , where px is the cart position and ϕ is the angular displacement of the pendulum from its vertical position; we seek to stabilize the system at x = ⃗0 by exerting horizontal forces u ∈ R on the cart. For a pendulum of length ℓ and mass mp, and for a cart of mass mc, the dynamics of the system are (as described in Tedrake (2009)): ˙x =         ˙px u+mp sin ϕ(ℓ ˙ϕ 2−g cos ϕ) mc+mp sin2 ϕ ˙ϕ (mc+mp)g sin ϕ−u cos ϕ−mpℓ ˙ϕ 2 cos ϕ sin ϕ l(mc+mp sin2 ϕ)         , (D.1) where g = 9.81 m/s 2 is the acceleration due to gravity. We rewrite this system as an NLDI by deﬁning ˙x = f (x, u) and then linearizing the system about its equilibrium point as ˙x = Jf (0, 0) [x u ] + Inw, ∥w∥ ≤ ∥Cx + Du∥, (D.2) 19 Published as a conference paper at ICLR 2021 where Jf is the Jacobian of the dynamics, w = f (x, u) − Jf (0, 0) [x u] T is the linearization error, and In is the n × n identity matrix. We bound this linearization error by numerically obtaining the matrices C and D, assuming that x and u are within a neighborhood of the origin. We describe this process in more detail below. As a note, while we employ an NLDI here to characterize the linearization error, it is also possible to characterize this error via polytopic uncertainty (see Ap- pendix J); we choose to use an NLDI here as it yields a much smaller problem description than a PLDI in this case. D.1 DERIVING Jf (0, 0) For ˙x = f (x, u), we see that Jf (x, u) =    0 1 0 0 0 0 0 ∂ ¨px/∂ϕ ∂ ¨px/∂ ˙ϕ ∂ ¨px/∂u 0 0 0 1 0 0 0 ∂ ¨ϕ/∂ϕ ∂ ¨ϕ/∂ ˙ϕ ∂ ¨ϕ/∂u,    , (D.3) where ∂ ¨px ∂ϕ = mp cos ϕ ( ˙ϕ2l − g cos ϕ) + gmp sin 2 ϕ mc + mp sin2 ϕ − 2mp sin ϕ cos ϕ ( mp sin ϕ ( ˙ϕ 2l − g cos ϕ) + u) ( mc + mp sin 2 ϕ)2 , ∂ ¨px ∂ ˙ϕ = 2 ˙ϕlmp sin ϕ mc + mp sin2 ϕ , ∂ ¨px ∂u = 1 mc + mp sin2 ϕ , ∂ ¨ϕ ∂ϕ = g(mc + mp) cos ϕ + ˙ϕ2lmp sin 2 ϕ − ˙ϕ2lmp cos 2 ϕ + u sin ϕ l ( mc + mp sin2 ϕ ) − 2mp sin ϕ cos ϕ(g(mc + mp) sin ϕ − ˙ϕ2lmp sin ϕ cos ϕ − u cos ϕ l ( mc + mp sin2 ϕ )2 , ∂ ¨ϕ ∂ ˙ϕ = −2 ˙ϕmp sin ϕ cos ϕ mc + mp sin 2 ϕ , ∂ ¨ϕ ∂u = − cos ϕ l(mc + mp sin 2 ϕ) . We thus see that Jf (0, 0) =    0 1 0 0 0 0 0 −mpg/mc 0 1/mc 0 0 0 1 0 0 0 g(mc+mp)/lmc 0 −1/mc    . (D.4) D.2 OBTAINING C AND D We then seek to construct matrices C and D that bound the linearization error w between the true dynamics ˙x and our ﬁrst-order linear approximation Jf (0, 0) [x u ] . To do so, we bound the error of this approximation entry-wise: that is, for each entry i = 1, . . . , s, we want to ﬁnd Fi such that for all x in some region x ≤ x ≤ ¯x, and all u in some region u ≤ u ≤ ¯u, w2 i = (∇fi(0) [x u ] − ˙xi )2 ≤ [x u ]T Fi [x u ] . (D.5) Then, given the matrix M = [F T /2 1 F T /2 2 F T /2 3 F T /2 4 F T /2 5 F T /2 6 ]T (D.6) we can then obtain C = M1:s and D = Ms:s+m (where the subscripts indicate column-wise index- ing). 20 Published as a conference paper at ICLR 2021 We solve separately for each Fi to minimize the difference between the right and left sides of Equa- tion (D.5) (while enforcing that the right side is larger than the left side) over a discrete grid of points within x ≤ x ≤ ¯x and u ≤ u ≤ ¯u. By assuming that Fi is symmetric, we are able to cast this as a linear program in the upper triangular entries of Fi. To obtain the matrices C and D used for the cart-pole experiments in the main paper, we let ¯x = [1.5 2 0.2 1.5]T , ¯u = 10, x = −¯x, and u = −¯u. As each entry-wise difference in Equation (D.5) contained exactly three variables (i.e., a total of three entries from x and u), we solved each entry-wise linear program over a mesh grid of 50 points per variable. E WRITING QUADROTOR AS AN NLDI In the planar quadrotor setting, our goal is to stabilize a quadcopter in the two-dimensional plane by controlling the amount of force provided by the quadcopter’s right and left thrusters. Speciﬁcally, the state of this system is deﬁned as x = [px pz ϕ ˙px ˙pz ˙ϕ]T , where (px, pz) is the position of the quadcopter in the vertical plane and ϕ is its roll (i.e., angle from the horizontal position); we seek to stabilize the system at x = ⃗0 by controlling the amount of force u = [ur, ul] T from right and left thrusters. We assume that our action u is additional to a baseline force of [mg/2 mg/2]T provided by the thrusters by default to prevent the quadcopter from falling. For a quadrotor with mass m, moment-arm ℓ for the thrusters, and moment of inertia J about the roll axis, the dynamics of this system are then given by (as modiﬁed from Singh et al. (2020)): ˙x =        ˙px cos ϕ − ˙pz sin ϕ ˙px sin ϕ + ˙pz cos ϕ ˙ϕ ˙pz ˙ϕ − g sin ϕ − ˙px ˙ϕ − g cos ϕ + g 0        +        0 0 0 0 0 0 0 0 1/m 1/m ℓ/J −ℓ/J        u, (E.1) where g = 9.81 m/s2. We linearize this system via a similar method as for the cart-pole setting, i.e., as in Equation (D.2). We describe this process in more detail below. We note that since the dependence of the dynamics on u is linear, we have that D = 0 for our resultant NLDI. As for cart-pole, while we employ an NLDI here to characterize the linearization error, it is also possible to characterize this error via polytopic uncertainty (see Appendix J); we choose to use an NLDI here as it yields a much smaller problem description than a PLDI in this case. E.1 DERIVING Jf (0, 0) For ˙x = f (x, u), we see that Jf (x, u) =        0 0 − ˙px sin ϕ − ˙pz cos ϕ cos ϕ − sin ϕ 0 0 0 0 ˙px cos ϕ − ˙pz sin ϕ sin ϕ cos ϕ 0 0 0 0 0 0 0 1 0 0 0 −g cos ϕ 0 ˙ϕ ˙pz 0 0 0 g sin ϕ − ˙ϕ 0 − ˙px 0 0 0 0 0 0 0 0        , (E.2) and thus Jf (0, 0) =        0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 −g 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0        . (E.3) E.2 OBTAINING C AND D We obtain the matrices C and D via a similar method as described in Appendix D, though in practice we only consider the linearization error with respect to x (i.e., since the dynamics are linear with respect to u, we have D = 0). We let ¯x = [1 1 0.15 0.6 0.6 1.3] and x = −¯x. As for cart-pole, each entry wise difference in the equivalent of Equation (D.5) contained exactly three variables (i.e., a total of three entries from x and u), and each entry-wise linear program was solved over a mesh grid of 50 points per variable. 21 Published as a conference paper at ICLR 2021 F DETAILS ON THE MICROGRID SETTING For our experiments, we build upon the microgrid setting given in Lam et al. (2016). In this system, the state x ∈ R3 captures voltage deviations, frequency deviations, and the amount of power gener- ated by a diesel generator connected to the grid; the action u ∈ R2 describes the current associated with a storage device and a solar PV inverter; and the disturbance w ∈ R describes the difference between the amount of power demanded and the amount of power produced by solar panels on the grid. The authors also deﬁne a performance index y ∈ R2 which captures voltage and frequency deviations (i.e., two of the entries of the state x). To construct an NLDI of the form (3) for this system, we directly use the A, B, and G matrices given in Lam et al. (2016). We generate C i.i.d. from a normal distribution and let D = 0, to represent the fact that the disturbance w and the entries of the state x are correlated, but that w is likely not correlated with the actions u. Finally, we let Q and R be diagonal matrices with 1 in the entries corresponding to quantities represented in the performance index y, and with 0.1 in the rest of the diagonal entries, to emphasize that the variables in y are the most important in describing the performance of the system. G GENERATING AN ADVERSARIAL DISTURBANCE In the NLDI settings explored in our experiments, we seek to construct an “adversarial” disturbance w(t) that obeys the relevant norm bounds ∥w(t)∥2 ≤ ∥Cx(t) + Du(t)∥2 while maximizing the loss. To do this, we use a model predictive control method where the actions taken are w(t). Speciﬁcally, for each policy π, we model w(t) as a neural network speciﬁc to that policy. Every 10 steps of a roll-out, we optimize w(t) through gradient descent to maximize the loss over a horizon of 40 steps, subject to the constraint ∥w(t)∥2 ≤ ∥Cx(t) + Du(t)∥2. H ADDITIONAL EXPERIMENTAL DETAILS Initial states. To pick initial states in our experiments, for the synthetic settings, we sample each attribute of the state i.i.d. from a standard Gaussian distribution. For cart-pole and planar quadrotor, we sample uniformly from bounds chosen such that the non-robust LQR algorithm (under the orig- inal dynamics) did not go unstable. For cart-pole, these bounds were chosen to be px ∈ [−1, 1], ϕ ∈ [−0.1, 0.1], ˙px = ˙ϕ = 0. For planar quadrotor, these bounds were px, pz ∈ [−1, 1], ϕ ∈ [−0.05, 0.05], ˙px = ˙pz = ˙ϕ = 0. Constructing NLDI bounds. Given these initial states, for the cart-pole and quadrotor settings, we needed to construct our NLDI disturbance bounds such that they would hold over the entire trajectory of the robust policy; if not, the robustness speciﬁcation (A.8) would not hold, and our agent might in fact increase the Lyapunov function. To ensure this approximately, we used a simple heuristic: we ran the (non-robust) LQR agent for a full episode with 50 different starting conditions, and constructed an L∞ ball around all states reached in any of these trajectories. We then used these L∞ balls on the states to construct the matrices C and D for our disturbance bounds, using the procedure described in Appendices D and E. Computing infrastructure and runtime. All experiments were run on an XPS 13 laptop with an Intel i7 processor. The planar quadrotor and synthetic NLDI experiment with D = 0 took about 1 day to run (since the projections were simple half-space projections), while all the other synthetic domains and cart-pole took about 3 days to run. The majority of the run-time was in computing the adversarial disturbances for test-time evaluations. Hyperparameter selection. For our experiments, we did not perform large parameter searches. The learning rate we chose for our model-based planner, (both robust and non-robust) remained constant for the different domains; we tried learning rates of 1 × 10−3, 1 × 10−4, 1 × 10 −5 and found 1 × 10 −3 worked best for the non-robust version and 1 × 10−4 worked best for the robust version. For our PPO hyperparameters, we simply used those used in the original PPO paper. One parameter we had to tune for each environment was the time step. In particular, we had to pick a time step high enough that we could run episodes for a reasonable total length of time (within which the non-robust agents would go unstable), but low enough to reasonably approximate a continuous- time setting (since, for our robustness guarantees, we assume the agent’s actions evolve in continu- ous time). Our search space was small, however, consisting of 0.05, 0.02, 0.01, and 0.005 seconds. 22 Published as a conference paper at ICLR 2021 0 2 4 6 8 10 t 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x x (a) LQR 0 2 4 6 8 10 t 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x x (b) Robust LQR 0 2 4 6 8 10 t 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x x (c) MBP 0 2 4 6 8 10 t 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x x (d) Robust MBP 0 2 4 6 8 10 t 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x x (e) PPO 0 2 4 6 8 10 t 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x x (f) Robust PPO Figure H.1: Trajectories of 6 different methods on the cart-pole domain under adversarial dynamics. Trajectory plots. Figure H.1 shows sample trajectories of different methods in the cart-pole do- main under adversarial dynamics. The non-robust LQR and model-based planning approaches both diverge and the non-robust PPO doesn’t diverge, but doesn’t clearly converge after 10 seconds. The robust methods, on the other hand, all clearly converge after 10 seconds. Runtime comparison. Tables H.1 and H.2 show the evaluation and training time of our methods and the baselines over 50 episodes run in parallel. In the NLDI cases where D = 0, i.e., Generic NLDI (D = 0) and Quadrotor, our projection adds only a very small computational cost. In the other cases, the additional computational cost is more signiﬁcant, but our method is still far less expensive than the Robust MPC method. 23 Published as a conference paper at ICLR 2021 Environment LQR MBP PPO Robust LQR Robust MPC RARL Robust MBP ∗ Robust PPO∗ Generic NLDI (D = 0) 0.63 0.61 0.84 0.57 718.06 0.71 0.73 0.94 Generic NLDI (D ̸= 0) 0.64 0.62 0.83 0.58 824.86 0.81 15.13 25.38 Cart-pole 0.55 0.67 0.84 0.53 646.90 0.84 10.12 13.37 Quadrotor 0.95 0.98 1.19 0.88 3348.68 1.14 1.15 1.30 Microgrid 0.58 0.61 0.79 0.57 601.90 0.74 8.14 10.25 Generic PLDI 0.57 0.54 0.76 0.51 819.24 0.73 69.35 64.03 Generic H∞ 0.84 0.80 1.03 0.76 N/A 1.00 47.81 63.67 Table H.1: Time (in seconds) taken to run each method on the test set of every environment for 50 episodes run in parallel. Environment MBP PPO RARL Robust MBP ∗ Robust PPO∗ Generic NLDI (D = 0) 26.36 101.77 102.37 30.78 114.60 Generic NLDI (D ̸= 0) 26.46 100.79 82.53 221.35 1158.28 Cart-pole 25.49 87.04 98.90 146.34 689.93 Quadrotor 41.24 131.48 112.95 46.13 159.06 Microgrid 23.03 112.52 87.71 113.61 436.64 Table H.2: Time (in minutes) taken to train each method in every environment. I EXPERIMENTS FOR PLDIS AND H∞ CONTROL SETTINGS In addition to the NLDI settings explored in the main text, we test the performance of our method on PLDI and H∞ control settings. As for the experiments in the main text, we choose a time discretization based on the speed at which the system evolves, and run each episode for 200 steps over this discretization. In both cases, we use a randomly generated LQR objective where the matrices Q 1/2 and R1/2 are drawn i.i.d. from a standard normal distribution. Synthetic PLDI setting. We generate PLDI instances (A.9) with s = 5, a = 3, and L = 3. Speciﬁ- cally, we generate convex hull matrices (A1, B1), . . . , (A3, B3) i.i.d. from normal distributions, and generate (A(t), B(t)) by using a randomly-initialized neural network with softmax output to weight the convex hull matrices. Episodes were run for 2 seconds at a discretization of 0.01 seconds. Synthetic H∞ setting. We generate H∞ control instances (A.11) with s = 5, a = 3, and d = 2 by generating matrices A, B and G i.i.d. from normal distributions. The disturbance w(t) was produced using a randomly-initialized neural network, with its output scaled to satisfy the L2 bound on the disturbance. Speciﬁcally, we scaled the output of the neural network to satisfy an attenuating norm- bound on the disturbance; at time t, the norm-bound was given by 20 × f (2 × t/T ), where T is the time horizon and f is the standard normal PDF function. Episodes were run for T = 2 seconds at a discretization of 0.01 seconds. Results are given in Figure I.1 and Table I.1. Environment LQR MBP PPO Robust LQR Robust MPC RARL Robust MBP ∗ Robust PPO ∗ Generic PLDI O 96.3 3.3 8.0 19.2 19.2 15.8 18.6 10.2 A ——— unstable ——— 43.3 44.1 unstable 21.9 16.1 Generic H∞ O 181 88 114 165 N/A 115 116 125 A 219 112 143 206 N/A 145 147 158 Table I.1: Performance of various approaches, both robust (right) and non-robust (left), on domains of interest. We report average quadratic loss over 50 episodes under the original dynamics (O) and under an adversarial disturbance (A). For the original dynamics (O), the best performance for both non-robust methods and robust methods is in bold (lower loss is better). We use “unstable” to indicate cases where the relevant method became unstable. Our robust methods (denoted by ∗) improve performance over Robust LQR in the average case, while remaining stable under adversarial dynamics, whereas the non-robust methods either went unstable or received much larger losses. 24 Published as a conference paper at ICLR 2021 100LossPLDI Non-robust Methods Robust Methods 0 250 500 750 1000 Training epochs 101LossH 0 250 500 750 1000 Training epochs Setting: MBP PPO RARL Robust MBP * Robust PPO * Original Adversarial Figure I.1: Representative results for our experimental settings. For each training epoch (10 updates for the MBP model and 18 for PPO), we report average quadratic loss over 50 episodes, and use “X” to indicate cases where the relevant method became unstable. (Lower loss is better.) Our robust methods (denoted by ∗) improve performance over Robust LQR in the average case, while (un- like the non-robust methods) remaining stable under adversarial dynamics throughout the training process. J NOTES ON LINEARIZATION VIA PLDIS AND NLDIS While we linearize the cart-pole and quadrotor dynamics via NLDIs in our experiments, we note that these dynamics can also be characterized via PLDIs. More generally, in this section, we show how we can use the framework of PLDIs to model linearization errors arising in the analysis of nonlinear systems. Consider the nonlinear dynamical system ˙x = f (x, u) with f (0, 0) = 0. (J.1) for x ∈ Rs and u ∈ Ra. Deﬁne ξ = (x, u). We would like to represent the above system as a PLDI in the region R := {ξ | ξ ≤ ξ ≤ ¯ξ} including the origin. The mean value theorem states that for each component of f , we can write fi(ξ) = fi(0) + ∇fi(z) T ξ, (J.2) for some z = tξ, where t ∈ [0, 1]. Now, let p = s + a. Deﬁning the Jacobian of f as Jf (z) =    ∇f1(z)T ... ∇fp(z) T    , (J.3) and recalling that f (0) = 0, we can rewrite (J.2) as f (ξ) = Jf (z)ξ. (J.4) Now, suppose we can ﬁnd component-wise bounds on the matrix Jf (z) over R, i.e, M ≤ Jf (z) ≤ ¯M for all z ∈ R. (J.5) We can then write Jf (z) = ∑ 1≤i,j≤p mij(t)Eij with mij(t) ∈ [mij, ¯mij], (J.6) where Eij = eieT j and ei is the i-th unit vector in Rp. 25 Published as a conference paper at ICLR 2021 We now seek to bound the Jacobian using polytopic bounds. To do this, note that we can write Jf (z) = 2p2 ∑ κ=1 γκAκ γκ ≥ 0, ∑ κ γκ = 1, (J.7) where Aκ’s are the vertices of the polytope in (J.6), i.e., Aκ ∈ V =    ∑ 1≤i,j≤p mijEij | mij ∈ {mij, ¯mij}    . (J.8) Together, Equations (J.2), (J.4), (J.7), and (J.8) characterize the original nonlinear dynamics as a PLDI. We note that this PLDI description is potentially very large; in particular, the size of V is exponential in the square of the number of non-constant entries in the Jacobian Jf (z), which could be as large as 2p2 = 2 (s+a) 2. This problem size may therefore become intractable for larger control settings. We note, however, that we can in fact express this PLDI more concisely as an NLDI. More precisely, we would like to ﬁnd matrices A, B, C parameterizing the form of NLDI below, which is equivalent to that presented in Equation (3) (see Chapter 4 of Boyd et al. (1994)): Df (z) ∈ {A + B∆C | ∥∆∥2 ≤ 1} for all z ∈ R. (J.9) It can shown that the solution to the SDP minimize tr(V + W ) subject to W ≻ 0 [ V (Aκ − A) T Aκ − A W ] ≽ 0, ∀Aκ ∈ V (J.10) yields the matrices A, B, and C with V = C T C and W = BBT , which can be used to construct NLDI (J.9). While the NLDI here is more concise than the PLDI, the trade-off is that the NLDI norm bounds obtained via this method may be rather loose. As such, for our settings, we obtain NLDI bounds numerically (see Appendices D and E), as these are tighter than NLDI speciﬁcations obtained via the above method (though they are potentially slightly inexact). An alternative approach would be to examine how to tighten the conversion from PLDIs to NLDIs, which has been explored in other work (e.g. Kuiava et al. (2013)). 26","libVersion":"0.3.2","langs":""}
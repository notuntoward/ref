{"path":"lit/lit_sources.backup/Piao24transFrmrFreqBiasFredFormer.pdf","text":"Fredformer: Frequency Debiased Transformer for Time Series Forecasting Xihao Piao* SANKEN, Osaka University Osaka, Japan park88@sanken.osaka-u.ac.jp Zheng Chen* SANKEN, Osaka University Osaka, Japan chenz@sanken.osaka-u.ac.jp Taichi Murayama SANKEN, Osaka University Osaka, Japan taichi@sanken.osaka-u.ac.jp Yasuko Matsubara SANKEN, Osaka University Osaka, Japan yasuko@sanken.osaka-u.ac.jp Yasushi Sakurai SANKEN, Osaka University Osaka, Japan yasushi@sanken.osaka-u.ac.jp ABSTRACT The Transformer model has shown leading performance in time se- ries forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high- frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across dif- ferent frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate fore- casting. Extensive experiments show the effectiveness of our pro- posed approach, which can outperform other baselines in differ- ent real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer CCS CONCEPTS â€¢ Computing methodologies â†’ Artificial intelligence; Neural networks. KEYWORDS Time series forecasting, Deep learning ACM Reference Format: Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Ya- sushi Sakurai. 2024. Fredformer: Frequency Debiased Transformer for * Indicates corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0490-1/24/08 https://doi.org/10.1145/3637528.3671928 Time Series Forecasting . In Proceedings of the 30th ACM SIGKDD Con- ference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“ 29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 18 pages. https: //doi.org/10.1145/3637528.3671928 1 INTRODUCTION Time series data are ubiquitous in everyday life. Forecasting time series could provide insights for decision-making support, such as potential traffic congestion [10] or changes in stock market trends [34]. Accurate forecasting typically involves discerning vari- ous informative temporal variations in historical observations, e.g., trends, seasonality, and fluctuations, which are consistent in fu- ture time series [42]. Benefiting from the advancements in deep learning, the community has seen great progress, particularly with Transformer-based methods [39, 41, 49]. Successful methods of- ten tokenize time series with multiresolution, such as time points [43] or sub-series [48], and model their dependencies leveraging the self-attention mechanism. Several state-of-the-art (SOTA) baselines have been proposed, namely PatchTST [29], Crossformer [48], and iTransformer [24], and demonstrate impressive performance. Despite their success, the effectiveness with which we can cap- ture informative temporal variations remains a concern. From a data perspective, a series of time observations is typically considered a complex set of signals or waves that varies over time [13, 17]. Vari- ous temporal variations, manifested as different frequency waves, such as low-frequency long-term periodicity or high-frequency fluctuation, often co-occur and are intermixed in the real world [19, 21, 42]. While tokenizing a time series may provide fine-grained information for the model, the temporal variations in resulting tokens or sub-series are also entangled. This issue may complicate the feature extraction and forecasting performance. Existing works have proposed frequency decomposition to represent the time se- ries and deployed Transformers on new representation to explicitly learn eventful frequency features [42, 43]. Learning often incorpo- rates feature selection strategies in the frequency domain, such as top-K or random-K [41, 50], to help Transformers better identify more relevant frequencies. However, such heuristic selection may introduce sub-optimal frequency correlations into the model (seen in Figure 1(a)), inadvertently misleading the learning process. From a model perspective, researchers have recently noticed a learning bias issue that is common in the Transformer. That is, the self- attention mechanism often prioritizes low-frequency features at thearXiv:2406.09009v4 [cs.LG] 3 Jul 2024 KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai (b) PatchTST (c) Ours(a) FEDformer FF A F Input Ground Truth Forecasting Figure 1: In contrast to a frequency modeling-based work FEDformer [50] and a SOTA work PatchTST [29], our model can accurately capture more significant mid-to-high frequency components. expense of high-frequency features [14, 30, 35, 36]. This subtle issue may also appear in time series forecasting, potentially biasing model outcomes and leading to information losses. Figure 1(b) shows an electricity case where the forecasting result successfully captures low-frequency features, neglecting some consistent mid-to-high frequencies. In practice, such high frequencies represent short- term variations, e.g., periodicities over short durations, which serve as good indicators for forecasting [10, 16, 34]. However, the low- frequencies typically carry a substantial portion of the energy in the spectrum and are dominant in time series. The amplitude of these low-frequency components far exceeds that of higher frequencies [51], which provides the Transformer with more observations. This may raise the possibility of frequency bias in time series forecasting, as the model might disproportionately learn from these dominant low-frequency components. This work explores one direction of capturing informative, com- plex variations by frequency domain modeling for accurate time se- ries forecasting. We introduce Fredformer, a Frequency-debiased Transformer model. Fredformer follows the line of frequency decomposition but further investigates how to facilitate the uses of Transformers in learning frequency features. To improve the effectiveness of our approach, we provide a comprehensive analy- sis of frequency bias in time series forecasting and a strategy for debiasing it. Our main contributions lie in three folds. - Problem definition. We undertake empirical studies to investi- gate how this bias is introduced into time series forecasting Trans- formers. We observe that the main cause is the proportional dif- ference between key frequency components. Notably, these key components should be consistent in the historical and ground truth of the forecasting. We also investigate the objective and key designs that affect debiasing. - Algorithmic design. Our Fredformer has three pivotal compo- nents: patching for the frequency band, sub-frequency-independent normalization to mitigate proportional differences, and channel- wise attention within each sub-frequency band for fairness learning of all frequencies and attention debiasing. - Applicability. Fredformer undertakes NystrÃ¶m approximation to reduce the computational complexity of the attention maps, thus achieving a lightweight model with competitive performance. This opens new opportunities for efficient time series forecasting. Remark. This is the first paper to study the frequency bias issue in time series forecasting. Extensive experimental results on eight datasets show the effectiveness of Fredformer, which achieves superior performance with 60 top-1 and 20 top-2 cases out of 80. 2 PRELIMINARY ANALYSIS We present two cases to show ( i ) how frequency attributes of time series data introduce bias into forecasting with the Transformer model and ( ii) an empirical analysis of the potential debiasing strategy. This section introduces the notation and a metric for the case studies in Sec. 2.1. The case analyses are detailed in Sec. 2.2. 2.1 Preliminary Time Series Forecasting. Let X = {ğ’™ (ğ‘ ) 1 , . . . , ğ’™ (ğ‘ ) ğ¿ } ğ¶ ğ‘=1 denote a multivariate time series consisting of ğ¶ channels, where each channel records an independent ğ¿ length look-back window. For simplicity, we omit channel index ğ‘ in subsequent discussions. The forecasting task is to predict ğ» time steps in the future data Ë†X: Ë†Xğ¿+1:ğ¿+ğ» = ğ‘“ (X1:ğ¿) where ğ‘“ (Â·) denotes the forecasting function, which is a Transformer- based model in this work. Our objective is to mitigate the learning bias in the Transformer and enhance the forecasting outcome Xâ€², that is, to minimize the error between Xâ€² and Ë†X. Discrete Fourier Transform (DFT). We use DFT to analyze the frequency content of X, Ë†X, and Xâ€². For example, given the input sequence {ğ’™1, ..., ğ’™ğ¿ }, the DFT can be formulated as ğ’‚ğ‘˜ = 1 ğ¿ ğ¿âˆ‘ï¸ ğ‘™=1 ğ’™ğ‘™ Â· ğ‘“ğ‘˜, ğ‘˜ = 1, . . . , ğ¿ where ğ‘“ğ‘˜ = ğ‘’ âˆ’ğ‘–2ğœ‹ğ‘˜/ğ¿ denotes the ğ‘˜-th frequency component. The DFT coefficients A = {ğ’‚1, ğ’‚2, . . . , ğ’‚ğ¿ } represent the amplitude in- formation of these frequencies. As illustrated in Figure 2 (b, left), four components are observed to have higher amplitudes in the historical observations (X) and the forecasting data ( Ë†X). We refer Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Relative Error 0.9 0.1 Ground TruthInput Forecasting (b) Time domain model + Non-normalization Time domain model + Normalization Frequency domain + Normalization F F F (a) #epoch k2 k1 k3 1 50 #epoch1 50 k2 k1 k3 F k2 k1 k3 k2 k1 k3 F Figure 2: Figure (a) shows the learning dynamics and results for two synthetic datasets, employing line graphs to illus- trate amplitudes in the frequency domain and heatmaps to represent training epoch errors. Figure (b) explores the in- fluence of amplitude and domain on learning by comparing Transformers in the time and frequency domains, both with and without frequency local normalization. to such consistent components as â€™key componentsâ€™ (defined in Sec. 3.1). Here, the inverse DFT (i.e., IDFT) is ğ’™ğ‘™ = Ãğ¿ ğ‘˜=1 ğ’‚ğ‘˜ Â· ğ‘“ âˆ’1 ğ‘˜ , which reconstructs the time series data from the DFT coefficients. Frequency Bias Metric. Inspired by the work of [5, 46], this study employs a Fourier analytic metric of relative error Î”ğ‘˜ to determine the frequency bias. Given the model outputs Aâ€² and the ground truth Ë†A, the mean-square error (MSE) for the ğ‘˜-th component is calculated as follows: MSEğ‘˜ = |ğ’‚â€²ğ‘˜ âˆ’ Ë†ğ’‚ğ‘˜ |, where | Â· | denotes the L2 norm of a complex number. Then, the relative error is applied to mitigate scale differences. In other words, the error may become larger as the proportion of amplitude increases. Î”ğ‘˜ = |ğ’‚â€²ğ‘˜ âˆ’ Ë†ğ’‚ğ‘˜ |/| Ë†ğ’‚ğ‘˜ | This metric is used in case study analyses and the experiments detailed in Section 5.2. 2.2 Case Studies We first generate single-channel time series data with a total length of 10000 timestamps and then employ a Transformer model [29] to forecast the data. The details are in Appendix A. For the first case study (Case 1), we generate two datasets with three key fre- quency components ({ğ‘˜1, ğ‘˜2, ğ‘˜3}). Each dataset contains a different proportion of these three components, as illustrated in the DFT visualization in Figure 2. On the left side of the figure, their ampli- tudes are arranged as ğ’‚ğ‘˜1 < ğ’‚ğ‘˜2 < ğ’‚ğ‘˜3, whereas on the right side, the arrangement is ğ’‚ğ‘˜1 > ğ’‚ğ‘˜2 > ğ’‚ğ‘˜3. We maintain these propor- tions so that they are consistent between the observed A and the ground truth Ë†A (i.e., Aâ‰ˆ Ë†A). Then, we assess the bias for different ğ‘˜ in the Transformer outputs Aâ€². Meanwhile, we track how Î”ğ‘˜ changes during the model training to show the learning bias, using heatmap values to represent the numerical values of Î”ğ‘˜ . Here, we generate a dataset with four key frequency components for the second case study (Case 2). This study analyzes different modeling strategies to investigate their flexibility for debiasing. 2.2.1 Investigating the Frequency Bias of Transformer (Case 1). As shown in Figure 2(a) (left), after 50 epochs of training, the model successfully captures the amplitude of low-frequency com- ponent ğ‘˜1 but fails to capture ğ‘˜2 and ğ‘˜3. Meanwhile, the heatmap values show that the model predominantly focuses on learning the ğ‘˜1 component. In other words, the relative error decreases to around 0.01 (red codes) during the training. But, it lacks opti- mization for ğ‘˜3, resulting in a high relative error of almost 0.95. These observations indicate that signals in the time domain can be represented by a series of frequency waves, typically dominated by low-frequency components [19, 27, 31]. When the Transformer is deployed on this mixed-frequency collection, the dominant propor- tion of frequencies experiences a learning bias. A similar result is also evident in the control experiment in the right Subfigure. Here, we introduce synthetic data with higher amplitudes in the mid and high-frequency ranges (resulting in ğ’‚ğ‘˜1 < ğ’‚ğ‘˜2 < ğ’‚ğ‘˜3). In response, the model shifts its focus towards the key component ğ‘˜3, leading to Î”ğ‘˜1 > Î”ğ‘˜2 > Î”ğ‘˜3. This learning bias aligns with recent theoretical analyses of the Transformer model [30, 35, 36]. In addition, Sec. 3.1 provides a formal definition of this frequency bias. 2.2.2 Debiasing the Frequency Learning for Transformer (Case 2). Based on the above discussion, we initially use the same experimental settings for a new dataset, as shown in Figure 2(b) (Left). We then perform two feasibility analyses for debiasing by (1) mitigating the influence of high proportionality and (2) providing the transformer with fine-grained frequency information. (1) Frequency normalization: We first decompose the frequency domain and normalize the amplitudes of the frequencies to elimi- nate their proportional differences. Specifically, we apply the DFT, normalize the amplitudes, and then use the IDFT to convert the fre- quency representation back into the time domain before inputting it into the Transformer, formulated as Xâ€² = (IDFT(Ağ‘›ğ‘œğ‘Ÿğ‘š)). As depicted in Figure 2(b) (middle and right), the four input components are adjusted so that they have the same amplitude value, shown by a blue dashed line. The middle subfigure shows that frequency normalization enhances the forecasting performance for the latter three frequencies, but relative errors remain high. (2) Frequency domain modeling: We further directly deploy the Transformer on the frequency domain to model the DFT matrix. Subsequently, we apply the IDFT to return the forecasting outcome to the time domain. Here, the purpose is to provide the transformer with more refined and disentangled frequency features. Formally, Xâ€² = IDFT((Ağ‘›ğ‘œğ‘Ÿğ‘š)). As shown in Figure 2(b) (right), there is a marked improvement in forecasting accuracy for the latter three KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai frequency components. Notably, the bias in the second frequency component (60-75 Hz) is effectively eliminated. These findings sug- gest the potential for direct frequency domain modeling with proportion mitigation in achieving the debiasing. 3 FREQUENCY BIAS FORMULATION This section defines the frequency bias in Sec.3.1, then describes the research problem in Sec.3.2. 3.1 Frequency Bias Definitions Given the aforementioned empirical analyses, which demonstrate that a frequency bias exists in key frequency components, we first define these key components in terms of two properties: 1) a key component should have a relatively high amplitude within the spectrum, and 2) it should be consistent in historical observations and future time series, as well as robust to time shifts [4, 31]. Definition 1. Key Frequency Components. Given a frequency spectrum A with length ğ¿, A can be segmented into ğ‘ sub-frequency bands {ğ’˜1, ğ’˜2, . . . , ğ’˜ğ‘ } by a sliding window, where ğ’˜ğ‘› âˆˆ R1Ã—ğ‘  . The maximum amplitude in the ğ‘›-th window is determined as follows: max(ğ’˜ğ‘›) = max |ğ’‚ğ‘˜ | : ğ’‚ğ‘˜ âˆˆ ğ’˜ğ‘› for ğ‘› = 1, 2, . . . , ğ‘ (1) where ğ’˜ğ‘› denotes ğ‘  amplitudes in the ğ‘›-th window. If ğ’‚ğ‘˜ is a key component in the ğ‘–-th window, then: ğ’‚ğ‘˜ = max(ğ’˜ğ‘›) and Ë†ğ’‚ğ‘˜ = max( Ë†ğ’˜ğ‘›) ËœA is a collection of all key components. Notably, ËœA should be present in historical A and ground truth Ë†A for accurate forecasting. Definition 2. Frequency Bias in Transformer. Given that a time series X contains ğ‘ key frequency components amplitudes ËœA = { Ëœğ’‚1, . . . , Ëœğ’‚ğ‘ }, for the ğ‘˜-th component Ëœğ’‚ğ‘˜ âˆˆ ËœA, we have ğ‘ƒ ( Ëœğ’‚ğ‘˜ ) = | Ëœğ’‚ğ‘˜ | Ãğ‘ ğ‘›=1 | Ëœğ’‚ğ‘› | , which refers to the proportion of Ëœğ’‚ğ‘˜ in the total sum of amplitudes of ËœA. Frequency bias can be defined as relative error Î”ğ‘˜ . Here, a larger proportion ğ‘ƒ ( Ëœğ’‚ğ‘˜ ) leads to a smaller Î”ğ‘˜ and exhibits a higher ranking: âˆ’|Î”ğ‘˜ | âˆ ğ‘ƒ ( Ëœğ’‚ğ‘˜ ) (2) Eventually, the Transformer pays more attention to high-ranked components during the training, as seen in Figure 2 (a) heatmaps. 3.2 Problem Statement Based on the discussions in Sec. 2, we argue that if the Transformer assigns attention to all key frequency components ËœA equally during learning, then the frequency bias could be mitigated. Problem 1. Debiasing Frequency Learning for Transformer. Given a Transformer output ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  (ğ‘‹ ), where ğ‘‹ contains several key frequency component Ëœğ’‚ğ‘˜ , our goal is to debias ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  and improve performance by making the relative error Î” Ëœğ’‚ğ‘˜ independent of ğ‘ƒ ( Ëœğ’‚ğ‘˜ ): âˆ’|Î”ğ‘˜ | Ì¸âˆ ğ‘ƒ ( Ëœğ’‚ğ‘˜ ) (3) thereby ensuring a balanced response by the Transformer to different key frequency components. 4 FREDFORMER Here, we discuss how to tackle the problem formulated in Sec.3.2 and propose Fredformer, a Frequency debiased Transformer model for accurate time series forecasting. Architecture Overview. Fredformer consists of four principal components: ( i ) a DFT-to-IDFT backbone, ( ii ) frequency domain refinement, ( iii ) local frequency independent learning, and ( iv ) global semantic frequency summarization. Figure 3 shows an ar- chitectural overview. The DFT-to-IDFT backbone breaks down the input time series X into its frequency components using DFT and learns a debiased representation of key frequency components by modules ( ii ) ( iii )and ( iv ). Based on the discussion in Sec. 2.2.2 (2), where we noted the significant potential of frequency modeling for debiasing, we first refine the overall frequency spectrum into sub-frequencies, which we achieve through a patching operation on DFT coefficients. Patches from different channels within the same sub-frequency band are embedded as tokens. That is, each sub-frequency band is encoded independently, which avoids the influence of other frequency components, as discussed in Section 2.2.2 (1). We deploy the Transformer to extract local frequency features for each sub-band across all channels. This mitigates the higher proportion crux defined in Def. 2. Finally, we summarize all the frequency information, which serves as IDFT for forecasting. A detailed workflow of Fredformer is in Appendix C. Below, we provide a description of each module. 4.1 Backbone Given X, we first use DFT to decompose X into frequency coeffi- cients A1 for all channels. We then extract the debiased frequency features by using a Transformer encoder to A âˆˆ Rğ¶ Ã—ğ¿. The fre- quency outputs are subsequently reconstructed to the time domain signal Xâ€² by IDFT. Xâ€² = IDFT(ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  (A)), A = DFT(X)) 4.2 Frequency Refinement and Normalization From the observations described in Sec. 2.2.2, we conclude that if there are significant proportional differences between different ËœAğ‘˜ values in the input data, it will lead to the model overly focusing on components with larger amplitudes. To address this issue, we propose frequency refinement and normalization. Specifically, a non-overlapping patching operation is applied to A along the ğ¶- axis (i.e., channel), resulting in a sequence of local sub-frequencies as follows: W = {W1, W2, . . . , Wğ‘ } = Patching(A), Wğ‘› âˆˆ R ğ¶ Ã—ğ‘† where ğ‘ is the total number of the patches, while ğ‘† represents the length of each patch. Mitigating information redundancy over fine-grained frequency bands, such as neighboring 1 Hz and 2 Hz, allows the model to learn the local features in each sub-frequency. Parameter ğ‘† is adaptable to the requirements of real-world scenarios, for example, an hourly sampling of daily recordings or the alpha waveform typically occurring at 8-12 Hz [8]. 1A consists of two coefficient matrices: a real part R âˆˆ Rğ¶ Ã—ğ¿ and an imaginary matrix I âˆˆ Rğ¶ Ã—ğ¿. Since all operations are conducted synchronously for these two matrices, we will refer to them as A in our subsequent discussions. Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain DFT IDFT Frequency Domain Modeling Transformer Encoder Frequency Summarization Frequency Channel-Wise Attention Norm Linear Linear Figure 3: Overview of our framework. Fredformer employs DFT to transform input sequences into the frequency do- main, normalizes locally, and segments into patches before employing channel-wise attention, yielding final predictions through a frequency-wise summarizing layer and IDFT. Since patching operation allows the model to manage each Mğ‘› independently, we further normalize each Wğ‘› along the ğ‘ -axis: W âˆ— ğ‘› = ğœ (Wğ‘›) ğ‘› = 1, 2, . . . , ğ‘ where ğœ (Â·) denotes the normalization, and it further projects the numerical value of each ËœAğ‘˜ into a range of 0-1. This operation eliminates proportionate differences in the maximum values within sub-frequency bands, thereby maintaining an equal Î” across all key components ËœA. Lemma 1. Frequency-wise Local Normalization: Given fre- quency patches âˆ€ Wğ‘›, Wğ‘š âˆˆ W for max(Wğ‘›) > max(Wğ‘š) and ğœ (Â·), the normalization strategy is defined by: Wâˆ— = {ğœ (W1), . . . , ğœ (Wğ‘ )} This ensures that within each localized frequency patch Wğ‘›, the am- plitude differences between key frequency components are minimized, promoting equal attention to all key frequencies by the model: max(W âˆ— ğ‘›) = max(Wâˆ— ğ‘š) Some studies also introduce patching operations in the time do- main and perform normalization within these time domain patches [29]. However, according to Parsevalâ€™s theorem [32], normalization within time domain patches is equivalent to normalizing across all frequencies. This could not address the issue of amplitude bias among key frequency components. A more detailed description can be found in Appendix G. 4.3 Frequency Local Independent Modeling Given the normalized Wâˆ—, we deploy frequency local independent Transformer encoders to learn the importance of each Wâˆ— ğ‘› inde- pendently. For W(1:ğ¶ ) ğ‘› = {ğ’˜ (1) ğ‘› , ğ’˜ (2) ğ‘› , . . . , ğ’˜ (ğ¶ ) ğ‘› }ğ‘ ğ‘›=1, a Transformer encoder ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  (Â·) accepts each ğ’˜âˆ— (ğ‘ ) ğ‘› as an input token: {Wâ€² (1:ğ¶ ) ğ‘› } = ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  (W âˆ— (1:ğ¶ ) ğ‘› ) where Wâ€² (1:ğ¶ ) ğ‘› is encoded by a channel-wise self-attention encoder: Attention(Qğ‘›, Kğ‘›, Vğ‘›) = Softmax ( Wâˆ— (1:ğ¶ ) ğ‘› W ğ‘ ğ‘› (Wâˆ— (1:ğ¶ ) ğ‘› Wğ‘˜ ğ‘›)ğ‘‡ âˆš ğ‘‘ ) W âˆ— (1:ğ¶ ) ğ‘› Wğ‘£ ğ‘› where W ğ‘ ğ‘›, Wğ‘˜ ğ‘›, Wğ‘£ ğ‘› âˆˆ Rğ‘† Ã—ğ‘€ are the weight matrices for generat- ing the query matrix Qğ‘›, key matrix Kğ‘›, and value matrix Vğ‘›. âˆš ğ‘‘ denotes a scaling operation. The attention module also includes normalization and a feed-forward layer with residual connections [12], and Attention(Qğ‘›, Kğ‘›, Vğ‘›) âˆˆ Rğ¶ Ã—ğ‘€ weights the correlations among ğ¶ channels for the ğ‘›-th sub-frequency band Mğ‘›. This de- sign ensures that the features of each sub-frequency are calculated independently, preventing learning bias. Lemma 2. Given Wâˆ— (1:ğ¶ ) ğ‘› = {ğ’˜âˆ—(1) ğ‘› , ğ’˜âˆ—(2) ğ‘› , . . . , ğ’˜âˆ—(ğ¶ ) ğ‘› }ğ‘ ğ‘›=1, if Wâ€²ğ‘› = ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  (Wâˆ— (1:ğ¶ ) ğ‘› ), then by modeling the relationships of iden- tical frequencies ğ’˜ğ‘ ğ‘› across different channels, for the ğ‘˜-th key com- ponent Ëœğ’‚ğ‘˜ presents in ğ’˜ (ğ‘ ) ğ‘› , we have âˆ’|Î”(ğ‘ ) ğ‘˜ | âˆ {|Î”(ğ‘ ) ğ‘˜ |}ğ¶ ğ‘=1. The Transformer encoders will focus on channel-wise correlations instead of the {|Î”(ğ‘ ) ğ‘˜ |}ğ¾ ğ‘˜=1, i.e., debiasing âˆ’|Î”(ğ‘ ) ğ‘˜ | Ì¸âˆ ğ‘ƒ ( Ëœğ’‚ğ‘˜ ). Lemma 2, which indicates a lower ğ‘ƒ ( Ëœğ’‚ğ‘˜ ) does not necessarily lead to an increase in |Î” Ëœğ’‚ğ‘˜ |, thus avoiding disproportionate attention to frequency components. Channel-wise attention is proposed in the work of [24, 48]. We include these studies as the baselines and the results in Sec. 5.2. This work has different modeling purposes: we deploy self-attention on the aligned local features, i.e., in the same frequency bands across channels, for frequency debiasing. 4.4 Frequency-wise Summarization Given the learned features of the sub-frequencies Wâ€² = {ğ’˜â€² 1, ğ’˜â€² 2, . . . , ğ’˜â€² ğ‘ } of the historical time series X, the frequency-wise summarizing operation contains linear projections and IDFT: Xâ€² = IDFT(Aâ€²) Aâ€² = Linear(Wâ€²) where Xâ€² âˆˆ Rğ¶ Ã—ğ» is the final output of the framework. 5 EXPERIMENTS 5.1 PROTOCOLS - Datasets. We conduct extensive experiments on eight real-world benchmark datasets: Weather, four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), Electricity (ECL), Traffic, and the Solar-Energy dataset [19], with all datasets being published in [24]2. The infor- mation these datasets provide is summarized in Table 3. And the full results of four selected datasets* will be shown in Figure 2. and further details are available in Appendix B. - Baselines. We select 11 SOTA baseline studies. Since we are focus- ing on Transformer, we first add seven proposed Transformer-based baselines, including iTransformer [24], PatchTST [29], Crossformer [48], Stationary [25], Fedformer [50], Pyraformer [23], Autoformer 2https://github.com/thuml/iTransformer KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai Table 1: Multivariate forecasting results with prediction lengths ğ‘† âˆˆ {96, 192, 336, 720} for all datasets and fixed look-back length ğ‘‡ = 96. The best and second best results are highlighted. The full results of four selected datasets* will be shown in Figure 2. Results are averaged from all prediction lengths. Full results for all datasets are listed in Appendix I. Models Fredformer iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear SCINet FEDformer Stationary Autoformer (Ours) ([24]) [22] [29] [48] [11] [42] [47] [26] [50] [25] [43] Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ECL 0.175 0.269 0.178 0.270 0.219 0.298 0.216 0.304 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338 ETTh1 0.435 0.426 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.548 0.450 0.456 0.452 0.747 0.647 0.440 0.460 0.570 0.537 0.496 0.487 ETTh2* 0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459 ETTm1* 0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517 ETTm2 0.279 0.324 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 0.571 0.537 0.305 0.349 0.306 0.347 0.327 0.371 Traffic 0.431 0.287 0.428 0.282 0.626 0.378 0.555 0.362 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379 Weather* 0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382 Solar-Energy* 0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711 Table 2: Full results of four selected datasets, with the best and second best results are highlighted. We compare extensive competitive models under different prediction lengths following the setting of iTransformer [24]. The input sequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths. Models Fredformer iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear SCINet FEDformer Stationary Autoformer (Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021] Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEETTh296 0.293 0.342 0.297 0.349 0.288 0.338 0.302 0.348 0.745 0.584 0.400 0.440 0.340 0.374 0.333 0.387 0.707 0.621 0.358 0.397 0.476 0.458 0.346 0.388 192 0.371 0.389 0.380 0.400 0.374 0.390 0.388 0.400 0.877 0.656 0.528 0.509 0.402 0.414 0.477 0.476 0.860 0.689 0.429 0.439 0.512 0.493 0.456 0.452 336 0.382 0.409 0.428 0.432 0.415 0.426 0.426 0.433 1.043 0.731 0.643 0.571 0.452 0.452 0.594 0.541 1.000 0.744 0.496 0.487 0.552 0.551 0.482 0.486 720 0.415 0.434 0.427 0.445 0.420 0.440 0.431 0.446 1.104 0.763 0.874 0.679 0.462 0.468 0.831 0.657 1.249 0.838 0.463 0.474 0.562 0.560 0.515 0.511 Avg 0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459ETTm196 0.326 0.361 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475 192 0.363 0.380 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496 336 0.395 0.403 0.426 0.420 0.424 0.415 0.399 0.410 0.532 0.515 0.428 0.425 0.410 0.411 0.413 0.413 0.490 0.485 0.445 0.459 0.495 0.464 0.621 0.537 720 0.453 0.438 0.491 0.459 0.487 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.478 0.450 0.474 0.453 0.595 0.550 0.543 0.490 0.585 0.516 0.671 0.561 Avg 0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517Weather96 0.163 0.207 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336 192 0.211 0.251 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367 336 0.267 0.292 0.278 0.296 0.292 0.307 0.278 0.297 0.272 0.335 0.287 0.335 0.280 0.306 0.283 0.335 0.309 0.378 0.339 0.380 0.321 0.338 0.359 0.395 720 0.343 0.341 0.358 0.349 0.364 0.353 0.354 0.348 0.398 0.418 0.351 0.386 0.365 0.359 0.345 0.381 0.377 0.427 0.403 0.428 0.414 0.410 0.419 0.428 Avg 0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382Solar-Energy96 0.185 0.233 0.203 0.237 0.322 0.339 0.234 0.286 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711 192 0.227 0.253 0.233 0.261 0.359 0.356 0.267 0.310 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692 336 0.246 0.284 0.248 0.273 0.397 0.369 0.290 0.315 0.750 0.735 0.368 0.430 0.319 0.330 0.353 0.415 0.304 0.389 0.282 0.376 0.290 0.296 0.941 0.723 720 0.247 0.276 0.249 0.275 0.397 0.356 0.289 0.317 0.769 0.765 0.370 0.425 0.338 0.337 0.356 0.413 0.308 0.388 0.357 0.427 0.285 0.295 0.882 0.717 Avg 0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711 1st Count 17 17 0 2 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Table 3: Benchmark dataset summary Datasets Weather Electricity ETTh1 ETTh2 ETTm1 ETTm2 Solar Traffic #Channel 21 321 7 7 7 7 137 862 #Timesteps 52969 26304 17420 17420 69680 69680 52179 17544 [43]. We also add 2 MLP-based and 2 TCN-based methods, including RLinear [22], DLinear [47], TiDE [11], TimesNet [42]. Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Table 4: The average forecasting accuracy (MSE) on ETTh1 dataset under 4 patch length settings. Patch length 8 16 32 Non MSE 0.417 0.425 0.440 0.449 Table 5: Averaged results for each setting in the ablation study. \"No-CW\" refers to removing channel-wise attention, and \"No-FR\" refers to removing frequency refinement. Setting Full No-CW No-FR MSE MAE MSE MAE MSE MAE ETTm1 0.384 0.396 0.418 0.419 0.539 0.485 Weather 0.246 0.273 0.262 0.290 0.293 0.322 - Setup and Evaluation. All baselines use the same prediction length with ğ» âˆˆ {96, 192, 336, 720} for all datasets. The look-back window ğ¿ = 96 was used in our setting for fair comparisons, refer- ring to [24, 50]. We used MSE and MAE as the forecasting metrics. We further analyzed the forecasting results between the model outputs and the ground truth in the time and frequency domains. Using heatmaps, we tracked the way in which Î”ğ‘˜ changes during training to show the debiased results of Fredformer compared with various SOTA baselines. 5.2 Results Forecasting Results. Table 1 shows the average forecasting per- formance across four prediction lengths. The best results are high- lighted in red, and the second-best in blue. With a default look-back window of ğ¿ = 96, our approach realizes leading performance levels on most datasets, securing 14 top-1 and 2 top-2 positions across two metrics over eight datasets. More detailed results for 4 of the eight datasets are shown in Table 2, where our method achieves 34 top-1 and 6 top-2 rankings out of 40 possible outcomes across the four prediction lengths. More comprehensive results regarding the different prediction length settings on all datasets and the impact of extending the look-back window are detailed in Appendix D and I. Frequency Bias Evaluation. Figure 4 is a case study visualization in the frequency domain, i.e., the DFT plot. The input, forecast out- put, and ground truth data series are shown in blue, red, and green, respectively. Similar to Section 2.2, the heat map shows the relative error for four selected mid-to-high frequency components over increasing epochs. After training, Fredformer accurately identifies ğ‘˜1, ğ‘˜2, and ğ‘˜3, with uniformly decreasing relative errors. Despite a larger learning error for ğ‘˜4, Î”ğ‘˜4 consistently diminishes. This performance contrasts with all the baselines, demonstrating a lack of effectiveness in capturing these frequency components, with unequal reductions in relative errors. In contrast, PatchTST demon- strates a sudden improvement in component accuracy (ğ‘˜2,ğ‘˜3) dur- ing the final stages of training. FEDformer fails to capture these frequency components, possibly because its strategy of selecting and learning weights for only a random set of ğ‘˜ components over- looks all unselected components. Notably, iTransformer overlooks mid-to-high frequency features, partially learning components ğ‘˜1 Method FEDformer PatchTST Crossformer iTransformer Ours Ours*(NystrÃ¶m) Complexity ğ‘‚ (ğ¿ğ¶) ğ‘‚ ( ğ¿2 ğ‘ƒ 2 ğ¶) ğ‘‚ ( ğ¿2 ğ‘ƒ 2 ğ¶) ğ‘‚ (ğ¶2) ğ‘‚ ( ğ¿ ğ‘ƒ ğ¶2) ğ‘‚ ( ğ¿ ğ‘ƒ ğ¶) Table 6: The theoretical computational complexity of Transformer-based methods. and ğ‘˜3 while ignoring ğ‘˜2 and ğ‘˜4, indicating a clear frequency bias. This may stem from its use of channel-wise attention alongside global normalization in the time domain, as discussed in Lemma 1 and further supported by our ablation study 5.3. This highlights the effectiveness of frequency refinement and normalization. 5.3 Ablation Study Channel-wise Attention and Frequency Refinement. We eval- uate the effectiveness of channel-wise attention and frequency refinement. To this end, we remove each component by ablation and compare it with the original Fredformer. Table 5 shows that our method consistently outperforms others in all experiments, highlighting the importance of integrating channel-wise attention with frequency local normalization in our design. Interestingly, em- ploying frequency local normalization alone yields better accuracy than channel-wise attention alone. This suggests that minimizing proportional differences in amplitudes across various key frequency components is crucial for enhancing accuracy. Effect of Patch Length. This ablation evaluates the impact of patch length using the ETTh1 dataset. We conduct four experi- ments with ğ‘† = [8, 16, 32, 48] patch lengths and corresponding patch numbers ğ‘ = [6, 3, 2, 1]. In this context, ğ‘ = 1 means fre- quency normalization and channel-wise attention are applied to the entire spectrum without a patching operation. Table 4 shows the forecasting accuracy for each setting. As the patch length increases, the granularity of the frequency features extracted by the model becomes coarser, decreasing forecasting accuracy. 5.4 Discussion of Applicability Beyond algorithmic considerations, we further discuss the practi- cal deployment of Fredformer in real-world scenarios, with the primary challenge being memory consumption during model train- ing. The ğ‘‚ (ğ‘›2) complexity of self-attention limits the use of longer historical time series for forecasting, generating the need for in- novations to reduce computational demands [21, 29, 49]. Through patching operations, we decrease the complexity from ğ‘‚ (ğ¿ğ¶2) to ğ‘‚ ( ğ¿ ğ‘ƒ ğ¶2). However, our channel-wise attention increases the com- putational costs with the number of channels, potentially limiting practical applicability with many channels. To address this, we pro- pose a lightweight Fredformer, inspired by NystrÃ¶mFormer [45], which applies a matrix approximation to the attention map. This de- sign allows us to further reduce our complexity to ğ‘‚ ( ğ¿ ğ‘ƒ ğ¶) without the need to modify the feature extraction (attention computation) or the data stream structure within the Transformer, unlike with previous methods [23, 43, 49, 50]. Figure 5 shows a tradeoff between KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai FEDformer #epoch1 50 F65 1150 k1 k2 k3 k4 k1 k2 k3 k4 PatchTST #epoch1 50 F65 1150 k1 k2 k3 k4 k1 k2 k3 k4 iTransformer #epoch1 50 k1 k2 k3 k4 F65 1150 k1 k2 k3 k4 Ours (Fredformer) k1 k2 k3 k4 #epoch1 50 650 F115 k1 k2 k3 k4 Input Ground Truth Forecasting Relative Error 0.9 0.1 Figure 4: Visualizations of the learning dynamics and results for Fredformer and baselines on the ETTh1 dataset, employing line graphs to illustrate amplitudes in the frequency domain and heatmaps to represent training epoch errors. ECLETTh1 MSE MSE Mb Mb iTransformer PatchTST Fedformer Ours Crossformer Ours* iTransformer PatchTST Crossformer Fedformer Ours Figure 5: This figure compares prediction accuracy and com- putational complexity (VRAM usage) among Transformer- based methods, Fredformer (Ours), and its optimized variant, NystrÃ¶m-Fredformer (Ours*). the model efficiency (VRAM usage) and accuracy in our method and the baselines. The plain Fredformer achieves high accuracy with low computational costs with fewer channels, such as ETTh1 with 7 channels. However, as shown in the ECL dataset (321 channels), the computational costs increase while maintaining high accuracy as the channel number increases. Here, NystrÃ¶m-Fredformer fur- ther reduces computational requirements without compromising accuracy (the right sub-figure), showing that our model can realize computational efficiency and forecasting accuracy. Further details and derivations are provided in Appendix H. 6 RELATED WORKS Transformer for Time Series Forecasting. Forecasting is impor- tant in time series analysis [1, 15]. Transformer has significantly progressed in time series forecasting[18, 29, 48]. Earlier attempts focused on improving the computational efficiency of Transform- ers for time series forecasting tasks[3, 23, 49]. Several studies have used Transformers to model inherent temporal dependencies in the time domain of time series[21, 23, 24, 29, 49]. Various studies have integrated frequency decomposition and spectrum analysis with the Transformer in modeling temporal variations [41, 43] to improve the capacity for temporal-spatial representation. In [50], attention layers are designed that directly function in the frequency domain to enhance spatial or frequency representation. Modeling Short-Term Variation in Time Series. Short-term variations are intrinsic characteristics of time series data and play a crucial role in effective forecasting [10, 25]. Numerous deep learning-based methods have been proposed to capture these tran- sient patterns [2, 7, 9, 28, 33, 37, 38, 40, 43]. Here, we summarize some studies closely aligned with our proposed method. Pyraformer [23] applies a pyramidal attention module with inter-scale and intra- scale connections to capture various temporal dependencies. FED- former [50] incorporates a Fourier spectrum within the attention computation to identify pivotal frequency components. Beyond Transformers, TimesNet [42] employs Inception blocks to capture intra-period and inter-period variations. Channel-wise Correlation. Understanding the cross-channel cor- relation is also critical for time series forecasting. Several studies aimed to capture intra-channel temporal variations and model the inter-channel correlations using Graph Neural Networks (GNNs) [6, 44]. Recently, Crossformer [48] and iTransformer [24] both adopted channel-wise Transformer-based frameworks, and exten- sive experimental results have demonstrated the effectiveness of channel-wise attention for time series forecasting. 7 CONCLUSION In this paper, we first empirically analyzed frequency bias, delving into its causes and exploring debiasing strategies. We then provided a formulation of this bias based on our analytical insights. We pro- posed the Fredformer framework with three critical designs to tackle this bias and thus ensure unbiased learning across frequency bands. Our extensive experiments across eight datasets confirmed the excellent performance of our proposed method. Visual anal- ysis confirmed that our approach effectively mitigates frequency bias. The model analysis further illustrated how our designs aid frequency debiasing and offered preliminary guidelines for future model design. Additionally, a lightweight variant of our model ad- dresses computational efficiency, facilitating practical application. Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain 8 ACKNOWLEDGMENTS We thank anonymous reviewers for their insightful comments and discussions. This work is supported by JSPS KAKENHI Grant- in-Aid for Scientific Research Number JP21H03446, JP23K16889, JP24K20778, NICT JPJ012368C03501, JST-AIP JPMJCR21U4, JST- CREST JPMJCR23M3, JST-RISTEX JPMJRS23L4. REFERENCES [1] Rob J. Hyndman Alysha M. De Livera and Ralph D. Snyder. 2011. Forecasting Time Series With Complex Seasonal Patterns Using Exponential Smoothing. J. Amer. Statist. Assoc. (2011), 1513â€“1527. [2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. Convolutional Sequence Modeling Revisited. (2018). [3] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long- Document Transformer. (2020). arXiv:2004.05150 [cs.CL] [4] S.A. Broughton and K. Bryan. 2011. Discrete Fourier Analysis and Wavelets: Applications to Signal and Image Processing. (2011). [5] Daniela Calvetti. 1991. A Stochastic Roundoff Error Analysis for the Fast Fourier Transform. (1991). [6] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Conguri Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2021. Spectral Tem- poral Graph Neural Network for Multivariate Time-series Forecasting. (2021). [7] Yen-Yu Chang, Fan-Yun Sun, Yueh-Hua Wu, and Shou-De Lin. 2018. A Memory- Network Based Solution for Multivariate Time-Series Forecasting. (2018). arXiv:1809.02105 [cs.LG] [8] Zheng Chen, Ziwei Yang, Lingwei Zhu, Wei Chen, Toshiyo Tamura, Naoaki Ono, Md Altaf-Ul-Amin, Shigehiko Kanaya, and Ming Huang. 2023. Automated Sleep Staging via Parallel Frequency-Cut Attention. IEEE Transactions on Neural Systems and Rehabilitation Engineering (2023), 1974â€“1985. [9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. (2014). arXiv:1412.3555 [cs.NE] [10] Jesus Crespo Cuaresma, Jaroslava Hlouskova, Stephan Kossmeier, and Michael Obersteiner. 2004. Forecasting Electricity Spot-Prices Using Linear Univariate Time-Series Models. Applied Energy 77 (2004), 87â€“106. [11] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and Rose Yu. 2023. Long-term Forecasting with TiDE: Time-series Dense Encoder. Transactions on Machine Learning Research (2023). [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations. [13] Filip Elvander and Andreas Jakobsson. 2020. Defining Fundamental Frequency for Almost Harmonic Signals. IEEE TRANSACTIONS ON SIGNAL PROCESSING (2020). [14] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. 2023. ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond. (2023). [15] James D Hamilton. 2020. Time series analysis. (2020). [16] Nicholas W. Hammond, FranÃ§ois Birgand, Cayelan C. Carey, Bethany Bookout, Adrienne Breef-Pilz, and Madeline E. Schreiber. 2023. High-frequency Sensor Data Capture Short-term Variability In Fe and Mn Concentrations Due to Hypolimnetic Oxygenation and Seasonal Dynamics in a Drinking Water Reservoir. Water Research 240 (2023). [17] Long Steven R. Wu Manli C. Shih Hsing H. Zheng Quanan Yen Nai-Chyuan Tung Chi Chao Huang Norden E. Shen Zheng and Liu Henry H. 1998. The Empirical Mode Decomposition and the Hilbert Spectrum for Nonlinear and Non-stationary Time Series Analysis. Proceedings of the Royal Society of London. Series A: mathematical, physical, and engineering sciences (1998), 903â€“995. [18] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023. PDFormer: Propagation Delay-aware Dynamic Long-range Transformer for Traf- fic Flow Prediction. (2023), 4365â€“4373. [19] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks. (2018), 95â€“104. [20] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks. (2018), 95â€“104. [21] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. 2019. Enhancing the Locality and Breaking the Memory Bottle- neck of Transformer on Time Series Forecasting. (2019). [22] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. 2023. Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping. (2023). [23] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting. (2022). [24] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. 2024. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting. In The Twelfth International Conference on Learning Representations. [25] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting. (2022). [26] Liu M., Zeng A., Chen M., Xu Z., Lai Q., Ma L., and Q. Xu. 2022. SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction. (2022), 5816â€“5828. [27] Sobhan Moosavi, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan Parthasarathy, and Rajiv Ramnath. 2019. Short and Long-Term Pattern Discovery Over Large-Scale Geo-Spatiotemporal Data. (2019), 2905â€“2913. [28] Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. 2016. Phased LSTM: Acceler- ating Recurrent Network Training for Long or Event-based Sequences. (2016). arXiv:1610.09513 [cs.LG] [29] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. (2023). [30] Namuk Park and Songkuk Kim. 2022. How Do Vision Transformers Work? (2022). [31] John G. Proakis and Dimitris G. Manolakis. 1996. Digital Signal Processing (3rd Ed.): Principles, Algorithms, and Applications. (1996). [32] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. 2019. On the Spectral Bias of Neural Networks. 97 (2019), 5301â€“5310. [33] Daniel Stoller, Mi Tian, Sebastian Ewert, and Simon Dixon. 2019. Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence Modelling. (2019). arXiv:1911.06393 [cs.LG] [34] James R. Thompson and James R. Wilson. 2016. Multifractal Detrended Fluctua- tion Analysis: Practical Applications to Financial Time Series. Mathematics and Computers in Simulation 126 (2016), 63â€“88. [35] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. 2023. Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Trans- former. (2023). [36] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. 2022. Anti- Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice. (2022). [37] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and Fan Zhou. 2022. Learning Latent Seasonal-Trend Representations for Time Series Forecasting. (2022). [38] Qingsong Wen, Zhe Zhang, Yan Li, and Liang Sun. 2020. Fast RobustSTL: Effi- cient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns. (2020), 2203â€“2213. [39] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. 2023. Transformers in Time Series: A Survey. (2023). [40] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022. CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting. (2022). [41] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. 2022. ETSformer: Exponential Smoothing Transformers for Time-series Forecasting. (2022). [42] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. (2023). [43] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Fore- casting. (2021). [44] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. (2020). [45] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. NystrÃ¶mformer: A NystrÃ¶m-based Algorithm for Approximating Self-Attention. (2021). [46] Zhi-Qin John Xu. 2020. Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks. Communications in Computational Physics 28 (2020), 1746â€“1767. [47] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers Effective for Time Series Forecasting? (2023). [48] Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross- Dimension Dependency for Multivariate Time Series Forecasting. (2023). [49] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. (2021). [50] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai Forecasting. (2022), 1â€“12. [51] Yunyue Zhu and Dennis Shasha. 2002. StatStream: Statistical Monitoring of Thousands of Data Streams in Real Time. (2002), 358â€“369. Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Fredformer: Frequency Debiased Transformer for Time Series Forecasting â€”â€”â€”â€”Appendixâ€”â€”â€”â€” Contents Abstract 1 1 Introduction 1 2 Preliminary Analysis 2 2.1 Preliminary 2 2.2 Case Studies 3 3 Frequency Bias Formulation 4 3.1 Frequency Bias Definitions 4 3.2 Problem Statement 4 4 Fredformer 4 4.1 Backbone 4 4.2 Frequency Refinement and Normalization 4 4.3 Frequency Local Independent Modeling 5 4.4 Frequency-wise Summarization 5 5 EXPERIMENTS 5 5.1 PROTOCOLS 5 5.2 Results 7 5.3 Ablation Study 7 5.4 Discussion of Applicability 7 6 Related Works 8 7 Conclusion 8 8 ACKNOWLEDGMENTS 9 References 9 Contents 11 A Details of the case studies 12 B More Details of the datasets 12 C Fredformer Algorithm 12 D Look-back window analysis 12 E Hyperparameter sensitivity 13 F Visualizations of the forecasting results 14 F.1 Time domain 14 F.2 Frequency domain 14 G Details of the Lemma 1 15 H NystrÃ¶m Approximation in Transformer Self-Attention Mechanism 16 I Detailed results of all datasets 17 KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai The full appendix can be found at: http://arxiv.org/abs/2406.09009 A DETAILS OF THE CASE STUDIES Here, we illustrate the details of how we generated the data for case study 2 in Sec.2.2.2: The generation of data for Case Study 2 from the original time series involves a sequence of steps to emphasize certain frequency components by manipulating their positions in the frequency domain. This process not only constructs a dataset with distinct frequency characteristics but also preserves the inherent noise and instability of the real data, enhancing the robustness and credibility of subsequent analyses. Specifically, the steps are as follows: (1) Apply the Discrete Fourier Transform (DFT) to the original time series data to obtain its frequency components, exclud- ing columns irrelevant for Fourier analysis (e.g., dates). (2) Select four prominent low-frequency components from the entire frequency spectrum and move them to the mid-frequency part. This modification aims to reduce the impact of fre- quency bias typically seen between low and high frequencies by placing important components in a non-low and non-high frequency position. (3) Split the frequency components into three equal parts. (4) Rearrange these parts according to a predefined order for frequency emphasis, ensuring that the first part is moved to the end while keeping the original second and third parts in their order. (5) Apply the Inverse Discrete Fourier Transform (IDFT) to the rearranged frequency data to convert it back into the time domain, thereby generating the modified \"mid\" frequency data. (6) Reinsert any excluded columns (e.g., dates) to maintain the original structure of the data. Through the operations described above, we have constructed a dataset with clearly high amplitude frequency components in the middle of the frequency domain. By moving significant low- frequency components to the mid-frequency section, we aim to mitigate the effects of frequency differences that arise from the dominance of low and high frequencies. The advantage of creating artificial data through these simple modifications to real data lies in its ability to preserve the inherent noise and instability present in the real data, thereby enhancing the robustness and credibility for subsequent analysis. B MORE DETAILS OF THE DATASETS Weather contains 21 channels (e.g., temperature and humidity) and is recorded every 10 minutes in 2020. ETT [49] (Electricity Transformer Temperature) consists of two hourly-level datasets (ETTh1, ETTh2) and two 15-minute-level datasets (ETTm1, ETTm2). Electricity [20], from the UCI Machine Learning Repository and preprocessed by, is composed of the hourly electricity consumption of 321 clients in kWh from 2012 to 2014. Solar-Energy [19] records the solar power production of 137 PV plants in 2006, sampled every 10 minutes. Traffic contains hourly road occupancy rates measured by 862 sensors on San Francisco Bay area freeways from January 2015 to December 2016. More details of these datasets can be found in Table.7. Algorithm 1 Generation of Modified Frequency Data for Case Study 2 1: procedure GenerateModifiedFreqencyData(ğ‘‘ğ‘ğ‘¡ğ‘) 2: ğ‘‘ğ‘ğ‘¡ğ‘’ğ¶ğ‘œğ‘™ğ‘¢ğ‘šğ‘› â† ğ‘‘ğ‘ğ‘¡ğ‘[â€²ğ‘‘ğ‘ğ‘¡ğ‘’â€²] âŠ² Preserve the date column for reinsertion 3: ğ‘‘ğ‘ğ‘¡ğ‘ğ‘‰ ğ‘ğ‘™ğ‘¢ğ‘’ğ‘  â† ğ‘‘ğ‘ğ‘¡ğ‘.ğ‘‘ğ‘Ÿğ‘œğ‘ (ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘  = [â€²ğ‘‘ğ‘ğ‘¡ğ‘’â€²]) âŠ² Exclude non-relevant columns 4: ğ‘“ ğ‘“ ğ‘¡ğ·ğ‘ğ‘¡ğ‘ â† DFT(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘‰ ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ ) âŠ² Apply DFT to obtain frequency components 5: ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ¶ğ‘œğ‘šğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  â† SelectLowFrequencyComponents(ğ‘“ ğ‘“ ğ‘¡ğ·ğ‘ğ‘¡ğ‘, 4) âŠ² Select 4 prominent low-frequency components 6: ğ‘“ ğ‘“ ğ‘¡ğ·ğ‘ğ‘¡ğ‘ğ‘€ğ‘œğ‘‘ğ‘– ğ‘“ ğ‘–ğ‘’ğ‘‘ â† MoveComponentsToMid(ğ‘“ ğ‘“ ğ‘¡ğ·ğ‘ğ‘¡ğ‘, ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ¶ğ‘œğ‘šğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘ ) âŠ² Move selected components to mid-frequency 7: ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘  â† Split(ğ‘“ ğ‘“ ğ‘¡ğ·ğ‘ğ‘¡ğ‘ğ‘€ğ‘œğ‘‘ğ‘– ğ‘“ ğ‘–ğ‘’ğ‘‘, 3) âŠ² Split frequency components into three equal parts 8: ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ â† DefineOrderForFrequencyEmphasis() âŠ² Define a new order for rearrangement 9: ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘  â† Rearrange(ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘ , ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ ) âŠ² Rearrange parts according to the predefined order 10: ğ‘šğ‘œğ‘‘ğ‘– ğ‘“ ğ‘–ğ‘’ğ‘‘ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ğ·ğ‘ğ‘¡ğ‘ â† IDFT(ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘ ) âŠ² Apply IDFT to generate modified time domain data 11: ğ‘šğ‘œğ‘‘ğ‘– ğ‘“ ğ‘–ğ‘’ğ‘‘ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ğ·ğ‘ğ‘¡ğ‘.ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ (0,â€² ğ‘‘ğ‘ğ‘¡ğ‘’â€², ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›) âŠ² Reinsert the date column 12: return ğ‘šğ‘œğ‘‘ğ‘– ğ‘“ ğ‘–ğ‘’ğ‘‘ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ğ·ğ‘ğ‘¡ğ‘ 13: end procedure C FREDFORMER ALGORITHM The algorithm 2 outlines our overall procedure. It includes several parts: (i) DFT-to-IDFT Backbone, where the input data is trans- formed using DFT and segmented into frequency bands; (ii & iii) Frequency Local Independent Learning, where normalization and a Transformer are applied to learn dependencies and features across channels; and (iv) Frequency-wise Summarizing, where the pro- cessed frequency information is summarized and transformed back to the time domain using IDFT to obtain the forecasting result. D LOOK-BACK WINDOW ANALYSIS We conducted further tests on our method using the ETTh1 and Weather datasets to investigate the impact of different look-back window lengths on forecasting accuracy. Four distinct lengths were chosen: {96, 192, 336, 720}, with 96 corresponding to the results presented in the main text and the other three lengths selected to compare the changes in forecasting accuracy with longer input sequences. Figure. 6 illustrates the variation in model forecasting accuracy across these input lengths. Overall, as the length of the input sequence increases, so does the model forecasting accuracy, demonstrating that our model is capable of extracting more features from longer input sequences. Specifically, comparing the longest window of 720 to the shortest of 96, the model forecasting accuracy improved by approximately 10% (0.343 â†’ 0.315 for Weather and 0.467 â†’ 0.449 for ETTh1). Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Table 7: Overview of Datasets Dataset Source Resolution Channels Time Range Weather Autoformer[43] Every 10 minutes 21 (e.g., temperature, humidity) 2020 ETTh1 Informer[49] Hourly 7 states of a electrical transformer 2016-2017 ETTh2 Informer[49] Hourly 7 states of a electrical transformer 2017-2018 ETTm1 Informer[49] Every 15 minutes 7 states of a electrical transformer 2016-2017 ETTm2 Informer[49] Every 15 minutes 7 states of a electrical transformer 2017-2018 Electricity UCI ML Repository Hourly 321 clientsâ€™ consumption 2012-2014 Solar-Energy [19] Every 10 minutes 137 PV plantsâ€™ production 2006 Traffic Informer[49] Hourly 862 sensorsâ€™ occupancy 2015-2016 Algorithm 2 Fredformer 1: Input: Historical data X âˆˆ Rğ¶ Ã—ğ¿, where ğ¶ is the number of channels and ğ¿ is the length of the data series. 2: Output: Forecasting result Xâ€² 3: Procedure: 4: ( i )DFT-to-IDFT Backbone 5: Perform DFT on X to obtain A âˆˆ Rğ¶ Ã—ğ¿ 6: for channel ğ‘ = 1 to ğ¶ do 7: for frequency band ğ‘› = 1 to ğ‘ do 8: Segment Ağ‘ into ğ‘ bands: (W(1:ğ¶ ) 1 , . . . , W(1:ğ¶ ) ğ‘ ) 9: end for 10: end for 11: ( ii&iii ) Frequency Local Independent Learning 12: for frequency band ğ‘› = 1 to ğ‘ do 13: Normalize cross-channel amplitude sequences for band ğ‘›: Wâˆ— ğ‘› = ğœ (Wğ‘›) ğ‘› = 1, 2, . . . , ğ‘ 14: Apply Transformer to learn channel-wise dependencies and joint features across channels: ğ’˜â€² (1:ğ¶ ) ğ‘› = ğ‘“ğ‘‡ ğ‘Ÿğ‘ğ‘›ğ‘  (ğ’˜âˆ— (1:ğ¶ ) ğ‘› ) 15: end for 16: ( iv )Frequency-wise Summarizing 17: Abstract overall frequency information to form new Aâ€²: Aâ€² = Linear(Wâ€²) 18: Perform IDFT using Aâ€² to generate Xâ€²: Xâ€² = IDFT(Aâ€²) E HYPERPARAMETER SENSITIVITY To evaluate our model robustness across various hyperparameter settings with input/predicting length ğ¿ = 96/ğ» = 720, we investi- gated four key hyperparameters: (1) model depth (cf depth), (2) feature dimension of self-attention (cf dim), (3) feature dimen- sion within self-attention multi-heads (cf head dim), (4) number of multi-heads (cf heads), and (5) feature dimension of the feed- forward layer in the Transformer Encoder (cf mlp). We tested one hundred hyperparameter combinations, with results shown in Figure E. The variation in the model accuracy, ranging from 0.433 to 0.400 with an average value of 0.415, shows our preference for stability in hyperparameter selection over chasing the highest possible accuracy. We decided to utilize the averaged accuracy as our benchmark result in Table I. Visual representations of each hyperparameter impact on model robustness are detailed as follows: 96 192 336 720MSE Look-back window Length ETTh1 Weather Figure 6: The forecasting performance on two datasets, ETTh1 and Weather, across four different look-back win- dow lengths. The x-axis indicates the window length, while the y-axis represents the MSE loss of the forecasting results. Figure 7: Overall view of all trials, plotting different trials on the x-axis against MSE accuracy on the y-axis. Yellow dots represent trials with poor accuracy, falling below our final selection MSE accuracy, while purple dots indicate trials with higher accuracy than our final decision. KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai Figure 8: Illustration of the cf depth parameter effect across all trials, showcasing MSE accuracy for different values. The x-axis represents the parameter values, while the y-axis dis- plays MSE accuracy. Purple indicates trials with MSE accu- racy higher than our final chosen result, and yellow signifies trials with lower accuracy. The box plot details the distribu- tion of MSE accuracy above (in yellow) or below (in purple) our final selection for each value. â€¢ Figure 8 illustrates the robustness across different cf depth settings. â€¢ Figure 9 showcases the impact of varying cf dim on model performance. â€¢ Figure 10 presents the model behavior with changes in cf head dim. â€¢ Figure 11 depicts the influence of different cf heads counts. â€¢ Figure 12 reveals how adjustments in cf mlp affect accuracy. F VISUALIZATIONS OF THE FORECASTING RESULTS Due to space constraints and the length of the paper, we have omitted a significant number of visualization results in the main text. In the appendix, we provide additional visualization results to demonstrate the effectiveness of our method. Here, we divide the visualization results into two categories: (1) time-domain visual- izations and (2) frequency-domain visualizations. These categories highlight two critical aspects of our model effectiveness: (1) the accuracy of predictions in the time domain and (2) the capability to capture important components in the frequency domain. F.1 Time domain We have included additional samples from two different channels of the ETTh1 dataset. Figure 13 presents a sample from channel #5, and Figure 14 showcases a sample from channel #2. The data char- acteristics across different channels vary. Our model, in contrast to FEDformer, adeptly learns the similarities and differences across various channels, underscoring the significance of channel-wise attention. Compared to iTransformer, our model captures more Figure 9: Depiction of the cf dim parameter influence, detail- ing MSE accuracy across various settings. Parameter values are plotted on the x-axis against MSE accuracy on the y-axis. Trials surpassing our final result accuracy are marked in pur- ple, whereas those falling short are in yellow. The distribu- tion of MSE accuracies, differentiated by outcomes exceeding or not meeting our final accuracy, is presented in a box plot for each parameter value. Figure 10: Analysis of the cf head dim parameter impact, with the x-axis indicating parameter values and the y-axis MSE accuracy. Purple highlights trials where MSE accuracy is above our finalized result, with yellow showing lower accu- racy trials. Each parameter value MSE accuracy distribution, categorized by exceeding or not our final accuracy, is visual- ized through a box plot. detailed features, effectively identifying both global and local char- acteristics, and highlighting the importance of frequency-domain modeling. F.2 Frequency domain Figure 15 visualizes another sample output of the model after 50 epochs of training in the frequency domain, with input and ground Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Figure 11: Visualization of the cf heads parameter role, plot- ting parameter values against MSE accuracy. Trials with higher MSE accuracy than our chosen outcome are in purple; those with lower accuracy are in yellow. A box plot repre- sents the spread of MSE accuracies for each value, segmented into results that are above (yellow) or below (purple) our final selection. Figure 12: Examination of the cf mlp parameter performance, with parameter values on the x-axis and MSE accuracy on the y-axis. Purple represents trials outperforming our final result in MSE accuracy, while Yellow indicates underperformance. The box plot shows the distribution of MSE accuracies for each parameter value, divided into outcomes that surpass or fall short of our final chosen accuracy. truth data from ETTh1. Similar to Section 2.2, the line graph dis- plays the frequency amplitudes, and the heat map shows the model relative error for four components over increasing epochs. We fo- cus on mid-to-high frequency features here, where the amplitudes of these four key components, ğ‘˜1, ğ‘˜2, ğ‘˜3, and ğ‘˜4, are significantly lower than low-frequency components, successfully capturing these components indicates the model ability to mitigate frequency bias. After training, our method accurately identifies ğ‘˜1, ğ‘˜2, and ğ‘˜3, with Fredformer iTransformerãƒ»Real data Fedfromer Figure 13: Visualization of forecasting results (ğ» = 336). Grey dots show the real data, and the grey zone represents the last part of the look-back window. Sample from chan- nel #5 of the ETTh1 dataset, illustrating the unique data characteristics of this channel. Fredformer iTransformerãƒ»Real data Fedfromer Figure 14: Visualization of forecasting results (ğ» = 336). Grey dots show the real data, and the grey zone represents the last part of the look-back window. Sample from channel #2 of the ETTh1 dataset, demonstrating the distinct data features specific to this channel. uniformly decreasing relative errors. Despite a larger learning error for ğ‘˜4, Î”ğ‘˜4 consistently diminishes. This performance contrasts with all baselines, which demonstrate a lack of effectiveness in capturing these frequency components, with unequal reductions in relative errors. G DETAILS OF THE LEMMA 1 Here, we give more details of Lemma.1 in Sec.4. First, we illustrate the full-spectrum normalization as: W âˆ— = {ğœ (W1 . . . Wğ‘ )} This shows why normalizing the entire spectrum together does not address the disparity in amplitude across different frequency bands Wğ‘›, failing to remove amplitude bias between key frequency components, that is, max(Wâˆ— ğ‘›) > max(Wâˆ— ğ‘š) (4) Then, we furthermore illustrate why time domain patching opera- tion and normalization: {ğœ (Wt1) . . . ğœ (Wtğ‘ )} = {ğœ (W1 . . . Wğ‘ )} (5) where Wt represents a series of time-domain patches of X. Ac- cording to Parsevalâ€™s theorem, the equivalence of the energy of the time-domain signal after local normalization and the energy of KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai Table 8: Comparison of GPU Memory Usage and MSE Accuracy on ETTh1 and ECl datasets, the best results are highlighted in red Model ETTh1 Dataset ECl Dataset GPU Memory (MB) MSE Accuracy GPU Memory (MB) MSE Accuracy Ours 408 0.453 4338 0.213 Ours(NystrÃ¶m) - - 3250 0.212 iTransformer 642 0.491 3436 0.225 PatchTST 1872 0.454 43772 0.256 Crossformer 2128 0.666 40066 0.280 FEDformer 1258 0.543 3824 0.246 iTransformer FEDformer Ground truthInput Forecasting Relative Error 0.99 0.01 PatchTST#epoch150 k1 k2 k3 k4 Fredformer(Proposed) F60 1400 k1 k2 k3 k4#epoch150 k1 k2 k3 k4F60 1400 k1 k2 k3 k4#epoch150 k1 k2 k3 k4F60 1400 k1 k2 k3 k4#epoch150 k1 k2 k3 k4F60 1400 k1 k2 k3 k4 Figure 15: Visualizations of the learning dynamics and re- sults for our method and baselines on the ETTh1 dataset, employing line graphs to illustrate amplitudes in the fre- quency domain and heatmaps to represent training epoch errors. the frequency-domain signal after global normalization results in similar: max(W âˆ— ğ‘›) > max(Wâˆ— ğ‘š) (6) which does not solve the issue of amplitude bias among key fre- quency components. Here, we furthermore discuss why this operation does not solve the issue of amplitude bias among key frequency components: Given the frequency components Wğ‘› in the frequency domain and their corresponding time-domain representations Wtğ‘›, Parse- valâ€™s theorem provides the foundation for understanding the energy equivalence between the time and frequency domains. Specifically, it states that the total energy in the time domain is equal to the total energy in the frequency domain: ğ‘âˆ‘ï¸ ğ‘›=1 âˆ« |ğœ (Wtğ‘›)|2ğ‘‘ğ‘¡ = âˆ« |ğœ (W1, . . . , Wğ‘ )|2ğ‘‘ ğ‘“ , where ğ‘ ğ‘–ğ‘”ğ‘šğ‘(ğ‘ğ‘‘ğ‘œğ‘¡) denotes the normalization operation. This theorem underscores the equivalence between applying local nor- malization to patches in the time domain and global normalization across the frequency spectrum. Each point in the time domain can be expressed as the sum of energies from all frequency components at that point. Therefore, a time-domain patch, consisting of ğ‘† points, can be represented as the sum of all frequency components over these ğ‘† points: Patchtime = ğ‘†âˆ‘ï¸ ğ‘ =1 ğ‘âˆ‘ï¸ ğ‘›=1 Wğ‘› (ğ‘ ), where Wğ‘› (ğ‘ ) represents the energy contribution of frequency component ğ‘› at time point ğ‘ . Normalizing this time-domain patch equates to normalizing the weights of all frequency components across these ğ‘† points. Mathematically, this normalization can be represented as: ğœ (Patchtime) = ğœ ( ğ‘†âˆ‘ï¸ ğ‘ =1 ğ‘âˆ‘ï¸ ğ‘›=1 Wğ‘› (ğ‘ ) ) . However, normalizing within each time-domain patch does not guarantee that the maximum amplitudes across all frequency com- ponents Wâˆ— ğ‘› and Wâˆ— ğ‘š are equalized across different patches. This leads to the critical insight: max(W âˆ— ğ‘›) > max(W âˆ— ğ‘š), âˆ€ğ‘›, ğ‘š, Indicating that local normalization in the time domain, and by ex- tension, global normalization in the frequency domain, does not ef- fectively address the amplitude bias problem among key frequency components. The inherent limitation is that while normalization can adjust the overall energy levels within patches or across the spectrum, it does not inherently correct for discrepancies in the am- plitude distributions among different frequency components. This underscores the necessity for approaches that can specifically target and mitigate amplitude biases to ensure equitable representation and processing of all frequency components. H NYSTRÃ–M APPROXIMATION IN TRANSFORMER SELF-ATTENTION MECHANISM Overview: To streamline the attention computation, we select ğ‘š landmarks by averaging rows or columns of the attention matrix, simplifying the matrices Qğ‘› and Kğ‘› into ËœQğ‘› and ËœKğ‘›. The NystrÃ¶m approximation for the ğ‘›-th channel-wise attention Ağ‘› is then cal- culated as Ağ‘› â‰ˆ ËœAğ‘› = ËœFğ‘› ËœAğ‘› ËœBğ‘›, where ËœFğ‘› = softmax(Qğ‘› ËœKğ‘›ğ‘‡ ), ËœAğ‘› = softmax( ËœQğ‘› ËœKğ‘›ğ‘‡ )+, ËœBğ‘› = softmax( ËœQğ‘›Kğ‘›ğ‘‡ ). Here, ËœAğ‘› + is the Fredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Moore-Penrose inverse of ËœAğ‘› [45]. This significantly reducing the computational load from ğ‘‚ ( ğ¿ ğ‘ƒ ğ¶2) to ğ‘‚ ( ğ¿ ğ‘ƒ ğ¶). Specifically: Details: We reduce the computational cost of self-attention in the Transformer encoder using the NystrÃ¶m method. Following, we describe how to use the NystrÃ¶m method to approximate the softmax matrix in self-attention by sampling a subset of columns and rows. Consider the softmax matrix in self-attention, defined as: ğ‘† = softmax ( ğ‘„ğ¾ğ‘‡ âˆšï¸ ğ‘‘ğ‘ ) This matrix can be partitioned as: ğ‘† = [ ğ´ğ‘† ğµğ‘† ğ¹ğ‘† ğ¶ğ‘† ] Where ğ´ğ‘† is derived by sampling ğ‘š columns and rows from ğ‘†. By employing the NystrÃ¶m method, the SVD of ğ´ğ‘† is given by: ğ´ğ‘† = ğ‘ˆ Î›ğ‘‰ ğ‘‡ Using this, an approximation Ë†ğ‘† of ğ‘† can be constructed: Ë†ğ‘† = [ ğ´ğ‘† ğµğ‘† ğ¹ğ‘† ğ¹ğ‘†ğ´+ ğ‘† ğµğ‘† ] Where ğ´+ ğ‘† is the Moore-Penrose inverse of ğ´ğ‘† . To further elaborate on the approximation, given a query ğ‘ğ‘– and a key ğ‘˜ ğ‘— , let: K (ğ‘ğ‘–, ğ¾) = softmax ( ğ‘ğ‘–ğ¾ğ‘‡ âˆšï¸ ğ‘‘ğ‘ ) K (ğ‘„, ğ‘˜ ğ‘— ) = softmax ( ğ‘„ğ‘˜ğ‘‡ ğ‘— âˆšï¸ ğ‘‘ğ‘ ) From the above, we can derive: ğœ™ (ğ‘ğ‘–, ğ¾) = Î›âˆ’ 1 2 ğ‘‰ ğ‘‡ K (ğ‘ğ‘–, ğ¾)ğ‘šÃ—1 ğœ™ (ğ‘„, ğ‘˜ ğ‘— ) = Î›âˆ’ 1 2 ğ‘ˆ ğ‘‡ K (ğ‘„, ğ‘˜ ğ‘— )ğ‘šÃ—1 Thus, the NystrÃ¶m approximation for a particular entry in Ë†ğ‘† is: Ë†ğ‘†ğ‘– ğ‘— = ğœ™ (ğ‘ğ‘–, ğ¾) ğ‘‡ ğœ™ (ğ‘„, ğ‘˜ ğ‘— ) In matrix form, Ë†ğ‘† can be represented as: Ë†ğ‘† = softmax ( ğ‘„ğ¾ğ‘‡ âˆšï¸ ğ‘‘ğ‘ ) ğ‘›Ã—ğ‘š ğ´+ ğ‘† softmax ( ğ‘„ğ¾ğ‘‡ âˆšï¸ ğ‘‘ğ‘ ) ğ‘šÃ—ğ‘› This method allows for the approximation of the softmax matrix in self-attention, potentially offering computational benefits. I DETAILED RESULTS OF ALL DATASETS Here, we show the detailed forecasting results of full datasets in the Table. I. The best and second best results are highlighted. With a default look-back window of ğ¿ = 96, our proposal shows lead- ing performance on most datasets and different prediction length settings, with 60 top-1 (29 + 31) cases out of 80 in total. KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai Table 9: Full results of the long-term forecasting task . We compare extensive competitive models under different prediction lengths following the setting of iTransformer [24]. The input sequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths. Models Fredformer iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear SCINet FEDformer Stationary Autoformer (Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021] Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEETTm196 0.326 0.361 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475 192 0.363 0.380 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496 336 0.395 0.403 0.426 0.420 0.424 0.415 0.399 0.410 0.532 0.515 0.428 0.425 0.410 0.411 0.413 0.413 0.490 0.485 0.445 0.459 0.495 0.464 0.621 0.537 720 0.453 0.438 0.491 0.459 0.487 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.478 0.450 0.474 0.453 0.595 0.550 0.543 0.490 0.585 0.516 0.671 0.561 Avg 0.384 0.395 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 0.485 0.481 0.448 0.452 0.481 0.456 0.588 0.517ETTm296 0.177 0.259 0.180 0.264 0.182 0.265 0.175 0.259 0.287 0.366 0.207 0.305 0.187 0.267 0.193 0.292 0.286 0.377 0.203 0.287 0.192 0.274 0.255 0.339 192 0.243 0.301 0.250 0.309 0.246 0.304 0.241 0.302 0.414 0.492 0.290 0.364 0.249 0.309 0.284 0.362 0.399 0.445 0.269 0.328 0.280 0.339 0.281 0.340 336 0.302 0.340 0.311 0.348 0.307 0.342 0.305 0.343 0.597 0.542 0.377 0.422 0.321 0.351 0.369 0.427 0.637 0.591 0.325 0.366 0.334 0.361 0.339 0.372 720 0.397 0.396 0.412 0.407 0.407 0.398 0.402 0.400 1.730 1.042 0.558 0.524 0.408 0.403 0.554 0.522 0.960 0.735 0.421 0.415 0.417 0.413 0.433 0.432 Avg 0.279 0.324 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 0.571 0.537 0.305 0.349 0.306 0.347 0.327 0.371ETTh196 0.373 0.392 0.386 0.405 0.386 0.395 0.414 0.419 0.423 0.448 0.479 0.464 0.384 0.402 0.386 0.400 0.654 0.599 0.376 0.419 0.513 0.491 0.449 0.459 192 0.433 0.420 0.441 0.436 0.437 0.424 0.460 0.445 0.471 0.474 0.525 0.492 0.436 0.429 0.437 0.432 0.719 0.631 0.420 0.448 0.534 0.504 0.500 0.482 336 0.470 0.437 0.487 0.458 0.479 0.446 0.501 0.466 0.570 0.546 0.565 0.515 0.491 0.469 0.481 0.459 0.778 0.659 0.459 0.465 0.588 0.535 0.521 0.496 720 0.467 0.456 0.503 0.491 0.481 0.470 0.500 0.488 0.653 0.621 0.594 0.558 0.521 0.500 0.519 0.516 0.836 0.699 0.506 0.507 0.643 0.616 0.514 0.512 Avg 0.435 0.426 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.458 0.450 0.456 0.452 0.747 0.647 0.440 0.460 0.570 0.537 0.496 0.487ETTh296 0.293 0.342 0.297 0.349 0.288 0.338 0.302 0.348 0.745 0.584 0.400 0.440 0.340 0.374 0.333 0.387 0.707 0.621 0.358 0.397 0.476 0.458 0.346 0.388 192 0.371 0.389 0.380 0.400 0.374 0.390 0.388 0.400 0.877 0.656 0.528 0.509 0.402 0.414 0.477 0.476 0.860 0.689 0.429 0.439 0.512 0.493 0.456 0.452 336 0.382 0.409 0.428 0.432 0.415 0.426 0.426 0.433 1.043 0.731 0.643 0.571 0.452 0.452 0.594 0.541 1.000 0.744 0.496 0.487 0.552 0.551 0.482 0.486 720 0.415 0.434 0.427 0.445 0.420 0.440 0.431 0.446 1.104 0.763 0.874 0.679 0.462 0.468 0.831 0.657 1.249 0.838 0.463 0.474 0.562 0.560 0.515 0.511 Avg 0.365 0.393 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 0.954 0.723 0.437 0.449 0.526 0.516 0.450 0.459ECL 96 0.147 0.241 0.148 0.240 0.201 0.281 0.195 0.285 0.219 0.314 0.237 0.329 0.168 0.272 0.197 0.282 0.247 0.345 0.193 0.308 0.169 0.273 0.201 0.317 192 0.165 0.258 0.162 0.253 0.201 0.283 0.199 0.289 0.231 0.322 0.236 0.330 0.184 0.289 0.196 0.285 0.257 0.355 0.201 0.315 0.182 0.286 0.222 0.334 336 0.177 0.273 0.178 0.269 0.215 0.298 0.215 0.305 0.246 0.337 0.249 0.344 0.198 0.300 0.209 0.301 0.269 0.369 0.214 0.329 0.200 0.304 0.231 0.338 720 0.213 0.304 0.225 0.317 0.257 0.331 0.256 0.337 0.280 0.363 0.284 0.373 0.220 0.320 0.245 0.333 0.299 0.390 0.246 0.355 0.222 0.321 0.254 0.361 Avg 0.175 0.269 0.178 0.270 0.219 0.298 0.216 0.304 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 0.268 0.365 0.214 0.327 0.193 0.296 0.227 0.338Traffic96 0.406 0.277 0.395 0.268 0.649 0.389 0.544 0.359 0.522 0.290 0.805 0.493 0.593 0.321 0.650 0.396 0.788 0.499 0.587 0.366 0.612 0.338 0.613 0.388 192 0.426 0.290 0.417 0.276 0.601 0.366 0.540 0.354 0.530 0.293 0.756 0.474 0.617 0.336 0.598 0.370 0.789 0.505 0.604 0.373 0.613 0.340 0.616 0.382 336 0.432 0.281 0.433 0.283 0.609 0.369 0.551 0.358 0.558 0.305 0.762 0.477 0.629 0.336 0.605 0.373 0.797 0.508 0.621 0.383 0.618 0.328 0.622 0.337 720 0.463 0.300 0.467 0.302 0.647 0.387 0.586 0.375 0.589 0.328 0.719 0.449 0.640 0.350 0.645 0.394 0.841 0.523 0.626 0.382 0.653 0.355 0.660 0.408 Avg 0.431 0.287 0.428 0.282 0.626 0.378 0.555 0.362 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 0.804 0.509 0.610 0.376 0.624 0.340 0.628 0.379Weather96 0.163 0.207 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336 192 0.211 0.251 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367 336 0.267 0.292 0.278 0.296 0.292 0.307 0.278 0.297 0.272 0.335 0.287 0.335 0.280 0.306 0.283 0.335 0.309 0.378 0.339 0.380 0.321 0.338 0.359 0.395 720 0.343 0.341 0.358 0.349 0.364 0.353 0.354 0.348 0.398 0.418 0.351 0.386 0.365 0.359 0.345 0.381 0.377 0.427 0.403 0.428 0.414 0.410 0.419 0.428 Avg 0.246 0.272 0.258 0.279 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 0.292 0.363 0.309 0.360 0.288 0.314 0.338 0.382Solar-Energy96 0.185 0.233 0.203 0.237 0.322 0.339 0.234 0.286 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711 192 0.227 0.253 0.233 0.261 0.359 0.356 0.267 0.310 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692 336 0.246 0.284 0.248 0.273 0.397 0.369 0.290 0.315 0.750 0.735 0.368 0.430 0.319 0.330 0.353 0.415 0.304 0.389 0.282 0.376 0.290 0.296 0.941 0.723 720 0.247 0.276 0.249 0.275 0.397 0.356 0.289 0.317 0.769 0.765 0.370 0.425 0.338 0.337 0.356 0.413 0.308 0.388 0.357 0.427 0.285 0.295 0.882 0.717 Avg 0.226 0.261 0.233 0.262 0.369 0.356 0.270 0.307 0.641 0.639 0.347 0.417 0.301 0.319 0.330 0.401 0.282 0.375 0.291 0.381 0.261 0.381 0.885 0.711 1st Count 29 31 4 8 1 1 2 1 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/Lee24accelGrokFast.pdf","text":"Grokfast: Accelerated Grokking by Amplifying Slow Gradients Jaerin Lee ∗♠ Bong Gyun Kang ∗♡ Kihoon Kim ♡ Kyoung Mu Lee ♠♡ ♠ASRI, Department of ECE, ♡Interdisciplinary Program in Artificial Intelligence, Seoul National University, Korea {ironjr,luckypanda,kihoon96,kyoungmu}@snu.ac.kr Abstract One puzzling artifact in machine learning dubbed grokking is where delayed gener- alization is achieved tenfolds of iterations after near perfect overfitting to the train- ing data. Focusing on the long delay itself on behalf of machine learning practition- ers, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a ran- dom signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding com- ponent and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than ×50 with only a few lines of code that amplifies the slow-varying components of gradients. The experi- ments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden gener- alization. Our code is available at https://github.com/ironjr/grokfast. 1 Introduction Figure 1: GROKFAST interprets training dynamics of a network parameter as a stochastic signal and amplify the low fre- quency variations for faster grokking. Grokking is a recently discovered phenomenon where generalization is achieved long after a model overfits to the training data. The phenomenon was first reported by Power et al. [2022] for a two-layer Transformer [Vaswani et al., 2017] trained using a simple algorithmic dataset. Later, Liu et al. [2022b] have shown that similar artifacts are observed for various model architectures trained with a variety of datasets, including images, languages, and graphs. Many theory-oriented works have tried to justify the effect by relating the grokking phenomenon to the previously known double descent phenomenon [Davies et al., 2023, Huang et al., 2024], yet its cause and sufficient conditions have not been fully characterized. Apart from theoretical studies, this work takes a practitioner’s standpoint to take advantage of the grokking phenomenon. In previous reports on grokking [Power et al., 2022, Liu et al., 2022b], generalization happens only after more than a tenfold of training iterations after overfitting. This high demand of computational resources means less practical appeal to general machine learning practitioners who are often under dire resource constraints. Achieving faster generalization in those overfitting systems is, therefore, a necessary step to fully explore the potential of this unusual behavior. Our goal is, to this end, to accelerate the grokking phenomenon. ∗Equal contribution. Preprint. Under review.arXiv:2405.20233v2 [cs.LG] 5 Jun 2024 101 102 103 104 1053.99 × 1047.90 × 102 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy Predict x · y (mod 97) (Training on 50% of data) baseline train baseline val Grokfast train Grokfast val ×50.49 faster (a) Accelerated grokking with GROKFAST. 101 102 103 104 1053.99 × 1047.90 × 102 Optimization Steps 0 2 4 6 8 10Loss Predict x · y (mod 97) (Training on 50% of data) baseline train baseline val Grokfast train Grokfast val (b) Corresponding loss curves. Figure 2: Accelerating generalization of a model under grokking phenomenon. Our GROKFAST is a simple algorithmic modification upon existing optimizers to pull forward the time of event of sudden generalization after overfitting, also known as the grokking phenomenon. In the example training curve of a model under grokking in Figure 2, the dynamics of the validation loss is few orders of magnitude slower than the dynamics of the training loss. The change in the losses is a direct consequence of the change in the parameter values throughout the training session. Hence, Figure 2 suggests that the parameter updates under grokking takes effect in two different timescales: the fast-varying component of the parameter updates contributes to the rapid overfitting, and the slow-varying component contributes to the slow generalization. We begin by treating the change of value u(t) of each model parameter θ over training iteration t from an optimizer as a discrete (random) signal over time. As the optimizer iterates the training data, the value of each parameter θ(t) , the loss l(t) , and its gradient g(t) := ∂l(t)/∂θ(t) drift with respect to the guidance from a sequence of randomly selected mini-batches sampled at each iteration t : θ(t + 1) = θ(t) + u(g(t), t) = θ(0) + t∑ τ =0 u(g(τ ), τ ) . (1) The parameter update function u(t) = u(g(t), t) = θ(t + 1) − θ(t) provides a simple abstraction of the underlying optimizer. Different instances of iterative optimizers including SGD with various hyperparameters, e.g., learning rate and momentum, can be dealt with this notation. Treating the optimization process as a collection of discrete random signals u(t) allows us to consider its dual representation U (ω) in the frequency domain. Taking the discrete-time Fourier transform F of u(t) with respect to the training iteration t , we obtain the spectral representation of the sequence of changes of a specific parameter θ : U (ω) = F{u(t)} = T∑ t=0 u(t)e−iωt , (2) where T is the total number of training iterations in this specific training session. Under our assump- tion that the delayed generalization of grokking is a consequence of the slow-varying component of the parameter updates u(t) , the grokking phenomenon is directly related to the low-frequency part of the dual representation U (ω) . Further, for a first-order optimizer u(g(t), t) , an almost unanimous choice in deep learning applications, the gradients g(t) are linearly correlated with the parameter updates u(t) . Therefore, we can relate the low-frequency part of the gradient signal G(ω) = ∑T t=0 g(t)e−iωt with the slow generalization under grokking. Our hypothesis is that amplifying this low-frequency component of G(ω) accelerates the speed of generalization under the grokking phenomenon. In the following sections, we empirically demonstrate this hypothesis with a simple low-frequency gradient amplifier in various scenarios. These include tasks involving various network architectures including Transformers [Vaswani et al., 2017], MLPs, RNNs and (Graph-)ConvNets and diverse datasets such as algorithmic data, images, languages, and graphs that are treated to exhibit the grokking phenomenon [Liu et al., 2022b]. Our method is simple, taking only a few lines of additional code and is applicable to most of machine learning frameworks such as PyTorch [Paszke et al., 2019], with ×50 faster exhibition of grokking as shown in Figure 2. 2 Algorithm 1 GROKFAST-MA 1: Param: window size w , scalar factor λ . 2: Input: initial parameters θ0 , stochastic ob- jective function f (θ) , optimizer’s parameter update u(g, t) from gradient g at timestep t . 3: begin: t ← 0 ; Q ← Queue(capacity = w) 4: while θt not converged do 5: t ← t + 1 6: gt ← ∇θf (θt−1) : Calculate gradients. 7: Insert(Q, gt) : Insert gradients to Q . 8: ˆgt ← gt + λ · Avg(Q) : Filter gradients. 9: ˆut ← u(ˆgt, t) : Calculate update. 10: θt ← θt−1 + ˆut : Update parameters. 11: end while Algorithm 2 GROKFAST-EMA (GROKFAST). 1: Param: scalar momentum α , factor λ . 2: Input: initial parameters θ0 , stochastic ob- jective function f (θ) , optimizer’s parameter update u(g, t) from gradient g at timestep t . 3: begin: t ← 0 ; µ ← θ0: EMA of gradients. 4: while θt not converged do 5: t ← t + 1 6: gt ← ∇θf (θt−1) : Calculate gradients. 7: µ ← αµ + (1 − α)gt : Calculate EMA. 8: ˆgt ← gt + λµ : Filter gradients. 9: ˆut ← u(ˆgt, t) : Calculate update. 10: θt ← θt−1 + ˆut : Update parameters. 11: end while 0 20 40 60 80 100 120 Time [it] 0.0 0.1 0.2 0.3 0.4 0.5Amplitude[dB] (a) Time plot of MA. −2 −1 0 1Angle[rad] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Frequency [rad/it] −30 −20 −10 0 10Amplitude[dB] (b) Freq. plot of MA. 0 20 40 60 80 100 120 Time [it] 0.0 0.1 0.2 0.3 0.4 0.5Amplitude[dB] (c) Time plot of EMA. −1.0 −0.8 −0.6 −0.4 −0.2 0.0Angle[rad] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Frequency [rad/it] −10 −5 0 5 10 15Amplitude[dB] (d) Freq. plot of EMA. Figure 3: Time and frequency domain plots of the gradient filters. Figures (a, b) and (c, d) depict the impulse responses and the transfer functions of the filters of Algorithm 1 and 2, i.e., the MA and the EMA filters h(t) . We treat training iterations as discrete timesteps. 2 Amplifying the Low-Frequencies of the Stochastic Gradients 2.1 Filter Design Amplifying the low-frequencies of the gradients g(t) can be achieved by adding a low-pass filtered signal g(t) to itself. Let h(t) be a discrete-time low-pass filter (LPF) defined over the training iteration t . For simplicity, we assume a univariate time-invariant low-pass filter h(t) uniformly applied across every model parameter θ . Using a convolution operator ∗ , we denote the modified gradient ˆg(t) as: ˆg(t) = g(t) + h(t) ∗ g(t) , (3) which can then be plugged into the parameter update function u of the optimizer: ˆu(t) = u(ˆg(t), t) = u(g(t) + h(t) ∗ g(t), t) . (4) In the dual domain, equation (3) is equivalent to: ˆG(ω) = G(ω) + H(ω)G(ω) = (1 + H(ω))G(ω) , (5) where H(ω) = ∑T t=0 h(t)e−iωt is the transfer function of the filter h(t) . Our goal can therefore be restated as to design a filter h(t) with low-pass characteristics in its transfer function H(ω) . For the demonstration of our initial claim, we first take the simplest strategy: the LPF h(t) is a windowed moving average (MA) over a fixed-size window of width w . h(t) = λ w Π ( t w − 1 2 ) = {λ/w , if 0 ≤ t < w . 0 , otherwise. (6) The function Π stands for the Heaviside Pi function, which has value of one in the interval [−0.5, 0.5] and of zero elsewhere. This filter has only two scalar hyperparameters: the scalar factor λ and the window size w . As shown in Algorithm 1, we implement this filter h with a fixed-capacity queue Q storing the intermediate parameter updates u(t) into the queue Q . The average of the queue Q is the low-pass filtered gradients which is added to the current parameter update at each optimizer step. 3 101 102 103 104 1052.94 × 103 3.99 × 104 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy baseline train baseline val λ = 1 train λ = 2 train λ = 5 train λ = 10 train λ = 1 val λ = 2 val λ = 5 val λ = 10 val ×13.57 faster (a) Accuracy with respect to amplifier gain λ . 101 102 103 104 1052.94 × 103 3.99 × 104 Optimization Steps 0 2 4 6 8 10 12Loss baseline train baseline val λ = 1 train λ = 2 train λ = 5 train λ = 10 train λ = 1 val λ = 2 val λ = 5 val λ = 10 val (b) Loss with respect to amplifier gain λ . 101 102 103 104 1052.94 × 103 3.99 × 104 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy baseline train baseline val w = 2 train w = 5 train w = 10 train w = 20 train w = 50 train w = 100 train w = 200 train w = 2 val w = 5 val w = 10 val w = 20 val w = 50 val w = 100 val w = 200 val ×13.57 faster (c) Accuracy with respect to window size w . 101 102 103 104 1052.94 × 103 3.99 × 104 Optimization Steps 0 2 4 6 8 10 12Loss baseline train baseline val w = 2 train w = 5 train w = 10 train w = 20 train w = 50 train w = 100 train w = 200 train w = 2 val w = 5 val w = 10 val w = 20 val w = 50 val w = 100 val w = 200 val (d) Loss with respect to window size w . Figure 4: Acceleration of delayed generation with GROKFAST-MA. The amount of acceleration relies on the two hyperparameters, amplifier gain λ and window size w . Each hyperparameter has a sweet spot; increasing one arbitrarily does not guarantee faster acceleration. Figures 4a and 4b use w = 100 except the baseline. Figures 4c and 4d use λ = 5 except the baseline. 2.2 Experiment We first demonstrate our conjecture on the algorithmic dataset used in the first report on grokking [Power et al., 2022]. The network is a two-layer decoder-only Transformer [Vaswani et al., 2017] trained to predict the answer of a modular binary multiplication operation x · y (mod 97) . The training curve of this task is shown in Figure 2 as ‘baseline.’ Comparing the time to reach the accuracy of 0.95 , the generalization, i.e., the late saturation of the validation accuracy, happens after ×97.3 iterations after the rapid saturation of the training accuracy (the overfitting). Figure 4 shows empirical proof of effectiveness of Algorithm 1, our GROKFAST-MA algorithm on this task. Choosing the hyperparameters from a simple grid search over λ ∈ {1, 2, 5, 10} and w ∈ {2, 5, 10, 20, 50, 100, 200} , we found that the filter works best when λ = 5 and w = 100 . As shown in Figure 4, the number of iterations taken to make the validation accuracy reach 95% of the training accuracy is reduced by ×13.57 , which is a remarkable reduction of training iterations. 2.3 Discussion 101 102 103 104 105500 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy baseline train baseline val 1-stage (Grokfast) train 1-stage (Grokfast) val 1-stage slow-only train 1-stage slow-only val 2-stage slow-only train 2-stage slow-only val Figure 5: Although adding the slow com- ponent of the gradients is effective in ac- celerating grokking, the slow component cannot be used alone as a replacement. Figure 4 demonstrates high effectiveness of our approach. However, few questions are still left unanswered regard- ing the modified training dynamics and the combined ef- fect with the weight decay, which is previously shown to be another important algorithmic factor that governs the grokking effect [Liu et al., 2022b]. We devise more experiments to answer these questions: Q1. Are both slow and fast gradients necessary? Our approach is based on our belief that the low-pass filtered gradient updates, or the slow gradients, contribute to the generalization. The most obvious question is then: can we not use the fast gradients and replace the original sequence 4 of gradients with the low-pass filtered components? Using only the slow gradients calculated from a moving average filter in Algorithm 1 is equivalent to using larger, overlapping minibatches. We conduct an experiment with a modified algorithm that replaces the line 8 of Algorithm 1 with ˆgt ← λ · Avg(Q) . Figure 5 shows the result. 1-stage means that the gradient replacement happens from the beginning of the training, which is set by default, and 2-stage means that the effect of GROKFAST happens after the model overfits to the training data at iteration 500 . The results clearly reveal that removing the original gradients leads to much slower and unstable training. In conjunction with the result in Figure 4, we can conclude that both the fast and the slow components of the gradients are necessary for faster grokking. 101 102 103 104 10529401920 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy baseline train baseline val 1-stage (Grokfast) train 1-stage (Grokfast) val 2-stage train 2-stage val 2-stage slow-only train 2-stage slow-only val ×1.53 faster Figure 6: We can further accelerate the grokking effect with a two-staged algo- rithm, by applying GROKFAST-MA af- ter the model is overfitted (after 500 its). Table 1: Summary of results of Figure 6. Name ˆgt at (A → B) ˆgt at (B → C) Iterations at acc ≥ 0.95 Baseline gt gt 39,890 [its] (×1) 1-Stage gt + λ · Avg(Q) gt + λ · Avg(Q) 2,940 [its] (×13.57) 2-Stage gt gt + λ · Avg(Q) 1,920 [its] (×20.78) 2-Stage Slow-only gt λ · Avg(Q) Not converged Q2. Exploiting state transition in the training of a model under grokking. We can alternatively interpret the training dynamics of a model under the grokking phe- nomenon as a state transition. In this viewpoint, the model sequentially goes through three distinct stages: (A) ini- tialized, where both training and validation losses are not saturated, (B) overfitted, where the training loss is fully saturated but the validation loss is not, and (C) generalized, where both losses are fully saturated. In the experimental setting of Figure 4, state transition of A → B happens roughly after iteration 500 . This interpretation allows us to try out a staged strategy for optimization, where dif- ferent algorithms are applied to the model during the two transition phases A → B (from iteration 0 to 499) and B → C (after iteration 500) as described in Table 1. Fig- ure 6 and Table 1 summarize the result of the experiment. As the results show, we can accelerate the grokking effect further by ×1.53 by separating the training stage of the model and applying GROKFAST-MA only after the model becomes overfitted, suggesting an adaptive optimizer. 101 102 103 104 1053.99 × 1042.94 × 1037.90 × 102 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy baseline train baseline val baseline wd = 0.01 train baseline wd = 0.01 val Grokfast wd = 0 train Grokfast wd = 0 val Grokfast wd = 0.01 train Grokfast wd = 0.01 val ×13.57 faster ×50.49 faster Figure 7: The acceleration effect of GROKFAST-MA is greatly enhanced when accompanied with appropriate value of weight decay. However, the weight decay alone not always yield beneficial results. Q3. Synergistic effect with weight de- cay. Besides from our gradient filtering approach, the authors of Omnigrok [Liu et al., 2022b] have suggested that the weight decay hyperparameter is a critical determinant of the grokking phenomenon. According to the report, the grokking phenomenon appears and even becomes faster when the weight decay becomes larger. We, therefore, conduct additional experiments to find out how these two ap- proaches affect the model when applied together. The results are summarized in Figure 7. Compared with the result from GROKFAST-MA with no weight decay (orange), applying the weight decay (red) generally yields even faster generaliza- tion. The maximum acceleration appears at wd = 0.01 with ×3.72 faster generalization than GROKFAST-MA with no weight decay. We choose this result of ×50.49 faster grokking to be our main demonstration in Figure 2a. Interestingly, Figure 7 also reveals that applying the same weight decay with no GROKFAST-MA (brown) makes the training unstable. The results demonstrates that applying our gradient filtering and setting up a proper weight decay together gives synergistic benefits. 2.4 Limitations Although Algorithm 1 shows highly effective results, it requires w times more memory to store all the previous gradients, limiting its utilization. Replication of the model parameters also makes the training slower; using w = 100 , the training time per iteration is increased by ×2.4 measured with a 5 101 102 103 104 1053.99 × 1049.10 × 102 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95Accuracy baseline train baseline val Grokfast train Grokfast val ×43.84 faster (a) Accuracy of the modular multiplication task. 101 102 103 104 1053.99 × 1049.10 × 102 Optimization Steps 0 2 4 6 8 10Loss baseline train baseline val Grokfast train Grokfast val (b) Loss of the modular multiplication task. 0.00 0.20 0.40 0.60 0.80 1.00 0.95TrainAccuracy baseline λ = 1 λ = 2 λ = 5 101 102 103 104 1058.40 × 103 3.99 × 104 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95ValAccuracy×4.75 faster baseline λ = 1 λ = 2 λ = 5 (c) Accuracy w.r.t. amplifier gain λ . 0.00 0.20 0.40 0.60 0.80 1.00 0.95TrainAccuracybaseline α = 0.8 α = 0.9 α = 0.95 α = 0.98 α = 0.99 101 102 103 104 1054.65 × 103 3.99 × 104 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95ValAccuracy×8.58 faster baseline α = 0.8 α = 0.9 α = 0.95 α = 0.98 α = 0.99 (d) Accuracy w.r.t. momentum α . 0.00 0.20 0.40 0.60 0.80 1.00 0.95TrainAccuracy baseline Grokfast wd = 0 Grokfast wd = 0.002 Grokfast wd = 0.005 Grokfast wd = 0.01 101 102 103 104 1053.99 × 1049.10 × 102 Optimization Steps 0.00 0.20 0.40 0.60 0.80 1.00 0.95ValAccuracy×43.84 faster baseline Grokfast wd = 0 Grokfast wd = 0.002 Grokfast wd = 0.005 Grokfast wd = 0.01 (e) Accuracy w.r.t. weight decay. Figure 8: Acceleration of delayed generation with GROKFAST-EMA (GROKFAST). The task is the same modular multiplication as in Figure 4. The amount of acceleration relies on three hyperparameters, the amplifier gain λ , the window size w , and the weight decay wd . Figures 8a and 8b use α = 0.98 , λ = 2.0 , and wd = 0.005 . Figures 8c and 8d show acceleration results when wd = 0 . Figures 8c, 8d, and 8e use the same set of hyperparameters unless specified otherwise. single 1080 Ti GPU. Still, the reduction of wall clock time before the delayed generalization of the results in Figure 7 is ×20.5 , which is also a notable reduction of time. Though the computation time does not linearly scale with the memory requirements, Algorithm 1 is not generally applicable to the larger models. This problem leads to our new design utilizing sequential filter in the next section. 3 Grokfast with Exponential Moving Average Gradient Filter In the previous section, we empirically prove that using an LPF to the sequence of model parameter updates leads to faster generalization under the grokking phenomenon. However, for practical purposes, we require an LPF design with a smaller memory footprint. To this end, we modify Algorithm 1 by replacing the windowed moving average with an exponential moving average (EMA) filter. The impulse response of the filter becomes: h(t) = λ(1 − α) t∑ τ =0 ατ δ(t − τ ) = λαt(1 − α) , (7) where δ(t) is the discrete unit impulse at the origin. This filter also has two hyperparameters: the scalar factor λ and the scalar momentum α . The corresponding Algorithm 2 only requires additional memory with the same size of the model itself, reducing ×50 amount of required memory compared to Algorithm 1. The time and the frequency responses of the filters are compared in Figure 3. 4 Experiment Although the grokking phenomenon was first reported in the algorithmic dataset, Omnigrok [Liu et al., 2022b] shows that such behavior can also be observed in a diverse set of tasks with larger and 6 101 102 103 104 1054.4 × 1042.0 × 103 Optimization Steps 0.0 0.2 0.4 0.6 0.8 1.0Accuracy 0.95 Accval = 0.85 MNIST Image Classiﬁcation baseline train baseline val Grokfast train Grokfast val ×22.0 faster (a) Accuracy of MNIST. 101 102 103 104 1054.4 × 1042.0 × 103 Optimization Steps 10−5 10−4 10−3 10−2 10−1 100 101 102 103MSELoss MNIST Image Classiﬁcation baseline train baseline val Grokfast train Grokfast val (b) Loss of MNIST. Figure 9: MNIST results with a three-layer MLP. Grokking phenomenon is almost gone with proper hyperparameters. 102 103 104 Optimization Steps 10−4 10−3 10−2 10−1 100 101 102MSELoss Min baseline val loss = 6.59E-3 Min Grokfast val loss = 3.48E-3 Thin curve: Raw value Thick curve: EMA(0.99) QM9 Molecule Isotropic Polarizability Prediction baseline train baseline val Grokfast train Grokfast val Figure 10: QM9 dataset results with a GCNN. GROKFAST achieves faster and better convergence. more complex datasets. This section validates the efficacy of our accelerating algorithm, GROKFAST , for those various tasks and models that exhibit the grokking phenomenon. 4.1 Algorithmic Data We first train the same task with the same model as in Section 2 using our new Algorithm 2. This is the same modular multiplication task devised to report the grokking phenomenon [Power et al., 2022]. Consuming much smaller computational resources (×50 less) compared to Algorithm 1, exponential moving average effectively captures the slow variation of the gradients necessary for accelerating the delayed generalization. Under the grokking phenomenon, the validation loss of the model first increases before it decreases again later during the late generalization stage as depicted in Figure 8b (baseline). This is well aligned with our state transition model interpretation in Q2 of Section 2.3. High difference in the validation loss implies that the generalization route B → C is much longer than the overfitting route A → B in the parameter space. In contrast, models under our GROKFAST training algorithms show significantly smaller peak in the validation loss as shown in Figures 2b and 8b. This implies that GROKFAST effectively keeps the generalization route B → C close to the global optimum at state C . We will revisit these conjectures in Section 5 with more visualization. We also conduct ablation studies to find out the effect of hyperparameters λ , α , and weight decay for our GROKFAST algorithm with an EMA filter. The optimal hyperparameters are found with grid search as in Section 2. Figures 8c through 8e summarizes the results. Recalling that our main idea is at the design of a low-pass filter, the momentum parameter α of Algorithm 2 as well as the window size parameter w of Algorithm 1 are equivalent to the cutoff frequency of the underlying filter. Experiments in Figures 8c through 8e as well as those in Figures 4 and 7 show that there exists a sweet spot in cutoff frequency that corresponds to the generalization-inducing gradient signal. From our empirical studies, we recommend λ ∈ [0.1, 5] and α ∈ [0.8, 0.99] . The weight decay is, like in typical optimization problems, dependent on the task of interest. 4.2 MNIST Besides the simple algorithmic reasoning task, where the data is relatively simple, Liu et al. [2022b] report the similar delayed generalization can also be observed in many conventional tasks if the model goes through a special treatment. To demonstrate the generalizability of our GROKFAST modification of the optimization process, we try to accelerate the speed of generalization under those reported models and tasks. The first is a three-layer ReLU-MLP trained for MNIST classification task [Deng, 2012] which exhibits the grokking phenomenon. Figure 9 summarizes the results, showing that our Algorithm 2 successfully accelerate the delayed generalization. With α = 0.8 , λ = 0.1 , and wd = 2.0 , the delay until grokking is reduced by ×22.0 . Moreover, the final evaluation accuracy becomes higher from 89.8% to 91.2%. 4.3 QM9 In the next experiment, we train a graph convolutional neural network (GCNN) trained for a molecule dataset QM9 [Ruddigkeit et al., 2012, Ramakrishnan et al., 2014]. Since this task does not have 7 100 101 102 103 104 Optimization Steps 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0Accuracy Best baseline val acc = 0.84 Best Grokfast val acc = 0.90 Thin curve: Raw value Thick curve: EMA(0.95) IMDb Binary Sentiment Analysis baseline train baseline val Grokfast train Grokfast val (a) Accuracy on IMDb sentiment analysis. 100 101 102 103 104 Optimization Steps 10−4 10−3 10−2 10−1 100BCELoss Min baseline val loss = 0.517 Min Grokfast val loss = 0.412 Thin curve: Raw value Thick curve: EMA(0.95) IMDb Binary Sentiment Analysis baseline train baseline val Grokfast train Grokfast val (b) Loss on IMDb sentiment analysis. Figure 11: IMDb results with a two-layer LSTM. LSTM exhibits grokking phenomenon if the training begins with larger weight values at initialization. GROKFAST algorithm produces faster generalization with higher validation error and lower validation loss. We show the exponential moving average (momentum 0.95) of the curves as thick lines for clearer visualization of the trend. an accuracy measure to compare the speed of convergence, we instead compare the convergence speed of the validation loss. With the same setup as in Omnigrok [Liu et al., 2022b], elaborated in Appendix B, we apply Algorithm 2 with α = 0.9 , λ = 1.0 , and wd = 0.01 to obtain the results in Figure 10. The validation loss drops faster and by a larger margin under GROKFAST. 4.4 IMDb Finally, we train a 2-layer LSTM [Hochreiter and Schmidhuber, 1997] network for sentiment analysis in the IMDb dataset [Maas et al., 2011] under the grokking phenomenon [Liu et al., 2022b]. Figure 11 compares the baseline with the model trained with an optimizer modified by Algorithm 2 with α = 0.98 , λ = 2.0 , and wd = 10.0. We visualize the convergence speed and quantitatively compare the best validation loss/accuracy. This experiment section suggests that GROKFAST generally boosts performance and convergence speed in diverse tasks under the grokking phenomenon. 5 More Discussion 5.1 Difference between Algorithm 2 and the Momentum in Typical Optimizers The lines 7-8 of Algorithm 2 take a similar form to the momentum variable, which is frequently used in optimizers in deep learning frameworks. However, notable differences exist: (1) Instead of using the scaled momentum as a parameter update, we use the smoothened gradient as a residual, which is added to the gradient before it is fed into the optimizer. Rather, the formula is more similar to Nesterov’s momentum; however, the filtering is applied before the optimizer, which is different from typical applications of Nesterov’s momentum such as NAdam [Dozat, 2016]. (2) The line 7-8 is applied to the gradients independently to the underlying optimizer. The optimizer can be of any type unless it is of the first-order gradient descent-based. Low-pass filtering the gradients g(t) has the same effect as filtering the post-optimizer parameter updates u(t) as mathematically explained in Appendix A with SGD and variants, and empirically proved in the previous sections with Adam [Kingma and Ba, 2014] and AdamW [Loshchilov and Hutter, 2018] optimizers. 5.2 Visualizing Trajectories In this final section, we elaborate on our state transition interpretation of grokking introduced in Section 2.3. Our signal space model of the training dynamics allows us to interpret the training of a model as a random drift of the state in the parameter space. To visualize the dynamics, we collect all the 423k parameters of the Transformer decoder [Vaswani et al., 2017] used in the experiment in Figure 2 for all iterations, and conduct the PCA to obtain the most salient projection of the parameter space. The sequence of evolving models are projected onto the space as in Figure 13a. Regarding state transition interpretation of grokking, we can observe the followings: First, Figure 12 suggests that, in the baseline setup, the model drifts through a significantly longer pathway from 8 0.0 0.2 0.4 0.6 0.8 1.0 1st Principal Axis 0.00 0.02 0.04 0.06 0.08 0.102ndPrincipalAxis Grokfast Trajectory common A (init) baseline B baseline CGrokfast B Grokfast C Trajectory in the Parameter Space 10 20 50 100 200 500 1k 2k 5k 10k 20k 50k 100k 200k 300k Gr okfast baseline (a) Parameter trajectories. 1 101 102 103 104 1051 500 3.00 × 105 Optimization Steps 10−2 10−1 100 101 102 103ℓ2Distance×1000 Curve=mean, Area=std, #Samples=5 Distance in Parameter Space from Initial Weights baseline GrokfaststateAstateBstateC (b) Deviation from initial weights. Figure 12: Trajectories of model parameters from experiments of Figure 2 projected onto two principal axes of the PCA of the interme- diate model parameters of the baseline. The two models travel along distinct pathways in the parameter space with different pace. Table 2: Parameter space dis- tances between the intermedi- ate models of experiments in Figure 2. Each state corre- sponds to each of the mark- ers of Figure 12. Average and standard deviations of five in- stances are shown. GROKFAST converges to a nearer point in the parameter space, and the baseline model travels longer to reach the final state. State Pair ℓ2 Distances (×1000) Baseline GROKFAST AB 0.97 ± 2.2 7.7 ± 0.42 BC 563.7 ± 199.4 15.3 ± 0.84 AC 570.7 ± 199.8 35.8 ± 0.98 the overfitting (state B , 500 steps) to its full generalization (state C , 300k steps), compared to the initial state (state A , 0 steps) to the overfitting state (state B). However, under GROKFAST, the ratio between the two distances AB and BC in the parameter space becomes more even. This is further acknowledged by Figure 12b and Table 2 showing the distances between the models at each state. Moreover, Table 2 suggests that the distances AC between the initial and the final state becomes much (×16) shorter with our GROKFAST algorithm. Although the generalization accuracy, the training accuracy, the training loss, and the validation loss at the final state (state C) are similar in both the baseline and GROKFAST as showcased in Figure 2, we cannot simply say that the states C of baseline and of GROKFAST belong to the same network state. Likewise the state B of the baseline and of GROKFAST are different. Figure 12b shows average deviation of parameter weights from the initialization point during training of the model under grokking phenomenon. Interestingly, at achieving overfitting at state B , the model under our algorithm deviates ×8 further from the initial point than the baseline does, with ×5 smaller standard deviation in distances from the initial state A . This suggests that although both algorithms exhibit overfitted behavior at state B , intermediate model instances at these states form distinct set of parameters with possibly different topologies. These observations support our interpretation to regard the grokking phenomenon as a state transition between at least three distinct states. The role of GROKFAST is then to provide supervision towards an alternative optimum much nearer from the initial points than the baseline optimizer. Lastly, the model trained with our GROKFAST algorithm shows hundredfold smaller variances of the distances than the baseline as claimed in Table 2. This implies that training under GROKFAST algorithm is much more deterministic than under typical first-order optimizers. This is possibly related to the similarity between the low-pass filtered gradients from small minibatches with normal gradients from larger minibatches. However, we have also demonstrated in Section 2.3 that using only the slow, more deterministic component of gradients and completely neglecting the original gradients lead to instability. Therefore, further investigation is needed to find out the source and the role of this determinism from our GROKFAST algorithm, and the reason of its benefits when jointly applied with the faster, more stochastic gradients from baseline optimizers. 6 Related Work Grokking. The recently discovered grokking phenomenon [Power et al., 2022] signifies one possibility of overparameterized neural networks generalizing (and reasoning) beyond memorization of the given dataset. Most of the works, thereafter, focus on identifying its mechanism. Recent studies associated grokking with a double descent phenomenon in the DNN mapping geometry training dynamics [Humayun et al., 2023], the speed of pattern learning [Davies et al., 2023], and the sizes of models and datasets [Huang et al., 2024], wherein the validation error initially increases and then decreases with the expansion of model parameters [Nakkiran et al., 2021, Belkin et al., 2019]. To investigate the internal roles of each model component during grokking, Nanda et al. [2023] employed mechanistic interpretability, a technique from the XAI domain, and revealed that grokking may not occur abruptly but rather exhibits an internal progress measure. Their assertion 9 posits that the model captures slow, generalizing patterns, underscoring the critical role of proper optimization. Interestingly, while weight decay amplifies the double descent effect [Pezeshki et al., 2022], it contributes to enhanced generalization in grokking scenarios [Power et al., 2022]. Liu et al. [2022b] found more examples of grokking in various tasks and analyzed their training mechanism through loss landscape analysis. Thilak et al. [2022] found a similarity between grokking and the slingshot mechanism in adaptive optimizers. Barak et al. [2022] argued that optimizers reach delayed generalization by amplifying sparse solutions through hidden progress. Regularizers such as weight decay [Nanda et al., 2023] and the choice of the optimizer [Liu et al., 2022a] are highlighted as important factors in training a model that groks. Our work is a continuation of this discussion by providing a generalizable tool for the practical study of the grokking phenomenon. Through our discussion, we suggest a state transition model of the grokking and visualize the trajectory of the model weights in the parameter space during training. Optimization techniques. At the core of the study of grokking lies optimization techniques [Thi- lak et al., 2022]. Studies have shown that generalization patterns of the model vary significantly depending on various optimization methods [Power et al., 2022, Gromov, 2023]. Power et al. [2022] demonstrated that various factors related to optimization, such as (mini-)batch training [Li et al., 2014], the choice of optimizer, weight decay [Loshchilov and Hutter, 2018], noise injection [Zur et al., 2009], dropout [Srivastava et al., 2014], and learning rate, influence the model’s grokking pattern. Nanda et al. [2023] argued that grokking does not occur without proper regularization. Further, they demonstrated that techniques such as weight decay, L2 norm, and dropout induce grokking, but L1 norm does not. On the other hand, Thilak et al. [2022] argued that grokking can occur without explicit regularization, attributing this to the optimizer’s “visible slingshot mechanism” acting as an implicit regularizer. Liu et al. [2022a] suggested using a larger learning rate for the input embedding segment, facilitating unified learning of the generalization pattern. Unlike these revisiting of the known training techniques, we started from a state space model and a dual domain of the training dynamics. This led us to develop an optimizer augmentation algorithm, GROKFAST, that can be applied to any existing first-order optimizers to accelerate the grokking effect for practical usage. 7 Conclusion Our reinterpretation of the deviation of each model parameter into a random signal over training iteration allows us to separate gradient updates into fast-varying and slow-varying components. By amplifying the latter with low-pass filtering, we can bring forward the moment of sudden late generalization, i.e., grokking, reducing the number of required training iterations by up to ×50 . Our comprehensive experiments and analyses suggest that our state space interpretation and the frequency representation of the training dynamics is useful for studying the grokking phenomenon. References Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. In NeurIPS, 2022. 10 Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854, 2019. 9 Xander Davies, Lauro Langosco, and David Krueger. Unifying grokking and double descent. arXiv preprint arXiv:2303.06173, 2023. 1, 9 Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine, 29(6):141–142, 2012. 7, 14 Timothy Dozat. Incorporating nesterov momentum into adam. In ICLR Workshop, 2016. 8 Andrey Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023. 10 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing Human- Level Performance on ImageNet Classification. In ICCV, 2015. 14, 15 Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016. 14 10 Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997. 9.8.1735. 8, 15 Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Unified view of grokking, double descent and emergent abilities: A perspective from circuits competition. arXiv preprint arXiv:2402.15175, 2024. 1, 9 Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. Training dynamics of deep network linear regions. arXiv preprint arXiv:2310.12977, 2023. 9 Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 8, 14 Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. In Advances in NIPS 2016 Deep Learning Symposium, 2016. 14 Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 661–670, 2014. 10 Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. In NeurIPS, 2022a. 10 Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data. arXiv preprint arXiv:2210.01117, 2022b. 1, 2, 4, 5, 6, 7, 8, 10, 14 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018. 8, 10, 15 Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150, 2011. 8, 15 Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021. 9 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. 9, 10 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 2, 13, 14, 15 Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale feature learning dynamics: Insights for double descent. In ICML, 2022. 10 Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022. 1, 4, 7, 9, 10, 14 Raghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1(1):140022, Aug 2014. ISSN 2052-4463. doi: 10.1038/sdata.2014.22. URL https://doi.org/10.1038/sdata.2014.22. 7, 15 Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17. Journal of Chemical Information and Modeling, 52(11):2864–2875, Nov 2012. ISSN 1549-9596. doi: 10.1021/ci300415d. URL https: //doi.org/10.1021/ci300415d. 7, 15 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1): 1929–1958, 2014. 10 Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua M Susskind. The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon. In Has it Trained Yet? NeurIPS 2022 Workshop, 2022. 10 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In NIPS, 2017. 1, 2, 4, 8, 14, 17 Richard M Zur, Yulei Jiang, Lorenzo L Pesce, and Karen Drukker. Noise injection for training artificial neural networks: A comparison with weight decay and early stopping. Medical physics, 36(10):4810–4818, 2009. 10 11 A Frequency Responses of the Parameter Updates under Grokfast In Section 1, we have silently assumed that under a first-order optimizer u(g(t), t) , amplifying the low-frequency components of the gradient signal g(t) of an arbitrary parameter θ(t) over a discrete timestep t has the same effect of amplifying the low-frequency component of the parameter updates u(t) = u(g(t), t) . This section mathematically elaborates on the effect of gradient filters h(t) to the parameter update signals u(t) in the most frequently-used type of optimizers: SGD with momentum. Stochastic gradient descent with optional momentum term is the simplest and the most widely used optimization algorithm in the deep learning communities. Here, the parameter update u(t) = θ(t + 1) − θ(t) of a parameter θ(t) at timestep t and its intermediate momentum m(t) is defined as: m(t) = µm(t − 1) + (1 − τ )g(t) , (8) u(t) = −ηm(t) , (9) where µ is the scalar momentum, τ is the dampening constant for the momentum, and η is the learning rate. This class of optimizers can be thought of as linear systems with state m(t) that receives an input g(t) to produce an output u(t) . To compare the difference between the frequency responses of the parameter update u(t) and of the modified update ˆu(t) in equation (4), we can think of an equivalent filter ˆh(t) defined to satisfy the following relationship in addition to equations (3) and (4): ˆu(t) = u(ˆg(t), t) = u(g(t) + h(t) ∗ g(t), t) = u(t) + ˆh(t) ∗ u(t) . (10) From our assumption of the linear time-invariant, scalar filters h(t) and the linear optimizer, we can deduce the equivalence between h(t) and ˆh(t) . The following theorem is a generalized claim that applies to any SGD-based first-order optimizers including Nesterov’s momentum. Theorem A.1. Let g(t) be a scalar signal defined over a discrete time t ∈ {0, 1, . . . , T } . Let h(t) be a univariate time-invariant filter defined over the same domain t . A linear optimizer O is defined as: x(t) = Ax(t − 1) + Bg(t) , t > 0 , (11) u(t) = Cx(t) + Dg(t) , t ≥ 0 , (12) with scalar coefficients A, B, C, and D , and x(0) = g(0) . The output of the system u(t) is, therefore, a function of g(t) and t , i.e., u(t) = u(g(t), t) . Let the modified input ˆg(t) , the modified output ˆu(t) , and the equivalent filter ˆh(t) be defined to satisfy the equations (3) and (10). Then, ˆh(t) = h(t) , (13) for t ∈ {0, 1, . . . , T } . Proof of Theorem A.1. For simplicity, we first adopt discrete-time Fourier transform over t ∈ Z . That is, we assume that the signals are defined across every positive and negative integer t . Since the value of u(t) can be defined arbitrarily outside the interval [0, T ] without modifying the optimization algorithm, we can manually assign g(t) and x(t) for t /∈ [0, T ] as: g(t) = 0 t /∈ [0, T ] , (14) x(t) = {At(1 − B)g(0) t < 0 , At−T x(T ) t > T . (15) Then, equations (11) and (12) hold for t /∈ [0, T ] . Note that to make the optimizer O stable, the scalar coefficient A should satisfy 0 < A < 1 . Therefore, the signals g(t) and x(t) are well-defined. Consider a discrete-time Fourier transform F defined as: F{f (t)}(ω) = ∞∑ t=−∞ f (t)e−iωt . (16) In the frequency domain, with G(ω) = F{g(t)} , U (ω) = F{u(t)} , and X(ω) = F{x(t)} , the optimizer O can be equivalently represented as: X(ω) = Ae−iωX(ω) + BG(ω) , (17) U (ω) = CX(ω) + DG(ω) . (18) 12 We can obtain the transfer functions Hin-state and Hin-out that converts G to X and then to U : Hin-state(ω) := X(ω) G(ω) = B 1 − Ae−iω , (19) Hin-out(ω) := U (ω) G(ω) = C X(ω) G(ω) + D = BC 1 − Ae−iω + D . (20) If the input g(t) is filtered with a convolutional filter h(t) and then added to itself as equations (3) and (5), the state x(t) and the output u(t) of the optimizer O is changed accordingly while keeping equations (11) and (12) hold. We denote ˆx(t) and ˆu(t) as the modified state and output of the system and ˆX(ω) and ˆU (ω) as their spectra. If the filter h(t) is causal, that is h(t) = 0 for t < 0 , then we can similarly let ˆx(0) = ˆg(0) and replace x(t) and u(t) with ˆx(t) and ˆu(t) in equations (14) and (15) to define an IIR system suitable for the infinite-window discrete-time Fourier transform F : ˆX(ω) = Ae−iω ˆX(ω) + B ˆG(ω) , (21) ˆU (ω) = C ˆX(ω) + D ˆG(ω) . (22) Since the coefficients of the linear systems are the same, the transfer functions are identical: ˆHin-state(ω) := ˆX(ω) ˆG(ω) = B 1 − Ae−iω ≡ Hin-state(ω) , (23) ˆHin-out(ω) := ˆU (ω) ˆG(ω) = BC 1 − Ae−iω + D ≡ Hin-out(ω) . (24) From equation (5), the transfer function of the filter Hamp(ω) is: Hamp(ω) = ˆG(ω) G(ω) = 1 + H(ω) , (25) where H(ω) = F{h(t)} . The equivalent post-filter ˆh(t) defined by equation (10) gives another transfer function between the outputs u(t) and ˆu(t) of the system: ˆHamp(ω) = ˆU (ω) U (ω) = 1 + ˆH(ω) . (26) From equations (20) and (24), we have: ˆHamp(ω) = ˆU (ω) U (ω) = ˆG(ω) G(ω) = Hamp(ω) . (27) Therefore, we get: ˆH(ω) ≡ H(ω) . (28) This completes the proof. In other words, applying any filter h(t) to the sequence of gradients g(t) is equivalent to the same filter h(t) applied to the parameter update u(t) for any linear optimizer O . This implies that a low-pass gradient filter h(t) guarantees the same low-pass property in the modified parameter update signal ˆu(t) . In many off-the-shelf autograd packages such as PyTorch [Paszke et al., 2019], filtering the gradients is easier and more straightforward than filtering the intermediate parameter updates. The former only adds a few more lines to the outermost application code 2, whereas the latter requires full implementation of the dedicated optimizer object. Note that the above proof holds regardless of the design of the filter h(t) unless there exists a one-to-one correspondence between Hamp and H . The followings are direct consequences of the Theorem A.1. Proposition A.2 (SGD with momentum). Let t ∈ {0, 1, . . . , T } be a discrete timestep. Let g(t) be a sequence of gradients of a parameter θ sampled from a stochastic machine learning framework g(t) ∼ M (θ(t), t) and a stochastic gradient descent optimizer O(µ, τ, η) with a parameter update 2See our implementation at https://github.com/ironjr/grokfast. 13 function u(g(t), t) = u(t) = θ(t + 1) − θ(t) , a momentum µ , a damping constant τ , and a learning rate η . The parameter update u(t) is, therefore, defined as: m(t) = µm(t − 1) + (1 − τ )g(t) , (29) u(t) = −ηm(t) , (30) with a scalar momentum term m(t) for each parameter θ with m(0) = g(0) . Let h(t) be a scalar, time-invariant, convolutional gradient filter. Let the modified input ˆg(t) , the modified output ˆu(t) , and the equivalent filter ˆh(t) be defined to satisfy the equations (3) and (10). Then, ˆh(t) = h(t) , (31) for t ∈ {0, 1, . . . , T } . Proof of Proposition A.2. Let A = µ , B = 1 − τ , C = −η , and D = 0 . By Theorem A.1, equation (31) holds. Proposition A.3 (SGD with Nesterov’s momentum). Let t ∈ {0, 1, . . . , T } be a discrete timestep. Let g(t) be a sequence of gradients of a parameter θ sampled from a stochastic machine learning framework g(t) ∼ M (θ(t), t) and a stochastic gradient descent optimizer O(µ, τ, η) with a parameter update function u(g(t), t) = u(t) = θ(t + 1) − θ(t) , a momentum µ , a damping constant τ , and a learning rate η . The parameter update u(t) is, therefore, defined as: m(t) = µm(t − 1) + (1 − τ )g(t) , (32) u(t) = −η(g(t) + µm(t)) , (33) with a scalar momentum term m(t) for each parameter θ with m(0) = g(0) . Let h(t) be a scalar, time-invariant, convolutional gradient filter. Let the modified input ˆg(t) , the modified output ˆu(t) , and the equivalent filter ˆh(t) be defined to satisfy the equations (3) and (10). Then, ˆh(t) = h(t) , (34) for t ∈ {0, 1, . . . , T } . Proof of Proposition A.3. Let A = µ , B = 1 − τ , C = −ηµ , and D = −η . By Theorem A.1, equation (34) holds. B Task Details For completeness, this section summarizes the implementation details of each task dealt in Section 4. The readers can also consult our official implementation in PyTorch [Paszke et al., 2019]. B.1 Binary Operation (Algorithmic Data) This is the description of algorithmic data used throughout the manuscript. Following the first report on the grokking phenomenon [Power et al., 2022], we demonstrate our acceleration algorithms with a binary operation x · y (mod p) , with p = 97 . The network is a two-layer decoder-only Transformer [Vaswani et al., 2017] with hidden dimension of 128 and 4 heads in its attention. The positional embedding has length of 5, and GELU [Hendrycks and Gimpel, 2016] and layer normalization [Lei Ba et al., 2016] is used throughout the network. After the Transformer blocks, the output is fed into a layer normalization and a linear output layer to return logits. We use cross entropy loss to train the network and an Adam [Kingma and Ba, 2014] with betas (β1, β2) = (0.9, 0.98) , a constant learning rate of 10−3 , batch size of 512 , and linear learning rate warmup schedule over the first 10 iterations. B.2 MNIST We train a three-layer MLP with hidden width of 200 and ReLU activations for the MNIST classi- fication task [Deng, 2012]. Under ×8 larger weight initialization than Kaiming initialization [He et al., 2015], the network is known to exhibit the grokking phenomenon [Liu et al., 2022b]. The 14 network receives flattened grayscale images of size 28 × 28 and outputs 10-dimensional logits to calculate MSE losses between one-hot encoded labels. We use the batch size of 200 and trained with an AdamW optimizer [Loshchilov and Hutter, 2018] with a constant learning rate of 10 −3 until 10 5 training iterations. We use a smaller subset of 1000 images from training images to train the network in order to simulate overfitting environment. All the other hyperparameters are set by default. B.3 QM9 To demonstrate the effectiveness of our algorithm on a graph convolutional neural network, we use QM9 small molecules dataset [Ruddigkeit et al., 2012, Ramakrishnan et al., 2014] to estimate the isotropic polarizability. Our Graph ConvNet has two graph convolution layers with input channel of 11 (QM9 edge features), output channel of 16, and hidden channel of 32. Each graph convolution is followed by a ReLU. Each convolution layer consists of two linear layers with an internal ReLU activation with hidden channel of 32. The output of the Graph ConvNet is a global average pooling, followed by a two-layer MLP with a ReLU and hidden channel of 32. To simulate the overfitting environment, we use the first 100 samples from the data. The data is again randomly split into train and validation sets with 50:50 size ratio. We use batch size of 32, an AdamW optimizer [Loshchilov and Hutter, 2018] with a constant learning rate of 10 −3 . The network is initialized with weights ×3 larger than that of Kaiming initialization [He et al., 2015] and trained for 50k iterations. B.4 IMDb For IMDb dataset [Maas et al., 2011], we use LSTM [Hochreiter and Schmidhuber, 1997] with two layers, embedding dimension of 64, hidden dimension of 256, and vocabulary size of 1001, including the padding token. The network is followed by a single fully connected layer with output dimension of 1 with sigmoid activation to classify the positive/negative sentiment of each review string. The dataset was preprocessed by tokenizing the 1000 most frequent words from the review. The list of integer tokens are padded by zeros to form an array of reviews with the same length of 500. The network was trained by a binary cross entropy loss and an AdamW optimizer [Loshchilov and Hutter, 2018] with learning rate of 3 × 10 −4 and batch size of 50. We trained the model with the first 1000 rows from the dataset, split into train and validation sets with 75:25 size ratio. We stopped the training at 10k iterations as shown in Figure 11. C Autograd Implementation We have argued that our implementation of Algorithm 1 and 2 costs only a few additional lines of code. We demonstrate this by presenting the exact code we developed with the PyTorch [Paszke et al., 2019] autograd package. The readers who are interested can also consult our official implementation 3. Algorithms 1 and 2 are implemented as follows: 1 # Grokfast-MA (Algorithm 1) 2 def gradfilter_ma( 3 m: nn.Module, 4 grads: Optional[Dict[str, deque]] = None, 5 window_size: int = 100, 6 lamb: float = 5.0, 7 filter_type: Literal[’mean’, ’sum’] = ’mean’, 8 warmup: bool = True, 9 ) -> Dict[str, deque]: 10 if grads is None: 11 grads = {n: deque(maxlen=window_size) for n, p in m.named_parameters() if p.requires_grad} 12 13 for n, p in m.named_parameters(): 14 if p.requires_grad: 15 grads[n].append(p.grad.data.detach()) 16 17 if not warmup or len(grads[n]) == window_size: 3https://github.com/ironjr/grokfast 15 18 if filter_type == \"mean\": 19 avg = sum(grads[n]) / len(grads[n]) 20 elif filter_type == \"sum\": 21 avg = sum(grads[n]) 22 else: 23 raise ValueError(f\"Unrecognized filter_type {filter_type}\") 24 p.grad.data = p.grad.data + avg * lamb 25 26 return grads 27 28 # Grokfast (Algorithm 2) 29 def gradfilter_ema( 30 m: nn.Module, 31 grads: Optional[Dict[str, torch.Tensor]] = None, 32 alpha: float = 0.98, 33 lamb: float = 2.0, 34 ) -> Dict[str, torch.Tensor]: 35 if grads is None: 36 grads = {n: p.grad.data.detach() for n, p in m.named_parameters() if p.requires_grad} 37 38 for n, p in m.named_parameters(): 39 if p.requires_grad: 40 grads[n] = grads[n] * alpha + p.grad.data.detach() * (1 - alpha) 41 p.grad.data = p.grad.data + grads[n] * lamb 42 43 return grads This helper method can be applied to any optimization framework involving the autograd package by inserting a single line between the calculation of the gradients and the optimizer call as follows: 1 # ... any initialization code before starting the training loop. 2 grads = None 3 4 # Training loop. 5 for batch in dataloader: 6 model.zero_grad() 7 output = model(batch) 8 loss = criteria(output) 9 10 # Calculate the gradients. 11 loss.backward() 12 13 # Option 1: Grokfast (has argument alpha, lamb) 14 grads = gradfilter_ema(model, grads=grads, alpha=alpha, lamb=lamb) 15 # Option 2: Grokfast-MA (has argument window_size, lamb) 16 # grads = gradfilter_ma(model, grads=grads, window_size=window_size, lamb=lamb) 17 18 # Call the optimizer. 19 optimizer.step() 20 21 # ... any additional logging codes. Note that line 2 and line 14 in the code above are the only modification we made. D Time and Memory Requirements This section delivers additional demonstration of the efficiency of our GROKFAST algorithm. As we have argued in Section 2.4 and Section 3, the additional computational burden from our augmentation is compensated by the larger-scale acceleration of the delayed generalization. The additional cost of 16 Table 3: Quantitative results of GROKFAST with a Transformer decoder trained for the algorithmic data (modular multiplication). The experiments corresponds to that of Figure 2 and 8a & 8b. Algorithm Iterations @ 95% Val. Acc. Wall Clock Time @ 95% Val. Acc. (s) VRAM (MB) Latency Per Iteration (s) Baseline 39890 5984 290 0.15 GROKFAST-MA 790 (× 50.49 ↓) 292 (× 20.49 ↓) 458 0.37 GROKFAST 910 (× 43.84 ↓) 137 (× 43.79 ↓) 294 0.15 Table 4: Quantitative results of GROKFAST with an MLP trained for MNIST (Figure 9). Algorithm Iterations @ 95% Val. Acc. Wall Clock Time @ 95% Val. Acc. (s) VRAM (MB) Latency Per Iteration (ms) Baseline 44022 1928 196 43.8 GROKFAST 2001 (× 22.00 ↓) 87.8 (× 21.96 ↓) 198 43.9 Table 5: Quantitative results of GROKFAST with a G-CNN trained for QM9 (Figure 10). Algorithm Minimum Val. Loss VRAM (MB) Latency Per Iteration (ms) Baseline 0.00659 216 40.2 GROKFAST 0.00348 216 41.4 Table 6: Quantitative results of GROKFAST with an LSTM trained for IMDb (Figure 11). Algorithm Best Val. Acc. Minimum Val. Loss VRAM (MB) Latency Per Iteration (ms) Baseline 0.84 0.517 754 20.4 GROKFAST 0.90 0.412 762 21.2 −0.2 0.0 0.2 0.4 0.6 0.8 1st Principal Axis −0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.62ndPrincipalAxis common A (init)baseline B (run1) baseline C (run1) Grokfast B (run1)Grokfast C (run1)baseline B (run2) baseline C (run2) Grokfast B (run2)Grokfast C (run2) baseline B (run3) baseline C (run3) Grokfast B (run3)Grokfast C (run3) baseline B (run4) baseline C (run4) Grokfast B (run4)Grokfast C (run4) baseline B (run5) baseline C (run5) Grokfast B (run5)Grokfast C (run5) Grokfast Trajectory Trajectory in the Parameter Space 10 20 50 100 200 500 1k 2k 5k 10k 20k 50k 100k 200k 300k Gr okfast baseline (a) Parameter trajectories. −0.015 −0.010 −0.005 0.000 0.005 0.010 1st Principal Axis −0.020 −0.015 −0.010 −0.005 0.000 0.005 0.010 0.0152ndPrincipalAxis common A (init) baseline B Grokfast B Grokfast C Magniﬁed Trajectory in the Parameter Space 10 20 50 100 200 500 1k 2k 5k 10k 20k 50k 100k 200k 300k Gr okfast baseline (b) Magnification of the orange box. Figure 13: Normalized trajectories of model parameters from five runs of the experiment of Figure 2. The baseline optimization algorithm without GROKFAST guide the model to the overfitting states (state B) relatively closer to the initialization states (state A). After reaching the overfitting state, however, the model parameters travel far away to reach the generalization states (state C). GROKFAST instead guide the model parameters to the alternative generalization states (state C), which are much closer to the initialization states (state A). VRAM memory is also negligible compared the baseline. The time and the memory requirements in the Tables 3 through 6 are measured with a single GTX 1080 Ti GPU. E More Visualization We finally provide more visualization in addition to Section 5 in order to understand the training dynamics under our GROKFAST algorithm. Figure 13 shows five more runs from the same experiments in Figure 2 and 12 with different seeds. We saved all the 423k parameters of the Transformer decoder [Vaswani et al., 2017] from every training iteration, likewise in Figure 12. The parameters of a model checkpoint from each run at each iteration are reshaped into a single long vector. The vectorized parameters are then normalized by subtracting them by the model’s initialized weights. This way, we can align the trajectories by centering the initial states (state A) of all the experiments at the origin. The sequence of parameter differences of the ten runs from the two algorithms, i.e., baseline and GROKFAST, forms a tensor of shape ((Number of Runs) · (Number of Iterations)) × (Number of Parameters) = ((Number of Sampled Iterations) × 422784) . From this we perform the PCA to obtain the projection matrix of shape 422784 × 2 . This matrix projects the parameter differences from each of the model checkpoint onto the two most salient directions of variations. 17 We mark the initialization state (state A), the overfitting state (state B , 500 iterations), and the generalization state (state C) from each run in the two-dimensional plot. The results are Figure 13. We first notice that the overfitting states (state B) from each of the two optimizers are clearly different. The baseline algorithm without GROKFAST reaches the overfitting states (state B), which are relatively nearer to the initialization states (state A) than those of GROKFAST algorithm. However, as soon as the model overfits, the weights continue to deviate far from the points where overfitting first occured (state B). As a result, the final generalization (state C) happens much far away from the initialized weights (state A). It is notable that the five generalization states (state C) from different instances of the baseline optimizer vary significantly. The difference between the states C of the baseline is much larger than that of the baseline’s overfitting states (state B) and that of the states B and C from our GROKFAST algorithm. In contrast, the training dynamics from GROKFAST results in a distinct set of trajectories that lead to the generalization states (state C) much closer to the initial weights. Moreover, difference within the trajectories from GROKFAST is much smaller than that of the baseline algorithms. This conclusion is also verifiable from Figure 13b and Table 2 in a quantitative manner. 18","libVersion":"0.3.2","langs":""}
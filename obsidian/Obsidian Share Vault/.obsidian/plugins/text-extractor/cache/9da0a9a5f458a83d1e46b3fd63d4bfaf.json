{"path":"lit/lit_sources/Kotak22ElectionPollsAre.pdf","text":"a publication of the behavioral science & policy association 1 Election polls are 95% confident but only 60% accurate Aditya Kotak & Don A. Moore abstract * Election polls in the United States are more confident than accurate— meaning the reported margins of error often do not encompass the actual election outcomes in spite of pollsters claiming a 95% confidence level (that is, a 95% chance that their predictions will fall within the margin of error). In an analysis of polls for more than 6,000 contests, we have found that the actual vote total for a given candidate fell within the 95% confidence interval for just 60% of the polls. This degree of accuracy was reached only when the polls were conducted in the week before an election; accuracy was worse for polls conducted earlier. Polls would, in fact, need margins of error at least twice their current standard reported width to achieve 95% accuracy. We have also found that when laypeople read about poll results, they tend to overestimate the poll’s accuracy, even when they have historical data demonstrating that the predictions made by polls are often inaccurate. These results illustrate polls’ vulnerability to overconfidence and the limitations of the lay public’s understanding of these shortcomings. We conclude by suggesting ways that pollsters and reporters could enable the public to interpret poll data more realistically. Kotak, A., & Moore, D. A. (2022). Election polls are 95% confident but only 60% accurate. Behavioral Science & Policy, 8(2), 1–12. finding 2 behavioral science & policy | volume 8 issue 2 2022 I n 2016, the world was stunned by a couple of surprising election outcomes. On June 23, 52% of voters in the United Kingdom elected to leave the European Union. Many eligible voters who supported staying in the European Union did not bother to vote that day, possibly because most of the credible polls forecast a likely victory for the “remain” side.1 Voter turnout was lowest in areas that most strongly favored remaining in the European Union, such as London, Scotland, and Northern Ireland.2 Then, in November, Donald Trump beat Hillary Clinton in a closely contested U.S. presidential election. On the eve of the election, poll aggregator Nate Silver’s website, FiveThirtyEight, gave Clinton a 71% chance of winning.3 Many Democrats who did not vote reportedly believed their votes did not matter.4 Polls influence more than voter turnout. The Commission on Presidential Debates allows candidates to participate only if polls indi- cate that they have the support of at least 15% of the electorate.5 Candidates leading in the polls attract more support, including financial contributions.6 And polls are powerful drivers of press attention and its notorious “horse race” coverage that focuses on who will win.7 Polls garner attention not only from the press and the public but also from politicians eager to divine the will of the electorate.8 Preelection polls have increased in number and frequency since their introduction by George Gallup in 1936. 9,10 Given the power of polls in democracies, all citizens ought to care about their accuracy. In this article, we compare poll- sters’ confidence with their accuracy. That is, we report on a project that assessed the extent to which polls’ confidence intervals encompassed the actual outcomes of races. In polling, as in statistical analyses in general, the term confi- dence has a specific meaning. Pollsters report a particular margin of error that quantifies their degree of uncertainty about their prediction. If a pollster reports that 50% of 800 likely voters favor a particular candidate in an upcoming election and claims to have 95% confidence that the poll has a margin of error of ±3.5%, the pollster is also claiming there is a 95% likelihood that the candidate should receive between 46.5% and 53.5% of the vote from the broader electorate. We assessed the match between confidence and accuracy by measuring how often the 95% confidence intervals encompassed the actual election outcomes. We conclude that common reporting formats put too much faith in poll results, and we discuss ways to report election outcomes that could help the public interpret polling data more accurately. Sources of Error Before we describe our studies, we review some factors that can cause polls to be inaccu- rate. Statistical procedures help to quantify one potential source of error known as sampling error. Sampling error occurs when, despite a researcher’s best efforts, a group of people chosen by random sampling methods may not actually be representative of the population of interest after all. For instance, if pollsters seeking a representative sample of the voting popula- tion just happened to reach a preponderance of Clinton supporters ahead of the 2016 election, the finding that 60% of the poll’s respondents favored Clinton probably would not mean that 60% of votes actually cast in the election would be for Clinton. When someone is calculating the confidence interval for an outcome, statis- tical methods helpfully take into account the possibility that an erroneous prediction may result from a chance failure of random sampling methodology. However, researchers who study survey errors have documented at least five additional sources of error that are more difficult to quan- tify.11,12 Specification error is the result of a mismatch between the survey question and the answer it produces. For example, the ques- tion “Which candidate is better qualified?” may not predict votes, because not everyone votes for the candidate they believe to be best qual- ified. Frame error describes the discrepancy between the population sampled and the larger population. For example, respondents who were attending a Clinton rally, even if sampled randomly, would poorly predict the opinions of the broader electorate. Nonresponse error a publication of the behavioral science & policy association 3 arises from nonrandom nonresponse. For example, Trump supporters’ suspicion of the mainstream media might have made many of them reluctant to participate in polls and thus could have led pollsters to undercount the likely Trump vote. Measurement error describes bias introduced by the method of measurement. For example, the race of an interviewer might affect respondents’ reported attitudes toward candidates of that race. Finally, data-processing errors can occur in the cleaning or analysis of poll results. Even pollsters who acknowledge the exis- tence of these error sources may have difficulty using that knowledge to provide more realistic confidence intervals.13 Data-processing errors illustrate the dilemma. Every researcher knows data-processing errors occur and tries to mini- mize them. To determine the full extent to which data-processing errors have influenced the results of a particular poll, pollsters would have to know which errors they have made; however, if they knew what mistakes they had made, they would have corrected them. The usual response to the difficulty of identi- fying and quantifying errors is to ignore them. That is, pollsters do their best to minimize these different sources of error and then pretend they have succeeded. The consequence is that reported confidence intervals are likely to be too small: 14 The stated interval will give the impres- sion that the election outcome will be closer to the poll’s result than is actually likely. From a psychological perspective, it is not entirely surprising that pollsters tend to be overly confident that their results will predict the actual election outcome. Overprecision, or being overly certain that one’s judgments are correct, is one of the most pervasive biases; it affects most human judgments, including forecasts. Poll results are not technically forecasts because they capture attitudes at a moment in time, but they are typi- cally interpreted as being forecasts of election outcomes. Research suggests that forecasters routinely act too sure that they know what will happen.15,16 They confidently underestimate their vulnerability to error and fail to consider all the ways they could be wrong.17 Philip E. Tetlock conceptualizes approaches to forecasting with the ancient Greek aphorism “The fox knows many things, but the hedgehog knows one big thing.”18,19 Experts who bring a strong ideological orientation to their work and neatly fit the messy details of reality’s complexity into their organizing narratives are hedgehogs. Foxes, by contrast, are generalists who are less likely to see universal laws and coherent ideol- ogies; they are open to revising their views and accept the possibility that they might be wrong. Foxes consistently make more accurate fore- casts than do hedgehogs, suggesting that one useful strategy people can apply to counteract their overconfidence is to consider the possi- bility that they could be mistaken.20 Research examining poll accuracy has primarily assessed how closely poll results correspond to the shares of votes that candidates receive in their elections.8,21 In the first of two studies described below, we sought to add to prior research by comparing poll accuracy against the degree of confidence claimed, thereby more finely assessing how well poll designs are calibrated to reflect reality. In the second study, we examined how much faith the public has in the accuracy of polls and tested whether informing the public of historical inaccuracies alters that faith. Study 1: Polls & Election Forecasts Method We preregistered a plan to investigate whether the confidence intervals stated for election polls reflected the polls’ accuracy in predicting outcomes. By preregistering, we sought to assure readers that we would not selectively report analyses and results. We were able to obtain data from RealClearPoli- tics (https://www.realclearpolitics.com/) on four election cycles: 2008, 2012, 2016, and 2020. We used data for Democratic presidential primaries in Iowa and New Hampshire during 2008, 2016, and 2020 and data for Republican presidential primaries in the same states during 2012 and 2016. We also used data from polls conducted ahead of the general presidential elections of 2008, 2012, 2016, and 2020. Prior to 2008, the 4 behavioral science & policy | volume 8 issue 2 2022 data did not consistently include sample sizes. Our preregistration, data, and code are available at https://osf.io/65za7/. We analyzed primary data only for races in Iowa and New Hampshire, which are the earliest in the election cycle, because primaries get more complicated after the ones held in those states. For instance, candidates may drop out of the race between the poll and the primary, making it difficult to assess the accuracy of forecasted vote shares for the absent candidates and changing the competitive landscape for the remaining candidates. The polling data we used can be accessed at the links below. (Print readers, find the links in the online text of this article.) • 2020 Iowa Democratic Presidential Caucus • 2016 Iowa Republican Presidential Caucus • 2016 Iowa Democratic Presidential Nomination • 2020 New Hampshire Democratic Presiden- tial Primary • 2016 New Hampshire Republican Presidential Primary • 2016 New Hampshire Democratic Primary • 2012 Iowa Republican Presidential Caucus • 2012 New Hampshire Republican Presidential Primary • 2008 Iowa Democratic Caucus • 2008 New Hampshire Democratic Primary • 2020 General Election: Trump vs. Biden • 2016 General Election: Trump vs. Clinton • 2012 General Election: Romney vs. Obama • 2008 General Election: McCain vs. Obama In total, we analyzed data for 14 sets of polls— 1,931 polls for election cycles from 2008 to 2020. Because some polls asked about several candidates, the 1,931 polls produced 6,654 vote-share estimates. We recorded the margins of error from all polls that reported them and calculated the others as described in note A. Results The analyses we present in this article are consistent with those we preregistered but proved more informative than the set we initially put forward. See the Supplemental Material for details of the preregistered analyses and visit https://osf.io/keswd/ for the results file. Our primary analysis examined hits—instances when a poll’s 95% confidence interval included the election’s actual result—as a function of time between the poll and the election. We were particularly interested in accuracy over time because we wanted to see whether polls conducted far in advance of an election were generally less accurate than those conducted closer to the election. We grouped the polls into seven-day intervals and averaged the hit rate in each interval to estimate the accuracy of the polls as a function of time to the election. Figure 1 shows that hit rates averaged around 60% in the week prior to the election. A year prior to the elec- tion, average hit rates were lower: around 40%. Because most polls ask participants to indicate how they would vote “if the election were held today,” it might be no surprise that accuracy decreases as the distance in time between the poll and the election increases. It is worrisome, however, that even just a few days before the election, the 95% confidence intervals captured the actual vote share only 60% of the time. We calculated how much wider the confidence intervals should have been to achieve 95% accu- racy. First, we identified all the misses in a weekly group—the instances in which the true election outcomes fell outside the stated confidence interval. Then, for each week, we calculated how much wider the interval should have been so that 95% of the true election outcomes would have fallen within the confidence intervals. Figure 2 visualizes our findings. A week before the elec- tion, reported margins of error would have to have been 2 times wider, on average, to be 95% accurate. A year before the election, margins of error would have to have expanded by more than a factor of 3 to be 95% accurate. a publication of the behavioral science & policy association 5 Figure 1. Poll accuracy, by weeks before the election Note. Each dot represents the percentage of polls reported during a given week that proved accurate—that is, that the actual election outcome fell within the stated margins of error for a 95% conﬁdence level. The orange line shows the best ﬁt for the data. In general, poll accuracy increased as the elections drew near; however, at best, only about 60% of the polls proved accurate. The conﬁdence interval (CI) shown on the graph refers to our data. Figure 2. Adjustment to conﬁdence intervals for polls to achieve a 95% hit rate Note. For the polls in Study 1 to have hit their targets—that is, to have encompassed the actual election outcomes within their 95% conﬁdence intervals (CIs)—their conﬁdence intervals would have needed to expand by a factor of 3.5 when the polls were conducted 70 weeks ahead of the election and by a factor of 2 in the week before the election. 6 behavioral science & policy | volume 8 issue 2 2022 Finally, we also compared election cycles to look for trends over many years. We conducted the same analysis as shown in Figure 1 but segmented the results by year. As Figure 3 shows, we found little evidence that polls have become less accurate over the years. Study 2: What Do Laypeople Think About Polling Accuracy? Given the low rates of poll accuracy, members of the public should be skeptical when reading reports about political polls. Are they? Do they understand that polls are often poor predictors of election outcomes? We delved into these questions in Study 2 and examined the extent to which the amount of time before the election affected faith in poll results. Find our preregistra- tion plan to assess responses to seven different poll reporting styles at https://osf.io/9qhmf. Method We conducted an online survey using partici- pants recruited from Amazon Mechanical Turk.22 We restricted our sample to residents of the United States, seeking a population that roughly matched the country’s demographics.23 We opened our survey to 230 people and wound up with 217 complete responses. Data, materials, and code are available at https://osf.io/5wmqe/. We randomly assigned participants to groups that read about a poll result that had been obtained one day, three months, or one year before an election. The survey then presented the poll finding to each participant using seven different reporting styles for the same poll result, in the following order (see note C): • Style 1 consisted of a point estimate: “The poll’s results give one of the candidates 49% of the vote.” • Style 2 consisted of a point estimate paired with a margin of error: “The poll’s results give one of the candidates 49% of the vote with a margin of error of ±3 percentage points.” • Style 3 consisted of a point estimate with a margin of error and a 95% confidence interval: “The poll’s results give one of the candidates 49% of the vote with a margin of error of ±3 percentage points for a 95% confidence interval.” Figure 3. Poll accuracy, by election year Note. Analyses of poll accuracy across four election cycles indicate that despite the concerns of some observers, accuracy— as measured by actual election results falling within a poll’s reported margin of error for a 95% conﬁdence level (CI)—has not declined in recent years. The shaded region shows the 95% conﬁdence interval around the 2020 results. See the Supplemental Material for additional data displays relating to the accuracy of elections by year. Number of Weeks Until Election 70% 60% 50% 40% 30% 20%Percentage of Polls within Margin of Error 70 60 50 40 30 120 10 2016 2012 2020 2008 2020 95% CI a publication of the behavioral science & policy association 7 • Style 4 consisted of an interval: “The poll’s results give one of the candidates between 46% and 52% of the vote.” • Style 5 consisted of an interval with a 95% confidence interval: “The poll’s results give one of the candidates between 46% and 52% of the vote with a 95% confidence interval.” • Style 6 consisted of a point estimate with a margin of error and a 95% confidence interval, as in Style 3, but with the addition of information about the historical accuracy of polls conducted in the time frame specific to the survey group. That is, the data for histor- ical accuracy varied according to the survey group’s time frame. For example, the one-year group’s survey said, “Historically, a year before the election, polls capture the true outcome 35% of the time.” For polls three months before and one day before the election, the figures given were 55% and 60%, respectively. • Style 7 consisted of an interval with a 95% confidence interval, as in Style 5, that was paired with information about historical accuracy that varied with the survey group’s time frame, as in Style 6. For each of the seven reporting styles, we asked participants to indicate, on a 0%–100% scale, how sure they were that the election outcome would be consistent with the poll’s forecast. We predicted that participants would have more faith in polls than was justified by the historical accu- racy of polls and, in particular, than was justified by the most commonly used approach of reporting a point estimate with a margin of error. Results As predicted, the average level of faith in the polls’ accuracy (M = 59.9%) exceeded the average reported historical accuracy of polls (M = 49.7%), p < 10−11. Overall, we saw exces- sive faith with all reporting styles. (See note B for a discussion of the statistical terms used in this article.) The time horizon and the reporting style each had an effect on the extent of the belief in the polls’ accuracy (p < 10−13 and p < 10−7, respectively); however, that faith varied by time horizon and reporting style. (See Figure 4 Figure 4. Faith in poll results, by time horizon & reporting style Note. The reporting style refers to how poll results were reported to participants; the x-axis labels describe the distinguishing features of each reporting style, which are deﬁned as follows: Style 1, point estimate, was a poll result reported as a single percentage. Style 2 had the point estimate and a margin of error (MoE) speciﬁed. Style 3 included a point estimate, a margin of error, and a 95% conﬁdence interval (CI). Style 4, interval, had the poll result reported as a range of values. Style 5 was an interval with a 95% conﬁdence interval also stated. Style 6 consisted of the point estimate, a margin of error, and a 95% conﬁdence interval, with historical accuracy—the percentage of polls that were accurate in the past at one day, three months, or one year before the election—speciﬁed. Style 7 consisted of a range of values with a 95% conﬁdence interval and historical accuracy speciﬁed. Conﬁdence rates were the average rating of participants’ faith that the poll will be accurate. Error bars show standard errors for our results. The data indicate that for the most part, participants’ faith in the accuracy of polls conducted one day, three months, or one year before an election exceeded the historical accuracy of polls conducted in the corresponding time frames. Even when participants were told of the historical accuracy of polls, they still overestimated the current poll’s accuracy. Style 70 60 50 40Conﬁdence of Accuracy % Point estimate (PE) One Day Historical Accuracy Three Month Historical Accuracy One Year Historical Accuracy One Day Conﬁdence Rates Three Months Conﬁdence Rates One Year Conﬁdence Rates PE + MoE PE + 95% MoE Interval 95% CI PE + 95% MoE + Accuracy 95% CI + Accuracy 8 behavioral science & policy | volume 8 issue 2 2022 and the Supplemental Material for more details of the data analyses.) With respect to the time horizon, for instance, participants generally placed their greatest faith in the polls conducted a day before the election. With respect to reporting style, consider the results relating to Style 2 and Style 7. Style 2, one of the most common reporting approaches, uses a point estimate along with a margin of error. For this style, faith in the poll’s result exceeded the historical accuracy of polls to a statistically significant extent only when partic- ipants were told that the poll came out a year before the election (p < 10−11) but not when they were told the results came out three months, (p = .29) or one day (p = .02) before the elec- tion. By contrast, Style 7 came with an explicit warning specifying polls’ historical accuracy. Although we had expected this disclosure to reduce participants’ confidence in the reported poll result, it had surprisingly little effect. The results displayed in Figure 4 underscore both people’s excessive faith in polls’ predictive accuracy and the challenge of correcting their misperception. Providing more information about polls’ poor record of accuracy in Styles 6 and 7 failed to bring the participants’ faith in the polls in line with the polls’ historical accuracy. General Discussion Overview Our analyses showed that the 95% confidence intervals reported for the polls we studied included the actual election result substantially less often than 95% of the time. For 95% confi- dence intervals to include the true results—in technical terms, to be “calibrated with their hit rates”—they would have to at least double in size. Moreover, variations in how the findings of polls are reported make little difference to people’s perceptions of their accuracy: Even when informed of the inaccuracy of past polls, participants in Study 2 continued to place exces- sive faith in the current poll’s ability to predict an election outcome—faith that is not justified by the past successes of polls. The only comfort provided by the data—and it is small comfort— is that participants were not totally unaware of the potential flaws in poll predictions. They were not highly confident in the accuracy of the polls; on average, they reported being only 60% confident. One hopes this skepticism might help voters take future poll results with a grain of salt. As we have explained, poll results might deviate from election results for many reasons—such as errors in sampling, specification, frame, nonre- sponse, measurement, and data processing.24 The statistics used to generate confidence intervals are easier to adjust for sampling error than for other sources of error, although all result from systematic but difficult-to-assess differences between the people who participate in the polls and the larger population of voters. Because statistical models have trouble quanti- fying these differences, the confidence intervals they produce are likely to be inaccurate and will contribute to pollsters’ excessive confidence in the accuracy of their polls. Are New Social Trends Affecting Poll Accuracy? FiveThirtyEight was not alone in underesti- mating the turnout for Donald Trump in the 2016 race against Hillary Clinton. His win prompted speculation that the accuracy of polling was declining.25,26 It is possible that the 2016 presidential election was affected by a new phenomenon: right-wing voters being unusu- ally reluctant to respond to polls because of a general suspicion of media organizations.27 If that speculation were accurate, polling errors should be increasing. However, as Figure 3 shows, we did not find that hit rates of the polls conducted in the 2020 election cycle were “Even when informed of the inaccuracy of past polls, participants continued to place excessive faith in the current poll’s ability to predict an election outcome” a publication of the behavioral science & policy association 9 lower than in previous years. In a 2018 report, Will Jennings and Christopher Wlezien also found no evidence of a decline in poll accuracy over time.10 A simpler explanation for the failure of polls to predict Donald Trump’s win is that their unusually poor showing represented an aberration; polls and elections will always include noise. Although many observers criticized Nate Silver and his FiveThirtyEight election forecasting website for giving Hillary Clinton a 71% chance of prevailing on the eve of the 2016 election, Silver sensibly defended himself by noting that events with a 29% probability of occurring do happen.28 Nevertheless, Trump’s surprisingly strong showing four years later in the 2020 election has underscored concerns that recent polling misses may stem in part from some broad social trend that should be taken into account when polls are designed and interpreted in the future. This concern has been heightened by the flaws of the presidential polls in 2020. Going into the 2020 presidential election, national polls favored Joe Biden by 8 percentage points.29 In fact, he won by only 4.5%.27 Poll watchers are now wondering whether the failure to predict Trump’s strong performances in 2016 and 2020 were due to chance or whether they might they be attributable to a “Trump effect,” some myste- rious factor related to Trump that systematically disrupts the accuracy of polls that involve him.30 Also unclear is the answer to the important question of whether the failures reflect long- term trends with implications for future polls. Ongoing Challenges for Pollsters Such questions highlight the challenge of adjusting poll methodologies to account for their shortcomings. In principle, it ought to be possible to adjust polls’ confidence inter- vals on the basis of their known limitations so that they become more accurate. And, in fact, sophisticated pollsters and poll aggregators have long attempted precisely this.31 However, these adjustments can take into account only the known discrepancies between the popula- tions sampled by polls and the larger population of people who vote. The list of discrepancies is long and changeable and omits unknown discrepancies, making it incomplete. How incomplete? Estimating that incompleteness requires specifying unknown unknowns, an epistemological impossibility. For instance, it is impossible to know what news stories might emerge between a poll and Election Day and how they will affect different voters. The approach we used in the studies reported in this article looks backward, measuring the historical discrepancies between poll results and election outcomes. This approach can provide at least some guidance for estimating the size of such discrepancies. However, unless the past is perfectly representative of the future, it will be an imperfect guide. And, of course, the world is ever changing, with a future course that is never perfectly predictable. The reasons that poll results may differ from election outcomes in the future are not limited to the reasons that explained differences in the past. Action Implications We see several action implications of our find- ings. First, pollsters should either lower the level of confidence they claim (to a percentage lower than 95%) or expand the size of the confidence intervals they report. If they wish to report confidence intervals that are well calibrated with reality, they ought to widen their confidence intervals substantially. Relatedly, journalists who report on poll data should point out that the true proportion of voters selecting a given candidate is likely to be smaller or larger than the polls predict. In addition, consumers of polling data ought to put less faith in the accuracy of poll results. Voters are making a mistake when they decide not to vote because they believe polling tells “Voters are making a mistake when they decide not to vote because they believe polling tells them how the election is going to turn out.” 10 behavioral science & policy | volume 8 issue 2 2022 them how the election is going to turn out. Unfortunately, data from our second study do not provide much reason for optimism with regard to voters’ corrigibility. Even when we provided information about the poor historical performance of polls, participants’ confidence in their accuracy was not dampened sufficiently. Finally, we are skeptical of rules, such as those governing the inclusion of candidates in polit- ical debates, that rely exclusively on polling data. Reliance on polls to decide which candi- dates deserve attention must be tempered with knowledge of the imperfections in polls’ predictive accuracy. We believe, therefore, that decisions relating to debate participation ought to err on the side of including more candidates. Without being able to anticipate why poll results will differ from the actual vote, pollsters will continue to overestimate their accuracy and underestimate their propensity for error. Even pollsters who are willing to admit, in the abstract, that polls are vulnerable to various sources of inaccuracy beyond simple sampling error may be reluctant to discount the accuracy of their own polls. After all, they have done all they can to minimize error and correct for known biases. Voters’ trust that pollsters are doing all they can to correct for sources of error, along with voters’ failure to understand that pollsters do not know all the sources of error, may explain why the participants in our second study were not sufficiently influenced by past inaccuracies in poll results and expressed optimism that the poll results reported to them in the study would be more accurate than polls have proven to be in the past. Beyond Polls An inability to compensate well for uncer- tainty is, of course, a problem that affects many statistical tools. Economists and sociologists have long been concerned about the issue.32,33 Sometimes a scientific theory to explain some phenomenon is wrong and applies the wrong statistical model to evaluate data related to the phenomenon. But because scientists cannot test all possible models, they cannot know whether their model is the best one and will come away overconfident in the model’s value. The overconfidence we have identified in poll- sters is not a problem unique to them or even to people who test scientific theories. As we explained earlier, overprecision—a form of overconfidence in which people are overly sure that they know the truth—is one of the biases in judgment most resistant to eradication.16 People can be overconfident in their correct- ness for many reasons. For instance, they may base beliefs on information that is biased or otherwise imperfect in ways they did not antic- ipate. And because it is difficult for us humans to consider information we do not know, it is easy for us to overestimate the accuracy of our beliefs. The results we report in this article suggest that overconfidence in one’s judgments routinely afflicts not only pollsters but also the people who interpret those reports. We fear that the public and candidates will continue to be ill served if polls continue to be conducted and reported in the same old ways. endnotes A. The size of a reported margin of error in a poll or study is determined by the confidence level, the sample variance (that is, the spread of responses from participants), and the sample size. We found that the most common margin of error reported was 1.96 standard deviations around the poll result, assuming a normal distribution in sampling error. (A normal distribution is symmetric about the mean.) That is, the most common 95% confi- dence interval reported was the range going from 1.96 standard deviations below the poll result to 1.96 standard deviations above it, which trans- lates to 3.5 percentage points above and below the poll result for a poll with 800 respondents. For polls that did not report a margin of error for a 95% confidence level, we used the poll’s result and sample size to compute a confidence interval of ±1.96 standard deviations around the result. (See note B for further discussion of the statistical terms used in this article.) B. Editors’ note to nonscientists: For any given data set, the statistical test used—such as the chi-square (χ 2) test, the t test, or the F test—depends on the number of data points and the kinds of variables being considered, such as proportions or means. F tests and t tests are parametric: they make some a publication of the behavioral science & policy association 11 assumptions about the characteristics of a popu- lation, such as that the compared groups have an equal variance on a compared factor. In cases violating these assumptions, researchers make some adjustments in their calculations to take into account dissimilar variances across groups. The p value of a statistical test is the probability of obtaining a result equal to or more extreme than would be observed merely by chance, assuming there are no true differences between the groups under study (this assumption is referred to as the null hypothesis). We preregistered p < .005 as the threshold of statistical significance, with lower values indicating a stronger basis for rejecting the null hypothesis. Standard deviation is a measure of the amount of variation in a set of values. Approximately two-thirds of the observations fall between one standard deviation below the mean and one standard deviation above the mean. Stan- dard error uses standard deviation to determine how precisely one has estimated a true popula- tion value from a sample. For instance, if one took enough samples from a population, the sample mean ±1 standard error would contain the true population mean around two-thirds of the time. C. The fixed order in which the survey presented the seven styles represents a deviation from our preregistered plan, which specified that every participant would see all of the styles “in a different randomly determined order.” After completing the preregistration but before launching the study, we decided that a more conservative test of our hypothesis would be to present the different reporting styles in order of increasing caveats. author affiliation Kotak and Moore: University of California, Berkeley. Corresponding author’s e-mail: dm@ berkeley.edu. author note Thanks to Amelia Dev for her support for this project and to Gabriel Lenz and David Broockman for valuable feedback on a draft of this article. supplemental material • http://behavioralpolicy.org/journal • Method & Analysis In Brief: Key Action Implications for Policymakers • Organizations that conduct polls should report larger margins of error for 95% confidence levels or admit that their margins of error have a lower likelihood of encompassing true election outcomes. The margins of error tend to be too narrow, particularly when polls are conducted long before an election. • News media should require reporters to specify margins of error and to indicate that election outcomes often fall outside the reported margins—so that voters can take that information into account in deciding whether to vote and for whom. references 12 behavioral science & policy | volume 8 issue 2 2022 1. YouGov. (2016, June 23). YouGov on the day poll: Remain 52%, leave 48%. https://yougov.co.uk/topics/ politics/articles-reports/2016/06/23/ yougov-day-poll 2. EU referendum results. (2016). BBC News. https://www.bbc.com/news/ politics/eu_referendum/results 3. Silver, N. (2016, November 8). 2016 election forecast. FiveThirtyEight. https://projects.fivethirtyeight. com/2016-election-forecast/ 4. Harvey, H. M. (2016, December 19). Skeptical 70,000 black voters abstained from presidential vote. The Hill. https:// thehill.com/blogs/pundits-blog/ national-party-news/311099-skeptical- 70000-black-voters-abstained-from 5. The Commission on Presidential Debates. (2020). The Commission on Presidential Debates: An overview. https://www.debates.org/about-cpd/ overview/ 6. McMinn, S., & Hurt, A. (2020, April 21). Tracking the money race behind the presidential primary campaign. NPR. https://www. npr.org/2019/04/16/711812314/ tracking-the-money-race-behind-the- presidential-campaign 7. Westwood, S. J., Messing, S., & Lelkes, Y. (2020). Projecting confidence: How the probabilistic horse race confuses and demobilizes the public. The Journal of Politics, 82(4), 1530–1544. https://doi. org/10.1086/708682 8. Ford, R., Wlezien, C., Pickup, M., & Jennings, W. (2017). Polls and votes. In K. Arzheimer, J. Evans, & M. S. Lewis- Beck (Eds.), The SAGE handbook of electoral behaviour (pp. 787–812). SAGE Publications. https://doi. org/10.4135/9781473957978.n34 9. Hillygus, D. S. (2011). The evolution of election polling in the United States. Public Opinion Quarterly, 75(5), 962–981. https://doi.org/10.1093/poq/ nfr054 10. Jennings, W., & Wlezien, C. (2018). Election polling errors across time and space. Nature Human Behaviour, 2(4), 276–283. https://doi.org/10.1038/ s41562-018-0315-6 11. Biemer, P. P. (2010). Total survey error: Design, implementation, and evaluation. Public Opinion Quarterly, 74(5), 817–848. https://doi.org/10.1093/poq/ nfq058 12. Groves, R. M., & Lyberg, L. (2010). Total survey error: Past, present, and future. Public Opinion Quarterly, 74(5), 849–879. https://doi.org/10.1093/poq/ nfq065 13. Voss, S., Gelman, A., & King, G. (1995). The polls—A review: Preelection survey methodology: Details from eight polling organizations, 1988 and 1992. Public Opinion Quarterly, 59(1), 98–132. https://doi.org/10.1086/269461 14. Goot, M. (2021). How good are the polls? Australian election predictions, 1993–2019. Australian Journal of Political Science, 56(1), 35–55. https:// doi.org/10.1080/10361146.2020.1825 616 15. Moore, D. A., Swift, S. A., Minster, A., Mellers, B., Ungar, L., Tetlock, P., Yang, H. H. J., & Tenney, E. R. (2017). Confidence calibration in a multiyear geopolitical forecasting competition. Management Science, 63(11), 3552–3565. https://doi.org/10.1287/ mnsc.2016.2525 16. Moore, D. A., Tenney, E. R., & Haran, U. (2015). Overprecision in judgment. In G. Wu & G. Keren (Eds.), Handbook of judgment and decision making (pp. 182–212). Wiley. https://doi. org/10.1002/9781118468333.ch6 17. Fischhoff, B., Slovic, P., & Lichtenstein, S. (1977). Knowing with certainty: The appropriateness of extreme confidence. Journal of Experimental Psychology: Human Perception and Performance, 3(4), 552–564. https://doi. org/10.1037/0096-1523.3.4.552 18. Tetlock, P. E. (2005). Expert political judgment: How good is it? How can we know? Princeton University Press. 19. Berlin, I. (1953). The hedgehog and the fox: An essay on Tolstoy’s view of history. Simon & Schuster. 20. Tetlock, P. E., & Gardner, D. (2015). Superforecasting: The art and science of prediction. Crown Publishers. 21. Jones, R. J., Jr. (2008). The state of presidential election forecasting: The 2004 experience. International Journal of Forecasting, 24(2), 310–321. https:// doi.org/10.1016/j.ijforecast.2008.03.002 22. Litman, L., Robinson, J., & Abberbock, T. (2017). TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences. Behavior Research Methods, 49(2), 433–442. https://doi.org/10.3758/ s13428-016-0727-z 23. Moss, A., & Litman, L. (2020, June 12). Demographics of people on Amazon Mechanical Turk. CloudResearch. https://www.cloudresearch.com/ resources/blog/who-uses-amazon- mturk-2020-demographics/ 24. Shirani-Mehr, H., Rothschild, D., Goel, S., & Gelman, A. (2018). Disentangling bias and variance in election polls. Journal of the American Statistical Association, 113(522), 607–614. https:// doi.org/10.1080/01621459.2018.1448 823 25. Cohn, N. (2017, May 31). A 2016 review: Why key state polls were wrong about Trump. The New York Times. https:// www.nytimes.com/2017/05/31/ upshot/a-2016-review-why-key-state- polls-were-wrong-about-trump.html 26. Mercer, A., Deane, C., & McGeeney, K. (2016). Why 2016 election polls missed their mark. Pew Research Center. https://www.pewresearch. org/fact-tank/2016/11/09/why-2016- election-polls-missed-their-mark/ 27. Russonello, G. (2020, November 5). The polls underestimated Trump— again. Nobody agrees on why. The New York Times. https://www.nytimes. com/2020/11/04/us/politics/poll- results.html 28. Silver, N. (2016, November 11). Why FiveThirtyEight gave Trump a better chance than almost anyone else. FiveThirtyEight. https://fivethirtyeight. com/features/why-fivethirtyeight-gave- trump-a-better-chance-than-almost- anyone-else/ 29. Silver, N. (2020, November 3). Biden’s favored in our final presidential forecast, but it’s a fine line between a landslide and a nail-biter. FiveThirtyEight. https:// fivethirtyeight.com/features/final-2020- presidential-election-forecast/ 30. Isakov, M., & Kuriwaki, S. (2020). Towards principled unskewing: Viewing 2020 election polls through a corrective lens from 2016. Harvard Data Science Review, 2(4). https://doi. org/10.1162/99608f92.86a46f38 31. Silver, N. (2012). The signal and the noise: Why so many predictions fail— But some don’t. Penguin Press. 32. Hansen, L., & Sargent, T. J. (2001). Robust control and model uncertainty. American Economic Review, 91(2), 60–66. https://doi.org/10.1257/ aer.91.2.60 33. Young, C., & Holsteen, K. (2017). Model uncertainty and robustness: A computational framework for multimodel analysis. Sociological Methods & Research, 46(1), 3–40. https://doi. org/10.1177/0049124115610347","libVersion":"0.3.2","langs":""}
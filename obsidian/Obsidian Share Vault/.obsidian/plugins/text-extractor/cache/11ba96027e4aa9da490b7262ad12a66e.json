{"path":"lit/sources/papers_added.SAVE/papers/Salinas19hiDimFrcstRNNcpla.pdf","text":"High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes David Salinas NAVER LABS Europe ∗ david.salinas@naverlabs.com Michael Bohlke-Schneider Amazon Research bohlkem@amazon.com Laurent Callot Amazon Research lcallot@amazon.com Roberto Medico Ghent University ∗ roberto.medico91@gmail.com Jan Gasthaus Amazon Research gasthaus@amazon.com Abstract Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, ﬁnancial risk management, causal analysis, or demand forecasting. However, the computational and numeri- cal difﬁculties of estimating time-varying and high-dimensional covariance matri- ces often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We pro- pose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to dras- tically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several real- world datasets that our method provides signiﬁcant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contribu- tions of the different components of our model. 1 Introduction The goal of forecasting is to predict the distribution of future time series values. Forecasting tasks frequently require predicting several related time series, such as multiple metrics for a compute ﬂeet or multiple products of the same category in demand forecasting. While these time series are often dependent, they are commonly assumed to be (conditionally) independent in high-dimensional settings because of the hurdle of estimating large covariance matrices. Assuming independence, however, makes such methods unsuited for applications in which the corre- lations between time series play an important role. This is the case in ﬁnance, where risk minimizing portfolios cannot be constructed without a forecast of the covariance of assets. In retail, a method providing a probabilistic forecast for different sellers should take competition relationships and can- nibalization effects into account. In anomaly detection, observing several nodes deviating from their expected behavior can be cause for alarm even if no single node exhibits clear signs of anomalous behavior. Multivariate forecasting has been an important topic in the statistics and econometrics literature. Several multivariate extensions of classical univariate methods are widely used, such as vector au- toregressions (VAR) extending autoregressive models [19], multivariate state-space models [7], or ∗Work done while being at Amazon Research 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1910.03002v2 [cs.LG] 24 Oct 2019 Figure 1: Top: covariance matrix predicted by our model for taxi trafﬁc time series for 1214 locations in New-York at 4 different hours of a Sunday (only a neighborhood of 103 series is shown here, for clearer visualization). Bottom: Correlation graph obtained by keeping only pairs with covariance above a ﬁxed threshold at the same hours. Both spatial and temporal relations are learned from the data as the covariance evolves over time and edges connect locations that are close to each other. multivariate generalized autoregressive conditional heteroskedasticity (MGARCH) models [2]. The rapid increase in the difﬁculty of estimating these models due to the growth in number of parameters with the dimension of the problem have been binding constraints to move beyond low-dimensional cases. To alleviate these limitations, researchers have used dimensionality reduction methods and regularization, see for instance [3, 5] for VAR models and [34, 9] for MGARCH models, but these models remain unsuited for applications with more than a few hundreds dimensions [23]. Forecasting can be seen as an instance of sequence modeling, a topic which has been intensively studied by the machine learning community. Deep learning-based sequence models have been suc- cessfully applied to audio signals [33], language modeling [13, 30], and general density estimation of univariate sequences [22, 21]. Similar sequence modeling techniques have also been used in the context of forecasting to make probabilistic predictions for collections of real or integer-valued time series [26, 36, 16]. These approaches ﬁt a global (i.e. shared) sequence-to-sequence model to a collection of time series, but generate statistically independent predictions. Outside the forecasting domain, similar methods have also been applied to (low-dimensional) multivariate dependent time series, e.g. two-dimensional time series of drawing trajectories [13, 14]. Two main issues prevent the estimation of high-dimensional multivariate time series models. The ﬁrst one is the O(N 2) scaling of the number of parameters required to express the covariance matrix where N denotes the dimension. Using dimensionality reduction techniques like PCA as a pre- processing step is a common approach to alleviate this problem, but it separates the estimation of the model from the preprocessing step, leading to decreased performance. This motivated [27] to perform such a factorization jointly with the model estimation. In this paper we show how the low rank plus diagonal covariance structure of the factor analysis model [29, 25, 10, 24] can be used in combination with Gaussian copula processes [37] to an LSTM-RNN [15] to jointly learn the tempo- ral dynamics and the (time-varying) covariance structure, while signiﬁcantly reducing the number of parameters that need to be estimated. The second issue affects not only multivariate models, but all global time series models, i.e. models that estimate a single model for a collection of time series: In real-world data, the magnitudes of the time series can vary drastically between different series of the same data set, often spanning several orders of magnitude. In online retail demand forecasting, for example, item sales follow a power- law distribution, with a large number of items selling only a few units throughout the year, and a few popular items selling thousands of units per day [26]. The challenge posed by this for estimating global models across time series has been noted in previous work [26, 37, 27]. Several approaches have been proposed to alleviate this problem, including simple, ﬁxed invertible transformations such as the square-root or logarithmic transformations, and the data-adaptive Box-Cox transform [4], 2 that aims to map a potentially heavy-tailed distribution to a Normal distribution. Other approaches includes removing scale with mean-scaling [26], or with a separate network [27]. Here, we propose to address this problem by modeling each time series’ marginal distribution sepa- rately using a non-parametric estimate of its cumulative distribution function (CDF). Using this CDF estimate as the marginal transformation in a Gaussian copula (following [17, 18, 1]) effectively ad- dresses the challenges posed by scaling, as it decouples the estimation of marginal distributions from the temporal dynamics and the dependency structure. The work most closely related to ours is the recent work [32], which also proposes to combine deep autoregressive models with copula to model correlated time series. Their approach uses a nonpara- metric estimate of the copula, whereas we employ a Gaussian copula with low-rank structure that is learned jointly with the rest of the model. The nonparametric copula estimate requires splitting a N -dimensional cube into ε−N many pieces (where N is the time series dimension and ε is a desired precision), making it difﬁcult to scale that approach to large dimensions. The method also requires the marginal distributions and the dependency structure to be time-invariant, an assumption which is often violated is practice as shown in Fig. 1. A concurrent approach was proposed in [35] which also uses Copula and estimates marginal quantile functions with the approach proposed in [11] and models the Cholesky factor as the output of a neural network. Two important differences are that this approach requires to estimate O(N 2) parameters to model the covariance matrix instead of O(N ) with the low-rank approach that we propose, another difference is the use of a non-parametric esti- mator for the marginal quantile functions. The main contributions of this paper are: • a method for probabilistic high-dimensional multivariate forecasting (scaling to dimensions up to an order of magnitude larger than previously reported in [23]), • a parametrization of the output distribution based on a low-rank-plus-diagonal covariance matrix enabling this scaling by signiﬁcantly reducing the number of parameters, • a copula-based approach for handling different scales and modeling non-Gaussian data, • an empirical study on artiﬁcial and six real-world datasets showing how this method im- proves accuracy over the state of the art while scaling to large dimensions. The rest of the paper is structured as follows: In Section 2 we introduce the probabilistic forecast- ing problem and describe the overall structure of our model. We then describe how we can use the empirical marginal distributions in a Gaussian copula to address the scaling problem and handle non-Gaussian distributions in Section 3. In Section 4 we describe the parametrization of the covari- ance matrix with low-rank-plus-diagonal structure, and how the resulting model can be viewed as a low-rank Gaussian copula process. Finally, we report experiments with real-world datasets that demonstrate how these contributions combine to allow our model to generate correlated predictions that outperform state-of-the-art methods in terms of accuracy. 2 Autoregressive RNN Model for Probabilistic Multivariate Forecasting Let us denote the values of a multivariate time series by zi,t ∈ D, where i ∈ {1, 2, . . . , N } indexes the individual univariate component time series, and t indexes time. The domain D is assumed to be either R or N. We will denote the multivariate observation vector at time t by zt ∈ DN . Given T observations z1, . . . , zT , we are interested in forecasting the future values for τ time units, i.e. we want to estimate the joint conditional distribution P (zT +1, ..., zT +τ |z1, . . . , zT ). In a nutshell, our model takes the form of a non-linear, deterministic state space model whose state hi,t ∈ Rk evolves independently for each time series i according to transition dynamics ϕ, hi,t = ϕθh (hi,t−1, zi,t−1) i = 1, . . . , N, (1) where the transition dynamics ϕ are parametrized using a LSTM-RNN [15]. Note that the LSTM is unrolled for each time series separately, but parameters are tied across time series. Given the state values hi,t for all time series i = 1, 2, . . . , N and denoting by ht the collection of state values for all series at time t, we parametrize the joint emission distribution using a Gaussian copula, p(zt|ht) = N ([f1(z1,t), f2(z2,t), . . . , fN (zN,t)] T | µ(ht), Σ(ht)). (2) 3 N     µ 1 t µ 2 t µ 4 t   ,   d 1 t 0 0 0 d 2 t 0 0 0 d 4 t   +   v 1 t v 2 t v 4 t     v 1 t v 2 t v 4 t   T   z 1 t h 1 t µ 1 t , d 1 t , v 1 t z 2 t h 2 t µ 2 t , d 2 t , v 2 t z 4 t h 4 t µ 4 t , d 4 t , v 4 t N     µ 1 t µ 3 t µ 4 t   ,   d 1 t 0 0 0 d 3 t 0 0 0 d 4 t   +   v 1 t v 3 t v 4 t     v 1 t v 3 t v 4 t   T   z 1 t h 1 t µ 1 t , d 1 t , v 1 t z 3 t h 3 t µ 3 t , d 3 t , v 3 t z 4 t h 4 t µ 4 t , d 4 t , v 4 t Figure 2: Illustration of our model parametrization. During training, dimensions are sampled at random and a local LSTM is unrolled on each of them individually (1, 2, 4, then 1, 3, 4 in the example). The parameters governing the state updates and parameter projections are shared for all time series. This parametrization can express the Low-rank Gaussian distribution on sets of series that varies during training or prediction. The transformations fi : D → R here are invertible mappings of the form Φ−1 ◦ ˆFi, where Φ denotes the cumulative distribution function of the standard normal distribution, and ˆFi denotes an estimate of the marginal distribution of the i-th time series zi,1, . . . , zi,T . The role of these functions fi is to transform the data for the i-th time series such that marginally it follows a standard normal distribution. The functions µ(·) and Σ(·) map the state ht to the mean and covariance of a Gaussian distribution over the transformed observations (described in more detail in Section 4). Under this model, we can factorize the joint distribution of the observations as p(z1, . . . zT +τ ) = T +τ∏ t=1 p(zt|z1, . . . , zt−1) = T +τ∏ t=1 p(zt|ht). (3) Both the state update function ϕ and the mappings µ(·) and Σ(·) have free parameters that are learned from the data. We denote θ the vector of all free parameters, consisting of the parameters of the state update θh as well as θµ and θΣ which denote the free parameters in µ(·) and Σ(·). Given θ and hT +1, we can produce Monte Carlo samples from the joint predictive distribution p(zT +1, . . . zT +τ |z1, . . . , zT ) = p(zT +1, . . . zT +τ |hT +1) = T +τ∏ t=T +1 p(zt|ht) (4) by sequentially sampling from P (zt|ht) and updating ht using ϕ 2. We learn the parameters θ from the observed data z1, . . . , zT using maximum likelihood estimation by i.e. by minimizing the loss function − log p(z1, z2, . . . , zT ) = − T∑ t=1 log p(zt|ht), (5) using stochastic gradient descent-based optimization. To handle long time series, we employ a data augmentation strategy which randomly samples ﬁxed-size slices of length T ′ + τ from the time series during training, where we ﬁx the context length hyperparameter T ′ to τ . During prediction, only the last T ′ time steps are used in computing the initial state for prediction. 3 Gaussian Copula A copula function C : [0, 1] N → [0, 1] is the CDF of a joint distribution of a collection of real random variables U1, . . . , UN with uniform marginal distribution [8], i.e. C(u1, . . . , uN ) = P (U1 ≤ u1, . . . , UN ≤ uN ). Sklar’s theorem [28] states that any joint cumulative distribution F admits a representation in terms of its univariate marginals Fi and a copula function C, F (z1, . . . , zN ) = C(F1(z1), . . . , FN (zN )). 2Note that the model complexity scales linearly with the number of Monte Carlo samples. 4 When the marginals are continuous the copula C is unique and is given by the joint distribution of the probability integral transforms of the original variables, i.e. u ∼ C where ui = Fi(zi). Furthermore, if zi is continuous then ui ∼ U(0, 1). A common modeling choice for C is to use the Gaussian copula, deﬁned by: C(F1(z1), . . . , Fd(zN )) = ϕµ,Σ(Φ−1(F1(z1)), . . . , Φ−1(FN (zN ))), where Φ : R → [0, 1] is the CDF of the standard normal and ϕµ,Σ is a multivariate normal distribu- tion parametrized with µ ∈ RN and Σ ∈ RN ×N . In this model, the observations z, the marginally uniform random variables u and the Gaussian random variables x are related as follows: x Φ −→ u F −1 −−−→ z z F −→ u Φ −1 −−−→ x. Setting fi = Φ −1 ◦ ˆFi results in the model in Eq. (2). The marginal distributions Fi are not given a priori and need to be estimated from data. We use the non-parametric approach of [17] proposed in the context of estimating high-dimensional distri- butions with sparse covariance structure. In particular, they use the empirical CDF of the marginal distributions, ˆFi(v) = 1 m m∑ t=1 1 zit≤v, where m observations are considered. As we require the transformations fi to be differentiable, we use a linearly-interpolated version of the empirical CDF resulting in a piecewise-constant derivative ˆF ′(u). This allow us to write the log-density of the original observations under our model as log p(z; µ, Σ) = log ϕµ,Σ(Φ−1( ˆF (z))) + log d dz Φ−1( ˆF (z)) = log ϕµ,Σ(Φ−1( ˆF (z))) + log d du Φ−1(u) + log d dz ˆF (z) = log ϕµ,Σ(Φ−1( ˆF (z))) − log ϕ(Φ −1( ˆF (z)) + log ˆF ′(z) which are the individual terms in the total loss (5) where ϕ is the probability density function of the standard normal. The number of past observations m used to estimate the empirical CDFs is an hyperparameter and left constant in our experiments with m = 100 3. 4 Low-rank Gaussian Process Parametrization After applying the marginal transformations fi(·) our model assumes a joint multivariate Gaussian distribution over the transformed data. In this section we describe how the parameters µ(ht) and Σ(ht) of this emission distribution are obtained from the LSTM state ht. We begin by describing how a low-rank-plus-diagonal parametrization of the covariance matrix can be used to keep the computational complexity and the number of parameters manageable as the number of time series N grows. We then show how, by viewing the emission distribution as a time-varying low-rank Gaussian Process gt ∼ GP(˜µt(·), kt(·, ·)), we can train the model by only considering a subset of time series in each mini-batch further alleviating memory constraints and allowing the model to be applied to very high-dimensional sets of time series. Let us denote the vector of transformed observations by xt = f (zt) = [f1(z1,t), f2(z2,t), . . . , fN (zN,t)] T , so that p(xt|ht) = N (xt|µ(ht), Σ(ht)). The covariance matrix Σ(ht) is a N × N symmetric positive deﬁnite matrix with O(N 2) free parameters. Evaluating the Gaussian likelihood naïvely 3This makes the underlying assumption that the marginal distributions are stationary, which is violated e.g. in case of a linear trend. Standard time series techniques such as de-trending or differencing can be used to pre-process the data such that this assumption is satisﬁed. 5 requires O(N 3) operations. Using a structured parametrization of the covariance matrix as the sum of a diagonal matrix and a low rank matrix, Σ = D + V V T where D ∈ RN ×N is diagonal and V ∈ RN ×r, results in a compact representation with O(N × r) parameters. This allows the likelihood to be evaluated using O(N r2+r3) operations. As the rank hyperparameter r can typically be chosen to be much smaller than N , this leads to a signiﬁcant speedup. In all our low-rank experiments we use r = 10. We investigate the sensitivity to this parameter of accuracy and speed in the Appendix. Recall from Eq. 1 that hi,t represents the state of an LSTM unrolled with values preceding zi,t. In order to deﬁne the mapping Σ(ht), we deﬁne mappings for its components Σ(ht) =    d1(h1,t) 0 . . . 0 dN (hN,t)    + [ v1(h1,t) . . . vN (hN,t) ] [ v1(h1,t) . . . vN (hN,t) ]T = Dt + VtV T t . Note that the component mappings di and vi depend only on the state hi,t for the i-th component time series, but not on the states of the other time series. Instead of learning separate mappings di, vi, and µi for each time series, we parametrize them in terms of the shared functions ˜d, ˜v, and ˜µ, respectively. These functions depend on an E-dimensional feature vector ei ∈ RE for each individual time series. The vectors ei can either be features of the time series that are known a priori, or can be embeddings that are learned with the rest of the model (or a combination of both). Deﬁne the vector yi,t = [hi,t; ei]T ∈ Rp×1, which concatenates the state for time series i at time t with the features ei of the i-th time series and use the following parametrization: µi(hi,t) = ˜µ(yi,t) = wT µ yi,t di(hi,t) = ˜d(yi,t) = s(wT d yi,t) vi(hi,t) = ˜v(yi,t) = Wvyi,t, where s(x) = log(1 + e x) maps to positive values, wµ ∈ Rp×1, wd ∈ Rp×1, Wv ∈ Rr×p are parameters. All parameters θµ = {wµ, w˜µ}, θΣ = {wd, Wv, w ˜d} as well as the LSTM update parameters θh are learned by optimizing Eq. 5. These parameters are shared for all time series and can therefore be used to parametrize a GP. We can view the distribution of xt as a Gaussian process evaluated at points yi,t, i.e. xi,t = gt(yi,t), where gt ∼ GP(˜µ(·), k(·, ·)), with k(y, y′) = 1 y=y′ ˜d(y) + ˜v(y)T ˜v(y′). Using this view it becomes apparent that we can train the model by evaluating the Gaussian terms in the loss only on random subsets of the time series in each iteration, i.e. we can train the model using batches of size B ≪ N as illustrated in Figure 2 (in our experiments we use B = 20). Further, if prior information about the covariance structure is available (e.g. in the case of spatial data the covariance might be directly related to the distance between points), this information can be easily incorporated directly into the kernel, either by exclusively using a pre-speciﬁed kernel or by combining it with the learned, time-varying kernel speciﬁed above. 5 Experiments Synthetic experiment. We ﬁrst perform an experiment on synthetic data demonstrating that our approach can recover complex time-varying low-rank covariance patterns from multi-dimensional observations. An artiﬁcial dataset is generated by drawing T observations from a normal distribution with time-varying mean and covariance matrix, zt ∼ N (ρtu, Σt) where ρt = sin(t), Σt = U StU T and St = [ σ2 1 ρtσ1σ2 ρtσ1σ2 σ2 2 ] The coefﬁcients of u ∈ RN ×1 and U ∈ RN ×r are drawn uniformly in [a, b] and σ1, σ2 are ﬁxed constants. By construction, the rank of Σt is 2. Both the mean and correlation coefﬁcient of the two underlying latent variables oscillate through time as ρt oscillates between -1 and 1. In our experiments, the constants are set to σ1 = σ2 = 0.1, a = −0.5, b = 0.5 and T = 24, 000. In Figure 3, we compare the one-step-ahead predicted covariance given by our model, i.e. the lower triangle of Σ(ht), to the true covariance, showing that the model is able to recover the complex underlying pattern of the covariance matrix. 6 0 10 20 30 40 50 60 70 0.004 0.002 0.000 0.002 0.004 0.006 0 10 20 30 40 50 60 70 0.004 0.002 0.000 0.002 0.004 0.006 Figure 3: True (solid line) and predicted (dashed line) covariance after training with N = 4 (left) and N = 8 (right) time series. Each line corresponds to an entry in the lower triangle of Σt (including the diagonal, i.e. 10 lines in the left plot, 28 in the right). Experiments with real-world datasets. The following publicly-available datasets are used to compare the accuracy of different multivariate forecasting models. • Exchange rate: daily exchange rate between 8 currencies as used in [16] • Solar: hourly photo-voltaic production of 137 stations in Alabama State used in [16] • Electricity: hourly time series of the electricity consumption of 370 customers [6] • Trafﬁc: hourly occupancy rate, between 0 and 1, of 963 San Francisco car lanes [6] • Taxi: spatio-temporal trafﬁc time series of New York taxi rides [31] taken at 1214 locations every 30 minutes in the months of January 2015 (training set) and January 2016 (test set) • Wikipedia: daily page views of 2000 Wikipedia pages used in [11] Each dataset is split into a training and test set by using all data prior to a ﬁxed date for the training and by using rolling windows for the test set. We measure accuracy on forecasts starting on time points equally spaced after the last point seen for training. For hourly datasets, accuracy is measured on 7 rolling time windows, for all other datasets we use 5 time windows, except for taxi, where 57 windows are used in order to cover the full test set. The number of steps predicted τ , domain, time-frequency, dimension N and time-steps available for training T is given in the appendix for all datasets. Evaluation against baseline and ablation study. As we are looking into modeling correlated time series, only methods that are able to produce correlated samples are considered in our compar- isons. The ﬁrst baseline is VAR, a multivariate linear vector auto-regressive model using lag 1 and a lag corresponding to the periodicity of the data. The second is GARCH, a multivariate conditional heteroskedasticity model proposed by [34] with implementation from [12]. More details about these methods can be found in the supplement. We also compare with different RNN architectures, distributions, and data transformation schemes to show the beneﬁt of the low-rank Gaussian Copula Process that we propose. The most straightfor- ward alternative to our approach is a single global LSTM that receives and predicts all target dimen- sions at once. We refer to this architecture as Vec-LSTM. We compare this architecture with the GP approach described in Section 4, where the LSTM is unrolled on each dimensions separately before reconstructing the joint distribution. For the output distribution in the Vec-LSTM architecture, we compare independent 4, low-rank and full-rank normal distributions. For the data transformation we compare the copula approach that we propose, the mean scaling operation proposed in [26], and no transformation. 4Note that samples are still correlated with a diagonal noise due to the conditioning on the LSTM state. 7 baseline architecture data transformation distribution CRPS ratio CRPS-Sum ratio num params ratio VAR - - - 10.0 10.9 35.0 GARCH - - - 7.8 6.3 6.2 Vec-LSTM-ind Vec-LSTM None Independent 3.6 6.8 13.9 Vec-LSTM-ind-scaling Vec-LSTM Mean-scaling Independent 1.4 1.4 13.9 Vec-LSTM-fullrank Vec-LSTM None Full-rank 29.1 44.4 103.4 Vec-LSTM-fullrank-scaling Vec-LSTM Mean-scaling Full-rank 22.5 37.6 103.4 Vec-LSTM-lowrank-Copula Vec-LSTM Copula Low-rank 1.1 1.7 20.3 GP GP None Low-rank 4.5 9.5 1.0 GP-scaling GP Mean-scaling Low-rank 2.0 3.4 1.0 GP-Copula GP Copula Low-rank 1.0 1.0 1.0 Table 1: Baselines summary and average ratio compared to GP-Copula for CRPS, CRPS-Sum and number of parameters on all datasets. The description of the baselines as well as their average performance compared to GP-Copula are given in Table 1. For evaluation, we generate 400 samples from each model and evaluate multi-step accuracy using the continuous ranked probability score metric [20] that measures the accuracy of the predicted distribution (see supplement for details). We compute the CRPS metric on each time series individually (CRPS) as well on the sum of all time series (CRPS-Sum). Both metrics are averaged over the prediction horizon and over the evaluation time points. RNN models are trained only once with the dates preceding the ﬁrst rolling time point and the same trained model is then used on all rolling evaluations. Table 2 reports the CRPS-Sum accuracy of all methods (some entries are missing due to models requiring too much memory or having divergent losses). The individual time series CRPS as well as mean squared error are also reported in the supplement. Models that do not have data transforma- tions are generally less accurate and more unstable. We believe this to be caused by the large scale variation between series also noted in [26, 27]. In particular, the copula transformation performs better than mean-scaling for GP, where GP-Copula signiﬁcantly outperforms GP-scaling. The GP-Copula model that we propose provides signiﬁcant accuracy improvements on most datasets. In our comparison CRPS and CRPS-Sum are improved by on average 10% and 40% (respectively) compared to the second best models for those metrics Vec-LSTM-lowrank-Copula and Vec-LSTM-ind-scaling. One factor might be that the training is made more robust by adding randomness, as GP models need to predict different groups of series for each training example, mak- ing it harder to overﬁt. Note also that the number of parameters is drastically smaller compared to Vec-LSTM architectures. For the trafﬁc dataset, the GP models have 44K parameters to estimate compared to 1.1M in a Vec-LSTM with a low-rank distribution and 38M parameters with a full-rank distribution. The complexity of the number of parameters are also given in Table 3. We also qualitatively assess the covariance structure predicted by our model. In Fig. 1, we plot the predicted correlation matrix for several time steps after training on the Taxi dataset. We following the approach in [17] and reconstruct the covariance graph by truncating edges whose correlation coefﬁcient is less than a threshold kept constant over time. Fig. 1 shows the spatio-temporal cor- relation graph obtained at different hours. The predicted correlation matrices show how the model reconstructs the evolving topology of spatial relationships in the city trafﬁc. Covariance matrices predicted over time by our model can also be found in the appendix for other datasets. Additional details concerning the processing of the datasets, hyper-parameter optimization, eval- uations, and model are given in the supplement. The code to perform the evaluations of our methods and different baselines is available at https://github.com/mbohlkeschneider/gluon- ts/tree/mv_release. 6 Conclusion We presented an approach to obtain probabilistic forecast of high-dimensional multivariate time series. By using a low-rank approximation, we can avoid the potentially very large number of pa- rameters of a full covariate matrix and by using a low-rank Gaussian Copula process we can stably optimize directly parameters of an autoregressive model. We believe that such techniques allow- ing to estimate high-dimensional time varying covariance matrices may open the door to several applications in anomaly detection, imputation or graph analysis for time series data. 8 CRPS-Sum dataset exchange solar elec trafﬁc taxi wiki estimator VAR 0.010+/-0.000 0.524+/-0.001 0.031+/-0.000 0.144+/-0.000 0.292+/-0.000 3.400+/-0.003 GARCH 0.020+/-0.000 0.869+/-0.000 0.278+/-0.000 0.368+/-0.000 - - Vec-LSTM-ind 0.009+/-0.000 0.470+/-0.039 0.731+/-0.007 0.110+/-0.020 0.429+/-0.000 0.801+/-0.029 Vec-LSTM-ind-scaling 0.008+/-0.001 0.391+/-0.017 0.025+/-0.001 0.087+/-0.041 0.506+/-0.005 0.133+/-0.002 Vec-LSTM-fullrank 0.646+/-0.114 0.956+/-0.000 0.999+/-0.000 - - - Vec-LSTM-fullrank-scaling 0.394+/-0.174 0.920+/-0.035 0.747+/-0.020 - - - Vec-LSTM-lowrank-Copula 0.007+/-0.000 0.319+/-0.011 0.064+/-0.008 0.103+/-0.006 0.326+/-0.007 0.241+/-0.033 GP 0.011+/-0.001 0.828+/-0.010 0.947+/-0.016 2.198+/-0.774 0.425+/-0.199 0.933+/-0.003 GP-scaling 0.009+/-0.000 0.368+/-0.012 0.022+/-0.000 0.079+/-0.000 0.183+/-0.395 1.483+/-1.034 GP-Copula 0.007+/-0.000 0.337+/-0.024 0.024+/-0.002 0.078+/-0.002 0.208+/-0.183 0.086+/-0.004 Table 2: CRPS-sum accuracy comparison (lower is better, best two methods are in bold). Mean and std are obtained by rerunning each method three times. input output independent low-rank full-rank Vec-LSTM O(N k) O(N k) O(N rk) O(N 2k) GP O(k) O(k) O(rk) O(N 2k) Table 3: Number of parameters for input and output projection of different models. We recall that N and k denotes the dimension and size of the LSTM state. References [1] Wolfgang Aussenegg and Christian Cech. A new copula approach for high-dimensional real world portfolios. University of Applied Sciences bﬁ Vienna, Austria. Working paper series, 68 (2012):1–26, 2012. [2] Luc Bauwens, Sébastien Laurent, and Jeroen VK Rombouts. Multivariate garch models: a survey. Journal of applied econometrics, 21(1):79–109, 2006. [3] Ben S Bernanke, Jean Boivin, and Piotr Eliasz. Measuring the effects of monetary policy: a factor-augmented vector autoregressive (favar) approach. The Quarterly journal of economics, 120(1):387–422, 2005. [4] G. E. P. Box and D. R. Cox. An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), 26(2):211–252, 1964. [5] Laurent AF Callot and Anders B Kock. Oracle efﬁcient estimation and forecasting with the adaptive lasso and the adaptive group lasso in vector autoregressions. Essays in Nonlinear Time Series Econometrics, pages 238–268, 2014. [6] Dua Dheeru and Eﬁ Karra Taniskidou. UCI machine learning repository, 2017. URL http: //archive.ics.uci.edu/ml. [7] James Durbin and Siem Jan Koopman. Time series analysis by state space methods, volume 38. Oxford University Press, 2012. [8] Gal Elidan. Copulas in machine learning. In Piotr Jaworski, Fabrizio Durante, and Wolf- gang Karl Härdle, editors, Copulae in Mathematical and Quantitative Finance, pages 39–60, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-642-35407-6. [9] Robert Engle. Dynamic conditional correlation: A simple class of multivariate generalized au- toregressive conditional heteroskedasticity models. Journal of Business & Economic Statistics, 20(3):339–350, 2002. [10] B S Everitt. An Introduction to Latent Variable Models. Chapman and Hill, 1984. [11] Jan Gasthaus, Benidis Benidis, Konstantinos, Yuyang Wang, Syama S. Rangapuram, David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quan- tile function RNNs. AISTATS, 2019. 9 [12] Alexios Ghalanos. rmgarch: Multivariate GARCH models., 2019. R package version 1.3-6. [13] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [14] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015. URL http://arxiv.org/ abs/1502.04623. [15] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9 (8):1735–1780, 1997. [16] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and short- term temporal patterns with deep neural networks. CoRR, abs/1703.07015, 2017. URL http: //arxiv.org/abs/1703.07015. [17] Han Liu, John Lafferty, and Larry Wasserman. The nonparanormal: Semiparametric estima- tion of high dimensional undirected graphs. Journal of Machine Learning Research, 10(Oct): 2295–2328, 2009. [18] Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman, et al. High-dimensional semiparametric Gaussian copula graphical models. The Annals of Statistics, 40(4):2293–2326, 2012. [19] Helmut Lütkepohl. New introduction to multiple time series analysis. Springer Science & Business Media, 2005. [20] James E Matheson and Robert L Winkler. Scoring rules for continuous probability distribu- tions. Management science, 22(10):1087–1096, 1976. [21] Junier Oliva, Avinava Dubey, Manzil Zaheer, Barnabas Poczos, Ruslan Salakhutdinov, Eric Xing, and Jeff Schneider. Transformation autoregressive networks. In Jennifer Dy and An- dreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3898–3907, Stockholmsmäs- san, Stockholm Sweden, 10–15 Jul 2018. PMLR. [22] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for den- sity estimation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish- wanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2338–2347. Curran Associates, Inc., 2017. [23] Andrew J Patton. A review of copula models for economic time series. Journal of Multivariate Analysis, 110:4–18, 2012. [24] Sam Roweis and Zoubin Ghahramani. A unifying review of linear Gaussian models. Neural Computation, 11(2):305–345, 1999. [25] Donald B Rubin and Dorothy T Thayer. EM algorithms for ML factor analysis. Psychometrika, 47(1):69–76, 1982. [26] David Salinas, Valentin Flunkert, and Jan Gasthaus. Deepar: Probabilistic forecasting with autoregressive recurrent networks. CoRR, abs/1704.04110, 2017. URL http://arxiv.org/ abs/1704.04110. [27] Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: A deep neural net- work approach to high-dimensional time series forecasting. arXiv preprint arXiv:1905.03806, 2019. [28] M Sklar. Fonctions de repartition à n dimensions et leurs marges. Publications de l’Institut de Statistique de l’Université de Paris,, 8:229–231, 1959. 10 [29] C. Spearman. \"general intelligence,\" objectively determined and measured. The American Journal of Psychology, 15(2):201–292, 1904. ISSN 00029556. URL http://www.jstor. org/stable/1412107. [30] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014. [31] NYC Taxi and Limousine Commission. TLC trip record data. https://www1.nyc.gov/ site/tlc/about/tlc-trip-record-data.page, 2015. [32] J. Toubeau, J. Bottieau, F. Vallée, and Z. De Grève. Deep learning-based multivariate proba- bilistic forecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems, 34(2):1203–1215, March 2019. ISSN 0885-8950. doi: 10.1109/TPWRS.2018. 2870041. [33] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A genera- tive model for raw audio. CoRR, abs/1609.03499, 2016. [34] Roy Van der Weide. Go-garch: a multivariate generalized orthogonal garch model. Journal of Applied Econometrics, 17(5):549–564, 2002. [35] Ruofeng Wen and Kari Torkkola. Deep generative quantile-copula models for probabilistic forecasting. arXiv preprint arXiv:1907.10697, 2019. [36] Ruofeng Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile recurrent forecaster. arXiv preprint arXiv:1711.11053, 2017. [37] Andrew Gordon Wilson and Zoubin Ghahramani. Copula processes. In Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2, NIPS’10, pages 2460–2468, USA, 2010. Curran Associates Inc. URL http://dl.acm.org/ citation.cfm?id=2997046.2997170. 11 High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes Supplementary material David Salinas NAVER LABS Europe ∗ david.salinas@naverlabs.com Michael Bohlke-Schneider Amazon Research bohlkem@amazon.com Laurent Callot Amazon Research lcallot@amazon.com Roberto Medico Ghent University ∗ roberto.medico91@gmail.com Jan Gasthaus Amazon Research gasthaus@amazon.com Contents A Multivariate Likelihood 2 B Empirical CDF 2 C Effect of rank on low-rank approximation 3 D Baseline additional description 3 E Hyperparameter optimization 4 F Dataset details 5 G Metrics 5 G.1 Continuous Ranked Probability Score (CRPS) . . . . . . . . . . . . . . . . . . . . 5 G.2 Mean Squared Error (MSE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 H Comparison with forecasting methods with diagonal covariance 6 I Predicted correlation matrices 8 J Effect of the number of evaluation samples on CRPS and inference runtime 8 K Additional experiments details 8 L Open-source implementation of our model 10 ∗Work done while being at Amazon Research 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1910.03002v2 [cs.LG] 24 Oct 2019 A Multivariate Likelihood The probability density function of a multivariate normal distribution can be expressed as ϕµ,Σ(x) = 1 √ (2π)d|L| exp (− 1 2 ||L −1(x − µ)|| 2) where µ ∈ Rd and L ∈ Rd×d is the Cholesky factor of the covariance matrix Σ = LL T . This form is particularly amenable to computation using common neural network frameworks, as we only need to compute the determinant of a triangular matrix and solve a triangular system, which can both be done in O(d 2). This approach works in modest dimensions, but the quadratic cost in computational time and number of parameters becomes prohibitive when considering larger dimensions. This issue can be avoided by utilizing a low-rank matrix Σ = D + V V T , where D ∈ Rd×d and diagonal, and V ∈ Rd×r where r ≪ d is a rank hyper-parameter. This parametrization of the covariance matrix also arises from the factor analysis model [18, 13, 3, 12], i.e. as the marginal distribution of x under the latent variable model y ∼ N (0, I), x ∼ N (V x, D) When the covariance matrix is restricted in this way, it has O(dr) parameters, and the Gaussian likelihood can be computed in O(dr2 + r3) time. In particular, the log-likelihood log ϕµ,Σ(x) can be evaluated by ﬁrst computing an r-by-r matrix C = Ir + V T D−1V in O(dr2) time, followed by computing its Cholesky decomposition C = LCL T C in O(r3) time. Using the matrix determinant lemma in [7] we can write log |Σ| = log |D + V V T | = log |C| + log |D| = 2 log |LC| + log |D| which can be computed in O(d + r) as LC and D are triangular. Given LC, the Mahalanobis distance x T Σ−1x can also be computed efﬁciently. By the Woodbury matrix identity we have Σ−1 = D−1 − D−1V C −1V T D−1. We can then write, xT Σ−1x = xT (D−1 − D−1V C −1V T D−1)x = xT D−1x − xT (D−1V C −1V T D−1)x = xT D−1x − yT C −1y, with y = V T D−1x = xT D−1x − ||L −1 C y|| 2 The ﬁrst term of the ﬁnal equality can be computed in O(d) and the second term can be computed with back-substitution in O(r2), so that the total time is O(dr2 + r3) and the number of parameters is O(dr). The factor analysis latent variable model is closely related to PCA [21]: If we restrict the diagonal matrix D to a multiple of the identity matrix, D = ψI, we obtain a probabilistic version of PCA, from which the classical PCA can recovered in the limit ψ → 0. Previous work [19] has applied PCA as a preprocessing step for uncovering latent structure. Here we propose an end-to-end approach that learns the structure of the covariance matrix jointly with the time series model. B Empirical CDF The naive empirical CDF estimator can exhibit large variance and the following truncated estimator from [10] is used instead: ˜Fi(v) =  | | δm if ˆFi(v) < δm ˆFi(v) if δm ≤ ˆFi(v) ≤ 1 − δm 1 − δm if ˆFi(v) > 1 − δm , 2 where choosing δm = 1 4m1/4√π log m strikes the right bias-variance trade-off [10]. Further, we add jitter noise at training when computing the mapping fi to smooth the CDF for discrete data. C Effect of rank on low-rank approximation The effect of the low-rank approximation is analyzed in Figure 1 and Table 1 on the electricity dataset. As expected, the negative log-likelihood training loss decreases as the rank r of V increases in Figure 1. Table 1 shows the impact of the rank on the training/test loss. While the training loss decreases as the rank is increased, our model reaches its best performance on the test dataset with rank values 32/64. For higher ranks (128 and 256), the difference between training and test loss increases. This indicates that the high rank models may over-ﬁt to the training data due to the ﬂexibility of high-rank covariance matrix approximation. 200 0 0 2000 4000 6000 8000 Training time 400 350 300 250Training loss 1 2 4 8 16 32 64 128 256 Figure 1: Training loss vs training time when in- creasing rank on the electricity dataset. rank test NLL train NLL 1 -291.4+/-8.2 -288.9+/-8.2 2 -306.2+/-6.7 -304.8+/-5.7 4 -319.3+/-4.9 -312.1+/-3.5 8 -333.6+/-7.7 -330.2+/-6.3 16 -334.8+/-4.9 -337.5+/-4. 32 -341.8+/-6.8 -345.2+/-17.0 64 -338.5+/-10.9 -360.5+/-10.7 128 -326.6+/-20.1 -393.7+/-26.1 256 -238.0+/-38.4 -423.1+/-20.7 Table 1: Error metrics when evaluating on the electricity dataset with increasing rank. We show the mean +/- 95% conﬁdence interval over ﬁve runs. D Baseline additional description The GARCH [22] (Generalize Orthogonal - Generalize Autoregressive Conditional Heteroskedastic- ity) model is a composit model providing dynamics for the conditional mean and conditional covari- ance matrix of a multivariate system. The model for the conditional mean is, here, an autoregressive model of order one, zi,t = αi + βizi,t−1 + ϵi,t. Deﬁne ϵt = [ϵ1,t, ..., ϵN,t] ′. To predict the conditional covariance matrix, the GARCH model maps ϵt to a set of F = min(N, T ) independent factors ft = [f1,t, ..., fF,t] ′, ϵt = Aft, where A is a time independent, invertible matrix of dimension [N × F ]. The conditional variance σ2 j,t, j = [1, ..., F ] of each of the factors is modeled using independent GARCH-type models, here a GARCH(1,0): σ2 j,t = ωj + γjσ2 j,t−1. The linear mapping A and the factors f are estimated using the ICA method of [2, 24]. Let ft = H 1/2 t xt, where xt is a vector of independent random variables with conditional mean zero and conditional variance one. Ht is the diagonal matrix of conditional variances of the factors with diagonal [σ2 1,t, ..., σ2 F,t]. The conditional covariance matrix zt is then given by Σt = A′HtA. We use the GARCH implementation of [5]. The VAR model is a multivariate linear vector auto-regression using lag 1 and a lag l where l corre- sponds to the periodicity of the data, zt = µ + B1zt−1 + Blzt−l + ϵt. µ is a vector of intercepts of dimension [N × 1], and B1 and Bl are parameter matrices of dimension [N ×N ]. Letting zi = [zi,l+1, ..., zi,T ] ′, xt = [z′ t−1, z′ t−l]′, and X = [xl+1, ..., xT ]′, each individual equation of the model can be written in stacked form as zi = µi + Xθi + ϵi, (1) 3 where µi is a scalar intercept parameter and θi is a parameter vector of length 2N . The parameters of equation 1 are estimated by the Lasso implemented in [4] using the procedure described in [8]. The estimated parameters are the minimizers of the loss function L(µi, θi) = 1 T − l ∥zi − µi − Xθi∥ 2 ℓ2 + 2λi ∥θi∥ℓ1 , where ∥.∥ℓ2 is the ℓ2-norm and ∥.∥ℓ1 ℓ1-norm. λi is a tuning parameter whose selection procedure is explained below. E Hyperparameter optimization Parameters are learned with SGD using ADAM optimizer with batch of 16 elements, l2 regulariza- tion with 1e-8 and gradient clipped to 10.0. For all methods, we apply 10000 gradient updates in total and decay the learning rate by 2 after 500 consecutive updates without improvement. Table 2 lists the parameters that are tuned as well as the value of hyper-parameters that are not tuned and kept constant across all datasets. HYPERPARAMETER VALUE OR RANGE SEARCHED learning rate [1e-4, 1e-4, 1e-2] LSTM cells [10, 20, 40] LSTM layers 2 rank 10 num eval samples 400 conditioning length m 100 sampling dimension B 20 dropout 0.01 batch size 16 Table 2: Hyper-parameters values ﬁxed or range searched in hyper-parameter tuning. To tune hyper-parameters of RNN methods we perform a grid-search of 12 parameters on Electricity and Exchange for Vec-LSTM-ind-scaling. The best hyperparameter for a method is set as the hyperparameter having the best average rank for CRPS. The best learning-rate/number-cells found for Vec-LSTM-ind-scaling is 1e-3 / 40, as LSTM and GP baselines has many variations, we use the same hyperparameter for all variants. The Lasso estimator of the VAR model has a single Hyperparameter λi for each equation. The best value of the parameter is selected within the sequence of values considered by the path-wise coordinate descent algorithm [4]. λi is in the range [λi,min, λi,max], where λi,max is the smallest value of λi such that all penalized parameters of the VAR are set to zero while λi,min = ελi,max where ε = 0.0001 if N < T and ε = 0.01 otherwise[4]. The best value of λi the value in the sequence that minimizes a Bayesian Information Criterion [8]. For the GARCH model, we performed a search among all combinations of mean and vari- ance model speciﬁcations. The mean models considered were: AR(1), AR(Seasonal), VAR(1), VAR(Seasonal). The models for the variance components considered were: GARCH(1,0), GARCH(1,1), fGARCH(1,0) and fGARCH(1,1) [5]. We found that the only speciﬁcation able to consistently converge in even the smallest of our datasets was using AR(1) as the mean model and GARCH(1,0) for the variance components. 4 F Dataset details dataset τ (num steps predicted) domain frequency dimension N time steps T Exchange rate 30 R + daily 8 6071 Solar 24 R + hourly 137 7009 Electricity 24 R + hourly 370 5790 Trafﬁc 24 R + hourly 963 10413 Taxi 24 N 30-min 1214 1488 Wikipedia 30 N daily 2000 792 Table 3: Summary of the datasets used to test the models. Number of steps forecasted, data domain D, frequency of observations, dimension of series N , and number of time steps T . Datasets (or their processing) will be made available after publication. Table 3 shows the properties of the used datasets. We only describe the processing for Taxi as all other datasets have been used in previous publications. The dataset obtained from [20] is preprocessed with the following steps similarly to [17]: • Data cleaning: removal of outliers in terms of average speed (> 45.31 mph), trip duration (> 720 minutes), trip distance (> 23 miles) and trip fare (> 86.6); • Data reduction: the dataset is reduced to the most active areas by retaining the area bounded by (40.70,-74.07) and (40.84,-73.95), expressed as (latitude, longitude) pairs; • Data binning: ﬁrst, the data is binned over time, using a frequency of 30 minutes; after- wards, the data is aggregated spatially, by binning latitude and longitude on a grid with spatial granularity of 0.001; • For each subregion in the spatial grid and within each 30 minutes interval, the number of pickups and dropoffs are summed; • Data ﬁltering: the least active areas are ﬁltered out from the data, by retaining only areas with at least 80% non-zero observations. This results in a total of 1214 time series. We use January 2015 for the training set and January 2016 for the test set as in [17]. G Metrics G.1 Continuous Ranked Probability Score (CRPS) The continuous ranked probability score (CRPS) [11, 6] measures the compatibility of a probability distribution F (represented by its quantile function F −1) with an observation y. The pinball loss (or quantile loss) at a quantile level α ∈ [0, 1] and with a predicted α-th quantile q is deﬁned as Λα(q, y) = (α − I[y<q])(y − q). (2) The CRPS has an intuitive deﬁnition as the pinball loss integrated over all quantile levels α ∈ [0, 1], CRPS(F −1, y) = ∫ 1 0 2Λα(F −1(α), y) dα. (3) An important property of the CRPS is that it is a proper scoring rule [6], implying that the CRPS is minimized when the predictive distribution is equal to the distribution from which the data is drawn. In our setting, we are interested in evaluating the accuracy of the prediction compared to an obser- vation zt ∈ RN . To do so, we generate predictions in the form of 400 samples which allows to estimate the quantile function F −1 predicted by the model. We then report average marginal CRPS over dimensions and over predicted steps in Table 4, e.g. we report Ei,t[CRPS(F −1 i , zi,t)] where F −1 i is obtained by sorting the samples drawn when predicting zi,t. 5 CRPS dataset exchange solar elec trafﬁc taxi wiki estimator VAR 0.015+/-0.000 0.595+/-0.000 0.060+/-0.000 0.222+/-0.000 0.410+/-0.000 4.101+/-0.002 GARCH 0.024+/-0.000 0.928+/-0.000 0.291+/-0.000 0.426+/-0.000 - - Vec-LSTM-ind 0.020+/-0.001 0.480+/-0.031 0.765+/-0.005 0.234+/-0.007 0.495+/-0.002 0.800+/-0.028 Vec-LSTM-ind-scaling 0.013+/-0.000 0.434+/-0.012 0.059+/-0.001 0.168+/-0.037 0.586+/-0.004 0.379+/-0.004 Vec-LSTM-fullrank 0.610+/-0.096 0.939+/-0.001 0.997+/-0.000 - - - Vec-LSTM-fullrank-scaling 0.377+/-0.115 1.003+/-0.021 0.749+/-0.020 - - - Vec-LSTM-lowrank-Copula 0.009+/-0.000 0.384+/-0.010 0.084+/-0.006 0.165+/-0.004 0.416+/-0.004 0.247+/-0.001 GP 0.029+/-0.000 0.834+/-0.002 0.900+/-0.023 1.255+/-0.562 0.475+/-0.177 0.870+/-0.011 GP-scaling 0.017+/-0.000 0.415+/-0.009 0.053+/-0.000 0.140+/-0.002 0.346+/-0.348 1.549+/-1.017 GP-Copula 0.008+/-0.000 0.371+/-0.022 0.056+/-0.002 0.133+/-0.001 0.360+/-0.201 0.236+/-0.000 Table 4: CRPS accuracy metrics (lower is better, best two methods are in bold). Mean and standard error are reported by running each method 3 times. MSE MSE-sum num_params estimator VAR - - - GARCH - - - Vec-LSTM-ind 17.0 52.3 13.6 Vec-LSTM-ind-scaling 1.3 1.3 13.6 Vec-LSTM-fullrank 801.8 1545.6 103.4 Vec-LSTM-fullrank-scaling 985.3 1937.5 103.4 Vec-LSTM-lowrank-Copula 4.7 10.4 19.2 GP 122.3 1080.7 1.0 GP-scaling 1.1 3.0 1.0 GP-Copula 1.0 1.0 1.0 Table 5: Baselines summary and average ratio compared to GP-Copula for MSE, MSE-Sum, and number of parameters on all datasets. To account for joint effect, we also report CRPS-Sum where accuracy is measured on the predicted distribution of the sum, e.g. Et[CRPS(F −1, ∑ i zi,t)] Where F −1 is obtained by ﬁrst summing samples across dimensions and then sorting to get quan- tiles. Integrals are estimated with 10 equally-spaced quantiles. G.2 Mean Squared Error (MSE) The MSE is deﬁned as the mean squared error over all time series, i.e., i = 1, . . . N , and over the whole prediction range, i.e., t = T − t0 + 1, . . . , T : MSE = 1 N (T − t0) ∑ i,t (zi,t − ˆzi,t) 2 (4) where z is the target and ˆz the predicted distribution mean. Tables 5 - 7 show the MSE results for the marginal MSE and the MSE-sum. The deﬁnition of MSE-sum is analogous to CRPS-sum. H Comparison with forecasting methods with diagonal covariance We evaluated our approach against DeepAR [14] and MQCNN [23], which we believe are a fair representation of the state-of-the-art in deep-learning-based forecasting. We also compared with DeepGLO [16] on two datasets provided by the authors. Table 8 lists the results of this compari- son. Note that none of these competing approaches models correlations across time series in their forecasts (DeepGLO only provides point forecasts). 6ExchangerateSolarElectricityTrafﬁcTaxiWikipedia Figure 2: Correlation matrix predicted by our model at four different equally spaced time-steps with step = 6 for all datasets in this study. Exchange rate is nearly homoscedastic and most correlations are close to 1, because all currencies are relative to US dollar and therefore highly correlated. The remaining datasets are clearly heteroscedastic. For example, solar has low correlation at night and electricity/trafﬁc/taxi follow day-night cycles. Wikipedia also shows heteroscedasticity across time. 7 MSE dataset exchange solar elec trafﬁc taxi wiki estimator VAR 4.4e-2+/-2.2e-5 7.0e3+/-2.5e1 1.2e7+/-5.4e3 5.1e-3+/-2.9e-6 - - GARCH 4.0e-2+/-5.3e-5 3.5e3+/-2.0e1 1.2e6+/-2.5e4 3.3e-3+/-1.8e-6 - - Vec-LSTM-ind 3.9e-4+/-2.0e-4 9.9e2+/-2.8e2 2.6e7+/-4.6e4 6.5e-4+/-1.1e-4 5.2e1+/-2.2e-1 5.2e7+/-3.8e5 Vec-LSTM-ind-scaling 1.6e-4+/-2.6e-5 9.3e2+/-1.9e2 2.1e5+/-1.2e4 6.3e-4+/-5.6e-5 7.3e1+/-1.1e0 7.2e7+/-2.1e6 Vec-LSTM-fullrank 5.2e-1+/-1.5e-1 3.8e3+/-1.8e1 2.7e7+/-2.3e2 - - - Vec-LSTM-fullrank-scaling 6.5e-1+/-4.3e-2 3.8e3+/-6.9e1 3.2e7+/-1.1e7 - - - Vec-LSTM-lowrank-Copula 1.9e-4+/-1.3e-6 2.9e3+/-1.1e2 5.5e6+/-1.2e6 1.5e-3+/-2.5e-6 5.1e1+/-3.2e-1 3.8e7+/-1.5e5 GP 3.0e-4+/-4.8e-5 3.7e3+/-5.7e1 2.7e7+/-2.0e3 5.1e-1+/-2.5e-1 5.9e1+/-2.0e1 5.4e7+/-2.3e4 GP-scaling 2.9e-4+/-3.5e-5 1.1e3+/-3.3e1 1.8e5+/-1.4e4 5.2e-4+/-4.4e-6 2.7e1+/-1.0e1 5.5e7+/-3.6e7 GP-Copula 1.7e-4+/-1.6e-5 9.8e2+/-5.2e1 2.4e5+/-5.5e4 6.9e-4+/-2.2e-5 3.1e1+/-1.4e0 4.0e7+/-1.6e9 Table 6: MSE accuracy metrics (lower is better). Mean and standard error are reported by running each method 3 times. MSE-sum dataset exchange solar elec trafﬁc taxi wiki estimator VAR 1.2e0+/-8.6e-4 1.1e8+/-3.9e5 1.8e10+/-1.3e7 2.5e3+/-3.4e0 - - GARCH 1.1e0+/-2.0e-3 5.6e7+/-3.2e5 2.7e9+/-3.3e7 1.1e3+/-2.1e0 - - Vec-LSTM-ind 1.3e-2+/-7.0e-3 1.1e7+/-4.6e6 5.3e10+/-9.2e8 1.2e2+/-8.1e1 2.7e7+/-2.8e5 2.6e13+/-1.5e12 Vec-LSTM-ind-scaling 3.2e-3+/-1.3e-3 1.1e7+/-2.4e6 1.2e8+/-7.9e6 5.5e1+/-2.8e1 4.0e7+/-1.0e6 1.2e12+/-9.7e10 Vec-LSTM-fullrank 2.3e1+/-8.0e0 5.8e7+/-3.0e5 8.9e10+/-7.9e6 - - - Vec-LSTM-fullrank-scaling 3.0e1+/-2.5e0 5.6e7+/-7.8e5 8.8e10+/-1.7e10 - - - Vec-LSTM-lowrank-Copula 4.6e-3+/-8.3e-5 4.2e7+/-2.0e6 8.1e9+/-9.0e8 7.0e2+/-6.4e0 2.5e7+/-2.8e5 5.8e11+/-6.5e10 GP 7.2e-3+/-2.4e-3 5.5e7+/-1.0e6 8.6e10+/-1.9e9 4.3e5+/-2.2e5 3.3e7+/-1.8e7 3.5e13+/-9.5e10 GP-scaling 7.3e-3+/-1.8e-3 1.2e7+/-6.4e5 1.4e8+/-1.9e7 7.0e1+/-3.4e0 7.9e6+/-8.9e6 2.7e13+/-4.5e13 GP-Copula 4.2e-3+/-6.5e-4 1.2e7+/-8.3e5 1.5e8+/-3.5e7 6.2e1+/-4.3e0 1.0e7+/-1.1e6 1.9e12+/-2.2e15 Table 7: MSE-sum accuracy metrics (lower is better). Mean and standard error are reported by running each method 3 times. dataset exchange solar elec trafﬁc taxi wiki DeepAR [14] 0.007 0.379 0.063 0.147 0.332 0.337 MQCNN [23] 0.013 0.482 0.078 0.177 0.657 0.277 GP-Copula (Ours) 0.008 0.371 0.056 0.133 0.360 0.236 dataset electricity trafﬁc DeepGLO [16] 0.109 0.221 TRMF [16] 0.105 0.210 GP-Copula (Ours) 0.083 0.168 Table 8: CRPS for additional baselines (left) and comparison with [16] when measuring WAPE (right). I Predicted correlation matrices We illustrate the learned correlations of our model on all datasets in Figure 2. J Effect of the number of evaluation samples on CRPS and inference runtime Figure 3 shows the effect of the number of evaluation samples on the CRPS and inference runtime. Drawing more than 100 samples only has a small effect on the CRPS and linearly increases the inference runtime. K Additional experiments details We use generic features to represent time. For hourly dataset, we use hour of day, day of week, day of month features. For daily dataset, we use day of week feature. For minutes granularity, we use minute of hour, hour of day and day of week features. All features are encoded with one number, for instance hour of day feature takes values in [0, 23[. Feature values are concatenated to the LSTM 8 0.053 0.054 0.055 0.056 0.057 0.058 0.059 0.060CRPS dataset = electricity 0.340 0.345 0.350 0.355 0.360 0.365 0.370 0.375 dataset = taxi_30min 0.41 0.42 0.43 0.44 0.45estimator = GP-Copula dataset = solar 1050100 200 400 600 Number of samples 0.16 0.18 0.20 0.22 0.24 0.26CRPS 1050100 200 400 600 Number of samples 0.47 0.48 0.49 0.50 0.51 0.52 1050100 200 400 600 Number of samples 0.82 0.84 0.86 0.88 0.90estimator = Vec-LSTM-lowrank-Copula (a) Effect of the number of evaluation samples (num eval samples) on the CRPS for electricity, solar, and taxi datasets. The line shows the mean CRPS over three independent runs and the shaded area shows the 95% conﬁdence interval. Increasing the number of samples from 100 to 600 has a small effect on the CRPS (average CRPS over all datasets decreases from 0.272 to 0.271 for GP-Copula and from 0.52 to 0.50 for Vec-LSTM-lowrank-Copula, respectively). 2 3 4 5 6 7 8Runtime / min dataset = electricity 40 60 80 100 120 140 160 180Runtime / min dataset = taxi_30min 0.5 1.0 1.5 2.0 2.5Runtime / minestimator = GP-Copula dataset = solar 1050100 200 400 600 Number of samples 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85Runtime / min 1050100 200 400 600 Number of samples 12 13 14 15 16 17Runtime / min 1050100 200 400 600 Number of samples 0.20 0.22 0.24 0.26 0.28 0.30 0.32Runtime / minestimator = Vec-LSTM-lowrank-Copula (b) Effect of the number of evaluation samples (num eval samples) on the inference runtime. The inference runtime scales linearly with the number of drawn samples. The inferences runtimes for 10, 50, and 100 samples are similar due to initialization overhead. Note that the samples are only drawn during inference. Thus, the num eval samples parameter does not affect training runtime. Figure 3: Effect of the number of evaluation samples on CRPS and inference runtime. 9 CRPS-sum dataset exchange solar elec trafﬁc taxi wiki estimator GP-Copula 0.007+/-0.000 0.337+/-0.024 0.024+/-0.002 0.078+/-0.002 0.208+/-0.183 0.086+/-0.004 GP-Copula (GluonTS) 0.007+/-0.000 0.404+/-0.009 0.027+/-0.001 0.050+/-0.003 0.159+/-0.001 0.055+/-0.005 Table 9: CRPS-sum accuracy metrics for the GluonTS implementation of our model (lower is better). Mean and standard error are reported by running each method 3 times. CRPS dataset exchange solar elec trafﬁc taxi wiki estimator GP-Copula 0.008+/-0.000 0.371+/-0.022 0.056+/-0.002 0.133+/-0.001 0.360+/-0.201 0.236+/-0.000 GP-Copula (GluonTS) 0.009+/-0.000 0.416+/-0.007 0.054+/-0.000 0.106+/-0.002 0.339+/-0.001 0.244+/-0.003 Table 10: CRPS accuracy metrics for the GluonTS implementation of our model (lower is better). Mean and standard error are reported by running each method 3 times. input at each time-step. We also lags values as input L according to the time-frequency, [1, 24, 168] for hourly data, [1, 7, 14] for daily, and [1, 2, 4, 12, 24, 48] for 30 minutes data. All models are evaluated on a Amazon Web Services c5.4xlarge instance with 16 cores and 32GB RAM. All RNNs models take under ﬁve hours to perform training and evaluation. Missing numbers in Table 4 happens either because Out-of-memory prevents training or NaNs appear during training because of unstable models. Finally, RNNs are combined with Zone-out regularization [9] and residual connections and MXNet is used as the neural network framework [15]. L Open-source implementation of our model We re-implemented the model described in this paper in GluonTS [1], an open-source time series toolkit. To ensure re-reproducibility, we released a static version of the code online that is not part of the latest GluonTS releases (for which we cannot guarantee reproducibility over time) at https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release. Tables 9 and 10 show the bench- mark results of our re-implementation. Our GluonTS implementation performs similar to the imple- mentation that was used in this paper. In the new implementation, we set the sampling dimension B to 2. Furthermore, we found that the piecewise-constant derivatives did not improve the results and removed them from our implementation. 10 References [1] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C Maddix, Syama Rangapuram, David Salinas, and Jasper Schulz. Gluonts: Probabilistic time series models in python. arXiv preprint arXiv:1906.05264, 2019. [2] Simon A Broda and Marc S Paolella. Chicago: A fast and accurate method for portfolio risk calculation. Journal of Financial Econometrics, 7(4):412–436, 2009. [3] B S Everitt. An Introduction to Latent Variable Models. Chapman and Hill, 1984. [4] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22, 2010. URL http://www.jstatsoft.org/v33/i01/. [5] Alexios Ghalanos. rmgarch: Multivariate GARCH models., 2019. R package version 1.3-6. [6] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estima- tion. Journal of the American Statistical Association, 102(477):359–378, 2007. [7] David A Harville. Matrix algebra from a statistician’s perspective, 1998. [8] Anders Bredahl Kock and Laurent Callot. Oracle inequalities for high dimensional vector autoregressions. Journal of Econometrics, 186(2):325–344, 2015. [9] David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron C. Courville, and Chris Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations. CoRR, abs/1606.01305, 2016. URL http://arxiv.org/abs/1606.01305. [10] Han Liu, John Lafferty, and Larry Wasserman. The Nonparanormal: Semiparametric Estima- tion of High Dimensional Undirected Graphs. 10:2295–2328, 2009. ISSN 1532-4435. doi: 10.1016/0006-291X(91)91267-G. URL http://arxiv.org/abs/0903.0649. [11] James E Matheson and Robert L Winkler. Scoring rules for continuous probability distribu- tions. Management science, 22(10):1087–1096, 1976. [12] Sam Roweis and Zoubin Ghahramani. A unifying review of linear Gaussian models. Neural Computation, 11(2):305–345, 1999. [13] Donald B Rubin and Dorothy T Thayer. EM algorithms for ML factor analysis. Psychometrika, 47(1):69–76, 1982. [14] David Salinas, Valentin Flunkert, and Jan Gasthaus. Deepar: Probabilistic forecasting with autoregressive recurrent networks. CoRR, abs/1704.04110, 2017. URL http://arxiv.org/ abs/1704.04110. [15] Matthias W. Seeger, Asmus Hetzel, Zhenwen Dai, and Neil D. Lawrence. Auto-differentiating linear algebra. CoRR, abs/1710.08717, 2017. URL http://arxiv.org/abs/1710.08717. [16] Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: A deep neural net- work approach to high-dimensional time series forecasting. arXiv preprint arXiv:1905.03806, 2019. [17] Gaurav Sharma. Taxi demand prediction new york city. https://github.com/ gauravtheP/Taxi-Demand-Prediction-New-York-City, 2018. [18] C. Spearman. \"general intelligence,\" objectively determined and measured. The American Journal of Psychology, 15(2):201–292, 1904. ISSN 00029556. URL http://www.jstor. org/stable/1412107. 11 [19] James H Stock and Mark W Watson. Dynamic factor models, factor-augmented vector autore- gressions, and structural vector autoregressions in macroeconomics. In Handbook of macroe- conomics, volume 2, pages 415–525. Elsevier, 2016. [20] NYC Taxi and Limousine Commission. TLC trip record data. https://www1.nyc.gov/ site/tlc/about/tlc-trip-record-data.page, 2015. [21] Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999. [22] Roy Van der Weide. Go-garch: a multivariate generalized orthogonal garch model. Journal of Applied Econometrics, 17(5):549–564, 2002. [23] Ruofeng Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile recurrent forecaster. arXiv preprint arXiv:1711.11053, 2017. [24] Kun Zhang and Laiwan Chan. Efﬁcient factor garch models and factor-dcc models. Quantita- tive Finance, 9(1):71–91, 2009. 12","libVersion":"0.3.1","langs":""}
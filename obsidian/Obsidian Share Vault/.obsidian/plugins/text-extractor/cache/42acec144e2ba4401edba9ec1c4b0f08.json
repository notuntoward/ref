{"path":"lit/lit_sources/Thissen23Mixtral8x7BalternGPT4.pdf","text":"4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/12 Mixtral 8x7B on Your Local Computer | Free GPT-4 Alternative Martin Thissen Â· Follow 6 min read Â· Dec 17, 2023 In this article I will point out the key features of the Mixtral 8x7B model and show you how you can run the Mixtral 8x7B model on your local computer. If you like videos more, feel free to check out my YouTube video to this article: 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/12 Wohoo, Mistral AI released a new highly competitive LLM called Mixtral, which is a sparse mixture-of-experts model (which GPT-4 is supposed to be too). Letâ€™s first have a look at the highlights of the Mixtral 8x7B model. Highlights ðŸŽ‰ Outperforms Llama 2-70B on most benchmarks with 6x faster inference Open-source model weights (Apache 2.0 license, training data/code not released) Proficient in English, French, Italian, German, and Spanish Watch on Mixtr al On Y our Computer | Mixtur e-of-Experts LLM | Free GPT-4 AMixtr al On Y our Computer | Mixtur e-of-Experts LLM | Free GPT-4 Aâ€¦â€¦ShareShare 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/12 Gracefully handles a context of 32k tokens (~ 50 pages text) How can the Mixtral model outperform Llama 2â€“70B on most benchmarks with 6x faster inference? This is possible due to Mixtralâ€™s mixture-of-experts architecture. Mistral states it this way: Mixtral has 45B total parameters but only uses 12B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12B model. In the table below you can see how the Mixtral model compares to the Llama 2â€“70B and GPT-3.5 model on various benchmarks. However, it must be said that LLM benchmarks should be taken with a grain of salt, as gaming benchmarks is possible: 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/12 https://mistral.ai/news/mixtral-of-experts/ Mixture-of-Expert LLMs Why does the Mixtral model achieve such a high score even though it has almost half the parameters of the Llama 2 70B model and almost a quarter of the GPT 3.5 model? Compared to these two dense LLMs, the Mixtral model is a sparse mixture-of-experts model. Sparse Mixture-of-Experts (MoE) models have the following advantages compared to dense models: 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/12 Faster pre-training Faster inference (only certain (2 for the Mixtral model) experts are passed during inference, which drastically reduces the computational effort) However, the use of MoEs also has some disadvantages: Require high VRAM as all experts are loaded in memory Fine-tuning is difficult (prone to overfitting) Scaling large language models has led to increasingly astounding results. More training data and a larger model with more capacity seem to be the way to better and better results. Given a fixed computational budget, it is better to train a larger model with fewer steps than a smaller model with more steps. Sparse mixture-of-experts allow LLMs to be trained with far less computational effort, meaning you can dramatically increase the size of the model or dataset with the same computational budget as a dense model. But how do Mixture-of-Experts networks work in LLMs? We can take a look at the following illustration: 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 6/12 Illustration from the GShard paper. It can be seen that the mixture-of-experts network replaces the dense feed- forward layer (also known as multi-layer perceptron (MLP)) within the transformer architecture. Instead of a dense feed-forward layer, the mixture- of-experts network initially has a router that decides which experts a particular token should be forwarded to. Then there are several sparse experts, which are themselves feed-forward networks, instead of one large dense feed-forward network. In the Mixtral model, a token is processed by two experts (out of a total of eight) per layer. The output of the two experts is combined additively. 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 7/12 In this way, a lot of computational effort can be saved during each forward pass, but also during backpropagation (pre-training, fine-tuning) by not passing all neurons within the LLM. This is also the reason why Mixture-of- Experts are called sparse and existing LLMs like Llama 2 are called dense in this context, as all neurons are passed through during a forward pass. Mistral AI specifies the computational savings for the Mixtral model as follows: Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model. â€” Mistral AI Running the Mixtral 8\u00007B On Your Computer The model weights have a size of around 93 GB, which already indicates that quite a bit of RAM or VRAM is needed to load the Mixtral model. In a blog post by Hugging Face, I saw the following minimum requirements for loading the Mixtral model: float16 precision: >90 GB VRAM 8-bit precision: >45 GB VRAM 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 8/12 4-bit precision: >23 GB VRAM It also looks like people have been able to run the model with their CPU using the llama.cpp library. However, in this article I will focus on loading the Mixtral model using a GPU. Personally, I will run the Mixtral model using an NVIDIA RTX 6000 Ada GPU, which NVIDIA kindly provided me to support my YouTube and Medium channels. If you would like me to write another article showing how to run the Mixtral model on the CPU, please let me know in the comments. Before I could use the Mixtral model, I first had to install some libraries, including the popular transformers library from Hugging Face: pip install transformers==4.36 bitsandbytes==0.41.3 accelerate==0.25.0 scipy==1. Once all libraries have been successfully installed, we can load the LLM. To do this, we first define the model_id. In this case, I decided to load the instruct fine-tuned version of the Mixtral model. Then I decided to load the Mixtral model with 4-bit precision, which requires about 23 GB of VRAM. On 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 9/12 my first attempt, I got a warning saying that the computation data type within the model does not match the 16-bit precision data type expected from the 4- bit quantized model layers. For this reason, I had to specifically configure the computation data type using the BitsAndBytesConfig. model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16 ) pipeline = transformers.pipeline( \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True, \"quantizatio ) Using the text generation pipeline of the transformers library is very convenient and takes a lot of work off your hands (e.g. model initialization or formatting instructions). Using the following code, we can see how convenient it is to use the pipeline. We simply pass a message list containing dicts with a role and a content property to the apply_chat_template method and get a formatted prompt that is ready to use. It is important to note that only 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 10/12 the values user and assistant are allowed for the role property in the message list. Also, the pipeline expects the roles to occur in the alternating pattern user/assistant/user/assistant. messages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts is in prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_ge Finally, we can use the prompt and generate a response using the pipeline: outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, print(outputs[0][\"generated_text\"]) Thanks to the RTX 6000 Ada GPU, generating the modelâ€™s response is lightning fast. Previously, I often used the RTX 3090, but the use of NVIDIA GPUs with Ada architecture definitely makes a difference here. 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 11/12 The LLM response is generated using a sampling strategy. The individual parameters are explained below: max_new_tokens: Maximum number of tokens to be generated. temperature: Diversity of the generated text (<0.5 ensures more deterministic results, while >0.5 increases creativity and randomness). do_sample: Select the next token from a subset of the most probable next tokens instead of always selecting the token with the highest probability. top_k: Select the next token from the k most likely next tokens. top_p: Select the next output token from a subset of all the most probable next tokens, where the cumulative probability of the subset is greater than p. If top_k is also defined, the subset can contain a maximum of k tokens. Final Thoughts I hope you enjoyed this article. I will publish more articles about how to use AI models and how they work in the future. Follow me if that sounds 4/16/24, 2:01 PM Mixtral 8x7B on Your Local Computer | Free GPT-4 A lternative | by Martin Thissen | Medium chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 12/12 interesting to you. :-) Isnâ€™t collaboration great? Iâ€™m always happy to answer questions or discuss ideas proposed in my articles. So donâ€™t hesitate to reach out to me! ðŸ™Œ Also, make sure to subscribe or follow to not miss out on new articles. YouTube: https://bit.ly/3LqA1Os LinkedIn: http://bit.ly/3i5Sc1g","libVersion":"0.3.2","langs":""}
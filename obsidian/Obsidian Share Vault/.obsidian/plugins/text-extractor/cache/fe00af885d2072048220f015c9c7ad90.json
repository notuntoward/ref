{"path":"lit/lit_sources/XuSequentialPredictiveConformal.pdf","text":"Sequential Predictive Conformal Inference for Time Series Chen Xu 1 Yao Xie 1 Abstract We present a new distribution-free conformal pre- diction algorithm for sequential data (e.g., time series), called the sequential predictive conformal inference (SPCI). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction al- gorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction resid- uals), upon exploiting the temporal dependence among them. More precisely, we cast the prob- lem of conformal prediction interval as predicting the quantile of a future residual, given a user- specified point prediction algorithm. Theoreti- cally, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real- data experiments, we demonstrate a significant reduction in interval width of SPCI compared to other existing methods under the desired empiri- cal coverage. 1. Introduction Uncertainty quantification for prediction algorithms is es- sential for statistical and machine learning models. Sequen- tial prediction or time-series prediction aims to predict the subsequent outcome based on past observations. Uncer- tainty quantification in the form of prediction intervals is of particular interest for high-stake domains such as finance, energy systems, healthcare, and so on (Harries et al., 1999; D´ıaz-Gonz´alez et al., 2012; Cochran et al., 2015). Clas- sic approaches for prediction interval are typically based on strong parametric assumptions of time-series models such as autoregressive and moving average (ARMA) mod- els (Brockwell et al., 1991), which impose strong distri- bution assumptions on the data-generating process. There 1H. Milton Stewart School of Industrial and Systems Engi- neering, Georgia Institute of Technology, Atlanta, Georgia, USA. Correspondence to: Yao Xie <yao.xie@isye.gatech.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). need to be principled ways to perform uncertainty quan- tification for complex prediction models such as random forests (Breiman, 2001) and neural networks (Lathuili`ere et al., 2019). Conformal prediction (CP) has become a popular distribution-free technique to perform uncertainty quantifi- cation for complex machine learning algorithms. However, conformal prediction for time series has been a challenging case because such data do not satisfy the exchangeability as- sumption in conformal inference, and thus we need to adjust existing or even develop new sequential CP algorithms with theoretical guarantees. The challenges also arise in real- world applications where time series data tend to have sig- nificant stochastic variations and strong correlations. These challenges are illustrated via a real-data example for solar energy prediction, as shown in Figure 1, where the predic- tion residuals (using random forest as prediction algorithm) are still highly correlated. Besides the temporal correlation in the prediction residuals (or conformity scores in general), we observe that a notable feature of sequential conformal prediction is that the prediction residuals can be obtained as “feedback” to the algorithm. For instance, for one-step ahead prediction, the prediction accuracy of the prediction algorithm is revealed immediately after one-time step. Thus, the recent prediction residuals reveal whether or not the predictive algorithm is performing well for that segment of data. Such feedback structure is illustrated in Figure 2, which highlights the conceptual difference between tradi- tional conformal and sequential conformal prediction meth- ods. We specifically exploited such feedback structure in designing the sequential conformal prediction algorithms. More precisely, both the traditional conformal inference and the sequential conformal inference considered in this paper are general-purpose wrappers that can be used around any predictive model for any data and proceed by defining “non-conformity scores”. However, there are also signifi- cant differences: Traditional conformal prediction assumes exchangeable training and test data to obtain performance guarantees, which leads to exchangeable non-conformity scores, and cannot receive feedback during prediction. In contrast, sequential CP observes non-exchangeable data sequences and leverages feedback during prediction. In this work, we propose a sequential predictive confor- 1 Sequential Predictive Conformal Inference for Time Series mal inference (SPCI) framework for time series with scalar outputs. The idea is to utilize the feedback structure of prediction residuals in the sequential prediction problem to obtain desired coverage. We specifically exploit the serial dependence across prediction residuals (conformity score) by performing quantile regression using past residuals for the future prediction intervals.; thus, the most recent past residuals contain information about the immediate future ones. Similar to most existing conformal prediction liter- ature, we make no assumptions about the data-generating process or the quality of estimation by the point estimator. Our main contributions are • The main novelty of SPCI is the time-adaptive re- estimation of residual quantiles over time, upon leveraging the temporal dependency among residuals. We use Random Forest for quantile regression here, but SPCI is applicable to other quantile regression methods. • Theoretically, we obtain asymptotic conditional coverage of the constructed intervals for dependent data, based on prior results for random forest quantile regression. When data are exchangeable, we show that SPCI enjoys the same finite-sample and distribution-free marginal coverage guar- antee as traditional conformal prediction methods. • Experimentally, we demonstrate competitive and/or im- proved empirical performance against baseline CP methods on sequential data. In particular, SPCI can obtain signifi- cantly narrower intervals on real data without coverage loss. We further demonstrate the benefit of SPCI in multi-step predictive inference. 1.1. Literature review Conformal prediction has been an increasingly popular framework for distribution-free uncertainty quantification. Initially proposed in (Shafer & Vovk, 2008), CP methods generally proceed as follows. First, one designs a type of “non-conformity score” based on the given point estimator ˆf , where the score measures how different a potential value of the response variable Y is to existing observations. A common choice for such scores in regression problems is the prediction residual. Second, one computes these scores on a hold-out set not used to train the estimator ˆf . Third, the prediction interval is defined as all potential values of Y whose non-conformity score is less than 1 − α fraction of these scores over the hold-out set. Many existing works such as (Papadopoulos et al., 2007; Gupta et al., 2021; An- gelopoulos et al., 2021; Romano et al., 2020) utilize this idea for uncertainty quantification in regression or classifica- tion problems. Comprehensive surveys and tutorials can be found in (Fontana et al., 2023; Angelopoulos & Bates, 2021). CP framework are distribution-free and model-free: they require neither distributional assumptions on data nor spe- cial classes of prediction functions, hence being particularly 65 0 126 Histogram of { t}T t = 1 0 20 40 0.0 0.5 1.0 PACF Figure 1: Solar power radiation prediction for downtown At- lanta, Georgia, USA (further explanation in Section 5.2). We use random forest for one-step-ahead prediction. The his- togram of prediction residuals (left) shows that residual dis- tribution is highly skewed, and the partial auto-correlation between residuals (right) shows a significant serial correla- tion among residuals. Thus, it is essential to consider serial dependency when constructing prediction intervals: the se- rial dependence means that the most recent past residuals contain information about the immediate future ones. attractive in practice. Nevertheless, the desired performance guarantee of CP methods relies on exchangeability (e.g., the simplest case is when data are i.i.d.), which hardly holds for time series. Recently, significant efforts have been made to extend CP methods beyond exchangeable data; several are towards building sequential conformal prediction methods. They typically do so via updating non-conformity scores (e.g., prediction residuals) (Xu & Xie, 2021a;b) and/or adjust significance level α based on rolling coverage of Yt. This in- clude (Gibbs & Candes, 2021; Zaffran et al., 2022; Feldman et al., 2022; Lin et al., 2022) and specifically, the AdaptCI al- gorithm, which adjusts the significance level α based on real- time coverage status during prediction—the significance level is lower when the prediction interval at time t fails to contain the actual observation Yt. The prediction intervals thus have adaptive width based on the updated significance levels and maintain coverage on stock market data in prac- tice. Furthermore, (Barber et al., 2022) proves the coverage gap for non-exchangeable data based on the total variation (TV) distance between the non-conformity scores. The work then proposes NEX-CP, a general re-weighting scheme for non-exchangeable data, where the weights should ideally be chosen to be inversely proportional to the TV distances. The authors demonstrate the robustness of NEX-CP on datasets with change points and/or distribution shifts. For sequential data, (Xu & Xie, 2021b) proposes EnbPI, which updates residuals of ensemble predictors during prediction to more accurately calibrate prediction intervals. In practice, EnbPI can maintain desired 1 − α coverage for different types of time series. Despite the existing efforts, these sequential CP methods have not exploited serial correlation among non- conformity scores (cf. Figure 1)—they only use empirical quantiles (possibly with fixed weights) of past residuals to 2 Sequential Predictive Conformal Inference for Time Series compute intervals, which is a drastic difference from SPCI. Besides conformal prediction, probabilistic forecasting ap- proaches have also been widely used when building pre- dicting intervals. These approaches typically train a single model to minimize the pinball loss, including the MQ-CNN (Wen et al., 2017), DeepAR (Salinas et al., 2020), Temporal Fusion Transformer (TFT) (Lim et al., 2021), etc. However, comparing to SPCI and related CP works, these approaches have two major limitations. First, they are not “model-free”: special designs of the predictive model and hyper-parameter tuning are required for satisfactory performances. Second, they are not “distribution-free”: distributional assumptions on time-series are often imposed, such as Gaussianity (Sali- nas et al., 2020). Corresponding theoretical guarantees on constructed prediction intervals are also often lacking. In our experiments, we demonstrate the improved performance of SPCI against DeepAR and TFT. 1.2. Connection with related works Through theoretical analysis, we find that when using ran- dom forest quantile regression, SPCI can be viewed as adap- tively learning the (data-dependent) weights of the predic- tion residuals/non-conformity scores when constructing the prediction intervals using weighted quantile values. Hence, it has an interesting connection to the recent work (Barber et al., 2022), which develops a general conformal prediction framework for non-exchangeable data. In that work, weights are pre-determined and non-adaptive (such as geometrically decaying weights), and the authors also pointed out that “how to choose weights optimally ... is an interesting and important question that we leave for future work” and “leave a more detailed investigation of data dependent weights for future work” (Barber et al., 2022). So our work is a step towards this direction. We further remark on several key differences of SPCI with prior works. Method-wise, our prediction intervals are con- structed using conditional quantile regression functions on non-conformity scores (e.g., residuals). In contrast, exist- ing quantile-regression-based conformal prediction methods (Romano et al., 2019; Gupta et al., 2021) directly fit con- ditional quantile functions on the response variables Y , after which the intervals are constructed using empirical quantiles of non-conformity scores. Theory-wise, we obtain similar asymptotic conditional coverage for dependent resid- uals as in (Xu & Xie, 2021b). However, different from that work, we do not assume a particular functional form of the conditional distribution of the scalar output given feature variables. 2. Problem setup Assume a sequence of observations (Xt, Yt), t = 1, 2, . . ., where Yt are continuous scalar variables and Xt ∈ Rd <latexit sha1_base64=\"sQF1243TM/DmkWvD9z93WHB4yXY=\">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqBeh6MVjBfsBbSybzaZdusmG3Y1SQv6HFw+KePW/ePPfuGlz0NYHA4/3ZpiZ58WcKW3b31ZpZXVtfaO8Wdna3tndq+4fdJRIJKFtIriQPQ8ryllE25ppTnuxpDj0OO16k5vc7z5SqZiI7vU0pm6IRxELGMHaSA8DkZtUp36WXmXDas2u2zOgZeIUpAYFWsPq18AXJAlppAnHSvUdO9ZuiqVmhNOsMkgUjTGZ4BHtGxrhkCo3nV2doROj+CgQ0lSk0Uz9PZHiUKlp6JnOEOuxWvRy8T+vn+jg0k1ZFCeaRmS+KEg40gLlESCfSUo0nxqCiWTmVkTGWGKiTRYVE4Kz+PIy6ZzVnfN6465Ra14XcZThCI7hFBy4gCbcQgvaQEDCM7zCm/VkvVjv1se8tWQVM4fwB9bnD0IKkwQ=</latexit> d = <latexit sha1_base64=\"Iei3UNXkTtBB7xq4vNWgARz8K44=\">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0mkqMeiF48V7Ac0IWy223bpZjfsTgol9J948aCIV/+JN/+N2zYHbX0w8Hhvhpl5cSq4Ac/7dkobm1vbO+Xdyt7+weGRe3zSNirTlLWoEkp3Y2KY4JK1gINg3VQzksSCdeLx/dzvTJg2XMknmKYsTMhQ8gGnBKwUuW4wIpAHLDVcKDmL/MitejVvAbxO/IJUUYFm5H4FfUWzhEmgghjT870Uwpxo4FSwWSXIDEsJHZMh61kqScJMmC8un+ELq/TxQGlbEvBC/T2Rk8SYaRLbzoTAyKx6c/E/r5fB4DbMuUwzYJIuFw0ygUHheQy4zzWjIKaWEKq5vRXTEdGEgg2rYkPwV19eJ+2rmn9dqz/Wq427Io4yOkPn6BL56AY10ANqohaiaIKe0St6c3LnxXl3PpatJaeYOUV/4Hz+ANPOk8s=</latexit> ˆ✏1 <latexit sha1_base64=\"uVS/gU1858AmdKVjgM3qpRX2qRI=\">AAAB+XicbVBNS8NAEJ3Ur1q/oh69LBbBU0lKUY9FLx4r2FpoQthst+3SzW7Y3RRK6D/x4kERr/4Tb/4bt20O2vpg4PHeDDPz4pQzbTzv2yltbG5t75R3K3v7B4dH7vFJR8tMEdomkkvVjbGmnAnaNsxw2k0VxUnM6VM8vpv7TxOqNJPi0UxTGiZ4KNiAEWysFLluMMImD2iqGZdiFtUjt+rVvAXQOvELUoUCrcj9CvqSZAkVhnCsdc/3UhPmWBlGOJ1VgkzTFJMxHtKepQInVIf54vIZurBKHw2ksiUMWqi/J3KcaD1NYtuZYDPSq95c/M/rZWZwE+ZMpJmhgiwXDTKOjETzGFCfKUoMn1qCiWL2VkRGWGFibFgVG4K/+vI66dRr/lWt8dCoNm+LOMpwBudwCT5cQxPuoQVtIDCBZ3iFNyd3Xpx352PZWnKKmVP4A+fzB9VSk8w=</latexit> ˆ✏2 <latexit sha1_base64=\"7BOouPcXp5QjgsxQNEzCdNCPdn8=\">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0m0qMeiF48VrC00IWy203bpZjfsbgol9J948aCIV/+JN/+N2zYHbX0w8Hhvhpl5ccqZNp737ZTW1jc2t8rblZ3dvf0D9/DoSctMUWhRyaXqxEQDZwJahhkOnVQBSWIO7Xh0N/PbY1CaSfFoJimECRkI1meUGCtFrhsMickDSDXjUkyjy8itejVvDrxK/IJUUYFm5H4FPUmzBIShnGjd9b3UhDlRhlEO00qQaUgJHZEBdC0VJAEd5vPLp/jMKj3cl8qWMHiu/p7ISaL1JIltZ0LMUC97M/E/r5uZ/k2YM5FmBgRdLOpnHBuJZzHgHlNADZ9YQqhi9lZMh0QRamxYFRuCv/zyKnm6qPlXtfpDvdq4LeIooxN0is6Rj65RA92jJmohisboGb2iNyd3Xpx352PRWnKKmWP0B87nD9bWk80=</latexit> ˆ✏3 <latexit sha1_base64=\"qPS0aYdhjHlcyuZz8DXMwFzdmPE=\">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0UQhJJIUZdFNy4r2Ac0IUym03boZBJmboQagr/ixoUibv0Pd/6N0zYLbT1w4XDOvdx7T5gIrsFxvq2l5ZXVtfXSRnlza3tn197bb+k4VZQ1aSxi1QmJZoJL1gQOgnUSxUgUCtYORzcTv/3AlOaxvIdxwvyIDCTvc0rASIF96A0JZB5LNBexzIMMztw8sCtO1ZkCLxK3IBVUoBHYX14vpmnEJFBBtO66TgJ+RhRwKlhe9lLNEkJHZMC6hkoSMe1n0+tzfGKUHu7HypQEPFV/T2Qk0nochaYzIjDU895E/M/rptC/8jMukxSYpLNF/VRgiPEkCtzjilEQY0MIVdzciumQKELBBFY2IbjzLy+S1nnVvajW7mqV+nURRwkdoWN0ilx0ieroFjVQE1H0iJ7RK3qznqwX6936mLUuWcXMAfoD6/MH7k6Vig==</latexit> ˆ✏t+1 <latexit sha1_base64=\"f3BP94/1oXtjY6SXHg1/F94EtbQ=\">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCp5KIqMeiF48V7Ac0IWy2m3bpZhN2J2IJ+StePCji1T/izX/jts1BWx8MPN6bYWZemAquwXG+rcra+sbmVnW7trO7t39gH9a7OskUZR2aiET1Q6KZ4JJ1gINg/VQxEoeC9cLJ7czvPTKleSIfYJoyPyYjySNOCRgpsOvemEDusVRzkcgiyKEI7IbTdObAq8QtSQOVaAf2lzdMaBYzCVQQrQeuk4KfEwWcClbUvEyzlNAJGbGBoZLETPv5/PYCnxpliKNEmZKA5+rviZzEWk/j0HTGBMZ62ZuJ/3mDDKJrP+cyzYBJulgUZQJDgmdB4CFXjIKYGkKo4uZWTMdEEQomrpoJwV1+eZV0z5vuZfPi/qLRuinjqKJjdILOkIuuUAvdoTbqIIqe0DN6RW9WYb1Y79bHorVilTNH6A+szx8JoZUa</latexit> ˆ✏t <latexit sha1_base64=\"4mSRiAOC1HPbUsbyd7QN48TyFAA=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNnGqGW+xWMa6E1DDpVC8hQIl7ySa0yiQ/CEY3878hyeujYjVPU4S7kd0qEQoGEUrNbFfrrhVdw6ySrycVCBHo1/+6g1ilkZcIZPUmK7nJuhnVKNgkk9LvdTwhLIxHfKupYpG3PjZ/NApObPKgISxtqWQzNXfExmNjJlEge2MKI7MsjcT//O6KYbXfiZUkiJXbLEoTCXBmMy+JgOhOUM5sYQyLeythI2opgxtNiUbgrf88ippX1S9y2qtWavUb/I4inACp3AOHlxBHe6gAS1gwOEZXuHNeXRenHfnY9FacPKZY/gD5/MH4xeNAQ==</latexit> t Traditional CP Sequential CP <latexit sha1_base64=\"uxNVdUQGPwvrd9fdckeGZr4oy7U=\">AAACJXicbZDLSgMxFIYzXmu9jbp0EyxChVJmSlEXLopuXFawF+iUkknTNjSTDMkZoQx9GTe+ihsXFhFc+Sqml4W2/hD4+c45nJw/jAU34Hlfztr6xubWdmYnu7u3f3DoHh3XjUo0ZTWqhNLNkBgmuGQ14CBYM9aMRKFgjXB4N603npg2XMlHGMWsHZG+5D1OCVjUcW/ywYBAGrDYcKHkuOMX8BIpFQLRVWAKSxwuOm7OK3oz4VXjL0wOLVTtuJOgq2gSMQlUEGNavhdDOyUaOBVsnA0Sw2JCh6TPWtZKEjHTTmdXjvG5JV3cU9o+CXhGf0+kJDJmFIW2MyIwMMu1Kfyv1kqgd91OuYwTYJLOF/USgUHhaWS4yzWjIEbWEKq5/SumA6IJBRts1obgL5+8auqlon9ZLD+Uc5XbRRwZdIrOUB756ApV0D2qohqi6Bm9onc0cV6cN+fD+Zy3rjmLmRP0R873D/ATpiw=</latexit> (ˆ✏1, ˆ✏2,. .., ˆ✏t) <latexit sha1_base64=\"pUxFculemZhNZgMVO0J+YVsEcdA=\">AAACQ3icdVDLSgMxFM34rPVVdekmWIQWpMyUoi6LblxWsA/slJJJ0zaYSYbkjlCG+Tc3/oA7f8CNC0XcCqaPhbZ64MLhnHNJ7gkiwQ247rOztLyyurae2chubm3v7Ob29htGxZqyOlVC6VZADBNcsjpwEKwVaUbCQLBmcHc59pv3TBuu5A2MItYJyUDyPqcErNTN3Rb8IYHEZ5HhQsm0m/iGD0JS8IrpCf7HK1vPFz0F5uSfBBTTYjeXd0vuBHiReDOSRzPUurknv6doHDIJVBBj2p4bQSchGjgVLM36sWERoXdkwNqWShIy00kmHaT42Co93FfajgQ8UX9uJCQ0ZhQGNhkSGJp5byz+5bVj6J93Ei6jGJik04f6scCg8LhQ3OOaURAjSwjV3P4V0yHRhIKtPWtL8OZPXiSNcsk7LVWuK/nqxayODDpER6iAPHSGqugK1VAdUfSAXtAbencenVfnw/mcRpec2c4B+gXn6xuOnbJ6</latexit> (ˆ✏\u0000(1), ˆ✏\u0000(2),. .., ˆ✏\u0000(t)) Figure 2: Differences between traditional and sequential Conformal Prediction (CP) methods. In traditional CP, residuals are exchangeable, and the same set of residuals is used throughout the prediction. In contrast, sequential CP assumes an ordering of the potentially non-exchangeable residuals; residuals are available feedback to the prediction algorithms: past residuals are updated to include the new prediction residual ˆϵt+1 during prediction. denote features, which may either be the history of Yt or contain exogenous variables helpful in predicting the value of Yt. We can allow observations to be highly correlated under an unknown conditional distribution Yt|Xt, . . . , X1, and do not assume a particular functional form of the con- ditional distribution Yt|Xt, . . . , X1. Let the first T samples {(Xt, Yt)}T t=1 be the training data. Our goal is to construct prediction intervals sequentially starting from time T + 1 such that the prediction intervals will contain the true outcome with a pre-specified high prob- ability 1 − α while the prediction interval is as narrow as possible. Here the significance level α is user-specified. The prediction intervals ̂Ct−1(Xt), which depend on α, are around point predictions ̂Yt := ˆf (Xt) for a given predic- tive model ˆf . A commonly used conformity score is the prediction residual: ˆϵt = Yt − ̂Yt. We emphasize that our algorithm provides prediction inter- vals for an arbitrary user-chosen predictive algorithm. Here the subscript t−1 indicates the interval is constructed using previous up to t − 1 many observations. There are two types of coverage guarantees to be satisfied by ̂Ct−1(Xt). The first is the weaker marginal coverage: P(Yt ∈ ̂Ct−1(Xt)) ≥ 1 − α, ∀t, (1) while the second is the stronger conditional coverage: P(Yt ∈ ̂Ct−1(Xt)|Xt) ≥ 1 − α, ∀t. (2) If ̂Ct−1(Xt) satisfies (1) or (2), it is called marginally or conditionally valid, respectively. In terms of the interval width, to avoid vacuous prediction interval ̂Ct−1(Xt) (in the extreme case, if one chooses the entire real line for all t, it will always contain the true outcome Yt with high proba- bility), we should construct intervals with width | ̂Ct−1(Xt)| as narrow as possible. 3 Sequential Predictive Conformal Inference for Time Series 𝑋!, … , 𝑋\" 𝑌! %𝑌!: = (𝑓 ⋅ ; (𝐶!#\" ⋅ ̂𝜖! = 𝑌! − %𝑌! nature generates 𝑌!|𝑋!, … , 𝑋\" Figure 3: Unlike traditional CP methods, sequential CP methods leverage feedback (in red arrow) during prediction. In this work, we use prediction residual ˆϵt = Yt − ̂Yt as an example of the non-conformity score. A natural approach in developing sequential CP methods is constructing sequential prediction intervals using the most recent feedback in predicting Yt, as shown in Figure 3. How- ever, using the empirical distribution of updated residuals may not fully exploit the temporal dependence across the residuals. Indeed, when residuals are temporally correlated, the past residuals contain information about the distribution of future residuals and can be used to perform “predictive” conformal inference. More precisely, we should use the past residuals to predict the tail probability of the new residual, as doing so may allow certain adaptivity. The above is the main idea of our proposed SPCI algorithm. 3. Algorithms Below, we first consider a simple split conformal prediction as a vanilla baseline approach based on traditional CP, which constructs prediction intervals without considering feedback during prediction. Then, we present the EnbPI (Xu & Xie, 2021b) method in sequential CP as a refined approach and illustrate its limitation in using empirical quantile of past residuals. Finally, we introduce the proposed SPCI as an improved algorithm for sequential CP for time series data. 3.1. Vanilla split conformal One of the most commonly used conformal prediction meth- ods is split conformal (Papadopoulos et al., 2007), so we describe it as a prototypical example. First, split the indices of training data [T ] := {1, . . . , T } into two halves I1 and I2. Second, fit the prediction model ˆf on {(Xt, Yt), t ∈ I1} to make point predictions ̂Yt = ˆf (Xt), t ̸= I1. Third, com- pute non-conformity score on I2, where a typical choice is the residual. Lastly, let E[I2] = {ˆϵj}j∈I2 and define the prediction interval ̂Ct−1(Xt) for t > T as [ ˆf (Xt) + qα/2(E[I2]), ˆf (Xt) + q1−α/2(E[I2])], (3) where q1−α is the 1 − α quantile function over a set of val- ues. In particular, the set of non-conformity scores {ˆϵj}j∈I2 is fixed during prediction. When (Xt, Yt) are exchangeable (i.e., we can shuffle the order of these random variables with- out affecting the joint distribution), split conformal intervals in (3) reaches exact finite-sample marginal coverage defined in (3). However, without further distribution assumptions, split conformal intervals cannot reach valid conditional cov- erage in (2) (Foygel Barber et al., 2021). 3.2. EnbPI: Ensemble version using empirical residuals Compared to split conformal in the previous section, EnbPI involves no data-splitting, trains ensemble predictors that make more accurate point predictions and utilizes feed- back during prediction on test data. Thus, EnbPI is more suitable than split conformal for sequential prediction in- terval construction. EnbPI has the following three steps. First, it leverages training data as much as possible by fit- ting “leave-one-out” (LOO) ensemble prediction models ˆft(Xt) := ϕ({ ˆfb(Xt) : t /∈ Sb}), where ϕ denotes an arbi- trary aggregation function (e.g., mean, median, etc.) over a set of scalars, and Sb ⊂ [T ] is the bootstrap index set used to train the b-th bootstrap estimator ˆfb. The point predictor on test data is defined as ˆf (Xt) := ϕ({ ˆfb(Xt)}), which aggre- gates all bootstrap predictions. Second, we obtain residuals using the LOO models ˆϵt := Yt − ˆft(Xt). Third, it updates the past residuals during predictions so that the prediction intervals have adaptive width. For a fixed w ≥ 1, define E w t := {ˆϵt−1, . . . , ˆϵt−w}. Then, EnbPI intervals ̂Ct−1(Xt) have the form: [ ˆf (Xt) + qα/2(E T t ), ˆf (Xt) + q1−α/2(E T t )], (4) which utilize the past w = T residuals and greatly resemble traditional CP intervals in (3) due to the use of empirical quantile function q1−α/2 to compute interval width. However, EnbPI intervals in (4) can have limitations under dependent residuals. Note that dependent residuals lead to non-equivalence between conditional and marginal dis- tributions of ˆϵt, namely ˆϵt|E w t ̸= ˆϵt in distribution. More precisely, let F (z|E w t ) := P(ˆϵt ≤ z|E w t ) be the unknown conditional distribution function of the residual ˆϵt, where we implicitly assume the conditional distribution function is invariant over time (i.e., residuals have identical conditional distributions). Based on (4), P(Yt ∈ ̂Ct−1(Xt)|Xt) (5) = P(ˆϵt ∈ [qα/2(E T t ), q1−α/2(E T t )]|Xt) = F (q1−α/2(E T t )|E w t ) − F (qα/2(E T t )|E w t ). (6) However, the distribution function F evaluated at the em- pirical quantiles may not yield the desired coverage. More precisely, define Qt(p) := inf{e∗ ∈ R : F (e∗|E w t ) ≥ p}, (7) which is the p-th quantile of the residual ˆϵt. By definition, F (Qt(1 − α/2)|E w t ) − F (Qt(α/2)|E w t ) = 1 − α. (8) 4 Sequential Predictive Conformal Inference for Time Series Thus, in order for EnbPI intervals in (4) to have the desired 1 − α coverage asymptotically, the empirical quantile must uniformly converge to the actual quantile value, namely: sup p∈[0,1] |qp(E T t ) − Qt(p)| → 0 as T → ∞. (9) However, the condition (9) requires strong assumptions: (Xu & Xie, 2021b) assumes a particular linear functional form of Yt|Xt (i.e., Yt = f (Xt) + ϵt), which further needs to be consistently estimated as sample size approaches infinity. Such assumptions can impose limitations in practice. 3.3. Proposed SPCI algorithm Due to the limitations above by split conformal and EnbPI, we propose SPCI in Algorithm 1 as a more general frame- work than both approaches. In particular, SPCI directly leverages the dependency of ˆϵt on the past residuals when constructing the prediction intervals. Based on the equiva- lence in (6) and the coverage property in (8), SPCI replaces the empirical quantile with an estimate by a conditional quantile estimator. Specifically, let ̂Qt(p) be an estimator of the true quantile Qt(p) in (7) and let ˆf be a pre-trained point predictor, SPCI intervals ̂Ct−1(Xt) are defined as [ ˆf (Xt) + ̂Qt( ˆβ), ˆf (Xt) + ̂Qt(1 − α + ˆβ)], (10) where ˆβ minimizes interval width: ˆβ = arg minβ∈[0,α]( ̂Qt(1 − α + β) − ̂Qt(β)). (11) In particular, SPCI is more general than both EnbPI and split conformal. If we train LOO point predictors, choose the quantile estimator ̂Qt(·) as the empirical quantile, and use ˆβ = α/2, SPCI in (10) reduces to EnbPI in (4). If we follow split conformal prediction to train the point predictor ˆf , train quantile predictor ̂Qt on residuals from calibration set, and do no update residuals during prediction, SPCI intervals reduce to the split conformal intervals in (3). We particularly comment on the computational aspect of fitting conditional quantile estimators ̂Qt, the essential step of SPCI. To train ̂Qt, one minimizes the pinball loss L(x, α) = { αx if x ≥ 0, (α − 1)x if x < 0, (12) which depends on the significance level α. Because SPCI aims to produce intervals as narrow as possible and refits the quantile regression models at each t, it is important to choose quantile regression algorithms that are efficient enough in this sequential setting. In this work, we will use quantile random forest (QRF) (Meinshausen, 2006) to train ̂Qt and establish coverage guarantees. We train QRF auto-regressively in SPCI to leverage the dependency in residuals. In short, we use the past w ≥ 1 Algorithm 1 Sequential Predictive Conformal Inference (SPCI) Require: Training data {(Xt, Yt)}T t=1, prediction algo- rithm A, significance level α, quantile regression al- gorithm Q . Output: Prediction intervals ̂Ct−1(Xt), t > T 1: Obtain ˆf and prediction residuals ̂ϵ with A and {(Xt, Yt)} T t=1 2: for t > T do 3: Use quantile regression to obtain ̂Qt ← Q(̂ϵ) 4: Obtain prediction interval ̂Ct−1(Xt) as in (10) 5: Obtain new residual ˆϵt 6: Update residuals ̂ϵ by sliding one index forward (i.e., add ˆϵt and remove the oldest one) 7: end for residuals to predict the conditional quantile of the future (unobserved) residual. More precisely, suppose we have T past residuals E T t available at prediction index t. Let ˜T := T − w. For t ′ = 1, . . . , ˜T , define ˜Xt′ := [ˆϵt′+w−1, . . . , ˆϵt′], ˜Yt′ := ˆϵt′+w. (13) Thus, feature ˜Xt′ contains w residuals useful for predicting the conditional quantile of ˜Yt′ , which is the residual at index t′ + w. We use the feature ˜X ˜T +1 to predict the conditional quantile of ˜Y ˜T +1. As a result, the QRF is trained using ˜T training data ( ˜Xt′, ˜Yt′), t ′ = 1, . . . , ˜T . When re-fitting the QRF at each prediction index, we re-design these ˜T training data using a sliding window of most recent T residuals. In our experiments, we use the Python implementation of QRF by (Roebroek, 2022). Remark 1 (SPCI vs. Quantile regression). We further high- light the essential difference of SPCI against quantile re- gression approaches. In general, quantile regression al- gorithms rely on minimizing the pinball loss for specific regression algorithms. Doing so can often lead to inaccurate results and require special hyper-parameter tuning. In gen- eral, these algorithms can also be computationally expensive to train for multiple significance levels, as the pinball loss depends on α. In contrast, SPCI is compatible with any user-specified point prediction model, remains distribution- free, and provides coverage guarantees (see Theorem 2). Hence, SPCI inherits the main benefits of CP methods. In addition, SPCI leverages the dependency of non-conformity scores by fitting a QRF model of the quantiles (one can use general quantile regression models if desired). Computa- tionally, SPCI is also efficient in test time as the fitting of the QRF model does not rely on the significance level alpha. In practice, we find such a hybrid approach to outperform quantile regression models using deep neural networks (see Table 3). 5 Sequential Predictive Conformal Inference for Time Series 4. Theory We first show that when data are exchangeable, one can reach exact marginal coverage when using the empirical quantile function as the quantile regression predictor. We then establish asymptotic coverage upon considering the dependency of estimated residuals. For dependent residuals, we adapt the proof in (Meinshausen, 2006) for independent observations, where we replace the independence assump- tion with stationary and decaying dependence assumptions. Most proofs and additional theoretical details appear in Ap- pendix A. 4.1. Under exchangeability We show below that SPCI maintains marginal coverage when data are exchangeable. The proof is standard based on showing the marginal coverage of split conformal prediction Proposition 1 (Finite-sample marginal coverage under ex- changeability (Papadopoulos et al., 2007)). Suppose the data (Xt, Yt), t ≥ 1 are exchangeable (e.g., independent and identically distributed). Prediction intervals obtained via Algorithm 2 (i.e., a special version of SPCI) satisfy P(Yt ∈ ̂Ct−1(Xt)) ≥ 1 − α. 4.2. Beyond exchangeability The primary theoretical contribution of our work is to show the asymptotic conditional validity of SPCI intervals when the quantile random forest (Meinshausen, 2006) is used as the conditional quantile estimator. Specifically, we have P(Yt ∈ ̂Ct−1(Xt)|Xt) → 1 − α as T → ∞, which by (6) and (7), is equivalent to proving sup p∈[0,1] | ̂Qt(p) − Qt(p)| → 0 as T → ∞, (14) where ̂Qt(p) is the QRF estimator. More precisely, we want to estimate the conditional quantile values of ˜Y ˜T +1 given ˜X ˜T +1, both of which are defined in (13). Note that (14) for i.i.d. observations has been proven in (Meinshausen, 2006, Theorem 1), so that our analysis also extends the original statement therein to observations with dependency. We follow the notation in (Meinshausen, 2006) to intro- duce QRF. For the feature ˜Xt, t ≥ 1, assume its support Supp( ˜Xt) ⊂ B ⊂ Rp. We grow the tree T (θ) with param- eter θ as follows: every leaf l = 1, . . . , L of a tree T (θ) is associated with a rectangular subspace Rl ⊂ B. In par- ticular, they are disjoint and cover the entire space B: for every x ∈ B, there is one and only one leaf l, thus denoted as l(x, θ), such that x ∈ Rl(x,θ). If we grow K trees, let each of them have separate parameter θk. Now, for a given x ∈ B and ˜T observed features ˜X1, . . . , ˜X ˜T , we define the following weights: kθ(l) := #{j ∈ {1, . . . , ˜T } : ˜Xj ∈ Rl(x,θ)} (15) wt(x, θ) := 1 ( ˜Xt ∈ Rl(x,θ)) kθ(l) (16) wt(x) := K −1 K∑ k=1 wt(x, θk) (17) For interpretation, (15) counts the “node size” of the leaf l(x, θ), (16) weighs the i-th observation using whether ˜Xt belongs to this leaf and its node size, and (17) weighs such weights from K trees. Based on weights in (17), the estimated conditional distribution function ˆF (z|x)= ˆF (z| ˜X ˜T +1 = x) is defined as ˆF (z|x) := ˜T∑ t=1 wt(x)1 ( ˜Yt ≤ z). (18) In retrospect, the estimation in (18) is similar to that under fixed weights by (Barber et al., 2022). The key difference is that (18) uses data-adaptive weights as it exploits the temporal autocorrelation of residuals. In contrast, (Barber et al., 2022) uses fixed and non-adaptive weights. To show the convergence of the estimated QRF quantile to the true value, we first have the following lemma relating the convergence of quantile estimates to the convergence of corresponding distribution functions. Lemma 1. For random variable ˆϵt (i.e., residual in our setup), let F (z|x) be its conditional distribution function and Q(p) := inf{z ∈ R : F (z|x) ≥ p} be the p-th quantile, which is assumed to be unique. Let ˆF (z|x) be an estimator trained on ˜T samples {( ˜Xt, ˜Yt)} ˜T t=1. If for all z and x it holds that ˆF (z|x) → F (z|x) in probability as ˜T → ∞, (19) then ̂Q(p) := inf{z ∈ R : ˆF (z|x) ≥ p} satisfies ̂Q(p) → Q(p) in probability for every p ∈ (0, 1) and x. Thus, the crux of the remaining analyses relies on show- ing the point-wise convergence in (19) for the QRF in (18). The case where all data are independent and identically distributed has been addressed in (Meinshausen, 2006, The- orem 1). We address the more general case for dependent observations in Proposition 2. Proposition 2. If Assumptions 1—4 defined in Appendix A hold, we obtain the point-wise convergence in (19) for QRF. We briefly explain and discuss the necessary theoretical assumptions 1—4 used in proving Proposition 2: 6 Sequential Predictive Conformal Inference for Time Series • Assumption 1: This assumption states two things. First, the dependency of the covariance of the indicator ran- dom variables (defined over the residual quantiles) only depends on the difference in index (see Eq. (22)). Such assumption on residual dependency resembles the weak or wide-sense stationarity assumption. Second, the value of covariances can be uniformly bounded over the conditioning variables by a function ˜g, and there is a growth order constraint on ˜g (see Eq. (24)). This condition is imposed to avoid strong dependency among the residuals, which prevents asymptotic con- sistency of the QRF estimator. • Assumption 2: This assumption requires that the weights wt(x) in QRF decay linearly with respect to the number of training samples for QRF. In practice, we often found that the weights decay at such an order. • Assumption 3 and 4: These distributional assumptions on the conditional quantile function follow those in QRF (Meinshausen, 2006), and they are reasonably mild. In particular, we are not assuming a particular parametric of the conditional quantile function so the results are distribution-free. We finally obtain the asymptotic guarantee on interval cov- erage. Theorem 2 (Asymptotic conditional coverage beyond ex- changeability). Under the same assumptions as Lemma 1 and Proposition 2, as the sample size T → ∞, we have for any α ∈ (0, 1) |P(Yt ∈ ̂Ct−1(Xt)|Xt) − (1 − α)| p → 0 (20) 4.3. Implications of results We discuss several implications of the results: (1) how the results are distribution-free and model-free; (2) challenges in obtaining interval convergence; (3) the generality of proving guarantees for QRF; (4) convergence analyses beyond using QRF. Distribution-free & model-free guarantees. Note that our coverage guarantee makes no explicit distributional assump- tions of the residuals (e.g., density function has certain parametric form). Instead, our assumptions are on the de- pendency among the residuals and the regularity of the density functions of the residuals. On the other hand, our results also make no assumptions on the underlying data generation process of Yt given Xt, in contrast to the linear assumption Yt = f (Xt) + ϵt in EnbPI (Xu & Xie, 2021b). One can also use arbitrary predictive model to obtain the residuals, rather than relying on special deep neural network architectures (Salinas et al., 2020; Lim et al., 2021). Interval convergence. Ideally, we wish SPCI intervals in (10) to converge in width to the oracle interval defined by Yt|Xt. However, doing so requires assumptions on the inverse CDF of Yt|Xt, which deviate from our focus on model-free interval construction. Even though such theoret- ical analyses are lacking, experiments in Section 5 demon- strate that SPCI improves over recent sequential conformal prediction models in many cases. Generality of QRF. Note that decision trees are simple func- tions, thus satisfying the assumptions of the Simple Function Approximation Theorem (Royden & Fitzpatrick, 1988). In other words, the QRF estimates can theoretically approxi- mate those of any other quantile estimates. As a result, this can be useful if one analyzes the convergence of QRF quan- tile estimates for residuals with a more general dependency. Convergence beyond using QRF. The convergence of quan- tile estimates has been a long-standing question in statistics. In our case, we are particularly interested in the quantile estimates under time-series data. In the past, several lines of work have established such results for different estima- tors under various assumptions on dependency. (Cai, 2002) studied weighted Nadaraya-Watson quantile estimates for α- mixing sequences. (Biau & Patra, 2011) proposes a nearest- neighbor strategy for stationary and ergodic data. (Zhou & Wu, 2009) analyzed local linear quantile estimators for locally stationary time series. More analyses appear in the survey (Xiao, 2012). 5. Experiments We empirically demonstrate the improved performance of SPCI over competing sequential CP methods and proba- bilistic forecasting methods in terms of interval coverage and width. The CP baselines are EnbPI (Xu & Xie, 2021b), AdaptiveCI (Gibbs & Candes, 2021), and NEX-CP (Bar- ber et al., 2022), whose details are in Appendix B. The two probabilistic forecasting methods are DeepAR (Salinas et al., 2020) and TFT (Lim et al., 2021). In all experiments, we obtain LOO point predictors ˆf and prediction residuals ̂ϵ as in EnbPI. Official implementation can be found at https: //github.com/hamrel-cxu/SPCI-code. 5.1. Simulation We first compare SPCI with EnbPI on non-stationary and/or heteroskedastic time-series. We then compare SPCI with NEX-CP on data with distribution drifts and change- points under the setting described in (Barber et al., 2022). Details on data simulation are in Appendix B.1. (1) Comparison with EnbPI. Given a feature Xt, we spec- ify the true data-generating process as Yt = f (Xt) + ϵt. We 7 Sequential Predictive Conformal Inference for Time Series Table 1: Simulation: EnbPI vs. SPCI on simulated time- series with α = 0.1. SPCI outperforms EnbPI in terms of interval width without sacrificing valid coverage. Nstat coverage Nstat width Hetero coverage Hetero width SPCI 0.94 (2.04e-3) 11.23 (3.37e-2) 0.89 (9.43e-3) 24.09 (8.27e-1) EnbPI 0.91 (1.11e-3) 25.22 (2.84e-2) 0.92 (1.18e-2) 25.84 (3.47e-1) simulate two types of time-series data. The first considers non-stationary (Nstat) time-series. The second considers heteroskedastic (Hetero) time-series in which the variance of ϵt depends on Xt. Table 1 compares EnbPI with SPCI, where both use the random forest regression model to fit the point estimator ˆf . We see clear improvement of SPCI. We suspect the im- provement lies in the more adaptive and accurate calibration of quantile values of residual distributions in prediction. (2) Comparison with NEX-CP. We consider data with dis- tribution drift and changepoints, where data are simulated according to examples in (Barber et al., 2022). Table 2 shows competitive results of both methods. We notice slight under-coverage by SPCI under both settings, despite the much narrower intervals by SPCI. When we slightly lower the significance level α, which is held con- stant when constructing all intervals, SPCI maintains valid coverage with comparable interval widths as NEX-CP. Fig- ure A.1 visualizes rolling coverage and width after a burn-in period, with a rolling window of 50 samples. The results are similar to the best model in (Barber et al., 2022, Figure 2). In Appendix B.1, we further explain why SPCI tends to under-cover in these settings before α adjustment. 5.2. Real-data examples We primarily consider three real time-series in this sec- tion, whose details are in Appendix B. We first compare the marginal coverage and width of SPCI against baseline methods. We then examine the rolling coverage and width of each method to assess their stability during prediction. Table 2: Simulation: NEX-CP vs. SPCI on simulated time- series with 90% target coverage. Entries in the bracket indicate standard deviation over ten trials where data are re-generated. The symbol * denotes results from (Barber et al., 2022, Table 1). Results from the second row are based on α = 0.09 (dist. shift) and α = 0.075 (change-point). Drift coverage Drift width Change coverage Change width SPCI 0.89 (5.04e-3) 3.33 (4.17e-2) 0.87 (2.75e-3) 3.85 (4.12e-2) SPCI, adjusted α 0.90 (4.63e-3) 3.43 (4.43e-2) 0.90 (3.71e-3) 4.18 (4.89e-2) NEX-CP* 0.91 3.45 0.91 4.13 We lastly apply SPCI on a more challenging multi-step ahead inference case to illustrates its usefulness. We fix α = 0.1 and use the first 80% (resp. rest 20%) data for training (resp. testing). For SPCI and EnbPI, we use the random forest regression model with 25 bootstrap models. (1) Marginal coverage and width. Table 3 shows the marginal coverage and width of all methods on the three time series. While all methods nearly maintain validity at α = 0.1, SPCI yields significantly narrower intervals, es- pecially on the wind speed prediction data. Such results illustrate the advantages of fitting conditional quantile re- gression on residuals for width calibration and training LOO regression predictors for point prediction. In the appendix, Table A.2 further compares SPCI against AdaptiveCI on stock market return data, which are similar to ones used in (Gibbs & Candes, 2021). We show that SPCI always maintains valid 1 − α coverage and yields narrower intervals than Adaptive CI. (2) Rolling coverage and width. Besides the marginal metric, we provide further insights into the dynamics of prediction intervals. Figure 4 visualizes the rolling coverage and width of each method, where the metric is computed over a rolling window of size 100 (resp. 50) for the solar and electricity (resp. wind) datasets. The results first show that SPCI barely loses rolling coverage when competing methods (e.g., EnbPI) can fail to do so. Secondly, SPCI intervals are adaptive: they are wider or narrower depending on the data index, which likely reflects higher or less uncertainty in test 660 680 700 720 740 760 Data index 0.6 0.7 0.8 0.9 1.0Rolling coverage SPCI EnbPI AdaptiveCI NEXCP 660 680 700 720 740 760 Data index 4 6 8Rolling width (a) Wind 1700 1750 1800 1850 1900 1950 2000 Data index 0.6 0.7 0.8 0.9 1.0Rolling coverage SPCI EnbPI AdaptiveCI NEXCP 1700 1750 1800 1850 1900 1950 2000 Data index 50 75 100 125Rolling width(b) Solar 2900 3000 3100 3200 3300 3400 Data index 0.6 0.7 0.8 0.9 1.0Rolling coverage SPCI EnbPI AdaptiveCI NEXCP 2900 3000 3100 3200 3300 3400 Data index 0.2 0.4Rolling width (c) Electric Figure 4: Rolling coverage and interval width over three real time series by different methods. SPCI in black not only yields valid rolling coverage but also consistently yields the narrowest prediction intervals. Furthermore, the variance of SPCI results over trials is also small, as shown by the shaded regions over coverage and width results. 8 Sequential Predictive Conformal Inference for Time Series Table 3: Marginal coverage and width by all methods on three real time series. The target coverage is 0.9, and entries in the bracket indicate standard deviation over three independent trials. SPCI outperforms competitors with a much narrower interval width and does not lose coverage. Wind coverage Wind width Electric coverage Electric width Solar coverage Solar width SPCI 0.95 (1.50e-2) 2.65 (1.60e-2) 0.93 (4.79e-3) 0.22 (1.68e-3) 0.91 (1.12e-2) 47.61 (1.33e+0) EnbPI 0.93 (6.20e-3) 6.38 (3.01e-2) 0.91 (6.84e-4) 0.32 (9.11e-4) 0.88 (4.25e-3) 48.95 (3.38e+0) AdaptiveCI 0.95 (5.37e-3) 9.34 (3.56e-2) 0.95 (1.81e-3) 0.51 (7.25e-3) 0.96 (1.39e-2) 56.34 (1.15e+0) NEX-CP 0.96 (8.21e-3) 6.68 (7.73e-2) 0.90 (2.05e-3) 0.45 (2.16e-3) 0.90 (7.73e-3) 102.80 (5.25e+0) DeepAR 0.95 (5.32e-3) 6.86 (7.86e-3) 0.91 (3.45e-3) 0.62 (2.56e0-3) 0.92 (5.35e-3) 80.23 (4.94e+0) TFT 0.92 (6.34e-2) 7.56 (5.34e-3) 0.95 (2.34e-2) 0.66 (2.34e-3) 0.93 (2.84e-3) 74.82 (4.23e+0) 625 650 675 700 725 750 Prediction Time Index 2.5 5.0 7.5 EnbPI C (Xt) around Y, coverage 0.93, width 6.42 (a) EnbPI, 1 step ahead 625 650 675 700 725 750 Prediction Time Index 2.5 5.0 7.5 EnbPI C (Xt) around Y, coverage 0.94, width 6.32 (b) EnbPI, 4 steps ahead 625 650 675 700 725 750 Prediction Time Index 2.5 5.0 7.5 SPCI C (Xt) around Y, coverage 0.95, width 2.65 (c) SPCI, 1 step ahead 625 650 675 700 725 750 Prediction Time Index 5 10 SPCI C (Xt) around Y, coverage 0.92, width 3.04 (d) SPCI, 2 step ahead 625 650 675 700 725 750 Prediction Time Index 5 10 SPCI C (Xt) around Y, coverage 0.95, width 3.50 (e) SPCI, 3 step ahead 625 650 675 700 725 750 Prediction Time Index 5 10 SPCI C (Xt) around Y, coverage 0.93, width 3.89 (f) SPCI, 4 step ahead Figure 5: Multi-step ahead prediction interval construction by SPCI and EnbPI on wind speed data. Compared to EnbPI results in subfigures (a) and (b), SPCI intervals are much narrower and more adaptive—SPCI intervals follow the trajectory of the time-series whereas EnbPI ones are overly conservative. In addition, SPCI interval increase in width as the predictive horizon increases, reflecting the existence of more uncertainty in long horizons. data. Thirdly, SPCI intervals are evidently narrower than those by competing methods. Lastly, SPCI rolling results have less variance than others such as NEX-CP. (3) Multi-step predictive inference. In practice, it is often desirable and important to construct S > 1 prediction in- tervals at once. This is a challenging problem for SPCI since it involves estimating the conditional joint distribution of S residuals ahead. We thus modify SPCI to tackle this problem through a “divide-and-conquer“ approach. Specif- ically, we apply SPCI S times on lagged training data (Xt, Yt+s), s = 0, . . . , S − 1, so that we obtain S fitted QRF estimators to compute the S prediction intervals simul- taneously. Additional details including the motivation and algorithm appear in Appendix B.3. Figure 5 compares SPCI with EnbPI on the wind dataset in terms of multi-step ahead coverage and width. We compare with EnbPI because it supports multi-step ahead predic- tion in the algorithm, although each batch of S−step ahead intervals have the same width by construction. We first note that EnbPI intervals are too wide and non-adaptive, as 4-step ahead intervals may even be narrower than 1-step ahead ones. In contrast, SPCI intervals closely follow the trajectory of actual data and are more adaptive: S−step ahead intervals with larger S yield wider intervals on av- erage. This increase in width is expected because there are greater uncertainty when predicting more prediction intervals simultaneously. 6. Conclusions In this work, we propose SPCI, a general framework for constructing prediction intervals for time series. Similar to existing conformal prediction methods, SPCI is model-free and distribution-free, making it applicable to any time se- ries with arbitrary predictive models. Unlike existing CP methods, SPCI fits quantile regression models on residuals to utilize temporal dependency among residuals to achieve more adaptive confidence intervals and better coverage. The- oretical analyses verify the asymptotic valid conditional coverage by SPCI. Experimental results consistently show improved performance by SPCI over existing sequential CP methods. In the future, we aim to extend SPCI for constructing con- fidence regions for multi-variate time-series, by further ex- ploiting the dependency among individual uni-variate time- series and designing non-conformity scores that enable effi- cient interval construction. How to develop the multi-step SPCI in Algorithm 3 to more precisely capture the joint distribution of future residuals is also a promising direction. Acknowledgement This work is partially supported by an NSF CAREER CCF-1650913, and NSF DMS-2134037, CMMI-2015787, CMMI-2112533, DMS-1938106, and DMS-1830210. 9 Sequential Predictive Conformal Inference for Time Series References Angelopoulos, A. N. and Bates, S. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021. Angelopoulos, A. N., Bates, S., Jordan, M., and Malik, J. Uncertainty sets for image classifiers using conformal prediction. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=eNdiU_DbM9. Barber, R. F., Candes, E. J., Ramdas, A., and Tibshirani, R. J. Conformal prediction beyond exchangeability. arXiv preprint arXiv:2202.13415, 2022. Biau, G. and Patra, B. Sequential quantile prediction of time series. IEEE Transactions on Information Theory, 57(3): 1664–1674, 2011. Breiman, L. Random forests. Machine learning, 45(1): 5–32, 2001. Brockwell, P. J., Davis, R. A., and Fienberg, S. E. Time series: theory and methods: theory and methods. Springer Science & Business Media, 1991. Cai, Z. Regression quantiles for time series. Econometric theory, 18(1):169–192, 2002. Cochran, J., Denholm, P., Speer, B., and Miller, M. Grid integration and the carrying capacity of the us grid to incorporate variable renewable energy. Technical report, National Renewable Energy Lab.(NREL), Golden, CO (United States), 2015. Cody, W. J. and Thacher, H. C. Chebyshev approximations for the exponential integral. Mathematics of Computation, 23:289–303, 1969. D´ıaz-Gonz´alez, F., Sumper, A., Gomis-Bellmunt, O., and Villaf´afila-Robles, R. A review of energy storage tech- nologies for wind power applications. Renewable and sustainable energy reviews, 16(4):2154–2171, 2012. Engle, R. F. Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation. Econometrica, 50:987–1007, 1982. Feldman, S., Bates, S., and Romano, Y. Conformalized online learning: Online calibration without a holdout set. arXiv preprint arXiv:2205.09095, 2022. Fontana, M., Zeni, G., and Vantini, S. Conformal prediction: a unified review of theory and new challenges. Bernoulli, 29(1):1–23, 2023. Foygel Barber, R., Candes, E. J., Ramdas, A., and Tibshirani, R. J. The limits of distribution-free conditional predictive inference. Information and Inference: A Journal of the IMA, 10(2):455–482, 2021. Gibbs, I. and Candes, E. Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems, 34:1660–1672, 2021. Gupta, C., Kuchibhotla, A. K., and Ramdas, A. Nested conformal prediction and quantile out-of-bag ensemble methods. Pattern Recognition, pp. 108496, 2021. Harries, M., of New South Wales. School of Computer Sci- ence, U., and Engineering. Splice-2 Comparative Evalua- tion: Electricity Pricing. PANDORA electronic collec- tion. University of New South Wales, School of Computer Science and Engineering, 1999. Lathuili`ere, S., Mesejo, P., Alameda-Pineda, X., and Horaud, R. A comprehensive analysis of deep regression. IEEE transactions on pattern analysis and machine intelligence, 2019. Lim, B., Arık, S. ¨O., Loeff, N., and Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4):1748–1764, 2021. Lin, Z., Trivedi, S., and Sun, J. Conformal prediction with temporal quantile adjustments. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=PM5gVmG2Jj. Meinshausen, N. Quantile regression forests. J. Mach. Learn. Res., 7:983–999, 2006. Papadopoulos, H., Vovk, V., and Gammerman, A. Con- formal prediction with neural networks. In 19th IEEE International Conference on Tools with Artificial Intelli- gence(ICTAI 2007), volume 2, pp. 388–395, 2007. Ridler-Rowe, C. J. A graduate course in probability. Journal of the Royal Statistical Society. Series A (General), 131 (2):230–231, 1968. ISSN 00359238. URL http:// www.jstor.org/stable/2343845. Roebroek, J. Sklearn-quantile, 2022. URL https://github.com/jasperroebroek/ sklearn-quantile. (visited on 2023-01-11). Romano, Y., Patterson, E., and Candes, E. Conformalized quantile regression. In Advances in Neural Information Processing Systems, pp. 3543–3553, 2019. Romano, Y., Sesia, M., and Candes, E. Classification with valid and adaptive coverage. Advances in Neural Infor- mation Processing Systems, 33:3581–3591, 2020. 10 Sequential Predictive Conformal Inference for Time Series Royden, H. L. and Fitzpatrick, P. Real analysis, volume 32. Macmillan New York, 1988. Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–1191, 2020. Shafer, G. and Vovk, V. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(Mar):371–421, 2008. Van der Vaart, A. W. Asymptotic statistics, volume 3. Cam- bridge university press, 2000. Wen, R., Torkkola, K., Narayanaswamy, B. M., and Madeka, D. A multi-horizon quantile recurrent forecaster. In NeurIPS 2017, 2017. URL https: //www.amazon.science/publications/ a-multi-horizon-quantile-recurrent-forecaster. Xiao, Z. Time series quantile regressions. In Handbook of statistics, volume 30, pp. 213–257. Elsevier, 2012. Xu, C. and Xie, Y. Conformal anomaly detection on spatio- temporal observations with missing data. In ICML 2021 Workshop on Distribution-free Uncertainty Quantifica- tion, 2021a. Xu, C. and Xie, Y. Conformal prediction interval for dy- namic time-series. In International Conference on Ma- chine Learning, pp. 11559–11569. PMLR, 2021b. Zaffran, M., Dieuleveut, A., F’eron, O., Goude, Y., and Josse, J. Adaptive conformal predictions for time series. In ICML, 2022. Zhou, Z. and Wu, W. B. Local linear quantile estimation for nonstationary time series. The Annals of Statistics, 37 (5B):2696–2729, 2009. Zhu, S., Zhang, H., Xie, Y., and Van Hentenryck, P. Multi- resolution spatio-temporal prediction with application to wind power generation. In 2022 INFORMS Workshop on Data Science, 2021. 11 Sequential Predictive Conformal Inference for Time Series A. Proof Proof of Proposition 1. The proof is standard in conformal prediction literature based on an exchangeability argument. By (3), we know that P(Yt ∈ ̂Ct−1(Xt)) = P(ˆϵt ∈ [qα/2({E[I2]), q1−α/2(E[I2])]). By exchangeability of the original data and the fact that ˆf is trained on (Xt, Yt), t ∈ I1, we have E[I2] = {ˆϵj}j∈I2 and ˆϵt are exchangeable. For p ∈ [0, 1], let qp := qα/2({ˆϵj}j∈I2 ). Thus, by exchangeability, we have P(ˆϵt ∈ [qα/2, q1−α/2]) = 1 |I2| ∑ j∈I2 P(ˆϵj ∈ [qα/2, q1−α/2]) = 1 |I2| E   ∑ j∈I2 1 (ˆϵj ∈ [qα/2, q1−α/2])   = 1 − α, where the last equality holds by the definition of the interval [qα/2, q1−α/2]. Proof of Lemma 1. First, by (Ridler-Rowe, 1968, Theorem 1, p.127-128), we know that (19) implies sup z∈R | ˆF (z|x) − F (z|x)| → 0 in probability. (21) Recall that Q(p) is unique. Thus, for any x, there exists ϵ = ϵ(x) > 0 such that δ = δ(ϵ) := min{p − F (Q(p) − ϵ|x), F (Q(p) + ϵ|x) − p} > 0. Namely, there exists a small pertubation of Q(p) whereby the change in the value of the distribution function is at least positive. Thus, we have that P(| ̂Q(p) − Q(p)| > ϵ) (i) = P(|F ( ̂Q(p)|x) − p| > δ) = P(|F ( ̂Q(p)|x) − ˆF ( ̂Q(p)|x)| > δ) ≤ P(sup z∈R |F (z|x) − ˆF (z|x)| > δ). Note that (i) holds because the event | ̂Q(p) − Q(p)| > ϵ means that ̂Q(p) is at least ϵ far away from Q(p). By monotonicity of the distribution function F , this event implies the occurrence of the event |F ( ̂Q(p)|x) − p| > δ. Now, (21) implies the convergence of estimated quantile values, hence finishing the proof. To prove Proposition 2, we need several assumptions followed by interpretation and examples. Assumption 1. Define Ut := F ( ˜Yt|X = ˜Xt) as the quantile of observations ˜Yt conditioning on the observed feature ˜Xt, where Ut ∼ Unif[0, 1]. For a x ∈ B := Supp({ ˜Xt}t≥1), define the scalar z[x] := F (z|X = x). Given g(i, j, x1, x2) := Cov(1 (Ui ≤ z[x1]), 1 (Uj ≤ z[x2])), we require that for any pair of x1, x2 ∈ B, g(i, j, x1, x2) = g(|i − j|, x1, x2) for i ̸= j. (22) In addition, there exists ˜g such that g(k, x1, x2) ≤ ˜g(k) ∀x1, x2 ∈ B, k ≥ 1 (23) lim ˜T →∞ [∫ ˜T 1 ∫ x 1 ˜g(u)dudx ] / ˜T 2 → 0. (24) 12 Sequential Predictive Conformal Inference for Time Series In other words, (22) assumes that the covariance of the indicator random variables only depends on the difference in index, where this assumption appears widely in the weak or wide-sense stationary processes. The difference is that we do not require constant mean values of the indicator variables. In fact, constant mean is impossible, as E[1 (Ut ≤ z[x])] = z[x], whose value changes depending on the conditioning value x. Meanwhile, there is a function ˜g(k) in (23) bounding the covariance uniformly over pairs of values x1, x2, and (24) further assumes a restriction on the order of growth of the function ˜g(k). Below are examples of ˜g(k) for which (24) holds and we can also characterize the decay rate of (24). Example 1 (Finite memory). For some cutoff index s ∈ Z and constants {c1, . . . , cs}, ˜g(k) = { ck k ≤ s 0 k > s Showing ˜g(k) in Example 1 satisfies (24) is trivial, with decay rate O(1/ ˜T 2). This example appears in stochastic processes with finite memory. Example 2 (Linear decay). For every k ≥ 1, ˜g(k) = 1 kp , p ≥ 1. Example 2 is weaker than Example 1. To characterize the decay rate, we see that ∫ ˜T 1 ∫ x 1 ˜g(u)dudx ≤ ∫ ˜T 1 ∫ x 1 1/ududx = ∫ ˜T 1 log(x)dx = ˜T (log ˜T − 1). Thus, ˜T −2 ∫ ˜T 1 ∫ x 1 ˜g(u)dudx ≤ ˜T (log ˜T −1) ˜T 2 = O(log( ˜T )/ ˜T ). Hence, (24) is proven for Example 2. Example 3 (Logarithmic decay). For every k ≥ 1, ˜g(k) = [ 1 log(k+1) ]p , p ≥ 1. Example 3 is weaker than the above two examples as it imposes a weaker decay order on the covariance. Lemma 2 presents the proof of (24) for this example, which decays at the order of O( 1 2 log ˜T ). In general, we wish to show (24) in this example when p ∈ (0, 1). However, doing so is difficult as the analysis of the integral ∫ ˜T 1 ∫ x 1 [ 1 log(u+1) ] pdudx is complicated. Furthermore, note that log(u + 1)p → 1 as p → 0, so this integral tends to ˜T 2/2, whereby (24) cannot be obtained for small enough p. Lemma 2. For p ≥ 1, we have lim ˜T →∞ [∫ ˜T 1 ∫ x 2 1 log(u)p dudx ] / ˜T 2 = O ( 1 2 log ˜T ) . Proof of Lemma 2. First, consider the case where p = 1. Define li(x) as the anti-derivative of 1/ log(x). To find the growth order of li(x), we note that li(x) = Ei(log x), where Ei(x) standards for the exponential integral with the form Ei(x) = ∫ x −∞ et t dt. This can be shown via the change of variable log(u) = t. Note that we have the following asymptotic expansion for Ei(x) (Cody & Thacher, 1969): Ei(x) = exp(x) x (1 + 1 x + 2 x2 + 6 x3 + . . .) = exp(x) x (1 + O(1/x)) when x > 1. Thus, Ei(log x) = x log x (1 + O(1/ log x)) ≈ x log x for large x. As a result, dropping the constants and small order terms yield ∫ ˜T 1 ∫ x 2 1 log(u) dudx = ∫ ˜T 1 Ei(log x)dx = ∫ ˜T 1 x log x dx = Ei(2 log ˜T ) 13 Sequential Predictive Conformal Inference for Time Series Hence, we have lim ˜T →∞ [∫ ˜T 1 ∫ x 2 1 log(u) dudx ] / ˜T 2 = lim ˜T →∞ Ei(2 log ˜T )/ ˜T 2 = O( 1 2 log ˜T ). Lastly, when p > 1, 1 log u > [ 1 log u ] p uniformly for all u > 1. Hence, we have lim ˜T →∞ [∫ ˜T 1 ∫ x 2 1 log(u)p dudx ] / ˜T 2 < lim ˜T →∞ [∫ ˜T 1 ∫ x 2 1 log(u) dudx ] / ˜T 2, where the latter limit decays at order O( 1 2 log ˜T ) as shown above. Assumption 2. The weights wt(x) in (17) satisfies that for all x ∈ B, wt(x) = O(1/ ˜T ). Assumption 2 imposes the condition on the decay order of each weights. Note that by the definition of wt(x) in (17) and (Meinshausen, 2006, Assumption 2), we know that wt(x) = o(1). Assumption 2 thus assumes an exact order of decay of the weights. Assumption 3. The true conditional distribution function is Lipschitz continuous with parameter L. That is, for all x, x ′ in the support of the random variable X. sup z |F (z|X = x) − F (z|X = x′)| ≤ L∥x − x′∥1. Assumption 4. For every x in the support of X, the conditional distribution function F (z|X = x) is continuous and strictly monotonically increasing in z. We remark that Assumption 3 and 4 are identical to (Meinshausen, 2006, Assumption 4 and 5), respectively. Proof of Proposition 2. The proof is motivated by the analyses in (Meinshausen, 2006), which assumes ( ˜Yt, ˜Xt), t ≥ 1 are independent and identically distributed. In essence, we analyze the point-wise difference between the estimate ˆF (z|x) in (18) and the true value F (z|x). The difference can then be broken into two terms. Both terms can be bounded by Chebyshev inequalities, leading to convergence to zero. For each observation t = 1, . . . , ˜T , denote Ut := F ( ˜Yt|X = ˜Xt) as the quantile of the t-th empirical residual ˜Yt. Note that Ut ∼ Unif[0, 1] by the property of the distribution function, which is continuous by Assumption 4. By the form of the estimator ˆF (z|x) in (18), we break it into two parts: ˆF (z|x) = ˜T∑ t=1 wt(x)1 ( ˜Yt ≤ z) (i) = ˜T∑ t=1 wt(x)1 (Ut ≤ F (z| ˜Xt)) = ˜T∑ t=1 wt(x)1 (Ut ≤ F (z|x)) + ˜T∑ t=1 wt(x)(1 (Ut ≤ F (z| ˜Xt)) − 1 (Ut ≤ F (z|x))). The equivalence (i) holds because the event { ˜Yt ≤ z} is identical to the event {Ut ≤ F (z|X = ˜Xt)} under Assumption 4. 14 Sequential Predictive Conformal Inference for Time Series Thus, we have that | ˆF (z|x) − F (z|x)| ≤ ∣ ∣ ∣ ∣ ∣ ∣ ˜T∑ t=1 wt(x)1 (Ut ≤ F (z|x)) − F (z|x) ∣ ∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ (a) + ∣ ∣ ∣ ∣ ∣ ∣ ˜T∑ t=1 wt(x)(1 (Ut ≤ F (z| ˜Xt)) − 1 (Ut ≤ F (z|x))) ∣ ∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ (b) . 1) Bound of term (a). The first term can be bounded using Chebyshev inequality. Let z′ := F (z|x). Define U ′ := ∑ ˜T t=1 wt(x)1 (Ut ≤ z′). By the linearity of expectation taken over Ut, we have E[U ′] = ˜T∑ t=1 wt(x)E[1 (Ut ≤ z′)] =   ˜T∑ t=1 wt(x)   z′ (i) = z′, where (i) holds under the definition of wt(x) in (17), which satisfies ∑ ˜T t=1 wt(x) = 1 as remarked earlier. Now, for any ϵ > 0, P   ∣ ∣ ∣ ∣ ∣ ∣ ˜T∑ t=1 wt(x)1 (Ut ≤ F (z|x)) − F (z|x) ∣ ∣ ∣ ∣ ∣ ∣ ≥ ϵ   =P(|U ′ − z′| ≥ ϵ) ≤ Var(U ′)/ϵ 2. Note that Var(U ′) =Var( ˜T∑ t=1 wt(x)1 (Ut ≤ z′)) = ˜T∑ t=1 wt(x) 2Var(1 (Ut ≤ z′)) ︸ ︷︷ ︸ (i) + ∑ i̸=j wi(x)wj(x)Cov(1 (Ui ≤ z′), 1 (Uj ≤ z′)) ︸ ︷︷ ︸ (ii) . (25) We need to show that (i) and (ii) in (25) both converge to zero. To show the convergence of (i), we have wt(x) = O(1/ ˜T ) by Assumption 2 and note that Var(1 (Ut ≤ z′)) = E(1 (Ut ≤ z′) 2)−E(1 (Ut ≤ z′))2 = z′−z′2. Hence, Var(1 (Ut ≤ z′)) < 1 and we have ∑ ˜T t=1 wt(x)2Var(1 (Ut ≤ z′)) < ∑ ˜T t=1 wt(x) 2 = O(1/ ˜T ). To show the convergence of (ii), we have by Assumption 1 that ∑ i̸=j wi(x)wj(x)Cov(1 (Ui ≤ z′), 1 (Uj ≤ z′)) ≤ ˜T −1∑ k=1 O ( ˜T − k ˜T 2 ) ˜g(k) ≤ ∫ ˜T 1 O ( ˜T − k ˜T 2 ) ˜g(k)dk =O ( ˜T −1) ∫ ˜T 1 ˜g(k)dk − O ( ˜T −2) ∫ ˜T 1 k˜g(k)dk =O (T −1) [G( ˜T ) − G(1)] − O ( ˜T −2) ∫ ˜T 1 k˜g(k)dk, 15 Sequential Predictive Conformal Inference for Time Series where G(x) := ∫ x 1 ˜g(k)dk is the anti-derivative. Using integration by part with u = k, dv = ˜g(k)dk, we have ∫ ˜T 1 k˜g(k)dk = ˜T G( ˜T ) − G(1) − ∫ ˜T 1 G(x)dx. Thus, dropping constants and small order terms yield ∑ i̸=j wi(x)wj(x)Cov(1 (Ui ≤ z′), 1 (Uj ≤ z′)) ≤ [∫ ˜T 1 [∫ x 1 ˜g(k)dk] dx ] / ˜T 2. By (24) in Assumption 1, we thus have the desired convergence result. 2) Bound of term (b). Define W := ∑ ˜T t=1 wt(x)1 (Ut ≤ F (z| ˜Xt)). Note that E(W ) = ∑ ˜T t=1 wt(x)F (z| ˜Xt). We have for any ϵ > 0, P(|W − E(W )| > ϵ) ≤ Var(W )/ϵ 2 = (ϵ) −2   ˜T∑ t=1 wt(x)2Var(1 (Ut ≤ F (z| ˜Xt))) + ∑ i̸=j wi(x)wj(x)Cov(1 (Ui ≤ F (z| ˜Xi)), 1 (Uj ≤ F (z| ˜Xj)))   . By the same argument for bounding term (a) above, we have that W p → E[W ] as sample size ˜T → ∞. As a result, we have ∣ ∣ ∣ ∣ ∣ ∣ ˜T∑ t=1 wt(x)(1 (Ut ≤ F (z| ˜Xt)) − 1 (Ut ≤ F (z|x))) ∣ ∣ ∣ ∣ ∣ ∣ p → ∣ ∣ ∣ ∣ ∣ ∣ ˜T∑ t=1 wt(x)(F (z| ˜Xt) − F (z|x)) ∣ ∣ ∣ ∣ ∣ ∣ . By Assumption 3, we have ∣ ∣ ∣ ∣ ∣ ∣ ˜T∑ t=1 wt(x)(F (z| ˜Xt) − F (z|x)) ∣ ∣ ∣ ∣ ∣ ∣ ≤ ˜T∑ t=1 wt(x)L∥ ˜Xt − x∥1. The rest of proof follows due to (Meinshausen, 2006, Lemma 2), which shows that ˜T∑ t=1 wt(x)∥ ˜Xt − x∥1 = op(1). Proof of Theorem 2. Under SPCI interval construction in (10), the equivalence in (6) implies that P(Yt ∈ ̂Ct−1(Xt)|Xt) = F ( ̂Qt(1 − α + ˆβ)|E w t ) − F ( ̂Qt( ˆβ)|E w t ), where ̂Qt(p), p ∈ [0, 1] is the estimated p-th quantile of ˆϵt, F (z|E w t ) is the unknown distribution function of ˆϵt, and ˆβ minimizes interval width per the procedure in Algorithm 1. To finish the proof, by Proposition 2, we know that the conditional distribution estimator ˆF (z|E w t ) using QRF converges point-wise to the true F (z|E w t ) as the sample size (hence the number of residuals) approaches infinity. By Lemma 1, we thus know that ̂Qt(p) → Qt(p) in probability for all p ∈ [0, 1]. We can thus use the continuous mapping theorem (Van der Vaart, 2000, Theorem 2.3) to finish the proof: by Assumption 3, the true conditional distribution function F is absolutely continuous and therefore differentiable almost everywhere. Thus, 16 Sequential Predictive Conformal Inference for Time Series the set of discontinuity points of F has measure zero. As the number of data ˜T → ∞ when training QRF, we finally have that in probability, F ( ̂Qt(1 − α + ˆβ)|E w t ) − F ( ̂Qt( ˆβ)|E w t ) →F (Qt(1 − α + ˆβ)|E w t ) − F (Qt( ˆβ)|E w t ) = 1 − α. B. Experimental details (1) Baseline methods. We compare SPCI with three recent CP methods for non-exchangeable data or time series, which have also been carefully described in the literature review. In particular, they all leverage the feedback Yt after it is sequentially revealed. • EnbPI (Xu & Xie, 2021b) proposes a general framework for constructing time-series prediction intervals. In particular, it fits LOO regression models and uses residuals as non-conformity scores. Comparing our use of SPCI in experiments, the only difference appears in using conditional rather than empirical quantiles for the calibration of interval width. • AdaptiveCI (Gibbs & Candes, 2021) is an adaptive procedure that adjusts the significance level α based on historical information of interval coverage. It leverages CQR (Romano et al., 2019) to produce intervals that maintain coverage validity in theory. We use the quantile random forest as the predictor and update α according to the simple online update (ibid., Eq (2)). • NEX-CP (Barber et al., 2022) uses weighted quantiles to tackle arbitrary distribution drift in test data. In particular, the implementation is based on full conformal with weighted least squares regression models, which empirically yields more stable coverage than the naive split conformal method. (2) Real-data description. We describe the three real time-series for results in Section 5.2. The first dataset is the wind speed data (m/s) at wind farms operated by the Midcontinent Independent System Operator (MISO) in the US (Zhu et al., 2021). The wind speed record was updated every 15 minutes over a one-week period in September 2020. The second dataset contains solar radiation information 1 in Atlanta downtown, which is measured in Diffuse Horizontal Irradiance (DHI). The full dataset contains a yearly record in 2018 and is updated every 30 minutes. We remark that uncertainty quantification for both wind and solar is important for accurate and reliable energy dispatch. The last dataset tracks electricity usage and pricing (Harries et al., 1999) in the states of New South Wales and Victoria in Australia, with an update frequency of 30 minutes over a 2.5-year period in 1996–1999. We are interested in tracking the quantity of electricity transferred between the two states. B.1. Simulation We first describe details regarding data simulation procedures. We then show additional rolling coverage and width results when comparing with NEX-CP. B.1.1. DATA SIMULATION For the results in Table 1, we simulate the non-stationary and heteroskedastic time-series as follows: 1. Non-stationary (Nstat) time-series: We let f (Xt) = g(t)h(Xt). (26) g(t) = log(t ′) sin(2πt ′/12), t ′ = mod(t, 12). h(Xt) = (|βT Xt| + (βT Xt) 2 + |βT Xt| 3)1/4. Note that the model in (26) can represent non-stationary time-series due to additional time-related effects (e.g., time drift, seasonality, periodicity, etc.). For a fixed window size w ≥ 1, each feature observation Xt = [Yt−w, . . . , Yt−1] contains 1Collected from National Solar Radiation Database (NSRDB): https://nsrdb.nrel.gov/. 17 Sequential Predictive Conformal Inference for Time Series Table A.1: Simulation on non-stationary time-series: the setup is identical to Table 1. We compare SPCI against baseline CP methods when no time information is assumed known (i.e., ˜Xt = Xt). SPCI EnbPI AdaptiveCI NEX-CP Coverage Width Coverage Width Coverage Width Coverage Width 0.92 (2.75e-3) 12.96 (2.56e-2) 0.90 (2.21e-3) 25.41 (4.79e-2) 0.90 (4.12e-3) 28.00 (5.81e-2) 0.93 (3.10e-3) 46.50 (6.29e-2) the past w observations of the response Y . We sample the errors ϵt from an AR(1) process, where ϵt = ρϵt−1 + et and et are i.i.d. normal random variables with zero mean and unit variance with ρ = 0.6. We want to compare the performance of EnbPI and SPCI assuming no feature mis-specification, so that the only difference in interval coverage/width lies in how the residuals are used to construct the intervals. Therefore, because f in (26) explicitly depends on t and Xt, we use the new feature ˜Xt := [mod(t, 12), Xt] to predict Yt. We acknowledge that in practice, the true periodicity constant 12 in (26) is unknown, and one must estimate it before constructing the new feature ˜Xt. Meanwhile, Table A.1 compares all four CP methods when the time information is unknown (i.e., ˜Xt = Xt), and we still observe much narrower intervals by SPCI than the baselines. 2. Heteroskedastic (Hetero) time-series: We let f (Xt) = (|βT Xt| + (βT Xt) 2 + |βT Xt| 3)1/4. (27) Var(ϵt) = σ(Xt) 2, σ(Xt) = 1 T Xt. (28) Note that the model above represents the generalized autoregressive conditional heteroskedasticity (GARCH) model (Engle, 1982), where variances of response Yt depend on its feature Xt. We let features Xt ∈ R20, with i.i.d. entries from Uniform[0, e 0.01mod(t,100)). Due to heteroskedastic errors, we estimate conditional quantile of normalized residuals ˆϵt := (Yt − ˆft(Xt))/ˆσ(Xt) and multiply the quantile values by estimates ˆσ(Xt) to construct the prediction intervals. For the simulated results in Table 2, the data with distribution-shift and change-points are simulated as follows. For N = 2000 and Xi ∼ N (0, I 4), i = 1, . . . , N : 1. Distribution-drift (Drift): Yi ∼ X T i βi + N (0, 1), where β1 = (2, 1, 0, 0), βN = (0, 0, 2, 1), and βi, i = 2, . . . , N − 1 is a linear interpolation of β1 and βN . 2. Changepoints (Change): Yi ∼ X T i βi + N (0, 1), β1 = . . . = β500 = (2, 1, 0, 0) β501 = . . . = β1500 = (0, −2, −1, 0) β1501 = . . . = βN = (0, 0, 2, 1). Similar to NEX-CP, we apply SPCI after a burn-in period of the first 100 sample points, and in addition, adaptively refit the point estimator ˆf using a rolling window of min(T, T0) points during testing for T = 101, . . . , 2000. We choose T0 = 300 under distribution shifts and T0 = 200 under changepoints. Similar to NEX-CP, we use weighted linear regression with exponentially decaying weights to train the point estimator ˆf in SPCI. B.1.2. COMPARISON WITH NEX-CP We explain why SPCI tends to under-cover in these settings before α adjustment. We suspect the primary reasons are that prediction residuals ˆϵi in these settings are (nearly) independent yet non-identically distributed. More precisely, regarding independence, suppose we use the split conformal framework in SPCI to train ˆf and obtain residuals on the calibration set. We thus have that for each prediction residual ˆϵi in the calibration set, ˆϵi = Yi − ̂Yi ∼ (X T i βi + N (0, 1)) − ˆf (Xi). (29) Note that Xi are all independent by design. Except for the possible dependency in βi, which is zero in the change-point setting, the (unobserved) test residual ˆϵT +1 ⊥⊥ ˆϵT +1−k, k ≥ 1, where ⊥⊥ denotes independence of random variables. 18 Sequential Predictive Conformal Inference for Time Series (a) Distribution shift (b) Changepoint Figure A.1: Rolling coverage and width during test time without adjusted α values. Target coverage at 0.9 is marked in the black lines. In (b), the two changepoints are marked in dotted red line at time indices 500 and 1500. Table A.2: Marginal coverage and width by SPCI and AdaptiveCI on three NASDAQ stock market data. The tarrget coverage is 0.9, and entries in the bracket indicate standard deviation over three independent trials. Method Company AJISF Company AGTC Company AAVL Coverage Width Coverage Width Coverage Width SPCI 0.89 (2.34e-3) 17.64 (1.24e-1) 0.95 (2.43e-3) 2.89 (5.23e-2) 0.81 (3.64e-3) 1.03 (2.34e-2) AdaptiveCI 0.94 (3.43e-3) 30.20 (2.53e-1) 0.71 (1.53e-2) 5.88 (7.43e-2) 0.64 (2.32e-2) 2.18 (3.37e-2) We empirically verify the independence of residuals through the PACF plot in Figure A.2. On the other hand, regarding non-identical distribution, because of drifts or changepoints through the changes in βi, the residuals do not follow the same distribution. Thus, the QRF estimated on past residuals may not be a desirable estimator for the conditional quantile of the test residual ˆϵT +1, hence weakening the performance of SPCI in this setting. B.2. Additional real-data comparisons We compare SPCI with AdaptiveCI on stock market data. Specifically, the dataset is publicly available on Kaggle https://www.kaggle.com/datasets/paultimothymooney/stock-market-data, where we are inter- ested in constructing the prediction intervals for the closing price. We randomly select three NASDAQ stock from three companies. Table A.2 shows several findings: • When AdaptiveCI and SPCI both yield valid coverage (on company AJISF), the width of SPCI is significantly narrower. • Even when AdaptiveCI loses coverage and SPCI maintains coverage (on company AGTC), the width of SPCI is still significantly narrower. • When both methods lose coverage (on company AAVL), the loss by SPCI is less and SPCI still yields narrower intervals. (a) Distribution shift (b) Changepoint Figure A.2: PACF using 300 residuals (dist. shift) and 200 residuals (change-point). We see near independence of the residuals, which are non-identically distributed due to the data generation. 19 Sequential Predictive Conformal Inference for Time Series Algorithm 2 SPCI for exchangeable data (based on split conformal) Require: Training data {(Xt, Yt)} T t=1, significance level α. Output: Prediction intervals ̂Ct−1(Xt), t > T 1: Randomly split {1, . . . , T } into disjoint index sets I1 and I2. 2: Train a point predictor ˆf with {(Xt, Yt)}t∈I1 . 3: Obtain residuals ˆϵt := Yt − ˆf (Xt) for t ∈ I2. 4: for t > T do 5: Return the prediction interval ̂Ct−1(Xt) as in (3). 6: end for B.3. Multi-step inference (1) Motivation and setup. We first motivate the study of multi-step ahead prediction interval. For examples in Section 5.2, all intervals are one step ahead: the response variable Yt is revealed before ̂Ct−1(Xt) is constructed, which is the prediction interval for Yt+1. Such immediate feedback is advantageous for all adaptive methods as they thus have access to the most up-to-date information about the data process. Nevertheless, such access can be neither feasible nor desirable for some use cases. In energy systems such as wind or solar prediction, we often need multiple forecasts spanning a long enough future horizon to allow enough time for subsequent dispatch. Meanwhile, lags in data collection can limit the availability of feedback—for S > 1, Yt may not be revealed until all S intervals ahead are constructed. We consider the following multi-step ahead prediction setting. Fix a value of S ≥ 1, which denotes the s−step ahead prediction setting (S = 1 refers to examples in earlier sections). Features Xt = [Yt−1, . . . , Yt−τ ] are auto-regressive with a pre-specified window τ ≥ 1. At prediction time t, we need to construct S prediction intervals at once for time indices t, . . . , t + S − 1. In particular, responses Yt, . . . , Yt+S−1 (and thus features Xt+1, . . . , Xt+S) are not available until we construct prediction intervals at indices t + S, . . . , t + 2S − 1. (2) Multi-step SPCI algorithm. Note that constructing multi-step ahead prediction intervals using SPCI involves estimating the joint distribution of ˆϵt+1, . . . , ˆϵt+S every S test indices. Doing so can be highly challenging. Instead, we take a simplified “divide-and-conquer” approach based on the LOO fitting in EnbPI. First, we train S sets of LOO predictors for estimating the value of ̂Yt+j, j = 0, . . . , S − 1. This is implemented by fitting B bootstrap models on each lagged data {(Xt, Yt+s)}T −s+1 t=1 , s = 1, . . . , S. Then, we compute residuals only at t = 1 + kS : kS ≤ T − 1. We do so because on test data, new feature Xt and output Yt are revealed only in every S step. Lastly, we fit QRF S times using past residuals with lags to obtain s prediction intervals at once. We briefly compare and contrast Algorithm 1 (SPCI) and 3 (multi-step ahead SPCI) when LOO point predictors are trained. Computationally, we need to refit S − 1 more sets of LOO predictors in multi-step ahead SPCI for point prediction. On the other hand, both algorithms fit the same number of QRF regressors for constructing prediction intervals. In practice, multi-step SPCI is expected to yield wider intervals as S increases because there is greater uncertainty when fitting the baseline regression or QRF on lagged data. A simple example is the AR(1) process where xt = axt−1 + ϵt, ϵt i.i.d. ∼ N (0, 1). Using the present feature xt−1, we have xt+S = a S+1xt−1 + ∑S i=1 ai−1ϵt+i, whereby the error distribution a i−1ϵt+i ∼ N (0, ∑S i=1 a 2(i−1)), so width naturally increases. C. Additional technical details We first present the SPCI algorithm for exchangeable data in Algorithm 2. We then present the SPCI algorithm for multi-step ahead inference in Algorithm 3. 20 Sequential Predictive Conformal Inference for Time Series Algorithm 3 Multi-step SPCI (based on LOO prediction in EnbPI (Xu & Xie, 2021b)) Require: Training data {(Xt, Yt)} T t=1, significance level α, number of bootstrap estimators B, aggregation function ϕ, conditional quantile regression algorithm Q, multi-step size S > 1. Output: Prediction intervals ̂Ct−1(Xt), t > T 1: for s = 1, . . . , S do {▷ s-step ahead model fitting} 2: Sample with replacement B index sets, each of size T − s + 1: {Sb : Sb ⊂ {1, . . . , T − s + 1}}B b=1. 3: Train B corresponding bootstrap estimators { ˆf b}B b=1 on data {(Xt, Yt+s−1) : t ∈ Sb}. {▷ Leave-one-out aggregation} 4: Initialize ̂ϵ = [ ] 5: for t = 1, 1 + S, . . . , 1 + kS such that kS ≤ T − 1 do 6: ˆf s t (Xt) = ϕ({ ˆf b(Xt), t /∈ Sb}B b=1) 7: ̂ϵ.append(Yt+s−1 − ˆf s t (Xt)) 8: end for 9: end for 10: for t > T do {▷ Interval construction} 11: Compute s = mod(t − T, S + 1) and t′ = t − s {▷ t′ denotes the most recent index where residual ˆϵt′ and feature Xt′+1 are available.} 12: if s = 1 then {▷ Fit quantile regressors with updated residuals} 13: Re-fit S quantile estimators { ̂Qt(· ; s ′)}S s′=1 with {(ˆϵw j , ˆϵj+s′−1)} t−1−(S−1) j=t−T +w . 14: end if 15: Compute ˆβ = arg minβ∈[0,α]( ̂Qt(1 − α + β; s) − ̂Qt(β; s)) using ˆϵ w t′ . 16: ̂Ct−1(Xt) = [ ˆYt + wleft(t), ˆYt + wright(t)], where ˆYt = ϕ({ ˆf s j (Xt′+1)} T /S j=1 ), wleft(t) = ̂Qt( ˆβ; s), wright(t) = ̂Qt(1 − α + ˆβ; s) . 17: end for 21","libVersion":"0.3.2","langs":""}
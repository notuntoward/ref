{"path":"lit/lit_sources/Sack12windEns2ProbFrcst.pdf","text":"From NWP Ensembles to Probabilistic Wind Energy Production Forecasts Jeremy Sack \u00031, Lueder von Bremen y2, Athanasios Kyriazis z1 and Rory Donelly x1 13E 2Forwind, University of Oldenburg Abstract Probabilistic wind power forecast has gained increasing importance in the recent years. The advantages of probabilistic information, for the integration of wind power to the electrical network, are discussed in numerous publications which mainly address advantages in energy trading and electricity network regulation. The value of the probabilistic wind power forecast is based on the spread of wind power forecast members originating either from a number of NWP models or ensem- ble products of global models (such as ECMWF, UKMO, Meteo France, NCEP and others). Wind power members provide an estimate of the expected uncertainty of power predictions. However, calibration is needed for reliable probabilistic information that is consistent with observations. The paper addresses the development and assesses the quality of the 100m ECMWF EPS (ensemble prediction system winds) which were introduced by ECMWF beginning of 2010 in two operational wind farms (onshore and o\u000bshore). In addition, the paper compares di\u000berent calibration method- ologies of Ensembles (in wind and in wind power mode) and addresses the implications for the operational environment. Results show that the probabilistic calibration methodologies examined in wind power mode success- fully optimise the quantiles ranging from 10% to 90% with some outliers outside of these limits, mainly due to limited amount of available observational data. The paper compares the di\u000berent approaches by means of resolution, sharpness, reliability and skill score. We address the training window of the calibration methodologies in the operational forecast environment and show that sim- ple models x yield equivalent accurate results in the operational forecast environment with training periods of less than a month compared with more complex models with longer training periods. Ensembles that have been calibrated in wind mode ((u,v) components at the same time) are still well calibrated when transformed into wind power and show good improvements in probabilistic skill compared to the raw ensemble. The bene\ft from calibration is less when veri\fcation is done with observed wind power instead of using simulated wind power for Germany. The spread of the calibrated ensemble is underestimated with respect to real observations mostly because the used 10m winds can not re\rect the dependence on thermal stability of the atmosphere. \u0003Jeremy.Sack@3e.eu yLueder.von.Bremen@uni-oldenburg.de zThanos.Kyriazis@3e.eu xRory.Donelly@3e.eu I 1 Introduction Since any meteorological forecast is deemed to a given uncertainty, stand alone point forecasts - al- though easy to understand - lack of information. For this reason, any decision making process based on meteorological forecasts should include an estimate of uncertainty. As this also and especially applies to wind power predictions, probabilistic wind power forecasts have experienced increasing popularity in the past years. The advantages of probabilistic information for the integration of wind power to the electrical network, are discussed in numerous publications which mainly address advantages in energy trading and electricity network regulation. As a consequence, di\u000berent methods of obtaining probabilistic forecasts have evolved between simply human guessing of the uncertainty associated to a forecast and elaborate automized statistical methods ([1]). One of the latter ones to have gained wide scienti\fc acceptance is the use of probabilistic forecasts from ensemble Numerical Weather Prediction (NWP) forecasts ([1]). As it has turned out, these ensemble forecasts yield best results when they are calibrated ([2]). The present work investigates 4 di\u000berent calibration methods on two di\u000berent data sets including wind forecasts from the European Centre for Weather Forecast (ECMWF) Ensemble Predictction System (EPS), and power production measurements from (1) an o\u000bshore wind farm located in the belgian north sea, and (2) from several wind farms in Germany. In the \frst section, the mentioned data sets are described in detail, followed by a short description of the power curve model used for the conversion from wind to power predictions. The second section highlights with the di\u000berent calibration methods and the (dis)advantages associated to them. In the next section, the results are presented, \frst for the belgian o\u000bshore wind farm, then for the german regional forecast. In a last section, we present our conclusions, an outlook for future studies and our acknowledgements. 2 Data Overview In the present work, probabilistic forecasts for the power production of a wind farm (obtained from NWP ensemble forecast time series), are compared to measured power production in order to test di\u000berent probabilistic forecast methods. The di\u000berent datasets used for this are speci\fed in this section. 2.1 Power Production Measurements The measured power production data from the SCADA monitoring system of an o\u000bshore wind farm located in the belgian North Sea (Lon: 2.79, Lat: 51.64) is used. The data contains the aggregated production of the wind farm on 15 min time steps during the time period July-December 2011, as well as the number of available turbines at each time step. 2.2 NWP Forecast Data The NWP forecast data - in the present work used as initial data for wind power forecasts - originates from the ECMWF forecasting system. The deterministic forecasts were processed by the ECMWF general circulation model, T1279L91, on a gaussian grid of approximately 16 km spatial resolution. The original temporal resolution of 3 hours was reduced to 15 minutes by linear interpolation. Although this might introduce considerable errors, this step was necessary in order to compare the NWP data to the quater hourly measurements. For calculation time reasons, the ensemble forecasts are processed with the same base model but on a coarser grid. This setup is called T639L61 (only up to 10 hPa) and has a resulting spatial resolution of 32 km. As for the deterministic model, the original 3 hourly time step has been reduced to 15 min through linear interpolation. II 3 Power Curve Model As power curve model, the manufacturer's power curve is used. It assumes, that the wind farm's aggregated power production is the sum of the power production of each single wind turbine in the wind farm as predicted by the manufacturers power curve. Although this assumption should lead to consirable errors as wake e\u000bects are neglected, it appears to provide similar or even better results than the statistical learning power curve model as described in the Riso report ([2]). As the aim of the present work is a comparison of calibration methods rather than a description of the best possible power curve model we decided to use the simple manufacturer's power curve model. 4 Calibration Methods In the present work, three di\u000berent Calibration methods are compared to each other: 4.1 In\ration (INF) The in\ration ([3]) expands the ensemble spread with the goal of optimal reliability under the constraints that the standard deviation of the in\rated prediction is the same as that of the measurements and that the correlation betweeen the in\rated prediction and the measurements remains invariant. With zi representing the ensemble mean prediction at time i and zij the di\u000berence between ensemble member j and the ensemble mean, the in\rated estimate of the ensemble member j can be expressed as wij = \u000bzi + \fzij: (1) The coe\u000ecients \u000b and \f are found under the constraints that (1) the standard deviation of the in\rated prediction is the same as that of the measurements sm and (2) the correlation ˆ between the in\rated prediction and the measurements does not change with the in\ration. In order to do so, the coe\u000ecients are de\fned as \u000b = abs(ˆ) sm sen (2) \f = p 1 \u0000 ˆ2 sm se (3) where sen is the standard deviation of the ensemble mean and se is the square root of the mean variance of the ensemble. This method corrects the under- or overestimation of the ensemble spread with no modi\fcation of the ensemble mean correlation. The coe\u000ecients are retrieved from a rolling training set looking either 20 or 50 days into the past. 4.2 Gaussian Dressing (GD) Other than the in\ration, the gaussian dressing method ([1]) does not modify the ensemble member trajectories. It assigns a gaussian kernel distribution to each ensemble member and averages these to obtain an overall distribution that can take any shape depending on the distribution of the underlying ensemble members. From this distribution, the quantiles for the probabilistic forecast can be obtained. The key parameter is the standard deviation ˙D of the kernels which in this case is the same for all ensemble members as the EPS-members can not be distinguished from each other: ˙2 D = ˙x\u0000y 2 \u0000 (1 + 1 nens )˙ens2 (4) Where ˙x\u0000y 2 is the error variance of the ensemble-mean forecast, nens is the amount of ensemble members and ˙ens2 is the average of the ensemble variances over the training set. As for the in\ration, III rolling training windows of 20 and of 50 days are used. The forecast probabilities for the quantiles q can now be calculated as P (v \u0014 q) = 1 nens nensX i=1 \b \u0014 q \u0000 ~xi ˙D \u0015 (5) with \b standing for the cumulative distribution function of a standard gaussian distribution and ~xi for the ith bias corrected ensemble-member. 4.3 Bootstrap (BS) The Bootstrap method ([4]) (also called rank histogram or talagrand diagram) is constructed from the notion that in an ideal EPS system the verifying analysis is equally likely to lie in any wind direction interval de\fned by any two ordered adjacent members, including when the measurement is outside the ensemble range on either side of the distribution. The probability for the analysis to fall into one speci\fc bin is therefore 1=(nens + 1), with nens being the amount of members. From this, a cumulative distrubution function can be \ftted to the inter ensemble member bins from which the quantiles for the probabilistic forecast are obtained. As for the gaussian dressing method, the bootstrap method does not modify the ensmble member trajectories and further bears the advantage of needing no parametric distribution assumption as well as no trainingset. 4.4 Evaluation Measures 4.4.1 Reliability Reliability ([2]) adresses wether quantile forecasts are indeed quantiles by comparing the nominal value of the quantile to the number of times the observation is actually below the quantile considered. Repeating this procedure for a range of quantiles allows a plot of the actual versus the nominal values to be constructed (Fig.1). Ideally, the resulting line should be the line of identity. 4.4.2 Sharpness Sharpness ([2]) measures the average uncertainty indicated by the quantile forecast system by adressing the average di\u000berence between the Q75 and Q25 quantiles. It is calculated individually for each forecast horizon and should ideally be small. It can be seen as a control measure for the reliabilty. The latter can be increased through in\rating the spread of the EPS forecast. A resulting increased sharpness would indicate the along going risk over estimation. 4.4.3 Resolution The resolution ([2]) measures the variation in uncertainty indicated by the quantile forecast system by adressing the same quantities as for sharpness, but calculating measures of variation intead of e.g. the mean. Here, it is de\fned as the standard deviation of the di\u000berence between Q75 and Q25 quantiles sorted by forecast range. The resolution indicates, how well the forecast system is capable of di\u000berentiating between certain and uncertain weather settings. 4.4.4 Spread-skill relationship Here, the spread is a time series of (Q75-Q25) and the skill is a time series of the absolute error of the ensemble median with respect to the real power production. One would expect, that where the spread is low (high), the ensemble median error should also be low (high). IV 4.4.5 CRPSS This is a skill score that summarizes the four previous evaluation measures to one intercomparable score value. If it is positive (negative), the the quality of the forecast is increased (decreased) by the calibration method. 5 Results An illustration of the output of the di\u000berent calibration methods described above is given in \fgures 1, 3 and 2. In \fgure 1, the reliability, sharpness and resolution are compared to the raw probabilistic forecast (red line). In terms of reliability, all calibration methods increase the forecast quality. Though, for each method this quality improvement is associated with a trade o\u000b concerning other evaluation measures. The In\ration for example, corrects the reliability nearly perfectly (orange line). This is no surprise, since this is exactly what the method targets at. Also, the sharpness is lower than for the raw forecast, which is positive as it states, that the reliability is not improved through simply exagerating the spread size so that all observations fall into it. The trade o\u000b for the in\ration is the resolution which decreases by 50% implying that the calibrated forecast loses some of the ability to di\u000ber between large and small spreads. This also re\rects in the spread skill relationship (Fig. 3), where the spread curve stays below the skill by 5% of the power relative to the installed capacity. A summary about wether the combination of the latter measures signi\fes an overall improvement in comparison to the raw forecast is given by the CRPSS score. In case of the In\ration it has a mean value of 0.35 without large variations with respect to forecast range (Fig.2). The In\ration has been performed with two di\u000berent rolling training window sizes: One looking 20 days into the past, the other 50 days. From the evaluation measure plots, one can see that this increase in training set size yields to very slight improvement in forecast quality. The CRPSS score re\rects this through an increase from 0.35 to 0.38. Also the Bootstrap method improves the reliability, although not as much as the in\ration does (gray line on Fig.1). Equally, it increases the resolution by very few and has the trade o\u000b of increasing the sharpness, which is a negative e\u000bect since it means that the spread overestimates risk. In comparison to the in\ration, the bootstrap results depending on forecast range (sharpness, resolution, spread-skill relation) follow much closer the exact variances of the raw forecast which might be due to the fact that this method leaves the ensemble members unmodi\fed and just retrieves the quantiles in a di\u000berent way. The increase in sharpness is so little, that it can not clearly be detected in the spread-skill relation ship (Fig.3. The CRPSS summarizes the di\u000berent measures to -0.23, judging this method to decrease the overall forecast quaility. It remains interesting to notice though, that this method increases the quality of some aspects of the probabilistic forecast (reliability) without the use of any training set. As the previous methods, also the Gaussian Dressing reveals remarquable trade o\u000bs. It signi\fcantly increases the reliability (green line on Figs.1), but at the same time decreases the quality of sharpness and resolution. The latter re\rects well in the spread skill relationship (Fig.3), where the spread is too high in comparison with the skill. Further, the spread seems to remain constant throughout all forecast horizons which is clearly negative because the forecast uncertainty should be smaller (larger) for shorter (longer) forecast ranges. In spite of three out of four negative results in the evaluation measures, the increase in reliability seems enough to keep the mean CRPSS positive at 0.14. An increase in training set here even reduces the forecast quality from 0.14 to 0.13. V0.00.20.40.60.81.0 0.00.20.40.60.81.0Reliabilitynominal actualRAWINF_manufGD_manufBS_manuf2530354045 0.000.050.100.150.200.250.30Sharpnessforecast range [h] norm.sharpness2530354045 0.000.050.100.150.200.250.30Resolutionforecast range [h] norm.resolution Figure 1: Reliability, Sharpness and Resolution resulting from di\u000berent calibration methods. Solid lines represent results for a rolling training set of 20 days, dotted lines represent the rolling training window of 50 days. (Reminder: RAW: raw forecast without calibration; INF: In\ration; GD: Gaussian Distribution; BS: Bootstrap)2530354045 −0.4−0.20.00.20.4INFfc range CRPSS2530354045 −0.4−0.20.00.20.4GDfc range CRPSS2530354045 −0.4−0.20.00.20.4BSfc range CRPSS Figure 2: CRPSS by range for all methods. The red lines represent results for longer trainingsets (50 days instead of 20) VI 25 30 35 40 450.050.100.150.20 Spread Skill Rel. RAW forecast range [h]rel. power 25 30 35 40 450.050.100.150.20 Spread Skill Rel. BS forecast range [h]rel. power 25 30 35 40 450.050.100.150.20 Spread Skill Rel. INF forecast range [h]rel. power 25 30 35 40 450.050.100.150.20 Spread Skill Rel. INF tr50 forecast range [h]rel. power 25 30 35 40 450.050.100.150.20 Spread Skill Rel. GD forecast range [h]rel. power 25 30 35 40 450.050.100.150.20 Spread Skill Rel. GD tr50 forecast range [h]rel. power Figure 3: Spread-skill relationships for all methods. Red lines: skill (MAE of EPS mean) , black lines: spread (average interval size) VII 6 Wind Power Forecasts with calibrated Wind Ensembles A di\u000berent strategy from calibrating wind power ensembles, is to calibrate the wind Ensemble prior the usage for wind power forecasting. 6.1 Wind Ensemble Calibration Within the EU-Project SafeWind (u,v)-wind ensemble forecasts in 10m height have been calibrated for the years 2007-2009 in a multivariate Gaussian framework over Europe ([5]). The approach was inspired by Bayesian model averaging (BMA) and adaptive kernel dressing of ensembles in a univariate framework. As described in ([5]) u and v components are jointly considered, instead of focusing on wind speed and direction individually. The months December 2006 to November 2007 have been used as a training set and the years 2008 and 2009 have been used to evaluate the bene\ft of calibrated Ensembles over the raw Ensemble for German wind power forecasts. Note, that as reference to the calibration ECMWF analyses and not real observations have been used. Since the analysis are issued only every six hours, the calibration and the evaluation is done with a relative coarse temporal resolution of six hours up to forecast step +120h. The ensemble forecasts are issued twice a day with a horizontal resolution of about 50km corresponding to about 0.42 \u000e. 6.2 Wind Power Forecast Model The used wind power forecasting model for Germany is intentionally kept very basic. The data base that holds the information about regional distribution and capacity of wind power deployment in Germany is evaluated on a monthly basis. All wind turbines in Germany are listed in the data base and are mapped to the model grid points of the EPS forecasts. The number of model grid points is 460 for Germany. However, not all model grid points are populated. For each model grid point the following information has been compiled and is used by the wind power forecast model: \u000f Installed wind power capacity \u000f Hub height (weighted according to wind turbines allocated to this model grid point) \u000f Lowest (20% quantile) surface roughness lenght z0 The winds are extrapolated with the logarithmic wind pro\fle for neutral conditions to hub height using the supplied surface roughness lenght. Unfortunately, the thermal strati\fcation of the atmosphere can not be computed from available EPS forecasts. Neither temperature forecasts on model levels nor the surface friction velocity are archived to compute a characterize stability. Consequently, large over (or under-) estimations of forecasted wind speeds in hub height occur in non-stable (stable) conditions. As during night non-stable conditions prevail wind power is underestimated during night and overestimated during the days. The usage of 00 and 12UTC forecasts explains the twelve hour cycle of the systematic forecast error (not shown). The twelve hour cycle in the root mean square forecast error (RMSE) is caused by this strong systematic forecast error (bias) (Fig. 4, middle). 6.2.1 Wind Power Bias Correction The strong diurnal bias in wind power forecasts for Germany is not satisfying and a simple ex-post bias correction has been developed and applied to the wind power forecast data. The bias correction is dependent on the time of the day and is done for each of the four TSO zones in Germany individually. The bias correction consists only of an additive component, i.e. no linear regression is performed be- cause a linear regession a\u000bects the ensemble spread. The forecast skill, expressed as RMSE normalized with the installed capacity, is shown in Fig. 4 (right) for the bias corrected wind power forecasts. In deterministic mode the wind power bias correction is very e\u000ecient to remove the diurnal bias. The calibrated ensemble mean and the calibrated control forecast (black lines) are substantially better VIII (up to 1%) than the raw ensemble. The ensemble mean is already at Day+1 outperforming the (sin- gle) control forecast for raw and calibrated forecasts. The calibration tends to decrease the ensemble spread, de\fned as root mean square di\u000berene of the ensemble members to the ensemble mean, for higher lead-times (dotted line). 0 20 40 60 80 100 120 fcstep [h] 0.00 0.05 0.10 0.15RMSE wind power, ensemble spread [1] 0 20 40 60 80 100 120 fcstep [h] 0.00 0.05 0.10 0.15RMSE wind power, ensemble spread [1] 0 20 40 60 80 100 120 fcstep [h] 0.00 0.05 0.10 0.15RMSE wind power, ensemble spread [1] 0 20 40 60 80 100 120 fcstep [h] 0.00 0.05 0.10 0.15RMSE wind power, ensemble spread [1] 0 20 40 60 80 100 120 fcstep [h] 0.00 0.05 0.10 0.15RMSE wind power, ensemble spread [1] 0 20 40 60 80 100 120 fcstep [h] 0.00 0.05 0.10 0.15RMSE wind power, ensemble spread [1] Figure 4: Ensemble wind power spread (dotted line), skill (RMSE) of Ensemble mean wind power forecast (full line) and Control forecast (dashed line) over forecast step. Wind power forecasts are computed for Germany with raw 10m winds (red) and calibrated 10m winds. Left veri\fcation with simulated wind power, middle veri\fcation with observed wind power and no wind power bias correction. A time-of-the-day dependent wind power bias correction is applied for the right \fgure (veri\fcation with observed wind power data.) 6.3 Veri\fcation with simulated Wind Power Not only wind speed forecast are converted into wind power utilizing the wind power forecast model but also wind analysis data is used to simulate the production of wind power in Germany. Those simulated wind power data has the same de\fciencies with respect to thermal strati\fcation e\u000bects as the wind power forecasts that base on 10m winds. Consequentely, no diurnal cycle occur in the skill of the ensemble mean (Fig. 4, left). Thus, simulated wind power data is ideal to evaluate the impact of 10m calibrated winds over the raw ensemble disregarding possible de\fcits in the coversion of wind into wind power. It can be seen that the lines for ensemble spread and the skill of the ensemble mean (RMSE) are much closer together than in case of veri\fcation with observed data. The closer skill and spread are together, the better extreme observations (high and low) are captured by the ensemble members. Talagrand (Rank) Histograms are shown in Fig. 5 at forecast Day+3. The Talagrand Diagram for the raw ensemble (left in red) is skewed to the left indicating that quite often simulated (equivalent to observed) wind power is lower than the lowest forecast members. The calibrated ensemble exhibits an almost \rat Talagrand Diagram. This is a very positive sign that the spread of the calibrated ensemble is able to capture the distribution (including the tails) of wind power events very well. The wind power forecasts with calibrated 10m winds show almost perfect reliability compared to the uncalibrated winds when verifying against simulated wind power (Fig. 7, left). For all forecast probablities the observed probablity of the event wind power > 50% lies within the calculated consis- tency bars. The consistency bars are calculated following a method suggested by ([6]) in order to take the limited number of cases into account. The raw ensemble is strongly over-con\fdent, i.e. forecast probabilities are always much higher than the observed frequency of wind power > 50%. This \fnding is in line with the result in Fig. 5 (left) that low wind power values can not be captured very well by the (lowest) ensemble members. IX 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0.0 0.2 0.4 0.6 0.8 1.0obs. prob. [1] BSEPS:0.028 ,BSSref:0.113 BSref:0.032 BSclimate:0.077 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0 1000 2000 3000 4000 5000 6000frequency 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm rate 0.0 0.2 0.4 0.6 0.8 1.0Hit rate ROCEPS:0.946 ROCref:0.965 Day 3 0.000 0.005 0.010 0.015 0.020 0.025 0.030frequency [1] Day 3 0.00 0.02 0.04 0.06frequency [1] 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0.0 0.2 0.4 0.6 0.8 1.0obs. prob. [1] BSEPS:0.028 ,BSSref:0.113 BSref:0.032 BSclimate:0.077 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0 1000 2000 3000 4000 5000 6000frequency 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm rate 0.0 0.2 0.4 0.6 0.8 1.0Hit rate ROCEPS:0.946 ROCref:0.965 Day 3 0.000 0.005 0.010 0.015 0.020 0.025 0.030frequency [1] Day 3 0.00 0.02 0.04 0.06frequency [1] 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 5: Talagrand Diagram at forecast Day+3 for uncalibrated (red) and calibrated (black) 10m EPS winds forecasting German wind power. The veri\fcation is done with simulated wind power. 6.4 Veri\fcation with observed Wind Power The veri\fcation with real feed-in data exhibit that the spread of both (the raw and the calibrated) ensemble forecast is too small to capture low and high wind power events properly (Fig. 6). The Talagrand Diagrams show that the observed wind power is too often lower (or higher) than the lowest (or highest) member of the ensemble. It must be noted that the ensemble spread is independent on the veri\fcation data. However, ensemble spread and skill of the ensemble mean are relatively close to each other when simulated wind power is used for veri\fcation (Fig. 4, left) compared to veri\fcation with observed wind power (Fig. 4, right). 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0.0 0.2 0.4 0.6 0.8 1.0obs. prob. [1] BSEPS:0.027 ,BSSref:0.135 BSref:0.031 BSclimate:0.064 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0 1000 2000 3000 4000 5000 6000frequency 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm rate 0.0 0.2 0.4 0.6 0.8 1.0Hit rate ROCEPS:0.950 ROCref:0.959 Day 3 0.00 0.05 0.10 0.15 0.20frequency [1] Day 3 0.00 0.02 0.04 0.06 0.08 0.10frequency [1] 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0.0 0.2 0.4 0.6 0.8 1.0obs. prob. [1] BSEPS:0.027 ,BSSref:0.135 BSref:0.031 BSclimate:0.064 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0 1000 2000 3000 4000 5000 6000frequency 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm rate 0.0 0.2 0.4 0.6 0.8 1.0Hit rate ROCEPS:0.950 ROCref:0.959 Day 3 0.00 0.05 0.10 0.15 0.20frequency [1] Day 3 0.00 0.02 0.04 0.06 0.08 0.10frequency [1] 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 6: Talagrand Diagram at forecast Day+3 for uncalibrated (red) and calibrated (black) 10m EPS winds forecasting German wind power. The veri\fcation is done with observed wind power. A post-processing is applied to remove a strong diurnal bias in the wind power forecast. The reliability of the calibrated ensemble is much better than the raw ensemble when observed wind power is used for veri\fcation (Fig. 7, right). However, the improvement is smaller than for X simulated wind power and de\fcits can be seen for low and for high forecast probabilities, i.e. forecast probabilities are higher than the observered probabilities and fall not into the 95% consistency bars. 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0.0 0.2 0.4 0.6 0.8 1.0obs. prob. [1] BSEPS:0.028 ,BSSref:0.113 BSref:0.032 BSclimate:0.077 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0 1000 2000 3000 4000 5000 6000frequency 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm rate 0.0 0.2 0.4 0.6 0.8 1.0Hit rate ROCEPS:0.946 ROCref:0.965 Day 3 0.000 0.005 0.010 0.015 0.020 0.025 0.030frequency [1] Day 3 0.00 0.02 0.04 0.06frequency [1] 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0.0 0.2 0.4 0.6 0.8 1.0obs. prob. [1] BSEPS:0.027 ,BSSref:0.135 BSref:0.031 BSclimate:0.064 0.0 0.2 0.4 0.6 0.8 1.0 forecast prob. [1] 0 1000 2000 3000 4000 5000 6000frequency 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm rate 0.0 0.2 0.4 0.6 0.8 1.0Hit rate ROCEPS:0.950 ROCref:0.959 Day 3 0.00 0.05 0.10 0.15 0.20frequency [1] Day 3 0.00 0.02 0.04 0.06 0.08 0.10frequency [1] 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 7: Reliability Diagram for the event wind power > 50% for calibrated (black) and uncalibrated (red) 10m EPS winds forecasting German wind power at forecast Day+3. The veri\fcation is done with simulated wind power (left) and real feed-in data (right). The vertical bars are consistency bars (95% con\fdence) to consider sampling errors. The overall skill of the calibrated ensembles relative to the raw ensemble is assessed utilizing the CRPSS (Section 4.4.5). The bene\ft of calibrating the 10m ensemble winds is largest when the veri\f- cation is done with simulated wind power (from 10m winds) (Fig. 8, left). The CRPSS shows a very strong 12h cycle that is also present in the Energy Score computed in the veri\fcation of the calibrated wind ([5]). The 12h cycle is also existent when only data from the 00UTC forecast run are used for the veri\fcation (not shown here). Thus, it can be speculated that the 12h cycle is introduced during the ensemble calibration as 00UTC and 12UTC data have been mixed when calibrated versus analyses. The improvement in probabilistic skill is less pronounced when veri\fed with observed wind power (Fig. 8, right). The de\fcits of 10m winds for wind power forecasting are very striking and lead to a negative CRPSS in the short-range when no wind power bias correction is applied. As discussed before, the bias correction in wind power is very bene\fcial to increase the skill of the calibrated winds relative to the raw ensemble. Almost at all lead times the CRPSS for the calibrated winds is positive demonstrating an improvement over the raw ensembles. 7 Conclusion Three di\u000berent ensemble calibration methods have been applied to to a raw probabilistic forecast performed by a simple power curve model after the conversion from wind to power. For this, a seven month data set including power production and ECMWF 100m ensemble wind forecasts have been used. Also, the calibration of wind ensemble forecasts has been performed prior to the conversion to power for a two year data set including regional power production in Germany and ECMWF 10m enemble wind forecast. It has been shown that the calibration of wind as well as of wind power ensembles improves the skill of probabilistic forecasts. In the case of the Belgian wind farm, 3 di\u000berent calibretion methods have been tested, the in\ration-, bootstrap- and gaussian dressing method. Considering all evaluation XI 0 12 24 36 48 60 72 84 96 108 120 fcstep [h] 0.00 0.05 0.10 0.15 0.20CRPSS 0 12 24 36 48 60 72 84 96 108 120 fcstep [h] -0.15 -0.10 -0.05 -0.00 0.05 0.10 0.15CRPSS Figure 8: CRPSS using calibrated 10m Ensemble winds to forecast simulated (left) and observed (right) German wind power. The reference is the Ensemble System with uncalibrated winds. A post- processing is applied to remove a strong diurnal bias (dashed line in right \fgure) in the wind power forecast. measures and calculated skill scores (CRPSS), the in\ration method works best. Two of these meth- ods, the in\ration and the gaussian dressing have been tested with di\u000berent training window sizes. Considering the in\ration, a slight increase of forcast quality could be noted, in terms of CRPSS the increased training window improves the forecast from 0.35 to 0.38. In case of the gaussian dressing, the forecast quality even decreases with an increased training window size. Overall, we conclude that the increase in training window size from 20 days to 50 days doesn't provide remarquably better results. Ensembles that have been calibrated in wind mode ((u,v) components at the same time) are still well calibrated when transformed into wind power. The (u,v) wind ensemble calibration done with ECMWF 10m winds improves the ensemble spread of wind power forecasts for Germany considerably as long as veri\fcation is done with simulated wind power, i.e. the veri\fcation is based on the same data that is used to derive the calibration (10m wind analyses). Compared to observed wind power the raw or the calibrated ensemble spread is far too small because of the disadvantage to model wind speeds in hub height with 10m winds disregarding the e\u000bect of thermal stability. The strong de\fcit to use 10m winds for wind power forecasting can be reduced by a certain extent via a time-of-the-day dependent wind power bias correction. After this additional post-processing the calibrated 10m winds show an improvement in probabilistic skill of up to 10% compared to the raw ensemble. However, it is strongly recommended not to use 10m winds for wind power forecasting in case thermal stability can not be considered (as it is the case for ECMWF's EPS). Moreover, in the future 100m EPS winds should be calibrated and used for wind power forecasting. Since the ensemble calibration prior as well as after conversion from wind to power improves the pobabilistic forecast quality, we suggest a study about ensemble calibration before as well as after the conversion from wind to power. XII 8 Acknowledgements This work has been partly done in the European Projects MeteoRES Services (Grant Agreement No 230719) and SafeWind (Grant Agreement No 213740). ECMWF is thanked for providing forecast data. References [1] Daniel. S. Wilks. Statistical Methods in the Atmospheric Sciences. International Geophysics Series, 3rd edition, 2011. [2] Giebel G. Wind power predictions using ensembles. Ris˝-R-1527(EN), 2005. [3] Doblas-Reyes F. The rationale behind the success of multi-model ensembles in seasonal forecasting - ii. calibration and combination. Tellus (2005), (57A):234{252, 2004. [4] ECMWF. Rank histogram (Talagrand diagram). URL: http://www.ecmwf.int/products/ forecasts/guide/Rank_histogram_Talagrand_diagram.html, last checked on 2011-04-12. [5] Pierre Pinson. Adaptive calibration of (u,v)-wind ensemble forecasts. Q. J. R. Meteorol. Soc., DOI: 10.1002/qj.1873, 2012. [6] Leonard A. Smith Jochen Broecker. Increasing the reliability of reliability diagrams. Weather and Forecasting, (22):651{661, DOI: 10.1175/WAF993.1, 2007. XIII","libVersion":"0.3.2","langs":""}
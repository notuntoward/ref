{"path":"lit/lit_sources/Lauret19VerificationSolarIrradiance.pdf","text":"HAL Id: hal-02351342 https://hal.archives-ouvertes.fr/hal-02351342 Submitted on 6 Nov 2019 HAL is a multi-disciplinary open access archive for the deposit and dissemination of sci- entific research documents, whether they are pub- lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Verification of solar irradiance probabilistic forecasts Philippe Lauret, Mathieu David, Pierre Pinson To cite this version: Philippe Lauret, Mathieu David, Pierre Pinson. Verification of solar irradiance probabilistic forecasts. Solar Energy, Elsevier, 2019, 194, pp.254-271. ￿10.1016/j.solener.2019.10.041￿. ￿hal-02351342￿ Veriﬁcation of solar irradiance probabilistic forecasts Philippe Laureta,∗, Mathieu Davida, Pierre Pinson b aUniversity of La R´eunion - PIMENT laboratory, 15, avenue Ren´e Cassin, 97715 Saint-Denis bTechnical University of Denmark, Centre for Electric Power and Energy, 2800 Kgs. Lyngby, Denmark Abstract We propose a framework for evaluating the quality of solar irradiance probabilistic fore- casts. The veriﬁcation framework is based on visual diagnostic tools and a set of scoring rules mostly originating from the weather forecast veriﬁcation community. Two types of probabilistic forecasts are used as a basis to illustrate the application of these veriﬁcation approaches. The ﬁrst one consists in ensemble forecasts commonly provided by national or international meteorological centres. The second one originates from statistical methods and produces a set of discrete quantile forecasts, the nominal proportions of which span the unit interval. These probabilistic forecasts are evaluated for two selected sites that experience very diﬀerent climatic conditions. The ﬁrst site is located in the continental US while the second one is situated on La R´eunion Island. Although visual diagnostic tools can help identify deﬁciencies in generated forecasts, it is recommended that a set of numerical scores be used to assess the quality of probabilistic forecasts. In particular, the Continuous Ranked Probability Score (CRPS) seems to have all the features needed to evaluate a probabilis- tic forecasting system and, as such, may become a standard for verifying solar irradiance probabilistic forecasts and by extension probabilistic forecasts of solar power generation. Keywords: probabilistic solar forecasting, evaluation framework, diagnostic tools, scoring rules, CRPS, Ignorance Score 1. Introduction1 Forecasts of solar energy generation are of utmost importance for eﬃciently integrating2 solar power generation into existing power grids and to decrease associated costs. Indeed,3 power production from photovoltaic (PV) or solar thermal plants is highly variable since4 weather dependent. Therefore, accurate knowledge of the future production from solar5 power generation capacities is necessary to limit the needs for additional balancing services6 and potentially storage. Therefore, increasing the value of solar power generation through7 the improvement of solar irradiance or PV power forecasting models (both usually referred to8 ∗corresponding author Email addresses: philippe.lauret@univ-reunion.fr (Philippe Lauret), mathieu.david@univ-reunion.fr (Mathieu David), ppin@elektro.dtu.dk (Pierre Pinson) Preprint submitted to Solar Energy November 6, 2019 as “solar forecasting models”) is of paramount importance. In the realm of solar irradiance9 forecasting, Global Horizontal Irradiance (GHI) is a prominent key variable. Therefore, this10 work will use this variable to illustrate the application of the proposed evaluation framework.11 Numerous works have been devoted to the development of models that generate point12 forecasts of solar power generation, commonly referred to as deterministic forecasts. Some13 of these models can be found in (Reikard, 2009; Dambreville et al., 2014; Marquez and14 Coimbra, 2011; Coimbra et al., 2013; Huang et al., 2013; Lauret et al., 2015; Voyant et al.,15 2017; Pedro and Coimbra, 2015; Lorenz and Heinemann, 2012). Furthermore, error metrics16 dedicated to evaluating the accuracy of these deterministic forecasts, like Mean Bias Error17 (MBE), Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) together with18 skill-score measures (Hoﬀ et al., 2013; Coimbra et al., 2013), are now quite standard and19 well accepted by the solar forecasting community.20 However, a forecast is inherently uncertain and in a context of decision-making faced by21 the grid operator, a point forecast plus an uncertainty (or, better say, prediction) interval is22 of genuine added value. Put diﬀerently, reliable probabilistic predictions may contribute to a23 more eﬃcient integration of intermittent sources in the energy network (Morales et al., 2014).24 Contrary to the wind power forecasting community where probabilistic forecasting appears25 to be a mature subject (Morales et al., 2014; Iversen et al., 2016; Jung and Broadwater,26 2014; Pinson et al., 2007), probabilistic solar forecasting is still in its infancy (Hong et al.,27 2016) albeit some recent works (Zamo et al., 2014; Sperati et al., 2016; Alessandrini et al.,28 2015; Grantham et al., 2016; Ben Bouall`egue, 2015; David et al., 2016; Golestaneh et al.,29 2016b) tend to moderate this statement.30 As mentioned by Pinson et al. (2007), the assessment of probabilistic forecasts is more31 complicated than for deterministic ones. Figures 1 and 2 show examples of GHI probabilis-32 tic forecasts. From the visual inspection of Figures 1 and 2, it is quite diﬃcult to state33 whether the prediction intervals are good or not. To objectively assess the performance of34 probabilistic forecasts and the methods used to generate those, it is necessary to employ35 appropriate diagnostic tools and quantitative scores.36 According to Murphy (1993), goodness of weather forecasts can be characterized by three37 types namely consistency, quality and value. Consistency is related to the correspondence38 between forecasters’ judgment and their forecasts. Quality refers to the correspondence39 between forecasts and the observations and value is linked to the beneﬁt (economical or40 others) gained from the use of these probabilistic forecasts in an operational context. In this41 work, we concentrate on the assessment of the quality of the models.42 Several attributes characterize the quality of probabilistic forecasts (Wilks, 2014; Jolliﬀe43 and Stephenson, 2003) but two main properties, i.e. reliability and resolution are used44 to measure the quality of the forecasts (Jolliﬀe and Stephenson, 2003). A third attribute45 namely sharpness can be used to evaluate how informative the forecasts are. In the weather46 forecasting veriﬁcation community, several diagnostic tools are used to characterize these47 required properties of reliability, resolution and sharpness. One can cite among others the48 reliability diagram (Pinson et al., 2010; Wilks, 2014) and rank histogram (Hamill, 2001;49 Wilks, 2014) for assessing reliability. Regarding forecasts of continuous variable, there is50 currently no visual tool to assess resolution. The sharpness property can be evaluated51 2 06:00 12:00 18:00 00:00 06:00 12:00 18:00GHI (W/m2) 0 250 500 750 1000 Desert Rock, March 25th and 26th 2012 Observations 95% 90% 80% 60% 40% 20% Prediction intervals Figure 1: Example of probabilistic solar irradiance forecasts: 2 days of measured GHI at the Desert Rock (NV) and associated day-ahead forecasts with prediction intervals provided by ECMWF-EPS (see section 3). 06:00 12:00 18:00 00:00 06:00 12:00 18:00GHI (W/m2) 0 250 500 750 1000 Tampon, August 12th and 13th 2012 Observations 80% 60% 40% 20% Prediction intervals Figure 2: Example of probabilistic solar irradiance forecasts: 2 days of measured GHI at Tampon and associated 1-hour ahead forecasts with prediction intervals generated with the Quantile Random Forest model QRF2 (see section 3). 3 through the use of sharpness diagrams (Pinson et al., 2007; Gneiting et al., 2007).52 In addition to these tools that permit to visually assess the attributes of a forecasting53 system, a metric called continuous ranked probability score (CRPS) (Hersbach, 2000) is54 commonly used by the weather forecasting community to objectively quantify the overall skill55 of the probabilistic forecasts. The CRPS is a metric capable of addressing both reliability56 and resolution simultaneously. Indeed, the CRPS can be decomposed into three components57 namely reliability, resolution and uncertainty. This decomposition provides a detailed picture58 of the performance of the forecasting methods (Hersbach, 2000) and consequently may help59 in the ranking of the probabilistic forecasts. A scoring rule originated from the information60 theory called the logarithm or ignorance score metric has also been proposed for assessing61 the quality of weather probabilistic forecasts (Roulston and Smith, 2002; Pinson et al., 2012).62 Although solar probabilistic forecasting is not as mature as wind probabilistic forecasting63 (Hong et al., 2016), some recent works (Alessandrini et al., 2015; Sperati et al., 2016; Zamo64 et al., 2014; Grantham et al., 2016; David et al., 2016, 2018; Chu and Coimbra, 2017;65 Golestaneh et al., 2016b; Verbois et al., 2018) proposed to assess the quality of the models66 with some classical diagnostic tools originated from the weather veriﬁcation community67 like rank histogram and reliability diagram. This literature review also revealed that the68 CRPS is a commonly used scoring rule. However, in our opinion, most of these works69 did not conduct a detailed analysis of how to use and interpret the veriﬁcation tools. For70 instance, the CRPS formula proposed by (Hersbach, 2000) is restricted to ensemble forecasts71 but David et al. (2018) and Lauret et al. (2017) used it to compute the CRPS of discrete72 quantile forecasts. Moreover, most of the previous works that evaluated the overall skill73 of competing methods through the use of the CRPS did not attempt to have a detailed74 performance of the methods which is possible from the decomposition of the CRPS into75 reliability, resolution and uncertainty. Besides, to our best knowledge, the ignorance score76 is not currently used by the solar forecasting community.77 In addition, other metrics are proposed to assess the properties of prediction intervals78 such as Prediction Interval Coverage Probability (PICP), Prediction Interval Normalized79 Averaged Width (PINAW) (Khosravi et al., 2013; Chu and Coimbra, 2017; Lauret et al.,80 2017). PICP is related to the reliability of the probabilistic forecasts while PINAW gives81 a measure of the sharpness of the predictive distributions. However, as discussed below,82 these two metrics (PICP and PINAW) are not the most appropriate for measuring the83 quality of interval forecasts. It is also worth noting that a metric called coverage width-84 based criterion (CWC), which assesses the quality of the prediction intervals by combining85 PICP and PINAW has been proposed by (Khosravi et al., 2013). But as demonstrated by86 (Pinson and Tastu, 2014), this score can lead to possible misinterpretations of the results.87 Unfortunately, some researchers in the solar community (Scolari et al., 2016; Chu et al.,88 2015; Li et al., 2018) recently used this metric to assess the quality of their forecasting89 models. Furthermore, the CWC score has been recently cited in a reference paper (Yang90 et al., 2018) and a review paper (van der Meer et al., 2018).91 This is why, we think that now is the time to take stock on the evaluation metrics of92 solar probabilistic forecasts. The objective of this work is therefore to provide the forecasting93 solar community a comprehensive overview of diagnostic tools and scoring rules that can94 4 be used to assess the performance of probabilistic forecasting methods. In particular, we95 propose an evaluation framework that may help the user to consistently evaluate the quality96 of the models. In others words, this paper aims at explaining how one should assess the97 quality of the probabilistic forecasts and how diagnostic tools and scores should be used and98 interpreted. In addition, we will propose a measure of resolution (through the decomposition99 of the CRPS) as this attribute is not currently assessed in the literature.100 In this paper, two types of GHI probabilistic forecasts are used to illustrate the pro-101 posed veriﬁcation framework. The ﬁrst one is the ensemble forecast commonly provided102 by Ensemble Prediction Systems (EPS) of the Numerical Weather Predictions (NWP) of103 meteorological utilities such as ECMWF. The second one, denoted by quantile forecasts,104 is based on statistical methods and produces a set of quantiles spanning the unit interval.105 Both types generate forecasts represented by predictive distributions that can be modelled106 either by a Cumulative distribution function (CDF) or a Probability distribution Function107 (PDF).108 Finally, note that in this paper, we restrict ourselves to the univariate context that corre-109 sponds to probabilistic forecasts that do not take into account spatio-temporal dependencies110 that are generated by stochastic processes like for instance cloud passing. The interested111 reader is referred to (Golestaneh et al., 2016a) who proposed a method to capture the112 spatio-temporal correlations in PV forecasts.113 The remainder of this paper is organized as follows. Section 2 deﬁnes the probabilistic114 forecast as the estimation of a predictive distribution of the variable of interest (GHI in115 our case). Section 3 presents the two sites that will serve as support for the application of116 the veriﬁcation tools on quantile and ensemble forecasts while Section 4 lists the properties117 required for skillful probabilistic forecasts. Section 5 presents in details the veriﬁcation tools118 and illustrates their application on quantile and ensemble forecasts. Finally, section 6 gives119 some concluding remarks.120 2. Nature of probabilistic forecasts of continuous variables121 Probabilistic forecasts correspond to the estimation of the statistical distribution of a122 future event. Thus, a probabilistic forecast may be deﬁned as a cumulative distribution123 function (CDF) F of a random variable X, such that F (x) = P r(X ≤ x). This CDF can be124 summarized by a set of quantiles. The quantile qτ , at probability level τ ∈ [0, 1], is deﬁned125 as follow126 qτ = F −1(τ ) = inf{x : F (x) ≥ τ }. (1) A quantile qτ informs there is a probability τ that the event x materializes below that127 quantile qτ . From a set of quantiles, prediction intervals (PIs) can be deduced. PIs deﬁne the128 range of values within which the observation is expected to be with a certain probability i.e.129 its nominal coverage rate (Pinson et al., 2007). To completely determine a PI, it is necessary130 to choose the way it should be centered on the probability density function (Pinson et al.,131 2007). The most common way is to center the PI on the median. Consequently, there is132 the same probability of risk below and above the median. Therefore, a central PI with a133 5 coverage rate of (1 − α)100% is estimated by using the α/2 quantile (ˆqτ =α/2) as the lower134 bound and the (1 − α/2) quantile (ˆqτ =1−α/2) as the upper bound. More precisely, a PI with135 (1 − α)100% nominal coverage rate is given by136 ̂P I (1−α)100% = [ˆqτ =α/2, ˆqτ =1−α/2] . (2) In the realm of weather predictions, three ways to deﬁne this cumulative distribution137 are available: parametric CDFs, discrete estimates of a CDF via a non-parametric method138 and ensemble forecasts. Parametric CDFs are easy to set up and to assess. Nevertheless,139 regarding solar forecasts, they are seldom proposed in the literature because they suﬀer140 from a lack of calibration. Indeed, the distribution of future observations of the solar power141 can not be accurately reproduced by a single probabilistic law. David et al. (2016) gave an142 example with the GARCH model that assumes a Gaussian distribution.143 An alternative to the parametric approach is the generation of discrete estimates of a144 CDF. This non-parametric method allows deﬁning a predictive CDF without any assumption145 on the distribution of the future event. The forecast is provided as a set of quantiles spanning146 the unit interval. This kind of probabilistic forecast is also called quantile forecasts (Pinson147 et al., 2007). The Global Energy Forecasting Competition 2014 (GEFCom 2014) (Hong148 et al., 2016) is a good example of this approach. Indeed, the solar forecasts were to be149 expressed in the form of 99 quantiles with various nominal proportions between zero and150 one. Widely used statistical models, like Quantile Regressions (QR) or Gradient Boosting151 Decision Trees (GBDT) can estimate these predictive distributions.152 The last type corresponds to ensemble forecasts classically generated by Numerical153 Weather Predictions (NWP) models. The distribution of the future event is given by an154 ensemble of members that are not directly linked to the notion of quantiles. For example, in155 the case of a NWP model, an ensemble forecast corresponds to a perturbed set of forecasts156 computed by slightly changing the initial conditions of the control run and of the modeling157 of unresolved phenomena (Leutbecher and Palmer, 2008). This ensemble prediction system158 (EPS) allows representing the uncertainties of the prediction scheme. Nevertheless, ensem-159 ble forecasts can be seen as discrete estimates of a CDF when they are sorted in ascending160 order. In the literature, diﬀerent ways to associate these sorted members to cumulative161 probabilities are proposed. Considering M sorted members of an ensemble E = (e1, ..., eM ),162 the most common deﬁnition in the domain of weather forecast assessment states that there163 is a probability of 1/M that the observation falls between two consecutive members ej and164 ej+1 (Anderson, 1996; Hersbach, 2000). If we assign a null probability for future events that165 fall outside the ensemble (i.e. xobs < e1 or xobs > eM ), the predictive distribution can be166 seen as a piecewise constant function167 ̂F (x) = M∑ k=1 αkH(x − ek). (3) H is the Heaviside function which is 1 if the argument is positive and zero otherwise.168 The weight αk = 1/M corresponds to the jump of probability that happens when x = ek.169 6 e1 e2 e3 e4Cumulative probability 0 0.25 0.5 0.75 1 1 M 1 M 1 M 1 M Classic (a) e0 e1 e2 e3 e4 eM+1Cumulative probability 0 0.125 0.375 0.625 0.875 1 1 2M 1 M 1 M 1 M 1 2M Non-uniform (b) e0 e1 e2 e3 e4 eM+1Cumulative probability 0 0.2 0.4 0.6 0.8 1 1 M+1 1 M+1 1 M+1 1 M+1 1 M+1 Uniform (c) Figure 3: Diﬀerent deﬁnitions of the CDF derived from an ensemble forecast (M = 4): (a) classical; (b) non-uniform spacing of the cumulative probabilities and a linear interpolation between the members; (c) uniform spacing and a linear interpolation between the members. Figure 3(a) gives a visual representation of this classical deﬁnition of a CDF derived from170 an ensemble with 4 members (M = 4).171 In the case of continuous variable, as the solar irradiance (GHI), the shape of the CDF172 resulting from the preceding deﬁnition is obviously not realistic. Several works (Br¨ocker,173 2012; Roulston and Smith, 2002; Pinson et al., 2010) proposed alternative approaches to174 face this issue. Among others, these alternatives allow deﬁning a continuous predictive175 distribution and non-null probabilities outside the ensemble. We brieﬂy present two other176 ways to build a CDF from an ensemble forecast.177 First, Br¨ocker (2012) proposes to preserve a jump of 1/M between two members but to178 assign a probability mass of 1/2M for the events that fall outside of the ensemble. It results179 in a non-uniform partition of the probability space [0; 1]. Figure 3(b) gives an example of180 this deﬁnition for an ensemble with 4 members (M = 4) and a linear interpolation between181 the members. The tails of the distributions are bounded by e0 and eM +1. The choice of182 these limits are arbitrary. For continuous variables, Roulston and Smith (2002) proposed183 to use the minimum and the maximum of the climatology. Notice that this non-uniform184 deﬁnition amounts to consider each ensemble member i as a quantile with probability level185 τ (i) = i−0.5 M .186 The second approach, described by (Pinson et al., 2010; Br¨ocker, 2012), assigns a prob-187 ability mass of 1/(M + 1) between two members and for the events that fall outside of the188 ensemble. Note that using this deﬁnition that an ensemble member can be interpreted as189 a quantile forecast by considering its rank within the ensemble. The probability level τ (i)190 associated with the member of rank i is deﬁned as: τ (i) = i M +1 . This approach leads to191 an uniform spacing of the cumulative probabilities. Figure 3(c) presents graphically the192 shape of the CDF when considering this last deﬁnition and a linear interpolation between193 the members. As for the non-uniform deﬁnition, the boundaries of the CDF, e0 and eM +1,194 are arbitrarily chosen (see appendix A for more details).195 Thus, when dealing with ensemble forecasts, three ways to build the CDF from the mem-196 bers are available. Unfortunately no deﬁnition can be favoured and each CDF construction197 has its pros and cons. The classic deﬁnition is the most used, speciﬁcally to compute the198 Continuous Rank Probability Score (CRPS, see section 5.3.1) with the methodology pro-199 posed by (Hersbach, 2000). As this commonly used deﬁnition assigns null probabilities to200 7 the events that fall outside of the ensemble, it can not be used to derive scores like ignorance201 (see section 5.3.4). The uniform and the non-uniform deﬁnitions requires to arbitrarily ﬁx202 the boundaries of the CDF. Therefore, they are user dependent. Nevertheless, they allow203 designing continuous CDF that contains all the possible events. Thus, the procedure used to204 verify the quality of ensemble forecasts can be exactly the same as for the parametric CDFs205 or for the predictive distributions summarized by discrete quantiles estimated by some kind206 of statistical method. Br¨ocker (2012) showed that the non-uniform deﬁnition corresponds207 to a minimization of the CRPS. But, considering this deﬁnition, the optimal shape of the208 corresponding rank histogram (see section 5.2.2) is not ﬂat. Indeed for this visual veriﬁca-209 tion tool, the height of the ﬁrst and last ranks should be the half of the other ones. Finally,210 if the aim is to compare diﬀerent forecasting models, whatever the chosen deﬁnition, the211 ranking will remain the same. Nevertheless, a unique framework has to be deﬁned to allow212 the comparison of diﬀerent works.213 3. Illustrative case studies214 Two sites will serve as benchmarks for the application of the diﬀerent tools and scores215 described below. The ﬁrst site, Desert Rock (USA), has an arid climate with a very sunny216 and stable sky. The second site, Tampon (R´eunion island), is located in a tropical island217 and experiences a very variable sky. The experimental dataset corresponds to two consec-218 utive years of recorded data of global horizontal irradiance (GHI). Table 1 gives detailed219 information about the data. The solar variability, quantiﬁed by the standard deviation of220 the changes in the clear sky index σ∆kt∗ (Hoﬀ and Perez, 2012), is the main diﬀerence221 between the two considered locations. We intentionally chose these two sites. Indeed, the222 solar variability is a key factor in the accuracy of deterministic forecasts. The higher the223 variability, the less accurate the forecasts are (Lauret et al., 2015). Finally, to build some224 of the models used in this work, we used the ﬁrst year of data (2012) as training set and225 the second year of data (2013) as testing set. Therefore, all the metrics and visual tools226 presented hereafter are derived from the testing set.227 Two forecasting time horizons will be addressed in this work. First, intra-day forecasts228 with lead times ranging from 1 to 6 hours will be appraised. These forecast are provided229 by state of the art forecasting models that generate predictive distributions from a set230 of quantiles spanning the unit interval. Second, day-ahead probabilistic forecasts will be231 studied. Generated by Numerical Weather Predictions (NWP) models, they are provided232 as ensemble forecasts.233 3.1. Intraday quantile forecasts234 Regarding intraday quantile forecasts, the quality of four state-of-the-art probabilistic235 models will be appraised. In this paper, we will not give the details of the implementation236 of these models as they have already been described in previous works (David et al., 2018;237 Pedro et al., 2018). In addition, we recall that the goal here is to illustrate the application238 of the proposed evaluation framework and not to have a detailed evaluation of these models.239 8 Table 1: Main characteristic of the solar measurements Desert Rock (USA) Tampon (R´eunion) Provider SURFRAD PIMENT Position 36.6N, 116.0W 21.3S, 55.5E Elevation 1007m 550m Cimate type Arid Insular tropic Period of record 2012-2013 2012-2013 Annual solar irradiation 2.105 MWh/m2 1.712 MWh/m2 Solar variability 1-h (σ∆kt∗) 0.146 0.241 Mean GHI (Testing set) 548 W/m 2 458 W/m2 Uncertainty component of the CRPS 29.1% 33.1% The selected models are based on two quantile regression techniques namely the quantile240 regression forest (QRF) and the Gradient Boosting (GB) techniques. Brieﬂy, the proposed241 techniques estimate directly the set of quantiles from a regression model Y = f (X) that242 relates the response variable Y (here GHI for lead time h = 1, 2, · · · , 6 hours) to a set of243 predictor variables (X). Two variants of regression models with diﬀerent sets of predictor244 variables are built. For the ﬁrst variant described in (Lauret et al., 2017), the vector of245 explanatory variables X consists of the actual measurement plus ﬁve past ground measure-246 ments while the second one takes as additional inputs two geometrical solar features related247 to the course of the sun in the sky namely the cosine of the zenith angle (cos(SZA)) and248 the cosine of the hour angle (cos(HA)). The adding of the two variables originates from249 the following reasons. First, some authors (Grantham et al., 2016; Lorenz and Heinemann,250 2012) showed a clear dependency of the forecasting error in relation to SZA. Second, we251 expect that the hour angle will bring some information regarding the asymmetry of the sky252 conditions between mornings and afternoons. This may be hold particularly for site like Le253 Tampon that experiences such a dichotomy between mornings and afternoons. Table 2 lists254 the acronyms of the resulting four quantile regression models.255 Table 2: Acronyms related to the four quantile regression models Quantile regression techniques Variant 1 Variant 2 Quantile Regression Forest QRF1 QRF2 Gradient Boosting GB1 GB2 3.2. Day-ahead ensemble forecasts256 The day-ahead ensemble predictions are provided by the Integrated Forecasting System257 (IFS) of the European Centre of Medium-Range Weather Forecasts (ECMWF). We will258 denote these ensemble forecasts as “ECMWF-EPS”. They consist in 50 perturbed members.259 The temporal resolution is of 3 hours and the spatial resolution is of 0.2 ◦ in both longitude260 9 and latitude. Consequently, 3h GHI (in W h/m 2) times series recorded on-site are compared261 with the nearest ECWMF pixel. In addition, we also propose a post-processed version of262 the original ECMWF-EPS forecasts. Indeed, the ensemble prediction systems of the NWP263 models commonly suﬀer from a lack of spread (Leutbecher and Palmer, 2008). To face264 this issue, Sperati et al. (2016) proposed a simple approach, named Variance Deﬁcit (VD),265 to calibrate the ensemble forecasts. Their method spreads the initial ensemble forecasts266 by correcting their variance. The correction factor is evaluated from a training set. The267 calibrated ensemble forecasts will be denoted by “ECMWF-EPS + VD”.268 4. Required properties for a skillful probabilistic system269 As mentioned in the introduction, two main attributes (reliability and resolution) char-270 acterize the quality of probabilistic forecasts (Pinson et al., 2007). The evaluation of these271 two attributes can be complemented by a sharpness assessment.272 4.1. Reliability273 Reliability or calibration refers to the statistical consistency between the forecasts and274 the observations. In other terms, the nominal coverage rate of the prediction intervals should275 be equal to the empirical one (e.g. a 90% PI should cover 90% of the observations). The276 reliability property is an important prerequisite as non reliable forecasts would lead to a277 systematic bias in subsequent decision-making processes (Pinson et al., 2007).278 4.2. Resolution and sharpness279 Resolution measures the capacity of a forecasting model to issue forecasts that are case-280 dependent. This important property, which is not easy to catch, is commonly not considered281 by the solar forecasting community. To understand concretely what resolution is, we will282 ﬁrst deﬁne the climatological forecast (i.e. climatology). Imagine a distribution built from283 all the available past data of the parameter to forecast. The climatological forecast uses284 this unique distribution to forecast any future events. A high resolution forecasting system285 generates forecasts that diﬀer from the climatology and, as a consequence, forecasts that are286 signiﬁcantly diﬀerent from each other. Climatological forecasts are perfectly reliable though287 having no resolution. Consequently, a skillful probabilistic forecasting system should issue288 reliable forecasts and with high resolution.289 Sharpness evaluates how informative the forecasts are. Practically, sharpness refers to290 the concentration of the predictive distributions (Pinson et al., 2007; Gneiting et al., 2007)291 and can be measured by the average width of the prediction intervals. Unlike the two292 previous attributes, sharpness is a function of the forecasts only and does not depend on293 the observations. Consequently, a forecasting system can produce sharp forecasts yet being294 useless if those probabilistic forecasts are not reliable.295 Unlike resolution and reliability, the sharpness property can be intuitively assessed. As296 an example, the ﬁrst day of Figure 1 well illustrates an extremely sharp forecasts with297 narrow prediction intervals. Conversely, the second day of Figure 2 shows a example of a298 low sharpness forecast with large predictions intervals.299 10 It must be emphasized here that these two components (sharpness and resolution) have300 diﬀerent interpretations according a meteorologist’s point of view or a statistician’s point301 of view. In the meteorological literature (Wilks, 2014; Jolliﬀe and Stephenson, 2003), the302 sharpness property refers to the ability of a forecasting system to generate forecasts that are303 able to deviate from the climatological value of the variable to predict (also called predictand)304 whereas from a statistical point of view the sharpness property relates to the concentration305 of the predictive distributions (Pinson et al., 2007; Gneiting et al., 2007).306 Similarly, from a meteorological point of view, resolution measures the ability of a fore-307 casting system to produce predictive distributions conditioned by the value of the predictand308 (i.e. forecasts that are case-dependent) (Pinson et al., 2007). From a statistical point of309 view, resolution amounts to evaluate the capacity of the forecast system to produce diﬀerent310 density forecasts depending on the forecast conditions (i.e. the predictive distributions are311 not only conditioned by the value of the predictand) (Pinson et al., 2007). For instance, the312 prediction intervals may exhibit increasing widths with increasing forecast horizon. Also,313 regarding the solar irradiance (GHI), the width of the PIs may vary according the sun’s314 position in the sky - see for the instance the work of (Grantham et al., 2016). In this work,315 we will not provide such a conditional assessment. Instead, we will propose a measure of316 resolution through the decomposition of the CRPS. From a meteorological perspective, it is317 also worth noting that, for perfectly reliable forecasts, sharpness is identical to resolution.318 In this work, we will clearly distinguish the deﬁnition of sharpness and resolution. That is to319 say, sharpness will refer to the concentration of the prediction intervals while resolution will320 quantify the ability of the forecasting system to generate conditional predictive distributions.321 Finally, it must be noted that reliability can be improved by means of statistical techniques322 also called calibration techniques (Gneiting et al., 2005), whereas this is not possible for323 resolution.324 5. Presentation and application of the veriﬁcation tools325 5.1. Proposed evaluation framework326 Diagnostic tools are used to visually assess the quality of probabilistic forecasts, while327 numerical scores are used to quantify the skills of a forecasting system and to rank competing328 prediction methods. Tables 3 and 4 summarize the diagnostic tools and scoring rules used to329 evaluate probabilistic forecasts generated either by ensemble methods or quantile techniques.330 Regarding pros and cons, and also the most common approaches already used in other ﬁelds331 (i.e. weather forecast veriﬁcation and wind power forecasting), we propose to diﬀerentiate332 the methodologies and the tools to assess the quality of quantile forecasts and ensemble333 prediction systems (EPS).334 Considering quantile forecasts, we advise to visually assess the quality of the forecasts335 using reliability diagrams with consistency bars. Then, to use the CRPS and its related336 decomposition as described in appendix C to quantify the overall performance of the methods337 and to measure the reliability and the resolution components.338 For ensemble forecasts, we propose to use the rank histogram including consistency bars339 and the CRPS as deﬁned by (Hersbach, 2000) (see appendix B) to respectively qualify and340 11 Table 3: Visual diagnostic tools. Diagnostic to ol Initially designed for Pros Cons Remarks Reliabilit y Diagram (RD) Reliabilit y assessmen t of quan tile forecasts -Departure from p erfect reliabilit y (ideal diagonal line) easily visualized - Easy to build Finiteness of the data and p ossible presence of serial correlation in sequence of observ ations/forecasts can cause devia tions from the ideal line ev en for reliable forecasts. This issue can b e mitigated b y plotting RD with consistency bars Can b e used for Ensem ble if mem b ers are assigned sp eciﬁc probabilit y lev els (uniform/non uniform CDF - see section 2) Rank Histogram (RH) Reliabilit y assessmen t of Ensem ble forecasts - Easy to build - Statistical consisten cy of the ensem ble quic kly c hec k ed (ﬂat RH) - Easy detection of deﬁciencies in ensem bl e calibration suc h as bias, under or o v er-disp ersion - As for RD, sensitivit y t o the ﬁniteness of the data (Need to dra w RH with consistency bars) - Caution: a ﬂat RH do es not imply a reliable forecast Can b e extended to quan tile forecasts if quan tiles are ev enly spaced PIT histogram (PIT) Reliabilit y assessmen t of quan tile forecasts - Departure from p erfect reliabilit y easily assessed - Calibration o f predictiv e CDF easily c hec k ed (ﬂat PIT histogra m) - Lik e RH, easy detection of calibration deﬁciencies - Sub ject to the ﬁnitene s s of the data (plot with con s i s t ency bars advised) - Need to sp ecify t he n um b e r of histograms bins. - Require the computation of the predictiv e CDF - In terp olation n eeded b et w een the discrete quan tiles to estimate the v alue the CDF attains at the observ ation. - As for RH, a ﬂat PIT is not a suﬃcien t condition to state that a forecast is reliable Can b e used for Ensem ble (un iform CDF) Sharpness diagram Ensem ble and quan tile forecasts - Easy to build - Sharpness is an in tuitiv e prop ert y that p ermits to asesss the con cen tration of the predictiv e distributio ns. - Sharpness diagr ams m ust b e i n terpreted with care b ecause they are only relev an t if the asso ciated forecasts are reliable. - Sharpness can only con tribute to a qualitativ e ev aluation of the probabilistic forecasts. - Ev en if narro w PIs are preferr ed, sharpness cannot b e seen as a prop ert y to v erify the qua lit y of probabilistic forecasts but more lik e the consequence of a high r esolution. - Can b e used fo r Ensem ble (uniform/non uniform CDF) Table 4: Scoring Rules Scores Pros Cons Remarks CPRS CRPS has the same dimension a s the v ariable to predict and can b e normalized . Therefor e, it p ermits comparisons b et w een diﬀeren t datasets. F or deterministic forecasts, CRPS turns to b e the MAE (Mean Absolute Error). Th us, the p erformance of a probabilistic metho d can b e compared against a deterministic one. Decomp osition of the CRPS in to reliabilit y and resolution p ro vides add itional insigh t in to the p erformance of a probabilistic m o del. As a non-lo cal score, CRPS is a ro bust score. No analytic form ulae except for sp eciﬁc distribution s (Gaussian, Studen t’s t, ...) - See R pac k age scoringRules for details. CRPS a v erages o v er the complete range of forecast thresholds. Consequen tly , deﬁciencies in diﬀeren t parts of the distributions (e.g. the tails of the distribution) can b e hidden. Sp eciﬁc form ulae for Ensem ble forecasts prop osed b y Herbasc h (see App endix A). Can b e calculated through n umerical in tegration (see Equ ation 6) but requires in terp olation of uniform/non uniform CDF. Can b e also computed through in tegration of the Brier Score (see App endix C) Ignorance Score Easy to compute esp ecially for Ensem ble forecasts. No detailed inform ation regarding the p erformance of a forecasting system as IGN cannot b e decomp osed in to reliabilit y and resolution. No sites’ comparisons can b e carried out as IGN cannot b e normalized. As a lo cal score, and as suc h, sensitiv e to the form of the predictiv e PDF, IGN is less robust than the CRPS. IGN cannot b e applied to predictiv e PDF with n ull probabilities. Sp eciﬁc form ula for Ensem b le forecasts prop osed b y Roulston assuming a linear in terp olation of the CDF b e t w een the mem b ers (see Equation 12). Otherwise, requires computation of the predictiv e PDF to estimate the v alue the PDF attains at the observ ation (requires in terp olation of uniform/no n uniform CDF). Quan tile Score QS p ermits to obtain detailed infor mation ab out the forecast qualit y of sp eciﬁc quan tiles that are of great in terest for the user. QS can b e decomp osed in to reliabil it y and resolution. Score restricted t o a sp eciﬁc quan tile. Cannot b e used to rank diﬀeren t forecast metho ds considering their o v erall p erformance. QS can rev eal deﬁciencie s in diﬀeren t parts of the predictiv e distribution (e.g. tails of the distribution) In terv al Score V ery easy to compute. IS has the same dimension as the v ariable to predict and can b e normalized . Cannot b e decomp osed in to reliabilit y and resolu tion. Designed sp eciﬁcally for in terv al forecasts 12 quantify the performances of the EPS. Indeed, these two tools does not require additional341 assumptions (i.e. to deﬁne the nature of the distribution and its boundaries) and they are342 already widely used.343 For both type of forecasts, ignorance score (IGN), interval score (IS), quantile score (QS)344 and sharpness diagrams can complement the characterization of the forecasting methods.345 However, sharpness diagrams must be interpreted with care because they are only relevant346 if the associated forecasts are reliable.347 Finally, if interval score, quantile score and sharpness diagrams are computed for ensem-348 ble forecasts, it is important to clearly indicate the assumption done to obtain the quantiles349 (e.g. uniform or non-uniform spacing).350 In the following sections, we will present in detail the veriﬁcation tools. Throughout the351 description, we will provide illustrations of the application of these tools to quantile and352 ensemble forecasts.353 5.2. Diagnostic tools354 5.2.1. Reliability diagram355 The reliability diagram is a graphical veriﬁcation display used to evaluate the reliability356 of the probabilistic forecasts. In this paper, we follow the methodology deﬁned by (Pinson357 et al., 2010) that is especially designed for predictive distributions summarized by quantile358 forecasts. More precisely, quantile forecasts are reliable if their nominal proportions are equal359 to the proportions of the observed value. It means that, over an evaluation set of signiﬁcant360 size, (statistically) the diﬀerence between observed and nominal probabilities should be as361 small as possible (Pinson et al., 2010). Notice that for ensemble forecasts, the uniform CDF362 or non uniform CDF (see section 2) must be chosen before applying this methodology.363 This representation is attractive since the deviations from perfect reliability (i.e. the364 diagonal line) can be easily visualized (Pinson et al., 2010). Nonetheless, due to the ﬁnite365 sample of pairs of observation/forecast and also due to possibly serial correlation in the366 sequence of forecasts and observations, it is possible that observed proportions are not367 exactly along the diagonal, even if the forecasts are perfectly reliable. (Pinson et al., 2010).368 In other words, reliability diagrams can be misinterpreted since even for perfectly reliable369 forecasts, deviations from the ideal diagonal case can be observed.370 To deal with the issue of limited number of pairs of observation/forecast, Br¨ocker and371 Smith (2007a) built reliability diagrams with consistency bars. In addition, Pinson et al.372 (2010) have proposed consistency bars taking into account the combined eﬀect of serial cor-373 relation and limited data. Interpretation of reliability diagrams with consistency bars is374 that one cannot reject the hypothesis of the quantile forecasts being reliable if the observed375 proportions lie within the consistency bars. In practice, adding consistency bars to the relia-376 bility diagrams may reinforce the user’s (possibly subjective) judgment about the reliability377 of the diﬀerent models.378 Finally, some preceding works (Chu and Coimbra, 2017; Lauret et al., 2017) proposed379 to evaluate the reliability component of a probabilistic system by calculating the prediction380 interval coverage probability (PICP) (Khosravi et al., 2013). PICP permits one to assess381 the empirical coverage probability of the central prediction intervals. However, this metric382 13 is not suitable to assess the reliability of probabilistic forecasts because as noted by Pinson383 et al. (2007), both quantiles that deﬁne the prediction interval may be biased. In other384 words, PICP it is not suﬃcient to check if the nominal coverage of the intervals is respected.385 It is also necessary to verify that both quantiles deﬁning the PI are unbiased. Nominal proportion 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Observed proportion 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 DESERT ROCK ideal GB1 QRF1 GB2 QRF2 (a) Nominal proportion 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Observed proportion 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 TAMPON ideal GB1 QRF1 GB2 QRF2 (b) Figure 4: Reliability diagrams related to the intra-day quantile forecasts. (a) Site of Desert Rock (b) Site of Le Tampon. Consistency bars for a 90% conﬁdence level around the ideal line are individually computed for each nominal proportion. 386 In order to visually assess the reliability of quantile forecasts, Figures 4(a) and 4(b) plot387 the reliability diagrams (averaged over all the forecasting horizons) for the two selected sites.388 Consistency bars for a 90% conﬁdence level are individually computed for each nominal pro-389 portion. From the visual inspection of the reliability diagrams of Desert Rock, one can390 possibly state that the GB1 and GB2 models are reliable as the observed proportions of all391 quantiles lie within the consistency bars. Conversely, for QRF1 and QRF2 models, observed392 proportions of some quantiles lie outside the consistency bars. In particular, quantile fore-393 casts generated by the QRF2 model should not be considered reliable. In addition, notice394 the particular signature of the QRF2 model that corresponds to an over dispersed predic-395 tive distribution (i.e. an underconﬁdent model). For the site of Le Tampon, it seems that,396 except the GB2 model, all the other models lead to possible reliable quantile forecasts since397 all of their observed proportions lie within the consistency bars. At this stage, the visual398 reliability assessment related to Le Tampon is not conclusive. This is why we recommend399 in a second step the use of proper score like the CRPS (and its related decomposition) to400 quantify objectively the performance of the methods. This will permit a clear cut ranking401 of the diﬀerent models.402 5.2.2. Rank histogram403 The rank histogram is a graphical display initially designed for assessing ensemble fore-404 casts (Wilks, 2014). But, it can be extended to quantile forecasts by assuming that all405 14 evenly spaced forecasted quantiles form an ensemble. Rank histograms permit to assess the406 statistical consistency of the ensemble, that is, if the observation can be seen statistically407 just like another member of the ensemble (Wilks, 2014). A ﬂat rank histogram is a neces-408 sary condition for ensemble consistency and shows an appropriate degree of dispersion of409 the ensemble. Put diﬀerently, the ﬂatness of the rank histogram indicates that the ensemble410 members are statistically indistinguishable from the observations (Wilks, 2014). An under-411 dispersed ensemble (i.e. ensemble dispersion consistently too small) leads to a U-shape rank412 histogram and shows that the observation will often be an outlier in the distribution of413 ensemble members. EPS, such as ECMWF-EPS, are known to suﬀer from a lack of spread.414 As a consequence the resulting rank histograms (Figures 5(a) and 6(a)) exhibit a U-shape.415 Conversely, an over-dispersed ensemble (i.e. ensemble dispersion consistently too large)416 gives a hump shape rank histogram and indicates that the observation may too often be in417 the middle of the ensemble distribution.418 In addition, rank histograms can also detect deﬁciencies in ensemble calibration or relia-419 bility (Wilks, 2014). For instance, some unconditional biases can be revealed by asymmetric420 (triangle shape) rank histograms. Furthermore, overpopulation of the smallest (resp. high-421 est) ranks will correspond to an overforecasting (resp. underforecasting) bias. Such a bias422 can be observed in ﬁgures 5(b) and 6(b). Indeed the calibration with the VD method reduces423 the under-dispersion but an overforecasting bias appears for both sites as a large number of424 the smallest ranks remain above the consistency bars. It must be stressed that one should425 be cautious when analyzing rank histograms. Indeed, as shown by (Hamill, 2001), a perfect426 ﬂat rank histogram does not state that the corresponding forecast is reliable. Further, when427 the number of observations is limited, consistency bars can also be calculated with the pro-428 cedure proposed by (Br¨ocker and Smith, 2007a). To build a rank histogram, it is necessary429 to ﬁnd the rank of the observations when pooled within the ordered ensemble and then plot430 the histogram of the ranks. For an ensemble of M members, the number of ranks of the431 histogram is M + 1. The histogram of veriﬁcation ranks will be uniform with theoretical432 relative frequency of 1 M +1 if the consistency condition is met.433 Finally, the two case studies (Figures 5 and 6) show that forecasts calibrated with the434 VD method are more reliable than the original ones. But as a large part of the ranks falls435 outside of the consistency bars the resulting forecasts can not be considered reliable.436 5.2.3. PIT histogram437 Although being at this stage redundant with the reliability diagram, we also present here438 the PIT histograms in order to discuss possible issues related with the use of this graphical439 tool. PIT histograms may help to assess the calibration property by verifying whether the440 observations can be seen as random samples of the predictive distributions (Gneiting et al.,441 2007). PIT histograms assess calibration of cumulative predictive distributions checking442 whether the observations can be considered as random samples of these distributions. Con-443 trary to rank histograms, PIT histograms require the computation of the predictive CDF.444 The PIT is the value that the predictive CDF has for a particular observation. PIT values445 can be calculated over a testing set of observations and one can then plot the histogram446 of the PIT values. Similarly to rank histograms, a ﬂat PIT histogram is a necessary but447 15 RankRelative frequency 10-2 10-1 100 ECMWF-EPS (a) RankRelative frequency 10-2 10-1 100 ECMWF-EPS + VD (b) Figure 5: Rank histograms for Desert Rock with consistency band for a 90% conﬁdence level of raw ECMWF- EPS (a) and ECMWF-EPS calibrated with Variance Deﬁcit (VD) method (b). RankRelative frequency 10-2 10-1 100 ECMWF-EPS (a) RankRelative frequency 10-2 10-1 100 ECMWF-EPS + VD (b) Figure 6: Rank histograms for Le Tampon with consistency band for a 90% conﬁdence level of raw ECMWF- EPS (a) and ECMWF-EPS calibrated with Variance Deﬁcit (VD) method (b) . 16 not suﬃcient condition to state that a forecast is reliable. As for rank histograms, depar-448 tures from ﬂatness is a sign of conditional biases in the forecasts or over/under-dispersion.449 Like rank histograms, consistency bars can be added to PIT histograms to see how much450 deviation from the ideal uniform line can be seen as acceptable, in view of sample size. Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 GB1 Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 QRF1 Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 GB2 Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 2 QRF2 (a) Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 GB1 Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 QRF1 Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 GB2 Probability Integral Transform 0 0.5 1Relative frequency 0 0.5 1 1.5 QRF2 (b) Figure 7: Assessment of the reliability of the intra-day quantile forecasts with PIT diagrams, (a) Site of Desert Rock (b) Site of Le Tampon. 451 Figure 7 shows the PIT histograms (averaged over all the lead times) related to the two452 sites. Following the preceding reliability analysis which possibly stated that, except the GB2453 model, all models were reliable for the site of Le Tampon (see Figure 4(b)), one may expect454 corresponding ﬂat PIT histograms for the GB1, QRF1 and QRF2 models (Figure 7(b)).455 However, this is not the case. We suspect that this may come from the fact that one needs to456 specify the number of histograms bins to plot the PIT histogram. In addition, interpolation457 is needed between the discrete quantiles to estimate the value the CDF attains at the458 observation. This may motivate the choice of reliability diagrams against PIT histograms459 for assessing calibration. However, it is worth noting that, in accordance with the reliability460 diagram, the PIT histogram of the QRF2 method for Desert Rock conﬁrms that this model461 corresponds to an over-dispersed forecasting system (i.e. too wide predictive distributions).462 5.2.4. Sharpness diagram463 A probabilistic forecast is sharp if prediction intervals are shorter on average than pre-464 diction intervals derived from na¨ıve methods, such as climatology or persistence.465 Similarly to Pinson et al. (2007), we propose to assess the sharpness of the predictive466 distributions by calculating the mean size of the central prediction intervals denoted by ¯δα 467 for diﬀerent nominal coverage rates (1 − α)%.468 This leads to a graphical veriﬁcation display called δ-diagrams. For an evaluation set of469 N forecasts, ¯δα is given by470 ¯δα = 1 N N∑ i=1 (ˆqτ =1−α/2 − ˆqτ =α/2) . (4) 17 Notice that Gneiting et al. (2007) proposed a diagnostic approach to evaluating proba-471 bilistic forecasts that is based on the paradigm of maximizing the sharpness of the predictive472 distributions subject to calibration. In the proposed evaluation framework,sharpness dia-473 grams take the form of box-plots of the width of the prediction intervals.474 As mentioned above, some researchers in the solar forecasting community used the475 PINAW metric to measure sharpness. This metric is the average width of the (1 − α)100%476 prediction interval normalized by the mean of variable x to predict (e.g. here GHI) for477 a testing set of N pairs of forecasts/observations. For a speciﬁc nominal coverage rate478 (1 − α)100%, PINAW reads as479 PINAW(α) = ∑N i=1 (ˆqτ =1−α/2 − ˆqτ =α/2) ∑N i=1 x . (5) However, even if it can be interesting to compare the performance of forecasting methods480 at diﬀerent locations, it must stressed that the sharpness is a property of the forecasts only481 and as such can not depend on the mean of the observations.482 For quantile forecasts, Figures 8(a) and 8(b) plot the ¯δα diagrams of the four models483 for diﬀerent coverage rates. It must be noted that the ¯δα values have been averaged over484 all the lead times. One may ﬁrst notice that prediction intervals are wider for the site of485 Le Tampon than for Desert Rock. As discussed in (Lauret et al., 2017), the variable sky486 conditions experienced by the site of Le Tampon have an impact on the shape of the predic-487 tive distributions. Conversely, the site of Desert Rock that experiences higher occurrences488 of clear and stable skies exhibits narrower prediction intervals. Nominal coverage rate (%) 20 30 40 50 60 70 80δα (W/m2) 20 40 60 80 100 120 140 160 180 200 GB1 QRF1 GB2 QRF2 (a) Nominal coverage rate (%) 20 30 40 50 60 70 80δα (W/m2) 50 100 150 200 250 300 350 400 450 GB1 QRF1 GB2 QRF2 (b) Figure 8: Sharpness diagrams of intra-day quantile forecasts for coverage rates ranging from 20% to 80% (a) Site of Desert Rock (b) Site of Le Tampon. 489 For both sites, it appears that the GB2 model leads to the lowest ¯δα values for all the490 forecasting horizons albeit the diﬀerence with the other models is less pronounced for the site491 of Desert Rock. At this point, the sharpness evaluation may favor the GB2 model for both492 18 Nominal coverage rate (%) 0 20 40 60 80 100/, Wh/m2 0 100 200 300 400 500 600 700 ECMWF-EPS ECMWF-EPS + VD (a) Nominal coverage rate (%) 0 20 40 60 80 100/, Wh/m2 0 200 400 600 800 1000 1200 1400 1600 1800 ECMWF-EPS ECMWF-EPS + VD (b) Figure 9: Sharpness diagrams for coverage rates ranging from 0% to 100% of ECMWF-EPS and ECMWF- EPS + VD for Desert Rock (a) and for Le Tampon (b). sites. However, while the GB2 model may possibly generate reliable forecasts for the Desert493 Rock site, this may not be the case for Le Tampon site. If one attempts to select the best494 approach for both sites by combining the two previous separate reliability and sharpness495 assessments, the picture is less clear. Hence evaluating separately reliability and sharpness496 and drawing conclusions on the sole examination of either one of these diagnostic tools may497 be misleading.498 Regarding ensemble forecasts, as none of the ensemble forecasts are reliable (see 5.2.2,499 there is normally no need to lead further investigations about the sharpness of the prediction500 intervals. Indeed, a comparison of the sharpness of the forecasts could lead to a misunder-501 standing. Nevertheless, we do it for this study case to illustrate this issue. Figure 9 shows502 sharpness diagrams for coverage rates ranging from 0% to 100%, for the two sites and for503 the two considered ensemble forecasts. To compute the mean size of the central prediction504 interval ¯δα, we assume an uniform spacing of the quantiles derived from the ensemble (see505 section 2). As shown by Figure 9, predictions intervals (PIs) of original ECMWF-EPS fore-506 casts are narrower than the calibrated ones. This is the consequence of the under-dispersion507 and therefore of the low reliability of the ECMWF-EPS forecasts. So, in this case, even508 if narrow PIs are prefered, sharpness diagrams should not be used as criteria to assess the509 quality of the forecasts. In the next section, we will use the CRPS and its related decompo-510 sition into reliability and resolution in an attempt to assess objectively and quantitatively511 the properties required for a skillfull probabilistic system.512 5.3. Scores513 Numerical scores provide summary measures for the evaluation of the quality of prob-514 abilistic forecasts (Gneiting and Raftery, 2007). Scoring rules are based on the predictive515 distribution of the forecast and on the observed value of the variable of interest. Scores516 may help to rank competing probabilistic models. Scores are required to be proper (Br¨ocker517 and Smith, 2007b; Gneiting and Raftery, 2007). A score is said to be proper if it insures518 19 that the perfect forecasts should be given the best score value. If it is not the case, one519 could then hedge the score, by ﬁnding tricks that permit to get better score values without520 attempting to issue better forecasts. More generally, employing a score that is not proper521 makes that one can never be sure of the validity of the results from an empirical comparison522 or benchmarking of rival approaches (Pinson and Tastu, 2014). The scoring rules proposed523 in this work (CRPS, Ignorance score, Interval score, quantile score) are proper. However,524 this is not the case of the CWC score discussed in section 1 as demonstrated by (Pinson and525 Tastu, 2014).526 In addition to the property of propriety, a score can be local or non-local. A score is said527 to be local if it depends only on the value of the predictive distribution at the observation,528 not on other features of the functional form of the predictive PDF.529 While diﬀerent proper scores have been proposed in the literature (Br¨ocker and Smith,530 2007b; Gneiting and Raftery, 2007), we focus here on proper scoring rules for probabilistic531 forecasts of continuous variables and particularly on the following scores: CRPS, Interval532 score, quantile score and Ignorance Score.533 Finally, it must noted that, in the following, the diﬀerent ﬁgures plot the relative counter-534 parts of the CRPS, Interval Score and Quantile Score. These relative metrics are normalized535 by dividing the absolute values by the mean of the GHI for the considered testing period536 (see Table 1).537 5.3.1. Continuous Rank Probability Score (CRPS) and its decomposition538 The CRPS measures the diﬀerence between the predicted and observed cumulative dis-539 tributions functions (CDF) (Hersbach, 2000). The formulation of the CRPS is540 CRP S = 1 N N∑ i=1 ∫ +∞ −∞ [ ˆF i f cst(x) − F i xobs(x)]2dx, (6) where ˆFf cst(x) is the predictive CDF of the variable of interest x (e.g. GHI) and Fxobs(x) is a541 cumulative-probability step function that jumps from 0 to 1 at the point where the forecast542 variable x equals the observation xobs (i.e. Fxobs(x) = 1{x≥xobs}). The squared diﬀerence543 between the two CDFs is averaged over the N forecast/observation pairs. The CRPS score544 rewards concentration of probability around the step function located at the observed value545 (Wilks, 2014). In other words, the CRPS penalizes lack of resolution of the predictive546 distributions as well as biased forecasts. In addition, for deterministic forecasts, the CRPS547 turns to be the MAE (Mean Absolute Error). This fact permits to compare directly the548 performance of a probabilistic model against a deterministic one or equivalently evaluate549 the added value brought by a probabilistic approach (Ben Bouall`egue, 2015). Notice that550 the CRPS is negatively oriented (smaller values are better) and the same dimension as the551 forecasted variable.552 For ensemble forecasts, Hersbach (2000) proposed a method to compute the CRPS using553 the classical deﬁnition of the CDF (see section 2 and ﬁgure 3(a)). In the realm of weather554 predictions, his method is widely used and at least embedded in one R-package (NCAR-555 Research applications laboratory, 2015). Appendix B summarizes the Hersbach’s method556 to compute the CRPS for ensemble forecasts.557 20 As mentioned above and as a proper score (Gneiting and Raftery, 2007), CRPS can be558 further partitioned into the two main attributes of probabilistic forecasts namely reliability559 and resolution. The decomposition of the CRPS leads to560 CRPS = RELIABILIT Y + U N CERT AIN T Y − RESOLU T ION. (7) The reliability term provides an estimation of the forecast biases while the resolution561 term quantiﬁes the improvement that results from issuing probability forecasts that are case562 dependent. The uncertainty term cannot be modiﬁed by the forecast system and depends563 only on the observations variability (Wilks, 2014). As the CRPS is negatively oriented, the564 goal of a forecast system is to minimize (resp. maximize) as much as possible the reliability565 term (resp. the resolution term). This decomposition of the CRPS may lead to a detailed566 picture of the performance of the forecasting methods.567 Regarding the calculation of these diﬀerent terms, two possibilities exist. The ﬁrst one568 is based on the work of (Hersbach, 2000) and as such best suited for ensemble forecasts rep-569 resented by the classical deﬁnition of the CDF. Appendix B gives the formulaes to calculate570 the three terms. The second possibility makes use of the fact that CRPS is the integral of571 the Brier Score over all the predictand thresholds. The Brier score is a proper score used572 to evaluate probabilistic forecasts of binary predictands (Wilks, 2014). Appendix C gives573 all the details regarding this second method. As the CRPS has the same unit as the vari-574 able to predict, it can be normalized by the mean (e.g. mean GHI) or the maximum (e.g.575 installed capacity) of the variable to forecast. The normalized CRPS permits to carry out576 comparisons between diﬀerent datasets (e.g. diﬀerent locations).577 Figures 10(a) and 10(b) plot the relative CRPS of the quantile forecasts in relation578 with the forecast horizon for the two considered sites. As expected, the performance of579 the models decreases as the lead-time increases (i.e. the lower the CRPS, the better the580 model). One also may note that the site of Le Tampon, which experiences variable sky581 conditions compared to Desert Rock, yields higher CRPS values. The interested reader is582 referred to (Lauret et al., 2017) where more details are given regarding the impact of the583 sky conditions on the quality of the probabilistic forecasts. As shown by Figures 10(a) and584 10(b), the two non linear models that include the two solar geometric predictors namely585 zenith angle and hour angle (i.e. GB2 and QRF2 models) perform clearly better than the586 variant 1 models regardless the site. Thus, it appears that adding the two solar geometric587 variables brings a clear improvement and especially for a site like Le Tampon which is known588 to experience a morning/afternoon sky asymmetry. Unlike the previous separate analysis of589 reliability and sharpness, CRPS establishes a clear-cut ranking of the models. However, some590 inconsistencies appear with the reliability analysis which showed that the the QRF2 model591 (resp. the GB2 model) was non reliable for Desert Rock (resp. for Le Tampon). Therefore,592 in order to gain a better understanding of the CRPS results, we use the decomposition of593 the CRPS depicted in Appendix C. This decomposition, detailed in Appendix D, shows594 that the reliability component makes a small contribution to the CRPS and that the higher595 quality of the variant 2 models comes from the resolution attribute.596 We close this subsection related to the CRPS with the CRPS skill score (CRPSS). In a597 similar manner that scores have been proposed to evaluate the skill of deterministic forecasts598 21 Forecast Horizon (h) 123456CRPS (%) 5.5 6 6.5 7 7.5 8 8.5 9 9.5 GB1 QRF1 GB2 QRF2 (a) Forecast Horizon (h) 123456CRPS (%) 14 15 16 17 18 19 20 21 22 23 24 GB1 QRF1 GB2 QRF2 (b) Figure 10: Relative (in % of mean GHI) CRPS of the diﬀerent intraday methods (a) Site of Desert Rock (b) Site of Le Tampon. The CRPS metric clearly shows the superiority of the variant 2 (GB2 and QRF2) models and particularly for Le Tampon. (Coimbra et al., 2013), (Pedro et al., 2018) used the CRPSS to gauge the performance of599 their probabilistic forecasting models against a reference easy-to-implement method i.e. the600 persistence ensemble (PeEn). In that case, the CRPSS reads as CRPSS = 1 − CRPSnew method CRPSPeEn .601 In this study, as our primary goal is to verify solar irradiance probabilistic forecasts and602 not to compare and rank forecasting models, we do not detail the implementation of the603 PeEn model. The interested reader should refer to (Pedro et al., 2018). However, as noted604 by (Yang, 2019), the previous deﬁnition of the CRPSS may lead to some misinterpretations605 of the skill score as the CRPS of the PeEn model varies according to certain parameters606 (e.g. number of members of the ensemble, forecast lead time, etc.). To address this issue,607 Yang (2019) proposed, instead of PeEn, a new baseline model called the complete-history608 PeEn (CHPeEn) model that gives a nearly constant CRPS.609 Another way to avoid a CRPSS that depends on the implementation of the reference610 model, and to beneﬁt from the decomposition of the CRPS mentioned above, is to use the611 uncertainty part of the CRPS as the baseline value. The uncertainty component corresponds612 to the CRPS of the climatology and is only sensitive to the observations variability and613 therefore, for a given location and temporal resolution of the data, does not depend on any614 other kind of parameters. Notice that, for meteorologists, when computing skill scores, the615 baseline model is commonly climatology.616 5.3.2. Interval Score (IS)617 Following Winkler (1972), Gneiting and Raftery (2007) proposed a proper score to specif-618 ically assess the quality of central (1 − α)100% prediction interval forecasts. This scoring619 rule called Interval Score (IS), averaged over the N pairs of forecasts and observations, is620 22 Forecast Horizon (h) 123456Interval Score (%) 35 40 45 50 55 60 65 GB1 QRF1 GB2 QRF2 (a) Forecast Horizon (h) 123456Interval Score (%) 85 90 95 100 105 110 115 120 125 GB1 QRF1 GB2 QRF2 (b) Figure 11: Relative (in % of mean GHI) Interval Score (IS0.2) (for 80% central prediction interval) of the diﬀerent intraday methods (a) Site of Desert Rock (b) Site of Le Tampon. This simple and very easy-to- compute scoring rule shows also that the variant 2 models outperform the variant 1 models. deﬁned by621 ISα = 1 N N∑ i=1 (U i − L i) + 2 α (L i − xi obs)1xi obs<Li + 2 α (xi obs − U i)1xi obs>U i, (8) where Li and U i represent respectively the α/2 lower quantile ˆqτ =α/2 and the 1 − α/2 upper622 quantile ˆqτ =1−α/2. As shown by Equation 8, the IS rewards narrow prediction intervals but623 penalizes (with the penalty term that depends on α) the forecasts for which the observation624 xobs is outside the interval.625 Figure 11 shows the IS score for the 80% central prediction interval. Again, variant 2626 models perform better than the other models. In our opinion, this easy-to-calculate score627 can advantageously complete the set of proper scores available to the user.628 5.3.3. Quantile Score (QS)629 Some users may be interested by the performance of some speciﬁc quantiles ( e.g. over-630 forecasting or underforecasting) and particularly those related to the tails of the predictive631 distribution. Quantile Score (QS) permits to obtain detailed information about the fore-632 cast quality at speciﬁc probability levels. A noted by (Bentzien and Friederichs, 2014), the633 CRPS averages over the complete range of forecast thresholds through integration of the634 Brier Score (see Appendix C). As a consequence, deﬁciencies in diﬀerent parts of the distri-635 bution, e.g. the tails of the distribution, might be hidden. Bentzien and Friederichs (2014)636 recommend to extend the veriﬁcation framework by calculating QS for diﬀerent probability637 levels. Notice also that, Bentzien and Friederichs (2014) proposed a decomposition of the638 QS into its reliability and resolution components.639 QS is based on an asymmetric piecewise linear function ψτ called the check or pinball loss640 function. The check function was ﬁrst deﬁned in the context of quantile regression (Koenker641 and Bassett, 1978) and is given by642 23 Probability level 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Quantile score (%) 1.5 2 2.5 3 3.5 4 4.5 5 5.5 GB1 QRF1 GB2 QRF2 (a) Probability level 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Quantile score (%) 4 6 8 10 12 14 16 GB1 QRF1 GB2 QRF2 (b) Figure 12: Relative (in % of mean GHI) Quantile Score of the diﬀerent intraday methods (a) Site of Desert Rock (b) Site of Le Tampon. QS permits to assess the performance of speciﬁc quantiles. For Desert Rock, the lowest quantiles are more penalized than the highest ones while for Le Tampon the intermediate quantiles exhibit higher scores. ψτ (u) = { τ u if u ≥ 0 (τ − 1)u if u < 0, (9) with τ representing the quantile probabilty level.643 QS is given by the mean of the check function applied to the N pairs of observations xi obs 644 and quantile forecasts for a speciﬁc probability level τ , ˆqi τ . QS reads as645 QS = 1 N N∑ i=1 ψτ ( xi obs − ˆqi τ ). (10) QS is negatively oriented (i.e. the lower, the better). Finally, notice that Br¨ocker (2012)646 showed that the CRPS can be seen as a weighted sum of quantiles scores applied to the647 quantiles derived from the non-uniform CDF.648 Figure 12 plots the quantile score in relation with the probability levels ranging from649 0.1 to 0.9. Again, this detailed analysis of the performance of the models favors the variant650 2 models (and particularly for Le Tampon site). Figure 12(b) reveals a symmetric pattern651 and shows that the highest quantiles and lowest quantiles are rather well estimated for Le652 Tampon. Conversely, regarding the site of Desert Rock, an asymmetric pattern is observed653 as the lowest quantiles are more penalized. This is possibly due to the high occurrences of654 clear skies experienced by Desert Rock.655 5.3.4. Ignorance Score (IGN)656 Initially proposed by (Good, 1952), this score is cited under various names: log score657 (Gneiting and Raftery, 2007), divergence (Weijs et al., 2010) or ignorance score (Roulston658 24 Forecast Horizon (h) 123456IGN (bans) 2.15 2.2 2.25 2.3 2.35 2.4 2.45 GB1 QRF1 GB2 QRF2 (a) Forecast Horizon (h) 123456IGN (bans) 2.6 2.65 2.7 2.75 2.8 2.85 GB1 QRF1 GB2 QRF2 (b) Figure 13: Ignorance Score of the diﬀerent intraday methods (a) Site of Desert Rock (b) Site of Le Tampon. The IGN score favors clearly the QRF2 model. Notice that the unit of this score is the bans and therefore cannot be normalized by the mean of the irradiance of the testing period. and Smith, 2002). Considering N veriﬁcation pairs of probabilistic forecasts given by their659 PDF ˆf i(x) and outcomes xi obs, the ignorance (IGN) is deﬁned as follow660 IGN = − 1 N N∑ i=1 log( ˆf i(xi obs) ). (11) This strictly proper score is appealing because it gathers interesting properties like ad-661 ditivity and locality (i.e. the score depends “only on the value of the probabilistic forecast662 at the veriﬁcation” (Br¨ocker and Smith, 2007b)). Like the CRPS, the IGN is a negatively663 oriented score (smaller values are better). Based on the log function, this score is strongly664 aﬀected by the large errors, when the observations fall far away from the highest forecasted665 probabilities. Equation 11 provides a simple way to compute the ignorance score from con-666 tinuous PDFs of parametric distributions or from predictive distributions (i.e. derived from667 discrete estimates, see section 2).668 Notice that (T¨odter and Ahrens, 2012) proposed a generalization of the IGN with an669 approach similar to Hersbach’s work (Hersbach, 2000) about the CRPS. They introduced670 a non-local version of the IGN for binary events and a new score called the Continuous671 Ranked Ignorance score (CRIGN) by analogy to the CRPS. For ensemble forecasts, no clear672 deﬁnition of the CDF to use to compute these non-local scores is provided. Thus, the CRIGN673 will not be addressed in this work.674 Regarding quantile forecasts, Figure 13 plots the ignorance score of the four models.675 This scoring rule conﬁrms the superiority of the variant 2 models although the QRF2 model676 appears to be the best performer. For this particular application, the ignorance score can677 complement the CRPS analysis and may increase the user’s conﬁdence to select the QRF2678 method.679 Considering ensemble forecasts, Roulston and Smith (2002) proposed a simple approach680 25 to compute the IGN. They used the “uniform” deﬁnition of the CDF derived from an681 ensemble forecast (see section 2 and ﬁgure 3(c)) combined with a linear interpolation of682 the probabilities between two consecutive members. Then, they applied Equation 11 to the683 corresponding PDF that is the ﬁrst derivative of the CDF (see appendix A for more details).684 Thus, the ignorance score of an outcome xobs that lies between two consecutive members685 [ek; ek+1] of an ensemble forecast with M members is given by Equation 12. We propose here686 a slightly diﬀerent formulation of the IGN deﬁned in the article of (Roulston and Smith,687 2002). They deﬁned the IGN using the binary logarithm (or log base 2) classically proposed688 by the ﬁeld of information theory. We prefer here to use the common logarithm function (or689 log base 10) to coincide with the general framework of the IGN (see Equation 11) mainly690 used in the literature. For ensemble forecasts, IGN is given by691 IGN = log(M + 1) + log∆Xk, (12) where692 ∆Xk = ek+1 − ek if 1 < k < M ∆X0 = e1 − e0 ∆XM = eM +1 − eM . (13) [e0; eM +1] is the a priori interval on which the outcome xobs is expected to be. Roulston693 and Smith (2002) proposed to use the minimum and the maximum of the climatology as694 boundaries of this interval. One can notice that this formulation of the IGN assigns the695 highest probabilities to the smallest diﬀerences between consecutive members. For a veriﬁ-696 cation dataset of N forecast-realization pairs, the ignorance score corresponds obviously to697 the arithmetical mean as in Equation 11. Notice that, unlike the CRPS, the ignorance score698 cannot be decomposed into reliability, resolution and uncertainty.699 In what follows, we show that the IGN score, as a local score, can be a less robust score700 than the CRPS. Tables 5 and 6 give the IGN, the CRPS and its decomposition for the tested701 ensemble forecasts. For Le Tampon and regarding both scores, the calibration brings an702 improvement. The decomposition of the CRPS highlights that the calibration increases the703 reliability but reduces the resolution. Regarding the site of Desert Rock, the two scores give704 an opposite ranking. The IGN assigns a better score to the calibrated ensemble. Conversely,705 the CRPS better rates the initial ECMWF forecasts. The decomposition of the CRPS shows706 that the increase in reliability, resulting from the calibration, does not counter-balance the707 reduction in resolution. Figure 14 illustrates this diﬀerence of scoring for a clear sky that has708 been forecasted and occurred. The original ECMWF forecast (blue line) already contains709 the observation (black line) and the associated CDF is very sharp. So, the IGN and the710 CRPS are already relatively low. The VD method (red dashed line) spreads the CDF and711 the observation falls close to the median of the calibrated CDF where the probability mass712 is the highest. As it is a local score that depends only on the probability at the observation,713 the IGN is slightly improved. Conversely, the CRPS, which takes into account the spread714 of the CDF, increases signiﬁcantly. Considering the large number of clear sky conditions715 that are forecasted and observed at Desert Rock, the results obtained for this speciﬁc case716 26 Table 5: Scores for Desert Rock CRPS (%) CRPS decomposition (%) IGN Reliability Resolution Uncertainty ECMWF-EPS 6.97 1.77 37.9 43.1 9.67 ECMWF-EPS + VD 7.37 0.97 36.7 43.1 7.84 Table 6: Scores for Le Tampon CRPS (%) CRPS decomposition (%) IGN Reliability Resolution Uncertainty ECMWF-EPS 25.1 6.03 23.5 42.6 9.13 ECMWF-EPS + VD 23.1 2.41 21.9 42.6 7.89 can be extended to a whole year. We can conclude that the VD calibration method spreads717 blindly the ECMWF forecasts, even when it is not necessary. As it is a local score, the IGN718 is not able to catch and to quantify such a behavior of forecasting models. Consequently, it719 seems less robust than the CRPS.720 GHI (Wh/m2) 0 500 1000 1500 2000 2500 3000 3500Probability 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ECMWF-EPS CRPS = 2.96% IGN = 5.99 ECMWF-EPS + VD CRPS = 6.05% IGN = 5.07 Desert Rock (13-Sep-2013 16:00:00) ECMWF-EPS ECMWF-EPS + VD Observation Figure 14: Illustration of the evolution of the CRPS and of the IGN between original and calibrated forecasts: case where these two scores give contradictory information. The CDFs are plotted using the classical deﬁnition for ensemble forecasts (see section 2). 6. Conclusions721 In this work, we proposed a framework for evaluating solar probabilistic forecasts. Two722 types of solar probabilistic forecasts namely ensemble forecasts and quantile forecasts were723 27 used to illustrate the evaluation framework. This latter is based on visual diagnostic tools724 and scoring rules originally designed by the weather forecast veriﬁcation community. For725 both types of probabilistic forecasts (quantile and ensemble forecasts), we proposed to follow726 the same approach to assess the quality of the models albeit some diagnostic tools are more727 appropriate depending on the type of forecast.728 The proposed approach consists in ﬁrst evaluating the reliability attribute. Graphical729 displays such as reliability diagrams and rank histograms with consistency bars, respectively730 for quantile forecasts and ensemble forecasts, are eﬃcient, easy-to-build graphical tools ded-731 icated to this purpose. Once the reliability attribute checked, a sharpness analysis can be732 conducted. However, in our opinion, even if sharpness is an intuitive property that can be733 visually assessed with diagrams, it can only contribute to a qualitative evaluation of the734 forecasting methods. More generally, visual diagnostic tools cannot allow one to objectively735 conclude on a higher quality of a given model. Therefore, we recommend to systematically736 compute an overall score i.e. the CRPS which, in our opinion, might be a standard in assess-737 ing probabilistic forecasts of continuous variable. This proper score allows allows ranking738 models and its relative counterpart (i.e. CRPS normalized by the mean irradiance) permit739 to carry out sites’ comparisons. Furthermore, the decomposition of the CRPS into reliability740 and resolution may provide additional insight into the performance of a forecasting system.741 Also, we recommend to complement the CRPS scoring rule with a set of proper scores like742 interval score, ignorance score and quantile score. For instance, quantile score may provide743 detailed performance of the models at speciﬁc parts of the predictive distributions. Re-744 garding the ignorance score, although it can advantageously complement the CRPS results,745 attention should be paid to its use, as its locality makes it less robust than the CRPS.746 Finally, when dealing with ensemble forecasts, dedicated veriﬁcation tools, such as rank747 histograms and the CRPS proposed by (Hersbach, 2000), can be used without any additional748 assumptions. Indeed, they assume a classical deﬁnition of the underlying CDF and it is749 not necessary to deﬁne the CDF boundaries. However, care must be taken while deriving750 quantiles, prediction intervals and associated metrics from ensembles. As several possibilities751 are available, it is important to clearly state which one is used (e.g. uniform or non-uniform752 spacing). The authors of this paper have a preference for the uniform spacing because it753 deﬁnes the quantiles such that the members of the ensemble can be seen as a predictive754 distribution.755 In terms of perspectives, applications related for example to energy management system756 or simply micro-grids should greatly beneﬁt from the evaluation framework proposed in757 this work. More precisely, the veriﬁcation tools (and particularly scoring rules like CRPS)758 should help selecting the best probabilistic forecasts in order to optimize the operation of759 the energy management system and consequently increase the economical beneﬁt of the760 associated energy systems.761 This work focused on the forecasting of the solar irradiance. However, the proposed762 methodology and associated tools can be extended to the evaluation of probabilistic forecasts763 of solar power generation.764 28 7. Appendices765 Appendix A “Uniform” deﬁnition of the CDF and PDF derived from an en-766 semble forecast767 Let E = (e1, ..., eM ) be an ensemble forecast with M members ek, k = 1, ..., M . The768 uniform deﬁnition of the resulting Cumulative Distribution Function (CDF) assigns a prob-769 ability mass of 1/(M + 1) between two consecutive members and for the events that fall770 outside of the ensemble range. The tails of the CDF are bounded by e0 and eM +1 (see771 ﬁgure 3(c)). Considering a linear interpolation between the consecutive members and the772 two limits deﬁned above, the analytic formulation of the CDF ˆFk(x) corresponding to the773 “uniform” deﬁnition is774 ˆFk(x) = x + (k∆Xk − ek) (M + 1)∆Xk , (14) where775 ∆Xk = ek+1 − ek with k = 0, ..., M. (15) The corresponding Probability Density Function (PDF) ˆfk(x) is the ﬁrst derivative of776 the CDF deﬁned above i.e.777 ˆfk(x) = d ˆFk(x) dx = 1 (M + 1)∆Xk . (16) Appendix B Hersbach’s method to compute the CRPS from ensemble fore-778 casts779 Here, we reproduce the methodology proposed by (Hersbach, 2000) to compute the CRPS780 and its decomposition. Let E = (e1, ..., eM ) be an ensemble forecast with M members ek,781 k = 1, ..., M and xobs the observation. It is important to notice that Hersbach assumes a782 classical deﬁnition of the CDF obtained from the ensemble (see ﬁgure 3(a)). Thus, the CRPS783 could be seen as the sum of areas deﬁned by the members E, the square of their associated784 cumulative probability pk and the position of the observation xobs. One then have785 CRP S = M∑ k=0 αkp2 k + βk(1 − pk) 2, (17) with786 pk = k M . (18) The values of α and β are determined with the position of the observation xobs when787 pooled within the sorted members. Table 7 gives the values of α and β for all the possible788 cases. Some care must be taken for k = 0 and k = M . Indeed, the corresponding intervals789 (i.e. (−∞, e1] and [eM , +∞)) contribute to the CRPS only if the observation falls outside790 29 Table 7: Determination of α and β 0 < k < M αk βk xobs > ek+1 ek+1 − ek 0 ek+1 > xobs > ek xobs − ek ek+1 − xobs xobs < ek 0 ek+1 − ek k = 1, M (Outliers) αk βk xobs < e1 0 e1 − xobs xobs > eM xobs − eM 0 the range of the ensemble (see second part of table 7 about the outliers). Finally, considering791 a veriﬁcation dataset of N forecast-realization pairs, the overall CRP S corresponds to the792 mean of the CRPS obtained for each individual forecast i.e. CRP S = 1 N ∑N i=1 CRP Si.793 Considering ensemble forecasts, the decomposition of the CRPS has no sense for a single794 forecast-realization pair. Indeed, such case has null uncertainty and resolution. Therefore,795 the decomposition of the CRP S proposed by Hersbach is based on the mean values ¯αk =796 1 N ∑N i=1 αi k and ¯βk = 1 N ∑N i=1 βi k. The components of the CRPS are797 REL = M∑ k=0 ¯gk[¯ok − pk] 2, (19) 798 U N C = ∑N i=1 ∑i j=1 |xi obs − xj obs| N 2 , (20) 799 CRP Spot = M∑ k=0 ¯gk ¯ok(1 − ¯ok), (21) 800 RES = U N C − CRP Spot, (22) with801 ¯gk = ¯αk + ¯βk, (23) 802 ¯ok = ¯βk ¯αk + ¯βk . (24) Appendix C Decomposition of the CRPS through decomposition of the Brier803 score804 Hersbach (2000) showed that the CRPS can be calculated through the integration of the805 Brier Score over all possible values of the predictand. The Brier Score (BS) is a scoring806 rule used for the prediction of the occurrence of a speciﬁc event. Usually, such an event is807 characterized by a threshold value x . The event happened if xobs ≤ x and not happened if808 xobs > x. One can then have809 CRP S = ∫ BS(x)dx = ∫ REL(x)dx − ∫ RES(x)dx + ∫ U N C(x)dx, (25) 30 Table 8: Contingency Table for threshold x Probability pk Event occurred xobs ≤ x Event not occurred xobs > x 0 n0 ˆn0 · · · · · · · · · i nk ˆni · · · · · · · · · 1 nM ˆnM with810 REL(x) = M∑ k=0 gk(x)[ok(x) − pk] 2, (26) 811 RES(x) = M∑ k=0 gk(x)[ok(x) − o(x)] 2, (27) 812 U N C(x) = o(x)[1 − o(x)]. (28) In our case, the integration over x of the diﬀerent components ranges for values of GHI from813 0 to the maximum of the climatology.814 For each value of the predictand x , terms necessary to compute the Brier Score compo-815 nents can be calculated from a 2x2 contingency table (see Table 8). In other words, the joint816 distribution of forecasts and observations for M +1 forecast probabilities can be summarized817 in a (M + 1) x 2 contingency table.818 The total number of pairs of forecasts/observations N (i.e. the sample size) is given by819 N = ∑M k=0 nk + ∑M k=0 ˆnk.820 gk(x) = lk N , (29) with lk = ni + ˆnk821 ok(x) = nk lk (30) 822 o(x) = M∑ k=0 gk(x)ok(x). (31) Figure 15 shows the components of the CPRS through the decomposition of the Brier823 Score.824 Appendix D Results of the CRPS decomposition for the intraday models825 First, it should be noted that the uncertainty part is given in Table 1. Figure 16 shows826 the resolution part of the CRPS which conﬁrms the lack of resolution of the diﬀerent models827 as the forecast horizon increases. Regarding resolution, the statements made regarding the828 31 Threshold value - GHI (W/m2) 0 200 400 600 800 1000 1200Brier Score 0 0.05 0.1 0.15 0.2 0.25 Decomposition of the Brier Score UNC(x) RES(x) REL(x) BS(x) Figure 15: CRPS components through decomposition of the Brier Score (BS) - The area under each curve corresponds to the related CRPS component. Integration of BS(x) for all threshold values x gives the CRPS CRPS still hold i.e. the two non-linear models (GB2 and QRF2) that include the solar829 geometric predictors lead to better resolution.830 Figure 17 plots the reliability component of the CRPS. Surprisingly, the reliability do831 not show a tendency to increase with the lead time. Indeed, we expect the reliability term832 to increase with increasing forecast horizon (we recall that the reliability term is negatively833 oriented i.e. a lower reliability value corresponds to a more reliable forecasts). However,834 in agreement with the reliability assessment, the GB2 model exhibits the lowest reliability835 for the site of Desert Rock while for Le Tampon, low reliability values are obtained with836 the QRF1 model. Nonetheless, it must be noted that the reliability component weakly837 contributes to the CRPS and that the higher quality of the probabilistic forecasts generated838 by the variant 2 models originates from the resolution attribute.839 References840 Alessandrini, S., Delle Monache, L., Sperati, S., Cervone, G., 2015. An analog ensemble for short-term841 probabilistic solar power forecast. Applied Energy 157, 95–110.842 Anderson, J.L., 1996. A Method for Producing and Evaluating Probabilistic Forecasts from Ensemble Model843 Integrations. Journal of Climate 9, 1518–1530.844 Ben Bouall`egue, Z., 2015. Assessment and added value estimation of an ensemble approach with a focus on845 global radiation forecasts. MAUSAN , 541–550.846 Bentzien, S., Friederichs, P., 2014. Decomposition and graphical portrayal of the quantile score. Quart. J.847 Roy. Meteor. Soc. 140, 1924–1934.848 Br¨ocker, J., 2012. Evaluating raw ensembles with the continuous ranked probability score. Quarterly Journal849 of the Royal Meteorological Society 138, 1611–1617.850 32 Forecast Horizon (h) 123456CRPS resolution (%) 20.5 21 21.5 22 22.5 23 23.5 24 24.5 GB1 QRF1 GB2 QRF2 (a) Forecast Horizon (h) 123456CRPS resolution (%) 10 11 12 13 14 15 16 17 18 19 20 GB1 QRF1 GB2 QRF2 (b) Figure 16: Relative (in % of mean GHI) resolution component of the CRPS of the diﬀerent intraday methods (a) Site of Desert Rock (b) Site of Le Tampon. As expected, resolution decreases with increasing lead time. The variant 2 models lead to better resolution. Forecast Horizon (h) 123456CRPS reliability (%) 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 GB1 QRF1 GB2 QRF2 (a) Forecast Horizon (h) 123456CRPS reliability (%) 0.85 0.9 0.95 1 1.05 1.1 1.15 GB1 QRF1 GB2 QRF2 (b) Figure 17: Relative (in % of mean GHI) reliability Component of the CRPS of the diﬀerent intraday methods (a) Site of Desert Rock (b) Site of Le Tampon. The reliability component weakly contributes to the CRPS. 33 Br¨ocker, J., Smith, L.A., 2007a. Increasing the Reliability of Reliability Diagrams. Weather and Forecasting851 22, 651–661.852 Br¨ocker, J., Smith, L.A., 2007b. Scoring Probabilistic Forecasts: The Importance of Being Proper. Weather853 and Forecasting 22, 382–388.854 Chu, Y., Coimbra, C.F., 2017. Short-term probabilistic forecasts for Direct Normal Irradiance. Renewable855 Energy 101, 526–536.856 Chu, Y., Li, M., Pedro, H.T., Coimbra, C.F., 2015. Real-time prediction intervals for intra-hour DNI857 forecasts. Renewable Energy 83, 234–244.858 Coimbra, C.F., Kleissl, J., Marquez, R., 2013. Overview of Solar-Forecasting Methods and a Metric for859 Accuracy Evaluation, in: Solar Energy Forecasting and Resource Assessment. Elsevier, pp. 171–194.860 Dambreville, R., Blanc, P., Chanussot, J., Boldo, D., 2014. Very short term forecasting of the Global861 Horizontal Irradiance using a spatio-temporal autoregressive model. Renewable Energy 72, 291–300.862 David, M., Mazorra Aguiar, L., Lauret, P., 2018. Comparison of intraday probabilistic forecasting of solar863 irradiance using only endogenous data. International Journal of Forecasting 34, 529–547.864 David, M., Ramahatana, F., Trombe, P., Lauret, P., 2016. Probabilistic forecasting of the solar irradiance865 with recursive ARMA and GARCH models. Solar Energy 133, 55–72.866 Gneiting, T., Balabdaoui, F., Raftery, A.E., 2007. Probabilistic forecasts, calibration and sharpness. Journal867 of the Royal Statistical Society: Series B (Statistical Methodology) 69, 243–268.868 Gneiting, T., Raftery, A.E., 2007. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the869 American Statistical Association 102, 359–378.870 Gneiting, T., Raftery, A.E., Westveld, A.H., Goldman, T., 2005. Calibrated Probabilistic Forecasting Using871 Ensemble Model Output Statistics and Minimum CRPS Estimation. Monthly Weather Review 133,872 1098–1118.873 Golestaneh, F., Gooi, H.B., Pinson, P., 2016a. Generation and evaluation of spacetime trajectories of874 photovoltaic power. Applied Energy 176, 80 – 91.875 Golestaneh, F., Pinson, P., Gooi, H.B., 2016b. Very Short-Term Nonparametric Probabilistic Forecasting of876 Renewable Energy Generation With Application to Solar Energy. IEEE Transactions on Power Systems877 31, 3850–3863.878 Good, I.J., 1952. Rational Decisions. Journal of the Royal Statistical Society. Series B (Methodological) 14,879 107–114.880 Grantham, A., Gel, Y.R., Boland, J., 2016. Nonparametric short-term probabilistic forecasting for solar881 radiation. Solar Energy 133, 465–475.882 Hamill, T.M., 2001. Interpretation of Rank Histograms for Verifying Ensemble Forecasts. Monthly Weather883 Review 129, 550–560.884 Hersbach, H., 2000. Decomposition of the Continuous Ranked Probability Score for Ensemble Prediction885 Systems. Weather and Forecasting 15, 559–570.886 Hoﬀ, T.E., Perez, R., 2012. Modeling PV ﬂeet output variability. Solar Energy 86, 2177–2189.887 Hoﬀ, T.E., Perez, R., Kleissl, J., Renne, D., Stein, J., 2013. Reporting of irradiance modeling relative888 prediction errors: Reporting of irradiance modeling relative prediction errors. Progress in Photovoltaics:889 Research and Applications 21, 1514–1519.890 Hong, T., Pinson, P., Fan, S., Zareipour, H., Troccoli, A., Hyndman, R.J., 2016. Probabilistic energy fore-891 casting: Global Energy Forecasting Competition 2014 and beyond. International Journal of Forecasting892 32, 896–913.893 Huang, J., Korolkiewicz, M., Agrawal, M., Boland, J., 2013. Forecasting solar radiation on an hourly time894 scale using a Coupled AutoRegressive and Dynamical System (CARDS) model. Solar Energy 87, 136–149.895 Iversen, E.B., Morales, J.M., Møller, J.K., Madsen, H., 2016. Short-term probabilistic forecasting of wind896 speed using stochastic diﬀerential equations. International Journal of Forecasting 32, 981–990.897 Jolliﬀe, I., Stephenson, D., 2003. Forecast Veriﬁcation. A practitioner’s guide in atmospheric science. Wiley,898 Chichester, England.899 Jung, J., Broadwater, R.P., 2014. Current status and future advances for wind speed and power forecasting.900 Renewable and Sustainable Energy Reviews 31, 762–777.901 34 Khosravi, A., Nahavandi, S., Creighton, D., 2013. Prediction Intervals for Short-Term Wind Farm Power902 Generation Forecasts. IEEE Transactions on Sustainable Energy 4, 602–610.903 Koenker, R., Bassett, G., 1978. Regression Quantiles. Econometrica 46, 33–50.904 Lauret, P., David, M., Pedro, H., 2017. Probabilistic Solar Forecasting Using Quantile Regression Models.905 Energies 10, 1591.906 Lauret, P., Voyant, C., Soubdhan, T., David, M., Poggi, P., 2015. A benchmarking of machine learning907 techniques for solar radiation forecasting in an insular context. Solar Energy 112, 446–457.908 Leutbecher, M., Palmer, T.N., 2008. Ensemble forecasting. Journal of Computational Physics 227, 3515–909 3539.910 Li, K., Wang, R., Lei, H., Zhang, T., Liu, Y., Zheng, X., 2018. Interval prediction of solar power using an911 Improved Bootstrap method. Solar Energy 159, 97–112.912 Lorenz, E., Heinemann, D., 2012. Prediction of solar irradiance and photovoltaic power., in: Comprehensive913 Renewable Energy. Elsevier, Oxford, UK, pp. 239–292.914 Marquez, R., Coimbra, C.F., 2011. Forecasting of global and direct solar irradiance using stochastic learning915 methods, ground experiments and the NWS database. Solar Energy 85, 746–756.916 van der Meer, D., Widn, J., Munkhammar, J., 2018. Review on probabilistic forecasting of photovoltaic917 power production and electricity consumption. Renewable and Sustainable Energy Reviews 81, 1484 –918 1512.919 Morales, J.M., Conejo, A.J., Madsen, H., Pinson, P., Zugno, M., 2014. Integrating Renewables in Electricity920 Markets. volume 205 of International Series in Operations Research & Management Science. Springer921 US, Boston, MA.922 Murphy, A.H., 1993. What Is a Good Forecast? An Essay on the Nature of Goodness in Weather Forecasting.923 Weather and Forecasting 8, 281–293.924 NCAR-Research applications laboratory, 2015. veriﬁcation: Weather Forecast Veriﬁcation Utilities. R925 package version 1.42.926 Pedro, H.T., Coimbra, C.F., 2015. Nearest-neighbor methodology for prediction of intra-hour global hori-927 zontal and direct normal irradiances. Renewable Energy 80, 770–782.928 Pedro, H.T., Coimbra, C.F., David, M., Lauret, P., 2018. Assessment of machine learning techniques for929 deterministic and probabilistic intra-hour solar forecasts. Renewable Energy 123, 191–203.930 Pinson, P., McSharry, P., Madsen, H., 2010. Reliability diagrams for non-parametric density forecasts of931 continuous variables: Accounting for serial correlation. Quarterly Journal of the Royal Meteorological932 Society 136, 77–90.933 Pinson, P., Nielsen, H.A., Møller, J.K., Madsen, H., Kariniotakis, G.N., 2007. Non-parametric probabilistic934 forecasts of wind power: required properties and evaluation. Wind Energy 10, 497–516.935 Pinson, P., Reikard, G., Bidlot, J.R., 2012. Probabilistic forecasting of the wave energy ﬂux. Applied Energy936 93, 364–370.937 Pinson, P., Tastu, J., 2014. Discussion of “Prediction Intervals for Short-Term Wind Farm Generation Fore-938 casts” and “Combined Nonparametric Prediction Intervals for Wind Power Generation”. IEEE Transac-939 tions on Sustainable Energy 5, 1019–1020.940 Reikard, G., 2009. Predicting solar radiation at high resolutions: A comparison of time series forecasts.941 Solar Energy 83, 342–349.942 Roulston, M., Smith, L., 2002. Evaluating Probabilistic Forecasts Using Information Theory. Monthly943 Weather Review 130, 1653–1660.944 Scolari, E., Sossan, F., Paolone, M., 2016. Irradiance prediction intervals for PV stochastic generation in945 microgrid applications. Solar Energy 139, 116–129.946 Sperati, S., Alessandrini, S., Delle Monache, L., 2016. An application of the ECMWF Ensemble Prediction947 System for short-term solar power forecasting. Solar Energy 133, 437–450.948 T¨odter, J., Ahrens, B., 2012. Generalization of the Ignorance Score: Continuous Ranked Version and Its949 Decomposition. Monthly Weather Review 140, 2005–2017.950 Verbois, H., Rusydi, A., Thiery, A., 2018. Probabilistic forecasting of day-ahead solar irradiance using951 quantile gradient boosting. Solar Energy 173, 313–327.952 35 Voyant, C., Motte, F., Fouilloy, A., Notton, G., Paoli, C., Nivet, M.L., 2017. Forecasting method for global953 radiation time series without training phase: Comparison with other well-known prediction methodologies.954 Energy 120, 199–208.955 Weijs, S.V., van Nooijen, R., van de Giesen, N., 2010. Kullback–leibler divergence as a forecast skill score956 with classic reliability–resolution–uncertainty decomposition. Monthly Weather Review 138, 3387–3399.957 Wilks, D.S., 2014. Statistical Methods in the Atmospheric Sciences. An Introduction. Elsevier Science,958 Burlington.959 Winkler, R.L., 1972. A Decision-Theoretic Approach to Interval Estimation. Journal of the American960 Statistical Association 67, 187–191.961 Yang, D., 2019. A universal benchmarking method for probabilistic solar irradiance forecasting. Solar962 Energy 184, 410–416.963 Yang, D., Kleissl, J., Gueymard, C.A., Pedro, H.T., Coimbra, C.F., 2018. History and trends in solar964 irradiance and PV power forecasting: A preliminary assessment and review using text mining. Solar965 Energy 168, 60–101.966 Zamo, M., Mestre, O., Arbogast, P., Pannekoucke, O., 2014. A benchmark of statistical regression methods967 for short-term forecasting of photovoltaic electricity production. Part II: Probabilistic forecast of daily968 production. Solar Energy 105, 804–816.969 36","libVersion":"0.3.1","langs":""}
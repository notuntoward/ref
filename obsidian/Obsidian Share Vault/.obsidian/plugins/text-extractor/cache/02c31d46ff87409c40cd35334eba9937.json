{"path":"lit/lit_notes_OLD_PARTIAL/Raieli24CosineSimEmbed.pdf","text":"4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/18 |LLMS| EMBEDDINGS| RETRIEVAL AUGMENTED GENERATION| Cosine Similarity and Embeddings Are Still in Love? Cosine similarity is the most used method, but it is really the best? Salvatore Raieli · Follow Published in Level Up Coding · 11 min read · Mar 14, 2024 Photo by Mayur Gala on Unsplash Cosine similarity is the most widely used method for calculating the similarity between two entities (words, sentences, or documents). It seems like an easy and robust system, but it also has a dark side. 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/18 In this article, we discuss, the limitations of cosine similarity and its complex relationship with embeddings How close are two points in space? Photo by Ryan Jacobson on Unsplash “Strength lies in differences, not in similarities.” — Stephen Covey Embedding has had a fundamental impact on NLP and has been the basis for the recent explosion of text applications. Indeed, embedding allows discrete entities to be mapped to dense real-valued vectors. Today, embeddings are 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/18 also the basis of Large Language Models, and recommendation systems are also fundamental. A Requiem for the Transformer? Will be the transformer the model leading us to artificial general intelligence? Or will be replaced? towardsdatascience.com These embeddings can be used for different applications either directly (frozen or fine-tuned) or as input to other models. Interest also stems from the fact that these embeddings have picked up and learned a notion of similarity. After embedding, analysis can be done on the learned vectors representing entities that were previously discrete. 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/18 image source: here we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. (source) These vectors can then be used for a variety of applications, and they have inherently encoded semantic value. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented. (source) 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/18 One of the most widely used methods is cosine similarity in which the proximity of two vectors is calculated. The higher the value the greater the similarity. This measure of similarity is also considered the measure of semantic similarity between two entities (or two embedded sentences). Among the various applications that have emerged in recent years, one has been particularly successful: Retrieval-Augmented Generation. These vectors are now used to find documents that are close to the query so that they can be retrieved and used to conditionally generate text by an LLM. The similarity between documents and queries is evaluated by cosine similarity. Follow the Echo: How to Get a Good Embedding from your LLM How to overcome the limits of Autoregressive Models for embedding levelup.gitconnected.com 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 6/18 image source: here But is cosine similarity despite being so widespread a good way to assess similarity? There have been several studies that have focused on the impact of term frequency and their embedding. For Low-frequency words, it is difficult to identify neighboring words, they have small inner products and high variance. Similarly, there is an impact on similarities and bias. This is not only true for static embeddings such as word2vec but also for contextual embeddings when using a language model. We find that the cosine of BERT embeddings underestimates the similarity of high frequency words (to other tokens of the same word or to different words) as 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 7/18 compared to human judgements. (source) The problem is not only for rare words but also for high-frequency words, and according to some studies, similarity is underestimated when using these embeddings and similarity cosines. image source: here In summary, we find that using cosine to measure the semantic similarity of words via their BERT embeddings gives systematically smaller similarities the higher the frequency of the word. (source) Can you tell me, if we are the same? 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 8/18 Photo by Caroline Veronez on Unsplash “We only see what we want to see; we only hear what we want to hear. Our belief system is just like a mirror that only shows us what we believe.” — Don Miguel Ruiz In one study they tried to analyze why this happens and go beyond empirical observations. According to the authors of this study, cosine similarity in some cases can lead to arbitrary results 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 9/18 We find that the underlying reason is not cosine similarity itself, but the fact that the learned embeddings have a degree of freedom that can render arbitrary cosine- similarities even though their (unnormalized) dot-products are well-defined and unique. (source) Is Cosine-Similarity of Embeddings Really About Similarity? Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their… arxiv.org The authors started with linear embedding models because they have a closed form so you can analyze the limitations of cosine similarity at a more theoretical level. Therefore, they decided to use the matrix-factorization (MF) model, in which a low-rank matrix is learned that can be considered as embedding. Once this is achieved, one can use cosine similarity on the learned embedding. image source: here The authors started from these two forms of L2-norm regularization. The first is equivalent to a form of denoising (methods such as dropout) and the second is comparable to weight decay in deep learning. The idea is to analyze two training objectives that are similar to those commonly used in deep learning. 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 10/18 Once embeddings are obtained using these two objectives, normalizations such as rotation or scaling can be conducted: While cosine similarity is invariant under such rotations R, one of the key insights in this paper is that the first (but not the second) objective is also invariant to rescalings of the columns. (source) image source: here For the authors, the solution clearly is either to avoid cosine similarity or to train the model for cosine similarity using layer normalization. In fact, the main problem for them is the normalization of embedding. Apart from that, it is also important to note that, in cosine-similarity, normalization is applied only after the embeddings have been learned. This can noticeably reduce the resulting (semantic) similarities compared to applying some normalization, or reduction of popularity-bias, before or during learning. (source) 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 11/18 So it is clearly better to scale the data prior to training (zero scaling for numerical values) and train by sampling negatives with a probability proportional to their frequency (popularity) when training by word embedding (word2vec). Being connected is not meaning to be the same Photo by Toa Heftiba on Unsplash “Research shows that couples who have a lot of similarities, including intellectual compatibility, end up staying together.” — Helen Fisher 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 12/18 We discussed what happens when we have embedding of document-derived vectors. But what happens if we have an embedding from another popular model such as Knowledge Graph Embeddings (KGE)? These are also often used to get information to augment the generation of LLMs. Knowledge Graph Embedding Models (KGEMs) take into account the semantic relationship between two entities to learn embeddings, it is often taken for granted that the resulting embeddings capture both the semantics and attributes of entities and their relationships in the KG. (source) These embeddings are seen as a measure of similarity for both entities and relationships and are used directly for various tasks such as entity similarity and conceptual clustering. Thus, several questions remain: How closely does proximity in an embedding space align with entity similarity in the knowledge graph (KG)? How do traditional rank-based metrics correlate with entity similarity? In other words, if we have a KG with good performance for link prediction or other tasks, is there a correlation on better entity distribution in the embedding space? Why are an entity’s neighbors in the embedding space different across models? 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 13/18 image source: here The notion of similarity in a graph is not a simple question, and it must also take into account for an entity its neighbors (and its neighbors’ neighbors). In other words, two entities must be neighbors in terms of both what they represent themselves and their relationships. This notion of similarity you should also maintain in embedding Based on a given entity e, it is possible to get its closest neighbors in the KG and in the embedding space. We subsequently need to measure how much the two lists of closests neighbors between these two approaches actually overlap. (source) The authors of this study decided to use different embedding methods for KGs. Considering 1-hop or 2-hop, the authors note that not all embedding models behave in the same way. Some methods show an improved alignment between neighbors in the graph for an entity and its neighbors in the embedding space. First, we noted a general misalignment in the representation of certain classes across various datasets, regardless of the KGEM used. Second, there is noticeable 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 14/18 inconsistency within specific classes, where entities might align well with their graph-based proximity in one KGEM but not in another. (source) The interesting result is that different models are better able to capture the nuances of specific classes. Even in the same class, there are sometimes consistency problems. image source: here Similarity is not universal among models; some models have a similar notion while others consider a different notion. So it does not consistently align between models and the graph itself. So one must be careful to define two similar entities based on embedding. 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 15/18 For the second question the authors, that in some cases a better graph means a better alignment of similarity in embedding. This is not for example for all cases or for all embedding models. So choosing a good graph for link prediction does not necessarily mean having good embedding. Parting thoughts Photo by Saif71.com on Unsplash “Reality is partial to symmetry and slight anachronisms”― Jorge Luis Borges 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 16/18 Embedding is both a simple and ingenious concept: transforming discrete entities into numerical vectors. This vector represents the context in which an entity is located as we try to predict its neighbors during training. Variations aside, it has remained the same since as far back as 2013, this idea has been so powerful that it is the first layer of our beloved LLM and the basis for endless applications. Soon after, we realized that we could work with these vectors. We can compute with cosine similarity how similar two entities are, and all this just by using their embedded vectors. So simple and so effective that we did not question the value of such a procedure. The problem is that we are not sure that cosine similarity and the embeddings themselves really represent the similarity of the two entities. We have seen this in the two most commonly used cases: word embeddings and knowledge graph embeddings. we showed that the choice of KGEM significantly influences the notion of entity similarity encoded in the resulting vector space. This finding has profound implications for a variety of downstream tasks where accurate entity similarity is crucial, e.g. recommender systems and semantic searches. (source) The focal point is that on the quality of an embedding representation many downstream tasks rest. Many of these applications rely on a very simple mathematical formula: cosine similarity. These works show, that embedding methods not only learn representations that are not universal, and that depend on the dataset but on how the embedding is learned. The similarity calculated with cosine similarity depends on the method, the dataset, and entity-specific factors. 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 17/18 As mentioned earlier, learning a representation is itself an elusive concept. intriguingly, it can depend on so many factors, but it also means that we have to pay attention to the method and other parameters when we try to calculate similarity. Learning to Learn: How AI and Humans Learn Understanding learning to create better AI and understand ourselves levelup.gitconnected.com To close, we need different formulas that are more robust and capture similarity better. At the same time, we need to be aware of the limitations of cosine similarity. If you have found this interesting: You can look for my other articles, and you can also connect or reach me on LinkedIn. Check this repository containing weekly updated ML & AI news. I am open to collaborations and projects and you can reach me on LinkedIn. You can also subscribe for free to get notified when I publish a new story. Get an email whenever Salvatore Raieli publishes. Get an email whenever Salvatore Raieli publishes. By signing up, you will create a Medium account if you don’t already… salvatore-raieli.medium.com Here is the link to my GitHub repository, where I am collecting code and many resources related to machine learning, artificial intelligence, and more. 4/16/24, 3:42 PM Cosine Similarity and Embeddings Are Still in Love? | by Salvatore Raieli | Mar, 2024 | Level Up Coding chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 18/18 Reference Here is the list of the principal references I consulted to write this article, only the first name of an article is cited. 1. Mikolov, 2013, Efficient Estimation of Word Representations in Vector Space, link 2. Gao, 2023, Retrieval-Augmented Generation for Large Language Models: A Survey, link 3. Hellrich, 2016, Bad Company — Neighborhoods in Neural Embedding Spaces Considered Harmful, link 4. Mimno, 2017, The strange geometry of skip-gram with negative sampling, link 5. Gessler, 2021BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology, link 6. Zhou, 2022, Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words, link 7. Steck, 2024, Is Cosine-Similarity of Embeddings Really About Similarity?, link 8. Hubert, 2023, Do Similar Entities have Similar Embeddings?, link 9. Wang, 2023, A Survey on Knowledge Graph Embeddings for Link Prediction,link","libVersion":"0.3.2","langs":""}
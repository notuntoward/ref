{"path":"lit/lit_sources.backup/Gueddari23decOnlyGenAI.pdf","text":"4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 1/10 The Dual Worlds of Decoder-only Transformers: Training vs. Inference Nouamane El Gueddari · Follow 4 min read · Nov 12, 2023 54 1 Open in app Search Write 4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 2/10 Source: Author with DALL-E3 In the swiftly evolving landscape of artificial intelligence, the launch of ChatGPT last year marked a pivotal moment, redefining the potential of natural language processing. Since then, a significant focus for AI practitioners has been to harness high- level frameworks to fine-tune or train models that aspire to reach the prowess of ChatGPT. 4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 3/10 While these tools offer powerful means to advance model capabilities, a deeper understanding of what transpires beneath the surface during the training and inference phases in transformers is crucial. In the realm of Natural Language Processing (NLP), the architecture of neural network models plays a pivotal role in defining their capabilities and applications. One such model is the ‘decoder-only’ transformer, a variant that, as the name suggests, consists solely of a decoder stack, as opposed to the encoder-decoder stack found in ‘vanilla’ transformers. The decoder-only model is tailored for generative tasks — it excels at predicting the next token in a sequence. Before we dive into the intricacies of the inference and training processes, let’s unpack the architecture of this decoder-only model. Decoder-only models architecture 4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 4/10 1. Input or Prompt: Think of this as the initial question or phrase you pose to the model, like “Who is the current president of the USA?” It sets the stage for the model to generate a continuation or response based on its training. 4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 5/10 2. Input Embedding: The model can’t understand words directly, it translates them into a numerical form it can process — a bit like transforming code into binary numbers for computer to understand 3. Positional Encoding: The model processes the tokens in a parallel manner, hence the need to explicitly encode the position of the words in the input sentence. 4. Masked Multi-Head Attention: This mechanism allows words(or Tokens) to interact with each other, determining how much attention to pay to each word in the sentence. the “masked” part ensures that the model can only looks to previous words to predict the next one, avoiding “cheating” by peeking at the answer. 5. add & Norm: combine the attention output and original output, an then normalizes it to stabilize the learning process. 6. Feed Forward: processes each word’s representation further, allowing for more complex patterns and relations. 7. Block 2 …N: the model isn’t just a single set of these components, it repeats them multiple times, each additional layer brings more depth and nuance to the understanding of the model. 8. Linear: shapes the output for predictions. 9. softmax: calculates the probability for each word being the next one. Training vs. Inference: Clarifying Model Approaches Using “Who is the current president of the USA?” Training: The model learns from the entire sentence in one go, predicting each word (token) by only looking at the preceding words. 4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 6/10 For example, to predict “current,” the model has access only to “Who is the,” not the words that follow. Future tokens are masked to prevent the model from ‘seeing’ the answer ahead of time, ensuring it learns to predict rather than just memorize. The objective during training is not to generate new text but to accurately predict the next word in the training data, effectively reproducing the input prompt. Inference: The model generates one token at a time, building upon the initial prompt given to it. Unlike training, inference is about creating new content, relying on the patterns and knowledge learned during the training phase. There’s no ‘peeking’ at the correct answer because the model is now generating text beyond the prompt, aiming for coherent and contextually appropriate continuations. Conclusion: While the differences between “training” and “inference” in machine learning may seem subtle, they are important to understand how Transformers work. During training, the model focuses on “reproduction”, learning to predict what comes next from a sequence it has been provided. This is akin to a student learning from a book — the goal is to understand and remember information. On the other hand, “inference” is about creation, where the model uses its acquired knowledge to generate new sentences. References: 4/13/24, 7:04 PM The Dual Worlds of Decoder-only Transformers: Training vs. Inference | by Nouamane El Gueddari | Medium https://medium.com/@ElGueddari/the-dual-worlds-of-decoder-only-transformers-training-vs-inference-1ea1c2aabaaa 7/10 Attention Is All You Need The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an… arxiv.org https://www.youtube.com/watch?v=IGu7ivuy1Ag Written by Nouamane El Gueddari 5 Followers Deep Learning Engineer and Enthusiast Follow More from Nouamane El Gueddari ChatGPT Artificial Intelligence Llm NLP Transformers Nouamane El Gueddari","libVersion":"0.3.2","langs":""}
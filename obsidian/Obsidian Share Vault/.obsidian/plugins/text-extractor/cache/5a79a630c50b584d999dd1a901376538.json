{"path":"lit/lit_sources/Loning19unifMLinterfaceTSsktime.pdf","text":"arXiv:1909.07872v1 [cs.LG] 17 Sep 2019 A PREPRINT - SEPTEMBER 18, 2019 of a uniﬁed API leads to unnecessary code replications and often error prone and statistically inappropriate reductions to those tasks that existing off-the-shelf toolboxes can deal with (e.g. reductions to tabular data supported by scikit- learn). A single API reduces confusion and enables us to focus on providing advanced time series analysis capabilities for researchers and practitioners. In addition, many tasks require common functionality such as distance measures and preprocessing routines. Providing them in a consistent and modular interface allows us to re-utilise them across different settings. As of now, sktime includes state-of-the-art algorithms for time series classiﬁcation, additional modular functionality for reduction, pipelining, ensembling and data transformations, as well as forecasting methods and benchmarking tools. We ﬁrst present in section 2 an overview of the most common time series tasks, and then discuss, in section 3, reduction as an approach to solving these tasks. Section 4 outlines the key design features built into the core interface. Section 5 describes the currently available functionality. We conclude by previewing future work in section 6. 2 Taxonomy of time series learning tasks sktime provides tooling for learning with time series, i.e. data observed at a ﬁnite number of known time points. More precisely, whenever we refer to time series, we consider them explicitly consisting of both (i) time points at which they are observed and (ii) observations at those time points. In ad-hoc short-hand notation, we write, for example, x(t1), x(t2), ..., x(tT ) for observations at time points t1, t2, ..., tT , and x for the time series object that contains exactly that information.4 An intrinsic characteristic of time series is that observations within time series are statistically dependent in assumed generative processes (which we avoid to introduce here to keep notation simple). Due to this dependency, time series data does not naturally ﬁt into the standard machine learning framework for tabular (or cross-sectional) data, which implicitly assumes observations to be independent and identically distributed (i.i.d.). Consequently, many toolboxes for learning on tabular data, such as scikit-learn, consider learning with time series out of scope [16]. When learning with time series, it is important to understand the different forms such data may take. The data can come in the form of a single (or univariate) time series; but in many applications, multiple time series are observed. It is crucial to distinguish the two fundamentally different ways in which this may happen: • Multivariate time series data, where two or more variables are observed over time, with variables representing different kinds of measurements within a single experimental unit; • Panel data, sometimes also called longitudinal data, where multiple independent instances of the same kinds of measurements are observed, e.g. time series from multiple industrial processes, chemical samples or patients.5 In multivariate data, it is implausible to assume the different univariate component time series are i.i.d. In panel data, the i.i.d. assumption applied to the different instances is plausible, while time series observations within a given instances may still depend on adjacent observations. In addition, panel data may be multivariate, which corresponds to i.i.d. instances of multivariate time series. In this case, the different instances are i.i.d., but the univariate component series within an instance are not. This richness of generative scenarios is mirrored in a richness of learning tasks applicable to such data. We later show that these tasks are closely related through reduction, but ﬁrst highlight some of the most common ones here: • Time series regression/classiﬁcation. We observe N i.i.d. panel data training instances of feature-label pairs (xi, yi), i = 1 . . . N . Each instance of features is a time series xi = (xi(t1) . . . xi(tT )). The task is to use the training data to learn a predictor ˆf that can accurately predict a new target value, such that ˆy = ˆf (x∗) for a new input time series x∗. For regression, yi ∈ R. For classiﬁcation, yi takes a value from a ﬁnite set of class values. Additionally, time-invariant features may be present. Compared to the tabular supervised setting, the only difference is that some features are time series, instead of being only primitives (e.g. numbers, categories or strings). Important sub-cases are (i) equally spaced observation times and (ii) equal length time series. For an overview of time series classiﬁcation, see [3, 25]. • Classical forecasting. Given past observations y = (y(t1) . . . y(tT )) of a single time series, the task is to learn a forecaster ˆf which can make accurate temporal forward predictions ˆy = ˆf (hj) of observations at given time A PREPRINT - SEPTEMBER 18, 2019 points h1 . . . hH of the forecasting horizon, where ˆy = (ˆy(h1) . . . ˆy(hH )) denotes the forecasted series. No i.i.d. assumption is made. Variants may be distinguished by the following: (i) whether one observes additional related time series (multivariate data); (ii) for multivariate data, whether one forecasts a single series or multiple series jointly (exogeneity vs vector forecasting) [45]; (iii) whether the forecasting horizon lies in the observed time horizon (in-sample predictions), in the future of the observed time series (forecasting), or for multivariate data, only in the future of the target variable but not the exogenous variables (nowcasting); (iv) whether there is a single time point to forecast (H = 1) or not (single-step vs multi-step); (v) whether the forecasting horizon is already known during training or only during forecasting (functional vs discrete forecast). For an overview of classical forecasting, see [14, 15, 35, 21]. • Supervised/panel forecasting. We observe N i.i.d. panel data training instances (yi), i = 1 . . . N . Each instance is a sequence of past observations yi = (yi(t1) . . . yi(tT )). The task is to use the training data to learn a supervised forecaster ˆf that can make accurate temporal forward predictions ˆyi = ˆf (y∗, hj) for a new instance y∗ at given time points h1 . . . hH of the forecasting horizon, where ˆyi = (ˆyi(h1) . . . ˆyi(hH )) is the forecasted series. Variants include panel data with additional time-constant features and the same variants as found in classical forecasting. For an overview, see [5, 58, 24]. • Time series annotation. For given observations x = (x(t1) . . . x(tT )) of a single time series, the task is to learn an annotator that accurately predicts a series of annotations ˆy = (ˆy(a1) . . . ˆy(aA)) for the observed series x, where a1 . . . aA denotes the time indices of the annotations. The task varies by value domain and interpretation of the annotations ˆy in relation to x: (i) in change-point detection, ˆy contains change points and the type of change point [30]; (ii) in anomaly detection, the aj are a sub-set of the tj and indicate anomalies, possibly with the anomaly type [14]; (iii) in segmentation, the aj are interpreted to subdivide the series x into segments, annotated by the type of segment [39]. Time series annotation is also found in supervised form, with partial annotations within a single time series, or multiple annotated i.i.d. panel data training instances [23, 28]. 3 Reductions with time series While these tasks deﬁne distinct learning settings, they are closely related, which enables us to solve them via reduction. Reduction essentially decomposes a given task into simpler tasks so that solutions to the simpler tasks can be composed to give a solution to the original task. A classical example of reduction in tabular supervised learning is one-vs-all classiﬁcation, reducing k-way multi-category classiﬁcation to k binary classiﬁcation tasks [11, 10, 9]. For time series, a common example of reduction is to solve classical forecasting through time series regression via a sliding window approach and iteration over the forecasting horizon [12]. Many reduction approaches are possible with time series, we highlight some of the most important ones in ﬁgure 1. Figure 1: Stylised overview of time series reduction approaches classical forecasting supervised forecasting time series regression tabular regression time series annotation supervised annotation time series classiﬁcation tabular classiﬁcation b c d g b c e g a a f f single time series panel data tabular data Notes: (a) annotate time series with future values, (b) rolling window method to convert single series into panel data with multiple output time periods [12], (c) ignore training set (e.g. ﬁt forecaster on test set only) or use training set for model selection, (d) iterate over output periods, optionally time binning/aggregation of output periods [12], (e) rolling window method to convert single series into panel data with single output period [23], (f) discretise output into one or more bins, (g) feature extraction [26, 19] or time binning/aggregation of input time points. Reduction offers several key advantages with regard to API design [8, 11]: First, reductions convert any algorithm for a particular task into a learning algorithm for the new task. Any progress on the base algorithm immediately transfers to the new task, saving both research and software development effort. Second, reductions are modular and composable. Applying some reduction approach to n base algorithms gives n new algorithms for the new task. Reductions can be 3 A PREPRINT - SEPTEMBER 18, 2019 composed to solve more complicated problems, e.g. ﬁrst reducing forecasting to time series regression which in turn can be reduced to tabular regression. Finally, reductions also help us better understand the relationship between tasks and reduce confusion between them. 4 API design The main goal of sktime is to create a uniﬁed API for multiple time series tasks, extending the common scikit-learn interface to the temporal setting, while staying close to its syntax and logic whenever possible. Following scikit- learn’s API allows us to re-utilise many of the algorithms available in scikit-learn, which is especially useful because of reduction to tabular tasks and because many specialised algorithms for time series are composites with tabular supervised learning algorithms as their components. The key design features of the sktime API are as follows: 4.1 Data representation Any machine learning library relies fundamentally on some data representation and the lack of powerful data structures for time series data has arguably been one of the main reasons for the lack of a uniﬁed interface. In order to combine different tasks and data formats, sktime requires a data container capable of handling multivariate and panel data with additional time-constant features, including time-heterogeneous time series, where the observed lengths and time points varies across instances and/or variables. While pandas [48] handles time series data, it is intended to store time series only in the long format, with rows representing time points and columns representing variables, precluding time-heterogeneous data. Technically, however, it is possible to store arbitrary types in the cells of pandas containers. Inspired by xpandas [20], we chose to exploit this feature and represent time series data in a nested format, with rows representing i.i.d. instances and columns representing different variables as before, but with cells now no longer representing only primitives but also entire time series. The reason for this choice is twofold: First, we can still make use of pandas, one of the most comprehensive and efﬁcient data container in Python, while at the same time having a consistent data representation across different tasks which is ﬂexible enough to handle multivariate, panel and time- heterogeneous data. Second, as rows still represent i.i.d. instances, this representation allows us to reuse many of the existing functionality in scikit-learn. Alternatives we considered but ultimately set aside include three-dimensional NumPy arrays [56] as used in [53] and xarray [34, 33], an extension of pandas to panel data, both however only support time-homogeneous data.6 4.2 Task-speciﬁc estimators We follow scikit-learn [57, 49] and Weka [32, 31] in adopting a uniform basic API for estimators, consisting of a ﬁt method used for learning a model from training data and a predict method used for making predictions based on the ﬁtted model, as well as a common interface for setting and retrieving hyper-parameters. Estimators in sktime are task speciﬁc, extending scikit-learn’s regressors and classiﬁers to their time series counterparts as well as adding new estimators such as forecasters and supervised forecasters among others, with the same ﬁt and predict methdods, but varying function signatures and internal behaviour. 4.3 Transformers Similar to estimators, transformers have a uniform API consisting of a ﬁt and a transform method used to trans- form input data, and a corresponding method for the inverse transformation if available, in addition to the common hyper-parameters interface. Since many data transformations are applicable for different tasks, we develop a uni- ﬁed transformer interface. To reconcile the different settings, we introduce the following kinds of transformations, distinguishing between ﬁtting over i.i.d. instances and ﬁtting over time points. • Tabular. scikit-learn like transformers, which operate over i.i.d. instances and are ﬁtted during training (e.g. prin- cipal component analysis). • Series-to-primitives. Operates over time points, transforms time series for each instance into a primitive number (e.g. feature extraction). If the transformer is ﬁttable, it is ﬁtted separately for each instance during both training and prediction. • Series-to-series. Like series-to-primitives, but output of transformation is itself a series instead of a primitive (e.g. Fourier transform or series of ﬁtted auto-regressive coefﬁcients). A PREPRINT - SEPTEMBER 18, 2019 • Detrending. Operates over time points and transforms an input time series, returning a detrended time series in the same domain as the input series (e.g. polynomial or seasonal detrending). Detrending tranformers keep track of the time index seen in ﬁtting, so that trends are correctly computed over new time indices when transforming new data. In forecasting, it is ﬁtted only during training. For panel data, it can be applied like a series-to-series transformation by iterating over instances. Detrenders are designed to take forecasters as input arguments, so that they internally ﬁrst ﬁt the forecaster to the input series and then return the residuals after subtracting the in-sample forecasts from the input series. For example, to detrend a time series with an exponential smoothing forecaster in sktime, we can write: A PREPRINT - SEPTEMBER 18, 2019 – Dictionary based. The symbolic aggregate approximation (SAX) [40] and symbolic Fourier approximation (SFA) [52] transform and the associated bag of patterns (BOP) [41] and bag of SFA symbols (BOSS) [51] classiﬁers. – Deep learning. A recent review paper described nine deep learning algorithms for time series classiﬁcation [25]. With the help of the authors, we have ported these algorithms into sktime in a deep learning extension package7 based on keras [18]. • Classical forecasting. We have implemented a range of statistical forecasting techniques, interfacing statsmodels [50] whenever possible, and reduction strategies to utilise supervised learning algorithms. • Transformers. Various transformers have been implemented for segmenting time series, series-to-primitives and series-to-series feature extraction and detrending. To reduce time series regression/classiﬁcation to tabular super- vised learning, a transformer for time binning input series is included. • Composition. Pipelining for both feature and target variables following scikit-learn’s API and multivariate com- posite strategies have been implemented. • Benchmarking. Inspired by the mlaut package [38], sktime includes tools for automatic orchestration of predic- tion experiments evaluating one or more models on one or more data sets, with post-hoc statistical methods for comparing predictive performances. The majority of the implemented algorithms has been tested for correctness against implementations in other languages and benchmarked on archive data [4]. We have reproduced the results presented in a comparative benchmarking study [3] and are in the process of recreating results from forecasting benchmarking studies [47, 46]. 6 Conclusion and future directions We have discussed the main rationale for a uniﬁed interface for machine learning with time series and outlined the key features of sktime’s API, including new meta-estimators for reduction and multivariate ensembling. For future devel- opment, there are several directions the sktime project aims to focus on and we are actively looking for contributors to implement task-speciﬁc interfaces and reduction approaches. At present, most methods in sktime only support data with equal length series and no missing values. We aim to extend existing functionality to cover these situations as well. In addition, the composite structure of many of the implemented time series classiﬁers allows us to easily refactor them into their regressor counterparts. Having designed and implemented the key building blocks of the API for time series classiﬁcation/regression and classical forecasting, the next major addition to sktime will be supervised forecast- ing, based on a modiﬁed pysf interface [29]. Finally, many implemented tools in sktime (e.g. distance measures) can be re-utilised for related unsupervised learning task, including time series clustering and motif discovery. Authors’ contributions ML made key contributions to architecture and design, including composition and reduction interfaces. ML is also one of sktime’s core contributors and maintainers, having implemented, and contributing to, almost all parts of it, including the overall framework, the forecasting module, and speciﬁc algorithms. ML drafted and wrote most of this manuscript, partly based on sketches and presentation content by FK. FK conceived the project and architectural outlines, including task taxonomy, composition and reduction. FK further made key contributions to architecture and design, and contributed to writing of this manuscript. AB implemented time series forest and the random interval spectral ensemble, and contributed design ideas and to writing of this manuscript. JL implemented and conceived modularised interfaces of several algorithms: distance based algorithms, including time series k-nearest-neighbours, Cython implementations of time series distance functions, and the shapelet transform. SG contributed to the initial design and implementation of the time series classiﬁcation setting, as well as implemen- tation of the overall framework. VK contributed to the design and implementation of the benchmarking module based on the mlaut package. All authors reviewed the manuscript and participated in ﬁnal copy-editing and proof-reading. A PREPRINT - SEPTEMBER 18, 2019 Acknowledgements We would like to thank all participants of the 2019 joint sktime/MLJ development sprint who helped implement various time series classiﬁcation algorithms, including Amaia Abanda Elustondo, Aaron Bostrom, Saurabh Dasgupta, David Guijo-Rubio, James Large, Matthew Middlehurst, George Oastler, Piotr Ole´skiewicz, Mathew Smith and Jeremy Sellier. The ﬁrst phase of development for sktime was done jointly between researchers at the University of East Anglia (UEA), University College London (UCL) and The Alan Turing Institute as part of a UK Research and Innovation (UKRI) project to develop tools for data science and artiﬁcial intelligence. Markus Löning’s contribution was supported by the Economic and Social Research Council (ESRC) [grant: ES/P000592/1], the Consumer Data Research Centre (CDRC) [ESRC grant: ES/L011840/1], and The Alan Turing Institute (EPSRC grant no. EP/N510129/1). References [1] A. Abanda, U. Mori, and J. Lozano. A review on distance based time series classiﬁcation. Data Mining and Knowledge Discovery, 33(2):378–412, 2019. [2] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, et al. Gluonts: Probabilistic time series models in python. arXiv preprint arXiv:1906.05264, 2019. [3] A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh. The great time series classiﬁcation bake off: a review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 31(3):606–660, 2017. [4] Anthony Bagnall, Markus Löning, Matthew Middlehurst, and George Oastler. A tale of two toolkits, report the ﬁrst: bench- marking time series classiﬁcation algorithms for correctness and efﬁciency. arXiv preprint arXiv:1909.05738, 2019. [5] Badi H. Baltagi. Econometric Analysis of Panel Data. John Wiley & Sons, 4 edition, 2008. [6] Stefan Behnel, Robert Bradshaw, Craig Citro, Lisandro Dalcin, Dag Sverre Seljebotn, and Kurt Smith. Cython: The best of both worlds. Computing in Science & Engineering, 13(2):31–39, 2011. [7] Christoph Bergmeir, Rob J Hyndman, and Bonsoo Koo. A note on the validity of cross-validation for evaluating autoregressive time series prediction. Computational Statistics & Data Analysis, 120:70–83, 2018. [8] Alina Beygelzimer, Varsha Dani, Tom Hayes, John Langford, and Bianca Zadrozny. Error limiting reductions between classiﬁcation tasks. In Proceedings of the 22nd international conference on Machine learning, pages 49–56. ACM, 2005. [9] Alina Beygelzimer, Hal Daumé, John Langford, and Paul Mineiro. Learning reductions that really work. Proceedings of the IEEE, 104(1):136–147, 2015. [10] Alina Beygelzimer, John Langford, and Bianca Zadrozny. Weighted one-against-all. In American Association for Artiﬁcial Intelligence (AAAI), pages 720–725, 2005. [11] Alina Beygelzimer, John Langford, and Bianca Zadrozny. Machine learning techniques—reductions between prediction quality metrics. In Performance Modeling and Engineering, pages 3–28. Springer, 2008. [12] Gianluca Bontempi, Souhaib Ben Taieb, and Yann-Aël Le Borgne. Machine Learning Strategies for Time Series Forecasting. In Business Intelligence, pages 62–77. Springer, Berlin, Heidelberg, 2013. [13] A. Bostrom and A. Bagnall. Binary shapelet transform for multiclass time series classiﬁcation. Transactions on Large-Scale Data and Knowledge Centered Systems, 32:24–46, 2017. [14] George E. P. Box, Gwilym M. Jenkins, Gregory C. Reinsel, and Greta M. Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [15] Peter J. Brockwell and Richard A. Davis. Introduction to Time Series and Forecasting. Springer Texts in Statistics. Springer International Publishing, 2016. [16] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas C Müller, Olivier Grisel, Vlad Niculae, Peter Pret- tenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. API design for machine learning software: experiences from the scikit-learn project. ArXiv e-prints, 2013. [17] David M Burns and Cari M Whyne. Seglearn: a python package for learning sequences and time series. The Journal of Machine Learning Research, 19(1):3238–3244, 2018. [18] François Chollet et al. Keras. https://keras.io, 2015. [19] Maximilian Christ, Nils Braun, Julius Neuffer, and Andreas W. Kempa-Liehr. Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package). Neurocomputing, 307:72–77, sep 2018. [20] Vitaly Davydov and Franz J Király. xpandas: Python data containers for structured types and structured machine learning tasks. openreview.net, 2018. 7 A PREPRINT - SEPTEMBER 18, 2019 [21] Jan G De Gooijer and Rob J Hyndman. 25 years of time series forecasting. International journal of forecasting, 22(3):443– 473, 2006. [22] Houtao Deng, George Runger, Eugene Tuv, and Martyanov Vladimir. A time series forest for classiﬁcation and feature extraction. Information Sciences, 239:142–153, 2013. [23] Thomas G. Dietterich. Machine Learning for Sequential Data: A Review. In Caelli T., Amin A., Duin R.P.W., de Ridder D., and Kamel M., editors, Structural, Syntactic, and Statistical Pattern Recognition, pages 15–30. Springer, Berlin, Heidelberg, 2002. [24] Peter Diggle, Patrick Heagerty, Kung-Yee Liang, and Scott Zeger. Analysis of longitudinal data. Oxford University Press, 2nd edition, 2013. [25] H. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P. Muller. Deep learning for time series classiﬁcation: a review. Data Mining and Knowledge Discovery, 33(4):917–963, 2019. [26] Ben D Fulcher and Nick S Jones. hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction. Cell systems, 5(5):527–531.e3, nov 2017. [27] J. Grabocka, N. Schilling, M. Wistuba, and L. Schmidt-Thieme. Learning time-series shapelets. In Proc. 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014. [28] Alex Graves. Supervised sequence labelling. In Supervised sequence labelling with recurrent neural networks, pages 5–13. Springer, 2012. [29] Ahmed Guecioueur. pysf: Supervised forecasting of sequential data in Python, 2018. [30] Valery Guralnik and Jaideep Srivastava. Event detection from time series data. In Proceedings of the ﬁfth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 33–42. ACM, 1999. [31] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. The WEKA data mining software. ACM SIGKDD Explorations Newsletter, 11(1):10, nov 2009. [32] G. Holmes, A. Donkin, and I.H. Witten. WEKA: a machine learning workbench. In Proceedings of ANZIIS ’94 - Australian New Zealnd Intelligent Information Systems Conference, pages 357–361. IEEE, 1994. [33] Stephan Hoyer, Clark Fitzgerald, Joe Hamman, et al. xarray: v0.8.0, August 2016. [34] Stephan Hoyer and Joseph J. Hamman. xarray: N-D labeled Arrays and Datasets in Python. Journal of Open Research Software, 5(1), apr 2017. [35] Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018. [36] James Max Kanter and Kalyan Veeramachaneni. Deep feature synthesis: Towards automating data science endeavors. In 2015 IEEE International Conference on Data Science and Advanced Analytics, DSAA 2015, Paris, France, October 19-21, 2015, pages 1–10. IEEE, 2015. [37] I. Karlsson, Papapetrou P, and H. Boström. Forests of randomized shapelet trees. In International Symposium on Statistical Learning and Data Sciences, pages 126–136. Springer, 2015. [38] Viktor Kazakov and Franz J Király. Machine learning automation toolbox (mlaut). arXiv preprint arXiv:1901.03678, 2019. [39] Eamonn Keogh, Selina Chu, David Hart, and Michael Pazzani. Segmenting time series: A survey and novel approach. In Data mining in time series databases, pages 1–21. World Scientiﬁc, 2004. [40] J. Lin, E. Keogh, L. Wei, and S. Lonardi. Experiencing SAX: a novel symbolic representation of time series. Data Mining and Knowledge Discovery, 15(2), 2007. [41] J. Lin, R. Khade, and Y. Li. Rotation-invariant similarity in time series using bag-of-patterns representation. Journal of Intelligent Information Systems, 39(2):287–315, 2012. [42] J. Lines and A. Bagnall. Time series classiﬁcation with ensembles of elastic distance measures. Data Mining and Knowledge Discovery, 29:565–592, 2015. [43] J. Lines, S. Taylor, and A. Bagnall. Time series classiﬁcation with HIVE-COTE: The hierarchical vote collective of transformation-based ensembles. ACM Trans. Knowledge Discovery from Data, 12(5), 2018. [44] B. Lucas, A. Shifaz, C. Pelletier, L. O’Neill, N. Zaidi, B. Goethals, F. Petitjean, and G. Webb. Proximity forest: an effective and scalable distance-based classiﬁer for time series. Data Mining and Knowledge Discovery, 33(3):607–635, 2019. [45] Helmut Lütkepohl. New introduction to multiple time series analysis. Springer Science & Business Media, 2005. [46] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. Statistical and machine learning forecasting methods: Concerns and ways forward. PloS one, 13(3):e0194889, 2018. [47] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting, 2019. [48] Wes McKinney. pandas: a Foundational Python Library for Data Analysis and Statistics. In Python for High Performance and Scientiﬁc Computing, 2011. 8 A PREPRINT - SEPTEMBER 18, 2019 [49] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine Learning in Python. The Journal of Machine Learning Research, 12:2825–2830, 2011. [50] Josef Perktold and Skipper Seabold. Statsmodels: Econometric and Statistical Modeling with Python Quantitative histology of aorta View project Statsmodels: Econometric and Statistical Modeling with Python. In Proceedings of the 9th Python in Science Conference, 2010. [51] P. Schäfer. The BOSS is concerned with time series classiﬁcation in the presence of noise. Data Mining and Knowledge Discovery, 29(6):1505–1530, 2015. [52] P. Schäfer and M. Högqvist. SFA: a symbolic Fourier approximation and index for similarity search in high dimensional datasets. In Proceedings of the 15th International Conference on Extending Database Technology, pages 516–527, 2012. [53] Romain Tavenard. tslearn: A machine learning toolkit dedicated to time-series data, 2017. [54] R Taylor. Pyﬂux: An open source time series library for python, 2016. [55] Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45, 2018. [56] Stéfan van der Walt, S Chris Colbert, and Gaël Varoquaux. The NumPy Array: A Structure for Efﬁcient Numerical Computa- tion. Computing in Science & Engineering, 13(2):22–30, mar 2011. [57] Gaël Varoquaux, L. Buitinck, Gilles Louppe, Olivier Grisel, F. Pedregosa, and A. Mueller. Scikit-learn: Machine Learning Without Learning the Machinery. GetMobile: Mobile Computing and Communications, 19(1):29–33, jun 2015. [58] Jeffrey M Wooldridge. Econometric analysis of cross section and panel data. MIT Press, 2010. 9","libVersion":"0.3.2","langs":""}
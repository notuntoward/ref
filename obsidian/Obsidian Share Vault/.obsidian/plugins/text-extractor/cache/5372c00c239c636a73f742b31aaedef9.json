{"path":"lit/lit_sources.backup/Ye18NovelTransferLearning.pdf","text":"Contents lists available at ScienceDirect Knowledge-Based Systems journal homepage: www.elsevier.com/locate/knosys A novel transfer learning framework for time series forecasting Rui Ye, Qun Dai⁎ College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 211106, China ARTIC L E I NF O Keywords: Time series prediction Transfer learning Extreme learning machine (ELM) Online learning Ensemble learning ABSTRAC T Recently, many excellent algorithms for time series prediction issues have been proposed, most of which are developed based on the assumption that suﬃcient training data and testing data under the same distribution are available. However, in reality, time-series data usually exhibit some kind of time-varying characteristic, which may lead to a wide variability between old data and new data. Hence, how to transfer knowledge over a long time span, when addressing time series prediction issues, poses serious challenges. To solve this problem, in this paper, a hybrid algorithm based on transfer learning, Online Sequential Extreme Learning Machine with Kernels (OS-ELMK), and ensemble learning, abbreviated as TrEnOS-ELMK, is proposed, along with its precise mathe- matic derivation. It aims to make the most of, rather than discard, the adequate long-ago data, and constructs an algorithm framework for transfer learning in time series forecasting, which is groundbreaking. Inspired by the preferable performance of models ensemble, ensemble learning scheme is also incorporated into our proposed algorithm, where the weights of the constituent models are adaptively updated according to their performances on fresh samples. Compared to many existing time series prediction methods, the newly proposed algorithm takes long-ago data into consideration and can eﬀectively leverage the latent knowledge implied in these data for current prediction. In addition, TrEnOS-ELMK naturally inherits merits of both OS-ELMK and ensemble learning due to its incorporation of the two techniques. Experimental results on three synthetic and six real- world datasets demonstrate the eﬀectiveness of the proposed algorithm. 1. Introduction As an important direction of dynamic data analysis and processing, time series prediction has stirred up broad attention in many research ﬁelds. For the pressing demand of forecasting the future data trend based on historical information, time series prediction comes into wide use in a variety of practical situations, involving engineering, econo- metrics and natural sciences, etc. [1,2]. Over the past decades, much excellent eﬀort has been contributed to the development of time series prediction, such as traditional linear models [3], e.g., exponential smoothing, autoregressive integrated moving average, and nonlinear models [4–7], e.g., neural networks, support vector machines with nonlinear kernel, extreme learning machines and other machine learning methods. Due to the attractive properties and eﬀective performances, Extreme Learning Machines (ELMs) have been widely applied to many ﬁelds [8]. ELMs are originally proposed for single hidden layer feedforward net- works (SLFNs) [8]. Unlike conventional neural networks, the weights connecting input layer and hidden layer and the bias values in SLFNs can be randomly generated in ELMs, where they need not to be adjusted during the training process. Besides, according to the theory of Moore- Penrose generalized matrix, ELMs can eﬀectively obtain the output weights and the output matrix of hidden layer, which vests the algo- rithm with extremely fast learning capability and good generalization performance. However, since conventional ELMs essentially rely on the linear combination of a ﬁxed number of nonlinear expansions of input vector, presupposing the number of hidden nodes before learning is of great importance [8,9]. Admittedly, how to appropriately determine the hidden-node number poses serious challenge for ELMs [10,11]. To make up for the deﬁciency of classical ELMs, an extended method, namely kernel extreme learning machine (ELMK), was pro- posed recently by Huang et al. [12,13]. It is derived from the modeling and solution process of support vector machines (SVMs) and other kernel methods. Instead of using random feature mappings, ELMK in- corporates kernel functions with ELMs and forms new kernel mappings, which can conquer the diﬃculty of determining appropriate number of hidden nodes [14,15]. ELMK eliminates the defect of classical ELMs and has been veriﬁed to have similar or even better prediction capacity. Nevertheless, since ELMK is essentially based on batch learning, it adopts the oﬄine learning strategy, that means, ELMK takes all the available data into account at once. With the sequential arrival of new samples, it is diﬃcult for ELMK to real-time update the output weights, https://doi.org/10.1016/j.knosys.2018.05.021 Received 15 December 2017; Received in revised form 14 May 2018; Accepted 16 May 2018 ⁎ Corresponding author. E-mail address: daiqun@nuaa.edu.cn (Q. Dai). Knowledge-Based Systems xxx (xxxx) xxx–xxx 0950-7051/ © 2018 Elsevier B.V. All rights reserved. Please cite this article as: Ye, R., Knowledge-Based Systems (2018), https://doi.org/10.1016/j.knosys.2018.05.021 which inevitably imposes limitations on addressing real-time situations. In allusion to this problem, some extensions have been proposed to strengthen the robustness of ELM and ELMK for dealing with online problems, such as online sequential ELM (OS-ELM) [16–18], online sequential fuzzy ELM (OS-fuzzy-ELM) [19] and the recently proposed algorithm called online sequential extreme learning machine with kernels (OS-ELMK) [20,21], where samples arrive in a sequential form, one by one or chunk by chunk. Empirical simulations demonstrate that the aforementioned algo- rithms usually have considerable performances in the applications of time series prediction issues. However, most algorithms designed for time series prediction are based on the assumption that training sam- ples and testing samples are drawn from the same distribution, and abundant data for training are available. Nevertheless, in many prac- tical situations, time series data are most likely to vary over time, re- sulting in a wide variability between old data and new data. In addition, in some real-time cases, size of sequentially arrived samples may be relatively small at a time, which may lead to insuﬃcient data for training in the early period. Hence, when tackling the issue of scant fresh training samples, as is often the case in time series prediction, how to transfer knowledge of previous data, whose observation time is far apart from new samples, to the predictive process is of great sig- niﬁcance. However, up till now, little research with respect to this as- pect has been conducted. In our paper, we lay much emphasis on how to deal with the aforementioned problem. Since the conundrum we now face is how to make the most of old data and maximally leverage their knowledge for implementing the current new prediction task, this problem, to some extent, is similar to that of transfer learning. Diﬀerent from the traditional machine learning paradigm, transfer learning breaks the constraint that training data and test data should be under the same distribution [22–24].It refers to the problem of reusing and transferring knowledge of one domain to other diﬀerent but related domains. Generally speaking, the primary objective of transfer learning is to retain experience learnt from past data and apply it to current problems. It is desirable to learn an eﬃcient model with only a handful of fresh samples and relatively abundant old data for time series prediction issues. Hence, based on the appealing properties of transfer learning, we incorporate it with online sequential extreme learning machines with kernels (OS-ELMK) for time series prediction. Inspired by the fact that performance of an ensemble composing of multiple models is generally superior to that of individual ones, the ensemble learning idea is incorporated in our implementation [25,26]. Consequently, a hybrid algorithm incorporating transfer learning, OS- ELMK and ensemble learning, abbreviated as TrEnOS-ELMK, is pro- posed ultimately in this paper. Compared to conventional time series prediction methods, TrEnOS-ELMK takes previous data, which possess a long time span with current data, into consideration, and ﬁrst forays into constructing a novel framework for time series prediction based on transfer learning, where knowledge learnt from old data can be eﬀec- tively leveraged for the present predictive task. While few related in- vestigations involving this aspect have been carried out till now. De- tailed descriptions of the proposed algorithm are speciﬁed in the sequel. Fig. 1 presents the construction of the proposed method. In the ﬁrst step, as shown in Fig. 1, abundant past data having a long time span with current new samples are extracted. Due to the property of time- varying, a wide variability usually lays athwart old samples and new samples. With the application of transfer learning, knowledge learnt from old data is retained and tactfully leveraged for a future predictive process. In particular, kernel matrix and the corresponding weight matrix of ELMK are ﬁrstly generated over the old data. Next, by virtue of the past experience and a handful of fresh sam- ples, new kernels and weight matrix can be obtained for the following prediction. In the second step, strategy imitating OS-ELM is im- plemented over the sequentially arriving time series data. Speciﬁcally, kernels and weight matrix are online updated with the arrival of new samples. Motivated by the preferable performance of ensemble learning, several models are generated and updated to construct an ensemble. Besides, to maintain a satisfactory performance, weights assigned to diﬀerent models are adaptively updated according to their corre- sponding performances on the available new data, where models with higher eﬃcacy are usually endowed with heavier weights. With the arrival of fresh samples, if the ensemble performance falls short of standards, the poorest model will be replaced with a newly trained one. The main contributions of this work can be summarized as follows: Firstly, most methods designed for time series prediction are based on the assumption that suﬃcient training data and testing data drawn from the same distribution are available. Nevertheless, in some prac- tical situations, this assumption may fail to be met because of the time- varying property of time series data. However, up to now, few re- searches regarding this aspect have been put forward. To alleviate this problem, a new hybrid algorithm incorporating transfer learning with OS-ELMK, abbreviated as TrOS-ELMK, is proposed with a mathematical derivation. It tactfully employs transfer learning for time series pre- diction and ﬁrst attempts to make the most of long-ago data rather than throwing away it. Due to the incorporation of kernel trick, OS-ELMK does not need to determine the number of hidden nodes, which, to some extent, en- hances its robustness and prediction ability. Admittedly, our newly proposed algorithm inherits the advantages of OS-ELMK and irons out its ﬂaw of failing to leveraging knowledge of previous data for new predictive tasks. Compared to existing methods, the newly proposed algorithm can make the most of past experience, and can get a pre- ferable performance, even though scarce fresh samples are available. Secondly, inspired by the considerable performance of ensemble learning, a collection of models endowed with weights is generated in our algorithm. Instead of equally treating all the components in the ensemble, weights allocated for diﬀerent models are updated with the arrival of new data, and superior models are remained while inferior ones are eliminated. Thus, the ensemble alters with new time series rather than remaining unchanged, aiming to highlight the eﬀects of well-performed models. More speciﬁcally, to ensure the ensemble's performance, weights assigned to diﬀerent models are adaptively ad- justed according to their prediction capacities over new coming data. Moreover, models with poor performance would be replaced if the ensemble's prediction ability was substandard. Therefore, the ﬁnal en- semble can maintain the preferable performance adaptively, owning good characters of better variety and higher feasibility. The rest of this paper is organized as follows. In Section 2, we brieﬂy review some previous work regarding time series prediction and transfer learning. In Section 3, exhaustive analysis of TrOS-ELMK is deduced. Then detailed elucidations of our newly proposed TrEnOS- ELMK are presented in Section 4. Experiments over synthetic and real- world datasets are carried out in Section 5, where corresponding ex- perimental results are reported and discussions are provided. Finally, a summary of our work is displayed in Section 6. 2. Related work 2.1. Existing methods for time series prediction Researches about time series prediction turn out to have a long history. Remarkable results and assorted breakthroughs have been made during past decades. Put crudely, the existing time series pre- diction methods can be roughly divided into the following two cate- gories: Linear predicting methods: With these methods, time series pre- diction models are constructed on basis of linear functional forms. For example, the earliest proposed autoregressive (AR) models aimed to R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 2 minimize the squared errors between the predictive results and the real results. Afterwards, some new linear methods, including moving average (MA) models and autoregressive moving average (ARMA) models, were put forward, consecutively, where ARMA has been widely used in many situations [3,27]. Nonlinear predicting methods: Since in most cases, time series data are usually nonlinear and non-stationary, conventional linear methods can hardly accommodate to these situations. To mitigate this problem, methods based on neural networks (NNs), radial basis function (RBF) neural networks [28] and support vector machines (SVMs) with non- linear kernel were proposed. Empirical results demonstrate that these nonlinear methods usually have preferable and more reliable perfor- mance in comparison with linear methods. 2.2. Application of ELM for time series prediction Extreme learning machine is a classical learning algorithm, which is ﬁrstly set forth by Huang et al. for SLFNs [8]. In ELM, input weights and bias values of hidden layer are randomly assigned. Since ELM only involves basic matrix operations of multiplication and Moore-Penrose generalized inverse, it usually has a faster learning speed. Due to these advantages, ELM has been widely applied to time series prediction. For example, an online extension of ELM called online sequential ELM (OS- ELM) is put forward by Liang et al. [29], and is employed for time series prediction, whether samples come one by one or chunk by chunk. On basis of OS-ELM, a modiﬁed algorithm called online sequential improved error minimized extreme learning machine (OSIEM-ELM) is proposed by Xue et al. [30]. Compared to OS-ELM, OSIEM-ELM can automatically choose the optimal number of hidden nodes. Algorithm called adaptive ensemble models of extreme learning machines is pre- sented by Mark van Heeswijk [31], aiming to apply adaptive ensemble model to time series prediction. Recently, a considerable algorithm incorporating kernel ELM with OS-ELM is raised, which not only in- herits the merits of ELMK, but also has a high eﬃciency and superior prediction accuracy. 2.3. Transfer learning and online learning Diﬀerent from traditional machine learning methods, transfer learning breaks the assumption that training data and test data must obey the same distribution. It hammers at remaining and leveraging knowledge from past experience for new diﬀerent but related domains. Up to now, many profound researches related to this subject have been put forward. For example, in [32], a creative method of evaluating the relatedness among source and target domains is proposed. By calcu- lating the transferred weight, eﬀectiveness of transferring knowledge from a given source domain could be measured. Then we can determine how much of knowledge from the source domain should be transferred. In [33], transfer learning methods based on computational in- telligence are systematically and exhaustively inspected. In the paper, authors elaborate on the application domains of transfer learning and methodically present several computational intelligence-based transfer learning methods. State-of-the-art knowledge of transfer learning is clearly provided in this paper. In [34], a novel transfer learning method based on Takagi-Sugeno fuzzy regression is proposed. It focuses on handling regression problems and tactfully extracts fuzzy rules from source domains, making it possible to preserve the privacy of the source data. S. Al-Stouhi et al. [35] tactfully integrated the idea of Adaboost with transfer learning and proposed an algorithm named TrAdboost for transfer learning issues. A novel method called Cross-Domain SVM (CDSVM), aiming to ﬁnd a reasonable boundary of tagged target data in virtue of support vectors extracted from source data, is proposed in [36]. Online learning has been studied for ages. It pays attention to the case that training samples arrive sequentially, while many typical ma- chine learning methods are under the assumption that all the training samples are available at a time. Online learning has been widely ap- plied to many real-world issues. Recently, some intriguing views about applying transfer learning to online learning tasks have been put forward by researchers. For in- stance, in [37], Zhao et al. creatively proposed an online transfer learning framework (OTL), aiming to transfer knowledge from source Fig. 1. Block diagram for the problem setting and the main idea of the proposed TrEnOS-ELMK algorithm. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 3 domain to an online learning target task. Two aspects, including ap- plying OTL to a homogeneous domain and applying OTL across het- erogeneous domains are discussed in this research. OTL is further ex- tended in [38]. In this paper, authors aims to tackle the challenges of negative transfer, when studying OTL in homogeneous domains, and the inconsistence of feature spaces, when studying OTL across hetero- geneous domains. In addition, OTL is also applied to addressing some complex real-world problems in this work. In [39], two novel online boosting algorithms are proposed, re- spectively, for transfer learning and multitask learning, enabling them to be used in an anytime setting. In [40], a novel method called online heterogeneous transfer (OHT) by hedge ensemble is proposed by Yan et al. In this method, oﬄine decision function and online decision function are combined by a hedge weighting strategy, making knowl- edge successfully transferred from source domains to online target domain. In [41], a novel online transfer learning technique is proposed by Wu et al. Instead of using a single source domain, this method aims to leverage knowledge from multiple homogeneous or heterogeneous source domains, whose weights are dynamically adjusted, to help im- prove performance on an online target domain. The research work in [42] focuses on investigating the way of making good use of multiple source domains, and transferring knowledge from them to the target domain. To better use these source domains, their transference are dynamically adjusted by the Hedge strategy. 3. Exhaustive derivation of the proposed TrOS-ELMK algorithm In order to clearly describe our proposed algorithms in this work, at ﬁrst, we summarize the frequently used notations in Table 1. 3.1. Extreme learning machine with kernels ELM was originally put forward for SLFNs and needs not to turn feature mappings of hidden layer. Similar to the structure of SLFNs, three layers, including input layer, hidden layer and output layer, are also involved in ELM. Supposing there are N arbitrary distinct samples =xt i N( , ), 1, 2, ...,ii , where =∈xx x x R[ , , ..., ]ii i in T n 12 , ti ∈ R, ELM can be mathematically modeled as ∑ += = = ∼ θg σ x b o j N( · ) , 1, 2, ..., , i N ii j i j 1 (1) where =σ σσ σ[ , , ..., ]ii i in T 12 represents the weight vector connecting input nodes and the i-th hidden node, and θi ∈ R is the weight con- necting the i-th hidden node and the output nodes, g( · ) is the activation function, oj ∈ R represents the output of the j-th sample, ∼ N is the number of hidden nodes, and bi represents the bias value of the i-th hidden node. For the sake of clarity, Eq. (1) can be compactly expressed as: =Hθ O (2) where =O oo o[ , , ..., ]N T 12 is the output of ELM, =⋯ ∼θθ θ θ[, , , ]N T 12 is weight vector connecting hidden nodes and output nodes, =⋯Xx x x[, , , ]N T 12 is the input of ELM and = ⎡ ⎣ ⎢ ⎢ +⋯ + ⋮⋮ +⋯ + ⎤ ⎦ ⎥ ⎥ × ∼∼ ∼∼ ∼ H gσ x b gσ x b gσ x b gσ x b (· ) ( · ) (· ) ( · ) NN N N N N NN 11 1 1 11 (3) In general, given a time series {h1, h2, ..., hn} composed of n ob- servations, one-step-ahead time series prediction aims to predict the next value +hn 1 of the time series. Hence, matrix =⋯Xx x x[, , , ]N T 12 is the input matrix of the model, where ==xx x x i N[ , , ..., ], 1, 2, ...,ii i in12 is the i-th sample. Prediction =O oo o[ , , ..., ]N T 12 of the N samples is the output of the model, where oi is the output value of xi. =Tt t t[ , , ..., ]N T 12 consists of the target values of N samples. Ideally, if ELM can learn all the training samples without residuals, there usually exists θ making the following equation true: =Hθ T (4) where =Tt t t[ , , ..., ]N T 12 is the target values vector, and the purpose of ELM is equivalent to ﬁnd a least-squares solution of Eq. (4). = +θH T͠ (5) where +H is the Moore-Penrose generalized inverse of H. Extreme learning machine with kernels (ELMK) is an extension of ELM. It omits the selection of hidden nodes number, and replaces the random feature mapping with kernel function mapping. Inspired by the idea of SVM, deﬁnition of ELM kernel was proposed as: ==K φx φx K x x()· ( ) ( , )ij i j i j (6) where φ( · ) is the unknown feature mapping. Then the hidden layer outputs matrix of Eq. (3) can be expressed as Eq. (7): = ⎡ ⎣ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ × H Kx x Kx x Kx x Kx x (, ) (, ) (, ) (, ) N NN N N N 11 1 1 (7) Since, in this work, we focus on the one-step-ahead time series prediction, output of each sample is a scalar, that is, ti ∈ R. Supposing φ(x) is the feature mapping function, mapping sample x into a vector in a feature space χ, we aim to ﬁnd an optimal vector β in χ, according to the optimization problem of ELMK, which can be formulated as: =+ ∑ =− = =Min L β C ξ st φ x β t ξ i N : . . ( )· , 1, 2, ..., P i N i ii i 1 2 2 1 2 1 2 (8) where C is the regularization parameter, and ξi is the training error with respect to the training sample xi. Based on Lagrangian multiplier method, to train ELMK is equivalent to solving the following Lagrangian problem: ∑∑=+ − − + == Lβ C ξ α φ x β t ξ 1 2 1 2 (()· )D i N i i N ii i i 2 1 2 1 (9) where αi is the i-th Lagrangian multiplier. 3.2. Mathematical derivation of the proposed TrOS-ELMK algorithm To better introduce and give an overview of the algorithms pro- posed in this work, here at ﬁrst, we brieﬂy introduce the overall fra- mework of our ﬁnally proposed algorithm, i.e., a hybrid algorithm in- corporating transfer learning, OS-ELMK and ensemble learning, called TrEnOS-ELMK for short. It can be roughly divided into two parts. The Table 1 Deﬁnitions of the major mathematical notations in our paper. Notations Descriptions ∈×xt R R( ,)k s k s n Old samples ∈×xt R R( ,)i t i t n Existing new samples ∈×xt R R( ,)m e m e n Newly arriving samples ws Output weight vector for old samples wt Output weight vector for existing new samples we Output weight vector for newly arriving samples Ns Number of old samples Nt Number of samples in existing new dataset Ne Number of samples in newly arriving dataset D Old Old dataset D New Newly arriving dataset De New Existing new dataset SEl c The L-th model's performance on the c-th newly coming data block Knew The L-th model's error on the c-th newly coming data block ωl c The L-th model's weight on the c-th newly coming data block NumC Number of newly coming data blocks ωl The L-th model's ﬁnal weight R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 4 ﬁrst part is spelled out in Section 3.2 and the second part is described in Section 4. In the ﬁrst part, several ELMK models, according to Phase 1 in TrOS- ELMK algorithm, are generated by transferring knowledge from old data to the existing new data. They are used to construct the initial ensemble, where each model is allocated with an equal weight. In the second part, to ensure the performance of the ensemble, parameters and weights of the models are updated with the newly coming data. Speciﬁcally, with the arrival of new samples, parameters of each model are updated according to Phase 2 of TrOS-ELMK algo- rithm. Simultaneously, as presented in TrEnOS-ELMK algorithm, the weight of each model is also updated adaptively. In our paper, we not only consider the model's predictive accuracy on the newly coming dataset, but also take its performance on the previous datasets into account. Broadly speaking, weights of models with poor comprehensive performance are usually decreased. Besides, if the ensemble's perfor- mance is not up to the predeﬁned standard, a new model trained on the newly coming dataset is used to replace the poorest one in the en- semble. 3.2.1. Transfer learning part of TrOS-ELMK As is mentioned before, many existing methods for time series prediction rely on the assumption that suﬃcient fresh samples are available, so that the training data and test data are under the same distribution. However, in reality, situations failing to meet the above conditions are also likely to be encountered. For example, in some cases, the size of new data coming chunk by chunk may be small, re- sulting in relatively small amount of fresh training data, while abundant old data can be obtained. Since time series data usually vary with time, samples over a long time span diﬀer widely from each other commonly. Hence, applying old data directly to prediction process is normally considered inadvisable. To solve this problem, an algorithm based on the incorporation of transfer learning with ELMK, abbreviated as TrELMK, is proposed in this paper. To compensate for traditional time series methods’ deﬁciency of failing to leveraging knowledge from old data, experiences drawn on transfer learning are incorporated with ELMK in our paper. The pro- blem of extracting knowledge from past data can be mathematically formulated as Eq. (10). Firstly, we need to construct an ELMK model over the old data. As can be seen in Eq. (5), since the output weight vector in the original ELM is a least-squares solution, it usually has the minimum norm. Hence, when generating ELMK models, we aim to ﬁnd a model with the minimal norm parameter vector and the least training errors for samples, which can be formulated as follows: =+ ∑ =− =Lw ξ st ϕ x w t ξ min .. ( )· s C k N k s k s s k s k s 1 2 2 2 1 2s s (10) where samples obtained from previous data are denoted as =xt k N( , ), 1, 2, ...,k s k s s, Cs ∈ R is the regularization parameter for the old dataset, ξk s is the training error, and ws denotes the output weight vector, equivalent to β in Eq. (8). The corresponding Lagrangian pro- blem of Eq. (10) is formulated as follows: ∑∑=+ − − + == Lw C ξα ϕ x w t ξ 1 22 (( )· )Ds s k N k s k N k s k s s k s k s2 1 2 1 ss (11) where αk s is the k-th Lagrangian multiplier, and the problem can be solved by Lagrangian multiplier method. ∑∑ ∂ ∂ =− = ⇒ = == L w w α φx w α φx() 0 ( ) D s s k N k s k s s k N k s k s 11 ss (12) ∂ ∂ =− = ⇒ = L ξ Cξ α α Cξ0 D k s s k s k s k s s k s (13) ∂ ∂ =− + = L α ϕx w t ξ()· 0 D k s k s s k s k s (14) Plugging Eq. (12) and Eq. (13) into Eq. (14) can we get: ∑ −+ = = ⇒ ∑ −+ = = ⇒ ⎛ ⎝ ⎜ ⎜ ⎡ ⎣ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ + ⎞ ⎠ ⎟ ⎟ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ = ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ = = αφ x φ x t k N αK x x t k N Kx x Kx x Kx x Kx x α α α t t t ( )· ( ) 0, 1, ..., ( , ) 0, 1, ..., (, ) (, ) (, ) (, ) j N j s j s k s k s α C s j N j s j s k s k s α C s ss s N s N ss N s N s I C s s N s s s N s 1 1 11 1 1 1 2 1 2 s k s s s k s s s ss s s ss (15) Here we use fs(x) to represent the output function based on ws: ∑== = = f xo xϕ x w α ϕ x ϕ x() () ()· ()· ( ).s ss k N k s k s 1 s (16) On analysis of Eq. (15), it can be concluded that, knowledge related to previous samples can be obtained by using our proposed method. Next, it is desirable for us to transfer the knowledge to current pre- diction tasks. Given the available training set =xt i N( , ), 1, 2, ...,i t i t t, the problem waiting to be settled can be formulated as Eq. (17). Firstly, as shown in the traditional ELM in Eq. (5), output weight vector is a least-squares solution with the minimum norm. Hence, when constructing ELMK models for the available new data, we need to en- sure the norm of wt as small as possible. Besides, since ws is the output weight vector for the long-ago data (source samples), it can be regarded as the knowledge transferred from source domain. Similarly, wt can be seen as the knowledge learned for target domain. To make good use of the source knowledge, it is desirable to minimize the diﬀerence be- tween ws and wt. Because, in this case, knowledge of the source domain can be furthest employed to help solve the target task, i.e., calculating the target output weight vector wt. Besides, in order to ensure the prediction performance of the model, it is necessary to minimize the training error on the available new samples. According to the above analyses, the problem of transferring knowledge from the old data can be formulated as Eq. (17). =+ − + ∑ =− =Lw μ w w ξ st ϕ x w t ξ min .. ( )· tt s C i N i t i t t i t i t 1 2 2 1 2 2 2 1 2t t (17) where wt is the output weight vector over the new data, and μ denotes the penalty parameter. Based on Lagrangian multiplier method, to solve Eq. (17) is equivalent to solving the corresponding Lagrangian problem: ∑ ∑ =+ − + −− + = = Lw μ w w C ξ αϕ x w t ξ 1 2 1 22 (( )· ) Dt t s t i N i t i N i t i t t i t i t 22 1 2 1 t t (18) ∑ ∑ ⇒ ∂ ∂ =+ − − = ⇒ = + ⎛ ⎝ ⎜ + ⎞ ⎠ ⎟ = = L w wμ ww α ϕ x w μ μw α ϕ x () ( ) 0 1 1 () D t tt s i N i t i t t s i N i t i t 1 1 t t (19) ⇒ ∂ ∂ =− = ⇒ = L ξ Cξ α α Cξ0 D i t t i t i t i t t i t (20) ⇒ ∂ ∂ =− + = L α ϕx w t ξ()· 0 D i t i t t i t i t (21) Plug Eq. (18) and Eq. (19) into Eq. (20), and we can get the fol- lowing formulations: R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 5 ⎜⎟ ⎜⎟ ⎛ ⎝ + ∑ ⎞ ⎠ −+ = ⇒ ⎛ ⎝ ∑ + ∑ ⎞ ⎠ −+ = ⇒ ∑ + ∑ −+ = ⇒ ⎛ ⎝ ⎜ ⎜ ⎡ ⎣ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ + ⎞ ⎠ ⎟ ⎟ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ = ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ − − ⋮ − ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ + = + == + = + = + + + + ϕx μw α ϕx ϕx t ξ μ α ϕx ϕx α ϕx ϕx t ξ αK x x α K x x t Kx x Kx x Kx x Kx x α α α fx t fx t fx t ()· ( )· () 0 ()· ( ) ( )· ( ) 0 (, ) ( , ) 0 (, ) (, ) (, ) (, ) () () () μ i t s j N j t j t i t i t i t μ k N k s k s i t j N j t j t i t i t i t μ μ k N k s k s i t μ j N j t j t i t i t α C μ tt t N t N tt N t N t I C t t N t μ μ s tt μ μ s tt μ μ s N t N t 1 1 1 1 1 11 1 1 1 1 1 1 1 11 1 1 1 2 1 11 1 22 1 t st st i t t t tt t t t tt (22) where fs deﬁned in Eq. (16) represents the output function. In summary, the process of transferring knowledge from the old data within the proposed algorithm can be concluded as follows: ﬁrstly, an initial ELMK model is trained over the previous dataset; afterwards, parameters of the initial model are updated based upon the available new data ac- cording to Eq. (22). 3.2.2. Online learning part of TrOS-ELMK For now, based on the incorporation of transfer learning with ELMK (Tr-ELMK), knowledge learnt from old data can be remained and le- veraged for new data. Nonetheless, due to the fact that either transfer learning or ELMK is essentially oﬄine learning method, they need to deal with all the data at a time. Hence, directly applying TrELMK to time series prediction is inconsiderable. To alleviate this problem, on- line learning is employed for our algorithm. Suppose that there are Ne newly arrival samples =xt m N( , ), 1, 2, ...,m e m e e. Similar to the principle of Eq. (10), before the accession of these fresh samples, the optimization problem is formulated as follows: ∑ ⎧ ⎨ ⎩ + ∑ =− ⇒− + = = = wξ st ϕ x w t ξ αϕ x ϕ x t α C min .. ( )· ()· ( ) 0 t C i N i t i t t i t i t j N j t j t i t i t i t t 1 2 2 2 1 2 1 t t t (23) where =…α αα α[, , , ]t tt N t T 12 t is calculated in Eq. (21). With the accession of newly coming samples, models constructed before need to be updated. Since the models are based on ELM algo- rithm, where the output weight vector is a least-squares solution, we aim to ﬁnd a model with the minimal norm parameter vector and the least training errors for both existing samples and newly coming sam- ples. Hence, Eq. (24) is constructed to ﬁnd a vector we satisfying the following optimization problem: ⎧ ⎨ ⎪ ⎩ ⎪ + ∑ =− =− =+ wξ st ϕ x w t ξ ϕx w t ξ min .. ( )· ()· e C i NN i t i e e i e i e j t e j t j t 1 2 2 2 1 2t te (24) In Eq. (24), =xt j N( , ), 1, 2, ...,j t j t t are the existing new samples, i.e. the samples in Eq. (23). Since both the newly arriving samples =xt m N( , ), 1, 2, ...,m e m e e and the existing new samples belong to the current prediction task, vector we should be appropriate for both of them. Since the optimization problem in Eq. (24) only contains equality constraint conditions, it can be solved by Lagrangian multiplier method. And Eq. (25) is the corresponding Lagrangian problem of Eq. (24). Hence solving the primal problem in Eq. (24) equals to solving Eq. (25) based on Lagrangian multiplier method, i.e., taking a deriva- tive of the parameters as listed in Eqs. (26) to (30). Thus, the corre- sponding Lagrangian problem of Eq. (24) is formulated as: ⎜⎟ =+ ∑ + ∑ − ∑ ⎛ ⎝ −+ ⎞ ⎠ − ∑ −+ == == Lw ξ ξ αϕ x w t ξ α ϕ xw t ξ* ()· ( ( )· ) De C j N j t C m N m e j N j t j t e j t j t m N m e m e e m e m e 1 2 2 2 1 2 2 1 2 11 t t t e te (25) According to Lagrangian multiplier method: ∑∑ ∂ ∂ =− − = == L w wα ϕ x α ϕ x() * () 0 D e e m N m e m e j N j t j t 11 et (26) ∂ ∂ =− = L ξ αC ξ 0 D m t m e t m t (27) ∂ ∂ =− = L ξ αC ξ* 0 D j t j t t j t (28) ∂ ∂ =− + = L α φx w t ξ()· 0 D m e m e e m e m t (29) ∂ ∂ =− + = L α φx w t ξ()· 0 D j t j t e j t j t (30) By analyzing Eq. (26) can we get that = ∑ + ∑ ==w αx α x() * ( )e m N m e m e j N j t j t 11 et . Similarly, from Eq. (28) can we get that =ξ * j t α C j t t . Then, Eq. (31) can be obtained by plugging we and ξ j t into Eq. (30). ∑∑−+ + = = == αφ x φ x t α C αφ x φ x i N* ()· ( ) * ( )· ( ) 0, 1, 2, ..., j N j t j t i t i t i t t m N m e m e i t t 11 te (31) where α*j t represents the changed αj t. Note that, in this way, the ob- tained formula can be subtracted by Eq. (23), obtaining the subsequent simpliﬁed formula as Eq. (32). ∑+ ∑ − ∑ + −= ⇒ ∑ + ∑ += ⇒ ⎛ ⎝ ⎜ ⎜ ⎡ ⎣ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ + ⎞ ⎠ ⎟ ⎟ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ =−⎡ ⎣ ⎢ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ ⎥ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ == = == α K x x α K xx α K xx α α αK x x K x x α α Kx x Kx x Kx x Kx x α α α Kx x Kx x Kx x Kx x α α α (, ) * (, ) (, ) ( * )0 (, ) ( , )Δ Δ 0 (, ) (, ) (, ) (, ) Δ Δ Δ (, ) (, ) (, ) (, ) m N m e m e i t j N j t j t i t j N j t j t i t C i t i t m N m e m e i t j N j t i t j t C i t tt t N t N tt N t N t I C t t N t te t N e N te N t N e e e N e 11 1 1 11 1 11 1 1 1 2 11 1 1 1 2 et t t et t t tt t t t e tt e e ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎜⎟⇒ ⎛ ⎝ + ⎞ ⎠ ⋮ =− ⋮ K I C α α α K α α α Δ Δ Δ tt t t t N t te e e N e 1 2 1 2 t e (32) On the analysis of Eq. (32), it is easy to ﬁnd that the original pro- blem is transformed into solving αα[Δ , ...,Δ ] t N t T 1 t and αα[ , ..., ] e N e T 1 e . Since αα[ , ..., ] e N e T 1 e is closely related to the newly arrival samples, we can plug these new data into Eq. (31), and get Eq. (33): R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 6 ∑ −+ + ∑ = ⇒ ∑ +− + + ∑ = ⇒∑ + ∑ − + +∑ = ⎛ ⎝ ⎜ ⎜ ⎡ ⎣ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ + ⎞ ⎠ ⎟ ⎟ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ + ⎡ ⎣ ⎢ ⎢ ⎢ ⋯ ⋮⋮ ⋯ ⎤ ⎦ ⎥ ⎥ ⎥ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⋮ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ = ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ − − ⋮ − ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ == == == = αφ x φ x t α φ x φ x α α φx φx t α K x x αK x x αK x x t αK x x Kx x Kx x Kx x Kx x α α α Kx x Kx x Kx x Kx x α α α tf x tf x tf x * ()· ( ) ( )· ( ) 0 (Δ ) ( )· ( ) ( , ) 0 (, ) Δ (, ) (, ) 0 (, ) (, ) (, ) (, ) (, ) ( , ) (, ) ( , ) Δ Δ Δ () () () j N j t j t i e i e α C m N m e m e i e j N j t j t j t i e i e α C m N m e m e i e j N j t j t i e j N j t j t i e i e α C m N m e m e i e ee e N e N ee N e N e I C e e N e te N te t N e N t N e t t N t e t e e t e N e t N e 11 11 11 1 11 1 1 1 2 11 1 1 1 2 11 22 t i e t e t i e t e tt i e t e e ee e t e t et e t ee ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎜⎟⇒ ⎛ ⎝ + ⎞ ⎠ ⋮ + ⋮ = − − ⋮ − K I C α α α K α α α tf x tf x tf x Δ Δ Δ () () () ee t e e N e teT t t N t e t e e t e N e t N e 1 2 1 2 11 22 e t ee (33) where ft(x) is the output function obtained before the accession of fresh coming samples. Similar to fs(x), ft(x) can be deﬁned as follows: ∑== = = f xo xϕ x w α ϕ x ϕ x() () ()· ()· ( )t tt i N i t i t 1 t (34) By solving Eq. (32) and Eq. (33), αα[Δ , ...,Δ ] t N t T 1 t and αα[ , ..., ] e N e T 1 e can be decided. Hence, new kernel and new Lagrangian multiplier, re- spectively abbreviated as K new and αnew, are formed: = ⎡ ⎣ ⎢ ⎤ ⎦ ⎥ K KK KK new tt te teT ee (35) = ⎡ ⎣ ⎢ + ⎤ ⎦ ⎥ α αα α Δnew tt e (36) For now, an integrated system of TrOS-ELMK has been completed. Algorithm 1 describes the formation of TrOS-ELMK in detail. Complexity of Algorithm 1 is +O NN()et 22 + O(Ne × Ne × Nt), where +O NN()et 22 is the complexity of Phase 1, O(Ne × Ne × Nt) is the com- plexity of Phase 2. Ne represents the number of samples in the newly arriving dataset, and Nt represents the number of samples in the ex- isting new dataset. 4. The proposed TrEnOS-ELMK algorithm Researches regarding ensemble learning turn out to have a long history. Many considerable works and assorted breakthroughs have been made during the past decades. Owing to the fact that an ensemble system usually has a preferable performance in a broad range of applications, ensemble systems have been successfully applied to address various machine learning problems, including incremental learning, feature selection, imbalanced data classiﬁcation, and so on [25,26]. Motivated by the profound performance of ensemble learning, we incorporate it with the TrOS-ELMK algorithm and obtain a new hybrid algorithm abbreviated as TrEnOS-ELMK. In the ﬁrst step, multiple base models endowed with equal weights are generated to construct an original ensemble. Next, to ensure the overall performance, the weights of diﬀerent models need to be adaptively updated according to their corresponding prediction accuracies over the existing new dataset. Models with superior performance generally possess heavier weights. For the sake of description, some relevant deﬁnitions are listed as follows: = ⎧ ⎨ ⎩ = +≥ − −SE iter SE e iter 0, 0 ·· , 1 l c l iter iter l c iter l c l 1 1 1l ll (37) where iterl is the number of the L-th model's current iterations, SEl c denotes the prediction performance of the L-th model on the c-th newly arriving da- taset, el c represents corresponding error of the L-th model on the c-th new dataset. We use root mean square error (RMSE) to calculate e,which is de- ﬁned as follows: ∑=− = = RMSE N output t et p N 1 ( arg ) , 1, ..., p N pp 1 2 (38) According to Eq. (37) and Eq. (38), the weights of diﬀerent models are updated adaptively as follows: ⎜ ⎟= ⎛ ⎝− − ⎞ ⎠ −ω ω SE mean SE mean SE ·exp () () l c l c l c c c 1 (39) where ωl c denotes the weight of the L-th model on the c-th new dataset. It is not diﬃcult to ﬁnd that, the weight of the L-th model will be de- creased if SEl c is larger than average, where SEl c is increased with the increment of the predictive error of the L-th model. This method can, to some extent, diminish the negative eﬀect of models with poor perfor- mance. In D Old, we not only consider the model's predictive error in the c-th iteration, but also take the performance in the previous iterations into account, aiming to fully consider the model's performance both on the previous time series and on the newly coming ones. With the arrival of fresh samples, ensemble generated over the previous time series may suﬀer performance degradation. Hence, training new models as a fresh injection is always considered advisable. Models with poor prediction accuracy would be replaced, if perfor- mance of the ensemble fell short of predeﬁned standards. To summarize, the ensemble process of TrEnOS-ELMK can be roughly generalized into the following steps. Firstly, a collection of base models is generated according to Eq. (22). Weights, denoted as =ω ωω ω[ , , ..., ]T12 ,are equally allocated for these base models at beginning. Besides, we use =ω ωω/ to ensure ω to be a unit vector. Secondly, with the newly arriving samples, the weights of the diﬀerent base models are updated according to Eq. (39). Simultaneously, the parameters of diﬀerent models are also updated according to Phase 2 of Algorithm 1. Thirdly, if the performance of the en- semble on the newly arriving data does not meet the standard, the worst model will be replaced with a new model trained on the newly coming data. Prediction value of the ensemble can be calculated by using the following formula: ∑= = o xω S x() () i T ii 1 (40) where Si is the i-th base model, and Si(x) is the corresponding prediction value of the i-th base model for sample x. Algorithm 1 The TrOS-ELMK algorithm. Input: D Old − the previous dataset ==D xt k N{( , ) 1, 2, ..., }Old kk s De New − the existing new dataset ==D xt i N{( , ) 1, 2, ..., }e New ii t D New − the newly arriving dataset ==D xt m N{( , ) 1, 2, ..., }New mm e Output: M − The model constructed by TrOS-ELMK Phase 1: 1: Compute αs,Kss over D Old according to Eq. (15). 2: Transfer knowledge from old data to current prediction according to Eq. (22) Phase 2: 3: Update α e and Δαt over De New and D New according to Eq. (33). 4: Calculate Knew according to Eq. (35). 5: Calculate αnew according to Eq. (36). 6: Return: The model constructed by TrOS-ELMK. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 7 Detailed process of TrEnOS-ELMK is presented in Algorithm 2. Complexity of Algorithm 2 is ×T +O NN()et 22 + NumC ×× × ×T O NNN()e c e c t , where T is the number of base models, NumC is the number of the newly arriving datasets, Ne represents the number of samples in the existing new datasets, Nt represents the number of samples in the existing new dataset, and Ne c represents the size of the c- th newly arriving dataset. In a nutshell, TrEnOS-ELMK can be roughly divided into two parts. In the ﬁrst part, a collection of ELMK models is generated by transfer- ring knowledge from old data to the new existing data. An original ensemble is constructed by these base models, whose initial weights are identical. In the second part, with the arrival of new samples, weights of diﬀerent models are adaptively adjusted according to their corre- sponding predictive accuracies. And simultaneously, the parameters of the models are respectively updated. Decision of replacing the poorest model with a newly trained model is made depending on whether the performance of the ensemble is substandard. Fig. 2 intuitively describes the framework of TrEnOS-ELMK. 5. Experiments 5.1. Experimental datasets In this paper, two synthetic datasets and six real-world datasets are employed for time series prediction, including Mackey-Glass dataset [43,44], Lorenz dataset [43,44], Monthly temperature dataset [45], Zuerich monthly sunspot numbers dataset [45], Mean daily Saugeen river ﬂows dataset [45], Johnson Outdoors, Inc. (JOUT) dataset [46], GlaxoSmithKline plc. (GSK) dataset [46] and Dow Jones Industrial Average (DJI) dataset [46]. 5.1.1. Mackey-Glass dataset Mackey-Glass model originally steams from the physiological con- trol systems, which represent the prototypical feedback systems. By virtue of derivative embeddings and time-delay embeddings, Mackey- Glass model provides a considerable insight into the chaotic dynamics. As a result, it is widely studied in time series prediction researches, with a mathematical form of nonlinear time delay diﬀerential equation: = + −> dx dt ax x ax a a a () 1 ,, , 0 τ τ a 1 21 2 33 (41) where a1, a2, a3, τ are real numbers, xτ represents the value of x at time −tτ(). Based upon the fact that time series generated by Eq. (41) is chaotic when τ > 16.8, here we set == = =aa a τ0.2, 0.1, 10, 1712 3 [43,44] for old data. And the new data are generated by altering the time step size. On this basis, time series data with length of 1000 are respectively generated for our experiments. Fig. 3 describes the corre- lation between old data and new data in a visual sweep. As can be seen from Fig. 3, old data and new data are drawn from diﬀerent time distributions. However, due to the fact that these two data are both generated by the Mackey-Glass model, they are essen- tially related to each other. 5.1.2. Lorenz dataset Lorenz is a three-dimensional dynamic system and is usually used to generate chaotic time series. Eq. (42) presents the mathematical ex- pression of Lorenz time series. =− =− − =− by t x t xt b z t y t xt y t b z t [( ) ( )] ()[ ()] () () () () xt t yt t zt t d( ) d 1 d( ) d 2 d( ) d 3 (42) where b1, b2, b3 are dimensionless parameters. According to literatures, here we set == =b bb10, 28,12 3 8 3 [43,44] and construct a time series with the length of 1000. Since Lorenz is a three-dimensional system, where diﬀerent dimensions are interrelated with each other, two time series data, denoted as Lorenz(X-Y) and Lorenz(Y-Z), are generated for our experiments. In Lorenz(X-Y), the X-coordinate and Y-coordinate time series are respectively regarded as the old data and the new data. Similar operations are performed on Lorenz(Y-Z). Fig. 4 and Fig. 5 are used to, from an intuitive perspective, show the relations between the old data and the new data. As can be seen from Fig. 4, X-coordinate and Y-coordinate time series of Lorenz are under diﬀerent time distributions, while they are essentially correlated with each other on analysis of Eq. (42). Similar observations can be made from Fig. 5. 5.1.3. Monthly temperature dataset This dataset comes from Time Series Data Library and keeps a re- cord of the monthly temperatures in England from 1723 to 1970 [45]. Due to the time-varying characteristic of weather patterns, diﬀerences are commonly emerged between temperatures across a long time span. Hence, time series of the ﬁrst 100 years and the last 70 years are Algorithm 2 The TrEnOS-ELMK algorithm. Input: D Old − the previous dataset ==D xt k N{( , ) 1, 2, ..., }Old kk s De New − the existing new dataset ==D xt i N{( , ) 1, 2, ..., }e New ii t T − the size of ensemble NumC − the number of the newly coming data points η − the threshold of ensemble's prediction accuracy [ω1, ω2, ..., ωT] − the initial weights of the models =iter iter iter[ , , ..., ] [0, 0, ...,0]T12 Output: −S The ﬁnal ensemble generated by TrEnOS-ELMK 1: For =tT1, 2, ..., 1.1: =D DBootstrap ()bt Old 1.2: Compute αsover Dbt according to Eq. (15) 1.3: Construct model St according to Eq. (22) 1.4: =∪S S St 2: End For 3: For =cNumC1, 2, ..., 3.1: For =tT1, 2, ..., 3.1.1: == +De RMSE S iter iter(, ), 1t c t c c new tt 3.1.2: Calculate SEt c according to Eq. (37) 3.1.3: Calculate ωt c according to Eq. (39) 3.1.4: =− DD DSTrOS ELMK Phase(, , ). 2tnew c old e New c New 3.1.5: Supdate S(, )tnew c 3.1.6: Calculate the RMSE of S over Dc new, abbreviated as SDRMSE (, )c new 3.1.7: If >SDRMSE η(, )c new 3.1.8: Train a new model Snew and set == =iter w mean w SE0, ( ), 0new new c new c 3.1.9: Replace(Sworst,Snew) 3.1.10: End If 3.2: End For 4: End For 5: Return: The ensemble S Fig. 2. Framework of TrEnOS-ELMK. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 8 extracted for our experiments. Speciﬁcally, we utilize the temperatures from 1723 to 1823 as the old data, while temperatures from 1900 to 1970 are regarded as the new data. 5.1.4. Zuerich monthly sunspot numbers dataset This dataset comes from Time Series Data Library [45] and contains the monthly sunspot numbers of Zuerich from 1749 to 1983, where data from 1749 to 1849 are regarded as the old data, and data from 1933 to 1983 are regarded as the new data. Similar to the monthly temperature dataset, sunspot numbers usually change over time, re- sulting in time distribution variation between the ﬁrst 100 years data and the last 50 years data. 5.1.5. Mean daily Saugeen river ﬂows dataset This dataset comes from Time Series Data Library [45] and keeps a record of the mean daily Saugeen river ﬂows from Jan 01, 1915 to Dec 31, 1979. Water ﬂow variation is usually emerged along with the en- vironmental changes, resulting in diﬀerent current distributions be- tween times wide apart. We utilize the time series from Jan 01, 1915 to Sept 12, 1917 as the old data, and the time series from Jan 01, 1979 to Jan 16, 1980 is regarded as the new data. 5.1.6. Johnson Outdoors, Inc. (JOUT) dataset JOUT dataset contains the stock time series of Johnson & Johnson, whose historical data are obtained from Yahoo Finance [46]. The entire Fig. 3. Time series data generated by the Mackey-Glass model. Fig. 4. X-coordinate and Y-coordinate time series of Lorenz. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 9 dataset covers the period from Jan 02, 1991 to Aug 07, 2017. Since dissimilitude of stock price ﬂuctuation at diﬀerent time periods is usually subsistent, in our experiments, to select time series under dif- ferent time distribution, we utilize data from Jan 02, 1991 to Dec 31, 1997 as the old data, and data from Jan 02, 2013 to Aug 07, 2017 as the new data. Four attributes, including open price, highest price, lowest price, and close price, are selected for our experiments, where daily close price is regarded as the forecasting goal. Speciﬁcally, mathema- tical formalism of the predictive task can be described as follows: =⋯−−ψ fψ ψ ψ O H L(, , , , , , )tt q t t tt t1 (43) where =−ψ dtq t( , ..., )d is the D-th close price, q denotes the time window size, Ot is the t-th open price, Ht is the t-th highest price, Lt is the t-th lowest price, and f( · ) represents the prediction algorithm. 5.1.7. GlaxoSmithKline plc. (GSK) dataset GlaxoSmithKline plc. (GSK) dataset contains the historical time series drawn from Yahoo Finance [46]. It covers the period from Jan 02, 1991 to Aug 07, 2017. We select data from Jan 02, 1991 to Dec 31, Fig. 5. Y-coordinate and Z-coordinate time series of Lorenz. Table 2 RMSE of diﬀerent algorithms over nine datasets. RMSE OS-ELM OS-EMELM AEE OLSVR OISVR CDSVR Tr-ELMK TrOS-ELMK TrEnOS-ELMK Mackey-Glass 1.4899E−2 1.2099E−2 1.1404E−2 8.3182E−2 1.1168E−1 5.9069E−2 8.1600E−3 1.1500E−2 8.1510E−3 Lorenz(X-Y) 5.3390E−3 4.6740E−3 3.9750E−3 7.5738E−2 9.7900E−2 3.3215E−2 1.3260E−3 1.3880E−3 1.3260E−3 Lorenz(Y-Z) 7.0840E−3 5.3040E−3 3.7200E−3 7.5738E−2 1.0769E−1 3.2999E−2 1.3270E−3 1.3890E−3 1.3260E−3 Temperature 8.3101E−2 7.9672E−2 7.7353E−2 1.0117E−1 7.3945E−2 7.4889E−2 7.1914E−2 7.1897E−2 7.1554E−2 SunSpot 7.0017E−2 7.0370E−2 7.1003E−2 1.2187E−1 7.3551E−2 7.2240E−2 6.9150E−2 6.9599E−2 6.9144E−2 DailyRiver 2.2504E−2 2.3262E−2 3.6872E−2 6.8010E−2 8.6547E−2 8.0684E−2 2.0640E−2 2.1113E−2 2.0667E−2 JOUT 3.9892E−2 3.0814E−2 3.9966E−2 1.7402E−1 5.2126E−2 1.0278E−1 1.6510E−2 1.6059E−2 1.6509E−2 GSK 1.1875E−2 1.3613E−2 1.1817E−2 1.0170E−1 5.3507E−2 6.6990E−3 4.8200E−3 4.8480E−3 4.7080E−3 DJI 1.0152E−2 1.0309E−2 7.9580E−3 1.8131E−1 3.9539E−2 1.1608E−1 7.1060E−3 9.4470E−3 7.0720E−3 Remark: In Table 2, values in bold fonts represent the lowest RMSE among all the RMSEs obtained by the corresponding algorithms. The way of expression of 8.3103E−02 is scientiﬁc notation, that is, 8.3101E−02 stands for × −8.3101 10 2. The same markings in the following tables have the same meaning. Table 3 MAPE of diﬀerent algorithms over nine datasets. MAPE OS-ELM OS-EMELM AEE OLSVR OISVR CDSVR Tr-ELMK TrOS-ELMK TrEnOS-ELMK Mackey-Glass 1.8838E−2 1.5554E−2 1.2674E−2 1.4649E−1 1.5697E−1 8.7131E−2 1.0424E−2 1.4250E−2 1.0431E−2 Lorenz(X-Y) 2.6798E−2 1.0737E−2 8.9280E−3 1.2947E−1 1.0477E−1 4.6242E−2 4.3090E−3 4.6420E−3 4.3080E−3 Lorenz(Y-Z) 2.2883E−2 1.4091E−2 8.6470E−3 1.2947E−1 1.1697E−1 4.6068E−2 4.3090E−3 4.6430E−3 4.3080E−3 Temperature 1.4183E−1 1.3658E−1 1.3056E−1 1.6400E−1 1.2965E−1 1.2471E−1 1.1851E−1 1.1803E−1 1.1181E−1 SunSpot 2.5278E−1 2.6225E−1 2.4146E−1 2.3023 2.4932E−1 2.3933E−1 2.2686E−1 2.2681E−1 2.2685E−1 DailyRiver 1.4783E−1 1.8669E−1 2.4124E−1 7.4110E−1 6.3787E−1 6.2828E−1 1.4434E−1 1.5680E−1 1.4433E−1 JOUT 3.6042E−2 2.6452E−2 3.8983E−2 5.4312E−1 1.2649E−1 1.6259E−1 2.5878E−2 2.5376E−2 2.5876E−2 GSK 1.3497E−2 1.6084E−2 1.3887E−2 1.3141E−1 7.3069E−2 8.2370E−3 5.7900E−3 5.8470E−3 5.6520E−3 DJI 7.0470E−3 7.1070E−3 5.9950E−3 2.2629E−1 3.4142E−2 1.4956E−1 6.2900E−3 1.0005E−2 6.2640E−3 Remark: In Table 3, values in bold fonts represent the lowest MAPE among all the MAPEs obtained by the corresponding algorithms. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 10 1997 as the old data, and data from Jan 02, 2013 to Aug 07, 2017 as the new data. Operation methods similar to those for JOUT are adopted, when handling GSK dataset. 5.1.8. Dow Jones industrial average (DJI) dataset Historical data of Dow Jones Industrial Average (DJI) dataset are drawn from Yahoo Finance [46]. The entire dataset covers the period from Jan 02, 1991 to Aug 07, 2017. In our experiments, data from Jan 02, 1991 to Dec 31, 1997 are selected as the old data, while data from Jan 02, 2013 to Aug 07, 2017 are selected as the new data. When dealing with DJI dataset, operations similar to those for JOUT and GSK are implemented. 5.2. Experimental settings To evaluate the performance of our newly proposed TrEnOS-ELMK, several basic transfer learning methods and state-of-the-art time series prediction algorithms, including Online Sequential ELM (OS-ELM) [29], Online Sequential Improved Error Minimized ELM (OS-EMELM) [30], Adaptive Ensemble Models of ELM (AEE) [31], Online Support Vector Regression (OLSVR) [47], Incremental Support Vector Regression Model (OISVR) [48], Cross Domain SVR (CDSVR) are selected to be compared with our algorithm. In addition, to present the role of in- dividual parts of TrEnOS-ELMK, Tr-ELMK and TrOS-ELMK are also utilized for contrast, where Tr-ELMK is constructed by discarding the online sequential learning part from TrEnOS-ELMK, and TrOS-ELMK is built by discarding the ensemble learning part from TrEnOS-ELMK. Among the baseline methods, OS-ELM and OS-EMELM are based on online sequential extreme learning machine and can learn data one by one or chunk by chunk, which is similar to TrEnOS-ELMK. Unlike the former two methods (i.e., OS-ELM and OS-EMELM), the proposed TrEnOS-ELMK algorithm introduces kernel functions and replaces the random feature mapping with kernel function mapping. Hence, TrEnOS-ELMK can omit the selection of hidden node number, while OS- ELM and OS-EMELM cannot. Besides, TrEnOS-ELMK is a novel transfer learning algorithm for time series prediction, which can leverage knowledge from old time series, distinguishing it from the other two methods. In AEE, ELM is used as the base model in the ensemble. At every time step, each model is retrained from scratch on past values. Diﬀerent from AEE, weights and parameters of models in TrEnOS-ELMK are updated with the arrival of newly coming samples. Besides, AEE is a batch learning algorithm, while TrEnOS-ELMK belongs to online learning methods. OLSVR and OISVR are based on SVR algorithms and widely used for time series prediction. Similar to TrEnOS-ELMK, the two methods also belong to online learning algorithms and are appropriate to batch ar- riving samples. However, a major distinction between them is that, TrEnOS-ELMK belongs to transfer learning paradigm, which can extract knowledge from old time series and apply it to the current prediction task, while the other two methods cannot. Compared to TrEnOS-ELMK, CDSVR can also transfer knowledge from old time series. However, TrEnOS-ELMK is based on kernel ELM, while CDSVR is based on SVR algorithm. Besides, to make CDSVR competent to address time series prediction issues, ideas of [36] and [47] are incorporated, that is, support vectors are updated with the arrival of new samples. Here, AEE and CDSVR are batch learning algorithms, while the other algorithms belong to online learning ones. For batch learning algorithms, all the samples in the training dataset are provided at a time to train the prediction models. While in online learning algorithms, we suppose that training samples arrive sequentially. That is, the training dataset is divided into several chunks, and prediction models are learned and updated with the arrival of sequential data chunks. In Table 4 RMSE t-test results between TrEnOS-ELMK and other compared methods. T-Test RMSE OS-ELM OS-EMELM AEE OLSVR OISVR CDSVR Tr-ELMK TrOS-ELMK Mackey-Glass H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 Lorenz(X-Y) H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 Lorenz(Y-Z) H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 Temperature H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=0 SunSpot H=1 H=1 H=1 H= 1 H =1 H =1 H=0 H=1 DailyRiver H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 JOUT H=1 H=1 H=1 H= 1 H =1 H =1 H=0 H=1 GSK H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 DJI H=1 H=1 H=1 H= 1 H =1 H =1 H=0 H=1 Remark: In Table 4, the items displayed in bold and with H = 1 represent that hypothesis H0 is rejected, i.e., the TrEnOS-ELMK algorithm signiﬁcantly improves the prediction performance of the other compared algorithms at 5% signiﬁcance level based on the RMSE measurement. In contrast, the items in normal font and with H = 0 indicate that hypothesis H0 cannot be rejected, i.e., the pairwise diﬀerence between the prediction performance of the TrEnOS-ELMK algorithm and the other compared algorithms is not signiﬁcant at 5% signiﬁcance level based on the RMSE measurement. Table 5 MAPE t-test results between TrEnOS-ELMK and other compared methods. T-Test MAPE OS-ELM OS-EMELM AEE OLSVR OISVR CDSVR Tr-ELMK TrOS-ELMK Mackey-Glass H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 Lorenz(X-Y) H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 Lorenz(Y-Z) H = 0 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 Temperature H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 SunSpot H=1 H=1 H=1 H= 1 H =1 H =1 H=0 H=1 DailyRiver H = 0 H = 0 H=1 H= 1 H =1 H =1 H=0 H=1 JOUT H=1 H=0 H=1 H= 1 H =1 H =1 H=0 H=1 GSK H=1 H=1 H=1 H= 1 H =1 H =1 H=1 H=1 DJI H=1 H=1 H=1 H= 1 H =1 H =1 H=0 H=1 Remark: In Table 5, the items displayed in bold and with H = 1 represent that hypothesis H0 is rejected, i.e., the TrEnOS-ELMK algorithm signiﬁcantly improves the prediction performance of the other compared algorithms at 5% signiﬁcance level based on the MAPE measurement. In contrast, the items in normal font and with H = 0 indicate that hypothesis H0 cannot be rejected, i.e., the pairwise diﬀerence between the prediction performance of the TrEnOS-ELMK algorithm and the other compared algorithms is not signiﬁcant at 5% signiﬁcance level based on the MAPE measurement. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 11 addition, to better compare the eﬀects of diﬀerent algorithms, data normalization is implemented in the preprocessing portion. The data normalization method used in this work is shown as below: ′ = − − x xx xx i imin max min (44) where ′xi is the normalized value, xi is the initial value, xmin is the minimum value of the original time series, and xmax is its maximum value. Data exhibited in Fig. 2 to Fig. 4 and the following ﬁgures are all normalized into (0,1). For OS-ELM and AEE, the optimal number of hidden nodes is de- termined from the set [1:50] by trial-and-error, and for OS-EMELM, the initial number of hidden nodes is set as 5 by trial-and-error. In OISVR, particle swarm optimization framework is adopted to Fig. 6. (a) Results of diﬀerent algorithms on Mackey-Glass dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on Mackey-Glass dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 12 select the corresponding parameters. The radial basis function =− −G cc x c x c(, , ) exp( )12 2 1 2 is selected both as the activation function for models based on ELM, and as the kernel function for SVR and ELMK. In TrEnOS-ELMK, 50 base ELMK models, as described in Algorithm 2, are generated to assemble the ensemble. On basis of the prediction errors, two measurements, including root mean square error (RMSE) and mean absolute percentage error (MAPE), are adopted to evaluate the prediction performances of the corresponding algorithms, where RMSE has been described in Eq. (38) and MAPE is described as follows: Fig. 7. (a) Results of diﬀerent algorithms on Lorenz(X-Y) dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on Lorenz(X-Y) dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 13 ∑= − × = MAPE N tet output output 1 arg 100% p N pp p1 (45) where N denotes the size of test dataset. Furthermore, to judge whether our proposed algorithm is signiﬁcantly superior to the mentioned comparison methods, t-test is carried out in our experiments. 5.3. Experimental results To reduce the accidental errors, individual experiments are re- spectively repeated for 20 times over each dataset. RMSE and MAPE values of diﬀerent algorithms are separately listed in Table 2 and Table 3. Fig. 8. (a) Results of diﬀerent algorithms on Lorenz(Y-Z) dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on Lorenz(Y-Z) dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 14 As can be seen from Table 2 and Table 3, among the experiments over the nine time series, TrEnOS-ELMK gets the lowest RMSE for seven times, while Tr-ELMK and TrOS-ELMK get the lowest RMSE for two times and one time, separately. When comparing MAPE, TrEnOS-ELMK gets the lowest value for ﬁve times, while both Tr-ELMK and AEE get the lowest value once. TrOS-ELMK gets the lowest MAPE for two times. From the results can we know that, among all the comparison methods, TrEnOS-ELMK has substantially better RMSE and MAPE performances Fig. 9. (a) Results of diﬀerent algorithms on Monthly temperature dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on Monthly temperature dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 15 on average. Among the remaining methods, TrOS-ELMK and Tr-ELMK own relatively superior results to the other algorithms. However, since Tr-ELMK lacks the online sequential learning part, it needs to deal with all the samples at once, resulting in serious limitations in managing practical time series issues. While TrOS-ELMK, by contrast, is more ﬂexible in response to various time series application situations. Therefore, in most real-world cases, TrOS-ELMK is more suitable to serve as a support for time series prediction in comparison with Tr- ELMK. By further analyzing the results of OS-ELM, OS-EMELM and AEE, it is not hard to ﬁnd that, among these three methods, AEE gets the lowest RMSE and MAPE for the most times, followed by OS-EMELM. Fig. 10. (a) Results of diﬀerent algorithms on Zuerich monthly sunspot numbers dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on Zuerich monthly sunspot numbers dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 16 Compared to OS-ELM and OS-EMELM, an additional ensemble learning portion is incorporated into AEE. Analyzing the preferable performance of TrEnOS-ELMK, we can roughly come to a conclusion that, appro- priate integration can, to some extent, cause an improvement to pre- dictive performance. Simultaneously, comparison between OS-ELM and OS-EMELM proves the superiority of the latter. That may be owing to the fact that, OS-EMELM overcomes the drawback of OS-ELM, which determines the hidden layer architecture randomly. Furthermore, by analyzing the performances of OLSVR, OISVR and CDSVR, we can come up with the ﬁndings that, CDSVR apparently Fig. 11. (a) Results of diﬀerent algorithms on Mean daily Saugeen river ﬂows dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on Mean daily Saugeen river ﬂows dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 17 outperforms the other two methods, getting the lowest RMSE and MAPE for the most times. One of the major distinctions between CDSVR and the other two methods is the ingenious combination with transfer learning, enabling it to leverage knowledge from old data for new prediction tasks. Combining the satisfactory results of Tr-ELMK, TrOS-ELMK and TrEnOS-ELMK, consequence can be drawn that, incorporation with transfer learning can not only explore and exploit old data's potentially useful information, but also play a positive role in new time series prediction issues. Meanwhile, to conﬁrm whether the proposed TrEnOS-ELMK algo- rithm is signiﬁcantly superior to the other methods, t-test at the 5% signiﬁcance level is applied to the results of TrEnOS-ELMK and the Fig. 12. (a) Results of diﬀerent algorithms on JOUT dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on JOUT dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 18 remaining eight methods. Table 4 and Table 5 separately record the t- test results of RMSE and MAPE. Through analyzing Table 4, it is easy to make the point that, for RMSE, the newly proposed TrEnOS-ELMK is signiﬁcantly superior to the other compared methods. Similar conclusion for MAPE can also be reached on the analysis to Table 5. According to the above results, it is not hard to ﬁnd that, when dealing with time series prediction issues, marked performance improvement is appended to TrEnOS-ELMK in comparison with other methods. Therefore, the eﬀectiveness of TrEnOS-ELMK can be veriﬁed. Fig. 13. (a) Results of diﬀerent algorithms on GSK dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on GSK dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 19 In addition, to intuitively compare the performances of diﬀerent methods, ﬁgures of the prediction values and the corresponding abso- lute predict errors are drawn for all the methods mentioned above. Since in our experiments, the entire dataset of one speciﬁc time series is divided into the corresponding training dataset and testing dataset. Figs. 3, 4, and 5 illustrate all the 1000 data points of the three bench- mark time series, while Figs. 6, 7, and 8 present prediction results of the 500 testing data points of these time series; therefore, the numbers of data points in Figs. 3, 4, and 5 are diﬀerent from those in Figs. 6, 7, and 8. Fig. 14. (a) Results of diﬀerent algorithms on DJI dataset (b) Absolute prediction errors in logarithmic scale of diﬀerent algorithms on DJI dataset. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 20 In addition, to better compare the performance and observe the eﬀectiveness of the proposed algorithms, logarithmic scale is applied to the absolute prediction errors obtained by diﬀerent algorithms on the nine benchmark time series datasets, and the results are illustrated in Fig. 6(b)–Fig. 14(b), accordingly. Fig. 6(a), Fig. 7(a) and Fig. 8(a) respectively present the predictive values of diﬀerent algorithms over Mackey Glass dataset and Lorenz dataset, while Fig. 6(b), Fig. 7(b) and Fig. 8(b) severally display the corresponding predict errors of diﬀerent algorithms. On analysis of these ﬁgures, it is not hard to ﬁnd that, among all the nine methods, the proposed TrEnOS-ELMK owns a higher ﬁt coherence with actual data. Furthermore, from ﬁgures showing predict errors can we know that predictive results of TrEnOS-ELMK agree with the real data satisfacto- rily, with relative deviation closely to zero. Besides, over the three datasets, predict errors of ELM based methods are generally less than those of the methods based on SVR. Therefore, a rough conclusion can be drawn that, compared with the other methods, TrEnOS-ELMK has obvious advantage in ﬁtting accuracy and applicability, when addres- sing the three time series dataset. Fig. 9(a) describes the predict results of diﬀerent algorithms over Monthly temperature dataset. As can be seen from this ﬁgure, the overall variation tendencies of all the methods roughly resemble with that of actual data, while TrEnOS-ELMK and Tr-ELMK have preferable ﬁtting qualities. Moreover, by analyzing Fig. 9(b), it is not hard to ﬁnd that, in most cases, TrEnOS-ELMK gets the lowest prediction errors. Fig. 10(a) displays the predict results of diﬀerent algorithms over Zuerich monthly sunspot numbers dataset, and Fig. 10(b) presents the corresponding predict errors. On analysis of Fig. 10(a), we can ﬁnd that, the curve simulated by TrEnOS-ELMK shows in good agreement with the raw data. Change tendencies of patterns portrayed by ELM-based methods are also similar to those of the real data. In comparison with OLSVR and OISVR, results provided by CDSVR have a better coin- cidence with the actual results, while OLSVR results in a relatively high quantity of outlier points. Similar conclusion can also be reached from Fig. 10(b), where TrEnOS-ELMK gets the lowest predict errors for most times and the deviations of OLSVR are relatively higher than those of the others. Fig. 11(a) and Fig. 11(b) separately exhibit the prediction results and errors of diﬀerent algorithms over Mean daily saugeen river ﬂows dataset. As can be seen from Fig. 11(a), movements of prediction curves relying on diﬀerent methods are analogous to those of the actual data. Among these curves, the one in terms of TrEnOS-ELMK accords sa- tisfactorily with the real results. Other ELM-based methods also provide responses closely ﬁtting the actual data, while performance of methods depending on SVR, by contrast, seems to fall short of those possessed by the others. Furthermore, by analyzing Fig. 11(b), it is easy to discover that, TrEnOS-ELMK owns a lower level of average errors in most si- tuations and, in comparison with SVR-based methods, models relying on ELM can provide relatively higher prediction accuracies. Above all, we can come to a conclusion that, the newly proposed Table 6 Running time (seconds) of diﬀerent algorithms over nine datasets. Time(s) OS-ELM OS-EMELM AEE OLSVR OISVR CDSVR Tr-ELMK TrOS-ELMK TrEnOS-ELMK Mackey-Glass 1.3425E−2 1.0729E−2 3.1274E−1 1.8398E2 2.6653E3 1.6340E−2 1.1620E−1 1.6494E−1 3.4647 Lorenz(X-Y) 5.4380E−3 9.8170E−3 3.2664E−1 6.5676E1 4.3980E2 1.0435E−2 1.3297E−1 1.5938E−1 2.8500 Lorenz(Y-Z) 6.1030E−2 1.2457E−2 4.7794E−1 2.6390E2 1.4090E2 7.3438E−2 1.4170E−1 3.5792E−1 2.9540 Temperature 6.8390E−3 6.5320E−3 2.9411E−1 3.3930E2 1.5836E2 2.8434E−1 1.4285E−1 1.6811E−1 1.9026 SunSpot 4.0465E−2 7.5060E−3 2.4937E−1 2.1806E2 5.8584E1 1.5224E−1 1.0702E−1 1.4274E−1 1.5747 DailyRiver 1.0414E−2 3.9706E−2 7.7998E−1 4.3082E1 8.3814E1 1.7161E−2 1.0117E−1 3.0452E−1 2.5265 JOUT 6.0800E−3 5.9740E−3 3.8721E−1 1.1666E2 5.2755E2 1.6310E−2 3.3359E−1 3.7462E−1 4.3288 GSK 1.0660E−2 1.6178E−2 3.7412E−1 1.1604E2 5.3040E2 1.7717E−2 3.0985E−1 3.4210E−1 4.2445 DJI 1.2121E−2 1.1807E−2 3.7306E−1 7.8818E1 5.2432E2 1.6809E−1 3.5540E−1 3.6791E−1 4.3315 Table 7 RMSE of TrOS-ELMK and TrEnOS-ELMK ( =μ 0.2). C 0.01 0.1 1 10 100 500 1000 Mackey-Glass TrEnOS-ELMK 3.6027E−1 8.9606E−2 1.9707E−2 8.1550E−3 1.9300E−3 2.3390E−3 3.8210E−3 TrOS-ELMK 3.6100E−1 8.9954E−2 1.9829E−2 8.1710E−3 1.9490E−3 2.3460E−3 3.8130E−3 Lorenz(X-Y) TrEnOS-ELMK 1.9701E−1 5.0801E−2 4.1460E−3 3.0470E−3 1.5040E−3 1.6940E−3 2.1210E−3 TrOS-ELMK 1.9778E−1 5.0984E−2 4.1500E−3 3.0540E−3 1.5050E−3 1.6970E−3 2.1260E−3 Lorenz(Y-Z) TrEnOS-ELMK 1.6342E−1 5.6588E−2 4.2270E−3 3.5720E−3 2.9700E−3 2.5650E−3 2.3320E−3 TrOS-ELMK 1.6295E−1 5.6483E−2 4.2400E−3 3.5810E−3 2.9760E−3 2.5710E−3 2.3390E−3 Temperature TrEnOS-ELMK 2.52467E−1 1.03545E−1 7.6229E−2 7.1547E−2 7.1399E−2 7.3801E−2 7.6215E−2 TrOS-ELMK 2.75458E−1 1.08592E−1 7.6574E−2 7.1914E−2 7.3535E−2 7.9861E−2 8.3613E−2 SunSpot TrEnOS-ELMK 2.0678E−1 1.2983E−1 7.3677E−2 6.9121E−2 6.9668E−2 6.9890E−2 6.9909E−2 TrOS-ELMK 2.0669E−1 1.2971E−1 7.3648E−2 6.9148E−2 6.9697E−2 6.9914E−2 6.9936E−2 DailyRiver TrEnOS-ELMK 6.0763E−2 3.3741E−2 2.3048E−2 2.0810E−2 2.3589E−2 2.6116E−2 2.6816E−2 TrOS-ELMK 6.0785E−2 3.3782E−2 2.3080E−2 2.0728E−2 2.3600E−2 2.6261E−2 2.7076E−2 JOUT TrEnOS-ELMK 2.1557E−1 1.8271E−1 3.6017E−2 1.5518E−2 1.3286E−2 1.3137E−2 1.3131E−2 TrOS-ELMK 2.1564E−1 1.8280E−1 3.6023E−2 1.5517E−2 1.3290E−2 1.3148E−2 1.3141E−2 GSK TrEnOS-ELMK 1.0119E−1 1.8227E−1 5.4640E−3 5.2780E−3 4.0910E−3 4.3620E−3 4.8230E−3 TrOS-ELMK 1.0077E−1 1.8362E−1 5.4700E−3 5.2840E−3 4.1050E−3 4.3780E−3 4.8360E−3 DJI TrEnOS-ELMK 2.1181E−1 1.2087E−1 1.0289E−2 8.9400E−3 7.2330E−3 7.4670E−3 7.4670E−3 TrOS-ELMK 2.1198E−1 1.2152E−1 1.0262E−2 8.9130E−3 7.1890E−3 7.4460E−3 7.4460E−3 R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 21 TrEnOS-ELMK algorithm can bring preferable results than the other compared methods, when coping with the aforementioned three time series. Fig. 12(a) describes the predict results of diﬀerent algorithms on JOUT dataset and Fig. 12(b) exhibits the corresponding predict errors. As can be seen from Fig. 12(a), curve in terms of TrEnOS-ELMK coin- cides well with the real results. Other methods, such as TrOS-ELMK and AEE also present favorable imitative eﬀects. Among ELM-based methods, OS-ELM has a relatively larger amount of outlier points, likely being caused by the randomly constructed network structures. Besides, among the methods relying on SVR, OISVR provides a better ﬁtting result, followed by CDSVR, whereas OLSVR has a relatively larger predict deviation. On analysis of Fig. 12(b), similar conclusions can also be reached. That is, TrEnOS-ELMK gets the lowest predict errors for most times, while OLSVR owns undesirable prediction accuracies. Fig. 13(a) and Fig. 13(b) separately describe the prediction results and predict errors of diﬀerent algorithms over GSK dataset. Through the observation of Fig. 13(a), it is not hard to ﬁnd that, variation ten- dencies belonging to ELM-based methods are generally analogous to those of actual values, and TrEnOS-ELMK presents a superior ﬁtting degree. Among the methods based on SVR, the curve drawn by CDSVR tallies better with the real data, while the predict errors pertaining to the other two methods are relatively larger. In addition, the analysis of Fig. 13(b) also demonstrates this conclusion mentioned above. Fig. 14(a) displays the predict results of diﬀerent algorithms over DJI dataset, and Fig. 14(b) shows the corresponding predict errors. As can be seen from Fig. 14(a), the predict results of TrEnOS-ELMK, as expected, closely conform to the of actual time series. In general, per- formances of the methods depending on ELM are more desirable in comparison with the SVR-based methods. Furthermore, similar con- clusions can be drawn from Fig. 14(b), where TrEnOS-ELMK has rela- tively lower errors close to zero, while the prediction eﬀects belonging to OLSVR is unsatisfactory. In conclusion, the practicability and eﬀectiveness of our proposed TrEnOS-ELMK algorithm can be veriﬁed by the experimental results shown in Fig. 6 to Fig. 14. Therefore, TrEnOS-ELMK, aligning with our expectations, can be feasibly applied to time series prediction issues with satisfactory performance. In addition, Table 6, which records each method's running time, is also listed as follows. Here, in Table 6, time is measured in seconds. From Table 6 can we know that, over all the nine benchmark da- tasets, OS-ELM and OS-EMELM usually take the least computation time, while OISVR and OLSVR usually take the most computation time. Compared to AEE, CDSVR, and Tr-ELMK, TrOS-ELMK is better than the former one, however, slightly worse than the latter two, in terms of eﬃciency. Due to the incorporation of ensemble learning, TrEnOS- ELMK needs to train several ELMK models, leading to longer compu- tation time. However, how to deﬁne the number of hidden nodes also has a signiﬁcant impact on the prediction accuracy of OS-ELMK and OSEM-ELMK, while TrOS-ELMK and TrEnOS-ELMK can avoid this issue. In addition, OS-ELM and OS-EMELM do not possess transfer learning capability, while TrOS-ELMK and TrEnOS-ELMK belong to transfer learning algorithms, which can leverage knowledge learnt from old data to fulﬁll current prediction tasks. Therefore, though TrOS-ELMK and TrEnOS-ELMK take more computation time, they have much better prediction performance than OS-ELM and OS-EMELM. Compared to AEE, CDSVR, and Tr-ELMK, TrOS-ELMK and TrEnOS- ELMK incorporate online learning to address the problem of data ar- riving sequentially. Hence, TrOS-ELMK and TrEnOS-ELMK can be ap- plied to some real-time cases, while AEE, CDSVR, and Tr-ELMK cannot. Since TrEnOS-ELMK incorporates ensemble learning scheme and is re- quired to train a collection of base models, it takes more computation time than TrOS-ELMK. However, from Table 2 and Table 3 can we ﬁnd that, in most cases, TrEnOS-ELMK has the best prediction performance. Besides, μ and C are the two important parameters in our proposed algorithms. To test the sensitivity of our proposed algorithms to the two parameters, we respectively ﬁx one of μ and C to test the other para- meter's impact on the prediction performance of our proposed TrOS- ELMK and TrEnOS-ELMK algorithms. Table 7 and Table 8 record the RMSEs of the two algorithms. In Table 7, μ is ﬁxed as 0.2, while the value of C is selected, one by one, from the set {0.01, 0.1, 1, 10, 100, 500, 1000}. In Table 8, C is ﬁxed as 50, while the value of μ is selected from the set {0.001, 0.01, 0.1, 1, 10, 100, 1000}, one by one. To intuitively compare the impact of diﬀerent values of C on the prediction performance of the proposed algorithms, RMSEs of TrEnOS- ELMK and TrOS-ELMK with diﬀerent values of C ranging from 0.01 to Table 8 RMSEs of TrEnOS-ELMK and TrOS-ELMK ( =C 50). μ 0.001 0.01 0.1 1 10 100 1000 Mackey-Glass TrEnOS-ELMK 7.9920E−3 7.9790E−3 7.9590E−3 1.4781E−2 1.2203E−1 8.3965E−1 2.5586 TrOS-ELMK 7.9920E−3 7.9810E−3 7.9700E−3 1.4787E−2 1.2161E−1 8.4121E−1 2.5280 Lorenz(X-Y) TrEnOS-ELMK 1.6140E−3 16150E−3 1.6370E−3 2.9180E−3 2.3839E−2 2.3403E−1 2.0810 TrOS-ELMK 1.6140E−3 1.6150E−3 1.6390E−3 2.8950E−3 2.3499E−2 2.2955E−1 2.0469 Lorenz(Y-Z) TrEnOS-ELMK 1.9100E−3 1.9270E−3 2.1720E−3 7.0590E−3 6.3155E−2 5.9609E−1 4.5143 TrOS-ELMK 1.9100E−3 1.9270E−3 2.1720E−3 7.1280E−3 6.4062E−2 6.0513E−1 4.5854 Temperature TrEnOS-ELMK 7.1454E−2 7.1180E−2 7.1351E−2 7.3421E−2 1.1513E−1 5.5895E−1 1.5081 TrOS-ELMK 7.1164E−2 7.1472E−2 7.1683E−2 7.4211E−2 1.2348E−1 6.5822E−1 1.9655 SunSpot TrEnOS-ELMK 6.9149E−2 6.9147E−2 6.9134E−2 6.9109E−2 7.3747E−2 1.0739E−1 2.8977E−1 TrOS-ELMK 6.9149E−2 6.9149E−2 6.9147E−2 6.9199E−2 7.4108E−2 1.2849E−1 1.9009E−1 DailyRiver TrEnOS-ELMK 2.0493E−2 2.0505E−2 2.0633E−2 2.2467E−2 5.5861E−2 2.8662E−1 1.15971 TrOS-ELMK 2.0493E−2 2.0502E−2 2.0602E−2 2.2200E−2 5.4105E−2 2.1303E−1 8.3443E−1 JOUT TrEnOS-ELMK 1.4931E−2 1.4947E−2 1.5106E−2 1.6678E−2 3.1018E−2 1.6808E−1 1.17250 TrOS-ELMK 1.4931E−2 1.4948E−2 1.5113E−2 1.6733E−2 3.2730E−2 1.8621E−1 1.39957 GSK TrEnOS-ELMK 5.4020E−3 5.4050E−3 5.4310E−3 5.8670E−3 1.6003E−2 1.9050E−1 2.4371 TrOS-ELMK 5.4020E−3 5.4050E−3 5.4330E−3 5.9150E−3 1.7414E−2 2.1637E−1 2.7020 DJI TrEnOS-ELMK 5.2410E−3 5.3290E−3 6.2910E−3 1.8255E−2 1.4674E−1 1.4121 13.5525 TrOS-ELMK 5.2410E−3 5.3270E−3 6.2720E−3 1.8266E−2 1.4713E−1 1.4066 13.6216 R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 22 1000 on the nine time series datasets are respectively presented in Fig. 15(a) and Fig. 15(b). From Table 7 and Fig. 15 can we know that, RMSEs of TrEnOS- ELMK and TrOS-ELMK vary greatly, when μ is ﬁxed and C is less than 1. When C is larger than 10, the changes tend to be gradual and gradually remain stable. Hence, we can conclude that, when C is less than 1, the proposed TrEnOS-ELMK and TrOS-ELMK are sensitive to the change of C, while when C is larger than 10, the two algorithms are not very Fig. 15. (a) Performance of TrEnOS-ELMK with diﬀerent values of C on diﬀerent datasets (b) Performance of TrOS-ELMK with diﬀerent values of C on diﬀerent datasets. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 23 sensitive to the changes of C. In addition, by analyzing Table 7, it is not hard to see that, prediction performances of the two algorithms when C is less than 1 are inferior to those when C is larger than 1. Hence generally, C should be selected from values larger than 1, according to speciﬁc datasets. To intuitively compare the eﬀects of diﬀerent values of μ on the proposed methods, RMSEs of TrEnOS-ELMK and TrOS-ELMK with dif- ferent values of μ on the nine datasets are respectively presented in Fig. 16(a) and Fig. 16(b). Since when =μ 1000, RMSE of DJI diﬀers far from those of the other datasets, two y-axes with diﬀerent scales are Fig. 16. (a) Performance of TrEnOS-ELMK with diﬀerent values of μ on diﬀerent datasets (b) Performance of TrOS-ELMK with diﬀerent values of μ on diﬀerent datasets. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 24 utilized in Fig. 16. As shown in Fig. 16, the right y-axis in red with scale from 0 to 15 is used for DJI (i.e., the red line), while the left y-axis in black with scale from 0 to 3 is used for the other eight datasets. From Table 8, Fig. 16(a), and Fig. 16(b), it is not hard to see that, when μ is larger than 1, RMSEs of TrEnOS-ELMK and TrOS-ELMK vary greatly with the increase of μ. While, when μ is less than 1, changes of the performances of the two methods show a slowdown trend and gradually come to stable. Thus, a rough conclusion can be drawn that, performances of TrEnOS-ELMK and TrOS-ELMK are sensitive to the changes of μ, when μ is larger than 1, however, not sensitive to the changes of μ, when μ is less than 1. Besides, by further analyzing Table 8 can we ﬁnd that, performances of the two methods with μ < 1 are much superior to those with μ >1. 6. Conclusions In this paper, a hybrid algorithm called TrEnOS-ELMK is put for- ward for time series prediction. Diﬀerent from many existing methods, TrEnOS-ELMK takes long-ago data into consideration, and aims to generate eﬀective models, though scarce fresh training samples are available. However, due to the time-varying property, time series data between a long time span usually diﬀers widely from each other. Hence, directly employing old samples for future prediction appears inad- visable. To alleviate this problem, transfer learning is adopted by our method, making knowledge transferred from old data possible. Besides, inspired by the desirable performance of ELMK, OS-ELMK is also uti- lized in our paper, overcoming the drawback of traditional OS-ELM. In addition, to further enhance the method's performance, ensemble up- dated adaptively is applied to our algorithm. In conclusion, the main contribution of our work is that it provides the ﬁrst taste of transfer learning for time series prediction. Instead of discarding all the old data directly, we attempt to make the most of them. Besides, precise deduction is conducted for eﬀectively leveraging knowledge for the current prediction tasks. Experimental results on nine synthetic and real-world datasets also demonstrate the practic- ability and eﬀectiveness of our algorithm. Acknowledgement This work is supported by the National Natural Science Foundation of China under Grant no. 61473150. References [1] Q.Y. Jiang, Time series prediction based on machine learning, Proceedings of the 2015 International Conference on Electrical, Automation and Mechanical Engineering (Eame 2015), 13 2015, pp. 128–129. [2] N.L. Sapankevych, R. Sankar, Time series prediction using support vector machines: a survey, IEEE Comput. Intell. Mag. 4 (2009) 24–38 May. [3] Y.H. Chen, B. Yang, J.W. Dong, Time-series prediction using a local linear wavelet neural network, Neurocomputing 69 (2006) 449–465 Jan. [4] W.Z. Cui, C.C. Zhu, W.X. Bao, J.H. Liu, Prediction of the chaotic time series using support vector machines, Acta Physica Sinica 53 (2004) 3303–3310 Oct. [5] F. Liu, C. Quek, G.S. Ng, Neural network model for time series prediction by re- inforcement learning, Proceedings of the International Joint Conference on Neural Networks (IJCNN), 1-5 2005, pp. 809–814 vols. [6] U. Thiessen, R. van Brakel, A.P. de Weijer, W.J. Melssen, L.M.C. Buydens, Using support vector machines for time series prediction, Chemom. Intell. Lab. Syst. 69 (2003) 35–49 Nov 28. [7] G.P. Zhang, V.L. Berardi, Time series forecasting with neural network ensembles: an application for exchange rate prediction, J. Oper. Res. Soc. 52 (2001) 652–664 Jun. [8] G.B. Huang, D.H. Wang, Y. Lan, Extreme learning machines: a survey, Int. J. Mach. Learn. Cybern. 2 (2011) 107–122 Jun. [9] R. Zhang, Y. Lan, G.B. Huang, Z.B. Xu, Universal approximation of extreme learning machine with adaptive growth of hidden nodes, IEEE Trans. Neural Netw. Learn. Syst. 23 (2012) 365–371 Feb. [10] S. Balasundaram, Kapil, Application of error minimized extreme learning machine for simultaneous learning of a function and its derivatives, Neurocomputing 74 (2011) 2511–2519 Sep. [11] G.R. Feng, G.B. Huang, Q.P. Lin, R. Gay, Error minimized extreme learning machine with growth of hidden nodes and incremental learning, IEEE Trans. Neural Netw. 20 (2009) 1352–1357. Aug. [12] G.B. Huang, An insight into extreme learning machines: random neurons, random features and kernels, Cognit. Comput. 6 (2014) 376–390 Sep. [13] W.M. Huang, N. Li, Z.P. Lin, G.B. Huang, W.W. Zong, J.Y. Zhou, et al., Liver tumor detection and segmentation using kernel-based extreme learning machine, 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2013, pp. 3662–3665. [14] Y.L. Jian, D.Y. Huang, J. Yan, K. Lu, Y. Huang, T.L. Wen, et al., A novel extreme learning machine classiﬁcation model for e-Nose application based on the multiple kernel approach, Sensors 17 (2017) Jun. [15] X.W. Liu, L. Wang, G.B. Huang, J. Zhang, J.P. Yin, Multiple kernel extreme learning machine, Neurocomputing 149 (2015) 253–264 Feb 3. [16] L. Guo, J.H. Hao, M. Liu, An incremental extreme learning machine for online se- quential learning problems, Neurocomputing 128 (2014) 50–58 Mar 27. [17] M.T.T. Hoang, H.T. Huynh, N.H. Vo, Y. Won, A robust online sequential extreme learning machine, Advances in Neural Networks - ISNN 2007, Pt 1, Proceedings, 4491 2007, pp. 1077–1086. [18] Y. Jun, M.J. Er, An enhanced online sequential extreme learning machine algo- rithm, 2008 Chinese Control and Decision Conference, 1-11 2008, pp. 2902–2907 vols. [19] H.J. Rong, G.B. Huang, N. Sundararajan, P. Saratchandran, Online sequential fuzzy extreme learning machine for function approximation and classiﬁcation problems, IEEE Trans. Syst. Man Cybern. B-Cybern. 39 (2009) 1067–1072. Aug. [20] S. Scardapane, D. Comminiello, M. Scarpiniti, A. Uncini, Online sequential extreme learning machine with kernels, IEEE Trans. Neural Netw. Learn. Syst. 26 (2015) 2214–2220 Sep. [21] X.Y. Wang, M. Han, Online sequential extreme learning machine with kernels for nonstationary time series prediction, Neurocomputing 145 (2014) 90–97 Dec 5. [22] S.J. Pan, Q.A. Yang, A survey on transfer learning, IEEE Trans. Knowl. Data Eng. 22 (2010) 1345–1359 Oct. [23] L. Shao, F. Zhu, X.L. Li, Transfer learning for visual categorization: a survey, IEEE Trans. Neural Netw. Learn. Syst. 26 (2015) 1019–1034 May. [24] Q. Yang, Transfer learning beyond text classiﬁcation, Adv. Mach. Learn., Proc. 5828 (2009) 10–22. [25] T.G. Dietterich, Ensemble methods in machine learning, Multiple Classiﬁer Syst 1857 (2000) 1–15. [26] Z.H. Zhou, J.X. Wu, W. Tang, Ensembling neural networks: many could be better than all, Artif. Intell. 137 (2002) 239–263 May. [27] S.K. Oh, K.H. Seo, J.J. Lee, Time series prediction by mixture of linear local models, IECON'03: The 29th Annual Conference of the Ieee Industrial Electronics Society, 2003, pp. 1905–1908 vols 1-3, Proceedings. [28] Y.H. Lu, C.G. Wu, Y.C. Liang, Center selection for RBF neural network in prediction of nonlinear time series, 2003 International Conference on Machine Learning and Cybernetics, 1-5 2003, pp. 1355–1359 volsProceedings. [29] N.Y. Liang, G.B. Huang, P. Saratchandran, N. Sundararajan, A fast and accurate online sequential learning algorithm for feedforward networks, IEEE Trans. Neural Netw. 17 (2006) 1411–1423 Nov. [30] J. Xue, Z.S. Liu, Y. Gong, Z.S. Pan, Time series prediction based on online sequential improved error minimized extreme learning machine, Proceedings of ELM-2015, Vols 1: Theory, Algorithms and Applications (I) 6 (2016), pp. 193–209. [31] M. van Heeswijk, Y. Miche, T. Lindh-Knuutila, P.A.J. Hilbers, T. Honkela, E. Oja, et al., Adaptive ensemble models of extreme learning machines for time series prediction, Artiﬁcial Neural Networks - ICANN 2009, Pt Ii, 5769 2009, pp. 305–314. [32] L. Yang, L.P. Jing, J. Yu, M.K. Ng, Learning transferred weights from co-occurrence data for heterogeneous transfer learning, IEEE Trans. Neural Netw. Learn. Syst. 27 (2016) 2187–2200 Nov. [33] J. Lu, V. Behbood, P. Hao, H. Zuo, S. Xue, G.Q. Zhang, Transfer learning using computational intelligence: a survey, Knowl.-Based Systems 80 (2015) 14–23 May. [34] H. Zuo, G.Q. Zhang, W. Pedrycz, V. Behbood, J. Lu, Fuzzy regression transfer learning in Takagi-Sugeno fuzzy models, IEEE Trans. Fuzzy Syst. 25 (6) (2017) 1795–1807 Dec. [35] S. Al-Stouhi, C.K. Reddy, Adaptive boosting for transfer learning using dynamic updates, Mach. Learn. Knowl. Discovery Databases, Pt I 6911 (2011) 60–75. [36] W. Jiang, E. Zavesky, S.F. Chang, A. Loui, Cross-domain learning methods for high- level visual concept classiﬁcation, 2008 IEEE International Conference on Image Processing, Proceedings, 2008, pp. 161–164. [37] P. Zhao, S.C.H. Hoi, OTL: a framework of online transfer learning, International Conference on International Conference on Machine Learning, 2010, pp. 1231–1238. [38] P.L. Zhao, S.C.H. Hoi, J.L. Wang, B. Li, Online transfer learning, Artif. Intell. 216 (2014) 76–102 Nov. [39] B. Wang, J. Pineau, Online boosting algorithms for anytime transfer and multitask learning, Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015, pp. 3038–3044. [40] Y. Yan, Q. Wu, M. Tan, M.K. Ng, H. Min, I.W. Tsang, Online heterogeneous transfer by hedge ensemble of oﬄine and online decisions, IEEE Trans. Neural Netw. Learn. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 25 Syst. (2017) 1–12 vol. PP. [41] Q.Y. Wu, H.R. Wu, X.M. Zhou, M.K. Tan, Y.H. Xu, Y.G. Yan, et al., Online transfer learning with multiple homogeneous or heterogeneous sources, IEEE Trans. Knowl. Data Eng. 29 (2017) 1494–1507 Jul 1. [42] Q.Y. Wu, X.M. Zhou, Y.G. Yan, H.R. Wu, H.Q. Min, Online transfer learning by leveraging multiple source domains, Knowl. Inf. Syst. 52 (2017) 687–707 Sep. [43] M. Ardalani-Farsa, S. Zolfaghari, Chaotic time series prediction with residual ana- lysis method using hybrid Elman-NARX neural networks, Neurocomputing 73 (2010) 2540–2553. Aug. [44] R. Chandra, M.J. Zhang, Cooperative coevolution of Elman recurrent neural networks for chaotic time series prediction, Neurocomputing 86 (2012) 116–123 Jun 1. [45] Time Series Data Library. Available: Time Series Data Library - Data provider — DataMarket. [46] Yahoo Finance[EB/OL]. Available: http://ﬁnance.yahoo.com/. [47] J. Ma, J. Theiler, S. Perkins, Accurate on-line support vector regression, Neural Comput. 15 (2003) 2683–2703 Nov. [48] Y.L. Yang, J.X. Che, Y.Y. Li, Y.J. Zhao, S.L. Zhu, An incremental electric load forecasting model based on support vector regression, Energy 113 (2016) 796–808 Oct 15. R. Ye, Q. Dai Knowledge-Based Systems xxx (xxxx) xxx–xxx 26","libVersion":"0.3.2","langs":""}
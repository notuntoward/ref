{"path":"lit/lit_notes_OLD_PARTIAL/Chernozhukov21DistributionalConformalPrediction.pdf","text":"1 Supplementary Information for2 Distributional conformal prediction3 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu4 Yinchu Zhu5 E-mail: yinchuzhu@brandeis.edu6 This PDF ﬁle includes:7 Supplementary text8 Fig. S19 SI References10 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 1 of 17 Supporting Information Text11 1. Details for Extension: Optimal DCP12 Here we describe in detail how to implement the optimal prediction intervals described in Extension: Optimal DCP.13 A. Implementation. We now consider the estimation of b(·, ·). We assume that ˆF (y, x) is monotonic in y; if not, we ﬁrst14 rearrange it. We deﬁne ˆQ(·, ·) by15 ˆQ(τ, x) = inf { y : ˆF (y, x) ≥ τ } .16 Deﬁne17 L(x) = min z∈[0,α] Q(z + 1 − α, x) − Q(z, x)18 and19 ˆL(x) = min z∈[0,α] ˆQ(z + 1 − α, x) − ˆQ(z, x).20 Let ˆb(x, α) be a function such that ˆb(x, α) ∈ [0, α] and21 ˆQ(ˆb(x, α) + 1 − α, x) − ˆQ(ˆb(x, α), x) = ˆL(x). [S1.1]22 We propose the following algorithm.23 Algorithm S1 (Optimal split DCP).24 Input: Data {(Yt, Xt)}T t=1, miscoverage level α ∈ (0, 1), and point XT +125 Process:26 1. Split {1, . . . , T + 1} into T1 := {1, . . . , T0} and T2 := {T0 + 1, . . . , T }.27 2. Obtain ˆF and ˆb based on {Zt}t∈T1 .28 3. Compute { ˆV ∗ t }t∈T2 with ˆV ∗ t = ˆF (Yt, Xt) − ˆb(Xt, α) − 1 2 (1 − α).29 4. Compute ˆQ ∗ T2 , the (1 − α)(1 + 1/|T2|) empirical quantile of { | ˆV ∗ t |} t∈T2 .30 Output: Return the prediction set31 ̂Cconf (1−α)(XT +1) = { y : ∣ ∣ ∣ ˆF (y, Xt) − ˆb(XT +1, α) − 1 2 (1 − α)∣ ∣ ∣ ≤ ˆQ ∗ T2 } .32 (Since ˆF (·, XT +1) is monotonic, ̂Cconf (1−α)(XT +1) is an interval.)33 B. Regularity conditions. We now provide the regularity condition for Theorem 4. For simplicity we focus on the case of iid34 data. Recall that a legitimate cumulative distribution function F (·) on R is a non-decreasing right-continuous function such35 that limz→−∞ F (z) = 0 and limz→∞ F (z) = 1.36 Assumption S1. Suppose that the following hold:37 1. The data {(Yt, Xt)}t∈T2 is iid and |T2| → ∞.38 2. For any x ∈ X , ˆF (·, x) is a legitimate cumulative distribution function on R with probability one.39 3. supx∈X supy∈Y(x) | ˆF (y, x) − F (y, x)| = oP (1) as |T1| → ∞, where Y(x) is the support of conditional distribution Yt | Xt =40 x.41 4. There exist constants C1, C2 > 0 such that miny∈Y(x) f (y, x) ≥ C1 and supy∈Y(x) |y| ≤ C2 for any x ∈ X .42 The following assumption is used to prove the results in Theorem 5.43 Assumption S2. Suppose that the following hold.44 1. |T2|−1 ∑ t∈T2 ( ˆF (Yt, Xt) − Ut)2 = oP (1) and |T2|−1 ∑ t∈T2 (ˆb(Xt, α) − b(Xt, α))2 = oP (1), where b(·, ·) is the unique45 function satisfying the requirement in Lemma 2.46 2. supv∈R | ˜G∗(v) − G∗(v)| = oP (1), where ˜G∗(v) = |T2|−1 ∑ t∈T2 1{ ˆV ∗ t ≤ v} and G∗(·) is the distribution function of47 V ∗ t = Ut − b(Xt, α) − 1 2 (1 − α).48 3. There exists a constant C1 > 0 such that for any x ∈ X , inf y∈s(x) f (y, x) ≥ C1, where s(x) = [s1(x), s2(x)] is the support49 of the distribution Y | X = x.50 4. supy∈R ∣ ∣ ˆF (y, XT +1) − F (y, XT +1) ∣ ∣ = oP (1) and ˆb(XT +1, α) = b(XT +1, α) + oP (1).51 5. There exists a constant C2 > 0 such that for any x ∈ X , max{|s1(x)|, |s2(x)|} ≤ C2.52 2 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu The key requirement in Assumption S2 is the consistency of ˆb. Since ˆb is a solution to the optimization problem in Eq. (S1.1), we can establish its consistency using the same argument for the consistency of an M-estimator. Under Assumption S1, we only need to impose the convexity of the mapping z ↦→ Q(z + 1 − α, x) − Q(z). A simple suﬃcient condition is that there exists constants κ1, κ2 > 0 such that for any x ∈ X and for any z with |b(x, α) − z| ≤ κ1, ∂2 ∂z2 (Q(z + 1 − α, x) − Q(z, x)) ≥ κ2. For uni-modal distributions, the above condition can be veriﬁed once we assume that the density f (·, x) is not too ﬂat53 around Q(b(x, α), x) and Q(b(x, α) + 1 − α, x). A similar condition is imposed as Assumption 2 in (1).54 2. Regression models for conditional distributions55 An important advantage of the proposed approach is that it allows researchers to leverage powerful regression methods for56 estimating conditional CDFs. This section discusses semiparametric (and potentially penalized) QR and DR models, which are57 very popular in applied research. We emphasize that our method is generic and also works in conjunction with nonparametric58 estimators (e.g., 2–4) as well as high-dimensional methods based on trees and random forests (e.g., 5, 6) and neural networks59 (e.g., 7).60 A. Quantile regression methods. QR methods impose a model for the conditional quantiles Q(τ, x). The implied model for the61 conditional CDF is (8)62 F (y, x) = ∫ 1 0 1 {Q(τ, x) ≤ y} dτ. [S2.2]63 A leading example is where Q(τ, x) is assumed to be linear:64 Q (τ, x) = x⊤β(τ ) [S2.3]65 If Xt is low dimensional, the parameter of interest β(τ ) can be estimated using linear QR (9) as the solution to a convex66 program67 ˆβ(τ ) ∈ arg min β∈Rp T +1∑ t=1 ρτ (Yt − X ⊤ t β) , [S2.4]68 where ρτ (u) := u(τ − 1{u < 0}) is the check function. In problems where Xt is high-dimensional, it may be convenient to69 consider a penalized version of program Eq. (S2.4):70 ˆβ(τ ) ∈ arg min β∈Rp T +1∑ t=1 ρτ (Yt − X ⊤ t β) + P(β), [S2.5]71 where P(β) is a penalty function. Examples of P(β) include ℓ1-penalties (e.g., 10–12) and SCAD (e.g., 13). The conditional72 distribution can be estimated as73 ˆF (y, x) = ∫ 1 0 1 {x ⊤ ˆβ(τ ) ≤ y} dτ.74 B. Distribution regression methods. Instead of modeling the conditional quantile function, one can directly model the conditional75 CDF using DR (e.g., 8, 14, 15). DR methods impose a generalized linear model for the CDF:76 F (y, x) = Λ (x⊤β(y) ) ,77 where β(y) is the parameter of interest and Λ(·) is a known link function, for example, the Probit or Logit link.78 If Xt is low dimensional, the parameters β(y) can be estimated as79 ˆβ(y) ∈ arg max β∈Rp T +1∑ t=1 [ 1 {Yt ≤ y} log (Λ (X ⊤ t β)) + 1 {Yt > y} log (1 − Λ (X ⊤ t β))] . [S2.6]80 When Λ(·) is the Probit (Logit) link, this is simply a Probit (Logit) regression of 1 {Yt ≤ y} on Xt. In high dimensional settings,81 one can use a penalized version of program Eq. (S2.6) (e.g., with an ℓ1-penalty or an elastic net penalty). The conditional82 distribution can be estimated as ˆF (y, x) = Λ (x⊤ ˆβ(y) ).83 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 3 of 17 3. Proofs84 A. Proof of Lemma 1. The argument is similar to Theorem 2 in (16). Let δ > 0 be a constant to be chosen later. Deﬁne85 quantities RT = supv∈R | ˜G(v) − G(v)| and W = supx1̸=x2 |G(x1) − G(x2)|/|x1 − x2|.86 Let A = {t : | ˆVt − Vt| ≥ δ}. Fix x ∈ R. Then (T + 1) ∣ ∣ ˆG(x) − ˜G(x) ∣ ∣ ≤ ∣ ∣ ∣ ∣ ∣ ∑ t∈A (1{ ˆVt < x} − 1{Vt < x})∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∑ t∈Ac (1{ ˆVt < x} − 1{Vt < x})∣ ∣ ∣ ∣ ∣ (i) ≤ |A| + ∣ ∣ ∣ ∣ ∣ ( ∑ t∈Ac 1{ ˆVt < x} ) − ( ∑ t∈Ac 1{Vt < x} )∣ ∣ ∣ ∣ ∣ [S3.7] where (i) follows by the fact that the diﬀerence of two indicators takes value in {−1, 0, 1}. We notice that for t ∈ A c,87 Vt − δ < ˆVt < Vt + δ. Therefore,88 ∑ t∈Ac 1{Vt < x − δ} ≤ ∑ t∈Ac 1{ ˆVt < x} ≤ ∑ t∈Ac 1{Vt < x + δ}.89 Since ∑ t∈Ac 1{Vt ≤ x} is also between ∑ t∈Ac 1{Vt ≤ x − δ} and ∑ t∈Ac 1{Vt ≤ x + δ}, it follows that ∣ ∣ ∣ ∣ ∣ ( ∑ t∈Ac 1{ ˆVt < x} ) − ( ∑ t∈Ac 1{Vt < x} )∣ ∣ ∣ ∣ ∣ ≤ ∣ ∣ ∣ ∣ ∣ ( ∑ t∈Ac 1{Vt ≤ x + δ} ) − ( ∑ t∈Ac 1{Vt ≤ x − δ} )∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣(T + 1) [ ˜G(x + δ) − ˜G(x − δ) ] − ( ∑ t∈A 1{Vt ≤ x + δ} ) + ( ∑ t∈A 1{Vt ≤ x − δ} )∣ ∣ ∣ ∣ ∣ ≤ (T + 1) [ ˜G(x + δ) − ˜G(x − δ) ] + ∣ ∣ ∣ ∣ ∣ ∑ t∈A (1{Vt ≤ x + δ} − 1{Vt ≤ x − δ}) ∣ ∣ ∣ ∣ ∣ (i) ≤ (T + 1) (G(x + δ) − G(x − δ) + 2RT ) + |A| ≤ (T + 1) (2δW + 2RT ) + |A|, where (i) follows by the fact that the diﬀerence of two indicators takes value in {−1, 0, 1}. Combining the above display with90 (S3.7), we obtain that91 (T + 1) ∣ ∣ ˆG(x) − ˜G(x) ∣ ∣ ≤ 2|A| + (T + 1) (2δW + 2RT ) .92 Since the right-hand side does not depend on x, we have that93 sup x∈R ∣ ∣ ˆG(x) − ˜G(x)∣ ∣ ≤ 2 |A| T + 1 + 2δW + 2RT .94 To bound |A|, we notice that95 |A|φ(δ) ≤ ∑ t∈A φ(| ˆVt − Vt|) ≤ T +1∑ t=1 φ(| ˆVt − Vt|) ≤ oP (T + 1).96 Hence, the above two displays imply that97 sup x∈R ∣ ∣ ˆG(x) − G(x) ∣ ∣ ≤ sup x∈R ∣ ∣ ˆG(x) − ˜G(x)∣ ∣ + RT ≤ oP (1/φ(δ)) + 2δW + 3RT . [S3.8]98 Now we ﬁx an arbitrary η ∈ (0, 1). Choose δ = η/(6W ). Since 1/φ(δ) is a constant and RT = oP (1) by assumption, (S3.8) implies that lim sup T →∞ P ( sup x∈R ∣ ∣ ˆG(x) − ˜G(x) ∣ ∣ > η) ≤ lim sup T →∞ P (|oP (1/φ(δ))| > η/3) + lim sup T →∞ P (|2δW | > η/3) + lim sup T →∞ P (|RT | > η/9) = 0. 4 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu Since η > 0 is arbitrary, we have99 sup x∈R ∣ ∣ ˆG(x) − G(x)∣ ∣ = oP (1).100 Thus,101 ˆG( ˆVT +1) = G( ˆVT +1) + oP (1) (i) = G(VT +1) + oP (1),102 where (i) follows by |G( ˆVT +1) − G(VT +1)| ≤ W | ˆVT +1 − VT +1|. The proof is complete.103 B. Proof of Theorem 2. Notice that P (YT +1 ∈ ̂Cfull (1−α) (XT +1)) = P ( 1 T + 1 T +1∑ t=1 1 { ˆV (YT +1) t ≥ ˆV (YT +1) T +1 } > α ) = P (1 − ˆG( ˆVT +1) > α) . By Lemma 1, ˆG( ˆVT +1) = G(VT +1) + oP (1). Since G(·) is continuous, G(VT +1) has the uniform distribution on (0,1). The104 desired result follows.105 C. Proof of Theorem 3. Notice that Ut = F (Yt, Xt) is independent of Xt. Since VT +1 = ψ(UT +1), VT +1 is also independent of106 XT +1. This means that107 P (G(VT +1) ≤ α | XT +1) = P (G(VT +1) ≤ α).108 Since G(·) is the distribution function of VT +1 and is a continuous function, we have that P (G(VT +1) ≤ α) = α. The desired result follows by Lemma 1 and P (YT +1 ∈ ̂Cfull (1−α) (XT +1) | XT +1) = P ( 1 T + 1 T +1∑ t=1 1 { ˆV (YT +1) t ≥ ˆV (YT +1) T +1 } > α | XT +1 ) = P (1 − ˆG( ˆVT +1) > α | XT +1) . D. Proof of Lemma 2. We proceed in three steps.109 Step 1: show Qψ(1 − α) = (1 − α)/2.110 By the same argument as in Lemma S1 (proved later),111 P (|F (Yt, Xt) − b(Xt, α) − (1 − α)/2| ≤ 1 − α 2 | Xt) = 1 − α.112 Therefore,113 P ( |F (Yt, Xt) − b(Xt, α) − (1 − α)/2| ≤ 1 − α 2 ) = 1 − α.114 In other words, Qψ(1 − α) = (1 − α)/2.115 Step 2: show µ (Copt (1−α)(XT +1) ) = µ (Cconf (1−α)(XT +1)).116 By the deﬁnition of Copt (1−α)(XT +1),117 µ ( Copt (1−α)(XT +1) ) = min F (z2,XT +1)−F (z1,XT +1)≥1−α z2 − z1. [S3.9]118 Since F (·, XT +1) is a continuous function, we have that119 min F (z2,XT +1)−F (z1,XT +1)≥1−α z2 − z1 = min F (z2,XT +1)−F (z1,XT +1)=1−α z2 − z1. [S3.10]120 We can see this by contradiction. Let (z∗ 1 , z∗ 2 ) be the solution to the optimization in Eq. (S3.9). Suppose that F (z∗ 2 , XT +1) −121 F (z∗ 1 , XT +1) > 1 − α. Notice that the mapping g(z) = F (z, XT +1) − F (z∗ 1 , XT +1) is continuous in z. Since g(z∗ 2 ) > 1 − α122 and g(z∗ 1 ) = 0 < 1 − α. By the intermediate value theorem, there exists z∗∗ 2 ∈ (z∗ 1 , z∗ 2 ) such that g(z∗∗ 2 ) = 1 − α. Thus,123 F (z∗∗ 2 , XT +1) − F (z∗ 1 , XT +1) ≥ 1 − α and z∗∗ 2 − z∗ 1 < z∗ 2 − z∗ 1 , contradicting the assumption that (z∗ 1 , z∗ 2 ) is the solution to the124 optimization in Eq. (S3.9). Therefore, F (z∗ 2 , XT +1) − F (z∗ 1 , XT +1) = 1 − α. Therefore, we have that125 µ ( Copt (1−α)(XT +1) ) = min F (z2,XT +1)−F (z1,XT +1)=1−α z2 − z1.126 Since F (z2, XT +1)−F (z1, XT +1) = 1−α, we can write F (z2, XT +1) = F (z1, XT +1)+1−α, which means z2 = Q(F (z1, XT +1)+ 1 − α, XT +1). Since F (z1, XT +1) + 1 − α ≤ 1, we have F (z1, XT +1) ≤ α. Therefore, µ (Copt (1−α)(XT +1)) = min F (z1,XT +1)∈[0,α] Q(F (z1, XT +1) + 1 − α, XT +1) − z1 (i) = min w∈[0,α] Q(w + 1 − α, XT +1) − Q(w, XT +1), [S3.11] Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 5 of 17 where (i) follows by a change of variables w = F (z1, XT +1) (and thus z1 = Q(w, XT +1)).127 We notice that Cconf (1−α)(XT +1) = {y : |F (y, XT +1) − b(XT +1, α) − (1 − α)/2| ≤ Qψ(1 − α)} (i) = {y : |F (y, XT +1) − b(XT +1, α) − (1 − α)/2| ≤ (1 − α)/2} = {y : b(XT +1, α) ≤ F (y, XT +1) ≤ b(XT +1, α) + 1 − α} = [Q(b(XT +1, α), XT +1), Q(b(XT +1, α) + 1 − α, XT +1)] , [S3.12] where (i) follows by Qψ = (1 − α)/2. Thus, µ (Cconf (1−α)(XT +1)) = Q(b(XT +1, α) + 1 − α, XT +1) − Q(b(XT +1, α), XT +1) (i) = min w∈[0,α] Q(w + 1 − α, XT +1) − Q(w, XT +1), where (i) follows by the assumption that b(x, α) ∈ arg minw∈[0,α] Q(w + 1 − α, x) − Q(w, x) for any x ∈ X . By Eq. (S3.11),128 µ (Copt (1−α)(XT +1)) = µ (Cconf (1−α)(XT +1) ).129 Step 3: show that if Copt (1−α)(x) is uniquely deﬁned for any x ∈ X , then Copt (1−α)(XT +1) = Cconf (1−α)(XT +1).130 Notice that Copt (1−α)(x) = [r1(x, α), r2(x, α)], where the pair (r1(x, α), r2(x, α)) uniquely solves131 min z1,z2 z2 − z1 s.t. F (z2, x) − F (z1, x) ≥ 1 − α.132 By the argument in Eq. (S3.10), the pair (r1(x, α), r2(x, α)) uniquely solves133 min z1,z2 z2 − z1 s.t. F (z2, x) − F (z1, x) = 1 − α.134 By the same change of variables in Eq. (S3.11), r1(x, α) uniquely solves135 min z1 Q(F (z1, x) + 1 − α, x) − z1 s.t. F (z1, XT +1) ≤ α136 and r2(x, α) = Q(F (r1(x, α), x) + 1 − α, x). Similar to Eq. (S3.11), this can be rewritten as an optimization problem on [0, α].137 Since b(x, α) solves minw∈[0,α] Q(w + 1 − α, x) − Q(w, x), we have138 r1(x, α) = Q(b(x, α), x)139 and r2(x, α) = Q(b(x, α) + 1 − α, x). Thus, Copt (1−α)(x) = [r1(x, α), r2(x, α)] = [Q(b(x, α), x), Q(b(x, α) + 1 − α, x)]. By the140 same argument as in Eq. (S3.12), Cconf (1−α)(x) = [Q(b(x, α), x), Q(b(x, α) + 1 − α, x)]. Therefore, Copt (1−α)(x) = Cconf (1−α)(x). Since141 this holds for any x ∈ X , we have completed the proof.142 E. Proof of Theorem 4. We ﬁrst prove three auxiliary lemmas.143 Lemma S1. Let Assumption S1 hold. Let ˜V ∗ t = F (Yt, Xt) − ˆb(Xt) − (1 − α)/2 for t ∈ T2, where ˆb(Xt) := ˆb(Xt, α). Then144 P (| ˜V ∗ t | ≤ 1 − α 2 | Xt) = 1 − α.145 Moreover, for any non-random δ ∈ [−α, α],146 P (| ˜V ∗ t | ≤ 1 − α 2 + δ) − (1 − α) ≤ δ if δ ∈ [−α, 0]147 and148 P (| ˜V ∗ t | ≤ 1 − α 2 + δ) − (1 − α) ≥ δ if δ ∈ [0, α].149 Proof. We show the two claims in two steps.150 Step 1: show the ﬁrst claim.151 We observe that P (| ˜V ∗ t | ≤ 1 − α 2 | Xt) = P (|Ut − ˆb(Xt) − (1 − α)/2| ≤ 1 − α 2 | Xt) = P ( ˆb(Xt) ≤ Ut ≤ ˆb(Xt) + 1 − α | Xt) . 6 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu Recall that Ut = F (Yt, Xt) is independent of Xt and has the uniform distribution on [0,1]. Since t ∈ T2, (Ut, Xt) is independent of ˆb(·). Since ˆb(Xt) ∈ [0, α], we have that [ˆb(Xt), ˆb(Xt) + 1 − α] ⊆ [0, 1]. Therefore, P (| ˜V ∗ t | ≤ 1 − α 2 | Xt) = P ( ˆb(Xt) ≤ Ut ≤ ˆb(Xt) + 1 − α | Xt) = ( ˆb(Xt) + 1 − α) − ˆb(Xt) = 1 − α. Step 2: show the second claim.152 By the same argument as in Step 1, we have P (| ˜V ∗ t | ≤ 1 − α 2 + δ | Xt) = P (|Ut − ˆb(Xt) − (1 − α)/2| ≤ 1 − α 2 + δ | Xt) = P ( ˆb(Xt) − δ ≤ Ut ≤ ˆb(Xt) + 1 − α + δ | Xt) (i) = P (max{ˆb(Xt) − δ, 0} ≤ Ut ≤ min{ˆb(Xt) + 1 − α + δ, 1} | Xt) (ii) = min{ˆb(Xt) + 1 − α + δ, 1} − max{ˆb(Xt) − δ, 0} = min{ˆb(Xt) + 1 − α + δ, 1} + min{δ − ˆb(Xt), 0} where (i) and (ii) follow by the fact that Ut has the uniform distribution on [0,1] and is independent of Xt and ˆb(·). Thus, P (| ˜V ∗ t | ≤ 1 − α 2 + δ | Xt) − (1 − α) = min{ˆb(Xt) + 1 − α + δ, 1} + min{δ − ˆb(Xt), 0} − (1 − α) = min{ˆb(Xt) + δ, α} + min{δ − ˆb(Xt), 0} = min{δ, α − ˆb(Xt)} + ˆb(Xt) + min{δ, ˆb(Xt)} − ˆb(Xt) = min{δ, α − ˆb(Xt)} + min{δ, ˆb(Xt)}. [S3.13] We now consider the random mapping δ ↦→ g(δ) = min{δ, α − ˆb(Xt)} + min{δ, ˆb(Xt)}, where the randomness is from the153 randomness of Xt. Clearly,154 Eg(δ) = δ + δ = 2δ ≤ δ for δ ∈ [−α, 0].155 For δ ∈ [0, max{α − ˆb(Xt), ˆb(Xt)}], we have g(δ) ≥ δ. For δ ∈ [max{α − ˆb(Xt), ˆb(Xt)}, α], we have that156 g(δ) = α ≥ δ.157 Hence, for any δ ∈ [0, α], we have g(δ) ≥ δ, which implies Eg(δ) ≥ δ. The proof is complete.158 Lemma S2. Let Assumption S1 hold. Then sup(a,x)∈[0,1]×X | ˆQ(a, x) − Q(a, x)| = oP (1) and supx∈X | ˆL(x) − L(x)| = oP (1).159 Proof. Let ε = supx∈X supy∈Y(x) | ˆF (y, x) − F (y, x)|. Fix an arbitrary δ > 0 and x ∈ X . Since ˆF (·, x) is right-continuous and160 ˆQ(a, x) = inf{y : ˆF (y, x) ≥ a} for any a ∈ [0, 1], we have that ˆF ( ˆQ(a, x), x) = a. For simplicity, we write ˆF (y, x), ˆQ(a, x),161 F (y, x) and Q(a, x) as ˆF (y), ˆQ(y), F (y) and Q(a), respectively whenever no confusion arises.162 We consider the event {Q(a) > ˆQ(a) + δ} : { Q(a) > ˆQ(a) + δ} ⊆ { F (Q(a)) ≥ F ( ˆQ(a) + δ) } = {a ≥ F ( ˆQ(a) + δ) } (i) ⊆ {a ≥ F ( ˆQ(a)) + C1δ} ⊆ {a ≥ ˆF ( ˆQ(a)) − ε + C1δ} (ii) = {ε ≥ C1δ} , where (i) follows by the fact that F (b + δ, x) = F (b, x) + ∫ b+δ b f (z, x)dz ≥ F (b, x) + ∫ b+δ b C1dz = F (b, x) + C1δ and (ii) follows by ˆF ( ˆQ(a)) = a. Similarly, we observe that { ˆQ(a) > Q(a) + δ} ⊆ { ˆF ( ˆQ(a)) ≥ ˆF (Q(a) + δ) } = {a ≥ ˆF (Q(a) + δ)} ⊆ {a ≥ F (Q(a) + δ) − ε} ⊆ {a ≥ F (Q(a)) + C1δ − ε} = {ε ≥ C1δ} . By the above two displays, we have that163 {∣ ∣ ˆQ(a) − Q(a)∣ ∣ > δ} = {Q(a) > ˆQ(a) + δ} ⋃ { ˆQ(a) > Q(a) + δ} ⊆ {ε ≥ C1δ} .164 Notice that the right-hand side {ε ≥ C1δ} does not depend on x or a. Therefore,165 { sup (a,x)∈[0,1]×X ∣ ∣ ˆQ(a, x) − Q(a, x) ∣ ∣ > δ} ⊆ {ε ≥ C1δ} .166 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 7 of 17 Hence,167 P ( sup (a,x)∈[0,1]×X ∣ ∣ ˆQ(a, x) − Q(a, x) ∣ ∣ > δ) ≤ P (ε ≥ C1δ) (i) = o(1),168 where (i) follows by ε = oP (1). Since δ > 0 is arbitrary, we have proved sup(a,x)∈[0,1]×X | ˆQ(a, x) − Q(a, x)| = oP (1).169 To show supx∈X | ˆL(x) − L(x)| = oP (1), we deﬁne η = sup(a,x)∈[0,1]×X | ˆQ(a, x) − Q(a, x)|. We observe that ˆL(x) = min z∈[0,α] ˆQ(z + 1 − α, x) − ˆQ(z, x) ≤ min z∈[0,α] (Q(z + 1 − α, x) − Q(z, x) + 2η) = L(x) + 2η and ˆL(x) = min z∈[0,α] ˆQ(z + 1 − α, x) − ˆQ(z, x) ≥ min z∈[0,α] (Q(z + 1 − α, x) − Q(z, x) − 2η) = L(x) − 2η. Thus, | ˆL(x) − L(x)| ≤ 2η. Since this holds for any x ∈ X , we have supx∈X | ˆL(x) − L(x)| ≤ 2η. Because we have proved170 η = oP (1), it follows that supx∈X | ˆL(x) − L(x)| = oP (1). The proof is complete.171 Lemma S3. Let Assumption S1 hold. Then ˆQ∗ T2 = (1 − α)/2 + oP (1).172 Proof. Fix an arbitrary δ ∈ (0, α). Deﬁne the event A = { max t∈T2 | ˆV ∗ t − ˜V ∗ t | ≤ δ/2} ⋂ {∣ ∣ ∣ ∣ ∣|T2| −1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 + δ/2} − P (| ˜V ∗ t | ≤ (1 − α)/2 + δ/2)∣ ∣ ∣ ∣ ∣ ≤ δ/4 } ⋂ {∣ ∣ ∣ ∣ ∣|T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 − δ/2} − P (| ˜V ∗ t | ≤ (1 − α)/2 − δ/2)∣ ∣ ∣ ∣ ∣ ≤ δ/4 } . Since ˆQ∗ T2 is the (1 − α)(1 + |T2|−1) sample quantile of {| ˆV ∗ t |}t∈T2 , we have that173 (1 − α)(1 + |T2| −1) − |T2|−1 ≤ |T2| −1 ∑ t∈T2 1{| ˆV ∗ t | ≤ ˆQ ∗ T2 } ≤ (1 − α)(1 + |T2|−1) + |T2|−1. [S3.14]174 We consider the two events M1 = { ˆQ ∗ T2 > (1 − α)/2 + δ} and M2 = { ˆQ ∗ T2 < (1 − α)/2 − δ}. We will show three claims:175 P (Ac) = o(1), P (M1 ⋂ A) = o(1) and P (M2 ⋂ A) = o(1). Then the desired result follows by P (| ˆQ∗ T2 − (1 − α)/2| > δ) =176 P (M1 ⋃ M2) together with177 P (M1 ⋃ M2) ≤ P (A c) + P ((M1 ⋃ M2) ⋂ A ) ≤ P (Ac) + P (M1 ⋂ A) + P (M2 ⋂ A).178 We show these three claims in three steps.179 Step 1: show P (Ac) = o(1).180 Since { ˜V ∗ t }t∈T2 is independent, we have E [ |T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 + δ/2} − P (| ˜V ∗ t | ≤ (1 − α)/2 + δ/2)]2 = |T2|−2 ∑ t∈T2 E [ 1{| ˜V ∗ t | ≤ (1 − α)/2 + δ/2} − P (| ˜V ∗ t | ≤ (1 − α)/2 + δ/2)]2 (i) ≤ 1 4|T2| , where (i) follows by the fact that E(Z − P (Z = 1)) 2 = P (Z = 1) · (1 − P (Z = 1)) ≤ maxx∈[0,1] x(1 − x) ≤ 1/4 for any Bernoulli181 variable Z. Thus,182 P (∣ ∣ ∣ ∣ ∣|T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 + δ/2} − P (| ˜V ∗ t | ≤ (1 − α)/2 + δ/2)∣ ∣ ∣ ∣ ∣ > δ/4 ) ≤ 1 δ|T2| = o(1).183 8 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu Similarly, we can show that184 P (∣ ∣ ∣ ∣ ∣|T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 − δ/2} − P (| ˜V ∗ t | ≤ (1 − α)/2 − δ/2)∣ ∣ ∣ ∣ ∣ > δ/4 ) ≤ 1 δ|T2| = o(1).185 We notice that | ˆV ∗ t − ˜V ∗ t | ≤ | ˆF (Yt, Xt) − F (Yt, Xt)| and thus186 P ( max t∈T2 | ˆV ∗ t − ˜V ∗ t | > δ/2) ≤ P ( sup x∈X sup y∈Y(x) | ˆF (y, x) − F (y, x)| > δ/2) = o(1).187 Therefore, P (Ac) = o(1).188 Step 2: show P (M1 ⋂ A) → 0.189 On the event M1 ⋂ A, we have that |T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 + δ/2} = |T2|−1 ∑ t∈T2 1{| ˆV ∗ t | ≤ (1 − α)/2 + δ/2 + | ˆV ∗ t | − | ˜V ∗ t |} (i) ≤ |T2|−1 ∑ t∈T2 1{| ˆV ∗ t | ≤ (1 − α)/2 + δ/2 + δ/2} (ii) ≤ |T2|−1 ∑ t∈T2 1{| ˆV ∗ t | ≤ ˆQ ∗ T2 } (iii) ≤ (1 − α)(1 + |T2|−1) + |T2|−1, where (i) follows by the deﬁnition of A, (ii) follows by the deﬁnition of M1 and (iii) follows by Eq. (S3.14). On the event A,190 we have191 |T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 + δ/2} ≥ P (| ˜V ∗ t | ≤ (1 − α)/2 + δ/2) − δ/4 (i) ≥ (1 − α) + δ/2 − δ/4 = (1 − α) + δ/4,192 where (i) follows by Lemma S1.193 The above two displays imply that on the event M1 ⋂ A, (1−α)+δ/4 ≤ (1−α)(1+|T2|−1)+|T2| −1, which is δ ≤ 4(2−α)/|T2|.194 Since |T2| → ∞ and δ > 0 is ﬁxed, we have195 P (M1 ⋂ A ) ≤ 1{δ ≤ 4(2 − α)/|T2|} = o(1).196 Step 3: show P (M2 ⋂ A) → 0.197 The argument is similar to Step 2. On the event M2 ⋂ A, we have that |T2|−1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 − δ/2} = |T2|−1 ∑ t∈T2 1{| ˆV ∗ t | ≤ (1 − α)/2 − δ/2 + | ˆV ∗ t | − | ˜V ∗ t |} ≥ |T2|−1 ∑ t∈T2 1{| ˆV ∗ t | ≤ (1 − α)/2 − δ/2 − δ/2} ≥ |T2|−1 ∑ t∈T2 1{| ˆV ∗ t | ≤ ˆQ ∗ T2 } ≥ (1 − α)(1 + |T2|−1) − |T2| −1. On the event A, we have |T2| −1 ∑ t∈T2 1{| ˜V ∗ t | ≤ (1 − α)/2 − δ/2} ≤ P (| ˜V ∗ t | ≤ (1 − α)/2 − δ/2) + δ/4 (i) ≤ (1 − α) − δ/2 + δ/4 = (1 − α) − δ/4, where (i) follows by Lemma S1.198 The above two displays imply that on the event M2 ⋂ A, (1 − α) − δ/4 ≥ (1 − α)(1 + |T2|−1) − |T2| −1, which is δ/4 ≤ α/|T2|.199 Since |T2| → ∞ and δ > 0 is ﬁxed, we have200 P (M2 ⋂ A ) ≤ 1{δ ≤ 4α/|T2|} = o(1).201 The proof is complete.202 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 9 of 17 We are now ready to prove Theorem 4.203 Proof of Theorem 4. Let ε1 = ˆQ ∗ T2 − (1 − α)/2, ε2 = supy,x | ˆF (y, x) − F (y, x)|, ε3 = sup(a,x)∈[0,1]×X | ˆQ(a, x) − Q(a, x)| and204 ε4 = supx∈X | ˆL(x) − L(x)|. For simplicity, we write ̂Cconf (1−α) instead of ̂Cconf (1−α)(XT +1). We proceed in two steps.205 Step 1: show asymptotic conditional validity.206 To show P (YT +1 ∈ ̂Cconf (1−α) | XT +1) = 1 − α + oP (1), it suﬃces to verify that P (| ˆV ∗ T +1| ≤ ˆQ ∗ T2 | XT +1) = 1 − α + oP (1). We notice that P (| ˆV ∗ T +1| ≤ ˆQ ∗ T2 | XT +1) [S3.15] = P (| ˆF (YT +1, XT +1) − ˆb(XT +1) − (1 − α)/2| ≤ ˆQ ∗ T2 | XT +1) = P ( ˆb(XT +1) + (1 − α)/2 − ˆQ∗ T2 ≤ ˆF (YT +1, XT +1) ≤ ˆb(XT +1) + (1 − α)/2 + ˆQ∗ T2 | XT +1) = P ( ˆb(XT +1) − ε1 ≤ ˆF (YT +1, XT +1) ≤ ˆb(XT +1) + (1 − α) + ε1 | XT +1) = P ( ˆF (YT +1, XT +1) ≤ ˆb(XT +1) + (1 − α) + ε1 | XT +1) − P ( ˆF (YT +1, XT +1) < ˆb(XT +1) − ε1 | XT +1) . Observe that P (F (YT +1, XT +1) < ˆb(XT +1) − ε1 − ε2 | XT +1) ≤ P ( ˆF (YT +1, XT +1) < ˆb(XT +1) − ε1 | XT +1) ≤ P (F (YT +1, XT +1) < ˆb(XT +1) − ε1 + ε2 | XT +1) . Since F (YT +1, XT +1) is independent of (ε1, ε2, ˆb(XT +1), XT +1) and has the uniform distribution on [0,1], it follows that207 β ( ˆb(XT +1) − ε1 − ε2) ≤ P ( ˆF (YT +1, XT +1) < ˆb(XT +1) − ε1 | XT +1) ≤ β ( ˆb(XT +1) − ε1 + ε2) ,208 where209 β(z) =    1 if z > 1 0 if z < 0 z otherwise. 210 Clearly, |β(z1) − β(z2)| ≤ |z1 − z2| for any z1, z2 ∈ R. Thus, |β(ˆb(XT +1) − ε1 + ε2) − β(ˆb(XT +1))| ≤ | − ε1 + ε2| and211 |β(ˆb(XT +1) − ε1 − ε2) − β(ˆb(XT +1))| ≤ | − ε1 − ε2|. This means that212 ∣ ∣P ( ˆF (YT +1, XT +1) < ˆb(XT +1) − ε1 | XT +1) − β ( ˆb(XT +1) )∣ ∣ ≤ |ε1| + ε2.213 Similarly,214 ∣ ∣P ( ˆF (YT +1, XT +1) ≤ ˆb(XT +1) + 1 − α + ε1 | XT +1) − β ( ˆb(XT +1) + 1 − α)∣ ∣ ≤ |ε1| + ε2.215 By ˆb(XT +1) ∈ [0, α], we have β(ˆb(XT +1)) = ˆb(XT +1) and β(ˆb(XT +1) + 1 − α) = ˆb(XT +1) + 1 − α. Hence, the above two displays imply that ∣ ∣P ( ˆF (YT +1, XT +1) ≤ ˆb(XT +1) + (1 − α) + ε1 | XT +1) − P ( ˆF (YT +1, XT +1) < ˆb(XT +1) − ε1 | XT +1) − (1 − α) ∣ ∣ ≤ 2|ε1| + 2ε2. By Eq. (S3.15), we have216 ∣ ∣P (| ˆV ∗ T +1| ≤ ˆQ ∗ T2 | XT +1) − (1 − α)∣ ∣ ≤ 2|ε1| + 2ε2.217 Since ε1 and ε2 are oP (1), we have ∣ ∣P (| ˆV ∗ T +1| ≤ ˆQ ∗ T2 | XT +1) − (1 − α) ∣ ∣ = oP (1).218 Step 2: show asymptotic eﬃciency.219 We can rewrite the interval ̂Cconf (1−α) = {y : | ˆF (y, XT +1) − ˆb(XT +1) − (1 − α)/2| ≤ ˆQ∗ T2 } as220 ̂Cconf (1−α) = {y : ˆb(XT +1) + (1 − α)/2 − ˆQ∗ T2 ≤ ˆF (y, XT +1) ≤ ˆb(XT +1) + (1 − α)/2 + ˆQ ∗ T2 } .221 In other words, we can write it as222 ̂Cconf (1−α) = [ ˆQ (β(ˆb(XT +1) + (1 − α)/2 − ˆQ ∗ T2 ), XT +1) , ˆQ (β(ˆb(XT +1) + (1 − α)/2 + ˆQ ∗ T2 ), XT +1)] .223 We can now compute the length of ̂Cconf (1−α). We observe µ ( ̂Cconf (1−α)) = ˆQ (β(ˆb(XT +1) + (1 − α)/2 + ˆQ ∗ T2 ), XT +1) − ˆQ (β(ˆb(XT +1) + (1 − α)/2 − ˆQ ∗ T2 ), XT +1) (i) = ˆQ (β(ˆb(XT +1) + 1 − α + ε1), XT +1) − ˆQ (β(ˆb(XT +1) − ε1), XT +1) ≤ Q (β(ˆb(XT +1) + 1 − α + ε1), XT +1) − Q (β(ˆb(XT +1) − ε1), XT +1) + 2ε3, [S3.16] 10 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu where (i) follows by ˆQ ∗ T2 = (1 − α)/2 + ε1.224 We notice that for any a1, a2 ∈ [0, 1] with a1 > a2 and for any x ∈ X ,225 Q(a1, x) − Q(a2, x) = ∫ a1 a2 ( ∂Q(z, x) ∂z ) dz = ∫ a1 a2 ( 1 f (Q(z, x), x) ) dz ≤ ∫ a1 a2 ( 1 C1 ) dz = (a1 − a2)/C1.226 Therefore,227 sup a1,a2∈[0,1], a1̸=a2 sup x∈X ∣ ∣ ∣ ∣ Q(a1, x) − Q(a2, x) a1 − a2 ∣ ∣ ∣ ∣ ≤ 1/C1.228 Since |β(z1) − β(z2)| ≤ |z1 − z2| for any z1, z2 ∈ R, it follows that ∣ ∣Q (β(ˆb(XT +1) + 1 − α + ε1), XT +1) − Q (β(ˆb(XT +1) + 1 − α), XT +1)∣ ∣ ≤ ∣ ∣β(ˆb(XT +1) + 1 − α + ε1) − β(ˆb(XT +1) + 1 − α)∣ ∣ /C1 ≤ |ε1|/C1 and229 ∣ ∣Q (β(ˆb(XT +1) − ε1), XT +1) − Q (β(ˆb(XT +1)), XT +1)∣ ∣ ≤ |ε1|/C1.230 The above two displays and Eq. (S3.16) imply µ ( ̂Cconf (1−α)) ≤ 2|ε1|/C1 + 2ε3 + Q (β(ˆb(XT +1) + 1 − α), XT +1) − Q (β(ˆb(XT +1)), XT +1) (i) = 2|ε1|/C1 + 2ε3 + Q ( ˆb(XT +1) + 1 − α, XT +1) − Q ( ˆb(XT +1), XT +1) ≤ 2|ε1|/C1 + 4ε3 + ˆQ ( ˆb(XT +1) + 1 − α, XT +1) − ˆQ ( ˆb(XT +1), XT +1) = 2|ε1|/C1 + 4ε3 + ˆL(XT +1) ≤ 2|ε1|/C1 + 4ε3 + ε4 + L(XT +1) (ii) = L(XT +1) + oP (1), where (i) follows by ˆb(XT +1) ∈ [0, α] and (ii) follows by ε3 = oP (1) and ε4 = oP (1) (Lemma S2) as well as ε1 = oP (1) (Lemma S3). The desired result follows by µ (Copt (1−α)(XT +1)) = min F (z1,XT +1)−F (z2,XT +1)≥1−α z1 − z2 = min F (z1,XT +1)−F (z2,XT +1)=1−α z1 − z2 = min z∈[0,α] Q(z + 1 − α, XT +1) − Q(z, XT +1) = L(XT +1). 231 F. Proof of Theorem 5. For simplicity, we may omit XT +1 and α when no confusion can arise. For example, we write F (y),232 Q(y), f (y) and b rather than F (y, XT +1), Q(τ, XT +1), f (y, XT +1) and b(XT +1, α), respectively.233 By Lemma 2, we have that234 Copt (1−α)(XT +1) = Cconf (1−α)(XT +1) = { y : ∣ ∣ ∣F (y) − b − 1 − α 2 ∣ ∣ ∣ ≤ Qψ(1 − α)} ,235 where Qψ(1 − α) is the (1 − α) quantile of V ∗ t = F (Yt, Xt) − b(Xt, α) − 1−α 2 . Again by Lemma 2, Qψ(1 − α) = 1−α 2 . Therefore,236 Copt (1−α)(XT +1) = {y : b ≤ F (y) ≤ b + 1 − α} . [S3.17]237 On the other hand, ̂Cconf (1−α)(XT +1) = { y : ˆb(XT +1, α) + 1 − α 2 − ˆQ ∗ T2 ≤ ˆF (y, XT +1) ≤ ˆb(XT +1, α) + 1 − α 2 + ˆQ∗ T2 } = {y : b + ε1(y) ≤ F (y, XT +1) ≤ b + 1 − α + ε2(y)} , where ε1(y) = ˆb(XT +1, α) − b + 1−α 2 − ˆQ ∗ T2 + F (y, XT +1) − ˆF (y, XT +1) and ε2(y) = ε1(y) + 2 ˆQ ∗ T2 − (1 − α).238 The rest of the proof proceeds in two steps.239 Step 1: show that ˆQ ∗ T2 = (1 − α)/2 + oP (1).240 Notice that241 ∣ ∣| ˆV ∗ t | − |V ∗ t |∣ ∣ ≤ ∣ ∣ ˆF (Yt, Xt) − F (Yt, Xt) ∣ ∣ + ∣ ∣ˆb(Xt, α) − b(Xt, α) ∣ ∣ .242 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 11 of 17 By the elementary inequality (a + b)2 ≤ 2a 2 + 2b2, we have that243 |T2|−1 ∑ t∈T2(| ˆV ∗ t | − |V ∗ t |)2 ≤ 2|T2|−1 ∑ t∈T2 ( ˆF (Yt, Xt) − F (Yt, Xt))2 + 2|T2| −1 ∑ t∈T2 ( ˆb(Xt, α) − b(Xt, α))2 = oP (1).244 We now show that G∗(·) is Lipschitz. Fix any y1, y2 in the support of V ∗ t such that y1 < y2. Notice that P (y1 ≤ |V ∗ t | ≤ y2 | Xt) = P (y1 ≤ ∣ ∣ ∣Ut − b(Xt, α) − 1 2 (1 − α) ∣ ∣ ∣ ≤ y2 | Xt) = P (y1 ≤ Ut − b(Xt, α) − 1 2 (1 − α) ≤ y2 | Xt) + P (y1 ≤ − [Ut − b(Xt, α) − 1 2 (1 − α)] ≤ y2 | Xt) (i) ≤ (y2 − y1) + (y2 − y1) ≤ 2(y2 − y1), where (i) follows by the fact that conditional on Xt, Ut follows the uniform distribution on (0, 1). Thus,245 G∗(y2) − G∗(y1) = P (y1 ≤ |V ∗ t | ≤ y2) ≤ 2(y2 − y1).246 Therefore, supy1̸=y2 |G∗(y2) − G∗(y1)|/|y2 − y1| ≤ 2. By the same argument as the in the proof of Lemma 1,247 sup v∈R ∣ ∣ ˜G∗(v) − G∗(v) ∣ ∣ = oP (1).248 By the continuity of G∗(·), we have that ˆQ∗ T2 = G −1 ∗ (1 − α) + oP (1) (since ˆQ ∗ T2 is the (1 − α)(1 + 1/|T2|) quantile of ˜G∗(·)).249 Notice that G −1 ∗ (1 − α) = Qψ(1 − α). By Lemma 2, Qψ(1 − α) = (1 − α)/2. This means that ˆQ ∗ T2 = (1 − α)/2 + oP (1).250 Step 2: derive the ﬁnal result.251 By Step 1 and the assumptions that ˆb(XT +1, α) − b = oP (1) and supy∈R ∣ ∣ ˆF (y, XT +1) − F (y, XT +1)∣ ∣ = oP (1), we have252 that ¯ε1 := supy∈R |ε1(y)| = oP (1) and ¯ε2 := supy∈R |ε2(y)| = oP (1). Deﬁne H1 = {y : b − ¯ε1 ≤ F (y) ≤ b + 1 − α + ¯ε2} and253 H2 = {y : b + ¯ε1 ≤ F (y) ≤ b + 1 − α − ¯ε2}. Clearly,254 H2 ⊆ ̂Cconf (1−α)(XT +1) ⊆ H1 almost surely. [S3.18]255 On the other hand, we observe that H1 is an interval that can be written as256 H1 = [Q (max{b − ¯ε1, 0}) , Q (min{b + 1 − α + ¯ε2, 1})] .257 Since P (|YT +1| ≤ C2 | XT +1) = 1 and F (·) is strictly increasing, Q(0) and Q(1) are well deﬁned and satisfy max{|Q(0)|, |Q(1)|} ≤258 C2 almost surely.259 By Eq. (S3.17), we can write Copt (1−α)(XT +1) as an interval260 Copt (1−α)(XT +1) = [Q(b), Q(b + 1 − α)].261 Therefore,262 µ (H1△Copt (1−α)(XT +1)) ≤ |Q (max{b − ¯ε1, 0}) − Q(b)| + |Q (min{b + 1 − α + ¯ε2, 1}) − Q(b + 1 − α)| .263 Notice that dQ(u)/du = 1/f (Q(u)). By assumption, the density is bounded below by C1 on the support of YT +1 | XT +1. It264 follows that |dQ(u)/du| is uniformly bounded by 1/C1. Thus,265 |Q (max{b − ¯ε1, 0}) − Q(b)| ≤ 1 C1 · | max{b − ¯ε1, 0} − b| ≤ 1 C1 · ¯ε1266 and similarly267 |Q (min{b + 1 − α + ¯ε2, 1}) − Q(b + 1 − α)| ≤ 1 C1 · ¯ε2.268 The above three displays imply269 µ (H1△Copt (1−α)(XT +1) ) ≤ (¯ε1 + ¯ε2)C −1 1 .270 Similarly, we can show that271 µ (H2△Copt (1−α)(XT +1) ) ≤ (¯ε1 + ¯ε2)C −1 1 .272 By Eq. (S3.18), we have that, almost surely273 ̂Cconf (1−α)(XT +1)△Copt (1−α)(XT +1) ⊆ (H1△Copt (1−α)(XT +1) ) ⋃ (H2△Copt (1−α)(XT +1)) .274 The above three displays imply275 µ ( ̂Cconf (1−α)(XT +1)△Copt (1−α)(XT +1)) ≤ 2(¯ε1 + ¯ε2)C −1 1 .276 The desired result follows by ¯ε1 = oP (1) and ¯ε2 = oP (1).277 12 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 4. Time series discussion278 For time series data {Zt} T +1 t=1 with Zt = (Xt, Yt), it is often plausible that these T + 1 observations are not independent. Here,279 we assume that data is strictly stationary, i.e., for any m > 1, the distribution (Zt−m, Zt−m+1, . . . , Zt−1) does not depend on280 t. This is a common assumption in the time series literature. Although the data is not independent, it is often not strongly281 dependent either. Usually, we work with various notions of weak dependence. A popular way of deﬁning weak dependence is282 in terms of mixing conditions. There are numerous mixing conditions, see, for example, (17–19). We focus on the β-mixing283 condition (also known as the absolute regularity condition): for any m > 1,284 β(m) = 1 2 ∥P{Zt}t≤s, {Zt}t≥s+m − P{Zt}t≤s ⊗ P{Zt}t≥s+m ∥T V ,285 where P{Zt}t≤s denotes the probability measure of {Zt}t≤s, P{Zt}t≥s+m denotes the probability measure of {Zt}t≥s+m and286 P{Zt}t≤s, {Zt}t≥s+m denotes the probability measure of the joint random components ({Zt}t≤s, {Zt}t≥s+m). Here, ⊗ denotes287 the product measure and ∥ · ∥T V is the total-variation norm. Since the data is strictly stationary, the above deﬁnition does not288 depend on s. We borrow the above deﬁnition of Section 1.6 of (20), but equivalent deﬁnitions can be found in (17, 18) among289 others. ∗ 290 We say that the sequence {Zt} is β-mixing if β(m) → 0 as m → ∞. The β-mixing condition captures the idea that291 observations that are far apart in time become nearly independent. As m increases, {Zt}t≤s and {Zt}t≥s+m become more292 independent, in the sense that the joint distribution P{Zt}t≤s, {Zt}t≥s+m is close to the product measure of the marginal293 distributions P{Zt}t≤s ⊗ P{Zt}t≥s+m .294 The β-mixing condition is satisﬁed for a large class of stochastic processes. The simplest examples are perhaps m-dependent295 processes, which satisfy that {Zj}j≤t and {Zj}j≥s are independent as long as s − t ≥ m for some ﬁxed m. Moving average296 processes are m-dependent. Autoregressive moving average (ARMA) processes with independent errors are β-mixing. In297 general, strictly stationary Markov chains that are Harris recurrent and aperiodic are β-mixing (e.g., 17, 21). Several stochastic298 volatility models for asset returns, including the popular generalized autoregressive conditionally heteroskedastic (GARCH)299 models are also β-mixing with β(m) decaying exponentially with m (e.g., 22–24).300 Now we consider the problem of empirical risk minimization mentioned in Theoretical Performance Guarantees. Let301 F be a model, i.e., a class of functions of Zt = (Xt, Yt). Deﬁne F ∗ = arg minf ∈F RT +1(f ), where RT +1(f ) = (T +302 1) −1 ∑T +1 t=1 E[L(Zt, f )], where L is a loss function. Let ˆF = arg minf ∈F ˆRT +1(f ), where ˆRT +1(f ) = (T + 1) −1 ∑T +1 t=1 L(Zt, f ).303 Suppose that the following entropy condition with brackets holds:304 ∫ 1 0 √ε−1 log N[](ε, L(F), ∥ · ∥1,P )dε < ∞,305 where N[] is the bracketing number (see (25)), L(F ) is the class {L(Zt, f ) : f ∈ F} and ∥·∥1,P is the L1-norm ∥f ∥1,P = E|f (Zt)|.306 By Theorem 8.3 of (20), supf ∈F | ˆRT +1(f ) − RT +1(f )| = OP (T −1/2) as long as ∑∞ m=1 β(m) < ∞. (Similar results for empirical307 processes of dependent data can be found in (26).) By the usual arguments, it follows that 0 ≤ RT +1( ˆF ) − RT +1(F ∗) ≤ oP (1).308 Suppose that the risk function is convex in a neighborhood of F ∗: there exist C1, C2 > 0 such that RT +1(f ) − RT +1(F ∗) ≥309 C2∥f − F ∗∥2 sup whenever ∥f − F ∗∥sup ≤ C1 and f ∈ F, where ∥f ∥sup = supz |f (z)|. Then supy,x | ˆF (y, x) − F ∗(y, x)| =310 supz | ˆF (z) − F ∗(z)| = oP (1). This implies the consistency requirement in Assumption 1. Importantly, F ∗ does not need to be311 the true CDF F because F may or may not be in F.312 For the popular linear QR model, we establish a more concrete result; similar results can be established for DR. Suppose313 that Xt ∈ Rd for a ﬁxed d. Let ˆγ(u) = arg minγ∈Γ ∑T +1 t=1 ρu(Yt − X ⊤ t γ) for u ∈ [cT , 1 − cT ], where Γ ⊂ R d is a compact set314 and cT > 0 is either a small constant or a sequence tending to zero. Deﬁne315 ˆF (y, x) = cT + ∫ 1−cT cT 1{x ⊤ˆγ(u) ≤ y}du.316 Let ∥ · ∥2 denote the Euclidean norm in R d. We have the following result.317 Theorem S1. Assume that the data (Xt, Yt) is strictly stationary. Let γ∗(u) = arg minγ∈Γ Eρu(Yt − X ⊤ t γ) with ρu(a) =318 a(u − 1{a ≤ 0}). Deﬁne319 F ∗(y, x) = cT + ∫ 1−cT cT 1{x ⊤γ∗(u) ≤ y}du.320 Suppose that the following conditions hold:321 1. There exists a constant C1 > 0 such that ∥Xt∥2 ≤ C1 and |Yt| ≤ C1 almost surely.322 2. The β-mixing coeﬃcient of (Xt, Yt) satisﬁes ∑∞ m=1 β(m) < ∞.323 3. There exists a function h(·) such that limδ→0 h(δ) = 0 and |F ∗(y1, x) − F ∗(y2, x)| ≤ h(|y1 − y2|) for any (y1, y2) and any324 x with ∥x∥2 ≤ C1.325 ∗To see that these deﬁnitions are equivalent, one can ﬁnd details in Theorem 3.29 of (18). Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 13 of 17 4. f (y, x) = ∂F (y, x)/∂y exists and there exists a constant C2 > 0 such that f (y, x) ≥ C2 for any x and any y ∈ [s1(x), s2(x)],326 where [s1(x), s2(x)] is the support of the distribution Yt | Xt = x.327 5. the smallest eigenvalue of E(XtX ⊤ t ) is bounded below by a constant C3 > 0.328 Then supy∈R, ∥x∥2≤C1 | ˆF (y, x) − F ∗(y, x)| = oP (1).329 Theorem S1 establishes the uniform consistency of ˆF , which guarantees the consistency requirement in Assumption 1. Notice330 that Theorem S1 does not assume that F ∗ is the true conditional distribution function F . It thus generalizes the analysis of331 QR under misspeciﬁcation in (27) to time series settings.332 The assumptions of Theorem S1 are relatively mild. The boundedness of Xt and Yt can be relaxed with extra technical333 arguments. The summability of β-mixing coeﬃcients holds if β(m) decays exponentially. The third assumption says that334 F ∗(y, x) is uniformly continuous in y. The last assumption states that the true conditional density of Yt | Xt is bounded away335 from zero on the support.336 Proof of Theorem S1. We proceed in two steps.337 Step 1: show that supu∈[cT ,1−cT ] ∥ˆγ(u) − γ∗(u)∥2 = oP (1).338 Let R(γ, u) = Eρu(Yt − X ⊤ t γ) and ˆR(γ, u) = (T + 1)−1 ∑T +1 t=1 ρu(Yt − X ⊤ t γ). For any u1, u2 ∈ [cT , 1 − cT ] and γ1, γ2 ∈ Γ, we observe ∣ ∣ρu1 (y − x⊤γ1) − ρu2 (y − x⊤γ2) ∣ ∣ ≤ ∣ ∣ρu1 (y − x⊤γ1) − ρu1 (y − x ⊤γ2)∣ ∣ + ∣ ∣ρu1 (y − x⊤γ2) − ρu2 (y − x ⊤γ2)∣ ∣ ≤ max{u1, 1 − u1} · |x ⊤γ1 − x⊤γ2| + |u1 − u2| · |y − x⊤γ2| (i) ≤ C1∥γ1 − γ2∥2 + ( 1 + sup γ∈Γ ∥γ∥2) C1|u1 − u2|, where (i) follows by |y − x⊤γ2| ≤ C1 + C1 supγ∈Γ ∥γ∥2. Since Γ is compact and d is ﬁxed, supγ∈Γ ∥γ∥2 is bounded by a positive339 constant. Hence, (γ, u) ↦→ ρu(y − x ⊤γ) is Lipschitz. By Theorem 2.7.11 of (25) and the usual covering number bounds for340 Euclidean balls (e.g., Corollary 4.2.13 of (28)), we have that for any norm ∥ · ∥, the bracketing number satisﬁes341 N[](ε, G, ∥ · ∥) ≤ (K1/ε)d,342 where K1 ≥ 1 is a constant depending only on C1, supγ∈Γ ∥γ∥2 and d and G is the class of functions ρu(y − x ⊤γ) with (γ, u) ∈ Γ × [c1, 1 − c2]. Notice that ∫ 1 0 √ε−1 log N[](ε, G, ∥ · ∥1,P )dε ≤ ∫ 1 0 √ε−1 log ((K1/ε)d)dε = ∫ 1 0 √dε−1(log K1 − log ε)dε ≤ ∫ 1 0 √(d log K1)ε−1dε + √ −dε−1 log εdε < ∞. Therefore, it follows, by Theorem 8.3 of (20), that δT := sup(γ,u)∈Γ×[c1,1−c2] | ˆR(γ, u) − R(γ, u)| = OP (T −1/2).343 By the deﬁnition of γ∗(u) and ˆγ(u), we observe that R(γ∗(u), u) ≤ R(ˆγ(u), u) ≤ ˆR(ˆγ(u), u) + δT ≤ ˆR(γ∗(u), u) + δT ≤344 R(γ∗(u), u) + 2δT . Hence,345 0 ≤ R(ˆγ(u), u) − R(γ∗(u), u) ≤ 2δT . [S4.19]346 For any γ ∈ Γ, we observe that E [ ρu(Yt − X ⊤ t γ) − ρu(Yt − X ⊤ t γ∗(u)) | Xt = x] = ∫ f (y, x) (ρu(y − x⊤γ) − ρu(y − x ⊤γ∗(u)) ) dy (i) = ∫ f (y, x) [ −x⊤(γ − γ∗(u)) (u − 1{y − x ⊤γ∗(u) ≤ 0})] dy + ∫ f (y, x) [∫ x ⊤(γ−γ∗(u)) 0 (1{y − x⊤γ∗(u) ≤ s} − 1{y − x ⊤γ∗(u) ≤ 0} ) ds ] dy = (F (x ⊤γ∗(u), x) − u)x ⊤(γ − γ∗(u)) + ∫ x ⊤(γ−γ∗(u)) 0 (F (s + x ⊤γ∗(u), x) − F (x ⊤γ∗(u), x) ) ds, 14 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu where (i) follows by Equation (4.3) of (29). By the optimality condition of γ∗(u) = arg minγ∈Γ Eρu(Yt − X ⊤ t γ), we have E(F (X ⊤ t γ∗(u), Xt) − u)X ⊤ t = 0. Thus, the above display implies that for any γ ∈ Γ, R(γ, u) − R(γ∗(u), u) = E [∫ X⊤ t (γ−γ∗(u)) 0 (F (s + X ⊤ t γ∗(u), x) − F (X ⊤ t γ∗(u), x) ) ds ] (i) ≥ 1 2 C2E (X ⊤ t (γ − γ∗(u)) )2 ≥ 1 2 C2C3∥γ − γ∗(u)∥2 2, where (i) follows by f (y, x) ≥ C2 for y ∈ [s1(x), s2(x)]. By Eq. (S4.19) and the above display,347 1 2 C2C3∥ˆγ(u) − γ∗(u)∥2 2 ≤ R(ˆγ(u), u) − R(γ∗(u), u) ≤ 2δT .348 Since this bound holds for any u, we have that349 sup u∈[c1,1−c1] ∥ˆγ(u) − γ∗(u)∥2 2 ≤ 4δT /(C2C3).350 Since we have proved δT = oP (1), we have supu∈[cT ,1−cT ] ∥ˆγ(u) − γ∗(u)∥2 = oP (1).351 Step 2: show the desired result.352 Let εT = supu∈[cT ,1−cT ] ∥ˆγ(u) − γ∗(u)∥2. Then353 sup ∥x∥2≤C1,u∈[c1,1−c1] |x⊤ˆγ(u) − x⊤γ∗(u)| ≤ C1εT . [S4.20]354 We observe that for any x ∈ Rd with ∥x∥2 ≤ C1,355 ˆF (y, x) = cT + ∫ 1−cT cT 1{x ⊤ˆγ(u) ≤ y}du (i) ≤ cT + ∫ 1−cT cT 1{x⊤γ∗(u) − C1εT ≤ y}du = F ∗(y + C1εT , x),356 where (i) follows by Eq. (S4.20). Similarly, we can show that ˆF (y, x) ≥ F ∗(y − C1εT , x). Therefore,357 | ˆF (y, x) − F ∗(y, x)| ≤ max {F ∗(y + C1εT , x) − F ∗(y, x), F ∗(y, x) − F ∗(y − C1εT , x)} ≤ h(C1εT ).358 Since this bounds holds for any y and x, we have that sup∥x∥2≤C1 supy∈R | ˆF (y, x) − F ∗(y, x)| ≤ h(C1εT ). By εT = oP (1)359 and limδ→0 h(δ) = 0, the desired result follows.360 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 15 of 17 5. Additional ﬁgures361 DCP−QR Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 DCP−QR* Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 DCP−DR Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 CQR Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 CQR−m Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 CQR−r Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 CP−OLS Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 CP−loc Predicted conditional coverage 0.5 0.6 0.7 0.8 0.9 1.0 Fig. S1. Histograms of estimated conditional coverage probability. Vertical line at nominal coverage of 1 − α = 0.9. References362 1. J Lei, L Wasserman, Distribution-free prediction bands for non-parametric regression. J. Royal Stat. Soc. Ser. B (Statistical363 Methodol. 76, 71–96 (2014).364 2. P Chaudhuri, Global nonparametric estimation of conditional quantile functions and their derivatives. J. Multivar. Analysis365 39, 246 – 269 (1991).366 3. R Koenker, P Ng, S Portnoy, Quantile smoothing splines. Biometrika 81, 673–680 (1994).367 4. X He, P Ng, S Portnoy, Bivariate quantile smoothing splines. J. Royal Stat. Soc. Ser. B (Statistical Methodol. 60, 537–550368 (1998).369 5. P Chaudhuri, WY Loh, Nonparametric estimation of conditional quantiles using quantile regression trees. Bernoulli 8,370 561–576 (2002).371 16 of 17 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 6. N Meinshausen, Quantile regression forests. J. Mach. Learn. Res. 7, 983–999 (2006).372 7. JW Taylor, A quantile regression neural network approach to estimating the conditional density of multiperiod returns. J.373 Forecast. 19, 299–311 (2000).374 8. V Chernozhukov, I Fernandez-Val, B Melly, Inference on counterfactual distributions. Econometrica 81, 2205–2268 (2013).375 9. R Koenker, G Bassett, Regression quantiles. Econometrica 46, 33–50 (1978).376 10. R Koenker, Quantile regression for longitudinal data. J. Multivar. Analysis 91, 74 – 89 (2004) Special Issue on377 Semiparametric and Nonparametric Mixed Models.378 11. Y Li, J Zhu, L1-norm quantile regression. J. Comput. Graph. Stat. 17, 163–185 (2008).379 12. A Belloni, V Chernozhukov, L1-penalized quantile regression in high-dimensional sparse models. The Annals Stat. 39,380 82–130 (2011).381 13. Y Wu, Y Liu, Variable selection in quantile regression. Stat. Sinica 19, 801–817 (2009).382 14. S Foresi, F Peracchi, The conditional distribution of excess returns: An empirical analysis. J. Am. Stat. Assoc. 90, 451–466383 (1995).384 15. V Chernozhukov, I Fernández-Val, B Melly, K Wüthrich, Generic inference on quantile and quantile eﬀect functions for385 discrete outcomes. J. Am. Stat. Assoc. 115, 123–137 (2020).386 16. V Chernozhukov, K Wüthrich, Z Yinchu, Exact and robust conformal inference methods for predictive machine learning387 with dependent data in Proceedings of the 31st Conference On Learning Theory, Proceedings of Machine Learning Research,388 eds. S Bubeck, V Perchet, P Rigollet. (PMLR), Vol. 75, pp. 732–749 (2018).389 17. RC Bradley, Basic properties of strong mixing conditions. a survey and some open questions. Probab. surveys 2, 107–144390 (2005).391 18. RC Bradley, Introduction to strong mixing conditions. (Kendrick Press Heber City Utah) Vol. 1, (2007).392 19. J Dedecker, et al., Weak dependence. (Springer New York, New York, NY), pp. 9–20 (2007).393 20. E Rio, Asymptotic Theory of Weakly Dependent Random Processes. (Springer), (2017).394 21. SP Meyn, RL Tweedie, Markov chains and stochastic stability. (Springer Science & Business Media), (2012).395 22. F Boussama, Ergodicité, mélange et estimation dans les modeles garch (PhD Thesis, Paris 7) (1998).396 23. M Carrasco, X Chen, Mixing and moment properties of various GARCH and stochastic volatility models. Econom. Theory397 18, 17–39 (2002).398 24. C Francq, JM Zakoïan, Mixing properties of a general class of GARCH (1,1) models without moment assumptions on the399 observed process. Econom. Theory 22, 815–834 (2006).400 25. A van der Vaart, J Wellner, Weak Convergence and Empirical Processes: With Applications to Statistics. (Springer Science401 & Business Media), (1996).402 26. J Dedecker, S Louhichi, Maximal inequalities and empirical central limit theorems in Empirical process techniques for403 dependent data, eds. H Dehling, T Mikosch, M Sorensen. (Springer), pp. 137–159 (2002).404 27. J Angrist, V Chernozhukov, I Fernández-Val, Quantile regression under misspeciﬁcation, with an application to the US405 wage structure. Econometrica 74, 539–563 (2006).406 28. R Vershynin, High-dimensional probability: An introduction with applications in data science. (Cambridge University407 Press) Vol. 47, (2018).408 29. R Koenker, Quantile regression, Econometric Society Monographs. (Cambridge University Press) No. 38, (2005).409 Victor Chernozhukov, Kaspar Wüthrich, Yinchu Zhu 17 of 17","libVersion":"0.3.2","langs":""}
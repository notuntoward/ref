{"path":"lit/lit_notes_OLD_PARTIAL/Lapedriza07HierarchicalApproachMultitask.pdf","text":"A Hierarchical Approach for Multi-Task Logistic Regression \u0012Agata Lapedriza1, David Masip2 and Jordi Vitri\u0012a1 1 Computer Vision Center-Dept. Inform\u0012atica Universitat Aut\u0012onoma de Barcelona, 08193 Bellaterra, Spain fagata, jordig@cvc.uab.es 2 Universitat de Barcelona (UB), 08007 Barcelona , Spain davidm@maia.ub.es Abstract. In the statistical pattern recognition \feld the number of sam- ples to train a classi\fer is usually insu\u000ecient. Nevertheless, it has been shown that some learning domains can be divided in a set of related tasks, that can be simultaneously trained sharing information among the di\u000berent tasks. This methodology is known as the multi-task learning paradigm. In this paper we propose a multi-task probabilistic logistic re- gression model and develop a learning algorithm based in this framework, which can deal with the small sample size problem. Our experiments per- formed in two independent databases from the UCI and a multi-task face classi\fcation experiment show the improved accuracies of the multi-task learning approach with respect to the single task approach when using the same probabilistic model. 1 Introduction Automatic pattern classi\fcation is one of the most active research topics in the machine learning \feld. This problem consists in assigning a given instance to a prede\fned group or class after observing di\u000berent samples of this group. Examples of these frameworks in scienti\fc areas are medical diagnosis, speech recognition or image categorization. Statistical procedures have been shown to be a powerful tool to treat these classi\fcation problems, where an underlying probability model is assumed in order to calculate the posterior probability upon which the classi\fcation decision is made. Nevertheless, in these classical approaches a considerable number of training examples is needed to correctly learn the parameters of the model. For this reason, their application can be not appropriate when the obtention of training samples is di\u000ecult. There are some situations where the estimation of a predictive model can take bene\ft from the estimation of other related ones. For instance, in a multiple speech recognition problem, we can share information from modelling the speech of di\u000berent subjects, in handwritten text classi\fcation from di\u000berent writers we could also take bene\ft from the several related classi\fcation tasks. Other ex- amples in the computer vision \feld are identity veri\fcation problems, or related 2 \u0012Agata Lapedriza, David Masip and Jordi Vitri\u0012a tasks in automatic drive guiding problems such as road lane tracking, broken or solid line classi\fcation, or direction marks identi\fcation. In these examples, each of the considered tasks belong to di\u000berent problems. Nevertheless it seems clear that they belong to a related domain, where they share common information that can be used to improve the classi\fcation accuracies obtained in the single task learning framework. One of the most important open problems in the statistical classi\fcation approach, is the lack of learning samples necessary to properly estimate the pa- rameters of the classi\fer. Usually, in classi\fcation problems the data lays on high dimensional subspaces, being the theoretical number of samples needed exponential in terms of the data dimensionality (known as the curse of dimen- sionality problem [1]). Recently, it has been proposed a new learning paradigm, the multi-task learning (MTL) [2], that has been shown to mitigate this small sample size problem [3, 4]. The MTL approach is based on simultaneously learn- ing a set of related tasks, sharing the hypothesis space of classi\fers or assuming some common generative process in the data from each tasks [5, 6]. The advan- tages of MTL have been proved in the recent theory, and can be summarized in: (i) the bias learned in a multiple related task environment is less speci\fc than in a single task problem, resulting in classi\fers with less generalization error; (ii) the number of samples needed to simultaneously learn several related tasks sub-linearly decreases as a function of the number of tasks [4]. More recently the idea of multi-task learning has been extended to some of the state of the art classi\fers: Evgeniou et al. applied MTL to the SVM [7] and Torralba et al. extended the Adaboost algorithm to the MTL case by sharing the feature space where each weak learned is trained [8]. In this work we propose a hierarchical Multi-task learning approach for the logistic regression model and also extend this idea to the multinomial logistic regression case. Once the model is presented we develop a learning algorithm ac- cording to this framework. The paper is organized as follows: in the next section the hierarchical multi-task logistic regression approach is explained in detail as well as the corresponding algorithm and its extension to the multinomial logis- tic regression case, section 3 describes the performed experiments and section 4 includes the discussion of the results. Finally, section 5 concludes this work. 2 A Hierarchical Learning Approach for Multi-Task Logistic Regression Let be T1; :::; TM a set of related binary tasks and D = fS1; :::; SM g the set of corresponding training data, Si = f(xi ; yi n)gn=1;::;N (i) such that xi 2 Rd, yi n 2 f\u00001; 1g. Consider for each task a logistic regression model, that is, for each Ti we learn a classi\fer fi, that will give the probability of the output y = 1 according to the i-th task for the input x, fi(x) = P (y = 1jx; Ti) = 1 1 + exp(\u0000w(i)xT ) (1) A Hierarchical Approach for Multi-Task Logistic Regression 3 where w(i) = (wi 1; :::; wi d) is the parameters vector of the i-th task. Let be W the parameters matrix, considering all the tasks, W = 0 w(1) 1 : : : w(M ) 1 . . . w(1) d : : : w(M ) d 1 To learn the parameters of the model we can apply a negated log-likelihood estimator L(D; W ) and impose a prior distribution on the elements of W as a regularization therm, R(W ). In that case, the negated log-likelihood estimator for all the tasks Ti is L(D; W ) = \u0000log[ MY i=1[ N (i)Y n=1 P (yn i jxn; W )]] = \u0000[ MX i=1[ N (i)X n=1 log(P (yn i jxn; W ))] (2) and regarding to the regularization therm, most of the current methods use centered Gaussian priors. Then, the elements of the matrix W are obtained by the minimization of the following loss function H(W ) = L(D; W ) + 1 ˙2 kW k2 (3) where ˙ 2 R+ is the variance of the imposed regularization distribution. This optimization problem can be solved applying any appropriated method, for example a gradient descent algorithm [9]. This method has shown to be e\u000ecient in many situations. However, observe that in this presented framework there is no transit of information between the models of the di\u000berent tasks. Suppose that we want to learn the parameters of the logistic regression for this classi\fcation scenario enforcing the di\u000berent classes to share information, following the principles of MTL. For this purpose, we can impose prior distributions on each row of W in a hierarchical way as follows. Consider the mean vector \u0016w = ( \u0016w1; :::; \u0016wd) where \u0016wj = PM=1 w(i) j M (4) First, we can impose a Gaussian centered prior to the mean vector \u0016w and after that we can enforce that each row of W follows a Gaussian distribution with \u0016wd mean. In short, this can be obtained by the minimization of the loss function G(W ) = L(D; W ) + 1 ˙2 1 k \u0016wk2 + 1 ˙2 2 MX i=1 kw(i) \u0000 \u0016wk2 = L(D; W ) + R(W ) (5) where L(D; W ) is again the negated log-likelihood estimator and ˙2 r are the corresponding variances of the imposed priors, r = 1; 2. 4 \u0012Agata Lapedriza, David Masip and Jordi Vitri\u0012a 2.1 Training Algorithm Any optimization method that allows to minimize G will yield a training algo- rithm for our purpose. In this case we can apply a gradient descent algorithm to optimize it given that the loss function in equation 5 is di\u000berentiable. More con- cretely, we have used the BFGS gradient descent method. The principal idea of the method is to construct an approximate Hessian matrix of second derivatives of the function to be minimized, by analyzing successive gradient vectors. This approximation of the function's derivatives allows the application of a quasi- Newton \ftting method in order to move towards the minimum in the parameter space. Thus, we need to compute the partial derivatives @G(W ) @w(s) k = @L(W; D) @w(s) k + @R(W ) @w(s) k (6) Observe that R(W ) can rewritten as follows R(W ) = dX j=1[ \u0016w2 ˙2 1 + 1 ˙2 2 MX i=1(wi j \u0000 \u0016wj)2] (7) and this is the only part of G(W ) that depends on \u0016w. Thus, given that we want to minimize this function, we can get an expression for \u0016wj depending on W by \u0016wj = arg min w ( w2 ˙2 1 + 1 ˙2 2 MX i=1(wi j \u0000 w)2) (8) that yields \u0016wj(W ) = ˙2 1 PM=1 wi j ˙2 2 + M ˙2 1 (9) and consequently @ \u0016wj(W ) @w(s) k = ( ˙2 1 ˙2 2 +M ˙2 1 if j = k 0 if j 6= k Moreover, @R(W ) @w(s) k = 2 \u0016wk ˙2 1 @ \u0016wk @w(s) k + 2 ˙2 2 MX i=1[(w(i) k \u0000 \u0016wk) @ \u0016wk @w(s) k ) (10) and substituting by the functions in equations 9 and the corresponding derivatives we obtain the \fnal expression for the partial derivatives of R(W ). A Hierarchical Approach for Multi-Task Logistic Regression 5 2.2 Extension to the Multinomial Logistic Regression Model Multinomial Logistic Regression model is a statistical model suitable for proba- bilistic multi-class classi\fcation problems. Formally, given M classes C1; :::; CM , any element x in the input space Rd is categorized according to the criterion class(x) = arg max Ci;i=1::M P (x 2 Ci) PM=1 P (x 2 Ck) (11) where P (x 2 Ci) = 1 1 + exp(\u0000w(i)xT ) (12) and each w(i) is the parameters vector corresponding to the ith-class, that is the ith-column of the parameters matrix W = 0 w(1) 1 : : : w(M ) 1 . . . w(1) d : : : w(M ) d 1 Assuming that we have a training set of samples D = f(xn; yn)gn=1;::;N , where each xn 2 Rd and yn 2 fC1; :::; CM g, we can consider the loss function described above (see 5) to \fx the parameters supposing that L(W; D) is now the negated log-likelihood estimator for this new situation, according to equation 11 and 12. 3 Experiments To test the presented model for both multi-task and multi-class problems we have performed di\u000berent experiments. For the multi-task case we have learned di\u000berent face veri\fcation tasks and have used images from the public ARFace Database [10]. For the multi-class case, we have performed classi\fcation experi- ments in two databases from the UCI Machine Learning Repository [11]. 3.1 Multi-task experiments To test the algorithm in the case of multiple binary related tasks we have per- formed a set of face veri\fcation experiments using the public database AR Face (http://rvl.www.ecn.purdue.edu/RVL/ ). Here we consider that a veri\fcation task is a binary problem consisting on decide whether a new unseen face image be- longs to the learned subject or not. The AR Face database contains 26 frontal face images from 126 di\u000berent subjects. The data set has from each person 1 sample of neutral frontal images, 3 samples with strong changes in the illumination, 2 samples with occlusions (scarf and glasses), 4 images combining occlusions and illumination changes, and 6 \u0012Agata Lapedriza, David Masip and Jordi Vitri\u0012a Fig. 1. Some samples of images in the AR Face database. Table 1. Obtained error and 95% con\fdence intervals for the logistic regression method trained separately (\frst row) and for our shared logistic approach (second row). When more than 4 veri\fcation tasks are simultaneously trained, the error rates of the shared approach become lower. No mean error is shown in the case of multi-task logistic regression when only one task is considered. 1 2 3 4 5 Logistic 32:9 \u0006 8:2 34:5 \u0006 6:4 30:5 \u0006 5:3 31:8 \u0006 4:2 30:2 \u0006 3:9 Multi-task Logistic - 41:6 \u0006 4:2 35:8 \u0006 5:4 32:1 \u0006 5:2 28:7 \u0006 5:2 6 7 8 9 10 Logistic 31:8 \u0006 3:6 31:4 \u0006 3:3 29:6 \u0006 3:3 30:2 \u0006 3:0 29:6 \u0006 2:9 Multi-task Logistic 27:2 \u0006 4:2 23:6 \u0006 3:1 21:8 \u0006 2:8 17:5 \u0006 2:4 15:4 \u0006 2:3 3 samples with gesture e\u000bects. Images where taken in two separately periods of time (two samples from each type). Some examples of images in the AR Face database are shown in \fgure 1. We have performed the experiments considering from 2 to 10 veri\fcation problems. In this experiments we have used 2 positive samples and 4 negative samples to train the system, and the test set includes 20 positive images and 40 negatives. We have performed 10 experiments for each case, and both train and test samples have been randomly selected. The parameters of the method that we have used in multi-task case are ˙1 = 2 and ˙2 = 6. In single task case we used ˙ = 2. Table 1 includes the mean error obtained in each case and the corresponding con\fdence intervals. 3.2 Multi-Class classi\fcation experiments We have used Balance and Iris databases from the UCI Machine Learning Repos- itory to perform multi-class classi\fcation experiments. In table 2 are detailed the characteristics of these databases. The parameters of the method have been adjusted by cross validation. The values were ˙1 = 2 and ˙2 = 6 for the multi-task case, and ˙ = 2 for the single task training. Given that multi-task learning frameworks are specially appropriated when there are few elements in the training set, we have used 10% of the data in A Hierarchical Approach for Multi-Task Logistic Regression 7 Table 2. Balance and Iris databases details. Database Number of elements Number of features Number of classes Balance 625 4 3 Iris 150 4 3 the training step and 90% in the test step. We have performed 10 10-fold cross validation experiments and the results are detailed in table 3. Table 3. Error and con\fdence interval in the classi\fcation experiments using Balance and Iris databases using single-task and multi-task training processes. Database Single-Task Multi-Task Balance 36:76% \u0006 1:48% 31:48% \u0006 1:29% Iris 14:45% \u0006 1:85% 7:04% \u0006 0:65% 3.3 Discussion In the multi-task learning experiments we observe a considerable improvement of the accuracy when using the proposed multi-task logistic regression approach. On the one hand, when the single task model is used, the accuracy does not variate when we consider more tasks. However, when we use the proposed MTL approach we can observe that the accuracy increases when we consider more tasks, and this improvement is specially signi\fcant when more than 7 tasks are considered, where we do not have overlapping between the obtained results with the corresponding con\fdence intervals in both cases. To justify this evolution of the results, it should be taken into account that with the presented model the method can detect in a more general way features that are relevant for any subject veri\fcation task. In these experiments, the task relatedness is clear: the features that can be relevant to determine whether a face belongs to a given subject or not can be as well interesting to verify another subject. In the multi-class learning experiments performed with Balance and Iris databases from the UCI data sets, there is also a signi\fcant improvement of the results when we use the MTL approach, although the statistical relationship of the features among the di\u000berent classes is not as clear as in the face veri\fcation case.Conclusions In this paper we propose a multi-task learning approach based on sharing knowl- edge from the parameter space of the probabilistic model. The contribution of the information sharing among the related classi\fcation tasks is specially noticeable when only a few samples per class are available. 8 \u0012Agata Lapedriza, David Masip and Jordi Vitri\u0012a The experiments performed using two data sets from the UCI database, and a face classi\fcation problem using the AR Face data base suggest that the multi- task approach fares better than a single task learning of the same tasks using the same probabilistic logistic regression model. Notice that the MTL restrictions that the model assumes are strong, for this reason it can not be appropriated in general data sets. However, there are cases where these restrictions do hold and in these cases the improvement of our MTL approach is notably. Therefore we plan as a future work to develop a less restrictive version of this MTL modelling. The probabilistic model presented in this paper suggests new lines of future research. In this formulation, we impose the knowledge sharing property by con- straining the parameter space of the classi\fers along the multiple tasks. However, more complex approaches based on hidden distributions on the parameters space can be considered. Moreover, in MTL topic there are still open lines of research, for example to de\fne formally the task relatedness concept. In our model, we impose statistical priors on the task distribution, assuming certain feature information share among the tasks. Given that this assumption is quite restrictive, the method will be appropriated only when the data distribution is agree with this considerations. Acknowledgments. This work is supported by MEC grant TIN2006-15308- C02-01, Ministerio de Ciencia y Tecnologia, Spain. References 1. R. Bellman: Adaptive Control Process: A Guided Tour. Princeton University Press, New Jersey (1961) 2. Caruana, R.: Multitask learning. Machine Learning 28(1) (1997) 41{75 3. S.Thrun, L.Pratt: Learning to Learn. Kluwer Academic (1997) 4. Baxter, J.: A model of inductive bias learning. Journal of Machine Learning Research 12 (2000) 149{198 5. Intrator, N., Edelman, S.: Making a low-dimensional representation suitable for diverse tasks. Connection Science 8 (1997) 205{224 6. Zhang, J., Ghahramani, Z., Yang, Y.: Learning multiple related tasks using latent independent component analysis. In Weiss, Y., Sch\u0000olkopf, B., Platt, J., eds.: Ad- vances in Neural Information Processing Systems 18. MIT Press, Cambridge, MA (2006) 7. T.Evgeniou, C.Micchelli, M.Pontil: Learning multiple tasks with kernel methods. Journal of Machine Learning Research 6 (2005) 615{637 8. Torralba, A., Murphy, K., Freeman, W.: Sharing features: e\u000ecient boosting pro- cedures for multiclass object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2004) 9. Madigan, D., Genkin, A., Lewis, D.D., Fradkin, D.: (Bayesian multinomial logistic regression for author identi\fcation) 10. Martinez, A., Benavente, R.: The AR Face database. Technical Report 24, Com- puter Vision Center (1998) 11. Blake, C., Merz, C.: UCI repository of machine learning databases (1998)","libVersion":"0.3.2","langs":""}
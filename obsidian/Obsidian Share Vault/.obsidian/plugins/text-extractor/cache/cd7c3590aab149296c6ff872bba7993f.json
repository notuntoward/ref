{"path":"lit/lit_notes_OLD_PARTIAL/Zaffran22adaptCnfrmPriceFrcst.pdf","text":"Adaptive Conformal Predictions for Time Series Margaux Zaffran 1 2 3 Olivier F´eron 1 4 Yannig Goude 1 5 Julie Josse 2 6 Aymeric Dieuleveut 3 Abstract Uncertainty quantification of predictive models is crucial in decision-making problems. Conformal prediction is a general and theoretically sound answer. However, it requires exchangeable data, excluding time series. While recent works tackled this issue, we argue that Adaptive Conformal In- ference (ACI, Gibbs & Cand`es, 2021), developed for distribution-shift time series, is a good pro- cedure for time series with general dependency. We theoretically analyse the impact of the learn- ing rate on its efficiency in the exchangeable and auto-regressive case. We propose a parameter-free method, AgACI, that adaptively builds upon ACI based on online expert aggregation. We lead ex- tensive fair simulations against competing meth- ods that advocate for ACI’s use in time series. We conduct a real case study: electricity price fore- casting. The proposed aggregation algorithm pro- vides efficient prediction intervals for day-ahead forecasting. All the code and data to reproduce the experiments are made available on GitHub. 1. Introduction The increasing use of renewable intermittent energy leads to more dependent and volatile energy markets. Therefore, an accurate electricity price forecasting is required to stabi- lize energy production planning, gathering loads of research work as evidenced by recent substantial reviews (Weron, 2014; Lago et al., 2018; 2021). Furthermore, probabilistic forecasts are needed to develop risk-based strategies (Gail- lard et al., 2016; Maciejowska et al., 2016; Nowotarski & Weron, 2018; Uniejewski & Weron, 2021). On the one hand, the lack of uncertainty quantification of predictive models is 1Electricit´e de France R&D, Palaiseau, France 2INRIA Sophia- Antipolis, Montpellier, France 3CMAP, ´Ecole Polytechnique, In- stitut Polytechnique de Paris, Palaiseau, France 4FiME, Universit´e Paris-Dauphine, France 5LMO, Universit´e Paris-Saclay, Orsay, France 6IDESP, Montpellier, France. Correspondence to: Margaux Zaffran <margaux.zaffran@inria.fr>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). a major barrier to the adoption of powerful machine learning methods. On the other hand, probabilistic forecasts are only valid asymptotically or upon strong assumptions on the data. Conformal prediction (CP, Vovk et al., 1999; 2005; Pa- padopoulos et al., 2002) is a promising framework to over- come both issues. It is a general procedure to build pre- dictive intervals for any (black box) predictive model, such as neural networks, which are valid (i.e. achieve nominal marginal coverage) in finite sample and without any distri- butional assumptions except that the data are exchangeable. Thereby, CP has received increasing attention lately, favored by the development of split conformal prediction (SCP, Lei et al., 2018, reformulated from inductive CP, Papadopoulos et al., 2002). More formally, suppose we have n training samples (xi, yi) ∈ R d × R, i ∈ J1, nK, realizations of random variables (X1, Y1), . . . , (Xn, Yn), and that we aim at predicting a new observation yn+1 at xn+1. Given a miscoverage rate α ∈ [0, 1] fixed by the user (typically 0.1 or 0.05) the aim is to build a predictive interval Cα such that: P {Yn+1 ∈ Cα (Xn+1)} ≥ 1 − α, (1) with Cα as small as possible, in order to be informative. For the sequel, we call a valid interval an interval satisfying equation (1) and an efficient interval when it is as small as possible (Vovk et al., 2005; Shafer & Vovk, 2008). To achieve this, SCP first splits the n points of the train- ing set into two sets Tr, Cal ⊂ J1, nK, to create a proper training set, Tr, and a calibration set, Cal. On the proper training set a regression model ˆµ (chosen by the user) is fitted, and then used to predict on the calibration set. A con- formity score is applied to assess the conformity between the calibration’s response values and the predicted values, giv- ing SCal = {(si)i∈Cal}. In regression, usually the absolute value of the residuals is used, i.e. si = |ˆµ(xi) − yi|. Finally, a corrected 1 (1 − ˆα)-th quantile of these scores ̂Q1− ˆα(SCal) is computed to define the size of the interval, which, in its simplest form, is centered on the predicted value: Cα (xn+1) = ̂C ˆα(xn+1) := [ˆµ(xn+1) ± ̂Q1− ˆα(SCal)]. These steps are detailed in Appendix A, and illustrated in Figure 9. More details on CP, including beyond regres- 1The correction α → ˆα is needed because of the inflation of quantiles in finite sample (see Lemma 2 in Romano et al. (2019) or Section 2 in Lei et al. (2018)). Adaptive Conformal Predictions for Time Series sion, are given in Vovk et al. (2005); Angelopoulos & Bates (2021). The cornerstone of SCP validity results is the exchange- ability assumption of the data (see Lei et al., 2018, and Appendix A.3). However, this assumption is not met in time series forecasting problems. Despite the lack of theoretical guarantees, several works have applied CP to time series. Dashevskiy & Luo (2008; 2011) apply original (inductive) CP (Papadopoulos et al., 2002) to both simulated (using Auto-Regressive Moving Average (ARMA) processes) and real network traffic data and obtain valid intervals. Wis- niewski et al. (2020); Kath & Ziel (2021) apply SCP respec- tively to financial data (e.g. markets makers’ net positions) and to electricity price forecasting on various markets. In order to account for the temporal aspect, they consider an online version of SCP. In both studies, the validity varied greatly depending on the markets and the underlying regres- sion model, suggesting that further developments of CP and theoretical guarantees for time series are needed. To this end, Chernozhukov et al. (2018) extend the CP theory to ergodic cases in order to include dependent data. Xu & Xie (2021a) improve on that theory and propose a new algorithm, Ensemble Prediction Interval (EnbPI), adapted to time series by adding a sequential aspect. Another case that breaks the exchangeability assumption is distribution shift, which allows for example to deal with cases where the test data is shifted with respect to the train- ing data. Tibshirani et al. (2019) consider covariate shift while Cauchois et al. (2020) tackle a joint distributional shift setting (that is, of (X, Y )). In both studies, a single shift in the distribution is considered, a major limitation for apply- ing these methods to time series. In an adversarial setting, Gibbs & Cand`es (2021) propose Adaptive Conformal Infer- ence (ACI), accounting for an undefined number of shifts on the joint distribution. It is based on refitting the predictive model, as well as updating online the quantile level used by a recursive scheme depending on an hyper-parameter γ (a learning rate). Furthermore, they prove an asymptotic validity result for any data distribution. We argue in this work that the design and guarantees of ACI can be beneficial for dependent data without distribution shifts. Contributions. We propose to analyse ACI (Gibbs & Cand`es, 2021) in the context of time series with general dependency and make the following contributions: • Relying on an asymptotic analysis of ACI’s behaviour for simple time series distribution, we prove that ACI de- teriorates efficiency in an exchangeable case (closed-form expression) while improving it in an AR setting (numerical approximation) with a well-chosen γ (Section 3). • We introduce AgACI, a parameter-free method using on- line expert aggregation, to avoid choosing γ, achieving good performances in terms of validity and efficiency (Section 4). • We compare ACI to EnbPI and online SCP on extensive synthetic experiments and we propose an easy-to-interpret visualisation combining validity and efficiency (Section 5). • We forecast and give predictive intervals on French elec- tricity prices, an area where accurate predictions, but also controlled predictive intervals, are required (Section 6). To allow for better benchmarking of existing and new meth- ods, we provide (re-)implementations in Python of (all) the described methods and a complete pipeline of analysis on GitHub. As explained in Section 4, the code for AgACI is, for now, the only one available only in R. Notations. In the sequel, the following notations are used: Ja, bK := {a, a + 1, . . . , b}; Q refers to the set of rational numbers; C4([0, 1]) refers to the set of 4-times continuously differentiable functions on [0, 1]; not. = defines a notation; #A is the cardinal of the set A. 2. Setting: ACI for time series In this section, we introduce ACI and our framework. We consider T0 observations (x1, y1) , . . . , (xT0, yT0) in R d × R. The aim is to predict the response values and give predictive intervals for T1 subsequent observations xT0+1, . . . , xT0+T1 sequentially: at any prediction step t ∈ JT0 + 1, T0 + T1K, yt−T0 , . . . , yt−1 have been revealed. Thereby, the data ((xt−T0, yt−T0) , . . . , (xt−1, yt−1)) are used for the construction of the predicted interval. Adaptive Conformal Inference. Proposed by Gibbs & Cand`es (2021), ACI is designed to adapt CP to temporal distribution shifts. The idea of ACI is twofold. First, one considers an online procedure with a random split 2, i.e., Trt and Calt are random subsets of the last T0 points. Second, to improve adaptation when the data is highly shifted, an effective miscoverage level αt, updated recursively, is used instead of the target level α. Set α1 = α, and for t ≥ 1 { ̂Cαt (xt) = [ˆµ(xt) ± ̂Q1−αt (SCalt)] αt+1 = αt + γ (α − 1{yt /∈ ̂Cαt (xt)}) , (2) for γ ≥ 03. If ACI does not cover at time t, then αt+1 ≤ αt, and the size of the predictive interval increases; conversely when it covers. Nothing prevents αt ≤ 0 or αt ≥ 1. While the later is rare (as α is small) and produces by convention ̂Cαt(·) = {ˆµ(·)} (i.e. ̂Q1−αt = 0) , the former can happen frequently for some γ, giving ̂Cαt ≡ R ( ̂Q1−αt = +∞). How to deal with infinite intervals. A specificity of ACI’s algorithm is thus to often produce infinite intervals. Defining 2Figure 5(a) with training and calibration part shuffled randomly. 3ACI actually wraps around any CP procedure, here the definition is given using mean regression SCP. Adaptive Conformal Predictions for Time Series 0 200 400 t −1 0 1εt 500 550 600 650 700 750 800 850 900 950 1000 t Figure 1. ACI on one simulated path εt, t = 1, . . . , 1000, from an AR(1) process (in black). The first 500 values form the initial calibration set (left subplot), and predicted interval bounds are computed on the last 500 points (right) for γ = 0, γ = 0.01 and γ = 0.05. the average length of an interval is then impossible. In order to assess the efficiency in the following, we consider two solutions: (i) imputing the length of infinite intervals by (twice) the overall maximum of the residuals, or Q(1) if the residual’s quantile function is known and bounded 4; (ii) focusing on the median instead. ACI on time series with general dependency. As high- lighted by Wisniewski et al. (2020); Kath & Ziel (2021), the first step to adapt a method for dependent time series is to work online which is the case for ACI. Moreover, the update of the quantile level according to the previous error implies that ACI could cope with a fitted model that has not correctly caught the temporal evolution, such as a trend, a seasonality pattern or a dependence on the past. Therefore, ACI is a perfect candidate for CP for time series with gen- eral dependency. To account for the temporal structure, we change the random split to a sequential split. 5 To gain understanding on ACI in the context of dependent temporal data, we analyse a situation where a fitted regres- sion model ˆµ produces AR(1) residuals, thus yt − ˆµ(xt) = εt, where εt is an AR(1) process: εt+1 = 0.99εt + ξt+1, with ξt ∼ N (0, 0.01). We plot this toy example in Figure 1, for T0 = T1 = 500. Three versions of ACI are compared: γ = 0, the quantile level is not updated but the calibration set Calt is; γ = 0.01 and γ = 0.05. To obtain an insightful visualisation6, we represent the interval [± ̂Q1−αt(SCalt )] instead of ̂Cαt(xt). When no intervals are displayed, ACI is predicting R. Here and in the sequel, we use α = 0.1. In this toy example, the coverage rate among many observa- tions is valid for γ ∈ {0.01, 0.05} (90% and 92% of points included) but not for γ = 0 (72.6%). Moreover, Figure 1 shows that the type of errors depends on γ. For γ = 0, ACI excludes consecutive observations (e.g. for t ∈ [810, 860], 4This happens in practice when the response and prediction are bounded, e.g., thanks to physical/real constraints as for the spot prices presented in Section 6.1, that are bounded by market rules. 5As in Figure 5(a). This is also consistent with OSSCP (Sec. 5.3). 6We suggest focusing the visualisation on the scores to analyse the behaviour of CP methods, as they are at the core of the validity proof. A detailed discussion on this is given in App. A.5 zoomed-in plot). For γ ∈ {0.01, 0.05}, ACI manages to adapt to these observations, and the higher the γ, the less the adaptation is delayed. Furthermore, when the residuals are small and far from both interval bounds, ACI quickly reduces the interval’s length and produces more efficient in- tervals. Consequently, ACI may also not cover on points for which the residuals have a relatively small values compared to the calibration’s values (e.g. for t ∈ [760, 785]). 3. Impact of γ on ACI efficiency The choice of the parameter γ strongly impacts the be- haviour of ACI: while the method always satisfies the asymp- totic validity property, i.e. 1 T ∑T t=1 1{yt /∈ ̂Cαt (xt)} a.s. −→ T →∞ α (Proposition 4.1 in Gibbs & Cand`es, 2021), this prop- erty does not give any insight on the length of resulting intervals. Besides, this guarantee directly stems from the fact that 1 T ∑T t=1 1{yt /∈ ̂Cαt (xt)} − α ≤ 2/(γT ). This tends to suggest the use of larger γ values, that unfortu- nately generate frequent infinite intervals. Here, we thus analyse the impact of γ on ACI’s efficiency in simple yet in- sightful cases: in Section 3.1, focusing on the exchangeable case, then in Section 3.2, with a simple AR process on the residuals. Approach. Our focus is on the impact of the key parame- ter γ. Analysing simple theoretical distributions allows to build intuition on the behaviour of the algorithm for more complex data structure. In order to derive theoretical results, we thus make supplementary modelling assumptions on the residuals, and do not consider the impact of the calibration set: we introduce Q the quantile function of the scores and assume, for all ˆα and t, ̂Q1− ˆα(SCalt) = Q(1 − ˆα). This corresponds to considering the limit as #Cal → ∞. This allows to focus on the impact of recursive updates in (2) and describe their behaviour by relying on Markov Chain theory. 3.1. Exchangeable case ACI is usually applied in an adversarial context. If the scores are actually exchangeable, ACI’s validity would not Adaptive Conformal Predictions for Time Series improve upon SCP (known to be quasi-exactly valid), thus assessing ACI’s impact on efficiency is necessary. Define L(αt) = 2Q(1 − αt) the length of the interval predicted by the adaptive algorithm at time t, and L0 = 2Q(1 − α) the length of the interval predicted by the non-adaptive algorithm (or equivalently, γ = 0). Theorem 3.1. Assume that: (i) α ∈ Q; (ii) the scores are ex- changeable with quantile function Q; (iii) the quantile func- tion is perfectly estimated at each time (as defined above); (iv) the quantile function Q is bounded and C4([0, 1]). Then, for all γ > 0, (αt)t>0 forms a Markov Chain, that admits a stationary distribution πγ, and 1 T T∑ t=1 L(αt) a.s. −→ T →+∞ Eπγ [L] not. = E ˜α∼πγ [L(˜α)]. Moreover, as γ → 0, Eπγ [L] = L0 + Q ′′(1 − α) γ 2 α(1 − α) + O(γ3/2). Interpretation of assumptions. Assumption (i) is weak since a practitioner will always select α ∈ Q while assump- tion (ii) describes the classical exchangeable setting. The main assumptions are (iii) and (iv): (iii) can be interpreted as considering an infinite calibration set while (iv) is neces- sary7 in order to define Eπγ [L]: here, we extend Q(1 − ˆα) by Q(1) for ˆα < 0. When ˆQ ≡ ˆQt is the empirical quantile function on a calibration set Cal, the convergence in Theo- rem 3.1 holds conditionally to Cal. Finally, the regularity assumption on Q is purely technical. Interpretation of the result. For standard distributions, Q ′′(1 − α) > 0,8 and Theorem 3.1 implies that ACI on exchangeable scores degrades the efficiency linearly with γ compared to CP. This is an important takeaway from the analysis, that underlines that such adaptive algorithms may actually hinder the performance if the data does not have any temporal dependency, and a small γ is preferable. For example, if the residuals are standard gaussians, for α = 0.01, setting γ = 0.03 (resp. γ = 0.05) will increase the length by 1.59% (resp. by 3.38%) with respect to γ = 0. 3.2. AR(1) case We now consider the case of (highly) correlated residuals, which happens in many practical time series applications. Definition 3.2 (AR(1) clipped). εt+1 = φεt + ξt+1 with (ξt)t i.i.d. random variables admitting a continuous density 7∀γ> 0, Pπγ ( ˜α ≤ 0) > 0: we need |Q(1)| < ∞ to define Eπγ [L]. 8as Q′(x) = 1 f (Q(x)) with f the scores’ probability density func- tion, Q ′(x) increases locally around x if and only if f decreases locally around Q(x) (Q is increasing). Thus, Q ′′(x) > 0 if and only if f decreases locally around Q(x). Thereby, for x = 1 − α high (usually the case), Q′′(1 − α) > 0 for standard distributions. with respect to Lebesgue measure, of support S clipped at a large value R, and [−R, R] ⊂ S Theorem 3.3. Assume that: (i) α ∈ Q; (ii) the residuals follow an AR(1) process clipped at R of parameter φ (Def- inition 3.2); (iii) the quantile function Q of the stationary distribution of (εt)t is known. Then (αt, εt−1) is a homoge- neous Markov Chain in R 2 that admits a unique stationary distribution πγ,φ. Moreover, 1 T T∑ t=1 L(αt) a.s. −→ T →+∞ Eπγ,φ[L]. We numerically estimate γ∗ φ = argminγ Eπγ,φ[L] in Fig- ure 2. To do so, AR(1) processes of length T = 106 are simulated for various φ and asymptotic variance 1. ACI is applied on each of them, with 100 different γ ∈ [0, 0.2]. Figure 2 (left) represents the average length depending on γ for each φ, and (right) the values of γ minimizing this average length for each φ (for 25 repetitions of the experi- ment). The average length is computed after imputing all the infinite intervals’ length by the maximum of the process, as explained in Section 2. A similar study using instead the median length is provided after the proofs in Appendix B. Interpretation. We make the following observations: 1. For high φ, ACI indeed improves for a strictly posi- tive γ upon γ = 0. This proves that ACI can be used to produce smaller intervals for time series CP. The function γ ↦→ Eπγ,φ[L] decreases until γ∗ φ, then increases again, as expected because very large γ cause the algorithm to be less stable and produce numerous infinite intervals. 2. In Figure 2 (left), zoomed-in plot, the black line repre- sents asymptotic result of Theorem 3.1. We retrieve here that the expected length is minimal for γ = 0 and grows linearly with γ around 0. This behaviour is very similar for φ = 0.6. 3. For any γ, the function φ ↦→ Eπγ,φ [L] is decreasing (Fig- ure 2, left). Indeed, stronger correlation between residuals (i.e., a higher φ), allows to build smaller intervals. This confirms that ACI’s impact strengthens with the strength of 0.00 0.05 0.10 0.15 0.20 γ 2 3 4Averagelength ϕ =0 ϕ =0.6 ϕ =0.85 ϕ =0.95 ϕ =0.98 ϕ =0.99 ϕ =0.997 ϕ =0.999 0.01 0.03 Thm. 3.1 0.0 0.6 0.85 0.95 0.980.99 0.997 0.999 ϕ 0.00 0.02 0.04 0.06 0.08γ∗ Figure 2. Left: evolution of the mean length depending on γ for various φ. Right: γ∗ minimizing the average length for each φ (each cross has a size proportional to the number of runs for which γ∗ was the minimizer). Adaptive Conformal Predictions for Time Series the temporal dependence. 4. Surprisingly, the function φ ↦→ γ∗ φ, that corresponds to the optimal learning rate for a given signal, is non- monotonic, (Figure 2, right). As γ = 0 is optimal for φ = 0, the function first increases. However, the optimal learning rate then diminishes as φ increases. This sheds light on the complex intrinsic tradeoffs of the method: for small values of φ, using γ > 0 simply degrades the efficiency; for “moderate” values of φ using a larger γ is necessary to quickly benefit from the short-term dependency between residuals; finally, for larger values of φ, the process exhibits a longer memory, thus it is crucial to find a smaller learning rate that produces more stable intervals, even if it means that the algorithm won’t adapt as quickly. What if Q ̸= ˆQ? While our analysis provides a first step by comparing ACI to CP in the ideal case where the quantile distribution is known (for both methods), the impact of the finite-Cal is of interest. Indeed, if Cal is small, ACI can help to attain coverage conditionally to a given Cal even in the i.i.d. case. Yet intuitively, marginally, the randomness induced by ACI in the i.i.d. case would negatively impact efficiency w.r.t. γ = 0, even in the finite-Cal case. Finite sample trade-offs and general analysis of the case Q ̸= ˆQ is an important open direction. Overall, these results highlight the importance of the choice of γ, as not choosing γ∗ can lead to significantly larger inter- vals. In addition, they provide insights on the corresponding dynamics. Yet the choice of γ in more complex practical settings remains difficult: this calls for adaptive strategies. 4. Adaptive strategies based on ACI To prevent the critical choice of γ an ideal solution is an adaptive strategy with a time dependent γ. We propose two strategies based on running ACI for K ∈ N values {(γk)k≤K} of γ, chosen by the user. Overall, this does not increase the computational cost because Trt and Calt are shared between all ACI; thus the only additional cost is the computation of the K different quantiles. For any xt, denote ̂Cαt,k (xt) the interval at time t built by ACI using γk. Naive strategy. A simple strategy is to use at each step the γ that achieved in the past the best efficiency while ensuring validity. For stability purposes, consider a warm- up period Tw ≤ T1 − 1. For each t ≥ T0 + Tw, we select k∗ t+1 ∈ argmink∈At { t −1 ∑t s=1 length( ̂Cαs,k (xs))} with At = {k ∈ J1, KK | t−1 ∑t s=1 1ys∈ ̂Cαs,k (xs) ≥ 1 − α} or k∗ t+1 ∈ argmink∈J1,KK{|1−α−t−1 ∑t s=1 1ys∈ ̂Cαs,k (xs)|} if At = ∅. For the first Tw steps, an arbitrary strategy is applied (in simulations, γ = 0 for t ≤ Tw = 50). Online Expert Aggregation on ACI (AgACI). Instead of picking one γ in the grid, we introduce an adaptive aggrega- tion of experts (Cesa-Bianchi & Lugosi, 2006), with expert k being ACI with parameter γk. This strategy is detailed in Algorithm 1, and schematised in Figure 3. At each step t, it performs two independent aggregations of the K-ACI inter- vals ̂Cαt,k (·) not. = [ˆb(ℓ) t,k(·), ˆb(u) t,k (·)], one for each bound, and outputs ̃Ct(·) not. = [˜b(ℓ) t (·), ˜b (u) t (·)]. Aggregation computes an optimal weighted mean of the experts (Line 11), where the weights ω(ℓ) t,k, ω(u) t,k assigned to expert k depend on all ex- perts performances (suffered losses) at time steps 1, · · · , t (Line 9). We use the pinball loss ρβ, as it is frequent in quantile regression, where the pinball parameter β is chosen to α/2 (resp. 1 − α/2) for the lower (resp. upper) bound. These losses are plugged in the aggregation rule Φ. Finally, the aggregation rule can include the computation of the gradients of the loss (gradient trick, see Cesa-Bianchi & Lu- gosi, 2006, for more details). As aggregation rules require bounded experts, a thresholding step is added (Line 5). Note that the pinball loss helps to avoid large intervals (e.g. it strongly penalizes infinite or very large intervals). We chose Φ to be the Bernstein Online Aggregation (BOA, Wintenberger, 2017, see Appendix C.1 for a brief de- scription), that was successfully applied for financial data (Berrisch & Ziel, 2021; Remlinger et al., 2021). We rely on R package OPERA (Gaillard & Goude, 2021), which al- lows the user to easily select among many other aggregation rules (EWA (Vovk, 1990), ML-Poly (Gaillard et al., 2014) or FTRL (Shalev-Shwartz & Singer, 2007; Hazan, 2019), etc.) that give similar results in our experiments. We use the gradient trick in the simulations. In the sequel, AgACI refers to AgACI using BOA and gradient trick. Algorithm 1 Online Expert Aggregation on ACI (AgACI) Input: Miscoverage rate α, grid {γk, k ∈ J1, KK}, aggre- gation rule Φ, threshold values M (ℓ), M (u). 1: Let β(ℓ) = α/2 and β(u) = 1 − α/2 2: for t ∈ JT0 + 1, T0 + T1K do 3: for k ∈ J1, KK do 4: Compute ˆb(·) t,k(xt) using ACI with γk. 5: if ˆb(·) t,k(xt) /∈ R then set ˆb(·) t,k(xt) = M (·) 6: end for 7: Set ̃Ct(xt) = [˜b (ℓ) t (xt), ˜b(u) t (xt)] 8: for k ∈ J1, KK do 9: ω(·) t,k = Φ ({ ρβ(·)(ys, ˆb(·) s,l(xs)), s ∈ JT0 + 1, tK, l ∈ J1, KK}) 10: end for 11: Define ˜b(·) t+1(x) = ∑K k=1 ω(·) t,kˆb(·) t+1,k(x) ∑K l=1 ω(·) t,l for any x ∈ R d 12: end for Adaptive Conformal Predictions for Time Series Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ExpertsPrevious upper bounds Weights Weighted mean Forecasts at t + 1 Figure 3. Scheme of AgACI algorithm, upper bound u only, for a forecast at time t + 1. A similar procedure is performed indepen- dently for the lower bound ℓ in parallel. 5. Numerical evaluation on synthetic data sets In this section we conduct synthetic experiments on a wide range of data sets presented in Section 5.1. The goal of this section is twofold. First, in Section 5.2, comparing our proposed adaptive strategies to ACI with a wide range of γ values. Second, in Section 5.4, comparing performances of AgACI and ACI to that of competitors – namely EnbPI and online sequential SCP, described in Section 5.3. 5.1. Data generation process and settings We generate data according to: Yt =10 sin (πXt,1Xt,2) + 20 (Xt,3 − 0.5) 2 + 10Xt,4 + 5Xt,5 + 0Xt,6 + εt, (3) where the Xt are multivariate uniformly distributed on [0, 1], and Xt,6 represents an uninformative variable. The noise εt is generated from an ARMA(1,1) process of parameters φ and θ, i.e. εt+1 = φεt + ξt+1 + θξt, with ξt a white noise called the innovation (see Appendix C.2 for details). When the noise is i.i.d., one retrieves the simulations from Friedman et al. (1983). The temporal dependence is present only in the noise in order to control its strength and its impact on the algorithms’ performance. Given the non-linear structure of the data generating process, we use a random forest (RF) as predictive model, with the same hyper-parameters through all the experiments (speci- fied in Appendix C.3). To assess the impact of the temporal structure, we vary φ and θ in {0.1, 0.8, 0.9, 0.95, 0.99}. To focus on the impact of the dependence structure, the value of the innovation’s variance is selected so that the asymptotic variance of εt is independent of φ, θ: here we choose limt→∞ Var(εt) = 10. For each set of parameters, we generate n = 500 samples (εt)t∈J1,T0+T1K with T0 = 200. In the sequel we display the results on an ARMA(1,1) which are representative of all the results obtained. For the sake of simplicity, we consider φ = θ. Complementary results (i) for an asymptotic variance of 1 (corresponding to a higher signal to noise ratio), (ii) for AR(1) and MA(1) models are available in Appendix D. Joint visualisation of validity & efficiency. In order to simultaneously assess validity and efficiency, in Figures 4, 6 and 8, we represent on the same graph the empirical coverage and average median length (used for efficiency as imputing the infinite bounds by the maximum of the whole sequence is not always feasible in practice). In those three figures, the vertical dotted line represents the target miscoverage rate, α = 0.1. Consequently, a method is valid when it lies at the right of this line, and the lower the better. 5.2. Impact of γ, performance of AgACI Figure 4 illustrates the behaviour of ACI (with multiple values of γ), the naive strategy (empty triangles) and AgACI (black stars) for increasing (from left to right) values of φ, θ, with T1 = 200. In particular, the top row shows the joint validity & efficiency and, for this figure only, we add in the bottom row the same graph using the average length after imputation (see details in Appendix D) to assess efficiency in another way. When γ is small, one observes an undercoverage, which increases when the temporal dependency of ε increases. Increasing γ enables ACI to increase the interval’s size faster when we do not cover, and thus to improve validity, which is achieved for high values of γ; however this also increases the frequency of uninformative (infinite) intervals, as deduced from the bottom row of Figure 4, where the average length after imputation grows with γ. Remark that these results do not contradict the validity result recalled at the beginning of Section 3, which is only asymptotic while we predict on 200 points. For φ, θ small, we observe that similarly to Theorem 3.1, the efficiency does not improve with γ. For moderate values of φ, θ ∈ {0.8, 0.9, 0.95}, we observe that the average median length is decreasing with γ for γ ≥ 0.01. This effect is observable on average but not present in all the 500 experiments. One possible explanation is that the shrinking effect of ACI on the predicted interval enables to significantly reduce the predicted interval when γ is large, and this effect is, on average, more important than the number of large intervals. Moreover, the naive strategy is clearly not valid: indeed it results in greedily choosing a γ that achieved good results in the past, and is consequently slightly more likely to fail Adaptive Conformal Predictions for Time Series 0.86 0.88 0.90 12 13 14Averagemedianlength ϕ = θ =0.1 0.86 0.88 0.90 ϕ = θ =0.8 0.86 0.88 0.90 ϕ = θ =0.9 0.86 0.88 0.90 ϕ = θ =0.95 0.86 0.88 0.90 ϕ = θ =0.99 0.86 0.88 0.90 Coverage 12 13 14 15 16Averagelength,afterimputation 0.86 0.88 0.90 Coverage 0.86 0.88 0.90 Coverage 0.86 0.88 0.90 Coverage 0.86 0.88 0.90 Coverage 0.0000 0.0001 0.0004 0.0007 0.0010 0.0040 0.0070 0.0100 0.0400 0.0700 γ AgACI Naive method Figure 4. ACI performance with various θ, φ and γ on data simulated according to equation (3) with a Gaussian ARMA(1,1) noise of asymptotic variance 10 (see Appendix C.2). Top row: average median length w.r.t. the coverage. Bottom row: average length after imputation w.r.t. the coverage. Stars correspond to AgACI, and empty triangles to the naive choice. to cover in future steps. Thereby, we do not consider it anymore. Finally, AgACI achieves valid coverage without increasing the median length with respect to each expert, and even improves the coverage. Overall, it appears to be a good candidate as a parameter-free method. 5.3. Description of baseline methods We consider as baseline online sequential split conformal prediction (OSSCP), a generalisation of SCP9. The other competitor is EnbPI (Xu & Xie, 2021a), specifically de- signed for time series. Pseudo-codes and details are given in Appendix C.4. Offline SCP (for which Trt ≡ Tr0 and Calt ≡ Cal0) is not considered as a competitor because it is unfair to compare an offline algorithm to one that uses more recent data points. This corresponds to comparing a prediction at horizon Tlarge to one at horizon Tsmall. This is a limitation of the comparison in Xu & Xie (2021a). OSSCP. We consider an online version of SCP by refitting the underlying regression model and recalibrating using the newest points. Moreover, to appropriately account for the temporal structure of the data, we use a sequential split as in Wisniewski et al. (2020): at any t, the time indices in Trt are smaller than those of Calt. Not randomizing aims at excluding future observations from Trt, which may lead to an under-estimation of the errors on Calt, thus eventually to smaller intervals with under-coverage. We compare both splitting strategies on simulations in Appendix D.4. OSSCP procedure is schematised in Figure 5(a). Original EnbPI. EnbPI, Ensemble Prediction Interval (Xu & Xie, 2021a), works by updating the list of conformity scores with the most recent ones so that the intervals adapt 9Recall here that inductive CP and SCP are equivalent methods. to latest performances, without refitting the underlying re- gression model. Thereby, the predicted intervals can adapt to seasonality and trend. In EnbPI, B bootstrap samples of the training set are generated and the regression algorithm is fitted on each bootstrap sample producing B predictors. Finally, the predictors are aggregated in two ways: first, for each training point of index t ≤ T0, EnbPI aggregates only the subset of predictors trained on bootstrap sample exclud- ing (xt, yt). This way, EnbPI constructs a set of hold-out calibration scores. Second, for test points of index t > T0 EnbPI aggregates all the B predictors. A sketch of EnbPI is presented in Figure 5(b). Note that in Xu & Xie (2021a) they use a classical bootstrap procedure, not dedicated to time series. They show empirically that it leads to valid coverage on real world time series, such as hourly wind power production and solar irradiation, while offline SCP fails to attain valid coverage. EnbPI V2. Xu & Xie (2021a) used the mean aggregation during the training phase and the (1 − α)-th quantile of the predictors for the prediction. We consider using the mean aggregation all along the procedure as mixing both aggrega- tions may hurt the performance of the algorithm (as shown in the following simulations). Note that simultaneously to (a) OSSCP (b) EnbPI Test pointUnused data Proper training set Calibration set Figure 5. Scheme of the two baselines: OSSCP and EnbPI. In (a), Tr and Cal have equal size, but it can be changed. Adaptive Conformal Predictions for Time Series our work, authors released an updated version on ArXiv (Xu & Xie, 2021b), incorporating a similar change. 5.4. Experimental results: impact of φ, θ Figure 6 presents the results for data generated as in Sec- tion 5.1, for various (φ, θ). Each sample contains 300 obser- vations, with T0 = 200 and T1 = 100. We compare AgACI (with K = 30 experts), ACI (with γ ∈ {0.01, 0.05}), OS- SCP, EnbPI and EnbPI V2 (with mean aggregation). To assess the impact and interest of an online procedure, we also add offline SCP. Finally, to ensure the robustness of our conclusions each experiment is repeated n = 500 times, and Figure 6 includes the standard errors (given by ˆσn√n , where ˆσn is the empirical standard deviation). Each color is associated to a set (φ, θ), each marker to an algorithm. To improve readability, we often link markers of the same method. There are thus two ways of analysing Fig- ure 6: for a given method, the lines highlight the evolution of its performance with (φ, θ); for a given data distribution, the set of markers of its color allows to compare the methods. Figure 6, and results on AR(1) in Appendix D.2.1, highlight that in an AR(1) or ARMA(1,1) process: • Refitting the method (OSSCP vs Offline SCP) brings a significant improvement, that increases with higher de- pendence (higher values for φ and θ). • All methods produce smaller intervals for φ = θ = 0.99. • EnbPI loses coverage while producing shorter intervals when the dependence increases. The performance of EnbPI depends significantly on the type and strength of dependence. • EnbPI V2 is closer to the target coverage than EnbPI. • OSSCP loses validity & coverage as φ and θ increase. • While ACI with γ = 0.01 also struggles for high values of φ and θ such as 0.99, we observe that it still attains valid coverage with a well chosen γ. Most importantly, ACI performances are robust to the increase of the dependence strength: except for the φ = θ = 0.99, its markers are really close to each other. • AgACI always nearly attains validity (coverage is over 89.8% for all φ), and achieves the best efficiency perfor- mance among valid methods. Note that ACI’s valid coverage with some γ comes at the price of predicting more infinite intervals. A more detailed analysis on this phenomenon is conducted in Appendix D.3. This can also be observed in graphs obtained with the aver- age length after imputation, which are similar to Figure 6 and Appendix D.2.1. In these graphs, the validity remains unchanged as expected, but the efficiency is more degraded for ACI with γ = 0.05 and for AgACI, since they produce more often uninformative intervals, as observed in Figure 4. Summary. We highlight the following takeaways:0.830.840.850.860.870.880.890.900.91Coverage1011121314Average median length0.8950.9000.905OSSCP(adaptedfromLeietal.,2018)O✏ineSSCP(ad.fromLeietal.,2018)EnbPI(Xu&Xie,2021)EnbPIV2ACI(Gibbs&Cand`es,2021),\u0000=0.01ACI(Gibbs&Cand`es,2021),\u0000=0.05AgACI'=✓=0.1'=✓=0.8'=✓=0.9'=✓=0.95'=✓=0.99 Figure 6. Performance of various CP methods on data simulated according to equation (3) with a Gaussian ARMA(1,1) noise of asymptotic variance 10 (see Appendix C.2). Results aggregated from 500 independent runs. Empirical standard errors displayed. 1. The temporal dependence impacts the validity. 2. Online is significantly better than offline. 3. OSSCP. Achieves valid coverage for φ and θ smaller than 0.9, but is not robust to the increasing dependence. 4. EnbPI. Its validity strongly depends on the data distri- bution (it is valid on a MA(1) noise, not in AR(1) and ARMA(1,1) noise). When the method is valid, it pro- duces the smallest intervals. EnbPI V2 method should be preferred. 5. ACI. Achieves valid coverage for every simulation set- tings with a well chosen γ, or for dependence such that φ < 0.95. It is robust to the strength of the dependence. 6. AgACI. Achieves valid coverage for every simulation setting, with good efficiency. 6. Forecasting French electricity spot prices In this last section, the task of forecasting French electricity spot prices with predictive intervals is considered in order to assess the methods on a real time series, and most impor- tantly to show the relevance of ACI and AgACI in practice for time series without distribution shifts. 6.1. Presentation of the price data The data set contains the French electricity spot prices, set by an auction market, from 2016 to 2019. Each day D before 12 AM, any producer (resp. supplier) submit their orders for the 24 hours of day D + 1. An order consists of an electricity volume in MWh offered for sale (resp. required to be purchased) and a price in e/MWh, at which they accept to sell (resp. buy) this volume. At 12 AM, the algorithm “Euphemia” (EUPHEMIA) fixes the 24 hourly prices of day D + 1 according to these offers and additional constraints. Thereby, it is an hourly data set, containing Adaptive Conformal Predictions for Time Series (3 × 365 + 366) × 24 = 35064 observations. Our aim is to predict at day D (before 12 AM) the 24 prices of day D + 1. Given the prices’ construction, we consider the following explanatory variables: day-ahead forecast consumption, day- of-the-week, 24 prices of the day D − 1 and 24 prices of the day D − 7. An extract of the considered data set is presented in Appendix E.1. 2016 2017 2018 2019 2020 Date 0 200 400 600 800Spotprice(€/MWh) 21/01 23/01 25/01 50 75 100 125 Observed price Predicted price Predicted interval Figure 7. French electricity spot prices, from 2016 to 2019. Pre- dicted intervals on the 25th of January 2019, using AgACI. These prices exhibits medium to high peaks, as illustrated in Figure 7 where the French prices had reached 800 e/MWh in fall 2016, compared to an average price of approximately 40 e/MWh in 2019. These extreme events are mainly due to the non-storability of electricity and the inelasticity of the demand: when the demand is high compared to the avail- able production, production units with expensive production costs must be called, leading to a huge market price. 6.2. Price prediction with predictive intervals in 2019 Since the 24 hours have very distinct patterns, we fit one model per hour, using again RF. We predict for the year 2019, using a sliding window of 3 years, as described in Figure 5(a), using one year and 6 months as proper training set and the most recent year and a half for calibration. The results are represented in Figure 8. OSSCP over-covers but to a lesser extent than the offline version. This can be explained by a low presence of peaks during the test period. Indeed, by updating the whole pro- 0.9 0.91 0.92 0.93 Coverage 22 24 26MedianlengthOSSCP Oﬄine SSCP EnbPI V2 ACI γ = 0 ACI γ = 0.01 ACI γ = 0.05 AgACI Figure 8. Performance of different CP methods on hourly spot elec- tricity prices in France, trained from 2016 to 2018 and forecasted on 2019. Median length with respect to empirical coverage. cedure, the high peaks are “forgotten” which leads to small intervals while it is not the case for the offline version which leads to too large intervals. Thereby, online versions can help to improve efficiency, in addition to validity. EnbPI at- tains a valid coverage by over-covering. The under-coverage observed in the simulation study is not systematic, as in Xu & Xie (2021a). ACI gives the smallest intervals with a correct coverage, for γ = 0.01 and γ = 0.05. The up- date of the quantile level enables to shrink the intervals. While the simulation in Section 5.4 study outlines that ACI improves validity, this application illustrates that it can pro- vide efficient interval. AgACI is more efficient than γ = 0 while maintaining validity. Yet it slightly over-covers, and is slightly less efficient than ACI with a well chosen γ. An illustration of the predicted intervals is given in the inset graphic of Figure 7, for AgACI, to highlight the practical relevance of such an approach on the spot prices. However, as expected, these intervals only enjoy a marginally valid frequency. They do not have conditional guarantees. Especially, in this forecasting task, the predicted intervals cover the true prices around 88% of the time on week ends and Mondays, and 93% of the time on Tuesdays to Fridays (see Appendix E.2). Further developments are needed to improve this unbalanced coverage. 7. Conclusion This article shows why and how ACI can be used for in- terval prediction in the context of time series with general dependencies. We prove that ACI deteriorates efficiency compared to CP in the exchangeable case and analyse the dependency on γ in the AR case with the support of numer- ical simulations. We propose an algorithm, AgACI, based on online expert aggregation, that wraps around ACI to avoid the choice of γ. We conduct extensive experiments on synthetic time series for various strengths and structures of time dependence, demonstrating ACI’s robustness and bet- ter performances than baselines, with well chosen γ or using AgACI. Finally we perform a detailed application study on the high-stakes electricity price forecasting problem in the energy transition era. Future work includes theoretical study of the proposed aggregation algorithm, including whether it preserves the asymptotic validity observed experimentally or to quantify its efficiency with respect to the performances of each expert. Acknowledgements We thank Maximilien Germain, Pablo Jim´enez and Con- stantin Philippenko for interesting discussions. We thank the anonymous reviewers for their useful comments on an ear- lier draft. The work of A. Dieuleveut is partially supported by ANR-19-CHIA-0002-01/chaire SCAI and Hi! Paris. Adaptive Conformal Predictions for Time Series References Angelopoulos, A. N. and Bates, S. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021. Berrisch, J. and Ziel, F. CRPS learning. Journal of Econo- metrics, 2021. Cai, Y. and Davies, N. A simple bootstrap method for time series. Communications in Statistics-Simulation and Computation, 41(5):621–631, 2012. Cauchois, M., Gupta, S., Ali, A., and Duchi, J. C. Robust Validation: Confident Predictions Even When Distribu- tions Shift. arXiv preprint arXiv:2008.04267, 2020. Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games. Cambridge University Press, 2006. Chernozhukov, V., W¨uthrich, K., and Yinchu, Z. Exact and Robust Conformal Inference Methods for Predictive Machine Learning with Dependent Data. In Conference On Learning Theory, pp. 732–749. PMLR, 2018. Dashevskiy, M. and Luo, Z. Network traffic demand pre- diction with confidence. In IEEE Global Telecommunica- tions Conference. IEEE, 2008. Dashevskiy, M. and Luo, Z. Time series prediction with performance guarantee. IET communications, 5(8):1044– 1051, 2011. EUPHEMIA. Euphemia public description, sin- gle price coupling algorithm, April 2019. URL https://www.nemo-committee.eu/assets/ files/190410_Euphemia%20Public% 20Description%20version%20NEMO% 20Committee.pdf. Friedman, J. H., Grosse, E., and Stuetzle, W. Multidimen- sional additive spline approximation. SIAM J. Sci. Stat. Comput., 1983. Gaillard, P. and Goude, Y. OPERA, 2021. URL https: //cran.r-project.org/package=opera. R package version 1.2.0. Gaillard, P., Stoltz, G., and Van Erven, T. A second-order bound with excess losses. In Conference on Learning Theory, pp. 176–196. PMLR, 2014. Gaillard, P., Goude, Y., and Nedellec, R. Additive models and robust aggregation for GEFCom2014 probabilistic electric load and electricity price forecasting. Interna- tional Journal of Forecasting, 32(3):1038–1050, 2016. Gibbs, I. and Cand`es, E. Adaptive conformal inference un- der distribution shift. In Advances in Neural Information Processing Systems, 2021. Goehry, B. Random forests for time-dependent processes. ESAIM: Probability and Statistics, 24:801–826, 2020. Goehry, B., Yan, H., Goude, Y., Massart, P., and Poggi, J.- M. Random forests for time series. HAL hal-03129751, 2021. H¨ardle, W., Horowitz, J., and Kreiss, J.-P. Bootstrap meth- ods for time series. International Statistical Review, 71 (2):435–459, 2003. Hazan, E. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019. Kath, C. and Ziel, F. Conformal prediction interval estima- tion and applications to day-ahead and intraday power markets. International Journal of Forecasting, 37(2): 777–799, 2021. Kreiss, J.-P. and Paparoditis, E. The hybrid wild bootstrap for time series. Journal of the American Statistical Asso- ciation, 107(499):1073–1084, 2012. Lago, J., De Ridder, F., and De Schutter, B. Forecasting spot electricity prices: Deep learning approaches and empirical comparison of traditional algorithms. Applied Energy, 221:386–405, 2018. Lago, J., Marcjasz, G., De Schutter, B., and Weron, R. Forecasting day-ahead electricity prices: A review of state-of-the-art algorithms, best practices and an open- access benchmark. Applied Energy, 293:116983, 2021. Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L. Distribution-Free Predictive Inference for Regression. Journal of the American Statistical Asso- ciation, 113(523):1094–1111, 2018. Maciejowska, K., Nowotarski, J., and Weron, R. Proba- bilistic forecasting of electricity spot prices using Factor Quantile Regression Averaging. International Journal of Forecasting, 32(3):957–965, 2016. Meyn, S. P. and Tweedie, R. L. Markov chains and stochas- tic stability. Springer Science & Business Media, 2012. Nowotarski, J. and Weron, R. Recent advances in electricity price forecasting: A review of probabilistic forecasting. Renewable and Sustainable Energy Reviews, 81:1548– 1568, 2018. Papadopoulos, H., Proedrou, K., Vovk, V., and Gammerman, A. Inductive Confidence Machines for Regression. In Machine Learning: ECML 2002, pp. 345–356. Springer, 2002. Remlinger, C., Bri`ere, M., Alasseur, C., and Mikael, J. Ex- pert aggregation for financial forecasting. arXiv preprint arXiv:2111.15365, 2021. Adaptive Conformal Predictions for Time Series Romano, Y., Patterson, E., and Cand`es, E. Conformalized Quantile Regression. Advances in Neural Information Processing Systems, 32, 2019. Saha, A., Basu, S., and Datta, A. Random forests for spa- tially dependent data. Journal of the American Statistical Association, 0(0):1–19, 2021. Shafer, G. and Vovk, V. A Tutorial on Conformal Prediction. JMLR, 9:51, 2008. Shalev-Shwartz, S. and Singer, Y. A primal-dual perspective of online learning algorithms. Machine Learning, 69(2-3): 115–142, 2007. Tibshirani, R. J., Barber, R. F., Cand`es, E., and Ramdas, A. Conformal Prediction Under Covariate Shift. Advances in Neural Information Processing Systems, 32:11, 2019. Uniejewski, B. and Weron, R. Regularized quantile regres- sion averaging for probabilistic electricity price forecast- ing. Energy Economics, pp. 105121, 2021. Vovk, V., Gammerman, A., and Saunders, C. Machine- Learning Applications of Algorithmic Randomness. In Proceedings of the Sixteenth International Conference on Machine Learning, pp. 444–453. Morgan Kaufmann Publishers Inc., 1999. Vovk, V., Gammerman, A., and Shafer, G. Algorithmic Learning in a Random World. Springer US, 2005. Vovk, V. G. Aggregating strategies. Proc. of Computational Learning Theory, 1990. Weron, R. Electricity price forecasting: A review of the state-of-the-art with a look into the future. International Journal of Forecasting, 30(4):1030–1081, 2014. Wintenberger, O. Optimal learning with bernstein online aggregation. Machine Learning, 106(1):119–141, 2017. Wisniewski, W., Lindsay, D., and Lindsay, S. Application of conformal prediction interval estimations to market mak- ers’ net positions. In Proceedings of the Ninth Symposium on Conformal and Probabilistic Prediction and Applica- tions, volume 128 of Proceedings of Machine Learning Research, pp. 285–301. PMLR, 2020. Xu, C. and Xie, Y. Conformal prediction interval for dy- namic time-series. In Proceedings of the 38th Interna- tional Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11559– 11569. PMLR, 2021a. Xu, C. and Xie, Y. Conformal prediction for dynamic time- series. arXiv preprint arXiv:2010.09107, 2021b. Adaptive Conformal Predictions for Time Series Appendices The appendices are organized as follows. First, Appendix A provides details about the Split Conformal Prediction procedure. Second, Appendix B proves the results of Section 3 and conducts the numerical analysis of Section 3.2 in the case where the efficiency is computed using the median length. Then, Appendix C contains details on the experimental setup (brief description of BOA, hyper-parameters, settings, pseudo-codes of competing algorithms). Finally, Appendices D and E contain complementary numerical results, respectively on synthetic data sets and on the French electricity spot prices data set. A. Details on Split Conformal Prediction In this section, we introduce and review the simplest theoretical properties of Split Conformal Prediction (SCP). More specifically, we present the whole algorithm, the theoretical guarantees and discuss the visualisation challenges arising when visualising a CP procedure. A.1. Split Conformal Prediction Algorithm Algorithm 2 Split Conformal Algorithm, with absolute value residuals scores Input: Regression algorithm A, significance level α, examples z1, . . . , zT with zt = (xt, yt). Output: Prediction interval ˆCα(x) for any x ∈ R d. 1: Randomly split {1, . . . , T } into two disjoint sets Tr and Cal. 2: Fit a mean regression function: ˆµ(·) ← A ({zt, t ∈ Tr}) 3: for j ∈ Cal do 4: Set sj = |yj − ˆµ(xj)|, the conformity scores 5: end for 6: Set SCal = {sj, j ∈ Cal} 7: Compute ̂Q1−αSCP (SCal), the 1 − αSCP-th empirical quantile of SCal, with 1 − αSCP := (1 − α) (1 + 1/#Cal). 8: Set ˆCα(x) = [ˆµ(x) ± ̂Q1−αSCP (SCal) ], for any x ∈ R d. A.2. Illustration of the SCP procedure Figure 9 provides a visualisation of the SCP procedure in a regression task. The conformity scores are taken to be the absolute value of the residuals. A.3. Theoretical guarantees of Split Conformal Prediction Conformal prediction relies on the assumption that the data is exchangeable. Definition A.1 (Exchangeability). (Xt, Yt) T t=1 are exchangeable if for any permutation σ of J1, T K we have: L ((X1, Y1) , . . . , (XT , YT )) = L ((Xσ(1), Yσ(1)) , . . . , (Xσ(T ), Yσ(T ))) , where L designates the joint distribution. Lei et al. (2018) proves the following Theorem A.2 about SCP quasi-exact validity. Theorem A.2. Suppose (Xt, Yt) T +1 t=1 are exchangeable, and we apply algorithm 2 on (Xt, Yt)T t=1 to predict an interval on XT +1, ˆCα (XT +1). Then we have: P { YT +1 ∈ ˆCα (XT +1) } ≥ 1 − α. If, in addition, the scores SCal have a continuous joint distribution, we also have an upper bound: P { YT +1 ∈ ˆCα (XT +1)} ≤ 1 − α + 1 #Cal + 1 . Adaptive Conformal Predictions for Time Series Step 1 0 2 4 x −2 0 2y ▶ Create a proper training set, a calibration set, and keep your test set, by randomly splitting your data set. Step 2 0 2 4 x −2 0 2y On the proper training set: ▶ Learn ˆµ Step 3 0 2 4 x −2 0 2y On the calibration set: ▶ Predict with ˆµ ▶ Get the residuals ˆεi and form the scores si = |ˆεi| ▶ Compute the (1 − α) × (1 + 1 #Cal ) empirical quan- tile of the si, noted Q1− ˆα (si) Step 4 0 2 4 x −2 0 2y On the test set: ▶ Predict with ˆµ ▶ Build ˆC ˆα(x): [ˆµ(x) ± Q1− ˆα (si)] Figure 9. Schematic illustration of the Split Conformal Prediction procedure. Special case of a regression task, where the conformity scores are the absolute value of the residuals, as it is standard. Adaptive Conformal Predictions for Time Series A.4. Examples of dependent scores when data noise is exchangeable In this subsection, we provide two examples that highlight the importance of adapting CP to time series. In these examples, the scores are non exchangeable while the true noise of the data is exchangeable. Example A.3 (Endogenous and not perfectly estimated). Assume Xt = Yt−1 ∈ R and that Yt = aYt−1 + εt, where εt is a white noise. This corresponds to an order-1 Auto-Regressive (i.e. AR(1)). Assume that the fitted model is ˆft(x) = ˆax, with ˆa ̸= a. Then, for any t, we have that: ˆεt = Yt − ˆYt = (a − ˆa) Yt−1 + εt ˆεt = aˆεt−1 + ξt with ξt = εt − ˆaεt−1. The residual process (ˆεt)t≥0 is an ARMA(1,1) (Auto-Regressive Moving-Average, see section C.2) of parameters φ = a and θ = −ˆa. Thus, we have generated dependent residuals (ARMA residuals) even though the underlying model only had white noise. Example A.4 (Exogenous and misspecified). Assume Xt ∈ R2 and that: Yt = aX1,t + bX2,t + εt, with εt ∼ i.i.d. N (0, 1), X2,t+1 = φX2,t + ξt, ξt ∼ i.i.d. N (0, 1) and X1,t can be any random variable. Assume that we misspecify the model so that the fitted model is ˆft(x) = ax1 for any t ≥ 0. Then, for any t ≥ 0, we have that ˆεt = Yt − ˆYt = bX2,t + εt. Thus, we have generated dependent residuals (Auto-Regressive residuals) even if the underlying model only had i.i.d. Gaussian noise. A.5. How should we visualise CP predicted intervals? We propose to have a closer look at how are constructed the prediction of this method. In this aim, we introduce model A.5. Model A.5. xt = cos ( 2π 180 t ) + sin ( 2π 180 t) + t 100 εt+1 = 0.99εt + ξt+1, ξt ∼ N (0, 0.01) . Yt = ft(xt) + εt = xt + εt In this model A.5, the explanatory variables are deterministic. A generation from this model is represented in Figure 10. The first subplot, Figure 10(a), represents xt across time. The second subplot, Figure 10(b), represents the noise εt across time. Finally, the last subplot, Figure 10(c), represents the whole process Yt across time. The aim is to predict intervals of coverage 0.9 for values of Yt, at t > 500, that is to say T0 = 500 here. For simplicity, we assume ˆft = ft at each time step t and we do not represent the points used to obtain this perfect regression model. There are two ways of visualizing the predictions, that are represented in each row of Figure 11. If the focus of the analysis is on a specific application with the aim of analysing the whole prediction, it is relevant to represent the response yt itself and the associated intervals. This is represented in the first row of Figure 11. Nevertheless, to better understand a CP method, it is relevant to represent the scores and the corresponding intervals, rescaled. This is represented in the second row of Figure 11 (even if the residuals are displayed and not their absolute value, i.e. the scores). To better understand the difference between the two visualizations, let’s look specifically at some observations. In the first line of the Figure 11, we can see that the intervals widen for t ∈ [801; 900], while struggling to include the observations. Adaptive Conformal Predictions for Time Series 0 500 1000 t 5 10 15ft(xt) (a) ft(xt) 0 500 1000 t −1 0 1εt (b) εt 0 500 1000 t 5 10 15yt=ft(xt)+εt (c) yt Figure 10. Representation of data simulated according to model A.5. 0 200 400 t 5 10 15yt 500 550 600 650 700 750 800 850 900 950 1000 t 0 200 400 t −1 0 1ˆεt 500 550 600 650 700 750 800 850 900 950 1000 t Figure 11. Visualisation of OSSCP on simulated data, from model model A.5. 1000 data points are generated. The 500 first ones form the initial calibration set, displayed on the first subplot of each row. The 500 last ones are the ones the algorithm tries to predict. They are displayed on the right subplot of each row. Observed values are in black, predicted intervals bounds are displayed in orange Nevertheless, it is difficult to understand the underlying phenomenon on such a plot. Indeed, the points seem very similar to those for t ∈ [660; 720]. What considerably influences the CP are the scores and not the observed values. Thus, in the second line, at times t ∈ [801; 900], we observe more clearly that the values go out of the previous range of values, being around 1.5 in absolute value. This explains why the intervals widen: the calibration set contains more and more high values, which increases the value of the quantile and, therefore, the length of the interval. To conclude, to analyse and assess the performances of CP procedures, we recommend representing the intervals around the conformity scores (or the residuals, depending on the score function) rather than the observed values. This is because the scores are what truly determine the conformal behaviour. B. Proof of the results presented in Section 3 and additional numerical experiments B.1. Proof of Theorem 3.1 We recall here Theorem 3.1. Theorem 3.1. Assume that: (i) α ∈ Q; (ii) the scores are exchangeable with quantile function Q; (iii) the quantile function is perfectly estimated at each time (as defined above); (iv) the quantile function Q is bounded and C4([0, 1]). Then, for all γ > 0, (αt)t>0 forms a Markov Chain, that admits a stationary distribution πγ, and 1 T T∑ t=1 L(αt) a.s. −→ T →+∞ Eπγ [L] not. = E ˜α∼πγ [L(˜α)]. Adaptive Conformal Predictions for Time Series Moreover, as γ → 0, Eπγ [L] = L0 + Q ′′(1 − α) γ 2 α(1 − α) + O(γ3/2). To prove Theorem 3.1, we rely on the following lemmas, that will be proved after the theorem. We denote Bβ a Bernoulli random variable of parameter β and P (x) designates the projection of x onto [0, 1]. Finally, for γ > 0, define the following Markov Chain: αt+1 = αt + γ (α − BP (αt)) for t > 0, (4) We introduce (p, q) ∈ N × N∗, p < q, s.t. α = p q , and: A = {α + γ gcd(q − p, p) q Z} ∩ ]γ(α − 1), 1 + γα[. (5) Lemma B.1 (Finite state space). Assume that α ∈ Q. Then, for any γ > 0, the Markov Chain defined by α1 ∈ A and αt+1 = αt + γ (α − BP (αt)), for t > 0 has a finite state space A. Lemma B.2 (Irreducibility). Assume that α ∈ Q. Then, for any γ > 0, the Markov Chain defined by Equation (4), for t > 0 and α1 ∈ A, is irreducible. Thereby we will prove that the chain admits a unique stationary distribution πγ, we now compute the first four moments of the stationary distribution in Lemmas B.3 to B.6. The final proof relies on a Taylor expansion, that requires to control these four moments. Lemma B.3 (Expectation). Let γ > 0 and consider again the Markov Chain defined in equation (4). We have: Eπγ [(P (˜α) − α)] = 0. Lemma B.4 (Second order moment). Let γ > 0 and consider again the Markov Chain defined in equation (4). As γ → 0, we have: Eπγ [(P (˜α) − α) 2] = γ 2 α(1 − α) + O(γ2). Lemma B.5 (Third order moment). Let γ > 0 and consider again the Markov Chain defined in equation (4). As γ → 0, we have: Eπγ [ (P (˜α) − α)3] = O(γ3/2). Lemma B.6 (Fourth order moment). Let γ > 0 and consider again the Markov Chain defined in equation (4). As γ → 0, we have: Eπγ [ (P (˜α) − α) 4] = O(γ2). The proofs of these Lemmas are postponed to Appendices B.2 and B.3. Here, we first give the proof of the main theorem. Proof of Theorem 3.1. Let γ > 0. For any t > 0 we have, for the recursion introduced in Equation (2), that αt+1 := αt + γ (α − 1yt /∈ ̂Cαt (xt)) = αt + γ (α − 1St> ˆQ1−P (αt) ) , where St is the conformity score at time t. Noting that 1St> ˆQt(1−P (αt)) d = BP(St> ˆQt(1−P (αt))), we obtain: αt+1 d = αt + γ (α − BP(St> ˆQt(1−P (αt))) ) d = αt + γ (α − BP(St>Q(1−P (αt)))) d = αt + γ (α − BP (αt)) , Adaptive Conformal Predictions for Time Series where the second line results from assumption (ii) and (iii), and the last equation from assumption (iii) only. Consequently, by induction, the chain defined by Equation (2) and αt+1 = αt + γ (α − BP (αt)) , (6) with α1 = α, have the same distribution. Using assumption (i), Lemma B.1 ensures that the state space A of the Markov Chain defined in equation (6) is finite. Furthermore, Lemma B.2 also ensures that the chain is irreducible. Therefore, the chain is irreducible on a finite state space, thus it admits a unique stationary distribution, noted πγ and for any positive function f such that ∫ f dπγ < ∞, we have (Meyn & Tweedie, 2012, Theorem 17.1.7): 1 T T∑ t=1 f (αt) a.s. −→ T →∞ ∫ f dπγ. Remark that L(β) = 2Q(1 − P (β)) for any β. Therefore, combined with previous result we get the first result of Theorem 3.1: 1 T T∑ t=1 L(αt) a.s. −→ T →+∞ E ˜α∼πγ [L(˜α)] . We now need to characterize E ˜α∼πγ [L(˜α)] = 2E ˜α∼πγ [Q(1 − P (˜α))] as γ → 0. Assume that Q ∈ C4([0, 1]). Using Taylor series expansion, for any ˜α ∈ A, there exists β(˜α) ∈ [0, 1]: Q(1 − P (˜α)) =Q(1 − α) + Q ′(1 − α)(α − P (˜α)) + Q ′′(1 − α) 2 (α − P (˜α)) 2 + Q ′′′(1 − α) 6 (α − P (˜α))3 + Q ′′′′(1 − β(˜α)) 24 (α − P (˜α))4. (7) To conclude, we take the expectation under πγ of equation (7), which gives: Eπγ [Q(1 − P (˜α))] =Q(1 − α) + Q ′(1 − α)Eπγ [(α − P (˜α))] + Q ′′(1 − α) 2 Eπγ [ (α − P (˜α)) 2] + Q ′′′(1 − α) 6 Eπγ [ (α − P (˜α))3] + Eπγ [ Q ′′′′(1 − β(˜α)) 24 (α − P (˜α))4] . (8) Injecting results of Lemmas B.3 to B.5 in equation (8), we obtain: Eπγ [Q(1 − P (˜α))] =Q(1 − α) + Q ′′(1 − α) 4 γα(1 − α) + O(γ3/2) + Eπγ [ Q ′′′′(1 − β(˜α)) 24 (α − P (˜α))4] . (9) Finally, we can control the last term since Q ∈ C4([0, 1]) by assumption, thus there exists M > 0 such that for any x ∈ [0, 1], |Q ′′′′(1 − x)| < M . Hence, using Lemma B.6 we obtain: ∣ ∣Eπγ [Q ′′′′(1 − β(˜α))(α − P (˜α))4]∣ ∣ ≤ Eπγ [ |Q ′′′′(1 − β(˜α))| (α − P (˜α))4] ≤ M Eπγ [ (α − P (˜α))4] ≤ M O(γ3/2) Eπγ [ Q ′′′′(1 − β(˜α))(α − P (˜α)) 4] = O(γ3/2). (10) Finally, combining equations (10) and (9) to conclude the proof by obtaining: Eπγ [Q(1 − P (˜α))] = Q(1 − α) + Q ′′(1 − α) 4 γα(1 − α) + O(γ3/2). (11) This concludes the proof of Theorem 3.1. Remark: is it possible to use only 3 moments? The proof here relies on the control of the first four moments. It is not clear that the same result could be obtained using only a third order Taylor expansion, as we would then require a bound on E[|P (˜α) − α| 3], which is not guaranteed to be O(γ3/2), contrary to E[(P (˜α) − α) 3]. Adaptive Conformal Predictions for Time Series B.2. Proof of Lemmas B.1 and B.2 Proof of Lemma B.1. Let γ > 0 and denote α = p q with 0 < p < q and (p, q) ∈ N2. We denote E the state space of the Markov Chain defined by equation (6), starting from a ∈ A. We show that E = A. First, (αt) is stritcly bounded by γ(α − 1) and 1 + γα. Thus E ⊂]γ(α − 1), γα[. Secondly, for any starting point α1 ∈ A, we can observe that: {αt, t ≥ 1} a.s. ⊂ α1 + {kγ(α − 1) + nγα, (k, n) ∈ N2} ⊂ α1 + {kγ(α − 1) + nγα, (k, n) ∈ Z2} = α1 + {kγ p − q q + nγ p q , (k, n) ∈ Z2} = α1 + γ q {(q − p)Z + pZ} = α1 + γ q gcd(q − p, p)Z = α + γ q gcd(q − p, p)Z where gcd(a, b) is the greatest common divisor of a and b. We have used at the last line that α1 ∈ A writes as α + γ q gcd(q − p, p)k, for some k ∈ Z. Combining both results, we get that: E ⊂ {α + γ q gcd(q − p, p)Z} ∩ ]γ(α − 1), γα[. This shows that the state space is finite and a subset of A. The reciprocal implication is proved in the following Lemma, together with irreducibility. Proof of Lemma B.2. Our objective is to show that there is a path of positive probability going from any point of the state space A to any point of the same state space A. Note that the chain always has at most two options when on a state x: make a step γα, with probability 1 − P (x), or a step γ(α − 1), with probability P (x). Let (x, y) ∈ A 2. Thereby, there exist (k, n), (l, m) ∈ N2 such that: x = α + kγα + nγ(α − 1) y = α + lγα + mγ(α − 1). Thus, starting from x, to attain y, the chain has to make the path y − x = (l − k)γα + (m − n)γ(α − 1). Noting that for any h ∈ N we have γα(q − p)h + γ(α − 1)hp = 0, we can equivalently write that: y − x = uγα + vγ(α − 1), (12) with (u, v) ∈ N2 \\ {(0, 0)}. Thus, for any (x, y) ∈ A 2 there exists (u, v) ∈ N2 \\ {(0, 0)} such that y − x = uγα + vγ(α − 1). Let’s show by induction on u + v that for any (u, v) ∈ N2, and (x, y) ∈ A 2 satisfying Equation (12) there exists a path of strictly positive probability between x and y. Initialization. Suppose first that u + v = 1. Then, there are two options: u = 1 and v = 0 or the reverse. Assume the former: Equation (12) gives y = x + γα and necessarily x < 1 since y < 1 + γα because y ∈ A. Thereby the step γα has a probability 1 − P (x) > 0 to occur. Thus the chain can attain y starting from x, i.e., P(α2 = y|α1 = x) > 0. The second case works similarly, by observing that necessarily x > 0. Heredity. Let m ∈ N. We assume that for any (u, v) ∈ N2 such that u + v = m, and (x, y) ∈ A 2 satisfying Equation (12) there exists a path of strictly positive probability between x and y, or formally there exists t ∈ N such that P(αt = y|α1 = x) > 0. Suppose now that u + v = m + 1 with m ∈ N∗. If v = 0, then y = x + uγα and similarly than for v = 0 and u = 1, the step γα is probable. Let z = x + γα. We have: Adaptive Conformal Predictions for Time Series • P(α2 = z|α1 = x) = 1 − P (x) > 0. • By our induction hypothesis, (y, z) satisfy Eq. 12 with u + v = m, thus there exists t such that P(αt = y|α2 = y) > 0. Overall, P(αt = y|α1 = x) > 0. If instead u = 0, then y = x + vγ(α − 1) and as for u = 0 and v = 1, the step γα is of strictly positive probability and we conclude similarly. Finally, if both u and v are non-null, then we can make the step γ(α − 1) if x > 0 and the step γα otherwise, before using our induction hypothesis. This shows that we can build a path of strictly positive probability for any (x, y) ∈ A 2, and thereby that the chain is irreducible. B.3. Control of the first four moments: Lemmas B.3 to B.6 In the following Lemmas, to compute the first order moments of πγ, we consider the chain αt+1 = αt + γ (α − BP (αt)) for t > 0, launched from the stationary distribution α1 ∼ πγ. Thanks to the stationarity property, for all t ≥ 1, αt ∼ πγ. Proof of Lemma B.3. Let γ > 0. To derive Eπγ [(P (α1) − α)] we start by equation (6) with t = 1: α2 = α1 + γ (α − BP (α1)) taking expectation E [α2] = E [α1] + γ (α − E [ BP (α1)]) using E [α1] = E [α2] = Eπγ [α], 0 = γ (α − Eπγ [ BP (α1)]) Eπγ [ E [ BP (α1)|α1]] = α Eπγ [P (α1)] = α. Proof of Lemma B.4. Let γ > 0. To derive Eπγ [ (P (α1) − α) 2] we start by equation (6) with t = 1: (α2 − α) 2 = (α1 − α) 2 + γ2(α − BP (α1))2 + 2γ(α − BP (α1))(α1 − α) Eπγ [ (α2 − α) 2] = Eπγ [ (α1 − α)2] + γ2Eπγ [ (α − BP (α1)) 2] + 2γEπγ [ (α − BP (α1))(α1 − α))] 0 = γ2Eπγ [ (α − BP (α1)) 2] + 2γEπγ [(α − P (α1))(α1 − α)] Consequently, 2γEπγ [(P (α1) − α)(α1 − P (α1) + P (α1) − α)] =γ2Eπγ [ (α − BP (α1) + P (α1) − P (α1))2] 2γEπγ [ (P (α1) − α)2] − 2γEπγ [(α − P (α1))(α1 − P (α1))] =γ2Eπγ [ (α − BP (α1) + P (α1) − P (α1))2] (2 − γ)Eπγ [ (P (α1) − α) 2] =γEπγ [P (α1)(1 − P (α1))] + 2Eπγ [(α − P (α1))(α1 − P (α1))] . (13) We can compute Eπγ [P (α1)(1 − P (α1))]: Eπγ [P (α1)(1 − P (α1)) − α(1 − α)] = Eπγ [(P (α1) − α)(1 − P (α1)) + α(1 − P (α1)) − α(1 − α)] = Eπγ [(P (α1) − α)(1 − P (α1)) + α(α − P (α1))] = Eπγ [(P (α1) − α)(1 − P (α1) − α)] = Eπγ [(P (α1) − α)(α − P (α1) + 1 − 2α)] = −Eπγ [ (P (α1) − α) 2] + Eπγ [(P (α1) − α)(1 − 2α)] = −Eπγ [ (P (α1) − α) 2] ⇒ Eπγ [P (α1)(1 − P (α1))] = α(1 − α) − Eπγ [ (P (α1) − α) 2] (14) Adaptive Conformal Predictions for Time Series Reinjecting equation (14) in equation (13): Eπγ [ (P (α1) − α) 2] = γ 2 α(1 − α) + Eπγ [(α − P (α1))(α1 − P (α1))] (15) We are now going to derive an upper and lower bound of Eπγ [ (P (α1) − α) 2] . Note that sign(α − P (α1)) = −sign(α1 − P (α1)), thus Eπγ [(α − P (α1))(α1 − P (α1))] ≤ 0. Hence we obtain the following upper bound: Eπγ [ (P (α1) − α)2] ≤ γ 2 α(1 − α). (16) Furthermore, using again this observation, and additionally that |α − P (α1)| ≤ 1 and |α1 − P (α1)| ≤ γ and from equation (15), we can obtain: Eπγ [ (P (α1) − α)2] ≥ γ 2 α(1 − α) − γPπγ (α1 /∈]0, 1[) ≥ γ 2 α(1 − α) − γC −1 α Eπγ [ (P (α1) − α) 2] Eπγ [ (P (α1) − α)2] ≥ 1 1 + γC −1 α γ 2 α(1 − α), (17) where the second inequality holds by observing that: Eπγ [ (P (α1) − α)2] ≥ (1 − α) 2Pπγ (α1 > 1) + α2Pπγ (α1 < 0) Eπγ [ (P (α1) − α)2] ≥ CαPπγ (α1 ̸∈ [0, 1]) ⇒ Pπγ (α1 ̸∈ [0, 1]) ≤ C −1 α Eπγ [ (P (α1) − α)2] with Cα = min(α2, (1 − α) 2). Gathering equations (17) and (16), we obtain: 1 (1 + γC −1 α ) γ 2 α(1 − α) ≤ Eπγ [ (P (α1) − α)2] ≤ γ 2 α(1 − α) ( 1 (1 + γC −1 α ) − 1) γ 2 α(1 − α) ≤ Eπγ [ (P (α1) − α) 2] − γ 2 α(1 − α) ≤ 0 ∣ ∣ ∣Eπγ [ (P (α1) − α) 2] − γ 2 α(1 − α) ∣ ∣ ∣ ≤ γ2C −1 α 2(1 + γC −1 α ) α(1 − α) Eπγ [ (P (α1) − α) 2] − γ 2 α(1 − α) = O(γ2). (18) Proof of Lemma B.5. Let γ > 0. We start again by using equation (6) and removing the first terms as Eπγ [ (α2 − α)3] = Eπγ [ (α1 − α) 3]. Then we will isolate Eπγ [ (P (α1) − α)3] and finally we will dominate each term obtained. Adaptive Conformal Predictions for Time Series 0 =3γEπγ [ (α1 − α) 2(α − BP (α1)) ] + 3γ2Eπγ [ (α1 − α)(α − BP (α1))2] + γ3Eπγ [ (α − BP (α1)) 3] 0 =3γEπγ [ (α1 − α) 2(α − P (α1))] + 3γ2Eπγ [ (α1 − α)(α − P (α1))2) ] + 6γ2Eπγ [ (α1 − α)(α − P (α1))(P (α1) − BP (α1)) ] + 3γ2Eπγ [ (α1 − α)(P (α1) − BP (α1)) 2] + γ3Eπγ [(α − BP (α1))3] 3γEπγ [ (P (α1) − α) 3] =3γEπγ [ (α1 − P (α1))2(α − P (α1))] + 6γEπγ [(α1 − P (α1))(P (α1) − α)(α − P (α1))] + 3γ2Eπγ [ (α1 − α)(α − P (α1)) 2) ] + 3γ2Eπγ [(α1 − α)P (α1)(1 − P (α1))] + γ3Eπγ [ (α − BP (α1)) 3] 3Eπγ [ (P (α1) − α) 3] =3Eπγ [ (α1 − P (α1))2(α − P (α1)) ] − 6Eπγ [ (α1 − P (α1))(P (α1) − α) 2] + 3γEπγ [ (α1 − α)(α − P (α1))2)] + 3γEπγ [(α1 − α)P (α1)(1 − P (α1))] + γ2Eπγ [ (α − BP (α1)) 3] 3 ∣ ∣Eπγ [ (P (α1) − α) 3]∣ ∣ ≤3 ∣ ∣Eπγ [ (α1 − P (α1))2(α − P (α1))]∣ ∣ + 6 ∣ ∣Eπγ [(α1 − P (α1))(P (α1) − α)2]∣ ∣ + 3γ ∣ ∣Eπγ [ (α1 − α)(α − P (α1))2)]∣ ∣ + 3γ ∣ ∣Eπγ [(α1 − α)P (α1)(1 − P (α1))] ∣ ∣ + γ2 ∣ ∣Eπγ [ (α − BP (α1)) 3]∣ ∣ . (19) To conclude, we can bound each term of the right hand side of equation (19). In order of appearance we obtain: ∣ ∣Eπγ [ (α1 − P (α1))2(α − P (α1)) ]∣ ∣ ≤ Eπγ [ (α1 − P (α1)) 2 |α − P (α1)| ] ∣ ∣Eπγ [ (α1 − P (α1))2(α − P (α1)) ]∣ ∣ ≤ γ2. (20) ∣ ∣Eπγ [ (α1 − P (α1))(P (α1) − α)2]∣ ∣ ≤ Eπγ [ |α1 − P (α1)| (P (α1) − α) 2] ≤ γEπγ [ (P (α1) − α) 2] ∣ ∣Eπγ [ (α1 − P (α1))(P (α1) − α)2]∣ ∣ ≤ γ2 2 α(1 − α) + O(γ3), (21) where the last equality is obtained by using Lemma B.4. γ ∣ ∣Eπγ [ (α1 − α)(α − P (α1))2) ]∣ ∣ ≤ γEπγ [ |α1 − α| (α − P (α1)) 2) ] ≤ γDγ,αEπγ [(α − P (α1))2)] γ ∣ ∣Eπγ [ (α1 − α)(α − P (α1))2) ]∣ ∣ ≤ Dγ,α γ2 2 α(1 − α) + O(γ3), (22) again using Lemma B.4, and with Dγ,α = max(1 + γα, γ(1 − α)) − α = O(1). γ ∣ ∣Eπγ [(α1 − α)P (α1)(1 − P (α1))] ∣ ∣ ≤γ ∣ ∣Eπγ [(α1 − P (α1))P (α1)(1 − P (α1))] ∣ ∣ + γ ∣ ∣Eπγ [(P (α1) − α)P (α1)(1 − P (α1))] ∣ ∣ ≤γ 1 4 Eπγ [|α1 − P (α1)|] + γ 1 4 Eπγ [|P (α1) − α|] ≤ γ2 4 + γ 4 √Eπγ [(P (α1) − α)2] ≤ γ2 4 + γ 4 √ γ 2 α(1 − α) + O(γ2) γ ∣ ∣Eπγ [(α1 − α)P (α1)(1 − P (α1))] ∣ ∣ ≤O(γ3/2), (23) Adaptive Conformal Predictions for Time Series where the last inequality comes from Lemma B.4 a third time. γ2 ∣ ∣Eπγ [ (α − BP (α1))3]∣ ∣ ≤ γ2 max(α3, (1 − α)3). (24) Gathering equations (20) to (24) together with equation (19), we obtain the following upper bound: 3 ∣ ∣Eπγ [ (P (α1) − α) 3]∣ ∣ ≤ 3γ2 + 3γ2α(1 − α) + O(γ3) + 3Dγ,α γ2 2 α(1 − α) + O(γ3) + O(γ3/2) + γ2 max(α3, (1 − α) 3), which leads to: Eπγ [ (P (α1) − α)3] = O(γ3/2). (25) Proof of Lemma B.6. Let γ > 0. For the fourth order moment, the proof works in the same way for the third order moment, Lemma B.5. 0 =4γEπγ [ (α1 − α)3(α − BP (α1)) ] + 6γ2Eπγ [ (α1 − α) 2(α − BP (α1))2] + 4γ3Eπγ [ (α1 − α)(α − BP (α1))3] + γ4Eπγ [ (α − BP (α1)) 4] 0 =4γEπγ [ (α1 − P (α1) + P (α1) − α) 3(α − P (α1))] + 6γ2Eπγ [ (α1 − α)2(α − P (α1) + P (α1) − BP (α1)) 2] + 4γ3Eπγ [ (α1 − α)(α − BP (α1))3] + γ4Eπγ [ (α − BP (α1)) 4] 4γEπγ [ (P (α1) − α) 4] =4γEπγ [ (α1 − P (α1))3(α − P (α1))] + 12γEπγ [ (α1 − P (α1))2(P (α1) − α)(α − P (α1))] + 12γEπγ [ (α1 − P (α1))(P (α1) − α) 2(α − P (α1))] + 6γ2Eπγ [ (α1 − α)2(α − P (α1)) 2] + 0 + 6γ2Eπγ [ (α1 − α)2(P (α1) − BP (α1)) 2] + 4γ3Eπγ [ (α1 − α)(α − BP (α1))3] + γ4Eπγ [ (α − BP (α1)) 4] 4Eπγ [ (P (α1) − α) 4] =4Eπγ [ (α1 − P (α1)) 3(α − P (α1))] − 12Eπγ [ (α1 − P (α1))2(P (α1) − α)2] − 12Eπγ [ (α1 − P (α1))(P (α1) − α)3] + 6γEπγ [ (α1 − α) 2(α − P (α1))2] + 6γEπγ [ (α1 − α) 2P (α1)(1 − P (α1))] + 4γ2Eπγ [ (α1 − α)(α − BP (α1))3] + γ3Eπγ [ (α − BP (α1)) 4] 4 ∣ ∣Eπγ [ (P (α1) − α) 4]∣ ∣ ≤4 ∣ ∣Eπγ [ (α1 − P (α1))3(α − P (α1)) ]∣ ∣ + 12 ∣ ∣Eπγ [ (α1 − P (α1))2(P (α1) − α)2]∣ ∣ + 12 ∣ ∣Eπγ [(α1 − P (α1))(P (α1) − α)3]∣ ∣ + 6γ ∣ ∣Eπγ [(α1 − α) 2(α − P (α1))2]∣ ∣ + 6γ ∣ ∣Eπγ [(α1 − α) 2P (α1)(1 − P (α1)) ]∣ ∣ + 4γ2 ∣ ∣Eπγ [ (α1 − α)(α − BP (α1))3]∣ ∣ + γ3 ∣ ∣Eπγ [ (α − BP (α1)) 4]∣ ∣ . (26) We are now going to dominate each term of the right hand side of equation (26) in order of appearance. ∣ ∣Eπγ [ (α1 − P (α1))3(α − P (α1)) ]∣ ∣ ≤ Eπγ [|α1 − P (α1)|3 |α − P (α1)|] ∣ ∣Eπγ [ (α1 − P (α1))3(α − P (α1))]∣ ∣ ≤ γ3 (27) ∣ ∣Eπγ [ (α1 − P (α1))2(P (α1) − α)2]∣ ∣ = Eπγ [ (α1 − P (α1))2(P (α1) − α)2] ∣ ∣Eπγ [ (α1 − P (α1))2(P (α1) − α)2]∣ ∣ ≤ γ2. (28) ∣ ∣Eπγ [ (α1 − P (α1))(P (α1) − α)3]∣ ∣ ≤ Eπγ [|α1 − P (α1)| |P (α1) − α| 3] ≤ γEπγ [|P (α1) − α| 3] ∣ ∣Eπγ [ (α1 − P (α1))(P (α1) − α)3]∣ ∣ ≤ O(γ5/2). (29) Adaptive Conformal Predictions for Time Series where the last inequality holds using Lemma B.5. γ ∣ ∣Eπγ [ (α1 − α)2(α − P (α1)) 2]∣ ∣ = γEπγ [ (α1 − α) 2(α − P (α1))2] ≤ γD2 γ,α( γ 2 α(1 − α) + O(γ2)) γ ∣ ∣Eπγ [ (α1 − α)2(α − P (α1)) 2]∣ ∣ ≤ D2 γ,α γ2 2 α(1 − α) + O(γ3). (30) again where we’ve used Lemma B.5, and re-used its notation Dγ,α = max(1 + γα, γ(1 − α)) − α = O(1). γ ∣ ∣Eπγ [ (α1 − α) 2P (α1)(1 − P (α1))]∣ ∣ =γ ∣ ∣Eπγ [(α1 − P (α1)) 2P (α1)(1 − P (α1)) ] + 2Eπγ [(α1 − P (α1))(P (α1) − α)P (α1)(1 − P (α1))] + Eπγ [ (P (α1) − α) 2P (α1)(1 − P (α1))]∣ ∣ ≤ γ 4 Eπγ [ (α1 − P (α1))2] + γ 2 Eπγ [|α1 − P (α1)| |P (α1) − α|] + γ 4 Eπγ [ (P (α1) − α) 2] γ ∣ ∣Eπγ [ (α1 − α) 2P (α1)(1 − P (α1))]∣ ∣ ≤ γ3 4 + γ2 2 + γ2 8 α(1 − α) + O(γ3). (31) again where we’ve used Lemma B.5. γ2 ∣ ∣Eπγ [ (α1 − α)(α − BP (α1)) 3]∣ ∣ ≤ γ2Eπγ [|α1 − α| ∣ ∣α − BP (α1)∣ ∣3] ≤ γ2Dγ,αEπγ [∣ ∣α − BP (α1)∣ ∣3] γ2 ∣ ∣Eπγ [ (α1 − α)(α − BP (α1)) 3]∣ ∣ ≤ γ2Dγ,α max(α3, (1 − α) 3). (32) γ3 ∣ ∣Eπγ [ (α − BP (α1))4]∣ ∣ ≤ γ3 max(α4, (1 − α)4). (33) Gathering equations (27) to (33) together with equation (26), we obtain finally: Eπγ [ (P (α1) − α) 4] = O(γ2). (34) B.4. Proof of Theorem 3.3 In this section, we prove Theorem 3.3. Recall the theorem: Theorem 3.3. Assume that: (i) α ∈ Q; (ii) the residuals follow an AR(1) process (i.e., εt+1 = φεt + ξt+1 with (ξt)t i.i.d. random variables admitting a continuous density with respect to Lebesgue measure, of support S) clipped at a large value R, and [−R, R] ⊂ S; (iii) the quantile function Q of the stationary distribution of (εt)t is known. Then (αt, εt−1) is a homogeneous Markov Chain in R 2 that admits a unique stationary distribution πγ,φ. Moreover, 1 T T∑ t=1 L(αt) a.s. −→ T →+∞ Eπγ,φ[L]. We consider Zt = (αt, εt−1) defined in the state-space Z = A × [−R, R] by { αt+1 = αt + γ (α − 1{|εt| > Q1−P (αt)} ) , εt = −R ∨ (φεt−1 + ξt) ∧ R Adaptive Conformal Predictions for Time Series That is, (αt)t≥0 is the recurrence defined by Equation (2), and (εt)t≥0 is an AR(1) process with parameters φ clipped at some large value R. Finally, (ξt)t is a sequence of i.i.d. r.v. admitting a continuous density with respect to the Lebesgue measure, of support S ⊃ [−R, R]. This chain is defined for parameters α, R considered as fixed, and we focus on the influence of γ, φ. The main difference w.r.t. the previous section is that the state space is not countable anymore. More precisely, the state space is a product of a finite discrete set and an interval of R. The state-space Z is A × [−R, R], where A is defined in the previous Appendix B.1, equation (5). We equip Z with the σ-algebra F = P(A) × B(R), where P(A) is the power-set of the finite set A and B(R) is the borel set of R. Lemma B.7. The sequence (Zt)t≥0 is a Markov chain. Moreover, the chain is Harris-recurrent, and admits a stationary distribution πγ,φ. Proof. We observe that Zt = ( αt+1 εt ) = ( αt + γ (α − 1{|φεt−1 + ξt| > Q1−P (αt)} ) −R ∨ (φεt−1 + ξt) ∧ R ) =: Fγ,φ(Zt−1, ξt). (35) For a function Fγ,φ : R 2 × R. Consequently, Zt follows a Non-Linear State Space model (Meyn & Tweedie, 2012, Section 2.2.2 and Chapter 7). We denote Pγ,φ the probability kernel or Markov transition function, that is, for any z = (a, e) ∈ Z, and F ∈ F: Pγ,φ(z, F ) = P(Z1 ∈ F |Z0 = z). Remark that relying on Equation (35), we have an explicit formula for Pγ,φ. Defining the sequence of functions (Ft)t≥1 such that Ft+1 (z0, ξ1, . . . ξt+1) = Fγ,φ (Ft (z0, ξ1, . . . ξt) , ξt+1) where z0 and (ξi) are arbitrary real numbers. By induction we have that for any initial condition Z0 = z0 ∈ Z and any t ∈ N, Zt = Ft (z0, ξ1, . . . , ξt) , which immediately implies that the t-step transition function may be expressed as P t γ,φ(z, F ) = P (Ft (z, ξ1, . . . , ξt) ∈ F ) = ∫ · · · ∫ 1 {Ft (z, ξ1, . . . , ξt) ∈ F } p (dξ1) . . . p (dξt) where p is the distribution of ξ. We first prove that the chain is ψ-irreductible, for ψ = µ ⊗ λLeb, with µ the uniform probability measure on A and λLeb the Lebesgue measure on [−R; R]. 10 For any z0 = (a0, e0) ∈ Z and F = {a ′} × O, with O open set, such that ψ(F ) ̸= 0 we have that, for some t large enough P(Zt ∈ F |Z0 = z0) > 0. Indeed, 1. There exists a path (a0, . . . , at = a ′) in A from a0 to a ′ such that for all s ∈ {1, . . . , t − 1}, 0 < as < 1; and as+1 − as ∈ {γ(α − 1), γα} similarly to the proof of Lemma B.2 since α ∈ Q. 2. Let Es+1 be the event such that we obtain as+1 from as. Technically, if a. if as+1 − as = γ(α − 1), Es+1 = {ξs such that |φεs−1 + ξs| > Q1−as } b. conversely, if as+1 − as = γα, Es+1 = {ξs such that |φεs−1 + ξs| ≤ Q1−as}. 10Moreover ψ is transformed to remove mass from the sets that cannot be reached by the chain (Zt)t, i.e., if B is such that P(Zt ∈ B) = 0 for all t. This only concerns extremely marginal points, possible only α > 1 or α = min A, for which we assign a zero mass to α × U for some U. Adaptive Conformal Predictions for Time Series 3. Then if 0 < a ′ < 1, we can directly conclude, as we have that for all s ∈ {1, . . . , t}, P (Zs+1 ∈ {as+1} × Es+1|Zs = (as, zs)) = P(Es+1) > δ > 0. Indeed (for case a.): P(Es+1) = P{ξs such that |φεs−1 + ξs| > Q1−as } > min (P{ξs such that ξs > Q1−min A∪R∗ +}, P{ξs such that ξs < −Q1−min A∪R∗ +} ) =: δ, with δ > 0 by the assumption (ii) (esp. the fact that the support S of ξs includes [−R, R]). The proof is similar for case b. Consequently, P(Zt ∈ F |Z0 = z0) > δt > 0. 4. The argument extends to the case where a ′ < (0, 1), relying on the fact that ψ(F ) > 0. Moreover, the argument can be extended to show that for any a ′, O, there exists δ′ such that for all a0, e0, there exists t ≤ Cα,γ (e.g., Cα,γ = 2 αγ ) such that P(Zt ∈ F |Z0 = z0) > δ′. Which proves that the chain will visit infinitely many times any Borel set F with probability 1, and is consequently Harris-recurrent (Meyn & Tweedie, 2012, Chapter 9). Using Theorem 10.0.1 in Meyn & Tweedie (2012), we conclude that the chain admits a unique stationary distribution πγ,φ. Finally, applying (Theorem 17.1.7 Meyn & Tweedie, 2012) to the later result gives: 1 T T∑ t=1 L(αt) a.s. −→ T →+∞ Eπγ,φ[L]. B.5. Numerical study of ACI efficiency with AR(1) residuals, with respect to the median length We here reproduce the same experiment as in Section 3.2, but focus on the efficiency as the median of the intervals’ lengths instead of the average (after imputation). Results are given in Figure 12. 0.00 0.05 0.10 0.15 0.20 γ 1.5 2.0 2.5 3.0 3.5Medianlength ϕ =0 ϕ =0.6 ϕ =0.85 ϕ =0.95 ϕ =0.98 ϕ =0.99 ϕ =0.997 ϕ =0.999 0.0 0.6 0.85 0.95 0.980.99 0.997 0.999 ϕ 0.00 0.05 0.10 0.15 0.20γ+ Figure 12. Left: evolution of the median length depending on γ for various φ. Right: γ+ minimizing the median length for each φ. Observations are very similar to the average length case, especially regarding (i) the monotonicity of the median interval length w.r.t. φ, (ii) the existence of a minimum γ+ φ to the function γ ↦→ Medπγ,φ[˜α] := argminm Eπγ,φ [|˜α − m|] (iii) the non-monotonicity of φ ↦→ γ+ φ . Adaptive Conformal Predictions for Time Series C. Experimental details. C.1. Details on the BOA procedure The Bernstein Online Aggregation (BOA) procedure (Wintenberger, 2017) is a type of aggregation rule Φ. The weights outputted by BOA have an exponential form. In the exponent is plugged the difference between the loss suffered by the last aggregated forecast and the current squared loss suffered by the expert, instead of plugging the losses suffered by the experts (this would be Exponential Weighted Aggregation, Vovk, 1990). As stated in Wintenberger (2017), “this procedure favors online learners that predicted accurately and which past predictions losses are close to the loss of the last aggregative online learner, ensuring the stability in time and a small quadratic variation”. For more details, we refer the reader to the original paper Wintenberger (2017). C.2. Details ARMA(1,1) processes Definition C.1 (ARMA(1,1) process). We say that εt is an ARMA(1,1) process if for any t: εt+1 = φεt + ξt+1 + θξt, with: • θ + φ ̸= 0, |φ| < 1 and |θ| < 1; • ξt is a white noise of variance σ2, called the innovation. The asymptotic variance of this process is: Var(εt) = σ2 1 − 2φθ + θ2 1 − φ2 . (36) An ARMA(1,1) is thus characterised by three parameters: the coefficients φ and θ and the innovation’s variance σ2. The larger the coefficients, in absolute value, the greater the time dependence and variance. Note that when φ = 0, the ARMA(0,1) process corresponds to a MA(1) and when θ = 0, the ARMA(1,0) process corresponds to an AR(1). To fix the asymptotic variance of an ARMA(1,1) of parameters φ and θ to v, we fix σ2 = v 1−φ 2 1−2φθ+θ2 . C.3. Random forest parameters All the random forests model have the same parameters, that are the following: • Number of trees: 1000 • Minimum sample per leaf: 1 (default) • Maximum number of features: d (default) Furthermore, for EnbPI, as there is already an individual bootstrap in the algorithm, the random forest regressors do not bootstrap them again. C.4. Details about the baselines and comparison C.4.1. ENBPI FULL ALGORITHM In order to be self-contained and precise the modifications done in EnbPI V2, the EnbPI algorithm from Xu & Xie (2021a) is recalled in the following. In purple we precise the difference in EnbPI V2. Remark on the bootstrap approach. The bootstrap scheme is not adapted to time series, even if such strategies have been developed (H¨ardle et al., 2003; Kreiss & Paparoditis, 2012; Cai & Davies, 2012), and could be used to improve the adequation of EnbPI with the time series framework. Furthermore, recent works have proposed modifications of RF in the dependent setting (Goehry, 2020; Goehry et al., 2021; Saha et al., 2021). Generalizing these improvements to any ensemble method and use it for EnbPI could also enhance its performance, but is out of the scope of this paper. Adaptive Conformal Predictions for Time Series Algorithm 3 Sequential Distribution-free Ensemble Batch Prediction Intervals (EnbPI) Input: Training data {(xi, yi)}T i=1, regression algorithm A, decision threshold α, aggregation function φ, number of bootstrap models B, the batch size s, and test data {(xt, yt)} T +T1 t=T +1, with yt revealed only after the batch of s prediction intervals with t in the batch are constructed. Output: Ensemble prediction intervals {Cα(xt)} T +T1 t=T +1 1: for b = 1, . . . , B do 2: Sample with replacement an index set Sb = (i1, . . . , iT ) from indices (1, . . . , T ) 3: Compute ˆf b = A ({(xi, yi) | i ∈ Sb}) 4: end for 5: Initialise ε = {} 6: for i = 1, . . . , T do 7: ˆf φ −i (xi) = φ ({ ˆf b (xi) | i /∈ Sb}) 8: Compute ˆε φ i = ∣ ∣ ∣yi − ˆf φ −i (xi)∣ ∣ ∣ 9: ε = ε ∪ {ˆεφ i } 10: end for 11: for t = T + 1, . . . , T + T1 do 12: Let ˆf φ −t (xt) = (1 − α) quantile of { ˆf φ −i (xt) }T i=1 EnbPI V2: this is replaced by ˆf φ −t (xt) = φ ({ ˆf φ −i (xt) }T i=1 ). 13: Let wφ t = (1 − α) quantile of ε 14: Return C φ,α T,t (xt) = [ ˆf φ −t (xt) ± wφ t ] 15: if t − T = 0 mod s then 16: for j = t − 1, . . . , t − 1 do 17: Compute ˆεφ j = ∣ ∣ ∣yj − ˆf φ −j (xt)∣ ∣ ∣ 18: ε = (ε − {ˆεφ 1 }) ∪ {ˆεφ i } and reset index of ε 19: end for 20: end if 21: end for Adaptive Conformal Predictions for Time Series C.4.2. DETAILS ON THE IMPLEMENTATION We conclude this section by summarizing computational aspects of the methods. One of the contributions is to provide a unified experimental framework. Therefore, in Table 1, we display the current available code for these methods, and what is available in the proposed repository. Table 1. Summary of available code online for each method and the proposed code in the repository. The programming language is specified, and, when relevant, the nature of the code. Currently available Contribution Methods Language Details Language Options CP R Python OSCP not available Python randomised split EnbPI Python Python same aggregation function ACI R script no general function Python randomised split D. Additional experiments on synthetic data sets In this section, we provide supplemental results on the synthetic data sets presented in Section 5.1. First, in Appendix D.1 the sensitivity analysis of ACI γ as well as the comparison to the naive strategy and AgACI is extended to AR(1) and MA(1) processes of asymptotic variance 10. Then, in Appendix D.2, the comparison of all the CP methods for time series (initiated in Section 5.4) is also extended to these noises, that is AR(1) and MA(1) processes of asymptotic variance 10 (Appendix D.2.1), and to ARMA(1,1), AR(1) and MA(1) processes of asymptotic variance 1 (Appendix D.2.2). Next, we discuss in Section 5.4 that the improved validity for γ = 0.05 in comparison to γ = 0.01 comes at the cost of more infinite intervals. This analysis is detailed in Appendix D.3. Finally, we compare randomized and sequential split in Appendix D.4. Imputation. The rationale to impute the infinite intervals is the following. We take the maximum of the absolute values of the residuals on the test set, noted |ε|max. Then, for any t ∈ JT0 + 1, T0 + T1K, if the predicted upper (resp. lower) bound ˆb(u) t (xt) is such that ˆbt(xt) > ˆµt(xt) + |ε|max (resp. ˆb (ℓ) t (xt) < ˆµt(xt) − |ε|max) we impute it by ˆµt(xt) + |ε|max (resp. ˆµt(xt) − |ε|max). D.1. Additional experimental results of ACI sensitivity to γ, presented in Section 5.2 In this subsection, we provide similar results to those of Section 5.2, for different models on the noise. Especially, we consider AR(1) and MA(1) processes. Observations. The behaviour of the AR(1) process is very similar to the one of ARMA(1,1). On the other hand, for the MA case, the dependence structure is too weak to observe a significant effect of γ. All ACI methods produce nearly valid intervals, with coverage above 89.25%. Results are given in Figures 13 and 14. D.2. Comparison to baselines, extension of Section 5.4 D.2.1. ASYMPTOTIC VARIANCE FIXED TO 10. Figure 15 displays the results on data generated according to Section 5.1, for an asymptotic variance of the noise of 10 (as in Figure 6), when this noise is an AR(1) or MA(1) process. Observations. As in the previous section, the methods’ performances are greatly impacted by the type and strength of dependence structure. Figure 15 shows that while ARMA(1,1) and AR(1) noises lead to similar patterns, it is not the case for an MA(1) noise. In the latter, θ has little influence: the five performances (one for each θ) are similar within each method. Adaptive Conformal Predictions for Time Series 0.86 0.88 0.90 12 13 14Averagemedianlength ϕ =0.1, θ = 0 0.86 0.88 0.90 ϕ =0.8, θ = 0 0.86 0.88 0.90 ϕ =0.9, θ = 0 0.86 0.88 0.90 ϕ =0.95, θ = 0 0.86 0.88 0.90 ϕ =0.99, θ = 0 0.86 0.88 0.90 Coverage 12 13 14 15 16Averagelength,afterimputation 0.86 0.88 0.90 Coverage 0.86 0.88 0.90 Coverage 0.86 0.88 0.90 Coverage 0.86 0.88 0.90 Coverage 0.0000 0.0001 0.0004 0.0007 0.0010 0.0040 0.0070 0.0100 0.0400 0.0700 γ AgACI Naive method Figure 13. ACI performance with various θ, φ and γ on data simulated according to equation (3) with a Gaussian AR(1) noise of asymptotic variance 10 (see Appendix C.2). Top row: average median length with respect to the coverage. Bottom row: percentage of infinite intervals. Stars correspond to the proposed online expert aggregation strategy, and empty triangles to the naive choice. 0.8925 0.8950 0.8975 0.9000 13.6 13.8Averagemedianlength ϕ = 0, θ =0.1 0.8925 0.8950 0.8975 0.9000 ϕ = 0, θ =0.8 0.8925 0.8950 0.8975 0.9000 ϕ = 0, θ =0.9 0.8925 0.8950 0.8975 0.9000 ϕ = 0, θ =0.95 0.8925 0.8950 0.8975 0.9000 ϕ = 0, θ =0.99 0.8925 0.8950 0.8975 0.9000 Coverage 14 15Averagelength,afterimputation 0.8925 0.8950 0.8975 0.9000 Coverage 0.8925 0.8950 0.8975 0.9000 Coverage 0.8925 0.8950 0.8975 0.9000 Coverage 0.8925 0.8950 0.8975 0.9000 Coverage 0.0000 0.0001 0.0004 0.0007 0.0010 0.0040 0.0070 0.0100 0.0400 0.0700 γ AgACI Naive method Figure 14. ACI performance with various θ, φ and γ on data simulated according to equation (3) with a Gaussian MA(1) noise of asymptotic variance 10 (see Appendix C.2). Top row: average median length with respect to the coverage. Bottom row: percentage of infinite intervals. Stars correspond to the proposed online expert aggregation strategy, and empty triangles to the naive choice. 0.84 0.86 0.88 0.90 Coverage 10 11 12 13 14Averagemedianlength AR(1) noise 0.895 0.900 0.905 Coverage 13.2 13.4 13.6 13.8 MA(1) noise 0.895 0.900 0.905 OSSCP (adapted from Lei et al., 2018) Oﬄine SSCP (adapted from Lei et al., 2018) EnbPI (Xu & Xie, 2021) EnbPI V2 ACI (Gibbs & Cand`es, 2021), γ = 0.01 ACI (Gibbs & Cand`es, 2021), γ = 0.05 AgACI ϕ = θ =0.1 ϕ = θ =0.8 ϕ = θ =0.9 ϕ = θ =0.95 ϕ = θ =0.99 Figure 15. Performance of various interval prediction methods on data simulated according to equation (3) with a Gaussian AR(1) (left) and MA(1) (right) noise of asymptotic variance 10 (see Appendix C.2). Results aggregated from 500 independent runs. Empirical standard errors are displayed. Adaptive Conformal Predictions for Time Series In addition, offline sequential SCP is very close to OSSCP. This is expected as a MA(1) process has very short memory, and the temporal dependence is thus small even for θ = 0.99. D.2.2. ASYMPTOTIC VARIANCE FIXED TO 1. We now fix the asymptotic variance of the noise to 1. The results are plotted in Figure 16. Note that this is an easier setting than previously, as the signal to noise ratio is higher for this asymptotic variance. Observations. Similarly to Figure 15, θ has little influence when the noise is a MA(1). On AR(1) and ARMA(1,1) noises (left and middle subplots), the patterns are similar. First, we observe again the improvement thanks to the online mode (empty squares versus solid ones), which increases when the dependence increases. Second, all the methods achieve validity or are significantly closer to achieving it than when the asymptotic variance is set to 10 (this is related to the high signal to noise ratio mentioned at the beginning of this section). Third, EnbPI V2 is valid for φ = θ ≤ 0.95 and provides the most efficient intervals for theses values. Nevertheless, its performances, as well as those of EnbPI, follow a clear trend (similar to that of Figure 6): when the dependence increases, the coverage decreases, as well as the length. EnbPI does not seem to be robust to the increasing temporal dependence in these experiments. 0.89 0.90 0.91 Coverage 8.0 8.5 9.0Averagemedianlength ARMA(1,1) noise 0.89 0.90 0.91 Coverage 7.5 8.0 8.5 9.0 AR(1) noise 0.895 0.900 0.905 0.910 Coverage 8.0 8.2 8.4 8.6 8.8 MA(1) noise OSSCP (adapted from Lei et al., 2018) Oﬄine SSCP (adapted from Lei et al., 2018) EnbPI (Xu & Xie, 2021) EnbPI V2 ACI (Gibbs & Cand`es, 2021), γ = 0.01 ACI (Gibbs & Cand`es, 2021), γ = 0.05 AgACI ϕ = θ =0.1 ϕ = θ =0.8 ϕ = θ =0.9 ϕ = θ =0.95 ϕ = θ =0.99 Figure 16. Performance of interval prediction methods on data simulated according to equation (3) with an ARMA(1,1) (left), AR(1) (center) and MA(1) (right) noise with a N (0, 1 1−φ 2 1−2φθ+θ2 ) innovation. Results aggregated from 500 independent runs. Empirical standard errors are displayed. D.3. Closer look at infinite intervals Table 2. Percentage of infinite intervals for ACI, on an ARMA(1,1) noise (first five rows), on an AR(1) noise (θ = 0, next five rows) and a MA(1) noise (φ = 0, last five rows). The central two columns present the percentage of infinite intervals, for γ = 0.01 and γ = 0.05. The last column represents the proportion of points for which γ = 0.05 predicts R and that are not covered for γ = 0.01. Noise parameters γ = 0.01 γ = 0.05 Intersection φ = θ = 0.1 0 1.12 53 out of 562 (9.43%) φ = θ = 0.8 0 2.76 263 out of 1381 (19.04%) φ = θ = 0.9 0 3.72 425 out of 1862 (22.83%) φ = θ = 0.95 0.03 4.45 514 out of 2224 (23.11%) φ = θ = 0.99 0.04 6.22 554 out of 3109 (17.82%) φ = 0.1 0 1 37 out of 500 (7.40%) φ = 0.8 0 2.75 212 out of 1373 (15.44%) φ = 0.9 0 3.24 359 out of 1622 (22.13%) φ = 0.95 0.03 4.32 488 out of 2160 (22.59%) φ = 0.99 0.06 6.15 560 out of 3073 (18.22%) θ = 0.1 0 1.03 38 out of 516 (7.36%) θ = 0.8 0 1.42 49 out of 710 (6.90%) θ = 0.9 0 1.54 47 out of 772 (6.09%) θ = 0.95 0 1.54 45 out of 770 (5.84%) θ = 0.99 0 1.56 53 out of 781 (6.79%) In this subsection, we investigate further the infite intervals generated by ACI for ARMA(1,1), AR(1) and MA(1) noise models. We report the results in Table 2. The central two columns present the percentage of infinite intervals, for γ = 0.01 and γ = 0.05. A first obvious observation is that the number of infinite intervals is orders of magnitude smaller for γ = 0.01 than for γ = 0.05. The last column represents the proportion of points for which γ = 0.05 predicts R and that are not covered for γ = 0.01. This suggests that for those intervals, predicting an infinite interval was somehow justified in the sense that the point was seemingly challenging to cover (as γ = 0.01 failed to cover). For example, in the first line (φ = θ = 0.1) we read that there are 562 points that result in infinite intervals for γ = 0.05, among which 53 lead to finite predictions for γ = 0.01 failing to cover on that point. This means only 9.43 % of 562 infinite intervals that can be considered as “somehow justified”. This analysis highlights that γ = 0.05 seem to predict more infinite Adaptive Conformal Predictions for Time Series intervals than necessary, to compensate for easy errors as explained in Section 2. D.4. Randomised, sequential and other splits. In Figure 17, we compare the sequential split strategy (dark markers) used in our experiments to the randomised version (clear markers), on online SCP. We observe that the intervals produced by the randomised version are significantly smaller than the sequential one, while covering slightly less. Another splitting strategy would consist in calibrating on the first points and training on the last ones. Up to our knowledge, this has not been used in practice. This way, we could hope to obtain a better model for the point prediction task. Nevertheless, we would be calibrating on really different data than the test ones. Thereby, the impact of this scheme regarding the interval prediction task performance is not straightforward. This is why we focus here on the sequential split, which is the most intuitive approach. Analysing further all of these effects theoretically or with extensive numerical experiments would be beneficial to the time series conformal prediction domain. 0.87 0.88 0.89 0.90 Coverage 11 12 13 14Averagemedianlength ARMA(1,1) noise 0.86 0.87 0.88 0.89 0.90 Coverage 11 12 13 14 AR(1) noise 0.900 0.901 0.902 0.903 0.904 Coverage 13.700 13.725 13.750 13.775 13.800 13.825 MA(1) noise Online Sequential SCP (OSSCP, adapted from Lei et al., 2018) Online Randomised SCP (Online SCP, adapted from Lei et al., 2018) ϕ = θ =0.1 ϕ = θ =0.8 ϕ = θ =0.9 ϕ = θ =0.95 ϕ = θ =0.99 Figure 17. Performance of interval prediction methods on data simulated according to equation (3) with a Gaussian ARMA(1,1) (left), AR(1) (middle) and MA(1) (right) noise of asymptotic variance 10 (see Appendix C.2). Randomised methods are displayed. Results aggregated from 500 independent runs. Empirical standard errors are displayed. E. Forecasting French electricity spot prices E.1. Details about the data set Table 3 presents an extract of the French electricity spot prices data set used in Section 6. In this table, 2 × 23 columns are hidden for clarity and space: the 24 prices of D − 7 and the 24 prices of D − 7 are used as variables. Table 3. Extract of the built data set, for French electricity spot price forecasting. Date and time Price Price D-1 Price D-7 For. cons. DOW 11/01/16 0PM 21.95 15.58 13.78 58800 Monday 11/01/16 1PM 20.04 19.05 13.44 57600 Monday ... ... ... ... ... ... 12/01/16 0PM 21.51 21.95 25.03 61600 Tuesday 12/01/16 1PM 19.81 20.04 24.42 59800 Tuesday ... ... ... ... ... ... 18/01/16 0PM 38.14 37.86 21.95 70400 Monday 18/01/16 1PM 35.66 34.60 20.04 69500 Monday ... ... ... ... ... ... Adaptive Conformal Predictions for Time Series E.2. Forecasting year 2019 In Figure 18 we observe that on January 25, 2019, the forecasts are very different from the actual values. Nevertheless, the prediction intervals manage to include these observations for almost all hours (except after 5 pm) and almost all methods (EnbPI does not include points earlier, starting at 11 am). 2019-01-21 2019-01-22 2019-01-23 2019-01-24 2019-01-25 2019-01-26 50 75 100 125Spotprice(€/MWh)Observed price Predicted price Predicted interval (a) OSSCP 2019-01-21 2019-01-22 2019-01-23 2019-01-24 2019-01-25 2019-01-26 50 75 100 125 150Spotprice(€/MWh)Observed price Predicted price Predicted interval (b) ACI with γ = 0.01 2019-01-21 2019-01-22 2019-01-23 2019-01-24 2019-01-25 2019-01-26 60 80 100 120Spotprice(€/MWh)Observed price Predicted price Predicted interval (c) EnbPI V2 2019-01-21 2019-01-22 2019-01-23 2019-01-24 2019-01-25 2019-01-26 50 75 100 125Spotprice(€/MWh)Observed price Predicted price Predicted interval (d) ACI with γ = 0.05 Figure 18. Representation of predicted intervals around point forecasts on the 25th of January of 2019. Adaptive Conformal Predictions for Time Series In Figure 19 we observe that the four algorithms suffer from an unbalanced coverage depending on the day-of-the-week (each algorithm in a different extent). That is, they cover more than 90% of the observations on Tuesdays to Fridays, but less than 90% on Mondays and week-ends (Saturdays and Sundays). Monda y T uesd a y W ednes da y Th ursda y F rida y Saturda y Sunda y 90 95Averagecoverage (a) OSSCP Monda y T uesd a y W ednes da y Th ursda y F rida y Saturda y Sunda y 90 95Averagecoverage (b) ACI with γ = 0.01 Monda y T uesd a y W ednes da y Th ursda y F rida y Saturda y Sunda y 90 95Averagecoverage (c) EnbPI V2 Monda y T uesd a y W ednes da y Th ursda y F rida y Saturda y Sunda y 90 95Averagecoverage (d) ACI with γ = 0.05 Monda y T uesd a y W ednes da y Th ursda y F rida y Saturda y Sunda y 90 95Averagecoverage (e) AgACI Figure 19. Coverage proportion during 2019 depending on the day-of-the-week.","libVersion":"0.3.2","langs":""}
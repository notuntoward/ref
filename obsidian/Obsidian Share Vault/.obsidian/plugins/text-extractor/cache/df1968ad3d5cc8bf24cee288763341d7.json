{"path":"lit/lit_sources/OpenAI24tokensWhatHowCount.pdf","text":"4 /1 0 /2 4 , 4 :5 0 P M W h a t a r e to k e n s a n d h o w to c o u n t th e m ? | O p e n A I H e lp C e n te r h ttp s ://h e lp .o p e n a i.c o m /e n /a r tic le s /4 9 3 6 8 5 6 - w h a t- a r e - to k e n s - a n d - h o w - to - c o u n t- th e m 1 /5 Search for articles... All Collections API General FAQ What are tokens and how to count them? What ar e t ok ens and how t o count them? Updated over a week ago Table of contents What ar e t ok ens? Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths: 1 token ~= 4 chars in English 1 token ~= Â¾ words 100 tokens ~= 75 words Or 1-2 sentence ~= 30 tokens 1 paragraph ~= 100 tokens 1,500 words ~= 2048 tokens To get additional context on how tokens stack up, consider this: W ayne Gretzkyâ€™s quote \" You miss 100% o f the shots y ou don't t ak e \" contains 11 tokens. OpenAIâ€™s charter contains 476 tokens. The transcript of the US Declar ation o f Independenc e contains 1,695 tokens. 4 /1 0 /2 4 , 4 :5 0 P M W h a t a r e to k e n s a n d h o w to c o u n t th e m ? | O p e n A I H e lp C e n te r h ttp s ://h e lp .o p e n a i.c o m /e n /a r tic le s /4 9 3 6 8 5 6 - w h a t- a r e - to k e n s - a n d - h o w - to - c o u n t- th e m 2 /5 How words are split into tokens is also language-dependent. For example â€˜CÃ³mo estÃ¡sâ€™ (â€˜ Ho w ar e y ou â€™ in Spanish) contains 5 tokens (for 10 chars). The higher token-to-char ratio can make it more expensive to implement the API for languages other than English. To further explore tokenization, you can use our interactive Tokenizer tool, which allows you to calculate the number of tokens and see how text is broken into tokens. Please note that the exact tokenization process varies between models. Newer models like GPT-3.5 and GPT-4 use a different tokenizer than previous models, and will produce different tokens for the same input text. Alternatively, if you'd like to tokenize text programmatically, use Tiktoken as a fast BPE tokenizer specifically used for OpenAI models. Tok en Limits Depending on the model used, requests can use up to 128,000 tokens shared between prompt and completion. Some models, like GPT-4 Turbo, have different limits on input and output tokens. There are often creative ways to solve problems within the limit, e.g. condensing your prompt, breaking the text into smaller pieces, etc. Tok en Pricing The API offers multiple model types at different price points. R equests to different models are priced differently. You can find details on token pricing here . Exploring t ok ens The API treats words according to their context in the corpus data. Models take the prompt, convert the input into a list of tokens, processes the prompt, and convert the predicted tokens back to the words we see in the response. What might appear as two identical words to us may be generated into different tokens depending on how they are structured within the text. Consider how the API generates token values for the word â€˜ r ed â€™ based on its context within the text: 4 /1 0 /2 4 , 4 :5 0 P M W h a t a r e to k e n s a n d h o w to c o u n t th e m ? | O p e n A I H e lp C e n te r h ttp s ://h e lp .o p e n a i.c o m /e n /a r tic le s /4 9 3 6 8 5 6 - w h a t- a r e - to k e n s - a n d - h o w - to - c o u n t- th e m 3 /5 In the first example above the token â€œ2266â€ for â€˜ redâ€™ includes a trailing space (Note, these are example token ID's for demonstration purposes). The token â€œ2296â€ for â€˜ R edâ€™ (with a leading space and starting with a capital letter) is different from the token â€œ2266â€ for â€˜ redâ€™ with a lowercase letter. 4 /1 0 /2 4 , 4 :5 0 P M W h a t a r e to k e n s a n d h o w to c o u n t th e m ? | O p e n A I H e lp C e n te r h ttp s ://h e lp .o p e n a i.c o m /e n /a r tic le s /4 9 3 6 8 5 6 - w h a t- a r e - to k e n s - a n d - h o w - to - c o u n t- th e m 4 /5 When â€˜R edâ€™ is used in the beginning of a sentence, the generated token does not include a leading space. The token â€œ7738â€ is different from the previous two examples of the word. Obser v ations: The more probable/frequent a token is, the lower the token number assigned to it: The token generated for the period is the same (â€œ13â€) in all 3 sentences. This is because, contextually, the period is used pretty similarly throughout the corpus data. The token generated for â€˜redâ€™ varies depending on its placement within the sentence: Lowercase in the middle of a sentence: â€˜ redâ€™ - (token: â€œ2266â€) Uppercase in the middle of a sentence: â€˜ R edâ€™ - (token: â€œ2297â€) Uppercase at the beginning of a sentence: â€˜R edâ€™ - (token: â€œ7738â€) R elat ed Ar ticles Using logit bias to alter token probability with the OpenAI API How do I check my token usage? GPT-4 Turbo in the OpenAI API GPTs vs Assistants 4 /1 0 /2 4 , 4 :5 0 P M W h a t a r e to k e n s a n d h o w to c o u n t th e m ? | O p e n A I H e lp C e n te r h ttp s ://h e lp .o p e n a i.c o m /e n /a r tic le s /4 9 3 6 8 5 6 - w h a t- a r e - to k e n s - a n d - h o w - to - c o u n t- th e m 5 /5 ChatGPT API D ALLÂ·E Service S tatus R etrieval Augmented Generation (RA G) and Semantic Search for GPTs Did this answer your question? ğŸ˜ğŸ˜ğŸ˜ƒ","libVersion":"0.3.1","langs":""}
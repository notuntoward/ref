{"path":"lit/lit_sources.backup/Gruhl21probMultivarNoveltyDet.pdf","text":"The Problem with Real-World Novelty Detection – Issues in Multivariate Probabilistic Models Christian Gruhl∗, Abdul Hannan∗, Zhixin Huang∗, Chandana Nivarthi∗, Stephan Vogt∗ ∗Universit¨at Kassel, Germany, Email: (cgruhl|a.hannan|zhixin.huang|chandana.nivarthi|stephan.vogt)@uni-kassel.de Abstract—Novelty and anomaly detection in real-world data streams are becoming more and more important for IoT, industry 4.0 and digital-twin applications. However, most of these algo- rithms are designed in-vitro and usually not very resilient against the failure behaviour of real-world systems, that is, minor system faults (e.g. a failing sensor, small damage, or ﬁrmware updates). In most scenarios, such a minor fault leads to a total failure of the detection engine, resulting either in the constant reporting of an anomaly or a total inability for further detection. In this article we investigate this problem in more detail and present simple approaches to circumvent them. Index Terms—Self-Improving System Integration, Self- Awareness, Novelty Detection, Anomaly Detection, Online Clus- tering, SISSY I. INTRODUCTION Novelty detection is of particular interest for the research conducted in Self-Improving System-Integration (SiSSY). It can be used as a building block to create self-aware systems [1] and is not only useful to detect threatening system conditions or malicious activities, but also to identify concept-drifts in the system itself or changes in its environments. Some changes might be more critical than others, but a reliable detection of either is surely an important property for future SiSSY Systems. However, there is a caveat. To the best of our knowledge, the vast majority of novelty and anomaly detection algorithms assume that the sensory information which they receive is reliable and they implicitly assume that the sensors will not fail. This can lead to the circumstance that in the case of sensor failures, the whole anomaly detection becomes unreliable. Either, because anomalies are masked by the failed sensor, or because the failed sensor is continuously identiﬁed as an anomaly, even if the system is completely healthy (except for the failed sensor). The terminology used in this article is based on the deﬁni- tions in [2], we assume that anomalies are single observations that differ from expected observations (which we call regulars) and that a novelty is a spatial and temporal accumulation of multiple anomalies. The ﬂow of the remainder of this article is structured as follows. • In Section II we discuss the problem description in more detail on the basis of a simple anomaly detection approach. • A probabilistic interpretation of the problem is then given in Section III. • To support our assumptions from the previous sections, we present real-world faults and novelties in wind turbine system in section IV. • Initial proposals for solutions are presented in Section V and related work in Section VI. • The article ends with a short conclusion in Section VII. II. INFORMAL PROBLEM DESCRIPTION Consider Fig. 1 that depicts an abstract system with three sensory inputs. The expected behaviour of the system is captured by a Gaussian Mixture Model (GMM) and its task is to identify anomalies and detect novelties (i.e. a novelty is a group of anomalies with temporal or spatial dependencies or proximity). In the ﬁrst image (Fig. 1a) all sensors work as expected. A novelty is present and reliably detected by the anomaly detector (here, we use an AlphaDetector [1]). The second image (Fig. 1b) shows the same situation, but this time the ﬁctional sensor that reports measurements for dimension x2 has faulted and reports a constant value of 0.0 (NB. the values could also be random). This time, the alpha detector is no longer able to detect the anomalies, the recall drops to 0.0 indicating that none of the actual anomalies is detected. We call this type of scenario fault masking. The last image (Fig. 1c again shows the same situation, but this time the sensor for x3 has failed with the result that anomalies are constantly reported. The precision of the AlphaDetector drops to 0.15 while the FPR is up to 1.0. We call this scenario hysteric since the detector constantly reports the detection of anomalies - a reliable detection is no longer possible. However, while the described fault masking leads to the problem that the novelty is no longer detectable, the system could still detect anomalies that deviate from the expected behaviour in x3, thus rendering the system not completely useless. For the hysteric scenario the system could be brought back to an operational state, by removing x3 from the detection model and only rely on x1 and x2 to detect future anomalies. To detect if a sensor has failed or if the system is ex- periencing a real anomalous situation we could investigate and montior the correlations between the different sensor dimensions (cf. Fig. 2), which is more thoroughly discussed later in this article. III. PROBABILISTIC INTERPRETATION Anomaly detection is usually based on observations from a ﬁxed countable set of D different sensors or other measurable or computable quantities. At time t, a sample in the form 204 2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C) 978-1-6654-4393-7/21/$31.00 ©2021 IEEE DOI 10.1109/ACSOS-C52956.2021.000552021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C) | 978-1-6654-4393-7/21/$31.00 ©2021 IEEE | DOI: 10.1109/ACSOS-C52956.2021.00055 Authorized licensed use limited to: University Kassel. Downloaded on December 06,2021 at 12:22:16 UTC from IEEE Xplore. Restrictions apply. (a) Normal operation, novelty present. (b) Faulted, novelty masked. (c) Faulted, hysteric detection of anomalies. Fig. 1: The top images shows the normal operation of an anomaly detection scheme with three dimensions. A novelty in the left corner is reliably detected. The center images shows the situation were the sensor for x2 has faulted and reports a steady value of 0.0. This leads to a masking of the present novelty so that it is no longer detectable - the detection scheme becomes unreliable. The bottom images shows a similar situation were sensor x3 has faulted, with a steady value (0.0) that does not lie in its operational range. The anomaly detection scheme fails here again, but instead of masking the real novelty it becomes hysteric, i.e. alarms are constantly raised even if the system is under normal operation. The blue ellipses are the components of a GMM that represents the normal, i.e. expected, behaviour of the system. Detected anomalies are in magenta, while observations identiﬁed as regular are in cyan. Fig. 2: Correlation of the three sensor dimensions of the examples in Fig. 1. of an observation vector xt =(x1(t),x2(t), ...xD(t))⊤ is present which can be seen as an instantiation of a random Variable X. For the sake of simplicity and readability xi(t) is noted as xi. In the following, we consider an anomaly to be an observation that seems improbable in the context of an upstream history of observations, which we refer to training data set D = {xτ } N τ =1. However, also due to the large number of possible normal instances, the probability of each individual instance may appear individually very unlikely. This can occur with large dimensions D, since many combinations of the individual elements in xt are imaginable or if elements in xt could be real and take any of an inﬁnite number of continuous values. Therefore, in practical anomaly detection procedures, observations are usually either compared to thresh- olds, intervals or some form of boundaries or evaluated based on probability densities. Since many of real world sensors actually measure continuous quantities and since in many applications multiple sensors are used, the goal is often to estimate a joint continuous multivariate probability density p(xt)= p(x1, ...,xD) or several conditional probability den- sities p(xi|x1, ...,xi−1,xi+1, ...,xD) where i ∈{1, ..., D}. In real world problems, there is usually a time de- pendence such that the probability of an observation p(xt|xt−1, xt−2, ...) ̸= p(xt). In the following, this will not be considered in detail and therefore, it will not be included in the notation without limiting the generality. A. Multivariate Context with Mutual Independence In the simplest case, single sensors are veriﬁed individ- ually with regard to their value ranges. This can be done, for example, on the basis of the 3σ method or on the basis of the interquartile range IQR. Since each sensor is evaluated individually it is assumed that there is no rela- tionship between the variables. In the case of independent sensors, the probability density can be modeled as the product p(xt)= p(x1)p(x2)...p(xD) of the marginal distributions. An advantage is that individual missing or faulty sensors can be easily removed in this model, avoiding hysterical detection. B. Multivariate Context with Mutual Dependence A disadvantage of the independence assumption is that the sensors are analyzed independently of each other, without exchange of information. In fact, most sensors are mutually 205 Authorized licensed use limited to: University Kassel. Downloaded on December 06,2021 at 12:22:16 UTC from IEEE Xplore. Restrictions apply. or single-sided conditioned on other sensors and can thus pro- vide an overall context for the individual measurements. For example, in the case of sensors of a wind turbine as depicted in Fig. 3, one would also expect a higher rotational speed or a higher torque in the case of high wind speed measurements. Similarly in Fig. 1a observing x1 will improve the expectation for x2 and vice versa. The novelty present in this ﬁgure would not have been detected under the independence assumption, since all contained observations lie in a plausible range of values, for each sensor x1, x2 and x3 individually. C. Fault Masking We formally deﬁne fault masking as the obfuscation of anomalies that appear to be regular when measured against their expected, estimated probability density, but are unlikely to originate from the initial distribution of regular observations due to their local accumulation, such as the repetition of a constant value. Many anomaly detection methods can be either seen as a de- terministic mapping f (xt): X ↦→{regular, anomaly}, where xt ∈X or a probabilistic mapping with f (xt): X ↦→ P, which assigns an anomaly probability to each sample. These detectors have the property that they produce consistent outputs, which means that once a sample is classiﬁed as regular, it is always classiﬁed as regular. A conspicuous, that means anomalous repetition of sensor values, e.g. if a single sensor delivers constant values, could therefore remain undetected in such a scenario. In some cases, considering the context can help here since potentially p(xi|xj = a) ̸= p(xi|xj = b). However, Fig. 1a shows a good example that this does not always help. A conditional distribution p(x2|x1), for example based on the GMM, would assigns different density values to x2 depending on x1. However, the constant value for x2 is in an unfortunate range that seems reasonable for smaller and larger values of x1.If x2 were constant at a slightly higher value, an anomaly would have been visible at larger values for x1. D. Hysteric Detection By hysterical detection we deﬁne that due to minor changes of the overall system, e.g. due to the defect of a single sensor, all or at least a disproportionate number of further observations are classiﬁed as anomaly. IV. REAL WORLD NOVELTIES AND FAULTS In the real world, all machines are modular, and components are connected. Each model contains several sensors, and the measured values of some sensors are usually linearly correlated. When the novelty is detected in one sensor, the experts need to diagnose the impact on the system. We will summarize the occurred novelties and faults through a real wind turbine data set 1. This industry data set is collected from 5 wind turbines composed of the following two parts: • NWP data:numeric weather prediction (NWP) data with 40 24-hour-ahead features. 1https://opendata.edp.com/explore/dataset/ Fig. 3: The physical presence of sensors across multiple com- ponents in turbines. 46 sensors are distributed in components inside. • Sensor data: measurement data from 46 sensors located in the different turbine components (see Fig. 3). Like the NWP data, the sensor data ranges from January 1. 2016 to December 31, 2017, with a 10-minute resolution. According to the observation of each sensor data, the following four categories of novelty are proposed: • Sensor Failure: Only one sensor shows abnormal data, and other sensors’ behavior is normal. That indicates only one sensor is broken and not required to shut down the whole system. Besides, we can refer to the measurement value of other sensors which have high correlations with the broken sensor as an emergency solution. • Concept Drift: When devices or sensors are replaced or sensor parameters are re-adjusted. The measured value may occur concept shift. Fig. 4b illustrates the concept drift of the temperature sensor in the controller compo- nent after the generator was reinstalled on August 20, 2017. The anomaly detector needs to be updated to adapt to the new measurement data. • Component Failure: Sensors with low correlation in one component occur anomalies simultaneously (see Fig. 4c), while other models work, that implicit the component is damaged. The low correlated sensors represent that the measuring signal may not have a front-to-back connection relationship. If the broken component is connected to others might result in system failure. • System Failure: Similar to component failure, but the anomalies are detected from multiple components by anomaly detectors. In the data set, we observed a sys- tem failure on November 2, 2016. All sensor values in different components except in ambient descended to zero in a short time. There may be two reasons for the system failure: (1) One component’s failure results in system failure. (2) Multiple components were damaged at the same time. When many sensors appear abnormal in the same time interval, it is often difﬁcult to trace the root 206 Authorized licensed use limited to: University Kassel. Downloaded on December 06,2021 at 12:22:16 UTC from IEEE Xplore. Restrictions apply. (a) Ground truth is the true value of the sensor. A single temperature sensor occurs a sharp peak in the generator component, but other sensors work regularly. (b) One sensor in the controller component occurs concept drift after reinstalling the generator. There is a long-term deviation between the predicted value and the actual value. (c) All sensors in the ambient component have sensor failures. The actual sensor values suddenly go to 0. Fig. 4: Examples of different categories of novelty in the wind turbine data set. cause. Experts can infer the cause of the error from where the anomalies were ﬁrst detected because device signals are transmitted in various modules in a sequence. V. SOLUTION CANDIDATES A. Anomaly Detector for Single Sensor In this subsection, the anomaly detector uses a Bayesian dropout approximation neural network (BNN) to predict the Fig. 5: Anomaly Detector for Single Sensor time series data of a single sensor. As shown in the Fig. 5, the speciﬁc process is forecasting the sensor data at the next time point by continuously introducing new weather data. BNN repeats forecast of each timestamp based on the same NWP data 10 times, then the mean and standard deviation of the set of predicted values will be calculated. The actual value of the sensor is subtracted from the predicted value to obtain the residual sequence. The three times standard deviation of the prediction will be used as the conﬁdence interval. When the absolute value of the residual is greater than three times of standard deviation, it indicates that the sensor is abnormal at this time point. Suppose many anomalies that occurred in one sensor during a day indicate novel events. The system will query experts to synthesize the results of all anomaly detectors to determine which category the novelty belongs to and respond accordingly. Fig. 4 shows how to detect different categories of novelty based on this proposed solution. B. Anomaly Detection on each Sensor Individually As we have already discussed that some of major problems in anomaly detection is that sometimes the whole system breaks if one of the sensors stops working or sometimes it does not detect an anomaly. This can be easily explained via the Fig. 6 also explained three dimensionally in I. As you can see in the image there are 4 kinds of anomalies here. Failure1 is the kind of anomaly when sensor 1 fails in such a way that it stops giving value. Failure 3 is also a failure of sensor 1 where sensor keep repeating its last value instead of failing. Same can be explained for Failure 2 and Failure 4. Failure 2 corresponds to sensor 2 failure where sensor 2 stops providing values while failure 4 is the anomaly where sensor 2 keep repeating its last value. There are very few basic problems if we simply apply Gaus- sian mixture model. We can see that two major problems that we discussed can be seen here in the Fig. 7. One is that it failed to recognize anomaly samples that were just being repeated. Secondly, imagine if sensor 2 is not an important sensor and if it breaks then the whole system should not break and should not detect as anomaly. We can simply avoid these problems as shown in Fig. 10 just by using anomaly detectors independently on each sensor and we can also give weight to 207 Authorized licensed use limited to: University Kassel. Downloaded on December 06,2021 at 12:22:16 UTC from IEEE Xplore. Restrictions apply. Fig. 6: Plot of sample along two sensor which are independent and Gaussian distributed. Different kinds of anomalies. Fig. 7: Anomaly detection through Gaussian mixture model. Magenta colour shows detected anomalies. each sensor so that whole system should not raise alarm or break if sensor is not that important. This can be easily explained in the Fig. 8 and Fig. 9. We can see we have solved both of the problems. In Fig. 9 we can see different kinds of anomalies are detected. It has detected the anomalies which are inside the mixture model because sensor broke and it kept repeating the same value itself. All anomalies are detected because of equal weights to the sensors. Now look in Fig 9 failure 2 and failure 4 are not detected simply because we have given sensor 2 less weight as compared to sensor 1 it will not detect it as anomaly and whole system will not break just because one sensor broke. VI. RELATED WORK Many machine learning approaches fail silently [3]. Kam- merer et al. [4] apply two anomaly detection methods in two real industrial scenarios: (1) Comparing the current signal with the predeﬁned pattern by distance. This method can only be applied to machines that generate ﬁxed-period signals. (2) The prediction model is updated in real-time based on historical Fig. 8: Anomaly detection through independent sensors. Ma- genta colour shows detected anomalies. Equal weights for sensor 1 and sensor 2 are used 0.5 each. Fig. 9: Anomaly detection through independent sensors. Ma- genta colour shows detected anomalies. Unequal weights for sensor 1 and sensor 2 are used, 0.6 and 0.4 respectively. data to predict anomalies. However, this method ignores concept drift, and the locations of failures are not traceable. D¨IOT [5] is a self-learning distributed system for security monitoring of multiple IoT devices. This device-type-speciﬁc anomaly detection approach can achieve accurate detection of anomalies while generating almost no false alarms. Hayes et al. [6] recommends utilizing spatial, temporal or semantic context while detecting anomalies in real-time sensor data, as point anomaly detection without context in complex data, results in false positives. Zhou et al. [7] propose RDA (Robust Deep Autoencoders) which isolate the noise and outliers in input and autoencoder is trained after isolation. This way we get faithful representation on noise-free data and the anomalies can be isolated too. VII. CONCLUSION In this article we call attention to certain problems that arise with novelty and anomaly detection task that are applied in 208 Authorized licensed use limited to: University Kassel. Downloaded on December 06,2021 at 12:22:16 UTC from IEEE Xplore. Restrictions apply. Sensors Sensor 1 Sensor k Sensor n Weight each Sensor Perform 3 sigma anomlay Check for sensor repeating values For each Sensor Vote anomly detection based on weight. Fig. 10: Flow chart for anomaly detection on each sensor individually. real world systems. Namely, that the detection performance is considerably reduces if the input data is no longer reliable. We present a probabilistic interpretation of the problem as well as an analysis of a real world application based in the ﬁeld of renewable wind-energy turbines. The problem will become • German Federal Ministry for Economic Affairs and Energy (BMWi) within the project “Verbundvorhaben: Digital-Twin-Solar - Nachweis der Machbarkeit und more important in future SiSSY systems and our next research steps will include to work out a detailed research agenda and a simulation analysis of the behaviour of existing novelty and anomaly detection approaches if the input data is not longer reliable. ACKNOWLEDGEMENTS This research has been partly funded by the following research grants: • German Ministry for Education and Research (BMBF) within the project “Ein Organic-Computing-basierter Ansatz zur Sicherstellung und Verbesserung der Re- silienz in technischen und IKT-Systemen (OCTIKT)” (01IS18064C) Demonstration des Nutzens eines ’Digitalen Zwillings’ im Bereich der Batterie- und PV-Systemtechnik, Teil- vorhaben: Maschinelles Lernen f¨ur Digitale Zwillinge im Bereich der Batterie- und PV-Systemtechnik (Digital- Twin-Solar)” (03EI6024E) . REFERENCES [1] C. Gruhl, B. Sick, A. Wacker, S. Tomforde, and J. H¨ahner, “A building block for awareness in technical systems: Online novelty detection and reaction with an application in intrusion detection,” in IEEE iCAST, pp. 194–200, IEEE, 2015. [2] C. Gruhl, B. Sick, and S. Tomforde, “Novelty detection in continuously changing environments,” Future Generation Computer Systems, vol. 114, pp. 138–154, 2021. [3] S. Rabanser, S. G¨unnemann, and Z. Lipton, “Failing loudly: An empirical study of methods for detecting dataset shift,” in Advances in Neural Information Processing Systems, pp. 1396–1408, 2019. [4] K. Kammerer, B. Hoppenstedt, R. Pryss, S. St¨okler, J. Allgaier, and M. Reichert, “Anomaly detections for manufacturing systems based on sensor data—insights into two challenging real-world production settings,” Sensors, vol. 19, no. 24, p. 5370, 2019. [5] T. D. Nguyen, S. Marchal, M. Miettinen, H. Fereidooni, N. Asokan, and A.-R. Sadeghi, “D¨ıot: A federated self-learning anomaly detection system for iot,” in 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), pp. 756–767, IEEE, 2019. [6] M. A. Hayes and M. A. Capretz, “Contextual anomaly detection in big sensor data,” in 2014 IEEE International Congress on Big Data, IEEE, 2014. [7] C. Zhou and R. C. Paffenroth, “Anomaly detection with robust deep autoencoders,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017. 209 Authorized licensed use limited to: University Kassel. Downloaded on December 06,2021 at 12:22:16 UTC from IEEE Xplore. Restrictions apply.","libVersion":"0.3.2","langs":""}
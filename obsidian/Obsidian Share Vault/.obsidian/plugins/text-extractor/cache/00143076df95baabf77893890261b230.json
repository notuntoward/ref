{"path":"lit/lit_notes_OLD_PARTIAL/Li22DogNosePrint.pdf","text":"Dog nose print matching with dual global descriptor based on Contrastive Learning Bin Li Yunnan University flyingsheep@mail.ynu.edu.cn Zhongan Wang ShanghaiTech University wangzha@shanghaitech.edu.cn Nan Wu Yunnan University deepfaker@mail.ynu.edu.cn Shuai Shi ShanghaiTech University shishuai@shanghaitech.edu.cn Qijun Ma Zhejiang A&F University skypow2012@gmail.com Abstract Recent studies in biometric-based identiﬁcation tasks have shown that deep learning methods can achieve bet- ter performance. These methods generally extract the global features as descriptor to represent the original im- age. Nonetheless, it does not perform well for biomet- ric identiﬁcation under ﬁne-grained tasks. The main rea- son is that the single image descriptor contains insufﬁ- cient information to represent image. In this paper, we present a dual global descriptor model, which combines multiple global descriptors to exploit multi level image fea- tures. Moreover, we utilize a contrastive loss to enlarge the distance between image representations of confusing classes. The proposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet Biometric Challenge. The source code and trained models are publicly available at: https://github.com/ﬂyingsheepbin/pet-biometrics 1. Introduction Vision-based pattern identiﬁcation (such as the face, ﬁn- gerprint, iris, etc.) has been successfully applied in human biometrics for a long history, however, there are few works that transfer these technologies to pet biometrics. Dog nose print matching aims to identify whether two dog nose print images belong to the same dog, as shown in Figure 1. Ideally, the image of a dog’s nose print is clearer and has a higher resolution, allowing you to see the texture of the image in detail. In practice, however, images of a dog’s nose tend to be of low resolution, and the motion of a moving object can be blurred. In this paper, inspired by contrastive learning [5] and image retrieval tasks, we develop a method to solve these problems. Firstly, we solve the image blurred problem by contrasting dog nose print images with positive images and negative images, which helps the model to learn the similar- ity between positive pair images and differ the unlikely part between negative pair images. On the other hand, to solve the small image size that exists in the practice environment, we augmented the images during the training stage by re- sizing images to a smaller size with a probability. Extensive experiments show that our method can solve the above two problems. The contributions of our proposed framework are as fol- lows: • To solve the dog nose print matching task, we designed a new model with dual global descriptor, which can exploit the multi-scaled features of image. • In the dog nose print matching task, we ﬁrstly ap- ply supervised contrastive learning to identify undis- tinguished sample. • Aiming at the problem of low image resolution, we apply the contrastive learning method and resetting im- age size method to alleviate the problem of blurred im- ages effectively. 2. Related Work Animals can be distinguished by using physical charac- teristics, such as how humans have distinct ﬁngerprint and iris patterns. In the past, animals were identiﬁed individu- ally by animal tags. Not only does this cause mental and physical damage to the animal, it is also prone to dupli- cation and forgery. Hence, biometric-based identiﬁcation technology become more approved as alternatives to indi- vidual animal identiﬁcation method. Handcrafted feature- based methods were utilized for identiﬁcation. Kumar et 1arXiv:2206.00580v1 [cs.CV] 1 Jun 2022Figure 1. Example of nose print by dog Lian Lian. al. [6] proposed a method to identify cattle using face im- ages. The method extracted face features by using conven- tional machine learning algorithm. Chen et al. [3] intro- duced an identiﬁcation method by using nose-print images with support vector method(SVM). Chakraborty et al. [2] used cropped muzzle images of pigs for breed identiﬁcation. However, handcrafted feature-based approaches are more dependent on the quality of the dataset and prior knowledge, and cannot maintain high performance under a complicated environment. Recently, with the development of deep learn- ing in computer vision, deep learning methods were widely used in image classiﬁcation and object detection. Conse- quently, biometric-based identiﬁcation based on deep learn- ing has gradually attracted attention. Deb et al. [4] intro- duced the PrimNet, a face recognition system, which can obtain images of three primates in the wild. Wang et al. [9] presented a residual CNN network to for gender classiﬁca- tion by using the facial features of pandas. Han [1] proposed a DNNet method to an individual dog’s nose-print pattern. 3. Model Our model with dual global descriptor, which is based on contrast learning, is inspired by the combination of multiple global descriptors (CGD). The CGD framework has been demonstrated that it is ﬂexible and expandable by global descriptors, backbones, losses and datasets. As illustrated in Figure 2, the architecture of our model is primarily di- vided into backbone and head. On the backbone side, it is straightforward to select the pretrained visual model as the feature extractor. On the head side, we used two types of the global descriptors, including SPoC descriptor based on sum-pooled convolutional features and MAC descriptor based on maximum activations of convolutions. Given an input of 3D tensor V C×H×W , we made Vc be the set of H × W activations for feature maps, where C is the number of feature maps and c ∈ {1, ..., C}. The vector f as output, which is produced by global descriptor, can be generalized as follows: f = [f1, ..., fc, ..., fC] T , fc = ( 1 |Vc| ∑ v∈Vc vpc ) 1 pc , (1) where pc = 1,pc → ∞ are respectively deﬁned as SPoC and MAC. For the output f , it ﬁrstly is generalized by normalization through the l2-normalization layer and dimensionality re- duction through the full-connect layer. Finally, we concate- nated the feature maps and trained it with the contrastive loss. In our model, we choosed the supervised contrastive loss as the contrastive loss function, while it is demonstrated that the gradient of supervised contrastive loss function en- courages learning from hard positives and hard negatives. For N images, the loss function takes the following form: L = − N∑ i=1 log exp(zT i · z+/τ ) ∑N j=1 exp(zT i · zj/τ ) , (2) where zi denotes the anchor, τ ∈ R+ is s a scalar tempera- ture parameter and z+ is the number of the positives. 4. Experiments 4.1. Datasets The pet biometric dataset is collected by Ant Group, which consists of a training dataset, a validating dataset, and a testing dataset. The training set consisted of 6,000 dogs with 20,000 photographs of their nose prints, and each dog had at least two photographs. In the validating dataset, there are 2,000 pairs of dog nose print images (1,000 pair positives and 1,000 pair negatives), which were selected by a new dog print set that consists of 2,694 images. The test- ing dataset has 2,000 pairs of dog nose print images, which were selected by a new dog print set that consists of 4,000 images. The dataset format of training, validating, and test- ing can be seen in Table 1 and Table 2. 4.2. Experiment settings The training process is divided into two stages. Stage one is training stage, in which input images are resized to 60 × 60 pixels with probability 0.5 and then resized to 224 × 224 pixels. During stage two, we just ﬁnetune our model with more image augmented strategies such as ran- dom brightness contrast and motion blur. To optimize our model, in stage one, we utilized the Adam optimizer with cosine weight decay scheduler for 30 epochs and tmax is set to 29. In stage two, we ﬁnetune our mdoel for 20 epochs with smaller learning rate, which is set to 3e-5 and tmax is 19. The embedding feature dimension is 512. In addition, we use EMA and AMP training strategies. 2 Figure 2. The architecture of our model with dual global descriptor based on Contrastive Learning. Table 1. Training dataset format. Dog ID Nose Print Image 0 x1FVNAVSRni...ARAD.jpg 0 –1WCesjS6C...ARAD.jpg 1 EJoldD9BQa-...ARAD.jpg 1 -27kQ7i7TMO...ARAD.jpg ... ... 5999 A*H4FeQZ5TNV...AAAQ.jpg 5999 dTUY37UVRhea...ARAD.jpg 5999 A*hxoTSL4Np6...AAAQ.jpg Table 2. Validata and test dataset format. imageA imageB A*p...AAQ.jpg A*F...1AA.jpg A*Y...AAQ.jpg A*W...1AA.jpg ... ... A*q...1AA.jpg A*g...1AA.jpg 4.3. Result of stage one In the ﬁrst stage, the model is trained on traing dataset and tested on validate dataset. At ﬁrst, we tried our model with different backbone networks, such as EfﬁcientNet-B1- 5 NS [10], Swin-B [7]. The results are shown in Table 3. We set this model with different batch sizes since CUDA memory is limited. We can see that large models perform better than small models. In table 3, the Swin-base model achieves the best performance. Secondly, based on the Swin-base model, we tried differ- ent tricks to improve the performance of our model, the re- sults are shown in Table5. The base model is a simple Swin- base model with a batch size of 32. Inspired by contrastive learning beneﬁts from the large batch sizes, we enlarge the batch size from 32 to 64. We can see that the AU Cval im- proved from 0.8393 to 0.8447. Since our model is devel- Table 3. The performance of our result varies from different back- bone. backbone image size batch size AU Cval EfﬁcientNet-B1 NS 224 128 0.8479 EfﬁcientNet-B2 NS 224 128 0.8460 EfﬁcientNet-B3 NS 224 128 0.8489 EfﬁcientNet-B4 NS 224 96 0.8540 EfﬁcientNet-B5 NS 224 64 0.8599 Swin-B 224 88 0.8730 Swin-B 384 20 0.8408 oped by a CGD model, which was originally designed to solve image retrieval problems, we believe that the classiﬁ- cation module would harm the performance of the dog nose print match task. We enlarge the weight of loss Lcr, and the result shows that ”Swin-base + 10*cr” is much higher than ”Swin-base”, which improved from 0.8477 to 0.8529. Taking a step further, we removed the classiﬁcation module and enlarged the batch size, and the performance of ”Swin- base w/o entropy” is achieved to 0.8630. Moreover, since the validating images are available during the training stage, we ﬁrst trained a ”Swin-base w/o entry” model, then infer the validating dataset and added the top 500 pair images to a new training stage. The result improved from 0.8630 to 0.8700. Next, the fully connected(FC) layer after GD may lose some information, we removed these FC layers in the infer stage. We can see that the performance of our model improved slightly. As shown in Figure 3 and Figure 4, we can see that the width of training images is normally larger than 500, while the width of validating images is smaller than 200. There exists a big image size gap between the training dataset and validating dataset. So we resize the training images to one of [50, 60, 70, 80] with a probability of 0.45. The result shows that the resized image size model “Swin-base w/o entropy + pseudo 500 w/o fc resize” improved from 0.8726 to 0.8785. At last, the original augmentation that existed in CGD may not be suitable for the dog nose print match 3 Table 4. Ablation results. “w/o” represent without, “entropy” represents a classiﬁcation module, and “pseudo 500” means that the most similar 500 pair validating images are labeled with the pseudo label, “fc” represents fully connected layer, “resize” means resize images with a probability, “aug” represents image augmentation. model batch size AU Cval Swin-base 32 0.8393 Swin-base 64 0.8477 +0.0084 Swin-base + 10*cr 64 0.8529 +0.0136 Swin-base w/o entropy 88 0.8630 +0.0237 Swin-base w/o entropy + pseudo 500 88 0.8700 +0.0307 Swin-base w/o entropy + pseudo 500 w/o fc 88 0.8726 +0.0333 Swin-base w/o entropy + pseudo 500 w/o fc resize 88 0.8785 +0.0392 Swin-base w/o entropy + pseudo 500 w/o fc resize + aug 88 0.8853 +0.0460 Figure 3. Image width frequency statistics of training dataset. Figure 4. Image width frequency statistics of testing dataset. task, so we removed the ﬂip and crop augmentation and only keep “resize” augmentation. The result also showed a signiﬁcant improvement. After applying these tricks in Swin-base, we also apply these tricks to some larger backbone models. Combined Table 5. Fusing multiple models. model image size AU Cval Swin-B 224 0.8853 dm-nfnet-f3 224 0.8915 Swin-B 384 0.8900 EffNetV2-L 224 0.8810 fusion - 0.9025 with the auto mix precision(AMP), some large backbones such as Swin384 can be trained by a larger batch size. In this setting, the dm-nfnet-f3 achieved 0.8915 on AU Cval, the Swin384, effv2-larger also achieved 0.8900 and 0.8810 on AU Cval, respectively. Last, we simply average the sim- ilarity of Swin-B(224), Swin-B(384), EffNetV2-L [8], and dm-nfnet-f3, and the result achieved to 0.9025. 4.4. Result of stage two In the second stage, the effect of the test dataset inferred directly from the model trained in the ﬁrst stage was rela- tively poor, and the online score was far lower than that in the ﬁrst stage. Our online scores compared to other teams’ online scores showed that their scores decreased even more. Our method outperformed many of the methods in the ﬁrst stage, and we could see that our model performs better in generalization. In order to continued to enhance the per- formance, the following work involved data augmentation, without pseudo labels, test time augmentation, and models fusion. Through the analysis of the data, we found that com- pared with the ﬁrst stage data, there were more blurred and cropped images in the second stage. We combined a vari- ety of augmentation schemes, ﬁrst carried out two different scaling according to the probability, and cut into the same 4 Table 6. Tricks ablation experiments tricks AU Ctest Efﬁcientnet-B7 NS 0.8450 Efﬁcientnet-B7 NS + data augmentation 0.8600 Efﬁcientnet-B7 NS w/o pseudo labels 0.8624 Efﬁcientnet-B7 NS + TTA 0.8460 ﬁnal Efﬁcientent-B7 NS 0.8804 fusion four models with tricks 0.8881 size of the image to facilitate input into the network. Subse- quently image compression and blur operation, enhance im- age contrast and so on. The experimental results are shown in Table 6. In the ﬁrst stage, part of the validata was used as pseudo labels. In the second stage, considering that the validata is a new dataset, it may have a bad inﬂuence to use the testset as pseudo labels. Therefore the previous pseudo labels were eliminated and the models were to retrain 20 epochs on the basis of the previously trained model. It was discovered that efﬁcientnet-b7 NS was considerably improved, while Swin-base was not improved. Our preliminary guess was that the Swin-Transformer need to be trained longer. Due to the limited time, we didn’t attempt more backbones, but only removed the pseudo labels on efﬁcientnet-b7 NS. The experimental results are shown in Table 6. With effective data augmentation, we tried a similar aug- mentation on the testset that using two scale scalings, ran- dom clipping, and random contrast augmentation. Image compression and blur were left out because some images were inherently fuzzy, and there were only slight differ- ences between dog breeds. As it was a probabilistic selec- tive augmentation, the amplitude of improvement of each experimental result was different, but each model could be steadily improved, as shown in Table 6. 5. Conclusion This paper, based on contrastive learning, proposes a new dog nose recognition network to extract the texture fea- tures of dog noses and then perform pairwise matching. Our approach is the ﬁrst attempt to match dog species by con- trastive learning. The method aims to obtain discriminative features and discriminate against unknown classes. Abla- tion experiments show that using only a single contrastive loss objective function is more stable than the combined performance ratio of classiﬁcation loss and contrast loss, which further reﬂects the superiority of contrastive learning in handling unknown class matching tasks. In this compe- tition, our model has a better performance in the real test scene, and the robustness is better than others. But the current test set AUC is 88.81% and there is space for im- provement. Directions that might be useful: try to extract features from key points of angle length instead of convo- lutional network features, try 3D reconstruction, try back- ground culling, try to explore the effect of color changes on the results, try to explore the reason why the AUC decreased after alignment by key points. References [1] Meo Vincent Caya, Emmanuel D Arturo, and Chezjon Q Bautista. Dog identiﬁcation system using nose print bio- metrics. In 2021 IEEE 13th International Conference on Humanoid, Nanotechnology, Information Technology, Com- munication and Control, Environment, and Management (HNICEM), pages 1–6. IEEE, 2021. 2 [2] Shoubhik Chakraborty, Kannan Karthik, and Santanu Banik. Investigation on the muzzle of a pig as a biometric for breed identiﬁcation. In Proceedings of 3rd International Confer- ence on Computer Vision and Image Processing, pages 71– 83. Springer, 2020. 2 [3] Yu-Chen Chen, Shintami C Hidayati, Wen-Huang Cheng, Min-Chun Hu, and Kai-Lung Hua. Locality constrained sparse representation for cat recognition. In Interna- tional Conference on Multimedia Modeling, pages 140–151. Springer, 2016. 2 [4] Debayan Deb, Susan Wiper, Sixue Gong, Yichun Shi, Cori Tymoszek, Alison Fletcher, and Anil K Jain. Face recogni- tion: Primates in the wild. In 2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS), pages 1–10. IEEE, 2018. 2 [5] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020. 1 [6] Santosh Kumar, Shrikant Tiwari, and Sanjay Kumar Singh. Face recognition of cattle: can it be done? Proceedings of the National Academy of Sciences, India Section A: Physical Sciences, 86(2):137–148, 2016. 2 [7] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012–10022, 2021. 3 [8] Mingxing Tan and Quoc Le. Efﬁcientnetv2: Smaller models and faster training. In International Conference on Machine Learning, pages 10096–10106. PMLR, 2021. 4 [9] Hongnian Wang, Han Su, Peng Chen, Rong Hou, Zhihe Zhang, and Weiyi Xie. Learning deep features for giant panda gender classiﬁcation using face images. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision Workshops, pages 0–0, 2019. 2 [10] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687– 10698, 2020. 3 5","libVersion":"0.3.2","langs":""}
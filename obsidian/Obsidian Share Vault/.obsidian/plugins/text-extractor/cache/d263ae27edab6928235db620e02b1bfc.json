{"path":"lit/lit_sources/Draelos20avgPrecAUC.pdf","text":"7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 1 /14 G lass Box M a c h i n e L e a r n i n g a n d M e d i c i n e , b y R a c h e l D r a e l o s , M D , P h D M ACHINE LEARNING T he Complete Guide to AUC and A verage Precision: Simulations and V isualizations D ate: July 14, 2020 A uthor: Rachel Draelos, MD, PhD T his post o ﬀ ers the clearest explanation on the w eb for how the popular metrics AUC (AUROC) and av erage precision can be used to understand how a classiﬁ er performs on balanced data, with the next post focusing on imbalanced data. This post includes numerous simulations and AUROC/av erage precision plots for classiﬁ ers with diﬀ erent properties. All code to replicate the plots and simulations is provided on GitHub . F irst, here is a brief intro to AUROC and av erage precision: A UROC: Area Under the Receiv er Operating Characteristic 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 2 /14 T he AUROC indicates whether your model can correctly rank examples. The AUROC is the probability that a randomly selected positiv e example has a higher predicted probability of being positiv e than a randomly selected negativ e example. The AUROC is calculated as the area underneath a curv e that measures the trade o ﬀ betw een true positiv e rate (TPR) and false positiv e rate (FPR) at diﬀ erent decision thresholds d: A random classiﬁ er (e.g. a coin toss) has an AUROC of 0.5, while a perfect classiﬁ er has an AUROC of 1.0. For more details about the AUROC, see this post . A v erage Precision (aka AUPRC): Area Under the Precision-Recall Curv e A v erage precision indicates whether your model can correctly identify all the positiv e examples without accidentally marking too many negativ e examples as positiv e. Thus, av erage precision is high when your model can correctly handle positiv es. A v erage precision is calculated as the area under a curv e that measures the trade o ﬀ betw een precision and recall at diﬀ erent decision thresholds: 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 3 /14 A random classiﬁ er (e.g. a coin toss) has an av erage precision equal to the percentage of positiv es in the class, e.g. 0.12 if there are 12% positiv e examples in the class. A perfect classiﬁ er has an av erage precision of 1.0. For more details about av erage precision, see this post . S imulation Setup I n the simulations, I generate a ground truth v ector indicating the true label for a series of examples (e.g. [0,0,1] for three examples that are [negativ e, negativ e, positiv e]) and a prediction v ector indicating a hypothetical model’s predictions on that series of examples (e.g. [0.1,0.25,0.99]). I generate the ground truth and predictions so that there are diﬀ ering numbers of true positiv e, false positiv es, true negativ es, and false negativ es: 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 4 /14 F or a more detailed review of confusion matrices, see this post . T he simulated model results in this post w ere created relativ e to an assumed decision threshold of 0.5. For example, to create true positiv es from an assumed decision threshold of 0.5, I uniformly sampled prediction v alues betw een 0.5001 and 1.0, and marked the ground truth as 1 for each sampled v alue. Note that a decision threshold of 0.5 w as used only for simulating the ground truth and prediction v ectors. The AUROC and av erage precision are calculated with a sliding decision threshold, following their de ﬁ nitions. A ll code to replicate the results and ﬁ gures in this post can be found on GitHub . S imulations for Balanced Data L et’s look at plots of the AUROC and av erage precision on a balanced data set, i.e. a data set for which the number of actual positiv es and the number of actual negativ es is equal. R andom Model 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 5 /14 I n the ﬁ gure abov e for “ModelBalanced” the left plot (red) shows the receiv er operating characteristic (ROC), with the title reporting the area under the ROC, or AUROC, in this case 0.48. T he right plot (blue) shows the precision-recall curv e, with the title reporting the area under the precision recall curv e (AUPRC) calculated using the av erage precision method. H ere, the AUROC is ~0.5, the baseline, and the av erage precision is also ~0.5, the baseline since the fraction of positiv es is 0.50. The v alues are not exactly 0.500 because of the random uniform sampling involv ed in the simulation. “ModelBalanced” means that the model isn’t skew ed tow ards making positiv e or negativ e predictions, and also isn’t skew ed tow ards making correct predictions. In other words, this is a random, useless model equiv alent to a coin toss. T he line below the plot title reports the number of true positiv es (tp), false negativ es (fn), true negativ es (tn), and false positiv es (fp) at a decision threshold of 0.5 (d=0.5): “at d=0.5, tp=100, fn = 100, tn = 100, fp = 100.” This point is also plo \u0000 ed on the curv e as the dot labeled “d = 0.5” for “decision threshold = 0.5.” A dditionally, I show the points on the curv es where the decision threshold is equal to 0.9, 0.5, and 0.1. These points are labeled d = 0.9, d = 0.5, and d = 0.1. W e can see that the decision threshold goes from 1 to 0 as w e sw eep from left to right. The curv es themselv es are relativ ely smooth because they w ere created using numerous decision thresholds; only 3 decision thresholds are explicitly shown as dots to emphasize properties of the curv es. B ad Model Predictions 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 6 /14 I n “ModelPredBad” the model is skew ed tow ards making bad predictions, i.e. it tends to get the wrong answ er and has high false negativ es and high false positiv es. Notice that here, the AUROC and av erage precision are both below the baseline. This illustrates an interesting fact about classiﬁ ers – in practice if you hav e a model that is “expertly bad” then you can just ﬂ ip the classiﬁ cation decision and get a model that is good. If w e ﬂ ipped all of the classiﬁ cation decisions here, w e could get a model with 1.0 – 0.11 = 0.89 AUROC. That is why the baseline for AUROC is alw ays 0.5; if w e hav e a classiﬁ er with AUROC below 0.5 w e ﬂ ip its decisions and get a be \u0000 er classiﬁ er with an AUROC betw een 0.5 and 1.0. T he “elbow” in the AUROC and av erage precision plots at d = 0.5 is due to the w ay the simulated results w ere created relativ e to a decision threshold of d = 0.5. G ood Model Predictions 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 7 /14 I n “ModelPredGood” w e hav e a good model that produces a lot of true positiv es and true negativ es. W e can see that the AUROC and av erage precision are both high. N egativ e-Skew ed Model Predictions I n “ModelPredNeg(HighTN,HighFN)” w e hav e balanced data and a model that is biased tow ards predicting negativ es. Although it predicts more negativ es in total, it predicts the same number of true negativ es as false negativ es. Because tp == fp and tn == fn, the AUROC 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 8 /14 and av erage precision are once again around their baseline v alues of 0.5, meaning that this is a useless model. W e can con ﬁ rm this by considering the formulas for TPR (true positiv e rate, recall), FPR (false positiv e rate), and precision. Since w e hav e tp == fp, w e can call this v alue a, i.e. tp == fp == a. Since w e hav e tn == fn, w e can call this v alue b, i.e. tn == fn == b. Then w e can write: F or AUROC: TPR = tp/(tp+fn) = a/(a+b), and FPR = fp/(fp+tn) = a/(a+b). Thus, TPR and FPR are alw ays equal to each other, meaning that the ROC is a straight line at y = x, meaning that the AUROC is 0.5. F or av erage precision: precision = tp/(tp+fp) = a/(a+a) = 1/2, and from before, TPR = recall = tp/(tp+fn) = a/(a+b). Thus, regardless of what the v alue of the recall is, the precision is alw ays about 1/2, and so w e get an area under the PR curv e of 0.5. P ositiv e-Skew ed Model Predictions I n “ModelPredPos(HighTP,HighFP)” w e can see the same e ﬀ ect as w e saw in “ModelPredNeg(HighTN,HighFN)”. This model is biased tow ards predicting positiv es, but although it predicts a larger number of positiv es in total, it predicts the same number of true positiv es as false positiv es, and so it is a useless model with AUROC and av erage precision at their baseline v alues. I t is also interesting to look at the points on the curv es corresponding to d = 0.9, 0.5, and 0.1. Here, when the model tends to predict positiv es, the points for d=0.5 and d=0.1 are squished closer together. Immediately abov e, in “DataBalanced/ModelPredNeg” w e instead hav e d=0.5 squished closer to d=0.9. G ood Model Predictions: High TNs 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 9 /14 I n “ModelPredGoodTN(HighTN)” the model is particularly good at identifying true negativ es. This produces be \u0000 er-than-random AUROC and be \u0000 er-than-random av erage precision. I n the ROC plot (red), w e see that the decision thresholds d = 0.9 to d = 0.5 span a small interv al of FPR = fp/(fp+tn). This is because for these high decision thresholds, the fps are especially low and the tns are especially high, which produces a small FPR. N ote that the av erage precision is not explicitly improv ed by the number of true negativ es, because true negativ es aren’t used in the calculation of av erage precision. The av erage precision is improv ed by the decrease in false positiv es that occurs because some examples w ere shifted from being false positiv es to true negativ es, which w as required in order to keep the assumption that the dataset is balanced. Precision = tp/(tp+fp), so when w e make the fp smaller, w e increase the precision. Also, precision tends to be highest when the decision threshold is highest (the left side of the plot) because the higher the decision threshold, the more stringent the requirement for being marked positiv e, which in general increases tps and low ers fps. B ad Model Predictions: High FPs 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 10 /14 I n “ModelPredBadFP(HighFP)” the model produces a lot of false positiv es. Once again because this model is “strategically bad” w e could get a good model out of it by ﬂ ipping its classiﬁ cation decisions. If w e ﬂ ipped its classiﬁ cation decisions, then the FPs would become TNs, and w e would hav e a model biased tow ards predicting TNs — the exact model shown abov e in “ModelPredGoodTN(HighTN)”. G ood Model Predictions: High TPs 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 1 1 /14 T his is our second-to-last ﬁ gure. In “ModelPredGoodTP(HighTP)” the model produces a lot of true positiv es. This leads to a be \u0000 er-than-random AUROC and a be \u0000 er-than-random av erage precision. N otice how this ROC is peaked more tow ards the top, whereas in “ModelPredGoodTN(HighTN)” the ROC curv e is peaked more tow ards the bo \u0000 om. The ROC here is peaked more tow ards the top because a small range of TPR=tp/(tp+fn) is cov ered by the low er decision thresholds d = 0.5 to d = 0.1. That is because when the decision thresholds are low, in this tp-biased model w e end up with a lot of tps and few fns, creating high recall across these v arious low er decision thresholds. The “elbow” being present at d = 0.5 is because of the w ay the synthetic results w ere generated relativ e to a decision threshold of 0.5. B ad Model Predictions: High FNs T his is the ﬁ nal plot. “ModelPredBadFN(HighFN)” shows plots for a model that produces a particularly large number of false negativ es. If w e ﬂ ipped this bad model’s classiﬁ cation decisions, all of the FNs would become TPs, and w e would get the good model shown abov e as “ModelPredGoodTP(HighTP)”. S ummary R OC is a plot of TPR vs. FPR across diﬀ erent decision thresholds. AUROC is the area under the ROC. AUROC indicates the probability that a randomly selected positiv e example has a higher predicted probability of being positiv e than a randomly selected negativ e example. A UROC ranges from 0.5 (random model) to 1.0 (perfect model). Note that it is possible to calculate an AUROC less than 0.5 if the model is “expertly bad” but in these cases, w e can ﬂ ip the model’s decisions to get a good model with an AUROC greater than 0.5. A PR curv e is a plot of precision vs. recall (TPR) across diﬀ erent decision thresholds. A v erage precision is one w ay of calculating the area under the PR curv e. A v erage precision 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 12 /14 indicates whether your model can correctly identify all the positiv e examples without accidentally marking too many negativ e examples as positiv e. A v erage precision ranges from the frequency of positiv e examples (0.5 for balanced data) to 1.0 (perfect model). I f the model makes “balanced” predictions that don’t tend tow ards being wrong or being right, then w e hav e a random model with 0.5 AUROC and 0.5 av erage precision (for frequency of positiv es = 0.5). This is exempliﬁ ed by “ModelBalanced”, “ModelPredNeg” (predicts many negativ es, but equally TNs and FNs), “ModelPredPos” (predicts many positiv es, but equally TPs and FPs). I f a model is “expertly bad” that means it tends to pick the wrong answ er. “Expertly bad” models include “ModelPredBad(HighFP,HighFN)”, “ModelPredBadFN(HighFN)”, and “ModelPredBadFP(HighFP).” These models are so good at picking the wrong answ er that they could be turned into useful models by ﬂ ipping their decisions. “M odelPredGood(HighTP,HighTN)” gets the best performance, as it identiﬁ es a lot of TPs and TNs. “ModelPredGoodTN(HighTN)” and “ModelPredGoodTP(HighTP)” also get be \u0000 er-than-random performance because they tend to pick the right answ er. P ublished by Rachel Draelos, MD, PhD I hav e an MD and a PhD in Computer Science from Duke Univ ersity. My research focuses on machine learning methods dev elopment for medical data. I am passionate about explainable AI for healthcare. V iew all posts by Rachel Draelos, MD, PhD 3 thoughts on “ The Complete Guide to AUC and A verage Precision: Simulations and V isualizations ” A dd Comment 1. P ingback: Convolutional Neural Networks (CNNs) in 5 minutes – Glass Box 2. P ingback: COVID-19 Chest CT Scan Analysis: Clinical Recommendations and Machine Learning Applications – Glass Box 3. P ingback: Designing Custom 2D and 3D CNNs in PyTorch: T utorial with Code – Glass Box C omments are closed. © 2023 GLASS BOX A U C A U PR C A U R O C A V E R A G E PR E C I S I O N C L A S S I F I C A T I O N PE R F O R M A N C E - M E T R I C 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 13 /14 W EBSITE BUIL T WITH WORDPRESS.COM . 7/5/23, 1 1:28 AM The Complete Guide to AUC and A verage Precision: Simulations and V isualizations – Glass Box https://glassboxmedicine.com/2020/07/14/the-complete-guide-to-auc-and-average-precision-simulations-and-visualizations/ 14 /14","libVersion":"0.3.1","langs":""}
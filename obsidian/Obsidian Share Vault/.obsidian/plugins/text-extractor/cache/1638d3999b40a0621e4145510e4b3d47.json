{"path":"lit/lit_sources/Kraskov04EstimatingMutualInformation.pdf","text":"Estimating mutual information Alexander Kraskov, Harald Stögbauer, and Peter Grassberger John-von-Neumann Institute for Computing, Forschungszentrum Jülich, D-52425 Jülich, Germany (Received 28 May 2003; published 23 June 2004) We present two classes of improved estimators for mutual information MsX , Yd, from samples of random points distributed according to some joint probability density msx , yd. In contrast to conventional estimators based on binnings, they are based on entropy estimates from k-nearest neighbor distances. This means that they are data ef®cient(with k = 1 we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to nonuniformity of the density at the smallest resolved scale, giving typically sys- tematic errors which scale as functions of k / N for N points. Numerically, we ®nd that both families become exact for independent distributions, i.e. the estimator MÃsX , Yd vanishes (up to statistical ¯uctuations ) if msx , yd =msxdmsyd. This holds for all tested marginal distributions and for all dimensions of x and y.In addition, we give estimators for redundancies between more than two random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation. DOI: 10.1103/PhysRevE.69.066138 PACS number(s): 05.90.1m, 02.50.2r, 87.10.1e I. INTRODUCTION Among the measures of independence between random variables, mutual information (MI) is singled out by its in- formation theoretic background [1]. In contrast to the linear correlation coef®cient, it is sensitive also to dependences which do not manifest themselves in the covariance. Indeed, MI is zero if and only if the two random variables are strictly independent. The latter is also true for quantities based on Renyi entropies [2], and these are often easier to estimate (in particular if their order is 2 or some other integer .2). Nev- ertheless, MI is unique in its close ties to Shannon entropy and the theoretical advantages derived from this. Some well- known properties of MI and some simple consequences thereof are collected in the Appendix. But it is also true that estimating MI is not always easy. Typically, one has a set of N bivariate measurements, zi = sxi , yid , i =1, ... , N, which are assumed to be iid (indepen- dent identically distributed) realizations of a random variable Z = sX , Yd with density msx , yd. Here, x and y can be either scalars or can be elements of some higher-dimensional space. In the following, we shall assume that the density is a proper smooth function, although we could also allow more singular densities. All we need is that the integrals written below exist in some sense. In particular, we will always as- sume that 0 logs0d = 0, i.e., we do not have to assume that densities are strictly positive. The marginal densities of X and Y are mxsxd = edymsx , yd and mysyd = edxmsx , yd. The MI is de®ned as IsX,Yd =EE dxdymsx,ydlog msx,yd mxsxdmysyd . s1d The base of the logarithm determines the units in which in- formation is measured. In particular, taking base 2 leads to information measured in bits. In the following, we always will use natural logarithms. The aim is to estimate IsX , Yd from the set hzij alone, without knowing the densities m,mx, and my. One of the main ®elds where MI plays an important role, at least conceptually, is independent component analysis (ICA)[3,4]. In the ICA literature, very crude approximations to MI based on cumulant expansions are popular because of their ease of use. But they are valid only for distributions close to Gaussians and can mainly be used for ranking dif- ferent distributions by interdependence, and much less for estimating the actual dependences. Expressions obtained by entropy maximalization using averages of some functions of the sample data as constraints [4] are more robust, but are still very crude approximations. Finally, estimates based on explicit parametrizations of the densities might be useful but are not very ef®cient. More promising are methods based on kernel density estimators [5,6]. We will not pursue these here either, but we will comment on them in Sec. IV A. The most straightforward and widespread approach for estimating MI more precisely consists in partitioning the supports of X and Y into bins of ®nite size, and approximat- ing Eq. (1) by the ®nite sum IsX,Yd< IbinnedsX,Yd; o ij psi, jdlog psi, jd pxsidpysjd , s2d where pxsid = eidx mxsxd, pysjd = ejdy mysyd, and psi , jd = eiejdxdy msx , yd, and ei means the integral over bin i.An estimator of IbinnedsX , Yd is obtained by counting the numbers of points falling into the various bins. If nxsidfnysjdg is the number of points falling into the ith bin of X fjth bin of Y], and nsi , jd is the number of points in their intersection, then we approximate pxsid< nxsid / N, pysjd< nysjd / N, and psi , jd < nsi , jd / N. It is easily seen that the right-hand side of Eq. (2) indeed converges to IsX , Yd if we ®rst letN ! ` and then PHYSICAL REVIEW E 69, 066138 (2004) 1539-3755/2004/69(6)/066138(16)/$22.50 ©2004 The American Physical Society69 066138-1 let all bin sizes tend to zero, if all densities exist as proper (not necessarily smooth) functions. If not, i.e., if the distri- butions are, e.g., (multi)fractal, this convergence might no longer be true. In that case, Eq. (2) would de®ne resolution- dependent mutual entropies which diverge in the limit of in®nite resolution. Although the methods developed below could be adapted to apply also to that case, we shall not do this in the present paper. The bin sizes used in Eq. (2) do not need to be the same for all bins. Optimized estimators [7,8] use indeed adaptive bin sizes which are essentially geared to having equal num- bers nsi , jd for all pairs si , jd with nonzero measure. While such estimators are much better than estimators using ®xed bin sizes, they still have systematic errors which result on the one hand from approximating IsX , Yd by IbinnedsX , Yd, and on the other hand by approximating (logarithms of) probabilities by (logarithms of) frequency ratios. The latter could be pre- sumably minimized by using corrections for ®nite nxsid and nsi , jd, respectively [9]. These corrections are in the form of asymptotic series which diverge for ®nite N, but whose ®rst two terms improve the estimates in typical cases. The ®rst correction termÐwhich often is not suf®cientÐwas taken into account in [6,10]. In the present paper we will not follow these lines, but rather estimate MI from k-nearest neighbor statistics. There exists an extensive literature on such estimators for the simple Shannon entropy HsXd =−E dxmsxdlog msxd, s3d dating back at least to [11,12]. But it seems that these meth- ods have hardly ever been used for estimating MI (for an exception see [13], where they were used to estimate transfer entropies).In [12,14±19] it is assumed that x is one- dimensional, so that the xi can be ordered by magnitude and xi+1 − xi ! 0 for N ! `. In the simplest case, the estimator based only on these distances is HsXd< 1 N −1 o i=1 N−1 logsxi+1 − xid + c s1d − c sNd. s4d Here, c sxd is the digamma function, c sxd = Gsxd−1dGsxd / dx. It satis®es the recursion c sx +1d =c sxd +1/ x and c s1d = −C, where C = 0.577 215 6. . . is the Euler-Mascheroni con- stant. For large x, c sxd< log x −1/2x. Similar formulas exist which use xi+k − xi instead of xi+1 − xi, for any integer k , N. Although Eq. (4) and its generalizations to k . 1 seem to give the best estimators of HsXd, they cannot be used for MI because it is not obvious how to generalize them to higher dimensions. Here we have to use a slightly different ap- proach, due to [20][see also [21,22]; the latter authors were only interested in fractal measures and estimating their infor- mation dimensions, but the basic concepts are the same as in estimating HsXd for smooth densities]. Assume some metrics to be given on the spaces spanned by X , Y and Z = sX , Yd. We can then rank, for each point zi = sxi , yid, its neighbors by distance di,j = izi − zji: di,j1 ø di,j2 ø di,j3 ø ¯. Similar rankings can be done in the subspaces X and Y. The basic idea of [20±22] is to estimate HsXd from the average distance to the k-nearest neighbor, averaged over all xi. Details will be given in Sec. II. Mutual information could be obtained by estimating in this way HsXd, HsYd, and HsX , Yd separately and using [1] IsX,Yd = HsXd + HsYd − HsX,Yd. s5d But this would mean that the errors made in the individual estimates would presumably not cancel, and therefore we proceed differently. Indeed we will present two slightly different algorithms, both based on the above idea. Both use for the space Z = sX , Yd the maximum norm, iz − z8i = maxhix − x8i,iy − y8ij, s6d while any norms can be used for ix − x8i and iy − y8i (they need not be the same, as these spaces could be completely different). Let us denote by esid / 2 the distance from zi to its kth neighbor, and by exsid / 2 and eysid / 2 the distances be- tween the same points projected into the X and Y subspaces. Obviously, esid = maxhexsid ,eysidj. In the ®rst algorithm, we count the numbernxsid of points xj whose distance from xi is strictly less than esid / 2, and similarly for y instead of x. This is illustrated in Fig. 1(a). Notice that esid is a random (¯uctuating ) variable, and there- fore also nxsid and nysid ¯uctuate. We denote by k¯l aver- ages both over all i P f1, ... , Ng and over all realizations of the random samples, k¯l = N−1o i=1 N Ef¯sidg. s7d The estimate for MI is then Is1dsX,Yd = cskd − kcsnx +1d + csny +1dl + csNd. s8d FIG. 1. Panel (a): Determination of esid, nxsid, and nysid in the ®rst algorithm, for k = 1 and some ®xed i. In this example, nxsid = 5 and nysid = 3. Panels (b),(c): Determination of exsid, eysid, nxsid, and nysid in the second algorithm for k = 2. Panel (b) shows a case in which exsid and eysid are determined by the same point, while panel (c) shows a case in which they are determined by different points. KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-2 Alternatively, in the second algorithm, we replace nxsid and nysid by the number of points with ixi − xji øexsid / 2 and iyi − y ji øeysid /2 [see Figs. 1(b) and 1(c)]. The estimate for MI is then Is2dsX,Yd = cskd −1/k − kcsnxd + csnydl + csNd. s9d The derivations of Eqs. (8) and (9) will be given in Sec. II. There we will also give formulas for generalized redundan- cies in higher dimensions, IsX1,X2, ... ,Xmd = HsX1d + HsX2d + ¯ + HsXmd − HsX1,X2, ¯ ,Xmd. s10d In general, both formulas give very similar results. For the same k, Eq. (8) gives slightly smaller statistical errors [be- cause nxsid and nysid tend to be larger and have smaller rela- tive ¯uctuations ], but have larger systematic errors. The lat- ter is only severe if we are interested in very high dimensions where esid tends typically to be much larger than the mar- ginal exjsid. In that case the second algorithm seems prefer- able. Otherwise, both can be used equally well. A systematic study of the performance of Eqs. (8) and (9) and comparison with previous algorithms will be given in Sec. III. Here we will just show results of Is2dsX , Yd for Gaussian distributions. Let X and Y be Gaussians with zero mean and unit variance, and with covariance r. In this case IsX , Yd is known exactly [8], IGausssX,Yd =− 1 2 logs1− r2d. s11d In Fig. 2, we show the errors Is2dsX , Yd − IGausssX , Yd for vari- ous values of r, obtained from a large number (typically 105 −10 7) of realizations of N-tuples of vectors sxi , yid.We show only results for k = 1, plotted against 1 / N. Results for k . 1 are similar. To a ®rst approximation Is1dsX , Yd and Is2d 3sX , Yd depend only on the ratio k / N. The most conspicuous feature seen in Fig. 2, apart from the fact that indeed Is2dsX , Yd − IGausssX , Yd ! 0 for N ! `,is that the systematic error is compatible with zero for r = 0, i.e., when the two Gaussians are uncorrelated. We checked this with high statistics runs for many different values of k and N (a priori one should expect that systematic errors become large for very small N), and for many more distributions (exponential, uniform, etc.). In all cases we found that both Is1dsX , Yd and Is2dsX , Yd become exact for independent vari- ables. Moreover, the same seems to be true for higher-order redundancies. We thus have the following conjecture. Conjecture. Equations (8) and (9) are exact for indepen- dent X and Y, i.e., Is1dsX , Yd = Is2dsX , Yd = 0 if and only if IsX , Yd =0. We have no proof for this very surprising result. We have numerical indications that moreover uIs1,2dsX,Yd − IsX,Ydu IsX,Yd ø const s12d as X and Y become more and more independent, but this is much less clean and therefore much less sure. In Sec. II we shall give formal arguments for our estima- tors, and for generalizations to higher dimensions. Detailed numerical results for cases where the exact MI is known will be given in Sec. III. In Sec. IV A we give two preliminary applications to gene expression data and to ICA. Conclusions are drawn in the ®nal section, Sec. V. Finally, some general aspects of MI are recalled in an Appendix. II. FORMAL DEVELOPMENTS A. Kozachenko-Leonenko estimate for Shannon entropies We ®rst review the derivation of the Shannon entropy estimate [20±23], since the estimators for MI are obtained by very similar arguments. Let X be a continuous random variable with values in some metric space, i.e., there is a distance function ix − x8i between any two realizations of X, and let the density msxd exist as a proper function. Shannon entropy is de®ned as HsXd =−E dxmsxdlog msxd, s13d where ªlogº will always mean natural logarithm so that in- formation is measured in natural units. Our aim is to estimate HsXd from a random sample sx1 ¯ xNd of N realizations of X. The ®rst step is to realize that Eq.(13) can be understood (up to the minus sign) as an average of log msxd.Ifwehad unbiased estimators log mÃ sxd of the latter, we would have an unbiased estimator HÃsXd =− N−1o i=1 N log mÃ sxid. s14d In order to obtain the estimate log mÃ sxid, we consider the probability distribution Pksed for the distance between xi and its kth nearest neighbor. The probability Pksedde is equal to the chance that there is one point within distance r FIG. 2. Estimates of Is2dsX , Yd − IexactsX , Yd for Gaussians with unit variance and covariances r = 0.9, 0.6, 0.3, and 0.0 (from top to bottom), plotted against 1 / N. In all cases k = 1. The number of trials is .2 3 10 6 for N ø 1000 and decreases to <10 5 for N = 40 000. Error bars are smaller than the sizes of the symbols. ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-3 P fe/2,e/2+ de/2g from xi, that there are k − 1 other points at smaller distances, and that N − k − 1 points have larger dis- tances from xk. Let us denote by pi the mass of the e ball centered at xi, pised = eij−xii,e/2 djmsjd. Using the trinomial formula we obtain Pksedde = sN −1d! 1! sk −1d ! sN − k −1d! dpised de de 3 pi k−1 3 s1− pidN−k−1 s15d or Pksed = kS N −1 k Ddpised de pi k−1s1− pidN−k−1. s16d One easily checks that this is correctly normalized, edePksed = 1. Using Eq. (16), one can also compute the ex- pectation value of log pised, Eslog pid =E 0 ` dePksedlog pised =kS N −1 k DE 0 1 dppk−1s1− pdN−k−1log p =cskd − csNd, s17d where csxd is the digamma function. The expectation is taken here over the positions of all other N − 1 points, with xi kept ®xed. An estimator for logmsxd is then obtained by assuming that msxd is constant in the entire e ball. The latter gives pised< cde dmsxid, s18d where d is the dimension of x and cd is the volume of the d-dimensional unit ball. For the maximum norm one has simply cd = 1, while cd = p d/2 / Gs1+ d /2d /2d for the Euclidean norm. Using Eqs. (17) and (18), one obtains log msxid< cskd − csNd − dEslog ed − log cd, s19d which ®nally leads to HÃsXd =− cskd + csNd + log cd + d No i=1 N log esid, s20d where esid is twice the distance from xi to its kth neighbor. From the derivation it is obvious that Eq. (20) would be unbiased, if the density msxd were strictly constant. The only approximation is in Eq. (18). For points on a torus (e.g., when x is a phase) with a strictly positive density one can easily estimate the leading corrections to Eq. (18) for large N. One ®nds that they are Os1/ N2d and that they scale, for large k and N,as ,sk / Nd2. In most other cases (including, e.g., Gaussians and uniform densities in bounded domains with a sharp cutoff) it seems numerically that the error is ,k / N or ,k / N logsN / kd. B. Mutual information: Estimator I—1–—X , Y– Let us now consider the joint random variable Z = sX , Yd with maximum norm. Again we take one of the N points zi and consider the distance e/ 2 to its kth neighbor. Again this is a random variable with distribution given by Eq. (16). Also Eq. (17) holds without changes. The ®rst difference from the previous subsection is in Eq. (18), where we have to replace d by dZ = dX + dY, cd by cdXcdY, and of course xi by zi = sxi , yid. With these modi®cations we obtain therefore HÃsX,Yd =− cskd + csNd + logscdXcdYd + dX + dY N o i=1 N log esid. s21d In order to obtain IsX , Yd, we have to subtract this from estimates for HsXd and HsYd. For the latter, we could use Eq. (20) directly with the same k. But this would mean that we would effectively use different distance scales in the joint and marginal spaces. For any ®xed k, the distance to the kth neighbor in the joint space will be larger than the distances to the neighbors in the marginal spaces. The bias in Eq. (20) results from the nonuniformity of the density. Since the ef- fect of the latter depends of course on the kth neighbor dis- tances, the biases in HÃsXd, HÃsYd, and in HÃsX , Yd would be very different and would thus not cancel. To avoid this, we notice that Eq. (20) holds for any value of k, and that we do not have to choose a ®xed k when estimating the marginal entropies. Assume, as in Fig. 1(a), that the kth neighbor of xi is on one of the vertical sides of the square of size esid. In this case, if there are altogether nxsid points within the vertical lines x = xi ±esid / 2, then esid /2 is the distance to the fnxsid +1gst neighbor of xi, and HÃsXd = −1 N o i=1 N cfnxsid +1g + csNd + log cdX + dX N o i=1 N log esid. s22d For the other direction [the y direction in Fig. 1(a)] this is not exactly true, i.e., esid is not exactly equal to twice the dis- tance to the fnysid +1gst neighbor, if nysid is analogously de- ®ned as the number of points with iy j − yii ,esid / 2. Never- theless, we can consider Eq. (22) also as a good approximation for HsYd, if we replace everywhere X by Y in its right-hand side [this approximation becomes exact when nysid ! `, and thus also when N ! `]. If we do this, subtract- ing HÃsX , Yd from HÃsXd + HÃsYd leads directly to Eq. (8). We should stress that the errors in HÃsXd, HÃsYd, and in HÃsX , Yd will not cancel eactly in general. But the chances that they will do so approximately are bigger with the above procedure than if we had used different length scales in the three estimates. The real proof that our proposed estimator is better than that obtained when using the same k in HÃsXd, HÃsYd, and HÃsX , Yd comes of course from detailed numerical tests. These arguments can be easily extended to m random variables and lead to KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-4 Is1dsX1,X2, ... ,Xmd = cskd + sm −1dcsNd − kcsnx1d + csnx2d + ¯ + csnxmdl. s23d C. Mutual information: Estimator I—2–—X , Y– The main drawback of the above derivation is that the Kozachenko-Leonenko estimator is used correctly in only one marginal direction. This seems unavoidable if one wants to stick to ªballs,º i.e., to (hyper-) cubes in the joint space. In order to avoid it we have to switch to (hyper) rectangles. Let us ®rst discuss the case of two marginal variables X and Y, and generalize later to m variables X1 , ... , Xm.As illustrated in Figs. 1(b) and 1(c), there are two cases to be distinguished [all other cases, where more points fall onto the boundaries xi ±exsid / 2 and yi ±eysid / 2, have zero prob- ability; see, however, the third paragraph of Sec. III]: Either the two sides exsid and eysid are determined by the same point [Fig. 1(b)], or by different points [Fig. 1(c)]. In either case we have to replace Pksed by a two-dimensional density, Pksex,eyd = Pk sbdsex,eyd + Pk scdsex,eyds24d with Pk sbdsex,eyd =S N −1 k D d2fqi kg dexdey s1− pidN−k−1 s25d and Pk scdsex,eyd = sk −1dS N −1 k D d2fqi kg dexdey s1− pidN−k−1. s26d Here, qi ; qisex ,eyd is the mass of the rectangle of size ex 3ey centered at sxi , yid, and pi is, as before, the mass of the square of size e= maxhex ,eyj. The latter is needed since by using the maximum norm we guarantee that there are no points in this square which are not inside the rectangle. Again we verify straightforwardly that Pk is normalized, while we have now instead of Eq. (17) Eslogqid =EE 0 ` dexdeyPksex,eydlog qisex,eyd = cskd −1/k − csNd. s27d Denoting now by nxsid and nysid the number of points with distance less than or equal to exsid / 2 and eysid / 2, respec- tively, we arrive at Eq. (9). For the generalization to m variables we have to consider m-dimensional densities Pksex1 , ... ,exmd. The number of dis- tinct cases [analogous to the two cases shown in Figs. 1(b) and 1(c)] proliferates as m grows, but fortunately we do not have to consider all these cases explicitly. One sees easily that each of them contributes to Pk a term ~ dmfqi kg dex1 ¯ dexm s1− pidN−k−1. s28d The direct calculation of the proportionality factors would be extremely tedious (we did it for m =3), but it can be avoided by simply demanding that the sum is correctly normalized. This gives Pksex1, ... ,exmd = km−1S N −1 k D dmfqi kg dex1 ¯ dexm 3 s1− pidN−k−1. s29d Calculating again Eslog qid =cskd − sm −1d / k −csNd analyti- cally and approximating the density by a constant inside the hyper-rectangle, we obtain ®nally Is2dsX1,X2, ... ,Xmd = cskd − sm −1d/k + sm −1dcsNd − kcsnx1d + csnx2d + ¯ + csnxmdl. s30d Before leaving this section, we should mention that we cheated slightly in deriving Is2dsX , Yd (and its generalization to m . 2). Assume that in a particular realization we have exsid ,eysid, as in Figs. 1(b) and 1(c). In that case we know that there cannot be any point in the two rectangles fxi −eysid /2, xi −exsid /2g 3 fyi −eysid /2, yi +eysid /2g and fxi +exsid /2, xi +eysid /2g 3 fyi −eysid /2, yi +eysid /2g (see Fig. 3). While we have taken this correctly into account when estimating HsX , Yd (where it was crucial), we have neglected it in HsXd and HsYd. There, the corrections are Os1/ nxd and Os1/ nyd, and should vanish for N ! `. It could be that their net effect vanishes, because they contribute with opposite signs to HsXd and HsYd. But we have no proof for it. Any- how, due to the approximation of constant density within each rectangle, we cannot expect our estimates to be exact for ®nite N, and any justi®cation ultimately relies on numer- ics. III. IMPLEMENTATION AND RESULTS A. Some implementation details Mutual information is invariant under reparametrization of the marginal variables. If X8 = FsXd and Y8 = GsYd are ho- FIG. 3. There cannot be any points inside the shaded rectangles. For method 2, this means that the estimates of the marginal entropy HsXdfHsYdg should be modi®ed, since part of the area outside [in- side] the stripe of with ex feyg is forbidden. This is neglected in Eq. (9). ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-5 meomorphisms, then IsX , Yd = IsX8 , Y8d (see the Appendix). This is in contrast to HsXd, which changes in general under a homeomorphism. This can be used to rescale both variables ®rst to unit variance. In addition, if the distributions are very skewed and/or rough, it might be a good idea to transform them such as to become more uniform (or at least single- humped and more or less symmetric). Although this is not required, strictly speaking it will reduce errors in general. One example is the G-exponential distribution in two vari- ables, msx , yd = xu exps−x − xyd / Gsud for x , y . 0 [24], when u, 1. For u! 0, the marginal distributions develop 1 / x and 1/ y singularities (for x ! 0 and for y ! `, respectively), and the joint distribution is nonzero only in a very narrow region near the two axes. In this case our algorithm failed when applied directly, but it gave excellent results after transform- ing the variables to x8 = log x and y8 = log y. When implemented straightforwardly, the algorithm spends most of the CPU time searching for neighbors. In the most naive version, we need two nested loops through all points which gives a CPU time OsN2d. While this is accept- able for very small data sets (say N ø 300), fast neighbor search algorithms are needed when dealing with larger sets. Let us assume that X and Y are scalars. An algorithm with complexity OsNÎkNd is then obtained by ®rst ranking thexi by magnitude (this can be done by any sorting algorithm such as QUICKSORT), and coranking the yi with them [25]. Nearest neighbors of sxi , yid can then be obtained by search- ing x neighbors on both sides of xi and verifying that their distance in the y direction is not too large. Neighbors in the marginal subspaces are found even easier by ranking both xi and yi. Most results in this paper were obtained by this method, which is suitable for N up to a few thousand. The fastest (but also most complex) algorithm is obtained by us- ing grids (ªboxesº )[26,27]. Indeed, we use three grids: A two-dimensional one with box size OsÎk / Nd and two one- dimensional ones with box sizes Os1/ Nd. First the k neigh- bors in 2D space are searched using the 2D grid, then the boxes at distances ±e from the central point are searched in the 1D grids to ®ndnx and ny. If the distributions are smooth, this leads to complexity OsÎkNd. The last algorithm is com- parable in speed to the algorithm of [8]. For all three ver- sions of our algorithm it costs only little additional CPU time if one also evaluates, together with IsX , Yd for some k . 1, the estimators for smaller k. Empirical data usually are obtained with few (e.g., 12 or 16) binary digits, which means that many points in a large set may have identical coordinates. In that case, the numbers nxsid and nysid need no longer be unique (the assumption of continuously distributed points is violated). If no precautions are taken, any code based on nearest-neighbor counting is then bound to give wrong results. The simplest way out of this dilemma is to add very low-amplitude noise to the data (<10 −10, say, when working with double precision) which breaks this degeneracy. We found this to give satisfactory results in all cases. Often, MI is estimated after rank ordering the data, i.e., after replacing the coordinate xi by the rank of the ith point when sorted by magnitude. This is equivalent to applying a monotonic transformation x ! x8 , y ! y8 to each coordinate, which leads to a strictly uniform empirical density, mx8sx8d =my8sx8d = s1/ Ndoi=1 N dsx8 − id. For N ! ` and k @ 1 this clearly leaves the MI estimate invariant. But it is not obvious that it leaves invariant also the estimates for ®nitek, since the trans- formation is not smooth at the smallest length scale. We found numerically that rank ordering gives correct estimates also for small k, if the distance degeneracies implied by it are broken by adding low-amplitude noise as discussed above. In particular, both estimators still gave zero MI for independent pairs. Although rank ordering can reduce statistical errors, we did not apply it in the following tests, and we did not study in detail the properties of the resulting estimators. B. Results: Two-dimensional distributions We shall ®rst discuss applications of our estimators to correlated Gaussians, mainly because we can in this way most easily compare with analytic results and with previous numerical analyses. In all cases we shall deal with Gaussians of unit variance and zero mean. For m such Gaussians with covariance matrix siki , k =1 ¯ m, one has IsX1, ... ,Xmd =− 1 2 logfdetssdg. s31d For m = 2 and using the notation r = sXY, this gives Eq. (11). First results for Is2dsX , Yd with k = 1 were already shown in Fig. 2. Results obtained with Is1dsX , Yd are very similar and would indeed be hard to distinguish in this ®gure. In Fig. 4 we compare values of Is1dsX , Yd (left panel) with those for Is2dsX , Yd (right panel) for different values of N and for r = 0.9. The horizontal axes show k / N (left) and sk −1/2d / N (right). Except for very small values of k and N, we observe scaling of the form Is1dsX,Yd< FS k ND, Is2dsX,Yd< FSk − 1/2 N D. s32d This is a general result and is found also for other distribu- tions. The scaling with k / N of Is1dsX , Yd results simply from the fact that the number of neighbors within a ®xed distance would scale ~N, if there were no statistical ¯uctuations. For large k these ¯uctuations should become irrelevant, and thus the MI estimate should depend only on the ratio k / N. For Is2dsX , Yd this argument has to be slightly modi®ed, since the smaller one of ex and ey is determined [for large k, where the situation illustrated in Fig. 1(c) dominates over that in Fig. 1(b)] by k − 1 instead of k neighbors. The fact that Is2dsX , Yd for a given value of k is between Is1dsX , Yd for k − 1 and Is1dsX , Yd for k is also seen from the variances of the estimates. In Fig. 5 we show the standard deviations, again for covariance r = 0.9. These statistical er- rors depend only weakly on r. For r = 0 they are roughly 10% smaller. As seen from Fig. 5, the errors of Is2dsX , Y ; kd are roughly halfway between those of Is1dsX , Y ; k −1d and Is1dsX , Y ; kd. They scale roughly as ,ÎN, except for very large k / N. Their dependence on k does not follow a simple scaling law. The fact that statistical errors increase when k decreases is intuitively obvious, since then the width of the distribution of e increases too. Qualitatively the same depen- KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-6 dence of the errors was observed also for different distribu- tions. For practical applications, it means that one should use k . 1 in order to reduce statistical errors, but too large values of k should be avoided since then the increase of systematic errors outweighs the decrease of statistical ones. We propose to use typically k =2±4, except when testing for indepen- dence. In the latter case we do not have to worry about systematic errors, and statistical errors are minimized by tak- ing k to be very large (up to k < N / 2, say). The above shows that Is1dsX , Yd and Is2dsX , Yd behave very similarly. Also CPU times needed to estimate them are nearly the same. In the following, we shall only show data for one of them, understanding that everything holds also for the other, unless the opposite is said explicitly. For N ! `, the systematic errors tend to zero, as they should. From Figs. 2 and 4 one might conjecture that Is1,2d 3sX , Yd − IexactsX , Yd, N−1/2, but this is not true. Plotting this difference on a double logarithmic scale (Fig. 6), we see a scaling ,N−1/2 for N < 10 3, but faster convergence for larger N. It can be ®tted by a scaling,1/ N0.85 for the largest values of N reached by our simulations, but the true asymptotic behavior is presumably just ,1/ N. As said in the Introduction, the most surprising feature of our estimators is that they seem to be exact for independent random variables X and Y. In Fig. 7 we show how the rela- tive systematic errors behave for Gaussians when r ! 0. More precisely, we show Is1,2dsX , Yd / Iexact s1,2dsX , Yd for k =1, plotted against N for four different values of r. Obviously these data converge, when r ! 0, to a ®nite function ofN.We FIG. 4. Mutual information estimates Is1dsX , Yd (left panel) and Is2dsX , Yd (right panel) for Gaussian deviates with unit variance and covariance r = 0.9, plotted against k / N (left panel) and sk −1/2d / N (right panel), respectively. Each curve corresponds to a ®xed value of N, with N = 125, 250, 500, 1000, 2000, 4000, 10 000, and 20 000, from bottom to top. Error bars are smaller than the size of the symbols. The dashed line indicates the exact value IsX , Yd = 0.830 366. FIG. 5. Standard deviations of the estimates Is1dsX , Yds+d and Is2dsX , Yds3d for Gaussian deviates with unit variance and covari- ance r = 0.9, multiplied by ÎN and plotted against kfIs1dsX , Ydg or k −1/2fIs2dsX , Ydg. Each curve corresponds to a ®xed value of N, with N = 125, 250, 500, 1000, 2000, 4000, 10 000, and 20 000, from bottom to top. FIG. 6. Systematic error Is2dsX , Yd − IexactsX , Yd for k = 3 plotted against N on a log-log scale, for r = 0.9. The dashed lines are ~N−0.5 and ~N−0.85. FIG. 7. Ratios Is2dsX , Yd / IexactsX , Yd for k = 1 plotted against 1/ N, for four different values of r. ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-7 have observed the same also for other distributions, which leads to a conjecture stronger than the conjecture made in the Introduction: Assume that we have a one-parameter family of 2D distributions with densities msx , y ; rd, with r being a real-valued parameter. Assume also that m factorizes for r = r0, and that it depends smoothly on r in the vicinity of r0, with ]msx , y ; rd / ]r ®nite. Then we propose that for many distributions (although not for all) Is1,2dsX,Yd/IexactsX,Yd ! Fsk,Nds33d for r ! r0, with some function Fsk , Nd which is close to 1 for all k and all N @ 1, and which converges to 1 for N ! `.We have not found a general criterion for which families of dis- tributions we should expect Eq. (33). The most precise and ef®cient previous algorithm for es- timating MI is that of Darbellay and Vajda [8], and we will compare here only with their algorithm (some less system- atic comparisons with a KDE method will be discussed in Sec. IV A). As far as speed is concerned, it seems to be faster than the present one, which might, however, be due to a more ef®cient implementation. In any case, also with the present algorithm we were able to obtain extremely high statistics on work stations within reasonable CPU times. To compare our statistical and systematic errors with those of [8], we have used the code basic.exe from Ref. [42]. We used the parameter settings recommended in its description. This code provides an estimate of the statistical error, even if only one data set is provided. When running it with many (typically <104) data sets, we found that these error bars are always underestimated, sometimes by rather large margins. This seems to be due to occasional outliers which point presumably to some numerical instability. Unfortu- nately, having no source code we could not pin down the troubles. In Fig. 8 we compare the predictions of the statis- tical errors provided by the code of [8], the actual errors obtained from the variance of the estimators provided by this code, and the error obtained from Is2dsX , Yd with k = 3. We see that the latter is larger than the theoretical error from [8], but smaller than the actual error. For Gaussians with smaller cor- relation coef®cients, the statistical errors of [8] decrease strongly with r, because the partitionings are followed to less and less depth. But, as we shall see, this comes with a risk for systematic errors. Systematic errors of [8] for Gaussians with various values of r are shown in Fig. 9. Comparing with Fig. 2 we see that they are, for r Þ 0, about an order of magnitude larger than ours, except for very large N, where they seem to decrease as 1/ N. Systematic errors of [8] are also very small when r = 0, but this seems to result from ®ne tuning the parameterds which governs the pruning of the partitioning tree in [8]. Bad choices of ds lead to wrong MI estimates, and optimal choices should depend on the problem to be analyzed. No such ®ne tuning is needed with our method. As examples of non-Gaussian distributions we studied (i) the G-exponential distribution [29], (ii) the ordered Weinman exponential distribution [29], and (iii) the ªcircle distribu- tionº of Ref. [28]. For all these, both exact formulas for the MI and detailed simulations using Darbellay-Vajda algorithm exist. In addition, we tested that Is1d and Is2d vanish, within statistical errors, for independent uniform distributions, for exponential distributions, and when X was Gaussian and Y was either uniform or exponentially distributed. Notice that ªuniformº means uniform within a ®nite interval and zero outside, so that the Kozachenko-Leonenko estimate is not exact for this case either. In all cases with independent X and Y we found that Is1,2dsX , Yd = 0 within the statistical errors (which typically were <10−3 ±10 −4). We do not show these data. The G-exponential distribution depends on a parameter u (after a suitable rescaling of x and y) and is de®ned [29] as msx,y;ud = 1 Gsud xue−x−xy s34d for x . 0 and y . 0, and msx , y ;ud = 0 otherwise. The MI is [29] IsX , Ydexact=csu+1d − log u. For u. 1 the distribution becomes strongly peaked at x = 0 and y = 0. Therefore, as we already said, our algorithms perform poorly for u@ 1, if we use xi and yi themselves. But using xi8 = log xi and yi8 = log yi we obtain excellent results, as seen from Fig. 10. FIG. 8. Statistical errors (one standard deviation) for Gaussian deviates with r = 0.9, plotted against N. Results from Is2dsX , Yd for k =1 (full line) are compared to theoretically predicted (dashed line) and actually measured (dotted line) errors from [8]. FIG. 9. Systematic errors for Gaussian deviates with r = 0.0, 0.3, 0.6, and 0.9, plotted against 1 / N, obtained with the algo- rithm of [8]. These should be compared to the systematic errors obtained with the present algorithm shown in Fig. 2. KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-8 There we plot again Is2dsX8 , Y8d / IsX , Ydexact for k = 1 against 1/ N for ®ve values of u. These data obviously support our conjecture that Is2dsX8 , Y8d / IsX , Ydexact tends towards a ®nite function as independence is approached. To compare with [29], we show in Fig. 11 our data together with those of [29] for the same four values of u also studied there, namely u = 0.1, 0.3, 2.0, and 100.0. We see that MI was grossly under- estimated in [29], in particular for large u where IsX , Yd is very small [for u@ 1, one has IsX , Yd< 1/2u]. The ordered Weinman exponential distribution depends on two continuous parameters. Following [29] we consider here only the case where one of these parameters (called u0 in [29]) is set equal to 1, in which case the density is msx,y;ud = 2 ue−2x−sy−xd/u s35d for x . 0 and y . 0, and msx , y ;ud = 0 otherwise. The MI is [29] IsX,Ydexact =5 log 2u 1−2u + cS 1 1−2uD − cs1d, u , 1 2 − cs1d, u = 1 2 log2u −1 u + cS 2u 2u −1D − cs1d, u . 1 2 . s36d Mutual information estimates using Is2dsX , Yd with k = 1 are shown in Fig. 12. Again we transformed sxi , yid ! slog xi , log yid since this improved the accuracy, albeit not as much as for the G-exponential distribution. More pre- cisely, we plot Is2dsX , Yd / IsX , Ydexact against 1 / N for the same four values of u studied also in [29], and we plot also the estimates obtained in [29]. We see that MI was severely un- derestimated in [29], in particular for large u where the MI is small (for u! `, one has IsX , Yd<fc8s1d −1g /2u = 0.32247/u). Our estimates are also too low, but much less so. It is clearly seen that Is2dsX8 , Y8d / IsX , Ydexact decreases for u! ` in contradiction to the above conjecture. This repre- sents the only case where the conjecture does not hold nu- merically. As we already said, we do not know which feature of the ordered Weinman exponential distribution is respon- sible for this difference. C. Higher dimensions In higher dimensions we shall only discuss applications of our estimators to m correlated Gaussians, because as in the case of two dimensions this is easily compared to analytic results [Eq. (31)] and to previous numerical results [30].As already mentioned in the Introduction and as shown above for 2D distributions (Fig. 7), our estimates seem to be exact for independent random variables. We choose the same one- parameter family of 3D Gaussian distributions with all the correlation coef®cients equal to r as in [30]. In Fig. 13 we show the behavior of the relative systematic errors of both proposed estimators. One can easily see that the data con- verge for r ! 0, i.e., when all three Gaussians become inde- pendent. This supports the conjecture made in the previous subsection. In addition, in Fig. 13 one can see the difference between the estimators Is1d and Is2d. For intermediate num- bers of points, N , 100− 200, the ªcubicº estimator has lower systematic error. Apart from that, Is2d evaluated for N is roughly equal to Is1d evaluated for 2N, re¯ecting the fact that Is2d effectively uses smaller length scales as discussed al- ready for d =2. To compare our results in high dimension with those pre- sented in [30], we shall calculate not the high-dimensional redundancies IsX1 , X2 , ... , Xmd but the MI I(sX1 , X2 , ... , Xm−1d , Xm) between two variables, namely an sm −1d-dimensional vector and a scalar. For estimation of this MI we can use the formulas as for the 2D case [Eqs. (8) and (9), respectively] where nx would be de®ned as the num- ber of points in the sm −1d-dimensional stripe of the (hyper) cubic cross section. Using directly Eq. (A3) would increase the errors in estimation [see the Appendix for the relation FIG. 10. Ratios IsX , Ydestim/ IexactsX , Yd for the G-exponential distribution, plotted against 1 / N. These data were obtained with Is2d using k = 1, after transforming xi and yi to their logarithms. The ®ve curves correspond to u= 0.1, 0.3, 1.0, 2.0, 10.0, and 100.0 (from bot- tom to top). FIG. 11. Ratios IsX , Ydestim/ IexactsX , Yd for the G-exponential distribution, plotted against 1 / N. Full lines are from estimator Is2d, dashed lines are from [29]. Our data were obtained with k = 1 after a transformation to logarithms. The four curves correspond to u = 0.1, 0.3, 2.0, and 100.0 (from bottom to top for our data, from top to bottom for the data of [29]). ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-9 between IsX1 , X2 , ... , Xmd and I(sX1 , X2 , ... , Xm−1d , Xm)]. In Fig. 14 we show the average values of Is1,2d. They are in very good agreement with the theoretical ones for all three values of the correlation coef®cient r and all dimensions tested here (in contrast, in [30] the estimators of MI signi®- cantly deviate from the theoretical values for dimensions ù6). It is impossible to distinguish (on this scale) between estimates Is1d and Is2d. In Fig. 15, statistical errors of our estimate are presented as a function of the number of neighbors k. More precisely, we plotted the standard deviation of Is1d multiplied by ÎN / m against k for the case where all correlation coef®cients are r = 0.9. Each curve corresponds to a different dimension m. The data scale roughly as ,m /ÎN for large dimension. Moreover, these statistical errors seem to converge to ®nite values for k ! `. This convergence becomes faster for in- creasing dimensions. The same behavior is observed for Is2d. IV. APPLICATIONS: GENE EXPRESSION DATA AND INDEPENDENT COMPONENT ANALYSIS A. Gene expression In the ®rst application to real world data, we study the gene expression ratios from [31], and compare our MI esti- mators to kernel density estimators (KDE) used in [6]. The authors of [31] considered N = 300 closely related yeast ge- nomes obtained by one or at most a few mutations from wild type, and indexed by i =1, ... , N. The measured raw data are expression ratios rim of M < 6000 genes [open reading frames (ORFs) labeled by index m =1, ... , M] for each of the genomes. These data form an N 3 M matrix which can be interpreted either as a set of N vectors Xi, each of dimension M and characterizing the expression activity of one genome, or as M 300-dimensional vectors Ym, each characterizing one ORF. According to these two points of view, we can consider two types of mutual information. Mutual information be- tween two genomes i and i8, quantifying the similarities of their expression pro®les, can be obtained by forming the M two-dimensional vectors ym = srim , ri8md which can be under- stood as 2D projections of Ym, and estimating the MI of this cloud of M 2D points. Alternatively, one can estimate simi- larities between two ORFs m and m8 by forming the N vec- tors xi = srim , rim8d and estimating the MI of the distribution represented by them. These MIs can then be used instead of covariance matrices to improve cluster analyses. In the following, we shall only follow the second alterna- tive, i.e., we only estimate MIs between ORFs, simply be- cause we want to compare our results with those of [6] where the authors also considered only the MI between ORFs. Bio- logically of interest are both alternatives. We shall not dis- cuss the subsequent cluster analysis, since this can be done with standard algorithms [31](a clustering algorithm speci®c to MI used as a (dis) similarity measure will be discussed elsewhere [32]).In [6] it was found that kernel density esti- mators performed much better than estimators based on bin- FIG. 12. Ratios IsX , Ydestim/ IexactsX , Yd for the ordered Weinman exponential distribution, plotted against 1 / N. Full lines are from estimator Is2d, dashed lines are from [29]. Our data were obtained with k = 1 after a transformation to logarithms. The four curves cor- respond to u= 0.1, 0.3, 1.0, and 100.0 (from top to bottom). FIG. 13. Ratios Is1,2dsX , Y , Zd / IexactsX , Y , Zd for k = 1 plotted against 1 / N, for four different values of r. All Gaussians have unit variance and all nondiagonal elements in the correlation matrix si,k , i Þ k (correlation coef®cients) take the value r. FIG. 14. Averages of Is1,2d(X1 , sX2 ¯ Xmd) for k = 1 plotted against m for three different values of r = 0.1, 0.5, 0.9. The sample size is 50 000; averaging is done over 100 realizations (same pa- rameters as in [30], Fig. 1). Full lines indicate theoretical values, pluses s+d are for Is1d, and crosses s3d are for Is2d. Squares and dotted lines are read off from Fig. 1 of Ref. [30]. KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-10 ning, but that the estimated MIs were so strongly correlated to linear correlation coef®cients that they hardly carried more useful information. Let us ®rst reinvestigate the MI estimates of the four ORF pairs ªAº to ªDº shown in Figs. 3, 5, and 7 of [6]. The claim that KDE was superior to binning was based on a surrogate analysis. For surrogates consisting of completely indepen- dent pairs, KDE was able to show that all four pairs were signi®cantly dependent, while binning-based estimators could disprove the null hypothesis of independence only for two pairs. In addition, KDE had both smaller statistical and systematic errors. Both KDE and binning estimators were applied to rank-ordered data [6]. In KDE, the densities are approximated by sums of N Gaussians with ®xed prescribed width h centered at the data points. In the limit h ! 0 the estimated MI diverges, while it goes to zero for h ! `. Our main criticism of [6] is that the authors used a very large value of h (roughly 1 2 to 1 3 of the total width of the distribution). This is recommended in the literature [33], since both statistical and systematic errors would become too large for smaller values of h. But with such a large value of h one is insensitive to ®ner details of the distributions, and should not be surprised to ®nd hardly anything beyond linear correlations. With our present estimators Is1d and Is2d we found indeed considerably larger statistical errors, when using small values of k (k , 10, say). But when using k < 50 (corresponding to Îk / N < 0.4, similar to the ratio h / s used in [6]), the statisti- cal errors were comparable to those in [6]. Systematic errors could be estimated by using the exact inequality Eq. (A5) given in the Appendix [when applying this, one has of course to remember that the estimate of the correlation coef®cient also contains errors which lead to systematic overestimation of the right-hand side of Eq. (A5)[8]]. For instance, for pair ªBº one ®nds I . 1.1 from Eq. (A5). While this is satis®ed for k , 5 within the expected uncertainty, it is violated both by the estimate of [6] sI < 0.9d and by our estimate for k =50 sI < 0.7d. With our method and with k < 50, we could also show that none of the four pairs is independent, with roughly the same signi®cance as in [6]. Thus the main advantage of our method is that it does not deteriorate as quickly as KDE does for high resolution. In addition, it seems to be faster, although the precise CPU time depends on the accuracy of the integration needed in KDE. In [6] also a simpli®ed algorithm is given [Eq. (33) of [6]] where the integral is replaced by a sum. Although it is sup- posed to be faster than the algorithm involving numerical integration (on which were based the above estimates),itis much slower than our present estimators [it is OsN2d and involves the evaluation of 3N2 exponential functions]. This simpli®ed algorithm (which is indeed just a generalized cor- relation sum with the Heaviside step function replaced by Gaussians) gives also rather big systematic errors, e.g., I = 0.66 for pair ªB.º Only this simpli®ed algorithm was used in[6] to estimate the MIs between all MsM −1d / 2 pairs of ORFs. When plot- ted against the (estimated) correlation coef®cients Csm , m8d, this gave a narrow half-moon-shaped distribution whose width was not signi®cantly larger than the estimated uncer- tainty (see Fig. 8 of [6]). In Fig. 16 we show our own results. We used the estimator Is1d with k = 30. Since the experimental data contained some outliers, we ®rst transformed to uniform density by rank-ordering the data. Without that, both Is1d and also the linear correlation would have been heavily biased for some pairs. In view of the inequality Eq. (A5) we actu- ally plot Is1dsm , m8d + 1 2 lnf1− C2sm , m8dg. From Fig. 16 we see several things: First of all, if the ORFs m and m8 were independent, we should have Is1d < 0 on average. This is not the case, even for Csm , m8d = 0. Sec- ondly, the average of Is1d + 1 2 lnf1− C2sm , m8dg for ®xed Csm , m8d is positive for all Csm , m8d. Thus MI is in general not uniquely given by Csm , m8d, and MI carries more infor- mation than linear correlations do. Third, from the violation of the inequality Is1dsm , m8d + 1 2 lnf1− C2sm , m8dg ù 0 one can estimate statistical errors. They are <0.03. Finally, while Is1dsm , m8d + 1 2 lnf1− C2sm , m8dg is roughly constant for Csm , m8d , 0.3, it grows sharply for large positive correla- tions. This effect seems not to be due to systematic or statis- tical errors. Indeed, systematic errors (which increase with k) would bring these points down, and the effect would not be FIG. 15. Standard deviations of the estimate Is1d for Gaussian deviates with unit variance and covariance r = 0.9, multiplied by ÎN / m and plotted against k. Each curve corresponds to a ®xed value of dimension m. Number of samples is N = 10 000. FIG. 16. Estimates Is1dsm , m8d − s1/2dlnf1− C2sm , m8dg for all pairs sm , m8d of ORFs, plotted against C2sm , m8d. According to Eq. (A5), this should be positive, which gives an indication of the errors involved in the estimation. ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-11 visible for k . 50. It would be interesting to see what these highly correlated ORF pairs are and why their MI is even higher than suggested by linear correlations, but we shall not pursue this here. B. ICA Independent component analysis (ICA) is a statistical method for transforming an observed multicomponent data set (e.g., a multivariate time series comprising n measure- ment channels) xstd = (x1std , x2std , ... , xnstd) into components that are statistically as independent from each other as possible [4]. In the simplest case, xstd could be a linear superposition of n independent sources sstd = (s1std , s2std , ... , snstd), xstd = Asstd, s37d where A is a nonsingular n 3 n ªmixingº matrix. In that case, we know that a decomposition into independent components is possible, since the inverse transformation sstd = Wxstd with W = A−1 s38d does exactly this. If Eq. (37) does not hold, then no decom- position into strictly independent components is possible by a linear transformation like Eq. (38), but one can still search for the least dependent components. In a slight misuse of notation, this is still called ICA. But even if Eq. (37) does hold, the problem of blind source separation (BSS), i.e., ®nding the matrix W without explicitly knowing A, is not trivial. Basically, it requires that x is such that all superpositions s8 = W8x with W8 Þ W are not independent. Since linear combinations of Gaussian vari- ables are also Gaussian, BSS is possible only if the sources are not Gaussian. Otherwise, any rotation (orthogonal trans- formation) s8 = Rs would again lead to independent compo- nents, and the original sources s could not be uniquely re- covered. This leads to basic performance tests for any ICA prob- lem: (i) How independent are the found ªindependentº compo- nents? (ii) How unique are these components? (iii) How robust are the estimated dependences against noise, against statistical ¯uctuations, and against outliers? (iv) How robust are the estimated components? Different ICA algorithms can then be ranked by how well they perform, i.e., whether they ®nd indeed the most inde- pendent components, whether they declare them as unique if and only if they indeed are, and how robust are the results. While questions (ii) and (iv) have often been discussed in the ICA literature (for a particularly interesting recent study, see [34]), the ®rst (and most basic, in our opinion) test has not attracted much interest. This might seem strange since MI is an obvious candidate for measuring independence, and the importance of MI for ICA was noticed from the very begin- ning. We believe that the reason was the lack of good MI estimators. We propose to use our MI estimators not only for testing the actual independence of the components found by standard ICA algorithms, but also to use them for testing for uniqueness and robustness. We will also show how our esti- mators can be used for improving the decomposition ob- tained from a standard ICA algorithm, i.e., for ®nding com- ponents which are more independent. Algorithms which use our estimators for ICA from scratch will be discussed else- where. It is useful to decompose the matrix W into two factors, W = RV, where V is a prewhitening that transforms the co- variance matrix into C8 = VCVT = 1, and R is a pure rotation. Finding and applying V is just a principal component analy- sis (PCA) together with a rescaling, so the core of the ICA problem reduces to ®nding a suitable rotation after having the data prewhitened. In the following we always assume that the prewhitening (PCA) step has already been done. Any rotation can be represented as a product of rotations which act only in some 2 3 2 subspace, R = pi,j Rijsfd, where Rijsfdsx1, ¯ xi ¯ xj ¯ xnd = sx1 ¯ xi8 ¯ xj8 ¯ xnd s39d with xi8 = cos fxi + sin fxj, xj8 = − sin fxi + cos fxj. s40d For such a rotation one has (see the Appendix) I—RijsfdXd − IsXd = IsXi8,Xj8d − IsXi,Xjd, s41d i.e., the change of IsX1 ¯ Xnd under any rotation can be com- puted by adding up changes of two-variable MIs. This is an important numerical simpli®cation. It would not hold if MI is replaced by some other similarity measure, and it indeed is not strictly true for our estimates Is1d and Is2d. But we found the violations to be so small that Eq. (41) can still be used when minimizing MI. Let us illustrate the application of our MI estimates to a fetal ECG recorded from the abdomen and thorax of a preg- nant woman (eight electrodes, 500 Hz, 5 s). We chose this data set because it was analyzed by several ICA methods [34,35] and is available on the web [37]. In particular, we will use both Is1d and Is2d to check and improve the output of the JADE algorithm [36](which is a standard ICA algorithm and was more successful with these data than TDSEP [38]; see [34]). The output of JADE for these data, i.e., the supposedly least dependent components, is shown in Fig. 17. Obviously channels 1±3 are dominated by the heartbeat of the mother, and channel 5 by that of the child. Channels 4 and 6 still contain large heartbeat components (of mother and child, respectively), but look much more noisy. Channels 7 and 8 seem to be dominated by noise, but with rather different spectral composition. The pairwise MIs of these channels are shown in Fig. 18 (left panel)[39]. One sees that most MIs are indeed small, but the ®rst three components are still highly interdependent. This could be a failure of JADE,orit could mean that the basic model does not apply to these components. To decide between these possibilities, we mini- mized IsX1 ¯ X8d by means of Eqs. (39)±(41). For each pair si , jd with i , j =1 ¯ 8 we found the angle which minimized KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-12 IsXi8 , Xj8d − IsXi , Xjd, and repeated this altogether <10 times. We did this both for Is1d and Is2d, with k = 1. We checked that IsX1 ... X8d, calculated directly, indeed decreased (from IJADE s1d = 1.782 to Imin s1d = 1.160 and from IJADE s2d = 2.264 to Imin s2d = 1.620). The resulting components are shown in Fig. 19. The ®rst two components look now much cleaner; all the noise from the ®rst three channels seems now concentrated in channel 3. But otherwise things have not changed very much. The pair- wise MI after minimization is shown in Fig. 18 (right panel). As suggested by Fig. 19, channel 3 is now much less depen- dent on channels 1 and 2. But the latter are still very strongly interdependent, and a linear superposition of independent sources as in Eq. (37) can be ruled out. This was indeed to be expected: In any oscillating system there must be at least two mutually dependent components involved, and generically one expects both to be coupled to the output signal. To test for the uniqueness of the decomposition, we com- puted the variances sij = 1 2pE 0 2p dffI—RsfdsXi,Xjd– − IsXi,Xjdg2, s42d where IsXi,Xjd = 1 2pE 0 2p dfI—RsfdsXi,Xjd–. s43d If sij is large, the minimum of the MI with respect to rota- tions is deep and the separation is unique and robust. If it is small, however, BSS cannot be achieved since the decompo- sition into independent components is not robust. Results for the JADE output are shown in Fig. 20 (left panel), and those for the optimized decomposition are shown in the right panel of Fig. 20. The most obvious difference between them is that the ®rst two channels have become much more clearly dis- tinct and separable from the rest, while channel 3 is less separable from the rest (except from channel 5). This makes sense, since channels 3, 4, 7, and 8 now contain mostly Gaussian noise, which is featureless and thus rotation invari- ant after whitening. Most of the signals are now contained in channel 5 (fetus) and in channels 1 and 2 (mother). FIG. 17. Estimated independent components using JADE. FIG. 18. Left panel: pairwise MIs between all ICA components obtained by JADE, estimated with Is1d , k = 1. The diagonal is set to zero. Right panel: pairwise MIs between the optimized channels shown in Fig. 19. FIG. 19. Estimated independent components after minimizing I1. FIG. 20. Square roots of variances, Îsij,of Is1dfsXi , Xjdg (with k =1) from JADE output (left panel) and after minimization of MI (right panel). Again, elements on the diagonal have been set to zero. ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-13 These results are in good agreement with those of [34], but are obtained with less numerical effort and can be inter- preted more straightforwardly. V. CONCLUSION We have presented two closely related families of mutual entropy estimators. Each family is parametrized by an inte- ger k ù 1 and uses kth neighbor distance statistics in the joint space. In general they perform very similarly, as far as CPU times, statistical errors, and systematic errors are concerned. Choosing small k reduces in general systematic errors, while large k leads to smaller statistical errors. The choice of the particular estimator depends thus on the size of the data sample and on whether bias or variance is to be minimized. Their biggest advantage seems to be in vastly reduced systematic errors (in particular for small k) when compared to previous estimators. This allows us to use them on very small data sets (even fewer than 30 points gave good results). It also allows us to use them in independent component analyses to estimate absolute values of mutual dependences. Traditionally, contrast functions have been used in ICA which allow us to minimize MI but not to estimate its abso- lute value. We expect that our estimators will also become useful in other ®elds of time series and pattern analysis. One large class of problems is interdependences in physiological time series, such as breathing and heartbeat, or in the output of different EEG channels. The latter is particularly relevant for diseases characterized by abnormal synchronization, such as epilepsy or Parkinson's disease. In the past, various mea- sures of interdependence have been used, including MI. But the latter was not employed extensively (see, however, [40]), mainly because of the supposed dif®culty in estimating it reliably. We hope that the present estimators might change this situation. ACKNOWLEDGMENTS One of us (P.G.) wants to thank Georges Darbellay for extensive and very fruitful e-mail discussions. We also want to thank Ralph Andrzejak, Thomas Kreuz, and Walter Nadler for numerous fruitful discussions, and for critically reading the manuscript. APPENDIX We collect here some well-known facts about MI, in par- ticular for higher dimensions, and some immediate conse- quences. The ®rst important property of IsX , Yd is its inde- pendence with respect to reparametrizations. If X8 = FsXd and Y8 = GsYd are homeomorphisms (smooth and uniquely invert- ible maps), and JX = i]X / ]X8i and JY = i]Y / ]Y8i are the Ja- cobi determinants, then m8sx8,y8d = JXsx8dJYsy8dmsx,ydsA1d and similarly for the marginal densities, which gives IsX8,Y8d =EE dx8dy8m8sx8,y8dlog m8sx8,y8d mx8sx8dmy8sy8d =EE dxdymsx,ydlog msx,yd mxsxdmysyd = IsX,Yd. sA2d The next important property, checked also directly from the de®nitions, is IsX,Y,Zd = I—sX,Yd,Z– + IsX,Yd. sA3d This is analogous to the additivity axiom for Shannon entro- pies [1], and says that MI can be decomposed into hierarchi- cal levels. By iterating it, one can decompose IsX1 ¯ Xnd for any n . 2 and for any partitioning of the set sX1 ¯ Xnd into the MI between elements within one cluster and MI between clusters. Let us now consider a homeomorphism sX8 , Y8d = FsX , Yd. By combining Eqs. (A2) and (A3), we obtain IsX8,Y8,Zd = I—sX8,Y8d,Z– + IsX8,Y8d = I—sX,Yd,Z– + IsX8,Y8d = IsX,Y,Zd + fIsX8,Y8d − IsX,Ydg. sA4d Thus, changes of high-dimensional redundancies under rep- arametrization of some subspace can be obtained by calcu- lating MIs in this subspace only. Although this is a simple consequence of well-known facts about MI, it seems to have not been noticed before. It is numerically extremely useful, and would not hold in general for other interdependence measures. Again it generalizes to any dimension and to any number of random variables. It is well known that Gaussian distributions maximize the Shannon entropy for given ®rst and second moments. This implies that the Shannon entropy of any distribution is bounded from above by s1/2dlog det C, where C is the co- variance matrix. For MI one can prove a similar result: For any multivariate distribution with joint covariance matrix C and variances si = Cii for the individual (scalar) random vari- ables Xi, the redundancy is bounded from below, IsX1, ¯ ,Xmd ù 1 2 log det C s1 ¯ sm . sA5d The right-hand side of this inequality is just the redundancy of the corresponding Gaussian, and to prove Eq. (A5) we must show that the distribution minimizing the MI is Gauss- ian. In the following we sketch only the proof for the case of two variables X and Y, the generalization to m . 2 being straightforward. We also assume without loss of generality that X and Y have zero mean. To prove Eq. (A5), we set up a minimization problem where the constraints [correct nor- malization and correct second moments; consistency rela- tions mxsxd = edy msx , yd and mysyd = edx msx , yd] are taken KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-14 into account by means of Lagrangian multipliers. The ªLa- grangian equationº dL /dmsx , yd = 0 leads then to msx,yd = 1 Zmxsxdmysyde−ax2−by2−cxy, sA6d where Z , a , b, and c are constants ®xed by the constraints. Since the minimal MI decreases when the variances sx = Cxx and sy = Cyy increase with Cxy ®xed, the constants a and b are non-negative. Equation (A6) is obviously consis- tent with msx , yd being a Gaussian. To prove uniqueness, we integrate Eq. (A6) over y and set x =−iz / c to obtain Ze−az2/c2 =E dy eizyfmysyde−by2g. sA7d This shows that e−by2mysyd is the Fourier transform of a Gaussian, and thus mysyd is also Gaussian. The same holds true of course for mxsxd, showing that the minimizing msx , yd must be Gaussian, QED. Finally, we should mention some possibly confusing no- tations. First, MI is often also called transinformation or re- dundancy. Secondly, what we call higher-order redundancies are called higher-order MIs in the ICA literature. We did not follow that usage in order to avoid confusion with cumulant- type higher-order MIs [41]. [1] T. M. Cover and J. A. Thomas, Elements of Information Theory (Wiley, New York, 1991). [2] A. Renyi, Probability Theory (North Holland, Amsterdam, 1971). [3] Independent Component Analysis: Principles and Practice, ed- ited by S. Roberts and R. Everson (Cambridge Univ. Press, Cambridge, 2001). [4] A. Hyvärinen, J. Karhunen, and E. Oja, Independent Compo- nent Analysis (Wiley, New York, 2001). [5] Y.-I. Moon, B. Rajagopalan, and U. Lall, Phys. Rev. E 52, 2318 (1995). [6] R. Steuer, J. Kurths, C. O. Daub, J. Weise, and J. Selbig, Bio- informatics 18 (Suppl. 2), S231 (2002). [7] A. M. Fraser and H. L. Swinney, Phys. Rev. A 33, 1134 (1986). [8] G. A. Darbellay and I. Vajda, IEEE Trans. Inf. Theory 45, 1315 (1999). [9] P. Grassberger, Phys. Lett. A 128, 369 (1988). [10] M. S. Roulston, Physica D 125, 285 (1999). [11] R. L. Dobrushin, Theor. Probab. Appl. 3, 462 (1958). [12] O. Vasicek, J. R. Stat. Soc. Ser. B. Methodol. 38,54 (1976). [13] A. Kaiser and T. Schreiber, Physica D 166,43 (2002). [14] E. S. Dudewicz and E. C. van der Meulen, J. Am. Stat. Assoc. 76, 967 (1981). [15] B. van Es, Scand. J. Stat. 19,61 (1992). [16] N. Ebrahimi, K. P¯ughoeft, and E. S. Soo®, Stat. Probab. Lett. 20, 225 (1994). [17] J. C. Correa, Commun. Stat: Theory Meth. 24, 2439 (1995). [18] A. B. Tsybakov and E. C. van der Meulen, Scand. J. Stat. 23, 75 (1996). [19] R. Wieczorkowski and P. Grzegorzewksi, Commun. Stat.- Simul. Comput. 28, 541 (1999). [20] L. F. Kozachenko and N. N. Leonenko, Probl. Inf. Transm. 23, 95 (1987). [21] P. Grassberger, Phys. Lett. 107A, 101 (1985). [22] R. L. Somorjai, Methods for Estimating the Intrinsic Dimen- sionality of High-Dimensional Point Sets, in Dimensions and Entropies in Chaotic Systems, edited by G. Mayer-Kress (Springer, Berlin, 1986). [23] J. D. Victor, Phys. Rev. E 66, 051903 (2002). [24] G. A. Darbellay and I. Vajda, IEEE Trans. Inf. Theory 46, 709 (2000). [25] W. H. Press et al., Numerical Recipes (Cambridge Univ. Press, New York, 1993). [26] P. Grassberger, Phys. Lett. A 148,63 (1990). [27] R. Hegger, H. Kantz, and T. Schreiber, TISEAN: Nonlinear Time Series Analysis Software Package (ULR: www.mpipks- dresden.mpg.de//Ätisean). [28] G. A. Darbellay, Comput. Stat. Data Anal. 32,1 (1999). [29] G. A. Darbellay and I. Vajda, Inst. of Information Theory and Automation, Technical Rep. No. 1921 (1998), to be obtained from http://siprint.utia.cas.cz/darbellay/ [30] G. A. Darbellay, 3rd IEEE European Workshop on Computer- intensive Methods in Control and Data Processing, Prague, 1998 (IEEE, Piscataway, NJ, 1999),p.83. [31] T. R. Hughes et al., Cell 102, 109 (2000); see also www.rii.com/publications/2000/cell_hughes.htm [32] H. Stögbauer, A. Kraskov, S. A. Astakhov, and P. Grassberger, e-print physics/0405044. [33] B. W. Silverman, Density Estimation for Statistics and Data Analysis (Chapman and Hall, London, 1986). [34] F. Meinecke, A. Ziehe, M. Kawanabe, and K.-R. Mller, IEEE Trans. Biomed. Eng. 49, 1514 (2002). [35] J.-F. Cardoso, Multidimensional Independent Component Analysis, Proceedings of ICASSP '98 (IEEE, Piscataway, NJ, 1998). [36] J.-F. Cardoso and A. Souloumiac, IEE Proc. F, Radar Signal Process. 140, 362 (1993). [37] Daisy: Database for the identi®cation of systems, edited by B. L. R. De Moor, www.esat.kuleuven.ac.be/sista/daisy (1997). [38] A. Ziehe and K.-R. Mller, TDSEPÐ An Ef®cient Algorithm for Blind Separation Using Time Structure, in Proceedings of the 8th International Conferences on Arti®cial Neural Networks, ICANN'98, edited by L. Niklasson et al. (Springer, Berlin, 1998), p. 675. [39] In Figs. 18±20 we used k = 1, since the time sequences were ESTIMATING MUTUAL INFORMATION PHYSICAL REVIEW E 69, 066138 (2004) 066138-15 suf®ciently long to give very small statistical errors. To ®nd components as independent as possible, we should have used much larger k, since this would reduce statistical errors at the cost of increased but nevertheless very small systematic errors. We checked that basically the same results were obtained with k up to 100. [40] B. Pompe, P. Blidh, D. Hoyer, and M. Eiselt, IEEE Eng. Med. Biol. Mag. 17,32 (1998). [41] H. Matsuda, Phys. Rev. E 62, 3096 (2000). [42] See http://siprint.utia.cas.cz/timeseries/ KRASKOV, STÖGBAUER, AND GRASSBERGER PHYSICAL REVIEW E 69, 066138 (2004) 066138-16","libVersion":"0.3.1","langs":""}
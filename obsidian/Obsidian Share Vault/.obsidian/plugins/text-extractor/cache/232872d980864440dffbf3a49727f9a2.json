{"path":"lit/lit_notes_OLD_PARTIAL/Agarwal22multivarSinglSpect.pdf","text":"ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS BY ANISH AGARWAL 1,a, ABDULLAH ALOMAR 1,b AND DEVAVRAT SHAH 1,c 1Electrical Engineering and Computer Science, Massachusetts Institute of Technology aanish90@mit.edu; baalomar@mit.edu; cdevavrat@mit.edu We introduce and analyze a variant of multivariate singular spectrum analysis (mSSA), a popular time series method to impute and forecast a multivariate time series. Under a spatio-temporal factor model we introduce, given N time series and T observations per time series, we establish pre- diction mean-squared-error for both imputation and out-of-sample forecast- ing effectively scale as 1/ √min(N, T )T . This is an improvement over: (i) 1/√ T error scaling of SSA, the restriction of mSSA to a univariate time se- ries; (ii) 1/ min(N, T ) error scaling for matrix estimation methods which do not exploit temporal structure in the data. The spatio-temporal model we introduce includes any ﬁnite sum and products of: harmonics, polynomi- als, differentiable periodic functions, and Hölder continuous functions. Our out-of-sample forecasting result could be of independent interest for online learning under a spatio-temporal factor model. Empirically, on benchmark datasets, our variant of mSSA performs competitively with state-of-the-art neural-network time series methods (e.g. DeepAR, LSTM) and signiﬁcantly outperforms classical methods such as vector autoregression (VAR). Finally, we propose extensions of mSSA: (i) a variant to estimate time-varying vari- ance of a time series; (ii) a tensor variant which has better sample complexity for certain regimes of N and T . Keywords and phrases: multivariate singular spectrum analysis, time series analysis, out-of-sample forecast- ing, generalization, high-dimensional statistics, error-in-variables, singular value thresholding, low-rank matrices, missing data, matrix and tensor completion. 1arXiv:2006.13448v5 [cs.LG] 19 Jun 2022 2 CONTENTS 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1 Multivariate Singular Spectrum Analysis . . . . . . . . . . . . . . . . . . . . 3 1.2 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.1 Spatio-Temporal Factor Model . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.2 A Diagnostic Test for the Spatio-Temporal Model . . . . . . . . . . . . . . . 11 4 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Finite-sample Analysis for Imputation and Forecasting . . . . . . . . . . . . 14 5 Approximate Low-Rank Hankel Representation . . . . . . . . . . . . . . . . . . . 15 5.1 Approximate Low-rank Hankel Representation and Hankel Calculus . . . . . 15 5.2 Examples of (G, ϵ)-Hankel Time Series . . . . . . . . . . . . . . . . . . . . 16 5.3 Extending Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 6.1 Imputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 6.2 Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 7 Algorithmic Extensions of mSSA . . . . . . . . . . . . . . . . . . . . . . . . . . 23 7.1 Variance Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 7.2 Tensor SSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 A Page vs. Hankel mSSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.2 Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.3 Parameters Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 C Time-varying Recommendation Systems . . . . . . . . . . . . . . . . . . . . . . . 31 D Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 E Proofs For Section 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.1 Proof of Proposition 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.2 Proof of Proposition 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.3 Proof of Proposition 5.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.4 Proof of Proposition 5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 F Helper Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 G Matrix Estimation via HSVT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.1 Setup, Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.2 Matrix Estimation using HSVT . . . . . . . . . . . . . . . . . . . . . . . . . 40 G.3 A Useful Linear Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 G.4 HSVT based Matrix Estimation: A Deterministic Bound . . . . . . . . . . . 41 G.5 HSVT based Matrix Estimation: Deterministic To High-Probability . . . . . 43 H Proof of Theorem 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 I Proof of Theorem 5.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 I.1 Proof of Proposition 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 J Proof of Theorem 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 K Proof of Theorem 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 L tSSA Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 L.1 Proof of Proposition 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 L.2 Proof of Proposition 7.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 L.3 Proof of Proposition C.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 3 1. Introduction. Multivariate time series data is of great interest across many applica- tion areas, including cyber-physical systems, ﬁnance, retail, healthcare to name a few. An important goal across these domains can be summarized as accurate imputation and forecast- ing of a multivariate time series in the presence of noisy and/or missing data. Setup. We consider a discrete time setting with time indexed as t ∈ Z. For N ∈ N, let the collection fn : Z → R, n ∈ [N ] := {1, . . . , N } be the latent time series of interest. For t ∈ [T ] and n ∈ [N ], we observe Xn(t) where for ρ ∈ (0, 1], Xn(t) = { fn(t) + ηn(t) with probability ρ ⋆ with probability 1 − ρ. (1) Here ⋆ represents a missing observation and ηn(t) represents the per-step noise, which we assume to be an independent (across t, n) mean-zero random variable. Though ηn(t) is inde- pendent, we note that the underlying time series, fn(·), is of course strongly dependent across t, n. Indeed the presence of per-step noise ηn(t) and missing values (denoted by ⋆) represent an additional challenge of measurement error in our setup. The generic spatio-temporal factor model for fn(·), n ∈ [N ] described in Section 3 without additional noise ηn(·) or missingness already provides an expressive model for a time series including any ﬁnite sum of products of harmonics and polynomials, any differentiable periodic function, and any Hölder continuous function. Goal. Our objective is two-folds, for n ∈ [N ]: (i) imputation – estimating fn(t) for all t ∈ [T ]; (ii) out-of-sample forecasting – predicting fn(t) for t > T . 1.1. Multivariate Singular Spectrum Analysis. Multivariate singular spectrum analy- sis (mSSA) is a known method to impute and forecast a multivariate time series (see [10, 28, 18, 27, 23, 22, 9]). mSSA has been used for both imputation and forecasting, and signal extraction—decomposing a time series into a small number of simpler time series (e.g., periodic, trend, autoregressive component). However, despite its heavy use in practice, the theoretical properties of mSSA are not well understood. Hence, we introduce a variant of mSSA for which we provide a rigorous ﬁnite-sample analysis of its imputation and out- of-sample forecasting properties; such a ﬁnite-sample analysis of mSSA has been missing from the literature. We note that we do not focus on the task of signal extraction which we leave as important future work. The variant of mSSA we introduce is arguably much sim- pler to implement than the original mSSA method and we begin by describing it in detail below. In Section 2, we compare the original mSSA method with this variant and discuss key differences. See Figure 1 for a visual depiction of the key steps in this variant of mSSA. Singular spectrum analysis (SSA). For ease of exposition and to build intuition, we start with N = 1, i.e. a univariate time series. There are two algorithmic parameters: 1 ≤ L ≤ T and k ≥ 1. For simplicity and without loss of generality assume that T is an integer multiple of L, i.e. T /L ∈ N and k ≤ min(L, T /L). When T /L /∈ N, by applying both the imputation and forecasting algorithms for two ranges, 1, . . . , ⌊T /L⌋ x L and (T mod L) + 1, . . . , T , this condition will be satisﬁed in each range and will provide imputation and forecasting for all T . Here ⌊T /L⌋ refers to the ﬂoor of T /L. We give guidance on how to pick L and k when we discuss our theoretical results. First, transform the time series X1(t), t ∈ [T ] into an L × T /L matrix where the entry of the matrix in row i ∈ [L] and column j ∈ [T /L] is X1(i + (j − 1) x L). This matrix induced by the time series is called the Page matrix, and we denote it as P(X1, T, L). 4 Fig 1: Key steps of our proposed variant of the mSSA algorithm. Imputation. After replacing missing values (i.e. ⋆) in the matrix P(X1, T, L) by 0, we com- pute its singular value decomposition, which we denote as P(X1, T, L) = min(L,T /L)∑ ℓ=1 sℓuℓvT ℓ , where s1 ≥ s2 · · · ≥ smin(L,T /L) ≥ 0 denote its ordered singular values, and uℓ ∈ RL, vℓ ∈ RT /L denote its left and right singular vectors, respectively, for ℓ ∈ [min(L, T /L)]. Let ̂ρ1 be the fraction of observed entries of X1, precisely deﬁned as (max(1, ∑T t=1 1(X1(t) ̸= ⋆)))/T . Let the normalized, truncated version of P(X1, T, L) be ̂P(X1, T, L; k) = 1 ̂ρ1 k∑ ℓ=1 sℓuℓvT ℓ ,(2) i.e., we perform Hard Singular Value Thresholding (HSVT) on P(X1, T, L) to obtain ̂P(X1, T, L; k). We then deﬁne the de-noised and imputed estimate of the original time series, denoted by ̂f1, as follows: for t ∈ [T ], ̂f1(t) equals the entry of ̂P(X1, T, L; k) in row (t − 1 mod L) + 1 and column ⌈t/L⌉. Here ⌈t/L⌉ refers to the ceiling of t/L. Forecasting. To forecast, we learn a linear model ˆβ(X1, T, L; k) ∈ RL−1, which is the solu- tion to minimize T /L∑ m=1 (ym − βT xm)2 over β ∈ RL−1, where ym = (1/̂ρ1)X1(L x m), xm = [ ̂f1(L x (m − 1) + 1) . . . ̂f1(L x (m − 1) + L − 1)] for m ∈ [T /L]. 1 Note to deﬁne ym we impute missing values in X1 by 0. We now describe how to use ˆβ(X1, T, L; k) to produce both in-sample and out-of-sample fore- casts. (i) In-sample forecast: for time t = L x m and m ∈ [T /L], the forecast is given by ¯f1(L x m) = ˆβ(X1, T, L; k)T xm . (ii) Out-of-sample forecast: for m > T /L, i.e., for time 1To establish theoretical results for the forecasting algorithm, we produce estimates ( ̂f1(L x (m − 1) + 1) . . . ̂f1(L x (m − 1) + L − 1)) for m ∈ [T /L] by applying the imputation algorithm on P(X1, T, L) after setting its Lth row equal to 0. Also, ̂ρ1 in the deﬁnition of ym is computed using only the ﬁrst L − 1 rows of P(X1, T, L). This avoids dependencies in the noise between ym and xm for m ∈ [T /L]. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 5 t > T , the forecast is given by ¯f1(L x m) = ˆβ(X1, T, L; k)T x′ m where x′ m = 1 ̂ρ1 [X1(L x (m − 1) + 1) . . . X1(L x (m − 1) + L − 1)] after imputing missing values in X1 by 0. Multivariate singular spectrum analysis (mSSA). Below we describe the variant of mSSA we propose, which is an extension of the SSA algorithm described above, to when we have a multivariate time series, i.e., N > 1. The key change is in the ﬁrst step where we con- struct the Page matrix—instead of considering the Page matrix of a single time series, we now consider a ‘stacked’ Page matrix, which is obtained by a column-wise concatenation of the Page matrices induced by each time series separately. Speciﬁcally, like SSA, it has two algorithmic parameters, L ≥ 1 and k ≥ 1. For each time series, n ∈ [N ], create its L × T /L Page matrix P(Xn, T, L), where the entry in row i ∈ [L] and column j ∈ [T /L] is Xn(i + (j − 1) x L). We then create a stacked Page matrix from these N time series by per- forming a column wise concatenation of the N matrices, P(Xn, T, L), n ∈ [N ]. We denote this matrix as SP((X1, . . . , XN ), T, L), and note that it has L rows and N x T /L columns. Imputation. We replace missing values (i.e. ⋆s) in SP((X1, . . . , XN ), T, L) by 0. Simi- lar to (2), we perform HSVT on SP((X1, . . . , XN ), T, L) and denote its normalized, trun- cated version as ̂SP((X1, . . . , XN ), T, L; k) (instead of ̂ρ1, we now normalize by ̂ρ := (max(1, ∑N n=1 ∑T t=1 1(Xn(t) ̸= ⋆)))/N T ). From ̂SP((X1, . . . , XN ), T, L; k), like in SSA, we can read off ˆfn(t) for n ∈ [N ], t ∈ [T ], the de-noised and imputed estimate of the N time series over T time steps. In particular, let ̂P(Xn, T, L; k) refer to sub-matrix of ̂SP((X1, . . . , XN ), T, L; k) induced by selecting only its [(n − 1) x (T /L) + 1, . . . , n x T /L] columns. Then for t ∈ [T ], ˆfn(t) equals the entry of ̂P(Xn, T, L; k) in row (t − 1 mod L) + 1 and column ⌈t/L⌉. Forecasting. Similar to SSA, to forecast, we learn a linear model ˆβ((X1, . . . , XN ), T, L; k) ∈ RL−1, which is the solution to minimize N x T /L∑ m=1 (ym − βT xm) 2 over β ∈ RL−1,(3) where ym is the mth component of (1/̂ρ)[X1(L), X1(2 x L), . . . , X1(T ), X2(L), . . . , X2(T ), . . . , XN (T )] ∈ RN x T /L, and xm ∈ RL−1 corresponds to the vector formed by the entries of the ﬁrst L − 1 rows in the mth column of ̂SP((X1, . . . , XN ), T, L; k)2 for m ∈ [N x T /L]. Note, to deﬁne ym, we impute missing values in X1, . . . , Xn by 0. (i) In-sample forecast: for time step t = L x m′ for m′ ∈ [T /L] and for time series n ∈ [N ], the forecast is given by ¯fn(L x m′) = ˆβ((X1, . . . , XN ), T, L; k)T xm where m = m′ + (n − 1) x T /L. (ii) Out-of-sample forecast: for m′ > T /L, i.e., for time t > T , and for time series n ∈ [N ], the forecast is given by ¯fn(L x m′) = ˆβ((X1, . . . , XN ), T, L; k)T x′ m′, where x′ m′ = 1 ̂ρ [Xn(L x m′ − (L − 1)) . . . Xn(L x m′ − 1)] after imputing missing values in Xn by 0. See Figure 1 for a visual depiction of the key steps above. Page vs. Hankel mSSA. See Appendix A for a detailed discussion of the various beneﬁts and drawbacks of the using the Page matrix representation as we propose in our variant, instead of the Hankel representation used in the original mSSA. 2Similar to the SSA forecasting algorithm, when creating a forecasting model in mSSA, we produce ̂SP((X1, . . . , XN ), T, L; k) by ﬁrst setting the Lth row of SP((X1, . . . , XN ), T, L; k) equal to zero before per- forming the SVD and the subsequent truncation. Also, ̂ρ in the deﬁnition of ym is computed only using the ﬁrst L − 1 rows of SP((X1, . . . , XN ), T, L; k). 6 Empirical performance of mSSA. This variant of mSSA we propose is fully described above, with its two major steps consisting of simply singular value thresholding and ordi- nary least squares. A key question is how well does it perform empirically? In Table 1, we provide a summary comparison of mSSA’s performance for imputation and forecasting on benchmark datasets with respect to state-of-the-art time series algorithms. We ﬁnd that by using the stacked Page matrix in mSSA, it greatly improves performance over SSA; indicat- ing that mSSA is effectively utilizing information across multiple time series. Surprisingly, our variant of mSSA performs competitively or outperforms popular neural network based methods, such as LSTM and DeepAR—we note that these state-of-the-art neural network based methods have no associated theoretical analysis. Further, it signiﬁcantly outperforms classical multivariate forecasting methods such as VAR. Indeed, apart from its use in practice, the empirical performance of (our variant of) mSSA strongly motivates a theoretical analysis of when and why mSSA works. TABLE 1 mSSA statistically outperforms SSA, other state-of-the-art algorithms, including LSTMs and DeepAR across many datasets. We use the average normalized root mean squared error (NRMSE) as our metric. Details of experiments run to produce results can are in Section 6. Mean Imputation (NRMSE) Mean Forecasting (NRMSE) Electricity Trafﬁc Synthetic Financial M5 Electricity Trafﬁc Synthetic Financial M5 mSSA 0.398 0.508 0.416 0.238 0.883 0.485 0.536 0.281 0.251 1.021 SSA 0.514 0.713 0.675 0.467 0.958 0.632 0.696 0.665 0.303 1.068 LSTM NA NA NA NA NA 0.558 0.478 0.559 1.205 1.034 DeepAR NA NA NA NA NA 0.479 0.464 0.415 0.316 1.050 TRMF 0.641 0.460 0.564 0.430 0.916 0.495 0.508 0.422 0.291 1.032 Prophet NA NA NA NA NA 0.569 0.614 1.010 1.286 1.100 VAR NA NA NA NA NA 1.291 1.092 2.987 1.218 1.120 1.2. Our Contributions. As our primary contribution, we provide an answer to the ques- tion posed above—under a spatio-temporal factor model that we introduce, the ﬁnite-sample analysis we carry out of mSSA’s estimation error for imputation and out-of-sample forecast- ing establishes consistency, as well as its ability to effectively utilize both the spatial and temporal structure in a multivariate time series. Below, we detail the various aspects of our contribution with respect to the: (a) spatio-temporal factor model; (b) ﬁnite sample analysis of mSSA; (c) algorithmic extensions (and associated theoretical analysis) of mSSA to do time-varying variance estimation, and a tensor variant of mSSA which we show has a better imputation error convergence rate compared to mSSA for certain relative scalings of N and T . Spatio-temporal factor model. Note that the collection of latent multivariate time series fn(t), for n ∈ [N ], t ∈ [T ] can be collectively viewed as a N × T matrix. To capture the spatial structure, i.e. the relationship across rows, we model this matrix to be low-rank— there exists a low-dimensional latent factor (or feature) associated with each of N time series; analogously, there exists a low-dimensional latent factor associated with each of the T time steps. To capture the temporal structure, we further assume that each component of the latent temporal factor has an approximately low-rank Hankel matrix representation (see Deﬁnition 3.1 for the Hankel matrix induced by a time series), i.e., the Hankel—and therefore Page— matrix induced by each component of the latent temporal factor is approximately low-rank. This additional structure imposed on the temporal factors is what motivates using the stacked Page matrix representation in mSSA, which is of dimension L × (N x T /L), where L is a ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 7 hyper-parameter. We note that for N = 1 this subsumes the model considered to explain the success of SSA in [1] as a special case. As stated earlier, our model is expressive in that it includes any ﬁnite sum of products of harmonics and polynomials, any differentiable periodic function, and any Hölder continuous function. Further, we establish that the set of time series that have an approximately low-rank Hankel representation is closed under component-wise addition and multiplication. Such a model calculus helps characterize the representational strength of the spatio-temporal factor model we introduce. Finite sample analysis of mSSA. Under the spatio-temporal factor model, we establish that mean squared imputation error scales as 1/√min(N, T )T (see Theorem 4.1) and the out-of- sample forecasting error scales as max(1/ √N T , N/T 2) (see Theorem 4.3, and Corollary 4.1). When N < T , the error rate is 1/ √N T . When N > T , one can simply divide the var- ious time series into sets of size O(T ); this will result in a mean squared error rate of 1/T . Hence, effectively the error is of order 1/ √min(N, T )T . For exact details on the relative scaling of N and T , please refer to Theorem 4.3. For N = 1, it implies that the SSA algo- rithm described above has imputation and forecasting error scaling as 1/√T . That is, mSSA improves performance by a √N factor over SSA by utilizing information across the N time series. This also improves upon the prior work of [1] which established the weaker result that SSA has imputation error scaling as 1/T 1 4 (i.e., when N = 1). Further [1] does not establish a result for the out-of-sample forecasting error of SSA. We note that the asymmetry in our ﬁnite-sample analysis between N and T is to be expected as we impose further structure on the latent temporal factors; they satisfy a low-rank Hankel representation, which is not assumed of the spatial factors. Further, existing matrix estimation based methods applied to the N × T matrix of time series observations (i.e, without ﬁrst performing the Page matrix transformation as done in mSSA) establish that the imputation prediction error scales as 1/ min(N, T ). This is indeed the primary result of the works [45, 29], as seen in Theorem 2 of [29]. 3 That is, while the algorithm stated in [45, 29] utilizes the temporal structure in addition to the spatial struc- ture, the theoretical guarantees do not reﬂect it—the guarantees provided by such methods are weaker (since 1/ min(N, T ) ≥ 1/√min(N, T )T ) than that obtained by mSSA. Again, we emphasize that the existing analysis of SSA and matrix estimation based methods (for example [1, 45, 29]) do not establish (ﬁnite-sample) bounds for out-of-sample forecasting error. Algorithmic extensions: variance and tensor SSA (tSSA). First, we extend mSSA to esti- mate the latent time-varying variance, i.e. E[η2 n(t)], n ∈ [N ], t ∈ [T ]. We establish the efﬁ- cacy of such an extension when the time-varying variance is also modeled through a spatio- temporal factor model. To the best of our knowledge, this is the ﬁrst result that provides provable ﬁnite-sample performance guarantees for estimating the time-varying variance of a time series. Second, we propose a novel tensor variant of SSA, termed tSSA, which exploits recent developments in the tensor estimation literature. In tSSA, rather than doing a column- wise stacking of the Page Matrices induced by each of the N time series to form a larger matrix, we instead view each Page matrix as a slice of a L × T /L × N order-three tensor. In other words, the entry of the tensor with indices i ∈ [L], j ∈ [T /L] and n ∈ [N ] equals the entry of P(Xn, L, T ) with indices i, j. In Proposition 7.2, with respect to imputation error, we characterize the relative performance of tSSA, mSSA, and “vanilla” matrix estimation (ME). We ﬁnd that when N = o(T 1/3), mSSA outperforms tSSA; when T 1/3 = o(N ), N = o(T ) 3There seems to be a typo in Corollary 2 of [45] in applying Theorem 2: square in Frobenius-norm error is missing. 8 tSSA outperforms mSSA; when T = o(N ), standard matrix estimation methods are equally as effective as mSSA and tSSA. See Figure 2 for a graphical depiction. N = T N = T 1 3 T N tSSA = mSSA = ME tSSA ≫ mSSA ≫ ME mSSA ≫ tSSA ≫ ME Fig 2: Relative effectiveness of tSSA, mSSA, ME for varying N, T . Summary of contributions. We now brieﬂy summarize our contributions: 1. A novel spatio-temporal factor model to analyze mSSA. We show that a large family of time series dynamics fall within our factor model. 2. Finite-sample analysis for imputation and out-of-sample forecasting. The tools we use for imputation borrow from the existing literature on matrix estimation. However, our out-of- sample forecasting requires making novel technical contributions. We believe these tools might be of interest for online learning with a spatio-temporal factor model. 3. A novel time-varying variance estimation algorithm with theoretical guarantees. To the best of our knowledge, neither such an algorithm nor an associated theoretical analysis exists. 4. A novel tensor variant of the mSSA algorithm called tSSA, which exploits recent devel- opments in the tensor estimation literature. We ﬁnd that when N is large compared to T , tSSA has better sample complexity compared to mSSA. We believe this tensor variant opens a direction to future work to understand the appropriate statistical and computa- tional trade offs for time series analysis. 2. Literature Review. Given the ubiquity of multivariate time series analysis, it will not be possible to do justice to the entire literature. We focus on a few techniques most relevant to compare against, either theoretically or empirically. SSA and mSSA. A good overview of the literature on SSA can be found in [19]. As alluded to earlier, the original SSA method differs from the variant discussed in [1] and in this work. The key steps of the original SSA method are: Step 1–create a Hankel matrix from the time series data; Step 2–do a Singular Value Decomposition (SVD) of it; Step 3–group the singular values based on user belief of the model that generated the process; Step 4–perform diagonal averaging to “Hankelize\" the grouped rank-1 matrices outputted from the SVD to create a set of time series; and Step 5–learn a linear model for each “Hankelized\" time series for the purpose of forecasting. The theoretical analysis of this original SSA method has been focused on proving that many univariate time series have a low-rank Hankel representation, and secondly on deﬁning sufﬁcient asymptotic conditions for when the singular values of the various time series components are separable, thereby justifying Step 3 of the method. Step 3 of the original SSA method requires user input and Steps 4 and 5 are not robust to noise ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 9 and missing values due to the strong dependence across entries of the Hankel representation of the time series. To overcome these limitations, in [1] a simpler and practically useful version as described in Section 1.1 was introduced. As discussed earlier, this work improves upon the analysis of [1] by providing stronger bounds for imputation prediction error, and gives new bounds for forecasting prediction error, which were missing in [1]. The original mSSA method, like the original SSA method, involves the ﬁve steps described above, but ﬁrst the Hankel matrices induced by each of the N time series are stacked either column- wise (horizontal mSSA) or row-wise (vertical mSSA); see [24]. We note given the popularity of mSSA, there are many algorithmic variants of it proposed in the literature motivated by different applications: see [10, 28, 18, 27, 23, 22, 9]. A signiﬁ- cant focus of these works is signal extraction, i.e., decomposing the observed time series into a small number of simpler time series (e.g., periodic, trend, autoregressive component); these extracted signals are then subsequently utilized for imputation and forecasting as described in the preceding paragraph. As stated earlier, despite the popularity of the mSSA framework, a rigorous ﬁnite-sample analysis of its imputation and out-of-sample forecasting properties are missing in the literature; the challenge in such an analysis is exacerbated with missing data and measurement error. In this work, as described in Section 1.1, we introduce a simpler variant of mSSA that uses the Page instead of the Hankel matrix representation. This variant is simpler as it focuses only on the task of imputation and forecasting, and not signal ex- traction. We do a ﬁnite-sample analysis of our variant of mSSA and establish its consistency with respect to imputation and forecasting, which so far has been missing from the mSSA literature. In Appendix A, we compare our variant to the original version of mSSA which use the Hankel matrix, both with respect to their theoretical and practical properties. Matrix factorization based methods for multivariate time series. There is a rich line of work in econometrics and statistics on viewing multiple time series as a matrix, and where some form of matrix factorization is performed to learn the spatial and temporal factors in- duced by the matrix; such models have also been called dynamic factor models. Some repre- sentative papers (and by no means exhaustive) include [35, 16, 21, 14, 5, 7]. [35] consider the estimation by principal components of this N × T matrix. They use the model for signal ex- traction and forecasting. Also, they proposed an expectation-maximization (EM) algorithm to handle missing data and imputation. [16, 21] also estimate principal components and restrict the singular vectors to be related to the Fourier basis. [14, 7] consider maximum likelihood estimation based on Kalman ﬁltering and also consider forecasting and signal extraction. [5] show how to handle missing data and imputation. Similar to the mSSA literature, the general focus of these works is ﬁrst signal extraction, which can then be subsequently used for imputation and forecasting. The theoretical analysis of these methods has generally been asymptotic in nature, and has focused on recovery of the spatial and temporal factors, i.e., signal extraction. Our work complements this literature as we focus directly on ﬁnite-sample analysis for imputation and out-of-sample forecasting (without ﬁrst needing to signal ex- traction), and establish consistency for the variant of mSSA we propose. To the best of our knowledge, ﬁnite-sample consistency results such as ours are limited in the literature. Additionally, there is a recent line of work from the machine learning literature which also employs matrix factorization based methods (see [40, 45]). Most such methods make strong prior model assumptions on the underlying time series and the algorithm changes based on the assumptions made on the time series dynamics that generated the data. Further, ﬁnite sample analysis, especially with respect to forecasting error, of such methods is usually lacking. We highlight one method, Temporal Regularized Matrix Factorization (TRMF) (see [45]), which we directly compare against due to its popularity, and as it achieves state-of- the-art empirical imputation and forecasting performance. The authors in [45] provide ﬁnite 10 sample imputation analysis for an instance of the model considered in this work, but fore- casting analysis is absent. As discussed earlier, they establish that imputation error scales as 1/ min(N, T ). This is a direct consequence of the low-rank structure of the original N × T matrix. But they fail to utilize, at least in the theoretical analysis, the temporal structure. In- deed, our analysis captures such temporal structure and hence our imputation error scales as 1/√min(N, T )T which is a stronger guarantee. For example, for N = Θ(1), their er- ror bound remains Θ(1) for any T , suggesting that TRMF [45] fails to utilize the temporal structure for better estimation, while the error for mSSA would vanish as T grows. Other relevant literature. We take a brief note of some popular time series methods in the recent literature. In particular, recently neural network (NN) based approaches have been popular and empirically effective. Some industry standard neural network methods include LSTMs, from the Keras library (a standard NN library, see [12]) and DeepAR (an indus- try leading NN library for time series analysis, see [31]). Though they have no theoretical guarantees, which is the focus of our work, we compare with them empirically. 3. Model. 3.1. Spatio-Temporal Factor Model. Below, we introduce the spatio-temporal factor model we use to explain the success of mSSA. In short, the model requires that the under- lying latent multivariate time series satisﬁes Properties 1 and 2, which capture the “spatial” and “temporal” structure within it, respectively. Spatial structure in data. Consider the matrix M ∈ RN ×T , where its entry in row n and column t, M nt is equal to fn(t), the value of the latent time series n at time t. We posit that the matrix M is low-rank. Precisely, PROPERTY 1. Let rank(M ) = R. That is, for any n ∈ [N ], t ∈ [T ], M nt = ∑R r=1 Unr Wrt, where |Unr| ≤ Γ1, |Wrt| ≤ Γ2 for constants Γ1, Γ2 > 0. Property 1 effectively captures the “spatial” structure amongst the N time series. Similar to the dynamic factor model literature, we can interpret this model as there existing R latent time series Wr· for r ∈ [R], and each time series fn(·) is a linear combination of these R time series, where the weights are given by Un·. Temporal structure in data. To explicitly capture the temporal structure in the data, we impose additional structure on Wr·. To that end, we introduce the notion of the Hankel matrix induced by a time series. DEFINITION 3.1 (Hankel Matrix). Given a time series g : Z → R, its Hankel ma- trix associated with observations over T time steps, {1, . . . , T }, is given by the matrix H ∈ R⌊T /2⌋×⌊T /2⌋ with Hij = g(i + j − 1) for i, j ∈ [⌊T /2⌋]. Now, for a given r ∈ [R], consider the time series Wrt for t ∈ [T ]. Let H(r) ∈ R⌊T /2⌋×⌊T /2⌋ denote its Hankel matrix restricted to [T ], i.e. H(r)ij = Wr(i+j−1) for i, j ∈ [⌊T /2⌋]. PROPERTY 2. For each r ∈ [R] and for any T ≥ 1, the Hankel Matrix H(r) ∈ R⌊T /2⌋×⌊T /2⌋ associated with time series Wrt, t ∈ [T ] has rank at most G. Property 2 captures the temporal structure within the latent factors associated with time; indeed, such a low-rank Hankel representation includes a rich family of time series dynamics as noted in Proposition 3.1 below. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 11 PROPOSITION 3.1 (Proposition 5.2, [1]). Consider a time series f : Z → R with its ele- ment at time t denoted as f (t) = A∑ a=1 exp(αat) · cos(2πωat + φa) · Pma(t),(4) where αa, ωa, φa ∈ R are parameters, Pma is a degree ma ∈ N polynomial in t. Then f (·) satisﬁes Property 2. In particular, consider the Hankel matrix of f over [T ], denoted as H(f ) ∈ R⌊T /2⌋×⌊T /2⌋ with H(f )ij = f (i + j − 1) for i, j ∈ [⌊T /2⌋]. For any T , the rank of H(f ) is at most G = A(mmax + 1)(mmax + 2), where mmax = maxa∈A ma. Proposition 3.1 states any ﬁnite sum of (products of) harmonics, polynomials, and expo- nentials has a low-rank Hankel representation. Each of these functions are popular to model various aspects of a time series such as periodicity and trend. Further, we note that the spec- tral representation of generic stationary processes, which includes autoregressive processes, implies that any sample-path of a stationary process can be decomposed into a weighted sum (precisely an integral) of harmonics, where the weights in the sum are sample path dependent—see Property 4.1, Chapter 4 of [30]. That is, a ﬁnite (weighted) sum of harmon- ics provides a good model representation for stationary processes with the model becoming more expressive as the number of harmonics grows. In Section 5, we extend this model when Property 2 is only approximately satisﬁed. In particular, we quantify the approximation error based on the smoothness of the underlying time series and the number of harmonics used in the summation to approximate it. Spatio-temporal model implies stacked Page matrix is low-rank. Recall that the primary representation utilized by mSSA, as described in Section 1.1, is the stacked Page matrix (with parameter L). Observe that the Page matrix of a univariate time series for any L ≤ ⌊T /2⌋ is simply the sub-matrix of the associated Hankel matrix: precisely, the Page matrix can be obtained by restricting to the top L rows and columns 1, L + 1, . . . of the Hankel matrix. Therefore, the rank of the Hankel matrix is a bound on the rank of the Page matrix. Under the spatio-temporal factor model satisfying Properties 1 and 2, we establish the following low- rank property of the Page matrix of any particular time series as well as that of the stacked Page matrix. PROPOSITION 3.2. Let Properties 1 and 2 hold. Then for any L ≤ ⌊T /2⌋ with any T ≥ 1, the rank of the Page matrix induced by the univariate time series fn(·) for n ∈ [N ] is at most R x G. Further, the rank of the stacked Page matrix induced by all N time series f1(·), . . . , fN (·) is also at most R x G. The proof is in Appendix D where a more general result is established in Proposition 5.1. 3.2. A Diagnostic Test for the Spatio-Temporal Model. In Sections 4 and 5, under the model described above, we theoretically establish the efﬁcacy of mSSA. Beyond this model though, our work does not provide any guarantees for mSSA. Therefore, to utilize the guar- antees of this work, it would be useful to have a data-driven diagnostic test that can help identify scenarios when the model of Section 3 may or may not hold. We discuss one such test in this section. In particular, Proposition 3.2 suggests a “data driven diagnosis test” to verify whether mSSA is likely to succeed as per the results of this work. Speciﬁcally, if the (effective) rank— deﬁned as the minimum number of singular values capturing > 90% of its spectral energy— of the Page matrix associated with any of the univariate components fn(·) and the (effective) 12 rank of stacked Page matrix associated with the multivariate time series with N component are very different, then mSSA may not be effective compared to SSA, but if they are very similar then mSSA is likely to be more effective compared to SSA. Our ﬁnite-sample results in Sections 4 and 5 indicate that the optimal value for L is √min(N, T )T . Thus as a further test, if the effective rank of the stacked Page matrix does not scale much slower than L for L ∼ √min(N, T )T , then SSA (and mSSA) are unlikely to be effective methods. Table 2 compares the (effective) rank of the stacked Page matrices for different benchmark time series data sets. The value of T equals 3993, 26304, and 10560 for the Financial, Elec- tricity, and Trafﬁc datasets respectively (see Appendix B for details on the datasets). We set L = ⌊ √min(N, T )T ⌋ for all datasets. When N = 1, this corresponds to L equals 63, 162, and 102 for the Financial, Electricity, and Trafﬁc datasets respectively. Table 2 shows the effective rank in each dataset as we vary N . As can be seen, for N = 1, the effective rank is much smaller than L (or T ) suggesting that SSA is likely to be effective. For Electricity and Financial datasets, the rank does not change by much as we increase N . However, relatively the rank does increase substantially for the Trafﬁc dataset. This might explain why mSSA is relatively less effective for the Trafﬁc dataset in contrast to the Financial and Electricity datasets as noted in Table 1. TABLE 2 Effective rank of stacked Page matrix across benchmarks as we vary N . Dataset N = 1 N =10 N = 100 N = 350 Electricity 19 37 44 31 Financial 1 3 3 6 Trafﬁc 14 32 69 116 4. Main Results. We now provide bounds on the imputation and forecasting prediction error for mSSA under the spatio-temporal model introduced in Section 3. We start by deﬁning the metric by which we measure prediction error. For imputation, we deﬁne prediction error as ImpErr(N, T ) = 1 N T N∑ n=1 T∑ t=1 E [ (fn(t) − ˆfn(t)) 2] .(5) Here, the imputed estimate ˆfn(·), n ∈ [N ] are produced by the imputation algorithm of Sec- tion 1.1. For forecasting, we deﬁne the in-sample prediction error as ForErr(N, T, L) = L N T N∑ n=1 T /L∑ m′=1 E [ (fn(L x m ′) − ¯fn(L x m′)) 2] .(6) Further, let T1 ∈ Z such that T1 ≥ L. Then, we deﬁne the out-of-sample prediction error as TestForErr(N, T, T1, L) = L N T1 N∑ n=1 T1/L∑ m′=1 E [ (fn(T + L x m′) − ¯fn(T + L x m ′)) 2] .(7) Again, the forecasted estimate ¯fn(·), n ∈ [N ] are produced by the forecasting algorithm of Section 1.1. In (5), (6), and (7), the expectation is with respect to the randomness in observations due to noise and missingness. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 13 4.1. Assumptions. To state the main results, we make the following assumptions. Recall from (1) that for each n ∈ [N ] and t ∈ [T ], we observe fn(t) + ηn(t) with probability ρ ∈ (0, 1] independently. We shall assume that noise ηn(·), n ∈ [N ] satisfy the following property. PROPERTY 3. For n ∈ [N ], t ∈ [T ], ηn(t) are independent sub-gaussian random vari- ables, with E[ηn(t)] = 0 and ∥ηn(t)∥ψ2 ≤ γ. For deﬁnition of ∥ · ∥ψα-norm, see [37], for example. PROPERTY 4. (Balanced spectra). Denote the L × (N T /L) stacked Page matrix asso- ciated with all N time series f1(·), . . . , fN (·) as SP(f ) := SP((f1, . . . , fN ), T, L). Under the setup of Proposition 3.2, rank(SP(f )) = ℓ ≥ 1 and ℓ ≤ R x G. Then, for L = √min(N, T )T , SP(f ) is such that σℓ(SP(f )) ≥ c √N T / √ℓ for some absolute constant c > 0, where σℓ is the ℓ-th largest singular value of SP(f ). Note that if σℓ(SP(f )) = Θ(σ1(SP(f ))), then one can verify that Property 4 holds. In- deed, assuming that the non-zero singular values are ‘well-balanced’ is standard in the matrix/tensor estimation literature. To state our results for out-of-sample forecasting error, let SP1(f ) be the L × (N T1/L) stacked Page matrix associated with all N time series f1(t), . . . , fN (t) entries for t ∈ [T +1, T +T1]. We assume an analogous condition on SP1(f ) as we do for SP(f ). PROPERTY 5. (Balanced spectra (out-of-sample)). Under the setup of Proposition 3.2, we have that rank(SP1(f )) = ℓ ≥ 1 and ℓ ≤ R x G. Then, for L = √min(N, T )T , SP1(f ) is such that σℓ(SP1(f )) ≥ c √N T1/√ℓ for some absolute constant c > 0, where σℓ is the ℓ-th largest singular value of SP1(f ). Again, note that if σℓ(SP1(f )) = Θ(σ1(SP1(f ))), then one can verify that Property 5 holds. Lastly, we shall ﬁrst impose some restrictions on the complexity of the N time series f1(t), . . . , fN (t) for t > T . Let SP ′(f ) denote the (L − 1) × (N T1/L) matrix formed us- ing the top L − 1 rows of SP(f ). Deﬁne SP ′ 1(f ) analogously with respect to SP1(f ). Let colspan(SP′(f )) and colspan(SP ′ 1(f )) denote the subspace of RL−1 spanned by the columns of SP ′(f ) and SP ′ 1(f ), respectively. We assume the following property. PROPERTY 6. (Subspace inclusion). colspan(SP′ 1(f )) ⊆ colspan(SP ′(f )). Intuitively, this requires that to effectively forecast, the associated stacked Page matrix of the out-of-sample time series colspan(SP ′ 1(f )) is only as “rich” as that of SP ′(f ). Picking hyper-parameter L. The proof of Theorems 4.1, 4.2, and 4.3 imply the optimal choice of L is to set it to √min(N, T )T . Intuitively, this choice of L leads to the stacked Page matrix SP(f ) to be as square as possible, and our analysis implies that the error rate is inversely proportional to the minimum of the number of the rows and columns of SP(f ). Hence, for the remainder of the paper, we state our results for L = √min(N, T )T . Picking hyper-parameter k. For our theoretical result, we assume that we pick k = ℓ, where ℓ is the rank of SP(f ). Empirically, we pick k to equal the “effective rank” of the observed Page matrix as deﬁned in Section 3.2. 14 4.2. Finite-sample Analysis for Imputation and Forecasting. Now we state the main re- sults. In what follows, we let C(c, Γ1, Γ2, γ) denote a constant thats depends only (polynomi- ally) on model parameters c, Γ1, Γ2, γ. We also remind the reader that R, Γ1, Γ2 are deﬁned in Property 1, G in 2, γ in Property 3 and c in Property 4. Imputation. We begin with our imputation result. THEOREM 4.1 (Imputation). Let Properties 1, 2, 3 and 4 hold. For a large enough abso- lute constant C > 0, let ρ ≥ C log N T√N T . Then with hyper-parameters L = √min(N, T )T and k = ℓ, ImpErr(N, T ) ≤ C(c, Γ1, Γ2, γ) ( R3G log N T ρ4√min(N, T )T ). In-sample forecasting. Recall from (3) that in mSSA, we learn a linear model between the last row of SP((X1, . . . , XN ), T, L) and the L − 1 rows above it (after de-noising the sub- matrix induced these L − 1 rows via HSVT). Hence, we ﬁrst establish that in the idealized scenario (no noise, no missing values), there does indeed exist a linear model between the last row and the L − 1 rows above of SP(f ). Let SP(f )L· denote the L-th row of SP(f ) and recall SP′(f ) ∈ R(L−1)×(N T /L) denotes the sub-matrix of SP(f ) formed by selecting top L − 1 rows. In the proposition below, we show there exists a linear relationship between SP(f )L· and SP′(f ). PROPOSITION 4.1. Let Properties 1 and 2 hold. Then there exists β∗ ∈ RL−1 such that SP(f )T L· = SP ′(f ) T β∗. Further, ∥β∗∥0 ≤ RG. THEOREM 4.2 (In-sample forecasting). Let the conditions of Theorem 4.1 hold. Then, with β∗ deﬁned in Proposition 4.1, we have ForErr(N, T, L) ≤ C(c, γ, Γ1, Γ2) max(1, ∥β∗∥2 1) ( R3G log N T ρ4√min(N, T )T ). Out-of-sample forecasting. THEOREM 4.3 (Out-of-sample Forecasting). Let Properties 1, 2, 3, 4, 5, and 6 hold. Let the hyper-parameters L = √min(N, T )T and k = ℓ. Then for a large enough abso- lute constant C > 0, let ρ ≥ C max ( log N T√N T , (γ + RΓ1Γ2) √ RG L ). Then, with β∗ deﬁned in Proposition 4.1, we have TestForErr(N, T, T1, L) ≤ C(γ, Γ1, Γ2, c) max(1, ∥β∗∥ 2 1) ( R9G3 log(N max(T, T1)) ρ4√min(N, T )T (max (1, N T ) + T T1 ) ) . COROLLARY 4.1. Let the conditions of Theorem 4.3 hold. Then, with T1 = Θ(T ), we have TestForErr(N, T, T1, L) ≤ C(γ, Γ1, Γ2, c) max(1, ∥β∗∥ 2 1) ( R9G3 log(N T ) max ( 1, N T ) ρ4√min(N, T )T ) . ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 15 Corollary 4.1 implies that when N = o(T ), then the error scales as ∼ 1/√N T . When T = o(N ), then one can simply divide the N time series up into sets of size T . Corollary 4.1 implies that this will result in error scaling as ∼ 1/T . Thus effectively, the error rate scales as ∼ 1/√min(N, T )T . We note that Theorems 4.1, 4.2 and Proposition 4.1 are special cases of Theorems 5.6, 5.7 and Proposition 5.6 stated in the next section, respectively. Their proofs are in Appendices H, I, and I.1, respectively. The proof of Theorem 4.3 is in Appendix J. 5. Approximate Low-Rank Hankel Representation. In this section, we extend the model presented in Section 3 by relaxing Property 2 to only hold approximately. We es- tablish a ‘calculus’ for this extended model – the set of time series functions which have this approximate low-rank Hankel representation is closed under component-wise addition and multiplication. We show important examples of time series dynamics studied in the literature have an approximate low-rank Hankel representation. Lastly, we present generalizations of Theorems 4.1 and 4.2 for this extended model. 5.1. Approximate Low-rank Hankel Representation and Hankel Calculus. We ﬁrst intro- duce the deﬁnition of the approximate rank of a matrix. DEFINITION 5.1 (ϵ-approximate rank). Given ϵ > 0, a matrix M ∈ Ra×b is said to have ϵ-approximate rank at most s ≥ 1 if there exists a rank s matrix Ms ∈ Ra×b such that ∥M − Ms∥∞ < ϵ. DEFINITION 5.2 ((G, ϵ)-Hankel Time Series). For a given ϵ ≥ 0 and G ≥ 1, a time series f : Z → R is called a (G, ϵ)-Hankel time series if for any T ≥ 1, its Hankel matrix has ϵ- approximate rank G. We extend the model of Section 3 by replacing Property 2 by the following. PROPERTY 7. For each r ∈ [R] and for any T ≥ 1, the Hankel Matrix H(r) ∈ R⌊T /2⌋×⌊T /2⌋ associated with time series Wrt, t ∈ [T ] has ϵ-approximate rank at most G for ϵ > 0. That is, for each r ∈ [R], Wr· is a (G, ϵ)-Hankel time series. We state an implication of the above stated properties on the stacked Page matrix. PROPOSITION 5.1. Let Properties 1 and 7 hold. For any L ≤ ⌊T /2⌋ with any T ≥ 1, the stacked Page matrix induced by the N time series f1(·), . . . , fN (·) has ϵ′-rank at most R x G for ϵ′ = RΓ1ϵ. Hankel calculus. We present a key property of the model class satisfying Property 7, i.e. time series that have an approximate low-rank Hankel matrix representation. To that end, we deﬁne ‘addition’ and ‘multiplication’ for time series. Given two time series f1, f2 : Z → R, deﬁne their addition, denoted f1 + f2 : Z → R as (f1 + f2)(t) = f1(t) + f2(t), for all t ∈ Z. Simi- larly, their multiplication, denoted f1 ◦f2 : Z → R as (f1 ◦f2)(t) = f1(t) x f2(t), for all t ∈ Z. Now, we state a key property for the model class satisfying Property 7 (proof in Appendix E). PROPOSITION 5.2. For i ∈ {1, 2}, let fi be a (Gi, ϵi)-Hankel time series for Gi ≥ 1, ϵi ≥ 0. Then, f1 + f2 is a (G1 + G2, ϵ1 + ϵ2)-Hankel time series and f1 ◦ f2 is a(G1G2, 3 max(ϵ1, ϵ2) · max(∥f1∥∞, ∥f2∥∞))-Hankel time series. 16 5.2. Examples of (G, ϵ)-Hankel Time Series. We establish that many important classes of time series dynamics studied in the literature are instances of (G, ϵ)-Hankel time series, i.e. they satisfy Property 7. In particular, any differentiable periodic function (Proposition 5.4), and any time series with a Hölder continuous latent variable representation (Proposition 5.5). Proofs of Propositions 5.3, 5.4, and 5.5 can be found in Appendix E. Example 1. (G, ϵ)-LRF time series. We start by deﬁning a linear recurrent formula (LRF), which is a standard model for linear time-invariant systems. DEFINITION 5.3 ((G, ϵ)-LRF). For G ∈ N and ϵ ≥ 0, a time series f is said to be a (G, ϵ)-Linear Recurrent Formula (LRF) if for all T ∈ Z and t ∈ [T ], there exists g : Z → R such that f (t) = g(t) + h(t), where for all t ∈ Z, (i) g(t) = ∑G l=1 αlg(t − l) with constants α1, . . . , αG, and (ii) |h(t)| ≤ ϵ. Now we establish a time series f that is a (G, ϵ)-LRF is also (G, ϵ)-Hankel. PROPOSITION 5.3. If f is (G, ϵ)-LRF representable, then it is (G, ϵ)-Hankel repre- sentable. LRF’s cover a broad class of time series functions, including any ﬁnite sum of products of harmonics, polynomials and exponentials. In particular, it can be easily veriﬁed that a time series described by (4) is a (G, 0)-LRF, where G ≤ A(mmax + 1)(mmax + 2) with mmax = maxa∈A ma. Example 2. “smooth” and periodic time series. We establish that any differentiable peri- odic function is (G, ϵ)-LRF and hence (G, ϵ)-Hankel for appropriate choices of G and ϵ. DEFINITION 5.4 (Ck(R, PER)). For k ≥ 1 and R > 0, we use Ck(R, PER) to denote the class of all time series f : R → R such that it is R periodic, i.e. f (t + R) = f (t) for all t ∈ R and the k-th derivative of f , denoted f (k), exists and is continuous. PROPOSITION 5.4. Any f ∈ Ck(R, PER) is (4G, C(k, R) ∥f (k)∥ Gk−0.5 ) − Hankel representable, for any G ≥ 1. Here C(k, R) is a term that depends only on k, R and ∥f (k)∥2 = 1 R ∫ R 0 (f (k)(t))2dt. Example 3. time series with latent variable model (LVM) structure. We now show that if a time series has a LVM representation, and the latent function is Hölder continuous, then it has a (G, ϵ)-Hankel representation for appropriate choice of G ≥ 1 and ϵ ≥ 0. We ﬁrst deﬁne the Hölder class of functions; this class of functions is widely adopted in the non-parametric regression literature [38]. Given a function g : [0, 1)K → R, and a multi-index κ ∈ NK , let the partial derivate of g at x ∈ [0, 1)K , if it exists, be denoted as ▽κg(x) = ∂|κ|g(x) (∂x)κ where |κ| = ∑K j=1 κj and (∂x)κ = ∂κ1x1 · · · ∂κK xK . ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 17 DEFINITION 5.5 ((α, L)-Hölder Class). Given α, L > 0, the Hölder class H(α, L) on [0, 1)K is deﬁned as the set of functions g : [0, 1)K → R whose partial derivatives satisfy for all x, x′ ∈ [0, 1)K , ∑ κ:|κ|=⌊α⌋ 1 κ! |▽κg(x) − ▽κg(x′)| ≤ L∥x − x′∥α−⌊α⌋ ∞ . Here ⌊α⌋ refers to the greatest integer strictly smaller than α and κ! = ∏K j=1 κj!. Note that if α ∈ (0, 1], then the deﬁnition above is equivalent to the (α, L)-Lipschitz con- dition, i.e., |g(x) − g(x′)| ≤ L∥x − x′∥α ∞, for x, x′ ∈ [0, 1)K . Given a time series f : Z → R, for any T ≥ 1, recall the Hankel matrix H ∈ R⌊T /2⌋×⌊T /2⌋ is deﬁned such that its entry in row i ∈ [⌊T /2⌋] and column j ∈ [⌊T /2⌋] is given by Hij = f (i + j − 1). We call a time series f to have (α, L)-Hölder smooth LVM representation for α, L > 0 if for any given T ≥ 1, the corresponding Hankel matrix H satisﬁes: for i, j ∈ [⌊T /2⌋], H ij = g(θi, ωj), where θi, ωj ∈ [0, 1)K are latent parameters and g(·, ω) ∈ H(α, L) for any ω ∈ [0, 1)K . It can be veriﬁed that a (G, 0)-Hankel time series is an instance of such a LVM representation with corresponding g(x, y) = xT y. Thus in a sense, this model is a natural generalization of the (G, 0)-Hankel matrix representation. The following proposition connects this LVM representation to the (G, ϵ)-Hankel representation for appropriately deﬁned G ≥ 1, ϵ > 0. PROPOSITION 5.5. Given α, L > 0, let f have (α, L)-Hölder smooth LVM representa- tion. Then for all ϵ > 0, f is (C(α, K) ( 1 ϵ )K, Lϵ α) − Hankel representable. Here C(α, K) is a term that depends only on α and K. 5.3. Extending Main Results. Below, we provide generalizations of the imputation and in-sample forecasting results stated in Section 4. To do so, we utilize Property 8 which is analogous to Property 4 but for the approximate low-rank setting. PROPERTY 8. (Approximately balanced spectra). Under the setup of Proposition 5.1, we can represent the L × (N T /L) stacked Page matrix associated with all N time se- ries f1(·), . . . , fN (·) as SP(f ) = ˜M + E with rank( ˜M ) = ℓ ≥ 1 and ℓ ≤ R x G and ∥E∥∞ ≤ RΓ1ϵ. Then, for L = √min(N, T )T , ˜M is such that σℓ( ˜M ) ≥ c √ N T / √ℓ for some absolute constant c > 0, where σℓ is the ℓ-th largest singular value of ˜M . THEOREM 5.6 (Imputation). Let Properties 1, 7, 3 and 8 hold. For a large enough abso- lute constant C > 0, let ρ ≥ C log N T√N T . Then, with hyper-parameters L = √min(N, T )T and k = ℓ, ImpErr(N, T ) ≤ C(c, Γ1, Γ2, γ) ( R3G log N T ρ4√min(N, T )T + R4G(ϵ + ϵ3) ρ2 ) where C(c, Γ1, Γ2, γ) is a positive constant dependent on model parameters including Γ1, Γ2, γ. We remind the reader that R, Γ1, Γ2 are deﬁned in Property 1, G in 2, γ in Property 3 and c in Property 8. Existence of Linear Model. We now state Proposition 5.6, which is analogous to Proposition 4.1, but for the approximate low-rank setting. 18 PROPOSITION 5.6. Let Properties 1 and 7 hold. Then, there exists β∗ ∈ RL−1, such that ∥SP(f )T L· − SP ′(f ) T β∗∥∞ ≤ RΓ1(1 + ∥β∗∥1)ϵ., Further ∥β∗∥0 ≤ RG. THEOREM 5.7 (In-sample forecasting). Let the conditions of Theorem 5.6 hold. Then with β∗ deﬁned in Proposition 5.6, we have ForErr(N, T, L) ≤ C(c, γ, Γ1, Γ2) max(1, ∥β∗∥2 1)( R3G log N T ρ4√min(N, T )T + R4G(ϵ + ϵ3) ρ2 ). 6. Experiments. We describe experiments supporting our theoretical results for mSSA. In particular, we provide details of the experiments run to create the summary results de- scribed earlier in Table 1. In Appendix B, we describe the datasets utilized and the various algorithms we compare with as well as the procedure for selecting the hyper-parameters in each algorithm. In Section 6.1 and 6.2, we report the imputation and forecasting results. Note that in all experiments, we use the Normalized Root Mean Squared Error (NRMSE) as out accuracy metric. That is, we normalize all the underlying time series to have zero mean and unit variance before calculating the root mean squared error. We use this metric as it weighs the error on each time series equally. 6.1. Imputation. Setup. We test the robustness of the imputation performance by adding two sources of corruption to the data - varying the percentage of observed values and varying the amount of noise we perturb the observations by. We test imputation performance by how accurately we recover missing values. We compare the performance of mSSA with TRMF, a method which achieves state-of-the-art imputation performance. Further, to analyze the added beneﬁt of exploiting the spatial structure in a multivariate time series using mSSA, we compare with the SSA variant introduced in [1] . Results. Figures 3a, 3c, 3e, 4a, and 4c show the imputation error in the aforementioned datasets as we vary the fraction of missing values, while Figures 3b, 3d, 3f, 4b, and 4d show the imputation error as we vary σ, the standard deviation of the gaussian noise. We see that as we vary the fraction of missing values and noise levels, mSSA outperforms both TRMF and SSA in ∼ 75% of experiments run. It is noteworthy the large empirical gain in mSSA over SSA, giving credence to the spatio-temporal model we introduce. The average NRMSE across all experiments for each dataset is reported in Table 1, where mSSA outperforms every other method across all datasets except for the Trafﬁc dataset. 6.2. Forecasting. Setup. We test the forecasting accuracy of the proposed mSSA against several state-of-the-art algorithms. For each dataset, we split the data into training, validation, and testing datasets as outlined in Appendix B.1. As was done in the imputation experiments, we vary how much each dataset is corrupted by varying the percentage of observed values and the noise levels. Results. Figures 5a, 5c, 5e, 6a, and 6c show the forecasting accuracy of mSSA and other methods in the aforementioned datasets as we vary the fraction of missing values, while Fig- ures 5b, 5d, 5f, 6b, and 6d show the forecasting accuracy as we vary the standard deviation of the added gaussian noise. We see that as we vary the fraction of missing values and noise level, mSSA is the best or comparable to the best performing method in ∼ 80% of experi- ments. In terms of the average NRMSE across all experiments, we ﬁnd that mSSA performs similar to or better than every other method across all datasets except for the trafﬁc dataset as was reported in Table 1. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 19 (a) (b) (c) (d) (e) (f) Fig 3: mSSA vs. TRMF vs. SSA - imputation performance on the Electricity, Trafﬁc and Syn- thetic datasets. Figures 3a, 3c, and 3e, show imputation accuracy of mSSA, TRMF and SSA as we vary the fraction of missing values; Figures 3b, 3d, and 3f show imputation accuracy as we vary the noise level (and with 50% of values missing). 20 (a) (b) (c) (d) Fig 4: mSSA vs. TRMF vs. SSA - imputation performance on the Financial and M5 datasets. Figures 4a, and 4c show imputation accuracy of mSSA, TRMF and SSA as we vary the fraction of missing values; Figures 4b, and 4d show imputation accuracy as we vary the noise level (and with 50% of values missing). ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 21 (a) (b) (c) (d) (e) (f) Fig 5: mSSA forecasting performance on standard multivariate time series benchmark is competitive with/outperforming industry standard methods as we vary the number of missing data and noise level. Figures 5a, 5c, and 5e show the forecasting accuracy of all methods (some of VAR results are not shown due to its relatively high error) on the Electricity, Trafﬁc and Synthetic datasets with varying fraction of missing values; Figures 5b, 5d, and 5f shows the forecasting accuracy on the same datasets with varying noise level. 22 (a) (b) (c) (d) Fig 6: Figures 6a, and 6c show the forecasting accuracy of all methods (some of VAR results are not shown due to its relatively high error) on the ﬁnancial and M5 datasets with varying fraction of missing values; Figures 6b, and 6d show the forecasting accuracy on the same datasets with varying noise levels. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 23 7. Algorithmic Extensions of mSSA. 7.1. Variance Estimation. We extend the mSSA algorithm to estimate the time-varying variance of a time series by making the following simple observation. If we apply mSSA to the squared observations, X 2 n(t), we will recover an estimate of E[X 2 n(t)] (for ρ = 1). However, observe that Var[Xn(t)] = E[X 2 n(t)] − E[Xn(t)]2. Therefore, by applying mSSA twice, once on Xn(t) and once on X 2 n(t) for n ∈ [N ] and t ∈ [T ], and subsequently taking the component-wise difference of the two estimates will lead to an estimate of the variance. This suggests a simple algorithm which we describe next. We note this observation suggests any mean estimation algorithm (or imputation) in time series analysis can be converted to estimate the time varying variance – this ought to be of interest in its own right. Algorithm. As described in Section 1.1, let L ≥ 1 and k, k′ ≥ 1 be algorithm parameters. First, apply mSSA on observations Xn(t), n ∈ [N ], t ∈ [T ] to produce imputed estimates ˆfn(t) using the hyper-parameters L and k. Next, apply mSSA on observations X 2 n(t), n ∈ [N ], t ∈ [T ] to produce imputed estimates ˆgn(t) using the hyper-parameters L and k′. Lastly, we denote ˆσ2 n(t) = max(0, ˆgn(t) − ˆfn(t)2), n ∈ [N ], t ∈ [T ] as our estimate of the time- varying variance. Model. For n ∈ [N ], t ∈ [T ], let σ2 n(t) = E[η2 n(t)] be the time-varying variance of the time se- ries observations, i.e., if ρ = 1 then σ2 n(t) = Var[Xn(t)] = E[X 2 n(t)] − f 2 n(t). Let Σ ∈ RN ×T be the matrix induced by the latent time-varying variances of the N time series of interest, i.e., the entry in row n at time t in Σ is Σnt = σ2 n(t). To capture the “spatial” and “temporal” structure across the N latent time-varying variances, we assume the latent variance matrix Σ satisﬁes Properties 9 and 10. These properties are analogous to those assumed about the latent mean matrix M (deﬁned in Section 3); in particular, Properties 1 and 2. We state them next. PROPERTY 9. Let R′ = rank(Σ), i.e, for any n ∈ [N ], t ∈ [T ], Σnt = ∑R′ r=1 U ′ nr W ′ rt, where the factorization is such that |U ′ nr| ≤ Γ′ 1, |W ′ rt| ≤ Γ′ 2 for Γ′ 1, Γ′ 2 > 0. Like Property 1, the above property captures the “spatial” structure within N time series of variances. To capture the “temporal” structure, next we introduce an analogue of Property 2. To that end, for each r ∈ [R′], deﬁne the ⌊T /2⌋ × ⌊T /2⌋ Hankel matrix of each time series W ′ rt, t ∈ [T ] as H ′(r) ∈ R⌊T /2⌋×⌊T /2⌋, where H ′(r)ij = W ′ r(i+j−1) for i, j ∈ [⌊T /2⌋]. PROPERTY 10. For each r ∈ [R′], the Hankel Matrix H ′(r) ∈ R⌊T /2⌋×⌊T /2⌋ associated with time series W ′ rt, t ∈ [T ] has rank at most G′. Result. To establish the estimation error for the variance estimation algorithm under the spatio-temporal model above, we need the following additional property (analogous to Prop- erty 4). PROPERTY 11 (Balanced spectra). Denote the L × (N T /L) stacked Page matrix asso- ciated with all N time series σ2 1(·), . . . , σ2 N (·) as SP(σ2) := SP((σ2 1, . . . , σ2 N ), T, L). Due to Properties 9 and 10, and a simple variant of Proposition 3.2, we have rank(SP(σ2)) = ℓ′ ≥ 1 and ℓ′ ≤ R′ x G′. Then, for L = √min(N, T )T , SP(σ2) is such that σ′ ℓ(M ) ≥ c √N T / √ℓ′ for some absolute constant c > 0, where σ′ ℓ is the ℓ-th singular value, order by magnitude, of SP(σ2). 24 Fig 7: The observations Page tensor. THEOREM 7.1 (Variance Estimation). Let Properties 1, 2, 3, 4, 9, 10, and 11 hold. Additionally let | ˆfn(t)| ≤ Γ3 for all n ∈ [N ], t ∈ [T ]. Lastly, let hyper-parameters L =√min(N, T )T , k = ℓ, k′ = ℓ′. Let ρ = 1. Then the variance prediction error is bounded above as 1 N T N∑ n=1 T∑ t=1 E[ (σn(t) 2 − ˆσ2 n(t))2] ≤ ˜C ( (G2 + G′) log2 N T √min(N, T )T . ) . where ˜C is a constant dependent (polynomially) on model parameters Γ1, Γ2, Γ3, Γ′ 1, Γ′ 2, γ, R, R′. Proof of Theorem 7.1 can be found in Appendix K. 7.2. Tensor SSA. Page tensor. We introduce an order-three tensor representation of a multivariate time series which we term the ‘Page tensor’. Given N time series, with observa- tions over T time steps and hyper-parameter L ≥ 1, deﬁne T ∈ RN ×T /L×L such that Tnℓs = fn((s − 1) × L + ℓ), n ∈ [N ], ℓ ∈ [L], s ∈ [T /L]. The corresponding observation tensor, T ∈ (R ∪ {⋆})N ×T /L×L, is Tnℓs = Xn((s − 1) × L + ℓ), n ∈ [N ], ℓ ∈ [L], s ∈ [T /L].(8) See Figure 7 for a visual depiction of T. Let the CP-rank of an order-d tensor T ∈ Rn1×n2×···×nd be the smallest value of r ∈ N such that T i1,...,id = ∑r k=1 ui1,k . . . uid,k, where uiℓ,· are latent factors for ℓ ∈ [d]. Under the model described in Section 3, we have the following properties. PROPOSITION 7.1. Let Properties 1, 2, and 3 hold. Then, for any 1 ≤ L ≤ √T , T has canonical polyadic (CP)-rank at most R x G. Further, all entries of T are independent ran- dom variables with each entry observed with probability ρ ∈ (0, 1], and E[T] = ρT. tSSA: time series imputation using the Page tensor representation. The Page tensor rep- resentation and Proposition 7.1 collectively suggest that time series imputation can be re- duced to low-rank tensor estimation, i.e., recovering a tensor of low CP-rank from its noisy, partial observations. Over the past decade, the ﬁeld of low-rank tensor (and matrix) estima- tion has received great empirical and theoretical interest, leading to a large variety of algo- rithms including spectral, convex optimization, and nearest neighbor based approaches. We list a few works which have explicit ﬁnite-sample rates for noisy low-rank tensor comple- tion [6, 42, 11, 44, 34]). As a result, we “blackbox” the tensor estimation algorithm used in tSSA as a pivotal subroutine. Doing so allows one the ﬂexibility to use the tensor estimation ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 25 algorithm of their choosing within tSSA. Consequently, as the tensor estimation literature continues to advance, the “meta-algorithm” of tSSA will continue to improve in parallel. To that end, we give a deﬁnition of a tensor estimation algorithm for a generic order-d tensor. Note that when d = 2, this reduces to standard matrix estimation (ME). DEFINITION 7.2 (Matrix/Tensor Estimation). For d ≥ 2, denote TEd : {⋆, R}n1×n2×...nd → Rn1×n2×...nd as an order-d tensor estimation algorithm. It takes as input an order-d tensor G with noisy, missing entries, where E[G] = ρG and ρ ∈ (0, 1] is the probability of each entry in G being observed. TEd then outputs an estimate of G denoted as ̂G = TEd(G). We assume the following ‘oracle’ error convergence rate for TEd; for ease of exposition, we restrict our attention to the setting where ρ = 1. PROPERTY 12. For d ≥ 2, assume TEd satisﬁes the following: the estimate ̂G ∈ Rn1×n2×...nd, which is the output of T Ed(G) with E[G] = G, satisﬁes 1 n1 . . . nd ∥ ̂G − G∥ 2 F = ˜Θ (1/ min(n1, . . . , nd)⌈d/2⌉) . Here, ˜Θ(·) suppresses dependence on noise, i.e., E = G − E[G], log(·) factors, and CP-rank of G. Property 12 holds for a variety of matrix/tensor estimation algorithms. For d = 2, it holds for HSVT as we establish in the proof of Theorem 4.1 for mSSA of ˜O(1/ √min(N, T ), T ). It is straightforward to show that this is the best rate achievable for TE2. For d ≥ 3, it has recently been shown that Property 12 provably holds for a spectral gradient descent based algorithm [11] (see Corollary 1.5 of [11]), conditioned on certain standard “incoherence” conditions imposed on the latent factors of G; another spectral algorithm that achieved the same rate was furnished in [42], which the authors also establish is minimax optimal. tSSA algorithm. We now deﬁne the “meta” tSSA algorithm; the two algorithmic hyper- parameters are L ≥ 1 (deﬁned in (8)) and TE3 (the order-three tensor estimation algorithm one chooses). First, using Xn(t) for n ∈ [N ], t ∈ [T ], construct Page tensor T as in (8). Second, obtain ̂T as the output of TE3(T) and read off ˆfn(t) by selecting appropriate entry in ̂T. Algorithmic comparison: tSSA vs. mSSA vs. ME. We now provide a uniﬁed view of tSSA, mSSA, and “vanilla” ME (which we describe below) to do time series imputation. All three methods have two key steps: (i) data transformation – converting the observations Xn(t) into a particular data representation/structure; (ii) de-noising– applying some form of ma- trix/tensor estimation to de-noise the constructed data representation. • tSSA – using Xn(t), create the Page tensor T ∈ RN ×L×T /L as in (8); apply TE3(T) to get ̂T (e.g. using the method in [11]); read off ˆfn(t) by selecting appropriate entry in ̂T. • mSSA – using Xn(t), create the stacked Page matrix SP((X1, . . . , XN ), T, L) ∈ RL×(N x T /L) as detailed in Section 1.1; apply TE2(SP((X1, . . . , XN ), T, L)) to get ̂SP((X1, . . . , XN ), T, L) (where we use HSVT for TE2(·)); read off ˆfn(t) by selecting appropriate entry in ̂SP((X1, . . . , XN ), T, L). • ME – using Xn(t), create X ∈ RN ×T , where X nt is equal to Xn(t); apply TE2(X) (e.g. using HSVT as in mSSA) to get ̂X; read off ˆfn(t) by selecting appropriate entry in ̂X. 26 This perspective also suggests that one can use any “blackbox” matrix estimation routine to de-noise the constructed stacked Page matrix in mSSA; HSVT is one such choice that we analyze. Theoretical comparison: tSSA vs. mSSA vs. ME. We now do a theoretical comparison of the relative effectiveness of tSSA, mSSA, and ME in imputing a multivariate time se- ries Xn(t) for n ∈ [N ], t ∈ [T ], as we vary N and T . To that end, let ImpErr(N, T ; tSSA), ImpErr(N, T ; mSSA), and ImpErr(N, T ; ME) denote the imputation error for tSSA, mSSA, and ME, respectively. PROPOSITION 7.2. For tSSA and mSSA, pick hyper-parameter L = √T , L =√min(N, T )T , respectively. Let Property 12 hold. Then, (i) T = o(N ): ImpErr(N, T ; tSSA), ImpErr(N, T ; mSSA) = ˜Θ(ImpErr(N, T ; ME)); (ii) T 1/3 = o(N ), N = o(T ): ImpErr(N, T ; tSSA) = ˜o(ImpErr(N, T ; mSSA)), ImpErr(N, T ; mSSA) = ˜o(ImpErr(N, T ; ME); (iii) N = o(T 1/3): ImpErr(N, T ; mSSA) = ˜o(ImpErr(N, T ; tSSA)), ImpErr(N, T ; tSSA) = ˜o(ImpErr(N, T ; ME)), where ˜o(·), ˜Θ(·) suppresses dependence on noise parameters, CP-rank, poly-logarithmic factors. We note given Property 12, L = √T is optimal for tSSA and L = √min(N, T )T is opti- mal for mSSA. See Figure 2 in Section 1 for a graphical depiction of the different regimes in Proposition 7.2. Proofs of Proposition 7.1 and 7.2 below can be found in Appendix L. Application to Time-varying Recommendation Systems In Appendix C, we discuss the extension of our spatio-temporal model and tSSA to time-varying recommendation systems. 8. Conclusion. We provide theoretical justiﬁcation of a practical, simple variant of mSSA, a method heavily used in practice but with limited theoretical understanding. We show how to extend mSSA to estimate time-varying variance and introduce a tensor variant, tSSA, which builds upon recent advancements in tensor estimation. We hope this work mo- tivates future inquiry into the connections between the classical ﬁeld of time series analysis and the modern, growing ﬁeld of matrix/tensor estimation. APPENDIX A: PAGE VS. HANKEL MSSA This section discusses the beneﬁts and drawbacks of using the Page matrix representation, as we propose in our variant, instead of the Hankel representation used in the original mSSA. Recall the key steps of the original SSA method in Section 2. The extension to mSSA is done by stacking the Hankel matrices induced by each of the N time series either column-wise (horizontal mSSA) or row-wise (vertical mSSA) [24]. In this section, we will use mSSA to denote our mSSA variant, and hSSA/vSSA to denote the original horizontal/vertical mSSA. In what follows, we will compare our mSSA variant with hSSA/vSSA in terms of their: (i) theoretical analysis; (ii) computational complexity; and (iii) empirical performance. Theoretical analysis. We re-emphasize that to the best of our knowledge, the theoretical analysis of the mSSA algorithm, both hSSA and vSSA, have been absent from the literature, despite their popularity. We do a comprehensive theoretical analysis of the variant of mSSA we propose. By utilizing the Page matrix, it allows us to invoke results from random matrix theory to prove our imputation and forecasting results. However, extending our analysis to the Hankel matrix representation is challenging as the Hankel matrix has repeated entries of ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 27 the same time series observation. This leads to correlation in the noise in the observation of the entries of the Hankel matrix, which prevents us from invoking the results from ran- dom matrix theory in a straightforward way. The Page matrix representation does not have repeated entries of the same observation, and thus allows us to circumvent this issue in our theoretical analysis. Computational complexity. Our mSSA variant is computationally far more efﬁcient than both hSSA and vSSA. This is because the Page matrix representation of a multivariate time series with N time series and T time steps is a matrix of dimension √N T × √N T (with L =√N T )., i.e., it has a total of O(N T ) entries. In contrast, the Hankel matrix representation is of dimension T /4 × 3N T /4 for hSSA and N T /4 × 3T /4 for vSSA (we set the parameter L to T /4 as recommended in [24]), i.e., both variants of the Hankel matrix have O(N T 2) entries. This makes computing the SVD (the most computationally intensive step of mSSA) prohibitive for hSSA and mSSA even for the standard time series benchmarks we consider in Section 6. To empirically demonstrate the computational efﬁciency of our variant of mSSA, we com- pare its training time to that of hSSA and vSSA. Speciﬁcally, we measure the training time for mSSA, hSSA, and vSSA as we increase the number of time steps T ∈ [400, 10000]. We perform this experiment on two datasets: (i) the synthetic dataset; (ii) a subset of the elec- tricity dataset, where we choose only 50 of the available 370 time series. Both datasets are described in details in Appendix B. Figure 8 shows that in both datasets, the training time of both hSSA and vSSA can be as 600-1000x as high as the training time of our mSSA variant as we increase T . 103 104 10−2 100 102 Timesteps (T)Seconds Training Time - Synthetic Dataset mSSA hSSA vSSA (a) 103 104 100 102 Timesteps (T)Seconds Training Time - Electricity Dataset mSSA hSSA vSSA (b) Fig 8: The training time of the original mSSA variants (hSSA in the orange dotted line and vSSA in the green dotted line) are orders of magnitude higher than that of the mSSA variant we propose (blue solid line). Empirical performance. Here, we compare the forecasting performance of mSSA to that of hSSA and vSSA. We report performance in terms of the NRMSE of the three methods as we increase the number of time steps T ∈ [400, 10000] in the aforementioned synthetic and electricity dataset. The goal in the synthetic dataset is to predict the next 50 time steps using one step ahead forecasts, while the goal in the electricity dataset is to predict the next three days using day-ahead forecasts. For hSSA and vSSA, we choose L = T /4 as recommended 28 in [24]; and for mSSA, we choose L = ⌊ √N T ⌋. For all three methods, we choose the number of retained singular values based on the thresholding procedure outlined in [17]. Figures 9 shows the performance of the three methods in both datasets. We ﬁnd that ini- tially, with few data points (T < 600 in the synthetic data and T < 4000 in the electric- ity data), both hSSA and vSSA outperform mSSA. As we increase T , mSSA performance signiﬁcantly improves and eventually outperforms vSSA. In the electricity dataset, mSSA performs similar to hSSA for T = 10000. These experiments suggest that if only a few ob- servations were available, hSSA and vSSA might provide better performance. However, if the number of observations were relatively large, then the performance of mSSA is superior to vSSA and relatively similar to hSSA. Importantly, the electricity dataset experiment illustrates a critical advantage of our mSSA variant. Speciﬁcally, when T is large such that running hSSA or vSSA is computationally infeasible, then one can achieve better accuracy using mSSA. For example, while we could not run the hSSA and vSSA on the electricity dataset with T = 20000 due to memory con- straints, we were able to run mSSA and achieve a lower NRMSE. This suggests that our mSSA variant is the more practical mSSA algorithm when it comes to efﬁciently utilizing large multivariate time series. 103 104 0.1 0.2 0.3 Timesteps (T)NormalizedRMSE Forecasting Accuracy - Synthetic Dataset mSSA hSSA vSSA (a) 103 104 0.4 0.5 0.6 Timesteps (T)NormalizedRMSE Forecasting Accuracy - Electricity Dataset mSSA hSSA vSSA (b) Fig 9: The forecasting error of the original mSSA variants (hSSA in the orange dotted line and vSSA in the green dotted line) and the proposed mSSA variant (blue solid line) as we increase T . APPENDIX B: EXPERIMENT DETAILS In Appendix B.1, we describe the datasets utilized. In Appendix B.2, we describe the various algorithms we compare with as well as the choice of hyper-parameters used for each of them. B.1. Datasets. We use four real-world datasets and one synthetic dataset. The descrip- tion and preprocessing we do for each of these datasets are as follows. Electricity Dataset. This is a public dataset obtained from the UCI repository which shows the 15-minutes electricity load of 370 households [36]. As was done in [45],[33],[31], we ag- gregate the data into hourly intervals and use the ﬁrst 25824 time-points for training, the next ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 29 TABLE 3 Dataset and training/validation/test split details. Dataset No.time series Observations per time series Forecast horizon (h) Training period No. validation windows Wval Validation period No. test windows Test period Electricity 370 26136 24 1 to 25824 2 25825 to 25968 7 25969 to 26136 Trafﬁc 963 10560 24 1 to 10248 2 10249 to 10392 7 10393 to 10560 Synthetic 50 15000 10 1 to 13700 10 13701 to 14000 100 14001 to 15000 Financial 839 3993 1 1 to 3693 40 3694 to 3813 180 3814 to 3993 M5 15678 1941 28 1 to 1829 1 1830 to 1913 1 1914 to 1941 288 points for validation, and the last 168 points for testing in the forecasting experiments. Speciﬁcally, in our testing period, we do 24-hour ahead forecasts for the next seven days (i.e. 24-step ahead forecast). See Table 3 for more details. Trafﬁc Dataset. This public dataset obtained from the UCI repository shows the occupancy rate of trafﬁc lanes in San Francisco [36]. The data is sampled every 15 minutes but to be consistent with previous work in [45], [33], we aggregate the data into hourly data and use the ﬁrst 10248 time-points for training, the next 288 points for validation, and the last 168 points for testing in the forecasting experiments. Speciﬁcally, in our testing period, we do 24-hour ahead forecasts for the next seven days (i.e. 24-step ahead forecast). See Table 3 for more details. Financial Dataset. This dataset is obtained from the Wharton Research Data Services (WRDS) and contains the average daily stocks prices of 839 companies from October 2004 till November 2019 [41]. The dataset was preprocessed to remove stocks with any null values, or those with an average price below 30$ across the aforementioned period. This was simply done to constrain the number of time series for ease of experimentation and we end up with 839 time series (i.e. stock prices of listed companies) each with 3993 readings of daily stock prices. In our forecasting experiments, we train on the ﬁrst 3693 time points, validate on the next 120 time points, while for testing we consider the task of predicting 180 time-points ahead one point at a time. That is, the goal here is to do one-day ahead forecasts for the next 180 days (i.e. 1-step ahead forecast). We choose to do so as this is a standard goal in ﬁnance. See Table 3 for more details. M5 Dataset. This public dataset obtained from Kaggle’s M5 Forecasting competition in- clude daily sales data of 30490 items across different Walmart stores for 1941 days [26]. The dataset was preprocessed to only include items that has more than zero sales in at least 500 days. For forecasting, as is the goal in the Kaggle competition, we consider the task of predicting the sales for the next 28 days (i.e. 28-step ahead forecast). We use the ﬁrst 1829 points for training, the next 84 points for cross validation, and the last 28 points for testing. Synthetic Dataset. We generate the observation tensor X ∈ Rn×m×T by ﬁrst randomly generating the two matrices U ∈ Rr×n = [u1, . . . , un] and V ∈ Rr×m = [v1, . . . , vm]; we do so by randomly sampling each coordinate of U, V independently from a standard normal. Then, we generate r mixtures of harmonics where each mixture gk(t), k ∈ [r], is generated as: gk(t) = ∑4 h=1 αh cos(ωht/T ) where the parameters αh, ωh are selected uniformly at ran- domly from the ranges [−1, 10] and [1, 1000], respectively. Then each value in the obser- vation tensor is constructed as follows: Xi,j(t) = ∑r k=1 uikvjkgk(t), where r is the tensor rank, i ∈ [n], j ∈ [m]. In our experiment, we select n = 5, m = 10, T = 15000, and r = 4. This gives us N = n x m = 50 time series each with 15000 observations per time series. In the forecasting experiments, we use the ﬁrst 13700 points for training, the next 300 points for validation, while for testing, we do 10-step ahead forecasts for the ﬁnal 1000 points. See Table 3 for more details. 30 B.2. Algorithms.. In this section, we describe the algorithms used throughout the ex- periments in more detail and the hyper-parameters/implementation used for each method. mSSA & SSA. Note that since the SSA’s variant described in [1] is a special case of our proposed mSSA algorithm, we use our mSSA’s implementation to perform the SSA exper- iments; key difference in SSA is that we do not “stack” the various Page matrices induced by each time series. For all experiments we choose the parameters through the cross vali- dation process detailed in Appendix B.3, where we perform a grid search for the following parameters: 1. The number of retained singular values, k. This parameter is chosen using one of the fol- lowing data-driven methods: (i) we choose k based on the thresholding procedure outlined in [17], where the threshold is determined by the median of the singular values and the shape of the matrix; (ii) we choose k as the minimum number of singular values capturing > 90% of its spectral energy; (iii) we choose a constant low rank, speciﬁcally k = 3. 2. The shape of the Page matrix. For mSSA, we vary the shape of the Page matrix by choosing L ∈ {500, 1000, 2000, 3000} for the electricity and Trafﬁc datasets, L ∈ {500, 700, 800} for the synthetic dataset, L ∈ {250, 500, 1000, 1500} for the ﬁnancial dataset, and L ∈ {10, 50, 100, 500} for the M5 dataset . For SSA, we choose L ∈ {50, 100, 150} in the electricity and Trafﬁc datasets, L ∈ {30, 50, 100} in the synthetic dataset, L ∈ {20, 30, 50} in the ﬁnancial dataset, and L ∈ {5, 10, 20, 40} in the M5 dataset. 3. Missing values initialization. Initializing the missing values is done according to one of two methods: (i) set the missing values to zero; (ii) perform forward ﬁlling where each missing value is replaced by the nearest preceding observation, followed by backward ﬁlling to accommodate the situation when the ﬁrst observation is missing. DeepAR. We use the “DeepAREstimator” algorithm provided by the GluonTS package. We choose the parameters through a grid search for the following parameters: 1. Context length. This parameter determines the number of steps to unroll the RNN for before computing predictions. We choose this from the set {h (default), 2h, 3h}, where h is the prediction horizon. 2. Number of Layers. This parameter determines the number of RNN layers. We choose this from the set {2 (default), 3}. TRMF. We use the implementation provided by the authors in the Github repository associ- ated with the paper ([45]). We choose the parameters through a grid search, as suggested by the authors in their codebase, for the following parameters: 1. Matrix rank k. This parameter represents the chosen rank for the T ×N time series matrix, we choose k from the set {5, 10, 20, 40, 60}. 2. Regularization parameters λf , λx, λw. We choose these parameters from {0.05, 0.5, 5, 50} as suggested in the authors repository. For the lag indices , we include the last day and the same weekday in the last week for the trafﬁc and electricity data, the last 30 points for the ﬁnancial and synthetic dataset, and the last 10 points for the M5 dataset. LSTM. Across all datasets, we use an LSTM network with H ∈ {2, 3, 4} hidden layers each, with 45 neurons per layer, as is done in [33]. We use the Keras implementation of LSTM. As with other methods’ parameters, H is chosen via cross validation. Prophet. We used Prophet’s Python library with the parameters selected using a grid search of the following parameters as suggested in [15]: ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 31 1. Changepoint prior scale. This parameter determines how much the trend changes at the detected trend changepoints. We choose this parameter from {0.001, 0.05, 0.2}. 2. Seasonality prior scale. This parameter controls the magnitude of the seasonality. We choose this parameter from {0.01, 10}. 3. Seasonality Mode. Which is chosen to be either ’additive‘ or ’multiplicative‘. VAR. We used the VAR estimator in the python package “statsmodels” ([32]). We apply the method on the ﬁrst difference of the time series and verify that the series are not non- stationary using a unit root test (speciﬁcally, Augmented Dickey–Fuller test). For all datasets except M5, we choose the best value for the parameter max_lag ∈ {1, 2, 5, 10, 20, 50}. This parameter corresponds to the maximum number of lags used in ﬁtting the VAR process. For M5, we choose max_lag ∈ {1, 2, 5}, as ﬁtting the model for larger values is computationally infeasible. B.3. Parameters Selection. In all experiments, we choose the hyperparameters for out method and for the baselines by using cross-validation. Below, we detail the procedure for both imputation and forecasting experiments. Imputation Experiments. To select the parameters in our imputation experiments, we ad- ditionally mask 10% of the observed data uniformly at random. Then, we evaluate the per- formance of each parameter choice in recovering these additionally masked observations. This process is repeated 3 times, and the choice of parameters that achieves the best perfor- mance (in NRMSE) across these runs is selected. In our results, we report the accuracy of the selected parameters in recovering the original missing values. Forecasting Experiments. For parameters selection in the forecasting experiments, we use cross-validation on a rolling basis as typically used in time-series forecasting models [25]. In this procedure, there are multiple validation sets. For each validation set, we train the model only on previous observations. That is, no future observations can be used in training the model, which will occur when a typical cross-validation procedure is followed for time series data. In our experiments, we start with a subset of the data used for training, then we forecast the ﬁrst validation set using h-step ahead forecasts for Wval windows , where the horizon h and the number of validation windows Wval are detailed in Table 3. We do this for three validation sets, each of length h × Wval, and select the choice of parameters that achieves the best performance (in NRMSE) for evaluation on the test set. When evaluating on the test set, both the training and validation periods are used for training. APPENDIX C: TIME-VARYING RECOMMENDATION SYSTEMS In Section 7.2, we considered the setting where the N × T matrix M induced by the latent time series f1(·), . . . , fN (·) is low-rank; in particular, Property 1 captures this spatial structure across these N time series. However, in many settings there is additional spatial structure across the N time series. Recommendation systems – time-varying matrices/tensors. For example, in recommendation systems, for each t ∈ T , there is a N1 × N2 matrix, M (t) ∈ RN1×N2 of interest. The n1- th row and n2-th column of M (t) denotes the latent rating user n1 has for product n2, i.e., M (t) n1,n2 denotes the value of the latent time series fn1,n2(·) at time step t. To capture the latent structure across users and products, one typically assumes that each M (t) is low-rank. More generally, at each time step t, M (t) ∈ RN1×N2,...,×Nd could be an order-d tensor. That is, M (t) n1,...,nd denotes the value of the latent time series fn1,...,nd(·) at time step t for n1, . . . , nd ∈ [N1] × · · · × [Nd]. For example, if d = 3, M (t) might represent the t-th measurement for a 32 collection of (x, y, z)-spatial coordinates. Let N ∈ RN1×N2,...,×Nd×T denote the d + 1 order tensor induced by viewing each order-d tensor M (t) as the t-th ‘slice’ of N , for t ∈ [T ]. Again, to capture the spatial and temporal structure of these latent time series, we posit the following spatio-temporal model for N , which is a higher-order analog of the model assumed in Property 1. PROPERTY 13. Let N have CP-rank at most R. That is, for any n1, . . . , nd ∈ [N1] × · · · × [Nd] N n1,...,nd,t = R∑ r=1 Un1,r . . . Und,r Wrt, where the factorization is such that |Un1,r|, . . . |Und,r| ≤ Γ1, |Wrt| ≤ Γ2 for constants Γ1, Γ2 > 0. As before, to explicitly model the temporal structure, we continue to assume Property 2 holds for the latent time factors Wr· for r ∈ [R]. Order-d + 2 Page tensor representation. We now consider the following order-d + 2 Page tensor representation of N . In particular, given the hyper-parameter L ≥ 1, deﬁne HT ∈ RN1×···×Nd×T /L×L such that for n1, . . . , nd ∈ [N1] × · · · × [Nd], ℓ ∈ [L], s ∈ [T /L], HTn1,...,nd,ℓ,s = fn1,...,nd((s − 1) × L + ℓ). The corresponding observation tensor, HT ∈ (R ∪ {⋆})N1×···×Nd×T /L×L, is HTn1,...,nd,ℓ,s = Xn1,...,nd((s − 1) × L + ℓ).(9) Recall from (1) that Xn1,...,nd(t) is the noisy, missing observation we get of fn1,...,nd(t). HT and HT then have the following property: PROPOSITION C.1. Let Properties 13, 2, and 3 hold. Then, for any 1 ≤ L ≤ √T , HT has CP-rank at most R x G. Further, all entries of HT are independent random variables with each entry observed with probability ρ ∈ (0, 1], and E[HT] = ρHT. Analogous to Proposition 7.1, Proposition C.1 also establishes that order-d + 2 Page tensor representation of the various latent time series fn1,...,nd(·) has CP-rank that continues to be bounded by R x G. Proof of Proposition C.1 can be found in Appendix L. Higher-order tensor singular spectrum analysis (htSSA). Proposition C.1 motivates the following algorithm, which exploits the further spatial structure amongst the N time series. We now deﬁne the “meta” htSSA algorithm. The two algorithmic hyper-parameters are L ≥ 1 (deﬁned in (8)) and TEd+2 (the order-d + 2 tensor estimation algorithm one chooses). First, using the observations Xn1,...,nd(t) for n1, . . . , nd ∈ [N1]×· · ·×[Nd], t ∈ [T ] we construct the higher-order Page tensor HT as in (9). Second, we obtain ̂HT as the output of TEd+2(HT), and read off ˆfn1,...,nd(t) by selecting the appropriate entry in ̂HT. Relative effectiveness of mSSA, htSSA, and tensor estimation (TE). Again, for ease of exposition, we consider the case where ρ = 1. We now brieﬂy discuss the relative ef- fectiveness of htSSA, mSSA,and “vanilla” tensor estimation (TE) in imputing Xn1,...,nd(·) to estimate fn1,...,nd(·). mSSA and htSSA have been previously described. In TE, one di- rectly de-noises the original order-d + 1 tensor induced by the noisy observations, which ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 33 we denote X ∈ RN1×N2,...,×Nd×T , where X n1,...,nd,t = Xn1,...,nd(t). In particular, one pro- duces an estimate of ̂N = TEd+1(X), and then produces the estimates ˆfn1,...,nd(t) by read- ing off the appropriate entry of ̂N . Let ImpErr(N, T ; htSSA), ImpErr(N, T ; mSSA), and ImpErr(N, T ; TE) denote the imputation error for htSSA, mSSA, and TE, respectively. Now if we assume Property 12 holds, we have ImpErr(N, T ; htSSA) = ˜Θ    1 min (N1, . . . , Nd, √T )⌈ d+2 2 ⌉    , ImpErr(N, T ; mSSA) = ˜Θ ( 1 √min(N, T )T ) , ImpErr(N, T ; TE) = ˜Θ ( 1 min (N1, . . . , Nd, T ) ⌈ d+1 2 ⌉ ) . Then just as was done in the proof of Proposition 7.2, for any given d, one can reason about the relative effectiveness of htSSA, mSSA, and TE for different asymptotic regimes of the relative ratio of N and T . APPENDIX D: PROOF OF PROPOSITION 5.1 Below, we present the proof of Proposition 5.1. First we deﬁne the stacked Hankel matrix of N time series over T time steps. Precisely, given N latent time series f1, . . . , fN , consider the stacked Hankel matrix induced by each of them over T time steps, [T ], deﬁned as follows. It is SH ∈ R⌊T /2⌋×N ⌊T /2⌋ where its entry in row i ∈ [⌊T /2⌋] and column j ∈ [N ⌊T /2⌋], SHij, is given by SHij = fn(i,j)(i + (j mod ⌊T /2⌋) − 1), where n(i, j) = ⌈ j ⌊T /2⌋ ⌉ . We now establish Proposition D.1, which immediately implies Proposition 5.1 – the stacked Page matrix can be viewed as a sub-matrix of SH, by selecting the appropriate columns. PROPOSITION D.1. Let Properties 1 and 7 hold for N latent time series of interest, f1, . . . , fN . Then for any T ≥ 1, the stacked Hankel Matrix of these N time series has ϵ′- approximate rank R x G with ϵ′ = RΓ1ϵ. PROOF. We have N latent time series f1, . . . , fn satisfying Properties 1 and 7. Consider their stacked Hankel matrix over [T ], SH ∈ R⌊T /2⌋×N ⌊T /2⌋. By deﬁnition for i ∈ [⌊T /2⌋] and j = (n − 1) x ⌊T /2⌋ + j′ for j′ ∈ [⌊T /2⌋], we have SHij′ = fn(i + j′ − 1). That is, SHij = fn(i + j′ − 1) = R∑ r=1 UnrWr(i+j′−1).(10) Let H(r) ∈ R⌊T /2⌋×⌊T /2⌋ be the Hankel matrix associated with Wr· over [T ]. Due to Property 7, there exists a low-rank matrix M (r) ∈ R⌊T /2⌋×⌊T /2⌋ such that (a) rank(M (r)) ≤ G, (b) 34 ∥H(r) − M (r)∥∞ ≤ ϵ. That is, for any i, j′ ∈ [⌊T /2⌋], we have that M (r)ij′ = ∑G g=1 ar igbr j′g for some ar i·, br j′· ∈ RG. Therefore, for any i, j′ ∈ [⌊T /2⌋], we have that Wr(i+j′−1) = H(r)ij′ = M (r)ij′ + (H(r)ij′ − M (r)ij′) = G∑ g=1 ar igb r j′g + (H(r)ij′ − M (r)ij′).(11) From (10) and (11), we conclude that SHij = R∑ r=1 G∑ g=1 Unra r igb r j′g + R∑ r=1 Unr(H(r)ij′ − M (r)ij′) = ∑ (r,g)∈[R]×[G] ar ig x (Unrb r j′g) + R∑ r=1 Unr(H(r)ij′ − M (r)ij′). Deﬁne matrix M ∈ R⌊T /2⌋×N ⌊T /2⌋ with its entry for row i ∈ [⌊T /2⌋] and column j = (n − 1) x ⌊T /2⌋ + j′ for j′ ∈ [⌊T /2⌋] given by Mij = ∑ (r,g)∈[R]×[G] a r ig x (Unrb r j′g) = ∑ (r,g)∈[R]×[G] αi(r,g)βj(r,g), where αi(r,g) = ar ig and βj(r,g) = Unrbr j′g. Further, |SHij − Mij| ≤ R∑ r=1 |Unr||(H(r)ij′ − M (r)ij′)| ≤ R∑ r=1 Γ1∥H(r) − M (r)∥∞ ≤ RΓ1ϵ. That is, the stacked Hankel matrix SH of N time series of [T ] has ϵ′-approximate rank G x R with ϵ′ = RΓ1ϵ. This completes the proof. APPENDIX E: PROOFS FOR SECTION 5 E.1. Proof of Proposition 5.2. PROOF. Let f1, f2 have a (G1, ϵ1) and (G2, ϵ2)-Hankel representation, respectively. For any T ≥ 1, let H 1, H 2 ∈ R⌊T /2⌋×⌊T /2⌋ be the Hankel matrices of f1, f2, respectively, over the time interval [T ]. By deﬁnition, there exists matrices M1, M2 ∈ R⌊T /2⌋×⌊T /2⌋ such that rank(M 1) ≤ G1, ∥M 1 − H 1∥∞ ≤ ϵ1 and rank(M 2) ≤ G2, ∥M 2 − H 2∥∞ ≤ ϵ2. Component-wise addition. Note the Hankel matrix of f1 + f2 over [T ] is H 1 + H 2. Then, matrix M = M 1 + M 2 has rank at most G1 + G2 since for any two matrices A and B, it is the case that rank(A + B) ≤ rank(A) + rank(B). Further, ∥H 1 + H 2 − (M 1 + M 2)∥∞ ≤ ϵ1 + ϵ2. Therefore it follows that f1 + f2 has (G1 + G2, ϵ1 + ϵ2)-Hankel representation. Component-wise multiplication. For f1 ◦ f2, its Hankel over [T ] is given by H 1 ◦ H 2 where we abuse notation of ◦ in the context of matrices as the Hadamard product of matrices. Let ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 35 M = M 1 ◦ M 2. Then rank(M ) ≤ G1 x G2 since for any two matrices A and B, rank(A ◦ B) ≤ rank(A)rank(B). Now ∥H 1 ◦ H 2 − M 1 ◦ M 2∥∞ ≤ ∥H 1 ◦ H 2 − H 1 ◦ M 2∥∞ + ∥H 1 ◦ M 2 − M 1 ◦ M 2∥∞ ≤ ∥H 1∥∞∥H 2 − M 2∥∞ + ∥M 2∥∞∥H 1 − M 1∥∞ ≤ ∥f1∥∞ϵ2 + (∥M 2 − H 2∥∞ + ∥H 2∥∞)ϵ1 ≤ ∥f1∥∞ϵ2 + (∥f2∥∞ + ϵ2)ϵ1 = ∥f1∥∞ϵ2 + ∥f2∥∞ϵ1 + ϵ1ϵ2 ≤ 3 max(ϵ1, ϵ2) max(∥f1∥∞, ∥f2∥∞). This completes the proof of Proposition 5.2. E.2. Proof of Proposition 5.3. PROOF. Proof is immediate from Deﬁnitions 5.2 and 5.3. E.3. Proof of Proposition 5.4. E.3.1. Helper Lemmas for Proposition 5.4. We begin by stating some classic results from Fourier Analysis. To do so, we introduce some notation. Throughout, we have R > 0. C[0, R] and L2[0, R] functions. C[0, R] is the set of real-valued, continuous functions de- ﬁned on [0, R]. L2[0, R] is the set of square integrable functions deﬁned on [0, R], i.e. ∫ R 0 f 2(t)dt ≤ ∞ Inner Product of functions in L2[0, R]. L2[0, R] is a space endowed with inner product deﬁned as ⟨f, g⟩ := 1 R ∫ R 0 f (t)g(t)dt, and associated norm as ∥f ∥ := √ 1 R ∫ R 0 f 2(t)dt. Fourier Representation of functions in L2[0, R]. For f ∈ L2[0, R], deﬁne its G ≥ 1-order Fourier representation, F(f, G) ∈ L2[0, R] as F(f, G)(t) = a0 + G∑ g=1(ag cos(2πgt/R) + bg cos(2πgt/R)), t ∈ [0, R],(12) where a0, ag, bg with g ∈ [G] are called the Fourier coefﬁcients of f , deﬁned as a0 := ⟨f, 1⟩ = 1 R ∫ R 0 f (t)dt, ag := ⟨f, cos(2πgt/R)⟩ = 1 R ∫ R 0 f (t) cos(2πgt/R)dt, bg := ⟨f, sin(2πgt/R)⟩ = 1 R ∫ R 0 f (t) sin(2πgt/R)dt. We now state a classic result from Fourier analysis. THEOREM E.1 ([20]). Given k ≥ 1, R > 0, let f ∈ Ck(R, PER). Then, for any t ∈ [0, R] (or more generally t ∈ R), lim G→∞ F(f, G)(t) → f (t). We next argue that if f ∈ Ck(R, PER), then its Fourier coefﬁcients decay rapidly. 36 LEMMA E.2. Given k ≥ 1, R > 0, let f ∈ Ck(R, PER). Then, for j ∈ [k], the G-order Fourier coefﬁcient of f (j), the j-th derivative of f , recursively satisfy the following relation- ship: for g ∈ [G], a (j) g = − ( 2πg R )b (j−1) g , b (j) g = ( 2πg R )a(j−1) g .(13) PROOF. We establish (13) for a(1) g , g ∈ [G]. Notice that an identical argument applies to establish (13) for any a (j) g , b (j) g for j ∈ [k] and g ∈ [G]. a(1) g = ⟨f (1), cos(2πgt/R)⟩ = 1 R ∫ R 0 f (1)(t) cos(2πgt/R)dt (a) = 1 R ([f (t) cos(2πgt/R)]R 0 − 2πg R [ 1 R ∫ R 0 f (t) sin(2πgt/R)dt ]) = − ( 2πg R ) b (0) g . (a) follows by integration by parts. E.3.2. Completing Proof of Proposition 5.4. PROOF. For G ∈ N, let F(f, G) be deﬁned as in (12). Then for t ∈ R |f (t) − F(f, G)(t)| (a) = ∣ ∣ ∣ ∞∑ g=G+1 (ag cos(2πgt/R) + bg cos(2πgt/R)) ∣ ∣ ∣ ≤ ∞∑ g=G+1 |ag| + |bg| (b) ≤ ∞∑ g=G+1 ( R 2πg )k(|a (k) g | + |b (k) g |) (c) ≤ √2( R 2π )k√ √ √ √ ∞∑ g=G+1 ( 1 g )2k√ √ √ √ ∞∑ g=G+1 ( |a (k) g |2 + |b (k) g |2) (d) ≤ √2 ( R 2π )k 1 Gk−0.5 √ √ √ √ ∞∑ g=G+1 (|a(k) g |2 + |b (k) g |2) (e) ≤ √2( R 2π )k ∥f (k)∥ Gk−0.5 = C(k, R) ∥f (k)∥ Gk−0.5 , where C(k, R) is a constant that depends only on k and R; (a) follows from Theorem E.1; (b) follows from Lemma E.2; (c) follows from Cauchy-Schwarz inequality and fact that (α + β)2 ≤ 2(α2 + β2) for any α, β ∈ R; (d) ∑∞ g=G+1 g−2k ≤ ∫ ∞ G x−2kdx which can be bounded as G−2k+1/(2k − 1) which is at most G−2k+1 since k ≥ 1; (e) follows from Bessel’s inequality, i.e. ∥f (k)∥2 ≥ ∑∞ g=0(|a (k) g |2 + |b (k) g |2). ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 37 Thus, for any t ∈ R, we have a uniform error bound for f being approximated by F(f, G) which is a sum of 2G harmonics. Noting 2G harmonics can be represented by an order-4G LRF (by Proposition 3.1),we complete the proof. E.4. Proof of Proposition 5.5. This analysis is adapted from [43]. PROOF. Step 1: Partitioning the space [0, 1)K . Consider an equal partition of [0, 1)K . Precisely, for any k ∈ N, we partition the the set [0, 1) into 1/k half-open intervals of length 1/k, i.e, [0, 1) = ∪k i=1 [(i − 1)/k, i/k) . It follows that [0, 1)K can be partitioned into kK cubes of forms ⊗K j=1 [(ij − 1)/k, ij/k) with ij ∈ [k]. Let Ek be such a partition with I1, I2, . . . , IkK denoting all such cubes and z1, z2, . . . , zkK ∈ RK denoting the centers of those cubes. Step 2: Taylor Expansion of g(·, ω). Consider a ﬁxed ω. To reduce notational overload, we suppress dependence of g on ω, and abuse notation by using g(·) = g(·, ω) in what follows. For every Ii with 1 ≤ i ≤ kK , deﬁne PIi,ℓ(x) as the degree-ℓ Taylor’s series expansion of g(x) at point zi: PIi,ℓ(x) = ∑ κ:|κ|≤ℓ 1 κ! (x − zi)κ ∇κg(zi),(14) where κ = (κ1, . . . , κd) is a multi-index with κ! = ∏K i=1 κi!, and ∇kg(zi) is the partial deriva- tive deﬁned in Section 5.2. Note similar to g, PIi,ℓ(x) really refers to PIi,ℓ(x, ω). Now we deﬁne a degree-ℓ piecewise polynomial PEk,ℓ(x) = kK ∑ i=1 PIi,ℓ(x)1(x ∈ Ii). For the remainder of the proof, let ℓ = ⌊α⌋ (recall ⌊α⌋ refers to the largest integer strictly smaller than α). Since f ∈ H(α, L), it follows that sup x∈[0,1)K |g(x) − PEk,ℓ(x)| = max 1≤i≤kK sup x∈Ii |g(x) − PIi,ℓ(x)| (a) = max 1≤i≤kK sup x∈Ii ∣ ∣ ∣ ∣ ∣ ∣ ∑ κ:|κ|≤ℓ−1 ∇κg(zi) κ! (x − zi) κ + ∑ κ:|κ|=ℓ ∇κg(˜zi) κ! (x − zi) ℓ − PIi,ℓ(x) ∣ ∣ ∣ ∣ ∣ ∣ (b) = max 1≤i≤kK sup x∈Ii ∣ ∣ ∣ ∣ ∣ ∣ ∑ κ:|κ|=ℓ ∇κg(˜zi) κ! (x − zi)ℓ − ∑ κ:|κ|=ℓ ∇κg(zi) κ! (x − zi)ℓ ∣ ∣ ∣ ∣ ∣ ∣ = max 1≤i≤kK sup x∈Ii ∣ ∣ ∣ ∣ ∣ ∣ ∑ κ:|κ|=ℓ ∇κg(˜zi) − ∇κg(zi) κ! (x − zi)ℓ ∣ ∣ ∣ ∣ ∣ ∣ (c) ≤ max 1≤i≤kK sup x∈Ii ∥x − zi∥ℓ ∞ sup x∈Ii ∑ κ:|κ|=ℓ 1 κ! |∇κg(˜zi) − ∇κg(zi)| (d) ≤ Lk−α.(15) 38 where (a) follows from multivariate version of Taylor’s theorem (and using the Lagrange form for the remainder) and ˜zi ∈ [0, 1)K is a vector that can be represented as zi + cx for c ∈ (0, 1); (b) follows from (14); (c) follows from Holder’s inequality; (d) follows from Deﬁnition 5.5. Step 3: Construct Low-Rank Approximation of Time Series Hankel Using PEk,ℓ. Recall the Hankel matrix, H ∈ R⌊T /2⌋×⌊T /2⌋ induced by the original time series over [T ], where H ts = g(θt, ωs), t, s ∈ [⌊T /2⌋] with g(·, ω) ∈ H(α, L) for any ω. We now construct a low- rank approximation of it using PEk,ℓ = PEk,ℓ(·, ω). Deﬁne ̃H ∈ R⌊T /2⌋×⌊T /2⌋, where ̃H ts = PEk,ℓ(θt, ωs), t, s ∈ [⌊T /2⌋]. By (15), we have that for all t, s ∈ [⌊T /2⌋], ∣ ∣ ∣H ts − ̃H ts∣ ∣ ∣ ≤ Lk−α. It remains to bound the rank of ̃H. Note that since PEk,ℓ(·, ω) is a piecewise polynomial of degree ℓ = ⌊α⌋ for any given ω, it has the following decomposition: for t, s ∈ [⌊T /2⌋], ̃H ts = PEk,ℓ(θt, ωs) = kK ∑ i=1⟨Φ(θt), βIi,s⟩1(θt ∈ Ii) where for any θ ∈ RK , Φ(θ) = ( 1, θ1, . . . , θK, . . . , θℓ 1, . . . , θℓ K)T , the vector of all monomials of degree less than or equal to ℓ, and βIi,s is a vector collecting the corresponding coefﬁcients. The number of such monomials is easily show to be equal to C(α, K) := ∑⌊α⌋ i=1 (i+K−1 i ). That is, ̃H ts = uT t vs where ut, vs are of dimension at most kKC(α, K) for each t, s ∈ [⌊T /2⌋]. That is, ̃H has rank at most kKC(α, K). Setting k = ⌈ 1 ϵ ⌉ completes the proof. APPENDIX F: HELPER LEMMAS We recall known concentration and perturbation inequalities that will be useful throughout. THEOREM F.1 (Bernstein’s Inequality [8]). Suppose that X1, . . . , Xn are independent random variables with zero mean, and M is a constant such that |Xi| ≤ M with probability one for each i. Let S := ∑n i=1 Xi and v := Var(S). Then for any t ≥ 0, P(|S| ≥ t) ≤ 2 exp(− 3t2 6v + 2M t ). THEOREM F.2 (Norm of matrices with sub-gaussian entries [37]). Let A be an m × n random matrix whose entries Aij are independent, mean zero, sub-gaussian random vari- ables. Then, for any t > 0, we have ∥A∥ ≤ CK( √m + √n + t) with probability at least 1 − 2 exp(−t2). Here, K = maxi,j∥Aij∥ψ2. LEMMA F.3 (Maximum of sequence of random variables [37]). Let X1, X2, . . . , Xn be a sequence of random variables, which are not necessarily independent, and satisfy E[X 2p i ] 1 2p ≤ Kp β 2 for some K, β > 0 and all i. Then, for every n ≥ 2, E max i≤n |Xi| ≤ CK log β 2 (n). ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 39 We note that Lemma F.3 implies that if X1, . . . , Xn are ψα random variables with ∥Xi∥ψα ≤ Kα for all i ∈ [n], then E max i≤n |Xi| ≤ CKα log 1 α (n). LEMMA F.4 (Modiﬁed Hoeffding Inequality [2] ). Let X ∈ Rn be random vector with independent mean-zero sub-Gaussian random coordinates with ∥Xi∥ψ2 ≤ K. Let a ∈ Rn be another random vector that satisﬁes ∥a∥2 ≤ b almost surely for some constant b ≥ 0. Then for all t ≥ 0, P(∣ ∣ ∣ n∑ i=1 aiXi∣ ∣ ∣ ≥ t ) ≤ 2 exp ( − ct2 K2b2 ), where c > 0 is a universal constant. LEMMA F.5 (Modiﬁed Hanson-Wright Inequality [2] ). Let X ∈ Rn be a random vector with independent mean-zero sub-Gaussian coordinates with ∥Xi∥ψ2 ≤ K. Let A ∈ Rn×n be a random matrix satisfying ∥A∥2 ≤ a and ∥A∥2 F ≤ b almost surely for some a, b ≥ 0. Then for any t ≥ 0, P (|X T AX − E[X T AX]| ≥ t) ≤ 2 · exp ( − c min ( t2 K4b , t K2a )). LEMMA F.6 (Weyl’s inequality). Given A, B ∈ Rm×n, let σi and ̂σi be the i-th singular values of A and B, respectively, in decreasing order and repeated by multiplicities. Then for all i ∈ [m ∧ n], |σi − ̂σi| ≤ ∥A − B∥2. APPENDIX G: MATRIX ESTIMATION VIA HSVT This section describes and analyzes a well-known matrix estimation method, Hard Sin- gular Value Thresholding (HSVT). While the analysis utilizes known arguments from the literature, we need to adapt it for the setting where the underlying ‘signal’ is only approxi- mately low-rank. G.1. Setup, Notations. Setup. Given a deterministic matrix M ∈ Rq×p with p, q ∈ N and q ≤ p, a random matrix Y ∈ Rq×p is such that all of its entries, Yij, i ∈ [q], j ∈ [p] are mutually independent and for any given i ∈ [q], j ∈ [p], Yij = { Mij + εij w.p. ρ, (i.e. observed) 0 w.p. 1 − ρ, (i.e. not observed) for some ρ ∈ (0, 1] with εij are independent random variables with E[εij] = 0 and ∥εij∥ψ2 ≤ σ. Given this, we have E[Y ] = ρM . Deﬁneff ̂ρ = max (1/(q p), ( q∑ i=1 p∑ j=1 1(Yij is obs.) )/(q p) ). Goal of Matrix Estimation. The goal of matrix estimation is to produce an estimate ̂M from observation Y so that ̂M is close to M . In particular, we will be interested in bounding the error between ̂M and M using the following metric: ∥ ̂M − M ∥2,∞. 40 G.2. Matrix Estimation using HSVT. Hard Singular Value Thresholding (HSVT) Map. We deﬁne the HSVT map. For any q, p ∈ N, consider a matrix B ∈ Rq×p such that B = ∑q∧p i=1 σi(B)xiyT i . Here for i ∈ [q ∧ p], σi(B) is the ith largest singular value of B and xi, yi are the corresponding left and right singular vectors respectively. Then, for given any λ > 0, we deﬁne the map HSVTλ : Rq×p → Rq×p, which simply shaves off the singular values of the input matrix that are below the threshold λ. Precisely, HSVTλ(B) = q∧p∑ i=1 σi(B)1(σi(B) ≥ λ)xiyT i . Matrix Estimating using HSVT map. We deﬁne a matrix estimation method using the HSVT map that is utilized by mSSA for imputation. Precisely, we estimate M from Y as follows: given parameter k ≥ 1, ̂M = 1 ̂ρ HSVTλk(Y ).(16) where λk = σk(Y ), i,e. the kth largest singular value of Y . G.3. A Useful Linear Operator. We deﬁne a linear map associated to HSVT. For a speciﬁc choice of λ ≥ 0, deﬁne ϕB λ : Rp → Rp as follows: for any vector w ∈ Rp (i.e. w ∈ Rp×1), ϕ B λ (w) = q∧p∑ i=1 1(σi(B) ≥ λ)yiyT i w.(17) Note that ϕB λ is a linear operator and it depends on the tuple (B, λ); more precisely, the singular values and the right singular vectors of B, as well as the threshold λ. If λ = 0, then we will adopt the shorthand notation: ϕB = ϕB 0 . The following is a simple, but curious relationship between ϕB λ and HSVTλ that will be useful subsequently. LEMMA G.1 (Lemma 35 of [3, 4]). Let B ∈ Rq×p and λ ≥ 0 be given. Then for any j ∈ [q], ϕB λ (BT j·) = HSVTλ(B)T j·, where Bj· ∈ R1×p represents the jth row of B, and HSVTλ(B) j· ∈ R1×p represents the jth row of the matrix obtained after applying HSVT over B with threshold λ. PROOF. By (17), the orthonormality of the right singular vectors and noting BT j· = BT ej with ej ∈ Rp with jth entry 1 and everything else 0, we have ϕ B λ (BT j·) = q∧p∑ i=1 1(σi(B) ≥ λ)yiyT i BT j· = q∧p∑ i=1 1(σi(B) ≥ λ)yiyT i BT ej = q∧p∑ i=1 1(σi(B) ≥ λ)yiyT i ( q∧p∑ i′=1 σi′(B)xi′yT i′ )T ej = q∧p∑ i,i′=1 σi′(B)1(σi(B) ≥ λ)yiyT i yi′xT i′ ej = q∧p∑ i,i′=1 σi′(B)1(σi(B) ≥ λ)yiδii′x T i′ ej = q∧p∑ i=1 σi(B)1(σi(B) ≥ λ)yixT i ej = HSVTλ(B)T ej = HSVTλ(B)T j·. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 41 G.4. HSVT based Matrix Estimation: A Deterministic Bound. We state the follow- ing result about property of the estimator. LEMMA G.2. For k ≥ 1, let M = M k + Ek with rank(M k) = k. Let ε = max(̂ρ/ρ, ρ/̂ρ) ≥ 1. Then, the HSVT estimate ̂M with parameter k is such that for all j ∈ [q], ∥ ̂M T j· − M T j·∥ 2 2 ≤ 2∥Y − ρM ∥2 2 + 2ρ2∥Ek∥2 2 (σk(ρM k) )2 (2 ∥ ∥ ∥[M k] T j·∥ ∥ ∥ 2 2 + 4ε2(∥Y T j· − ρM T j·∥2)2 ρ2 ) + 4ε2 ρ2 ∥ ∥ ∥ϕ M k(Y T j· − ρM T j·) ∥ ∥ ∥ 2 2 + 2(ε − 1) 2∥M T j·∥ 2 2 + 2∥ ∥ ∥[Ek] T j·∥ ∥ ∥2 2. PROOF. We prove our lemma in four steps. G.4.0.1. Step 1. Decomposing ̂M T j· − M T j· in two terms.. Fix a row index j ∈ [q]. Let λk be the kth largest singular value of Y , as used by HSVT algorithm with parameter k ≥ 1. ̂M T j· − M T j· = ( ̂M T j· − ϕY λk(M T j·)) + (ϕ Y λk(M T j·) − M T j·). By deﬁnition per (17), ϕY λk : Rp → Rp is the projection operator onto span{u1, . . . , uk}, the span of top k right singular vectors of Y , denoted as u1, . . . , uk. Therefore, ϕ Y λk(M T j·) − M T j· ∈ span{u1, . . . , uk}⊥. By design, rank( ̂M ) = k. Therefore, by Lemma G.1 ̂M j· − ϕ Y λk(M T j·) = 1 ̂ρ ϕ Y λk(Y T j·) − ϕ Y λk(M T j·) ∈ span{u1, . . . , uk}. Therefore, ⟨ ̂M T j· − ϕY λk(M T j·), ϕY λk(M T j·) − M T j·⟩ = 0, and hence ∥ ∥ ∥ ̂M T j· − M T j·∥ ∥ ∥ 2 2 = ∥ ∥ ∥ ̂M T j· − ϕY λk(M T j·)∥ ∥ ∥ 2 2 + ∥ ∥ ∥ϕ Y λk(M T j·) − M T j·∥ ∥ ∥ 2 2 (18) by the Pythagorean theorem. G.4.0.2. Step 2. Bounding Term 1, ∥ ∥ ∥ ̂M T j· − ϕY λk( M T j·)∥ ∥ ∥ 2.. We begin by bounding the ﬁrst term on the right hand side of (18). By Lemma G.1, ̂M j· − ϕ Y λk(M T j·) = 1 ̂ρ ϕ Y λk(Y T j·) − ϕ Y λk(M T j·) = ϕ Y λk( 1 ̂ρ Y T j· − M T j·) = 1 ̂ρ ϕ Y λk(Y T j· − ρM T j·) + ρ − ̂ρ ̂ρ ϕ Y λk(M T j·). Using the Parallelogram Law (or, equivalently, combining Cauchy-Schwartz and AM-GM inequalities), we obtain ∥ ̂M T j· − ϕY λk(M j·)T ∥2 2 = ∥ 1 ̂ρ ϕ Y λk(M T j· − ρM T j·) + ρ − ̂ρ ̂ρ ϕ Y λk(M T j·)∥2 2 ≤ 2 ∥ 1 ̂ρ ϕ Y λk(Y T j· − ρM T j·)∥ 2 2 + 2 ∥ ρ − ̂ρ ̂ρ ϕ Y λk(M T j·)∥2 2 ≤ 2 ̂ρ2 ∥ϕY λk(Y T j· − ρM T j·)∥ 2 2 + 2( ρ − ̂ρ ̂ρ )2∥M T j·∥2 2 ≤ 2ε2 ρ2 ∥ϕ Y λk(Y T j· − ρM T j·)∥2 2 + 2(ε − 1) 2∥M T j·∥2 2.(19) 42 From deﬁnition of ε, 1 ̂ρ ≤ ε ρ and ( ρ−̂ρ ̂ρ )2 ≤ (ε − 1)2. The ﬁrst term of (19) can be decomposed as, ∥ϕ Y λk(Y T j· − ρM T j·)∥2 2 ≤ 2 ∥ ∥ ∥ϕ Y λk(Y T j· − ρM T j·) − ϕ M k(Y T j· − ρM T j·) ∥ ∥ ∥ 2 2 + 2 ∥ ∥ ∥ϕ M k(Y T j· − ρM T j·) ∥ ∥ ∥ 2 2.(20) In above, we have used notation ϕM k = ϕ M k 0 . Given that M k is rank k matrix, ϕM k : Rp → Rp is the projection operator mapping any element in Rp to the projection onto the subspace spanned by {µ1, . . . , µk}, where µ1, . . . , µk ∈ Rp are the k non-trivial right singular vectors of M k. Similarly, by deﬁnition ϕY λk is a map Rp → Rp mapping any element in Rp to its projection onto the subspace spanned by {u1, . . . , uk}, the top k right singular vectors of Y –this can be seen by noting λk = σk(Y ) is the k-th top singular value of Y . Recall σj(Y ), j ∈ [q ∧ p] is the jth largest singular value of Y . Next, we bound the ﬁrst term on the right hand side of (20). To that end, by Wedin sin Θ Theorem (see [13, 39]) and recalling rank(M k) = k, ∥ ∥ϕ Y λk − ϕM k∥ ∥ 2 ≤ ∥Y − ρM k∥2 σk(ρM k) ≤ ∥Y − ρM ∥2 σk(ρM k) + ρ∥M − M k∥2 σk(ρM k) ≤ ∥Y − ρM ∥2 σk(ρM k) + ρ∥Ek∥2 σk(ρM k) .(21) Then it follows that ∥ ∥ ∥ϕY λk(Y T j· − ρM T j·) − ϕ M k(Y T j· − ρM T j·) ∥ ∥ ∥ 2 ≤ ∥ϕY λk − ϕM k∥2∥Y T j· − ρM T j·∥2 ≤ (∥Y − ρM ∥2 + ρ∥Ek∥2)(∥Y T j· − ρM T j·∥2) σk(ρM k) .(22) Using (20) and (22) in (19), ∥ ̂M j· − ϕY λk(M T j·)∥ 2 2 ≤ 4ε2 ρ2 (∥Y − ρM ∥2 + ρ∥Ek∥2)2(∥Y T j· − ρM T j·∥2)2 (σk(ρM k) )2 + 4ε2 ρ2 ∥ ∥ ∥ϕ M k(Y T j· − ρM T j·) ∥ ∥ ∥ 2 2 + 2(ε − 1) 2∥M T j·∥ 2 2.(23) G.4.0.3. Step 3. Bounding Term 2, ∥ ∥ ∥ϕY λk( M T j·) − M T j·∥ ∥ ∥2 2.. Recall M = M k + Ek and using (21), ∥ ∥ ∥ϕ Y λk( M T j·) − M T j·∥ ∥ ∥2 2 = ∥ ∥ ∥ϕ Y λk([M k] T j· + [Ek]T j·) − [M k] T j· − [Ek] T j·∥ ∥ ∥2 2 ≤ 2∥ ∥ ∥ϕY λk([M k] T j·) − [M k] T j·∥ ∥ ∥2 2 + 2∥ ∥ ∥ϕ Y λk( [Ek]T j·) − [Ek] T j·∥ ∥ ∥ 2 2 = 2∥ ∥ ∥ϕY λk([M k]T j·) − ϕ M k λk ( [M k] T j·)∥ ∥ ∥ 2 2 + 2∥ ∥ ∥ϕY λk([Ek] T j·) − [Ek]T j·∥ ∥ ∥ 2 2 ≤ 2∥ ∥ ∥ϕY λk − ϕ M k λk ∥ ∥ ∥ 2 2 ∥ ∥ ∥[M k] T j·∥ ∥ ∥ 2 2 + 2∥ ∥ ∥[Ek]T j·∥ ∥ ∥ 2 2 ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 43 ≤ 2 (∥Y − ρM ∥2 + ρ∥Ek∥ )2 (σk(ρM k))2 ∥ ∥ ∥[M k] T j·∥ ∥ ∥ 2 2 + 2∥ ∥ ∥[Ek] T j·∥ ∥ ∥ 2 2.(24) G.4.0.4. Step 4. Putting everything together.. Inserting (23) and (24) back to (18), we have that for each j ∈ [q], ∥ ∥ ∥ ̂M T j· − M T j·∥ ∥ ∥2 2 ≤ 2 (∥Y − ρM ∥2 + ρ∥Ek∥2)2 (σk(ρM k))2 ∥ ∥ ∥[M k]T j·∥ ∥ ∥ 2 2 + 2∥ ∥ ∥[Ek] T j·∥ ∥ ∥2 2 + 4ε2 ρ2 (∥Y − ρM ∥2 + ρ∥Ek∥2)2(∥Y T j· − ρM T j·∥2)2 (σk(ρM k))2 + 4ε2 ρ2 ∥ ∥ ∥ϕ M k(Y T j· − ρM T j·)∥ ∥ ∥ 2 2 + 2(ε − 1) 2∥M T j·∥ 2 2 ≤ 2∥Y − ρM ∥2 2 + 2ρ2∥Ek∥2 2 ( σk(ρM k) )2 (2∥ ∥ ∥[M k] T j·∥ ∥ ∥2 2 + 4ε2(∥Y T j· − ρM T j·∥2)2 ρ2 ) + 4ε2 ρ2 ∥ ∥ ∥ϕ M k(Y T j· − ρM T j·)∥ ∥ ∥ 2 2 + 2(ε − 1) 2∥M T j·∥ 2 2 + 2∥ ∥ ∥[Ek]T j·∥ ∥ ∥ 2 2, where we used (a + b)2 ≤ 2a2 + 2b2. This completes the proof. G.5. HSVT based Matrix Estimation: Deterministic To High-Probability. Next, we convert the bound obtained in Lemma G.2 to a bound in expectation (as well as one in high- probability) for our metric of interest: ∥ ̂M − M ∥2,∞. In particular, we establish THEOREM G.3. For k ≥ 1, let M = M k +Ek with rank(M k) = k. Let ϵ = ∥Ek∥∞ and Γ = ∥M k∥∞. Let ρ ≥ C log(qp)/q for C large enough and q ≤ p. Then, the HSVT estimate ̂M with parameter k is such that E [ max j∈[q] 1 p ∥ ̂M T j· − M T j·∥2 2] ≤ p(Cσ2 + ρ2ϵq) ρ2σk(M k)2 (Γ2 + σ2 ρ2 ) + Cσ2k log p pρ2 + C(Γ + ϵ)2 p + 2ϵ 2 + C (pq)2 . PROOF. We start by identifying certain high probability events. Subsequently, using these events and Lemma G.2, we shall conclude the proof. High Probability Events. For some positive absolute constant C > 0, deﬁne E1 := { |̂ρ − ρ| ≤ ρ/20 } , E2 := { ∥Y − ρM ∥2 ≤ Cσ√p } ,(25) E3 := { ∥Y − ρM ∥∞,2, ∥Y − ρM ∥2,∞ ≤ Cσ√p } ,(26) E4 := { max j∈[q]∥ϕ B σk(B)(Y T j· − ρM T j·)∥ 2 2 ≤ Cσ2k log(p)} ,(27) E5 := {(1 − √ 20 log(qp) ρqp )ρ ≤ ̂ρ ≤ 1 1 − √ 20 log(qp) ρqp ρ } . 44 In (27) above, B ∈ Rq×p is a deterministic matrix. Let the singular value decomposition of B be given as B = ∑q i=1 σi(B)xiyT i , where σi(B) are the singular vectors of B in decreasing order and xi, yi are the left and right singular vectors respectively. Recall the deﬁnition of ϕB λ in (17). In particular, we choose λ = σk(B), the kth singular value of B in (27). As a result, in effect, we are bounding norm of projection of random vector Y j· − ρM j· for any given deterministic subspace of Rp of dimension k. LEMMA G.4. For some positive constant c1 > 0 and C > 0 large enough in deﬁnitions of E1, . . . , E5, P(E1) ≥ 1 − 2e −c1pqρ − (1 − ρ)pq, P(E2) ≥ 1 − 2e−p, P(E3) ≥ 1 − 2e−p,(28) P(E4) ≥ 1 − 2 (qp)10 . P(E5) ≥ 1 − 2 (qp)10 . PROOF. We bound the probability of events E1, . . . , E5 in that order. Bounding E1. Let ̂ρ0 = ( q∑ i=1 p∑ j=1 1(Yij is obs.))/(q p). That is, ̂ρ = max(̂ρ0, 1/(pq)) and E[̂ρ0] = ρ. We deﬁne the event E6 := {̂ρ0 = ̂ρ}. Thus, we have that P(Ec 1) = P(Ec 1 ∩ E6) + P(Ec 1 ∩ Ec 6) = P(|̂ρ0 − ρ| ≥ ρ/20) + P(Ec 1 ∩ Ec 6) ≤ P(|̂ρ0 − ρ| ≥ ρ/20) + P(Ec 6) = P(|̂ρ0 − ρ| ≥ ρ/20) + (1 − ρ)qp, where the ﬁnal equality follows by the independence of observations assumption and the fact that ̂ρ0 ̸= ̂ρ only if we do not have any observations. By Bernstein’s Inequality, we have that P(|̂ρ0 − ρ| ≥ ρ/20) ≥ 1 − 2e −c1ρqp. Bounding E2. To start with, E[Y ] = ρM . For any i ∈ [q], j ∈ [p], the Yij are independent, 0 with probability 1 − ρ and with probability ρ equal to Mij + εij with ∥εij∥ψ2 ≤ σ. Therefore, it follows that ∥Yij − ρMij∥ψ2 ≤ C′σ for a constant C′ > 0. Since q ≤ p, using Theorem F.2 it follows that for an appropriately large constant C > 0, P(E2) ≥ 1 − 2e −p. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 45 Bounding E3. Recall that we assume q ≤ p. Observe that for any matrix A ∈ Rq×p, ∥A∥∞,2, ∥A∥2,∞ ≤ ∥A∥2. Thus using the argument to bound E2, we have (28). Bounding E4. Consider for j ∈ [q], ∥ϕ B σk(B)(Y T j· − ρM T j·)∥ 2 2 = k∑ i=1∥yiyT i (Y T j· − ρM T j·)∥ 2 2 ≤ k∑ i=1 (yT i (Y T j· − ρM T j·) )2 2 = k∑ i=1 Z2 i , where Zi = yT i (Y T j· − ρM T j·). By deﬁnition of the ψ2 norm of a random variable and since yi is unit norm vector that is deterministic (and hence independent the of random vector Y T j· − pM T j·), it follows that ∥Zi∥ψ2 = ∥yT i (Y j· − pM j·)∥ψ2 ≤ ∥(Y j· − pM j·)∥ψ2. Since the coordinates of Y T j· − ρM T j· are mean-zero and independent, with ψ2 norm bounded by √Cσ for some absolute constant C > 0, using arguments from [3, 4], it follows that P( k∑ i=1 Z2 i > t) ≤ 2k exp ( − t kCσ2 ). Therefore, for choice of t = Cσ2k log p with large enough constant C > 0, q ≤ p, and taking a union bound over all j ∈ [p], we have that P(Ec 4) ≤ 2 (qp)10 . Bounding E5. Recall the deﬁnition of ̂ρ. By the binomial Chernoff bound, for ε > 1, P(̂ρ > ερ ) ≤ exp (− (ε − 1)2 ε + 1 qpρ ) , and P(̂ρ < 1 ε ρ ) ≤ exp (− (ε − 1)2 2ε2 qpρ ) . By the union bound, P( 1 ε ρ ≤ ̂ρ ≤ ρε) ≥ 1 − P(̂ρ > ερ ) − P(̂ρ < 1 ε ρ ). Noticing ε + 1 < 2ε < 2ε2 for all ε > 1, and substituting ε = (1 − √ 20 log(qp) qpρ )−1 completes the proof. The following are immediate corollaries of the above stated bounds. COROLLARY G.1. Let E := E1 ∩ E2. Then, for ρ ≥ C log(qp)/q, P(Ec) ≤ C1e −c2p, where C1 and c2 are positive constants. COROLLARY G.2. Let E := E2 ∩ E3 ∩ E4 ∩ E5. Then, P(Ec) ≤ C1 (qp)10 , where C1 is an absolute positive constant. 46 Probabilistic Bound for HSVT based Matrix Estimation. Recall ϵ = ∥Ek∥∞. Then ∥Ek∥2 F ≤ ϵqp. And ∥Ek∥2 2 ≤ ∥Ek∥2 F ≤ ϵqp. Let ρ ≥ C log(qp)/q for C large enough and recall q ≤ p. Further, recall Γ = ∥M k∥∞; thus, ∥M ∥∞ ≤ Γ + ϵ. Then ∥[M k]T j·∥2 ≤ Γ √p and ∥[M ]T j·∥2 ≤ (Γ + ϵ)√p. Deﬁne E = E1 ∩ E2 ∩ E3 ∩ E4 ∩ E5. Then, from Corollaries G.1 and G.2, we have that P(Ec) ≤ C1 (qp)10 for large enough constant C1 > 0. Under E5, we have ε = max(̂ρ/ρ, ρ/̂ρ) ≤ (1 − √ 20 log(qp) qpρ )−1. Under this choice of ε and using ρ ≥ C log(qp)/q, we have that for C large enough, ε ≤ C and (ε − 1)2 ≤ C/p. Given this setup, under event E, Lemma G.2 leads to the following: for all j ∈ [q] and with appropriately (re-deﬁned) large enough constant C > 0, ∥ ̂M T j· − M T j·∥2 2 ≤ C σ2p + ρ2ϵqp ρ2σk(M k)2 (pΓ 2 + σ2p ρ2 ) + Cσ2k log p ρ2 + C(Γ + ϵ) 2 + 2pϵ2.(29) That is, under event E, max j∈[q] 1 p ∥ ̂M T j· − M T j·∥2 2 ≤ C p(σ2 + ρ2ϵq) ρ2σk(M k)2 ( Γ2 + σ2 ρ2 ) + Cσ2k log p pρ2 + C(Γ + ϵ)2 p + 2ϵ 2.(30) For any random variable X and event A, such that under event A, X ≤ B and P(Ac) ≤ δ, we have E[X] = E[X1(A)] + E[X1(Ac)] ≤ E[X1(A)] + E[X 2] 1 2 P(Ac) 1 2 ≤ B + E[X 2] 1 2 δ 1 2 .(31) We shall use this reasoning above to bound E [ maxj∈[q] 1 p ∥ ̂M T j· − M T j·∥2 2]: let X = maxj∈[q] 1 p ∥ ̂M T j· − M T j·∥2 2 and A = E; B is given by right hand side of (30), δ = C1 (qp)10 ; the only missing quantity that remains to be bounded is E[X 2]. We do that next. To begin with, for any j ∈ [q], ∥ ̂M T j· − M T j·∥2 ≤ ∥ ̂M T j·∥2 + ∥M T j·∥2(32) by triangle inequality. As stated earlier, ∥[M ]T j·∥2 ≤ (Γ + ϵ) √p. Next, we bound ∥ ̂M j·∥T 2 . From (16), the fact that ̂ρ ≥ 1/(qp), and Lemma G.1, we have ∥ ̂M T j·∥2 = 1 ̂ρ ∥HSVTλk(Y )T j·∥2 ≤ q p∥φY λk(Y T j·)∥2 ≤ q p∥φY λk∥2∥Y T j·∥2 ≤ q p∥Y T j·∥2,(33) ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 47 where we used the fact that φY λk is a projection operator and hence ∥φY λk∥2 = 1. Note that Yij = Bij x (Mij + εij), where Bij is an independent Bernoulli variable with P(Bij = 1) = ρ representing whether (Mij + εij is observed or not. Therefore, |Yij| = |Bij| x |Mij + εij| ≤ (Γ + ϵ) + |εij|. Therefore, from (32) and (33), max j∈[q]∥ ̂M T j· − M T j·∥2 ≤ (Γ + ϵ) √p + qp ( max j∈[q] ∥Y T j·∥2) ≤ (Γ + ϵ) √p + qp x √p( max i∈[p],j∈[q] |Yij|) ≤ 2qp 3 2 ( Γ + ϵ + max i∈[p],j∈[q] |εij|) .(34) Using (a + b)2 ≤ 2a2 + 2b2 twice, we have (a + b)4 ≤ 8(a4 + b4). Therefore, from (34) max j∈[q]∥ ̂M T j· − M T j·∥ 4 2 ≤ 16q4p 6( (Γ + ϵ)4 + max i∈[p],j∈[q] |εij|4).(35) Recall E[εij] = 0, ∥εij∥ψ2 ≤ σ and εij are independent across i, j. A property of ψ2-random variables is that ∣ ∣ηij∣ ∣θ is a ψ2/θ-random variable for θ ≥ 1. With choice of θ = 4, we have E [ max ij |εij| 4] ≤ C′σ4 log2(qp),(36) for some C′ > 0 by Lemma F.3. From (33), (35), and (36), we have that (E [ max j∈[q] 1 p2 ∥ ̂M T j· − M T j·∥4 2]) 1 2 ≤ 4q2p2((Γ + ϵ) 4 + C′σ4 log2(qp)) 1 2 .(37) Finally, using (30), (31) and (37), we conclude E [ max j∈[q] 1 p ∥ ̂M T j· − M T j·∥2 2] ≤ p(Cσ2 + ρ2ϵq) ρ2σk(M k)2 (Γ2 + σ2 ρ2 ) + Cσ2k log p pρ2 + C(Γ + ϵ)2 p + 2ϵ 2 + C (pq)2 . This completes the proof of Theorem G.3. APPENDIX H: PROOF OF THEOREM 5.6 The proof of Theorem 5.6 will utilize Theorem G.3. To begin with, given N time series with observations over [T ], the mSSA algorithm as described in Section 1.1 constructs the L × (N T /L) stacked page matrix SP((X1, . . . , XN ), T, L) with L = √min(N, T )T , i.e. L ≤ T . As per the model described by (1) and Section 3, it follows that each entry of SP((X1, . . . , XN ), T, L) is an independent random variable; it is observed with probability ρ ∈ (0, 1] independently and when it is observed, its equal to value of the latent time series plus zero-mean sub-Gaussian noise. In particular, E [ SP((X1, . . . , XN ), T, L) ] = ρSP((f1, . . . , fN ), T, L), where SP((f1, . . . , fN ), T, L) ∈ RL×(N T /L) with entry in row ℓ ∈ [L] and column (n − 1) x T /L + j equal to fn(ℓ + (j − 1) x L). Further, when entry in row ℓ ∈ [L] and col- umn (n − 1) x T /L + j in SP((X1, . . . , XN ), T, L) is observed, i.e. Xn(ℓ + (j − 1) x L) ̸= ⋆, it is equal to fn(ℓ + (j − 1) x L) + ηn(ℓ + (j − 1) x L) where ηn(·) are independent, zero-mean sub-Gaussian variables with ∥ηn(·)∥ψ2 ≤ γ as per the Property 3. 48 Under Properties 1 and 7, as a direct implication of Proposition D.1, SP((f1, . . . , fN ), T, L) has ϵ′-rank at most R x G with ϵ′ = RΓ1ϵ. That is, there exist rank k ≤ R x G matrix M k ∈ RL×(N T /L) so that SP((f1, . . . , fN ), T, L) = M k + Ek, where ∥Ek∥∞ ≤ ϵ′. Due to Property 1, it follows that ∥M k∥∞ ≤ RΓ1Γ2 + ϵ′. Under Property 8, we have σk(M k) ≥ c √N T / √k for some constant c > 0. Deﬁne Γ = RΓ1Γ2 + ϵ ′ = RΓ1(Γ2 + ϵ). Recall from Section 1.1, the elements of the imputed multivariate time series are sim- ply the entries of the matrix ̂SP((X1, . . . , XN ), T, L) where ̂SP((X1, . . . , XN ), T, L) = 1 ̂ρ HSVTk(SP((X1, . . . , XN ), T, L)). That is, imputation in mSSA is carried out by applying HSVT to the stacked page matrix SP((X1, . . . , XN ), T, L). All in all, the above description precisely meets the setup of Theorem G.3. To apply Theorem G.3, we require ρ ≥ C log(N T )/ √N T for C > 0 large enough. Note that the number of columns in ̂SP((X1, . . . , XN ), T, L) is equal to N T /L for L = √min(N, T )T – for this choice of L, note that N T /L ≥ L. Using σ2 k(M k) ≥ cN T /k, for some absolute constant = c ≥ 0, and using Theorem G.3, we obtain E [ 1 (N T /L) ∥̂SP((X1, . . . , XN ), T, L) − SP((f1, . . . , fN ), T, L)∥2 2,∞] (38) ≤ k(N T /L)(Cγ2 + ρ2ϵ′L) ρ2c2N T (Γ 2 + γ2 ρ2 ) + Cγ2k log N T (N T /L)ρ2 + C(Γ + ϵ′)2 (N T /L) + 2(ϵ ′) 2 + C (N T )2 Recall that k ≤ R x G, ϵ′ = RΓ1ϵ, and Γ = RΓ1(Γ2 + ϵ). Hence, simplifying (38), we obtain that E [ 1 (N T /L) ∥̂SP((X1, . . . , XN ), T, L) − SP((f1, . . . , fN ), T, L)∥2 2,∞] ≤ ˜C( RG(1 + ρ2RϵL) ρ2L (R2(1 + ϵ 2) + 1 ρ2 ) + RG log N T (N T /L)ρ2 + (R(1 + ϵ))2 (N T /L) + (Rϵ) 2) ≤ ˜C( R3G log N T ρ4L + R4G(ϵ + ϵ2 + ϵ3) ρ2 ),(39) where ˜C = C(c, Γ1, Γ2, γ) is a positive constant dependent on model parameters including Γ1, Γ2, γ. It can be easily veriﬁed that for any matrix, A ∈ Rm×n, 1 mn ∥A∥ 2 F ≤ 1 n ∥A∥2 ∞,2.(40) Further, there is a one-to-one mapping of ˆfn(·) (resp. fn(·)) to the entries of ̂SP((X1, . . . , XN ), T, L) (resp. SP((f1, . . . , fN ), T, L)). Hence, ImpErr(N, T ) = E [ 1 N T ∥̂SP((X1, . . . , XN ), T, L) − SP((f1, . . . , fN ), T, L)∥2 F ] (41) ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 49 Therefore, from (39), (40), and (41) it follows that ImpErr(N, T ) ≤ C(c, Γ1, Γ2, γ) ( R3G log N T ρ4L + R4G(ϵ + ϵ2 + ϵ3) ρ2 ) This completes the proof of Theorem 5.6. APPENDIX I: PROOF OF THEOREM 5.7 The forecasting algorithm, as described in Section 1.1, computes a linear model between the recent past and immediate future to forecast. We shall bound the forecasting error, ForErr(N, T, L) as deﬁned in (6). We start with some setup and notations, followed by a key proposition that establishes the existence of a linear model under the setup of Theorem 5.7, and then conclude with a detailed analysis of noisy, mis-speciﬁed least-squares. Setup, Notations. For L ≥ 1, k ≥ 1, for ease of notations, we deﬁne ◦ SP(X) = SP((X1, . . . , XN ), T, L) ∈ RL×(N T /L), ◦ SP(f ) = SP((f1, . . . , fN ), T, L) ∈ RL×(N T /L), ◦ SP ′(X) ∈ R(L−1)×(N T /L) as the top L − 1 rows of SP((X1, . . . , XN ), T, L), ◦ SP ′(f ) ∈ R(L−1)×(N T /L) as the top L − 1 rows of SP((f1, . . . , fN ), T, L). It is worth noting that E[SP(X)] = ρSP(f ) and hence SPL·(X) T = ρSPL·(f ) T + η,(42) where η ∈ R(N T )/L is a random vector with each component being independent, zero-mean with its distribution given as: it is 0 with probability 1 − ρ and with probability ρ, due to Property 3, it equals a zero-mean sub-Gaussian random variable with ∥ · ∥ψ2 ≤ γ. Therefore, using arguments in [3, 4], each component of η is an independent, zero-mean random variable with ∥ · ∥ψ2 bounded above by C′(γ2 + RΓ1Γ2) for some absolute constant C′ > 0. Let K = C′(γ2 + RΓ1Γ2) and hence each component of η has ∥ · ∥ψ2 bounded by K. Now, recall that for forecasting, we ﬁrst apply the imputation algorithm (i.e. HSVT) to SP((X1, . . . , XN ), T, L) by replacing ⋆s, i.e. missing observations by 0 as well as setting all the entries in the last row equal to 0. Equivalently, the imputation algorithm is applied to SP ′(X) after setting all missing values to 0. Let ̂SP ′ ∈ RL−1×(N T /L) be the estimate pro- duced from the imputation algorithm applied to SP ′(X). Under the setup of Theorem 5.6, by following arguments identical to that of Theorems G.3 and 5.6–in particular, refer to (39)–it follows that by selecting the right choice of k ≤ R x G, we have E [ 1 (N T /L) ∥ ̂SP ′ − SP′(f )∥ 2 2,∞] ≤ ˜C( R3G log N T ρ4L + R4G(ϵ + ϵ2 + ϵ3) ρ2 ),(43) where ˜C = C(c, Γ1, Γ2, γ) > 0 is a constant dependent on c, Γ1, Γ2, γ. Now, the mSSA forecasting algorithm ﬁnds ̂β = ̂β((X1, . . . , XN ), T L; k), by solving the following Ordinary Least Squares (OLS): ̂β ∈ minimize ∥ 1 ̂ρ SP(X)L· − ̂SP ′T β∥2 2 over β ∈ RL−1.(44) And subsequently, ̂SP ′T ̂β is used as the estimate for SP(f )L· ∈ RN T /L, the Lth row of the latent SP(f ). The goal is to bound the forecasting error ForErr(N, T, L), which is given by ForErr(N, T, L) = E [ 1 (N T /L) ∥SP(f )L· − ̂SP ′T ̂β∥2 2]. 50 Therefore, our interest is in bounding E [ ∥SPL·(f ) − ̂SP ′T ̂β∥2 2]. Now, we recall from Proposition 5.6 that there exists β∗ ∈ RL−1, such that ∥SP(f ) T L· − SP ′(f ) T β∗∥∞ ≤ C2ϵ, where C2 := RΓ1(1 + ∥β∗∥1). Bounding E [∥SPL·(f ) − ̂SP′T ̂β∥2 2]. By (44) and (42) ∥ 1 ̂ρ SP(X)L· − ̂SP′T ̂β∥2 2 ≤ ∥ 1 ̂ρ SP(X)L· − ̂SP ′T β∗∥ 2 2 = ∥ ρ ̂ρ SP(f )L· + η − ̂SP ′T β∗∥2 2 = ∥ ρ ̂ρ SP(f )L· − ̂SP′T β∗∥ 2 2 + ∥η∥ 2 2 + 2ηT ( ρ ̂ρ SP(f )L· − ̂SP ′T β∗).(45) Also, ∥ 1 ̂ρ SP(X)L· − ̂SP ′T ̂β∥ 2 2 = ∥ ρ ̂ρ SP(f )L· + η − ̂SP ′T ̂β∥2 2 = ∥ ρ ̂ρ SP(f )L· − ̂SP ′T ̂β∥ 2 2 + ∥η∥ 2 2 + 2ηT ( ρ ̂ρ SP(f )L· − ̂SP ′T ̂β).(46) From (45) and (46) E [ ∥ ρ ̂ρ SP(f )L· − ̂SP ′T ̂β∥ 2 2] (47) ≤ E [ ∥ ρ ̂ρ SP(f )L· − ̂SP ′T β∗∥2 2] + 2E [ ηT ̂SP ′T (β∗ − ̂β)] η is independent of ̂SP ′, β∗, and ̂ρ; E[η] = 0; thus, we have that E [ηT ̂SP ′T β∗] = 0.(48) By (44), we have ̂β = ̂SP ′T,† 1 ̂ρ SP(X)L·, where ̂SP ′T,† is pseudo-inverse of ̂SP′T . That is, ̂β = ̂SP ′T,† ρ ̂ρ SP(f )L· + 1 ̂ρ ̂SP ′T,†η.(49) Using cyclic and linearity of Trace operator; the independence properties of η; and (49); we have E[ηT ̂SP ′T ̂β] = E[ηT ̂SP′T ̂SP′T,† ρ ̂ρ SP(f )L·] + E[ 1 ̂ρ ηT ̂SP′T ̂SP′T,†η] = E[η] T E[ ̂SP ′T ̂SP ′T,† ρ ̂ρ ]SP(f )L· + E[ 1 ̂ρ Tr(ηT ̂SP′T ̂SP′T,†η)] = E[ 1 ̂ρ Tr( ̂SP′T ̂SP′T,†ηηT )] = Tr(E[ 1 ̂ρ ̂SP ′T ̂SP ′T,†]E[ηηT ]) ≤ C(γ)k/ρ,(50) ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 51 where C(γ) is a function only of γ. To see the last inequality, we use various facts. First, by the deﬁnition of the HSVT algorithm ̂SP ′T has rank at most k. Second, let ̂SP ′T = U SV T be the singular value decomposition of ̂SP ′T , we have ̂SP′T ̂SP′T,† = U SV T V S†U T = U ˜IU T , That is, 1 ̂ρ ̂SP ′T ̂SP ′T,† is a positive semi-deﬁnite matrix and Tr( 1 ̂ρ ̂SP′T ̂SP′T,†) ≤ k/̂ρ. The ma- trix E[ηηT ] is diagonal with all the non-zero entries on diagonal (variance of components of η) bounded above by a constant that depends on γ. For a positive semi-deﬁnite matrix A and positive semi-deﬁnite diagonal matrix B, Tr(AB) ≤ ∥B∥2Tr(A). For ρ ≥ C log(N T )/√N T for large enough C, one can verﬁy that E[1/̂ρ] ≤ 2/ρ. This completes the justiﬁcation of the last step of (50). Now consider the term ∥ ρ ̂ρ SP(f )L· − ̂SP′T β∗∥2 2. Note, ∥ ρ ̂ρ SP(f )L· − ̂SP ′T β∗∥2 2 =∥ (SP(f )L· − ̂SP′T β∗) + ( ρ − ̂ρ ̂ρ )SP(f )L·∥2 2 ≤ 2∥ (SP(f )L· − ̂SP ′T β∗) ∥2 2 + 2∥ ρ − ̂ρ ̂ρ SP(f )L·∥2 2.(51) We will bound the two terms on the r.h.s of (51) separately. We now consider the ﬁrst term. ∥SP(f )L· − ̂SP′T β∗∥ 2 2 ≤ 2∥SP(f )L· − SP ′(f ) T β∗∥2 2 + 2∥SP ′(f ) T β∗ − ̂SP′T β∗∥ 2 2.(52) By Proposition 5.6 ∥SP(f )L· − SP ′(f ) T β∗∥2 ≤ ∥SP(f )L· − SP ′(f )T β∗∥∞√N T /L ≤ C2ϵ √N T /L,(53) where we used the fact that for any v ∈ Rp, ∥v∥2 ≤ ∥v∥∞√ p. And, ∥SP ′(f ) T β∗ − ̂SP′T β∗∥2 = ∥(SP ′(f ) − ̂SP′) T β∗∥2 ≤ ∥SP′(f ) − ̂SP ′∥2,∞∥β∗∥1,(54) where we used the fact that for any A ∈ Rq×p, v ∈ Rp, ∥Av∥2 ≤ ∥AT ∥2,∞∥v∥1. Finally, note that ∥SP(f )L· − ̂SP ′T ̂β∥ 2 2 ≤ 2∥ ρ ̂ρ SP(f )L· − ̂SP′T ̂β∥2 2 + 2∥ ρ − ̂ρ ̂ρ SP(f )L·∥2 2.(55) Using (47), (48), (50), (51),(52), (53), (54), and the bound in (55), we obtain E [ ∥SP(f )L· − ̂SP ′T ̂β∥2 2] (56) ≤ 4C(γ)k/ρ + 6E [ ∥ ρ − ̂ρ ̂ρ SP(f )L·∥2 2] + 2C2ϵ2(N T /L) + 2∥β∗∥2 1∥SP′(f ) − ̂SP ′∥2 2,∞. Note that ∥SP(f )∥∞ ≤ RΓ1Γ2. Hence, ∥SP(f )L·∥2 2 ≤ C(Γ1, Γ2)R2(N T /L), for large enough constant C(Γ1, Γ2) that may depend on Γ1, Γ2. Using the bounds derived in Lemma G.4, one can verify that E[( ρ−̂ρ ̂ρ )2] ≤ C/(N T /L) for large enough positive constant C. Therefore, we have that 6E [ ∥ ρ − ̂ρ ̂ρ SP(f )L·∥ 2 2] ≤ C(Γ1, Γ2)R2(57) 52 Using (43), (57), and the bound in (56); diving by 1/(N T /L) on both sides; and noting k ≤ R x G, we obtain E [ 1 (N T /L) ∥SP(f )L· − ̂SP′T ̂β∥2 2] ≤ C(c, γ, Γ1, Γ2) ( RG ρ(N T /L) + R2 (N T /L) + R(1 + ∥β∗∥1)ϵ 2 + ∥β∗∥ 2 1 ( R3G log N T ρ4L + R4G(ϵ + ϵ2 + ϵ3) ρ2 )) ≤ C(c, γ, Γ1, Γ2) (max(1, ∥β∗∥1, ∥β∗∥ 2 1) ( R3G log N T ρ4L + R4G(ϵ + ϵ2 + ϵ3) ρ2 )) (58) Letting L = √min(N, T )T , using (58), and noting that ForErr(N, T, L) = E [ 1 (N T /L) ∥SP(f )L· − ̂SP ′T ̂β∥2 2] completes the proof of Theorem 5.7. I.1. Proof of Proposition 5.6. For this proof, we utilize a modiﬁed version of the stacked Hankel matrix deﬁned in Appendix D. Deﬁne the modiﬁed Hankel matrix for time series fn, for n ∈ [N ], as ̃H(n) ∈ RT ×2T , where for i ∈ [T ], j ∈ [2T ], we have ̃H(n)ij = fn(i + j − 1 − T ). Deﬁne ̃SH ∈ RT ×N T as the column wise concatenation of the matrices ̃H(n) for n ∈ [N ], i.e., ̃SH := [̃H(1), . . . , ̃H(N )]. By a straightforward modiﬁcation of the proof of Proposition D.1, we have ̃SH has ϵ′-rank bounded by R x G with ϵ′ = RΓ1ϵ. That is, there exists a matrix M ∈ RT ×N T such that, rank(M) ≤ RG, ∥ ̃SH − M∥∞ ≤ ϵ ′ Since rank(M) ≤ RG, it must be the case that within the last RG rows of M, there exists at least one row, which we denote as r∗, that can be written as a linear combination of at most RG rows above it, which we denote as r1, . . . , rRG. Speciﬁcally there exists a vector θ := (θ1, . . . , θRG) ∈ RRG such that Mr∗,· = RG∑ ℓ=1 θℓMrℓ,· Hence for j ∈ [2T ], ∣ ∣ ∣ ∣ ̃SHr∗,j − RG∑ ℓ=1 θℓ ̃SHrℓ,j ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ̃SHr∗,j ± Mr∗,j − RG∑ ℓ=1 θℓ ̃SHrℓ,j ± RG∑ ℓ=1 θℓMrℓ,t ∣ ∣ ∣ ∣ ≤ ∣ ∣ ∣ ∣ ̃SHr∗,j − Mr∗,j ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ RG∑ ℓ=1 θℓ ̃SHrℓ,j − RG∑ ℓ=1 θℓMrℓ,t ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣Mr∗,j − RG∑ ℓ=1 θℓMrℓ,t ∣ ∣ ∣ ∣ ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 53 = ∣ ∣ ∣ ∣ ̃SHr∗,j − Mr∗,j ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ RG∑ ℓ=1 θℓ( ̃SHrℓ,j − Mrℓ,t)∣ ∣ ∣ ∣ ≤ ϵ ′ + ∥θ∥1∥ ̃SHrℓ,j − Mrℓ,t∥∞ ≤ RΓ1(1 + ∥θ∥1)ϵ.(59) Observe that every entry of SP(f )L· appears within ̃SHr∗,·; this can be seen by noting that ̃SH is skew-symmetric and thus every entry in the last row of ̃SH appears along the appropriate diagonal. Using this skew-symmetric property of ̃SH and (59), it implies that by appropriately selecting entries in ̃SH, there exists β∗ ∈ RL−1, ∥SP(f )T L· − SP′(f )T β∗∥∞ ≤ RΓ1(1 + ∥β∥1)ϵ, where the non-zero entries in β∗ correspond to the entries of θ. Noting that θ ∈ RRG implies ∥β∗∥0 ≤ RG. This completes the proof. APPENDIX J: PROOF OF THEOREM 4.3 Notation. For integers t1 < t2 where t2 − t1 + 1 ≥ L, let SP((X1, . . . , XN ), t1 : t2, L) represents the stacked page matrix constructed using the contiguous observations Xn(t1), . . . , Xn(t2), ∀n ∈ [N ]. Throughout, we use the following notations: • SP0(X) = SP((X1, . . . , XN ), 1 : T, L) ∈ RL×(N T /L), with zeros replacing missing values. • SP1(X) = SP((X1, . . . , XN ), T + 1 : T + T1, L) ∈ RL×(N T1/L), with zeros replacing missing values. • SP0(f ) = SP((f1, . . . , fN ), 1 : T, L) ∈ RL×(N T /L). • SP1(f ) = SP((f1, . . . , fN ), T + 1 : T + T1, L) ∈ RL×(N T1/L). • SP1(η) = SP((η1, . . . , ηN ), T + 1 : T + T1, L) ∈ RL×(N T1/L). • SP′ 0(X) ∈ R(L−1)×(N T /L) as the top L − 1 rows of SP0(X). Let SP′ 1(X), SP ′ 0(f ), SP ′ 1(f ) and SP′ 1(η) be deﬁned analogously. • ̂ρ := (max(1, ∑L−1 i=1 ∑N T /L j=1 1(SP0(X)ij ̸= ⋆)))/(N T − N T /L) Recall that we are interested in bounding the following out-of-sample prediction error: TestForErr(N, T, T1, L) = L N T1 N∑ n=1 T1/L∑ m′=1 E [ (fn(T + L x m′) − ¯fn(T + L x m ′)) 2] . Where the forecasted estimate ¯fn(·), n ∈ [N ] are produced by the algorithm detailed in Section 1.1. Based on the algorithm, we can write TestForErr(N, T, T1, L) as follows: TestForErr(N, T, T1, L) = 1 (N T1/L) E [∥ 1 ̂ρ SP ′ 1(X) T ̂β − SP1(f )T L·∥2 2] = 1 (N T1/L) E [∥ 1 ̂ρ SP ′ 1(X) T ̂β − SP ′ 1(f )T β∗∥ 2 2]. Before bounding this term, we introduce the following important notation. For i ∈ {0, 1}, let U iΣiV T i denote the Singular Value Decomposition (SVD) of SP′ i(f ). Also, let ̃U i ̃Σi ̃V T i denote the top k singular components of the SVD of SP ′ i(X), while ̃U ⊥ i ̃Σ ⊥ i ( ̃V ⊥ i )T denote the remaining L − k − 1 components such that SP ′ i(X) = ̃U i ̃Σi ̃V T i + ̃U ⊥ i ̃Σ ⊥ i ( ̃V ⊥ i )T . Finally, 54 let V ⊥ i and U ⊥ i be matrices of orthornormal basis vectors that span the null space of SP ′ i(f ) and SP ′ i(f )T , respectively. Further, let ̂SP ′ i be the HSVT estimate of SP ′ i(f ). That is ̂SP ′ i = 1 ̂ρ ̃U i ̃Σi ̃V T i . Also, let ̂SP′ i⊥ = 1 ̂ρ ̃U ⊥ i ̃Σ ⊥ i ( ̃V ⊥ i )T . We start the proof by providing a deterministic upper bound for out-of-sample error. Deterministic Bound. Due to triangle inequality, we have ∥ 1 ̂ρ SP ′ 1(X) T ̂β − SP ′ 1(f )T β∗∥ 2 2 = ∥ 1 ̂ρ SP ′ 1(X) T ̂β − SP ′ 1(f )T β∗ + ̂SP ′ 1T ̂β − ̂SP′ 1T ̂β∥ 2 2 ≤ 2∥ 1 ̂ρ SP′ 1(X)T ̂β − ̂SP ′ 1T ̂β∥ 2 2 + 2∥̂SP ′ 1T ̂β − SP ′ 1(f ) T β∗∥2 2. Next, we proceed to bound each of the two terms on the right hand side. First term: ∥ 1 ̂ρ SP ′ 1(X) T ̂β − ̂SP′ 1T ̂β∥2 2. ∥ 1 ̂ρ SP ′ 1(X) T ̂β − ̂SP ′ 1T ̂β∥2 2 = ∥(̂SP′ 1⊥) T ̂β∥ 2 2(60) = ∥ 1 ̂ρ ̃V ⊥ 1 ̃Σ ⊥ 1 ( ̃U ⊥ 1 )T ̂β∥2 2 ≤ ∥ 1 ̂ρ ̃Σ ⊥ 1 ∥2 2∥( ̃U ⊥ 1 )T ̂β∥2 2. Note that ∥ ̃Σ ⊥ 1 ∥2 equals the (k + 1)-th singular value of SP′ 1(X). Recall that E[SP ′ 1(X)] = ρSP′ 1(f ) and hence SP ′ 1(X) = ρSP ′ 1(f ) + ζ1,(61) where ζ1 ∈ R(L−1)×(N T1)/L is a random matrix with zero-mean i.i.d. entries where each entry is 0 with probability 1 − ρ and equals a zero-mean sub-Gaussian random variable with ∥ · ∥ψ2 ≤ γ with probability ρ (due to Property 3). Next, we show that each component of ζ1 is an independent, zero-mean random variable with ∥ · ∥ψ2 bounded above by C′(γ + RΓ1Γ2) for some absolute constant C′ > 0. Let ζij for i ∈ [L − 1] and j ∈ [N T /L] denotes the ij-th entry in ζ1. Further, let Pij ∈ {0, 1} denotes the random mask which takes the value 1 with probability ρ such that SP ′ 1(X)ij = Pij(SP ′ 1(f )ij + SP′ 1(η)ij). Then, we have ∥ζij∥ψ2 = ∥SP ′ 1(X)ij − ρSP′ 1(f )ij∥ψ2 = ∥PijSP′ 1(f )ij + PijSP′ 1(η)ij − ρSP ′ 1(f )ij∥ψ2 ≤ ∥PijSP ′ 1(η)ij∥ψ2 + ∥PijSP ′ 1(f )ij − ρSP ′ 1(f )ij∥ψ2 ≤ Cγ + SP ′ 1(f )ij∥Pij − ρ∥ψ2 ≤ C′(γ + RΓ1Γ2), where C, C′ > 0 are absolute constants. The ﬁrst inequality is due to triangle inequality, and the last follows since Pij − ρ is a random variable bounded between [−ρ, 1 − ρ] and SP ′ 1(f )ij is bounded by RΓ1Γ2. With a similar argument, we can also write SP ′ 0(X) = ρSP ′ 0(f ) + ζ0, ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 55 where each component of ζ0 is again an independent, zero-mean random variable with ∥ · ∥ψ2 bounded above by C′(γ + RΓ1Γ2). Now, recalling that SP ′ 1(X) = ρSP ′ 1(f ) + ζ1 and using Weyl’s inequality (see Lemma F.6), we can bound the (k + 1)-th singular value of SP ′ 1(X) by the largest singular value of ζ1. That is, ∥ ̃Σ ⊥ 1 ∥ 2 2 ≤ ∥ζ1∥2 2.(62) Next, we bound the term ∥( ̃U ⊥ 1 )T ̂β∥2 2. ∥( ̃U ⊥ 1 ) T ̂β∥ 2 2 = ∥ ̃U ⊥ 1 ( ̃U ⊥ 1 )T ̂β ∥2 2(63) = ∥ ̃U ⊥ 1 ( ̃U ⊥ 1 )T β∗ + ̃U ⊥ 1 ( ̃U ⊥ 1 ) T ( ̂β − β∗)∥2 2 ≤ 2∥ ̃U ⊥ 1 ( ̃U ⊥ 1 ) T β∗∥2 2 + 2∥ ̃U ⊥ 1 ( ̃U ⊥ 1 )T ( ̂β − β∗)∥ 2 2 ≤ 2∥ ̃U ⊥ 1 ( ̃U ⊥ 1 ) T β∗∥2 2 + 2∥ ̂β − β∗∥2 2. First, consider ∥ ̃U ⊥ 1 ( ̃U ⊥ 1 )T β∗∥2 = ∥ ̃U ⊥ 1 ( ̃U ⊥ 1 )T U 1(U 1) T β∗∥2 (64) ≤ ∥ ∥ ∥U ⊥ 1 (U ⊥ 1 ) T U 1(U 1) T β∗∥ ∥ ∥ 2 + ∥ ∥ ∥ ( ̃U ⊥ 1 ( ̃U ⊥ 1 ) T U 1(U 1) T − U ⊥ 1 (U ⊥ 1 ) T U 1(U 1) T ) β∗∥ ∥ ∥ 2 ≤ ∥ ∥ ∥ ( ̃U ⊥ 1 ( ̃U ⊥ 1 ) T − U ⊥ 1 (U ⊥ 1 )T ) β∗∥ ∥ ∥ 2 ≤ ∥ ∥ ∥ ̃U ⊥ 1 ( ̃U ⊥ 1 ) T − U ⊥ 1 (U ⊥ 1 ) T ∥ ∥ ∥2 ∥β∗∥2 = ∥ ∥ ∥ ̃U 1 ̃U T 1 − U 1U T 1 ∥ ∥ ∥ 2 ∥β∗∥2 . Where in the ﬁrst equality we use the fact that β∗ = U 1(U 1)T β∗, i.e., β∗ lives in the column space of SP′ 1(f ) (Property 6). Next, by Wedin sin Θ Theorem (see [13, 39]) we bound ∥ ∥ ∥ ̃U 1 ̃U T 1 − U 1U T 1 ∥ ∥ ∥ 2 as follows: ∥ ∥ ∥ ̃U 1 ̃U T 1 − U 1U T 1 ∥ ∥ ∥ 2 ∥β∗∥2 ≤ ∥SP ′ 1(X) − ρSP′ 1(f )∥2 σk(ρSP ′ 1(f )) ∥β∗∥2 = ∥ζ1∥2 σk(ρSP ′ 1(f )) ∥β∗∥2 .(65) For ∥ ̂β − β∗∥2, we have: ∥ ̂β − β∗∥2 2 = ∥ ̃U ⊥ 0 ( ̃U ⊥ 0 ) T ( ̂β − β∗) + ̃U 0( ̃U 0) T ( ̂β − β∗)∥2 2 = ∥ ̃U ⊥ 0 ( ̃U ⊥ 0 ) T ( ̂β − β∗)∥2 2 + ∥ ̃U 0( ̃U 0) T ( ̂β − β∗)∥ 2 2 = ∥ ̃U ⊥ 0 ( ̃U ⊥ 0 ) T ( ̂β − β∗)∥2 2 + ∥ ̃U T 0 ( ̂β − β∗)∥ 2 2 = ∥ ̃U ⊥ 0 ( ̃U ⊥ 0 ) T (β∗)∥ 2 2 + ∥ ̃U T 0 ( ̂β − β∗)∥2 2.(66) Note that the last equality follow from the fact that ̂β = ̂SP′ 0T,† 1 ̂ρ SP0(X)L· = ̃U 0( ̃Σ0)† ̃V T SP0(X)L·, where ̂SP ′ 0T,† is the pseudoinverse of ̂SP ′ 0T , and thus ( ̃U ⊥ 0 )T ̂β = 0. 56 The ﬁrst term in (66) can be bounded using the same argument in (64) and (65), where we utilize the fact that β∗ = U 0(U 0)T β∗ and Wedin sin Θ Theorem to get ∥ ̃U ⊥ 0 ( ̃U ⊥ 0 ) T β∗∥2 ≤ ∥ζ0∥2 σk(ρSP ′ 0(f )) ∥β∗∥2 .(67) What is left is bounding ∥ ̃U T 0 ( ̂β − β∗)∥2 2. To that end, ﬁrst consider ∥̂SP ′ 0T ( ̂β − β∗)∥2 2 ≤ 2∥̂SP′ 0T ̂β − SP ′ 0(f ) T β∗∥2 2 + 2∥SP ′ 0(f ) T β∗ − ̂SP ′ 0T β∗∥2 2 ≤ 2∥̂SP ′ 0T ̂β − SP ′ 0(f ) T β∗∥2 2 + 2∥SP ′ 0(f ) − ̂SP′ 0∥2 2,∞∥β∗∥ 2 1.(68) Also, consider ∥̂SP′ 0T ( ̂β − β∗)∥2 2 = ( ̂β − β∗) T 1 ̂ρ2 ̃U 0 ̃Σ 2 0 ̃U T ( ̂β − β∗) ≥ σk(̂SP′ 0) 2∥ ̃U T 0 ( ̂β − β∗)∥ 2 2.(69) From (69) and (68) we get, ∥ ̃U T 0 ( ̂β − β∗)∥ 2 2 ≤ 2 σk(̂SP′ 0)2 (∥̂SP′ 0T ̂β − SP ′ 0(f ) T β∗∥2 2 + ∥SP ′ 0(f ) − ̂SP ′ 0∥ 2 2,∞∥β∗∥2 1).(70) Note that, similar to argument in (61), SP0(X)L· = ρSP0(f )L· + ζ L 0 , where ζ L 0 is a vector of i.i.d. entries with ∥ · ∥ψ2 ≤ C′(γ + RΓ1Γ2). Then the term ∥̂SP ′ 0T ̂β − SP′ 0(f )T β∗∥2 2 can be bounded as follows ∥̂SP ′ 0T ̂β − 1 ρ SP0(X)L·∥ 2 2 =∥̂SP ′ 0T ̂β − SP0(f )L· − 1 ρ ζ L 0 ∥2 2 =∥̂SP ′ 0T ̂β − SP ′ 0(f ) T β∗∥2 2 + ∥ 1 ρ ζ L 0 ∥ 2 2 − 2 ρ (̂SP ′ 0T ̂β − SP ′ 0(f )T β∗) T ζ L 0 .(71) Also, we have ∥̂SP ′ 0T ̂β − 1 ρ SP0(X)L·∥2 2 ≤∥̂SP ′ 0T β∗ − 1 ρ SP0(X)L·∥ 2 2 =∥(̂SP ′ 0T − SP′ 0(f ) T )β∗ − 1 ρ ζ L 0 ∥ 2 2 =∥(̂SP ′ 0T − SP′ 0(f ) T )β∗∥ 2 2 + ∥ 1 ρ ζ L 0 ∥2 2 − 2 ρ ((̂SP ′ 0T − SP′ 0(f ) T )β∗)T ζ L 0 .(72) From (71) and (72) we have, ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 57 ∥̂SP ′ 0T ̂β − SP′ 0(f ) T β∗∥2 2 ≤ ∥(̂SP ′ 0T − SP′ 0(f ) T )β∗∥ 2 2 + 2 ρ ((̂SP′ 0T )( ̂β − β∗))T ζ L 0(73) ≤ ∥̂SP ′ 0 − SP ′ 0(f )∥ 2 2,∞∥β∗∥2 1 + 2 ρ ((̂SP ′ 0T )( ̂β − β∗))T ζ L 0 . Finally, from (70) and (73) we get ∥ ̃U T 0 ( ̂β − β∗)∥2 2 ≤ 4 σk(̂SP ′ 0)2 ( ∥SP′ 0(f ) − ̂SP′ 0∥ 2 2,∞∥β∗∥2 1 + 1 ρ ((̂SP′ 0T )( ̂β − β∗))T ζ L 0 ) . (74) From (66), (67), and (74) we have ∥ ̂β − β∗∥2 2 ≤ ∥ζ0∥2 2 σk(ρSP ′ 0(f ))2 ∥β∗∥ 2 2(75) + 4 σk(̂SP′ 0)2 ( ∥SP ′ 0(f ) − ̂SP ′ 0∥2 2,∞∥β∗∥2 1 + 1 ρ ((̂SP ′ 0T )( ̂β − β∗) )T ζ L 0 ) . For ease of exposition, let ∆1 := ∥SP′ 0(f ) − ̂SP ′ 0∥ 2 2,∞∥β∗∥2 1 + 1 ρ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ∆2 := ∥ζ0∥2 2 σk(ρSP′ 0(f ))2 ∥β∗∥2 2 + 4 σk(̂SP ′ 0)2 (∆1).(76) Using this deﬁnition, (60), (62), (63), (65), and (75), we have ∥ 1 ̂ρ SP′ 1(X)T ̂β − ̂SP ′ 1T ̂β∥ 2 2 ≤ ∥ 1 ̂ρ ζ1∥ 2 2 ( 2∥ζ1∥2 2 ∥β∗∥ 2 2 σk(ρSP ′ 1(f ))2 + 2∆2 ) .(77) Second term: ∥SP ′ 1(f ) T β∗ − ̂SP ′ 1T ̂β∥2 2. To bound the second term, we follow a similar proof to that shown in [2]. ∥SP′ 1(f ) T β∗ − ̂SP′ 1T ̂β∥ 2 2 = ∥SP′ 1(f )T β∗ + ̂SP ′ 1T β∗ − ̂SP ′ 1T β∗ − ̂SP ′ 1T ̂β∥2 2(78) ≤ 2∥(SP ′ 1(f ) − ̂SP ′ 1) T β∗∥2 2 + 2∥̂SP′ 1T (β∗ − ̂β)∥2 2. Next, we bound the two terms on the right hand side. First, we bound ∥(SP′ 1(f ) − ̂SP ′ 1)T β∗∥2 2 as follows. ∥(SP ′ 1(f ) − ̂SP ′ 1) T β∗∥2 2 ≤ ∥SP ′ 1(f ) − ̂SP ′ 1∥2 2,∞∥β∗∥ 2 1.(79) Next, we bound the second term ∥̂SP ′ 1T (β∗ − ̂β)∥2 2. 58 ∥̂SP ′ 1T (β∗ − ̂β)∥ 2 2 ≤ 1 ̂ρ2 ∥( ̃V 1 ̃Σ1 ̃U T 1 + ρSP′ 1(f ) T − ρSP ′ 1(f )T )(β∗ − ̂β)∥2 2 ≤ 2 ̂ρ2 ∥( ̃V 1 ̃Σ1 ̃U T 1 − ρSP′ 1(f ) T )(β∗ − ̂β)∥ 2 2 + 2ρ2 ̂ρ2 ∥SP ′ 1(f ) T (β∗ − ̂β)∥ 2 2 ≤ 2 ̂ρ2 ∥ ̃V 1 ̃Σ1 ̃U T 1 − ρSP ′ 1(f )T ∥2 2∥(β∗ − ̂β)∥ 2 2 + 2ρ2 ̂ρ2 ∥SP′ 1(f )T (β∗ − ̂β)∥2 2. Further, note that ∥ ̃V 1 ̃Σ1 ̃U T 1 − ρSP′ 1(f )T ∥2 2 ≤ 2∥ ̃V 1 ̃Σ1 ̃U T 1 − SP ′ 1(X) T ∥ 2 2 + 2∥SP ′ 1(X) T − ρSP ′ 1(f )T ∥2 2 ≤ 4∥SP ′ 1(X) T − ρSP ′ 1(f )T ∥2 2 = 4∥ζ1∥2 2. Where the last inequality follows from the fact that ∥ ̃V 1 ̃Σ1 ̃U T 1 − SP ′ 1(X)T ∥2 is the k + 1-th singular value of SP ′ 1(X) and hence is bounded by ∥SP ′ 1(X) T − ρSP ′ 1(f )T ∥2 using Weyl’s inequality. Therefore, ∥̂SP′ 1T (β∗ − ̂β)∥2 2 ≤ 8 ̂ρ2 ∥ζ1∥2 2∥β∗ − ̂β∥ 2 2 + 2ρ2 ̂ρ2 ∥SP ′ 1(f ) T (β∗ − ̂β)∥ 2 2.(80) Next, we bound ∥SP ′ 1(f )T (β∗ − ̂β)∥2 2. Recall that U 0 span the column space of SP′ 1(f ). Thus SP ′ 1(f )T = SP′ 1(f ) T U 0U T 0 , therefore, ∥SP ′ 1(f )T (β∗ − ̂β)∥2 2 = ∥SP ′ 1(f ) T U 0U T 0 (β∗ − ̂β)∥2 2(81) ≤ ∥SP ′ 1(f )∥2 2∥U 0U T 0 (β∗ − ̂β)∥2 2. Recall that ̃U 0 denote the top k left singular vectors of SP ′ 0(x), and consider ∥U 0U T 0 (β∗ − ̂β)∥2 2 = ∥(U 0U T 0 + ̃U 0 ̃U T 0 − ̃U 0 ̃U T 0 )(β∗ − ̂β)∥2 2(82) ≤ 2∥U 0U T 0 − ̃U 0 ̃U T 0 ∥2 2∥β∗ − ̂β∥ 2 2 + 2∥ ̃U 0 ̃U T 0 (β∗ − ̂β)∥2 2. Using (82), (74) and Wedin sin Θ Theorem, we obtain, ∥U 0U T 0 (β∗ − ̂β)∥2 2 ≤ 2∥ζ0∥2 2 σk(ρSP ′ 0(f ))2 ∥β∗ − ̂β∥2 2 (83) + 8 σk(̂SP ′ 0)2 ( ∥SP ′ 0(f ) − ̂SP ′ 0∥ 2 2,∞∥β∗∥2 1 + 1 ρ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ) . Using (81) and (83), we have ∥SP ′ 1(f ) T (β∗ − ̂β)∥ 2 2 ≤ ∥SP′ 1(f )∥2 2 2∥ζ0∥2 2 σk(ρSP′ 0(f ))2 ∥β∗ − ̂β∥ 2 2 (84) + 8∥SP′ 1(f )∥2 2 σk(̂SP ′ 0)2 ( ∥SP ′ 0(f ) − ̂SP ′ 0∥ 2 2,∞∥β∗∥2 1 + 1 ρ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ) . ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 59 Finally, using (84) and (80), we have ∥̂SP ′ 1T (β∗ − ̂β)∥ 2 2 ≤ 8 ̂ρ2 ∥ζ1∥ 2 2∥β∗ − ̂β∥2 2 (85) + 4 ̂ρ2 ∥ζ0∥2 2∥SP ′ 1(f )∥2 2 σk(SP′ 0(f ))2 ∥β∗ − ̂β∥2 2 + 16ρ2 ̂ρ2 ∥SP′ 1(f )∥2 2 σk(̂SP ′ 0)2 ( ∥SP ′ 0(f ) − ̂SP ′ 0∥2 2,∞∥β∗∥2 1 + 1 ρ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ) . Finally, combining (85), (79), (78), and (76) yields, ∥SP′ 1(f ) T β∗ − ̂SP′ 1T ̂β∥ 2 2 ≤ C∥SP ′ 1(f ) − ̂SP′ 1∥2 2,∞∥β∗∥ 2 1 + C ̂ρ2 ∥ζ1∥ 2 2∆2 + C ̂ρ2 ∥ζ0∥2 2∥SP ′ 1(f )∥2 2 σk(SP′ 0(f ))2 ∆2 + Cρ2 ̂ρ2 ∥SP ′ 1(f )∥2 2∆1 σk(̂SP′ 0)2 .(86) Combining. Incorporating the two bounds in (77) and (86) yields, ∥ 1 ̂ρ SP ′ 1(X) T ̂β − SP ′ 1(f )T β∗∥ 2 2 ≤ C∥ 1 ̂ρ ζ1∥ 2 2 ( ∥ζ1∥2 2 ∥β∗∥ 2 2 σk(ρSP ′ 1(f ))2 + ∆2 ) + C∥SP′ 1(f ) − ̂SP′ 1∥2 2,∞∥β∗∥ 2 1 + C ̂ρ2 ∥ζ0∥2 2∥SP ′ 1(f )∥2 2 σk(SP′ 0(f ))2 ∆2 + Cρ2 ̂ρ2 ∥SP ′ 1(f )∥2 2∆1 σk(̂SP′ 0)2 .(87) For some absolute constant C > 0. High Probability Bound. We start by deﬁning the following high probability events. Let C(Γ1, Γ2, γ) be a positive constant dependent on model parameters Γ1, Γ2, γ, and let C > 0 be some positive absolute constant, deﬁne ¯E1 := { ∥ζ0∥2 ≤ C(γ + RΓ1Γ2) √N T /L} , ¯E2 := { ∥ζ1∥2 ≤ C(γ + RΓ1Γ2) √N T1/L } , ¯E3 := {(1 − √ 20 log(N T ) ρN T )ρ ≤ ̂ρ ≤ 1 1 − √ 20 log(N T ) ρN T ρ } , ¯E4 := { ∥SP ′ 0(f ) − ̂SP′ 0∥2 2,∞ ≤ C(γ, Γ1, Γ2) ( (N T )2R2 ρ4σk(SP ′ 0(f ))2L2 + kR2 log N T /L ρ2 ) } , (88) 60 ¯E5 := { ∥SP ′ 1(f ) − ̂SP ′ 1∥2 2,∞ ≤ C(γ, Γ1, Γ2) ( (N T1)2R2 ρ4σk(SP ′ 1(f ))2L2 + kR2 log N T1/L ρ2 + R2T1 T ) } . (89) Using Theorem F.2, we have the following, P( ¯E1) ≥ 1 − 2 exp ( −N T L ) , P( ¯E2) ≥ 1 − 2 exp ( −N T1 L ) . Further by Lemma G.4, P( ¯E3) ≥ 1 − 2 (N T )10 . Finally, the probabilities of ¯E4 and ¯E5 are bounded as we show next. LEMMA J.1. Let ¯E4 and ¯E5 be deﬁned as in (88) and (89). Then, for a constant C > 0, P( ¯E4) ≥ 1 − C (N T )10 , P( ¯E5) ≥ 1 − C (N T1)10 − C (N T )10 . PROOF. Bounding ¯E4 and ¯E5. P( ¯E4) and P( ¯E5) can be bounded using a direct utiliza- tion of Lemma G.2 and the high probability events deﬁned in Appendix G.5. Starting with ¯E4, using (29), and recalling that in this theorem setup ϵ = 0, Γ = RΓ1Γ2 (Property 1 and Property 2) and σ = γ (Property 3), we have that with probability 1 − C (N T )10 , ∥SP ′ 0(f ) − ̂SP ′ 0∥2 2,∞ ≤ C γ2(N T )2 ρ2σk(SP ′ 0(f ))2L2 ( (RΓ1Γ2)2 + γ2 ρ2 ) + Cγ2k log N T /L ρ2 + C(RΓ1Γ2) 2 ≤ C(γ, Γ1, Γ2) ( (N T )2R2 ρ4σk(SP ′ 0(f ))2L2 + kR2 log N T /L ρ2 ) . A similar argument can be used for ¯E5, while noting that the term C (N T )10 shows up due to utilizing the estimate ̂ρ, which is estimated from the ﬁrst T observations. Precisely, we get the following, ∥SP ′ 1(f ) − ̂SP ′ 1∥2 2,∞ ≤ C γ2(N T1)2 ρ2σk(SP ′ 1(f ))2L2 ( (RΓ1Γ2)2 + γ2 ρ2 ) + Cγ2k log(N T1/L) ρ2 + C (RΓ1Γ2)2T1 T ≤ C(γ, Γ1, Γ2) ( (N T1)2R2 ρ4σk(SP ′ 1(f ))2L2 + R2k log(N T1/L) ρ2 + R2T1 T ) . Now, given these events, we will provide the high probability bound. Let ¯E := ¯E1 ∩ ¯E2 ∩ ¯E3 ∩ ¯E4 ∩ ¯E5. P( ¯Ec) ≤ C0 (N T )10 + C1 (N T1)10 ,(90) ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 61 for some absolute constants C0, C1 > 0. Note that under event ¯E3, we have that ̂ρ ≥ ρ (1 − √ 20 log(N T ) ρN T ). By further using the assumption ρ ≥ C log(N T )/√N T for a sufﬁciently large C we have that ̂ρ ≥ C′ρ and (̂ρ−ρ) 2 ̂ρ2 ≤ C√N T . Now, recall ∆1 and ∆2 deﬁnition in (76). Under event ¯E, we can bound ∆1 as follows, ∆1 = ∥̂SP ′ 0 − SP ′ 0(f )∥ 2 2,∞∥β∗∥2 1 + 1 ρ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ≤ C(γ, Γ1, Γ2)∥β∗∥ 2 1 ( (N T )2R2 ρ4σk(SP′ 0(f ))2L2 + kR2 log(N T /L) ρ2 ) + 1 ρ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 . Similarly, under event ¯E, we can bound ∆2 as follows, ∆2 ≤ C(γ, Γ1, Γ2)∥β∗∥ 2 1 ( N T R2 Lσk(ρSP ′ 0(f ))2 + 1 σk(̂SP ′ 0)2 ( (N T )2R2 ρ4σk(SP′ 0(f ))2L2 + kR2 log N T /L ρ2 ) ) + C ρσk(̂SP ′ 0)2 ((̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ) . Further, using Weyl’s inequality (see Lemma F.6), we can bound |σk(̂SP ′ 0) − σk(SP′ 0(f ))| as follows, |σk(̂SP ′ 0) − σk(ρSP ′ 0(f ))| = 1 ̂ρ |σk( ̃Σ0) − ̂ρσk(SP′ 0(f ))| ≤ 1 ̂ρ |σk( ̃Σ0) − ρσk(SP′ 0(f ))| + |̂ρ − ρ| ̂ρ σk(SP ′ 0(f )) ≤ ∥ζ0∥2 ̂ρ + |̂ρ − ρ| ̂ρ σk(SP′ 0(f )) Under ¯E, and using property 4, we have that with probability of at least 1 − 1 (N T )10 , |σk(̂SP ′ 0) − σk(SP ′ 0(f ))| σk(SP ′ 0(f )) ≤ C(γ + RΓ1Γ2) √N T /L ρσk(SP′ 0(f )) + |̂ρ − ρ| ̂ρ ≤ C(γ + RΓ1Γ2) √k ρ√L + C √N T . Using ρ ≥ C(γ + RΓ1Γ2)√ k L we get 1 σk( ̂SP′ 0)2 ≤ C σk(SP′ 0(f ))2 . Using property 4, we get the following bounds for ∆1 and ∆2, ∆1 ≤ C(γ, Γ1, Γ2, c)∥β∗∥ 2 1kR2 ( N T L2ρ4 + log(N T /L) ρ2 ) + 1 ρ (̂SP ′ 0T ( ̂β − β∗) )T ζ L 0 .(91) 62 ∆2 ≤ C(γ, Γ1, Γ2, c)∥β∗∥2 1 ( kR2 Lρ2 + k2R2 N T ( N T L2ρ4 + log(N T /L) ρ2 ) ) (92) + Ck ρN T ((̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ) ≤ C(γ, Γ1, Γ2, c)∥β∗∥2 1k2R2( 1 Lρ2 + log(N T /L) L ) + Ck ρN T ((̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ) , where ρ ≥ C(γ + RΓ1Γ2) √ k L is used to obtain the last inequality. Finally, using properties 4 and 5, ̂ρ ≥ C′ρ, and (87), (91), and (92), we have under event ¯E, ∥ 1 ̂ρ SP′ 1(X)T ̂β − SP ′ 1(f ) T β∗∥2 2(93) ≤ C(γ, Γ1, Γ2, c) ( k3N T1R6 L2ρ4 + RT1 T ) ∥β∗∥2 1 + C(γ, Γ1, Γ2, c) ( k3R6 log(N T /L) ρ2 ( N T1 L2 + T1 T ) + kR2 log(N T1/L) ρ2 ) ∥β∗∥2 1 + C(γ, Γ1, Γ2, c) R4k2T1 T ρ3 (̂SP′ 0T ( ̂β − β∗) )T ζ L 0 . Expectation Bound. We get the bound in expectation using the high probability bound above, and by assuming that our forecast is bounded such that | ¯fn(T + L x m′)| ≤ RΓ1Γ2 for m′ ∈ [T1/L]. Speciﬁcally, we have using (93) and (90), TestForErr(N, T, T1, L) = 1 (N T1/L) E [∥ ∥ ∥ ∥ 1 ̂ρ SP′ 1(X)T ̂β − SP ′ 1(f ) T β∗∥ ∥ ∥ ∥ 2 2 ] ≤ 1 (N T1/L) E [∥ ∥ ∥ ∥ 1 ̂ρ SP′ 1(X)T ̂β − SP ′ 1(f ) T β∗∥ ∥ ∥ ∥ 2 2 ∣ ∣ ∣ ∣ ∣ ¯E ] + CR2Γ2 1Γ2 2 (N min(T, T1))10 ≤ L N T1 C(γ, Γ1, Γ2, c) ( ( k3N T1R6 L2ρ4 + RT1 T ) ∥β∗∥2 1 + ( k3R6 log(N T /L) ρ2 ( N T1 L2 + T1 T ) + kR2 log(N T1/L) ρ2 ) ∥β∗∥ 2 1 + R4k2T1 T ρ3 E [ (̂SP ′ 0T ( ̂β − β∗))T ζ L 0 ∣ ∣ ∣ ¯E]) + CR2Γ2 1Γ2 2 (N min(T, T1))10 . ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 63 Noting that the E[ζ L 0 ∣ ∣ ¯E] = 0, and ζ L 0 is independent of ̂SP ′ 0, ̂ρ, β∗ and the event ¯E; we have E [ (̂SP ′ 0T β∗)T ζ L 0 ] = 0. By (3), we have ̂β = ̃U 0( ̃Σ0)† ̃V T SP0(X)L·. That is, ̂β = ̃U 0( ̃Σ0) † ̃V T ρSP0(f )L· + ̃U 0( ̃Σ0)† ̃V T ζ L 0 .(94) Using cyclic and linearity of Trace operator; the independence properties of ζ L 0 ; and (94); we have E [ (̂SP ′ 0T ̂β)T ζ L 0 ] (95) = E [ (̂SP′ 0T ̃U 0( ̃Σ0)† ̃V T ρSP0(f )L· )T ζ L 0 ] + E [ ( ̃V 0 ̃V T ζ L 0 )T ζ L 0 ] = E[Tr((ζ L 0 ) T ̃V 0 ̃V T ζ L 0 )] = E[Tr( ̃V 0 ̃V T ζ L 0 (ζ L 0 )T ] = Tr(E[ ̃V 0 ̃V T ]E[ζ L 0 (ζ L 0 )T ] ) ≤ C(γ + Γ1Γ2R)2k. Where to obtain the last inequality we use the trace property Tr(AB) ≤ ∥B∥2Tr(A) for pos- itive semi-deﬁnite matrices A, B, and that rank of ̂SP ′ 0 is k. Finally, using (95), and recalling that T1 ≥ L and L ≤ T we get, TestForErr(N, T, T1, L) ≤ L N T1 C(γ, Γ1, Γ2, c) ( ( R6k3N T1 L2ρ4 + RT1 T ) ∥β∗∥ 2 1 + ( R6k3 log(N T /L) ρ2 ( N T1 L2 + T1 T ) + R2k log(N T1/L) ρ2 ) ∥β∗∥ 2 1 + R6k3T1 T ρ3 ) + CR2Γ2 1Γ2 2 (N L)10 ≤ L N T1 C(γ, Γ1, Γ2, c) max(1, ∥β∗∥2 1) ( R6k3N T1 L2ρ4 + R6k3T1 T ρ3 + R6k3 log(N T ) ρ2 ( N T1 L2 + T1 T ) + R2k log(N T1) ρ2 + R2 (N L)10 ) 64 ≤ L N T1 C(γ, Γ1, Γ2, c) max(1, ∥β∗∥2 1) ( R6k3 log(N T ) ρ4 ( N T1 L2 + T1 T ) + R2k log(N T1) ρ2 ) . Then, with L = √min(N, T )T , we get, TestForErr(N, T, T1, L) ≤ √min(N, T )T N T1 C(γ, Γ1, Γ2, c) max(1, ∥β∗∥2 1) ( R6k3 log(N T ) ρ4 ( N T1 T min(N, T ) + T1 T ) + R2k log(N T1) ρ2 ) ≤ T T1 √min(N, T )T N T C(γ, Γ1, Γ2, c) max(1, ∥β∗∥ 2 1) ( R6k3 log(N T ) ρ4 ( N T1 T min(N, T ) + T1 T ) + R2k log(N T1) ρ2 ) ≤ √min(N, T )T N T C(γ, Γ1, Γ2, c) max(1, ∥β∗∥2 1) ( R6k3 log(N T ) ρ4 ( N min(N, T ) + 1) + T R2k log(N T1) T1ρ2 ) ≤ C(γ, Γ1, Γ2, c) max(1, ∥β∗∥2 1) ( R6k3 log(N max(T, T1)) ρ4√min(N, T )T (max(1, N T ) + T T1 ) ) . Choosing k = RG completes the proof. APPENDIX K: PROOF OF THEOREM 7.1 Setup, Notations. For L ≥ 1, k ≥ 1, for ease of notations, we deﬁne ◦ SP(X) = SP((X1, . . . , XN ), T, L) ∈ RL×(N T /L), ◦ SP(X 2) = SP((X 2 1 , . . . , X 2 N ), T, L) ∈ RL×(N T /L), ◦ SP(f ) = SP((f1, . . . , fN ), T, L) ∈ RL×(N T /L), ◦ SP(f 2) = SP((f 2 1 , . . . , f 2 N ), T, L) ∈ RL×(N T /L), ◦ SP(σ2) = SP((σ2 1, . . . , σ2 N ), T, L) ∈ RL×(N T /L), ◦ SP(f 2 + σ2) = SP(f 2) + SP(σ2). Recalling that ρ = 1, we note that E[SP(X)] = SP(f ), E[SP(X 2)] = SP(f 2 + σ2). Further, from the deﬁnition of the variance estimation algorithm, we recall ̂SP(f ) := ̂SP((X1, . . . , XN ), T, L) = 1 ̂ρ HSVTk(SP((X1, . . . , XN ), T, L)) ̂SP(f 2 + σ2) := ̂SP((X 2 1 , . . . , X 2 N ), T, L) = 1 ̂ρ HSVTk(SP((X 2 1 , . . . , X 2 N ), T, L)) We denote ◦ ̂SP(f 2) = ̂SP(f ) ◦ ̂SP(f ) ◦ ̂SP(σ2) = max (̂SP(f 2 + σ2) − ̂SP(f 2), 0), where 0 ∈ RL×(N T /L) is a matrix of all zeroes, and we apply the max(·) above entry-wise. We remind the reader the output of the variance estimation algorithm is ̂SP(σ2). Thus, we ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 65 have 1 N T N∑ n=1 T∑ t=1 (σn(t) 2 − ˆσ2 n(t))2 = 1 N T ∥SP(σ2) − ̂SP(σ2)∥ 2 F . Initial Decomposition. Note that since σ2 n(t) ≥ 0 for n ∈ [N ] and t ∈ [T ], we have that 1 N T ∥SP(σ2) − ̂SP(σ2)∥2 F ≤ 1 N T ∥SP(σ2) − (̂SP(f 2 + σ2) − ̂SP(f 2))∥ 2 F = 1 N T ∥SP(f 2 + σ2) − SP(f 2) − (̂SP(f 2 + σ2) − ̂SP(f 2)∥2 F ≤ 2 N T ∥SP(f 2 + σ2) − ̂SP(f 2 + σ2)∥ 2 F + 2 N T ∥SP(f 2) − ̂SP(f 2)∥ 2 F(96) We bound the two terms on the r.h.s of (96) separately. Bounding E[∥SP(f 2) − ̂SP(f 2)∥2 F ]. ∥SP(f 2) − ̂SP(f 2)∥ 2 F = N∑ n=1 T∑ t=1 (f 2 n(t) − ˆf 2 n(t) )2 = N∑ n=1 T∑ t=1 ( fn(t) − ˆfn(t))2(fn(t) + ˆfn(t) )2 ≤ [ max n∈[N ],t∈[T ] ( fn(t) + ˆfn(t))2] [ N∑ n=1 T∑ t=1 (fn(t) − ˆfn(t))2] (a) ≤ C(Γ1, Γ2, Γ3)R2 [ N∑ n=1 T∑ t=1 (fn(t) − ˆfn(t) )2] = C(Γ1, Γ2, Γ3)R2∥SP(f ) − ̂SP(f )∥ 2 F(97) Bounding ∥SP(f 2 + σ2) − ̂SP(f 2 + σ2)∥2 F . To bound ∥SP(f 2 + σ2) − ̂SP(f 2 + σ2)∥2 F , we modify the proof of Theorem 5.6 in a straightforward manner. The need for the modiﬁcation is that Theorem 5.6 was proven for the case where the coordinate wise noise, ηn(t) = Xn(t)− fn(t) are independent sub-gaussian random variables, and ∥η∥ψ2 ≤ γ. However, one can verify that X 2 n(t) − f 2 n(t) − σ2 n(t) is a sub-exponential random variable with ∥ · ∥ψ1 norm bounded as ∥X 2 n(t) − f 2 n(t) − σ2 n(t)∥ψ1 ≤ ∥X 2 n(t)∥ψ1 = ∥f 2 n(t) + 2fn(t)ηn(t) + η2 n(t)∥ψ1 ≤ 2∥f 2 n(t)∥ψ1 + 2∥η2 n(t)∥ψ1 = 2∥fn(t)∥ 2 ψ2 + 2∥ηn(t)∥2 ψ2 ≤ C(Γ1, Γ2)R2 + 2γ2 ≤ C(Γ1, Γ2, γ)R2, 66 where we have use the standard facts that for a random variable A, ∥A − E[A]∥ψ1 ≤ ∥A∥ψ1 and ∥A2∥ψ1 = ∥A∥2 ψ2. Further, note that by using Properties 1, 2, 9, and 10, and a straightforward modiﬁcation of Proposition D.1, we have rank(SP(f 2 + σ2)) ≤ rank(SP(f 2)) + rank(SP(σ2)) ≤ (RG) 2 + (R′G′), where we have used that for any two matrices A, B, we have rank(A ◦ A) ≤ rank(A)2, where ◦ denotes Hadamard product, and rank(A + B) ≤ rank(A) + rank(B). We deﬁne ˜k := (RG)2 + (R′G′). Modiﬁed Theorem 5.6. Below, we state the modiﬁed version of Theorem 5.6 to get our desired result. LEMMA K.1 (Imputation Error). Let the conditions of Theorem 7.1 hold. Then, E [ max j∈[L] 1 (N T /L) ∥SP(f 2 + σ2)T L,· − ̂SP(f 2 + σ2) T L,·∥2 2] ≤ C(Γ1, Γ2, Γ ′ 1, Γ ′ 2, γ, R, R′) ( (G2 + G′) log2 N T L . ) , where C(Γ1, Γ2, Γ′ 1, Γ′ 2, γ, R, R′) is a term that depends only polynomially on Γ1, Γ2, Γ′ 1, Γ′ 2, γ, R, R′. PROOF. To reduce redundancy, we provide an overview of the argument needed for this proof, focusing only the parts of the arguments made in Theorem 5.6 that need to be modiﬁed. For ease of exposition, we let ˜C = C(Γ1, Γ2, Γ′ 1, Γ′ 2, γ, R, R′). We being by matching notation with that used in Theorem 5.6; in particular with respect to ρ, k, ϵ, Γ. Under the setup of Theorem 7.1, we have ρ = 1, k = ˜k, ϵ = 0, Γ ≤ ˜C Further, recall the deﬁnition of Y , M , p, q, σ from Appendix G.1. We will now use Y = SP(X 2), and M = SP(f 2 + σ2)), σ = γ, p = (N T /L), q = L. One can verify that there is only required change to the proof of Theorem 5.6; in particular, in the argument made to prove The- orem G.3, we need to re-deﬁne events E2, E3, E4 in (25), (26), (27) for the case where (Y − M )ij is mean-zero sub-exponential. Using the result from [3, 4], which bounds the operator norm of a matrix with sub-exponential mean-zero entries, we have with probability at least 1 − 1/((N T )10) ∥Y − M ∥2 ≤ ˜C√(N T /L) log2 N T(98) As a result (98), and standard concentration inequalities for sub-exponential random vari- ables, we have the modiﬁed events, ˜E2, ˜E3, ˜E4. ˜E2 := { ∥Y − ρM ∥2 ≤ ˜C√(N T /L) log2 N T } , ˜E3 := { ∥Y − ρM ∥∞,2, ∥Y − ρM ∥2,∞ ≤ ˜C√(N T /L) log2 N T } , ˜E4 := { max j∈[q]∥ϕ B σk(B)(Y T j· − ρM T j·)∥ 2 2 ≤ ˜C˜k log2(N T /L) } , Using these modiﬁed events in the proofs of Theorem G.3 and Theorem 5.6, and appropri- ately simplifying leads to the desired result. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 67 By Lemma K.1 and (40), we have that 1 N T E[∥SP(f 2 + σ2) − ̂SP(f 2 + σ2)∥2 F ≤ E [ max j∈[L] 1 (N T /L) ∥SP(f 2 + σ2) T L,· − ̂SP(f 2 + σ2) T L,·∥2 2] ≤ C(Γ1, Γ2, Γ ′ 1, Γ ′ 2, γ, R, R′) ( (G2 + G′) log2 N T L .) .(99) Completing proof. Substituting (97) and (99) into (96) and letting L = √min(N, T )T 1 N T ∥SP(σ2) − ̂SP(σ2)∥ 2 F ≤ C(Γ1, Γ2, Γ3, Γ′ 1, Γ′ 2, γ, R, R′) ( (G2 + G′) log2 N T √min(N, T )T . ) . This completes the proof. APPENDIX L: TSSA PROOFS L.1. Proof of Proposition 7.1. Consider n ∈ [N ], ℓ ∈ [L], s ∈ [T /L]. By Property 1, Tnℓs = fn((s − 1) × L + ℓ) = R∑ r=1 UnrWr((s−1)×L+ℓ).(100) The Hankel matrix induced by time series Wr· has rank at most G as per Property 2. The Page matrix associated with it is of dimension L × T /L with entry in its ℓ-th row and s-th column equal to Wr((s−1)×L+ℓ). Since this Page matrix can be viewed as a sub-matrix of the Hankel matrix, it has rank at most G as well. That is, there exists vectors wr ℓ·, vr s· ∈ RG such that Wr((s−1)×L+ℓ) = G∑ g=1 wr ℓgvr sg.(101) From (100) and (101), it follows that Tnℓs = R∑ r=1 Unr( G∑ g=1 wr ℓgvr sg) = ∑ r∈[R],g∈[G] Unrwr ℓgvr sg = ∑ r∈[R],g∈[G] an (r,g)bℓ (r,g)cs (r,g),(102) where an (r,g) = Unr, bℓ (r,g) = wr ℓg and cs (r,g) = vr sg. Thus (102) implies that T has CP-rank at most R x G. By the setup and model deﬁnition, it follows Tnℓs = Xn((s − 1) × L + ℓ). And Xn((s − 1) × L + ℓ) = ⋆ with probability 1 − ρ and fn((s − 1) × L + ℓ) + ηn((s − 1) × L + ℓ) with probability ρ, where ηn((s − 1) × L + ℓ) are independent and zero-mean. Therefore, it follows that the entries of T are independent and E[Tnℓs] = E[Xn((s − 1) × L + ℓ)] = ρfn((s − 1) × L + ℓ) = ρTnℓs. 68 That is, E[T] = ρT. This concludes the proof. L.2. Proof of Proposition 7.2 . From Property 12, and our choice of parameter L for mSSA (L = √min(N, T )T ) and tSSA (L = √T ), we have that ImpErr(N, T ; tSSA) = ˜Θ    1 min (N, √T )2    = ˜Θ ( 1 min (N 2, T ) ) ,(103) ImpErr(N, T ; mSSA) = ˜Θ ( 1 √min(N, T )T ) ,(104) ImpErr(N, T ; ME) = ˜Θ ( 1 min (N, T ) ) .(105) We proceed in cases. Case 1: T = o(N ). In this case, from (103), (104), and (105), we have ImpErr(N, T ; tSSA), ImpErr(N, T ; mSSA), ImpErr(N, T ; ME) = ˜Θ ( 1 T ) Case 2: N = o(T ). In this case, from (103), (104), and (105), we have ImpErr(N, T ; tSSA) = ˜Θ ( 1 N 2 ) ,(106) ImpErr(N, T ; mSSA) = ˜Θ ( 1 √N T ) ,(107) ImpErr(N, T ; ME) = ˜Θ ( 1 N ) . In this case, we have ImpErr(N, T ; tSSA), ImpErr(N, T ; mSSA) = ˜o(ImpErr(N, T ; ME)). It remains to compare the relative performance of tSSA and mSSA for the regime N = o(T ). Towards this, note from (106) and (107) that ImpErr(N, T ; tSSA) = ˜o(ImpErr(N, T ; mSSA)) ⇐⇒ 1 N 2 = ˜o( 1 √N T ) ⇐⇒ T 1/3 = o(N ) This completes the proof. L.3. Proof of Proposition C.1. PROPOSITION L.1. Let Properties 13, 2, and 3 hold. Then, for any 1 ≤ L ≤ √T , HT has CP-rank at most R x G. Further, all entries of HT are independent random variables with each entry observed with probability ρ ∈ (0, 1], and E[HT] = ρHT. ON MULTIVARIATE SINGULAR SPECTRUM ANALYSIS AND ITS VARIANTS 69 Consider n1, . . . , nd ∈ [N1] × · · · × [Nd], ℓ ∈ [L], s ∈ [T /L]. By Property 13, HTn1,...,nd,ℓ,s = fn1,...,nd((s − 1) × L + ℓ) = R∑ r=1 Un1,r . . . Und,r Wr,((s−1)×L+ℓ), The rest of the proof follows in a similar fashion to that of Proposition 7.1. REFERENCES [1] AGARWAL, A., AMJAD, M. J., SHAH, D. and SHEN, D. (2018). Model Agnostic Time Series Analysis via Matrix Estimation. Proceedings of the ACM on Measurement and Analysis of Computing Systems 2 40. [2] AGARWAL, A., SHAH, D. and SHEN, D. (2020). On Principal Component Regression in a High- Dimensional Error-in-Variables Setting. arXiv preprint arXiv:2010.14449. [3] AGARWAL, A., SHAH, D., SHEN, D. and SONG, D. (2019). On robustness of principal component regres- sion. In Advances in Neural Information Processing Systems 9889–9900. [4] AGARWAL, A., SHAH, D., SHEN, D. and SONG, D. (2021). On Robustness of Principal Component Re- gression. Accepted to appear in Journal of the American Statistical Association. [5] BANBURA, M. and MODUGNO, M. (2014). Maximum Likelihood Estimation of Factor Models on Datasets with Arbitrary Pattern of Missing Data. Journal of Applied Econometrics 29 133-160. [6] BARAK, B. and MOITRA, A. (2016). Noisy Tensor Completion via the Sum-of-Squares Hierarchy. In Pro- ceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016 (V. FELDMAN, A. RAKHLIN and O. SHAMIR, eds.). JMLR Workshop and Conference Proceedings 49 417–445. JMLR.org. [7] BARIGOZZI, M. and LUCIANI, M. (2019). Quasi maximum likelihood estimation of non-stationary large approximate dynamic factor models. arXiv preprint arXiv:1910.09841. [8] BERNSTEIN, S. (1946). The Theory of Probabilities. Gastehizdat Publishing House. [9] BÓGALO, J., PONCELA, P. and SENRA, E. (2020). Understanding ﬂuctuations through Multivariate Circu- lant Singular Spectrum Analysis. arXiv preprint arXiv:2007.07561. [10] BROOMHEAD, D. and KING, G. (1986). On the Qualitative Analysis of Experimental Dynamical Systems 11. [11] CAI, C., LI, G., POOR, H. V. and CHEN, Y. (2019). Nonconvex Low-Rank Tensor Completion from Noisy Data. 32 1863–1874. [12] CHOLLET, F. (2015). keras. https://github.com/fchollet/keras. [13] DAVIS, C. and KAHAN, W. M. (1970). The rotation of eigenvectors by a perturbation. III. SIAM Journal on Numerical Analysis 7 1–46. [14] DOZ, C., GIANNONE, D. and REICHLIN, L. (2012). A Quasi–Maximum Likelihood Approach for Large, Approximate Dynamic Factor Models. The Review of Economics and Statistics 94 1014-1024. [15] FACEBOOK (2020). Prophet. https://facebook.github.io/prophet/. Online; accessed 25 February 2020. [16] FORNI, M., HALLIN, M., LIPPI, M. and REICHLIN, L. (2000). The Generalized Dynamic-Factor Model: Identiﬁcation and Estimation. The Review of Economics and Statistics 82 540–554. [17] GAVISH, M. and DONOHO, D. L. (2014). The optimal hard threshold for singular values is 4/√ 3. IEEE Transactions on Information Theory 60 5040–5053. [18] GHIL, M., ALLEN, M. R., DETTINGER, M. D., IDE, K., KONDRASHOV, D., MANN, M. E., ROBERT- SON, A. W., SAUNDERS, A., TIAN, Y., VARADI, F. and YIOU, P. (2002). Advanced Spectral Method for Climatic Time Series. Reviews of Geophysics 40 3-1-3-41. [19] GOLYANDINA, N., NEKRUTKIN, V. and ZHIGLJAVSKY, A. A. (2001). Analysis of time series structure: SSA and related techniques. Chapman and Hall/CRC. [20] GRAFAKOS, L. (2008). Classical fourier analysis 2. Springer. [21] HALLIN, M. and LIŠKA, R. (2007). Determining the Number of Factors in the General Dynamic Factor Model. Journal of the American Statistical Association 102 603–617. [22] HASSANI, H., HERAVI, S. and ZHIGLJAVSKY, A. (2013). Forecasting UK industrial production with mul- tivariate singular spectrum analysis. Journal of Forecasting 32 395–408. [23] HASSANI, H. and MAHMOUDVAND, R. (2013). Multivariate singular spectrum analysis: A general view and new vector forecasting approach. International Journal of Energy and Statistics 1 55–83. [24] HASSANI, H. and MAHMOUDVAND, R. (2018). Singular spectrum analysis: Using R. Springer. 70 [25] HYNDMAN, R. J. and ATHANASOPOULOS, G. (2018). Forecasting: principles and practice. OTexts. [26] MAKRIDAKIS, S., SPILIOTIS, E. and ASSIMAKOPOULOS, V. (2020). The M5 accuracy competition: Re- sults, ﬁndings and conclusions. Int J Forecast. [27] OROPEZA, V. and SACCHI, M. (2011). Simultaneous seismic data denoising and reconstruction via multi- channel singular spectrum analysis. Geophysics 76 V25–V32. [28] PLAUT, G. and VAUTARD, R. (1994). Spells of Low-Frequency Oscillations and Weather Regimes in the Northern Hemisphere. Journal of Atmospheric Sciences 51 210 - 236. [29] RAO, N., YU, H.-F., RAVIKUMAR, P. K. and DHILLON, I. S. (2015). Collaborative Filtering with Graph Information: Consistency and Scalable Methods. In Advances in Neural Information Processing Sys- tems 28 (C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett, eds.) 2107–2115. Curran Associates, Inc. [30] ROBERT H. SHUMWAY, D. S. S. (2015). Time Series Analysis and It’s Applications, 3rd ed. Blue Printing. [31] SALINAS, D., FLUNKERT, V., GASTHAUS, J. and JANUSCHOWSKI, T. (2019). DeepAR: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting. [32] SEABOLD, S. and PERKTOLD, J. (2010). statsmodels: Econometric and statistical modeling with python. In 9th Python in Science Conference. [33] SEN, R., YU, H.-F. and DHILLON, I. S. (2019). Think globally, act locally: A deep neural network ap- proach to high-dimensional time series forecasting. In Advances in Neural Information Processing Systems 4838–4847. [34] SHAH, D. and YU, C. L. (2019). Iterative Collaborative Filtering for Sparse Noisy Tensor Estimation. In 2019 IEEE International Symposium on Information Theory (ISIT) 41–45. IEEE. [35] STOCK, J. H. and WATSON, M. W. (2002). Forecasting Using Principal Components from a Large Number of Predictors. Journal of the American Statistical Association 97 1167–1179. [36] TRINDADE, A. (2014). UCI Machine Learning Repository - Individual Household Electric Power Con- sumption Data Set. [37] VERSHYNIN, R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027. [38] WASSERMAN, L. (2006). All of nonparametric statistics. Springer. [39] WEDIN, P.-Å. (1972). Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics 12 99–111. [40] WILSON, K. W., RAJ, B. and SMARAGDIS, P. (2008). Regularized non-negative matrix factorization with temporal dependencies for speech denoising. In Ninth Annual Conference of the International Speech Communication Association. [41] WRDS (2021). The Trade and Quote (TAQ) database. [42] XIA, D., YUAN, M. and ZHANG, C.-H. (2018). Statistically Optimal and Computationally Efﬁcient Low Rank Tensor Completion from Noisy Entries. [43] XU, J. (2017). Rates of convergence of spectral methods for graphon estimation. arXiv preprint arXiv:1709.03183. [44] YU, C. L. (2020). Tensor Estimation with Nearly Linear Samples. arXiv preprint arXiv:2007.00736. [45] YU, H.-F., RAO, N. and DHILLON, I. S. (2016). Temporal regularized matrix factorization for high- dimensional time series prediction. In Advances in neural information processing systems 847–855.","libVersion":"0.3.2","langs":""}
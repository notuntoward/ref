{"path":"lit/lit_notes_OLD_PARTIAL/SwimmTeam24TransfrmrBasics7Models.pdf","text":"4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/15 LARGE LANGUAGE MODELS Transformer Model: The Basics and 7 Models You Should Know Swimm Team 9 min read What Is a Transformer Model? The Transformer model is a type of model used in machine learning, particularly in the area of natural language processing (NLP). Introduced in the 2017 paper “Attention is All You Need” by Vaswani et al., the model has since revolutionized the understanding and application of NLP. Unlike traditional recurrent neural network (RNN) models that process data sequentially, the Transformer model processes all data points in parallel, making it more ef\u0000cient. On this page: What Is a Transformer Model? Architecture of the Transformer Model Common Applications of the Transformer Model 7 Notable Transformer-Based Models 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/15 The Transformer model has been key to many breakthroughs in NLP tasks. It has achieved state-of- the-art results in translation, summarization, and information extraction, by capturing the context of words in a sentence irrespective of their position. Additionally, it has inspired various other models like BERT, T5 and GPT, and is the basis for modern large language models0 (LLMs) and many generative AI applications. Architecture of the Transformer Model The Transformer architecture is composed of several layers, each of which plays a critical role in processing sequences of text.  Input Embedding Layer Initially, the model is fed with raw text data, which is then converted into numerical vectors in the input embedding layer. These numerical vectors, or ’embeddings’, represent the words in a way that the model can understand and process. 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/15 The embedding layer maps each word in the input sequence to a high-dimensional vector. These vectors capture the semantic meaning of words, and words with similar meanings have vectors that are close to each other in the vector space. This layer paves the way for the model to understand and process language data. Positional Encoding Unlike RNNs, the Transformer model processes all words in a sentence simultaneously. While this parallel processing enhances the ef\u0000ciency of the model, it also presents a challenge: the model cannot inherently understand the order or position of the words in a sentence. Positional encoding is a technique used to give the model information about the position of words in a sentence. It adds a vector to each input embedding, which represents the position of the word in the sentence. This way, even though the Transformer processes all words simultaneously, it still understands the order of words. 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/15 Multi-Head Self-Attention Mechanism After positional encoding, the model processes the data through a multi-head self-attention mechanism. This mechanism allows the model to focus on different parts of the input sequence for each word, giving it the ability to understand the context of words in a sentence. The self-attention mechanism works by assigning weights to each word in the sentence based on its relevance to the other words. These weights determine how much attention the model should pay to each word when processing a particular word. The ‘multi-head’ part means the model has multiple self-attention mechanisms, or ‘heads’, each of which focuses on different aspects of the input data. Feed-Forward Neural Networks After the multi-head self-attention mechanism, the output goes through a feed-forward neural 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/15 network (FFNN). This network consists of two linear transformations with a ReLU (Recti\u0000ed Linear Unit) activation function in between. The FFNN is applied independently to each position, processing the output from the self-attention mechanism. This layer helps add complexity and depth to the transformation. Normalization and Residual Connections Normalization and residual connections help to stabilize the learning process and increase the depth of the neural network used to generate output. Normalization standardizes the inputs to the next layer, reducing the training time and improving the performance of the model. Residual connections, or skip connections, allow the gradient to \u0000ow directly from the output to the input, bypassing the layer’s transformation. This makes it possible to use a deeper neural network with more layers, without 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 6/15 facing the vanishing gradient problem. Output Layer This layer generates the \u0000nal output of the model. For tasks like translation or text generation, this layer usually consists of a softmax function that outputs a probability distribution over the vocabulary for predicting the next word. The output layer brings together all the preceding layers’ computations to generate a \u0000nal result. The result could be a translated sentence, a summary of a document, or any other NLP tasks that the Transformer model is trained to perform. Common Applications of the Transformer Model The Transformer model has found applications into numerous domains, providing innovative solutions and improving the ef\u0000ciency of existing systems.  4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 7/15 Machine Translation You’ve likely used online translation tools like Google Translate before. Modern machine translation techniques primarily rely on Transformers. They use attention mechanisms to understand the context and semantic meaning of words in different languages, enabling a more accurate translation than previous generation models.  The Transformer’s ability to handle long sequences of data makes it particularly adept at this task, allowing it to translate entire sentences with unprecedented accuracy. Text Generation When you type a query into a search engine and it auto-\u0000lls the rest of your sentence, this is also likely powered by a Transformer model. By analyzing patterns and sequences in the input data, the Transformer can predict and generate coherent and contextually relevant text. This technology is used in a wide range 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 8/15 of applications, from email auto- complete features to chatbots and virtual assistants. More advanced models such as OpenAI GPT and Google PaLM, which power new consumer applications like ChatGPT and Bard, use a Transformer architecture to generate human- like text and code based on natural language prompts.  Sentiment Analysis Sentiment analysis is a tool for businesses that want to understand customer opinions and feedback. A Transformer can analyze text data, such as product reviews or social media posts, and determine the sentiment behind them (for example, positive, negative, or neutral). By doing this at scale, businesses can extract valuable insights about their products or services and make informed decisions. Named Entity Recognition Named Entity Recognition (NER) is a task in Natural Language 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 9/15 Processing (NLP) that involves identifying and classifying entities in text into prede\u0000ned categories like names of persons, organizations, locations, expressions of times, quantities, etc. The Transformer model, with its self-attention mechanism, can recognize these entities even in complex sentences. Related content: Read our guide to transformer neural networks 7 Notable Transformer-Based Models The Transformer model has evolved, with researchers developing several advancements and variants to improve its performance and applicability. BERT BERT, introduced by Google in 2018, marked a signi\u0000cant advance in how the Transformer model is applied to NLP tasks. Unlike its predecessors, BERT operates in a bidirectional manner, 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 10/15 meaning it considers the context from both left and right sides of a token in a sentence. This allows for a deeper understanding of language context.  BERT has been especially successful in tasks like question- answering and language inference. Its architecture is mainly based on the encoder part of the traditional Transformer model and is pre-trained on a large corpus of text before being \u0000ne-tuned for speci\u0000c tasks. GPT OpenAI’s GPT series, leading up to GPT-4, released in April 2023, represent a signi\u0000cant leap in language modeling. These models excel in generating human-like text and can perform a variety of NLP tasks without task-speci\u0000c training. They rely on massive amounts of training data and increasingly complex architectures.  GPT models are autoregressive, meaning they predict the next word in a sequence, considering all 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 11/15 the previous words. This makes them particularly adept at tasks like text generation, translation, and even creative writing. Transformer-XL Transformer-XL, developed by Google, addresses one of the limitations of the standard Transformer model—handling long-range dependencies in text. Traditional Transformers struggle with longer text sequences due to \u0000xed-length context windows.  Transformer-XL incorporates a novel segment-level recurrence mechanism and a relative positional encoding scheme to capture longer-range dependencies, allowing it to remember information from the distant past of a text sequence. This makes it particularly useful for tasks involving large texts, like document summarization or long- form question-answering. Universal Transformer The Universal Transformer, a variation of the standard model, 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 12/15 adds a recurrent inductive bias to the architecture. This means that, unlike the original Transformer, which processes each position in the input sequence in parallel but independently, the Universal Transformer revisits each position multiple times through a dynamic, data-dependent process.  This iterative processing allows the model to re\u0000ne its representations and make more accurate predictions, especially bene\u0000cial for tasks requiring deep reasoning or iterative re\u0000nement, such as complex language understanding or mathematical problem solving. T5 T5, developed by Google, adopts a unique approach by framing all NLP tasks as a text-to-text problem. Whether it’s translation, question answering, or classi\u0000cation, every task is reformulated to involve converting one type of text into another.  T5 is pre-trained on a large corpus, similar to BERT and GPT, 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 13/15 but distinguishes itself with its text-to-text framework. This universality allows T5 to handle a wide range of tasks with a single model, simplifying the process of applying the model to different NLP problems. Vision Transformer (ViT) Vision Transformer (ViT) is a novel application of the Transformer architecture to the \u0000eld of computer vision. Unlike traditional convolutional neural networks (CNNs) that process images through local receptive \u0000elds, ViT applies the Transformer’s self- attention mechanism to sequences of image patches. This allows it to capture global dependencies within the image, leading to highly effective image classi\u0000cation and recognition tasks.  ViT’s success has spurred interest in applying Transformers beyond NLP, showcasing their versatility and potential in other domains of AI. VisualBERT 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 14/15 VisualBERT extends the Transformer model to incorporate both visual and textual inputs, making it ideal for tasks involving multimodal data, such as image captioning and visual question answering. This model is pre- trained on a dataset of text-image pairs, allowing it to learn the associations between visual content and natural language.  VisualBERT can understand nuanced concepts expressed in both modalities, such as identifying objects in images and comprehending their context within the accompanying text. VisualBERT’s multimodal nature makes it a powerful tool in applications where understanding the interplay between text and image is crucial. It is an early predecessor for modern multi- modal systems such as Microsoft Copilot and Google Bard. Conclusion The Transformer model represents a paradigm shift in machine learning, especially within the 4/8/24, 6:15 P M Transformer Model: The B asics and 7 Models You S hould K now chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 15/15 domain of NLP. Its unique architecture that enables parallel processing, coupled with its self- attention mechanism, allows it to handle the nuances of language with unprecedented effectiveness.  The ability to capture word context without being constrained by sequence order has led to more accurate and ef\u0000cient models, today used by billions around the world, such as BERT and GPT. From enhancing machine translation to generating human- like text and code with modern LLM technology, the Transformer model underpins numerous AI applications, shaping the way humanity interacts with technology.","libVersion":"0.3.2","langs":""}
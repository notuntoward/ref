{"path":"lit/lit_sources.backup/Kurenkov20briefHistoryNN.pdf","text":"OVERVIEWS / Deep Learning A Brief History of Neural Nets and Deep Learning The story of how neural nets evolved from the earliest days of AI to now. Prologue: The Deep Learning Tsunami This may sound hyperbolic - to say the established methods of an entire \u0000eld of research are quickly being superseded by a new discovery, as if hit by a research ‘tsunami’. But, this catastrophic language is appropriate for describing the meteoric rise of Deep Learning over the last several years - a rise characterized by drastic improvements over reigning approaches towards the hardest problems in AI, massive investments from industry giants such as Google, and exponential growth in research publications (and Machine Learning graduate students). Having taken several classes on Machine Learning, and even used it in undergraduate research, I could not help but wonder if this new ‘Deep Learning’ was anything fancy or just a scaled up version of the ‘arti\u0000cial neural nets’ that were already developed by the late 8 0s . And let me tell you, the answer is quite a story - the story of not just neural nets, not just of a sequence of research breakthroughs that make Deep Learning somewhat more interesting than ‘big neural nets’ (that I will attempt to explain in a way that just about anyone can understand), but most of all of how several unyielding researchers made it through dark decades of banishment to \u0000nally redeem neural nets and achieve the dream of Deep Learning. Disclaimer: more in depth sources, corrections, origins » Sections I. Prologue: The Deep Learning Tsunami II. Part 1: The Beginnings (1950s-1980s) 1. The Centuries Old Machine Learning Algorithm 2. The Folly of False Promises 3. The Thaw of the AI Winter “ D e e p L e a r n i n g w a v e s h a v e l a p p e d a t t h e s h o r e s o f c o m p u t a t i o n a l l i n g u i s t i c s f o r s e v e r a l y e a r s n o w , b u t 2 0 1 5 s e e m s l i k e t h e y e a r w h e n t h e f u l l f o r c e o f t h e t s u n a m i h i t t h e m a j o r N a t u r a l L a n g u a g e P r o c e s s i n g ( N L P ) c o n f e r e n c e s . ” - D r . C h r i s t o p h e r D . M a n n i n g , D e c 2 0 1 5 1 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 1/39 III. Part 2: Neur al Nets Blossom (1980s-2000s) 1. Neural Nets Gain Vision 2. Neural Nets Go Unsupervised 3. Neural Nets Gain Beliefs 4. Neural Nets Make Decisions 5. Neural Nets Get Loopy \u0000. Neural Nets Start to Speak 7. A New Winter Dawns IV. Part 3: Deep Learning (2000s-2010s) 1. The Funding of More Layers 2. The Development of Big Data 3. The Importance of Brute Force 4. The Deep Learning Equation V. Epilogue: The Decade of Deep Learning Part 1 : The Beginnings (1 9 5 0 s-1 9 8 0 s) The beginning of a story spanning half a century, about how we learned to make computers learn. In this part, we shall cover the birth of neural nets with the Perceptron in 1958, the AI Winter of the 70s , and neural nets’ return to popularity with backpropagation in 1986. The Centuries Old Machine Learning Algorithm Let’s start with a brief primer on what Machine Learning is. Take some points on a 2D graph, and draw a line that \u0000ts them as well as possible. What you have just done is generalized from a few example of pairs of input values (x) and output values (y) to a general function that can map any input value to an output value. This is known as linear regression, and it is a wonderful little 200 year old technique for extrapolating a general function from some set of input-output pairs. And here’s why having such a technique is wonderful: there is an incalculable number of functions that are hard to develop equations for directly, but are easy to collect examples of input and output pairs for in the real world - for instance, the function mapping an input of recorded audio of a spoken word to an output of what that spoken word is. Linear regression is a bit too wimpy a technique to solve the problem of speech recognition, but what it does is essentially what supervised Machine Learning is all about: ‘learning’ a function given a training set of examples, where each example is a pair of an input and output from the function (we shall touch on the unsupervised \u0000avor in a little while). In particular, machine learning methods should derive a function that can generalize well to inputs not in the training set, since then we can actually apply it to inputs for which we do not have an output. For instance, Google’s current speech recognition technology is powered by Machine Learning with a massive training set, but not nearly as big a training set as all the possible speech inputs you might task your phone with understanding.Linear regression (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 2/39 This generalization principle is so important that there is almost always a test set of data (more examples of inputs and outputs) that is not part of the training set. The separate set can be used to evaluate the effectiveness of the machine learning technique by seeing how many of the examples the method correctly computes outputs for given the inputs. The nemesis of generalization is over\u0000tting - learning a function that works really well for the training set but badly on the test set. Since machine learning researchers needed means to compare the effectiveness of their methods, over time there appeared standard datasets of training and testing sets that could be used to evaluate machine learning algorithms. Okay okay, enough de\u0000nitions. Point is - our line drawing exercise is a very simple example of supervised machine learning: the points are the training set (X is input and Y is output), the line is the approximated function, and we can use the line to \u0000nd Y values for X values that don’t match any of the points we started with. Don’t worry, the rest of this history will not be nearly so dry as all this. Here we go. The Folly of False Promises Why have all this prologue with linear regression, since the topic here is ostensibly neural nets? Well, in fact linear regression bears some resemblance to the \u0000rst idea conceived speci\u0000cally as a method to make machines learn: Frank Rosenblatt’s Perceptron . A psychologist, Rosenblatt conceived of the Percetron as a simpli\u0000ed mathematical model of how the neurons in our brains operate: it takes a set of binary inputs (nearby neurons), multiplies each input by a continuous valued weight (the synapse strength to each nearby neuron), and thresholds the sum of these weighted inputs to output a 1 if the sum is big enough and otherwise a 0 (in the same way neurons either \u0000re or do not). Most of the inputs to a Perceptron are either some data or the output of another Perceptron, but an extra detail is that Perceptrons also have one special ‘bias’ input, which just has a value of 1 and basically ensures that more functions are computable with the same input by being able to offset the summed value. This model of the neuron built on the work of Warren McCulloch and Walter Pitts Mcculoch-Pitts , who showed that a neuron model that sums binary inputs and outputs a 1 if the sum exceeds a certain threshold value, and otherwise outputs a 0, can model the basic OR/AND/NOT functions. This, in the early days of AI, was a big deal - the predominant thought at the time was that making computers able to perform formal logical reasoning would essentially solve AI. 2A diagram showing how the Perceptron works. (Source) 3Another diagram, showing the biological inspiration. The activation function is what people now call the non-linear function applied to the weightedinput sum to produce the output of the arti\u0000cial neuron - in the case of Rosenblatt's Perceptron, the function just a thresholding operation. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 3/39 However, the Mcculoch-Pitts model lacked a mechanism for learning, which was crucial for it to be usable for AI. This is where the Perceptron excelled - Rosenblatt came up with a way to make such arti\u0000cial neurons learn, inspired by the foundational work of Donald Hebb. Hebb put forth the unexpected and hugely in\u0000uential idea that knowledge and learning occurs in the brain primarily through the formation and change of synapses between neurons - concisely stated as Hebb’s Rule: The Perceptron did not follow this idea exactly, but having weights on the inputs allowed for a very simple and intuitive learning scheme: given a training set of input-output examples the Perceptron should ‘learn’ a function from, for each example increase the weights if the Perceptron output for that example’s input is too low compared to the example, and otherwise decrease the weights if the output is too high. Stated ever so slightly more formally, the algorithm is as follows: 1. Start off with a Perceptron having random weights and a training set 2. For the inputs of an example in the training set, compute the Perceptron’s output 3. If the output of the Perceptron does not match the output that is known to be correct for the example: If the output should have been 0 but was 1, decrease the weights that had an input of 1. If the output should have been 1 but was 0, increase the weights that had an input of 1. 4. Go to the next example in the training set and repeat steps 2-4 until the Perceptron makes no more mistakes This procedure is simple, and produces a simple result: an input linear function (the weighted sum), just as with linear regression, ‘squashed’ by a non-linear activation function (the thresholding of the sum). It’s \u0000ne to threshold the sum when the function can only have a \u0000nite set of output values (as with logical functions, in which case there are only two - True/1 and False/0) , and so the problem is not so much to generate a continuous-numbered output for any set of inputs - regression - as to categorize the inputs with a correct label - classi\u0000cation. Rosenblatt implemented the idea of the Perceptron in custom hardware (this being before fancy programming languages were in common use), and showed it could be used to learn to classify simple shapes correctly with 20x20 pixel-like inputs. And so, machine learning was born - a computer was built that could approximate a function given known input and output pairs from it. In this case it learned a little toy function, but it was not di\u0000cult to envision useful applications such as converting the mess that is human handwriting into machine-readable text. But wait, so far we’ve only seen how one Perceptron is able to learn to output a one or a zero - how can this be extended to work for classi\u0000cation tasks with many categories, such as human handwriting (in which there are many letters and digits as the categories)? This is impossible for one Perceptron, since it has only one output, but functions with multiple outputs can be learned by having multiple Perceptrons in a layer, such that all these Perceptrons receive the same input and each one is responsible for one output of the function. Indeed, neural nets (or, formally, ‘Arti\u0000cial Neural Networks’ - ANNs) are nothing more than layers of Perceptrons - or neurons, or units, as they are usually called today - and at this stage there was just one layer - the output layer. So, a prototypical example of neural net use is to classify an image of a handwritten digit. The inputs 4 “ W h e n a n a x o n o f c e l l A i s n e a r e n o u g h t o e x c i t e a c e l l B a n d r e p e a t e d l y o r p e r s i s t e n t l y t a k e s p a r t i n f i r i n g i t , s o m e g r o w t h p r o c e s s o r m e t a b o l i c c h a n g e t a k e s p l a c e i n o n e o r b o t h c e l l s s u c h t h a t A ’ s e f f i c i e n c y , a s o n e o f t h e c e l l s f i r i n g B , i s i n c r e a s e d . ”'Mark I Perceptron at the Cornell Aeronautical Laboratory', hardware implementation of the \u0000rst Perceptron (Source: Wikipedia / Cornell Library) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 4/39 are the pixels of the image , and there are 10 output neurons with each one corresponding to one of the 10 possible digit values. In this case only one of the 10 neurons output 1, the highest weighted sum is taken to be the correct output, and the rest output 0. It is also possible to conceive of neural nets with arti\u0000cial neurons different from the Perceptron. For instance, the thresholding activation function is not strictly necessary; Bernard Widrow and Tedd Hoff soon explored the option of just outputting the weight input in 1960 with “An adaptive “ADALINE” neuron using chemical “memistors” , and showed how these ‘Adaptive Linear Neurons’ could be incorporated into electrical circuits with ‘memistors’ - resistors with memory. They also showed that not having the threshold activation function is mathematically nice, because the neuron’s learning mechanism can be formally based on minimizing the error through good ol’ calculus. See, with the neuron’s function not being made weird by this sharp thresholding jump from 0 to 1, a measure of how much the error changes when each weight is changed (the derivative) can be used to drive the error down and \u0000nd the optimal weight values. As we shall see, \u0000nding the right weights using the derivatives of the training error with respect to each weight is exactly how neural nets are typically trained to this day. Aside: a bit more on the math » If we think about ADALINE a bit more we will come up with a further insight: \u0000nding a set of weights for a number of inputs is really just a form of linear regression. And again, as with linear regression, this would not be enough to solve the complex AI problems of Speech Recognition or Computer Vision. What McCullough and Pitts and Rosenblatt were really excited about is the broad idea of Connectionism: that networks of such simple computational units can be vastly more powerful and solve the hard problems of AI. And, Rosenblatt said as much, as in this frankly ridiculous New York Times quote from the time : Or, have a look at this TV segment from the time:A neural net with multiple outputs. 5 6 “ T h e N a v y r e v e a l e d t h e e m b r y o o f a n e l e c t r o n i c c o m p u t e r t o d a y t h a t i t e x p e c t s w i l l b e a b l e t o w a l k , t a l k , s e e , w r i t e , r e p r o d u c e i t s e l f a n b e c o n s c i o u s o f i t s e x i s t e n c e … D r . F r a n k R o s e n b l a t t , a r e s e a r c h p s y c h o l o g i s t a t t h e C o r n e l l A e r o n a u t i c a l L a b o r a t o r y , B u f f a l o , s a i d P e r c e p t r o n s m i g h t b e f i r e d t o t h e p l a n e t s a s m e c h a n i c a l s p a c e e x p l o r e r s ” 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 5/39 This sort of talk no doubt irked other researchers in AI, many of whom were focusing on approaches based on manipulation of symbols with concrete rules that followed from the mathematical laws of logic. Marvin Minsky, founder of the MIT AI Lab, and Seymour Papert, director of the lab at the time, were some of the researchers who were skeptical of the hype and in 1969 published their skepticism in the form of rigorous analysis on of the limitations of Perceptrons in a seminal book aptly named Perceptrons . Interestingly, Minksy may have actually been the \u0000rst researcher to implement a hardware neural net with 1951’ s SNARC (Stochastic Neural Analog Reinforcement Calculator) , which preceded Rosenblatt’s work by many years. But the lack of any trace of his work on this system and the critical nature of the analysis in Perceptrons suggests that he concluded this approach to AI was a dead end. The most widely discussed element of this analysis is the elucidation of the limits of a Perceptron - they could not, for instance, learn the simple boolean function XOR because it is not linearly separable. Though the history here is vague, this publication is widely believed to have helped usher in the \u0000rst of the AI Winters - a period following a massive wave of hype for AI characterized by disillusionment that causes a freeze to funding and publications. The Thaw of the AI Winter So, things were not good for neural nets. But why? The idea, after all, was to combine a bunch of simple mathematical neurons to do complicated things, not to use a single one. In other terms, instead of just having one output layer, to send an input to arbitrarily many neurons which are called a hidden layer because their output acts as input to another hidden layer or the output layer of neurons. Only the output layer’s output is ‘seen’ - it is the answer of the neural net - but all the intermediate computations done by the hidden layer(s) can tackle vastly more complicated problems than just a single layer.The stuff promised in this video - still not really around. 7 8Visualization of the limitations of Perceptrons. Finding a linear function on the inputs X,Y to correctly ouput + or - is equivalent to drawing a line on this2D graph separating all + cases from - cases; clearly, for the third case this is impossible. 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 6/39 The reason hidden layers are good, in basic terms, is that the hidden layers can \u0000nd features within the data and allow following layers to operate on those features rather than the noisy and large raw data. For example, in the very common neural net task of \u0000nding human faces in an image, the \u0000rst hidden layer could take in the raw pixel values and \u0000nd lines, circles, ovals, and so on within the image. The next layer would receive the position of these lines, circles, ovals, and so on within the image and use those to \u0000nd the location of human faces - much easier! And people, basically, understood this. In fact, until recently machine learning techniques were commonly not applied directly to raw data inputs such as images or audio. Instead, machine learning was done on data after it had passed through feature extraction - that is, to make learning easier machine learning was done on preprocessed data from which more useful features such as angles or shapes had been already extracted. Aside: why have non-linear activation functions » So, it is important to note Minsky and Papert’s analysis of Perceptrons did not merely show the impossibility of computing XOR with a single Perceptron, but speci\u0000cally argued that it had to be done with multiple layers of Perceptrons - what we now call multilayer neural nets - and that Rosenblatt’s learning algorithm did not work for multiple layers. And that was the real problem: the simple learning rule previously outlined for the Perceptron does not work for multiple layers. To see why, let’s reiterate how a single layer of Perceptrons would learn to compute some function: 1. A number of Perceptrons equal to the number of the function’s outputs would be started off with small initial weights 2. For the inputs of an example in the training set, compute the Perceptrons’ output 3. For each Perceptron, if the output does not match the example’s output, adjust the weights accordingly 4. Go to the next example in the training set and repeat steps 2-4 until the Perceptrons no longer make mistakesNeural net with two hidden layers (Excellent Source)Visualization of traditional handcrafted feature extraction. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 7/39 The reason why this does not work for multiple layers should be intuitively clear: the example only speci\u0000es the correct output for the \u0000nal output layer, so how in the world should we know how to adjust the weights of Perceptrons in layers before that? The answer, despite taking some time to derive, proved to be once again based on age-old calculus: the chain rule. The key realization was that if the neural net neurons were not quite Perceptrons, but were made to compute the output with an activation function that was still non-linear but also differentiable, as with Adaline, not only could the derivative be used to adjust the weight to minimize error, but the chain rule could also be used to compute the derivative for all the neurons in a prior layer and thus the way to adjust their weights would also be known. Or, more simply: we can use calculus to assign some of the blame for any training set mistakes in the output layer to each neuron in the previous hidden layer, and then we can further split up blame if there is another hidden layer, and so on - we backpropagate the error. And so, we can \u0000nd how much the error changes if we change any weight in the neural net, including those in the hidden layers, and use an optimization technique (for a long time, typically stochastic gradient descent) to \u0000nd the optimal weights to minimize the error. Backpropagation was derived by multiple researchers in the early 60’ s and implemented to run on computers much as it is today as early as 1970 by Seppo Linnainmaa , but Paul Werbos was \u0000rst in the US to propose that it could be used for neural nets after analyzing it in depth in his 1974 PhD Thesis . Interestingly, as with Perceptrons he was loosely inspired by work modeling the human mind, in this case the psychological theories of Freud as he himself recounts : Despite solving the question of how multilayer neural nets could be trained, and seeing it as such while working on his PhD thesis, Werbos did not publish on the application of backprop to neural nets until 1982 due to the chilling effects of the AI Winter. In fact, Werbos thought the approach would make sense for solving the problems pointed out in Perceptrons, but the community at large lost any faith in tackling those problems:The basic idea of backpropagation. (Source) 9 10 11 “ I n 1 9 6 8 , I p r o p o s e d t h a t w e s o m e h o w i m i t a t e F r e u d ’ s c o n c e p t o f a b a c k w a r d s f l o w o f c r e d i t a s s i g n m e n t , f l o w i n g b a c k f r o m n e u r o n t o n e u r o n … I e x p l a i n e d t h e r e v e r s e c a l c u l a t i o n s u s i n g a c o m b i n a t i o n o f i n t u i t i o n a n d e x a m p l e s a n d t h e o r d i n a r y c h a i n r u l e , t h o u g h i t w a s a l s o e x a c t l y a t r a n s l a t i o n i n t o m a t h e m a t i c s o f t h i n g s t h a t F r e u d h a d p r e v i o u s l y p r o p o s e d i n h i s t h e o r y o f p s y c h o d y n a m i c s ! ” 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 8/39 It seems that it was because of this lack of academic interest that it was not until more than a decade later, in 1986, that this approach was popularized in “Learning representations by back-propagating errors” by David Rumelhart, Geoffrey Hinton, and Ronald Williams . Despite the numerous discoveries of the method (the paper even explicitly mentions David Parker and Yann LeCun as two people who discovered it beforehand) the 1986 publication stands out for how concisely and clearly the idea is stated. In fact, as a student of Machine Learning it is easy to see that the description in their paper is essentially identical to the way the concept is still explained in textbooks and AI classes. A retrospective in IEEE echoes this notion: But the three authors went much further than just present this new learning algorithm. In the same year they published the much more in-depth “Learning internal representations by error propagation” , which speci\u0000cally addressed the problems discussed by Minsky in Perceptrons. Though the idea was conceived by people in the past, it was precisely this formulation in 1986 that made it widely understood how multilayer neural nets could be trained to tackle complex learning problems. And so, neural nets were back! Next, we shall see how just a few years later backpropagation and some other tricks discussed in “Learning internal representations by error propagation” were applied to a very signi\u0000cant problem: enabling computers to read human handwriting. Part 2 : Neural Nets Blossom (1 9 8 0 s-2 0 0 0 s) Neural Nets Gain Vision With the secret to training multilayer neural nets uncovered, the topic was once again ember-hot and the lofty ambitions of Rosenblatt seemed to perhaps be in reach. It took only until 1989 for another key \u0000nding now universally cited in textbooks and lectures to be published : “Multilayer feedforward networks are universal approximators”. Essentially, it mathematically “ M i n s k y ’ s b o o k w a s b e s t k n o w n f o r a r g u i n g t h a t ( 1 ) w e n e e d t o u s e M L P s [ m u l t i l a y e r p e r c e p t r i o n s , a n o t h e r t e r m f o r m u l t i l a y e r n e u r a l n e t s ] e v e n t o r e p r e s e n t s i m p l e n o n l i n e a r f u n c t i o n s s u c h a s t h e X O R m a p p i n g ; a n d ( 2 ) n o o n e o n e a r t h h a d f o u n d a v i a b l e w a y t o t r a i n M L P s g o o d e n o u g h t o l e a r n s u c h s i m p l e f u n c t i o n s . M i n s k y ’ s b o o k c o n v i n c e d m o s t o f t h e w o r l d t h a t n e u r a l n e t w o r k s w e r e a d i s c r e d i t e d d e a d - e n d – t h e w o r s t k i n d o f h e r e s y . W i d r o w h a s s t r e s s e d t h a t t h i s p e s s i m i s m , w h i c h s q u a s h e d t h e e a r l y “ p e r c e p t r o n ” s c h o o l o f A I , s h o u l d n o t r e a l l y b e b l a m e d o n M i n s k y . M i n s k y w a s m e r e l y s u m m a r i z i n g t h e e x p e r i e n c e o f h u n d r e d s o f s i n c e r e r e s e a r c h e r s w h o h a d t r i e d t o f i n d g o o d w a y s t o t r a i n M L P s , t o n o a v a i l . T h e r e h a d b e e n i s l a n d s o f h o p e , s u c h a s t h e a l g o r i t h m w h i c h R o s e n b l a t t c a l l e d “ b a c k p r o p a g a t i o n ” ( n o t a t a l l t h e s a m e a s w h a t w e n o w c a l l b a c k p r o p a g a t i o n ! ) , a n d A m a r i ’ s b r i e f s u g g e s t i o n t h a t w e m i g h t c o n s i d e r l e a s t s q u a r e s [ w h a t i s t h e b a s i s o f s i m p l e l i n e a r r e g r e s s i o n ] a s a w a y t o t r a i n n e u r a l n e t w o r k s ( w i t h o u t d i s c u s s i o n o f h o w t o g e t t h e d e r i v a t i v e s , a n d w i t h a w a r n i n g t h a t h e d i d n o t e x p e c t m u c h f r o m t h e a p p r o a c h ) . B u t t h e p e s s i m i s m a t t h a t t i m e b e c a m e t e r m i n a l . I n t h e e a r l y 1 9 7 0 s , I d i d i n f a c t v i s i t M i n s k y a t M I T . I p r o p o s e d t h a t w e d o a j o i n t p a p e r s h o w i n g t h a t M L P s c a n i n f a c t o v e r c o m e t h e e a r l i e r p r o b l e m s … B u t M i n s k y w a s n o t i n t e r e s t e d ( 1 4 ) . I n f a c t , n o o n e a t M I T o r H a r v a r d o r a n y p l a c e I c o u l d f i n d w a s i n t e r e s t e d a t t h e t i m e . ” 12 13 “ U n f o r t u n a t e l y , W e r b o s ’ s w o r k r e m a i n e d a l m o s t u n k n o w n i n t h e s c i e n t i f i c c o m m u n i t y . I n 1 9 8 2 , P a r k e r r e d i s c o v e r e d t h e t e c h n i q u e [ 3 9 ] a n d i n 1 9 8 5 , p u b l i s h e d a r e p o r t o n i t a t M . I . T . [ 4 0 ] . N o t l o n g a f t e r P a r k e r p u b l i s h e d h i s f i n d i n g s , R u m e l h a r t , H i n t o n , a n d W i l l i a m s [ 4 1 ] , [ 4 2 ] a l s o r e d i s c o v e r e d t h e t e c h n i q u e s a n d , l a r g e l y a s a r e s u l t o f t h e c l e a r f r a m e w o r k w i t h i n w h i c h t h e y p r e s e n t e d t h e i r i d e a s , t h e y f i n a l l y s u c c e e d e d i n m a k i n g i t w i d e l y k n o w n . ” 14Yann LeCun's LeNet demonstrated (Source).15 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 9/39 proved that multiple layers allow neural nets to theoretically implement any function, and certainly XOR. But, this is mathematics, where you could imagine having endless memory and computation power should it be needed - did backpropagation allow neural nets to be used for anything in the real world? Oh yes. Also in 1989, Yann LeCun et al. at the AT&T Bell Labs demonstrated a very signi\u0000cant real-world application of backpropagation in \"”Backpropagation Applied to Handwritten Zip Code Recognition” . You may think it fairly unimpressive for a computer to be able to correctly understand handwritten digits, and these days it is indeed quite quaint, but prior to the publication the messy and inconsistent scrawls of us humans proved a major challenge to the much more tidy minds of computers. The publication, working with a large dataset from the US Postal Service, showed neural nets were entirely capable of this task. And much more importantly, it was \u0000rst to highlight the practical need for a key modi\u0000cations of neural nets beyond plain backpropagation toward modern deep learning: Or, more concretely: the \u0000rst hidden layer of the neural net was convolutional - instead of each neuron having a different weight for each pixel of the input image (4 0x60= 2400 weights), the neurons only have a small set of weights (5x5= 25) that were applied a whole bunch of small subsets of the image of the same size. So, for instance instead of having 4 different neurons learn to detect 45 degree lines in each of the 4 corners of the input image, a single neuron could learn to detect 45 degree lines on subsets of the image and do that everywhere within it. Layers past the \u0000rst work in a similar way, but take in the ‘local’ features found in the previous hidden layer rather than pixel images, and so ‘see’ successively larger portions of the image since they are combining information about increasingly larger subsets of the image. Finally, the last two layers are just plain normal neural net layers that use the higher-order larger features generated by the convolutional layers to determine which digit the input image corresponds to. The method proposed in this 1989 paper went on to be the basis of nationally deployed check- reading systems, as demonstrated by LeCun in this gem of a video: 16 “ C l a s s i c a l w o r k i n v i s u a l p a t t e r n r e c o g n i t i o n h a s d e m o n s t r a t e d t h e a d v a n t a g e o f e x t r a c t i n g l o c a l f e a t u r e s a n d c o m b i n i n g t h e m t o f o r m h i g h e r o r d e r f e a t u r e s . S u c h k n o w l e d g e c a n b e e a s i l y b u i l t i n t o t h e n e t w o r k b y f o r c i n g t h e h i d d e n u n i t s t o c o m b i n e o n l y l o c a l s o u r c e s o f i n f o r m a t i o n . D i s t i n c t i v e f e a t u r e s o f a n o b j e c t c a n a p p e a r a t v a r i o u s l o c a t i o n o n t h e i n p u t i m a g e . T h e r e f o r e i t s e e m s j u d i c i o u s t o h a v e a s e t o f f e a t u r e d e t e c t o r s t h a t c a n d e t e c t a p a r t i c u l a r i n s t a n c e o f a f e a t u r e a n y w h e r e o n t h e i n p u t p l a c e . S i n c e t h e p r e c i s e l o c a t i o n o f a f e a t u r e i s n o t r e l e v a n t t o t h e c l a s s i f i c a t i o n , w e c a n a f f o r d t o l o s e s o m e p o s i t i o n i n f o r m a t i o n i n t h e p r o c e s s . N e v e r t h e l e s s , a p p r o x i m a t e p o s i t i o n i n f o r m a t i o n m u s t b e p r e s e r v e d , t o a l l o w t h e n e x t l e v e l s t o d e t e c t h i g h e r o r d e r , m o r e c o m p l e x f e a t u r e s ( F u k u s h i m a 1 9 8 0 ; M o z e r 1 9 8 7 ) . ”A visualization of how this neural net works. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 10/39 The reason for why this is helpful is intuitively if not mathematically clear: without such constraints the network would have to learn the same simple things (such as detecting 45 degree lines, small circles, etc) a whole bunch of times for each portion of the image. But with the constraint there, only one neuron would need to learn each simple feature - and with far fewer weights overall, it could do so much faster! Moreover, since the pixel-exact locations of such features do not matter the neuron could basically skip neighboring subsets of the image - subsampling, now known as a type of pooling - when applying the weights, further reducing the training time. The addition of these two types of layers - convolutional and pooling layers - are the primary distinctions of Convolutional Neural Nets (CNNs/ConvNets) from plain old neural nets. At that time, the convolution idea was called ‘weight sharing’, and it was actually discussed in the 1986 extended analysis of backpropagation by Rumelhart, Hinton, and Williams . Actually, the credit goes even further back - Minsky and Papert’s 19 69 analysis of Perceptrons was thorough enough to pose a problem that motivated this idea. But, as before, others have also independently explored the concept - namely, Kunihiko Fukushima in 1980 with his notion of the Neurocognitron . And, as before, the ideas behind it drew inspiration from studies of the brain:A nice visualization of CNN operation (Source) 17 18 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 11/39 LeCun continued to be a major proponent of CNNs at Bell Labs, and his work on them resulted in major commercial use for check-reading in the mid 90s - his talks and interviews often include the fact that “At some point in the late 1990s , one of these systems was reading 10 to 20% of all the checks in the US.” . Neural Nets Go Unsupervised Automating the rote and utterly uninteresting task of reading checks is a great instance of what Machine Learning can be used for. Perhaps a less predictable application? Compression. Meaning, of course, \u0000nding a smaller representation of some data from which the original data can be reconstructed. Learned compression may very well outperform stock compression schemes, when the learning algorithm can \u0000nd features within the data stock methods would miss. And it is very easy to do - just train a neural net with a small hidden layer to just output the input: This is an autoencoder neural net, and is a method for learning compression - e\u0000ciently translating (encoding) data to a compact format and back to itself (auto). See, the output layer computes its outputs, which ideally are the same as the input to the neural net, using only the hidden layer’s outputs. Since the hidden layer has fewer outputs than does the input layer, the output of the hidden layer is the compressed representation of the input data, which can be reconstructed with the output layer. Notice a neat thing here: the only thing we need for training is some input data. This is in contrast to the requirement of supervised machine learning, which needs a training set of input-output pairs (labeled data) in order to approximate a function that can compute such outputs from such inputs. And indeed, autoencoders are not a form of supervised learning; they are a form of unsupervised learning, which only needs a set of input data (unlabeled data) in order to \u0000nd some hidden structure within that data. In other words, unsupervised learning does not approximate a function so much as it derives one from the “ A c c o r d i n g t o t h e h i e r a r c h y m o d e l b y H u b e l a n d W i e s e l , t h e n e u r a l n e t w o r k i n t h e v i s u a l c o r t e x h a s a h i e r a r c h y s t r u c t u r e : L G B ( l a t e r a l g e n i c u l a t e b o d y ) - > s i m p l e c e l l s - > c o m p l e x c e l l s - > l o w e r o r d e r h y p e r c o m p l e x c e l l s - > h i g h e r o r d e r h y p e r c o m p l e x c e l l s . I t i s a l s o s u g g e s t e d t h a t t h e n e u r a l n e t w o r k b e t w e e n l o w e r o r d e r h y p e r c o m p l e x c e l l s a n d h i g h e r o r d e r h y p e r c o m p l e x c e l l s h a s a s t r u c t u r e s i m i l a r t o t h e n e t w o r k b e t w e e n s i m p l e c e l l s a n d c o m p l e x c e l l s . I n t h i s h i e r a r c h y , a c e l l i n a h i g h e r s t a g e g e n e r a l l y h a s a t e n d e n c y t o r e s p o n d s e l e c t i v e l y t o a m o r e c o m p l i c a t e d f e a t u r e o f t h e s t i m u l u s p a t t e r n , a n d , a t t h e s a m e t i m e , h a s a l a r g e r r e c e p t i v e f i e l d , a n d i s m o r e i n s e n s i t i v e t o t h e s h i f t i n p o s i t i o n o f t h e s t i m u l u s p a t t e r n . … H e n c e , a s t r u c t u r e s i m i l a r t o t h e h i e r a r c h y m o d e l i s i n t r o d u c e d i n o u r m o d e l . ” 19An autoencoder neural net. (Source)A more explicit view of an autoencoder compression. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 12/39 input data to another useful representation of that data. In this case, this representation is just a smaller one from which the original data can still be reconstructed, but it can also be used for \u0000nding groups of similar data (clustering) or other inference of latent variables (some aspect that is known to exist for the data but the value of which is not known). There were other unsupervised applications of neural networks explored prior to and after the discovery of backpropagation, most notably Self Organizing Maps , which produce a low-dimensional representation of data good for visualization, and Adapative Resonance Theory , which can learn to classify arbitrary input data without being told correct classi\u0000cations. If you think about it, it is intuitive that quite a lot can be learned from unlabeled data. Say you have a dataset of a bunch of images of handwritten digits, without labels of which digit each image corresponds to. Well, an image with some digit in that dataset most likely looks most like all the other images with that same digit, and so though a computer may not know which digit all those images correspond to, it should still be able to \u0000nd that they all correspond to the same one. This, pattern recognition, is really what most of machine learning is all about, and arguably also is the basis for the great powers of the human brain. But, let us not digress from our exciting deep learning journey, and get back to autoencoders. As with weight-sharing, the idea of autoencoders was \u0000rst discussed in the aforementioned extensive 1986 analysis of backpropagation , and as with weight-sharing it resurfaced in more research in the following years , including by Hinton himself . This paper, with the fun title “Autoencoders, Minimum Description Length, and Helmholts Free Energy”, posits that “A natural approach to unsupervised learning is to use a model that de\u0000nes probability distribution over observable vectors” and uses a neural net to learn such a model. So here’s another neat thing you can do with neural nets: approximate probability distributions. Neural Nets Gain Beliefs In fact, before being co-author of the seminal 1986 paper on backpropagation learning algorithm, Hinton worked on a neural net approach for learning probability distributions in the 1985 “A Learning Algorithm for Boltzmann Machines” . Boltzmann Machines are networks just like neural nets and have units that are very similar to Perceptrons, but instead of computing anClustering, a very common unsupervised learning application. (Source) 20 21Self Organizing Maps - mapping a large vector of inputs into a grid of neuron outputs, where each output is a cluster. Nearby neurons represent similarclusters. (Source) 17 22 23 24 25 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 13/39 output based on inputs and weights, each unit in the network can compute a probability of it having a value of 1 or 0 given the values of connected units and weights. The units are therefore stochastic - they behave according to a probability distribution, rather than in a known deterministic way. The Boltzmann part refers to a probability distribution that has to do with the states of particles in a system based the particles’ energy and on the thermodynamic temperature of that system. This distribution de\u0000nes not only the mathematics of the Boltzmann machines, but also the interpretation - the units in the network themselves have energies and states, and learning is done by minimizing the energy of the system - a direct inspiration from thermodynamics. Though a bit unintuitive, this energy-based interpretation is actually just one example of an energy-based model, and \u0000ts in the energy-based learning theoretical framework with which many learning algorithms can be expressed . Aside: a bit more Energy Based Models » Back to Boltzmann Machines. When such units are put together into a network, they form a graph, and so are a graphical model of data. Essentially, they can do something very similar to normal neural nets: some hidden units compute the probability of some hidden variables (the outputs - classi\u0000cations or features for data) given known values of visible units that represent visible variables (the inputs - pixels of images, characters in text, etc.). In our classic example of classifying images of digits, the hidden variables are the actual digit values, and the visible variables are the pixels of the image; given an image of the digit ‘1’ as input, the value of visible units is known and the hidden unit modeling the probability of the image representing a ‘1’ should have a high output probability. 26A simple belief, or bayesian, network - a Boltzmann machine is basically this but with undirected/symmetric connections and trainable weights to learnthe probabilities in a particular fashion. (Source)An example Boltzmann machine. Each line has an associated weight, as with a neural net. Notice there are no layers here - everything can sort of beconnected to everything. We'll talk about this variation on neural net in a little bit... (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 14/39 So, for the classi\u0000cation task, there is now a nice way of computing the probability of each category. This is very analogous to actually computing the output values of a normal classi\u0000cation neural net, but these nets have another neat trick: they can generate plausible looking input data. This follows from the probability equations involved - not only does the net learn the probabilities of values for the hidden variables given known values for the visible variables, but also the inverse of that - visible probabilities given known hidden values. So, if we want to generate a ‘1’ digit image, the units corresponding to the pixel variables have known probabilities of outputting a 1 and an image can be probabilistically generated; these networks are generative graphical models. Though it is possible to do supervised learning with very similar goals as normal neural nets, the unsupervised learning task of learning a good generative model - probabilistically learning the hidden structure of some data - is commonly what these nets are used for. Most of this was not really that novel, but the learning algorithm presented and the particular formulation that enabled it were, as stated in the paper itself: Aside: more explanation of Boltzmann Machines » Without delving into the full details of the algorithm, here are some highlights: it is a variant on maximum-likelihood algorithms, which simply means that it seeks to maximize the probability of the net’s visible unit values matching with their known correct values. Computing the actual most likely value for each unit all at the same time turns out to be much too computationally expensive, so in training Gibbs Sampling - starting the net with random unit values and iteratively reassigning values to units given their connections’ values - is used to give some actual known values. When learning using a training set, the visible units are just set to have the value of the current training example, so sampling is done to get values for the hidden units. Once some ‘real’ values are sampled, we can do something similar to backpropagation - take a derivative for each weight to see how we can change so as to increase the probability of the net doing the right thing. As with neural net, the algorithm can be done both in a supervised fashion (with known values for the hidden units) or in an unsupervised fashion. Though the algorithm was demonstrated to work (notably, with the same ‘encoding’ problem that autoencoder neural nets solve), it was soon apparent that it just did not work very well - Redford M. Neal’s 1992 “Connectionist learning of belief networks” justi\u0000ed a need for a faster approach by stating that: “These capabilities would make the Boltzmann machine attractive in many applications, were it not that its learning procedure is generally seen as being painfully slow”. And so Neal introduced a similar idea in the belief net, which is essentially like a Boltzmann machine with directed, forward connections (so that there are again layers, as with the the neural nets we have seen before, and unlike the Boltzmann machine image above). Without getting into mucky probability math, this change allowed the nets to be trained with a faster learning algorithm. We actually saw a ‘belief net’ just above with the sprinkler and rain variables, and the term was chosen precisely because this sort of probability-based modeling has a close relationship to ideas from the mathematical \u0000eld of probability, in addition to its link to the \u0000eld of Machine Learning. “ P e r h a p s t h e m o s t i n t e r e s t i n g a s p e c t o f t h e B o l t z m a n n M a c h i n e f o r m u l a t i o n i s t h a t i t l e a d s t o a d o m a i n - i n d e p e n d e n t l e a r n i n g a l g o r i t h m t h a t m o d i f i e s t h e c o n n e c t i o n s t r e n g t h s b e t w e e n u n i t s i n s u c h a w a y t h a t t h e w h o l e n e t w o r k d e v e l o p s a n i n t e r n a l m o d e l w h i c h c a p t u r e s t h e u n d e r l y i n g s t r u c t u r e o f i t s e n v i r o n m e n t . T h e r e h a s b e e n a l o n g h i s t o r y o f f a i l u r e i n t h e s e a r c h f o r s u c h a l g o r i t h m s ( N e w e l l , 1 9 8 2 ) , a n d m a n y p e o p l e ( p a r t i c u l a r l y i n A r t i f i c i a l I n t e l l i g e n c e ) n o w b e l i e v e t h a t n o s u c h a l g o r i t h m s e x i s t . ” 27 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 15/39 Though this approach was an advance upon Boltzmann machines, it was still just too slow - the math for correctly deriving probabilistic relations between variables is such that a ton of computation is typically required without some simplifying tricks. And so Hinton, along with Neal and two other co-authors, soon came up with extra tricks in the 1995 “The wake-sleep algorithm for unsupervised neural networks” . These tricks called for a slightly different belief net setup, which was now deemed “The Helmholtz Machine” . Skirting the details once again, the key idea was to have separate sets of weights for inferring hidden variables from visible variables (recognition weights) and vice versa (generative weights), and to keep the directed aspect of Neal’s belief nets. This allows the training to be done much faster, while being applicable to the unsupervised and supervised learning problems of Boltzmann Machines. Aside: the gross simplifying assumption of the wake-sleep algorithm » Finally, belief nets could be trained somewhat fast! Though not quite as in\u0000uential, this algorithmic advance was a signi\u0000cant enough forward step for unsupervised training of belief nets that it could be seen as a companion to the now almost decade- old publication on backpropagation. But, by this point new machine learning methods had begun to also emerge, and people were again beginning to be skeptical of neural nets since they seemed so intuition-based and since computers were still barely able to meet their computational needs. As we’ll soon see, a new AI Winter for neural nets began just a few years later… Neural Nets Make Decisions Having discovered the application of neural nets to unsupervised learning, let us also quickly see how they were used in the third branch of machine learning: reinforcement learning. This one requires the most mathy notation to explain formally, but also has a goal that is very easy to describe informally: learn to make good decisions. Given some theoretical agent (a little software program, for instance), the idea is to make that agent able to decide on an action based on its current state, with the reception of some reward for each action and the intent of getting the maximum utility in the long term. So, whereas supervised learning tells the learning algorithm exactly what it should learn to output, reinforcement learning provides ‘rewards’ as a by-product of making good decisions over time, and does not directly tell the algorithm the correct decisions to choose. From the outset it was a very abstracted decision making model - there were a \u0000nite number of states, and a known set of actions with known rewards for each state. This made it easy to write very elegant equations for \u0000nding the optimal set of actions, but hard to apply to real problems - problems with continuous states or hard-to-de\u0000ne rewards.An explanation of belief nets. (Source) 28 29 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 16/39 This is where neural nets come in. Machine learning in general, and neural nets in particular, are good at dealing with messy continuous data or dealing with hard to de\u0000ne functions by learning them from examples. Although classi\u0000cation is the bread and butter of neural nets, they are general enough to be useful for many types of problems - the descendants of Bernard Widrow’s and Ted Hoff’s Adaline were used for adaptive \u0000lters in the context of electrical circuits, for instance. And so, following the resurgence of research caused by backpropagation, people soon devised ways of leveraging the power of neural nets to perform reinforcement learning. One of the early examples of this was solving a simple yet classic problem: the balancing of a stick on a moving platform, known to students in control classes everywhere as the inverted pendulum problem . As with adaptive \u0000ltering, this research was strongly relevant to the \u0000eld of Electrical Engineering, where control theory had been a major sub\u0000eld for many decades prior to neural nets’ arrival. Though the \u0000eld had devised ways to deal with many problems through direct analysis, having a means to deal with more complex situations through learning proved useful as evidenced by the hefty 7000 (!) citations of the 1990 “Identi\u0000cation and control of dynamical systems using neural networks” . Perhaps predictably, there was another \u0000eld separate from Machine Learning where neural nets were useful - robotics. A major example of early neural net use for robotics came from CMU’s NavLab with 1989’ s “Alvinn: An autonomous land vehicle in a neural network” :Reinforcement learning. (Source) 30The double pendulum control problem - a step up from the single pendulum version, which is a classic control and reinforcement learning task.(Source) 31 32 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 17/39 As discussed in the paper, the neural net in this system learned to control the vehicle through plain supervised learning using sensor and steering data recorded while a human drove. There was also research into teaching robots using reinforcement learning speci\u0000cally, as exempli\u0000ed by the 1993 PhD thesis “Reinforcement learning for robots using neural networks” . The thesis showed that robots could be taught behaviors such as wall following and door passing in reasonable amounts of time, which was a good thing considering the prior inverted pendulum work requires impractical lengths of training. These disparate applications in other \u0000elds are certainly cool, but of course the most research on reinforcement learning and neural nets was happening within AI and Machine Learning. And here, one of the most signi\u0000cant results in the history of reinforcement learning was achieved: a neural net that learned to be a world class backgammon player. Dubbed TD-Gammon, the neural net was trained using a standard reinforcement learning algorithm and was one of the \u0000rst demonstrations of reinforcement learning being able to outperform humans on relatively complicated tasks . And it was speci\u0000cally a reinforcement learning approach that worked here, as the same research showed just using a neural net without reinforcement learning did not work nearly as well. But, as we have seen happen before and will see happen again in AI, research hit a dead end. The predictable next problem to tackle using the TD-Gammon approach was investigated by Sebastian Thrun in the 1995 “Learning To Play the Game of Chess”, and the results were not good . Though the neural net learned decent play, certainly better than a complete novice at the game, it was still far worse than a standard computer program (GNU-Chess) implemented long before. The same was true for the other perennial challenge of AI, Go . See, TD-Gammon sort of cheated - it learned to evaluate positions quite well, and so could get away with not doing any ‘search’ over multiple future moves and instead just picking the one that led to the best next position. But the same is simply not possible in chess or Go, games which are a challenge to AI precisely because of needing to look many moves ahead and having so many possible move combinations. Besides, even if the algorithm were smarter, the hardware of the time just was not up to the task - Thrun reported that “NeuroChess does a poor job, because it spends most of 33 34The neural net that learned to play expert-level Backgammon. (Source) 35 36 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 18/39 its time computing board evaluations. Computing a large neural network function takes two orders of magnitude longer than evaluating an optimized linear evaluation function (like that of GNU-Chess).” The weakness of computers of the time relative to the needs of the neural nets was a very real issue, and as we shall see not the only one… Neural Nets Get Loopy As neat as unsupervised and reinforcement learning are, I think supervised learning is still my favorite use case for neural nets. Sure, learning probabilistic models of data is cool, but it’s simply much easier to get excited for the sorts of concrete problems solved by backpropagation. We already saw how Yann Lecun achieved quite good recognition of handwritten text (a technology which went on to be nationally deployed for check-reading, and much more a while later…), but there was another obvious and greatly important task being worked on at the same time: understanding human speech. As with writing, understanding human speech is quite di\u0000cult due to the practically in\u0000nite variation in how the same word can be spoken. But, here there is an extra challenge: long sequences of input. See, for images it’s fairly simple to crop out a single letter from an image and have a neural net tell you which letter that is, input->output style. But with audio it’s not so simple - separating out speech into characters is completely impractical, and even \u0000nding individual words within speech is less simple. Plus, if you think about human speech, generally hearing words in context makes them easier to understand than being separated. While this structure works quite well for processing things such as images one at a time, input->output style, it is not at all well suited to long streams of information such as audio or text. The neural net has no ‘memory’ with which an input can affect another input processed afterward, but this is precisely how we humans process audio or text - a string of word or sound inputs, rather than a single large input. Point being: to tackle the problem of understanding speech, researchers sought to modify neural nets to process input as a stream of input as in speech rather than one batch as with an image. One approach to this, by Alexander Waibel et. al (including Hinton), was introduced in the 1989 “Phoneme recognition using time-delay neural networks” . These time-delay neural networks (TDNN) were very similar to normal neural networks, except each neuron processed only a subset of the input and had several sets of weights for different delays of the input data. In other words, for a sequence of audio input, a ‘moving window’ of the audio is input into the network and as the window moves the same bits of audio are processed by each neuron with different sets of weights based on where in the window the bit of audio is. This is best understood with a quick illustration: In a sense, this is quite similar to what CNNs do - instead of looking at the whole input at once, each unit looks at just a subset of the input at a time and does the same computation for each small subset. The main difference here is that there is no idea of time in a CNN, and the ‘window’ of input for each neuron is always moved across the whole input image to compute a result, whereas in a TDNN there actually is sequential input and output of data. Fun fact: according to Hinton, the idea of TDNNs is what inspired LeCun to develop convolutional neural nets. But, funnily enough CNNs became essential for image processing, whereas in speech recognition TDNNs have been surpassed to another approach - recurrent neural nets (RNNs). See, all the networks that have been discussed so far have been feedforward networks, meaning that the output of neurons in a given layer acts as input to only neurons in a next layer. But, it does not have to be so - there is nothing prohibiting us brave computer scientists from connecting output of the last layer act as an input to the \u0000rst layer, or just connecting the output of a neuron to itself. By having the output of the network ‘loop’ back into the network, the problem of giving the network memory as to past inputs is solved so elegantly! 37Time delay neural networks. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 19/39 Aside: more on RNNs vs TDNNs » Well, it’s not quite so simple. Notice the problem - if backpropagation relies on ‘propagating’ the error from the output layer backward, how do things work if the \u0000rst layer connects back to the output layer? The error would go ahead and propagate from the \u0000rst layer back to the output layer, and could just keep looping through the network, in\u0000nitely. The solution, independently derived by multiple groups, is backpropagation through time. Basically, the idea is to ‘unroll’ the recurrent neural network by treating each loop through the neural network as an input to another neural network, and looping only a limited number of times. This fairly simple idea actually worked - it was possible to train recurrent neural nets. And indeed, multiple people explored the application of RNNs to speech recognition. But, here is a twist you should now be able to predict: this approach did not work very well. To \u0000nd out why, let’s meet another modern giant of Deep Learning: Yoshua Bengio. Starting work on speech recognition with neural nets around 1986, he co-wrote many papers on using ANNs and RNNs for speech recognition, and ended up working at the AT&T Bell Labs on the problem just as Yann LeCun was working with CNNs there. In fact, in 1995 they co-wrote the summary paper “Convolutional Networks for Images, Speech, and Time-Series” , the \u0000rst of many collaborations among them. But, before then Bengio wrote the 1993 “A Connectionist Approach to Speech Recognition” . Here, he summarized the general failure of effectively teaching RNNs:Diagram of a Recurrent Neural Net. Recall Boltzmann Machines from before? Surprise! Those were recurrent neural nets. (Source)The wonderfully intuitive backpropagation through time concept. (Source) 38 39 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 20/39 Neural Nets Start to Speak Speaking of Bengio, his contributions with respect to neural nets go well beyond this work with CNNs and RNNs. In particular, this history would not be complete without covering his seminal paper 2003 “A Neural Probabilistic Language Model”. As the title implies, this work had to do with using neural nets to do language modeling – a fundamental task in the \u0000eld of Natural Language Processing, which boils down to predicting what seeing some number of words and predicting what words come next (essentially, what autocomplete does). The task has by then been studied for a long time, with the classical approaches essentially being to count how often individual words and varios word combiations (otherwise known as n-grams) happen in a bunch of text, and then using these counts to estimate the probability for any given word being the one to come next: While this approach was highly succesful, it was also inherently limited, since it required seeing a word or a combination of words to predict where it would occur, and human language has an huge number of words which have exponentially many combinations. In contrast, us humans don’t only reason about language in terms of what we have seen but also what we know about each word’s meaning; if we have only seen “I have a pet dog” and then learn that there exists such as word as “cat” that is also a common pet, we would have no trouble imagining that “I have a pet cat” as something that someone is likely to say. So, how do we get computers to understand how similar different words are? After all, the meaning of words are quite subtle, and something simple like comparing words’ de\u0000nitions in terms of what words are in them is therefore not likely to work well. Well, it turns out that the subtlety of meaning can nicely be captured in a format that also easily allows for evaluating similarity: lists of numbers. Just as any (x,y) point in a 2D space has a known distance from any other point in that space, so does any point in a 100D space have known distances from all other points, and it could be said that closer points are more similar and farther points are less similar. Therefore, if we could map every word to an appropriate point in a many-dimensional space, such that similar words are mapped to nearby points, that should be quite useful for language modeling. Hinrich Schütze introduced this idea in his 1993 papers “Word Space”, in which he showed how to compute “Word Vectors” with matrix processing over counts of phrase co-occurences, resulting in the ability to \u0000nd the most similar words for any given query word: “ A l t h o u g h r e c u r r e n t n e t w o r k s c a n i n m a n y i n s t a n c e s o u t p e r f o r m s t a t i c n e t w o r k s , t h e y a p p e a r m o r e d i f f i c u l t t o t r a i n o p t i m a l l y . O u r e x p e r i m e n t s t e n d e d t o i n d i c a t e t h a t t h e i r p a r a m e t e r s s e t t l e i n a s u b o p t i m a l s o l u t i o n w h i c h t a k e s i n t o a c c o u n t s h o r t t e r m d e p e n d e n c i e s b u t n o t l o n g t e r m d e p e n d e n c i e s . F o r e x a m p l e i n e x p e r i m e n t s d e s c r i b e d i n ( c t a t i o n ) w e f o u n d t h a t s i m p l e d u r a t i o n c o n s t r a i n t s o n p h o n e m e s h a d n o t a t a l l b e e n c a p t u r e d b y t h e r e c u r r e n t n e t w o r k . … A l t h o u g h t h i s i s a n e g a t i v e r e s u l t , a b e t t e r u n d e r s t a n d i n g o f t h i s p r o b l e m c o u l d h e l p i n d e s i g n i n g a l t e r n a t i v e s y s t e m s f o r l e a r n i n g t o m a p i n p u t s e q u e n c e s t o o u t p u t s e q u e n c e s w i t h l o n g t e r m d e p e n d e n c i e s e g f o r l e a r n i n g f i n i t e s t a t e m a c h i n e s , g r a m m a r s , a n d o t h e r l a n g u a g e r e l a t e d t a s k s . S i n c e g r a d i e n t b a s e d m e t h o d s a p p e a r i n a d e q u a t e f o r t h i s k i n d o f p r o b l e m w e w a n t t o c o n s i d e r a l t e r n a t i v e o p t i m i z a t i o n m e t h o d s t h a t g i v e a c c e p t a b l e r e s u l t s e v e n w h e n t h e c r i t e r i o n f u n c t i o n i s n o t s m o o t h . ”The wonderfully intuitive backpropagation through time concept. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 21/39 So, where do neural nets enter this picture? Well, there are many ways one could get word vectors, and the above paper’s approach was just one. What if instead, you used the word vectors as inputs to a neural net that was optimized to correctly do language modeling? And then both the vectors associated with the word’s and the overall neural net’s ability to correctly do language modeling can be jointly optimized using backpropagation from the appropriate error function. And that’s where we get back to A Neural Probabilistic Language Model, since that’s what the paper essentially describes. This is now a hugely cited work, but as with the other things we’ve seen above, it took some time for the usefulness of word vectors and neural nets for language modeling to be appreciated. And we shall see why next… A New Winter Dawns Back to the 90s – there was a problem. A big problem. And the problem, basically, was what so recently was a huge advance: backpropagation. See, convolutional neural nets were important in part because backpropagation just did not work well for normal neural nets with many layers. And that’s the real key to deep-learning - having many layers, in today’s systems as manyUsing word vectors to \u0000nd the most similar words for some query words. (Source)The diagram of the neural net used for language modeling in this paper. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 22/39 as 20 or more. But already by the late 1980’ s, it was known that deep neural nets trained with backpropagation just did not work very well, and particularly did not work as well as nets with fewer layers. The reason, in basic terms, is that backpropagation relies on \u0000nding the error at the output layer and successively splitting up blame for it for prior layers. Well, with many layers this calculus-based splitting of blame ends up with either huge or tiny numbers and the resulting neural net just does not work very well - the ‘vanishing or exploding gradient problem’. Jurgen Schmidhuber, another Deep Learning luminary, summarizes the more formal explanation well : Backpropagation through time is essentially equivalent to a neural net with a whole lot of layers, so RNNs were particularly di\u0000cult to train with Backpropagation. Both Sepp Hochreiter, advised by Schmidhuber, and Yoshua Bengio published papers on the inability of learning long-term information due to limitations of backpropagation . The analysis of the problem did reveal a solution - Schmidhuber and Hochreiter introduced a very important concept in 1997 that essentially solved the problem of how to train recurrent neural nets, much as CNNs did for feedforward neural nets - Long Short Term Memory (LSTM) . In simple terms, as with CNNs the LTSM breakthrough ended up being a small alteration to the normal neural net model : But, this did little to \u0000x the larger perception problem that neural nets were janky and did not work very well. They were seen as a hassle to work with - the computers were not fast enough, the algorithms were not smart enough, and people were not happy. So, around the mid 90s , a new AI Winter for neural nets began to emerge - the community once again lost faith in them. A new method called Support Vector Machines, which in the very simplest terms could be described as a mathematically optimal way of training an equivalent to a two layer neural net, was developed and started to be seen as superior to the di\u0000cult to work with neural nets. In fact, the 1995 “Comparison of Learning Algorithms For Handwritten Digit Recognition” by LeCun et al. found that this new approach worked better or the same as all but the best designed neural nets: Other new methods, notably Random Forests, also proved to be very effective and with lovely mathematical theory behind them. So, despite the fact that CNNs consistently had state of the art performance, enthusiasm for neural nets dissipated and the machine learning community at large once again disavowed them. Winter was back. But worry not, for next we shall see how a small group of researchers persevered in this research climate and ultimately made Deep Learning what it is today. 40 “ A d i p l o m a t h e s i s ( H o c h r e i t e r , 1 9 9 1 ) r e p r e s e n t e d a m i l e s t o n e o f e x p l i c i t D L r e s e a r c h . A s m e n t i o n e d i n S e c . 5 . 6 , b y t h e l a t e 1 9 8 0 s , e x p e r i m e n t s h a d i n d i c a t e d t h a t t r a d i t i o n a l d e e p f e e d f o r w a r d o r r e c u r r e n t n e t w o r k s a r e h a r d t o t r a i n b y b a c k p r o p a g a t i o n ( B P ) ( S e c . 5 . 5 ) . H o c h r e i t e r ’ s w o r k f o r m a l l y i d e n t i f i e d a m a j o r r e a s o n : T y p i c a l d e e p N N s s u f f e r f r o m t h e n o w f a m o u s p r o b l e m o f v a n i s h i n g o r e x p l o d i n g g r a d i e n t s . W i t h s t a n d a r d a c t i v a t i o n f u n c t i o n s ( S e c . 1 ) , c u m u l a t i v e b a c k p r o p a g a t e d e r r o r s i g n a l s ( S e c . 5 . 5 . 1 ) e i t h e r s h r i n k r a p i d l y , o r g r o w o u t o f b o u n d s . I n f a c t , t h e y d e c a y e x p o n e n t i a l l y i n t h e n u m b e r o f l a y e r s o r C A P d e p t h ( S e c . 3 ) , o r t h e y e x p l o d e . “ 41 42 43 40 “ T h e b a s i c L S T M i d e a i s v e r y s i m p l e . S o m e o f t h e u n i t s a r e c a l l e d C o n s t a n t E r r o r C a r o u s e l s ( C E C s ) . E a c h C E C u s e s a s a n a c t i v a t i o n f u n c t i o n f , t h e i d e n t i t y f u n c t i o n , a n d h a s a c o n n e c t i o n t o i t s e l f w i t h ﬁ x e d w e i g h t o f 1 . 0 . D u e t o f ’ s c o n s t a n t d e r i v a t i v e o f 1 . 0 , e r r o r s b a c k p r o p a g a t e d t h r o u g h a C E C c a n n o t v a n i s h o r e x p l o d e ( S e c . 5 . 9 ) b u t s t a y a s t h e y a r e ( u n l e s s t h e y “ ﬂ o w o u t ” o f t h e C E C t o o t h e r , t y p i c a l l y a d a p t i v e p a r t s o f t h e N N ) . C E C s a r e c o n n e c t e d t o s e v e r a l n o n l i n e a r a d a p t i v e u n i t s ( s o m e w i t h m u l t i p l i c a t i v e a c t i v a t i o n f u n c t i o n s ) n e e d e d f o r l e a r n i n g n o n l i n e a r b e h a v i o r . W e i g h t c h a n g e s o f t h e s e u n i t s o f t e n p r o ﬁ t f r o m e r r o r s i g n a l s p r o p a g a t e d f a r b a c k i n t i m e t h r o u g h C E C s . C E C s a r e t h e m a i n r e a s o n w h y L S T M n e t s c a n l e a r n t o d i s c o v e r t h e i m p o r t a n c e o f ( a n d m e m o r i z e ) e v e n t s t h a t h a p p e n e d t h o u s a n d s o f d i s c r e t e t i m e s t e p s a g o , w h i l e p r e v i o u s R N N s a l r e a d y f a i l e d i n c a s e o f m i n i m a l t i m e l a g s o f 1 0 s t e p s . ” 44 “ T h e [ s u p p o r t v e c t o r m a c h i n e ] c l a s s i f i e r h a s e x c e l l e n t a c c u r a c y , w h i c h i s m o s t r e m a r k a b l e , b e c a u s e u n l i k e t h e o t h e r h i g h p e r f o r m a n c e c l a s s i f i e r s , i t d o e s n o t i n c l u d e a p r i o r i k n o w l e d g e a b o u t t h e p r o b l e m . I n f a c t , t h i s c l a s s i f i e r w o u l d d o j u s t a s w e l l i f t h e i m a g e p i x e l s w e r e p e r m u t e d w i t h a f i x e d m a p p i n g . I t i s s t i l l m u c h s l o w e r a n d m e m o r y h u n g r y t h a n t h e c o n v o l u t i o n a l n e t s . H o w e v e r , i m p r o v e m e n t s a r e e x p e c t e d a s t h e t e c h n i q u e i s r e l a t i v e l y n e w . ” 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 23/39 Part 3 : Deep Learning (2 0 0 0 s-2 0 1 0 s) In this last part of our history, we will get to the end of our story and see how deep learning emerged from the slump neural nets found themselves in by the late 90s , and the amazing state of the art results it has achieved since. The Funding of More Layers With the ascent of Support Vector Machines and the failure of backpropagation, the early 2000s were a dark time for neural net research. LeCun and Hinton variously mention how in this period their papers or the papers of their students were routinely rejected from being published due to their subject being Neural Nets. Certainly research in Machine Learning and AI was still very active, and other people were also still working with neural nets, but citation counts from the time make it clear that the excitement had leveled off, even if it did not completely evaporate. Still, Hinton, Bengio, and Lecun in particular persevered in their belief neural nets merit research. And they found a strong ally outside the research realm: The Canadian government. Funding from the Canadian Institute for Advanced Research (CIFAR), which encourages basic research without direct application, was what motivated Hinton to move to Canada in 1987, and funded his work afterward. But, the funding was ended in the mid 90s just as sentiment towards neural nets was becoming negative again. Rather than relenting and switching his focus, Hinton fought to continue work on neural nets, and managed to secure more funding from CIFAR as told well in this exemplary piece : The funding was modest, but su\u0000cient to enable a small group of researchers to keep working on the topic. As Hinton tells it, they hatched a conspiracy: “rebrand” the frowned-upon \u0000eld of neural nets with the moniker “deep learning” . Then, what every researcher must dream of actually happened: Hinton, Simon Osindero, and Yee-Whye Teh published a paper in 2006 that was seen as a breakthrough, a breakthrough signi\u0000cant enough to rekindle interest in neural nets: A fast learning algorithm for deep belief nets . Though, as we’ll see, the approaches used in the paper have been superceded by newer work, the movement that is ‘deep learning’ can be said to have started precisely with this paper. But, more important than the name was the idea - that neural networks with many layers really could be trained well, if the weights are initialized in a clever way rather than randomly. Hinton once expressed the need for such an advance at the time: “ A s k a n y o n e i n m a c h i n e l e a r n i n g w h a t k e p t n e u r a l n e t w o r k r e s e a r c h a l i v e a n d t h e y w i l l p r o b a b l y m e n t i o n o n e o r a l l o f t h e s e t h r e e n a m e s : G e o f f r e y H i n t o n , f e l l o w C a n a d i a n Y o s h u a B e n g i o a n d Y a n n L e C u n , o f F a c e b o o k a n d N e w Y o r k U n i v e r s i t y . ” 4 5 45 “ B u t i n 2 0 0 4 , H i n t o n a s k e d t o l e a d a n e w p r o g r a m o n n e u r a l c o m p u t a t i o n . T h e m a i n s t r e a m m a c h i n e l e a r n i n g c o m m u n i t y c o u l d n o t h a v e b e e n l e s s i n t e r e s t e d i n n e u r a l n e t s . “ I t w a s t h e w o r s t p o s s i b l e t i m e , ” s a y s B e n g i o , a p r o f e s s o r a t t h e U n i v e r s i t é d e M o n t r é a l a n d c o - d i r e c t o r o f t h e C I F A R p r o g r a m s i n c e i t w a s r e n e w e d l a s t y e a r . “ E v e r y o n e e l s e w a s d o i n g s o m e t h i n g d i f f e r e n t . S o m e h o w , G e o f f c o n v i n c e d t h e m . ” “ W e s h o u l d g i v e ( C I F A R ) a l o t o f c r e d i t f o r m a k i n g t h a t g a m b l e . ” C I F A R “ h a d a h u g e i m p a c t i n f o r m i n g a c o m m u n i t y a r o u n d d e e p l e a r n i n g , ” a d d s L e C u n , t h e C I F A R p r o g r a m ’ s o t h e r c o - d i r e c t o r . “ W e w e r e o u t c a s t a l i t t l e b i t i n t h e b r o a d e r m a c h i n e l e a r n i n g c o m m u n i t y : w e c o u l d n ’ t g e t o u r p a p e r s p u b l i s h e d . T h i s g a v e u s a p l a c e w h e r e w e c o u l d e x c h a n g e i d e a s . ” ” 45 46 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 24/39 So what was the clever way of initializing weights? The basic idea is to train each layer one by one with unsupervised training, which starts off the weights much better than just giving them random values, and then \u0000nishing with a round of supervised learning just as is normal for neural nets. Each layer starts out as a Restricted Boltzmann Machine (RBM), which is just a Boltzmann Machine without connections between hidden and visible units as illustrated above, and is taught a generative model of data in an unsupervised fashion. It turns out that this form of Boltzmann machine can be trained in an e\u0000cient manner introduced by Hinton in the 2002 “Training Products of Experts by Minimizing Contrastive Divergence” . Basically, this algorithm maximizes something other than the probability of the units generating the training data, which allows for a nice approximation and turns out to still work well. So, using this method, the algorithm is as such: 1. Train an RBM on the training data using contrastive-divergence. This is the \u0000rst layer of the belief net. 2. Generate the hidden values of the trained RBM for the data, and train another RBM using those hidden values. This is the second layer - ‘stack’ it on top of the \u0000rst and keep weights in just one direction to form a belief net. 3. Keep doing step 2 for as many layers as are desired for the belief net. 4. If classi\u0000cation is desired, add a small set of hidden units that correspond to the classi\u0000cation labels and do a variation on the wake-sleep algorithm to ‘\u0000ne-tune’ the weights. Such combinations of unsupervised and supervised learning are often called semi-supervised learning. The paper concluded by showing that deep belief networks (DBNs) had state of the art performance on the standard MNIST character recognition dataset, signi\u0000cantly outperforming normal neural nets with only a few layers. Yoshua Bengio et al. followed up on this work in 2007 with “Greedy Layer-Wise Training of Deep Networks” , in which they present a strong argument that deep machine learning methods (that is, methods with many processing steps, or equivalently with hierarchical feature representations of the data) are more e\u0000cient for di\u0000cult problems than shallow methods (which two-layer ANNs or support vector machines are examples of). “ H i s t o r i c a l l y , t h i s w a s v e r y i m p o r t a n t i n o v e r c o m i n g t h e b e l i e f t h a t t h e s e d e e p n e u r a l n e t w o r k s w e r e n o g o o d a n d c o u l d n e v e r b e t r a i n e d . A n d t h a t w a s a v e r y s t r o n g b e l i e f . A f r i e n d o f m i n e s e n t a p a p e r t o I C M L [ I n t e r n a t i o n a l C o n f e r e n c e o n M a c h i n e L e a r n i n g ] , n o t t h a t l o n g a g o , a n d t h e r e f e r e e s a i d i t s h o u l d n o t a c c e p t e d b y I C M L , b e c a u s e i t w a s a b o u t n e u r a l n e t w o r k s a n d i t w a s n o t a p p r o p r i a t e f o r I C M L . I n f a c t i f y o u l o o k a t I C M L l a s t y e a r , t h e r e w e r e n o p a p e r s w i t h ‘ n e u r a l ’ i n t h e t i t l e a c c e p t e d , s o I C M L s h o u l d n o t a c c e p t p a p e r s a b o u t n e u r a l n e t w o r k s . T h a t w a s o n l y a f e w y e a r s a g o . A n d o n e o f t h e I E E E j o u r n a l s a c t u a l l y h a d a n o f f i c i a l p o l i c y o f [ n o t a c c e p t i n g y o u r p a p e r s ] . S o , i t w a s a s t r o n g b e l i e f . ”A Restricted Boltzmann Machine. (Source) 47The layerwise pre-training that Hinton introduced. (Source) 48 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 25/39 They also present reasons for why the addition of unsupervised pre-training works, and conclude that this not only initializes the weights in a more optimal way, but perhaps more importantly leads to more useful learned representations of the data. In fact, using RBMs is not that important - unsupervised pre-training of normal neural net layers using backpropagation with plain Autoencoders layers proved to also work well. Likewise, at the same time another approach called Sparse Coding also showed that unsupervised feature learning was a powerful approach for improving supervised learning performance. So, the key really was having many layers of computing units so that good high-level representation of data could be learned - in complete disagreement with the traditional approach of hand-designing some nice feature extraction steps and only then doing learning using those features. Hinton and Bengio’s work had empirically demonstrated that fact, but more importantly, showed the premise that deep neural nets could not be trained well to be false. This, LeCun had already demonstrated with CNNs throughout the 90s , but neural nets still went out of favor. Bengio, in collaboration with Yann LeCun, reiterated this on “Scaling Algorithms Towards AI” : And inspire they did. Or at least, they started; though deep learning had not yet gained the tsumani momentum that it has today, the wave had unmistakably begun. Still, the results at that point were not that impressive - most of the demonstrated performance in the papers up to this point was for the MNIST dataset, a classic machine learning task that had been the standard benchmark for algorithms for about a decade. Hinton’s 200 6 publication demonstrated a very impressive error rate of only 1. 25% on the test set, but SVMs had already gotten an error rate of 1. 4%, and even simple algorithms could get error rates in the low single digits. And, as was pointed out in the paper, Yann LeCun already demonstrated error rates of 0. 95% in 1998 using CNNs in the paper “Gradient-based learning applied to document recognition”. So, doing well on MNIST was not necessarily that big a deal. Aware of this and con\u0000dent that it was time for deep learning to take the stage, Hinton and two of his graduate students, Abdel-rahman Mohamed and George Dahl, demonstrated their effectiveness at a far more challenging AI task: Speech Recognition . Using DBNs, the two students and Hinton managed toAnother view of unsupervised pre-training, using autoencoders instead of RBMs. (Source) 49 “ U n t i l r e c e n t l y , m a n y b e l i e v e d t h a t t r a i n i n g d e e p a r c h i t e c t u r e s w a s t o o d i f f i c u l t a n o p t i m i z a t i o n p r o b l e m . H o w e v e r , a t l e a s t t w o d i f f e r e n t a p p r o a c h e s h a v e w o r k e d w e l l i n t r a i n i n g s u c h a r c h i t e c t u r e s : s i m p l e g r a d i e n t d e s c e n t a p p l i e d t o c o n v o l u t i o n a l n e t w o r k s [ L e C u n e t a l . , 1 9 8 9 , L e C u n e t a l . , 1 9 9 8 ] ( f o r s i g n a l s a n d i m a g e s ) , a n d m o r e r e c e n t l y , l a y e r - b y - l a y e r u n s u p e r v i s e d l e a r n i n g f o l l o w e d b y g r a d i e n t d e s c e n t [ H i n t o n e t a l . , 2 0 0 6 , B e n g i o e t a l . , 2 0 0 7 , R a n z a t o e t a l . , 2 0 0 6 ] . R e s e a r c h o n d e e p a r c h i t e c t u r e s i s i n i t s i n f a n c y , a n d b e t t e r l e a r n i n g a l g o r i t h m s f o r d e e p a r c h i t e c t u r e s r e m a i n t o b e d i s c o v e r e d . T a k i n g a l a r g e r p e r s p e c t i v e o n t h e o b j e c t i v e o f d i s c o v e r i n g l e a r n i n g p r i n c i p l e s t h a t c a n l e a d t o A I h a s b e e n a g u i d i n g p e r s p e c t i v e o f t h i s w o r k . W e h o p e t o h a v e h e l p e d i n s p i r e o t h e r s t o s e e k a s o l u t i o n t o t h e p r o b l e m o f s c a l i n g m a c h i n e l e a r n i n g t o w a r d s A I . ” 50 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 26/39 improve on a decade-old performance record on a standard speech recognition dataset. This was an impressive achievement, but in retrospect seems like only a hint at what was coming – in short, many more broken records. The Development of Big Data So, algorithmic advancements were certainly made and led to increasing excitement about neural nets. But, this alone did not overcome the limitations of neural nets seen in the 90s . After all, to train a neural net you don’t only need the optimization algorithm, you also need another crucial ingredient: the data. As we’ve covered long ago near the start of this piece, neural nets are often trained via supervised training from labeled examples, and so to apply them to any given problem requires having this data. But, to tackle di\u0000cult tasks neural nets need lots of such data, getting large datasets is not trivial. While it may not seem as conceptually di\u0000cult as coming up with clever algorithms, it’s still a lot of work, and having the insight to decide on the right inputs and outputs to enable new research it also important. So, it’s crucial not to overlook this topic or take it for granted. We’ve already mentioned The MNIST database of handwritten digits (“a classic machine learning task that had been the standard benchmark for algorithms for about a decade”), which was made by modifying data \u0000rst released by the National Institute of Standards and Technology in 1995. Next, we’ll look at the datasets that emerged in the 2000s and were crucial to the development of deep learning. But, there is only so much you can do with a dataset of hand-written digits. Computer Vision aims at enabling machines to understand images in ways analogous to humans, which of course includes recognizing what objects are present in a given image. So, in the 2000s researchers set out to create datasets that could be used to work on this problem. Starting in 2005, there was the annual The PASCAL Visual Object Classes (VOC) Challenge. Then there were also Caltech 101 and Caltech 256 datasets, likewise in\u0000uential for Computer Vision research. But for our topic of deep learning, there is undoubtedly a most important development to focus on: ImageNet. As covered well in Quartz’s The data that transformed AI research — and possibly the world, Professor Fei-Fei Li’s idea of creating a dataset containing images for most of the concepts in the massive WordNet database (which is like a giant dictionary of english words grouped by their meanings) required creating a dataset of unprecedented size. Fortunately, soon after the idea came about so did the option to crowdsource (split the work of labeling data to many people via the internet), and the project could go ahead. Still, it took years of work before Li and her students and collaborators released their dataset and a paper in 2009. By then, the dataset had 3. 2 million images for 5247 concepts, still a long way away from the \u0000nal goal of 50 million images but also orders of magnitude larger than the scale of prior datasets.The MNIST dataset. (Source) 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 27/39 Still, it was not immediately apparent to the Computer Vision community that this dataset would indeed enable major advancements. After all, and learning to categorize so many different types of objects from so large a set of images was not yet attempted that the algorithms of that day. Even when the dataset was paired down for the ImageNet Large Scale Visual Recognition Challenge (which had just 1000 types of objects and only 150, 000 images), the \u0000rst year’s winning entry could not pick the right category with 5 guesses for 30% of the test dataset. So, it was a hard problem, and therefore also a good challenge with which to demonstrate the potential of neural nets. But, it will take several more years and the last piece of the deep learning puzzle for us to get to that… The Importance of Brute Force The algorithmic advances and new datasets described above were undoubtedly important to the emergence of deep learning, but there was another essential component that had emerged in the decade since the 1990s : pure computational power. Following Moore’s law, computers got dozens of times faster since the slow days of the 90s , making learning with large datasets and many layers much more tractable. But even this was not enough - CPUs were starting to hit a ceiling in terms of speed growth, and computer power was starting to increase mainly through weakly parallel computations with several CPUs. To learn the millions of weights typical in deep models, the limitations of weak CPU parallelism had to be left behind and replaced with the massively parallel computing powers of GPUs. Realizing this is, in part, how Abdel-rahman Mohamed, George Dahl, and Geoff Hinton accomplished their record breaking speech recognition performance : It’s hard to say just how much faster using GPUs over CPUs was in this case, but the paper “Large-scale Deep Unsupervised Learning using Graphics Processors” of the same year suggests a number: 70 times faster. Yes, 70 times - reducing weeks of work into days, even a single day. The authors, who had previously developed Sparse Coding, included the proli\u0000c Machine Learning researcher Andrew Ng, who increasingly realized that making use of lots of training data and of fast computation had been greatly undervalued by researchers in favor of incremental changes in learning algorithms. This idea was strongly supported by 2010’ s “Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition” (notably co-written by J. Schmidhuber, one of the inventors of the recurrent LTSM networks), which showed a whopping %0. 35 error rate could be achieved on the MNIST dataset without anything more special than really big neural nets, a lot of variations on the input, and e\u0000cient GPU implementations of backpropagation. These ideas had existed for decades, so although it could not be said that algorithmic advancements did not matter, this result did strongly support the notion that the brute force approach of big training sets and fast parallelized computations were also crucial.Images from the ImageNet dataset. (Source) 51 “ I n s p i r e d b y o n e o f H i n t o n ’ s l e c t u r e s o n d e e p n e u r a l n e t w o r k s , M o h a m e d b e g a n a p p l y i n g t h e m t o s p e e c h - b u t d e e p n e u r a l n e t w o r k s r e q u i r e d t o o m u c h c o m p u t i n g p o w e r f o r c o n v e n t i o n a l c o m p u t e r s – s o H i n t o n a n d M o h a m e d e n l i s t e d D a h l . A s t u d e n t i n H i n t o n ’ s l a b , D a h l h a d d i s c o v e r e d h o w t o t r a i n a n d s i m u l a t e n e u r a l n e t w o r k s e f f i c i e n t l y u s i n g t h e s a m e h i g h - e n d g r a p h i c s c a r d s w h i c h m a k e v i v i d c o m p u t e r g a m e s f e a s i b l e o n p e r s o n a l c o m p u t e r s . T h e y a p p l i e d t h e s a m e m e t h o d t o t h e p r o b l e m o f r e c o g n i z i n g f r a g m e n t s o f p h o n e m e s i n v e r y s h o r t w i n d o w s o f s p e e c h , ” s a i d H i n t o n . “ T h e y g o t s i g n i f i c a n t l y b e t t e r r e s u l t s t h a n p r e v i o u s m e t h o d s o n a s t a n d a r d t h r e e - h o u r b e n c h m a r k . ” ” 52 53 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 28/39 Dahl and Mohamed’s use of a GPU to get record breaking results was an early and relatively modest success, but it was su\u0000cient to incite excitement and for the two to be invited to intern at Microsoft Research . Here, they would have the bene\u0000t from another trend in computing that had emerged by then: Big Data. That loosest of terms, which in the context of machine learning is easy to understand - lots of training data. And lots of training data is important, because without it neural nets still did not do great - they tended to over\u0000t (perfectly work on the training data, but not generalize to new test data). This makes sense - the complexity of what large neural nets can compute is such that a lot of data is needed to avoid them learning every little unimportant aspect of the training set - but was a major challenge for researchers in the past. So now, the computing and data gathering powers of large companies proved invaluable. The two students handily proved the power of deep learning during their three month internship, and Microsoft Research has been at the forefront of deep learning speech recognition ever since. Microsoft was not the only BigCompany to recognize the power of deep learning (though it was handily the \u0000rst). Navdeep Jaitly, another student of Hinton’s, went off to a summer internship at Google in 2011. There, he worked on Google’s speech recognition, and showed their existing setup could be much improved by incorporating deep learning. The revised approach soon powered Android’s speech recognition, replacing much of Google’s carefully crafted prior solution . Besides the impressive effects of humble PhD interns on these gigantic companies’ products, what is notable here is that both companies were making use of the same ideas - ideas that were out in the open for anyone to work with. And in fact, the work by Microsoft and Google, as well as IBM and Hinton’s lab, resulted in the impressively titled “Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups” in 2012. Four research groups - three from companies that could certainly bene\u0000t from a briefcase full of patents on the emerging wonder technology of deep learning, and the university research group that popularized that technology - working together and publishing their results to the broader research community. If there was ever an ideal scenario for industry adopting an idea from research, this seems like it. Not to say the companies were doing this for charity. This was the beginning of all of them exploring how to commercialize the technology, and most of all Google. But it was perhaps not Hinton, but Andrew Ng who incited the company to become likely the world’s biggest commercial adopter and proponent of the technology. In 2011, Ng incidentally met with the legendary Googler Jeff Dean while visiting the company, and chatted about his efforts to train neural nets with Google’s fantastic computational resources. This intrigued Dean, and together with Ng they formed Google Brain - an effort to build truly giant neural nets and explore what they could do. The work resulted in unsupervised neural net learning of an unprecedented scale - 16, 000 CPU cores powering the learning of a whopping 1 billion weights (for comparison, Hinton’s breakthrough 2006 DBN had about 1 million weights). The neural net was trained on Youtube videos, entirely without labels, and learned to recognize the most common objects in those videos - leading of course to the internet’s collective glee over the net’s discovery of cats: 45 45 54 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 29/39 Cute as that was, it was also useful. As they reported in a regularly published paper, the features learned by the model could be used for record setting performance on a standard computer vision benchmark . With that, Google’s internal tools for training massive neural nets were born, and they have only continued to evolve since. The wave of deep learning research that began in 2006 had now undeniably made it into industry. The Deep Learning Equation While deep learning was making it into industry, the research community was hardly keeping still. The discovery that e\u0000cient use of GPUs and computing power in general was so important made people examine long-held assumptions and ask questions that should have perhaps been asked long ago - namely, why exactly does backpropagation not work well? The insight to ask why the old approaches did not work, rather than why the new approaches did, led Xavier Glort and Yoshua Bengio to write “Understanding the di\u0000culty of training deep feedforward neural networks” in 2010 . In it, they discussed two very meaningful \u0000ndings: 1. The particular non-linear activation function chosen for neurons in a neural net makes a big impact on performance, and the one often used by default is not a good choice. 2. It was not so much choosing random weights that was problematic, as choosing random weights without consideration for which layer the weights are for. The old vanishing gradient problem happens, basically, because backpropagation involves a sequence of multiplications that invariably result in smaller derivatives for earlier layers. That is, unless weights are chosen with difference scales according to the layer they are in - making this simple change results in signi\u0000cant improvements.Google's famous neural-net learned cat. This is the optimal input to one of the neurons. (Source) 55 56 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 30/39 The second point is quite clear, but the \u0000rst opens the question: ‘what, then, is the best activation function’? Three different groups explored the question (a group with LeCun, with “What is the best multi-stage architecture for object recognition?” , a group with Hinton, in “Recti\u0000ed linear units improve restricted boltzmann machines” , and a group with Bengio -“Deep Sparse Recti\u0000er Neural Networks” ), and they all found the same surprising answer: the very much non-differentiable and very simple function f(x)=max(0, x) tends to be the best. Surprising, because the function is kind of weird - it is not strictly differentiable, or rather is not differentiable precisely at zero, so on paper as far as math goes it looks pretty ugly. But, clearly the zero case is a pretty small mathematical quibble - a bigger question is why such a simple function, with constant derivatives on either side of 0, is so good. The answer is not precisely known, but a few ideas seem pretty well established: 1. Recti\u0000ed activation leads to sparse representations, meaning not many neurons actually end up needing to output non- zero values for any given input. In the years leading up to this point sparsity was shown to be bene\u0000cial for deep learning, both because it represents information in a more robust manner and because it leads to signi\u0000cant computational e\u0000ciency (if most of your neurons are outputting zero, you can in effect ignore most of them and compute things much faster). Incidentally, researchers in computational neuroscience \u0000rst introduced the importance of sparse computation in the context of the brain’s visual system, a decade before it was explored in the context of machine learning. 2. The simplicity of the function, and its derivatives, makes it much faster to work with than the exponential sigmoid or the trigonometric tanh. As with the use of GPUs, this turns out to not just be a small boost but really important for being able to scale neural nets to the point where they perform well on challenging problems. 3. A later analysis titled “Recti\u0000er Nonlinearities Improve Neural Network Acoustic Models” , co-written by Andrew Ng, also showed the constant 0 or 1 derivative of the ReLU not too detrimental to learning. In fact, it helps avoid the vanishing gradient problem that was the bane of backpropagation. Furthermore, beside producing more sparse representations, it also produces more distributed representations - meaning is derived from the combination of multiple values of different neurons, rather than being localized to individual neurons. At this point, with all these discoveries since 2006, it had become clear that unsupervised pre-training is not essential to deep learning. It was helpful, no doubt, but it was also shown that in some cases well-done, purely supervised training (with the correct starting weight scales and activation function) could outperform training that included the unsupervised step. So, why indeed, did purely supervised learning with backpropagation not work well in the past? Geoffrey Hinton summarized the \u0000ndings up to today in these four points: 1. Our labeled datasets were thousands of times too small. 2. Our computers were millions of times too slow. 3. We initialized the weights in a stupid way. 4. We used the wrong type of non-linearity. So here we are. Deep learning. The culmination of decades of research, all leading to this:Different activation functions. The ReLU is the **recti\u0000ed linear unit**. (Source) 57 58 59 60 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 31/39 Not to say all there was to \u0000gure out was \u0000gured out by this point. Far from it. What had been \u0000gured out is exactly the opposite: that peoples’ intuition was often wrong, and in particular unquestioned decisions and assumptions were often very unfounded. Asking simple questions, trying simple things - these had the power to greatly improve state of the art techniques. And precisely that has been happening, with many more ideas and approaches being explored and shared in deep learning since. An example: “Improving neural networks by preventing co-adaptation of feature detectors” by G. E. Hinton et al. The idea is very simple: to prevent over\u0000tting, randomly pretend some neurons are not there while training. This straightforward idea - called Dropout - is a very e\u0000cient means of implementing the hugely powerful approach of ensemble learning, which just means learning in many different ways from the training data. Random Forests, a dominating technique in machine learning to this day, is chie\u0000y effective due to being a form of ensemble learning. Training many different neural nets is possible but is far too computationally expensive, yet this simple idea in essence achieves the same thing and indeed signi\u0000cantly improves performance. Still, having all these research discoveries since 2006 is not what made the computer vision or other research communities again respect neural nets. What did do it was something somewhat less noble: completely destroying non-deep learning methods on a modern competitive benchmark. Geoffrey Hinton enlisted two of his Dropout co-writers, Alex Krizhevsky and Ilya Sutskever, to apply the ideas discovered to create an entry to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)-2012 computer vision competition. To me, it is very striking to now understand that their work, described in “ImageNet Classi\u0000cation with deep convolutional neural networks” , is the combination of very old concepts (a CNN with pooling and convolution layers, variations on the input data) with several new key insight (very e\u0000cient GPU implementation, ReLU neurons, dropout), and that this, precisely this, is what modern deep learning is. So, how did they do? Far, far better than the next closest entry: their error rate was %15. 3, whereas the second closest was %26. 2. This, the \u0000rst and only CNN entry in that competition, was an undisputed sign that CNNs, and deep learning in general, had to be taken seriously for computer vision. Now, almost all entries to the competition are CNNs - a neural net model Yann LeCun was working with since 198 9. And, remember LSTM recurrent neural nets, devised in the 90s by Sepp Hochreiter and Jürgen Schmidhuber to solve the backpropagation problem? Those, too, are now state of the art for sequential tasks such as speech processing. This was the turning point. A mounting wave of excitement about possible progress has culminated in undeniable achievements that far surpassed what other known techniques could manage. The tsunami metaphor that we started with in part 1, this is where it began, and it has been growing and intensifying to this day. Deep learning is here, and no winter is in sight. D e e p L e a r n i n g = L o t s o f t r a i n i n g d a t a + P a r a l l e l C o m p u t a t i o n + S c a l a b l e , s m a r t a l g o r i t h m sI wish I was \u0000rst to come up with this delightful equation, but it seems others came up with it before me. (Source) 61 62 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 32/39 Epilogue: The Decade of Deep Learning If this were a movie, the 2012 ImageNet competition would likely have been the climax, and now we would have a progression of text describing ‘where are they now’. Yann LeCun - Facebook. Geoffrey Hinton - Google. Andrew Ng - Coursera, Google, Baidu, and more. Bengio, Schmidhuber, and Li actually still in academia but with their own industry a\u0000liations too, and presumably with many more citations and/or grad students (and, the many more who contributed to the emergence of Deep Learning . Though the ideas and achievements of deep learning are de\u0000nitely exciting, while writing this I was inevitably also moved that these people, who worked in this \u0000eld for decades (even as most abandoned it), are now rich, successful, and most of all better situated to do research than ever. All these peoples’ ideas are still very much out in the open, and in fact basically all these companies are open sourcing their deep learning frameworks, like some sort of utopian vision of industry-led research. What a story. Since 2012, it’s fair to say Deep Learning has revolutionized much of AI as a \u0000eld. As we read at the start of all this, “2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences.” And so it was with Computer Vision, Robotics, Audio Processing, AI for Medicine, and so much more. To summarize all the ground breaking developments in this period would take its own lengthy sub-history, and has already been done nicely in the blog post “The Decade of Deep Learning”. Su\u0000ce it today, progress since 20 12 was swift and ongoing, and has seen all the applications of neural nets we have seen so far (to reinforcement learning, language modeling, image classi\u0000cation, and much more) extended to leverage Deep Learning resulting in ground breaking accomplishments.A good retrospective given in a TED talk by Fei-Fei Li. 63 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 33/39 With such progress came much excitement, and the \u0000eld of AI rapidly grew: And now here were at 2020. AI as a \u0000eld is huge and still moving fast, but many of the low-hanging fruit with respect to tackling AI problems with Deep Learning have been plucked, and we are increasingly in a time expanding outwards in terms of varied applications of neural nets and Deep Learning . And for good reason: Deep Learning still works best only when there is a huge dataset of input-output examples to learn from, which is not true for many problems in AI, and has other major limitations (interpretability, veri\u0000ability, and more). Although this is where this brief history, the history of neural nets is very much still being written, and shall be for some time. Let us hope this powerful technology continues to blossom, and is used primarily to further human well-being and progress well into the future. Citation This piece is an updated and expanded version of blog posts originally released in 2015 on www.andreykurenkov.com. Please cite this version.Performance on the ImageNet Benchmark over the years. From the 2019 AI Index Report.Attendance of major AI conference over the years. From the 2019 AI Index Report. 64 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 34/39 For attribution in academic contexts or books, please cite this work as BibTeX citation: 1 . Christopher D. Manning. (2 0 1 5 ). Computational Linguistics and Deep Learning Computational Linguistics, 4 1 (4 ), 7 0 1 –7 0 7 . ↩ 2 . F. Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1 9 5 7 . ↩ 3 . W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5 (4 ):1 1 5 –1 3 3 , 1 9 4 3 . ↩ 4 . The organization of behavior: A neuropsychological theory. D. O. Hebb. John Wiley And Sons, Inc., New York, 1 9 4 9  ↩ 5 . B. Widrow et al. Adaptive ”Adaline” neuron using chemical ”memistors”. Number Technical Report 1 5 5 3 -2 . Stanford Electron. Labs., Stanford, CA, October 1 9 6 0 . ↩ \u0000 . “New Navy Device Learns By Doing”, New York Times, July 8 , 1 9 5 8 . ↩ 7 . Perceptrons. An Introduction to Computational Geometry. MARVIN MINSKY and SEYMOUR PAPERT. M.I.T. Press, Cambridge, Mass., 1 9 6 9 . ↩ \u0000 . Minsky, M. (1 9 5 2 ). A neural-analogue calculator based upon a probability model of reinforcement. Harvard University Pychological Laboratories internal report. ↩ 9 . Linnainmaa, S. (1 9 7 0 ). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master’s thesis, Univ. Helsinki. ↩ 1 0 . P. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University, Cambridge, MA, 1 9 7 4 . ↩ 1 1 . Werbos, P.J. (2 0 0 6 ). Backwards differentiation in AD and neural nets: Past links and new opportunities. In Automatic Differentiation: Applications, Theory, and Implementations, pages 1 5 -3 4 . Springer. ↩ 1 2 . Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1 9 8 6 ). Learning representations by back-propagating errors. Nature, 3 2 3 , 5 3 3 –5 3 6 . ↩ 1 3 . Widrow, B., & Lehr, M. (1 9 9 0 ). 3 0 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proceedings of the IEEE, 7 8 (9 ), 1 4 1 5 -1 4 4 2 . ↩ 1 4 . D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1 9 8 6 . Learning internal representations by error propagation. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1 , David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group (Eds.). MIT Press, Cambridge, MA, USA 3 1 8 -3 6 2  ↩ 1 5 . Kurt Hornik, Maxwell Stinchcombe, Halbert White, Multilayer feedforward networks are universal approximators, Neural Networks, Volume 2 , Issue 5 , 1 9 8 9 , Pages 3 5 9 -3 6 6 , ISSN 0 8 9 3 -6 0 8 0 , http://dx.doi.org/1 0 .1 0 1 6 /0 8 9 3 -6 0 8 0 (8 9 )9 0 0 2 0 -8 . ↩ 1 \u0000 . LeCun, Y; Boser, B; Denker, J; Henderson, D; Howard, R; Hubbard, W; Jackel, L, “Backpropagation Applied to Handwritten Zip Code Recognition,” in Neural Computation , vol.1 , no.4 , pp.5 4 1 -5 5 1 , Dec. 1 9 8 9 8 9  ↩ 1 7 . D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1 9 8 6 . Learning internal representations by error propagation. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1 , David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group (Eds.). MIT Press, Cambridge, MA, USA 3 1 8 -3 6 2  ↩ ↩ 1 \u0000 . Fukushima, K. (1 9 8 0 ), ‘Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position’, Biological Cybernetics 3 6 , 1 9 3 –2 0 2 . ↩ 1 9 . Gregory Piatetsky, ‘KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab’ Feb 2 0 , 2 0 1 4 . http://www.kdnuggets.com/2 0 1 4 /0 2 /exclusive-yann-lecun-deep-learning-facebook-ai-lab.html ↩ 2 0 . Teuvo Kohonen. 1 9 8 8 . Self-organized formation of topologically correct feature maps. In Neurocomputing: foundations of research, James A. Anderson and Edward Rosenfeld (Eds.). MIT Press, Cambridge, MA, USA 5 0 9 -5 2 1 . ↩ A n d r e y K u r e n k o v , “ A B r i e f H i s t o r y o f N e u r a l N e t s a n d D e e p L e a r n i n g ” , S k y n e t T o d a y , 2 0 2 0 . @ a r t i c l e { k u r e n k o v 2 0 2 0 b r i e f h i s t o r y , a u t h o r = { K u r e n k o v , A n d r e y } , t i t l e = { A B r i e f H i s t o r y o f N e u r a l N e t s a n d D e e p L e a r n i n g } , j o u r n a l = { S k y n e t T o d a y } , y e a r = { 2 0 2 0 } , h o w p u b l i s h e d = { \\ u r l { h t t p s : / / s k y n e t t o d a y . c o m / o v e r v i e w s / n e u r a l - n e t - h i s t o r y } } , } 2 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 35/39 2 1 . Gail A. Carpenter and Stephen Grossberg. 1 9 8 8 . The ART of Adaptive Pattern Recognition by a Self-Organizing Neural Network. Computer 2 1 , 3 (March 1 9 8 8 ), 7 7 -8 8 . ↩ 2 2 . H. Bourlard and Y. Kamp. 1 9 8 8 . Auto-association by multilayer perceptrons and singular value decomposition. Biol. Cybern. 5 9 , 4 -5 (September 1 9 8 8 ), 2 9 1 -2 9 4 . ↩ 2 3 . P. Baldi and K. Hornik. 1 9 8 9 . Neural networks and principal component analysis: learning from examples without local minima. Neural Netw. 2 , 1 (January 1 9 8 9 ), 5 3 -5 8 . ↩ 2 4 . Hinton, G. E. & Zemel, R. S. (1 9 9 3 ), Autoencoders, Minimum Description Length and Helmholtz Free Energy., in Jack D. Cowan; Gerald Tesauro & Joshua Alspector, ed., ‘NIPS’ , Morgan Kaufmann, , pp. 3 -1 0 . ↩ 2 5 . Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1 9 8 5 ). A learning algorithm for boltzmann machines*. Cognitive science, 9 (1 ), 1 4 7 -1 6 9 . ↩ 2 \u0000 . LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2 0 0 6 ). A tutorial on energy-based learning. Predicting structured data, 1 , 0 . ↩ 2 7 . Neal, R. M. (1 9 9 2 ). Connectionist learning of belief networks. Arti\u0000cial intelligence, 5 6 (1 ), 7 1 -1 1 3 . ↩ 2 \u0000 . Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1 9 9 5 ). The” wake-sleep” algorithm for unsupervised neural networks. Science, 2 6 8 (5 2 1 4 ), 1 1 5 8 - 1 1 6 1 . ↩ 2 9 . Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1 9 9 5 ). The helmholtz machine. Neural computation, 7 (5 ), 8 8 9 -9 0 4 . ↩ 3 0 . Anderson, C. W. (1 9 8 9 ). Learning to control an inverted pendulum using neural networks. Control Systems Magazine, IEEE, 9 (3 ), 3 1 -3 7 . ↩ 3 1 . Narendra, K. S., & Parthasarathy, K. (1 9 9 0 ). Identi\u0000cation and control of dynamical systems using neural networks. Neural Networks, IEEE Transactions on, 1 (1 ), 4 -2 7 . ↩ 3 2 . Pomerleau, D. A. (1 9 8 9 ). Alvinn: An autonomous land vehicle in a neural network (No. AIP-7 7 ). Carnegie-Mellon Univ Pittsburgh Pa Arti\u0000cial Intelligence And Psychology Project. ↩ 3 3 . Lin, L. J. (1 9 9 3 ). Reinforcement learning for robots using neural networks (No. CMU-CS-9 3 -1 0 3 ). Carnegie-Mellon Univ Pittsburgh PA School of Computer Science. ↩ 3 4 . Tesauro, G. (1 9 9 5 ). Temporal difference learning and TD-Gammon. Communications of the ACM, 3 8 (3 ), 5 8 -6 8 . ↩ 3 5 . Thrun, S. (1 9 9 5 ). Learning to play the game of chess. Advances in neural information processing systems, 7 . ↩ 3 \u0000 . Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1 9 9 4 ). Temporal difference learning of position evaluation in the game of Go. Advances in Neural Information Processing Systems, 8 1 7 -8 1 7 . ↩ 3 7 . Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. J. (1 9 8 9 ). Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing, IEEE Transactions on, 3 7 (3 ), 3 2 8 -3 3 9 . ↩ 3 \u0000 . Yann LeCun and Yoshua Bengio. 1 9 9 8 . Convolutional networks for images, speech, and time series. In The handbook of brain theory and neural networks, Michael A. Arbib (E()d.). MIT Press, Cambridge, MA, USA 2 5 5 -2 5 8 . ↩ 3 9 . Yoshua Bengio, A Connectionist Approach To Speech Recognition Int. J. Patt. Recogn. Artif. Intell., 0 7 , 6 4 7 (1 9 9 3 ). ↩ 4 0 . J. Schmidhuber. “Deep Learning in Neural Networks: An Overview”. “Neural Networks”, “6 1 ”, “8 5 -1 1 7 ”. http://arxiv.org/abs/1 4 0 4 .7 8 2 8  ↩ ↩ 4 1 . Hochreiter, S. (1 9 9 1 ). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institutfur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. Advisor: J. Schmidhuber. ↩ 4 2 . Bengio, Y.; Simard, P.; Frasconi, P., “Learning long-term dependencies with gradient descent is di\u0000cult,” in Neural Networks, IEEE Transactions on , vol.5 , no.2 , pp.1 5 7 -1 6 6 , Mar 1 9 9 4  ↩ 4 3 . Sepp Hochreiter and Jürgen Schmidhuber. 1 9 9 7 . Long Short-Term Memory. Neural Comput. 9 , 8 (November 1 9 9 7 ), 1 7 3 5 -1 7 8 0 . DOI=http://dx.doi.org/1 0 .1 1 6 2 /neco.1 9 9 7 .9 .8 .1 7 3 5 . ↩ 4 4 . Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, U. A. Muller, E. Sackinger, P. Simard and V. Vapnik: Comparison of learning algorithms for handwritten digit recognition, in Fogelman, F. and Gallinari, P. (Eds), International Conference on Arti\u0000cial Neural Networks, 5 3 -6 0 , EC2 & Cie, Paris, 1 9 9 5  ↩ 4 5 . Kate Allen. How a Toronto professor’s research revolutionized arti\u0000cial intelligence Science and Technology reporter, Apr 1 7 2 0 1 5 http://www.thestar.com/news/world/2 0 1 5 /0 4 /1 7 /how-a-toronto-professors-research-revolutionized-arti\u0000cial-intelligence.html ↩ ↩  ↩  ↩  ↩ 4 \u0000 . Hinton, G. E., Osindero, S., & Teh, Y. W. (2 0 0 6 ). A fast learning algorithm for deep belief nets. Neural computation, 1 8 (7 ), 1 5 2 7 -1 5 5 4 . ↩ 4 7 . Hinton, G. E. (2 0 0 2 ). Training products of experts by minimizing contrastive divergence. Neural computation, 1 4 (8 ), 1 7 7 1 -1 8 0 0 . ↩ 4 \u0000 . Bengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2 0 0 7 ). Greedy layer-wise training of deep networks. Advances in neural information processing systems, 1 9 , 1 5 3 . ↩ 4 9 . Bengio, Y., & LeCun, Y. (2 0 0 7 ). Scaling learning algorithms towards AI. Large-scale kernel machines, 3 4 (5 ). ↩ 5 0 . Mohamed, A. R., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G. E., & Picheny, M. (2 0 1 1 , May). Deep belief networks using discriminative features for phone recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2 0 1 1 IEEE International Conference on (pp. 5 0 6 0 -5 0 6 3 ). IEEE. ↩ 2 2 3 4 5 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 36/39 5 1 . November 2 6 , 2 0 1 2 . Leading breakthroughs in speech recognition software at Microsoft, Google, IBM Source: http://news.utoronto.ca/leading- breakthroughs-speech-recognition-software-microsoft-google-ibm ↩ 5 2 . Raina, R., Madhavan, A., & Ng, A. Y. (2 0 0 9 , June). Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 2 6 th annual international conference on machine learning (pp. 8 7 3 -8 8 0 ). ACM. ↩ 5 3 . Claudiu Ciresan, D., Meier, U., Gambardella, L. M., & Schmidhuber, J. (2 0 1 0 ). Deep big simple neural nets excel on handwritten digit recognition. arXiv preprint arXiv:1 0 0 3 .0 3 5 8 . ↩ 5 4 . Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N., … & Kingsbury, B. (2 0 1 2 ). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 2 9 (6 ), 8 2 -9 7 . ↩ 5 5 . Le, Q. V. (2 0 1 3 , May). Building high-level features using large scale unsupervised learning. In Acoustics, Speech and Signal Processing (ICASSP), 2 0 1 3 IEEE International Conference on (pp. 8 5 9 5 -8 5 9 8 ). IEEE. ↩ 5 \u0000 . Glorot, X., & Bengio, Y. (2 0 1 0 ). Understanding the di\u0000culty of training deep feedforward neural networks. In International conference on arti\u0000cial intelligence and statistics (pp. 2 4 9 -2 5 6 ). ↩ 5 7 . Jarrett, K., Kavukcuoglu, K., Ranzato, M. A., & LeCun, Y. (2 0 0 9 , September). What is the best multi-stage architecture for object recognition?. In Computer Vision, 2 0 0 9 IEEE 1 2 th International Conference on (pp. 2 1 4 6 -2 1 5 3 ). IEEE. ↩ 5 \u0000 . Nair, V., & Hinton, G. E. (2 0 1 0 ). Recti\u0000ed linear units improve restricted boltzmann machines. In Proceedings of the 2 7 th International Conference on Machine Learning (ICML-1 0 ) (pp. 8 0 7 -8 1 4 ). ↩ 5 9 . Glorot, X., Bordes, A., & Bengio, Y. (2 0 1 1 ). Deep sparse recti\u0000er neural networks. In International Conference on Arti\u0000cial Intelligence and Statistics (pp. 3 1 5 -3 2 3 ). ↩ \u0000 0 . Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2 0 1 3 , June). Recti\u0000er nonlinearities improve neural network acoustic models. In Proc. ICML (Vol. 3 0 ). ↩ \u0000 1 . Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2 0 1 2 ). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1 2 0 7 .0 5 8 0 . ↩ \u0000 2 . Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2 0 1 2 ). Imagenet classi\u0000cation with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1 0 9 7 -1 1 0 5 ). ↩ \u0000 3 . http://www.technologyreview.com/news/5 2 4 0 2 6 /is-google-cornering-the-market-on-deep-learning/ ↩ \u0000 4 . The website Papers with Code, which tracks AI papers along with their code and results, now has entries for 1 7 0 2 tasks, 3 1 3 8 benchmarks, 2 7 6 7 dataset, and 2 7 9 3 2 papers. ↩ Andrey Kurenkov @andrey_kurenkov Prologue: The Deep Learning Tsunami Part 1: The Beginnings (1950s-1980s) Part 2: Neur al Nets Blossom (1980s-2000s) Part 3: Deep Learning (2000s-2010s) Epilogue: The Decade of Deep LearningSeptember 27, 2020 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 37/39 MORE LIKE THIS < > / December 30, 2023 OVERVIEWSOverview AI News in 2023: a DigestAn overview of the big AI-related stories from 2023Andrey Kurenkov / , May 1, 2023 OVERVIEWSOverviewPolicy Rights and Regulation: The Future of Generative AI Under the First AmendmentThe Supreme Court may soon be willing to grant First Amendment rights to generative AI, which is likely to add to the existingdi\u0000culties of passing AI regulation.Archer Amon / , December 31, 2021 OVERVIEWSOverviewNews AI News in 2021: a DigestAn overview of the big AI-related stories from 2021 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 38/39 © 2024 Andr ey Kurenkov Privacy Policy / Contact Subscribe & follow Skynet Today for the latest AI news and trends We'd love to hear from you! Provide feedback Suggest coverage Express interest in helping Submit an article We would like to thank Stanford HAI for its support. 4/26/24, 8:50 PM A B rief H istory of Neural Nets and D eep Learning – Skynet Today https://www.skynettoday.com/overviews/neural-net-history 39/39","libVersion":"0.3.2","langs":""}
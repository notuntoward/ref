{"path":"lit/lit_sources.backup/Jakobs24treeAdaptModelSelTSfrcst.pdf","text":"Explainable Adaptive Tree-based Model Selection for Time-Series Forecasting Matthias Jakobs Lamarr Institute for Machine Learning and Artificial Intelligence TU Dortmund University Dortmund, Germany matthias.jakobs@tu-dortmund.de Amal Saadallah Lamarr Institute for Machine Learning and Artificial Intelligence TU Dortmund University Dortmund, Germany amal.saadallah@cs.tu-dortmund.de Abstract—Tree-based models have been successfully applied to a wide variety of tasks, including time series forecasting. They are increasingly in demand and widely accepted because of their comparatively high level of interpretability. However, many of them suffer from the overfitting problem, which limits their application in real-world decision-making. This problem becomes even more severe in online-forecasting settings where time series observations are incrementally acquired, and the distributions from which they are drawn may keep changing over time. In this context, we propose a novel method for the online selection of tree-based models using the TreeSHAP explainability method in the task of time series forecasting. We start with an arbitrary set of different tree-based models. Then, we outline a performance- based ranking with a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series. In this framework, adequate model selection is performed online, adaptively following drift detection in the time series. In addition, explainability is supported on three levels, namely online input importance, model selection, and model output explanation. An extensive empirical study on various real- world datasets demonstrates that our method achieves excellent or on-par results in comparison to the state-of-the-art approaches as well as several baselines. Index Terms—Online Model Selection, Tree-based Models, Time Series Forecasting, TreeSHAP, Explainability I. INTRODUCTION Time series forecasting has always been recognized as a main component of informed decision-making across various real-world applications, including but not limited to smart manufacturing control, predictive maintenance, energy man- agement, transport planning in smart cities, and financial in- vestments [1]–[3]. However, due to the complex and dynamic nature of time series data, forecasting is often considered to be one of the most challenging tasks in time series analysis. Time series data can contain sources of non-stationary variations and are therefore susceptible to the concept drift phenomenon [4], which further adds to the complexity of forecasting. There have been numerous Machine Learning (ML) models sug- gested for addressing this task. One option involves handling the data as sequential observations either in an online or streaming fashion [5]. Another approach is to utilize time series embeddings that convert a group of target observations into a feature space of k dimensions corresponding to the observation’s past k-lagged values [6], [7]. Although various ML forecasting methods have been pro- posed, it is widely recognized that none of them can uni- versally apply to all applications. This limitation is a direct consequence of the No Free Lunch theorem established by Wolpert [8], which states that no learning algorithm can per- form optimally for all learning tasks. Furthermore, even within the same application, models exhibit varying performance over time [6], [7], [9], [10], which can be attributed to the aforemen- tioned difficulties in time series modeling [6], [11], [12]. It can also be explained by the fact that different forecasting models have expected areas of expertise or competence, referred to as Regions of Competence (RoCs), distributed across different regions of the input time series, which can lead to varying relative performance in these regions [10]–[12]. The nature of the forecasting application can provide a guideline for selecting the adequate family of ML models to be applied. With the increasing number of safety-critical scenarios, model explainability is becoming more and more required [13]. In this case, interpretable models per construction, such as tree- based models, are favored [14], [15]. Still, adequate model selection has to cope with the time-evolving nature of time se- ries data that may be subject to the concept drift phenomenon. However, most of the existing tree-based models, including Decision Trees and their ensembles, such as Random Forests and Gradient-boosted Trees, are restricted to operate in a static manner in the ML literature, i.e., they do not take into account changes in the time series [14], [16]–[18]. One possible way to mitigate this issue is either to retrain them periodically in a blind manner, i.e., without any knowledge or assumption about the presence of concept drift, or to apply them together with a drift detection mechanism that triggers their update in an informed manner only when necessary [6], [11], [12]. More recently, the concept of Regions of Competence (RoCs) has been used for model selection [7], [10]–[12], [19] in the forecasting task. Different ways to determine these RoCs have been proposed [7], [10], [11], including model- type-independent approaches that use meta-learning by either training meta-models to predict the performance of candidate models on the most recent time series pattern of k-lagged values [10] or at a particular test time [7], [19]. The selection approaches in these works are performed online in a blindarXiv:2401.01124v1 [cs.LG] 2 Jan 2024 manner at each time step, i.e., without taking into account the occurrence of significant changes in either the time series data or the performance of the candidate models. In [11], a model-specific approach, in particular for Deep Neural Networks (DNNs), was developed to compute RoCs. Although the RoCs are updated in an informed manner following a concept drift in the time series, the proposed method is specific to a particular class of DNNs, namely Convolutional Neural Networks (CNNs), and cannot be generalized to any family of forecasting models as it is based on Grad-CAM [11], a gradient-based heatmapping method for highlighting important input regions in CNNs. The advantage of the developed method is that it provides some visual explanations for the reason for selecting a particular CNN at a particular test time. Even though the model selection process is made explainable via heatmaps generated by Grad-CAM, the candidate CNNs themselves are considered black-box models that suffer from lack of interpretability and transparency, thus limiting their applicability in many real-world scenarios and their acceptance by end-users [20], [21]. In this paper, we propose an explainable online adaptive tree-based model selection strategy for time series forecasting. The selection among candidate tree-based models is based on the concept of RoCs, which are determined using Tree- SHAP, which is a tree-based method for explaining individual predictions by computing the contribution of each feature to the prediction method in the form of Shapley values [22]. We devise a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series using a performance-based ranking over a time-sliding window validation set. At each time step, the distances between the recently observed window of time series observations (i.e., lagged values used to compute the forecast) and the pre-computed RoCs are determined. The model corresponding to the RoC with the lowest distance is selected to perform the forecasting. At test time, to account for the potential emergence of new concepts in the data, the pre-calculated RoCs are dynamically updated when a concept drift is identified in the time series. This is achieved by shifting the validation set, thereby enabling the computation of RoCs that reflect the changing nature of the data. In addition, our method supports explainability on three levels, namely, input time series importance, model selection, and model output explanation. We carried out an extensive empirical evaluation of our framework by applying it to 1032 real-world time series datasets from diverse domains. The results obtained indicate that our method outperforms state-of-the-art techniques for online model selection and several baselines for time series forecasting. It is worth mentioning that our experiments are entirely reproducible, and we have made both the code and how to access the datasets publicly available1. The main contributions of this paper are thus summarized as follows: • We present a novel method for online tree-based-model 1https://github.com/MatthiasJakobs/tsms selection for time series forecasting by computing candi- date models’ RoCs using an adaption of the TreeSHAP method. • We update the RoCs in an informed manner following concept drift detection in the time series data. • We exploit the RoCs to support explainability on three levels, namely, input time series selection, model selec- tion, and model output explanation. • We provide a comparative empirical study with state-of- the-art methods and discuss their implications in terms of predictive performance and scalability. II. RELATED WORK Online single model selection has been revealed to be challenging in the context of time series forecasting due to the dynamic nature of the data [6] and the resulting time- evolving models’ performance/competence [11]. Several tech- niques have been proposed to choose the appropriate model from a set of candidate models for a specific forecasting task. These techniques can be classified into three primary families. The first family involves approximating a posterior distribution over each candidate’s expected error using parametric or non- parametric estimation methods, such as Gaussian approxima- tion [23] or Bayesian estimation [24]. However, these meth- ods are not practical in forecasting contexts as they require approximating continuous composite densities for the error based on target and estimated time series values. The second family of selection methods involves estimating the unseen error of a given model through empirical evaluations, such as using an independent validation or calibration dataset. Models with the lowest estimated error are subsequently chosen [25]. However, the empirical error may in many cases not serve as an accurate estimate of the true error [26]. The third family of methods is based on the meta-learning paradigm, where the selection of the appropriate method is decided by another machine learning model that learns from previous selection realizations characterized by a set of devised meta- features [27]. Meta-learning has been established as a useful tool for model selection in time series forecasting [6], [7], [11]. More recently, the concept of Regions of Competence (RoCs) has been used both for the selection of a single model [11] and for the selection of ensemble members [7], [10], [19] in the forecasting task. In [10], at test time, the most similar pattern to the current input (i.e., in our case, time series input sequence) is determined, and the model with the smallest error is selected for prediction. In [7], [19], meta-learning is used to build models capable of modeling the competence of each of the ensemble members across the input space. The authors frame their ensemble learning as a ranking task, in which ensemble members are ranked sequentially by their decreasing weight (i.e., the one predicted to perform better is ranked first). Correlation among the output of the base learners is used to quantify their redundancy. A given learner is penalized for its correlation to each learner already ranked. If it is fully correlated with other learners already ranked, its weight becomes zero. Opposingly, if it is completely uncorrelated with its ranked peers, it gets ranked with its original weight. In the above methods, the meta- models responsible for computing the RoCs are kept static over time. Only distances or error comparisons are performed online in a blind-manner at each time step, i.e., without taking into account the occurrence of significant changes in either the time series data or the performance of the candidate models. In [11], RoCs are determined using performance gradient- based saliency maps, which establish a mapping between the input time series and the performance of the DNNs. A drift de- tection mechanism in the data is deployed to update the RoCs. These maps are inspired by class activation maps (Grad-CAM) [28], which have been widely used as a visual explainability tool in computer vision tasks. More recently, Grad-CAM has been applied in the context of time series classification to explain which time series features in which time intervals are responsible for a given predicted class [29]. The authors in [11] exploit the gradient maps to provide explanations for the reason behind selecting a given CNN at a particular time instant or interval. However, the computation of RoCs and the resulting explanations are specific to CNNs and can not be generalized to any other model. In addition, despite making the model selection process explainable, the candidate CNNs are still considered black-box models that lack interpretability and transparency. This limitation restricts their usability in many real-world situations and their acceptance among end-users [20], [21]. Therefore, in this work, we focus on interpretable models per construction, namely tree-based models, and we leverage the RoCs concept to adapt them to the online forecasting application through an adaptive model selection procedure. III. METHODOLOGY Our proposed method builds upon OS-PGSM, presented in [11]. First, we will define the used notation. Second, we describe Shapley values with a focus on TreeSHAP [22]. Third, we will reformulate the earlier method and highlight the changes we propose to allow for the efficient selection of tree-based models. A. Preliminaries In this work, we will focus on univariate time series forecasting. Let X be a series of values, with X (t) denoting the value at time step t. We use shift operator notation: T kX (t) = X (t+k) Note that we default to shifting to the right, forwards in time, instead of backward in time. Additionally, we define the shift operator on intervals as follows: T (u:v)X (t) = (T uX (t), T u+1X (t), . . . , T vX (t)) For ease of notation, we further define T (:v) := T (1:v), T (−v:) := T (−v:0) The goal of forecasting at time t is to predict the next H values T (:H)X (t) from known L lagged values T (−L:)X (t). B. Shapley values and TreeSHAP Shapley values, originally proposed in Game Theory [30], have become a popular method for explaining ML models in recent years [31]. They attribute the outcome of a game, given by a value function v, onto each player i who participates in this game. In ML applications, the players are often chosen as the features {i ∈ N } of a specific data point x with an optional label y. Definition 1: The Shapley value ϕi with a defined value function v is given by ϕi(v) = ∑ S⊆N \\{i} (|N | − 1 |S| )−1 v(x, y, S ∪ {i}) − v(x, y, S) |N | One reason often stated for the use of Shapley values in explaining machine learning models is due to desired axioms for which Shapley values are the only solution [31]. We want to highlight one axiom that we will utilize later, namely Linearity, which states that the Shapley value of a linear combination of value functions is equal to the linear combination of the individual Shapley values: ϕi(va) + ϕi(vb) = ϕi(va + vb) ∀i ∈ N TreeSHAP [22] is a method to estimate Shapley values in polynomial time if the models are tree-based. The method can either compute Shapley values directly via the tree structure (tree-dependent approach) or via a held-out set of data points (interventional approach). We use the second approach since the official reference implementation2 allows us to explain not only the prediction but also the squared loss of tree-based models, whereas the tree-dependent approach is limited to explaining the prediction. TreeSHAP defines, depending on the problem to be ex- plained, two different value functions. Let g : RN → R be a trained regression decision tree, which takes N features as input. If the decision is to be explained, the value function is defined by vg,pred(x, S) = E X∼X [g(X | XS = xS)] (1) where X is a set of reference data, often referred to as background data. We set the background data to be equal to the training data for the trees in our experiments. The other value function is used if we want to explain the squared loss of a tree’s prediction and is defined by vg,loss(x, y, S) = E X∼X [(g(X | Xs = xS) − y)2] (2) TreeSHAP implements an efficient, dynamic programming- based algorithm to compute the Shapley values for one data- point in polynomial time by going down the tree just once. By leveraging the Linearity axiom, the computed Shapley values of individual trees in an ensemble can be combined to give the Shapley values of the ensemble [22]. 2https://github.com/slundberg/shap C. TreeSHAP Model Selection (TSMS) Next, we present our approach, TreeSHAP Model Selection (TSMS). Assume a model pool P = {f1, . . . , fM } of M forecasters from which we want to choose the forecaster ˆf that minimizes a loss L for a window T (−L:)X (t) and its corresponding horizon T (:H)X (t) i.e., ˆf = arg min i∈{1,...,M } L (fi(T (−L:)X (t)), T (:H)X (t)) Each time series X is partitioned into three subseries Xtrain, Xval and Xtest. The forecasters are first trained on windowed segments of Xtrain. Afterwards, let R := {R1, . . . , RM } define each forecasters Region of Competence (RoC) over Xval. RoC Creation: A RoC is filled with subseries of data on which the forecaster performed best in comparison to all other forecasters. First, we partition Xval into nω chunks of size ω: Xval,ω = { T (0:ω−1)X (1), T (0:ω−1)X (1+ω), . . . , T (0:ω−1)X (1+(nω−1)ω) } with corresponding forecasting labels Yval,ω = { T (:H)X (ω), T (:H)X (2ω), . . . , T (:H)X (nωω) } For each pair (X k val,ω, Y k val,ω) with k ∈ {1, . . . , nω}, we find the best performing forecaster ˆfk in terms of squared loss L: ˆfk = arg min i∈{1,...,M } L(fi(X k val,ω), Y k val,ω) Next, we split each X k val,ω further into windows of size L with a step size of one: xval,ω,k = { T (0:L−1)X k,(1) val,ω, T (0:L−1)X k,(2) val,ω, . . . , T (0:L)X k,(ω−L−H+1) val,ω } yval,ω,k = { T (:H)X k,(L) val,ω , T (:H)X k,(L+1) val,ω , . . . , T (:H)X k,(N −H) val,ω } Again, we consider each pair (xp val,ω,k, yp val,ω,k) and compute Shapley values, decomposing the loss L( ˆfk(x p val,ω,k), yp val,ω,k) onto each input feature, resulting in explanation ϕp k ∈ RL. Our aim is to add refined versions of each xp val,ω,k to the RoC of ˆfk to retain only the most salient parts of the input that reduced the achieved loss the most. Thus, to get a measure of saliency, we need to invert the Shapley values so that large positive values now indicate a large reduction of loss and vice versa. To refine, we threshold −ϕp k by a positive constant τ , setting every explanation below the threshold to zero. We then use the indices of consecutive, non-zero subsequences resulting from the thresholding to extract subsequences of xp val,ω,k if they are longer than 2. For example, consider the thresholded explanation −ϕ = (0, 0.5, 0.3, 0, 0.1, 0.2, 1.3) for some sequence x. Then, we would extract x(5:7) and add it to the Region of Competence. We decided to discard subseries of size 2 and below since they do not reveal clear patterns and thus add a lot of noise to the RoCs. Model selection: During testing, for each new window T (−L:)X (t) test, the distance to all RoC members for all fore- casters is evaluated, using Dynamic Time Warping (DTW). We chose DTW because it allows us to have RoC members of different lengths since DTW does not constrain its inputs to have equal lengths. The forecaster ˆf , whose RoC R contains the datapoint with the smallest DTW distance to T (−L:)X (t) test is chosen to forecast, i.e., ˆf = arg min i∈{1,...,M } min({DT W (T (−L:)X (t) test, r) | r ∈ Ri}) Over time, the distribution of data might change, which is a phenomenon known and studied as concept drifts [4]. We want our method to adapt to those changes by enriching the Regions of Competence with novel concepts when we detect such drifts. To account for drifts, we utilize the same concept drift detection method using Hoeffding bounds presented in [11], which monitors the deviation of the mean of the time series [6]. Let (tstart, tend) be the first and last timestep of the validation data range inside the entire time series X. Let µ0 = E[T (tstart:tend)X] be the initial expected value over the data. Then, for each new timestep j, j > tend to forecast, we compute ∆j = |µj − µ0| with µj = E[T (tend+1:j)X]. The Hoeffding bound states that after W observations of a random variable with range r the true mean has diverged from 0 with probability σ if ∆j > √ r2ln(2/σ) 2W where r encodes the range of the data, which we estimate empirically, and W = j − tend + 1. We set the user-defined parameter σ = 0.99 to be confident in concept drifts actually occurring. Also, after each concept drift, we set µ0 = µj and reset tstart and tend with the range of µj. Most importantly, we create a new RoC for each forecaster from that data. Afterward, we enrich the old RoCs with the newly created RoCs to retain both old information as well as adapt to changes in the data. IV. EXPERIMENTS In this section, we aim to answer the following research questions: • Q1: How does TSMS perform against state-of-the-art model selection methods for tree-based models and other relevant baselines? • Q2: What is the impact of the concept drift adaptation in terms of performance and computational resources, com- pared to both static pre-computation and blind periodic recreation of the RoCs? • Q3: How can TSMS in conjunction with tree-based models serve explainability for time-series forecasting? Name Nr. of time series Min. length M4 548 251 tourism 100 252 Australian Electricity Demands 5 500 Dominick 100 251 Bitcoin 16 500 Pedestrian counts 63 500 KDD Cup 100 500 Weather 100 500 Total 1032 TABLE I BREAKDOWN OF THE USED UNIVARIATE TIME-SERIES FROM THE MONASH FORECASTING REPOSITORY [1]. Model family Hyperparameters Decision Tree dmax ∈ {4, 8, 16} Random Forest dmax ∈ {2, 4, 6} ntrees ∈ {16, 32, 64} Gradient Boosting Trees dmax ∈ {2, 4, 6} ntrees ∈ {16, 32, 64} TABLE II LIST OF ALL MODELS IN THE MODEL SELECTION POOL. THROUGH COMBINATION OF THE LISTED HYPERPARAMETERS, THERE ARE 21 MODELS IN THE FINAL MODEL POOL. A. Experimental Setup We utilize 1032 univariate datasets from various appli- cation domains, including financial, weather, and synthetic data. These datasets are provided by the Monash Forecasting Repository [1]. We process each time-series X by using the first 50% for training, the following 25% for validation, and the remaining 25% for testing. We normalize the entire time series using mean and standard deviation estimated over the training portion of the time series. Due to this way of splitting the time-series, we discard series that are shorter than 250 to allow enough training and validation data. Additionally, we took a random subsample of length 500 for series that are longer to limit computation time. To create the model pool P, we train a total of 21 tree- based models. We set L = 15 and H = 1 in our experiments to allow for a larger context in terms of lags. The single models consist of various parametrizations of Decision Trees, Random Forests, and Gradient Boosting Trees from scikit-learn. We vary the maximum tree depth dmax and (if available) the number of ensemble estimators ntrees (details shown in Table II). All experiments have been performed on consumer hardware, namely on a 2022 MacBook Pro. B. TSMS Setup and Baselines To investigate the impact of drift detection on model per- formance and runtime, we propose a total of three variants of our method: • TSMS: Our described method, using drift detection to adapt the RoCs to changes in the time-series • TSMS-St: Static variant where drift-detection is disabled. RoCs are created at the beginning of the forecasting process and remain unchanged over time • TSMS-Per: A periodic, blind update of the RoCs. We chose to update the RoCs automatically 10 times in total over the length of Xtest at fixed, equally spaced intervals Additionally, there are two hyperparameters in our method that we tuned: ω, the size of larger, equal-size windows during the RoC creation process, and τ , the threshold used when refining the data points before addition to an RoC. We conducted a hyperparameter search and found ω = 25 and τ = 0.01 to perform very well. Next, we shortly describe the baseline methods we compare ourselves against: • Exponential Smoothing ETS [32] and ARIMA [32] which are included as simple, yet important baselines. • KNN-RoC [10] computes static RoCs using a validation set as input and the rank of the individual candidates in the pool P on each interval as labels for a KNN classifier, using DTW distance and for single model selection K = 1, is used for comparison. The KNN predicts which candidate should be selected at test time • CNN and CNN-LSTM [33] are Convolutional Neu- ral Network baselines with either a fully-connected or LSTM-based forecaster after the features are extracted • DETS [19] is an extension of SWE [3], which is an ensemble method that weights the ensemble members based on past performance. DETS extends this by se- lecting a subset only and using a smoothing function on the average over recent errors for weighting. We compare against selecting the model with the largest weight • ADE [7], [19] was recently developed for an online dynamic ensemble of forecasters construction. A meta- learning strategy is used that specializes base models across the time series to determine their RoC (see Section II). However, instead of selecting many models, we select the best-performing model using the same principle. Because of our use of tree-based model, we want to high- light that we cannot compare ourselves to OS-PGSM [11] because the authors utilize Grad-CAM [28], which only works for Convolutional Neural Networks. C. Performance Comparison Results Table III shows a comparison between our method, in- cluding its different variations, to the previously mentioned baselines over 1032 datasets. We computed the average rank achieved after measuring the loss using the RMSE score. As can be seen, the drift-aware variant of TSMS achieves the smallest average rank and outperforms other online-selection baselines such as DETS, ADE, and KNN-RoC. For com- parison, we also include the best-performing model from P, denoted as Best-Single, and find that it also outperforms the model-selection baselines, indicating that they are not able to predict the performance of this single model on the test data very well. TSMS is also able to outperform CNN-LSTM and Method Avg. Rank Std Deviation Wins Losses ETS 7.14 4.53 789 (740) 212 (209) ADE 7.03 3.01 687 (601) 314 (218) ARIMA 6.46 5.04 737 (679) 264 (200) KNN-RoC 6.36 2.71 633 (578) 368 (221) DETS 6.31 2.61 641 (596) 360 (295) CNN 5.49 3.35 682 (638) 319 (314) CNN-LSTM 4.85 2.68 602 (518) 399 (378) TSMS-St 4.64 2.88 361 (279) 640 (427) Best-Single 4.36 2.71 582 (502) 419 (414) TSMS-Per 4.19 2.60 334 (240) 667 (482) TSMS 4.13 2.49 - (-) - (-) TABLE III COMPARISON (IN TERMS OF AVERAGE RANK ACHIEVED OVER 1032 DATASETS) BETWEEN OUR METHOD AND THE BASELINES. BEST-SINGLE IS THE MODEL FROM P THAT PERFORMED BEST OVER ALL DATASETS. Method TSMS TSMS-St TSMS-Per Runtime [s] 1.39 ± 1.28 0.41 ± 0.31 1.71 ± 1.28 TABLE IV MEAN RUNTIME (IN SECONDS) PLUS/MINUS ONE STANDARD DEVIATION, MEASURED OVER ALL DATASETS. CNN while being more interpretable at the same time due to our limitations of utilizing depth-restricted tree-based models. We also ran a signed Wilcoxon rank test [34] on the wins and losses and indicate the significant wins/losses in paranthesis (significance level 0.05). The results clearly answer research question Q1. In comparison to two different versions of TSMS, namely TSMS-St, which only creates the RoCs once at the start of in- ference and keeps static, as well as TSMS-Per, where the RoCs are blindly recreated, we find that an informed, i.e., based on the detection of concept drifts, and infrequent recreation of the RoCs leads to better results in terms of predictive performance. We observe that the periodic approach is only slightly better in terms of average ranking than the static approach, indicating that a blind recreation of RoCs is in most cases not worth the additional computational overhead. We additionally compare the runtime of each variant over a set of all datasets and report the average runtime, and its standard deviation, in Table IV. Unsurprisingly, TSMS-St is, on average, the fastest method since the RoC recreation procedure takes up most of the runtime. The adaptive variant TSMS falls in between the static and periodic variants, suggesting that the small trade in runtimes is worthwhile for improving predictive performance. With these insights, we provide answers for research question Q2. D. Explainability In this part, we answer research question Q3 by showing how TSMS supports explainability and covers multiple aspects of the forecasting task: • A1: Which input time series parts, i.e., lagged values, are more relevant for the prediction, and how this relevance evolves over time? 15 13 11 9 7 5 3 1 lag −2.5 −2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 Why was forecaster fi chosen? Window to forecast Closest RoC Furthest RoC 15 12 9 6 3 0 lag −0.5 0.0 0.5 1.0 1.5 Why did forecaster fi predict its output? Feature intervals Window to forecast Ground Truth Forecasted value Closest RoC Forecasted value Furthest RoC Shapley values for prediction Fig. 1. Visualization of A2 and A3 by investigating the closest and furthest RoC members, in addition to Shapley values and feature decision boundaries. • A2: Why a given model i is chosen to forecast the series at time t? • A3: Why a specific predicted value ˆx(t+1) is output by the selected model at time instant t? • A4: What is the impact of concept drift occurrence on the pre-computed RoCs? First, on the lower half of Figure 1, we can see the Shapley values of the 15-time-lagged values used to forecast the true value of the time series marked in blue. As mentioned earlier, by swapping the value function used for estimating Shapley values, we can explain both the achieved loss and the prediction itself. For the prediction, the higher the bars (in blue), the more important the lag information fed to the forecasting model is. 15 12 9 6 3 0 lag −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 Feature importance f7 on x(t) before drift 15 12 9 6 3 0 lag −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 Feature importance f9 on x(t+19) after drift Prediction before Ground Truth Shapley values Prediction after Fig. 2. Comparison of Shapley values for prediction before and after concept drift where the chosen forecaster changes. As can be seen, many lags among the 15 lags seem to be irrelevant to predict the next value of the series. Surprisingly, this information is sparse and not strongly condensed around the most recent lags, in opposite to the assumption made by most of the traditional state-of-the-art models, including the Box-Jenkins ARIMA family of models [35]. This can be ex- plained by the fact that these models assume the stationarity of the time series-generating process and the absence of concept drifts. So, once analyzed, lag information and contribution in these models are kept fixed and restricted to the most three to four recent lagged observations [3]. However, this is not the case for many real-world scenarios. As can be seen in Figure 2, the second most recent lag (lag 2) that used to have very low importance for the prediction before the occurrence of concept drift has become the most relevant one after the detection of drift. This shift in the importance of the input highlights the necessity of either triggering an update of the model using retraining or switching to another model that better handles the new dependence structure in the data after the occurrence of concept drift. Figure 2 shows that our framework ensures this update by switching to another model after drift detection, which predicts the true value of the series better. The prediction marked in red would be the forecast value if we stuck to the old model after drift detection, while the prediction marked in orange is the value output by the model selected by our method, which is closer to the true target value marked in green. Another potential insight provided by the tree-based models in relation to understanding the contribution of the lag information in the input time series to the prediction is provided by the paths taken in each tree. These paths enable the construction of boundaries around each lag via each node split criterion. This means that changing the input value anywhere between the yellow bars in Fig. 1 will have no effect on the models’ prediction, which helps in understanding the robustness of the tree-based models. This knowledge is also of great help to forecasting practitioners as it helps them in conducting input change sensitivity analysis which is required in many forecasting financial and trading applications. This addresses the explainability aspect raised in Question A1. As previously explained, the reason for selecting forecaster i at time t is because its RoC contains a subseries that is the most similar to the current input pattern T (−L:)X (t) in terms of Dynamic Time Warping. In Figure 1 (upper half), we show the closest RoC marked in green to the current input pattern T (−L:)X (t). Its corresponding forecaster is thus chosen to forecast this data point. Examining the predictions of the model with the furthest RoC shown in red in Figure 1 (lower half) and the prediction of our selected model marked in green validates our RoC-based selection as showing competence on patterns similar to the input pattern in question reflects the readiness of the model to process this pattern and output a prediction based on it. Hence, the large deviation between the RoC of the furthest model and the current pattern in Figure 1 on the upper half is reflected in its prediction shown in the lower half. This answers the explainability aspect raised in Question A2. Next, we provide some insights into how our method can help us to anticipate the model’s output. This is one of the main important aspects of explainability as stated by Kim et al. [36], who define explainability as the degree to which a human can consistently predict the model’s result. In Figure 3, we visualize the RoC of the selected model f17, including their subsequent values seen during RoC creation, which are marked in a dotted line. A clear similarity between the output forecast by the model and the values that are subsequent to the RoCs sequences can be observed. The range of the subsequent values helps us in estimating the expected forecast value by the selected model on average. Comparing the predicted value by the model with this estimated average can be considered a debugging tool that helps us in detecting abnormal behavior of the model or check for significant changes in the data. This addresses the explainability aspect raised in Question A3. To get a better understanding of the impact of drift detection, we investigate two scenarios. First, shown in Fig. 4, is a scenario where the drift detection was triggered but where the same model that would have been chosen without a drift was chosen regardless. Nevertheless, as can be seen in a comparison between the middle and lower figures, in this case, the closest RoC member did, in fact, change. The new closest 15 12 9 6 3 0 lag −0.5 0.0 0.5 1.0 15 12 9 6 3 0 lag −1.0 −0.5 0.0 0.5 1.0 1.5 2.0 Visualization of RoC for model f17 Ground Truth Prediction Closest RoC member Fig. 3. Visualization for R17 RoC member (shown in orange) much more resembles the input in comparison to the previously closest RoC member (shown in red), indicating that the enrichment of the RoCs after the detected drift did adapt the RoCs to the time-changing data. Lastly, we consider a scenario where the chosen model did, in fact, change after a drift was triggered. As can be seen in Fig. 5, without enrichment of the RoCs, f13 would have been chosen over f14 in this case. However, when investigating the prediction (upper figure), the prediction of f16 (yellow) is closer to the ground truth value (green) compared to the prediction of f13 (red). Additionally, by investigating each model’s RoC (middle and lower figure), it becomes clear that f16 contains a member that resembles the input a lot more in comparison to the closest member in R13. These observations highlight the usefulness of the drift-aware adaptation of the RoCs over time. This answers the research question A4. E. Discussion and future work The empirical results indicate that TSMS performs very well in comparison to the tested baselines while simultane- ously allowing insight into the decision-making process. We argue that this combination is crucial when these methods are to be applied in real-world settings, especially in safety- critical applications where explainability and ML transparency 15 13 11 9 7 5 3 1 lag −1.0 −0.5 0.0 0.5 1.0 1.5 2.0 Datapoint to forecast 15 13 11 9 7 5 3 1 lag −1.0 −0.5 0.0 0.5 1.0 1.5 2.0 R10 before drift 15 13 11 9 7 5 3 1 lag −1.0 −0.5 0.0 0.5 1.0 1.5 2.0 R10 after drift Change in closest subseries for model f10 Fig. 4. Comparison of R10 before and after a drift. Closest RoC member shown in orange and red, respectively. are required. We showcased, moreover, that TSMS is able to adapt well to changes in the underlying time series data without the need to retrain every model in the pool. In future work, we plan to extend our method to hybrid model pools by using the most efficient Shapley value estima- tion methods for each model family, such as TreeSHAP for tree-based models, DeepSHAP [37] for Neural Networks, as well as KernelSHAP [37] for remaining models. Additionally, we plan to investigate the effect of other drift detection meth- ods, especially in terms other than monitoring the deviation of the time series mean value. Lastly, we want to enhance the 15 12 9 6 3 0 lag 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Prediction using selected models 15 12 9 6 3 0 lag 0.175 0.200 0.225 0.250 0.275 0.300 RoCs of f13 (best before drift) 15 12 9 6 3 0 lag −0.4 −0.2 0.0 0.2 0.4 0.6 0.8 RoCs of f14 (best after drift) Change in model selection after drift detection Window to forecast Ground truth Prediction f13 Prediction f14 Closest in R13 RoC member Closest in R14 Fig. 5. Comparison of R13 and R14 before and after a drift. Closest RoC member shown in orange and red, respectively. explainability aspects further by reducing the RoCs number to the most important ones, which is highly important, especially for long time series where the number of RoCs is expected to grow very largely. V. CONCLUDING REMARKS This paper introduces TSMS a novel method for online adaptive model selection on a pool of tree-based models. Through the use of Shapley values, we are able to gain insight into its decision-making process, both for model selection, as well as for the individual model predictions. We showed the advantages of TSMS on 1032 real-world datasets, both in terms of predictive performance as well as its explainability aspects. ACKNOWLEDGEMENT This research has been funded by the Federal Ministry of Education and Research of Germany and the state of North Rhine-Westphalia as part of the Lamarr Institute for Machine Learning and Artificial Intelligence REFERENCES [1] R. Godahewa, C. Bergmeir, G. I. Webb, R. J. Hyndman, and P. Montero- Manso, “Monash time series forecasting archive,” in Neural Information Processing Systems Track on Datasets and Benchmarks, 2021, forthcom- ing. [2] R. J. Hyndman, A. B. Koehler, R. D. Snyder, and S. Grose, “A state space framework for automatic forecasting using exponential smoothing methods,” International Journal of forecasting, vol. 18, no. 3, pp. 439– 454, 2002. [3] A. Saadallah, L. Moreira-Matias, R. Sousa, J. Khiari, E. Jenelius, and J. Gama, “Bright-drift-aware demand predictions for taxi networks,” IEEE Transactions on Knowledge and Data Engineering, 2018. [4] J. Gama, I. ˇZliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia, “A survey on concept drift adaptation,” ACM computing surveys (CSUR), vol. 46, no. 4, pp. 1–37, 2014. [5] R. J. Hyndman, E. Wang, and N. Laptev, “Large-scale unusual time series detection,” in 2015 IEEE international conference on data mining workshop (ICDMW). IEEE, 2015, pp. 1616–1619. [6] A. Saadallah, F. Priebe, and K. Morik, “A drift-based dynamic ensemble members selection using clustering for time series forecasting,” in Joint European conference on machine learning and knowledge discovery in databases. Springer, 2019. [7] V. Cerqueira, L. Torgo, F. Pinto, and C. Soares, “Arbitrated ensemble for time series forecasting,” in Joint European conference on machine learning and knowledge discovery in databases. Springer, 2017, pp. 478–494. [8] D. H. Wolpert, “The lack of a priori distinctions between learning algorithms,” Neural computation, vol. 8, no. 7, pp. 1341–1390, 1996. [9] A. Saadallah, L. Moreira-Matias, R. Sousa, J. Khiari, E. Jenelius, and J. Gama, “Bright—drift-aware demand predictions for taxi networks,” IEEE Transactions on Knowledge and Data Engineering, vol. 32, no. 2, pp. 234–245, 2020. [10] F. Priebe, “Dynamic model selection for automated machine learning in time series,” 2019. [11] A. Saadallah, M. Jakobs, and K. Morik, “Explainable online deep neural network selection using adaptive saliency maps for time series fore- casting,” in Machine Learning and Knowledge Discovery in Databases. Research Track, N. Oliver, F. P´erez-Cruz, S. Kramer, J. Read, and J. A. Lozano, Eds. Cham: Springer International Publishing, 2021, pp. 404– 420. [12] ——, “Explainable online ensemble of deep neural network pruning for time series forecasting,” Machine Learning, vol. 111, no. 9, 2022. [13] C. Molnar, Interpretable Machine Learning, 2020. [Online]. Available: https://christophm.github.io/interpretable-ml-book/ [14] S. B. Taieb and R. J. Hyndman, “A gradient boosting approach to the kaggle load forecasting competition,” International journal of forecast- ing, vol. 30, no. 2, pp. 382–394, 2014. [15] I. Ilic, B. G¨org¨ul¨u, M. Cevik, and M. G. Baydo˘gan, “Explainable boosted linear regression for time series forecasting,” Pattern Recognition, vol. 120, p. 108144, 2021. [16] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp. 5–32, 2001. [17] J. H. Friedman, “Greedy function approximation: a gradient boosting machine,” Annals of statistics, pp. 1189–1232, 2001. [18] A. Galicia, R. Talavera-Llames, A. Troncoso, I. Koprinska, and F. Mart´ınez- ´Alvarez, “Multi-step forecasting for big data time series based on ensemble learning,” Knowledge-Based Systems, vol. 163, pp. 830–841, 2019. [19] V. Cerqueira, L. Torgo, F. Pinto, and C. Soares, “Arbitrage of forecasting experts,” Machine Learning, 2018. [20] Y. Liang, S. Li, C. Yan, M. Li, and C. Jiang, “Explaining the black- box model: A survey of local interpretation methods for deep neural networks,” Neurocomputing, vol. 419, pp. 168–182, 2021. [21] A. Saadallah and K. Morik, “Active sampling for learning interpretable surrogate machine learning models,” in 2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA). IEEE, 2020, pp. 264–272. [22] S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb, N. Bansal, and S.-I. Lee, “From local explanations to global understanding with explainable AI for trees,” vol. 2, pp. 56–67, 2020. [Online]. Available: https://www.nature.com/articles/s42256-019-0138-9 [23] L. Birg´e and P. Massart, “Gaussian model selection,” Journal of the European Mathematical Society, vol. 3, no. 3, pp. 203–268, 2001. [24] R. Argiento, A. Guglielmi, and A. Pievatolo, “Bayesian density esti- mation and model selection using nonparametric hierarchical mixtures,” Computational Statistics & Data Analysis, vol. 54, no. 4, pp. 816–832, 2010. [25] I. Rivals and L. Personnaz, “On cross validation for model selection,” Neural computation, vol. 11, no. 4, pp. 863–870, 1999. [26] S. Shalev-Shwartz and S. Ben-David, Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. [27] D. H. Wolpert, “Stacked generalization,” Neural networks, vol. 5, no. 2, pp. 241–259, 1992. [28] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-cam: Visual explanations from deep networks via gradient-based localization,” in Proceedings of the IEEE international conference on computer vision, 2017. [29] R. Assaf and A. Schumann, “Explainable deep neural networks for multivariate time series predictions.” in IJCAI, 2019, pp. 6488–6490. [30] L. S. Shapley, “A value for n-person games,” vol. 2, no. 28, pp. 307–317, 1953. [31] B. Rozemberczki, L. Watson, P. Bayer, H.-T. Yang, O. Kiss, S. Nilsson, and R. Sarkar. The Shapley Value in Machine Learning. [Online]. Available: http://arxiv.org/abs/2202.05594 [32] G. Jain and B. Mallick. A Study of Time Series Models ARIMA and ETS. [Online]. Available: https://papers.ssrn.com/abstract=2898968 [33] P. Romeu, F. Zamora-Mart´ınez, P. Botella-Rocamora, and J. Pardo, “Time-Series Forecasting of Indoor Temperature Using Pre-trained Deep Neural Networks,” in Artificial Neural Networks and Machine Learning – ICANN 2013, ser. Lecture Notes in Computer Science, V. Mladenov, P. Koprinkova-Hristova, G. Palm, A. E. P. Villa, B. Appollini, and N. Kasabov, Eds. Springer, 2013, pp. 451–458. [34] A. Benavoli, G. Corani, and F. Mangili, “Should we really use post-hoc tests based on mean-ranks?” The Journal of Machine Learning Research, vol. 17, no. 1, pp. 152–161, 2016. [35] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: forecasting and control. John Wiley & Sons, 2015. [36] B. Kim, R. Khanna, and O. O. Koyejo, “Examples are not enough, learn to criticize! criticism for interpretability,” Advances in neural information processing systems, vol. 29, 2016. [37] S. M. Lundberg and S.-I. Lee, “A Unified Approach to Interpreting Model Predictions,” in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4765–4774. [Online]. Available: http://papers.nips.cc/ paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf","libVersion":"0.3.2","langs":""}
{"path":"lit/lit_sources/Kasa24AdaptingConformalPrediction.pdf","text":"Adapting Conformal Prediction to Distribution Shifts Without Labels Kevin Kasa ∗1,2, Zhiyu Zhang3, Heng Yang3, and Graham W. Taylor 1,2 1School of Engineering, University of Guelph 2Vector Institute for Artificial Intelligence 3Harvard University Abstract Conformal prediction (CP) enables machine learning models to output prediction sets with guaranteed coverage rate, assuming exchangeable data. Unfortunately, the exchangeability assumption is frequently violated due to distribution shifts in practice, and the challenge is often compounded by the lack of ground truth labels at test time. Focusing on classification in this paper, our goal is to improve the quality of CP-generated prediction sets using only unlabeled data from the test domain. This is achieved by two new methods called ECP and EACP, that adjust the score function in CP according to the base model’s uncertainty on the unlabeled test data. Through extensive experiments on a number of large-scale datasets and neural network architectures, we show that our methods provide consistent improvement over existing baselines and nearly match the performance of supervised algorithms. 1 Introduction Advances in deep learning are fundamentally changing the autonomous decision making pipeline. While most works have focused on accurate point predictions, calibrating the uncertainty of the model is arguably as important. Taking autonomous driving as an example: if a detection model predicts the existence of an obstacle, it would be reasonable to take different maneuvering strategies depending on the confidence of the prediction. But is that reliable? In a possible failure mode, the model could report 60% (resp. 99%) confidence, but the probability of an obstacle actually showing up is 99% (resp. 60%). Such discrepancy between the model’s own uncertainty evaluation and the ground truth probability (or post-hoc frequency) is actually very common [Guo et al., 2017, Liang et al., 2023], and can significantly compromise the safety in downstream decision making. Conformal Prediction (CP; Vovk et al., 2005) is a simple post-hoc and distribution-free method that addresses this problem. Given a fixed black-box machine learning model (called the base model ) and a covariate xtest, the goal of CP is to generate a small prediction set Ctest that contains (or, covers) the unknown ground truth label ytest with a pre-specified probability. Crucially, CP relies on the assumption that the distribution of the data stream is exchangeable (a weaker variant of i.i.d.), which allows the fairly straightforward inference of ytest from xtest and the base model’s performance on a pre-collected calibration dataset. Note that the target label ytest does not need to be revealed after the set prediction is made: exchangeability together with a large enough calibration dataset is sufficient to ensure the desirable coverage probability. This is particularly important for autonomous decision making, where real-time data annotation is expensive or even infeasible. However, real-world data streams are usually corrupted by all sorts of distribution shifts, violating the exchangeability assumption. Even when the data stream itself is exchangeable, we often want to continually update the base model rather than keeping it fixed, and this can be effectively understood as a distribution shift in the CP context. In such cases, simply applying exchangeability-based CP methods could fail miserably [Tibshirani et al., 2019, Bhatnagar et al., 2023a, Kasa and Taylor, 2023]. Therefore, making CP compatible with distribution shifts has become a focal point of recent works. ∗Work done while visiting Harvard University. 1arXiv:2406.01416v1 [cs.LG] 3 Jun 2024 A number of solutions have been proposed, but the key challenge still remains. For example, Gibbs and Candes [2021] formulated the connection between CP and Online Convex Optimization (OCO; Zinkevich, 2003), and the latter is able to handle arbitrarily distribution-shifted environments. The weakness is that ground truth labels are now required at test time, as opposed to the standard CP procedure. On the other direction, there are CP methods that combat distribution shifts without test time labels [Tibshirani et al., 2019, Roth, 2022, Barber et al., 2023, Cauchois et al., 2024], but they assume the distribution shifts are “easy”, such that even without labels, we can still rigorously infer the test distribution to a certain extent using the labeled calibration dataset. Overall, it appears that handling both difficulties – distribution shifts and the lack of test time labels – is a formidable but important challenge remaining in the CP literature. Contributions In this paper, we focus on classification, and address the above problem by exploiting the uncertainty evaluation of the base model itself. Although such a quantity is not necessarily calibrated in a strict sense, it has been empirically shown to strongly correlate with the magnitude of distribution shifts [Wang et al., 2021], thus providing a valuable way to probe the test distribution without label access. Under this high level idea, we make the following contributions: • First, we propose a new method named ECP (Entropy scaled Conformal Prediction), which scales the score function in CP by an “entropy quantile” 1 of the base model on the test distribution. In the CP jargon, the score function is determined by the fixed base machine learning model, and assigns each candidate label a “propensity score” after observing the covariate xtest. Then, the CP prediction set Ctest simply includes all the labels whose score is above a certain threshold.2 By scaling up the score function, ECP makes the prediction sets larger, which naturally corresponds to the intuition that the uncertainty of the base model should be inflated under distribution shifts. Moreover, the amount of such inflation is correlated with the magnitude of the distribution shift, through the use of the entropy quantile. • Second, we refine the above using techniques from unsupervised Test Time Adaptation (TTA) [Niu et al., 2022], and the resulting method is named as EACP (Entropy base-Adapted Conformal Prediction). The key idea is that while ECP keeps the base model fixed during testing, we may concurrently update it using entropy minimization [Grandvalet and Bengio, 2004, Wang et al., 2021] – a widely adopted idea in unsupervised TTA, alongside the aforementioned entropy scaling. This “adaptively” reduces the scaling effect that ECP applies on the score function, thus making the prediction sets of ECP smaller. • Finally, despite the loss of statistical guarantees, we evaluate the proposed methods on a wide range of large-scale datasets under distribution shifts, as well as different neural network architectures. We find that exchangeability-based CP (with and without TTA on the base model) consistently leads to lower-than-specified coverage frequency, but our methods can effectively mitigate this under-coverage issue while keeping the sizes of the prediction sets moderate. Furthermore, our methods also significantly improve the prediction sets generated by the base model itself (without CP). It shows that by bridging the CP procedure (which is statistically sound) and the base model’s own uncertainty evaluation (which is often informative), our methods enjoy the practical benefit from both worlds. 2 Related works CP in decision making Our interest in the considered setting – distribution shifts without test time labels – is mainly motivated by the growing applications of CP in autonomous decision making. A very much incomplete list: see [Lekeufack et al., 2023] for a generic treatment; [Lindemann et al., 2023] for trajectory optimization in robotics; [Yang and Pavone, 2023, Gao et al., 2024] for 3D vision; [Kumar et al., 2023, Quach et al., 2024] for large language models (LLM); and [Ren et al., 2023] for LLM-powered robotics. CP under distribution shifts Considerable efforts have been devoted to developing CP methods robust to distribution shifts. We now discuss two possible directions and their respective limitations. 1Enforced to be larger than 1 to ensure that the prediction sets do not become smaller on in-distribution data. 2Since we mainly focus on classification, the score functions are assumed to be positively oriented [Sadinle et al., 2019]. 2 • The first direction does not require test time labels, but the distribution shift is assumed to be simple in some sense. For example, Tibshirani et al. [2019] studied CP under covariate shifts, where the distribution of the label Y conditioned on the covariate X remains unchanged. Here, it suffices to use the classical likelihood ratio reweighting on the calibration dataset, but due to the difficulty of likelihood ratio estimation, one could also take a robust optimization perspective [Cauchois et al., 2024] and apply multicalibration on the possible test domain, as sketched in [Roth, 2022, Chapter 8]. Barber et al. [2023] generalized the reweighting approach to handle mild general distribution shifts, but the choice of weights is generally unclear in practice. • The second direction is connecting CP to adversarial online learning. A line of works [Gibbs and Candes, 2021, Angelopoulos et al., 2023, Gibbs and Candès, 2023, Bhatnagar et al., 2023b, Zhang et al., 2024] applied regret minimization algorithms in OCO to select the score threshold in CP, and Bastani et al. [2022] achieved this task using multicalibration. By relaxing the CP objective from the coverage probability to the post-hoc coverage frequency, these methods can handle arbitrary continual distribution shifts. However, they require the true label to be provided after every prediction, which is a limiting requirement for many use cases. Our experiments show that it is possible to achieve comparable performance in these settings without this limitation, i.e., “label free”. Test Time Adaptation (TTA) Our EACP method is inspired by core ideas in TTA. The goal of TTA is to update a trained machine learning model at test time, using unlabeled data from shifted distributions. To achieve this, one could update the batch-norm statistics on the test data [Nado et al., 2020, Schneider et al., 2020, Khurana et al., 2021], or minimize the test-time prediction entropy [Wang et al., 2021, Zhang et al., 2022, Niu et al., 2022, Song et al., 2023, Press et al., 2024]. Notably, these methods can be applied on any probabilistic and differentiable model (such as modern neural networks), which is naturally congruent with the strength of CP. However, to date this line of work has been disconnected from the conformal prediction literature, and to our knowledge there are no studies investigating the benefit of TTA to the distribution-shifted CP setting. 3 Preliminaries of CP We begin by introducing the standard background of CP. For clarity, we assume i.i.d. data in our exposition, rather than the slightly weaker notion of exchangeability. Also see [Roth, 2022, Angelopoulos and Bates, 2023, Tibshirani, 2023]. Let D be an unknown distribution on the space X × Y of covariate-label pairs, and let α ∈ (0, 1) be the error rate we aim for. Given a dataset D consisting of n i.i.d. samples (xi, yi)i∈[n] ∼ Dn, the goal of CP is to generate a set-valued function CD : X → 2Y , such that P(x,y)∼D,D∼Dn [y ∈ CD(x)] ≥ 1 − α. (1) That is, for a fresh sample (x, y) ∼ D, our prediction set CD(x) covers the label y with guaranteed high probability. 3 Notice that Eq.(1) alone is a trivial objective, since it suffices to predict the entire label space CD(x) = Y for all x. Therefore, CP is essentially a bi-objective problem: as long as Eq.(1) is satisfied, we want the prediction set CD(x) to be small. The main difficulty of this set-valued prediction problem is that the range of output 2Y is too large. In this regard, the key idea of CP is reducing the problem to 1D prediction via a trained machine learning model, such as a neural network. Specifically, we assume access to a (positively oriented; i.e., larger is better) score function s : X × Y → R+ given by the base model, such that for each covariate x ∈ X and candidate label y′ ∈ Y, s(x, y′) measures how likely the model believes that y′ is the true label y. Then, all there is left for CP is to pick a threshold τD ∈ R that depends on the dataset D, and predict the label set (if the score function is negatively oriented, then ≥ is replaced by ≤) CD (x; τD) := {y′ ∈ Y : s (x, y′) ≥ τD} . (2) 3In the literature this is called a marginal (w.r.t. the covariate x), in-expectation (w.r.t. the dataset D) guarantee – see [Roth, 2022, Chapter 7] for various ways to strengthen it. 3 Under the i.i.d. assumption, the coverage objective Eq.(1) is satisfied by picking τD as the α(1 + n−1)- quantile of the empirical scores {s(xi, yi)}i∈[n]. Since the training data of the base model is split from the calibration dataset used to determine τD, this approach is commonly known as split conformal prediction, which we also refer to as SplitCP. Notably, τD is determined by the calibration dataset; once the latter is fixed, there is no need to access the ground truth labels at test time. Example In classification (which we focus on), the standard practice is to define the score function s(x, y) = πθ(x)y [Sadinle et al., 2019], where πθ is a trained neural network parameterized by θ, and πθ(x)y ∈ [0, 1] is the softmax score corresponding to one of the k-classes y ∈ [k]. Such a score function is positively oriented (i.e., larger means more likely to be included in the prediction set). Distribution shift For the rest of this paper, we study a deviation of the above problem where the test sample (x, y) is not drawn from D, but rather some new unknown distribution ̃Dtest. In general, it is impossible to guarantee Eq.(1) without restrictions on the shift, but we will show that with more help from the base model, the above CP procedure can be modified to work well in practice. 4 Our methods In this section, we first propose a method called ECP (Entropy scaled Conformal Prediction), which improves the coverage rate of CP by enlarging its prediction sets using the uncertainty evaluation of the base model itself. Crucially, this notion of uncertainty can be directly minimized and refined through unsupervised TTA, leading to an improved method called EACP (Entropy base-Adapted Conformal Prediction). It is able to both i) recover the desired error rate on many challenging distribution-shifted datasets, while ii) significantly reduce inflated set sizes under increased uncertainty. 4.1 Scaling conformal scores by uncertainty Let us start by motivating our idea. A common criticism of standard SplitCP (although typically studied in the regression setting rather than classification) is that the size of the CP prediction set is invariant to the covariate x. To provide greater “local adaptivity” (w.r.t. x), uncertainty-aware techniques have been proposed to adjust (or scale) the score function s(x, y) based on some notion of uncertainty (or difficulty) that the base model decides at each x [Papadopoulos et al., 2008, Johansson et al., 2015, Lei et al., 2018, Izbicki et al., 2019, Romano et al., 2019, Seedat et al., 2023, Rossellini et al., 2024]. This has the effect of inflating the prediction set on the base model’s uncertain regions, and has been shown to improve the conditional coverage rate of CP [Angelopoulos and Bates, 2023, Tibshirani, 2023]. 1 2 3 4 5 Entropy 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Softmax Value of True LabelImageNet ImageNet-V2 ImageNet-R ImageNet-A Contrast Brightness Motion Blur Gaussian Noise Figure 1: Entropy vs. the softmax score of true label, averaged on each dataset. Key idea Building on these results, our key idea is to ap- ply an analogous uncertainty scaling on the score function, in order to improve the performance of CP under distribu- tion shifts. However, instead of using the uncertainty of the base model at each covariate x, we crucially evaluate its uncertainty on the whole distribution-shifted test dataset, effectively aggregating its “localized” uncertainty at each x. In other words, instead of aiming for “local adaptivity” (w.r.t. x) as in prior works, we aim for the adaptivity w.r.t. the unknown distribution shift. Prediction entropy More concretely, which uncertainty measure do we use on the base model? As discussed above, the ideal dataset-specific uncertainty measure would follow from a “localized” uncertainty measure at each x, and a 4 particularly useful one for the latter is the entropy of the model’s predictions, h(x) = − ∑ y∈[k] πθ(x)ylogπθ(x)y. Previous works have established the relation between such an entropy notion (of the base model) and the magnitude of the distribution shift, showing that larger shifts are strongly correlated with higher entropy (thus uncertainty in the base model) [Wang et al., 2021]. We provide a consistent but unique observation in Figure 1, which plots the relation between the entropy (averaged over all x values in the dataset) and the softmax score of the true label (also averaged over x), across a range of different datasets. 4 For the true label to be included in the CP prediction set, which is eventually what we aim for, its softmax value should be greater than the CP threshold τD. From Figure 1, one could see that an increase in entropy is associated with a decrease in the softmax score of the true label, which crucially means that the prediction sets should be larger when the entropy is higher. 0.7 0.8 0.9 Target Coverage 0.5 0.6 0.7 0.8 0.9Empirical coverage with ECPIN-v2 0.7 0.8 0.9 Target Coverage 0.0 0.2 0.4 0.6 0.8 1.0Empirical coverage with ECPIN-C Brightness Severity 1 0.85 (1 ) 0.9 (1 ) 1.0 (1 ) 1.1 (1 ) 1.15 (1 ) 0.7 0.8 0.9 Target Coverage 0.6 0.7 0.8 0.9Empirical coverage with ECPIN-C Brightness Severity 5 0.7 0.8 0.9 Target Coverage 0.5 0.6 0.7 0.8 0.9Empirical coverage with ECPIN-R 0.7 0.8 0.9 Target Coverage 0.6 0.7 0.8 0.9Empirical coverage with ECPIN-C Contrast Severity 1 0.7 0.8 0.9 Target Coverage 0.2 0.4 0.6 0.8Empirical coverage with ECPIN-C Contrast Severity 5 Figure 2: The targeted coverage rate 1 − α vs. the empirical coverage rate, induced by ECP with different β values. It shows that simply setting β = 1 − α in ECP consistently works well for all but the most severe distribution shifts (e.g., ImageNet-R and ImageNet-C Contrast Severity 5). Such an observation also holds across various α values, suggesting the general effectiveness of this hyperparameter choice. Now consider going from the “localized” uncertainty measure h(x) to an uncertainty measure on the test dataset, denoted as uDtest. We introduce another design choice: defining uDtest as a specified, β-quantile of h(x) on the test dataset. Surprisingly, we find that simply setting β to the desired coverage rate 1 − α is a fairly reliable choice in practice (see Figure 2), which gives a robust (over)-estimate of typical h(x) values on the test dataset. We perform all the experiments with this direct relationship to avoid excessive hyperparameter tuning, but it can be further refined if desired. Method: ECP Now we are ready to use uDtest above to scale the score functions on the test dataset, without label access. The resulting method is named as ECP (Entropy scaled Conformal Prediction). 4Darker shades represent greater severity levels of ImageNet-C corruptions; see Section 5 for more details on the datasets. 5 Formally, let us denote qβ(·) as the β-th quantile of its argument, and hDtest = {h(x1), ..., h(x)Ntest} as the entropy values on the test dataset. Then, we compute the “entropy quantile” uDtest = q1−α (hDtest ) , (3) and on any test sample, modified from Eq.(2), we scale the score function by max(1, uDtest ) to form the CP prediction set CDtest (x; τD) := {y′ ∈ [k] : s(x, y′) · max(1, uDtest ) ≥ τD} . (4) Here, we take a maximum with 1 to ensure that the prediction sets of ECP cannot be smaller than those of standard SplitCP. The intuition is that a larger distribution shift will result in larger entropy, and thus lead to a correspond- ingly larger up-scaling of the score function. In this way, more labels have score larger than the fixed CP threshold τD, and the prediction set grows. This can help mitigate the under-coverage issue of standard SplitCP under distribution shifts (without accessing test labels), and further details are provided in our experiments (Section 5). 4.2 Optimizing uncertainty using TTA While our ECP method already improves the coverage of SplitCP on several datasets, it inevitably leads to larger set sizes and, like all post-hoc CP methods, still relies on a fixed base model. To remedy this, we refine ECP using techniques from unsupervised TTA, which are able to update the base model’s parameters on the unlabeled test dataset. While these techniques have been often investigated in the context of top-1 accuracy, we take a different perspective and study their ability to improve set-valued classifiers like conformal prediction. Key idea Concretely, consider the following loss function (of the base model’s parameter θ), L(x; θ) := h(x) = − ∑ y∈[k] πθ(x)ylogπθ(x)y. (5) Our insight is that by directly minimizing this loss (through updating θ), we implicitly minimize the entropy quantile uDtest used to scale the score functions. Consequently, the sizes of ECP prediction sets can be reduced, and the coverage rate can be increased. Method: EACP A number of specific TTA methods have been developed to minimize Eq.(5), while ensuring certain notions of stability. In this work, we leverage the recent ETA (Efficient Test-time Adaptation) method [Niu et al., 2022] as it is a simple approach shown to be effective even in continuously shifting distributions [Press et al., 2023]. Adopting this with ECP results in a new CP method, which we call EACP (Entropy base-Adapted Conformal Prediction). In practice, one could simply call ETA as a subroutine, so we only present its high level idea for the sake of completeness. On a batch of new test data (i.e., xi with a collection of indices i), ETA filters the base model’s outputs (i.e., softmax scores) s(xi, ·) by excluding the outputs similar to those already seen. Then, it reweights the remaining outputs based the associated entropy h(xi), with lower entropy (less uncertain) indices receiving higher weights. This leads to a weighted variant of the loss function Eq.(5), which is then minimized by performing a single gradient update. Subsequently, the updated base model is applied to ECP to form the prediction sets of EACP, according to Eq.(4). In Section 5, we demonstrate that EACP can further improve the performance of ECP, by increasing the coverage rate while maintaining informative set sizes. 4.3 Uncertainty scaling function Previously, in Eq.(4) we scale the score functions linearly by the entropy quantile uDtest of the base model. However, in principle this can be adjusted by any (potentially non-linear) function f (·) of the entropy quantile, 6 f (uDtest ). The best choice of f (·) depends on the unknown relation between uDtest and the (1 − α)-quantile of the true labels’ conformal scores, denoted as τDtest := q1−α [s(x, y); (x, y) ∈ Dtest] . Specifically, we aim to find an f (·) satisfying f (uDtest ) = τD/τDtest . 0.2 0.4 0.6 0.8 1.0 1.2 log(uDtest) 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50log(D/Dtest) IN-V2 Different Linear fit Polynomial Order: 1.5 0.4 0.6 0.8 1.0 1.2 1.4 log(uDtest) 0.8 1.0 1.2 1.4 1.6 1.8log(D/Dtest) IN-C Contrast Severity 1 Different Linear fit Polynomial Order: 1.1 1.0 1.1 1.2 1.3 log(uDtest) 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0log(D/Dtest) IN-R Different Linear fit Polynomial Order: 4.2 Figure 3: uDtest versus τD/τDtest on a log-log scale. On mild and moderate distribution shifts, the linear fit on the log-log plot has slope between 1 and 2. This suggests the effectiveness of using a linear or quadratic function as f (·), which acts on the entropy quantile. However, we also observe that a higher-order polynomial is required on more difficult shifts, such as ImageNet-R. While finding this optimal f (·) is obviously infeasible without observing the labels in Dtest, in Figure 3 we empirically evaluate the ideal choice in a post-hoc manner, across different datasets. Recall that both uDtest and τDtest depend on the desired error rate α. Therefore, for each dataset, we vary α and plot the resulting uDtest versus τD/τDtest on a log-log scale. Furthermore, if we mildly restrict f (·) to polynomials, then its optimal order can be approximated by the slope of a linear fit on the log-log plot. We only use not-too-small α values for the linear fit, since it is closer to the practice of CP and less prone to noise. Figure 3 shows that the optimal polynomial order generally increases with the severity of the distribution shift. A larger polynomial order would lead to larger prediction sets. While end-users can refine f (·) based on a preference towards ensuring coverage or small set sizes, we empirically validate that both linear (ECP1 / EACP1) and quadratic (ECP2 / EACP2) scaling are sufficient for coverage and moderate set sizes, on a diverse range of distribution shifts. 5 Experiments We conduct experiments across a number of large-scale datasets and neural network architectures. Our setup builds on the standard SplitCP procedure introduced in Section 3, which relies on a held-out, in-distribution, “development set” for calibrating the CP threshold. On ImageNet variants, we split the original ImageNet development set, into a calibration and validation set, resulting in 25,000 calibration points. we refer to Appendix A.1 for more details on the calibration splits and sizes for all datasets. The conformal threshold is found on the calibration set, and used for all subsequent distribution-shifted datasets. After the conformal threshold is estimated in-distribution, all subsequent steps are unsupervised. We show results on both stationary and continuously shifting distributions. Baselines We compare our proposed ECP and EACP methods to a number of possible baselines. This includes using a pre-calibrated threshold on new distribution-shifted data as in the standard SplitCP approach, using this calibrated threshold while updating the base model using ETA, and an approach we call NAIVE which forms prediction sets by including classes until their cumulative probability is greater or equal to the desired 7 1 − α coverage level, without any prior calibration. In all experiments, the target coverage is 0.90. We also analyze linear and quadratic scaling of the uncertainty adjustment, as described in Section 4.3. Datasets We investigate a number of ImageNet [Deng et al., 2009] variants including: ImageNet-V2 [Recht et al., 2019], ImageNet-R [Hendrycks et al., 2021a], and ImageNet-A [Hendrycks et al., 2021b]. We also test our approach on datasets from the WILDS Benchmark [Koh et al., 2021] which represent in-the-wild distribution shifts across many real world applications, including iWildCam (animal trap images), RXRX1 (cellular images), and FMOW (satellite images). While the previous datasets present a single distribution shift, the ImageNet-C [Hendrycks and Dietterich, 2019] dataset allows us to investigate shifts across many types and severities. Specifically, ImageNet-C applies 19 visual corruptions to the ImageNet validation set across four corruption categories — noise, blur, weather, and digital, with five severity levels for each corruption. Please refer to Appendix A.1 for more details on the studied datasets. 5.1 Stationary shifts Table 1: ECP and EACP can achieve the desired coverage on a number of distribution-shifted datasets, across a variety of imaging domains (ecological, cellular, satellite, etc.). The coverage gap is significantly reduced even on extremely challenging datasets. All results are from ResNet-50 models except FMOW, which uses a DenseNet-121 [Huang et al., 2016]. Quadratic uncertainty scaling provides better coverage rates, however, linear scaling results in smaller set sizes. Coverage Set Size Dataset SplitCP NAIVE ETA ECP1 ECP2 EACP1 EACP2 SplitCP NAIVE ETA ECP1 ECP2 EACP1 EACP2 ImageNet-V2 0.81 0.88 0.81 0.86 0.91 0.86 0.91 2.5 11.7 2.5 4.2 7.6 4.5 8.7 ImageNet-R 0.50 0.69 0.62 0.61 0.72 0.71 0.80 3.4 20.9 3.0 9.1 23.3 6.8 16.1 ImageNet-A 0.03 0.14 0.05 0.10 0.27 0.14 0.30 3.4 12.7 3.6 7.4 15.1 8.7 10.1 iWildCam 0.84 0.76 0.84 0.84 0.88 0.84 0.89 3.9 2.5 3.8 3.8 5.5 3.7 5.6 RXRX1 0.84 0.48 0.87 0.87 0.90 0.90 0.93 81.8 6.4 100 105 137 133 177 FMOW 0.87 0.83 0.87 0.93 0.96 0.93 0.94 6.2 5.8 6.5 10.3 15.3 11.1 16.4 Table 1 summarizes our performance on various natural distribution-shifted datasets. We observe that ETA alone is unable to fully recover the desired coverage, however, ECP closes the gap significantly. Coverage is further improved by adaptation through EACP, which helps to reduce set sizes on some datasets – it recovers the desired coverage on all but the most challenging datasets. Here, we also see the trade-off between linear and quadratic uncertainty scaling. We observe EACP2 consistently achieves higher coverage rates, however this also leads to “over-coverage” on some datasets and thus larger sets. In contrast, EACP1 leads to lower coverage but also smaller set sizes. This trade-off can be selected by end-users depending on their preference for more accurate or more efficient prediction sets. In subsequent experiments, we will focus on demonstrating the benefit of EACP2 on coverage, while noting that the observed set sizes are nonetheless practically useful and far from trivial. Table 2: Coverage on four different corruption types representing each ImageNet-C category. By explicitly leveraging uncertainty under distribution shift, our ECP2 method closes the coverage gap on most severity levels. EACP2 improves on this by refining the uncertainty at test-time, resulting in a recovered coverage rate of 0.90 on nearly all corruption types and severities. Method Contrast Brightness Gaussian Noise Motion Blur 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 NAIVE 0.91 0.89 0.87 0.83 0.76 0.92 0.92 0.91 0.91 0.90 0.88 0.85 0.79 0.69 0.79 0.91 0.90 0.85 0.77 0.71 SplitCP [Sadinle et al., 2019] 0.83 0.78 0.66 0.36 0.09 0.88 0.87 0.86 0.83 0.78 0.79 0.69 0.50 0.26 0.07 0.83 0.74 0.57 0.37 0.27 ETA [Niu et al., 2022] 0.87 0.86 0.84 0.79 0.63 0.88 0.88 0.87 0.86 0.84 0.86 0.82 0.76 0.69 0.54 0.86 0.84 0.80 0.73 0.68 ECP2 (ours) 0.93 0.92 0.89 0.79 0.60 0.94 0.94 0.94 0.93 0.92 0.92 0.88 0.80 0.86 0.38 0.94 0.92 0.86 0.75 0.68 EACP2 (ours) 0.93 0.93 0.93 0.92 0.87 0.93 0.93 0.93 0.93 0.93 0.93 0.93 0.92 0.90 0.84 0.93 0.92 0.92 0.91 0.89 8 In Table 2, we show fine-grained results of our method’s performance on one corruption type for each ImageNet-C category and across each severity level. Here, we can see the benefit of leveraging an uncertainty notion that can be directly minimized and refined on new test samples. By both adjusting the conformal scores by an entropy-based notion of uncertainty while simultaneously refining this uncertainty, EACP is able to recover the target coverage rate on most corruption types and severities. 0 10 20 30 40 50 60 Set Size 0.3 0.4 0.5 0.6 0.7 0.8 0.9Coverage 0 5 10 15 20 Set Size 0.3 0.4 0.5 0.6 0.7 0.8 0.9CoverageTarget Coverage = 0.9 Methods SplitCP ETA ECP EACP Architecture ResNet-50/152 ViT-S/B Figure 4: Our EACP2 method is able to improve coverage using various neural network models and architectures, as well as a large and diverse range of distribution shifts. It consistently “hugs” the desired coverage rate, while maintaining practical set sizes. Results are averaged across five severity levels for each corruption type in the ImageNet-C dataset. We zoom in on the right to clearly see the benefit of adapting at test time on coverage and set sizes. We report performance using a number of neural networks architectures and sizes on all 19 corruption types averaged across severities for each corruption in Figure 4. Here, the methodology and hyper-parameters are exactly the same for all models, including the vision transformers. We can see that even with no hyper-parameter tuning (including on the TTA algorithm), our method considerably reduces the coverage gap across a variety of neural networks. An immediate observation is that while ETA does not recover coverage, it also consistently leads to smaller set sizes. While minimizing set sizes is indeed desirable, this may be of secondary importance to the primary goal of achieving the desired coverage level. Further, we note that the set sizes without ECP are in fact consistent and with little variance, regardless of the empirical coverage. We argue that this is actually an undesirable quality. As an uncertainty quantification framework, conformal prediction is considered to convey uncertainty through its set sizes; greater uncertainty is supposed to be reflected in larger sets, and vice versa. Our results suggest that explicitly incorporating uncertainty into the conformal predictor seems necessary to accurately convey the underlying uncertainty under distribution shift. While even large mis-coverage still results in small set sizes using ETA, larger sets under EACP consistently correspond to lower coverage — an important and practical quality for end-users. 5.2 Continuous shifts We further investigate the setting of continuous distribution shift in Table 3. This setting is similar to that studied in previous Online Conformal Prediction (OCP) works [Zhang et al., 2024, Bhatnagar et al., 2023b, Gibbs and Candès, 2023, Gibbs and Candes, 2021]. This involves shifting between ImageNet-C severity level 1 to level 5 (either suddenly or gradually), while sampling random corruptions at each corresponding severity. We emphasize that this is a particularly challenging setting, as it presents a continuous shift in both the magnitude as well as type of corruption. See Appendix A.2 for more details on this set-up. In addition to unsupervised methods, we also compare with supervised OCP methods that rely on the correct label being revealed after every prediction. We notice that on gradual shifts in severity, EACP2 achieves nearly the same coverage results as these strong baselines, with greatly reduced set-sizes. We believe this considerable reduction in set size is due to two factors. First, EACP2 is able to update the underlying neural network 9 Table 3: We evaluate performance on the challenging setting of continuously shifting distributions. The “label free” column denotes whether a method relies on labels at test-time from the target data. We recall that SplitCP does not adapt to new data. In addition to the average coverage (↑) and average size (↓), we also measure the worst local corruption error LCE128 (↓) and worst local set size LSS128, (↓) on a sliding window of 128 test points. Gradual shift Sudden shift Label Free Method Avg. Cov Avg. Size LCE128 LSS128 Avg. Cov Avg. Size LCE128 LSS128 - SplitCP [Sadinle et al., 2019] 0.59 3.1 0.70 3.6 0.59 2.8 0.71 3.5 ✗ SAOCP [Seedat et al., 2023] 0.79 145 0.24 353 0.78 139 0.28 349 ✗ FACI [Gibbs and Candes, 2021] 0.90 101 0.07 455 0.90 142 0.09 450 ✗ MAGL-D [Zhang et al., 2024] 0.90 403 0.05 856 0.90 355 0.05 844 ✗ MAGL [Zhang et al., 2024] 0.90 117 0.06 573 0.90 168 0.3 704 ✗ MAGDIS [Zhang et al., 2024] 0.90 417 0.06 841 0.90 372 0.07 852 ✓ ETA [Niu et al., 2022] 0.69 2.9 0.52 3.4 0.67 2.7 0.54 3.5 ✓ ECP2 (ours) 0.84 36.6 0.35 90.4 0.82 37.5 0.38 88.5 ✓ EACP2 (ours) 0.88 22.4 0.20 47.8 0.86 23.1 0.28 55.7 on new data. Second, while OCP is able to learn the (potentially expansive) necessary threshold, EACP2 instead directly updates the conformal scores themselves. In addition to average coverage and set size, we also measure the local coverage error LCE128 across the worst sliding window of 128 samples, and similarly the worst local set size, LSS128. While the supervised methods unsurprisingly result in better local coverage, they also lead to local set sizes that are an order of magnitude larger than those produced by EACP2. 6 Conclusion This paper studies how to adapt CP methods to distribution-shifted data, without relying on labels from the target dataset. This is an important challenge in many real world settings, where exchangeability assumptions are violated and labels may be difficult to attain. We propose an uncertainty-aware method based on the prediction entropy (ECP), and leverage unsupervised test time adaptation to update the base model and refine its uncertainty (EACP). We demonstrate that the proposed methods are able to recover the desired error rate on a wide range of distribution shifts, while maintaining efficient set sizes. Furthermore, they are even competitive with supervised approaches on challenging and continuously shifting distributions. We hope this inspires future works continuing to tackle this important challenge. References Anastasios Angelopoulos, Emmanuel Candes, and Ryan J Tibshirani. Conformal pid control for time series prediction. Advances in Neural Information Processing Systems, 36, 2023. (Cited on page 3) Anastasios N. Angelopoulos and Stephen Bates. Conformal prediction: A gentle introduction. Foundations and Trends® in Machine Learning, 16(4):494–591, 2023. ISSN 1935-8237. doi: 10.1561/2200000101. URL http://dx.doi.org/10.1561/2200000101. (Cited on pages 3, 4, and 15) Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra Malik. Uncertainty sets for image classifiers using conformal prediction. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=eNdiU_DbM9. (Cited on page 15) Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal prediction beyond exchangeability. The Annals of Statistics, 51(2):816–845, 2023. (Cited on pages 2 and 3) Osbert Bastani, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Practical adversarial multivalid conformal prediction. Advances in Neural Information Processing Systems, 35:29362–29373, 2022. (Cited on page 3) 10 Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset. arXiv preprint arXiv:2004.10340, 2020. (Cited on page 15) Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In International Conference on Machine Learning, pages 2337–2363. PMLR, 2023a. (Cited on pages 1, 15, and 16) Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023b. (Cited on pages 3 and 9) Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C Duchi. Robust validation: Confident predictions even when distributions shift. Journal of the American Statistical Association, pages 1–66, 2024. (Cited on pages 2 and 3) Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. (Cited on page 15) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. (Cited on page 8) Yihuai Gao, Yukai Tang, Han Qi, and Heng Yang. Closure: Fast quantification of pose uncertainty sets. arXiv preprint arXiv:2403.09990, 2024. (Cited on page 2) Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems, 34:1660–1672, 2021. (Cited on pages 2, 3, 9, and 10) Isaac Gibbs and Emmanuel Candès. Conformal inference for online prediction with arbitrary distribution shifts, 2023. (Cited on pages 3 and 9) Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in Neural Information Processing Systems, 17, 2004. (Cited on page 2) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017. (Cited on page 1) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. (Cited on pages 8 and 15) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021a. (Cited on pages 8 and 15) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. CVPR, 2021b. (Cited on pages 8 and 15) Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, 2016. URL https://api.semanticscholar.org/CorpusID:9433631. (Cited on page 8) Rafael Izbicki, Gilson T Shimizu, and Rafael B Stern. Flexible distribution-free conditional predictive bands using density estimators. arXiv preprint arXiv:1910.05575, 2019. (Cited on page 4) Ulf Johansson, Cecilia Sönströd, and Henrik Linusson. Efficient conformal regressors using bagged neural nets. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1–8, 2015. doi: 10.1109/IJCNN.2015.7280763. (Cited on page 4) 11 Kevin Kasa and Graham W. Taylor. Empirically validating conformal prediction on modern vision architectures under distribution shift and long-tailed data. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023. URL https://openreview.net/forum?id=TMxpy0aluL. (Cited on page 1) Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. (Cited on page 3) Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021. (Cited on pages 8 and 15) Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam. Conformal prediction with large language models for multi-choice question answering. arXiv preprint arXiv:2305.18404, 2023. (Cited on page 2) Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523):1094–1111, 2018. (Cited on page 4) Jordan Lekeufack, Anastasios A Angelopoulos, Andrea Bajcsy, Michael I Jordan, and Jitendra Malik. Confor- mal decision theory: Safe autonomous decisions from imperfect predictions. arXiv preprint arXiv:2310.05921, 2023. (Cited on page 2) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. (Cited on page 1) Lars Lindemann, Matthew Cleaveland, Gihyun Shim, and George J Pappas. Safe planning in dynamic environments using conformal prediction. IEEE Robotics and Automation Letters, 2023. (Cited on page 2) Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in Neural Information Processing Systems, 34:15682–15694, 2021. (Cited on page 15) Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. (Cited on page 3) Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. (Cited on pages 2, 3, 6, 8, 10, 15, and 16) Harris Papadopoulos, Alex Gammerman, and Volodya Vovk. Normalized nonconformity measures for regression conformal prediction. In Proceedings of the 26th IASTED International Conference on Artificial Intelligence and Applications, AIA ’08, page 64–69, USA, 2008. ACTA Press. ISBN 9780889867109. (Cited on page 4) Ori Press, Steffen Schneider, Matthias Kümmerer, and Matthias Bethge. Rdumb: A simple approach that questions our progress in continual test-time adaptation. Advances in Neural Information Processing Systems, 36, 2023. (Cited on page 6) Ori Press, Ravid Shwartz-Ziv, Yann LeCun, and Matthias Bethge. The entropy enigma: Success and failure of entropy minimization. In International Conference on Machine Learning. PMLR, 2024. (Cited on page 3) Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina Barzilay. Conformal language modeling. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=pzUhfQ74c5. (Cited on page 2) 12 Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, 2019. (Cited on pages 8 and 15) Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots that ask for help: Uncertainty alignment for large language model planners. In Proceedings of the Conference on Robot Learning (CoRL), 2023. (Cited on page 2) Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. Advances in Neural Information Processing Systems, 32, 2019. (Cited on page 4) Raphael Rossellini, Rina Foygel Barber, and Rebecca Willett. Integrating uncertainty awareness into conformalized quantile regression. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 1540–1548. PMLR, 02–04 May 2024. URL https://proceedings.mlr.press/v238/rossellini24a.html. (Cited on page 4) Aaron Roth. Uncertain: Modern topics in uncertainty estimation. Unpublished Lecture Notes, 2022. URL https://www.cis.upenn.edu/~aaroth/uncertainty-notes.pdf. (Cited on pages 2 and 3) Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least Ambiguous Set-Valued Classifiers with Bounded Error Levels. Journal of the American Statistical Association, 114(525):223–234, January 2019. ISSN 0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1395341. URL http://arxiv.org/abs/1609.00451. arXiv:1609.00451 [cs, stat]. (Cited on pages 2, 4, 8, and 10) Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. (Cited on page 3) Nabeel Seedat, Alan Jeffares, Fergus Imrie, and Mihaela van der Schaar. Improving adaptive conformal prediction using self-supervised learning. In International Conference on Artificial Intelligence and Statistics, pages 10160–10177. PMLR, 2023. (Cited on pages 4 and 10) Junha Song, Jungsoo Lee, In So Kweon, and Sungha Choi. Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11920–11929, 2023. (Cited on page 3) J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosinski. Rxrx1: An image set for cellular morphological variation across many experimental batches. In International Conference on Learning Representations (ICLR), 2019. (Cited on page 15) Ryan Tibshirani. Advanced topics in statistical learning: Conformal prediction. https://www.stat.berkeley. edu/~ryantibs/statlearn-s23/lectures/conformal.pdf, 2023. (Cited on pages 3 and 4) Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. Advances in Neural Information Processing Systems, 32, 2019. (Cited on pages 1, 2, and 3) Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world, volume 29. Springer, 2005. (Cited on page 1) Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. (Cited on pages 2, 3, 5, 15, and 17) Heng Yang and Marco Pavone. Object pose estimation with statistical guarantees: Conformal keypoint detection and geometric uncertainty propagation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8947–8958, 2023. (Cited on page 2) 13 Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022. (Cited on pages 3 and 15) Zhiyu Zhang, David Bombara, and Heng Yang. Discounted adaptive online prediction. International Conference on Machine Learning, 2024. (Cited on pages 3, 9, 10, 15, and 16) Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA, pages 928–936. AAAI Press, 2003. URL http://www.aaai.org/Library/ICML/ 2003/icml03-120.php. (Cited on page 2) 14 Appendix Appendix A contains detailed information on our studied datasets and experimental protocols, including TTA hyper-parameters, CP procedure, and continuous shift set-up. Appendix B discusses other possible uncertainty measures and their deficiencies. Appendix C presents results with another base TTA method, Appendix D has results of more neural network architectures on natural distribution shifts, Appendix E has graphs performance on continuous-shifts at each time-step, and Appendix F has full ImageNet-C results on each severity level. A Experimental details A.1 Dataset details We perform experiments on a number of large-scale datasets that are frequently used to evaluate deep learning performance under distribution shift [Koh et al., 2021, Wang et al., 2021, Minderer et al., 2021, Niu et al., 2022, Zhang et al., 2022, Bhatnagar et al., 2023a, Zhang et al., 2024]: • ImageNet-V2 [Recht et al., 2019] is a new ImageNet test-set that contains 10,000 images that were collected by closely following the original ImageNet data collection process. • ImageNet-R [Hendrycks et al., 2021a] includes renditions (e.g., paintings, sculptures, drawings, etc.) of 200 ImageNet classes, resulting in a test set of 30,000 images. • ImageNet-A [Hendrycks et al., 2021b] consists of 7,500 real-world, unmodified, and naturally occurring adversarial images which a ResNet-50 model failed to correctly classify. • ImageNet-C [Hendrycks and Dietterich, 2019] applies 19 visual corruptions across four categories and at five severity levels to the original ImageNet validation set. • iWildCam [Koh et al., 2021, Beery et al., 2020] contains camera-trap images from different areas of the world, representing geographic distribution-shift. It includes a validation set of 7,314 images from the same camera traps the model was trained on, which is used as our calibration data, as well as 42,791 images from different camera traps that is used as our test set. The images contain one of the 182 possible animal species. • RXRX1 [Koh et al., 2021, Taylor et al., 2019] consists of high resolution fluorescent microscopy images of human cells which have been given one of 1,139 genetic treatments, with the goal of generalizing across experimental batches. It is split into a 40,612 in-distribution validation set and 34,432 test set. • FMOW [Koh et al., 2021, Christie et al., 2018] is a satellite imaging dataset with the goal of classifying images into one of 62 different land use or building types. It consists of 11,483 validation images from the years from 2002–2013, and 22,108 test images from the years from 2016–2018. A.2 Experimental protocols Conformal prediction Our split conformal prediction set-up follow previous works [Angelopoulos et al., 2021, Angelopoulos and Bates, 2023], which divides a held-out dataset into a calibration and test set. On ImageNet variants, we split the original validation set in half to produce 25,000 calibration points and 25,000 in-distribution test points. The calibrated scores and / or threshold are then used for subsequent distribution-shifted data. On the WILDS datasets, we similarly split the in-distribution validation sets. Adaptation procedure Our ImageNet-based experiments are conducted on pre-trained ResNets provided by the torchvision library 5, and ViTs provided by the timm library 6. Experiments on WILDS datasets are conducted using pre-trained models provided by the authors of that study 7. For EACP and ETA, we closely 5https://github.com/pytorch/vision 6https://github.com/huggingface/pytorch-image-models 7https://github.com/p-lambda/wilds 15 follow the optimization hyperparameters from the original paper [Niu et al., 2022]: we use SGD optimizer with a momentum of 0.9 and learning rate of 0.00025. We use a batch size of 64 for all ImageNet experiments, 128 for RXRX1 and FMOW, and 42 for iWildCam. Our experiments are conducted using a single NVIDIA A40 GPU. Continuous shift We adopt a slightly modified version of the experimental design for continuous distribution shift presented in previous works [Bhatnagar et al., 2023a, Zhang et al., 2024]. This involves sampling random corruptions from the ImageNet-C dataset under two regimes: gradual shifts where the severity level first increases in order from {1, ..., 5} then decreases from {5, ..., 1}, and sudden shifts where the severity level alternates between 1 and 5. In addition to sampling random corruptions, we also consider in Figure 6 results on the “easier” setting of shifting severity’s on a single corruption type. B What is the right measure of uncertainty? 0.0000 0.0005 0.0010 0.0015 0.0020 Softmax Variance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Softmax Value of True Label ImageNet ImageNet-V2 ImageNet-R ImageNet-A Contrast Brightness Motion Blur Gaussian Noise (a) Softmax variance 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 - Max Softmax 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Softmax Value of True LabelImageNet ImageNet-V2 ImageNet-R ImageNet-A Contrast Brightness Motion Blur Gaussian Noise (b) 1 - Max softmax Figure 5: Similarly to Figure 1, we present the relation between different uncertainty measures and the average score of the true label. We see that softmax variance (left) has an inverse relation with distribution shift, and 1− maximum softmax is a bounded metric that may provide an insufficient adjustment. Although in Section 4.3 we propose adjusting the conformal scores by the prediction entropy of the base model, it is worth asking if there exist other notions of uncertainty that may instead be used. Here, we consider two additional uncertainty measures and their relation with the softmax value of the true label (which is ultimately what we would like to include in our prediction set), and show they are ill-suited for our task. Firstly, in Figure 5a we consider the variance of the softmax scores. Perhaps surprisingly, we see that distribution shift most often leads to a smaller variance, thus conveying that the base model is less uncertain. This suggests that softmax variance is a deficient uncertainty measure as it fails to capture the actual underlying uncertainty on distribution-shifted data. We also consider 1− maximum softmax score as another possible uncertainty measure, and see in Figure 5b that distribution shift is associated with smaller maximum softmax values. Unlike softmax variance, this does appear to better capture the uncertainty, as we would expect the base model to be less confident on distribution-shifted data. However, this uncertainty measure can still only take a maximum value of one and thus may not provide necessary adjustment magnitude, and it is unknown if it can reliably update the base model label-free. While there may exist better uncertainty measures that future works can explore, these results suggest that the prediction entropy is a simple and reliable measure for conformal adjustments that can effectively capture the underlying uncertainty. 16 Table 4: Our proposed EACP performs well with other TTA methods, as seen here using Tent Wang et al. [2021] as the TTA update. Dataset SplitCP Tent ECP2 EACP2 (coverage / set size) (coverage / set size) (coverage / set size) (coverage / set size) ImageNet-V2 0.81 / 2.5 0.81 / 2.6 0.91 / 8.0 0.92 / 9.6 ImageNet-R 0.50 / 3.2 0.58 / 3.3 0.73 / 23 0.77 / 17 ImageNet-A 0.07 / 1.5 0.21 / 3.1 0.58 / 204 0.40 / 24 iWildCam 0.83 / 3.5 0.81 / 2.6 0.89 / 5.7 0.85 / 3.4 RXRX1 0.85 / 83 0.87 / 101 0.90 / 136.7 0.92 / 176 FMOW 0.87 / 6.3 0.85 / 5.7 0.96 / 15.6 0.94 / 13.4 C Other TTA methods We investigate our methods performance with another base TTA method in Table 4. Here, we use the Tent update [Wang et al., 2021], which is a simpler version of ETA with no re-weighing of the entropy loss. While our proposed methods are also compatible with Tent, we notice that the more powerful ETA leads to better coverage and set sizes as seen in Table 1. We can expect that additional improvements in TTA will similarly lead to improvements in our EACP method. D More architecture comparisons In Table 5, we further demonstrate our methods improvements to coverage loss on natural distribution shifts using diverse neural network architectures. As expected, larger and more accurate neural networks result in better coverage and smaller set sizes using ECP and EACP ˙This is encouraging as it demonstrates our methods can scale along with the underlying model. Table 5: On natural distribution shifts, the performance of our methods scale well with the performance of the base classifier. This is encouraging as it suggests compatibility Dataset Model SplitCP ETA ECP2 EACP2 (coverage / set size) (coverage / set size) (coverage / set size) (coverage / set size) ImageNet-V2 Resnet50 0.81 / 2.5 0.81 / 2.5 0.91 / 7.6 0.91 / 8.7 Resnet152 0.81 / 2.0 0.81 / 2.1 0.89 / 4.6 0.91 / 6.3 Vit-S 0.80 / 1.5 0.80 / 1.5 0.90 / 3.4 0.90 / 3.4 ViT-B 0.80 / 1.2 0.80 / 1.2 0.90 / 2.4 0.90 / 2.4 ImageNet-R Resnet50 0.50 / 3.4 0.62 / 3.0 0.72 / 23.3 0.80 / 16.1 Resnet152 0.53 / 2.7 0.60 / 2.6 0.71 / 15.3 0.79 / 17.3 Vit-S 0.52 / 1.3 0.53 / 1.3 0.74 / 12.3 0.75 / 11.8 ViT-B 0.58 / 0.9 0.59 / 0.9 0.78 / 8.3 0.79 / 8.0 ImageNet-A Resnet50 0.03 / 3.4 0.05 / 3.6 0.27 / 15.1 0.30 / 19.1 Resnet152 0.18 / 3.0 0.17 / 3.3 0.43 / 11.8 0.50 / 19.6 Vit-S 0.37 / 1.7 0.37 / 1.7 0.65 / 8.4 0.66 / 8.3 ViT-B 0.47 / 1.2 0.47 / 1.2 0.76 / 6.5 0.76 / 6.4 E Continuous shifts In Figure 6, we visualize the coverage and set-sizes of our unsupervised methods and a number of supervised baselines on the previously described continuous distribution shifts. We show results on random corruption types as well as fixed corruption types. Our proposed methods perform well across all these settings; they closely maintain coverage even on sudden and severe shifts, while leading to substantially smaller set sizes than the baselines. 17 F ImageNet-C all severity levels In Figure 7, we present full results across all ImageNet-C severity levels. We see that our method is effective in recovering coverage even under many highly severe distribution shifts, and nearly always recovers the desired coverage on less severe shifts. 2 4 6 8 10 Set Size 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95Coverage Target Coverage = 0.9 Methods SplitCP ETA EACP Architecture ResNet-50/152 ViT-S/B (a) Severity level 1 2 4 6 8 10 12 14 16 Set Size 0.4 0.5 0.6 0.7 0.8 0.9Coverage Target Coverage = 0.9 Methods SplitCP ETA EACP Architecture ResNet-50/152 ViT-S/B (b) Severity level 2 0 5 10 15 20 Set Size 0.3 0.4 0.5 0.6 0.7 0.8 0.9Coverage Target Coverage = 0.9 Methods SplitCP ETA EACP Architecture ResNet-50/152 ViT-S/B (c) Severity level 3 0 5 10 15 20 25 Set Size 0.2 0.4 0.6 0.8Coverage Target Coverage = 0.9 Methods SplitCP ETA EACP Architecture ResNet-50/152 ViT-S/B (d) Severity level 4 0 10 20 30 40 Set Size 0.0 0.2 0.4 0.6 0.8Coverage Target Coverage = 0.9 Methods SplitCP ETA EACP Architecture ResNet-50/152 ViT-S/B (e) Severity level 5 Figure 7: Performance on 19 ImageNet-C corruptions on each severity level. EACP2 hugs the desired coverage line on nearly all severity levels. 18 0 100 200 300 400 500 600 700 0.3 0.4 0.5 0.6 0.7 0.8 0.9Local Coverage 0 100 200 300 400 500 600 700 800 0.3 0.4 0.5 0.6 0.7 0.8 0.9Local Coverage 0 100 200 300 400 500 600 700 0 200 400 600 800Local Set Size 0 100 200 300 400 500 600 700 800 0 200 400 600 800Local Set Size 0 100 200 300 400 500 600 700 Time 1 2 3 4 5Corruption Level 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level SplitCP ETA ECP EACP MagnitudeLearner MagLearnUndiscounted MagnitudeLearnerV2 SAOCP FACI (a) Shifting (random) corruptions 0 100 200 300 400 500 600 700 800 0.2 0.4 0.6 0.8 1.0Local Coverage 0 100 200 300 400 500 600 700 800 0.2 0.4 0.6 0.8 1.0Local Coverage 0 100 200 300 400 500 600 700 800 0 200 400 600 800Local Set Size 0 100 200 300 400 500 600 700 800 0 200 400 600 800Local Set Size 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level SplitCP ETA ECP EACP MagnitudeLearner MagLearnUndiscounted MagnitudeLearnerV2 SAOCP FACI (b) Contrast corruption 0 100 200 300 400 500 600 700 800 0.75 0.80 0.85 0.90 0.95Local Coverage 0 100 200 300 400 500 600 700 800 0.75 0.80 0.85 0.90 0.95Local Coverage 0 100 200 300 400 500 600 700 800 0 50 100 150 200Local Set Size 0 100 200 300 400 500 600 700 800 0 50 100 150 200 250Local Set Size 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level SplitCP ETA ECP EACP MagnitudeLearner MagLearnUndiscounted MagnitudeLearnerV2 SAOCP FACI (c) Brightness corruption 19 0 100 200 300 400 500 600 700 800 0.2 0.4 0.6 0.8 1.0Local Coverage 0 100 200 300 400 500 600 700 800 0.0 0.2 0.4 0.6 0.8 1.0Local Coverage 0 100 200 300 400 500 600 700 800 0 200 400 600 800Local Set Size 0 100 200 300 400 500 600 700 800 0 200 400 600 800Local Set Size 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level SplitCP ETA ECP EACP MagnitudeLearner MagLearnUndiscounted MagnitudeLearnerV2 SAOCP FACI (d) Gaussian noise corruption 0 100 200 300 400 500 600 700 800 0.2 0.4 0.6 0.8 1.0Local Coverage 0 100 200 300 400 500 600 700 800 0.2 0.4 0.6 0.8 1.0Local Coverage 0 100 200 300 400 500 600 700 800 0 200 400 600Local Set Size 0 100 200 300 400 500 600 700 800 0 200 400 600Local Set Size 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level 0 100 200 300 400 500 600 700 800 Time 1 2 3 4 5Corruption Level SplitCP ETA ECP EACP MagnitudeLearner MagLearnUndiscounted MagnitudeLearnerV2 SAOCP FACI (e) Motion blur corruption Figure 6: Our unsupervised methods ECP and EACP are able to provide nearly the same empirical coverage, and considerably smaller set sizes, that supervised methods on continuously shifting distributions. Dashed lines denote methods that rely on a ground truth label being revealed at test time. 20","libVersion":"0.3.2","langs":""}
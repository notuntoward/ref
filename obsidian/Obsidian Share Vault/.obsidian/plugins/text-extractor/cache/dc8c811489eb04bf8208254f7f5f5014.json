{"path":"lit/lit_notes_OLD_PARTIAL/Cui12SemiparametricGaussianVarianceMean.pdf","text":"International Scholarly Research Network ISRN Probability and Statistics Volume 2012, Article ID 345784, 18 pages doi:10.5402/2012/345784 Research Article Semiparametric Gaussian Variance-Mean Mixtures for Heavy-Tailed and Skewed Data Kai Cui Department of Statistical Science, Duke University, Durham, NC 27708, USA Correspondence should be addressed to Kai Cui, kc52@stat.duke.edu Received 29 October 2012; Accepted 20 November 2012 Academic Editors: A. Guillou, ´E. Marchand, and C. Proppe Copyright q 2012 Kai Cui. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. There is a need for new classes of ﬂexible multivariate distributions that can capture heavy tails and skewness without being so ﬂexible as to fully incur the curse of dimensionality intrinsic to nonparametric density estimation. We focus on the family of Gaussian variance-mean mixtures, which have received limited attention in multivariate settings beyond simple special cases. By using a Bayesian semiparametric approach, we allow the data to infer about the unknown mixing distribution. Properties are considered and an approach to posterior computation is developed relying on Markov chain Monte Carlo. The methods are evaluated through simulation studies and applied to a variety of applications, illustrating their ﬂexible performance in characterizing heavy tails, tail dependence, and skewness. 1. Introduction There is an increasing awareness of the importance of developing new classes of multivariate distributions that ﬂexibly characterize heavy tails and skewness, while accommodating tail dependence. Such tail dependence arises in many applications and is a natural consequence of dependence in outlying events. Such dependence is well known to occur in ﬁnancial data, communication networks, weather, and other settings, but is not adequately characterized by common approaches such as Gaussian copula models. Salmon [1] provides a compelling commentary on how reliance on a single measure of correlation in two variables based on a Gaussian copula may have played a substantial role in the ﬁnancial crisis. We need statistical methods based on new classes of distributions that do not rely on such unrealistic assumptions but that are still tractable to apply even in moderate- to high-dimensional settings. There is an existing literature relevant to this topic. Wang et al. [2] proposed a class of skew-symmetric distributions having probability density functions (pdfs) of the form 2 ISRN Probability and Statistics 2f(x)Q(x), where f is a continuous symmetric density and Q : ℜn → [0, 1] is a skewing function. Choosing f as normal leads to the skew normal class [3, 4], with other special cases corresponding to skew-t [5, 6],skewslash [7] and skew elliptical distributions [8]. These parametric models are useful in providing computationally tractable distributions that have parameters regulating skewness and kurtosis in the data. However, choosing a speciﬁc parametric family for f and Q can be challenging in practice, with diﬀerent choices yielding potentially diﬀerent results. Although one can potentially conduct model selection or averaging, this adds to the computational burden. Alternatively, nonparametric approaches have been explored to handle heavy-tailed and skewed observations with more ﬂexibility. For instance, mixtures of normal distributions have been widely used to approximate arbitrary distributions. Venturini et al. [9] uses mixtures of gamma distributions over the shape parameter to model heavy-tailed medical expenditure data. Mixtures of other heavy-tailed distributions have also been proposed. Such nonparametric density estimation approaches face substantial challenges in multivariate cases due to the curse of dimensionality. Fully nonparametric density estimation is almost too ﬂexible in allowing densities that have arbitrary numbers of modes and complex shapes, which are diﬃcult to estimate accurately based on available data in many cases. There has been some attempts to reduce dimensionality in multivariate density estimation using mixtures of factor analyzers [10] and alternative approaches, but nonetheless the curse is only partly thwarted by such eﬀorts. We focus on Gaussian variance-mean mixtures (GVMMs), introduced by Barndorﬀ- Nielsen [11] as a ﬂexible class of multivariate distributions induced through the following hierarchical model for yi =(yi1,...,yip)′, yi = μ + γVi + √ViΣ1/2Zi,Zi ∼ N(0,Ip),Vi ∼ G, (1.1) where μ ∈ℜp is a location parameter, γ ∈ℜp is a drift or skewness parameter, Vi ∼ G, G is a mixture distribution on [0, ∞), Zi ⊥ Vi,and Σ is a positive deﬁnite matrix. Conditions such as |Σ| = 1 are typically imposed to avoid an unidentiﬁable scale factor. This model corresponds to a generalization of the multivariate normal distribution, which is obtained in the special case in which γ = 0and G is a point mass. Current literature on Gaussian variance-mean mixtures is mostly focused on univari- ate models in which the mixing distribution G belongs to a parametric family. Some special cases corresponding to diﬀerent choices of G include Student’s t-test, Laplace, hyperbolic, normal inverse Gaussian, and variance gamma distributions, and such models have been applied broadly (see e.g., [11–16]). However, as with skew-symmetric distributions, limiting to a particular parametric class is clearly restrictive. In addition, some appealing subfamilies of GVMM, such as generalized hyperbolic (GH) distributions, are not analytically tractable. This is partially due to the ﬂatness of the likelihood function [17], which makes it hard to obtain reliable parameter estimates without prior information or some form of penalization, even with large sample sizes [18]. We propose Bayesian semiparametric Gaussian variance-mean mixture models, in which the mixing distribution G is modeled nonparametrically to ﬂexibly accommodate heavy tails and skewness, while letting the data inform about the appropriate distribution choice. Eﬃcient Bayesian computational strategies are developed for reliable inference on parameters. ISRN Probability and Statistics 3 2. Proposed Modeling Framework 2.1. Semiparametric GVMM Consider the p-dimensional multivariate continuous heavy-tailed and/or skewed observa- tions yi =(yi1,...,yip) ∼ f, (i = 1,...,N). Our goal is to obtain a ﬂexible GVMM model for the density f via modeling the mixing distribution G nonparametrically. To achieve this, we use Dirichlet process mixture (DPM) of generalized log-normal (log GN) prior with an unknown order s for G. Mathematically, the model is represented as yi = μ + γVi + √ ViΣ1/2Zi, Vi ∼ logGN( mi,ψ−1 i ,s ) ,Zi ∼ N(0,I), (mi,ψi) ∼ H, H ∼ DP(α0H0), (2.1) where α0 is the DP precision and H0 is the base measure for mi and ψi, which we choose independent normal and inverse gamma, respectively. The family of log GN distributions considered here was initially introduced by Vianelli [19], which can be obtained through the exponential transformation of a random variable that follows the generalized normal distribution [20, 21]. The pdf of a logGN(m, ψ, s) distributed random variable V is given by f(v) = s 2vψ1/2Γ(1/s) exp ( − ∣ ∣ ∣ ∣ ∣ log(v) − m ψ1/2 ∣ ∣ ∣ ∣ ∣ s) , (2.2) where v> 0, m ∈ℜ, ψ> 0, and s> 0. log GN densities are more ﬂexible than log-normal densities in including an additional parameter s controlling tail behavior, with log-normal corresponding to the special case in which s = 2 and double-exponential obtained by taking s = 1. Although log GN distributions are appealing in providing a simple generalization of the log-normal which is more ﬂexible in the tails, such distributions have been rarely implemented even in simpler settings due to the computational hurdles involved. Fortunately, for Bayesian posterior computation via MCMC we can rely on a data augmentation algorithm based on Fact 1, with the adaption to our setting using DPMs of generalized log-normals shown in Section 3.2. Fact 1. Let V and W be two random variables such that (1) f(vw)=(1/2ψ1/2w1/sv)I(| log v − m/ψ1/2|s <w), (2) W ∼ Gamma(1 + 1/s, 1) then V ∼ logGN(m, ψ, s). DPMs of Gaussians provide a highly ﬂexible approximation to arbitrary densities. As a prior for f directly, DPMs of Gaussians have appealing asymptotic properties and lead to minimax optimal adaptive rates of convergence under some conditions on the true data- generating density, with these conditions unfortunately ruling out heavy-tailed densities [22]. 4 ISRN Probability and Statistics As motivated above, our focus is on obtaining a ﬂexible prior for multivariate heavy-tailed and skewed densities, which is not fully nonparametric in the sense of allowing multimodal and other very irregular density shapes but can ﬂexibly approximate a broad class of unimodal densities without inducing restrictive tail constraints. Hence, it is not appropriate to choose a DPM of Gaussians directly for the density of the data f. Expression (2.1) instead uses a DPM of log GNs for the mixture distribution G within the GVMM framework. By using such DPM prior and setting s unknown, we obtain a large ﬂexibility of the mixing distribution and its tail decay, which can adopt diverse degrees of heavy tailness for the marginal density f of data, while allowing the data to fully infer about the unknown mixing distribution. After considering a broad variety of alternatives, we have found this speciﬁcation to be excellent at capturing a rich variety of multivariate heavy-tailed and skewed distributions, while also allowing symmetric data. Some basic properties are detailed below. 2.2. Tail Behavior It is important to understand the relationship between the tail behavior of the mixture distribution G and the induced tail behavior of f, the marginal distribution of yi.We start by considering the univariate case. Theorem 2.2 of Barndorﬀ-Nielsen and Sorensen [23] describes the relationship in the special case in which γ = 0 and hence the marginal distribution is symmetric. Theorem 2.1 (see [13]). Suppose f(y) is the pdf of a Gaussian variance mixture as described in (1.1) with γ = 0, and that the tail of the mixing distribution G with pdf g satisﬁes g(V ) ∼ e−ψ+V V λ−1L(V ),as V −→ +∞, (2.3) where ψ+ ≥ 0 and L is a function of slow variation with limV →∞(L(aV )/L(V )) = 1, for all a> 0. Then (1) if ψ+ = 0, f(y) ∼|y|2λ−1L(y2), |y|→∞, (2) if ψ+ > 0, f(y) ∼|y|λ−1 exp(−(2ψ+)1/2|y|)L(|y|), |y|→∞. Observe that the tail behavior of a Gaussian variance mixture (when γ = 0) mainly depends on the tail behavior of the mixing distribution. To generalize this to arbitrary Gaussian variance-mean mixtures, we ﬁrst introduce the following Lemma. Lemma 2.2. Suppose fG(y) is the pdf of a Gaussian variance mixture with mixing distribution G, and let ϕ denote the moment generating function of G. Then for all γ ∈ℜ s.t. ϕ(γ 2/2) < ∞: fG∗ (y) = exp (γy) ϕ (γ 2/2 ) fG(y) (2.4) is the pdf of a Gaussian variance-mean mixture with skewness parameter γ and mixing distribution G ∗ with pdf g∗ satisfying g∗(V )=(exp(γ 2V/2)/ϕ(γ 2/2))g(V ),where g is the pdf of G. The converse is also true. ISRN Probability and Statistics 5 Proofs can be found in the appendix. This lemma provides a link between the tail behavior of a Gaussian variance-mean mixture and that of a Gaussian variance mixture, via the link between tails behaviors of the two mixing distributions. This relationship is used in the following theorem. Theorem 2.3. Suppose fG(y) is the pdf of a Gaussian variance-mean mixture as described in (1.1), with skewness parameter γ and mixing distribution G. If the tail of the mixing density g satisﬁes g(V ) ∼ e−ψ+V V λ−1L(V ) as V −→ +∞, (2.5) where ψ+ ≥ 0, and L is a function of slow variation with limV →∞(L(aV )/L(V )) = 1, for all a> 0, then: (1) fG(y) ∼ yλ−1 exp(−(√ 2ψ+ + γ 2 − γ)y)L(y),as y → +∞, (2) fG(y) ∼|y|λ−1 exp(−(√ 2ψ+ + γ 2 + γ)|y|)L(|y|),as y →−∞. Theorem 2.3 shows that the tail behavior of a Gaussian variance-mean mixture also depends on that of the mixing distribution. Generally, heavier tails in the mixing distribution induce heavier tails of the Gaussian variance-mean mixture. Thus, by placing a DPM of generalized log-normals on the mixing distribution, we induce a prior on the density f with ﬂexible degrees of tail decay. Another observation is that if the mixing distributions have subexponential tails (such as log-concave distributions), then the Gaussian variance-mean mixture also has sub-exponential tails, which also illustrates the limitation in the ﬂexibility of using particular parametric cases of the Gaussian variance-mean mixtures to ﬁt data. 2.3. Moments To compute the moments of Gaussian variance-mean mixtures, we can directly apply the law of total cumulance. Let k1, k2, k3,and k4 denote the expectation, variance, skewness, and kurtosis of the Gaussian variance-mean mixtures described in (1.1),and let k∗ 1,k∗ 2,k∗ 3,and k∗ 4 denote those of the mixing distribution. Given they all exist, we have simply k1 = μ + γk∗ 1,k2 = k∗ 1 + γ 2k∗ 2,k3 = γk∗ 2 + γ 3k∗ 3. (2.6) More generally, we have ϕf (s)= exp(μs)ϕ(γs + 1/2s 2), where ϕf and ϕ are moment generating functions of GVMM and the corresponding mixing distribution, respectively. So clearly, the existence of moments of mixing distributions indicates the existence of moments of Gaussian variance-mean mixtures, and γ can control expectation, variance, and kurtosis, in addition to being a skewness parameter. 3. Bayesian Computation 3.1. Priors In the semiparametric GVMM framework, the |Σ| = 1 constraint is typically imposed to guarantee model identiﬁability. To improve computational eﬃciency in our proposed 6 ISRN Probability and Statistics Bayesian computational algorithm, we use a parameter-expansion approach in which priors are placed on parameters in an unidentiﬁable working model without the |Σ| = 1 constraint. We then include a postprocessing step to transform the parameters back to an identiﬁable inferential model, which includes the |Σ| = 1 constraint. A related strategy was used in Gaussian factor models by Ghosh and Dunson [24]. We use fairly diﬀuse priors for unidentiﬁable parameters, as an aid to mixing and because it is diﬃcult to elicit priors for these parameters. However, we avoid completely noninformative priors because the ﬂatness of the likelihood function in some GVMM models [13] can lead to unreliable inferences in the absence of some prior information or penalization. To address this problem, we propose an empirical Bayes approach to incorporate skewness information from the data in estimating hyperparameters for the skewness parameter γ. In particular, we transform the original data to have a positive sample skewness and a unit sample variance. The data are ﬁrst normalized and the sample skewness is calculated. If it is negative, we multiply the normalized data by a negative one. In conducting inferences, we transform back to the scale and sign of the original data. As GVMMs are closed under linear transformations, this will induce a GVMM. Because the transformed data are more likely to be right skewed or symmetric, we can more easily elicit a default weakly informative prior for the skewness parameter γ, which we choose to be normal with positive mean μγ , with a gamma hyperprior placed on μγ to improve prior robustness. For the order parameter s, a truncated inverse-gamma prior is used. Diﬀuse priors are placed on the remaining unknowns. To summarize γ ∼ N(μγ ,φγ ),μγ ∼ Gamma(αγ ,βγ ),φγ ∼ Gamma(aφ,bφ),s ∼ Gamma(αs,βs), μ ∼ N(0,φμ), Σ ∼ Inv-Wishart(m0, Ψ0). (3.1) These priors were used in an unconstrained working model for the transformed data and induce priors on the parameters in the identiﬁable inferential model having the |Σ| = 1 constraint. As for the DPM of log GN prior for the mixing distribution G, for ease in computation, we use a similar stick-breaking representation of the DPM as proposed by Sethuraman [25] but with generalized log-normal instead of normal components, and truncated at M components following Ishwaran and James [26]. Furthermore, as pointed out before, since it is hard to directly update parameters of the generalized log-normal distributions, we further utilize Fact 1 to introduce augmented data w =(w1,...,wN) to improve the computational eﬃciency. To summarize, the data-augmented stick-breaking representation of the DPM of log GN prior is shown as follows: f(vi | m, ψ, K,s, w) = 1 2ψ1/2 Ki w1/s i vi I ⎛ ⎝ ∣ ∣ ∣ ∣ ∣ ∣ log vi − mKi ψ1/2 Ki ∣ ∣ ∣ ∣ ∣ ∣ s <wi ⎞ ⎠, wi | s ∼ Gamma(1 + 1 s , 1 ). ISRN Probability and Statistics 7 Note that these are equivalent to Vi | m, ψ, K,s ∼ logGN ( Vi; mKi ,ψ−1 Ki ,s ) , Ki | p ∼ M∑ k=1pkδk(·),i = 1,...,N, (mk,ψk) ∼ H0(m, ψ),k = 1,...,M, p1 = ν1,pk = νk k−1∏ l=1 (1 − νl),k = 2,...,M − 1, νk ∼ Beta(1,α0),k = 1,...,M − 1,νM = 1, (3.2) where K =(K1,...,KN), m =(m1,...,mM), ψ =(ψ1,...,ψM), p =(p1,...,pM) and for H0,we use independent normal and inverse gamma for m and ψ, respectively. Here, Ki is augmented data representing the mixture component index for observation i. 3.2. Full Conditionals and Posterior Analysis Given the model and priors as speciﬁed by (2.1), (3.1),and (3.2), we use a data-augmented Gibbs sampler to update the unknown quantities, including parameters in the GVMM framework (μ, γ,and Σ), parameters in the DPM of log GN (m, ψ,and s), augmented data (K and w), hyperparameters, and the mixing variables (Vi). The Gibbs sampler is computationally eﬃcient and mixes rapidly as most of the full conditional distributions have closed forms, except those for μγ , Vi,and s, which are all univariate and updated using Metropolis-Hastings steps within the Gibbs sampler. Key steps in each Gibbs sampler iteration are listed as follows. (I) Samples μ, γ,and Σ: given normal priors for (μ, γ) and inverse-Wishart prior for Σ and the model that yi | Vi ∼ N(μ + γVi,ViΣ), (μ, γ) is sampled from conditionally normal distribution, and Σ from a conditionally inverse Wishart. (II) Samples m and ψ: given the following priors are used for mk and ψk for the kth log-normal component, k = 1,...,M: mk ∼ N( μm,φm),ψk ∼ Inv-Gamma(αψ,βψ). (3.3) Sample mk, k = 1,...,M, from conditionally truncated normal distribution: mk | ··· ∼ N(μm,φm)I(max l:Kl=k { log Vl − ψ1/2 k w1/s l } <mk < min l:Kl=k { log Vl + ψ1/2 k w1/s l }) (3.4) and sample ψk from conditionally truncated inverse-gamma distribution: ψk | ··· ∼ Inv-Gamma( αψ + Nk 2 ,βψ) I ( ψ1/2 > max l:Kl=k { ∣ ∣log Vl − mk∣ ∣ w1/s l }) , (3.5) where Nk is the total number of Vi in the kth mixture component. 8 ISRN Probability and Statistics (III) Sample w. wi, i = 1,...,N, is sample from conditionally truncated exponential distribution: wi | ··· ∼ Exp(1)I ⎛ ⎝wi > ⎛ ⎝ ∣ ∣log Vi − mKi∣ ∣ ψ1/2 Ki ⎞ ⎠ s⎞ ⎠. (3.6) (IV) Sample s: given the Gamma(αs,βs) for s, the full conditional distribution is sampled using Metropolis-Hastings algorithm, from the following full conditional truncated kernel: f(s | ··· ) ∝ s αs+N−1 exp (−βss ) ΓN(1/s) I( max i∈S− (0,ai) <s< min i∈S+ (ai) ), (3.7) where ai =(log wi)/ log(| log Vi − mKi|/ψ1/2 Ki ), i = 1,...,N. S− = {i :log(| log Vi − mKi|/ψ1/2 Ki ) < 0},and S+ = {i :log(| log Vi − mKi|/ψ1/2 Ki ) > 0}. (V) Sampling K: Ki, i = 1,...,N is sampled from the conditionally multinomial (MN) distribution, with Pr(Ki = k) | ··· ∝ pk ψ1/2 k exp { − ( ∣ ∣log Vi − mk∣ ∣ ψ1/2 k )s} . (3.8) (VI) νk, k = 1,...,M − 1 are updated from conditionally beta distribution as shown in Ishwaraman and James [26]. Also update univariate Vi, i = 1,...,N using a Metropolis-Hastings step within the Gibbs sampler. 4. Simulation Study and Real Data Analysis 4.1. Univariate Semiparametric GVMM To test the semiparametric framework, a simulated dataset from univariate GVMM is ﬁrst modeled. Speciﬁcally, observations yi (i = 1,...,N = 1000) are generated from model (2.1), with the true values of the parameters as shown in μ = 2,γ = 2, Σ= 1,Vi ∼ Gamma(3, 1), ∀i = 1,...,N. (4.1) The histogram of simulated data is shown in Figure S2(B) in Supplementary Material available online at doi:10.5402/2012/345784, which shows signiﬁcant heavy tailness and skewness, with sample kurtosis and skewness being 4.60 and 1.01, respectively. For Bayesian inference, we preprocess the original simulated data and place priors as described previously. We run the MCMC for 10000 iterations with the ﬁrst 5000 as burn- in. Several aspects of the posterior distributions are analyzed to evaluate the model ﬁtting. First of all, posterior samples of Vi and parameters allow us to reconstruct and visualize the unknown mixing distribution G. As shown in Figure S1, a comparison between the true ISRN Probability and Statistics 9 Table 1: Posterior quantile estimation to show the model ﬁtting. Posterior quantile C.I.s are obtained by simulating 200 reconstructed datasets based (each consisting of 5000 data points) on posterior samples of unknown quantities, each dataset giving one set of quantile point estimates. Observed quantiles are obtained from the 1000 observed simulated data. Quantiles Observed Posterior Posterior 95% quantile mean C.I. 2.5% −1.474 −1.492 [−1.535, −1.450] 5% −1.328 −1.322 [−1.363, −1.284] 25% −0.738 −0.711 [−0.742, −0.676] 50% −0.175 −0.162 [−0.190, −0.124] 75% 0.555 0.534 [0.492, 0.584] 95% 1.783 1.871 [1.776, 1.985] 97.5% 2.394 2.414 [2.283, 2.566] Table 2: Quantile estimates are obtained from models ﬁtted to the observed dataset using maximum likelihood estimation. To obtain the maximum likelihood estimators in skewed-Gaussian and t- distributions, the snR package is used [27]. Quantiles Observed Gaussian Skewed Gaussian Skewed t 2.5% −1.474 −1.960 −1.451 −1.446 5% −1.328 −1.645 −1.306 −1.300 25% −0.738 −0.674 −0.751 −0.749 50% −0.175 0.000 −0.173 −0.185 75% 0.555 0.674 0.590 0.566 95% 1.783 1.645 1.887 1.895 97.5% 2.394 1.960 2.339 2.379 mixing distribution (Gamma(3,1), panel (B)) and 100 reconstructed mixing distributions (panel (A)) shows signiﬁcant similarity, indicating that the model can eﬀectively capture the underlying structure. This is further illustrated by the posterior distributions of μ and γ, which have 95% C.I.s being [0.8676,3.2390] and [1.2532,2.9660], and posterior means being 2.0804 and 2.1523, respectively, which also very well cover the true values. It is worth mentioning that later in the real data analysis where the true values are unknown, the posterior distribution of γ provides us with a useful tool to assess skewness, with signiﬁcant positive values of γ indicating skewness. Furthermore, we can reconstruct the dataset based on the posterior samples of model parameters and the mixing distribution G and directly visualize whether the posterior predictive distribution resembles the observed one. We reconstructed 200 such datasets, each containing 5000 data points. While the reconstructed datasets resemble the observed one (shown in Figure S2), we speciﬁcally looked at the posterior quantile estimates of the ﬁtted semiparametric GVMM, which is done by getting the 95% posterior C.I.s for a series of quantiles based on the 200 reconstructed datasets. This is shown in Table 1 and compared to the quantile estimates obtained by ﬁtting other models, such as normal, skewed normal, and skewed t-distributions (Table 2). Compared to the fact that simple Gaussian model fails to capture neither heavy tailness and skewness, our GVMM model ﬁts the data as well as skewed-Gaussian and skewed t-distributions, while maintaining unique advantages of relatively easy forms, convenient sampling, and interpretability. 10 ISRN Probability and Statistics Table 3: Posterior quantile estimation to show the model ﬁtting. Posterior quantile C.I.s are obtained by simulating 200 reconstructed datasets based (each consisting of 5470 data points) on posterior samples of unknown quantities, each dataset giving one set of quantile point estimates. Real data quantiles are obtained from the 5470 observed S&P 500 returns. Quantiles Real data Posterior Posterior 95% quantile mean C.I. 2.5% −1.965 −1.965 [−2.097, −1.867] 5% −1.511 −1.513 [−1.589, −1.440] 25% −0.493 −0.495 [−0.534, −0.469] 50% −0.0315 −0.0175 [−0.0421, 0.0087] 75% 0.467 0.476 [0.440, 0.512] 95% 1.537 1.560 [1.482, 1.649] 97.5% 2.056 2.064 [1.943, 2.225 ] 4.2. Modeling the S&P 500 Returns It is well known that stock returns do not always conﬁrm well with a Gaussian distribution. Modeling both heavy tailness and asymmetry of returns is becoming important in economics and ﬁnance. Here, we look at daily returns of Standard & Poor’s 500 Composite (S&P 500) index from 01/02/1990 to 09/13/2011. Totally 5470 observations are shown in Figure S3(A), with sample skewness being 0.189 (after preprocessing), which suggests that the return distribution may be slightly right skewed. Similar univariate semiparametric GVMM and prior setup are applied to the dataset to access the capability of the model in capturing the return distribution. To evaluate the model ﬁtting, we also reconstructed 200 datasets based on the posterior samples of unknown quantities (each consisting of 5470 observations), and a quick comparison (Figure S3) between the observed and reconstructed datasets shows a signiﬁcant similarity, indicating that our model captures the return distribution well. We also look at posterior quantile estimates based on the 200 simulated datasets (Table 3), which further illustrate the goodness of ﬁt of the model. Furthermore, we speciﬁcally look at the posterior distribution of γ. Generally speaking, when the sample skewness is relatively small (as in this case),itisdiﬃcult to claim whether the true distribution is skewed or symmetric, because samples from symmetric heavy-tailed distributions can also exhibit signiﬁcant sample skewness due to the presence of extreme values with a ﬁnite sample size. However, one feature of the Bayesian semipara- metric GVMM framework is that the skewness can be simultaneously inferred by looking at the sign of the γ parameter, which can test the actual existence of skewness directly against an “artiﬁcial sample skewness” due to heavy tailness. To illustrate this, the histogram of 5000 posterior samples of γ is shown in Figure 1, which gives a 95% posterior C.I. [0.0033,0.0838] and thus claims that the distribution is slightly right skewed at the 95% conﬁdence level. As a striking comparison, we generated 5000 samples from t-distribution (df = 3, shown in the appendix). The sample skewness is 0.228, which appears to suggest a slight right skewness. However, when we ﬁt the data using our Bayesian semiparametric GVMM and look at the posterior distribution of γ (shown in Figure 2, as compared to Figure 1), the posterior 95% C.I. is [-0.0371, 0.0757], with posterior mean at 0.0213, which argue against the existence of skewness and suggest that the sample skewness is due to the fact that t- distribution is heavy tailed. ISRN Probability and Statistics 11 −0.04 −0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0 50 100 150 200 250 300 350 400 450 500 Figure 1: Posterior distribution of γ is presented. S&P 500 daily returns are modeled via Bayesian semiparametric GVMM, and γ conﬁrms the existence of skewness with the sample skewness being 0.189. −0.1 −0.05 0 0.05 0.1 0.15 0 100 200 300 400 500 600 Figure 2: Posterior distribution of γ is presented. A set of random samples from t-distribution with df = 3 are modeled via Bayesian semiparametric GVMM, and γ conﬁrms no skewness although the sample skewness is 0.228. 4.3. Modeling Multivariate Monthly Precipitation There has also been a growing interest for ﬂexible families of non-Gaussian distributions allowing skewness and heavy tails in environmental science and climatology, as more heavy- tailed and skewed data are observed practically. Speciﬁcally, it is well known that monthly rainfall is strongly skewed to the right with high positive values of skewness coeﬃcients (e.g., [28]). Various distributions have been suggested to model the precipitation data, among which there are the exponential, gamma (e.g., [29, page 98]), log-normal (e.g., [30]), and log-skewed-normal/t-distributions [31]. However, most of the studies are focusing on univariate modeling, and there has been a little physical justiﬁcation to why a speciﬁc distribution is used. Although skewed-Gaussian/t-type distributions have been extended to the multivariate setting (see e.g., [27, 31]), they do not handle substantial skewness well 12 ISRN Probability and Statistics −2 0 2 4 0 10 20 30 −2 0 2 4 6 0 10 20 30 40 0 2 4 0 10 20 30 40 0 10 20 30 −2 024−2 Figure 3: Monthly log precipitation data for July from 1895 to 2010 (116 observations) obtained from four stations in North Carolina show heavy right skewness. [18]. We are motivated to consider applying the Bayesian semiparametric GVMM framework to model multivariate precipitation data. One appealing feature is that the extension to multivariate case is both straightforward and interpretable. US national and regional precipitation data are publicly available from the United States Historical Climatology Network (USHCN). For the purpose of exposition, we used monthly precipitation data measured in inches from four local stations (Albemarle, Chapel Hill, Edenton, and Elizabeth City) in the state of North Carolina, for the period from 1895 through 2010 (116 data per station for each month). Figure 3 presents the histogram of log monthly precipitation data for July every year. Data from all four stations exhibit right skewness, with sample skewness coeﬃcients being 0.675, 1.440, 0.632, and 0.971, respectively. We ﬁt the semiparametric multivariate GVMM (2.1) to the data (dimension p = 4). We run the Markov Chain for 10000 iterations, which shows good mixing and convergence, and discarded the ﬁrst 5000 as burn-in. To illustrate the model ﬁtting, we reconstruct a precipitation dataset with 5000 observations based on posterior samples of all unknown quantities and compare the reconstructed posterior predictive distribution with the observed. Speciﬁcally, we test both whether the marginal univariate distributions of each dimension and the covariance structure are captured correctly. As shown in Figure 4, the curves represent the corresponding univariate kernel smoothing density estimates of the posterior predictive distributions based on 5000 samples from the ﬁtted model, which show good ﬁtting to the observed dataset shown with the histograms. The goodness of ﬁt is further illustrated by the PP-plot between the observed and ﬁtted distributions (Figure 5). Covariance structure is also modeled well (a comparison between Figures 6 and 7). ISRN Probability and Statistics 13 −5 0 510 0 0. 1 0. 2 0. 3 0. 4 0. 5 −5 0 510 0 0. 2 0. 4 0. 6 0. 8 −5 0 5 10 0 0. 1 0. 2 0. 3 0. 4 0. 5 −5 0 510 0 0. 1 0. 2 0. 3 0. 4 0. 5 Figure 4: Monthly log-precipitation data for July from 1895 to 2010 (116 observations) obtained from four stations in North Carolina (shown in histogram) are ﬁtted using Bayesian semiparametric GVMM. Red line shows the kernel density of ﬁtted distributions for the stations, estimated from 5000 posterior predictive samples of the ﬁtted GVMM. All of these suggest that our framework eﬀectively captures the underlying structure of the precipitation data. As a comparison, we also ﬁtted the log-precipitation data with multivariate skewed t-distributions [32], which have also been used to model skewed and heavy-tailed data, with two tails behaving as polynomials. The ﬁtted model is obtained via maximum likelihood estimation using the snR package provided by Azzalini [27], and both the marginal distributions and covariance structure of the ﬁtted multivariate skewed t-model are compared to those of the observed dataset. We can ﬁnd that although the multivariate skewed t-distribution captures most of the univariate marginal distributions with a slightly lighter right tails and heavier left tails (Figure S4), it failed to ﬁnd the correct covariance structure (Figure S5 compared to Figures 6 and 7). This could result from getting stuck in a local maximum and obtaining a suboptimal solution due to the relatively complicated likelihood function of multivariate skewed t-distributions. In any case, our Bayesian semiparametric multivariate GVMM seems to provide a computationally and structurally simpler yet more eﬀective family of distributions for multivariate skewed and heavy-tailed data. 5. Discussion This paper proposes the use of Bayesian semiparametric Gaussian variance-mean mixtures as a ﬂexible, interpretable, and computationally tractable model for heavy-tailed and skewed 14 ISRN Probability and Statistics −2 −1 0 1 2 3 4 −3 −2 −1 0 1 2 3 4Fitted GVMM quantiles Observed quantiles −2 0 2 4 6 −2 −1 0 1 2 3 4 5 Observed quantilesFitted GVMM quantiles −2 −1 0 1 2 3 4 −3 −2 −1 0 1 2 3 4 5 Observed quantilesFitted GVMM quantiles −2 −1 0 1 2 3 4 −2 −1 0 1 2 3 4 Observed quantilesFitted GVMM quantiles Figure 5: PP-plots for the Bayesian semiparametric GVMM model ﬁtted to log-precipitation for July in all four stations. Ede C–H Eli Alb Figure 6: Sample covariance structure of monthly log-precipitation data from four local stations in the state of North Carolina. Alb: Albemarle, C-H: Chapel Hill, Ede: Edenton, and Eli: Elizabeth City. ISRN Probability and Statistics 15 Ede C–H Eli Alb Figure 7: Covariance structure of monthly log-precipitation when ﬁtted with Bayesian semiparametric GVMM. Alb: Albemarle, C-H: Chapel Hill, Ede: Edenton, and Eli: Elizabeth City are four stations in the state of North Carolina. observations. The model assumes the mixing distribution G in the general Gaussian variance- mean mixtures to be unknown, so the data inform about the appropriate distribution choice, the degree of skewness, heaviness of tails, and the shape of the distributions and thus provide a more ﬂexible framework for heavy-tailed and skewed data analysis. Although we test the model with univariate and multivariate simulated and real data, the broader question of scaling the framework to higher dimensions, possibly combined with factor models for sparse modeling, is interesting and challenging. Under the same scenario, the assumption that a single mixing variable is controlling the tails and skewness of all dimensions seems restrictive, although this may be an acceptable assumption with low dimensions. Some hierarchical nonparametric models that allow multiple mixing variable/distributions may help in this case, but how to deﬁne an identiﬁable parametrization will deﬁnitely be an issue whenever multiple mixing variables are assumed, which is also worth more thoughts and theory to support. On the other hand, we consider time-varying semiparametric GVMM a natural extension to the GVMM framework, taking advantage of the interpretability of model parameters. However, the more general class of spatial-temporal models, where additional structures and sources of information are included, is still challenging and yet unexplored. Appendix Proofs Proof of Fact 1. Let V and W be two random variables such that (1) f(v | w)=(1/2ψ1/2w1/sv)I(| log v − m/ψ1/2|s <w), (2) W ∼ Gamma(1 + 1/s, 1) 16 ISRN Probability and Statistics then f(v) = ∫ f(v | w)f(w)dw ∝ ∫ 1 w1/sv I (∣ ∣ ∣ ∣ ∣ log v − m ψ1/2 ∣ ∣ ∣ ∣ ∣ s <w ) · w1/s exp(−w)dW ∝ ∫ 1 v I ( exp(−w) < exp ( − ∣ ∣ ∣ ∣ ∣ log v − m ψ1/2 ∣ ∣ ∣ ∣ ∣ s)) exp(−w)dw ∝ 1 v exp ( − ∣ ∣ ∣ ∣ ∣ log v − m ψ1/2 ∣ ∣ ∣ ∣ ∣ s) (A.1) which is the kernel of a log GN(m, ψ, and s) as deﬁned in (2.2). Proof of Lemma 2.2. Given p-dimensional observation y from a Gaussian variance-mean mixtures described in (1.1),let f ∗ G denote the pdf of the Gaussian variance-mean mixture with mixing distribution G ∗,and let fG denote the pdf of the Gaussian variance mixture (γ = 0) with mixing distribution G. Without the loss of generality, we considered the case with μ = 0 (otherwise, a linear transformation μ + y will complete the proof) f(y) ∝ ∫ ∞ 0 V −p/2 exp(− 1 2 y′ 1 V Σ−1yg(V )dV ) ∝ ∫ ∞ 0 V −p/2 exp(− 1 2 y′ 1 V Σ−1y) exp(− 1 2 γ ′Σ−1γV ) exp( 1 2 γ ′Σ−1γV )g(V )dV. (A.2) Let δ =(1/2)γ′Σ−1γ,with δ =(1/2)γ 2 ≥ 0 in the univariate case, and deﬁne g∗(V ) = g(V ) exp(δV ) ϕ(δ) , (A.3) where g and g∗ are densities for G and G ∗, respectively, and ϕ is the moment generating function for g,with ϕ(δ) < ∞.Plug g∗ in, and we can see that fg∗ (y) = exp(γy) ϕ (γ 2/2 ) fg(y). (A.4) References [1] F. Salmon, “The formula that killed wall street,” Signiﬁcance, vol. 9, no. 1, pp. 16–20, 2012. [2] J. Wang, J. Boyer, and M. G. Genton, “A skew-symmetric representation of multivariate distributions,” Statistica Sinica, vol. 14, pp. 1259–1270, 2004. [3] A. Azzalini, “A class of distributions which includes the normal ones,” Scandinavian Journal of Statistics, vol. 12, no. 2, pp. 171–178, 1985. ISRN Probability and Statistics 17 [4] A. Azzalini and A. Dalla Valle, “The multivariate skew-normal distribution,” Biometrika, vol. 83, no. 4, pp. 715–726, 1996. [5] S. K. Sahu, D. K. Dey, and M. D. Branco, “A new class of multivariate skew distributions with applications to Bayesian regression models,” The Canadian Journal of Statistics, vol. 31, no. 2, pp. 129– 150, 2003. [6] A. K. Gupta, “Multivariate skew t-distribution,” Statistics, vol. 37, no. 4, pp. 359–363, 2003. [7] J. Wang and M. G. Genton, “The multivariate skew-slash distribution,” Journal of Statistical Planning and Inference, vol. 136, no. 1, pp. 209–220, 2006. [8] M. G. Genton and N. M. R. Loperﬁdo, “Generalized skew-elliptical distributions and their quadratic forms,” Annals of the Institute of Statistical Mathematics, vol. 57, no. 2, pp. 389–401, 2005. [9] S. Venturini, F. Dominici, and G. Parmigiani, “Gamma shape mixtures for heavy-tailed distributions,” The Annals of Applied Statistics, vol. 2, no. 2, pp. 756–776, 2008. [10] M. Chen, J. Silva, J. Paisley, C. Wang, D. Dunson, and L. Carin, “Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: algorithm and performance bounds,” IEEE Transactions on Signal Processing, vol. 58, no. 12, pp. 6140–6155, 2010. [11] O. Barndorﬀ-Nielsen, “Exponentially decreasing distributions for the logarithm of particle size,” Proceedings of the Royal Society of London A, vol. 353, no. 1674, pp. 401–419, 1977. [12] O. E. Barndorﬀ-Nielsen, J. L. Jensen, and M. Sorensen, “Parametric modelling of turbulence,” Philosophical Transactions, vol. 332, no. 1627, pp. 439–455, 1990. [13] O. Barndorﬀ-Nielsen, “The hyperbolic distribution in statistical physics,” Scandinavian Journal of Statistics, vol. 9, no. 1, pp. 43–46, 1982. [14] O. E. Barndorﬀ-Nielsen, “Normal inverse Gaussian distributions and stochastic volatility modelling,” Scandinavian Journal of Statistics, vol. 24, no. 1, pp. 1–13, 1997. [15] O. Arslan and A. ˙I. Genc¸, “The skew generalized t (SGT) distribution as the scale mixture of a skew exponential power distribution and its applications in robust estimation,” Statistics, vol. 43, no. 5, pp. 481–498, 2009. [16] O. Arslan, “An alternative multivariate skew Laplace distribution: properties and estimation,” Statistical Papers, vol. 51, no. 4, pp. 865–887, 2010. [17] E. Eberlein and K. Prause, “The generalized hyperbolic model: ﬁnancial derivatives and risk measures,” in Mathematical Finance C Bachelier Congress 2000, pp. 245–267, Springer, Berlin, Germany, 1998. [18] K. Aas and I. H. Haﬀ, “The generalized hyperbolic skew students t-distribution,” Journal of Financial Econometrics, vol. 4, no. 2, pp. 275–309, 2006. [19] S. Vianelli, “The family of normal and lognormal distributions of order r,” Metron,vol. 41, no.1-2,pp. 3–10, 1983. [20] M. Varanasi and B. Aazhang, “Parametric generalized gaussian density estimation,” Journal of the Acoustical Society of America, vol. 86, no. 4, pp. 1404–1415, 1989. [21] S. Nadarajah, “A generalized normal distribution,” Journal of Applied Statistics, vol. 32, no. 7, pp. 685– 694, 2005. [22] W. Shen and S. Ghosal, “Adaptive bayesian multivariatedensity estimation with dirichlet mixtures,” submitted. [23] O. E. Barndorﬀ-Nielsen, J. Kent, and M. Sørensen, “Normal variance-mean mixtures and z distributions,” International Statistical Review, vol. 50, no. 2, pp. 145–159, 1982. [24] J. Ghosh and D. B. Dunson, “Default prior distributions and eﬃcient posterior computation in Bayesian factor analysis,” Journal of Computational and Graphical Statistics, vol. 18, no. 2, pp. 306–320, 2009. [25] J. Sethuraman, “A constructive deﬁnition of Dirichlet priors,” Statistica Sinica, vol. 4, no. 2, pp. 639– 650, 1994. [26] H. Ishwaran and L. F. James, “Gibbs sampling methods for stick-breaking priors,” Journal of the American Statistical Association, vol. 96, no. 453, pp. 161–173, 2001. [27] A. Azzalini, R Package sn: The Skew-Normal and Skew-t Distributions (Version 0.4–17), Universit‘a di Padova, Padova, Italia, 2011. [28] H. Ishwaran and L. F. James, “Gibbs sampling methods for stick breaking priors,” Journal of the American Statistical Association, vol. 96, no. 453, pp. 161–173, 2001. 18 ISRN Probability and Statistics [29] D. S. Wilks, Statistical Methods in the Atmospheric Sciences, vol. 91 of International Geophysics,Academic Press, 2nd edition, 2005. [30] E. L. Crow and K. Shimizu, Lognormal Distributions: Theory and Applications, Marcel Dekker, 1988. [31] Y. V. Marchenko and M. G. Genton, “Multivariate log-skew-elliptical distributions with applications to precipitation data,” Environmetrics, vol. 21, no. 3-4, pp. 318–340, 2010. [32] B. E. Hansen, “Autoregressive conditional density estimation,” International Economic Review, vol. 35, no. 3, pp. 705–730, 1994. Submit your manuscripts at http://www.hindawi.com Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Mathematics Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Mathematical Problems in Engineering Hindawi Publishing Corporation http://www.hindawi.com Differential Equations International Journal of Volume 2014 Applied Mathematics Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Probability and Statistics Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Mathematical Physics Advances in Complex Analysis Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Optimization Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Combinatorics Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 International Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Operations Research Advances in Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Function Spaces Abstract and Applied Analysis Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 International Journal of Mathematics and Mathematical Sciences Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 The Scientific World Journal Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Algebra Discrete Dynamics in Nature and Society Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Decision Sciences Advances in Discrete Mathematics Journal of Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Hindawi Publishing Corporation http://www.hindawi.com Volume 2014 Stochastic Analysis International Journal of","libVersion":"0.3.2","langs":""}
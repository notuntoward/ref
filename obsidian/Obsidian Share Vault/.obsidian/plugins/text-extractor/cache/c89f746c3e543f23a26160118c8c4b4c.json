{"path":"lit/lit_sources/Wiggers24hallucRAGwontSolve.pdf","text":"Why RAG won’t solve generative AI’s hallucination problem Kyle Wiggers @kyle_l_wiggers / 7:00 AM PDT • May 4, 2024 Image Credits: D3Damon / Getty Images Hallucinations — the lies generative A I models tell, basically — are a big problem for businesses looking to integrate the technology into their operations. B ecause models have no real intelligence and are simply predicting words, images, speech, music and other data according to a private schema, they sometimes get it wrong. Very wrong. In a recent piece in The Wall Street Journal, a source recounts an instance where Microsoft’s generative A I invented meeting attendees and implied that conference calls were about subjects that weren’t actually discussed on the call. A s I wrote a while ago, hallucinations may be an unsolvable problem with today’s transformer-based model architectures. B ut a number of generative A I vendors suggest that they can be done away with, 5/4/24, 11:47 A M Why RA G won’t solve generative A I’s hallucination problem | TechC runch chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/5 more or less, through a technical approach called retrieval augmented generation, or RA G. Here’s how one vendor, Squirro, pitches it: Here’s a similar pitch from SiftHub: RA G was pioneered by data scientist Patrick Lewis, researcher at Meta and University C ollege London, and lead author of the 2020 paper that coined the term. A pplied to a model, RA G retrieves documents possibly relevant to a question — for example, a Wikipedia page about the Super B owl — using what’s essentially a keyword search and then asks the model to generate answers given this additional context. “When you’re interacting with a generative A I model like C hatGPT or Llama and you ask a question, the default is for the model to answer from its ‘parametric memory’ — i.e., from the knowledge that’s stored in its parameters as a result of training on massive data from the web,” D avid Wadden, a research scientist at A I2, the A I-focused research division of the nonprofit A llen Institute, explained. “B ut, just like you’re likely to give more accurate answers if you have a reference A t the core of the offering is the concept of Retrieval A ugmented LLMs or Retrieval A ugmented Generation (RA G) embedded in the solution … [our generative A I] is unique in its promise of zero hallucinations. Every piece of information it generates is traceable to a source, ensuring credibility. Using RA G technology and fine-tuned large language models with industry-specific knowledge training, SiftHub allows companies to generate personalized responses with zero hallucinations. This guarantees increased transparency and reduced risk and inspires absolute trust to use A I for all their needs. 5/4/24, 11:47 A M Why RA G won’t solve generative A I’s hallucination problem | TechC runch chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/5 [like a book or a file] in front of you, the same is true in some cases for models.” RA G is undeniably useful — it allows one to attribute things a model generates to retrieved documents to verify their factuality (and, as an added benefit, avoid potentially copyright-infringing regurgitation). RA G also lets enterprises that don’t want their documents used to train a model — say, companies in highly regulated industries like healthcare and law — to allow models to draw on those documents in a more secure and temporary way. B ut RA G certainly can’t stop a model from hallucinating. A nd it has limitations that many vendors gloss over. Wadden says that RA G is most effective in “knowledge-intensive” scenarios where a user wants to use a model to address an “information need” — for example, to find out who won the Super B owl last year. In these scenarios, the document that answers the question is likely to contain many of the same keywords as the question (e.g., “Super B owl,” “last year”), making it relatively easy to find via keyword search. Things get trickier with “reasoning-intensive” tasks such as coding and math, where it’s harder to specify in a keyword-based search query the concepts needed to answer a request — much less identify which documents might be relevant. Even with basic questions, models can get “distracted” by irrelevant content in documents, particularly in long documents where the answer isn’t obvious. Or they can — for reasons as yet unknown — simply ignore the contents of retrieved documents, opting instead to rely on their parametric memory. RA G is also expensive in terms of the hardware needed to apply it at scale. 5/4/24, 11:47 A M Why RA G won’t solve generative A I’s hallucination problem | TechC runch chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/5 That’s because retrieved documents, whether from the web, an internal database or somewhere else, have to be stored in memory — at least temporarily — so that the model can refer back to them. A nother expenditure is compute for the increased context a model has to process before generating its response. For a technology already notorious for the amount of compute and electricity it requires even for basic operations, this amounts to a serious consideration. That’s not to suggest RA G can’t be improved. Wadden noted many ongoing efforts to train models to make better use of RA G-retrieved documents. Some of these efforts involve models that can “decide” when to make use of the documents, or models that can choose not to perform retrieval in the first place if they deem it unnecessary. Others focus on ways to more efficiently index massive datasets of documents, and on improving search through better representations of documents — representations that go beyond keywords. “We’re pretty good at retrieving documents based on keywords, but not so good at retrieving documents based on more abstract concepts, like a proof technique needed to solve a math problem,” Wadden said. “Research is needed to build document representations and search techniques that can identify relevant documents for more abstract generation tasks. I think this is mostly an open question at this point.” So RA G can help reduce a model’s hallucinations — but it’s not the answer to all of A I’s hallucinatory problems. B eware of any vendor that tries to claim otherwise. 5/4/24, 11:47 A M Why RA G won’t solve generative A I’s hallucination problem | TechC runch chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/5 5/4/24, 11:47 A M Why RA G won’t solve generative A I’s hallucination problem | TechC runch chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/5","libVersion":"0.3.2","langs":""}
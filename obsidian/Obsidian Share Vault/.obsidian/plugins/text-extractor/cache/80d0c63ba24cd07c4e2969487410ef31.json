{"path":"lit/lit_sources.backup/Kloss18cmbnPhysLrnMdl.pdf","text":"Combining learned and analytical models for predicting action effects from sensory data Alina Kloss∗, Stefan Schaal†∗, and Jeannette Bohg‡ ∗Autonomous Motion Department, Max Planck Institute for Intelligent Systems, Germany †Computational Learning and Motor Control Lab, University of Southern California, USA ‡Department of Computer Science, Stanford University, USA Abstract—One of the most basic skills a robot should possess is predicting the effect of physical interactions with objects in the environment. This enables optimal action selection to reach a certain goal state. Traditionally, dynamics are approximated by physics-based analytical models. These models rely on speciﬁc state representations that may be hard to obtain from raw sensory data, especially if no knowledge of the object shape is assumed. More recently, we have seen learning approaches that can predict the effect of complex physical interactions directly from sensory input. It is however an open question how far these models generalize beyond their training data. In this work, we investigate the advantages and limitations of neural network based learning approaches for predicting the effects of actions based on sensory input and show how analytical and learned models can be combined to leverage the best of both worlds. As physical interaction task, we use planar pushing, for which there exists a well-known analytical model and a large real- world dataset. We propose to use a convolutional neural network to convert raw depth images or organized point clouds into a suitable representation for the analytical model and compare this approach to using neural networks for both, perception and prediction. A systematic evaluation of the proposed approach on a very large real-world dataset shows two main advantages of the hybrid architecture. Compared to a pure neural network, it signiﬁcantly (i) reduces required training data and (ii) improves generalization to novel physical interaction. I. INTRODUCTION We approach the problem of predicting the consequences of physical interaction with objects in the environment based on raw sensory data. Traditionally, interaction dynamics are described by a physics-based analytical model [26, 19, 28] which relies on a certain representation of the environment state. This approach has the advantage that the underlying function and the input parameters to the model have physical meaning and can therefore be transferred to problems with variations of these parameters. They also make the underlying assumptions in the model transparent. However, deﬁning such models for complex scenarios and extracting the required state representation from raw sensory data may be very hard, especially if no assumptions about the shape of objects are made. More recently, we have seen approaches that successfully replace the physics-based models with learned ones [29, 4, 22, 16, 3]. While often more accurate than analytical models, these methods still assume a predeﬁned state representation as input and do not address the problem of how it may be extracted from the raw sensory data. Some neural network based methods instead simultaneously learn a representation of the input and the associated dynamics from large amounts of training data, e.g. [5, 2, 24, 8]. They have shown impressive results in predicting the effect of physical interactions. In [2], the authors argue that a neural network may beneﬁt from choosing its own representation of the input data instead of being forced to use a predeﬁned state representation. They reason that a problem can often be parametrized in different ways and that some of these parametrizations might be easier to obtain from the given sensory input than others. The disadvantage of a learned representation is however that it usually cannot be be mapped to physical quantities. This makes it hard to intuitively under- stand the learned functions and representations. In addition, it remains unclear how these models could be transferred to similar problems. Neural networks often have the capacity to memorize their training data [27] and learn a mapping from inputs to outputs instead of the “true” underlying function. This can make perfect sense if the training data covers the whole problem domain. However, when data is sparse (e.g. because a robot learns by experimenting), the question of how to generalize beyond the training data becomes very important. Our hypothesis is that using prior knowledge from existing physics-based models can provide a way to reduce the amount of required training data and at the same time ensure good generalization beyond the training domain. In this paper, we thus investigate using neural networks for extracting a suitable representation from raw sensory data that can then be con- sumed by an analytical model for prediction. We compare this hybrid approach to using a neural network for both perception and prediction and to the analytical model applied on ground truth input values. As example physical interaction task, we choose planar pushing. For this task, a well-known physical model [19] is available as well as a large, real-world dataset [26] which we augmented with simulated images. Given a depth image of a tabletop scene with one object and the position and movement of the pusher, our models need to predict the object position in the given image and its movement due to the push. Although the state-space of the object is rather low- dimensional (2D position plus orientation), pushing is already a quite involved manipulation problem: The system is under- actuated and the relationship between the push and the object movement is highly non-linear. The pusher can slide alongarXiv:1710.04102v3 [cs.RO] 19 Oct 2018 the object and dynamics change drastically when it transitions between sticking and sliding-contact or makes and breaks contact. Our experiments show that despite of relying on depth images to extract position and contact information, all our models perform similar to the analytical model applied on the ground truth state. Given enough training data and evaluated inside of its training domain, the pure neural network imple- mentation performs best and even outperforms the analytical model baseline signiﬁcantly. However, when it comes to generalization to new actions the hybrid approach is much more accurate. Additionally, we ﬁnd that the hybrid approach needs signiﬁcantly less training data than the neural network model to arrive at a high prediction accuracy. A. Contributions In this work, we make the following contributions: • We show how analytical dynamics models and neural networks can be combined and trained end-to-end to predict the effects of robot actions based on depth images or organized point clouds. • We compare this hybrid approach to using a pure neural network for learning both, perception and prediction. Evaluations on a real world physical interaction task demonstrate improved data efﬁciency and generalization when including the analytical model into the network over learning everything from scratch. • For evaluation, we augmented an existing dataset of pla- nar pushing with depth and RGB images and additional contact information. The code for this is available online. The present paper is an extension of preliminary results in [15] (under review). Here we extend our evaluations and add results for an architecture that combines the analytical model with a learned error-correction term to better compen- sate for possible inaccuracies of the analytical model. While in previous work, the visual setup was limited to a top- down view of the scene we also demonstrate in Section VI, that our proposed architecture is still applicable without this simplifying assumption. B. Outline This paper is structured as follows: We begin with a review of related work in Section II. In Section III we formally describe the problem we address, introduce an analytical model for planar pushing (III-A) and the dataset we used for our experiments (III-B). Section IV introduces and compares the different ap- proaches for learning perception and prediction. The eval- uation in Section V focuses on the data-efﬁciency (V-C) and generalization abilities (V-D, V-E, V-F) of the different architectures and thus mainly addresses the prediction part of the problem. The perception task is kept simple for these experiments by using a top-down view of the scene. This changes in Section VI where we demonstrate that the hybrid approach also performs well in a less constrained visual setup. Section VII ﬁnally summarizes our results and gives an outlook to future work. II. RELATED WORK A. Models for pushing Analytical models of quasi-static planar pushing have been studied extensively in the past, starting with Mason [21]. Goyal et al. [9] introduced the limit surface to relate frictional forces with object motion, and much work has be done on different approximate representations of it [10, 11]. In this work, we use a model by Lynch et al. [19], which relies on an ellipsoidal approximation of the limit surface. More recently, there has also been a lot of work on data- driven approaches to pushing [29, 4, 22, 16, 3]. Kopicki et al. [16] describe a modular learner that outperforms a physics engine for predicting the results of 3D quasi-static pushing even for generalizing to unseen actions and object shapes. This is achieved by providing the learner not only with the trajectory of the global object frame, but also with multiple local frames that describe contacts. The approach however re- quires knowledge of the object pose from an external tracking system and the learner does not place the contact-frames itself. Bauza and Rodriguez [3] train a heteroscedastic Gaussian Process that predicts not only the object movement under a certain push, but also the expected variability of the outcome. The trained model outperforms an analytical model [19] given very few training examples. It is however speciﬁcally trained for one object and generalization to different objects is not attempted. Moreover, this work, too, assumes access to the ground truth state, including the contact point and the angle between the push and the object surface. B. Learning dynamics based on raw sensory data Many recent approaches in reinforcement learning aim to solve the so called “pixels to torque” problem, where the network processes images to extract a representation of the state and then directly returns the required action to achieve a certain task [18, 17]. Jonschkowski and Brock [13] argue that the state-representation learned by such methods can be improved by enforcing robotic priors on the extracted state, that may include e.g. temporal coherence. This is an alternative way of including basic principles of physics in a learning approach, compared to what we propose here. While policy learning requires understanding the effect of actions, the above methods do not acquire an explicit dynamics model. We are interested in learning such an explicit model, as it enables optimal action selection (potentially over a larger time horizon). The following papers share this aim. Agrawal et al. [2] consider a learning approach for pushing objects. Their network takes as input the pushing action and a pair of images: one before and one after a push. After encoding the images, two different network streams attempt to predict (i) the encoding of the second image given the ﬁrst and the action and (ii) the action necessary to transition from the ﬁrst to the second encoding. Simultaneously training for both tasks improves the results on action prediction. The authors do not enforce any physical models or robotic priors. As the learned models directly operate on image encodings instead of physical quantities, we cannot compare the accuracy of the forward prediction part (i) to our results. SE3-Nets [5] process organized (i.e. image shaped) 3D point clouds and an action to predict the next point cloud. For each object in the scene, the network predicts a segmentation mask and the parameters of an SE3 transform (linear velocity, rotation angle and axis). In newer work [6], an intermediate step is added, that computes the 6D pose of each object, before predicting the transforms based on this more structured state representation. The output point cloud is obtained by transforming all input pixels according to the transform for the object they correspond to. The resulting predictions are very sharp and the network is shown to correctly segment the objects and determine which are affected by the action. An evaluation of the generalization to new objects or forces was however not performed. Our own architecture is inspired by this work. The pure neural network we use to compare to our hybrid approach can be seen as a simpliﬁed variant of SE3-Nets, that predicts SE2 transforms (see Sec. IV). Since we deﬁne the loss directly on the predicted movement of the object, we omit predicting the next observation and the segmentation masks required for this. We also use a modiﬁed perception network, which relies mostly on a small image patch around the robot end-effector. Finn et al. [8] is similar to [5] and explores different possibilities of predicting the next frame of a sequence of actions and RGB images using recurrent neural networks. Visual Interaction Networks [24] also take temporal infor- mation into account. A convolutional neural network encodes consecutive images into a sequence of object states. Dynamics are predicted by a recurrent network that considers pairs of objects to predict the next state of each object. C. Combining analytical models and learning The idea of using analytical models in combination with learning has also been explored in previous work. Degrave et al. [7] implemented a differentiable physics engine for rigid body dynamics in Theano and demonstrate how it can be used to train a neural network controller. In [23], the authors signiﬁcantly improve Gaussian Process learning of inverse dynamics by using an analytical model of robot dynamics with ﬁxed parameters as the mean function or as feature transform inside the covariance function of the GP. Both works however do not cover visual perception. Most recently, Wu et al. [25] used a graphics and physics engine to learn to extract object-based state representations in an unsupervised way: Given a sequence of images, a network learns to produce a state representation that is predicted forward in time using the physics engine. The graphics engine is used to render the predicted state and its output is compared to the next image as training signal. In contrast to the aforementioned work, we not only combine learning and analytical models, but also evaluate the advantages and limitations of this approach. Finally, [20] present an interesting approach to learning func- tions by training a neural network to combine a number of mathematical base operations (like multiplication, division, sine and cosine). This enables their “Equation Learner” to learn functions which generalize beyond the domain of the training data, just like traditional analytical models. Training these networks is however challenging and involves training many different models and choosing the best in an additional model selection step. III. PROBLEM STATEMENT Our aim is to analyse the beneﬁts of combining neural networks with analytical models. We therefore compare this hybrid approach to models that exclusively rely on either approach. As a test bed, we use planar pushing, for which a well-known analytical model and a real-world dataset are available. We consider the following problem: The input consists of a depth image D t of a tabletop scene with one object and the pusher at time t, the starting position p t of the pusher and its movement between this and the next timestep u t = p t+1 −p t. With this information, the models need to predict the object position o t before the push is applied and its movement vot = ot+1 − o t due to the push. This can be divided into two subproblems: Perception: Extract a suitable state representation of the scene at time t (before the push) xt from the input image. This representation should contain the object position ot. fperception(D t) = x t Prediction: Given the state representation xt, the position p t of the pusher and its movement u t:t+1, predict how the object will move: fprediction(x t, p t, u t:t+1) = vot:t+1 In the following sections, we will introduce an analytical model for computing fprediction and the dataset of real robot pushes that we use for training and evaluation. A. An Analytical Model of Planar Pushing We use the analytical model of quasi-static planar pushing that was devised by Lynch et al. [19]. It predicts the object movement vo given the pusher velocity u, the contact point c and associated surface normal n as well as two friction-related parameters l and µ. The model is illustrated in Figure 1, which also contains a list of symbols. Note that this model is still approximate and far from perfectly modelling the stochastic process of planar pushing [26]. Predicting the effect of a push with this model has two stages: First, it determines whether the push is stable (“sticking contact”) or whether the pusher will slide along the object (“sliding contact”). In the ﬁrst case, the velocity of the object at the contact point will be the same as the velocity of the pusher. In the sliding case however, the pusher movement can be almost orthogonal to the resulting motion at the contact point. We call the motion at the contact point “effective push x x o position of the object vo linear and angular object velocity vp linear velocity at the contact point - effective push velocity p position of the pusher u linear pusher velocity - action c contact point (global) c′ contact point relative to o n surface normal at c l ratio between maximal torsional and linear friction force µ friction coefﬁcient pusher-object x x fb left or right boundary force of the friction cone mb torques corresponding to the boundary forces vo,b object velocities resulting from boundary forces vp,b effective push velocities corre- sponding to the boundary forces b = l, r placeholder for left or right boundary s contact indicator, s ∈ [0, 1] k rotation axis Fig. 1: Overview and illustration of the terminology for pushing. velocity” vp. It is the output of the ﬁrst stage. Given vp and the contact point, the second stage then predicts the resulting translation and rotation of the object’s centre of mass. Stage 1: Determining the contact type and computing vp: To determine the contact type (slipping or sticking), we have to ﬁnd the left and right boundary forces fl, fr of the friction cone (i.e. the forces for which the pusher will just not start sliding along the object) and the corresponding torques ml, mr. The opening angle α of the friction cone is deﬁned by the friction coefﬁcient µ between pusher and object. The forces and torques are then computed by α = arctan(µ) (1) fl = R(−α)n fr = R(α)n (2) ml = c ′ xfly − c ′ yflx mr = c ′ xfry − c ′ yfrx (3) where R(α) denotes a rotation matrix given α and c′ = c − o is the contact point relative to the object’s centre of mass. To relate the forces to object velocities, Lynch et al. [19] use an ellipsoidal approximation to the limit surface. To simplify notation, we use subscript b to refer to quantities associated with either the left l or right r boundary forces. vo,b and ωo,b denote linear and angular object velocity, respectively. vp,b are the push velocities that would create the boundary forces. They span the so called ”motion cone”. vo,b = ωo,bl2 mb fb (4) vp,b = ωo,b( l2 mb fb + k × c′) (5) ωo,b acts as a scaling factor. Since we are only interested in the direction of vp,b and not in its magnitude, we set ωo,b = mb: vp,b = l2fb + mbk × c′ (6) To compute the effective push velocity vp, we need to determine the contact case: If the push velocity lies outside of the motion cone, the contact will slip. The resulting effective push velocity then acts in the direction of the boundary velocity vp,b which is closer to the push direction: vp = u · n vp,b · n vp,b (7) Otherwise contact is sticking and we can use the pusher velocity as effective push velocity vp = u. When the norm of n is zero (due to e.g. a wrong prediction of the perception neural network), we set the output vp,b to zero. Stage 2: Using vp to predict the object motion: Given the effective push velocity vp and the contact point c′ relative to the object centre of mass, we can compute the linear and angular velocity vo = [vox, voy, ω] of the object. The object will of course only move if the pusher is in contact with the object. To use the model also in cases where no force acts on the object, we introduce the contact indicator variable s. It takes values between zero and one and is multiplied with vp to switch off responses when there is no contact. We allow s to be continuous instead of binary to give the model a chance to react to the pusher making or breaking contact during the interaction. vox = (l2 + c ′2 x )svpx + c′ xc ′ ysvpy l2 + c′2 x + c′2 y (8) voy = (l2 + c ′2 y )svpy + c′ xc ′ ysvpx l2 + c′2 x + c′2 y (9) ω = c ′ xvoy − c′ yvox l2 (10) Discussion of Underlying Assumptions: The analytical model is built on three simplifying assumptions: (i) quasi- static pushing, i.e. the force applied to the object is big enough to move the object, but not to accelerate it (ii) the pressure distribution of the object on the surface is uniform and the limit-surface of frictional forces can be approximated by an ellipsoid (iii) the friction coefﬁcient between surface and object is constant. The analysis performed by Yu et al. [26] shows that as- sumption (ii) and (iii) are violated frequently by real world data. Assumption (i) holds for push velocities below 50 mm s . In addition, the contact situation may change during pushing (as the pusher may slide along the object and even lose contact), such that the model predictions become increasingly inaccurate the longer ahead it needs to predict in one step. B. Data We use the MIT Push Dataset [26] for our experiments. It contains object pose and force recordings from real robot experiments, where eleven different planar objects are pushed on four different surface materials. For each object-surface Fig. 2: Rendered objects of the Push Dataset [26]: rect1-3, butter, tri1-3, hex, ellip1-3. Red dots indicate the subset of contact points we use to collect a test set with held-out pushes for experiment V-D. combination, the dataset contains about 6000 pushes that vary in the manipulator (“pusher”) velocity and acceleration, the point on the object where the pusher makes contact and the angle between the object surface and the push direction. Pushes are 5 cm long and data was recorded at 250 Hz. As this dataset does not contain RGB or depth images, we render them using OpenGL and the mesh-data supplied with the dataset. In this work, we only use the depth images, RGB will be considered in future work. A rendered scene consists of a ﬂat surface with one of four textures (for the four surface materials), on which one of the objects is placed. The pusher is represented by a vertical cylinder, with no arm attached. Figures 2 and 3 show the different objects and example images. We also annotated the dataset with all information necessary to apply the analytical model to use it as a baseline. The code for annotation and rendering images is available online at https://github.com/mcubelab/pdproc. For each experiment, we construct datasets for training and testing from a subset of the Push Dataset. As the analytical model does not take acceleration of the pusher into account, we only use push variants with zero pusher acceleration. We however do evaluate on data with high pusher velocities, that break the quasi-static assumption made in the analytical model (in Sec. V-E). One data point in our datasets consists of a depth image showing the scene before the push is applied, the object position before and after the push and the initial position and movement of the pusher. The prediction horizon is 0.5 seconds in all datasets 1. More information about the speciﬁc datasets for each experiment can be found in the corresponding sections. We use data from multiple randomly chosen timesteps of each sequence in the Push Dataset. Some of the examples thus contain shorter push-motions than others, as the pusher starts moving with some delay or ends its movement during the 0.5 seconds time-window. To achieve more visual variance and to balance the number of examples per object type, we sample a number of transforms of the scene relative to the camera for each push. Finally, about a third of our dataset consists of examples where we moved the pusher away from the object, such that it is not affected by the push movement. Fig. 3: Rendered RGB and depth images on surfaces plywood and abs. IV. COMBINING NEURAL NETWORKS AND ANALYTICAL MODELS We now introduce the neural network variants that we will analyse in the following section. All architectures share the same ﬁrst network stage that processes raw depth images and outputs a lower-dimensional encoding and the object position. Given this output, the pushing action (movement u and position p) of the pusher and the friction parameters µ and l2, the second part of these networks predicts the linear and angular velocity vo of the object. This predictive part differs between the network variants. While three of them (simple, full, error) use variants of the analytical dynamics model established in Sec. III-A, variant neural has to learn the dynamics with a neural network. The prediction part has about 1.8 million trainable parameters for all variants except for error, which has 2.7 million parameters. We implement all our networks as well as the analytical model in tensorﬂow [1], which allows us to propagate gradi- ents through the analytical models just like any other layer. A. Perception The architecture of the network part that processes the image is depicted in Fig. 4. We assume that the robot knows the position of its end-effector, which allows us to extract a small (80 × 80 pixel) image patch (“glimpse”) around the tip of the pusher. If the pusher is close enough to the object to make contact, the contact point and the normal to the object surface can be estimated from this smaller image. It thus serves as an attention-mechanism to focus the computations on the most relevant part of the image. Only the position of the object needs to be estimated from the full image. Taken together, this is all the information necessary to predict the object movement. The glimpse is processed with three convolutional layers with ReLu non-linearity, each followed by max-pooling and batch normalization [12]. The full image is processed with a sequence of four convolutional and three deconvolution layers, of which the last has only one channel. This output feature map resembles an object segmentation map. We use spatial softmax [17] to get the pixel location of the object centre. Initial experiments showed that not using the glimpse strongly decreased performance for all networks. We also found that using both, the glimpse and an encoding of the full image, for estimating all physical parameters was disadvanta- geous: Using the full image increases the number of trainable parameters in the prediction network but adds no information that is not already contained in the glimpse. B. Prediction Neural Network only (neural): Figure 5 a) shows the prediction part of the variant neural, which uses a neural network to learn the dynamics of pushing. The input to this part is a concatenation of the output from perception with the action and friction parameter l. The network processes this input with three fully-connected layers before predicting the object velocity vo. All intermediate fully-connected layers use ReLu non-linearities. The output layers do not apply a non- linearity. Full analytical model (hybrid) : This variant uses the complete analytical model as described in Section III-A. Sev- eral fully-connected layers extract the necessary input values from the glimpse encoding and the action, as shown in Figure 5 b). These are the contact point c, the surface normal n and the contact indicator s. For predicting s, we use a sigmoidal non-linearity to limit the predicted values to [0, 1]. Simpliﬁed analytical model (simple): Simple (Figure 5 c) only uses the second stage of the analytical model. As for hybrid, a neural network extracts the model inputs (effective push velocity vp, contact point c) from the encoded glimpse and the action. We use this variant as a middle ground between the two other options: It still contains the main mechanics of how an effective push at the contact point moves the object, but leaves it to the neural network to deduce the effective push velocity from the scene and the action. This gives the model more freedom to correct for possible shortcomings of the analytical model. We expect these to manifest mostly in the ﬁrst stage of the model, as small errors can have a big effect there when they inﬂuence whether a contact is estimated as sticking or slipping. The second stage of the analytical model does not specify how the input action relates to the object movement and simple therefore allows us to evaluate the importance of this particular aspect of the analytical model. Full analytical model + error term (error): One concern when using a predeﬁned analytical model is that the trained network cannot improve over the performance of the analytical model. If the analytical model is inaccurate, the hybrid archi- tecture can only compensate to some degree by manipulating the input values of the model, i.e. by predicting “incorrect” values for the components of the state representation. This limits its ability to compensate for model errors as it might not be possible to account for all types of errors in this way. As an alternative, we propose to learn an error-correction term which is added to the output prediction of the analytical model. The error-term is thus not constrained by the model and should be able to compensate for a broader class of model errors. Figure 5 d) shows the architecture. As input for predicting the error-term, we use the same values that neural receives for predicting the object velocity, i.e. the glimpse encoding, the action, the predicted object position and the friction parameter. Note that we do not propagate gradients to the inputs of the error-prediction module. The intuition behind this is that we do not want the error-prediction to interfere with the 240x320 80x80conv1conv2 conv3 conv4conv1conv3conv2Glimpse: Out:Input:deconv1deconv2deconv3 Encoding: Localization:glimpseencodingobject positionspatial softmax Fig. 4: Perception part for all network variants. White boxes represent tensors, arrows represent network layers (green) and dataﬂow (black). prediction of the inputs for the analytical model. We evaluate the effect of this architectural decision in Section V-H. A second variant that we compare to in this section aims to improve the generalizability of the error-prediction to faster push movements. This is achieved by normalizing the input action to unit length before feeding it into the error-prediction module. C. Training For training, we use Adam optimizer [14] with a learning rate of 1−4 and a batch-size of 32 for 75,000 steps. The loss L penalizes the Euclidean distance between the predicted and the real object position in the input image (pos), the Euclidean error of the predicted object translation (trans), the error in the magnitude of translation (mag) and in angular movement (rot) in degree (instead of radian, to ensure that all components of the loss have the same order of magnitude). We use weight decay with λ = 0.001. Let ˆvo and ˆo denote the predicted and vo, o the real object movement and position. w are the network weights and νo = [vox, voy] denotes linear object velocity. L(ˆvo, ˆo, vo, o) = trans + mag + rot + pos + λ ∑ w ∥ w ∥ trans = ∥ˆνo − νo∥ mag = |∥ˆνo∥ − ∥νo|∥ rot = 180 π |ω − ˆω| pos =∥ o − ˆo ∥ When using the variant hybrid, a major challenge is the contact indicator s: In the beginning of training, the direction of the predicted object movement is mostly wrong. s therefore receives a strong negative gradient, causing it to decrease quickly. Since the predicted motion is effectively multiplied by s, a low s results in the other parts of the network receiving small gradients and thus greatly slows down training. We therefore add the error in the magnitude of the predicted velocity to the loss to prevent s from decreasing too far in the early training phase. V. EVALUATING GENERALIZATION In this section, we test our hypothesis that using an ana- lytical model for prediction together with a neural network Out:Input: Neural Network: a) Neural Network only (neural) object velocityfc2 256512fc1fc3 128 action l c) Simpliﬁed analytical model (simple) contact point fc2 256512fc1fc3 128 eﬀective push velocity l glimpse encoding actionAnalytical model, part 2Full analytical model b) Full analytical model (hybrid) contact indicator normalfc2 256512fc1fc3 128 l glimpse encoding action contact pointFull analytical model d) Full analytical model with error-correction (error) contact indicator normalfc2 256512fc1fc3 128 l glimpse encoding action contact pointfc2 128256fc1fc3 64 l error term glimpse encodingconcatconcatconcatconcatconcat Fig. 5: Prediction parts of the four network variants neural, hy- brid, simple and error. White boxes represent tensors, arrows show network layers (green) and dataﬂow (black). The red bar in architecture (d) indicates that no gradients are propagated to the inputs of this layer. for perception improves data efﬁciency and lead to better generalization than using neural networks for both, perception and prediction. We evaluate how the performance of the networks depends on the amount of training data and how well they generalize to (i) pushes with new pushing angles and contact points, (ii) new push velocities and (iii) unseen objects. For the experiments in this section, we use a top-down view of the scene, such that the object can only move in the image plane and the z-coordinate of all scene components remains constant. This simpliﬁes the application of the analytical model by removing the need for an additional transform between the camera and the table. It also simpliﬁes the perception task and allows us to focus this evaluation on the comparison of the hybrid and the purely neural approach. In Section VI we will show how to extend the proposed model to work on more difﬁcult camera settings. A. Baselines We use three baselines in our experiments. All of them use the ground truth input values of the analytical model (action, object position, contact point, surface normal, contact indicator and friction coefﬁcients) instead of depth images, and thus only need to predict the object velocity, but not its initial position. If the pusher makes contact with the object during the push, but is not in contact initially, we use the contact point and normal from when contact is ﬁrst made and shorten the action accordingly. Note that this gives the baseline models a big advantage over architectures that have to infer the input values from raw sensory data. The ﬁrst baseline is just the average translation and rotation over the dataset. This is equal to the error when always predicting zero movement, and we therefore name it zero. The second (physics) is the full analytical model evaluated on the ground truth input values. In addition to the networks described in Section IV, we also train a neural network (neural dyn) on the (ground truth) input values of the analytical model. It uses the same architecture of fully-connected layers for prediction as neural. This allows us to evaluate whether neural beneﬁts from being able to choose its own state representation. B. Metrics For evaluation, we compute the average Euclidean distance between the predicted and the ground truth object translation (trans) and position (pos) in millimetres as well as the average error on object rotation (rot) in degree. As our datasets differ in the overall object movement, we report errors on translation and rotation normalized by the average motion in the corresponding dataset. C. Data efﬁciency The ﬁrst hypothesis we test is that combining the analytical model with a neural network for perception reduces the required training data as compared to a pure neural network. Data: We use a dataset that contains all objects from the MIT Push dataset and all pushes with velocity 20 mm s and split it randomly into training and test set. This results in about 190k training examples and about 38k examples for testing. To evaluate how the networks’ performance develops with the amount of training data, we train the models on different subsets of the training split with sizes from 2500 to the full 190k. We always evaluate on the full test split. To reduce the inﬂuence of dataset composition especially on the small datasets, we average results over multiple different datasets with the same size. Results: Figure 6 shows how the errors in predicted trans- lation, rotation and object position develop with more training data and Table I contains numeric values for training on the biggest and smallest training split. As expected, the combined approach of neural network and analytical model (hybrid and error) already performs very well on the smallest dataset (2500 examples) and beats the other models including the neural dyn baseline, which uses the ground truth state representation, by a large margin. It takes more than 20k training examples for the other models to reach the performance of hybrid, where predicting rotation seems to be harder to learn than translation. Despite of having to rely on raw depth images instead of the ground truth state representation, all models perform at least close to the physics baseline when using the full training set. However, only the pure neural network and the hybrid model with error-correction are able to improve on the baseline. This shows that the analytical model limits hybrid in ﬁtting the training data perfectly, since the model itself is not perfect and does not allow for overﬁtting to noise in the training data. Neural and error have more freedom for ﬁtting the training distribution, which however also increases the risk of overﬁtting. Combining the learned error-correction with the ﬁxed ana- lytical model is especially helpful for predicting the translation of the object. To also improve the prediction of rotations, the model needs more than 20k training examples, which is similar to neural. While neural makes a larger improvement on the full dataset, error combines the comparably good performance of hybrid on few training examples with the ability to improve on the model given enough data. The variant simple, which uses only the second part of the analytical model, also combines learning and a ﬁxed model for predicting the dynamics. But in contrast to error, this variant seems to combine the disadvantages of both approaches: It needs much more training data than hybrid but is still limited by the performance of the analytical model and gets quickly outperformed by the pure neural network when more data is available. The comparison of neural and the baseline neural dyn shows that despite of having access to the ground truth data, neural dyn actually performs worse than neural on the full dataset. This seems to agree with the theory of Agrawal et al. [2], that training perception and prediction end-to-end and letting the network chose its own state representation instead of forcing it to use a predeﬁned state may be beneﬁcial for neural learning. D. Generalization to new pushing angles and contact points The previous experiment showed the performance of the different models when testing on a dataset with a very similar distribution to the training set. Here, we evaluate the perfor- mance of the networks on held-out push conﬁgurations that were not part of the training data. Note that while the test set contains combinations of object pose and push action that the networks have not encountered during training, the pushing actions or object poses themselves do not lie outside of the TABLE I: Error in predicted translation (trans) and rotation (rot) as percentage of the average movement (standard errors in brackets). pos denotes the error in predicted object position. Values shown are for training on the full training set (190k examples) and on a 2500 examples subset. trans rot pos [mm]2.5k neural 33.6 (0.18) % 62.54 (0.42) % 0.46 (0.002) simple 32.3 (0.19) % 53.6 (0.37) % 0.44 (0.002) hybrid 25.4 (0.17) % 45.5 (0.36) % 0.46 (0.002) error 24.7 (0.16) % 46.8 (0.36) % 0.45 (0.002) neural dyn 32.6 (0.19) % 63.5 (0.46) % -190kneural 17.4 (0.12) % 33.4 (0.28) % 0.31 (0.002) simple 19.3 (0.13) % 35.7 (0.3) % 0.33 (0.002) hybrid 19.3 (0.13) % 36.1 (0.3) % 0.32 (0.002) error 18.4 (0.12) % 34.6 (0.29) % 0.31 (0.002) neural dyn 19.2 (0.12) % 36.3 (0.29) % - physics 18.95 (0.13) % 35.4 (0.3) % - zero 2.95 (0.02) mm 1.9 (0.01) ◦ - training data value range. This experiment thus test the model’s interpolation abilities. Data: We again train the networks on a dataset that contains all objects and pushes with velocity 20 mm s . For constructing the test set, we collect all pushes with (i) pushing angles ±20◦ and 0◦ to the surface normal (independent from the contact points) and (ii) at a set of contact points illustrated in Figure 2 (independent from the pushing angle). The remaining pushes are split randomly into a training and a validation set, which we use to monitor the training process. There are about 114k data points in the training split, 23k in the validation split and 91k in the test set. Results: As Table II shows, hybrid and error perform best for predicting the object velocity for pushes that were not part of the training set. Although still being close, none of the networks can outperform the physics baseline on this test set. Note that the difﬁculty of the test set in this experiment differs from the one in the previous experiment, as can be seen from the different performance of the physics baseline: Due to the central contact points and small pushing angles, the test set contains a high proportion of pushes with sticking contact, for which the movement of the object is similar to the movement of the pusher. This makes it hard to compare the results between Table I and Table II in terms of absolute values. With more than 100k training examples, we supply enough data for the pure neural model to clearly outperform the com- bined approach and the baseline in the previous experiment (i.e. when the test set is similar to the training set,see Figure 6). The fact that neural now performs worse than hybrid and physics indicates that its advantage over the physics baseline may not come from it learning a more accurate dynamics model. Instead, it probably memorizes speciﬁc input-output combinations that the analytical model cannot predict well, for example due to noisy object pose data. The same is supposedly also true for error, which cannot improve on hybrid for new pushes and even performs a little worse for predicting rotations when evaluated on the testset. 2.5 5 7.510 1520 50 100 190 15 20 25 30 35 thousand training examplestrans[%] 2.5 5 7.510 1520 50 100 190 30 40 50 60 thousand training examplesrot[%] neural simple hybrid error neural dyn physics 2.5 5 7.510 1520 50 100 190 0.3 0.4 0.5 thousand training examplespos[mm] Fig. 6: Prediction errors versus training set size (x-axis in logarithmic scale). Errors on translation and rotation are given as percentage of the average movement in the test set. The model-based architecture hybrid performs much better than the other networks when training data is sparse. TABLE II: Prediction errors for testing on pushes with pushing angles and contact points not seen during training. trans rot pos [mm] neural 16.5 (0.06) % 36.1 (0.17) % 0.31 (0.001) simple 16.4 (0.06) % 37.1 (0.18) % 0.31 (0.001) hybrid 15.6 (0.07) % 35.3 (0.19) % 0.31 (0.001) error 15.6 (0.07) % 34.5 (0.18) % 0.32 (0.001) neural dyn 18.1 (0.07) % 44.1 (0.2) % - physics 14.6 (0.06) % 32.8 (0.18) % - zero 4.36 (0.013) mm 2.27 (0.009) ◦ - The difference in performance is however much smaller than for neural. In contrast to hybrid and error, simple again does not seem to proﬁt from the simpliﬁed analytical model for generalization and performs similar to neural. If we supply fewer training data, the difference between hybrid and the other networks is again much more pronounced: Hybrid achieves 20.3 % translation and 43.8 % rotation error whereas neural lies at 38.7 % and 63.4 % respectively. E. Generalization to Different Push Velocities In this experiment, we test how well the networks generalize to unseen push velocities. In contrast to the previous experi- ment, the test actions in this experiment have a different value range than the actions in the training data, and we are thus looking at extrapolation. As neural networks are usually not good at extrapolating beyond their training domain, we expect the model-based network variants to generalize better to push- velocities not seen during training. Data: We use the networks that were trained in the ﬁrst experiment (V-C) on the full (190k) training set. The push velocity in the training set is thus 20 mm s . We evaluate on datasets with different push velocities ranging from 10 mm s to 150 mm s . Results: Results are shown in Figure 7. Since the input action does not inﬂuence perception of the object position, we only report the errors on the predicted object motion. On higher velocities, we see a very large difference between the performance of our combined approach and the pure neural 0 20 40 60 80trans[%] 10 20 50 75 100 150 20 40 60 80 100 push velocity [ mm s ]rot[%] physics neural simple hybrid error neural dyn Fig. 7: Errors on predicted translation and rotation for testing on different push velocities. All models were trained on push velocity 20 mm s . While hybrid stays close to the physics baseline, the other models have trouble extrapolating. network. Neural’s predictions quickly become very inaccurate, with the error on predicted rotation rising to more than 88 % of the error when predicting zero movement always. The performance of hybrid on the other hand is most constant over the different push velocities and declines only slightly more than the physics baseline. A reason for the decreasing performance of physics on higher velocities is that the quasi-static assumption is violated: For pushes faster than 50 mm s , the object gets accelerated and can continue sliding even after contact to the pusher was lost. Simple, neural, error and neural-dyn all get worse with increasing velocity, but simple degrades much less when predicting rotations. The reason for this is that all four archi- tectures struggle mostly with predicting the correct magnitude of the object translation and not so much with predicting the translation’s direction. By using the second stage of the analytical model, simple has information about how the direction of the object translation relates to its rotation, which results in much more accurate predictions. The advantage of hybrid for extrapolation lies in the ﬁrst stage of the analytical model, which allows it to scale its predictions according to the magnitude of the action. This is in essence a multiplication operation. However, a general multiplication of inputs cannot be expressed using only fully- connected layers (as used by simple, neural, neural dyn and the error-prediction part of error) because fully-connected layers essentially perform weighted additions of their inputs. So instead of learning the underlying function, the networks are forced to resort to memorizing input-output relations for the magnitude of the object motion, which explains why extrapolation does not work well. When combining the prediction of the analytical model with a learned error-term, the resulting model thus suffers from the same issues as the other network-based variants. The decline is however less pronounced than for neural, especially for predicting translations and only starts after 50 mm s . A possible reason for this is that the error-correction term is rather small compared to the output of the analytical model. This means that the weights with which the action enters the computation of the error term are smaller than for neural, simple or neural dyn. In V-H, we show that the error-prediction can also be made more robust to higher velocities by normalizing the push action before using it as input to the error-prediction. F. Generalization to Different Objects This experiment tests how well the networks generalize to unseen object shapes and how many different objects the networks have to see during training to generalize well. Data: We train the networks on three different datasets: With one object (butter), two objects (butter and hex) and three objects (butter, hex and one of the ellipses or triangles). The datasets with fewer objects contain more augmented data, such that the total number of data points is about 35k training examples in each. As test sets, we use one dataset containing the three ellipses and one containing all triangles. While this is fewer training data than in the previous experiments, it should be sufﬁcient for the pure neural network to perform as well as hybrid, since the test sets contain only few objects. Results: The results in Figure 8 show that neural is consis- tently worse than the other networks when predicting rotations. It also improves most notably when one example of the test objects is in the training set. The differences between the models are less pronounce when predicting translation, except for simple which performs particularly bad on triangles. The different models do not differ much when predicting position, which is not surprising, since they share the same perception architecture. The architecture with added error-term does not perform very different from hybrid, which implies that the error-correction term does not depend much on the shape of the object. In general, all models perform surprisingly well on ellipses, even if the models only had access to data from the butter object. Reaching the baseline performance on triangles is however only possible with a triangle in the training set. Predicting the object’s position is most sensitive to the shapes seen during training: It generalizes well to ellipses which have similar shape and size as the butter or hex object. The triangles on the other hand are very different from the other objects in the dataset and the error for localizing triangles is by factor ten higher than for ellipses. G. Visualizations As a qualitative evaluation, we plot the predictions of our networks, the physics baseline and the ground truth object motion for 200 repetitions of the same push conﬁguration. All pushes have the same pushing angle (0 ◦), velocity (20 mm s ) and contact point, but we sample a different transformation of the whole scene for each push, such that the object’s pose in the image varies between the pushes. The networks were trained on the full dataset from Experiment V-C. The results shown in Figure 9 illustrate that the ground truth object motion for the same push varies greatly with two distinct modes. By comparing with the prediction of the analytical model we can estimate how much of this variance is due to slight changes in the push conﬁguration (which also reﬂect in the analytical model) and how much is caused by other, non-deterministic effects. The predictions of hybrid and the analytical model are very similar. This shows that the state-representation that the neural network part of hybrid predicts is mostly accurate. This is conﬁrmed in Figure 10 where we plot the estimated contact points and normals. Adding an error-correction term to the hybrid architecture improves the average estimation quality a little, but also increases the variance of the predictions. The other models, too, make good predictions in this example, but especially simple and neural dyn have more variance in the direction of the predicted translation.. Figure 10 also shows that simple is not very accurate in predicting the contact points. It is however possible that the error in the contact point prediction is compensated for by vp, which in simple is not predicted by the analytical model but estimated by the neural network. It is also interesting to see that neural, neural-dyn and simple all make very similar predictions that slightly overestimate the rotation. H. Evaluation of the error-architecture The previous results have shown that adding a learned error-correction term to the output of the analytical model in the hybrid architecture enables the network to improve over the performance of the analytical model. The error model we analysed is able to outperform hybrid and the physics baseline if the training set and the test set are similar (see Experiment V-C). As explained in Section IV, we chose to block the prop- agation of gradients from the error-correction module to the glimpse-encoding, because we did not want the error- computation to interfere with the prediction of the state representation. Here, we also evaluate an architecture err-grad that does not block the gradient propagation. This architecture trained ontested on20 40trans[%] 20 40 60rot[%] 0 0.5pos[mm] 20 40trans[%] 20 40 60rot[%] 0 5pos[mm] neural simple hybrid error physics Fig. 8: Prediction errors in translation, rotation and position on objects not seen during training. Training objects are shown on the x-axis. The top row shows results for evaluating on ellipses, the bottom row on triangles. All networks generalize well to ellipses, but are worse for triangles, where the error in predicted position is by factor ten higher than for the other objects. Neural particularly struggles with predicting rotations of previously unseen objects. TABLE III: Evaluation of different architectures for predicting an error-correction term. In contrast to error, error-grad allows the propagation of gradients from the error-prediction module to the glimpse encoding. Error-norm instead normalizes the push action to unit length before using it as input to the error- prediction. Values shown are for training on the full training set (190k examples). Results for hybrid and neural are repeated for reference. trans rot pos [mm] neural 17.4 (0.12) % 33.4 (0.28) % 0.31 (0.002) hybrid 19.3 (0.13) % 36.1 (0.3) % 0.32 (0.002) error 18.4 (0.12) % 34.6 (0.29) % 0.31 (0.002) error-grad 17.9 (0.12) % 34.4 (0.29) % 0.29 (0.002) error-norm 18.3 (0.12) % 35.3 (0.29) % 0.31 (0.002) physics 18.95 (0.13) % 35.4 (0.3) % - zero 2.95 (0.02) mm 1.9 (0.01) ◦ - manages to beat hybrid by an even bigger margin, as shown in Table III. The downside of propagating the gradients becomes appar- ent if we look at generalization to new pushing velocities: While the predictions of error become worse with increasing velocity, they still remain more accurate than the predictions of neural, as illustrated in Figure 11. Error-grad on the other hand performs even worse than the pure neural network. A reason for this difference could be that error-grad relies more strongly on the error-correction term than error. This allows it to ﬁt the training data more closely but at the same time impedes generalization to unseen actions. As explained before, the reason for the decline in perfor- mance when extrapolating is that the neural networks cannot scale their predictions correctly according to the input velocity. One possibility to make the error-prediction more robust to higher input velocities is the architecture we call error-norm. In this model, we scale the push action to unit length before using it as input to the error-prediction. This makes the error-prediction independent of the magnitude of the action, while still giving it information about the push direction. The resulting model performs only slightly worse than error inside the training domain, but much better for extrapolation. It is still worse than hybrid though, since it cannot properly scale the error term to match higher velocities. Using the error-correction term of course becomes much more interesting if the analytical model is bad. To test how well the hybrid and error architecture can compensate for wrong models, we manipulate the friction parameter l by setting it to 1.5 or 3 times its real value. The results are shown in Table IV. Manipulating the friction values is especially harmful for predicting the rotation of the object, and both hybrid and error perform much better than the physics baseline with the wrong friction parameter. The visualization in Figure 12 shows that both models predicted incorrect contact points to counter the effect of the higher friction value. This makes sense, since the location of the contact point inﬂuences the tradeoff between how much the object rotates and how much it translates. The predictions from error deviate farther from the ground truth values, which shows that the additional error-term does not prevent the model from manipulating the input values to the analytical model. Instead, it achieves its good results by combining both forms of correction. VI. EXTENSION TO NON-TRIVIAL VIEWPOINTS In the previous part, we used depth images that showed a top-down view of the scene. This simpliﬁed the perception part and allowed us to focus our experiments on comparing the different architectures for learning the dynamics model. In this Section, we brieﬂy describe what changes when we move away from the top-down perspective and show that the proposed hybrid approach still works well on scenes that were recorded from an arbitrary viewpoint. (a) Ground truth (b) Physics [19] (c) Hybrid (d) Simple (e) Neural (f) Neural-dyn (g) Error Fig. 9: Qualitative evaluation on 200 repeated pushes with the same push conﬁguration (angle, velocity, contact point). The green rectangles show the (predicted) pose of the object after the push and the blue lines illustrate the object’s translation (for better visibility, we upscaled the lines by factor 5). The thicker orange rectangle is the average ground truth position of the object after the push. Red crosses indicate the predicted initial object positions. All models predict the movement of the object and its initial position well, but cannot capture the multimodal distribution of the ground truth data. (a) Contact points pre- dicted by simple (b) Contact points pre- dicted by hybrid (c) Contact points pre- dicted by error (d) Contact normal pre- dicted by hybrid (e) Contact normal pre- dicted by error Fig. 10: Predicted contact points and normals from 200 repeated pushes with the same push conﬁguration (angle, velocity, contact point). The black point marks the (average) ground truth contact point. While hybrid and error make fairly accurate predictions, simple predicts the contact points not on the edge of the object but close to its center. Inspired by Byravan and Fox [5], we extend the input data from depth images to full 3d point clouds. These point clouds are still image-shaped and can thus be treated like normal images whose channels encode coordinate values instead of colour or intensity. A. Challenges For describing the scene geometrically, we need three coordinate frames, the world frame, the camera frame and a frame that is attached to the object. In our rendered scenes, the origin of the world frame is located at the centre of the table and its x − y plane is aligned with the table surface. The object frame is attached to the centre of mass of the object. In planar pushing, we expect that the object can only move and rotate in the x − y plane of its supporting surface and the x − y plane of object and world frame are thus aligned. The movement of the object is described relative to the world frame. The camera-frame is located at the sensor position and its z-axis points towards the origin of the world frame. Depth images contain the distance of each visible point to the sensor along the z axis, i.e. the z coordinates of said point in camera frame. Finally, a 3D point (x y z) T in camera coordinates is mapped to a pixel (u v) T by applying the perspective 20 40 60trans[%] 10 20 50 75 100 150 20 40 60 80 100 push velocity [ mm s ]rot[%] physics neural hybrid error error-grad error-norm Fig. 11: Evaluation of the different architectures for predicting an error-correction term on unseen push velocities. All models were trained on push velocity 20 mm s . None of the error- prediction models is as robust as hybrid to higher input velocities. Error-norm performs best because its predicted error terms are independent from the push velocity. Error- grad presumably relies more on the error-prediction term than the other architectures and therefore performs worst outside of the training domain. TABLE IV: Prediction errors of physics, hybrid and error when using a manipulated friction parameter l. In contrast to physics, both neural networks can compensate for the resulting error of the analytical model. Hybrid can however only modify the input values to the analytical model, while error can correct the model’s output directly and thus compensates the error of the analytical model much better. trans rot pos [mm] 1.5 · l hybrid 20.7 (0.13) % 40.5 (0.32) % 0.3 (0.002) error 19.2 (0.14) % 35.9 (0.3) % 0.25 (0.002) physics 23.9 (0.15) % 46.1 (0.37) % - 3 · l hybrid 25.1 (0.15) % 66.9 (0.45) % 0.31 (0.002) error 19.6 (0.13) % 37.2 (0.3) % 0.32 (0.002) physics 35.6 (0.23) % 80.1 (0.53) % - projection with focal length f : (u v ) = f z ( x y ) (11) In the special case of a top-down view, the x − y-plane of world and camera frame are aligned, such that we can apply the analytical model presented in Section III-A in both frames. In addition, the measured depth (z in camera frame) of the table and the object are independent of their x and y position. This reduces the perspective projection to a scaling operation with a constant factor and allowed us to predict the object movement directly in pixel space. It also made segmenting the object very easy, since it is associated with one speciﬁc depth value. Without the assumption of a top-down view, predicting object movement in pixel-space becomes more challenging, since the same movement will span more pixels the further away the object is from the camera. In addition, perspective distortion now affects the shape and size of the objects in pixel space, which is e.g. relevant for localizing the object. B. Network architecture We now describe the changes we made to the architectures from Section V to adapt to new camera conﬁgurations. To be able to relate pixel coordinates to 3d coordinates, we assume that we have access to the parameters of the camera (focal length) and the transform between camera frame and world frame. 1) Perception: The perception part remains largely as it was before. As our architecture estimates the position of the object in pixel space (using spatial softmax), we use the given camera parameters and transform to calculate the corresponding position in 3d world-coordinates. For this we need the z coordinate that corresponds to the predicted pixel coordinates, which we get by interpolating between the values of the four pixels closest to the predicted coordinates. Due to inaccuracies in the depth-values and the projection matrix, this operation introduces an error of about 0.3 mm, which we however consider negligible. 2) Prediction and Training: If we do not use a top-down view of the scene, the question becomes relevant in which coordinate-frame the network should predict the contact point and the normal: pixel space, camera coordinates or world coordinates. We decided to continue using pixel coordinates since predictions in this space can be most directly related to the input image and the predicted feature maps. After prediction, we however have to transfer the predicted contact point and normal from pixel-space to world-coordinates, since the analytical model can only be used in the world frame where the movement of the object falls exclusively onto the x − y plane. We found training the model on data from arbitrary view- points more challenging than in the top-down scenario. To facilitate the process, we modiﬁed the network and the training loss to treat contact prediction and velocity prediction sepa- rately: Instead of using the predicted contact indicator in the analytical model, the network now predicts the contact indi- cator s as a separate output. We interpret s as the probability that the pusher is in contact with the object and place a cross- entropy loss on it. For the predicted velocity, we only penalize errors if there was (ground truth) contact. This prevents non- informative gradients to the velocity-prediction if the object did not move and also solves the problem that the contact- prediction tries to compensate for wrong velocity predictions. At test-time, the predicted contact indicator is multiplied with the predicted object movement from the analytical model to get the ﬁnal velocity prediction. The new loss for a single training example looks as follows: Let ˆs, ˆvo and ˆo denote the predicted and s, vo, o the real contact indicator, object movement and position. w are the (a) Physics (b) Hybrid (c) Error (d) Contact points predicted by hy- brid (e) Contact points predicted by er- ror (f) Contact normal predicted by hybrid (g) Contact normal predicted by error Fig. 12: Predicted movement, contact points and normals from 200 repeated pushes when using a wrong friction parameter (1.5 · l). The black point marks the (average) ground truth contact point. Both networks compensate for the wrong friction parameter by predicting the contact point in a slightly wrong position, but the deviation from the ground truth is stronger for error, which also ﬂipps the direction of the predicted normal. network weights and νo = [vox, voy] denotes linear object velocity. L(s, ˆvo, ˆo, s, vo, o) = trans + rot + pos + . . . . . . λ2 contact + λ1 ∑ w ∥ w ∥ trans = s ∥ˆνo − νo∥ rot = s 180 π |ω − ˆω| pos =∥ o − ˆo ∥ contact = −(s log(ˆs) + (1 − s) log(1 − ˆs)) To ensure that all components of the loss are of the same magnitude, we compute translation and position error in millimetres and rotation in degree. We set λ1 = 10 and λ3 = 0.001. C. Evaluation To show that our approach is not dependent on the top- down perspective, we train and evaluate it on images collected from a more natural viewpoint: The camera is located at(0 − 0.25 0.4 ) in world-coordinates, which corresponds to a more natural perspective where the viewer stands in front of the table instead of hovering above the centre. Figure 13 shows the depth channel of three example images. Data: As in Section V, we use a dataset that contains all objects from the MIT Push dataset and all pushes with velocity 20 mm s for evaluating on this new viewpoint. We split it randomly into about 190k training examples and about 37k examples for testing. Results: After 75k training steps, the hybrid model performs very similar to the top-down case: 19.4 % translation error and 37.2 % error in rotation, as compared to 19.3 % and 36.1 % in the top-down case. The only notable difference is that the error in predicted position climbs from 0.32 mm to 1.12 mm. This can be partially explained by the inaccuracy introduced when converting from pixel coordinates to world coordinates. It can also be an effect of the perspective distortion, that inﬂuences shape and size of the objects depending on how close they are to the camera. In relation to the size of the object, an error of around 1 mm is however still very small. VII. CONCLUSION AND FUTURE WORK In this paper, we considered the problem of predicting the effect of physical interaction from raw sensory data. We compared a pure neural network approach to a hybrid approach that uses a neural network for perception and an analytical model for prediction. Our test bed involved pushing of planar objects on a surface - a non-linear, discontinuous manipulation task for which we have both, millions of data points and an analytical model. We observed two main advantages of the hybrid architec- ture. Compared to the pure neural network, it signiﬁcantly (i) reduces required training data and (ii) improves gener- alization to novel physical interaction. The analytical model Fig. 13: Example depth images recorded from a non-top-down viewpoint. The depth values increase towards the back of the scene and the perspective transform affects the shape of the objects. aides generalization by limiting the ability of the hybrid architecture to (over-)ﬁt the training data and by providing multiplication operations for scaling the output according to the input action. This kind of mathematical operation is hard to learn for fully-connected architectures and requires many parameters and training examples for covering a large value range. The drawback of the hybrid approach is that it cannot easily improve on the performance of the underlying analytical model. Neural on the other hand can beat both, the hybrid approach and the analytical model (with ground truth input values) if trained on enough data. This however only holds when we evaluate on actions encountered during training and does not transfer to new push conﬁgurations, velocities or object shapes. The challenge in these cases is that the distribution of the training and test data differ signiﬁcantly. To enable the hybrid approach to improve more on the prediction accuracy of its analytical model, we experimented with learning an additional additive error-correction term. These models are almost as data-efﬁcient as hybrid and can to some extend retain the ability to generalize to different test data provided by the analytical model. Our experiments with a wrong analytical model showed that they can compensate for errors of the model much better than hybrid, which can only inﬂuence the prediction by manipulating the input values of the analytical model. The last architecture, simple, showed that combining learn- ing and analytical models is not automatically guaranteed to lead to good performance. By replacing the ﬁrst stage of the analytical model with a neural network, we instead combined the disadvantages of both approaches: The architecture needs lots of training data and does not generalize well to new pushes, because it misses the part of the analytical model that explains the inﬂuence of the pushing action on the resulting object velocity. In contrast to the pure neural network, it however also cannot improve on the performance of the analytical model. A limitation of the presented hybrid approach is that it may be hard to ﬁnd an accurate analytical model for some physical processes and that not all existing models are suitable for our approach, as they e.g. need to be differentiable everywhere. Especially the switching dynamics encountered when the contact situation changes proved to be challenging and more work needs to be done in this direction. In such cases, learning the predictive model with a neural network is still a very good option. In perception on the other hand, the strengths of neural networks can be well exploited to extract the input parameters of the analytical model from raw sensory data. By training end-to-end through a given model, we can avoid the effort of labelling data with the ground truth state. Our experiments also showed that training end-to-end allows the hybrid models to compensate for smaller errors in the analytical model by adjusting the predicted input values. Using the state representation of the analytical model for the hybrid architecture also the advantage that the predictions of the network can be visualized and interpreted. This is not easily possible for the intermediate representations learned in the pure neural network. Our results however suggest that the pure neural network beneﬁts from being free to chose its own state representation, as learning the dynamics model from the ground truth state representation lead to worse prediction results. In the future, we want to extend our work to more complex perception problems, like training on RGB images or scenes with multiple objects. An interesting question is if perception on point clouds could be facilitated by using methods like pointnet++ [? ] that are speciﬁcally designed for this type of input data instead of treating the point cloud like a normal image. A logical next step is also using our hybrid models for predicting more than one step into the future. Working on sequences instead makes it for example possible to guide learning by enforcing constraints temporal consistency or to exploit temporal cues like optical ﬂow. The models could also be used in a ﬁltering scenario to track the state of an object and at the same time infer latent variables of the system like the friction coefﬁcients. Similarly, we can also use the learned models to plan and execute robot actions using model predictive control. ACKNOWLEDGMENTS This research was supported in part by National Science Foundation grants IIS-1205249, IIS-1017134, EECS-0926052, the Ofﬁce of Naval Research, the Okawa Foundation, German Research Foundation and the Max-Planck-Society. Any opin- ions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the funding organizations. NOTES 1We also evaluated two different prediction horizons but found no signif- icant effect on the performance. 2We provide these inputs as friction related information cannot be obtained from single images. Estimation from sequences is considered future work. REFERENCES [1] Mart´ın Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org. [2] Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Advances in Neural Inform. Process. Syst., pages 5074–5082, 2016. [3] M. Bauza and A. Rodriguez. A probabilistic data- driven model for planar pushing. In 2017 IEEE Interna- tional Conference on Robotics and Automation (ICRA), pages 3008–3015, May 2017. doi: 10.1109/ICRA.2017. 7989345. [4] Dominik Belter, Marek Kopicki, Sebastian Zurek, and Jeremy Wyatt. Kinematically optimised predictions of object motion. In Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pages 4422–4427. IEEE, 2014. [5] Arunkumar Byravan and Dieter Fox. Se3-nets: Learning rigid body motion using deep neural networks. In Robotics and Automation (ICRA), 2017 IEEE Int. Conf. on, pages 173–180. IEEE, 2017. [6] Arunkumar Byravan, Felix Leeb, Franziska Meier, and Dieter Fox. Se3-pose-nets: Structured deep dynamics models for visuomotor planning and control. to appear at Robotics and Automation (ICRA), 2018 IEEE Int. Conf. on, abs/1710.00489, 2017. [7] Jonas Degrave, Michiel Hermans, Joni Dambre, and Francis Wyffels. A differentiable physics engine for deep learning in robotics. CoRR, abs/1611.01652, 2016. [8] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsu- pervised learning for physical interaction through video prediction. In Advances in Neural Inform. Process. Syst. 29, pages 64–72. 2016. [9] Suresh Goyal, Andy Ruina, and Jim Papadopoulos. Pla- nar sliding with dry friction part 1. limit surface and moment function. Wear, 143(2):307 – 330, 1991. ISSN 0043-1648. doi: https://doi.org/10.1016/0043-1648(91) 90104-3. [10] Soo Hong Lee and Mark Cutkosky. Fixture planning with friction. Journal of Engi- neering for Industry, 113, 08 1991. URL http://manufacturingscience.asmedigitalcollection. asme.org/article.aspx?articleid=1447458. [11] Robert D. Howe and Mark R. Cutkosky. Practical force- motion models for sliding manipulation. The Inter- national Journal of Robotics Research, 15(6):557–572, 1996. doi: 10.1177/027836499601500603. [12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. 32nd Int. Conf. on Machine Learning, volume 37, pages 448–456, 07–09 Jul 2015. [13] Rico Jonschkowski and Oliver Brock. Learning state representations with robotic priors. Autonomous Robots, 39(3):407–428, Oct 2015. ISSN 1573-7527. doi: 10.1007/s10514-015-9459-7. [14] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [15] Alina Kloss, Stefan Schaal, and Jeannette Bohg. Com- bining learned and analytical models for predicting action effects. CoRR, abs/1710.04102, 2018. URL http://arxiv. org/abs/1710.04102. Submitted to RSS’18. [16] Marek Kopicki, Sebastian Zurek, Rustam Stolkin, Thomas Moerwald, and Jeremy L. Wyatt. Learning modular and transferable forward models of the motions of push manipulated objects. Autonomous Robots, 41(5): 1061–1082, 6 2017. doi: 10.1007/s10514-016-9571-3. [17] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. J. Mach. Learning Research, 17(39):1–40, 2016. [18] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforce- ment learning. CoRR, abs/1509.02971, 2015. [19] K. M. Lynch, H. Maekawa, and K. Tanie. Manipulation and active sensing by pushing using tactile feedback. In Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems, volume 1, pages 416–421, Jul 1992. doi: 10.1109/IROS.1992.587370. [20] Georg Martius and Christoph H. Lampert. Extrapolation and learning equations. CoRR, abs/1610.02995, 2016. URL http://arxiv.org/abs/1610.02995. [21] Matthew T Mason. Mechanics and planning of manip- ulator pushing operations. The International Journal of Robotics Research, 5(3):53–71, 1986. [22] Tekin Meric¸li, Manuela Veloso, and H Levent Akın. Push-manipulation of complex passive mobile objects us- ing experimentally acquired motion models. Autonomous Robots, 38(3):317–329, 2015. [23] Duy Nguyen-Tuong and Jan Peters. Using model knowl- edge for learning inverse dynamics. In Robotics and Automation (ICRA), 2010 IEEE Int. Conf. on, pages 2677–2682. IEEE, 2010. [24] N. Watters, A. Tacchetti, T. Weber, R. Pascanu, P. Battaglia, and D. Zoran. Visual Interaction Networks. ArXiv e-prints, June 2017. [25] Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and Josh Tenenbaum. Learning to see physics via visual de-animation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Sys- tems 30, pages 152–163. Curran Associates, Inc., 2017. [26] K. T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez. More than a million ways to be pushed. a high-ﬁdelity exper- imental dataset of planar pushing. In 2016 IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS), pages 30–37, Oct 2016. doi: 10.1109/IROS.2016.7758091. [27] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Ben- jamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. [28] L. Zhang and J. C. Trinkle. The application of particle ﬁltering to grasping acquisition with visual occlusion and tactile sensing. In 2012 IEEE Int. Conf. on Robotics and Automation, pages 3805–3812, May 2012. [29] Jiaji Zhou, Robert Paolini, J Andrew Bagnell, and Matthew T Mason. A convex polynomial force-motion model for planar sliding: Identiﬁcation and application. In Robotics and Automation (ICRA), 2016 IEEE Inter- national Conference on, pages 372–377. IEEE, 2016.","libVersion":"0.3.2","langs":""}
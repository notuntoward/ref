{"path":"lit/sources/papers_added/papers/Su20cloudMotionDpLrn.pdf","text":"atmosphere Article Prediction of Short-Time Cloud Motion Using a Deep-Learning Model Xinyue Su 1,*, Tiejian Li 1,2 , Chenge An 1 and Guangqian Wang 1 1 State Key Laboratory of Hydroscience and Engineering, Tsinghua University, Beijing 100084, China; litiejian@tsinghua.edu.cn (T.L.); anchenge08@163.com (C.A.); dhhwgq@mail.tsinghua.edu.cn (G.W.) 2 State Key Laboratory of Plateau Ecology and Agriculture, Qinghai University, Xining 810016, China * Correspondence: suxy16@mails.tsinghua.edu.cn Received: 24 August 2020; Accepted: 22 October 2020; Published: 26 October 2020 \u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\u0001 \u0001\u0002\u0003\u0004\u0005\u0006\u0007 Abstract: A cloud image can provide signiï¬cant information, such as precipitation and solar irradiation. Predicting short-time cloud motion from images is the primary means of making intra-hour irradiation forecasts for solar-energy production and is also important for precipitation forecasts. However, it is very challenging to predict cloud motion (especially nonlinear motion) accurately. Traditional methods of cloud-motion prediction are based on block matching and the linear extrapolation of cloud features; they largely ignore nonstationary processes, such as inversion and deformation, and the boundary conditions of the prediction region. In this paper, the prediction of cloud motion is regarded as a spatiotemporal sequence-forecasting problem, for which an end-to-end deep-learning model is established; both the input and output are spatiotemporal sequences. The model is based on gated recurrent unit (GRU)- recurrent convolutional network (RCN), a variant of the gated recurrent unit (GRU), which has convolutional structures to deal with spatiotemporal features. We further introduce surrounding context into the prediction task. We apply our proposed Multi-GRU-RCN model to FengYun-2G satellite infrared data and compare the results to those of the state-of-the-art method of cloud-motion prediction, the variational optical ï¬‚ow (VOF) method, and two well-known deep-learning models, namely, the convolutional long short-term memory (ConvLSTM) and GRU. The Multi-GRU-RCN model predicts intra-hour cloud motion better than the other methods, with the largest peak signal-to-noise ratio and structural similarity index. The results prove the applicability of the GRU-RCN method for solving the spatiotemporal data prediction problem and indicate the advantages of our model for further applications. Keywords: cloud motion prediction; deep learning; gated recurrent unit; convolutional long short-term memory; satellite cloud image 1. Introduction Recently, cloud-motion prediction has received signiï¬cant attention because of its importance for the prediction of both precipitation and solar-energy availability [1]. Research has shown that the prediction of the short-time motion of clouds, especially of convective clouds, is important for precipitation forecasts [2â€“7]. Since most models of solar variability [8,9] and of solar irradiation [10â€“12] require cloud motion velocity as the main input, accurate cloud motion estimation is also essential for the intra-hour forecast of solar energy [13â€“16]. The diï¬€erence between weather forecasts and solar forecasts is that the latter are usually conducted in a shorter time window (less than one hour). Otherwise, cloud-motion prediction is essentially similar in these two ï¬elds. Because the temperature of clouds is lower than that of the ground, clouds can be identiï¬ed from infrared (IR) satellite images (with wavelengths of 10.5 to 12.5 Âµm) in which the intensity of IR radiation is correlated with Atmosphere 2020, 11, 1151; doi:10.3390/atmos11111151 www.mdpi.com/journal/atmosphere Atmosphere 2020, 11, 1151 2 of 17 temperature [1,17]. Therefore, cloud motion can be estimated from a given sequence of IR images for weather forecasting [18] or intra-hour solar forecasting. Nevertheless, cloud motion is a complex phenomenon involving nonrigid motion and nonlinear events [19], and predicting it remains challenging. Several methods have been proposed for the prediction of cloud motion; most of them are correspondence-based approaches. In general, cloud motion vectors (CMVs) are obtained by ï¬rst locating salient image features, such as brightness gradients, corners, cloud edges, or brightness temperature gradients [20,21], and subsequently tracking these features in successive images with the assumption that they do not change signiï¬cantly over a short interval. CMVs can be obtained from data collected by sky-imaging devices, such as whole-sky imagers (WSIs) [22], or by satellites. CMVs derived from WSI data are used for short-term forecasts (less than 20 min) in the local spatial area [11], whereas CMVs obtained from satellite images are commonly utilized to ï¬nd the global atmospheric motion and the climate status of a large area [21,23]. Adopting a similar concept to CMVs, Brad and Letia [19] developed a model combining a block matching algorithm (BMA) and a best candidate block search, along with vector median regularization, to estimate cloud motion. This method divides successive images into blocks, restricting the candidate list of blocks to a predeï¬ned number, while in the full search BMA, the best match is found between the two blocks of successive frames in a full domain. Based on the idea of block matching, Jamaly and Kleissl [24] applied the cross-correlation method (CCM) and cross-spectral analysis (CSA) as matching criteria on cloud motion estimation. Additional quality-control measures, including removing conditions with low variability and less-correlated sites, can help to ensure that CSA and CCM reliably estimate cloud motion. Nevertheless, CCM can lead to relatively large errors because the assumption of uniform cloud motion does not hold in the presence of cloud deformation, topographically induced wind-speed variations, or a changing optical perspective [25]. This is a common problem for other block matching methods as well. One approach to overcoming the challenges brought by variations in cloud motion is to compute the CMV of every pixel. Chow et al. [26] proposed a variational optical ï¬‚ow (VOF) technique to determine the subpixel accuracy of cloud motion for every pixel. They focused on cloud motion detection, and did not extend their work to prediction. Shakya and Kumar [27] applied a fractional-order optical-ï¬‚ow method to cloud-motion estimation and used extrapolations based on advection and anisotropic diï¬€usion to make predictions. However, their method is not an end-to-end method of cloud-motion prediction. Since the CMV is computed by extracting and tracking features, ameliorating feature extraction is another approach to improving performance. The deep convolutional neural network (CNN) [28] has proved able to extract and utilize image features eï¬€ectively; it has achieved great success in visual recognition tasks, such as the ImageNet classiï¬cation challenge [29]. Methods based on deep CNN have been introduced to cloud classiï¬cation [30,31], cloud detection [32], and satellite video processing [33] in recent years. Although deep CNN has performed excellently when dealing with spatial data, it discards temporal information [34] that provides important clues in the forecasting of cloud motion. A prominent class of deep neural network called recurrent neural network (RNN) could learn complex and compound relationships in the time domain. However, the simple RNN model lacks the ability to backpropagate the error signal through a long-range temporal learning. Long short-term memory (LSTM) [35] was proposed to tackle this problem and this model is widely used in the solar power forecasting ï¬eld [36,37]. Recent deep-learning models trained on videos have been used successfully for captioning and for encoding motion. Ji et al. [38] formulated a video as a set of images and directly applied deep CNN on the frames. Zha et al. [39] extended deep 2-D CNN to deep 3-D CNN and performed a convolutional operation on both the spatial and the temporal dimensions. Donahue et al. [40] combined convolutional networks with LSTM and proposed long-term recurrent convolutional network (LRCN). LRCN ï¬rst processes the inputs with CNN and then feds the outputs of CNN into stacked LSTM. This method created a precedent on a combination of CNN and RNN regarded as recurrent convolutional network (RCN). Unlike previous proposals that Atmosphere 2020, 11, 1151 3 of 17 focused on high-level deep CNN â€œvisual perceptsâ€, the novel convolutional long short-term memory (ConvLSTM) network proposed by Shi et al. [41] has convolutional structures in both the input-to-state and state-to-state transitions to extract â€œvisual perceptsâ€ for precipitation now-casting. Ballas et al. [42] extended this work and proposed a variant form of the gated recurrent unit (GRU). They captured spatial information using an RNN with convolutional operation and empirically validated their GRU-RCN model on a video classiï¬cation task. GRU-RCN has fewer parameters than ConvLSTM. Since both the input and output of a cloud-motion forecast are spatiotemporal sequences, cloud-motion prediction is a spatiotemporal-sequence forecast problem for which GRU-RCN would seem well suited. However, Ballas et al. [42] focused on video classiï¬cation, which is quite diï¬€erent from our forecast problem. Given the input video data, the output of their model is a number that depends on the class of the video; in our problem, the output should have a spatial domain as well. We need to modify the structure of the GRU-RCN model and apply it directly on the pixel level. Moreover, there exists another challenge in the cloud motion prediction problem: new clouds often appear suddenly, at the boundary. To overcome this challenge, our model includes information about the surrounding context in which each small portion of the cloud is embedded; this was not considered in previous methods. In this paper, we suggest the use of deep-learning methods to capture nonstationary information regarding cloud motion and deal with nonrigid processes. We propose a multiscale-input end-to-end model with a GRU-RCN layer. The model takes the surrounding context into account, achieves precise localization, and extracts information from multiple scales of resolution. Using a database of FenYun-2G IR satellite images, we compare our modelâ€™s intra-hour predictions to those of the state-of-the-art variational optical-ï¬‚ow (VOF) method and three deep learning models (ConvLSTM, LSTM, and GRU); our model performs better than the other methods. The remainder of this paper is organized as follows: Section 2 introduces the GRU-RCN model. Section 3 describes the data we used and the experiments we conducted. Section 4 presents the results, as well as brieï¬‚y describes the other methods with which the GRU-RCN model was compared. Section 5 discusses the advantages and disadvantages of our model and our plans for future work. Section 6 provides our concluding remarks. 2. Methodology 2.1. Deep CNN Deep CNNs [28] have been proven to extract and utilize image features eï¬€ectively and have achieved great success in visual recognition tasks. Regular neural networks do not scale well to full images because, in the case of large images, the number of model parameters increases drastically, leading to low eï¬ƒciency and rapid overï¬tting. The deep-CNN architecture avoids this drawback. The deep CNN contains a sequence of layers, typically a convolutional layer, a pooling layer, and a fully connected layer. In a deep CNN, the neurons in a given layer are not connected to all the neurons in the preceding layer but only to those in a kernel-size region of it. This architecture provides a certain amount of shift and distortion invariance. 2.2. GRU A GRU [43] is a type of RNN. An RNN is implemented to process sequential data; it deï¬nes a recurrent hidden state, the activation of which depends on the previous state. Given a variable-length sequence X = (x1, x2, . . . xt), the hidden state ht of the RNN at each time step t is updated by: ht = Ï•(htâˆ’1, xt), (1) where Ï• is a nonlinear activation function. Atmosphere 2020, 11, 1151 4 of 17 An RNN can be trained to learn the probability distribution of sequences and thus to predict the next element in the sequence. At each time step t, the output can be represented as a distribution of probability. However, because of the vanishing-gradient and exploding-gradient problems, training an RNN becomes diï¬ƒcult when input/output sequences span long intervals [44]. Variant RNNs with complex activation functions, such as LSTMs and GRUs, have been proposed to overcome this problem. LSTMs and GRUs both perform well on machine-translation and video-captioning tasks, but a GRU has a simpler structure and lower memory requirement [45]. A GRU compels each recurrent unit to capture the dependencies of diï¬€erent timescales adaptively. The GRU model is deï¬ned by the following equations: zt = Ïƒ(Wzxt + Uzhtâˆ’1), (2) rt = Ïƒ(Wrxt + Urhtâˆ’1), (3) Ìƒht = tanh(Wxt + U(rt âŠ™ htâˆ’1)), (4) ht = (1 âˆ’ zt)htâˆ’1 + ztÌƒht, (5) where âŠ™ denotes element-wise multiplication; W, Wz, Wr and U, Uz, Ur are weight matrices; xt is current input; htâˆ’1 is the previous hidden state; zt is an update gate; rt is a reset gate; Ïƒ is the sigmoid function; Ìƒht is a candidate activation, which is computed similarly to that of the traditional recurrent unit in an RNN; and ht is the hidden state at time step t. The update gate determines the extent to which the hidden state is updated when the unit updates its contents, and the reset gate determines whether the previous hidden state is preserved. More speciï¬cally, when the value of the reset gate of a unit is close to zero, the information from the previous hidden state is discarded and the update is based exclusively on the current input of the sequence. By such a mechanism, the model can eï¬€ectively ignore irrelevant information for future states. When the value of the reset gate is close to one, on the other hand, the unit remembers long-term information. 2.3. GRU-RCN In this section, we will introduce the GRU-RCN layer utilized in our model. A GRU converts input into a hidden state by fully connected units, but this can lead to an excessive number of parameters. In cloud imaging, the inputs of satellite images are 3-D tensors formed from the spatial dimensions and input channels. We regard the inputs as a sequence X = (x1, x2, . . . xt); the size of the hidden state should be the same as that of the input. Let H, Wid, and C be the height, width, and number of the channels of input at every time step, respectively. If we apply GRU on inputs directly, the size of both the weight matrix W and the weight matrix U should be H Ã— Wid Ã— C. Images are composed of patterns with strong local correlation that are repeated at diï¬€erent spatial locations. Moreover, satellite images vary smoothly over time: the position of a tracked cloud in successive images will be restricted to a local spatial neighborhood. Ballas et al. [42] embedded convolution operations into the GRU architecture and proposed the GRU-RCN model. In this way, recurrent units have sparse connectivity and can share their parameters across diï¬€erent input spatial locations. The structure of the GRU-RCN model is expressed in the following equations: z l t = Ïƒ (Wl z âˆ— xl t + Ul z âˆ— hl tâˆ’1), (6) r l t = Ïƒ (Wl r âˆ— xl t + Ul r âˆ— h l tâˆ’1), (7) Ìƒh l t = tan h(Wl âˆ— x l t + Ul âˆ— (rl t âŠ™ hl tâˆ’1)), (8) h l t = (1 âˆ’ zl t)h l tâˆ’1 + z l t Ìƒh l t, (9) Atmosphere 2020, 11, 1151 5 of 17 where âˆ— denotes convolution and the superscript l denotes the layer of the GRU-RCN; the weight matrices Wl, Wl z, Wl r and Ul, Ul z, Ul r are 2-D convolutional kernels; and hl t = hl t(i, j), where hl t(i, j) is a feature vector deï¬ned at the location (i, j). With convolution, the sizes of Wl, Wl z, Wl r and Ul, Ul z, Ul r are all K1 Ã— K2 Ã— C, where K1 Ã— K2 is the convolutional-kernel spatial size (chosen in this paper to be 3 Ã— 3), signiï¬cantly lower than that of the input frame H Ã— Wid. Furthermore, this method preserves spatial information, and we use zero padding in the convolution operation to ensure that the spatial size of the hidden state remains constant over time. The candidate hidden representation Ìƒhl t(i, j), the activation gate zl t(i, j), and the reset gate rl t(i, j) are deï¬ned based on a local neighborhood of size K1 Ã— K2 at the location (i, j) in both the input data xl t and the previous hidden state hl tâˆ’1. In addition, the size of the receptive ï¬eld associated with hl t(i, j) increases with every previous timestep hl tâˆ’1, hl tâˆ’2 . . . as we go back in time. The model implemented in this paper is, therefore, capable of characterizing the spatiotemporal pattern of cloud motion with high spatial variation over time. 2.4. Multi-GRU-RCN Model In this section, we will introduce the model structure of Multi-GRU-RCN. Ballas et al. [42] focused on the problem of video classiï¬cation and therefore implemented a VGG16 model structure in their paper. However, this does not ï¬t our problem well: we need to operate on the pixel level directly. The model structure of Shi et al. [41] consists of an encoding network as well as a forecasting network, and both networks are formed by stacking several ConvLSTM layers. In their model, there is a single input, and the input and output data have the same dimension. We modiï¬ed this model structure and proposed a new one, which can extract information from the surrounding context. The model structure is presented in Figure 1. AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  5Â  ofÂ  16Â  bothÂ theÂ inputÂ dataÂ  ğ‘¥ Â  andÂ theÂ previousÂ hiddenÂ stateÂ  â„ .Â InÂ addition,Â theÂ sizeÂ ofÂ theÂ receptiveÂ fieldÂ  associatedÂ  withÂ  â„ ğ‘–, ğ‘—Â  increasesÂ  withÂ  everyÂ  previousÂ  timestepÂ  â„ , â„ ...Â  asÂ  weÂ  goÂ  backÂ  inÂ  time.Â  TheÂ  modelÂ  implementedÂ  inÂ  thisÂ  paperÂ  is,Â  therefore,Â  capableÂ  ofÂ  characterizingÂ  theÂ  spatiotemporalÂ  patternÂ ofÂ cloudÂ motionÂ withÂ highÂ spatialÂ variationÂ overÂ time.Â  2.4.Â Multiâ€GRUâ€RCNÂ ModelÂ  InÂ  thisÂ  section,Â  weÂ  willÂ  introduceÂ  theÂ  modelÂ  structureÂ  ofÂ  Multiâ€GRUâ€RCN.Â  BallasÂ  etÂ  al.Â  [42]Â  focusedÂ onÂ theÂ problemÂ ofÂ videoÂ classificationÂ andÂ thereforeÂ implementedÂ aÂ VGG16Â modelÂ structureÂ  inÂ theirÂ paper.Â However,Â thisÂ doesÂ notÂ fitÂ ourÂ problemÂ well:Â weÂ needÂ toÂ operateÂ onÂ theÂ pixelÂ levelÂ  directly.Â TheÂ modelÂ structureÂ ofÂ ShiÂ etÂ al.Â [41]Â consistsÂ ofÂ anÂ encodingÂ networkÂ asÂ wellÂ asÂ aÂ forecastingÂ  network,Â andÂ bothÂ networksÂ areÂ formedÂ byÂ stackingÂ severalÂ ConvLSTMÂ layers.Â InÂ theirÂ model,Â thereÂ  isÂ aÂ singleÂ input,Â andÂ theÂ inputÂ andÂ outputÂ dataÂ haveÂ theÂ sameÂ dimension.Â WeÂ modifiedÂ thisÂ modelÂ  structureÂ andÂ proposedÂ aÂ newÂ one,Â whichÂ canÂ extractÂ informationÂ fromÂ theÂ surroundingÂ context.Â TheÂ  modelÂ structureÂ isÂ presentedÂ inÂ FigureÂ 1.Â  Â  FigureÂ  1.Â  OutlineÂ  ofÂ  theÂ  multiâ€gatedÂ  recurrentÂ  unitÂ  (GRU)â€recurrentÂ  convolutionalÂ  networkÂ  (RCN)Â  model.Â  ThereÂ areÂ multipleÂ inputsÂ inÂ thisÂ model,Â andÂ theÂ inputÂ fromÂ eachÂ smallÂ regionÂ hasÂ theÂ sameÂ centerÂ  asÂ theÂ inputÂ fromÂ aÂ largerÂ region.Â TheÂ inputÂ fromÂ theÂ smallÂ regionÂ hasÂ theÂ sameÂ dimensionÂ asÂ theÂ  output,Â whileÂ theÂ inputÂ fromÂ theÂ largeÂ regionÂ hasÂ fourÂ timesÂ asÂ muchÂ area.Â WeÂ considerÂ theÂ regionÂ  thatÂ isÂ includedÂ inÂ theÂ largeÂ regionÂ butÂ excludedÂ inÂ theÂ smallÂ regionÂ asÂ theÂ surroundingÂ context.Â TheÂ  purposeÂ  ofÂ  utilizingÂ  multipleÂ  inputsÂ  fromÂ  differentÂ  regionsÂ  isÂ  toÂ  enrichÂ  theÂ  informationÂ  withÂ  theÂ  surroundingÂ context.Â LikeÂ theÂ modelÂ ofÂ ShiÂ etÂ al.Â [41],Â ourÂ modelÂ consistsÂ ofÂ anÂ encodingÂ partÂ andÂ aÂ  forecastingÂ part.Â InÂ additionÂ toÂ stackedÂ GRUâ€RCNÂ layers,Â batchÂ normalizationÂ [46]Â wasÂ introducedÂ  intoÂ bothÂ theÂ encodingÂ andÂ forecastingÂ partsÂ toÂ accelerateÂ theÂ trainingÂ processÂ andÂ avoidÂ overfitting.Â  WhenÂ utilizingÂ inputÂ fromÂ aÂ largeÂ region,Â weÂ usedÂ aÂ maxÂ poolingÂ layerÂ toÂ reduceÂ theÂ dimensionÂ andÂ  improveÂ theÂ abilityÂ ofÂ theÂ modelÂ toÂ captureÂ invarianceÂ informationÂ ofÂ theÂ objectÂ inÂ theÂ image.Â TheÂ  initialÂ  statesÂ  ofÂ  theÂ  forecastingÂ  partÂ  areÂ  copiedÂ  fromÂ  theÂ  finalÂ  stateÂ  ofÂ  theÂ  encodingÂ  part.Â  WeÂ  concatenateÂ theÂ outputÂ statesÂ ofÂ theÂ twoÂ inputsÂ andÂ subsequentlyÂ feedÂ thisÂ intoÂ aÂ 1Â Ã—Â 1Â convolutionalÂ  layerÂ withÂ ReLUÂ activationÂ toÂ obtainÂ theÂ finalÂ predictionÂ results.Â  3.Â ExperimentÂ andÂ DataÂ  3.1.Â ExperimentalÂ SetupÂ  InÂ thisÂ paper,Â weÂ usedÂ aÂ setÂ ofÂ satelliteÂ dataÂ fromÂ 2018.Â ForÂ computationalÂ convenience,Â weÂ firstÂ  normalizedÂ theÂ dataÂ toÂ theÂ rangeÂ 0Â toÂ 1.Â WeÂ randomlyÂ selectedÂ dataÂ fromÂ 200Â daysÂ asÂ trainingÂ data,Â  dataÂ fromÂ 20Â daysÂ asÂ validationÂ data,Â andÂ dataÂ fromÂ anotherÂ 20Â daysÂ asÂ testÂ data.Â BecauseÂ eachÂ imageÂ  wasÂ tooÂ largeÂ (512Â Ã—Â 512Â pixels)Â forÂ training,Â weÂ dividedÂ itÂ intoÂ smallÂ patches,Â andÂ setÂ theÂ patchÂ sizesÂ  atÂ 64Â Ã—Â 64Â pixelsÂ andÂ 128Â Ã—Â 128Â pixels.Â EveryÂ 64Â Ã—Â 64Â patchÂ wasÂ pairedÂ withÂ aÂ 128Â Ã—Â 128Â patchÂ (theÂ 64Â Ã—Â  64Â patchÂ beingÂ inÂ theÂ centerÂ regionÂ ofÂ theÂ 128Â Ã—Â 128Â patch).Â Thus,Â eachÂ 512Â Ã—Â 512Â frameÂ wasÂ dividedÂ  intoÂ 16Â  pairsÂ ofÂ  patches.Â  TheÂ  patchesÂ  wereÂ  atÂ theÂ  sameÂ  locationÂ butÂ acrossÂ  consecutiveÂ timeÂ  steps.Â  BecauseÂ theÂ averageÂ velocityÂ ofÂ cloudÂ motionÂ inÂ theÂ trainingÂ datasetÂ isÂ 14.35Â m/sÂ accordingÂ toÂ theÂ FYâ€ Figure 1. Outline of the multi-gated recurrent unit (GRU)-recurrent convolutional network (RCN) model. There are multiple inputs in this model, and the input from each small region has the same center as the input from a larger region. The input from the small region has the same dimension as the output, while the input from the large region has four times as much area. We consider the region that is included in the large region but excluded in the small region as the surrounding context. The purpose of utilizing multiple inputs from diï¬€erent regions is to enrich the information with the surrounding context. Like the model of Shi et al. [41], our model consists of an encoding part and a forecasting part. In addition to stacked GRU-RCN layers, batch normalization [46] was introduced into both the encoding and forecasting parts to accelerate the training process and avoid overï¬tting. When utilizing input from a large region, we used a max pooling layer to reduce the dimension and improve the ability of the model to capture invariance information of the object in the image. The initial states of the forecasting part are copied from the ï¬nal state of the encoding part. We concatenate the output states of the two inputs and subsequently feed this into a 1 Ã— 1 convolutional layer with ReLU activation to obtain the ï¬nal prediction results. Atmosphere 2020, 11, 1151 6 of 17 3. Experiment and Data 3.1. Experimental Setup In this paper, we used a set of satellite data from 2018. For computational convenience, we ï¬rst normalized the data to the range 0 to 1. We randomly selected data from 200 days as training data, data from 20 days as validation data, and data from another 20 days as test data. Because each image was too large (512 Ã— 512 pixels) for training, we divided it into small patches, and set the patch sizes at 64 Ã— 64 pixels and 128 Ã— 128 pixels. Every 64 Ã— 64 patch was paired with a 128 Ã— 128 patch (the 64 Ã— 64 patch being in the center region of the 128 Ã— 128 patch). Thus, each 512 Ã— 512 frame was divided into 16 pairs of patches. The patches were at the same location but across consecutive time steps. Because the average velocity of cloud motion in the training dataset is 14.35 m/s according to the FY-2G Atmospheric Motion Vector data, about 81.05% of the pixels in each patch can be tracked in the next time stepâ€™s patch at the same location. The data instances were partitioned into nonoverlapping sequences, each of which is n frames long. For each sequence, we used the ï¬rst n âˆ’ 1 frames as input to predict the last frame. For example, the ï¬rst sequence implements frames 1 to n âˆ’ 1 to predict frame n, and the second sequence implements frames n + 1 to 2n âˆ’ 1 to predict frame 2n. To select the exact value of n, we designed a pretraining process. For each value of n between 2 and 10, we set the batch size to 32 and randomly selected 100 batches from the training dataset. We trained the model using these batches of data and computed the average mean squared error (MSE) among all batches and the running time. The results are presented in Figure 2. AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  6Â  ofÂ  16Â  2GÂ AtmosphericÂ MotionÂ VectorÂ data,Â aboutÂ 81.05%Â ofÂ theÂ pixelsÂ inÂ eachÂ patchÂ canÂ beÂ trackedÂ inÂ theÂ  nextÂ timeÂ stepâ€™sÂ patchÂ atÂ theÂ sameÂ location.Â  TheÂ dataÂ instancesÂ wereÂ partitionedÂ intoÂ nonoverlappingÂ sequences,Â eachÂ ofÂ whichÂ isÂ nÂ framesÂ  long.Â ForÂ eachÂ sequence,Â weÂ usedÂ theÂ firstÂ nâ€1Â framesÂ asÂ inputÂ toÂ predictÂ theÂ lastÂ frame.Â ForÂ example,Â  theÂ  firstÂ  sequenceÂ  implementsÂ  framesÂ  1Â  toÂ  ğ‘›1 Â  toÂ  predictÂ  frameÂ  ğ‘› ,Â  andÂ  theÂ  secondÂ  sequenceÂ  implementsÂ framesÂ  ğ‘›1Â  toÂ  2ğ‘›1Â  toÂ predictÂ frameÂ  2ğ‘›.Â ToÂ selectÂ theÂ exactÂ valueÂ ofÂ  ğ‘›,Â weÂ designedÂ  aÂ pretrainingÂ process.Â ForÂ eachÂ valueÂ ofÂ  ğ‘›Â  betweenÂ 2Â andÂ 10,Â weÂ setÂ theÂ batchÂ sizeÂ toÂ 32Â andÂ randomlyÂ  selectedÂ 100Â batchesÂ fromÂ theÂ trainingÂ dataset.Â WeÂ trainedÂ theÂ modelÂ usingÂ theseÂ batchesÂ ofÂ dataÂ andÂ  computedÂ theÂ averageÂ meanÂ squaredÂ errorÂ (MSE)Â amongÂ allÂ batchesÂ andÂ theÂ runningÂ time.Â TheÂ resultsÂ  areÂ presentedÂ inÂ FigureÂ 2.Â  Â  FigureÂ 2.Â ResultsÂ ofÂ averageÂ meanÂ squaredÂ errorÂ (MSE),Â minimumÂ MSE,Â andÂ runningÂ timeÂ utilizingÂ  differentÂ valuesÂ forÂ theÂ frameÂ numberÂ  ğ‘›.Â  AsÂ  seenÂ  inÂ  FigureÂ  2,Â  theÂ  runningÂ  timeÂ  increasesÂ  almostÂ  linearlyÂ  withÂ  theÂ  increaseÂ  inÂ  ğ‘›.Â  TheÂ  averageÂ MSEÂ evidentlyÂ fallsÂ asÂ  ğ‘›Â  increasesÂ fromÂ 2Â toÂ 6Â butÂ thereafterÂ remainsÂ almostÂ constant.Â TheÂ  minimumÂ MSEÂ (i.e.,Â theÂ minimumÂ batchÂ MSEÂ amongÂ allÂ batches),Â however,Â isÂ notÂ veryÂ sensitiveÂ toÂ  theÂ valueÂ ofÂ  ğ‘›.Â ThisÂ indicatesÂ thatÂ whenÂ  ğ‘›Â  isÂ largerÂ thanÂ 6,Â theÂ runningÂ timeÂ increasesÂ asÂ  ğ‘›Â  increases,Â  butÂ  thisÂ  doesÂ  notÂ  leadÂ  toÂ  aÂ  reductionÂ  inÂ  theÂ  MSE.Â  Therefore,Â  theÂ  valueÂ  ğ‘› 6Â  wasÂ  chosenÂ  forÂ  theÂ  experimentsÂ reportedÂ inÂ thisÂ paper.Â TheÂ satelliteÂ collectedÂ dataÂ everyÂ hour;Â therefore,Â thereÂ wereÂ 12,800Â  casesÂ inÂ theÂ trainingÂ dataset,Â 1280Â casesÂ inÂ theÂ validationÂ dataset,Â andÂ 1280Â casesÂ inÂ theÂ testÂ dataset.Â  TheÂ outlineÂ ofÂ theÂ GRUâ€RCNÂ layerÂ isÂ demonstratedÂ inÂ FigureÂ 3.Â TheÂ outputÂ ofÂ theÂ encoderÂ willÂ  beÂ  implementedÂ  inÂ  theÂ  decoderÂ  forÂ  theÂ  productionÂ  ofÂ  theÂ  finalÂ  output.Â  WeÂ  trainedÂ  theÂ  GRUâ€RCNÂ  modelÂ byÂ minimizingÂ theÂ MSEÂ lossÂ usingÂ backpropagationÂ throughÂ timeÂ (BPTT)Â withÂ aÂ learningÂ rateÂ  ofÂ 10âˆ’3.Â TheÂ kernelÂ sizeÂ ofÂ theÂ convolutionalÂ structureÂ inÂ ourÂ GRUâ€RCNÂ layerÂ wasÂ setÂ toÂ 3Â Ã—Â 3.Â  Figure 2. Results of average mean squared error (MSE), minimum MSE, and running time utilizing diï¬€erent values for the frame number n. As seen in Figure 2, the running time increases almost linearly with the increase in n. The average MSE evidently falls as n increases from 2 to 6 but thereafter remains almost constant. The minimum MSE (i.e., the minimum batch MSE among all batches), however, is not very sensitive to the value of n. This indicates that when n is larger than 6, the running time increases as n increases, but this does not lead to a reduction in the MSE. Therefore, the value n = 6 was chosen for the experiments reported in this paper. The satellite collected data every hour; therefore, there were 12,800 cases in the training dataset, 1280 cases in the validation dataset, and 1280 cases in the test dataset. Atmosphere 2020, 11, 1151 7 of 17 The outline of the GRU-RCN layer is demonstrated in Figure 3. The output of the encoder will be implemented in the decoder for the production of the ï¬nal output. We trained the GRU-RCN model by minimizing the MSE loss using backpropagation through time (BPTT) with a learning rate of 10âˆ’3. The kernel size of the convolutional structure in our GRU-RCN layer was set to 3 Ã— 3. AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  7Â  ofÂ  16Â  Â  FigureÂ  3.Â  OutlineÂ  ofÂ  theÂ  gatedÂ  recurrentÂ  unitÂ  (GRU)â€recurrentÂ  convolutionalÂ  networkÂ  (RCN)Â  layerÂ  appliedÂ  toÂ  cloudâ€imageÂ  predictionÂ  overÂ  time.Â  CloudÂ  imagesÂ  wereÂ  obtainedÂ  fromÂ  theÂ  FYâ€2GÂ  IR1Â  database.Â  3.2.Â TestÂ BenchmarkÂ  TheÂ performanceÂ ofÂ theÂ proposedÂ methodÂ wasÂ determinedÂ byÂ twoÂ metrics:Â theÂ peakÂ signalâ€toâ€ noiseÂ ratioÂ (PSNR)Â [47]Â andÂ structuralÂ similarityÂ (SSIM)Â indexÂ [48].Â PSNRÂ isÂ aÂ widelyÂ usedÂ metricÂ forÂ  evaluatingÂ theÂ accuracyÂ ofÂ algorithms.Â ThisÂ metricÂ indicatesÂ theÂ reconstructionÂ qualityÂ ofÂ theÂ methodÂ  used.Â  TheÂ  observedÂ  valueÂ  atÂ  theÂ  predictionÂ  timeÂ  stepÂ  isÂ  notÂ  ofÂ  practicalÂ  relevance.Â  InformationÂ  regardingÂ futureÂ eventsÂ isÂ notÂ involvedÂ inÂ theÂ generationÂ ofÂ theÂ forecastÂ satelliteÂ image;Â however,Â itÂ  stillÂ servesÂ asÂ aÂ usefulÂ benchmark.Â TheÂ signalÂ hereÂ wasÂ takenÂ toÂ beÂ theÂ observedÂ value,Â whereasÂ theÂ  noiseÂ wasÂ theÂ errorÂ ofÂ theÂ forecastÂ image.Â TheÂ PSNRÂ betweenÂ theÂ observedÂ imageÂ  ğ‘“Â  andÂ predictedÂ  imageÂ  ğ‘”Â  wasÂ definedÂ asÂ  ğ‘ƒğ‘†ğ‘ğ‘…ğ‘“, ğ‘” 10 ğ‘™ğ‘œğ‘”ğ¼/ğ‘€ğ‘†ğ¸ğ‘“, ğ‘”,Â  (10)Â  whereÂ  ğ¼ 2 1 Â  isÂ  theÂ  maximumÂ  pixelÂ  intensity.Â  ğ‘€ğ‘†ğ¸ğ‘“, ğ‘”Â  isÂ  theÂ  meanÂ  squaredÂ  errorÂ  betweenÂ theÂ observedÂ andÂ predictedÂ image,Â definedÂ as:Â  ğ‘€ğ‘†ğ¸ğ‘“, ğ‘” âˆ‘ğ‘“ ğ‘” ,Â  (11)Â  whereÂ  ğ‘Â  isÂ theÂ numberÂ ofÂ pixelsÂ inÂ theÂ satelliteÂ image.Â  ForÂ aÂ smallerÂ MSE,Â theÂ PSNRÂ willÂ beÂ larger,Â and,Â therefore,Â theÂ algorithmÂ accuracyÂ willÂ beÂ higher.Â  SSIMÂ isÂ aÂ qualityÂ assessmentÂ methodÂ usedÂ toÂ measureÂ theÂ similarityÂ betweenÂ twoÂ images.Â ItÂ wasÂ  proposedÂ  underÂ  theÂ  assumptionÂ  thatÂ  theÂ  qualityÂ  perceptionÂ  ofÂ  theÂ  humanÂ  visualÂ  systemÂ  (HVS)Â  isÂ  correlatedÂ  withÂ  structuralÂ  informationÂ  ofÂ  theÂ  scene.Â  Therefore,Â  itÂ  considersÂ  imageÂ  degradationsÂ  asÂ  perceivedÂ  changesÂ  inÂ  structuralÂ  information,Â  whileÂ  PSNRÂ  estimatesÂ  imageÂ  degradationsÂ  basedÂ  onÂ  errorÂ  sensitivity.Â  TheÂ  structuralÂ  informationÂ  isÂ  decomposedÂ  intoÂ  threeÂ  components:Â  luminance,Â  contrast,Â andÂ structure.Â TheÂ SSIMÂ betweenÂ  ğ‘“Â  andÂ  ğ‘”Â  isÂ definedÂ as:Â  ğ‘†ğ‘†ğ¼ğ‘€ğ‘“, ğ‘” ğ‘™ğ‘“, ğ‘”ğ‘ğ‘“, ğ‘”ğ‘ ğ‘“, ğ‘”,Â  (12)Â  Figure 3. Outline of the gated recurrent unit (GRU)-recurrent convolutional network (RCN) layer applied to cloud-image prediction over time. Cloud images were obtained from the FY-2G IR1 database. 3.2. Test Benchmark The performance of the proposed method was determined by two metrics: the peak signal-to-noise ratio (PSNR) [47] and structural similarity (SSIM) index [48]. PSNR is a widely used metric for evaluating the accuracy of algorithms. This metric indicates the reconstruction quality of the method used. The observed value at the prediction time step is not of practical relevance. Information regarding future events is not involved in the generation of the forecast satellite image; however, it still serves as a useful benchmark. The signal here was taken to be the observed value, whereas the noise was the error of the forecast image. The PSNR between the observed image f and predicted image g was deï¬ned as PSNR( f , g) = 10 log10(Imax2/MSE( f , g)), (10) where Imax = 28 âˆ’ 1 is the maximum pixel intensity. MSE( f , g) is the mean squared error between the observed and predicted image, deï¬ned as: MSE( f , g) = 1 N Nâˆ‘ i=1( fi âˆ’ gi)2, (11) where N is the number of pixels in the satellite image. For a smaller MSE, the PSNR will be larger, and, therefore, the algorithm accuracy will be higher. Atmosphere 2020, 11, 1151 8 of 17 SSIM is a quality assessment method used to measure the similarity between two images. It was proposed under the assumption that the quality perception of the human visual system (HVS) is correlated with structural information of the scene. Therefore, it considers image degradations as perceived changes in structural information, while PSNR estimates image degradations based on error sensitivity. The structural information is decomposed into three components: luminance, contrast, and structure. The SSIM between f and g is deï¬ned as: SSIM( f , g) = l( f , g)c( f , g)s( f , g), (12) where l( f , g), c( f , g), and s( f , g) are the luminance comparison, contrast comparison, and structure comparison between f and g, respectively: l( f , g) = 2Âµ f Âµg + c1 Âµ f 2 + Âµg2 + c1 , (13) c( f , g) = 2Ïƒ f Ïƒg + c2 Ïƒ f 2 + Ïƒg2 + c1 , (14) s( f , g) = Ïƒ f g + c3 Ïƒ f Ïƒg + c3 , (15) where Âµ f and Âµg are the averages of f and g, Ïƒx are the variances of f and g, Ïƒ f g is the covariance of f and g, and c1, c2, and c3 are positive constants to stabilize the division with a zero denominator. Besides, we also considered the mean bias error (MBE) as a supplementary metric. Although the value of MBE could not indicate the model reliability because the errors often compensate each other, it could show the degree to which the method underestimates or overestimates the results. With the purpose of exhibiting the degree more intuitively, the MBE was calculated as a percentage and the MBE between f and g was deï¬ned as MBE( f , g) = 1 N Nâˆ‘ i=1 gi âˆ’ fi fi Ã— 100%. (16) 3.3. Data FY-2, the ï¬rst Chinese geostationary meteorological satellite, was launched on 31 December 2014, and positioned above the equator at 105â—¦ E. With the Stretched Visible and Infrared Spin Scan Radiometer (S-VISSR) on board, FY-2 can observe the Earthâ€™s atmosphere with high temporal and spatial resolutions. The IR1 channel (10.3~11.3 Âµm) China land-area images are obtained hourly for the spatial range 50â—¦ E~160â—¦ E, 4â—¦ N~65â—¦ N. The size of each image is 512 Ã— 512 pixels, and the spatial resolution of each pixel is 13 km in both the north-south and east-west directions. The intensity of the pixels is 0~255. The relationship between the intensity count and brightness temperature is negative but not linear. An image instance of FY-2 is depicted in Figure 4. Atmosphere 2020, 11, 1151 9 of 17AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  8Â  ofÂ  16Â  whereÂ  ğ‘™ğ‘“, ğ‘”,Â  ğ‘ğ‘“, ğ‘”,Â  andÂ  ğ‘ ğ‘“, ğ‘”Â  areÂ  theÂ  luminanceÂ  comparison,Â  contrastÂ  comparison,Â  andÂ  structureÂ comparisonÂ betweenÂ  ğ‘“Â  andÂ  ğ‘”,Â respectively:Â  ğ‘™ğ‘“, ğ‘” ,Â  (13)Â  ğ‘ğ‘“, ğ‘” ,Â  (14)Â  ğ‘ ğ‘“, ğ‘” , (15)Â  whereÂ  ğœ‡ Â  andÂ  ğœ‡ Â  areÂ  theÂ  averagesÂ  ofÂ  ğ‘“ Â  andÂ  ğ‘” ,Â  ğœ Â  areÂ  theÂ  variancesÂ  ofÂ  ğ‘“ Â  andÂ  ğ‘” ,Â  ğœ Â  isÂ  theÂ  covarianceÂ ofÂ  ğ‘“Â  andÂ  ğ‘”,Â andÂ  ğ‘,Â  ğ‘,Â andÂ  ğ‘Â  areÂ positiveÂ constantsÂ toÂ stabilizeÂ theÂ divisionÂ withÂ aÂ zeroÂ  denominator.Â  Besides,Â weÂ alsoÂ consideredÂ theÂ meanÂ biasÂ errorÂ (MBE)Â asÂ aÂ supplementaryÂ metric.Â AlthoughÂ theÂ  valueÂ ofÂ MBEÂ couldÂ notÂ indicateÂ theÂ modelÂ reliabilityÂ becauseÂ theÂ errorsÂ oftenÂ compensateÂ eachÂ other,Â  itÂ couldÂ showÂ theÂ degreeÂ toÂ whichÂ theÂ methodÂ underestimatesÂ orÂ overestimatesÂ theÂ results.Â WithÂ theÂ  purposeÂ ofÂ exhibitingÂ theÂ degreeÂ moreÂ intuitively,Â theÂ MBEÂ wasÂ calculatedÂ asÂ aÂ percentageÂ andÂ theÂ  MBEÂ betweenÂ  ğ‘“Â  andÂ  ğ‘”Â  wasÂ definedÂ asÂ  ğ‘€ğµğ¸ğ‘“, ğ‘” âˆ‘ 100%.Â  (16)Â  3.3.Â DataÂ  FYâ€2,Â theÂ firstÂ ChineseÂ geostationaryÂ meteorologicalÂ satellite,Â wasÂ launchedÂ onÂ 31Â DecemberÂ 2014,Â  andÂ  positionedÂ  aboveÂ  theÂ  equatorÂ  atÂ  105Â°Â  E.Â  WithÂ  theÂ  StretchedÂ  VisibleÂ  andÂ  InfraredÂ  SpinÂ  ScanÂ  RadiometerÂ (Sâ€VISSR)Â onÂ board,Â FYâ€2Â canÂ observeÂ theÂ Earthâ€™sÂ atmosphereÂ withÂ highÂ temporalÂ andÂ  spatialÂ resolutions.Â TheÂ IR1Â channelÂ (10.3~11.3Â Î¼m)Â ChinaÂ landâ€areaÂ imagesÂ areÂ obtainedÂ hourlyÂ forÂ  theÂ spatialÂ rangeÂ 50Â°Â E~160Â°Â E,Â 4Â°Â N~65Â°Â N.Â TheÂ sizeÂ ofÂ eachÂ imageÂ isÂ 512Â Ã—Â 512Â pixels,Â andÂ theÂ spatialÂ  resolutionÂ ofÂ eachÂ pixelÂ isÂ 13Â kmÂ inÂ bothÂ theÂ northâ€southÂ andÂ eastâ€westÂ directions.Â TheÂ intensityÂ ofÂ theÂ  pixelsÂ isÂ 0~255.Â TheÂ relationshipÂ betweenÂ theÂ intensityÂ countÂ andÂ brightnessÂ temperatureÂ isÂ negativeÂ  butÂ notÂ linear.Â AnÂ imageÂ instanceÂ ofÂ FYâ€2Â isÂ depictedÂ inÂ FigureÂ 4.Â  Â  FigureÂ 4.Â FYâ€2GÂ IR1Â ChinaÂ landâ€areaÂ image,Â producedÂ atÂ 12Â a.m.Â onÂ 1Â MarchÂ 2017.Â  Â  Figure 4. FY-2G IR1 China land-area image, produced at 12 a.m. on 1 March 2017. 4. Results and Analysis One epoch is one training cycle through the entire training dataset. The models described in the previous sections were trained on the training dataset for 50 epochs and evaluated on the validation dataset after every epoch. The MSE loss is presented in Figure 5. AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  9Â  ofÂ  16Â  4.Â ResultsÂ andÂ AnalysisÂ  OneÂ epochÂ isÂ oneÂ trainingÂ cycleÂ throughÂ theÂ entireÂ trainingÂ dataset.Â TheÂ modelsÂ describedÂ inÂ theÂ  previousÂ sectionsÂ wereÂ trainedÂ onÂ theÂ trainingÂ datasetÂ forÂ 50Â epochsÂ andÂ evaluatedÂ onÂ theÂ validationÂ  datasetÂ afterÂ everyÂ epoch.Â TheÂ MSEÂ lossÂ isÂ presentedÂ inÂ FigureÂ 5.Â  Â  FigureÂ 5.Â MeanÂ squaredÂ errorÂ (MSE)Â lossÂ inÂ modelÂ trainingÂ andÂ validation.Â  InÂ FigureÂ 5,Â itÂ isÂ apparentÂ thatÂ theÂ MSEÂ lossÂ declinedÂ dramaticallyÂ inÂ theÂ firstÂ 10Â epochs;Â thereafter,Â  theÂ declineÂ rateÂ graduallyÂ decreased,Â andÂ theÂ MSEÂ lossÂ eventuallyÂ convergedÂ toÂ aÂ lowerÂ level.Â WhenÂ  theÂ trainingÂ timeÂ wasÂ overÂ 40Â epochs,Â theÂ lossÂ wasÂ relativelyÂ smallÂ comparedÂ toÂ thatÂ withinÂ theÂ firstÂ  10.Â DespiteÂ theÂ fluctuationÂ ofÂ theÂ validationÂ loss,Â theÂ integratedÂ trendÂ continuedÂ toÂ decline,Â whichÂ  indicatesÂ  thatÂ  theÂ  modelÂ  wasÂ  notÂ  overfitting.Â  Thus,Â  theÂ  trainingÂ  procedureÂ  wasÂ  effectiveÂ  andÂ  convergedÂ toÂ aÂ quiteÂ satisfactoryÂ result.Â  WeÂ thenÂ randomlyÂ selectedÂ 20Â daysÂ fromÂ 2018Â asÂ theÂ testÂ dataset,Â onÂ whichÂ weÂ comparedÂ ourÂ  methodÂ  (Multiâ€GRUâ€RCN)Â  withÂ  theÂ  VOFÂ  technique,Â  ConvLSTM,Â  LSTM,Â  andÂ  GRU.Â  ForÂ  theÂ  VOFÂ  algorithm,Â weÂ usedÂ theÂ methodÂ ofÂ ChowÂ etÂ al.Â [26],Â whichÂ minimizesÂ theÂ objectiveÂ functionÂ byÂ usingÂ  brightnessÂ constancyÂ andÂ globalÂ smoothnessÂ asÂ modelÂ assumptionsÂ toÂ realizeÂ VOF.Â WeÂ setÂ theÂ sizeÂ ofÂ  theÂ inputÂ patchesÂ asÂ 128Â Ã—Â 128Â andÂ theÂ sizeÂ ofÂ theÂ outputÂ patchesÂ asÂ 64Â Ã—Â 64Â toÂ produceÂ comparableÂ  resultsÂ  withÂ  Multiâ€GRUâ€RCN.Â  ForÂ  ConvLSTM,Â  weÂ  adoptedÂ  theÂ  modelÂ  structureÂ  ofÂ  ShiÂ  etÂ  al.Â  [41],Â  settingÂ theÂ kernelÂ sizeÂ atÂ 3Â Ã—Â 3Â forÂ convolution.Â TheÂ inputÂ frameÂ hadÂ theÂ sameÂ sizeÂ asÂ theÂ outputÂ frame.Â  ForÂ LSTMÂ andÂ GRU,Â weÂ deployedÂ fiveÂ framesÂ toÂ predictÂ theÂ nextÂ frame.Â BecauseÂ LSTMÂ andÂ GRUÂ  cannotÂ extractÂ spatialÂ information,Â weÂ treatedÂ everyÂ pixelÂ inÂ aÂ frameÂ asÂ anÂ independentÂ sample;Â thus,Â  thereÂ wereÂ 4096Â samplesÂ inÂ aÂ frame.Â AllÂ theÂ experimentsÂ wereÂ carriedÂ outÂ withÂ NVIDIAÂ TeslaÂ T4Â GPU.Â  ItÂ takesÂ 7.78Â hÂ toÂ trainÂ theÂ GRUÂ model,Â 9.44Â hÂ toÂ trainÂ theÂ LSTMÂ model,Â 12.29Â hÂ toÂ trainÂ theÂ ConvLSTMÂ  model,Â  andÂ  13.96Â  hÂ  toÂ  trainÂ  theÂ  Multiâ€GRUâ€RCNÂ  model.Â  ThereÂ  isÂ  noÂ  trainingÂ  processÂ  ofÂ  theÂ  VOFÂ  method.Â InÂ theÂ testÂ process,Â itÂ requiresÂ 2.57,Â 3.65,Â 3.72,Â 4.28,Â andÂ 4.73Â sÂ toÂ predictÂ oneÂ frameÂ withÂ theÂ  VOFÂ  method,Â  GRUÂ  model,Â  LSTMÂ  model,Â  ConvLSTMÂ  model,Â  andÂ  Multiâ€GRUâ€RCNÂ  model,Â  respectively.Â  TheÂ MBEsÂ predictedÂ byÂ VOF,Â GRU,Â LSTM,Â ConvLSTM,Â andÂ Multiâ€GRUâ€RCNÂ areÂ 0.50%,Â 1.47%,Â  1.64%,Â âˆ’0.51%,Â andÂ 0.45%.Â TheÂ nearlyÂ zeroÂ MBEsÂ illustrateÂ thatÂ noneÂ ofÂ theseÂ methodsÂ underÂ orÂ overÂ  forecastÂ andÂ noÂ postprocessingÂ stepsÂ areÂ neededÂ toÂ calibrateÂ theÂ results.Â QuantitativeÂ resultsÂ inÂ termsÂ  ofÂ PSNRÂ andÂ SSIMÂ overÂ theÂ testÂ datasetÂ areÂ summarizedÂ inÂ TableÂ 1.Â TheÂ resultsÂ shownÂ inÂ TableÂ 1Â  confirmÂ thatÂ Multiâ€GRUâ€RCNÂ achievesÂ theÂ mostÂ promisingÂ resultsÂ onÂ bothÂ PSNRÂ andÂ SSIMÂ metricsÂ  overÂ theÂ entireÂ testÂ datasetÂ amongÂ theseÂ methods.Â ToÂ beÂ specific,Â comparedÂ withÂ ConvLSTM,Â Multiâ€ Figure 5. Mean squared error (MSE) loss in model training and validation. In Figure 5, it is apparent that the MSE loss declined dramatically in the ï¬rst 10 epochs; thereafter, the decline rate gradually decreased, and the MSE loss eventually converged to a lower level. When the training time was over 40 epochs, the loss was relatively small compared to that within the ï¬rst 10. Despite the ï¬‚uctuation of the validation loss, the integrated trend continued to decline, which indicates that the model was not overï¬tting. Thus, the training procedure was eï¬€ective and converged to a quite satisfactory result. Atmosphere 2020, 11, 1151 10 of 17 We then randomly selected 20 days from 2018 as the test dataset, on which we compared our method (Multi-GRU-RCN) with the VOF technique, ConvLSTM, LSTM, and GRU. For the VOF algorithm, we used the method of Chow et al. [26], which minimizes the objective function by using brightness constancy and global smoothness as model assumptions to realize VOF. We set the size of the input patches as 128 Ã— 128 and the size of the output patches as 64 Ã— 64 to produce comparable results with Multi-GRU-RCN. For ConvLSTM, we adopted the model structure of Shi et al. [41], setting the kernel size at 3 Ã— 3 for convolution. The input frame had the same size as the output frame. For LSTM and GRU, we deployed ï¬ve frames to predict the next frame. Because LSTM and GRU cannot extract spatial information, we treated every pixel in a frame as an independent sample; thus, there were 4096 samples in a frame. All the experiments were carried out with NVIDIA Tesla T4 GPU. It takes 7.78 h to train the GRU model, 9.44 h to train the LSTM model, 12.29 h to train the ConvLSTM model, and 13.96 h to train the Multi-GRU-RCN model. There is no training process of the VOF method. In the test process, it requires 2.57, 3.65, 3.72, 4.28, and 4.73 s to predict one frame with the VOF method, GRU model, LSTM model, ConvLSTM model, and Multi-GRU-RCN model, respectively. The MBEs predicted by VOF, GRU, LSTM, ConvLSTM, and Multi-GRU-RCN are 0.50%, 1.47%, 1.64%, âˆ’0.51%, and 0.45%. The nearly zero MBEs illustrate that none of these methods under or over forecast and no postprocessing steps are needed to calibrate the results. Quantitative results in terms of PSNR and SSIM over the test dataset are summarized in Table 1. The results shown in Table 1 conï¬rm that Multi-GRU-RCN achieves the most promising results on both PSNR and SSIM metrics over the entire test dataset among these methods. To be speciï¬c, compared with ConvLSTM, Multi-GRU-RCN achieves a performance gain by 4.11% on PSNR and 2.60% on SSIM. In order to investigate the results in detail, we calculated the average PSNR and SSIM over the total 64 test samples for each day. The PSNR and the SSIM results using VOF, GRU, LSTM, ConvLSTM, and Multi-GRU-RCN on the test data for each day are compared in Figures 6 and 7, respectively. Table 1. Comparison of Variation Optical-ï¬‚ow (VOF), Gated Recurrent Unit (GRU), Long Short-term Memory (LSTM), Convolutional Long Short-term Memory (ConvLSTM), and Multi-Gated Recurrent Unit (GRU)-Recurrent Convolutional Network (RCN) on the test dataset (highest measures are in bold). Method PSNR SSIM VOF 22.98 0.41 GRU 24.49 0.66 LSTM 24.58 0.66 ConvLSTM 28.45 0.77 Multi-GRU-RCN 29.62 0.79 AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  10Â  ofÂ  16Â  GRUâ€RCNÂ achievesÂ aÂ performanceÂ gainÂ byÂ 4.11%Â onÂ PSNRÂ andÂ 2.60%Â onÂ SSIM.Â InÂ orderÂ toÂ investigateÂ  theÂ resultsÂ inÂ detail,Â weÂ calculatedÂ theÂ averageÂ PSNRÂ andÂ SSIMÂ overÂ theÂ totalÂ 64Â testÂ samplesÂ forÂ eachÂ  day.Â TheÂ PSNRÂ andÂ theÂ SSIMÂ resultsÂ usingÂ VOF,Â GRU,Â LSTM,Â ConvLSTM,Â andÂ Multiâ€GRUâ€RCNÂ onÂ  theÂ testÂ dataÂ forÂ eachÂ dayÂ areÂ comparedÂ inÂ FiguresÂ 6Â andÂ 7,Â respectively.Â  TableÂ 1.Â ComparisonÂ ofÂ VariationÂ Opticalâ€flowÂ (VOF),Â GatedÂ RecurrentÂ UnitÂ (GRU),Â LongÂ Shortâ€termÂ  MemoryÂ (LSTM),Â ConvolutionalÂ LongÂ Shortâ€termÂ MemoryÂ (ConvLSTM),Â andÂ Multiâ€GatedÂ RecurrentÂ  UnitÂ  (GRU)â€RecurrentÂ  ConvolutionalÂ  NetworkÂ  (RCN)Â  onÂ  theÂ  testÂ  datasetÂ  (highestÂ  measuresÂ  areÂ  inÂ  bold).Â  MethodÂ  PSNRÂ  SSIMÂ  VOFÂ  22.98Â  0.41Â  GRUÂ  24.49Â  0.66Â  LSTMÂ  24.58Â  0.66Â  ConvLSTMÂ  28.45Â  0.77Â  Multiâ€GRUâ€RCNÂ  29.62Â  0.79Â  Â  FigureÂ 6.Â PeakÂ Signalâ€toâ€noiseÂ RatioÂ (PSNR)Â whenÂ applyingÂ fiveÂ differentÂ methodsÂ toÂ theÂ testÂ dataÂ forÂ  variousÂ days.Â  Â  FigureÂ  7.Â  StructuralÂ  SimilarityÂ  (SSIM)Â  whenÂ  applyingÂ  fiveÂ  differentÂ  methodsÂ  toÂ  theÂ  testÂ  dataÂ  forÂ  variousÂ days.Â  AccordingÂ toÂ FiguresÂ 6Â andÂ 7,Â theÂ forecastÂ resultsÂ ofÂ theseÂ methodsÂ wereÂ consistentÂ inÂ termsÂ ofÂ  eachÂ metric.Â ForÂ instance,Â theÂ PSNRsÂ andÂ SSIMsÂ ofÂ theÂ fiveÂ methodsÂ wereÂ theÂ highestÂ onÂ 2Â February,Â  whichÂ meansÂ thatÂ allÂ fourÂ methodsÂ performedÂ theÂ bestÂ onÂ theÂ dataÂ ofÂ thatÂ day.Â BasedÂ onÂ theÂ forecastÂ  results,Â theÂ Multiâ€GRUâ€RCNÂ methodÂ consistentlyÂ outperformedÂ theÂ otherÂ fourÂ methodsÂ duringÂ theÂ  wholeÂ computationalÂ time.Â TheÂ VOFÂ wasÂ theÂ worstâ€performingÂ methodÂ onÂ theÂ testÂ data.Â Multiâ€GRUâ€ RCNÂ  andÂ  ConvLSTMÂ  hadÂ  quiteÂ  similarÂ  performanceÂ  inÂ  termsÂ  ofÂ  bothÂ  MSEÂ  andÂ  PSNRÂ  valuesÂ  butÂ  Figure 6. Peak Signal-to-noise Ratio (PSNR) when applying ï¬ve diï¬€erent methods to the test data for various days. Atmosphere 2020, 11, 1151 11 of 17AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  10Â  ofÂ  16Â  GRUâ€RCNÂ achievesÂ aÂ performanceÂ gainÂ byÂ 4.11%Â onÂ PSNRÂ andÂ 2.60%Â onÂ SSIM.Â InÂ orderÂ toÂ investigateÂ  theÂ resultsÂ inÂ detail,Â weÂ calculatedÂ theÂ averageÂ PSNRÂ andÂ SSIMÂ overÂ theÂ totalÂ 64Â testÂ samplesÂ forÂ eachÂ  day.Â TheÂ PSNRÂ andÂ theÂ SSIMÂ resultsÂ usingÂ VOF,Â GRU,Â LSTM,Â ConvLSTM,Â andÂ Multiâ€GRUâ€RCNÂ onÂ  theÂ testÂ dataÂ forÂ eachÂ dayÂ areÂ comparedÂ inÂ FiguresÂ 6Â andÂ 7,Â respectively.Â  TableÂ 1.Â ComparisonÂ ofÂ VariationÂ Opticalâ€flowÂ (VOF),Â GatedÂ RecurrentÂ UnitÂ (GRU),Â LongÂ Shortâ€termÂ  MemoryÂ (LSTM),Â ConvolutionalÂ LongÂ Shortâ€termÂ MemoryÂ (ConvLSTM),Â andÂ Multiâ€GatedÂ RecurrentÂ  UnitÂ  (GRU)â€RecurrentÂ  ConvolutionalÂ  NetworkÂ  (RCN)Â  onÂ  theÂ  testÂ  datasetÂ  (highestÂ  measuresÂ  areÂ  inÂ  bold).Â  MethodÂ  PSNRÂ  SSIMÂ  VOFÂ  22.98Â  0.41Â  GRUÂ  24.49Â  0.66Â  LSTMÂ  24.58Â  0.66Â  ConvLSTMÂ  28.45Â  0.77Â  Multiâ€GRUâ€RCNÂ  29.62Â  0.79Â  Â  FigureÂ 6.Â PeakÂ Signalâ€toâ€noiseÂ RatioÂ (PSNR)Â whenÂ applyingÂ fiveÂ differentÂ methodsÂ toÂ theÂ testÂ dataÂ forÂ  variousÂ days.Â  Â  FigureÂ  7.Â  StructuralÂ  SimilarityÂ  (SSIM)Â  whenÂ  applyingÂ  fiveÂ  differentÂ  methodsÂ  toÂ  theÂ  testÂ  dataÂ  forÂ  variousÂ days.Â  AccordingÂ toÂ FiguresÂ 6Â andÂ 7,Â theÂ forecastÂ resultsÂ ofÂ theseÂ methodsÂ wereÂ consistentÂ inÂ termsÂ ofÂ  eachÂ metric.Â ForÂ instance,Â theÂ PSNRsÂ andÂ SSIMsÂ ofÂ theÂ fiveÂ methodsÂ wereÂ theÂ highestÂ onÂ 2Â February,Â  whichÂ meansÂ thatÂ allÂ fourÂ methodsÂ performedÂ theÂ bestÂ onÂ theÂ dataÂ ofÂ thatÂ day.Â BasedÂ onÂ theÂ forecastÂ  results,Â theÂ Multiâ€GRUâ€RCNÂ methodÂ consistentlyÂ outperformedÂ theÂ otherÂ fourÂ methodsÂ duringÂ theÂ  wholeÂ computationalÂ time.Â TheÂ VOFÂ wasÂ theÂ worstâ€performingÂ methodÂ onÂ theÂ testÂ data.Â Multiâ€GRUâ€ RCNÂ  andÂ  ConvLSTMÂ  hadÂ  quiteÂ  similarÂ  performanceÂ  inÂ  termsÂ  ofÂ  bothÂ  MSEÂ  andÂ  PSNRÂ  valuesÂ  butÂ  Figure 7. Structural Similarity (SSIM) when applying ï¬ve diï¬€erent methods to the test data for various days. According to Figures 6 and 7, the forecast results of these methods were consistent in terms of each metric. For instance, the PSNRs and SSIMs of the ï¬ve methods were the highest on 2 February, which means that all four methods performed the best on the data of that day. Based on the forecast results, the Multi-GRU-RCN method consistently outperformed the other four methods during the whole computational time. The VOF was the worst-performing method on the test data. Multi-GRU-RCN and ConvLSTM had quite similar performance in terms of both MSE and PSNR values but Multi-GRU-RCN performed slightly better. The MSE of Multi-GRU-RCN forecasts on the test dataset was 72.93, which means that the average intensity diï¬€erence per pixel between ground truth and prediction was 8.54 (a satisfactory result, given that the intensity range was 0~255). To show the results more intuitively, we randomly picked three input sequences from the test dataset: May 2 between 0 and 5 am, January 31 between 6 and 11 pm, and July 7 between 6 and 11 am. Figure 8 shows the predictions of the next hour produced by VOF, GRU, LSTM, ConvLSTM, and Multi-GRU-RCN. The PSNRs predicted by VOF are 22.99, 23.01, and 22.91; those predicted by GRU are 23.92, 24.63, and 23.50; those predicted by LSTM are 23.98, 24.61, and 23.42; those predicted by ConvLSTM are 28.40, 28.50, and 27.67; and those predicted by Multi-GRU-RCN are 29.66, 30.37, and 29.86. The PSNR values predicted by Multi-GRU-RCN are consistently larger than those of the other methods, which indicates that its predictions are more accurate. This result also agrees with the diï¬€erence between the ground truth and prediction. Even though the predictions by VOF have sharper outlines, the predictions by Multi-GRU-RCN have better accuracy. When a cloud appears at the edge of the prediction domain, Multi-GRU-RCN can predict it better than VOF. This proves that some of the complex spatiotemporal patterns in the dataset can be learned by the nonlinear and convolutional structure of the network. The model also performs well at predicting nonstationary processes, such as inversion and deformation, whereas VOF does not: in the VOF prediction for such situations, an abrupt change of intensity between adjacent pixels occurs at the bottom of the image. Multi-GRU-RCN gives a better prediction result without a blocky appearance. Atmosphere 2020, 11, 1151 12 of 17AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  11Â  ofÂ  16Â  Multiâ€GRUâ€RCNÂ performedÂ slightlyÂ better.Â TheÂ MSEÂ ofÂ Multiâ€GRUâ€RCNÂ forecastsÂ onÂ theÂ testÂ datasetÂ  wasÂ 72.93,Â whichÂ meansÂ thatÂ theÂ averageÂ intensityÂ differenceÂ perÂ pixelÂ betweenÂ groundÂ truthÂ andÂ  predictionÂ wasÂ 8.54Â (aÂ satisfactoryÂ result,Â givenÂ thatÂ theÂ intensityÂ rangeÂ wasÂ 0~255).Â  ToÂ showÂ theÂ resultsÂ moreÂ intuitively,Â weÂ randomlyÂ pickedÂ threeÂ inputÂ sequencesÂ fromÂ theÂ testÂ  dataset:Â MayÂ 2Â betweenÂ 0Â andÂ 5Â am,Â JanuaryÂ 31Â betweenÂ 6Â andÂ 11Â pm,Â andÂ JulyÂ 7Â betweenÂ 6Â andÂ 11Â  am.Â FigureÂ 8Â showsÂ theÂ predictionsÂ ofÂ theÂ nextÂ hourÂ producedÂ byÂ VOF,Â GRU,Â LSTM,Â ConvLSTM,Â andÂ  Multiâ€GRUâ€RCN.Â TheÂ PSNRsÂ predictedÂ byÂ VOFÂ areÂ 22.99,Â 23.01,Â andÂ 22.91;Â thoseÂ predictedÂ byÂ GRUÂ  areÂ 23.92,Â 24.63,Â andÂ 23.50;Â thoseÂ predictedÂ byÂ LSTMÂ areÂ 23.98,Â 24.61,Â andÂ 23.42;Â thoseÂ predictedÂ byÂ  ConvLSTMÂ areÂ 28.40,Â 28.50,Â andÂ 27.67;Â andÂ thoseÂ predictedÂ byÂ Multiâ€GRUâ€RCNÂ areÂ 29.66,Â 30.37,Â andÂ  29.86.Â TheÂ PSNRÂ valuesÂ predictedÂ byÂ Multiâ€GRUâ€RCNÂ areÂ consistentlyÂ largerÂ thanÂ thoseÂ ofÂ theÂ otherÂ  methods,Â  whichÂ  indicatesÂ  thatÂ  itsÂ  predictionsÂ  areÂ  moreÂ  accurate.Â  ThisÂ  resultÂ  alsoÂ  agreesÂ  withÂ  theÂ  differenceÂ  betweenÂ  theÂ  groundÂ  truthÂ  andÂ  prediction.Â  EvenÂ  thoughÂ  theÂ  predictionsÂ  byÂ  VOFÂ  haveÂ  sharperÂ outlines,Â theÂ predictionsÂ byÂ Multiâ€GRUâ€RCNÂ haveÂ betterÂ accuracy.Â WhenÂ aÂ cloudÂ appearsÂ atÂ  theÂ edgeÂ ofÂ theÂ predictionÂ domain,Â Multiâ€GRUâ€RCNÂ canÂ predictÂ itÂ betterÂ thanÂ VOF.Â ThisÂ provesÂ thatÂ  someÂ  ofÂ  theÂ  complexÂ  spatiotemporalÂ  patternsÂ  inÂ  theÂ  datasetÂ  canÂ  beÂ  learnedÂ  byÂ  theÂ  nonlinearÂ  andÂ  convolutionalÂ structureÂ ofÂ theÂ network.Â TheÂ modelÂ alsoÂ performsÂ wellÂ atÂ predictingÂ nonstationaryÂ  processes,Â suchÂ asÂ inversionÂ andÂ deformation,Â whereasÂ VOFÂ doesÂ not:Â inÂ theÂ VOFÂ predictionÂ forÂ suchÂ  situations,Â anÂ abruptÂ changeÂ ofÂ intensityÂ betweenÂ adjacentÂ pixelsÂ occursÂ atÂ theÂ bottomÂ ofÂ theÂ image.Â  Multiâ€GRUâ€RCNÂ givesÂ aÂ betterÂ predictionÂ resultÂ withoutÂ aÂ blockyÂ appearance.Â  Â  Â  AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  12Â  ofÂ  16Â  Â  FigureÂ  8.Â  ThreeÂ  examplesÂ  ofÂ  satelliteÂ  imageÂ  andÂ  predictionÂ  resultsÂ  areÂ  shownÂ  inÂ  (a),Â  (b)Â  andÂ  (c),Â  respectively.Â InÂ eachÂ panel,Â theÂ firstÂ horizontalÂ rowÂ showsÂ theÂ inputÂ sequenceÂ andÂ theÂ groundÂ truth;Â  theÂ secondÂ horizontalÂ rowÂ displaysÂ theÂ predictionsÂ ofÂ fiveÂ differentÂ methods;Â andÂ theÂ thirdÂ horizontalÂ  rowÂ showsÂ theÂ differenceÂ betweenÂ theÂ predictionÂ byÂ eachÂ ofÂ theÂ fiveÂ methodsÂ andÂ theÂ groundÂ truth.Â  5.Â DiscussionÂ  TheÂ relationshipÂ ofÂ GRU,Â LSTM,Â ConvLSTM,Â GRUâ€RCN,Â andÂ Multiâ€GRUâ€RCNÂ isÂ illustratedÂ inÂ  FigureÂ 9.Â GRUÂ isÂ aÂ simplificationÂ ofÂ LSTMÂ byÂ replacingÂ theÂ forgetÂ gateÂ andÂ inputÂ gateÂ withÂ theÂ updateÂ  gate,Â  andÂ  combiningÂ  theÂ  cellÂ  stateÂ  andÂ  hiddenÂ  state.Â  EmbeddedÂ  convolutionalÂ  operationÂ  inÂ  theÂ  recurrentÂ unit,Â ConvLSTM,Â andÂ GRUâ€RCNÂ wereÂ implementedÂ forÂ spatialâ€temporalÂ dataÂ andÂ GRUâ€ RCNÂ hasÂ lessÂ parametersÂ thanÂ ConvLSTM.Â AsÂ theÂ GRUâ€RCNÂ modelÂ structureÂ proposedÂ byÂ BallasÂ etÂ al.Â  [42]Â focusedÂ onÂ theÂ videoÂ classificationÂ problem,Â weÂ changedÂ theÂ modelÂ structureÂ toÂ adaptÂ forÂ theÂ  pixelwiseÂ  cloudÂ  motionÂ  predictionÂ  problem.Â  WeÂ  consideredÂ  theÂ  ConvLSTMÂ  modelÂ  structureÂ  proposedÂ byÂ ShiÂ etÂ al.Â [41]Â asÂ aÂ reference,Â andÂ replacedÂ theÂ ConvLSTMÂ layerÂ withÂ theÂ GRUâ€RCNÂ layer.Â  Besides,Â theÂ surroundingÂ contextÂ wasÂ introducedÂ intoÂ ourÂ modelÂ toÂ enrichÂ inputÂ information.Â  Â  FigureÂ  9.Â  RelationshipÂ  ofÂ  GatedÂ  RecurrentÂ  UnitÂ  (GRU),Â  LongÂ  Shortâ€termÂ  MemoryÂ  (LSTM),Â  ConvolutionalÂ  LongÂ  Shortâ€termÂ  MemoryÂ  (ConvLSTM),Â  GatedÂ  RecurrentÂ  UnitÂ  (GRU)â€RecurrentÂ  ConvolutionalÂ  NetworkÂ  (RCN),Â  andÂ  Multiâ€GatedÂ  RecurrentÂ  UnitÂ  (GRU)â€RecurrentÂ  ConvolutionalÂ  NetworkÂ (RCN).Â  InÂ  predictingÂ  cloudÂ  motion,Â  bothÂ  temporalÂ  andÂ  spatialÂ  informationÂ  provideÂ  importantÂ  clues.Â  Temporally,Â theÂ currentÂ frameÂ correlatesÂ withÂ theÂ previousÂ frame;Â spatially,Â theÂ intensityÂ ofÂ aÂ givenÂ  pixelÂ  correlatesÂ  withÂ  thoseÂ  ofÂ  theÂ  surroundingÂ  pixels.Â  AÂ  GRUÂ  capturesÂ  temporalÂ  informationÂ  butÂ  ignoresÂ spatialÂ information;Â therefore,Â itÂ underperformsÂ whenÂ comparedÂ toÂ theÂ ConvLSTMÂ model,Â  whichÂ capturesÂ both.Â However,Â inÂ theÂ ConvLSTMÂ model,Â theÂ inputÂ frameÂ hasÂ theÂ sameÂ shapeÂ asÂ theÂ  outputÂ frame:Â asÂ aÂ resultÂ ofÂ theÂ convolutionalÂ operationÂ andÂ sameâ€paddingÂ method,Â itÂ losesÂ boundaryÂ  information.Â InÂ addition,Â theÂ movementÂ ofÂ theÂ cloudÂ isÂ veryÂ complicatedÂ andÂ cannotÂ beÂ determinedÂ  Figure 8. Three examples of satellite image and prediction results are shown in (a)â€“(c), respectively. In each panel, the ï¬rst horizontal row shows the input sequence and the ground truth; the second horizontal row displays the predictions of ï¬ve diï¬€erent methods; and the third horizontal row shows the diï¬€erence between the prediction by each of the ï¬ve methods and the ground truth. Atmosphere 2020, 11, 1151 13 of 17 5. Discussion The relationship of GRU, LSTM, ConvLSTM, GRU-RCN, and Multi-GRU-RCN is illustrated in Figure 9. GRU is a simpliï¬cation of LSTM by replacing the forget gate and input gate with the update gate, and combining the cell state and hidden state. Embedded convolutional operation in the recurrent unit, ConvLSTM, and GRU-RCN were implemented for spatial-temporal data and GRU-RCN has less parameters than ConvLSTM. As the GRU-RCN model structure proposed by Ballas et al. [42] focused on the video classiï¬cation problem, we changed the model structure to adapt for the pixelwise cloud motion prediction problem. We considered the ConvLSTM model structure proposed by Shi et al. [41] as a reference, and replaced the ConvLSTM layer with the GRU-RCN layer. Besides, the surrounding context was introduced into our model to enrich input information. AtmosphereÂ 2020,Â 11,Â xÂ FORÂ PEERÂ REVIEWÂ  12Â  ofÂ  16Â  Â  FigureÂ  8.Â  ThreeÂ  examplesÂ  ofÂ  satelliteÂ  imageÂ  andÂ  predictionÂ  resultsÂ  areÂ  shownÂ  inÂ  (a),Â  (b)Â  andÂ  (c),Â  respectively.Â InÂ eachÂ panel,Â theÂ firstÂ horizontalÂ rowÂ showsÂ theÂ inputÂ sequenceÂ andÂ theÂ groundÂ truth;Â  theÂ secondÂ horizontalÂ rowÂ displaysÂ theÂ predictionsÂ ofÂ fiveÂ differentÂ methods;Â andÂ theÂ thirdÂ horizontalÂ  rowÂ showsÂ theÂ differenceÂ betweenÂ theÂ predictionÂ byÂ eachÂ ofÂ theÂ fiveÂ methodsÂ andÂ theÂ groundÂ truth.Â  5.Â DiscussionÂ  TheÂ relationshipÂ ofÂ GRU,Â LSTM,Â ConvLSTM,Â GRUâ€RCN,Â andÂ Multiâ€GRUâ€RCNÂ isÂ illustratedÂ inÂ  FigureÂ 9.Â GRUÂ isÂ aÂ simplificationÂ ofÂ LSTMÂ byÂ replacingÂ theÂ forgetÂ gateÂ andÂ inputÂ gateÂ withÂ theÂ updateÂ  gate,Â  andÂ  combiningÂ  theÂ  cellÂ  stateÂ  andÂ  hiddenÂ  state.Â  EmbeddedÂ  convolutionalÂ  operationÂ  inÂ  theÂ  recurrentÂ unit,Â ConvLSTM,Â andÂ GRUâ€RCNÂ wereÂ implementedÂ forÂ spatialâ€temporalÂ dataÂ andÂ GRUâ€ RCNÂ hasÂ lessÂ parametersÂ thanÂ ConvLSTM.Â AsÂ theÂ GRUâ€RCNÂ modelÂ structureÂ proposedÂ byÂ BallasÂ etÂ al.Â  [42]Â focusedÂ onÂ theÂ videoÂ classificationÂ problem,Â weÂ changedÂ theÂ modelÂ structureÂ toÂ adaptÂ forÂ theÂ  pixelwiseÂ  cloudÂ  motionÂ  predictionÂ  problem.Â  WeÂ  consideredÂ  theÂ  ConvLSTMÂ  modelÂ  structureÂ  proposedÂ byÂ ShiÂ etÂ al.Â [41]Â asÂ aÂ reference,Â andÂ replacedÂ theÂ ConvLSTMÂ layerÂ withÂ theÂ GRUâ€RCNÂ layer.Â  Besides,Â theÂ surroundingÂ contextÂ wasÂ introducedÂ intoÂ ourÂ modelÂ toÂ enrichÂ inputÂ information.Â  Â  FigureÂ  9.Â  RelationshipÂ  ofÂ  GatedÂ  RecurrentÂ  UnitÂ  (GRU),Â  LongÂ  Shortâ€termÂ  MemoryÂ  (LSTM),Â  ConvolutionalÂ  LongÂ  Shortâ€termÂ  MemoryÂ  (ConvLSTM),Â  GatedÂ  RecurrentÂ  UnitÂ  (GRU)â€RecurrentÂ  ConvolutionalÂ  NetworkÂ  (RCN),Â  andÂ  Multiâ€GatedÂ  RecurrentÂ  UnitÂ  (GRU)â€RecurrentÂ  ConvolutionalÂ  NetworkÂ (RCN).Â  InÂ  predictingÂ  cloudÂ  motion,Â  bothÂ  temporalÂ  andÂ  spatialÂ  informationÂ  provideÂ  importantÂ  clues.Â  Temporally,Â theÂ currentÂ frameÂ correlatesÂ withÂ theÂ previousÂ frame;Â spatially,Â theÂ intensityÂ ofÂ aÂ givenÂ  pixelÂ  correlatesÂ  withÂ  thoseÂ  ofÂ  theÂ  surroundingÂ  pixels.Â  AÂ  GRUÂ  capturesÂ  temporalÂ  informationÂ  butÂ  ignoresÂ spatialÂ information;Â therefore,Â itÂ underperformsÂ whenÂ comparedÂ toÂ theÂ ConvLSTMÂ model,Â  whichÂ capturesÂ both.Â However,Â inÂ theÂ ConvLSTMÂ model,Â theÂ inputÂ frameÂ hasÂ theÂ sameÂ shapeÂ asÂ theÂ  outputÂ frame:Â asÂ aÂ resultÂ ofÂ theÂ convolutionalÂ operationÂ andÂ sameâ€paddingÂ method,Â itÂ losesÂ boundaryÂ  information.Â InÂ addition,Â theÂ movementÂ ofÂ theÂ cloudÂ isÂ veryÂ complicatedÂ andÂ cannotÂ beÂ determinedÂ  Figure 9. Relationship of Gated Recurrent Unit (GRU), Long Short-term Memory (LSTM), Convolutional Long Short-term Memory (ConvLSTM), Gated Recurrent Unit (GRU)-Recurrent Convolutional Network (RCN), and Multi-Gated Recurrent Unit (GRU)-Recurrent Convolutional Network (RCN). In predicting cloud motion, both temporal and spatial information provide important clues. Temporally, the current frame correlates with the previous frame; spatially, the intensity of a given pixel correlates with those of the surrounding pixels. A GRU captures temporal information but ignores spatial information; therefore, it underperforms when compared to the ConvLSTM model, which captures both. However, in the ConvLSTM model, the input frame has the same shape as the output frame: as a result of the convolutional operation and same-padding method, it loses boundary information. In addition, the movement of the cloud is very complicated and cannot be determined by looking at the current region exclusively; more information must be brought into the model. To improve prediction accuracy, especially in the boundary region, we incorporated the surrounding context into our new end-to-end model. The performance improvement of Multi-GRU-RCN is also contributed to the model structure. For instance, in the experiment, we set the large region as the input and the small region as the output for the VOF algorithm, while we conducted a control experiment with the small region as both the input and output. The average PSNR and SSIM on the test dataset of the control experiment is 22.69 and 0.41, which indicates that the introduction of the large region only achieves a performance gain of 1.28% and 1.22% with the VOF algorithm. In the model structure aspect, we exploited the max pooling layer for dimension reduction and improved ability of the model to capture invariance information of the cloud while moving and fuse features from diï¬€erent scales. In addition, the activation functions introduce non-linearity into the model [49]. Accumulation of activation functions produces a promising model to learn sophisticated patterns. The essential advantage of the end-to-end structure is that all the parameters of the model can be simultaneously trained, making the training process more eï¬€ective. The predictions of our model have consistently higher PSNR and SSIM than those of other methods. The spatial and temporal patterns learned by the model from the region of interest provides the fundamental of cloud motion. The utilization of external information out of the region of interest enriched the model understanding of environmental circumstances. This illustrates that utilizing information from both the internal and external region reveals a more accurate pattern of cloud motion. There are three possible explanations for the better performance of Multi-GRU-RCN over the VOF algorithm. First, Multi-GRU-RCN can learn complex patterns during the training process. The clouds Atmosphere 2020, 11, 1151 14 of 17 often seem to appear instantaneously, indicating that they either derive from outside or are suddenly formed. If similar situations happened in the training dataset, Multi-GRU-RCN could learn these patterns during the training process and subsequently provide reasonable predictions in the test dataset. However, this could not be detected by the VOF algorithm. The second explanation is that Multi-GRU-RCN is trained end-to-end for this task. The VOF algorithm is not an end-to-end model, and it is diï¬ƒcult to ï¬nd a reasonable way to update the future ï¬‚ow ï¬elds. The ï¬nal reason is that Multi-GRU-RCN can smooth a blocky appearance, whereas the predictions of VOF will have a blocky appearance whenever there are abrupt changes in the motion vectors and therefore in the intensity between adjacent pixels. Although the proposed Multi-GRU-RCN can achieve promising intra-hour cloud motion prediction, there are still limitations of this model. Compared with the VOF algorithm, the Multi- GRU-RCN produces blur prediction. This property is associated with the MSE loss when training the model. The future of the satellite cloud image is uncertain and by nature multimodal. When there are multiple valid outcomes with equally possibility, the MSE loss aims to accommodate the uncertainty by averaging all the possible outcomes, thus resulting in a blur prediction. Generative adversarial networks (GANs) have emerged as a powerful alternative to enhance prediction sharpness. In the future work, we will combine MSE loss with adversarial training loss to improve the visual quality of the predictions. Besides, limited by the number of layers in the architecture, the model could not totally eliminate the inï¬‚uence of interference, such as complex surface conditions. Li et al. [50] proposed a multi-scale convolutional feature fusion method for the cloud detection method. Their research conï¬rmed that the usage of dilated convolutional layers and feature fusion of shallow appearance information and deep semantic information helps to improve the interference tolerance. In this paper, the current forecasting range was an hour. The extension of the forecast time will convert the output from one frame to a sequence of frames. The weakness of the encoder-decoder architecture is that it lacks the alignment of the input and output sequence. Bahdanau et al. [51] proposed an attention mechanism utilizing a context vector to align the source and target inputs. The context vector preserves information from all hidden states in encoder cells and aligns them with the current target output. The attention mechanism allows the decoder to â€œattendâ€ to diï¬€erent parts of the source sentence at each step of output generation; this concept has revolutionized the ï¬eld. The introduction of the attention mechanism will address issues carried out in long-term horizon prediction. Furthermore, we plan to implement more data sources to enrich the information in the dataset and introduce data fusion techniques into the model improve the accuracy. Combining our current research with the precipitation forecast problem also merits further research. 6. Conclusions In this paper, we introduced deep-learning methods into the ï¬eld of cloud-motion prediction. This work is innovative, since traditional methods for cloud-motion prediction are mostly based on block matching and linear extrapolation, neglecting the nonstationary process during cloud movement. By formulating cloud-motion prediction as a spatial temporal data prediction problem, an end-to-end model with GRU-RCN was developed. Inclusion of the surrounding context enriched the information used. We tested this modelâ€™s applicability on the cloud images of the FY-2G dataset for intra-hour prediction. Despite the relatively simple structure of our model, it can learn complex patterns through the nonlinear and convolutional structure of the network and works well when predicting the movement of clouds on a short timescale. This provides another example of the applicability of the GRU-RCN method in dealing with spatiotemporal data and learning complex patterns of images. Author Contributions: Conceptualization, X.S. and T.L.; methodology, X.S. and T.L.; validation, X.S.; formal analysis, X.S.; writingâ€”original draft preparation, X.S.; writingâ€”review and editing, C.A.; visualization, X.S.; supervision, G.W. All authors have read and agreed to the published version of the manuscript. Atmosphere 2020, 11, 1151 15 of 17 Funding: This research was funded by the National Key Research and Development Program of China, grant numbers 2016YFE0201900 and 2017YFC0403600; the Xinjiang Production and Construction Corps, grant number 2017AA002; and the Sate Key Laboratory of Hydroscience and Engineering, grant number 2017-KY-04. Conï¬‚icts of Interest: The authors declare no conï¬‚ict of interest. References 1. Mandal, A.; Pal, S.; De, A.; Mitra, S. Novel approach to identify good tracer clouds from a sequence of satellite images. IEEE Trans. Geosci. Remote Sens. 2005, 43, 813â€“818. [CrossRef] 2. Das, S.K.; Chanda, B.; Mukherjee, D.P. Prediction of cloud for weather now-casting application using Topology Adaptive Active Membrane. In Proceedings of the 3rd International Conference on Pattern Recognition and Machine Intelligence, New Delhi, India; Springer: Berlin/Heidelberg, Germany, 2009; pp. 303â€“308. 3. Allen, M.R.; Ingram, W.J. Constraints on future changes in climate and the hydrologic cycle. Nature 2002, 419, 224â€“232. [CrossRef] [PubMed] 4. Held, I.M.; Soden, B.J. Robust Responses of the Hydrological Cycle to Global Warming. J. Clim. 2006, 19, 5686â€“5699. [CrossRef] 5. Mitchell, J.; Wilson, C.; Cunnington, W. On CO2 climate sensitivity and model dependence of results. Q. J. R. Meteorol. Soc. 1987, 113, 293â€“322. [CrossRef] 6. Naegele, A.; Randall, D.A. Geographical and Seasonal Variability of Cloud-Radiative Feedbacks on Precipitation. J. Geophys. Res. Atmos. 2019, 124, 684â€“699. [CrossRef] 7. Muhammad, E.; Muhammad, W.; Ahmad, I.; Khan, N.M.; Chen, S. Satellite precipitation product: Applicability and accuracy evaluation in diverse region. Sci. China Ser. E Technol. Sci. 2020, 63, 819â€“828. [CrossRef] 8. Hoï¬€, T.E.; Perez, R. Modeling PV ï¬‚eet output variability. Sol. Energy 2012, 86, 2177â€“2189. [CrossRef] 9. Lave, M.; Kleissl, J. Cloud speed impact on solar variability scalingâ€”Application to the wavelet variability model. Sol. Energy 2013, 91, 11â€“21. [CrossRef] 10. Chow, C.W.; Urquhart, B.; Lave, M.; Dominguez, A.; Kleissl, J.; E Shields, J.; Washom, B. Intra-hour forecasting with a total sky imager at the UC San Diego solar energy testbed. Sol. Energy 2011, 85, 2881â€“2893. [CrossRef] 11. Marquez, R.; Gueorguiev, V.; Coimbra, C.F. Forecasting of Global Horizontal Irradiance Using Sky Cover Indices. J. Sol. Energy Eng. 2012, 135, 0110171â€“0110175. 12. Perez, R.; Kivalov, S.N.; Schlemmer, J.; Hemker, K.; Hoï¬€, T.E. Short-term irradiance variability: Preliminary estimation of station pair correlation as a function of distance. Sol. Energy 2012, 86, 2170â€“2176. [CrossRef] 13. Bosch, J.; Kleissl, J. Cloud motion vectors from a network of ground sensors in a solar power plant. Sol. Energy 2013, 95, 13â€“20. [CrossRef] 14. Fung, V.; Bosch, J.L.; Roberts, S.W.; Kleissl, J. Cloud speed sensor. Atmos. Meas. Tech. Discuss. 2013, 6, 9037â€“9059. [CrossRef] 15. Huang, H.; Xu, J.; Peng, Z.; Yoo, S.; Yu, D.; Huang, D.; Qin, H. Cloud Motion Estimation for Short Term Solar Irradiation Prediction. In Proceedings of the IEEE International Conference on Smart Grid Communications, Vancouver, BC, Canada, 21â€“24 October 2013. 16. Quesada-Ruiz, S.; Chu, Y.; Tovar-Pescador, J.; Pedro, H.; Coimbra, C.F.M. Cloud-tracking methodology for intra-hour DNI forecasting. Sol. Energy 2014, 102, 267â€“275. [CrossRef] 17. Turiel, A.; Grazzini, J.; Yahia, H. Multiscale Techniques for the Detection of Precipitation Using Thermal IR Satellite Images. IEEE Geosci. Remote Sens. Lett. 2005, 2, 447â€“450. [CrossRef] 18. Vila, D.A.; Machado, L.A.T.; Laurent, H.; Velasco, I. Forecast and Tracking the Evolution of Cloud Clusters (ForTraCC) Using Satellite Infrared Imagery: Methodology and Validation. Weather. Forecast. 2008, 23, 233â€“245. [CrossRef] 19. Brad, R.; Letia, I.A. Cloud Motion Detection from Infrared Satellite Images. In Proceedings of the Second International Conference on Image and Graphics, Hefei, China, 31 July 2002; pp. 408â€“412. 20. Bedka, K.; Mecikalski, J.R. Application of Satellite-Derived Atmospheric Motion Vectors for Estimating Mesoscale Flows. J. Appl. Meteorol. 2005, 44, 1761â€“1772. [CrossRef] 21. Menzel, W.P. Cloud Tracking with Satellite Imagery: From the Pioneering Work of Ted Fujita to the Present. Bull. Am. Meteorol. Soc. 2001, 82, 33â€“47. [CrossRef] Atmosphere 2020, 11, 1151 16 of 17 22. Shields, J.E.; Karr, M.E.; Tooman, T.P.; Sowle, D.H.; Moore, S.T. The whole sky imagerâ€”A year of progress. In Proceedings of the Eighth Atmospheric Radiation Measurement (ARM) Science Team Meeting, Tucson, AZ, USA, 23â€“27 March 1998; pp. 677â€“685. 23. Hammer, A.; Heinemann, D.; Lorenz, E.; LÃ¼ckehe, B. Short-term forecasting of solar radiation: A statistical approach using satellite data. Sol. Energy 1999, 67, 139â€“150. [CrossRef] 24. Jamaly, M.; Kleissl, J. Robust cloud motion estimation by spatio-temporal correlation analysis of irradiance data. Sol. Energy 2018, 159, 306â€“317. [CrossRef] 25. Yang, H.; Kurtz, B.; Nguyen, D.; Urquhart, B.; Chow, C.W.; Ghonima, M.; Kleissl, J. Solar irradiance forecasting using a ground-based sky imager developed at UC San Diego. Sol. Energy 2014, 103, 502â€“524. [CrossRef] 26. Chow, C.W.; Belongie, S.; Kleissl, J. Cloud motion and stability estimation for intra-hour solar forecasting. Sol. Energy 2015, 115, 645â€“655. [CrossRef] 27. Shakya, S.; Kumar, S. Characterising and predicting the movement of clouds using fractional-order optical ï¬‚ow. IET Image Process. 2019, 13, 1375â€“1381. [CrossRef] 28. Lecun, Y.; Bengio, Y. Convolutional Networks for Images, Speech, and Time Series. In The Handbook of Brain Theory and Neural Networks; MIT Press: Cambridge, MA, USA, 1995; Volume 3361, pp. 255â€“258. 29. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet Classiï¬cation with Deep Convolutional Neural Networks. In Neural Information Processing Systems; ACM: Lake Tahoe, CA, USA, 2012. 30. Ye, L.; Cao, Z.; Xiao, Y.; Li, W. Ground-Based Cloud Image Categorization Using Deep Convolutional Visual Features. In Proceedings of the IEEE International Conference on Image Processing, QuÃ©bec City, QC, Canada, 27â€“30 September 2015; pp. 4808â€“4812. 31. Ye, L.; Cao, Z.; Xiao, Y. DeepCloud: Ground-Based Cloud Image Categorization Using Deep Convolutional Features. IEEE Trans. Geosci. Remote Sens. 2017, 55, 5729â€“5740. [CrossRef] 32. Shi, M.; Xie, F.; Zi, Y.; Yin, J. Cloud detection of remote sensing images by deep learning. In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium, Beijing, China, 10â€“15 July 2016; pp. 701â€“704. 33. Xu, F.; Hu, C.; Li, J.; Plaza, A.; Datcu, M. Special focus on deep learning in remote sensing image processing. Sci. China Inf. Sci. 2020, 63, 140300. [CrossRef] 34. Wang, H.; Klaser, A.; Schmid, C.; Liu, C.-L. Action recognition by dense trajectories. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 20â€“25 June 2011. 35. Bengio, Y.; Simard, P.; Frasconi, P. Learning long-term dependencies with gradient descent is diï¬ƒcult. IEEE Trans. Neural Netw. 1994, 5, 157â€“166. [CrossRef] [PubMed] 36. Srivastava, S.; Lessmann, S. A comparative study of LSTM neural networks in forecasting day-ahead global horizontal irradiance with satellite data. Sol. Energy 2018, 162, 232â€“247. [CrossRef] 37. Qing, X.; Niu, Y. Hourly day-ahead solar irradiance prediction using weather forecasts by LSTM. Energy 2018, 148, 461â€“468. [CrossRef] 38. Ji, S.; Xu, W.; Yang, M.; Yu, K. 3D Convolutional Neural Networks for Human Action Recognition. IEEE Trans. Pattern Anal. Mach. Intell. 2012, 35, 221â€“231. [CrossRef] 39. Zha, S.; Luisier, F.; Andrews, W.; Srivastava, N.; Salakhutdinov, R. Exploiting Image-Trained CNN Architectures for Unconstrained Video Classiï¬cation. arXiv 2015, arXiv:1503.04144. 40. Donahue, J.; Hendricks, L.A.; Rohrbach, M.; Venugopalan, S.; Guadarrama, S.; Saenko, K.; Darrell, T. Long-term recurrent convolutional networks for visual recognition and description. IEEE Trans. Pattern Anal. Mach. Intell. 2016, 39, 677â€“691. [CrossRef] [PubMed] 41. Shi, X.; Chen, Z.; Wang, H.; Yeung, D.-Y.; Wong, W.; Woo, W. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Proceedings of the Neural Information Processing Systems, MontrÃ©al, QC, Canada, 13 June 2015. 42. Ballas, N.; Yao, L.; Pal, C.; Courville, A. Delving Deeper into Convolutional Networks for Learning Video Representations. International Conference on Learning Representations. arXiv 2016, arXiv:1511.06432. 43. Cho, K.; Van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; Bengio, Y. Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation. arXiv 2014, arXiv:1406.1078. 44. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural. Comput. 1997, 9, 1735â€“1780. [CrossRef] 45. Chung, J.; Gulcehre, C.; Cho, K.H.; Bengio, Y. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv 2014, arXiv:1412.3555. Atmosphere 2020, 11, 1151 17 of 17 46. Ioï¬€e, S.; Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv 2015, arXiv:1502.03167. 47. Cuevas, E.; ZaldÃ­var, D.; Perez-Cisneros, M.; Oliva, D. Block-matching algorithm based on diï¬€erential evolution for motion estimation. Eng. Appl. Artif. Intell. 2013, 26, 488â€“498. [CrossRef] 48. Wang, Z.; Bovik, A.; Sheikh, H.R.; Simoncelli, E.P. Image quality assessment: From error visibility to structural similarity. IEEE Trans Image Process 2004, 13, 600â€“612. [CrossRef] 49. Nair, V.; Hinton, G.E. Rectiï¬ed linear units improve restricted boltzmann machines. In Proceedings of the International Conference on Machine Learning, Haifa, Israel, 21 June 2010. 50. Li, Z.; Shen, H.; Cheng, Q.; Liu, Y.; You, S.; He, Z. Deep learning based cloud detection for medium and high resolution remote sensing images of diï¬€erent sensors. ISPRS J. Photogramm. Remote Sens. 2019, 150, 197â€“212. [CrossRef] 51. Bahdanau, D.; Cho, K.; Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv 2014, arXiv:1409.0473. Publisherâ€™s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional aï¬ƒliations. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).","libVersion":"0.3.1","langs":""}
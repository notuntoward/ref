{"path":"lit/lit_sources/Sesia20cnfrmQRcmpr.pdf","text":"Received: 12 September 2019 Revised: 14 October 2019 Accepted: 22 November 2019 DOI: 10.1002/sta4.261 ORIGINAL ARTICLE A comparison of some conformal quantile regression methods Matteo Sesia1 Emmanuel J. Candès1,2 1Department of Statistics, Stanford University, Stanford, 94305, CA, USA 2Department of Mathematics, Stanford University, Stanford, 94305, CA, USA Correspondence Matteo Sesia, Department of Statistics, 390 Jane Stanford Way, Stanford University, Stanford, CA 94305, USA. Email: msesia@stanford.edu Funding information Army Research Office, Grant/Award Number: W911NF-17-1-0304; Division of Mathematical Sciences, Grant/Award Number: DMS 1712800 We compare two recent methods that combine conformal inference with quantile regression to produce locally adaptive and marginally valid prediction intervals under sample exchangeability (Romano, Patterson, & Candès, 2019, arXiv:1905.03222; Kivaranovic, Johnson, & Leeb, 2019, arXiv:1905.10634). First, we prove that these two approaches are asymptotically efficient in large samples, under some additional assumptions. Then we compare them empirically on simulated and real data. Our results demonstrate that the method of Romano et al. typically yields tighter prediction intervals in finite samples. Finally, we discuss how to tune these procedures by fixing the relative proportions of observations used for training and conformalization. Our empirical results suggest that using between 70% and 90% of the data for training often achieves a good balance between minimizing the average width of the predictions intervals and the variability in their practical coverage. KEYWORDS conformal inference, neural networks, quantile regression, random forests 1 INTRODUCTION 1.1 Background and motivation Given a set of n points {(Xi, Yi)} n i=1,with Yi ∈ R and Xi ∈ Rd, we consider the problem of constructing a prediction interval for a new point Yn+1 based on the observed value of Xn+1, assuming only that {(Xi, Yi)} n+1 i=1 are drawn exchangeably from some common distribution PXY.There exists a vast selection of statistical and machine learning algorithms that can provide approximate answers to this question (Papadopoulos, Edwards, & Murray, 2001; Wager, Hastie, & Efron, 2014). However, the uncertainty in any of their predictions cannot be quantified without making strong assumptions and large-sample asymptotic approximations that may not be easily justifiable in applications. Conformal inference (Vovk, Gammerman, & Saunders, 1999; Vovk, Gammerman, & Shafer, 2005; Vovk, Nouretdinov, & Gammerman, 2009; Papadopoulos, Proedrou, Vovk, & Gammerman, 2002; Papadopoulos, Vovk, & Gammermam, 2007; Papadopoulos, Gammerman, & Vovk, 2008; Papadopoulos, 2008; Papadopoulos, Vovk, & Gammerman, 2011; Lei, G'Sell, Rinaldo, Tibshirani, & Wasserman, 2018) addresses this problem by constructing an exact marginal prediction interval ̂C𝛼(Xn+1) such that P[Yn+1 ∈ ̂C𝛼(Xn+1)] ≥ 1 − 𝛼, (1) while relying only on the exchangeability of the n + 1 points. This interval is said to be marginal because all variables in (1) are treated as random, including (Xn+1, Yn+1) and the data used to train ̂C. Therefore, it is not guaranteed that the interval will cover Yn+1 conditional on a particular observed value of Xn+1,or a fixed prediction model ̂C. Despite this limitation, conformal prediction intervals are attractive because their coverage is guaranteed on average regardless of the distribution of the data. Early work on conformal prediction focused on estimating a mean regression function for Y|X andbuilding a fixed-widthbandaroundit (Vovk et al., 1999; Vovk et al., 2005; Vovk et al., 2009; Lei et al., 2018). Even though this strategy produces valid marginal prediction intervals regardless of PY|X, it is clearly designed with a homoscedastic regression model in mind, and it may lead to unnecessarily wide intervals in other cases. Locally adaptive conformal prediction (Papadopoulos et al., 2007; Papadopoulos et al., 2008; Papadopoulos et al., 2011; Lei et al., 2018) goes a step beyond this model by weighting the residuals according to a local estimate of their variance. However, in general, this still cannot achieve the ideal goal of constructing prediction intervals that are as narrow as possible while maintaining coverage. This goal can be stated precisely as follows. Let us denote by q𝛼(xn+1) the 𝛼th quantile of the conditional distribution of Y given Xn+1 = xn+1. Then a desirable oracle prediction interval would be Coracle 𝛼 (Xn+1)=[q𝛼∕2(Xn+1), q1−𝛼∕2(Xn+1)]. (2) Stat. 2020;9:e261. wileyonlinelibrary.com/journal/sta4 © 2020 John Wiley & Sons, Ltd. 1of 8 https://doi.org/10.1002/sta4.261 2of 8 SESIA AND CANDÈS By construction, this is the narrowest symmetric prediction interval that has valid coverage conditional on the value of Xn+1. Here, we say that a prediction interval is symmetric if Yn+1 is equally likely to be smaller or larger than predicted. Unfortunately, the oracle interval in (2) is unachievable in practice because we do not know PY|X. Conformal quantile regression (Romano, Patterson, & Candès, 2019) constructs a practical prediction interval ̂C𝛼 that estimates (2) as closely as possible while satisfying (1) exactly. In this work, we compare theoretically and empirically the method from Romano et al. (2019), which has already been shown to outperform earlier methods in practice(Romano et al., 2019), with a similar approach that was proposed independently in Kivaranovic, Johnson, and Leeb (2019). 1.2 Conformal quantile regression Throughout this paper, we follow the split-conformal approach to conformal inference (Papadopoulos et al., 2002; Papadopoulos, 2008; Lei et al., 2018) adopted in Romano et al. (2019) and Kivaranovic et al. (2019), since it is computationally feasible even with large data. The first step of the conformal quantile regression method in Romano et al. (2019) is to split the data samples into two disjoint subsets, 1 and 2. Lower and upper quantile regression functions, namely, ̂q𝛼∕2, ̂q1−𝛼∕2 ∶ R d → R, are fitted on the observations in 1. Any algorithm can be employed for this purpose; for example, one may rely on linear regression(Koenker & Bassett Jr, 1978), neural networks(Taylor, 2000), or random forests(Meinshausen, 2006). In any case, this algorithm is treated as a black box. The estimated quantile functions are used to compute a conformity score for each i ∈ 2: ECQR i = max {̂q𝛼∕2(Xi)− Yi, Yi − ̂q1−𝛼∕2(Xi)} . (3) Then, with ̂Q1−𝛼(ECQR; 2) defined as the ⌈(1 − 𝛼)(|2| + 1)|⌉-th largest element 1 of {Ei}i∈2 , the conformal prediction interval for Xn+1 is given by ̂CCQR 𝛼 (Xn+1)= [̂q𝛼∕2(Xn+1)− ̂Q1−𝛼(ECQR; 2), ̂q1−𝛼∕2(Xn+1)+ ̂Q1−𝛼(ECQR; 2)] (4) This method is summarized in Algorithm 1 as CQR. It is shown in Romano et al. (2019) that ̂CCQR 𝛼 (Xn+1) has marginal coverage at level 1 − 𝛼. The method described in Kivaranovic et al. (2019) differs from CQR in the choice of the conformity scores, as outlined in Algorithm 1 as CQR-m. Instead of (3), one computes 2 ECQR-m i = max { ̂q𝛼∕2(Xi)− Yi ̂q1∕2(Xi)− ̂q𝛼∕2(Xi) , Yi − ̂q1−𝛼∕2(Xi) ̂q1−𝛼∕2(Xi)− ̂q1∕2(Xi) } , (5) 1For a motivation of this definition, see the result on the empirical quantiles of exchangeable random variables (Lemma 1) in Romano et al. (2019). 2Note that we present CQR-m with a slightly different notation than in Kivaranovic et al. (2019) to facilitate the comparison. SESIA AND CANDÈS 3of 8 where ̂q1∕2 indicates an estimated median regression function obtained with the same black box algorithm as ̂q𝛼∕2 and ̂q1−𝛼∕2. Then the conformal prediction interval for Xn+1 is given by ̂CCQR-m 𝛼 (Xn+1)= [̂q𝛼∕2(Xn+1)− ̂Δ CQR-m 𝛼,lo , ̂q1−𝛼∕2(Xn+1)+ ̂Δ CQR-m 𝛼,up ] ̂Δ CQR-m 𝛼,lo = ̂Q1−𝛼(ECQR-m; 2) [̂q1∕2(Xn+1)− ̂q𝛼∕2(Xn+1)] , ̂Δ CQR-m 𝛼,up = ̂Q1−𝛼(ECQR-m; 2) [̂q1−𝛼∕2(Xn+1)− ̂q1∕2(Xn+1)] . (6) One can show that ̂CCQR-m 𝛼 (Xn+1) also has marginal coverage at level 1 − 𝛼 (Kivaranovic et al., 2019). We also find it interesting to consider a modified version of CQR-m that does not require estimating the regression median.3 This third approach, listed in Algorithm 1 as CQR-r, is based on the following conformity scores: ECQR-r i = max { ̂q𝛼∕2(Xi)− Yi ̂q1−𝛼∕2(Xi)− ̂q𝛼∕2(Xi) , Yi − ̂q1−𝛼∕2(Xi) ̂q1−𝛼∕2(Xi)− ̂q𝛼∕2(Xi) } . (7) The CQR-r prediction intervals are ̂CCQR-r 𝛼 (Xn+1)= [̂q𝛼∕2(Xn+1)− ̂Δ CQR-r 𝛼 , ̂q1−𝛼∕2(Xn+1)+ ̂Δ CQR-r 𝛼 ] (8) ̂Δ CQR-r 𝛼 = ̂Q1−𝛼(ECQR-r; 2) [̂q1−𝛼∕2(Xn+1)− ̂q𝛼∕2(Xn+1)] . It is easy to show that ̂CCQR-r 𝛼 (Xn+1) also attains marginal coverage at level 1 − 𝛼. A proof is omitted because it would be identical to those in Romano et al. (2019) and Kivaranovic et al. (2019). CQR-r is similar in spirit to CQR-m, but it has a more direct interpretation: the conformity scores of CQR-r in (7) weight the distance of Y from the corresponding prediction interval by the inverse width of the interval. Therefore, the conformalization expands or contracts the black box prediction bands proportionally to their width, instead of adding a constant shift as in CQR. Since it is not clear how the regression median q1∕2 should generally be related to the upper and lower 𝛼-quantiles of PY|X, we find this approach slightly more intuitive than CQR-m. 2 THEORETICAL ANALYSIS We show that the output of Algorithm 1 converges to the oracle bands in (2) as n grows, if the black box quantile regression estimates are consistent and a few additional assumptions hold. This can be established for any of the three alternative types of conformity scores discussed in this paper, which are therefore asymptotically equivalent in this sense. Assumption 1 (i.i.d.). The points {(Xi, Yi)} n+1 i=1 are drawn i.i.d. from some distribution PXY. Assumption 2 (Regularity). The probability density of the conformity scores, either in (3), (5), or (7), depending on the conformalization method in Algorithm 1, is bounded away from zero in an open neighbourhood of zero. Assumption 3 (Consistency). For simplicity, denote by n the size of the training data set 1 used to fit the quantile regression functions ̂q. Let X be a new observation independent of 1. Then the assumption is that, for n large enough, P[E[(̂q𝛼∕2(X)− q𝛼∕2(X))2|̂q𝛼∕2, ̂q1−𝛼∕2] ≤ 𝜂n] ≥ 1 − 𝜌n, P[E[(̂q1−𝛼∕2(X)− q1−𝛼∕2(X))2|̂q𝛼∕2, ̂q1−𝛼∕2] ≤ 𝜂n] ≥ 1 − 𝜌n, for some sequences 𝜂n = o(1) and 𝜌n = o(1),as n → ∞. Assumption 3 is similar to that used in Lei et al. (2018) for mean regression estimators, and it is weaker than requiring ̂q𝛼∕2(X) L2 −−→ q𝛼∕2(X) and ̂q1−𝛼∕2(X) L2 −−→ q1−𝛼∕2(X) as n → ∞, by Markov's inequality. Theorem 1. Under Assumptions 1–3, the conformal quantile regression bands ̂C𝛼 obtained with Algorithm 1 satisfy L ( ̂C𝛼(Xn+1) △ Coracle 𝛼 (Xn+1)) = oP(1), as |1|, |2| → ∞. Here, L(A) indicates the Lebesgue measure of the set A,and △ is the symmetric difference operator, i.e., A △ B =(A ∖ B)∪(B ∖ A). The proof of Theorem 1 can be found in the Supporting Information and is inspired by that of Theorem 3.4 in Lei et al. (2018), although the oracle and the conformalization methods considered here are different. Theorem 1 establishes a stronger form of statistical efficiency for conformal quantile regression compared with the result in Lei et al. (2018), which assumes Y = 𝜇(X)+ 𝜖, for some regression function 𝜇,and homoscedastic noise 𝜖. In general, the conformal prediction intervals described in Lei et al. (2018) will not converge to those of our oracle if 3This was first suggested by Yaniv Romano through personal communication. 4of 8 SESIA AND CANDÈS the noise is heteroscedastic, regardless of the consistency of the black box regression estimator ̂𝜇. By contrast, conformal quantile regression is efficient in the sense that, under Theorem 1, the prediction bands converge to those of the oracle, which are the narrowest possible bands achieving conditional coverage. Finally, the asymptotic consistency assumption may be verified theoretically for some specific algorithms under certain conditions, for example, random forests(Meinshausen, 2006). In any case, our result provides some theoretical backing to conformal quantile regression even if the assumptions cannot be verified in practice. It also follows as a corollary that conformal quantile regression has asymptotic conditional coverage, defined as in Lei et al. (2018); the proof of this corollary can be found in the Supporting Information. Definition 1 (Asymptotic conditional coverage). We say that a sequence ̂Cn of random prediction bands has asymptotic conditional coverage at the level 1 − 𝛼 if there exists a sequence of random sets Λn ⊆ R d such that P[X ∈Λn]= 1 − o(1) and sup x∈Λn | | | P[Y ∈ ̂Cn(x)|X = x]−(1 − 𝛼)| | | = oP(1). Corollary 1 (Asymptotic conditional coverage). Under the assumptions of Theorem 1, assuming also for simplicity that Y has a continuous density with respect to the Lebesgue measure, conformal quantile regression bands satisfy the asymptotic conditional coverage property in Definition 1. Despite being asymptotically efficient under Assumptions 1–3, the three conformalization methods in Algorithm 1 typically perform differently with finite data, as discussed next. 3 EMPIRICAL COMPARISON 3.1 Black box quantile regression In the following, we utilize two alternative black box quantile regressors, implemented and trained as in Romano et al. (2019). The first procedure is based on quantile regression forests (Meinshausen, 2006). We fit 1,000 trees and set the other tuning parameters equal to their default values. The second black box is a neural network (Taylor, 2000) with three fully connected layers and ReLU non-linearities. We have chosen this design, which is slightly different from that in Kivaranovic et al. (2019), because it leads to conformal prediction intervals that are tighter than those reported in Kivaranovic et al. (2019). If the estimated lower and upper quantiles overlap, which may sometimes occur, we swap them. The nominal level of the black boxes is tuned so that their empirical coverage, estimated by cross-validation, is approximately equal to 1 − 2𝛼.We have observed that this heuristic generally leads to tighter conformal intervals compared with those obtained by directly requesting the black boxes to estimate q𝛼∕2 and q1−𝛼∕2; for empirical evidence, see Section 3.3 and Figures S2–S11. Throughout this section, we set 𝛼 = 0.1,while the results of additional experiments exploring different values of 𝛼 are shown in Figure S1. 3.2 Experiments with artificial data We begin by considering the same experiment based on artificial data as in Kivaranovic et al. (2019). We simulate X ∼ Unif([0, 1]d),for d = 100, and Y ∈ R from Y = f(𝛽′X)+ 𝜖√1 +(𝛽′X)2, (9) where f(x)= 2sin(𝜋x)+ 𝜋x, 𝛽′ =(1, 1, 1, 1, 1, 0, … , 0) and 𝜖 is independent standard Gaussian noise. Here, we have access to a natural benchmark: the oracle that knows PY|X exactly. It follows from (9) that the expected width of the oracle prediction bands is E[q1−𝛼∕2(X)− q𝛼∕2(X)] = 2 E√1 +(𝛽′X)2Q1−𝛼∕2(𝜖)≈ 8.91, where Q𝛼(𝜖) is the 𝛼-quantile of the standard Gaussian distribution. The performances of CQR, CQR-m, and CQR-r are compared in Figure 1 as a function of the number of data points n. The proportion of observations used to train the black box is 3∕4, as in Kivaranovic et al. (2019). The coverage and average width of the prediction bands is evaluated on an independent test set of size 20,000. The experiment is repeated for 100 independent realizations of the data and of the test set. The width and coverage of the conformal prediction bands approach those of the oracle as the sample size increases. This suggests that the estimated black box quantiles may be approximately consistent. However, CQR typically produces narrower bands compared with the other methods. 3.3 Experiments with real data We now apply Algorithm 1 on the same data analysed in Romano et al. (2019) and Kivaranovic et al. (2019).4 Some details about these data sets and information on the corresponding sources are summarized in Table 1. For all data sets except homes, we randomly hold out 20% of the samples for testing. Then we divide the remaining observations into two disjoint sets, 1 and 2, to train the black box and conformalize the prediction bands, respectively. The response variables Y are standardized as in Romano et al. (2019) and Kivaranovic et al. (2019). We explore 4We have excluded the X-ray data in Kivaranovic et al. (2019) because we are unsure of how to replicate the pre-processing. SESIA AND CANDÈS 5of 8 FIGURE 1 Conformal prediction bands obtained with different conformalization methods on artificial data, as a function of the sample size. The dotted line on the left indicates the width of the oracle predictions. The dotted line on the right indicates the nominal coverage level (90%) TABLE 1 Data sets for our empirical analysis, with numbers of samples (n)and features (d) Name Description nd Source bike Bike sharing 10,886 18 Bike sharing data set (2013) bio Properties of protein structures 45,730 9 Physicochemical properties of protein tertiary structure data set (2013) blog Blog feedback 52,397 280 BlogFeedback data set (2014) community Community and crime 1,994 100 Communities and crime data set (2009) concrete Concrete compressive strength 1,030 8 Concrete compressive strength data set (2007) facebook 1 Facebook comment volume 40,948 53 Facebook comment volume data set (2016) facebook 2 Facebook comment volume 81,311 53 Facebook comment volume data set (2016) homes Home sale prices in King County 21,613 19 House sales in King County data set (2016) meps 19 Medical expenditure survey 15,785 139 Medical expenditure panel survey (panel 19) data set (2018) meps 20 Medical expenditure survey 17,541 139 Medical expenditure panel survey (panel 20) data set (2018) meps 21 Medical expenditure survey 15,656 139 Medical expenditure panel survey (panel 21) data set (2018) star Student–teacher achievement ratio 2,161 39 Achilles et al. (2008) Dataset Width CQR CQR-r CQR-m bike 0.503 (0.024) 0.520 (0.024) 0.521 (0.023) bio 0.995 (0.037) 1.048 (0.049) 1.114 (0.019) blog 1.269 (0.040) 1.462 (0.148) 1.351 (0.109) community 1.461 (0.116) 1.548 (0.066) 1.617 (0.063) concrete 0.378 (0.056) 0.387 (0.063) 0.384 (0.059) facebook-1 1.117 (0.048) 1.188 (0.043) 1.164 (0.127) facebook-2 1.110 (0.051) 1.172 (0.066) 1.116 (0.066) homes 0.477 (0.013) 0.491 (0.013) 0.492 (0.013) meps-19 2.300 (0.164) 2.349 (0.175) 2.442 (0.364) meps-20 2.309 (0.121) 2.467 (0.313) 2.467 (0.168) meps-21 2.201 (0.076) 2.273 (0.119) 2.343 (0.337) star 0.179 (0.006) 0.180 (0.010) 0.181 (0.006) Note. The corresponding coverage is reported in Table 3. The smallest value on each row is written in bold. TABLE 2 Average width (and standard deviation) of the conformal quantile regression prediction bands obtained with different conformalization methods on the data sets listed in Table 1 different values of the fraction of samples used for training: |1| = 𝛾n,with 𝛾 ∈{0.1, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98}. We are interested in this comparison because different values are used in Romano et al. (2019) and Kivaranovic et al. (2019): 𝛾 = 0.5 and 𝛾 = 0.75, respectively. In the case of the homes data, we follow in the footsteps of Kivaranovic et al. (2019): First, we randomly hold out 3,613 test samples; then we train the black box on 15,000 samples and conformalize on the remaining 3,000. All experiments are repeated 10 times, starting from the data splitting. The test-set performances of CQR, CQR-m, and CQR-r are summarized in Tables 2 and 3. These quantities correspond to the optimal choices of black box and value of the hyperparameter 𝛾, defined as those leading to the shortest intervals on average, separately for each algorithm. The CQR method consistently produces the narrowest valid prediction bands, while CQR-m and CQR-r are often comparable. The performances obtained with different black boxes and values of 𝛾 are reported in Figure 2 for the community data and in Figures S2–S11 for the other data sets. The results are shown as a function of 𝛾, which affects the average width of the prediction intervals as well as their variability. If 𝛾 is small, the prediction intervals are not sufficiently adaptive because the black box cannot estimate the regression quantiles accurately. Larger values of 𝛾 may lead to tighter predictions on average but at the cost of increased variability in the conditional coverage. In fact, the conditional coverage for new observations given the data may be lower than the expected marginal level, especially when 𝛾 is very close to oneand thesamplesizeisnot very large. Theempirical results inFigure2andinFigures S2–S11 suggest that fixing 𝛾 ∈[0.7, 0.9] achieves a 6of 8 SESIA AND CANDÈS TABLE 3 Average coverage (and standard deviation) of the prediction bands in Table 2 Dataset Coverage CQR CQR-r CQR-m bike 0.899 (0.012) 0.901 (0.012) 0.900 (0.012) bio 0.891 (0.012) 0.893 (0.016) 0.895 (0.008) blog 0.905 (0.003) 0.901 (0.007) 0.903 (0.004) community 0.896 (0.025) 0.899 (0.025) 0.902 (0.017) concrete 0.875 (0.061) 0.879 (0.061) 0.877 (0.059) facebook-1 0.901 (0.006) 0.898 (0.004) 0.902 (0.002) facebook-2 0.900 (0.003) 0.900 (0.002) 0.900 (0.002) homes 0.902 (0.009) 0.904 (0.009) 0.904 (0.009) meps-19 0.902 (0.008) 0.902 (0.007) 0.900 (0.011) meps-20 0.897 (0.004) 0.898 (0.004) 0.899 (0.006) meps-21 0.899 (0.008) 0.898 (0.009) 0.897 (0.009) star 0.905 (0.024) 0.904 (0.024) 0.903 (0.020) FIGURE 2 Conformal prediction bands obtained with different black boxes and conformalization methods on the community data set from Table 1. The dotted line in the lower plots indicates the nominal coverage level (90%). A different black box is considered in each column. The vertical axis in the upper panels is discontinuous to facilitate the visualization of values on different scales reasonable compromise for all data sets analysed in this paper. More explicitly, we see that larger values of 𝛾 typically yield shorter intervals, but there is often little improvement between 0.7 and 0.9, while increasing 𝛾 above 0.9 usually significantly increases the variance of the conditional coverage. Finally, we note that our recommendation is consistent with the choice in Kivaranovic et al. (2019). TheCQR-m method sometimesproduces verywideintervals becausethe denominator in(5) canbeclose to zero (weaddeda small constant to prevent overflowing). An example is visible in the second plot in Figure 2, where some of the CQR-m prediction intervals based on a random forest black box are extremely large (hence the discontinuous vertical axis in Figure 2) when 𝛾 ≥ 0.9. The CQR-r method is less susceptible to this problem because the denominator in the conformity scores in (7) is larger. SESIA AND CANDÈS 7of 8 4 CONCLUSION In this paper, we have strengthened the case for conformal quantile regression as a method to obtain valid marginal prediction intervals that are adaptive to heteroscedasticity, by proving that it is asymptotically efficient in large samples if the quantile regression estimates are consistent. The empirical comparison of three alternative conformity scores has shown that those proposed in Romano et al. (2019) are preferable because they typically lead to shorter prediction intervals in practice. Even though we have only explicitly considered symmetric intervals for simplicity,it is straightforward to generalize these methods to asymmetric intervals and conformity scores(Romano et al., 2019). Finally, we have highlighted a bias-variance trade-off in the choice of the proportion of data points used to train the black box quantile regressors. Our empirical results show that it is usually better to invest more of the available data (between 70% and 90%, indicatively) to train the black box than to conformalize the predictions. We hope that these results will be helpful to practitioners and may inspire others to develop even more powerful variations of conformal quantile regression. ACKNOWLEDGEMENTS M.S. and E.C. are supported by the National Science Foundation under Grant DMS 1712800. E.C. is also supported by the Army Research Office under Grant W911NF-17-1-0304. We thank Yaniv Romano for helpful discussions, during which he suggested the CQR-r method. DATA AVAILABILITY STATEMENT The data used in this work are openly available from the sources listed in Table 1. The code to reproduce the numerical experiments in Section 3 can be found on https://github.com/msesia/cqr-comparison. ORCID Matteo Sesia https://orcid.org/0000-0001-9046-907X REFERENCES Achilles, C. M., Bain, H. P., Bellott, F., Boyd-Zaharias, J., Finn, J., Folger, J., ..., & Word, E. (2008). Tennessee's student teacher achievement ratio (STAR) project data set. Harvard Dataverse, Accessed: July, 2019. Bike sharing data set (2013). https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset, Accessed: July, 2019. BlogFeedback data set (2014). https://archive.ics.uci.edu/ml/datasets/BlogFeedback, Accessed: July, 2019. Communities and crime data set (2009). http://archive.ics.uci.edu/ml/datasets/communities+and+crime, Accessed: July, 2019. Concrete compressive strength data set (2007). http://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength, Accessed: July, 2019. Facebook comment volume data set (2016). https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset, Accessed: July, 2019. House sales in King County data set (2016). https://www.kaggle.com/harlfoxem/housesalesprediction/metadata, Accessed: August, 2019. Kivaranovic, D., Johnson, K. D., & Leeb, H. (2019). Adaptive, distribution-free prediction intervals for deep neural networks. arXiv preprint arXiv:1905.10634. Koenker, R., & Bassett Jr, G. (1978). Regression quantiles. Econometrica, 46(1), 33–50. Lei, J., G'Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523), 1094–1111. Medical expenditure panel survey (panel 19) data set (2018). https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber= HC-181, Accessed: July, 2019. Medical expenditure panel survey (panel 20) data set (2018). https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber= HC-181, Accessed: July, 2019. Medical expenditure panel survey (panel 21) data set (2018). https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber= HC-192, Accessed: July, 2019. Meinshausen, N. (2006). Quantile regression forests. Journal of Machine Learning Research, 7, 983–999. Papadopoulos, H. (2008). Inductive conformal prediction: Theory and application to neural networks. In Fritzsche, P. (Ed.), Tools in artificial intelligence. Rijeka: IntechOpen, 315–330. Papadopoulos, G., Edwards, P. J., & Murray, A. F. (2001). Confidence estimation methods for neural networks: A practical comparison. IEEE Transactions on Neural Networks, 12(6), 1278–1287. Papadopoulos, H., Gammerman, A., & Vovk, V. (2008). Normalized nonconformity measures for regression conformal prediction. In Proceedings of the 26th IASTED international conference on artificial intelligence and applications, AIA '08, ACTA Press, Anaheim, CA, USA, pp. 64–69. Papadopoulos, H., Proedrou, K., Vovk, V., & Gammerman, A. (2002). Inductive confidence machines for regression. In European Conference on Machine Learning, Berlin, Heidelberg: Springer, pp. 345–356. Papadopoulos, H., Vovk, V., & Gammermam, A. (2007). Conformal prediction with neural networks. In 19th IEEE international conference on tools with artificial intelligence (ICTAI), 2, Patras, Greece, pp. 388–395. Papadopoulos, H., Vovk, V., & Gammerman, A. (2011). Regression conformal prediction with nearest neighbours. Journal of Artificial Intelligence Research, 40(1), 815–840. Physicochemical properties of protein tertiary structure data set (2013). https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+ Tertiary+Structure, Accessed: July, 2019. 8of 8 SESIA AND CANDÈS Romano, Y., Patterson, E., & Candès, E. J. (2019). Conformalized quantile regression. arXiv preprint arXiv:1905.03222. Taylor, J. W. (2000). A quantile regression neural network approach to estimating the conditional density of multiperiod returns. Journal of Forecasting, 19(4), 299–311. Vovk, V., Gammerman, A., & Saunders, C. (1999). Machine-learning applications of algorithmic randomness. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML '99, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, pp. 444–453. Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic learning in a random world. Berlin, Heidelberg: Springer-Verlag. Vovk, V., Nouretdinov, I., & Gammerman, A. (2009). On-line predictive linear regression. Annals of Statistics, 37(3), 1566–1590. Wager, S., Hastie, T., & Efron, B. (2014). Confidence intervals for random forests: The jackknife and the infinitesimal jackknife. Journal of Machine Learning Research, 15(1), 1625–1651. SUPPORTING INFORMATION Additional supporting information may be found online in the Supporting Information section at the end of the article. How to cite this article: Sesia M, Candès E. A comparison of some conformal quantile regression methods. Stat. 2020;9:e261. https://doi.org/10.1002/sta4.261","libVersion":"0.3.1","langs":""}
{"path":"lit/lit_notes_OLD_PARTIAL/Instantinopaul24MambaVsTransformers.pdf","text":"3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/21 Sort By: Best | Discussion Heard all the buzz about Mamba, the new kid on the sequence modeling block. Supposedly it's faster, handles longer sequences better, and even outperforms Transformers on some tasks. But is it really a throne-stealer or just another ï¬‚ash in the pan? My perception: Strengths: Mamba boasts eï¬ƒcient memory usage, linear scaling with sequence length, and impressive performance in language and DNA modeling. Plus, it ditches the attention mechanism, potentially paving the way for faster inference. Weaknesses: Still early days, so Mamba's long-term stability and performance across diverse tasks remain to be seen. And while it doesn't need attention, its state space approach might be trickier to grasp for some folks. To the AI aï¬cionados out there, is Mamba just the next shiny toy, or a genuine paradigm shift in sequence modeling? Will it dethrone the mighty Transformer, or coexist as a specialized tool? Let's hear your thoughts! https://arxiv.org/abs/2312.00752 ï…¯ 103 Comments ïŠ‚ Share ïˆ© 306 ï ï†— Posted by u/Instantinopaul 3 months ago ïˆ™ [D] So, Mamba vs. Transformers... is the hype real? Comment as wreckable Search commentsï‰º What are your thoughts? ï„­ ï‡’ ï‡¦ ïŠ ï…¢ ïŠ¢ ïŠ” ïŠ¶ ï‡¨ ïˆ© Markdown Mode r/MachineLearning 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/21 View discussions in 1 other community 314kabinet Â· 3 mo. ago Iâ€™ve read the paper. The S6 layers of Mamba have a memory which they modify with each new token. All state space modeling nets work this way but the advantage here is that S6 has control over how each token is remembered (if at all), as opposed to just trying to memorize a compressed version of the entire input sequence. This means it can in theory hold on to important info from a million tokens ago while only keeping short-term details for as long as theyâ€™re needed. Whether or not it works like that in practice for practical-sized models remains to be seen, but even if it doesnâ€™t, more sophisticated versions of the memory state will be developed. Intuitively it makes sense for a system to accept an input one token at a time and have an internal memory (instead of just taking in the entire uncompressed sequence in one go as attention does), so Iâ€™m optimistic. 214 ï…¯ Reply Share ïˆ©ï ï†— ArnoF7 Â· 3 mo. ago Intuitively I think this makes sense. I do have one question tho. I havenâ€™t dived deep into the mamba paper but whatâ€™s the diï¬€erence between mamba and LSTM and the likes then? Is Mamba more hardware friendly like they claim in the paper, compared to LSTM? 52 ï…¯ Reply Share ïˆ©ï ï†— beariï¬c Â· 3 mo. ago The diï¬€erences as I understand it: Mamba has a linear activation function between each hidden state, while LSTM and RNN have a nonlinearity, which makes backpropagation through time a lot more stable for Mamba. Mamba can still be calculated in one forward pass through a parallel scan (preï¬x sum) operation, compared to e.g. RNNs and LSTMs where we need to calculate the previous timestep before we can calculate the next. The Mamba authors developed a hardware-aware algorithm in the same vein als FlashAttention which further improves eï¬ƒciency. The authors mention that RNNs without time-wise nonlinearities such as QRNN are the most similar to Mamba, but those do not use state expansion or selective B and C params, and they use a heuristic gating mechanism, while the parameterizations and initializations of Mamba are based on principled SSM theory. 85 ï…¯ Reply Share ïˆ©ï ï†— 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/21 Cryptheon Â· 3 mo. ago What's the impact of having no non-linearities? The whole point of performance eï¬ƒcient deep learning is to have non-linearities to map to a more disentangled data space. How does getting rid of this assumption lead to a similar performing NN? 22 ï…¯ Reply Share ïˆ©ï ï†— beariï¬c Â· 3 mo. ago No time-wise nonlinearities, there are still nonlinearities between the mamba layers 46 ï…¯ Reply Share ïˆ©ï ï†— sortphotos Â· 3 mo. agoï†¦ extracoffeeplease Â· 3 mo. ago From a math perspective, they do weird stuï¬€ using 'downprojecting on orthogonal functions' which isn't even learned which is strange as hell. Spoiler alert: Albert Gu never really explains on YouTube what the gist of this is or why it works, so you meed need to read the HiPPO paper. I don't understand it yet. Anyway, due to this they can do a compression of history according to some measure (like exponential decay L2 error). Because of that they can do without attention, and the network is simultaneously interpretable as CNN and an RNN due to black magic stuï¬€, and this is great. Parallel training like a CNN, superfast online inference like an RNN, and super long contexts because it grows linearly, not quadratically. 8 ï…¯ Reply Share ïˆ©ï ï†— PsecretPseudonym Â· 3 mo. ago Â· edited 3 mo. ago It becomes more natural if youâ€™re familiar with this area of mathematics. Itâ€™s similar to a Fourier decomposition; youâ€™re projecting some original function from its original domain into a new space whose coordinates are given in terms of linear coeï¬ƒcients of orthogonal basis functions. They apply this transformation but with Legendre polynomials as a sort of convolutional kernel operation for the â€œstructureâ€ of S4. Most of the math will probably be more familiar for people coming from signal processing (like radio/electrical engineers), some areas of physics, control/feedback systems, etc. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/21 You can think about it like doing a regression of a target variable against some set of analytically derived special input functions. Itâ€™s just that these special functions are deï¬ned such that they are linearly independent and have some other nice properties. Then, in this space, each point is represented via a set of coordinates for a functional approximation to its entire history. You can then try to predict how the sequence progresses almost like a set of diï¬€erential additive updates in this space from one time step to the next. You then can decompose those updates as some matrix operation on the previous state plus some matrix operation of the new raw inputs mapped into this space (or directly). Think of it like creating an embedding of the entire history at every single point, then trying to predict how that embedding evolves from one time step to the next in the embedding space, then mapping that back to make a prediction for the next time step at each point. This is recurrent in the space of the structured decomposition, and can be done with just linearly operators in that space. Itâ€™s kind of like old school kernel trick methods where youâ€™re doing a nonlinear mapping the original data into some new space where then the recurrent relation from one time step to the next is then a linear function and so can be computed much more eï¬ƒciently, then mapped back. Thatâ€™s S4 (but not Mamba) went a step further by using the FFT and inverse FFT and the convolution theorem to make it so you can essentially compute convolutions over time via just products in the frequency domain. Mamba goes a step further in making the same evolution of the structured representation itself time-varying depending on the state and input at each step, which means you have to actually sequentially compute that I believe. Theyâ€™re just then doing so in a cache-aware and hardware optimized way to make it fast. Honestly RNNs were always just dumb for having used exponential forgetting, and LSTMs were kind of ham-ï¬sted. The idea of using linear time invariant recurrent functions in the space of the evolution of the orthogonal basis coeï¬ƒcients makes way more sense. 17 ï…¯ Reply Share ïˆ©ï ï†— extracoffeeplease Â· 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/21 You can then try to predict how the sequence progresses almost like a set of diï¬€erential additive updates in this space from one time step to the next. You then can decompose those updates as some matrix operation on the previous state plus some matrix operation of the new raw inputs mapped into this space (or directly). Thank you for the detailed explanation! I can deal with projecting functions onto some basis, however how the state matrix with rows like [1 3 5 7 etc] comes into play I do not fully understand. Is it coupled to the base functions? Another question: In the diagram of HiPPO in an RNN, what is going on, speciï¬cally where is the learning going on? Apparently not in the HiPPO operator? What does the dashed line linking c_t-1 with hippo operator represent? 3 ï…¯ Reply Share ïˆ©ï ï†— PsecretPseudonym Â· 3 mo. ago Â· edited 3 mo. ago Just glad if it was helpful! For a more detailed explanation, youâ€™ll want to review ï¬gure 1 and section 2.1 of this paper. 1 ï…¯ Reply Share ïˆ©ï ï†— extracoffeeplease Â· 2 mo. ago Wasn't aware of this paper, thanks! 2 ï…¯ Reply Share ïˆ©ï ï†— thntk Â· 3 mo. ago If I understand what you said correctly, they tried \"more complicated\" Legendre polynomials ï¬rst, then went back to \"simpler\" FFT in S4? Your explanation of the linear recurrent in another (non-linear?) space is cool, it makes a lot of sense. However, RNNs are quite versatile and can do the same thing (with learned instead of ï¬xed decompositions). For example, the RNN cell can be 2-layered, with layer 1 nonlinear (input transform), layer 2 linear (recurrent output), recurrent connection only going back to layer 2 (thus linear recurrent). I think Mamba develops on some ideas of 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 6/21 other RNNs variants, including LSTM (input dependent transform vs. input gate). 2 ï…¯ Reply Share ïˆ©ï ï†— nedw Â· 3 mo. ago I think the convolutional training is speciï¬c to S4 and is lost as part of making the state transition matrix conditional on the input data. The second half of the mamba paper is about making up for the loss of the eï¬ƒciency when doing convolution. I believe I saw the video youâ€™re referring to a few weeks ago and I think itâ€™s really good as a background for understanding these models but Iâ€™m struggling to form a full integrated picture myself. 3 ï…¯ Reply Share ïˆ©ï ï†— haukzi Â· 3 mo. ago Using ï¬xed or non-learned basis functions is pretty standard such as when using fourier or polynomial bases, you don't want to learn that since then you run the risk of your basis not being a basis anymore. For a recent example see PadÃ© units and the PadÃ© activation functions paper where they compare both. The whole \"interpretable as a CNN and as an RNN\" is undergrad fourier and numerical analysis. 1 ï…¯ Reply Share ïˆ©ï ï†— audiencevote Â· 3 mo. ago Mamba has a linear activation function between each hidden state, while LSTM and RNN have a nonlinearity, which makes backpropagation through time a lot more stable for Mamba. nitpick: LSTM has a linear activation inside its cell, too. It's only the gates that are nonlinear. 6 ï…¯ Reply Share ïˆ©ï ï†— swfsql Â· 3 mo. ago I'm new to ML let alone Mamba, but I believe some of the information is incorrect or incomplete. The linearity makes it possible to adjust the weights such that backprop through time is more stable - It's not that it's automatically more stable by being linear. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 7/21 S4 could in one batched forward call do a parallel scan, based on convolutions. S6 (Mamba) can't because now the input interferes more thoroughly in the internal weights, but they still made it go fast thanks to cuda kernel optimizations. 4 ï…¯ Reply Share ïˆ©ï ï†— beariï¬c Â· 3 mo. ago S4 does not use a parallel scan, but like you say a convolution. The parallel scan is used in mamba precisely because the weights are now input dependent, however the calculation is not sequentially dependent but associative across timesteps, so it can still be calculated in parallel. A parallel scan is just a lot less hardware eï¬ƒcient than a convolution, so they implemented the hardware-aware algorithm. 6 ï…¯ Reply Share ïˆ©ï ï†— intentionallyBlue Â· 3 mo. ago Afaik the diï¬€erence is that these modern state space models can be run in parallel over the sequence dim (like a transformer) which is important for eï¬ƒcient training. This can't be done in any obvious way in an LSTM. 21 ï…¯ Reply Share ïˆ©ï ï†— vman512 Â· 3 mo. ago If you are referring to being able to run the recurrent model as a convolution, that only works for linear + time invariant models, which was true for S4, but went away in Mamba. 7 ï…¯ Reply Share ïˆ©ï ï†— steveofsteves Â· 3 mo. agoï†¦ AdditionAlone3851 Â· 3 mo. ago https://www.reddit.com/r/MachineLearning/comments/18iph6a/d_can_someone_d escribe_how_the_ssm_in_mamba_is/ I have no idea but this might help 4 ï…¯ Reply Share ïˆ©ï ï†— Top-Smell5622 Â· 3 mo. ago But isnâ€™t it also a disadvantage that the model has to decide what to keep from a token before having seen all future tokens? (Same as for RNNs) 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 8/21 6 ï…¯ Reply Share ïˆ©ï ï†— 314kabinet Â· 3 mo. ago It is. Any network where tokens come in one at a time gets one opportunity to do whatever it needs with that token and then thatâ€™s it. 7 ï…¯ Reply Share ïˆ©ï ï†— Linooney Â· 3 mo. ago Â· edited 2 mo. ago Researcher I assume you could do a bidirectional SSM like you do with biLSTMs. 2 ï…¯ Reply Share ïˆ©ï ï†— graphicteadatasci Â· 3 mo. ago Would it have to be bidirectional? Couldn't we just have the model read the text multiple times? Like an MI would do (meat intelligence). 2 ï…¯ Reply Share ïˆ©ï ï†— VelveteenAmbush Â· 3 mo. ago This means it can in theory hold on to important info from a million tokens ago while only keeping short-term details for as long as theyâ€™re needed. I guess this would predict that transformers will win if your use case is an Instruct- tuned model where you give it a long piece of text and then asking speciï¬c questions about it? Like if you paste in a chapter from Harry Potter and then ask it to identify the ï¬rst split inï¬nitive or something, it wouldn't have known from the start to remember when there is a split inï¬nitive. Whereas if you put the question at the start of the prompt and then pasted the text, it would? Not claiming that this constitutes a critical weakness or anything, just want to see if I'm understanding the strengths and limitations correctly. 7 ï…¯ Reply Share ïˆ©ï ï†— 314kabinet Â· 3 mo. ago I think so. This exact thing (putting the question before va after) is an issue with RWKV, another recurrent architecture. Probably an issue with recurrent architectures in general. Works for humans too! 9 ï…¯ Reply Share ïˆ©ï ï†— VelveteenAmbush Â· 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 9/21 Oh for sure. A transformer is like a human who can go back and review the text with each question. I guess Mamba and RWKV are like a human who only gets one read-through. 5 ï…¯ Reply Share ïˆ©ï ï†— 314kabinet Â· 3 mo. ago Itâ€™s easy to give them another read through too! 5 ï…¯ Reply Share ïˆ©ï ï†— prumf Â· 2 mo. ago Thatâ€™s what I was wondering. Transformers are expensive because of attention. They read every single word of the book at the same time, and look how every other word compares go it, which is complete madness computation-wise. Humans on the other hand read one word at a time, update internal knowledge, and go back if some information was missed or is suddenly deemed relevant. For really eï¬ƒcient systems, we need something that can go back-and- forth as required, as many times as needed. 1 ï…¯ Reply Share ïˆ©ï ï†— prumf Â· 2 mo. ago But maybe that isnâ€™t necessarily more eï¬ƒcient, as computers are really good ad doing things in parallel, whereas humans cannot really multitask. 2 ï…¯ Reply Share ïˆ©ï ï†— PsecretPseudonym Â· 3 mo. ago I donâ€™t think thatâ€™s necessarily the case. The model will have been trained to use the potentially gigabytes of retained state information to store and forward the relevant information as needed. Itâ€™s just storing the history of the sequence in a far more eï¬ƒcient and highly structured way, and it will have been trained to very thoughtfully and eï¬ƒciently retain recent information nearly perfectly and historical information if and when it may be relevant again or relates to recent information. Think of it like this: 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 10/21 A transformer will need and work from the exact raw sequence data up to a history of N. The transformer must then truncate and completely 100% forget anything beyond N tokens ago. The state space model instead is eï¬ƒciently encoding and storing the relevant information, and predicting what might still be relevant, so holding onto it. Also, because things that are still relevant are inherently related in some way, that means it can really cleverly and eï¬ƒciently compress and store that history in an eï¬ƒcient and structured way. So, rather than 100% forgetting via truncating beyond a given sequence length, it is incrementally holding onto whatever could potentially be relevant in an eï¬ƒcient way. If it doesnâ€™t retain information you might need later, that would be a sign that it simply hasnâ€™t been well trained. If itâ€™s well trained and allowed to be very large, it should virtually always still have what you need. 5 ï…¯ Reply Share ïˆ©ï ï†— EmbarrassedHelp Â· 3 mo. agoï†¦ danysdragons Â· 3 mo. ago What happens if a token from near the beginning is irrelevant across the ï¬rst 1,000,000 tokens and forgotten, but suddenly becomes relevant again with the next tokens? Can it be unforgotten? 1 ï…¯ Reply Share ïˆ©ï ï†— Honest_Science Â· 2 mo. ago Can you unforget? 1 ï…¯ Reply Share ïˆ©ï ï†— DigThatData Â· 3 mo. ago Researcher i've heard promising results from colleagues doing casual experiments. if SSMs have the potential people think they do, we should see some interesting papers popping up between now and April. if no particularly interesting results pop up by April/May, I'd predict SSMs aren't going to eat Transformer's lunch. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 11/21 29 ï…¯ Reply Share ïˆ©ï ï†— idontcareaboutthenam Â· 3 mo. ago If I may ask, why isn't RWKV similarly hyped? Isn't it also linear in sequence length and parallel in computation? 34 ï…¯ Reply Share ïˆ©ï ï†— vatsadev Â· 3 mo. ago It does most of the things, but theres a paper for mamba, which makes it easier, one codebase, versus spread out repos, Mambas newer, and also extrapolates longer, with mamba work from 256-> 1m token ctx len, and rwkv double the trained ctx Also I'm guessing TriDao & albert gu are well known vs rwkv being random discord come together? 40 ï…¯ Reply Share ïˆ©ï ï†— JustOneAvailableName Â· 3 mo. ago RWKV also changed quite a lot in various versions. I frankly have no clue what the current idea is 18 ï…¯ Reply Share ïˆ©ï ï†— vatsadev Â· 3 mo. ago It's somewhere in v5.2 and v6 8 ï…¯ Reply Share ïˆ©ï ï†— Disastrous_Elk_6375 Â· 3 mo. ago I frankly have no clue what the current idea is At some point they started adding attention to it, if I'm not mistaken :) 5 ï…¯ Reply Share ïˆ©ï ï†— vatsadev Â· 3 mo. ago No, still pure linear, but it is getting more like mamba 1 ï…¯ Reply Share ïˆ©ï ï†— vman512 Â· 3 mo. ago RWKV has a paper too 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 12/21 8 ï…¯ Reply Share ïˆ©ï ï†— vatsadev Â· 3 mo. ago That's v4, pretty old now 9 ï…¯ Reply Share ïˆ©ï ï†— themiro Â· 3 mo. ago SSMs perform better and also have a cleaner tradeoï¬€ between token independence and token awareness 8 ï…¯ Reply Share ïˆ©ï ï†— currentscurrents Â· 3 mo. agoï†¦ heuristic_al Â· 3 mo. ago My take is that any ï¬xed memory scheme will eventually suï¬€er at long contexts vs true attention. And correct me if I'm wrong, but true attention is actually more computationally eï¬ƒcient than Mamba for sequences less long than the hidden width of the network (4096 for llama). So Mamba only has this region of context lengths where it could actually be better. 7 ï…¯ Reply Share ïˆ©ï ï†— Dump7 Â· 3 mo. ago Looking at this thread, there is a lot I need to read and understand. Love this thread...! 11 ï…¯ Reply Share ïˆ©ï ï†— Gody_Godee Â· 3 mo. ago intelligent is O(n2). prove me wrong 6 ï…¯ Reply Share ïˆ©ï ï†— ItsJustMeJerk Â· 2 mo. ago Human intelligence certainly isn't. 5 ï…¯ Reply Share ïˆ©ï ï†— prumf Â· 2 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 13/21 Counter example : humans. You donâ€™t compare every word of the book to every other word. You build internal knowledge like RNN and might need to do multiple reads to understand everything. 2 ï…¯ Reply Share ïˆ©ï ï†— Gody_Godee Â· 1 mo. ago *general intelligent 1 ï…¯ Reply Share ïˆ©ï ï†— FallMindless3563 Â· 3 mo. ago FWIW Iâ€™ve tried out a few of the Mamba models natural language tasks such as Question Answering, and the results were not even close to larger transformers yet. I tried everything from prompt engineering to ï¬ne-tuning the models. This could be due to parameter count or lack of pre-training data for the mamba models that were released. I heard the authors say these early versions of Mamba are very much a proof of concept, and weâ€™d need to train larger parameter count and on more data to be competitive with the transformers that are out there today. On SQuAD Mamba-2.8b with a 3-shot prompt only got 7.5% accuracyâ€¦ whereas models like Mistral 7B Iâ€™ve seen get 70%+ with zero-shot. I documented my process and ï¬ndings here if anyone is interested ğŸ‘‡ https://blog.oxen.ai/practical-ml-dive-how-to-train-mamba-for-question-answering/ 12 ï…¯ Reply Share ïˆ©ï ï†— graphicteadatasci Â· 3 mo. ago Very cool blog post. But when you say Mamba vs Mistral you aren't comparing two models trained on the same data set, are you? Data is more important than architecture imho. 6 ï…¯ Reply Share ïˆ©ï ï†— FallMindless3563 Â· 3 mo. ago Correct! I was pointing out that the current iteration of Mamba is not at all useable for NLP. It needs to be scaled up in parameters and data before we can really do an apples to apples comparison to Transformers that are useable today. 3 ï…¯ Reply Share ïˆ©ï ï†— Jattoe Â· 1 mo. agoï†¦ 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 14/21 seraine Â· 3 mo. agoï†¦ themiro Â· 3 mo. ago SSM-style architectures are the future and I have believed that since the H3 paper came out. Maybe attention will stick around for shorter lengths but models need a way to have a ï¬xed length memory bank and SSMs provide it. 11 ï…¯ Reply Share ïˆ©ï ï†— 314kabinet Â· 3 mo. ago After reading the Mamba paper, attention feels like a hack to avoid engineering a memory representation. â€œWe donâ€™t know how to make the network remember the content of the previous tokens so letâ€™s just feed all of them into it over and over.â€ Hence the quadratic scaling with context size: each new token depends on all previous tokens instead of a ï¬xed-size state. 18 ï…¯ Reply Share ïˆ©ï ï†— A_HumblePotato Â· 3 mo. ago Â· edited 3 mo. ago SSM-style architectures are the future Funnily enough theyâ€™re the past too. As someone from a ï¬eld where state space modeling is the norm itâ€™s pretty funny seeing it loop around to become state-of-the-art in machine learning. 18 ï…¯ Reply Share ïˆ©ï ï†— I_will_delete_myself Â· 3 mo. ago Kalman ï¬lter was here..... 8 ï…¯ Reply Share ïˆ©ï ï†— DigThatData Â· 3 mo. ago Researcher H3? 4 ï…¯ Reply Share ïˆ©ï ï†— Disastrous_Elk_6375 Â· 3 mo. ago https://arxiv.org/abs/2212.14052 6 ï…¯ Reply Share ïˆ©ï ï†— DigThatData Â· 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 15/21 Researcher ah 1 ï…¯ Reply Share ïˆ©ï ï†— cspenn Â· 3 mo. ago I hope however it turns out, one of the ï¬rst implementations is named Marconi just so that Starship lyric ï¬nally makes sense decades later. 3 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 3 mo. ago Researcher mamba still underperforms relative to transformer, not to mention transformers didn't get much attention until bert, so until ssms have its own bert moment it will not overtake transformers not to mention sub quadratic scaling wrt length isn't a selling point anymore (not that it was to begin with). fa2 solves that issue, and attention cost becomes increasingly marginal as you scale up model size, that for frontier models the attention cost is minor compared to the matmuls even without fa 16 ï…¯ Reply Share ïˆ©ï ï†— rrenaud Â· 3 mo. ago Flash attention 2 is just a really good implementation, it doesn't solve the quadratic scaling problem. 46 ï…¯ Reply Share ïˆ©ï ï†— Forsaken-Data4905 Â· 3 mo. ago Â· edited 3 mo. ago It kind of does, though? Sure, you still have quadratic compute, but it's not a signiï¬cant bottleneck, or at least I'm not aware of any evidence of it. Quadratic memory was not only a resources problem, but it also massively slowed down training and inference speeds, due to the I/O operations. I guess when scaling beyond low hundreds of thousands of tokens it would become problematic, but I'm not sure it's a very relevant issue. 6 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 3 mo. ago Researcher Fa already gives you linear memory scaling, and again, ï¬‚ops are already dominated by matmuls the marginal cost of increasing seq len isn't that big a deal for practical purposes 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 16/21 7 ï…¯ Reply Share ïˆ©ï ï†— fasttosmile Â· 3 mo. ago That would obviously change with longer context lengths. Which people want. 5 ï…¯ Reply Share ïˆ©ï ï†— the_aligator6 Â· 3 mo. ago Where are you seeing they are underperforming against transformers? Every benchmark I've seen has transformers beat by Mamba. 17 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 3 mo. ago Â· edited 3 mo. ago Researcher every benchmark = 1 benchmark at 300b tokens, which is meaningless in current context when you're using 5x compute to train the models vs pythia/opt etc. much clearer picture when you look at scaling laws in ï¬g 5 4 and shows no advantage vs transformers 4 ï…¯ Reply Share ïˆ©ï ï†— the_aligator6 Â· 3 mo. ago Â· edited 3 mo. ago Where did you get the 5x compute ï¬gure from? Here is a 5x ï¬gure (from the paper): \"Mamba can achieve 5Ã— higher throughput than Transformers.\". low Inference cost is more important than training cost due to the economics of pay per use APIs. Training happens only so often, they are eï¬€ectively ï¬xed costs. Additionally real time inference speed opens up the doors to crazy new applications. Comparing a model that came out 4 weeks ago with implementations of a model that has had 5 years+ of optimization doesn't tell the entire story. \"X is underperforming Y\" without a slew of qualiï¬ers is not a rational statement. Here is another benchmark (not conclusive, small models, i know, just wanted to add another data point): https://www.reddit.com/r/MachineLearning/comments/18d65bz/d_thoughts_on _mamba/ > not to mention sub quadratic scaling wrt length isn't a selling point anymore (not that it was to begin with) 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 17/21 This is not true, it is deï¬nitely still a selling point. Also from the ï¬gure I saw in FA2, I believe attention is 70% of the way there to achieving matmul parity. Thats not insigniï¬cant. Regardless, we cant assume models will generalize the same way as they scale. Any new model has the potential to replace transformers (or carve out some part of the application space) if they demonstrate emergent capabilities which are in some fundamental way beyond the reach of transformer models. There is zero conclusive research on this to my knowledge, we simply don't know. (if you know of any, please share) If I were to speculate, We will see hybrid SSMs-Transformer architectures in the next year. 22 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 3 mo. ago Â· edited 3 mo. ago Researcher Where did you get the 5x compute ï¬gure from? because the table is for only 300b tokens, most 3b models are being trained for >=1.5t tokens Comparing a model that came out 4 weeks ago with implementations of a model that has had 5 years+ of optimization doesn't tell the entire story. 5 years+ of optimizations is a meme. the only major architecture change is rope, the rest are only minor changes like pre-layernorm + some tweaks to adam beta values, and even then the results aren't even that signiï¬cant. the reason transformers have improved since 2017 isn't due to any architecture/training improvements, it's just data + compute. look at the llm settings from the past 6 years, not that much has changed https://docs.google.com/spreadsheets/d/14vbBbuRMEHoqeuMHkTfw3uiZVm yXNuoSp8s-aHvfvZk/edit#gid=0 0 ï…¯ Reply Share ïˆ©ï ï†— we_are_mammals Â· 3 mo. ago Â· edited 3 mo. ago PhD 5 years+ of optimizations is a meme If you look at Fig 4 (left), the diï¬€erence between Transformer and Transformer++ is equivalent to roughly a 4x diï¬€erence in compute. This is 2*log(4, 2) = 4 years' worth of compute progress, according to Moore's law (Even more, if Moore's progress is slowing down) While the architectural tweaks might not be the biggest contributor, they are not negligible either. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 18/21 10 ï…¯ Reply Share ïˆ©ï ï†— dogesator Â· 2 mo. ago They are comparing Mamba vs a transformer++ model trained on exact same context length, exact same dataset and exact same tokenizer and same parameter count. Is this not the best way to compare the architectures, do you think it somehow makes sense to compare the mamba model against something trained on an entirely diï¬€erent tokenizer, diï¬€erent parameter count, private dataset and diï¬€erent context length? 1 ï…¯ Reply Share ïˆ©ï ï†— we_are_mammals Â· 3 mo. ago PhD much clearer picture when you look at scaling laws in ï¬g 5 and shows no advantage vs transformers ?! In Fig 5 (left), Mamba matches much bigger Transformer++ (3-4x). 5 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 3 mo. ago Researcher Sorry ï¬g 4, on pile 2 ï…¯ Reply Share ïˆ©ï ï†— dogesator Â· 2 mo. ago Even in ï¬gure 4 itâ€™s showing equal results at 2K context length and superior results at 8K context length 1 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 2 mo. ago Researcher A diï¬€erence that can be explained by the initialization, data order, etc. and without signiï¬cant baseline tuning... 1 ï…¯ Reply Share ïˆ©ï ï†— dogesator Â· 2 mo. ago Sure you can say that, but the model is getting atleast equal results in regular perplexity tests while getting signiï¬cantly better results in real world tasks against transformer++ model trained on exact 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 19/21 same dataset, exact tokenizer, same parameter count and same context length. The real world task benchmarks are far more signiï¬cant than any variation you would get from diï¬€erent shuï¬„ing ids for the dataset, especially the benchmarks testing for long context recall abilities 1 ï…¯ Reply Share ïˆ©ï ï†— koolaidman123 Â· 2 mo. ago Researcher getting signiï¬cantly better results in real world tasks against transformer++ model trained on exact same dataset, exact tokenizer, same parameter count and same context length. in a setting that's unrealistic by today's standards when you're using orders of magnitudes of compute, that's why we look at scaling laws if you actually care about real world setting, no one is using ssms when llama and mistral exist. until you have a ssm that outperforms llama2 on mmlu, no one will care. that's what i mean when i said in the original post of so until ssms have its own bert moment it will not overtake transformers 1 ï…¯ Reply Share ïˆ©ï ï†— dogesator Â· 2 mo. ago Already multiple groups now working on Mamba pretrainings of llama and mistral sized models for trillions of tokens, so I guess youâ€™ll just have to wait a few months. 1 ï…¯ Reply Share ïˆ©ï ï†— Continue this threadÂ  Comment deleted by user Â· 3 mo. ago koolaidman123 Â· 3 mo. ago Researcher Bert was before gpt2... 2 ï…¯ Reply Share ïˆ©ï ï†— CatalyzeX_code_bot Â· 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 20/21 Found 2 relevant code implementations for \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". If you have code to share with the community, please add it here ğŸ˜ŠğŸ™ -- To opt out from receiving code links, DM me. 13 ï…¯ Reply Share ïˆ©ï ï†— thatShawarmaGuy Â· 3 mo. ago Can someone explain the diï¬€erence in beginner friendly terms? I'm learning DL rn, but this sounds like something that'd inspire to learn more (pun intended) 6 ï…¯ Reply Share ïˆ©ï ï†— jloverich Â· 3 mo. ago Transformers you create a similarity matrix of all the inputs and use positional embedding so that it can determine the positional information... this seems unintuitive and its a little surprising that the positional embeddings work. Mamba borrows from control theory and looks more like you are evolving a diï¬€erential equation so it actually looks sequential. No positional embedding and no masking so it seems much less hacky. You're lucky! You may not even need to learn about transformers. I think for sequence modeling, transformers are ï¬nished. 34 ï…¯ Reply Share ïˆ©ï ï†— pyepyepie Â· 3 mo. ago Seems pretty far fetching... 13 ï…¯ Reply Share ïˆ©ï ï†— Instantinopaul OP Â· 3 mo. ago This is a simple on boarding https://youtu.be/TQQlZhbC5ps?si=zZs72ZkoXEgCEDQA 1 ï…¯ Reply Share ïˆ©ï ï†— j_lyf Â· 3 mo. agoï†¦ akshaylive Â· 3 mo. ago RetNet is simpler compared to MAMBA. It has also proven to scale well to 7B parameters. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 21/21 2 ï…¯ Reply Share ïˆ©ï ï†— lennox_wrld Â· 3 mo. ago I thought I knew ml at least the fundamentals but after reading comments to this post I now know I'm not even a rookie esp the maths part. I thought gradient descent and back propagation was almost all. what's like a descent book that would put me to pace 2 ï…¯ Reply Share ïˆ©ï ï†— Instantinopaul OP Â· 3 mo. ago There is nothing to get discouraged about. It is gradient descent and back propagration only at the core. These are build ups on top. Try to explore stuï¬€ on top. Ex: attention, SSMs etc 3 ï…¯ Reply Share ïˆ©ï ï†— Separate_Flower4927 Â· 2 mo. agoï†¦","libVersion":"0.3.2","langs":""}
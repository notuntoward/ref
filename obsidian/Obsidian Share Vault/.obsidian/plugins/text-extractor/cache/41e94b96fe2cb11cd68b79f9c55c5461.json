{"path":"lit/lit_notes_OLD_PARTIAL/Instantinopaul24MambaVsTransformers.pdf","text":"3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 1/21 Sort By: Best | Discussion Heard all the buzz about Mamba, the new kid on the sequence modeling block. Supposedly it's faster, handles longer sequences better, and even outperforms Transformers on some tasks. But is it really a throne-stealer or just another ﬂash in the pan? My perception: Strengths: Mamba boasts eﬃcient memory usage, linear scaling with sequence length, and impressive performance in language and DNA modeling. Plus, it ditches the attention mechanism, potentially paving the way for faster inference. Weaknesses: Still early days, so Mamba's long-term stability and performance across diverse tasks remain to be seen. And while it doesn't need attention, its state space approach might be trickier to grasp for some folks. To the AI aﬁcionados out there, is Mamba just the next shiny toy, or a genuine paradigm shift in sequence modeling? Will it dethrone the mighty Transformer, or coexist as a specialized tool? Let's hear your thoughts! https://arxiv.org/abs/2312.00752  103 Comments  Share  306   Posted by u/Instantinopaul 3 months ago  [D] So, Mamba vs. Transformers... is the hype real? Comment as wreckable Search comments What are your thoughts?           Markdown Mode r/MachineLearning 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 2/21 View discussions in 1 other community 314kabinet · 3 mo. ago I’ve read the paper. The S6 layers of Mamba have a memory which they modify with each new token. All state space modeling nets work this way but the advantage here is that S6 has control over how each token is remembered (if at all), as opposed to just trying to memorize a compressed version of the entire input sequence. This means it can in theory hold on to important info from a million tokens ago while only keeping short-term details for as long as they’re needed. Whether or not it works like that in practice for practical-sized models remains to be seen, but even if it doesn’t, more sophisticated versions of the memory state will be developed. Intuitively it makes sense for a system to accept an input one token at a time and have an internal memory (instead of just taking in the entire uncompressed sequence in one go as attention does), so I’m optimistic. 214  Reply Share   ArnoF7 · 3 mo. ago Intuitively I think this makes sense. I do have one question tho. I haven’t dived deep into the mamba paper but what’s the diﬀerence between mamba and LSTM and the likes then? Is Mamba more hardware friendly like they claim in the paper, compared to LSTM? 52  Reply Share   beariﬁc · 3 mo. ago The diﬀerences as I understand it: Mamba has a linear activation function between each hidden state, while LSTM and RNN have a nonlinearity, which makes backpropagation through time a lot more stable for Mamba. Mamba can still be calculated in one forward pass through a parallel scan (preﬁx sum) operation, compared to e.g. RNNs and LSTMs where we need to calculate the previous timestep before we can calculate the next. The Mamba authors developed a hardware-aware algorithm in the same vein als FlashAttention which further improves eﬃciency. The authors mention that RNNs without time-wise nonlinearities such as QRNN are the most similar to Mamba, but those do not use state expansion or selective B and C params, and they use a heuristic gating mechanism, while the parameterizations and initializations of Mamba are based on principled SSM theory. 85  Reply Share   3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 3/21 Cryptheon · 3 mo. ago What's the impact of having no non-linearities? The whole point of performance eﬃcient deep learning is to have non-linearities to map to a more disentangled data space. How does getting rid of this assumption lead to a similar performing NN? 22  Reply Share   beariﬁc · 3 mo. ago No time-wise nonlinearities, there are still nonlinearities between the mamba layers 46  Reply Share   sortphotos · 3 mo. ago extracoffeeplease · 3 mo. ago From a math perspective, they do weird stuﬀ using 'downprojecting on orthogonal functions' which isn't even learned which is strange as hell. Spoiler alert: Albert Gu never really explains on YouTube what the gist of this is or why it works, so you meed need to read the HiPPO paper. I don't understand it yet. Anyway, due to this they can do a compression of history according to some measure (like exponential decay L2 error). Because of that they can do without attention, and the network is simultaneously interpretable as CNN and an RNN due to black magic stuﬀ, and this is great. Parallel training like a CNN, superfast online inference like an RNN, and super long contexts because it grows linearly, not quadratically. 8  Reply Share   PsecretPseudonym · 3 mo. ago · edited 3 mo. ago It becomes more natural if you’re familiar with this area of mathematics. It’s similar to a Fourier decomposition; you’re projecting some original function from its original domain into a new space whose coordinates are given in terms of linear coeﬃcients of orthogonal basis functions. They apply this transformation but with Legendre polynomials as a sort of convolutional kernel operation for the “structure” of S4. Most of the math will probably be more familiar for people coming from signal processing (like radio/electrical engineers), some areas of physics, control/feedback systems, etc. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 4/21 You can think about it like doing a regression of a target variable against some set of analytically derived special input functions. It’s just that these special functions are deﬁned such that they are linearly independent and have some other nice properties. Then, in this space, each point is represented via a set of coordinates for a functional approximation to its entire history. You can then try to predict how the sequence progresses almost like a set of diﬀerential additive updates in this space from one time step to the next. You then can decompose those updates as some matrix operation on the previous state plus some matrix operation of the new raw inputs mapped into this space (or directly). Think of it like creating an embedding of the entire history at every single point, then trying to predict how that embedding evolves from one time step to the next in the embedding space, then mapping that back to make a prediction for the next time step at each point. This is recurrent in the space of the structured decomposition, and can be done with just linearly operators in that space. It’s kind of like old school kernel trick methods where you’re doing a nonlinear mapping the original data into some new space where then the recurrent relation from one time step to the next is then a linear function and so can be computed much more eﬃciently, then mapped back. That’s S4 (but not Mamba) went a step further by using the FFT and inverse FFT and the convolution theorem to make it so you can essentially compute convolutions over time via just products in the frequency domain. Mamba goes a step further in making the same evolution of the structured representation itself time-varying depending on the state and input at each step, which means you have to actually sequentially compute that I believe. They’re just then doing so in a cache-aware and hardware optimized way to make it fast. Honestly RNNs were always just dumb for having used exponential forgetting, and LSTMs were kind of ham-ﬁsted. The idea of using linear time invariant recurrent functions in the space of the evolution of the orthogonal basis coeﬃcients makes way more sense. 17  Reply Share   extracoffeeplease · 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 5/21 You can then try to predict how the sequence progresses almost like a set of diﬀerential additive updates in this space from one time step to the next. You then can decompose those updates as some matrix operation on the previous state plus some matrix operation of the new raw inputs mapped into this space (or directly). Thank you for the detailed explanation! I can deal with projecting functions onto some basis, however how the state matrix with rows like [1 3 5 7 etc] comes into play I do not fully understand. Is it coupled to the base functions? Another question: In the diagram of HiPPO in an RNN, what is going on, speciﬁcally where is the learning going on? Apparently not in the HiPPO operator? What does the dashed line linking c_t-1 with hippo operator represent? 3  Reply Share   PsecretPseudonym · 3 mo. ago · edited 3 mo. ago Just glad if it was helpful! For a more detailed explanation, you’ll want to review ﬁgure 1 and section 2.1 of this paper. 1  Reply Share   extracoffeeplease · 2 mo. ago Wasn't aware of this paper, thanks! 2  Reply Share   thntk · 3 mo. ago If I understand what you said correctly, they tried \"more complicated\" Legendre polynomials ﬁrst, then went back to \"simpler\" FFT in S4? Your explanation of the linear recurrent in another (non-linear?) space is cool, it makes a lot of sense. However, RNNs are quite versatile and can do the same thing (with learned instead of ﬁxed decompositions). For example, the RNN cell can be 2-layered, with layer 1 nonlinear (input transform), layer 2 linear (recurrent output), recurrent connection only going back to layer 2 (thus linear recurrent). I think Mamba develops on some ideas of 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 6/21 other RNNs variants, including LSTM (input dependent transform vs. input gate). 2  Reply Share   nedw · 3 mo. ago I think the convolutional training is speciﬁc to S4 and is lost as part of making the state transition matrix conditional on the input data. The second half of the mamba paper is about making up for the loss of the eﬃciency when doing convolution. I believe I saw the video you’re referring to a few weeks ago and I think it’s really good as a background for understanding these models but I’m struggling to form a full integrated picture myself. 3  Reply Share   haukzi · 3 mo. ago Using ﬁxed or non-learned basis functions is pretty standard such as when using fourier or polynomial bases, you don't want to learn that since then you run the risk of your basis not being a basis anymore. For a recent example see Padé units and the Padé activation functions paper where they compare both. The whole \"interpretable as a CNN and as an RNN\" is undergrad fourier and numerical analysis. 1  Reply Share   audiencevote · 3 mo. ago Mamba has a linear activation function between each hidden state, while LSTM and RNN have a nonlinearity, which makes backpropagation through time a lot more stable for Mamba. nitpick: LSTM has a linear activation inside its cell, too. It's only the gates that are nonlinear. 6  Reply Share   swfsql · 3 mo. ago I'm new to ML let alone Mamba, but I believe some of the information is incorrect or incomplete. The linearity makes it possible to adjust the weights such that backprop through time is more stable - It's not that it's automatically more stable by being linear. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 7/21 S4 could in one batched forward call do a parallel scan, based on convolutions. S6 (Mamba) can't because now the input interferes more thoroughly in the internal weights, but they still made it go fast thanks to cuda kernel optimizations. 4  Reply Share   beariﬁc · 3 mo. ago S4 does not use a parallel scan, but like you say a convolution. The parallel scan is used in mamba precisely because the weights are now input dependent, however the calculation is not sequentially dependent but associative across timesteps, so it can still be calculated in parallel. A parallel scan is just a lot less hardware eﬃcient than a convolution, so they implemented the hardware-aware algorithm. 6  Reply Share   intentionallyBlue · 3 mo. ago Afaik the diﬀerence is that these modern state space models can be run in parallel over the sequence dim (like a transformer) which is important for eﬃcient training. This can't be done in any obvious way in an LSTM. 21  Reply Share   vman512 · 3 mo. ago If you are referring to being able to run the recurrent model as a convolution, that only works for linear + time invariant models, which was true for S4, but went away in Mamba. 7  Reply Share   steveofsteves · 3 mo. ago AdditionAlone3851 · 3 mo. ago https://www.reddit.com/r/MachineLearning/comments/18iph6a/d_can_someone_d escribe_how_the_ssm_in_mamba_is/ I have no idea but this might help 4  Reply Share   Top-Smell5622 · 3 mo. ago But isn’t it also a disadvantage that the model has to decide what to keep from a token before having seen all future tokens? (Same as for RNNs) 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 8/21 6  Reply Share   314kabinet · 3 mo. ago It is. Any network where tokens come in one at a time gets one opportunity to do whatever it needs with that token and then that’s it. 7  Reply Share   Linooney · 3 mo. ago · edited 2 mo. ago Researcher I assume you could do a bidirectional SSM like you do with biLSTMs. 2  Reply Share   graphicteadatasci · 3 mo. ago Would it have to be bidirectional? Couldn't we just have the model read the text multiple times? Like an MI would do (meat intelligence). 2  Reply Share   VelveteenAmbush · 3 mo. ago This means it can in theory hold on to important info from a million tokens ago while only keeping short-term details for as long as they’re needed. I guess this would predict that transformers will win if your use case is an Instruct- tuned model where you give it a long piece of text and then asking speciﬁc questions about it? Like if you paste in a chapter from Harry Potter and then ask it to identify the ﬁrst split inﬁnitive or something, it wouldn't have known from the start to remember when there is a split inﬁnitive. Whereas if you put the question at the start of the prompt and then pasted the text, it would? Not claiming that this constitutes a critical weakness or anything, just want to see if I'm understanding the strengths and limitations correctly. 7  Reply Share   314kabinet · 3 mo. ago I think so. This exact thing (putting the question before va after) is an issue with RWKV, another recurrent architecture. Probably an issue with recurrent architectures in general. Works for humans too! 9  Reply Share   VelveteenAmbush · 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 9/21 Oh for sure. A transformer is like a human who can go back and review the text with each question. I guess Mamba and RWKV are like a human who only gets one read-through. 5  Reply Share   314kabinet · 3 mo. ago It’s easy to give them another read through too! 5  Reply Share   prumf · 2 mo. ago That’s what I was wondering. Transformers are expensive because of attention. They read every single word of the book at the same time, and look how every other word compares go it, which is complete madness computation-wise. Humans on the other hand read one word at a time, update internal knowledge, and go back if some information was missed or is suddenly deemed relevant. For really eﬃcient systems, we need something that can go back-and- forth as required, as many times as needed. 1  Reply Share   prumf · 2 mo. ago But maybe that isn’t necessarily more eﬃcient, as computers are really good ad doing things in parallel, whereas humans cannot really multitask. 2  Reply Share   PsecretPseudonym · 3 mo. ago I don’t think that’s necessarily the case. The model will have been trained to use the potentially gigabytes of retained state information to store and forward the relevant information as needed. It’s just storing the history of the sequence in a far more eﬃcient and highly structured way, and it will have been trained to very thoughtfully and eﬃciently retain recent information nearly perfectly and historical information if and when it may be relevant again or relates to recent information. Think of it like this: 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 10/21 A transformer will need and work from the exact raw sequence data up to a history of N. The transformer must then truncate and completely 100% forget anything beyond N tokens ago. The state space model instead is eﬃciently encoding and storing the relevant information, and predicting what might still be relevant, so holding onto it. Also, because things that are still relevant are inherently related in some way, that means it can really cleverly and eﬃciently compress and store that history in an eﬃcient and structured way. So, rather than 100% forgetting via truncating beyond a given sequence length, it is incrementally holding onto whatever could potentially be relevant in an eﬃcient way. If it doesn’t retain information you might need later, that would be a sign that it simply hasn’t been well trained. If it’s well trained and allowed to be very large, it should virtually always still have what you need. 5  Reply Share   EmbarrassedHelp · 3 mo. ago danysdragons · 3 mo. ago What happens if a token from near the beginning is irrelevant across the ﬁrst 1,000,000 tokens and forgotten, but suddenly becomes relevant again with the next tokens? Can it be unforgotten? 1  Reply Share   Honest_Science · 2 mo. ago Can you unforget? 1  Reply Share   DigThatData · 3 mo. ago Researcher i've heard promising results from colleagues doing casual experiments. if SSMs have the potential people think they do, we should see some interesting papers popping up between now and April. if no particularly interesting results pop up by April/May, I'd predict SSMs aren't going to eat Transformer's lunch. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 11/21 29  Reply Share   idontcareaboutthenam · 3 mo. ago If I may ask, why isn't RWKV similarly hyped? Isn't it also linear in sequence length and parallel in computation? 34  Reply Share   vatsadev · 3 mo. ago It does most of the things, but theres a paper for mamba, which makes it easier, one codebase, versus spread out repos, Mambas newer, and also extrapolates longer, with mamba work from 256-> 1m token ctx len, and rwkv double the trained ctx Also I'm guessing TriDao & albert gu are well known vs rwkv being random discord come together? 40  Reply Share   JustOneAvailableName · 3 mo. ago RWKV also changed quite a lot in various versions. I frankly have no clue what the current idea is 18  Reply Share   vatsadev · 3 mo. ago It's somewhere in v5.2 and v6 8  Reply Share   Disastrous_Elk_6375 · 3 mo. ago I frankly have no clue what the current idea is At some point they started adding attention to it, if I'm not mistaken :) 5  Reply Share   vatsadev · 3 mo. ago No, still pure linear, but it is getting more like mamba 1  Reply Share   vman512 · 3 mo. ago RWKV has a paper too 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 12/21 8  Reply Share   vatsadev · 3 mo. ago That's v4, pretty old now 9  Reply Share   themiro · 3 mo. ago SSMs perform better and also have a cleaner tradeoﬀ between token independence and token awareness 8  Reply Share   currentscurrents · 3 mo. ago heuristic_al · 3 mo. ago My take is that any ﬁxed memory scheme will eventually suﬀer at long contexts vs true attention. And correct me if I'm wrong, but true attention is actually more computationally eﬃcient than Mamba for sequences less long than the hidden width of the network (4096 for llama). So Mamba only has this region of context lengths where it could actually be better. 7  Reply Share   Dump7 · 3 mo. ago Looking at this thread, there is a lot I need to read and understand. Love this thread...! 11  Reply Share   Gody_Godee · 3 mo. ago intelligent is O(n2). prove me wrong 6  Reply Share   ItsJustMeJerk · 2 mo. ago Human intelligence certainly isn't. 5  Reply Share   prumf · 2 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 13/21 Counter example : humans. You don’t compare every word of the book to every other word. You build internal knowledge like RNN and might need to do multiple reads to understand everything. 2  Reply Share   Gody_Godee · 1 mo. ago *general intelligent 1  Reply Share   FallMindless3563 · 3 mo. ago FWIW I’ve tried out a few of the Mamba models natural language tasks such as Question Answering, and the results were not even close to larger transformers yet. I tried everything from prompt engineering to ﬁne-tuning the models. This could be due to parameter count or lack of pre-training data for the mamba models that were released. I heard the authors say these early versions of Mamba are very much a proof of concept, and we’d need to train larger parameter count and on more data to be competitive with the transformers that are out there today. On SQuAD Mamba-2.8b with a 3-shot prompt only got 7.5% accuracy… whereas models like Mistral 7B I’ve seen get 70%+ with zero-shot. I documented my process and ﬁndings here if anyone is interested 👇 https://blog.oxen.ai/practical-ml-dive-how-to-train-mamba-for-question-answering/ 12  Reply Share   graphicteadatasci · 3 mo. ago Very cool blog post. But when you say Mamba vs Mistral you aren't comparing two models trained on the same data set, are you? Data is more important than architecture imho. 6  Reply Share   FallMindless3563 · 3 mo. ago Correct! I was pointing out that the current iteration of Mamba is not at all useable for NLP. It needs to be scaled up in parameters and data before we can really do an apples to apples comparison to Transformers that are useable today. 3  Reply Share   Jattoe · 1 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 14/21 seraine · 3 mo. ago themiro · 3 mo. ago SSM-style architectures are the future and I have believed that since the H3 paper came out. Maybe attention will stick around for shorter lengths but models need a way to have a ﬁxed length memory bank and SSMs provide it. 11  Reply Share   314kabinet · 3 mo. ago After reading the Mamba paper, attention feels like a hack to avoid engineering a memory representation. “We don’t know how to make the network remember the content of the previous tokens so let’s just feed all of them into it over and over.” Hence the quadratic scaling with context size: each new token depends on all previous tokens instead of a ﬁxed-size state. 18  Reply Share   A_HumblePotato · 3 mo. ago · edited 3 mo. ago SSM-style architectures are the future Funnily enough they’re the past too. As someone from a ﬁeld where state space modeling is the norm it’s pretty funny seeing it loop around to become state-of-the-art in machine learning. 18  Reply Share   I_will_delete_myself · 3 mo. ago Kalman ﬁlter was here..... 8  Reply Share   DigThatData · 3 mo. ago Researcher H3? 4  Reply Share   Disastrous_Elk_6375 · 3 mo. ago https://arxiv.org/abs/2212.14052 6  Reply Share   DigThatData · 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 15/21 Researcher ah 1  Reply Share   cspenn · 3 mo. ago I hope however it turns out, one of the ﬁrst implementations is named Marconi just so that Starship lyric ﬁnally makes sense decades later. 3  Reply Share   koolaidman123 · 3 mo. ago Researcher mamba still underperforms relative to transformer, not to mention transformers didn't get much attention until bert, so until ssms have its own bert moment it will not overtake transformers not to mention sub quadratic scaling wrt length isn't a selling point anymore (not that it was to begin with). fa2 solves that issue, and attention cost becomes increasingly marginal as you scale up model size, that for frontier models the attention cost is minor compared to the matmuls even without fa 16  Reply Share   rrenaud · 3 mo. ago Flash attention 2 is just a really good implementation, it doesn't solve the quadratic scaling problem. 46  Reply Share   Forsaken-Data4905 · 3 mo. ago · edited 3 mo. ago It kind of does, though? Sure, you still have quadratic compute, but it's not a signiﬁcant bottleneck, or at least I'm not aware of any evidence of it. Quadratic memory was not only a resources problem, but it also massively slowed down training and inference speeds, due to the I/O operations. I guess when scaling beyond low hundreds of thousands of tokens it would become problematic, but I'm not sure it's a very relevant issue. 6  Reply Share   koolaidman123 · 3 mo. ago Researcher Fa already gives you linear memory scaling, and again, ﬂops are already dominated by matmuls the marginal cost of increasing seq len isn't that big a deal for practical purposes 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 16/21 7  Reply Share   fasttosmile · 3 mo. ago That would obviously change with longer context lengths. Which people want. 5  Reply Share   the_aligator6 · 3 mo. ago Where are you seeing they are underperforming against transformers? Every benchmark I've seen has transformers beat by Mamba. 17  Reply Share   koolaidman123 · 3 mo. ago · edited 3 mo. ago Researcher every benchmark = 1 benchmark at 300b tokens, which is meaningless in current context when you're using 5x compute to train the models vs pythia/opt etc. much clearer picture when you look at scaling laws in ﬁg 5 4 and shows no advantage vs transformers 4  Reply Share   the_aligator6 · 3 mo. ago · edited 3 mo. ago Where did you get the 5x compute ﬁgure from? Here is a 5x ﬁgure (from the paper): \"Mamba can achieve 5× higher throughput than Transformers.\". low Inference cost is more important than training cost due to the economics of pay per use APIs. Training happens only so often, they are eﬀectively ﬁxed costs. Additionally real time inference speed opens up the doors to crazy new applications. Comparing a model that came out 4 weeks ago with implementations of a model that has had 5 years+ of optimization doesn't tell the entire story. \"X is underperforming Y\" without a slew of qualiﬁers is not a rational statement. Here is another benchmark (not conclusive, small models, i know, just wanted to add another data point): https://www.reddit.com/r/MachineLearning/comments/18d65bz/d_thoughts_on _mamba/ > not to mention sub quadratic scaling wrt length isn't a selling point anymore (not that it was to begin with) 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 17/21 This is not true, it is deﬁnitely still a selling point. Also from the ﬁgure I saw in FA2, I believe attention is 70% of the way there to achieving matmul parity. Thats not insigniﬁcant. Regardless, we cant assume models will generalize the same way as they scale. Any new model has the potential to replace transformers (or carve out some part of the application space) if they demonstrate emergent capabilities which are in some fundamental way beyond the reach of transformer models. There is zero conclusive research on this to my knowledge, we simply don't know. (if you know of any, please share) If I were to speculate, We will see hybrid SSMs-Transformer architectures in the next year. 22  Reply Share   koolaidman123 · 3 mo. ago · edited 3 mo. ago Researcher Where did you get the 5x compute ﬁgure from? because the table is for only 300b tokens, most 3b models are being trained for >=1.5t tokens Comparing a model that came out 4 weeks ago with implementations of a model that has had 5 years+ of optimization doesn't tell the entire story. 5 years+ of optimizations is a meme. the only major architecture change is rope, the rest are only minor changes like pre-layernorm + some tweaks to adam beta values, and even then the results aren't even that signiﬁcant. the reason transformers have improved since 2017 isn't due to any architecture/training improvements, it's just data + compute. look at the llm settings from the past 6 years, not that much has changed https://docs.google.com/spreadsheets/d/14vbBbuRMEHoqeuMHkTfw3uiZVm yXNuoSp8s-aHvfvZk/edit#gid=0 0  Reply Share   we_are_mammals · 3 mo. ago · edited 3 mo. ago PhD 5 years+ of optimizations is a meme If you look at Fig 4 (left), the diﬀerence between Transformer and Transformer++ is equivalent to roughly a 4x diﬀerence in compute. This is 2*log(4, 2) = 4 years' worth of compute progress, according to Moore's law (Even more, if Moore's progress is slowing down) While the architectural tweaks might not be the biggest contributor, they are not negligible either. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 18/21 10  Reply Share   dogesator · 2 mo. ago They are comparing Mamba vs a transformer++ model trained on exact same context length, exact same dataset and exact same tokenizer and same parameter count. Is this not the best way to compare the architectures, do you think it somehow makes sense to compare the mamba model against something trained on an entirely diﬀerent tokenizer, diﬀerent parameter count, private dataset and diﬀerent context length? 1  Reply Share   we_are_mammals · 3 mo. ago PhD much clearer picture when you look at scaling laws in ﬁg 5 and shows no advantage vs transformers ?! In Fig 5 (left), Mamba matches much bigger Transformer++ (3-4x). 5  Reply Share   koolaidman123 · 3 mo. ago Researcher Sorry ﬁg 4, on pile 2  Reply Share   dogesator · 2 mo. ago Even in ﬁgure 4 it’s showing equal results at 2K context length and superior results at 8K context length 1  Reply Share   koolaidman123 · 2 mo. ago Researcher A diﬀerence that can be explained by the initialization, data order, etc. and without signiﬁcant baseline tuning... 1  Reply Share   dogesator · 2 mo. ago Sure you can say that, but the model is getting atleast equal results in regular perplexity tests while getting signiﬁcantly better results in real world tasks against transformer++ model trained on exact 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 19/21 same dataset, exact tokenizer, same parameter count and same context length. The real world task benchmarks are far more signiﬁcant than any variation you would get from diﬀerent shuﬄing ids for the dataset, especially the benchmarks testing for long context recall abilities 1  Reply Share   koolaidman123 · 2 mo. ago Researcher getting signiﬁcantly better results in real world tasks against transformer++ model trained on exact same dataset, exact tokenizer, same parameter count and same context length. in a setting that's unrealistic by today's standards when you're using orders of magnitudes of compute, that's why we look at scaling laws if you actually care about real world setting, no one is using ssms when llama and mistral exist. until you have a ssm that outperforms llama2 on mmlu, no one will care. that's what i mean when i said in the original post of so until ssms have its own bert moment it will not overtake transformers 1  Reply Share   dogesator · 2 mo. ago Already multiple groups now working on Mamba pretrainings of llama and mistral sized models for trillions of tokens, so I guess you’ll just have to wait a few months. 1  Reply Share   Continue this thread  Comment deleted by user · 3 mo. ago koolaidman123 · 3 mo. ago Researcher Bert was before gpt2... 2  Reply Share   CatalyzeX_code_bot · 3 mo. ago 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 20/21 Found 2 relevant code implementations for \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". If you have code to share with the community, please add it here 😊🙏 -- To opt out from receiving code links, DM me. 13  Reply Share   thatShawarmaGuy · 3 mo. ago Can someone explain the diﬀerence in beginner friendly terms? I'm learning DL rn, but this sounds like something that'd inspire to learn more (pun intended) 6  Reply Share   jloverich · 3 mo. ago Transformers you create a similarity matrix of all the inputs and use positional embedding so that it can determine the positional information... this seems unintuitive and its a little surprising that the positional embeddings work. Mamba borrows from control theory and looks more like you are evolving a diﬀerential equation so it actually looks sequential. No positional embedding and no masking so it seems much less hacky. You're lucky! You may not even need to learn about transformers. I think for sequence modeling, transformers are ﬁnished. 34  Reply Share   pyepyepie · 3 mo. ago Seems pretty far fetching... 13  Reply Share   Instantinopaul OP · 3 mo. ago This is a simple on boarding https://youtu.be/TQQlZhbC5ps?si=zZs72ZkoXEgCEDQA 1  Reply Share   j_lyf · 3 mo. ago akshaylive · 3 mo. ago RetNet is simpler compared to MAMBA. It has also proven to scale well to 7B parameters. 3/25/24, 11:45 P M (8) [D] S o, Mamba vs. Transformers... is the hype real? : MachineLearning chrome-extension://mpiodijhokgodhhofbcjdecpffjipkle/src/ui/pages/editor.html 21/21 2  Reply Share   lennox_wrld · 3 mo. ago I thought I knew ml at least the fundamentals but after reading comments to this post I now know I'm not even a rookie esp the maths part. I thought gradient descent and back propagation was almost all. what's like a descent book that would put me to pace 2  Reply Share   Instantinopaul OP · 3 mo. ago There is nothing to get discouraged about. It is gradient descent and back propagration only at the core. These are build ups on top. Try to explore stuﬀ on top. Ex: attention, SSMs etc 3  Reply Share   Separate_Flower4927 · 2 mo. ago","libVersion":"0.3.2","langs":""}
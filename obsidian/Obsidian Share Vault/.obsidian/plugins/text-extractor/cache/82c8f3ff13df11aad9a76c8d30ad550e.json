{"path":"lit/lit_notes_OLD_PARTIAL/Guo23CuriousDeclineLLM.pdf","text":"The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text Yanzhu Guo 1, Guokan Shang 4, Michalis Vazirgiannis 1, Chloé Clavel 2,3 1LIX, École Polytechnique, Institut Polytechnique de Paris, France 2LTCI, Télécom-Paris, Institut Polytechnique de Paris, France 3Inria, Paris, France 4Linagora, France {yanzhu.guo, guokan.shang}@polytechnique.edu mvazirg@lix.polytechnique.fr chloe.clavel@telecom-paris.fr Abstract This study investigates the consequences of training large language models (LLMs) on syn- thetic data generated by their predecessors, an increasingly prevalent practice aimed at ad- dressing the limited supply of human-generated training data. Diverging from the usual empha- sis on performance metrics, we focus on the impact of this training methodology on linguis- tic diversity, especially when conducted recur- sively over time. To assess this, we developed a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in re- cursive fine-tuning experiments across various natural language generation tasks. Our findings reveal a marked decrease in the diversity of the models’ outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of lin- guistic richness. Our study highlights the need for careful consideration of the long-term ef- fects of such training approaches on the linguis- tic capabilities of LLMs. 1 Introduction Scaling law reveals a predictable smooth increase in model performance as the amount of data, com- pute power, and model parameters are amplified in tandem (Ganguli et al., 2022). Even assuming that we can boost the other two ingredients indef- initely, the amount of data is limited. By one es- timate, the world’s entire supply of high-quality text ranges up to 17 trillion tokens, with a 4-5% yearly growth rate (Villalobos et al., 2022). This in- cludes all the world’s books, scientific papers, news articles, Wikipedia pages, available code, and the rest of filtered web content. Meta’s Llama 2, one of today’s leading LLMs, was trained on around 2 trillion tokens (Touvron et al., 2023). In other words, we might be approaching the exhaustion of the world’s entire stock of usable language training data, potentially within an order of magnitude. Is it possible for LLMs to train on their self- generated samples, thereby offering a solution to the looming data shortage? In fact, whether inten- tionally or unintentionally, this would happen with the widespread recognition and usage of LLMs. Regarding pretraining data, which is often sourced from the Internet, a significant trend is occurring: an increasing volume of online content is either generated or assisted by models, and such content is nearly indistinguishable from data produced by humans (Uchendu et al., 2023). Consequently, the subsequent generations of models will inevitably be pretrained on deeply blended data. Regarding finetuning data, employing LLM-generated exam- ples is already a widely adopted data augmentation approach in the NLP community. The work of self-instruct (Wang et al., 2023) prompts language models to solicit synthetic multi-task instruction- tuning data in an iterative bootstrapping way, start- ing with a seed set of manually-written instructions. Concerning single-task training, Zhou et al. (2023) build a large-scale multi-scenario multi-domain di- alogue summary corpus annotated by ChatGPT (Ouyang et al., 2022) to enhance their pretrained dialogue summarization model. However, recent studies raise concerns that the above approach of training on predecessor- generated text—LLMs are trained on the data pro- duced by previous models—is not a panacea with- out side effects, especially when conducted recur- sively over time. This would introduce a new set of challenging issues, a phenomenon described as model collapse (Shumailov et al., 2023; Alemo- hammad et al., 2023). On one hand, incorporating model-generated content in training may lead to irreversible flaws in the resulting models, where tails of the original distribution of genuine human content disappear. On the other hand, even when these models remain free of defects, they could converge to excessively uniform behaviours, with very small variance, due to the recursive samplingarXiv:2311.09807v1 [cs.CL] 16 Nov 2023 of only probable events over generations. In this study, rather than focusing on shifts in task-solving performance, our primary interest lies in exploring changes in language usage caused by the degenerative learning process of recursively training language models on the data generated by their predecessors. Our work is motivated by and contributes to, answering the following two key research questions: First, how can linguistic diversity be quantified effectively? Second, does recursive training on predecessor-generated text result in a reduction of linguistic diversity in model outputs? To address these questions, we first develop a comprehensive set of novel metrics aimed at assess- ing diverse aspects of linguistic variation, encom- passing lexical, semantic, and syntactic diversity. Subsequently, we proceed to conduct a series of recursive finetuning experiments spanning three natural language generation tasks, each demanding varying levels of creativity: news summarization (Hasan et al., 2021), scientific abstract generation, and story generation (Fan et al., 2018). Our results indicate a notable trend: with the progression of recursive finetuning iterations, there is indeed a re- markable decrease in the diversity of the generated outputs. This observation highlights the significant impact that training on text generated by predeces- sors has on the linguistic diversity of LLMs. 2 Related Work In this section, we explore two avenues of related work: current approaches to evaluate linguistic di- versity and recent research on training with syn- thetic data generated by language models. 2.1 Evaluating Linguistic Diversity Efforts to evaluate language models predominantly concentrate on their performance in task-solving. While some studies extend their scope to include aspects like factual consistency, reasoning capabil- ity, and robustness (Chang et al., 2023), there is a notable lack of attention to linguistic diversity, which remains significantly understudied. Furthermore, the existing studies that do address the diversity issue typically focus on lexical diver- sity alone. For example, in quantifying diversity, research on decoding strategies (Li et al., 2022b; Vijayakumar et al., 2018; Ippolito et al., 2019) usu- ally considers the proportion between the number of unique n-grams and total number of n-grams in generated text, known as distinct-n metric (Li et al., 2016). This very approach can also be found in the literature related to specific NLG tasks, especially those regarding creative text generation, such as poetry (Chakrabarty et al., 2022), lyric (Tian et al., 2023), and pun (Mittal et al., 2022) generation. Alternatively, Zhang et al. (2021) propose to use Shannon entropy to quantify diversity. However, such approach is still calculated on the tokens (i.e., lexical level), demonstrating a strong correlation with distinct-n. Zhu et al. (2018) introduce Self- BLEU which calculates the BLEU similarity score (Papineni et al., 2002) between different sentences of the same document, with higher Self-BLEU im- plying lower diversity. This metric is adopted as a proxy for diversity in evaluating the capability of LLMs in the context of producing content for disinformation operations (Liang et al., 2022). Nev- ertheless, the BLEU score is based on n-gram over- lap and thus also represents diversity solely from the lexical aspect. Very few works study diversity beyond the lexi- cal level. As an exploratory approach to quantify syntactic diversity, Clercq and Housen (2017) first manually annotate a small corpus of texts produced by second language learners for syntactic features such as syntactic length and clause types, whose variation is then viewed as a diversity index. Re- cently, Padmakumar and He (2023) bring up the semantic aspect of diversity and define the average pairwise BERTScore among a set of documents as the homogenization index. They also use ChatGPT to annotate key points on a small set of documents, counting the percentage of unique key points as content diversity. Our work is the first to comprehensively evaluate text generation on all three aspects of linguistic di- versity: lexical, syntactic and semantic. In addition to utilizing the well verified lexical diversity mea- sures, we also propose novel metrics for automat- ically evaluating syntactic and semantic diversity on a large scale. 2.2 Training with Synthetic Text Ever since the introduction of generative adversar- ial networks (Goodfellow et al., 2014), training new models with synthetic data produced by various generators has become a means of data augmen- tation (Li et al., 2022a), a practice that has been expanding to all modalities of machine learning research, including image, audio, and text. However, the large-scale usage of this approach, Model 1 Base 1 Data 0 Data 1 Model 2 Base 2 fitting sampling Data 2 Data n-1 Model n Base n … Figure 1: Our recursive fitting-sampling finetuning process. Beginning with authentic, human-curated Data (0), Base (1) model undergoes finetuning to develop Model (1), which is the first model subject to our language diversity research. Following that, we use the Model (1) to create a synthetic Data (1) to train a successor Model (2) of the next generation, on the basis of Base (2) model. particularly employing tremendous quantities of synthetic text to train generative models, is a more recent trend (Dai et al., 2023; Marwala et al., 2023). To name a few, the self-instruct study by Wang et al. (2023) guides a language model to iteratively generate synthetic multi-task instruct-tuning data, beginning with an initial set of manually-written instructions. Huang et al. (2022) demonstrate that LLMs are capable of self-improving their reason- ing abilities, with generated high-quality answers for unlabeled questions, using chain-of-thought (Wei et al., 2022) and self-consistency (Wang et al., 2022) prompting techniques. Meanwhile, Xu et al. (2023) introduce a pipeline that autonomously gen- erates a high-quality, multi-turn chat corpus by en- abling ChatGPT to converse with itself, which is then used to enhance a LLaMA model. As aforementioned in Section 1, concerns have been raised about such training methodology (Shu- mailov et al., 2023), especially considering the impact on the future LLMs, generation after gen- eration. Training data (often crawled from the Internet) will be abundantly generated by previ- ous models and LLMs will be recursively trained on predecessor-generated text. Studies show that this process will eventually lead to model collapse, causing performance degeneration, regardless of potential data filtering or refinement (Alemoham- mad et al., 2023). Our research is motivated by the same concept, yet our main focus is on inves- tigating how linguistic diversity is affected by this degenerative learning process, rather than on the decline in performance. 3 Methodology This section introduces our recursive training methodology and outlines the metrics employed to evaluate both performance and diversity. 3.1 Recursive Training Simulation Following the work of Shumailov et al. (2023), we simulate the process of recursively training LLMs on predecessor-generated text, under a finetuning setting. As illustrated in Figure 1, we begin with human-generated task-finetuning Data (0), which is used to train Base (1) model to create a task- specialized version, referred to as Model (1). After that, we use Model (1) to produce synthetic task- finetuning Data (1), which serves to train the next generation, Model (2), built upon Base (2) model. This procedure is repeated until we reach the n-th generation. Due to finite sampling, we are only able to approximate the original distribution via fitting, and errors compound over time during this process. For the sake of simplicity, we start from a new instance of the same base model across different generations, i.e., Base (1) = Base (2) = , ..., = Base (n). In addition, we only use Data (n − 1) to train Model (n), however in a setting closer to the real- life scenario, we have access to the accumulated data ensemble of all predecessors, i.e., Data {(0), (1) , ..., (n − 1)}. This simplification draws from the results of Shumailov et al. (2023), indicating that model collapse is unavoidable, even if the train- ing involves the full ensemble of accumulated data or a sampled subset version, though the effect is somewhat attenuated. In terms of finetuning tasks, we chose three dis- tinct natural language generation tasks, each char- acterized by varying degrees of constraint, from the most restrictive to the least: news summariza- tion, where summaries must closely align with the original content; scientific abstract generation, with some initial context provided, but room for creative expansion; and story generation, which allows for the most creativity and freedom in expression. In the end, we conduct our linguistic diversity research with the fine-tuned Model {(1), (2) , ..., (n)} for each task, subjecting them to evaluation on the test set of corresponding task. 4 Perplexity Our research primarily centers on linguistic diver- sity, yet we also require a reliable metric to verify that our fine-tuned models are well-aligned with the training data. Perplexity, a standard metric for assessing language modeling, evaluates a model’s level of \"surprise\" or \"confusion\" when encoun- tering a given sequence of tokens. Models that more accurately mirror the training data’s distri- bution exhibit lower perplexity. While useful for model comparison, perplexity doesn’t fully reflect text quality (Meister et al., 2023). A low perplex- ity score suggests higher predictive precision, but texts can be grammatically sound and contextually coherent yet still score high in perplexity if they include unusual or creative language not included in the model’s training data (Basu et al., 2021). In our study, a model with lower perplexity isn’t deemed superior by default. Our aim is to ensure that perplexity remains within a reasonable limit, producing texts of sufficient quality for our linguis- tic diversity evaluation. 4.1 Linguistic Diversity Metrics We approach the evaluation of linguistic diversity from three different perspectives: lexical diversity, semantic diversity and syntactic diversity. 4.1.1 Lexical diversity Lexical diversity metrics are used to measure the variety of words used in a text, which is contended to mirror the extent of vocabulary possessed by a writer or speaker. We believe a degenerated LLM, who presumably has a smaller vocabulary, will use a narrower variety of lexical items than non- degenerated LLMs. We select different metrics operating at different levels of textual granularity: word, n-gram, and sentence. Type-Token Ratio (TTR) (Johnson, 1944; Tem- plin, 1957), the most well-known metric, which is calculated as the number of unique words (types) t divided by the number of running words (to- kens) c, i.e., TTR = t/c. This metric was used to study the language development in child lan- guage research, a low value is probably indicative of a language-specific deficiency (Miller, 1981). The length of a text inherently skews vanilla TTR values, with longer texts generally yielding lower TTR scores due to an inexorably decreased occur- rence of unique novel words (draw from a limited vocabulary) as the text lengthens (Richards, 1987). Following a common practice, we truncate all texts to fixed length before computing the TTR. Distinct-n (Li et al., 2016), which is characterized by the proportion between the number of unique n-grams and total number of n-grams in tested text (Xing et al., 2017). This metric is originally in- troduced in the context of enhancing the response diversity of dialogue generation LLMs, which fre- quently produce safe and fluent but dull and unin- formative generic responses at time (e.g., I don’t know) (Han et al., 2022). Similar to naive TTR, distinct-n varies as a function of text length, so we report the results at fixed sizes for n = 2 and n = 3 (distinct-1 is equivalent to TTR). Self-BLEU (Zhu et al., 2018), a recently developed method for evaluating the diversity of artificially generated text. This method assesses the similarity between one sentence and the rest in a group of generated sentences. It treats one sentence as the hypothesis and the others as references to calculate BLEU scores (Papineni et al., 2002). The final Self- BLEU score averages these BLEU scores across all generated sentences. We use a publicly available implementation (Alihosseini et al., 2019)1. We report 1 − Self-BLEU, so a higher value reflects richer diversity of the generation (Palumbo et al., 2020). 4.1.2 Semantic diversity According to recent studies (Tevet and Berant, 2021; Stasaski and Hearst, 2022), the above lexical- level evaluation metrics often fail to capture seman- tic diversity, since texts including similar words can have different semantics and texts with differ- ent words can have similar semantics (Yarats and Lewis, 2018). We tackle this problem by transform- ing sentences into semantically meaningful sen- tence embeddings using Sentence-BERT (Reimers and Gurevych, 2019). We quantify semantic diver- sity as the dispersion of sentence embeddings over the semantic space. The dispersion is measured by either the average pairwise cosine-distance of all embedding vectors (D_sem_p) or the mean cosine 1https://github.com/Danial-Alh/fast-bleu distance of each embedding vector to the centroid (D_sem_c). 4.1.3 Syntactic Diversity The significance of syntactic diversity is often un- derestimated in natural language processing, de- spite its importance. For language learners (as well as language models), exposure to a wide range of syntactic structures is beneficial for developing a more comprehensive understanding of the language (Aggarwal et al., 2022). Moreover, a range of syn- tactic forms enhances expressiveness and subtlety in writing, influencing the style and tone of a text (Edwards and Bastiaanse, 1998). While linguis- tic and language acquisition research (Clercq and Housen, 2017) has explored this aspect, these stud- ies typically rely on manual annotation of features, a process that can be costly and prone to human error. Our research introduces the first automatic met- ric to quantify syntactic diversity. We use a neural parser (Qi et al., 2020) to construct dependency trees from sentences, following the Universal De- pendencies formalism. These trees are then trans- formed into graph representations, with nodes de- noting the words and edges indicating the depen- dency relationships between them. Subsequently, we employ the Weisfeiler-Lehman graph kernel (Shervashidze et al., 2011; Siglidis et al., 2020) to map these graphs into a vector space. This kernel, rooted in the Weisfeiler-Lehman isomorphism test, effectively positions graphs that are structurally alike closer to each other in the embedding space. To assess syntactic diversity, we calculate it simi- larly to semantic diversity, using either the average pairwise distance (D_syn_p) or the average cen- troid distance (D_syn_c) among all graph embed- dings. 5 Experimental Setup We conduct our experiments on three generative tasks, as introduced in Section 3.1, with decreasing degrees of constraint: abstractive news summa- rization, scientific abstract generation, and story generation. Separately for each task, we simulate 6 iterations of the recursive training chain, i.e., n = 6 on Figure 1. Following previous work (Shumailov et al., 2023), we select OPT (Zhang et al., 2022) as our base model, and each iteration begins with a new instance of the base model. We use OPT- 350M instead of OPT-125M to maintain the quality of generated text over iterations, avoiding exces- sive noise. Model (1) is fine-tuned on Data (0)—the training set of the finetuning task—which is human- authored. From Model (2) to Model (6), they are fine-tuned on synthetic Data (n − 1) generated by their predecessor Model (n − 1). We go through all of the original training examples in Data (0) to produce a comparable synthetic dataset of the same size. The models are finetuned for 5 epochs with AdamW optimizer (Loshchilov and Hutter, 2017) on a cluster of two NVIDIA RTX A6000 GPUs. In the following, we explain each of the three tasks in detail. Task1: Abstractive News Summarization For abstractive news summarization, we use the XL-SUM dataset (Hasan et al., 2021), one of the most recently proposed datasets for this task. In comparison with the other prominent news sum- marization datasets, XL-SUM is more abstractive than CNN/DailyMail (Hermann et al., 2015; Cheng and Lapata, 2016) and more factual than XSUM (Narayan et al., 2018; Guo et al., 2022). It is also larger in scale, consisting of 306522 samples in the training set and 11535 samples in the test set. The average length of the news articles is 386 tokens, while the average length of the summaries is 28 tokens. This is a generation task with “low entropy” since there is abundant context and the content is restricted. Task2: Scientific Abstract Generation For scientific abstract generation, we parse the bib- liography database (BibTeX) file with abstracts of ACL Anthology2 in June, 2023. ACL Anthology hosts papers published at computational linguis- tics or natural language processing venues since 1965. We split the bibliography entries into the training and the test set, resulting in 40512 samples for train and 2132 samples for test. We use the title of the paper and the first sentence of the abstract as the prompt, asking the model to finish generating the rest of the abstract. The prompt (title + first sentence) is 42 tokens long on average, while the mean of the full abstract length is 145 tokens. This is a task of “medium entropy”, the provided title already lays out the general idea of the paper and the first sentence provides a fair amount of context. Task3: Story Generation For story generation, we use the WritingPrompts dataset (Fan et al., 2018). It is made up of human 2https://aclanthology.org Lexical Diversity Semantic Diversity Syntactic Diversity Iteration perplexity TTR Distinct-2 Distinct-3 Self-BLEU D_sem_c D_sem_p D_syn_c D_syn_p Human — 3.47 2.51 2.11 26.46 9.60 13.57 3.38 4.76 1 12.5 3.10 (-10%) 2.25 (-10%) 1.89 (-10%) 20.64 (-20%) 9.63 (+0.3%) 13.61 (+0.3%) 3.33 (-1%) 4.68 (-1%) 2 3.42 3.09 (-0.3%) 2.24 (-0.4%) 1.88 (-0.5%) 19.03 (-7%) 9.63 (=) 13.61 (=) 3.32 (-0.3%) 4.67 (-0.2%) 3 3.09 3.06 (-0.9%) 2.22 (-0.8%) 1.86 (-10%) 17.03 (-10%) 9.61 (-0.2%) 13.57 (-0.3%) 3.27 (-1%) 4.60 (-1%) 4 2.86 2.85 (-6%) 2.06 (-7%) 1.73 (-7%) 14.78 (-13%) 9.57 (-0.4%) 13.53 (-0.3%) 3.22 (-1%) 4.51 (-2%) 5 2.62 2.70 (-5%) 1.95 (-5%) 1.63 (-6%) 12.52 (-15%) 9.52 (-0.5%) 13.45 (-0.6%) 3.20 (-0.6%) 4.49 (-0.4%) News Summarization 6 2.48 2.64 (-2%) 1.91 (-2%) 1.57 (-3%) 10.96 (-12%) 9.45 (-0.7%) 13.37 (-0.6%) 3.19 (-0.3%) 4.46 (-0.6%) Human — 3.98 2.83 2.32 13.55 8.86 12.53 5.62 7.89 1 17.8 2.66 (-33%) 1.88 (-33%) 1.54 (-33%) 10.48 (-22%) 8.91 (+0.5%) 12.58 (+0.3%) 6.08 (+8%) 8.48 (+7%) 2 6.89 2.45 (-7%) 1.74 (-7%) 1.42 (-7%) 9.59 (-8%) 8.96 (+0.5%) 12.65 (+0.5%) 6.05 (-0.5%) 8.42 (-7%) 3 5.78 2.36 (-3%) 1.67 (-4%) 1.37 (-3%) 8.05 (-16%) 8.93 (-0.3%) 12.61 (-0.3%) 6.01 (-0.6%) 8.36 (-7%) 4 4.89 2.39 (+1%) 1.69 (+1%) 1.31 (-3%) 7.16 (-11%) 8.88 (-0.5%) 12.54 (-0.5%) 5.65 (-0.6%) 7.89 (-5%) 5 3.92 2.15 (-10%) 1.63 (-3%) 1.24 (-5%) 6.36 (-11%) 8.88 (=) 12.53 (-0.1%) 5.55 (-2%) 7.78 (-1%) Scientific Abstract Generation 6 3.14 2.03 (-5%) 1.52 (-6%) 1.20 (-3%) 5.89 (-7%) 8.87 (-0.1%) 12.51 (-0.1%) 5.30 (-4%) 7.42 (-4%) Human — 5.48 3.88 3.29 10.62 9.40 13.33 20.02 27.83 1 14.1 1.20 (-78%) 0.85 (-78%) 0.69 (-79%) 5.76 (-46%) 9.36 (-0.4%) 13.26 (-0.5%) 15.12 (-24%) 21.34 (-23%) 2 4.41 1.15 (-4%) 0.82 (-4%) 0.67 (-3%) 4.99 (-13%) 9.42 (+0.6%) 13.31 (+0.3%) 14.53 (-4%) 20.13 (-5%) 3 3.37 1.06 (-7%) 0.75 (-9%) 0.61 (-9%) 4.51 (-10%) 9.45 (+0.3%) 13.35 (+0.3%) 13.66 (-6%) 18.95 (-5%) 4 2.99 1.03 (-3%) 0.73 (-3%) 0.60 (-2%) 4.35 (-4%) 9.44 (-0.1%) 13.34 (-0.1%) 13.23 (-3%) 18.22 (-3%) 5 2.82 0.99 (-4%) 0.70 (-4%) 0.57 (-5%) 4.30 (-1%) 9.42 (-0.2%) 13.33 (-0.1%) 12.84 (-3%) 17.74 (-2%) Story Generation 6 2.70 0.94 (-5%) 0.67 (-4%) 0.54 (-5%) 4.24 (-1%) 9.41 (-0.1%) 13.32 (-0.1%) 12.64 (-2%) 17.53 (-1%) Table 1: Perplexity and linguistic diversity metrics for texts generated over different iterations. The value in brackets indicate percentage of variation compared to the previous iteration. In the case of iteration 1, the comparison is against human reference text. written stories paired with writing prompts from Reddit’s WritingPrompts forum. There are 545200 samples in the training set and 30276 samples in the test set. The writing prompts consists of 30 tokens on average and the resulting stories 389 tokens. The prompts are generally short and in most cases do not contain a plot (narrative structure), making this a “high-entropy” generation task with limited context. Decoding Strategy. We use a combination of nucleus sampling (p) and temperature sampling (τ ) to achieve nuanced control over the language model’s outputs. We adapt the specific parameter to the characteristic of each task. For news summa- rization, we emphasize precision and set p = 0.1, τ = 0.3. For story generation, we care more about creativity and set p = 0.9, τ = 0.7. For scientific abstract generation, we want something in between and set p = 0.5, τ = 0.5. The max_new_tokens value is chosen according to the length of human- written references for each task: 50 for news sum- marization, 500 for story generation and 300 for scientific abstract generation. 6 Results In Table 1, we display the perplexity and linguistic diversity metrics for texts generated across various iterations. Our findings indicate that the perplexity values fall within an acceptable range (Holtzman et al., 2020), suggesting that models effectively as- similate training data and generate texts of a qual- ity viable for diversity analysis. A general decline in all linguistic diversity metrics underscores the pressing issue of diminishing linguistic diversity. We highlight some key observations in the follow- ing. The decline of diversity is more important for “high entropy” tasks. We deliberately select three language generation tasks of varying “en- tropy”, which is reflected by the amount and nature of given context, i.e., constraint. News summariza- tion involves a lengthy context with the summary confined to a very limited space, whereas story generation is characterized by brief prompts and a vast array of potential narrative directions. In the highest \"entropy\" task, story generation, the gap in linguistic diversity between human-written and model-generated texts is the most pronounced, and the decline over iterations is the fastest. The signif- icant gap between humans and models is expected, given that story generation demands substantial creativity, a domain where language models are known to fall short (Chakrabarty et al., 2023). The rapid decrease in diversity can also be explained by the creative nature of the task. Models initially learn from diverse original human-written stories but suffer greatly when later exposed solely to syn- thetic data, which already exhibits a notable loss in diversity. Even for “lower entropy” tasks, training on syn- thetic texts will eventually lead to vanishing di- versity. In tasks like news summarization and scientific abstract generation, which have “lower entropy” compared to story generation, there is still a noticeable decrease in linguistic diversity over it- erations. Consider the task of generating scientific abstracts: initially, the syntactic diversity in texts created by Model (1) shows an increase compared to those written by humans. This might be because scientific abstracts inherently possess less varied syntactic structures than the broader range of texts in the pre-training data of OPT-350M. However, as the iterations advance, the syntactic diversity scores of the texts produced by the model steadily decline, eventually dropping significantly below those of human-written abstracts. This trend might be partly attributed to catastrophic forgetting (Mc- Closkey and Cohen, 1989). Additionally, while human-written abstracts may have limited syntactic diversity, their structure is markedly different from the pre-training data, thus introducing new learning elements for the model. In contrast, the synthetic data produced by Model (1), despite its marginally higher internal syntactic diversity, closely mirrors the model’s own training distribution. This lack of novel information leads to a subsequent reduction in variation. Syntactic Diversity suffers significantly. We no- tice that syntactic diversity consistently decreases across all three tasks, comparable to the decline in lexical diversity and to a greater extent than in semantic diversity. While the reduction in lexical diversity is well-researched and somewhat antici- pated, our study is the first to highlight the decrease in syntactic diversity. Syntax, although more im- plicit, is equally important as lexicons in maintain- ing linguistic richness. The significant yet often overlooked decline in syntactic diversity empha- sizes the need for future natural language gener- ation research to include syntactic diversity mea- surements alongside the commonly reported lexical diversity metrics. 7 Conclusion Our study provides critical insights into the impli- cations of recursively training LLMs on synthetic data generated by their predecessors. Through our innovative approach, focusing on novel linguistic diversity measures rather than traditional perfor- mance metrics, across various NLG tasks, we have uncovered a noticeable reduction in lexical, syn- tactic, and semantic diversity in LLM outputs over successive iterations of recursive training on syn- thetic text. These findings highlight a concerning trend: as LLMs increasingly rely on predecessor- generated text for training, there is a tangible risk of diminishing linguistic richness and variety in their outputs. Our research underscores the necessity for a more nuanced and forward-thinking approach in the development of LLMs, emphasizing the impor- tance of preserving linguistic diversity alongside improving technical performance. References Arshiya Aggarwal, Jiao Sun, and Nanyun Peng. 2022. Towards robust NLG bias evaluation with syntactically-diverse prompts. In Findings of the As- sociation for Computational Linguistics: EMNLP 2022, pages 6022–6032, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G Bara- niuk. 2023. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850. Danial Alihosseini, Ehsan Montahaei, and Mahdieh So- leymani Baghshah. 2019. Jointly measuring diversity and quality in text generation models. In Proceedings of the Workshop on Methods for Optimizing and Eval- uating Neural Language Generation, pages 90–98, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Sourya Basu, Govardana Sachitanandam Ramachan- dran, Nitish Shirish Keskar, and Lav R. Varshney. 2021. {MIROSTAT}: A {neural} {text} {decoding} {algorithm} {that} {directly} {controls} {perplexity}. In International Conference on Learning Representa- tions. Tuhin Chakrabarty, Philippe Laban, Divyansh Agar- wal, Smaranda Muresan, and Chien-Sheng Wu. 2023. Art or artifice? large language models and the false promise of creativity. arXiv preprint arXiv:2309.14556. Tuhin Chakrabarty, Vishakh Padmakumar, and He He. 2022. Help me write a poem: Instruction tuning as a vehicle for collaborative poetry writing. In Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6848–6863, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A sur- vey on evaluation of large language models. arXiv preprint arXiv:2307.03109. Jianpeng Cheng and Mirella Lapata. 2016. Neural sum- marization by extracting sentences and words. In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 484–494, Berlin, Germany. As- sociation for Computational Linguistics. Bastien De Clercq and Alex Housen. 2017. A cross- linguistic perspective on syntactic complexity in l2 development: Syntactic elaboration and diversity. The Modern Language Journal, 101(2):315–334. Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al. 2023. Chataug: Lever- aging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007. Susan Edwards and Roelien Bastiaanse. 1998. Diversity in the lexical and syntactic abilities of fluent aphasic speakers. Aphasiology, 12(2):99–117. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational Linguistics. Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Con- erly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. 2022. Predictability and surprise in large gen- erative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Trans- parency, pages 1747–1764. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad- versarial nets. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc. Yanzhu Guo, Chloé Clavel, Moussa Kamal Eddine, and Michalis Vazirgiannis. 2022. Questioning the valid- ity of summarization datasets and improving their factual consistency. In Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing, pages 5716–5727, Abu Dhabi, United Arab Emirates. Association for Computational Lin- guistics. Seungju Han, Beomsu Kim, and Buru Chang. 2022. Measuring and improving semantic diversity of di- alogue generation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 934–950, Abu Dhabi, United Arab Emirates. Associ- ation for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is- lam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XL- sum: Large-scale multilingual abstractive summariza- tion for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693–4703, Online. Association for Computa- tional Linguistics. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text de- generation. In International Conference on Learning Representations. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610. Daphne Ippolito, Reno Kriz, João Sedoc, Maria Kustikova, and Chris Callison-Burch. 2019. Compar- ison of diverse decoding methods from conditional language models. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 3752–3762, Florence, Italy. Asso- ciation for Computational Linguistics. Wendell Johnson. 1944. Studies in language behavior: A program of research. Psychological Monographs, 56(2):1–15. Bohan Li, Yutai Hou, and Wanxiang Che. 2022a. Data augmentation approaches in natural language pro- cessing: A survey. AI Open, 3:71–90. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting ob- jective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 110–119, San Diego, California. Association for Computational Linguistics. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022b. Contrastive de- coding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku- mar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110. Ilya Loshchilov and Frank Hutter. 2017. Decou- pled weight decay regularization. arXiv preprint arXiv:1711.05101. Tshilidzi Marwala, Eleonore Fournier-Tombs, and Serge Stinckwich. 2023. The use of synthetic data to train ai models: Opportunities and risks for sustainable development. arXiv preprint arXiv:2309.00652. Michael McCloskey and Neal J. Cohen. 1989. Catas- trophic interference in connectionist networks: The sequential learning problem. In Gordon H. Bower, editor, Psychology of learning and motivation, vol- ume 24 of Psychology of Learning and Motivation, pages 109–165. Academic Press. Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2023. Locally typical sampling. Transac- tions of the Association for Computational Linguis- tics, 11:102–121. J.F. Miller. 1981. Assessing Language Production in Children: Experimental Procedures. Assessing com- municative behavior. University Park Press. Anirudh Mittal, Yufei Tian, and Nanyun Peng. 2022. AmbiPun: Generating humorous puns with ambigu- ous context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 1053–1062, Seattle, United States. Association for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797–1807, Brussels, Bel- gium. Association for Computational Linguistics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744. Vishakh Padmakumar and He He. 2023. Does writ- ing with language models reduce content diversity? arXiv preprint arXiv:2309.05196. Enrico Palumbo, Andrea Mezzalira, Cristina Marco, Alessandro Manzotti, and Daniele Amberti. 2020. Semantic diversity for natural language understand- ing evaluation in dialog systems. In Proceedings of the 28th International Conference on Computational Linguistics: Industry Track, pages 44–49, Online. In- ternational Committee on Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics: System Demonstrations, pages 101–108, Online. As- sociation for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Association for Com- putational Linguistics. Brian Richards. 1987. Type/token ratios: What do they really tell us? Journal of child language, 14(2):201– 209. Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. 2011. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12(null):2539–2561. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. The curse of recursion: Training on generated data makes models forget. Giannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Konstantinos Skianis, and Michalis Vazirgiannis. 2020. Grakel: A graph ker- nel library in python. Journal of Machine Learning Research, 21(54):1–5. Katherine Stasaski and Marti Hearst. 2022. Semantic diversity in dialogue with natural language inference. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 85–98, Seattle, United States. Association for Computational Linguistics. Mildred C. Templin. 1957. Certain Language Skills in Children: Their Development and Interrelationships, ned - new edition edition, volume 26. University of Minnesota Press. Guy Tevet and Jonathan Berant. 2021. Evaluating the evaluation of diversity in natural language generation. In Proceedings of the 16th Conference of the Euro- pean Chapter of the Association for Computational Linguistics: Main Volume, pages 326–346, Online. Association for Computational Linguistics. Yufei Tian, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Gunnar Sigurdsson, Chenyang Tao, Wenbo Zhao, Tagyoung Chung, Jing Huang, and Nanyun Peng. 2023. Unsupervised melody-to-lyrics generation. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9235–9254, Toronto, Canada. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Dongwon Lee, et al. 2023. Does human collaboration enhance the accuracy of identifying llm-generated deepfake texts? In Proceedings of the AAAI Con- ference on Human Computation and Crowdsourcing, volume 11, pages 163–174. Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2018. Diverse beam search for improved description of complex scenes. Proceed- ings of the AAAI Conference on Artificial Intelligence, 32(1). Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceed- ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484–13508, Toronto, Canada. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea- soning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837. Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic aware neural response generation. In Proceedings of the AAAI conference on artificial intelligence, volume 31. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196. Denis Yarats and Mike Lewis. 2018. Hierarchical text generation and planning for strategic dialogue. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5591–5599. PMLR. Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021. Trading off diversity and quality in natural language generation. In Proceed- ings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 25–33, Online. Associa- tion for Computational Linguistics. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi- haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre- trained transformer language models. Weixiao Zhou, Gengyao Li, Xianfu Cheng, Xinnian Liang, Junnan Zhu, Feifei Zhai, and Zhoujun Li. 2023. Multi-stage pre-training enhanced by chatgpt for multi-scenario multi-domain dialogue summariza- tion. arXiv preprint arXiv:2310.10285. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A benchmarking platform for text generation models. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 1097–1100.","libVersion":"0.3.2","langs":""}
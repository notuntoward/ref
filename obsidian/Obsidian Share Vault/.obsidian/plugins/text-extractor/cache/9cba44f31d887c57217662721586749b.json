{"path":"lit/sources/papers_added/papers/Fowler11densitySumCorrRVs.pdf","text":"px y e xy xy xx yy xx y y xy xy (, ) () () () ( )( ) = − − − + − − −−⎛ ⎝ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ − 1 21 2 2 2 2 2 2 2 21 ρσ σ ρ σσ πσ σ ρ pz dP Z dZ px p z x dxz z xy() () () ( )== −∫ −∞ ∞ pz dP Z dZ px z x dxz z xy() () (, )== −∫ −∞ ∞ PZ p x y dxdyzxy D () ( , )= ∫∫ P Z p x p y dy dxzx y Zx () ( ) ( )= ∫ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟∫ −∞ − −∞ ∞ PZ p x y dy dxzxy Zx () ( , )= ∫ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟∫ −∞ − −∞ ∞ Density Function for the Sum of Correlated Random Variables John W. Fowler 27 December 2011 When two random variables are independent, the probability density function for their sum is the convolution of the density functions for the variables that are summed. We consider here the case when these two random variables are correlated. Let x and y be the two correlated random variables, and z = x + y. The standard procedure for obtaining the distribution of a function z = g(x,y) is to integrate the joint density function pxy(x,y) over the region D of the xy plane where g(x,y) < Z to obtain the cumulative distribution Pz(Z). This is then differentiated with respect to Z to obtain the density function pz(z). The cumulative distribution is: (1) In some cases, D may be disjoint, but for z = x + y it is just the area under that line. When x and y are independent, the joint density function separates into a product of the two marginal density functions px(x) and py(y), and the procedure we are about to describe, along with y = z - x, leads directly to the convolution: (2) Since we are considering x and y to be correlated, we must retain the joint density function itself: (3) The failure of the joint density function to separate results in pz(z) no longer having the form of a convolution. To illustrate this we consider the Gaussian case. The joint density function for Gaussian x and y coupled through a correlation coefficient ρ is (4) where we don’t assume that either x or y is zero-mean, because we must define them on an absolute ()() () pz e z xy x y zx y xy x y () )( = ++ − −+ ++ 2 2222 22 22 σσ ρσ σ πσ σ ρσ σ z x y zx y x y = + =++σσσ ρσ σ 22 2 2 zxi i N = ∑ −1 () px e NN N N N () / / r = − χ π 2 2 2 2 Ω zz x x xi i N i i N i i N == ∑ = ∑ = ∑ == =11 1 σ ρσ σ zi i N i i N ii i N ii i N jj j N ii j j j N i N ii j j j N i N ij i j j N i zz x x x x x x xx x x xx xx x x 22 11 2 1 2 11 11 111 =− = ∑ − ∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟ =−∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟ =−∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟ −∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟ =− −∑∑ =− − =∑∑∑ == = == == === () ( ) () ( ) ()( ) ()( ) = ∑ 1 N scale in order to take into account the fact that in general the means may be different. Using this in Equation 3 produces (5) which is Gaussian with (6) So the mean is unaffected by the correlation, but the variance is made larger or smaller according to whether the correlation is positive or negative, respectively. The variance is in fact the sum of the elements of the covariance matrix. In order to investigate whether this is a general property, we define a set of N random variables (not necessarily Gaussian), xi, i = 1 to N, with corresponding means xGi, variances σ2i, and correlation matrix with elements ρij, i,j = 1 to N. Then we define (7) Taking expectation values, (8) and we see that the first line in Equation 6 is just a special case of a general property of random variables that does not depend on distribution. The second line of Equation 6 can also be seen to be a special case of such a general property as follows: (9) So we don’t need the density function to find the mean and variance of correlated random variables. For the case in which all xi are Gaussian, the joint density function is (10) ()( )χNij i j j N i N wx xx x 2 11 =− −∑∑ == zxi i N zij i j j N i N = ∑ = ∑∑ = == 1 2 11 σρ σ σ σσ ρzij j N i N 22 11 = ∑∑ == Ω N NN NN NN N N N = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ σρ σ σ ρ σ σ ρσ σ σ ρ σ σ ρσ σ ρ σ σ σ 1 2 12 1 2 1 1 12 1 2 2 2 22 11 2 2 2 K L MM O M L z x p x dx dx xx p x dx dx i i N NN N zi i i N NN N = ∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟∫∫ =−∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟∫∫ =−∞ ∞ −∞ ∞ =−∞ ∞ −∞ ∞ L r K L r K 1 1 22 1 1 () () ( )σ where the x vector is the set of xi, ΩN is the covariance matrix, and |ΩN | is its determinant: (11) and (12) where the wij are the elements of the inverse of the covariance matrix. Explicit computations of (13) for N equal to 2, 3, 4, and 5 show that indeed in each case (14) as we would expect. In this Gaussian case, because of the associative law of arithmetic, we can assume that the density function for z is Gaussian for all N, since it is Gaussian for x1 + x2, and therefore it is Gaussian for (x1 + x2) + x3, etc., and therefore the mean and variance in Equation 14 can simply be plugged into the density function for a single Gaussian random variable. When the σ2i are all equal, as in the case of random draws from a single population, the variance of the sum becomes (15) i.e., the population variance multiplied by the sum over the elements of the correlation matrix. Since the sample mean is zG/N, the uncertainty of the sample mean may be greater or smaller than for a sample of uncorrelated random draws, depending on the off-diagonal elements of the correlation matrix.","libVersion":"0.3.1","langs":""}
{"path":"lit/lit_sources/Gupta24DebiasingInSamplePolicy.pdf","text":"Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization Vishal Gupta, Michael Huang, and Paat Rusmevichientong Data Science and Operations, USC Marshall School of Business, Los Angles, CA 90089, guptavis@usc.edu, huan076@usc.edu, rusmevic@marshall.usc.edu Motivated by the poor performance of cross-validation in settings where data are scarce, we propose a novel estimator of the out-of-sample performance of a policy in data-driven optimization. Our approach exploits the optimization problem’s sensitivity analysis to estimate the gradient of the optimal objective value with respect to the amount of noise in the data and uses the estimated gradient to debias the policy’s in-sample performance. Unlike cross-validation techniques, our approach avoids sacriﬁcing data for a test set, utilizes all data when training and, hence, is well-suited to settings where data are scarce. We prove bounds on the bias and variance of our estimator for optimization problems with uncertain linear objectives but known, potentially non-convex, feasible regions. For more specialized optimization problems where the feasible region is “weakly-coupled” in a certain sense, we prove stronger results. Speciﬁcally, we provide explicit high- probability bounds on the error of our estimator that hold uniformly over a policy class and depends on the problem’s dimension and policy class’s complexity. Our bounds show that under mild conditions, the error of our estimator vanishes as the dimension of the optimization problem grows, even if the amount of available data remains small and constant. Said diﬀerently, we prove our estimator performs well in the small-data, large-scale regime. Finally, we numerically compare our proposed method to state-of-the- art approaches through a case-study on dispatching emergency medical response services using real data. Our method provides more accurate estimates of out-of-sample performance and learns better-performing policies. Key words : Large-scale, data-driven optimization. Small-data, large-scale regime. Cross-validation. 1. Introduction The crux of data-driven decision-making is using past data to identify decisions that will have good out-of-sample performance on future, unseen data. Indeed, estimating out-of-sample performance is key to both policy evaluation (assessing the quality of a given policy), and to policy learning (identifying the best policy from a potentially large set of candidates). Estimating out-of-sample performance, however, is non-trivial. Naive estimates that leverage the same data to train a policy and to evaluate its performance often suﬀer a systematic, optimistic bias, referred to as “in-sample bias” in machine learning and the “optimizer’s curse” in optimization (Smith and Winkler 2006). Consequently, cross-validation and sample-splitting techniques have emerged as the gold- standard approach to estimating out-of-sample performance. Despite the multitude of cross- 1arXiv:2107.12438v4 [math.OC] 2 Aug 2022 Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 2 validation methods, at a high-level, these methods all proceed by setting aside a portion of the data as “testing” data not to be used when training the policy, and then evaluating the policy on these testing data. The policy’s performance on testing data then serves as an estimate of its performance on future, unseen data, thereby circumventing the aforementioned in-sample bias. Cross-validation is ubiquitous in machine learning and statistics with provably good performance in large sample settings (Bousquet and Elisseeﬀ 2001, Kearns and Ron 1999). Unfortunately, when data are scarce, cross-validation can perform poorly. Gupta and Rus- mevichientong (2021) prove that for the small-data, large-scale regime — when the number of uncertain parameters in an optimization problem is large but the amount of relevant data per parameter is small — each of hold-out, 5-fold, 10-fold and leave-one-out cross validation can have poor performance when used for policy learning, even for very simple optimization problems. Shao (1993) observes a similar failure for leave-one-out cross-validation in a high-dimensional linear regression setting. The key issue in both cases is that when relevant data are scarce, estimates of uncertain parameters are necessarily imprecise, and omitting even a small amount of data when training a policy dramatically degrades its performance. Hence, the performance of a policy trained with a portion of the data on the test set is not indicative of the performance of the policy trained with all the data on future unseen data. We elucidate this phenomenon with a stylized example in Section 1.2 below. Worse, this phenomenon is not merely an intellectual curiosity. Optimization problems plagued by numerous low-precision estimates are quite common in modern, large-scale operations. For exam- ple, optimization models for personalized pricing necessarily include parameters for each distinct customer type, and these parameters can be estimated only imprecisely since relevant data for each type are limited. Similar issues appear in large-scale supply-chain design, promotion optimization, and dispatching emergency response services; see Section 2 for further discussion. In this paper, we propose a new method for estimating out-of-sample performance without sacriﬁcing data for a test set. The key idea is to debias the in-sample performance of the policy trained on all the data. Speciﬁcally, we focus on the optimization problem x∗ ∈ arg min x∈X ⊆[0,1]n µ ⊤x (1.1) where X is a known, potentially non-convex feasible region contained within [0, 1]n, and µ ∈ Rn is an unknown vector of parameters. We assume access to a vector Z of noisy, unbiased predictions of µ (based on historical data) and are interested in constructing a policy x(Z) with good out-of- sample performance µ ⊤x(Z). (For clarity, the in-sample performance of x(Z) is Z ⊤x(Z).) Note that for many applications of interest, µ ⊤x∗ = O(n) as n → ∞; i.e., the full-information solution Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 3 grows at least linearly as the dimension grows. Hence, the unknown out-of-sample performance µ ⊤x(Z) must also be at least Op(n) as n → ∞.1 See Section 2 for examples. Despite its simplicity, Problem (1.1) subsumes a wide class of optimization problems because X can be non-convex and/or discrete. This class includes mixed-binary linear optimization problems such as facility location, network design, and promotion maximization. By transforming decision variables, even some non-linear optimization problems such as personalized pricing can be rewritten as Problem (1.1); see Section 2. In this sense, Problem (1.1) is fairly general. Our estimator applies to classes of aﬃne plug-in policies which are formally deﬁned in Section 2. Loosely, aﬃne plug-in policies are those obtained by solving Problem (1.1) after “plugging-in” some estimator rj(Zj) in place of µj, and rj(Zj) depends aﬃnely on Zj. Many policies used in practice and previously studied in the literature can be viewed as elements of an aﬃne plug-in policy class including Sample Average Approximation (SAA), estimate-then-optimize policies based on regression, the Bayes-Inspired policies of Gupta and Rusmevichientong (2021), and the SPO+ policy of Elmachtoub and Grigas (2021). Thus, our estimator provides a theoretically rigorous approach to assessing the quality of optimization policies based on many modern machine learning techniques. We debias Z ⊤x(Z) by exploiting the structure of Problem (1.1) with the plug-in r(Z). Specif- ically, by leveraging this problem’s sensitivity analysis, we approximately compute the gradient of its objective value with respect to the variance of Z, and use the estimated gradient to debias the in-sample performance. We term this correction the Variance Gradient Correction (VGC). Because our method strongly exploits optimization structure, the VGC is Lipschitz continuous in the plug-in values r(Z). This continuity is not enjoyed by other techniques such as those in Gupta and Rusmevichientong (2021). Although the VGC’s continuity may seem like mere a mathematical nicety, empirical evidence suggests it improves empirical performance. Similar empirical phenomena – where an estimator that varies smoothly in the data often outperforms similar estimators that change discontinuously – are rife in machine learning. Compare k-nn regression with Gaussian kernel smoothing (Friedman et al. 2001), CART trees with bagged trees (Breiman 1996), or best subset-regression with lasso regression (Hastie et al. 2020a). Theoretically, we exploit this smoothness heavily to establish bounds that hold uniformly over the policy class. Speciﬁcally, we show that, when Z is approximately Gaussian, the bias of our estimator for out-of- sample performance is ˜O(h) as h → 0, where h is a user-deﬁned parameter that controls the accuracy 1 Following Van der Vaart (2000), we say a sequence of random variables Xn = Op(an) if the sequence Xn/an is stochastically bounded, i.e., for every ϵ > 0, there exists ﬁnite M > 0 and ﬁnite N > 0 such that P {Xn/an ≥ M } < ϵ, for all n > N . Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 4 of our gradient estimate (Theorem 3.2). Characterizing the variance is more delicate. We introduce the concept of Average Solution Instability, and prove that if the instability of the policy vanishes at rate O(n−α) for α ≥ 0, then the variance of our estimator is roughly O(n3−α/h). Collectively, these results suggest interpreting h as a parameter controlling the bias-variance tradeoﬀ of our estimator. Moreover, when α > 1, the variance of our estimator is o(n2). Since, as mentioned, the unknown out-of-sample performance often grows at least linearly in n, i.e., µ ⊤x(Z) = Op(n), our variance bound shows that when α > 1 and n is large, the stochastic ﬂuctuations of our estimator are negligible relative to the out-of-sample performance. In other words, our estimator is quite accurate in these settings. Our notion of Average Solution Instability is formally deﬁned in Section 3.3. Loosely, it measures the expected change in the jth component of the policy after replacing the kth data point with an i.i.d. copy, where j and k are chosen uniformly at random from {1, . . . , n}. This notion of stability is similar to hypothesis stability (Bousquet and Elisseeﬀ 2001), but, to the best of our knowledge, is distinct. Moreover, insofar as we expect that a small perturbation of the data is unlikely to have a large change on the solution for most real-world, large-scale optimization problems, we expect Average Solution Instability to be small and our estimator to have low variance. We then prove stronger high-probability tail bounds on the error of our estimator for two special classes of “weakly-coupled” instances of Problem (1.1): weakly-coupled-by-variables and weakly-coupled-by-constraints. In Section 4.1, we consider problems that are weakly-coupled-by- variables, i.e., problems that decouple into many, disjoint subproblems once a small number of deci- sion variables are ﬁxed. In Section 4.2 we consider problems that are weakly-coupled-by-constraints, i.e., problems that decouple into many, disjoint subproblems once a small number of constraints are removed. For each problem class, we go beyond bounding the variance to provide an explicit tail bound on the relative error of our estimator that holds uniformly over the policy class. We show that for problems weakly-coupled-by-variables the relative error scales like ˜O(CPI polylog(1/ϵ) 3√n ) where CPI is a constant measuring the complexity of the policy class; see Theorem 4.3. Similarly, we show the relative error for problems weakly-coupled-by-constraints scales like ˜O(CPI polylog(1/ϵ) 4√ n ), where CPI measures both the complexity of the policy class and number of constraints of the problem; see Theorem 4.7. Importantly, since these bounds hold uniformly, our debiased in-sample performance can be used both for policy evaluation and policy learning, even when data are scarce, so long as n (the dimension of the problem) is suﬃciently large. Said diﬀerently, our estimator of out-of-sample performance is particularly well-suited to small-data, large-scale optimization. Admittedly, weakly-coupled problems as described above do not cover all instances of Prob- lem (1.1) and the appropriateness of modeling Z as approximately Gaussian is application speciﬁc. Nonetheless, our results and their proofs highly suggest our estimator will have strong performance Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 5 whenever the underlying optimization problem is well-behaved enough for certain uniform laws of large numbers to pertain. Finally, to complement these theoretical results, we perform a numerical case study of dispatch- ing emergency medical services with real data from cardiac arrest incidents in Ontario, Canada. With respect to policy evaluation and learning, we show that our debiased in-sample performance outperforms both traditional cross-validation methods and the Stein correction of Gupta and Rus- mevichientong (2021). In particular, while the bias of cross-validation is non-vanishing as the problem size grows for a ﬁxed amount of data, the bias of our VGC converges to zero. Similarly, while both the Stein correction and our VGC have similar asymptotic performance, the smoothness of VGC empirically leads to lower bias and variance for moderate and sized instances. 1.1. Our Contributions We summarize our contributions as follows: 1. We propose an estimator of out-of-sample performance for Problem (1.1) by debiasing in-sample performance through a novel Variance Gradient Correction (VGC). Our VGC applies to a general class of aﬃne plug-in policies that subsumes many policies used in prac- tice. Most importantly, unlike cross-validation, VGC does not sacriﬁce data when training, and, hence, is particularly well-suited to settings where data are scarce. 2. We prove that under some assumptions on the data-generating process, for general instances of Problem (1.1), the bias of our estimator is at most ˜O(h) as h → 0, where h is a user-deﬁned parameter. For policy classes that satisfy a certain Average Solution Instability condition, we also prove that that its variance scales like o ( n2 h ) as n → ∞. 3. We prove stronger results for instances of Problem (1.1) in which the feasible region is only weakly-coupled. When the feasible region is weakly-coupled by variables, we prove that, with probability at least 1 − ϵ, debiasing in-sample performance with our VGC recovers the true out-of-sample performance up to relative error that is at most ˜O (CPI log(1/ϵ) n1/3 ) as n → ∞, uniformly over the policy class, where CPI is a constant that measures the complexity of the plug-in policy class (Theorem 4.3). Similarly, for certain linear optimization problems that are weakly coupled by constraints, we prove that, with probability at least 1 − ϵ, debiasing in- sample performance with VGC estimates the true out-of-sample performance uniformly over the policy class with relative error that is at most ˜O (CPI √log(1/ϵ) n1/4 ) where CPI is a constant measuring the complexity of the plug-in policy class and the number of of constraints of the problem. (Theorem 4.7). We stress that since both these bounds hold uniformly, our debiased in-sample performance can not only be used for policy evaluation, but also policy learning, even when data are scarce, so long as n (the size of the problem) is suﬃciently large. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 6 4. Finally, we present a numerical case study based on real data from dispatching emergency response services to highlight the strengths and weaknesses of our approach relative to cross- validation and the Stein correction of Gupta and Rusmevichientong (2021). Overall, we ﬁnd that since our VGC exploits the optimization structure of Problem (1.1), it outperforms the benchmarks when the number of uncertain parameters is suﬃciently large. Additionally, in settings where the signal to noise ratio is low, VGC more eﬀectively balances the bias-variance trade-oﬀ than cross-validation which can be quite sensitive to the number of folds used. 1.2. A Motivating Example: Poor Performance of Cross-Validation with Limited Data Before proceeding, we present an example that highlights the shortcomings of cross-validation and the beneﬁts of our method when data are limited. Consider a special case of Problem (1.1) max x∈{0,1}n n∑ j=1 µjxj (1.2) where the true parameters µ ∈ {−1, 1}n are unknown, but we observe S samples Y1, . . . , YS where Yi ∈ Rn and Yi ∼ N (µ, 2I) for all i and I is the identity matrix. A standard data-driven policy in this setting is Sample Average Approximation (SAA), also called empirical risk minimization, which prescribes the policy xSAA(Z) ∈ arg max x∈{0,1}n n∑ j=1 Zjxj where Zj = 1 S S∑ i=1 Yij. The key question, of course, is “What is SAA’s out-of-sample behavior µ⊤xSAA(Z)?” To study this question, the left panel of Fig. 1 shows diﬀerent estimators for the out-of-sample performance of SAA E [µ⊤xSAA(Z)] when S = 3 in Problem (1.2). The right panel shows the expected relative error (with respect to the oracle) of these estimators as the number of samples S grows. To account for the noise level of the samples, we plot the estimation error with respect to the signal-to-noise ratio (SNR) of Zj 2. For reference, Hastie et al. (2020b) argues that SNR greater than 1 is “rare” when working with “noisy, observational data,” and an SNR of 0.25 is more “typical.” The ﬁrst row of Fig. 1 presents the in-sample performance, i.e, the objective of the SAA problem. As expected, we see in-sample performance signiﬁcantly over-estimates the out-of-sample perfor- mance. The right panel of Fig. 1 suggests this eﬀect persists across SNRs and the relative error is at least 23% for SNRs less than 2. 2 Following Hastie et al. (2020b), we deﬁne SN R = V ar(µπ)/V ar(Zπ) = S 2n ∑n j=1 ( µj − 1 n ∑n i=1 µi)2 where π is an index drawn uniformly random from 1 to n. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 7 xSAA(Z) xSAA(Z −i) In-Sample 18.36 22.33 Cross-Val −1.86 −9.98 Our Method 2.95 −1.89 Oracle 2.97 −1.87 -100% 0% 100% 200% 0.75 1.00 1.25 1.50 1.75 2.00 Signal-to-Noise RatioEstimationError(%ofOracle) In-Sample Our Method Cross-Val Figure 1 Expected Estimates of Out-of-Sample Performance by Policy for Problem (1.2). In the left table, we take n = 100, S = 3 and µj = 1 if j ≤ 14 and µj = −1 otherwise. We estimate the expected out-of-sample perf. across 1,000,000 simulations. Std. errors are less than 0.005. In the right graph, we plot the bias of the estimates with respect to the expected out-of-sample performance as we increase the signal-to-noise ratio. The In-Sample error not shown at 0.75 SNR exceeds 500%. The second row of the left panel of Fig. 1 shows the leave-one-out cross-validation error, which aims to correct the over-optimistic bias and computes 1 S ∑S i=1 Y ⊤ i xSAA(Z −i), where Z −i = 1 S−1 ∑ j̸=i Yj. Cross-validation is also fairly inaccurate, suggesting SAA performs worse than the trivial, non-data-driven policy x = 0, which has an out-of-sample performance of 0. In the right panel, this incorrect implication occurs for SNRs less than about 0.875. Why does cross-validation perform so poorly? By construction cross-validation omits some data in training, and hence, does not estimate the out-of-sample performance of xSAA(Z), but rather, that of xSAA(Z −1). 3 From the second column of Fig. 1, we see the cross-validation estimate does nearly match the true (oracle) performance of xSAA(Z −1). When data are scarce, sacriﬁcing even a small amount of data in training can dramatically degrade a policy. As seen in the right panel of Fig. 1, this phenomenon is non-negligible (at least 10% relative error) for signal-to-noise ratios less than or equal to 1.75. Thus, the performance of xSAA(Z −1) may not always be a good proxy of the performance of xSAA(Z). How then might we resolve the issue? The third row of the left panel of Fig. 1 presents our estimator based on debiasing the in-sample performance of xSAA(Z) with our VGC. Our estimate is essentially unbiased (see also Theorem 3.2 below). The right panel of Fig. 1 conﬁrms this excellent behavior across a range of signal-to-noise ratios. Finally, although this example focuses on the bias of our estimator, our results in Section 4 are stronger and bound the (random) error of our estimator directly, rather than its expectation. 3 For clarity, this is the same as the performance as xSAA(Z −2) because the data are i.i.d. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 8 1.3. Relationship to Prior Work Cross-validation is the gold-standard for estimating out-of-sample performance in the large-sample regime with i.i.d. data; see Bousquet and Elisseeﬀ (2001), Kearns and Ron (1999) for some fun- damental results. As discussed above, when estimating the performance of a ﬁxed-policy, these approaches entail sacriﬁcing some data in the training step to set aside for validation, and, hence, may be ill-suited to data-scarce settings. Similar issues arise in a variety of other sample-splitting methods, including “honest-trees” Wager and Athey (2018) and most forms of doubly-robust esti- mation Dud´ık et al. (2011). By contrast, our VGC based approach to debiasing the in-sample performance eﬀectively uses all the data when training, making it somewhat better suited to data- scarce settings and small-data, large-scale optimization. Our work also contributes to the growing literature on “optimization-aware” estimation. These works employ a variety of names including operational statistics (Liyanage and Shanthikumar 2005), learning-enabled optimization (Deng and Sen 2018), decision-focused learning (Wilder et al. 2019a), end-to-end learning (Wilder et al. 2019b) and task-based learning (Donti et al. 2017). Fundamentally, this area of research seeks estimators that optimize the out-of-sample performance of a policy in a downstream optimization problem rather than the prediction error of the estimate. Closest to our work is the “Smart ‘Predict then Optimize’” framework studied in Elmachtoub and Grigas (2021) and Elmachtoub et al. (2020). These works also study Problem (1.1), but in a slightly diﬀerent data setting, and propose policy selection methods for aﬃne and tree-based policies, respectively. Also related is Ito et al. (2018) which develops an unbiased estimate of the sample average approximation (SAA) policy for Problem (1.1), but does not consider higher level moments, policy evaluation for other policies, or policy learning. Recently, El Balghiti et al. (2019), Hu et al. (2022) have sought to establish generalization guar- antees for data-driven policies for plug-in policies for Problem (1.1), i.e., bounds on the diﬀerence between in-sample performance and out-of-sample performance that hold uniformly over the policy class. Both works prove generalization guarantees that vanish in the large-sample regime (when the amount of data grows large). We similarly bound the diﬀerence between our debiased in-sample performance and out-of-sample performance, uniformly over a policy class. However, unlike the previous works, our bounds are speciﬁcally constructed to vanish relative to the unknown out-of- sample performance in the small-data, large-scale regime. When applied to the types of policies studied in El Balghiti et al. (2019), Hu et al. (2022), our debiased in-sample performance equals the ordinary in-sample performance (see discussion of “Policy Classes that Do Not Depend on Z” in Section 3). Hence, our results can be reinterpreted as generalization guarantees for these classes, showing that generalization error vanishes relative to the out-of-sample performance as the Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 9 problem size grows. In this sense, our work complements the large-sample analysis of El Balghiti et al. (2019), Hu et al. (2022) with new, small-data, large-scale analysis. More generally, there has been somewhat less work in the small-data, large-scale regime, most notably Gupta and Kallus (2021) and Gupta and Rusmevichientong (2021). Of these, Gupta and Rusmevichientong (2021), henceforth GR 2021, is most closely related to our work. Loosely, GR 2021 study a class of weakly coupled linear optimization problems and propose an estimator of the out-of-sample performance based on Stein’s Lemma. By leveraging a careful dual- ity argument, the authors prove that the estimation error of their procedure vanishes in both the large-sample and small-data, large-scale regime. Our work diﬀers in two important respects: First, our estimator applies to a more general class of problems and more general policy classes. Indeed, we focus on Problem (1.1) with specialized results for weakly-coupled instances. Our weakly-coupled by constraints variant in Section 4.2 mirrors the setting of GR 2021, and our weakly-coupled by variables variant in Section 4.1 is more general, allowing us to model, for example, discrete optimization problems. Moreover, our aﬃne plug-in policy class signiﬁcantly generalizes the “Bayes-Inspired” policy class of GR 2021 by incorporating covariate information. The second important diﬀerence from GR 2021 relates to exploiting optimization structure in Problem (1.1). GR 2021 fundamentally relies on Stein’s lemma, a result which applies to general functions and does not speciﬁcally leverage optimization structure. By contrast, our method directly leverages the structure of Problem (1.1) through its sensitivity analysis and Danskin’s theorem. By leveraging optimization structure, our VGC is, by construction, continuous in the policy class. Evidence from Section 5 suggests this smoothness yields an empirical advantage of our method. Finally, our work also contributes to a growing literature on debiasing estimates in high- dimensional statistics, most notably for LASSO regression (Javanmard and Montanari 2018, Zhang and Zhang 2014) and M -estimation (Javanmard and Montanari 2014). Like these works, VGC involves estimating a gradient of the underlying system and using this gradient information to form a correction. Unlike these works, however, our gradient estimation strongly leverages ideas from sensitivity analysis in optimization. Moreover, the proofs of our performance guarantees involve substantively diﬀerent mathematical techniques. 2. Model As mentioned, our focus is on data-driven instances of Problem (1.1) where the feasible region X is known, but the parameters µ are unknown. Despite its simplicity, several applications can be modeled in this form after a suitable transformation of variables. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 10 Example 2.1 (Promotion Optimization) Promotion optimization is an increasingly well- studied application area (Cohen et al. 2017, Baardman et al. 2019). Our formulation mirrors a formulation from the ride-sharing company Lyft around incentive allocation (Schmoys and Wang 2019), but also resembles the online advertising portfolio optimization problem (Rusmevichientong (2006), Pani and Sahin (2017),GR 2021). The decision-maker (platform) seeks to allocate J diﬀerent types of coupons (promotions) to K diﬀerent customer (passenger) types. Coupons are costly, and there is a ﬁnite budget C available. Let µjk be the reward (induced spending) and cjk be the cost of assigning coupon type j to customer type k. Using xjk to denote the fraction of customers of type k who receive coupons of type j, we can formulate the following linear optimization problem of the form of Problem (1.1). max x≥0 { K∑ k=1 J∑ j=1 µjkxjk : J∑ j=1 xjk ≤ 1 for each k = 1, . . . , K, K∑ k=1 J∑ j=1 cjkxjk ≤ C } . In typical instances, the cost cjk are likely known (a “$10 oﬀ” coupon costs $10), whereas the reward µjk must be estimated from historical data. In settings with many types of coupons and customers, we might further expect that the reward estimates may be imprecise. Some reﬂection suggests many linear optimization problems including shortest-path with uncertain edge costs, or even binary linear optimization problems like multi-choice knapsack with uncertain rewards can be cast as above. We next observe that some two-stage linear optimization problems can also be framed as Prob- lem (1.1). Example 2.2 (Drone-Assisted Emergency Medical Response) In recent years, emergency response systems have begun utilizing drones as part of their operations, speciﬁcally for rapid delivery of automatic electronic deﬁbrillators (AEDs) for out-of-hospital cardiac arrests (OHCA) (Sanfridsson et al. 2019, Cheskes et al. 2020). The intention is that a drone might reach a patient in a remote region before a dispatched ambulance, and (untrained) bystanders can use the AED to assist the patient until the ambulance arrives. Consequently, researchers have begun studying both how to design a drone-assisted emergency response network (where to locate depots) (Boutilier and Chan 2019) and how to create optimal dispatch rules (to which locations should we allocate a drone and from which depot) (Chu et al. 2021). Combining these two problems yields a two-stage optimization problem, similar to facility location, aimed at minimizing the response time. Namely, let µkl be the response time of drone routed from a source l for to a patient location k, l = 1, . . . , L and k = 1, . . . , K. Let yl ∈ {0, 1} be a binary decision variable encoding if we build a drone depot at location l, and let xkl be a binary decision variable encoding if, after building Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 11 the network, we should dispatch a drone from location l to patient requests at location k. We let xk0 be the choice not to route a drone (sending only an ambulance) to location k and µk0 be the corresponding ambulance travel time. Suppose we can build at most B depots. Then, we have the following optimization problem. min y∈{0,1}L, x∈{0,1}K×L K∑ k=1 L∑ l=0 µklxkl, s.t. L∑ l=1 yl ≤ B, xkl ≤ yl, L∑ l=0 xkl = 1, ∀k = 1, . . . , K, l = 1, . . . , L. Insofar as some drone response times are diﬃcult to predict (depending on the weather, local environment, ability of bystanders to locate and use the drone’s payload), we expect in typical instances that estimates µkl may be imprecise. Interestingly, some non-linear problems can be transformed into the form of Problem (1.1). Example 2.3 (Personalized Pricing) Personalized pricing strategies seek to assign a tailored price to each of many customer types reﬂecting their heterogeneous willingness-to-pay (Cohen et al. 2021, Javanmard et al. 2020, Aouad et al. 2019). One simple formulation posits distinct demand models Dj(p) = mjφj(p) + bj in each customer segment j, for some decreasing function φj(p). This yields the revenue maximization problem max p≥0 n∑ j=1 mjpjφj(pj) + bjp, where pj is the price for the jth segment. We can cast this nonlinear objective in the form Prob- lem (1.1) by transforming variables, max p≥0,x { n∑ j=1 mjxj + bjpj : xj = pjφj(pj) for each j = 1, . . . , n } where the resulting feasible region is now non-convex. In typical settings, we expect the parameters mj, bj are unknown, and estimated via machine learning methods (Aouad et al. 2019). When there are many customer types, these estimates may be imprecise for rarely occurring types. Finally, we mention as an aside that some dynamic programs like the economic lot-sizing problem in inventory management can be cast in the above form through a careful representation of the dynamic program; see Elmachtoub and Grigas (2021) for details. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 12 2.1. Data Following GR 2021, we do not assume an explicit data generation procedure. Rather, we assume that as a result of analyzing whatever raw data are available, we obtain noisy, unbiased predictions Z such that E [Z] = µ with known precision E [(Zj − µj)2] = 1/νj for j = 1, . . . , n. These predictions might arise as sample averages as in Section 1.2, or as the outputs of some pre-processing regression procedure. We further assume that for each j = 1, . . . , n, we observe a non-random covariate of feature data Wj ∈ Rp, which may (or may not) be informative for the unknown µj. We believe this set-up reasonably reﬂects many applications. In the case of drone-assisted emer- gency response (Example 2.2), Wj encodes features that are predictive of EMS response times such as physical road distance between the patient and the responding ambulance, time of day, day of week, and weather conditions (Chu et al. 2021), while Zk0 may be an average of historical response times to location k. An advantage of modeling Z in lieu of the data generation process is that the precisions νj implicitly describe the amount of relevant data available for each µj. Let νmin ≡ minj νj and νmax ≡ maxj νj. Then, loosely speaking, the large-sample regime describes instances where νmin is large, i.e., where data are plentiful and we can estimate µ easily. By contrast, the small-data, large- scale regime describes instances in which n is large (large-scale), but there are limited relevant data, and, hence, νmax is small. To simplify our exposition, we will also assume: Assumption 2.4 (Independent Gaussian Corruptions) For each j = 1, . . . , n, Zj has Gaus- sian distribution with Zj ∼ N (µj, 1/νj) where νj is the known precision of Zj. Moreover, Z1, . . . , Zn are independent. Assumption 2.4 is common. GR 2021 employ a similar assumption; Javanmard and Montanari (2018) strongly leverages a Gaussian design assumption when debiasing lasso estimates, and Ito et al. (2018) also assumes Gaussian errors in their debiasing technique. In each case, the idea is if a technique enjoys provably good performance under Gaussian corruptions, it will likely have good practical performance when data are approximately Gaussian. Indeed, if Z is obtained by maximum likelihood estimation, ordinary linear regression, simple averaging as in Section 1.2, or Gaussian process regression, then the resulting estimates will be approximately Gaussian. We adopt the same perspective in our work. Note, the independence assumption in Assumption 2.4 is without loss of generality as illustrated in the following example. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 13 Example 2.5 (Correlated Predictions) Suppose we are given an instance of Problem (1.1) and predictions Z ∼ N (µ, Σ) where Σ is a known, positive semideﬁnite matrix. Consider a Cholesky decomposition Σ = LL ⊤ and the transformed predictions Z ≡ L −1Z. Notice Z ∼ N (µ, I) where µ ≡ L −1µ. We then recast Problem (1.1) as the equivalent problem min x∈X µ⊤x, where X ≡ {L ⊤x : x ∈ X }. Our new problem is of the required form with transformed predictions independent across j. Most importantly, Assumption 2.4 is not crucial to many of our results. Violating the Gaussian assumption only aﬀects the bias of our estimator (see Theorem 3.2). Our analysis bounding the variance and tails of the stochastic errors utilize empirical process theory, and can easily be adapted for non-Gaussian corruptions. Moreover, although the bias of our estimator is non-negligible when Z is non-Gaussian, we can bound this bias in terms of the Wasserstein distance between Z and a multivariate Gaussian, suggesting our method has good performance as long as corruptions are approximately Gaussian and this distance is small (see Lemma B.4 in Appendix for the bound). Finally, similar results also hold when ν is, itself, estimated noisily with the addition of a small bias term related to its estimate’s accuracy (see Lemma B.3 in Appendix). 2.2. Aﬃne Plug-in Policy Classes A data-driven policy for Problem (1.1) is a mapping Z ↦→ x(Z) ∈ X that determines a feasible decision x(Z) from the observed data Z. We focus on classes of aﬃne plug-in policies. Intuitively, a plug-in policy ﬁrst proxies the unknown µ by some estimate, r(Z), and then solves Problem (1.1) after “plugging-in” this estimate for µ. Deﬁnition 2.6 (Aﬃne Plug-in Policy Classes) For j = 1, . . . , n, let rj(z, θ) = aj(θ)z + bj(θ) be an aﬃne function of z where aj(θ) and bj(θ) are arbitrary functions of the parameter θ ∈ Θ. Let r(Z, θ) = (r1(Z1, θ), r2(Z2, θ), . . . , rn(Zn, θ))⊤ ∈ R n. The plug-in policy with respect to r(·, θ) is given by x (Z, θ) ∈ arg min x∈X r(Z, θ)⊤x, (2.1) where ties are broken arbitrarily. Furthermore, we let XΘ(Z) ≡ {x(Z, θ) ∈ X : θ ∈ Θ} ⊆ X denote the corresponding class of plug-in policies over Θ. When θ is ﬁxed and clear from context, we suppress its dependence, writing x(Z) and r(Z). Moreover, for a ﬁxed θ, rj(Zj, θ) only depends on the data (linearly) through the jth component. Plug-in policies are attractive in data-driven optimization because computing x(Z, θ) involves solving a problem of the same form as Problem (1.1) . Thus, if a specialized algorithm exists for Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 14 solving Problem (1.1) – e.g., as with many network optimization problems – the same algorithm can be used to compute the policy. This property does not necessarily hold for other classes of policies such as regularization based policies (GR 2021). Moreover, many policies used in practice are of the form x(Z, ˆθ(Z)) for some ˆθ(Z). (See examples below.) Such policies are not aﬃne plug-in policies; rj(Zj, ˆθ(Z)) may depend nonlinearly on all the data Z. Nonetheless, our analysis will bound the error of our estimator applied to such policies. Namely, in Section 4, we provide error bounds on our estimator that hold uniformly over XΘ(Z). Since these bounds hold uniformly, such bounds also hold for all policies of the form x(Z, ˆθ(Z)). For clarity, we make no claim about the optimality of aﬃne plug-in policies for Problem (1.1); for a particular application, there may exist non-aﬃne policies with superior performance. Our focus on aﬃne plug-ins is motivated by their ubiquity and computational tractability. We next present examples: • Sample Average Approximation (SAA). The Sample Average Approximation (SAA) is a canonical data-driven policy for Problem (1.1). It is deﬁned by xSAA(Z) ∈ arg min x∈X Z ⊤x. (2.2) SAA is thus an aﬃne plug-in policy where the function rj(z, θ) = z. • Plug-ins for Regression Models. Consider the linear model rj (Z, θ) = θ⊤Wj, which does not depend on Z, and the aﬃne plug-in policy xLM(Z, θ) ∈ arg min x∈X n∑ j=1 W ⊤ j θ · xj. (2.3) As mentioned, many policies in the literature are of the form xLM(Z, ˆθ(Z)) for a particular ˆθ(Z). For example, letting θOLS(Z) ∈ arg minθ ∑n j=1(Zj − θ⊤Wj)2 be the ordinary least-squares ﬁt yields the estimate-then-optimize policy xLM(Z, θOLS(Z)). Similarly, by appropriately padding the covariate with zeros, we can write the “optimization-aware” SPO and SPO+ methods of Elmachtoub and Grigas (2021) over linear hypothesis classes in the form xLM(Z, θSPO(Z)) and θSPO+(Z)) where θSPO(Z) and θSPO+(Z) are obtained by minimizing the so-called SPO and SPO+ losses, respectively. Other methods, e.g., (Wilder et al. 2019a), can be rewritten similarly. As mentioned, our analysis will bound the error when debiasing these polices as well. Of course, we are not limited to a linear model for rj(z, θ). We could alternatively use a non- linear speciﬁcation rj(z, θ) = f (Wj, θ) for some, given, nonlinear regression f with parameters θ. This speciﬁcation of r(Z, θ) still gives rise to a class of aﬃne plug-in policies. Again, many policies in the literature, including estimate-then-optimize policies and SPO+ over non-linear hypothesis classes can be written in the form x(Z, θ(Z)) for some particular mapping θ(Z) ∈ Θ. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 15 • Mixed-Eﬀects Policies. When Wj is not informative for µj, plug-ins for regression models perform poorly because no choice of θ yields a good estimate of µ. By contrast, if νmin is large, SAA performs quite well. Mixed-eﬀects policies interpolate between these choices. Deﬁne xME(Z, (τ, β)) ∈ arg min x∈X n∑ j=1 ( νj νj + τ Zj + τ νj + τ W ⊤ j β) xj, (2.4) where we have focused on a linear model for simplicity and made the dependence on θ = (τ, β) explicit for clarity. Mixed-eﬀects policies are strongly motivated by Bayesian analysis (Gelman et al. 2014). These policies generalize the Bayes-Inspired policy class considered in GR 2021. Again, we observe that xME(Z, (τ, β)) is an aﬃne-plug in policy. Moreover, we can also consider shrinking towards a nonlinear regression model as in (Ignatiadis and Wager 2019). Note in Deﬁnition 2.6, we require that rj(Zj) depends only on Zj, not on Zk for k ̸= j. We exploit this structure in the design and analysis of our debiasing technique. However, this requirement precludes certain types of plug-ins, e.g., those based on linear smoothers (Buja et al. 1989) including local polynomial regression and k-nearest neighbors. Extension of our method to these settings remains an interesting open research question. The choice of which aﬃne plug-in policy class to use is largely application dependent. Our bounds in Section 4 provide some preliminary guidance, suggesting a tradeoﬀ between the expressiveness of the policy class and the error of our estimator. 3. Variance Gradient Correction We make the following assumption on problem parameters for the remainder of the paper: Assumption 3.1 (Assumptions on Parameters) There exists a constant Cµ > 1 such that ∥µ∥∞ ≤ Cµ, and constants 0 < νmin < 1 < νmax < ∞ such that νmin ≤ νj ≤ νmax for all j. Moreover, we assume that n ≥ 3. The assumptions for Cµ and νmin, νmax are without loss of generality. These assumptions and the assumption on n allow us to simplify the presentation of some results by absorbing lower order terms into leading constants. The in-sample performance of a policy x(Z) is Z ⊤x(Z). Let ξ = Z − µ. We call the diﬀerence between in-sample and out-of-sample performance, corresponding to (Z − µ)⊤x(Z) = ξ⊤x(Z), the in-sample optimism. The expected in-sample optimism E [ξ⊤x(Z)] is the in-sample bias. Our method estimates the in-sample optimism of an aﬃne, plug-in policy x(Z, θ). To this end, denote the plug-in objective value by V (Z, θ) ≡ r(Z, θ)⊤x(Z, θ) = min x∈X r(Z, θ)⊤x. (3.1) Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 16 Because it is the minimum of linear functions, Z ↦→ V (Z, θ) is always concave. We then estimate the in-sample optimism by the Variance Gradient Correction (VGC) deﬁned by D(Z, (θ, h)) ≡ n∑ j=1 Dj(Z, (θ, h)), (3.2) where for j = 1, 2, . . . , n, Dj(Z, (θ, h)) ≡    E [ 1 h √νj aj (θ) (V (Z + δjej) − V (Z))∣ ∣ ∣ ∣ Z] , if aj ̸= 0, 0, otherwise, (3.3) and δ1, . . . , δn are independent Gaussian random variables such that δj ∼ N (0, h2 + 2h √νj ) for all j. To reduce notation, we deﬁne ¯Θ ≡ Θ × [hmin, hmax] for 0 < hmin ≤ hmax and write D(Z, θ) with θ ∈ ¯Θ. We utilize the deﬁned notation for results that require separating h and θ. The VGC is deﬁned as a (conditional) expectation over the auxiliary random variables δj. In practice, we can approximate this expectation to arbitrary precision by simulating δj and averaging; see Appendix B.5 for more eﬃcient implementations. Given the VGC, we estimate the out-of-sample performance by µ⊤x(Z, θ) ≈ Z ⊤x(θ) − D(Z, θ). (3.4) In Section 3.1, we motivate the VGC. We then establish some of its key properties, namely that it is almost an unbiased estimator for the in-sample optimism, its variance is often vanishing as n → ∞, and it is smooth in the policy class. Policy Classes that Do Not Depend on Z: Recall our plug-ins for linear regression models example from Section 2.2. From Eq. (2.3), we can see that D(Z, (θ, h)) = 0 uniformly over the class. Said diﬀerently, the in-sample performance is already an unbiased estimator of out-of-sample performance. This happy coincidence occurs whenever the plug-in function rj(z, θ) does not depend on z for each j and θ. That said, we stress that although in-sample performance is an unbiased estimator, it is not immediately clear what the variance of this estimator is. We discuss this further in Sections 3.3 and 4. Moreover, when rj(z, θ) does depend on z, the VGC is typically non-zero. 3.1. Motivating the Variance Gradient Correction (VGC) Throughout this section, θ is ﬁxed so we drop it from the notation. Our heuristic derivation of D(Z) proceeds in three steps. Step 1: Re-expressing the In-Sample Optimism via Danskin’s Theorem. Fix some j. If aj = 0, then from the plug-in policy problem (Problem (2.1)) we see that x(Z) is independent of Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 17 x(Z + λξjej) Z + λξjej Z λξjej ξ µ Figure 2 When X is polyhedral, xj(Z + λξjej) must occur at a vertex if it is unique. Hence, small perturbations to λ do not change the solution (see ﬁgure), and the derivative of V (Z + λξjej) is entirely determined by the derivative of r(Z + λξjej). Similar intuition holds for non-polyhedral X . Zj and the corresponding term in the in-sample bias is mean-zero, i.e., E [ξjx(Z)] = 0 = Dj(Z). In other words, we do not correct such terms. When aj ̸= 0, consider the function λ ↦→ V (Z + λξjej). (3.5) This function is an example of a parametric optimization problem. Danskin’s Theorem (Bertsekas 1997, Section B.5) characterizes its derivative with respect to λ. 4 Speciﬁcally, for any λ ∈ R such that x(Z + λξjej) is the unique optimizer to Problem (2.1), we have ∂ ∂λ V (Z + λξjej) = ajξjxj(Z + λξjej). When x(Z + λξjej) is not the unique optimizer, ajξjxj(Z + λξjej) is a subgradient, see Fig. 2 for intuition. Notice that ∂ ∂λ V (Z + λξjej) is the derivative of the plug-in value when we make the jth com- ponent of Z more variable, i.e., variance increases by a factor (1 + ∂λ)2, where ∂λ represents an inﬁnitely small perturbation to λ. This observation motivates our nomenclature “Variance Gradient Correction.” Evaluating the above derivative at λ = 0, dividing by aj, and summing over j such that aj ̸= 0, allows us to re-express the in-sample bias whenever x(Z) is unique as n∑ j=1 ξjxj(Z) = ∑ j:aj ̸=0 1 aj · ∂ ∂λ V (Z + λξjej) ∣ ∣ ∣ ∣λ=0 . Unfortunately, it is not clear how to evaluate these derivatives from the data. This leads to the second step in our derivation. 4 See Theorem B.1 in Appendix B for a statement of Danskin’s Theorem. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 18 Step 2: Approximating the Derivative via Randomized Finite Diﬀerencing. As a ﬁrst attempt, we approximate the above derivatives with ﬁrst-order, forward ﬁnite-diﬀerences (LeVeque 2007, Chapter 1). Intuitively, we expect that for a suﬃciently small step size h > 0, ∂ ∂λ V (Z + λξjej)∣ ∣ ∣ ∣λ=0 = 1 h √νj (V (Z + h √νjξjej) − V (Z)) + op(1) as h → 0, (3.6) which suggests that n∑ j=1 ξjxj(Z) = ∑ j:aj ̸=0 [ 1 h√ νjaj (V (Z + h √νjξjej) − V (Z))] + op(n) as h → 0. (3.7) Unfortunately, the right side of Eq. (3.7) is not computable from the data, because we do not observe µj, and, hence, do not observe ξj = Zj − µj. To circumvent this challenge, recall that ξj is Gaussian and independent across j, and let δj be the independent Gaussian random variables deﬁned in the deﬁnition of the VGC (Eq. (3.3)). A direct computation shows, Z + h √νjξjej ∼d Z + δjej, because both Z + h √νjξjej and Zj + δjej are Gaussians with matching mean and covariances. 5 Hence, V (Z + h √νjξjej) ∼d V (Z + δjej). Inspired by this relation, we replace the unknown V (Z + h √νjξjej) by V (Z + δjej) in our ﬁrst-order, ﬁnite diﬀerence approximation, yielding a randomized ﬁnite diﬀerence: n∑ j=1 ξjxj(ξ + µ; θ) ︸ ︷︷ ︸ In-Sample Optimism ≈ ∑ j:aj ̸=0 [ 1 h √νjaj(θ) (V (µ + ξ + δjej) − V (µ + ξ) )] ︸ ︷︷ ︸ Randomized Finite Diﬀerence . (3.8) Step 3: De-Randomizing the Correction. Finally, in the spirit of Rao-Blackwellization, we then de-randomize this correction by taking conditional expectations over δ. This de-randomization reduces the variability of our estimator and yields the VGC (Eq. (3.3)). Higher Order Finite Diﬀerence Approximations: Our heuristic motivation above employs a ﬁrst- order ﬁnite diﬀerence approximation, and our theoretical analysis below focuses on this setting for simplicity. However, it is possible to use higher order approximations, which in turn reduce the bias. Theoretical analysis of such higher order approximations is tedious, but not substantively diﬀerent from the ﬁrst-order case. Hence, it is omitted. In our experiments, we use a particular second-order approximation described in Appendix B.5. 3.2. Bias of Variance Gradient Correction Our ﬁrst main result shows that one can make the heuristic derivation of the previous section rigorous when quantifying the bias of the VGC. 5 Here, ∼d denotes equality in distribution. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 19 Theorem 3.2 (Bias of the Variance Gradient Correction) Under Assumptions 2.4 and 3.1, for any 0 < h < 1/e and any aﬃne, plug-in policy x(Z, θ), there exists a constant C (depending on νmin) such that 0 ≤ E [ n∑ j=1 ξjxj(µ + ξ, θ) − n∑ j=1 Dj(µ + ξ, θ) ] ≤ C · hn log ( 1 h ) Recall we expect that, in typical instances, the full-information performance of Problem (1.1) is O(n) as n → ∞. Thus, this theorem asserts that as long as h is small, say h = hn = o(1) as n → ∞, the bias of VGC is negligible relative to the true out-of-sample performance. In this sense, VGC is asymptotically unbiased for large n. The proof of the theorem is in Appendix B.1 and proceeds similarly to our heuristic derivation but uses the following monotonicity property to precisely quantify the “little oh” terms. Lemma 3.3 (Monotonicity of Aﬃne Plug-in Policies) For any z and j, the function t ↦→ x(z + tej) is non-increasing if aj ≥ 0, and the function is non-decreasing if aj < 0. Intuitively, the lemma holds because z ↦→ V (z) is a concave function (it is the minimum of aﬃne functions). In particular, t ↦→ V (z + t) is also concave, and by Danskin’s Theorem, d dt V (z + t) = ajxj(z + t) whenever x(z + t) is unique. Informally, the lemma then follows since the derivative of a concave function is non-increasing. Appendix B.1 provides a formal proof accounting for points of non-diﬀerentiability. Before proceeding, we remark that Theorem 3.2 holds with small modiﬁcations under mild violations of the independent Gaussian assumption (Assumption 2.4). Speciﬁcally, in cases where νj are not known but are estimated, the bias of the VGC constructed with the estimated νj increases by a small term depending on the accuracy of the precisions. See Lemma B.3 in the appendix for formal statements and proof. 3.3. The Variance of the VGC As mentioned in the contributions, the parameter h controls the trade oﬀs between bias and variance in our estimator. Unfortunately, while Theorem 3.2 gives a direct analysis of the bias under mild assumptions, a precise analysis of the variance (or tail behavior) of the VGC is more delicate. In this section we provide a loose, but intuitive bound on the variance of VGC that illustrates the types of problems for which our estimator should perform well. The main message of this section is that the VGC concentrates at its expectation so long as the policy x(Z) is “stable” in the sense that perturbing one element of Z does not cause x(Z) solution to change too much. The main challenge in showing D(Z) concentrates at its expectation is that D(Z) is a sum of dependent random variables Dj(Z). Worse, this dependence subtly hinges on the structure Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 20 of Problem (1.1) and the plug-in policy problem (Problem (2.1)) and hence is not amenable to techniques based on mixing or bounding the correlations between terms. We require a diﬀerent approach. As a ﬁrst step towards analyzing D(Z), we upper bound its variance by a related, fully- randomized estimator. Lemma 3.4 (Fully-Randomized VGC) Suppose that the solution x(Z) to Problem (2.1) is almost surely unique. For each j such that aj ̸= 0, let DR j (Z) ≡ δj h √νjaj xj(Z + δj ˜Ujej) (3.9) where ˜Uj ∼ Uniform[0, 1] and δj is deﬁned in Eq. (3.3). Let DR(Z) = ∑ j:aj ̸=0 DR j (Z) denote the fully-randomized VGC. Then, for any j such that aj ̸= 0, Dj(Z) = E [ DR j (Z) | Z] and Var(D(Z)) ≤ Var(DR(Z)). Proof: Again, by Danskin’s Theorem and the fundamental theorem of calculus, V (Z +δjej)−V (Z) = ∫ δj 0 ajxj(Z +tej)dt = ∫ 1 0 ajδjxj(Z +tδjej)dt = E [ajδjx(Z + δj ˜U ej) | Z, δj] . Scaling both sides by 1 aj h √νj and taking expectations over δj proves the ﬁrst statement. The second then follows from Jensen’s inequality. □ We next propose upper bounding Var(DR(Z)) with the Efron-Stein Inequality. In particular, let Z, δ and U be i.i.d. copies of Z, δ and ˜U respectively, and let Z k denote the vector Z but with the kth component replaced by Z k. Deﬁne δk and ˜U k similarly. Let DR(Z, δ, ˜U ) be the fully- randomized VGC with dependence on all constituent random variables made explicit. Then, by the Efron-Stein Inequality Var(DR(Z)) ≤ 1 2 n∑ k=1 E [(DR(Z, δ, ˜U ) − DR(Z k, δ, ˜U ))2] (3.10a) + 1 2 n∑ k=1 E [(DR(Z, δ, ˜U ) − DR(Z, δk, ˜U )) 2] (3.10b) + 1 2 n∑ k=1 E [(DR(Z, δ, ˜U ) − DR(Z, δ, ˜U k)) 2] . (3.10c) Recall that in the typical case, µ ⊤x(Z) = Op(n). Hence in what follows, we will focus on develop- ing conditions for which the upper bound in Eqs. (3.10a) to (3.10c) is o(n2). Indeed, such a bound would suggest D(Z) − E [D(Z)] = op(n), i.e., the stochastic ﬂuctuations in the VGC are negligible relative to the magnitude of the out-of-sample error for n suﬃciently large. With this perspective, Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 21 it is not diﬃcult to argue that both Eqs. (3.10b) and (3.10c) both contribute at most O ( n h ) (see proof of Theorem 3.5). Thus, we focus on Eq. (3.10a). Consider the kth element of the sum. Write, ∣ ∣DR(Z) − DR(Z k)∣ ∣ ≤ 1 hakνmin ∣ ∣ ∣ ∣ ∣ n∑ j=1 δj(xj(Z + δj ˜Ujej) − xj(Z k + δj ˜Ujej)) ∣ ∣ ∣ ∣ ∣ ≤ ∥δ∥2√ n hakνmin · ( 1 n n∑ j=1 ∣ ∣ ∣xj(Z + δj ˜Ujej) − xj(Z k + δj ˜Ujej)∣ ∣ ∣ 2)1/2 , (3.11) where the last inequality follows from Cauchy-Schwarz’s inequality. Since each δj is Gaussian, we expect ∥δ∥ 2 2 to concentrate sharply at its mean, i.e., ∥δ∥ 2 2 = Op(hn). Thus by squaring Eq. (3.11), taking expectations and substituting into Eq. (3.10a), we roughly expect Var(DR(Z)) ≤ O ( n h ) ︸ ︷︷ ︸ Eqs. (3.10b) and (3.10c) + E [ ∥δ∥ 2 2n2 h2ν2 mina 2 min · 1 n2 n∑ k=1 n∑ j=1 ∣ ∣ ∣xj(Z + δj ˜U ej) − xj(Z k + δj ˜U ej)∣ ∣ ∣ 2] ≈ O ( n h ) + O ( n3 h ) · 1 n2 n∑ k=1 n∑ j=1 E [∣ ∣ ∣xj(Z + δj ˜Ujej) − xj(Z k + δj ˜Ujej) ∣ ∣ ∣ 2] ︸ ︷︷ ︸ Avg. Solution Instability , where amin ≡ minj:aj ̸=0 |aj|. We call the indicated term the Average Solution Instability. In the worst case, it is at most 1 since X ⊆ [0, 1] n. If, however, it were O(n−α) for some α > 1, then the Var(DR(Z)) = o(n 2) as desired. How do we intuitively interpret Average Solution Instability? Roughly, in the limit as h → 0, we might expect that x(Z + δj ˜Ujej) ≈ x(Z) because δj = Op(√ h). Then the Average Solution Insta- bility is essentially the expected change in the solution in a randomly chosen component j when we replace the data for a randomly chosen component k with an i.i.d. copy. This interpretation suggests Average Solution Instability should be small so long as a small perturbation to the kth component doesn’t change the entire solution vector x(Z) by a large amount, i.e., if small per- turbations lead to small, local changes in the solutions. Intuitively, many large-scale optimization problems exhibit such phenomenon (see, e.g., Gamarnik (2013)), so we broadly expect the VGC to have low variance. The above heuristic argument can be made formal as in the following theorem. Theorem 3.5 (Variance of the VGC) Suppose that the solution x(Z) to Problem (2.1) is almost surely unique, that there exists a constant C1 (not depending on n) such that E [ 1 n2 ∑n k=1 ∑n j=1 (xj(Z + δj ˜Ujej) − xj(Z k + δj ˜Ujej))2] ≤ C1n−α and that Assumption 3.1 holds. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 22 Then, there exists a constant C2 (depending on νmin and amin ≡ minj aj) such that for any 0 < h < 1/e Var(D(Z)) = C2 h max(n3−α, n). In particular, in the typical case where the full-information solution to Problem (1.1) is O(n), the stochastic ﬂuctuations in the VGC are negligible relative to the out-of-sample performance if α > 1. The proof of Theorem 3.5 is in Appendix B.4. We remark that Theorem 3.5 provides a suﬃcient condition for the variance of the VGC to be negligible asymptotically and to show that h controls the bias-variance tradeoﬀ, however, the bound is not tight. In Section 4 we provide a tighter analysis given more stringent assumptions on Problems (1.1) and (2.1), which then also provides us guidance on how to select h to approximately balance the bias-variance tradeoﬀ. 3.4. Smoothness and Boundedness of the VGC One of the key advantages of our VGC is that it is smooth in the policy class, provided θ ↦→ r(·, θ) is “well-behaved.” Other corrections, like the Stein Correction of GR 2021, do not enjoy such smoothness. In Section 5, we argue this smoothness improves the empirical performance of our method. We formalize “well-behaved” in the next assumption: Assumption 3.6 (Plug-in Function is Smooth) We assume the functions aj(θ), bj(θ) are each L-Lipschitz continuous for all j = 1, . . . , n. Moreover, we assume there exists amax, bmax < ∞ such that sup θ∈Θ |aj(θ)| ≤ amax and sup θ∈Θ |bj(θ)| ≤ bmax ∀j = 1, . . . , n. Finally, we assume there exists amin such that 0 < amin ≤ inf{|aj(θ)| : aj(θ) ̸= 0, j = 1, . . . , n, θ ∈ Θ} In words, Assumption 3.6 requires the functions aj(θ) and bj(θ) to be Lipschitz smooth, bounded, and that the non-zero components of aj(θ) be bounded away from 0. Bias and Variance of VGC for Plug-In Linear Regression Models. Recall our Plug-in Linear Model class from Section 2.2. Since aj(θ) = 0 for all j, D(Z, θ) = 0 for all Z and (non data-driven) θ for this class. Said diﬀerently, the in-sample performance of a policy is, itself, our estimate of the out-of-sample performance, and, both Theorems 3.2 and 3.5 can both be strengthened; the bias of our estimator and variance of the correction are both zero. More generally, D(Z, θ) = 0 whenever the plug-in functionals rj(z, θ) do not depend on z for all j. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 23 We stress however that this analysis does not immediately guarantee that the in-sample per- formance of policies of the form xLM(Z, θ(Z)) is a good estimate of out-of-sample performance, because θ(Z) depends on Z. In Section 4 we provide suﬃcient conditions to ensure that in-sample performance is, indeed, a good estimate of out-of-sample performance. Moreover, when r(Z, θ) does depend on Z, e.g., as with our Mixed Eﬀects Regression class, D(Z, θ) is generally non-zero. Lemma 3.7 (Smoothness of Variance Gradient Correction) Under Assumptions 3.1 and 3.6, the following hold: i) There exists a constant C1 (depending on amin, amax, bmax, and νmin) such that for any z ∈ Rn and any 0 < h < 1/e, the function θ ↦→ D(z, (θ, h)) is Lipschitz continuous with parameter C1n2L h √νmin (∥z∥∞ + 1) . Moreover, there exists a constant C2 (depending on Cµ and C1) such that for any R > 1, with probability at least 1 − e −R, the (random) function θ ↦→ D(Z, (θ, h)) is Lipschitz continuous with parameter C2L h √ R νmin · n 2√log n. ii) Consider D(z, (θ, h)) where h ∈ [hmin, hmax] and 0 < hmax − hmin < 1. There exists an absolute constant C3 such that for any z ∈ Rn and θ ∈ Θ, the following holds, ∣ ∣D(z, (θ, h)) − D(z, (θ, h)) ∣ ∣ ≤ C3n hminν3/4 min √∣ ∣h − h ∣ ∣. See Appendix B.2 for a proof. Intuitively, the result follows because θ ↦→ V (z, θ) is Lipschitz by Danskin’s theorem and D(z, θ) is a linear combination of such functions. The second part follows from a high-probability bound on ∥Z∥∞. In addition to being smooth, the VGC is also bounded as a direct result of taking the conditional expectation over the perturbation parameters δj. Lemma 3.8 (VGC is Bounded) Suppose that Assumptions 3.1 and 3.6 hold. For any z, and any j = 1, . . . , n, |Dj(z)| ≤ √3 ν3/4 min√h . The proof can be found in Appendix B.2. The result follows from observing that the jth component of the VGC is the diﬀerence of the optimal objectives values of two optimization problems whose cost vector diﬀers by O(|δj|) in one component. Thus, the two optimal objective values can only diﬀer by O(|δj|) which is at most a constant once we take the conditional expectation. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 24 4. Estimating Out-of-sample Performance for Weakly-Coupled Problems In this section we provide high-probability tail bounds on the error of our estimator for out-of-sample performance that hold uniformly over a given policy class. Such bounds justify using estimator for policy learning, i.e., identifying the best policy within the class. They are also sub- stantively stronger than the variance analysis of Theorem 3.5 as they provide exponential bounds on the tail behavior, rather than bounding the second moment. Additionally, we show the uniform results hold even when θ ∈ ¯Θ is chosen in a data-driven manner (which recall also includes h). From the deﬁnition of the VGC out-of-sample estimator (Eq. (3.4)), the error of our estimator of out-of-sample performance for x(Z, θ(Z)) is ∣ ∣µ⊤x(Z, θ(Z)) − (Z ⊤x(Z, θ(Z)) − D(Z, θ(Z)) )∣ ∣ ︸ ︷︷ ︸ Error Estimating Out of Sample Perf. ≤ sup θ∈ ¯Θ ∣ ∣ξ⊤x(Z, θ) − D(Z, θ)∣ ∣ ︸ ︷︷ ︸ Error Estimating In-Sample Optimism ≤ sup θ∈ ¯Θ ∣ ∣ξ⊤x(Z, θ) − E [ξ⊤x(Z, θ) ]∣ ∣ (4.1a) + sup θ∈ ¯Θ |D(Z, θ) − E [D(Z, θ)]| (4.1b) + sup θ∈ ¯Θ ∣ ∣E [ µ⊤x(Z, θ) − Z ⊤x(Z, θ) − D(Z, θ)]∣ ∣ . (4.1c) We bounded Eq. (4.1c) in Theorem 3.2. Our goal will be to ﬁnd suﬃcient conditions to show the remaining terms are also op(n) uniformly over θ ∈ Θ. Then, in the typical case where the out-of-sample performance is Op(n), the error of our estimator will be negligible relative to the true out-of-sample performance. Our strategy will be to leverage empirical process theory since the argument of each suprema is a sum of random variables. Importantly, this empirical process analysis does not strictly require the independent Gaussian assumption (Assumption 2.4). The challenge of course is that the constraints of Problem (2.1) introduce a complicated dependence between the terms. Inspired by the average stability condition of Theorem 3.5, we focus on classes of “weakly- coupled” optimization problems. We consider two such classes of problems, those weakly-coupled by variables in Section 4.1 and those weakly-coupled by constraints Section 4.2. We provide formal deﬁnitions below. 4.1. Problems Weakly-Coupled by Variables We say an instance of Problem (1.1) is weakly-coupled by variables if ﬁxing a small number of variables causes the problem to separate into many, decoupled subproblems. Generically, such problems can be written as min x (µ0)⊤ x0 + K∑ k=1 (µ k)⊤ xk (4.2) Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 25 s.t. x0 ∈ X 0, xk ∈ X k(x0), ∀k = 1, . . . , K. Here, x0 represents the coupling variables and k = 1, . . . , K represent distinct subproblems. Notice that once x0 is ﬁxed, each subproblem can be solved separately. Intuitively, if dim(x0) is small relative to n, the subproblems of Eq. (4.2) are only “weakly” coupled. Some reﬂection shows both Examples 2.2 and 2.3 from Section 2 are weakly-coupled by variables. Let Sk ⊆ {1, . . . , n} be the indices corresponding to xk for k = 0, . . . , K, and Smax = maxk≥0 |Sk|. The sets S0, . . . , SK form a disjoint partition of {1, . . . , n}. Without loss of generality, reorder the indices so that the Sk occur “in order,”; i.e., (j : j ∈ S0), . . . , (j : j ∈ SK) is a consecutive sequence. Given the weakly-coupled structure of Eq. (4.2), we deﬁne a generalization of x(Z, θ): For each x0 ∈ X 0 and θ ∈ ¯Θ, let xk(Z, θ, x0) ∈ arg min xk∈X k(x0) rk(Z, θ) ⊤xk, k = 1, . . . , K, (4.3) where rk(Z, θ) = (rj(Z, θ) : j ∈ Sk). Intuitively, the vector x(Z, θ, x0) ≡ ((x0)⊤, x1(Z, θ, x0)⊤, . . . , xK(Z, θ, x0)⊤)⊤ satisﬁes the Average Instability Condition of Theorem 3.5 so long as Smax is not too large since the jth component of the solution changes when perturbing the kth data point if and only if j and k belong to the same subproblem. This event happens with probability at most Smax/n2. The key to making this intuition formal and obtaining exponential tails for the error of the out-of-sample estimator (Eq. (4.1)) is that sup θ∈ ¯Θ ∣ ∣ξ⊤x(Z, θ) − E [ξ⊤x(Z, θ) ]∣ ∣ ≤ sup θ∈ ¯Θ,x0∈X 0 ∣ ∣ξ⊤x(Z, θ, x0) − E [ ξ⊤x(Z, θ, x0)]∣ ∣ and, sup θ∈ ¯Θ |D(Z, θ) − E [D(Z, θ)]| ≤ sup θ∈ ¯Θ,x0∈X 0 ∣ ∣D(Z, θ, x0) − E [D(Z, θ, x0)]∣ ∣ , where both ξ⊤x(Z, θ, x0) and D(Z, θ, x0) can, for a ﬁxed θ, x0, be seen as sums of K independent random variables. To obtain uniform bounds, we then need to control only the metric entropy of the resulting (lifted) stochastic processes indexed by (θ, x0). We propose a simple assumption on the policy class to control this metric entropy. We believe this assumption is easier to verify than other assumptions used in the literature (e.g., bounded linear subgraph dimension or bounded Natarajan dimension), but admittedly slightly more stringent. Assumption 4.1 (Lifted Aﬃne Plug-in Policy) Given an aﬃne plug-in policy class deﬁned by r(·, θ) for θ ∈ Θ, we say this class satisﬁes the lifted aﬃne plug-in policy assumption for problems weakly-coupled by variables (Eq. (4.2)) if there exists mapping φ(·) and mappings gk(·) for k = 1, . . . , K such that xk(Z, θ, x0) ∈ arg min xk∈X k(x0) φ(θ)⊤gk(Z k, xk, x0) k = 1, . . . , K, ∀x0 ∈ X 0. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 26 We stress that the mapping φ(·) is common to all K subproblems and all x0 ∈ X 0, and both φ(·) and gk(·) can be arbitrarily nonlinear. Moreover, gk(·) may implicitly depend on the precisions ν and covariates W as these are ﬁxed constants. With the exception of policies from linear smoothers, each of our examples from Section 2.2, satisﬁes Assumption 4.1. For example, for plug-ins for linear regression models, we can simply take φ(θ) = θ and gk(Z k, xk, x0) = ∑ j∈Sk W ⊤ j xj. When xk(Z, θ, x0) is not the unique minimizer to the problem weakly-coupled variables deﬁned in Eq. (4.3), we require that ties are broken consistently. Let Ext(Conv(X k(x0))) denote the set of extreme points of Conv(X k(x0)) and let Xmax = maxk≥0 Ext(Conv(X k(x0))). Note, if xk(Z, θ, x0) is unique, it is an extreme point. Assumption 4.2 (Consistent Tie-Breaking) We assume there exists functions σkx0 : 2X k(x 0) ↦→ Ext(Conv(X k(x0))) such that xk(Z, θ, x0) = σkx0 ( arg min xk∈X k(x0) φ(θ)⊤gk(Z k, xk, x0) ) k = 1, . . . , K, ∀x0 ∈ X 0. Consistent tie-breaking requires that if (θ1, x0 1) and (θ2, x0 2) induce the same minimizers in Eq. (4.3) for some Z, then xk(Z, θ1, x0 1) = xk(Z, θ2, x0 2), and this point is an extreme point of X k(x0). Assumptions 4.1 and 4.2 allow us to bound the cardinality of the set {(x0, x1(Z, θ, x0), . . . , xK(Z, θ, x0)) : x0 ∈ X 0, θ ∈ Θ} by adapting a geometric argument counting regions in a hyperplane arrangements from Gupta and Kallus (2021) (Lemma C.7). The cardinality of the set characterizes the metric entropy of the policy class. Finally, for this section, we say a constant C is dimension-independent if C does not depend on {K, Smax, h, X 0, Xmax, dim(φ)}, but may depend on {νmin, Cµ, L}. We now present the main result of this section: Theorem 4.3 (Policy Learning for Problems Weakly-Coupled by Variables) Suppose Assumptions 2.4, 3.1, 3.6, 4.1 and 4.2 all hold. Let Xmax ≥ ∣ ∣Ext(Conv(X k(x0))) ∣ ∣ for all k = 1, . . . , K and x0 ∈ X 0, and assume Xmax < ∞. Then, for 0 < hmin ≤ hmax ≤ 1, there exists a dimension-independent constant C such that, for any R > 1, with probability at least 1 − 2 exp(−R), sup θ∈ ¯Θ ∣ ∣ξ⊤x(Z, θ) − D(Z, θ, h)∣ ∣ ≤ CKSmax · hmax log ( 1 hmin ) + CSmaxR √ K log(1 + |X 0|) hmin ( log(K) √ √ √ √log(Smax) log ( h −1 min · N (√ hmin Kn2 , Θ )) + √log(K)dim(φ) log(1 + Xmax) ) . where N (ϵ, Θ) is the ϵ-covering number of the set Θ. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 27 In the typical case where Θ does not depend on n or K and max(Smax, |X0| , dim(φ)) = ˜O(1) as K → ∞, (4.4) we can approximately minimize the above bound by selecting h ≡ hk = O(K −1/3) and noting the relevant covering number grows at most logarithmically in K. This choice of h approximately balances the deterministic and stochastic contributions, and the bound reduces to ˜O(CPIK 2/3) for some CPI (depending on |X0|, dim(φ), dim(θ)) that measures the complexity of the policy class. Many applications satisfy the conditions in Eq. (4.4), including the drone-assisted emergency medical response application (Example 2.2). To illustrate, recall Example 2.2. Here, y represents the binding variables “x0”, and we see |X 0| = (L B) . Moreover, Smax = L, since x decouples across k. Inspecting the constraints, Xmax ≤ B, since for each k, we choose exactly one depot from which to serve location k, and there are most B available depots. Finally, for a ﬁxed policy class, dim(φ) is constant and the log covering number above grows at most logarithmically in K. Most importantly, we expect L (the number of possible depots) and B (the budget) to be fairly small relative to K since regulations and infrastructure limit placement of depots, but there are many possible locations for cardiac events. Here typical instances of Example 2.2 satisfy Eq. (4.4). We return to Example 2.2 in Section 5 where we study the performance of our method numerically. 4.2. Problems with Weakly-Coupled by Constraints An instance of Problem (1.1) is weakly-coupled by constraints if, after removing a small number of binding constraints, the problem decouples into many separate subproblems. Data-driven linear optimization problems of this form have been well studied by Li and Ye (2019) and GR 2021. In order to facilitate comparisons to the existing literature, we study the speciﬁc problem max x∈X n∑ j=1 µjxj, X = { x ∈ [0, 1]n : 1 n n∑ j=1 Ajxj ≤ b } , (4.5) and corresponding plug-in policies x(Z, θ) ∈ arg max x∈X n∑ j=1 rj(Zj, θ)xj. (4.6) Here, Aj ∈ Rm with m ≥ 1. In particular, we consider a maximization instead of a minimization. We next introduce a dual representation of Problem (4.6). Speciﬁcally, scaling the objective of Problem (4.6) by 1 n and dualizing the binding constraints yields λ(Z, θ) ∈ arg min λ∈Rm + { b⊤λ + max x∈[0,1]n 1 n n∑ j=1(rj(Zj, θ) − A⊤ j λ)xj } (4.7) Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 28 For a ﬁxed λ, the inner maximization of Eq. (4.7) can be solved explicitly, yielding λ(Z, θ) ∈ arg min λ∈Rm + L(λ, Z, θ), where L(λ, Z, θ) ≡ b ⊤λ + 1 n n∑ j=1 (rj(Zj, θ) − A⊤ j λ )+ . By strong duality, V (Z, θ) = r(Z, θ)⊤x(Z, θ) = nL(λ(Z, θ), Z, θ). This dual representation highlights the weakly-coupled structure. Indeed, the dependence across terms in the sum in L(λ(Z, θ), Z, θ) arises because λ(Z, θ) depends on the entire vector Z. How- ever, this dependence has to be “channeled” through the m dimensional vector λ(Z, θ), and, hence, when m is small relative to n, cannot create too much dependence between the summands. Indeed, we will show that if m is small relative to n, then λ(Z, θ) concentrates at its expectation, i.e., a constant, as n → ∞, and, hence, the summands become independent asymptotically. This insight is key to the analysis. To formalize these ideas, we make assumptions similar to those in GR 2021 and Li and Ye (2019): Assumption 4.4 (s0-Strict Feasibility) There exists an s0 > 0 and x0 ∈ X such that 1 n ∑n j=1 Ajx 0 j + s0e ≤ b. Assumption 4.5 (Regularity of Matrix A) There exists a constant CA such that ∥Aj∥∞ ≤ CA for all 1 ≤ j ≤ n. Moreover, there exists a constant β > 0 such that the minimal eigenvalue of 1 n ∑n j=1 AjA⊤ j is at least β. The strict feasibility assumption can often be satisﬁed by perturbing A or b and ensures the dual optimal values λ(Z, θ) are bounded with high probability. The regularity assumptions on A ensure the function λ ↦→ E [L(λ, Z, θ)] is strongly convex, a key property in our proof (see below). Such a property holds, e.g., if the columns Aj are drawn randomly from some distribution. Like Section 4.1, in order to obtain uniform bounds we must also control the metric entropy of the diﬀerent stochastic error terms in out-of-sample error Eq. (4.1). Generalizing GR 2021, we make the following assumption: Assumption 4.6 (VC Policy Class) There exists a function ρ(·) such that rj(zj, θ) = ρ((zj, νj, Wj, Aj), θ), j = 1, . . . , n and a constant V such that the class of functions {(z, ν, W , A) ↦→ ρ((z, ν, W , A), θ) − A⊤λ : θ ∈ Θ, λ ∈ R m + } has a pseudo-dimension at most V . Without loss of generality, we further assume V ≥ max(m, 2). Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 29 The size of the constant V captures the complexity of the policy class which typically depends upon the dimension of θ as well as the number of binding constraints m. As an illustration, recall the plug-in for linear regression models policy class from Section 2. By (Pollard 1990, Lemma 4.4), this policy class satisﬁes Assumption 4.6 with V = dim(θ) + m. Finally, we say a constant C is dimension-independent if C does not depend on {n, h, m, V, dim(θ)}, but may depend on {νmin, CA, Cµ, β, s0, amin, amax, bmax, L}. The main result of this section is then: Theorem 4.7 (Estimation Error for Problems Weakly Coupled Constraints) Under Assumptions 2.4, 3.1, 3.6 and 4.4 to 4.6, for 0 < hmin ≤ hmax ≤ 1 there exists dimension- independent constants C and n0, such that for any R > 1 and all n ≥ n0eR, we have that with probability 1 − C exp(−R), sup θ∈ ¯Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξjxj(Z, θ) − Dj(Z, θ) ∣ ∣ ∣ ∣ ∣ ≤ Cn · hmax log ( 1 hmin ) + C · V 3 log3 V · R · √n log (n · N (n−3/2, Θ)) · log5 n hmin To build intuition, consider instances where Θ does not depend on n. Then, V = O(1) and the covering number grows at most logarithmically as n → ∞. We can then minimize the above bound (up to logarithmic terms) by taking taking h ≡ hn = O(n−1/4), yielding a bound of order ˜O(n3/4). In particular, in the typical instance where the full-information optimum (c.f. Problem (4.5)) is O(n), the relative error of our estimate is ˜O(n−1/4) which is vanishing as n → ∞. Remark 4.8 The rate above ( ˜O(n−1/4)) is slightly slower than the rate of convergence of the Stein correction in GR 2021 ( ˜O(n −1/3)). We attribute this diﬀerence to our choice of a ﬁrst order ﬁnite diﬀerence when constructing the VGC. A heuristic argument strongly suggests that had we instead used a second order forward ﬁnite diﬀerence scheme as in Appendix B.5, we would recover the rate ˜O(n−1/3). Moreover, our numerical experiments (with the second order scheme) in Section 5 shows our second-order VGC to be very competitive. Proof Intuition: Approximate Strong-Convexity and Dual Stability To build intuition, recall that to show the convergence of VGC, it suﬃces to bound the Average Solution Instability deﬁned in Theorem 3.5. By complementary slackness, xj(Z) = I {rj(Zj) > A⊤ j λ(Z) } except possibly for m fractional components. Hence, by rounding the frac- tional components, we have, for j ̸= k, xj(Z + δj ˜Ujej) − xj(Z k + δj ˜Ujej) Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 30 ≤ I { rj(Zj + δj ˜Ujej) ≥ A⊤ j λ(Z + δj ˜Ujej) } − I { rj(Zj + δj ˜Ujej) > A⊤ j λ(Z k + δj ˜Ujej)} ≤ I { rj(Zj + δj ˜Ujej) ∈ 〈A⊤ j λ(Z + δj ˜Ujej), A⊤ j λ(Z k + δj ˜Ujej)〉} , where we use ⟨l, u⟩ to denote the interval [min(l, u), max(l, u)]. By symmetry, the same bound holds for xj(Z k + δj ˜Ujej) − xj(Z + δj ˜Ujej). Since summands where j = k each contribute at most 1 to the Average Solution Instability, we thus have that 1 n2 n∑ k=1 n∑ j=1 (xj(Z + δj ˜Ujej) − xj(Z k + δj ˜Ujej))2 ≤ 1 n2 n∑ k=1 n∑ j=1 I { rj(Zj + δj ˜Ujej) ∈ 〈A⊤ j λ(Z + δj ˜Ujej), A⊤ j λ(Z k + δj ˜Ujej)〉} + Op ( 1 n ) . The principal driver of the Solution Instability is the ﬁrst double sum; in a worst-case, it might be Op(1). It will be small if ∥λ(Z + δj ˜Ujej) − λ(Z k + δj ˜Ujej)∥ is small for most j and k. Said diﬀerently, problems like Problem (4.6) that are weakly-coupled by constraints will have small Solution Instability if the dual solutions λ(·) are, themselves, stable, i.e., if the dual solution does not change very much when we perturb one data point. Our analysis thus focuses on establishing this dual solution stability. Unfortunately, solutions of linear optimization problems need not be stable – a small perturba- tion to the cost vector might cause the optimal solution to switch between extreme points, inducing a large change. By contrast, solutions of convex optimization problems with strongly-convex objec- tives are stable (see, e.g., Shalev-Shwartz et al. (2010)). The next key step of our proof is to show that although λ ↦→ L(λ, Z) is not strongly-convex, it is still “approximately” strongly-convex with high probability in a certain sense. To be more precise, recall that a function f (λ) is κ-strongly-convex if f (λ2) − f (λ1) ≥ ∇λf (λ1)⊤(λ2 − λ1) + κ 2 ∥λ2 − λ1∥2 2, ∀λ1, λ2 ∈ Dom(f ). (4.8) where κ > 0 and ∇λ denotes the subgradient. The left panel of Fig. 3 depicts this condition graphically. For any two points λ2 and λ1, the ﬁrst-order Taylor series underestimates the function value, and one can “squeeze in” a quadratic correction κ 2 ∥λ2 − λ1∥2 2 in the gap. The function λ ↦→ L(λ, Z) does not satisfy this condition, as seen in the right panel for points λ1 and λ′ 2. This function is piecewise-linear, and, for two points on the same line-segment, the ﬁrst-order Taylor series is exact. However, for points on diﬀerent line segments, such as λ1 and λ2, the ﬁrst-order Taylor series is a strict underestimation, and we can squeeze in a quadratic correction. Said diﬀerently, Eq. (4.8) does not hold for all λ1, λ2, but holds for most λ1, λ2. In this sense, λ ↦→ L(λ, Z) is “approximately” strongly-convex. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 31 f (λ) f (λ1) + ∂λf (λ1)⊤(λ − λ1) λ2 λ1 L(λ) L(λ1) + ∂λL(λ1)⊤(λ − λ1) L(λ1) + ∂λL(λ1)⊤(λ − λ1) + κ 2 ∥λ − λ1∥2 λ2 λ1 λ′ (a) (b) f (λ1) + ∂λf (λ1)⊤(λ − λ1) + κ 2 ∥λ − λ1∥ 2 Figure 3 Approximate Strong Convexity of L(λ). Figure (a) shows a strongly convex function f (λ) and visualizes the strong convexity condition Eq. (4.8). Figure (b) shows that because L(λ) is piecewise linear, it does not satisfy Eq. (4.8) for points on same line segment (λ1 and λ ′ 2). However, when λ1, λ2 are suﬃciently far apart, they are on diﬀerent line segments and Eq. (4.8) is satisﬁed. To make a formal statement, it is more convenient to use a diﬀerent, equivalent deﬁnition of strong-convexity. Equation (4.8) is equivalent to the condition (∇λf (λ1) − ∇λf (λ2))⊤(λ1 − λ2) ≥ κ∥λ1 − λ2∥2 2 ∀λ1, λ2 ∈ Dom(f ). (4.9) Lemma D.9 then shows that λ ↦→ L(λ, Z) is approximately strongly convex in the sense that, with high probability, there exists a C > 0 such that (∇λL(λ1, Z) − ∇λL(λ2, Z))⊤(λ1 − λ2) ≥ C∥λ1 − λ2∥ 2 2 − ∥λ1 − λ2∥ 3/2 2 C√n , (4.10) for all ∥λ1∥1 ≤ λmax, ∥λ2∥1 ≤ λmax, and ∥λ1 − λ2∥2 ≥ 4 n where λmax is a dimension independent constant satisfying E∥λ(Z, θ)∥1 ≤ λmax. Equation (4.10) mirrors Eq. (4.9). As n → ∞, Eq. (4.10) reduces to the analogue of Eq. (4.9) for ∥λ1∥1, ∥λ2∥1 ≤ λmax. Moreover, for ∥λ1 − λ2∥2 ≫ 1 n , the ﬁrst term on the left of Eq. (4.10) above dominates the second, so that the right hand side is essentially a quadratic in ∥λ1 − λ2∥2. These relations motivate our terminology “approximately strongly-convex.” Using this notion of approximate strong-convexity, we show in Lemma D.12 that there exists a set En ⊆ Rn such that Z ∈ En with high probability, and, more importantly, for any z ∈ En, the dual solutions are stable, i.e., ∥λ(z, θ) − λ(z, θ)∥2 ≤ C n n∑ j=1 I {zj ̸= zj} ∀z s.t. ∥λ(z)∥1 ≤ λmax. (4.11) Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 32 Equipped with this dual-stability condition, we can bound the average solution instability and the variance of D(Z) as in Theorem 3.5. However, since the above stability condition holds with high probability instead of in expectation, we can actually use a modiﬁcation of McDiarmid’s inequality (see Theorem A.5) to prove the following, stronger tail bound: Lemma 4.9 (Pointwise Convergence of VGC) Fix some θ ∈ Θ. Under the assumptions of Theorem 4.7, there exists a dimension independent constants C, n0 such that, for any R > 1 and n ≥ n0eR, we have with probability at least 1 − 4 exp(−R), |D(Z, θ) − E [D(Z, θ)]| ≤ CV 3 log2(V ) log4(n) √n h √R. We then complete the proof of Theorem 4.7 by i) invoking a covering argument over Θ to extend this tail bound to a uniform concentration for the VGC and ii) again leveraging dual stability to show the in-sample optimism (Eq. (4.1a)) concentrates similarly. See Appendix D.5 for the details. Comparison to Proof Technique to GR 2021 and Li and Ye (2019) GR 2021 and Li and Ye (2019) also analyze the behavior of λ(Z) in the limit as n → ∞. The key to their analysis is observing that the function λ ↦→ E [L(λ, Z)] is strongly-convex. Using this property, they prove that ∥λ(Z) − λ ∗∥2 = ˜Op ( 1 √n ) (4.12) for some constant λ∗ that does not depend on the realization of Z. Our analysis via approximate strong-convexity takes a diﬀerent perspective. Speciﬁcally, instead of studying the function λ ↦→ E [L(λ, Z)], we study the (random) function λ ↦→ L(λ, Z). While more complex, this analysis permits a tighter characterization of the behavior of the dual variables. In particular, leveraging Eq. (4.11), one can prove a statement similar to Eq. (4.12) (see Lemma D.17), however, to the best of our knowledge, one cannot easily prove Eq. (4.11) given the strong-convexity of λ ↦→ E [L(λ, Z)] or Eq. (4.12). A simple triangle inequality from Eq. (4.12) would suggest the much slower rate ∥λ(Z) − λ( ¯Z)∥ = Op ( 1√n ). It is an open question whether this tighter analysis might yield improved results for the online linear programming setting studied in Li and Ye (2019). 5. Numerical Case Study: Drone-Assisted Emergency Medical Response We reconsider Example 2.2 using real data from Ontario, Canada. Our data analysis and set-up largely mirror Boutilier and Chan (2019), however, unlike that work, our optimization formulation explicitly models response time uncertainty. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 33 Data and Setup. Recall, our formulation decides the location of drone depots (yl) and dispatch rules (xkl) where a dispatch rule determines whether to send a drone from depot l to location k when requested. Our objective is to minimize the expected response time to out-of-hospital cardiac arrest (OHCA) events. We consider L = 31 potential drone depot locations at existing paramedic, ﬁre, and police stations in the Ontario area. To study the eﬀect of problem dimension on our estimator, we vary the number of OHCA events via simulation similarly to Boutilier and Chan (2019). Speciﬁcally, we estimate a (spatial) probability density over Ontario for the occurrence of OHCA events using a kernel density estimator trained on 8 years of historical OHCA events. We then simulate K (ranging from 50 to 3,200) events according to this density giving the locations k used in our formulation. In our case-study, µkl represents the excess time a drone-assisted response takes over an ambulance-only response. (This objective is typically negative). We learn these constants by ﬁrst training a k-nearest neighbors regression (kNN) for the historical ambulance response times to nearby OHCAs. (For a sense of scale, the maximum ambulance response time is less than 1500s = 25 min.) We estimate a drone response time based on the (straight-line) distance between k and l assuming an average speed of 27.8 m/s and 20s for take-oﬀ and landing (assumptions used in Boutilier and Chan (2019)). We then set µkl to the diﬀerence of the drone time minus the ambu- lance time. These values are ﬁxed constants throughout our simulations and range from −3100 seconds to 1200 seconds. We take Zkl be normally corrupted predictions of µkl where the precisions νkl are determined by bootstrapping. Speciﬁcally, we take many bootstrap samples of our original historical dataset and reﬁt the k-nearest neighbor regression and recompute an estimate of ambulance and drone response times. The precision νkl is taken to be the reciprocal of the variance of these bootstrap replicates. Precisions range from νmin = 4 × 10−7 to νmax = 2 × 10−4. Policy Class. To determine dispatch rules for our case study, we consider the following policies: x(Z, W , (τ, µ0)) ∈ arg min x∈X K∑ k=1 L∑ j=1 ( νjk νjk + τ Zjk + τ νjk + τ (Wjk − µ0)) xjk, where Wjk is the computed drone travel time between facility j and OHCA k. Our policy class consists of policies where τ ∈ [0, 100νmin] and µ0 ∈ [0, 1500]. Similar to the Mixed-Eﬀects Policies from Section 2.2, each policy is a weighted average between the SAA policy and a deterministic policy that dispatches to any location whose drone travel-time is at most µ0. For the ﬁrst three experiments, we generate out-of-sample estimates using our VGC, the Stein- correction of GR 2021, and cross-validation using hold-out (2-fold) cross-validation. We assume that we are given two samples Z 1, Z 2 so that Zjk = 1 2 (Z 1 jk + Z 2 jk). We set h = n−1/6 for both the Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 34 0 20 40 60 80 100 300 1,000 3,000 OHCA Events (K)Abs.Bias(%Full-Info) 0 20 40 60 80 100 300 1,000 3,000 OHCA Events (K)RootVariance(%Full-Info)Cross-Val Stein Correction VGC Figure 4 Bias and variance as K → ∞. The two graphs plot the bias and variance of the diﬀerent out-of- sample performance estimators for the Sample Average Approximation (SAA) policy. The bias and variance were estimated across 100 simulations for each K. Although variance vanishes for all methods as K increases, cross-validation exhibits a non-vanishing bias and is uniformly worse for all K. 0.0% 2.0% 4.0% 6.0% 1e-05 2e-05 3e-05 4e-05 τSub-Optimality 0.0% 2.0% 4.0% 6.0% 1e-05 2e-05 3e-05 4e-05 τSub-Optimality Cross-Val VGC Oracle Stein Correction Figure 5 Estimating Performance across Policy Class. The ﬁrst graph shows the estimates of out-of-sample performance across the policy class for the parameter τ ∈ [25νmin, 100νmin] and µ0 = 1000 for one sample path when K = 400. The second graph is similar, but for K = 3200. Both plots highlight the smoothness of VGC relative to the Stein-Correction. VGC and the Stein-Correction based on the recommendation of GR 2021. In the last experiment, we are given one hundred samples Z i for i = 1, . . . , 100 where Zjk = 1 100 ∑100 i=1 Z i jk and generate out-of-sample estimates for 2, 5, 10, 20, 50, and 100 fold cross-validation. For ease of comparison, we present all results as a percentage relative to full-information optimal performance. 5.1. Results In our ﬁrst experiment, we evaluate the bias and square root variance of each method for the out-of-sample performance of the SAA policy (τ = 0) as K, the number of OHCA events grows Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 35 Time Saved (sec) 1 ⌧600 0 800 (a) (b) (a) (b) Figure 6 Comparing Policy Decisions. Left (resp. right) panel shows routing decisions for the policy selected by VGC (resp. cross-validation). Color indicates time-saved relative to an ambulance-only policy (green is good, red is bad) computed relative to the ground truth. Although routing is largely similar, Regions (a) and (b) highlight some diﬀerences where the cross-validation policy makes poorer routing decisions (more orange dots). The larger black points are drone depots. (see Fig. 4). As predicted by our theoretical analysis, the quality of the out-of-sample estimates improve as we increase the problem size for both the VGC and the Stein Correction. However, cross-validation incurs a non-vanishing bias because it only leverages half the data in training. As a second experiment, in Fig. 5, we can observe the quality of the estimators over multiple policies in the policy class. We highlight the smoothness of the VGC as τ varies. Since, for large K, the true performance is quite smooth, the worst-case error of VGC is generally smaller than that of the Stein Correction. We also note that while it appears both Stein and VGC systematically over- estimate performance, this is an artifact of the particular sample path chosen. By contrast, cross- validation does systematically underestimate performance, because it estimates the performance of a policy trained with half the data, which is necessarily worse. In our third experiment, we highlight the diﬀerences in the policy selected by the VGC estimator and the policy selected by cross-validation. In Fig. 6, we plot the routing decisions of each policy and color code them by the true (oracle) amount of time saved. We highlight two regions (labeled (a) and (b)) on the map where drones arrive after the ambulance. We see that in those regions, the cross-validation policy routes to more patients/regions where the drone arrives after the ambulance, thus potentially wasting drone resources and availability. We see in the regions outside of (a) and (b) that routing decisions of the two polices are similar and result in the drone arriving before the Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 36 0.00 0.25 0.50 0.75 1.00 400 800 1200 Coverage Ring (sec) (a)FractionServed Cross-Val VGC Full Info Northern Region Southern Region 400 800 1200 400 800 1200 0 25000 50000 75000 100000 Coverage Ring (sec) (b)TimeSaved(sec)Cross-Val VGC Figure 7 Estimating Performance across Policy Class. Each data point in the graph represents the per- formance metric of each selected policy for a ring shaped region corresponding to distance in time from a drone depot. Graph (a) shows the fraction of patients served in each region for the patients serviced by the Southern depot in Fig. 6. Graph (b) plots the time saved by each policy. The plots highlight the performance diﬀerence in routing decisions between the two policies. the ambulance. Additionally, we note that the drone depots in the southern region of the map are the same for both policies while the drone depots in the northern region are diﬀerent. In Fig. 7, we plot key performance metrics for regions organized by their distance from a drone depot for these two policies. Speciﬁcally, we group OHCA events into “coverage rings” based on the travel time from the depot to the event. Each ring is of “width” 100s. For example, the 800 seconds coverage ring corresponds to all OHCA events that are between 701 and 800 seconds away from the drone depot. In Panel (a) of Fig. 7, we restrict attention to the the southern region Fig. 6 where both policies have selected the same drone depot so that we can focus on routing decisions. We plot the fraction of patients served for each coverage ring. We see that the policy chosen by VGC is more conservative with routing in comparison to the policy chosen by cross-validation and more closely aligns the full information benchmark. In panel (b) of Fig. 7, we compare the time saved between the two policies. We organize the regions into the North and South corresponding to the servicing depots. In the northern region, we see that the VGC policy saves more time in coverage rings further away from the drone depot by sacriﬁcing time saved in closer coverage rings. This diﬀerence partially corresponds with the more conservative routing decisions of the VGC, but also can be attributed to the choice of drone depot. We see the VGC policy chooses a depot in less densely populated region that is more centralized overall, while the cross-validation chooses a depot closer to more densely populated regions in terms of OHCA occurrences. In total, we see that the VGC policy saves 1.43% more time in comparison to the cross-validation policy. However, if we breakdown the time saved with respect to minimum distance from a depot, we see that for patients within 600 seconds of an existing drone depot, the Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 37 VGC performs less than 1% worse in comparison to the cross-validation policy. However, if we consider patients more than 600 seconds away from existing drone depot, the VGC policy saves 13.8% more time in comparison to cross-validation. We interpret this to mean that both the VGC and cross-validation policies make similar performing depot decisions, but VGC makes signiﬁcantly better routing decisions, particularly at long distances. Since these long distances are precisely where the imprecision is most crucial, we argue this is a relevant advantage. Finally, we also compare how higher fold cross-validation performs with respect to VGC. In (a) of Fig. 8, we ﬁrst show how the cross-validation estimators for three diﬀerent policies performs in estimation error relative to VGC as we vary the number of folds. For all the policies (where increasing τ corresponds to lower variance policies), the plot shows the root mean-squared error (MSE) of cross-validation is uniformly larger than VGC over the folds considered. Furthermore, the plot highlights a drawback of cross-validation, which is that it is not even clear how to select the optimal number of folds in order to minimize the bias variance trade-oﬀ of the cross-validation estimator. In comparison, the VGC with minimal guidance on the choice of the h parameter out- performs common choices of folds for cross-validation such as 5-fold cross-validation and leave-one- out cross-validation. In (b) of Fig. 8, we show how the performance of policy evaluation translates to policy learning in regions further away from the depot, where drone decisions are most crucial. As expected, VGC out-performs cross-validation across all choices of folds and the fold that performs most similarly to VGC, 20-fold, also corresponds to the cross-validation fold with the lowest MSE. This observation suggests with an “ideal” number of folds, cross-validation can perform well in this example, but identifying the right number of folds is non-trivial. 6. Conclusion Motivated by the poor performance of cross-validation in data-driven optimization problems where data are scarce, we propose a new estimator of the out-of-sample performance of an aﬃne plug-in policy. Unlike cross-validation, our estimator avoids sacriﬁcing data and uses all the data when training, making it well-suited to settings with scarce data. We prove that our estimator is nearly unbiased, and for “stable” optimization problems – problems whose optimal solutions do not change too much when the coeﬃcient of a single random component changes – the estimator’s variance vanishes. Our notion of stability leads us to consider two special classes of weakly-coupled opti- mization problems: weakly-coupled-by-variables and weakly-coupled-by-constraints. For each class of problems, we prove an even stronger result and provide high-probability bounds on the error of our estimator that holds uniformly over a policy class. Additionally, in our analysis of optimiza- tion problems weakly-coupled-by-constraints, we provide new insight on the stability of the dual Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 38 1.0 1.5 2.0 2.5 3.0 5 10 20 50 100 Folds (a)RootMSE(%Full-Info)CV τ = 0 CV τ = 8 × 10−7 CV τ = 3.96 × 10−5 VGC τ = 0 VGC τ = 3.96 × 10−5 VGC τ = 8 × 10−7 0.0% 5.0% 10.0% 15.0% 20.0% 2 5 10 20 50 100 Folds (b)VGCPerformanceImprovement OHCAs >930s Away OHCAs >870s Away OHCAs >810s Away Figure 8 Varying Cross-Validation Folds. We plot the policy evaluation and learning performance of cross- validation with diﬀerent folds across 500 simulations. In each simulation there are 100 samples of Z i. In (a), we plot the mean squared error of cross-validation for three diﬀerent policies and compare them with the respective VGC estimates represented by the dotted lines. In (b), we plot the percent improvement VGC has over cross-validation, so larger bars indicate lower cross-validation performance. solutions. This new insight may provide further insight in problems that leverage the dual solution such as online linear programming. Our work oﬀers many exciting directions for future research. Our solution approach to the weakly-coupled problems exploits the decomposability of the underlying optimization problems. We believe that such an approach can be generalized to other settings. Finally, our analysis strongly leverages the linearity of the aﬃne plug-in policy class; it is an open question if similar debiasing techniques might be developed to handle nonlinear objective functions as well. Acknowledgments The ﬁrst two author were partially supported by the National Science Foundation under Grant No. CMMI- 1661732. All three authors thank Prof. Tim Chan and Prof. Justin Boutilier for sharing simulation results and details pertaining to their paper (Boutilier and Chan 2019), and the two anonymous reviewers who provided feedback on an earlier revision. References Aouad, Ali, Adam N Elmachtoub, Kris J Ferreira, Ryan McNellis. 2019. Market segmentation trees. arXiv preprint arXiv:1906.01174 . Baardman, Lennart, Maxime C. Cohen, Kiran Panchamgam, Georgia Perakis, Danny Segev. 2019. Scheduling promotion vehicles to boost proﬁts. Management Science 65(1) 50–70. Bertsekas, Dimitri P. 1997. Nonlinear programming. Journal of the Operational Research Society 48(3) 334–334. Bousquet, Olivier, Andr´e Elisseeﬀ. 2001. Algorithmic stability and generalization performance. Advances in Neural Information Processing Systems. 196–202. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 39 Boutilier, Justin J, Timothy CY Chan. 2019. Response time optimization for drone-delivered automated external deﬁbrillators. arXiv preprint arXiv:1908.00149 . Breiman, Leo. 1996. Bagging predictors. Machine Learning 24(2) 123–140. Buja, Andreas, Trevor Hastie, Robert Tibshirani. 1989. Linear smoothers and additive models. The Annals of Statistics 453–510. Cheskes, Sheldon, Shelley L McLeod, Michael Nolan, Paul Snobelen, Christian Vaillancourt, Steven C Brooks, Katie N Dainty, Timothy CY Chan, Ian R Drennan. 2020. Improving access to automated external deﬁbrillators in rural and remote settings: a drone delivery feasibility study. Journal of the American Heart Association 9(14) e016687. Chu, Jamal, KH Benjamin Leung, Paul Snobelen, Gordon Nevils, Ian R Drennan, Sheldon Cheskes, Timo- thy CY Chan. 2021. Machine learning-based dispatch of drone-delivered deﬁbrillators for out-of-hospital cardiac arrest. Resuscitation 162 120–127. Cohen, Maxime C, Jeremy J Kalas, Georgia Perakis. 2021. Promotion optimization for multiple items in supermarkets. Management Science 67(4) 2340–2364. Cohen, Maxime C., Ngai-Hang Zachary Leung, Kiran Panchamgam, Georgia Perakis, Anthony Smith. 2017. The impact of linear optimization on promotion planning. Operations Research 65(2) 446–468. Combes, Richard. 2015. An extension of mcdiarmid’s inequality. arXiv preprint arXiv:1511.05240 . Deng, Yunxiao, Suvrajeet Sen. 2018. Learning enabled optimization: Towards a fusion of statistical learning and stochastic programming. INFORMS Journal on Optimization (submitted) . Donti, Priya, Brandon Amos, J Zico Kolter. 2017. Task-based end-to-end model learning in stochastic optimization. Advances in Neural Information Processing Systems. 5484–5494. Dud´ık, Miroslav, John Langford, Lihong Li. 2011. Doubly robust policy evaluation and learning. Proceedings of the 28th International Conference on International Conference on Machine Learning. 1097–1104. El Balghiti, Othman, Adam N Elmachtoub, Paul Grigas, Ambuj Tewari. 2019. Generalization bounds in the predict-then-optimize framework. Advances in Neural Information Processing Systems. 14412–14421. Elmachtoub, Adam, Jason Cheuk Nam Liang, Ryan McNellis. 2020. Decision trees for decision-making under the predict-then-optimize framework. International Conference on Machine Learning. PMLR, 2858–2867. Elmachtoub, Adam N, Paul Grigas. 2021. Smart “predict, then optimize”. Management Science . Friedman, J., T. Hastie, R. Tibshirani. 2001. The Elements of Statistical Learning. Springer, Berlin. Gamarnik, David. 2013. Correlation decay method for decision, optimization, and inference in large-scale networks. Theory Driven by Inﬂuential Applications. INFORMS, 108–121. Gelman, A., J.B. Carlin, H.S. Stern, D.B. Rubin. 2014. Bayesian Data Analysis, vol. 2. Chapman & Hall/CRC Boca Raton, FL, USA. Gupta, Vishal, Nathan Kallus. 2021. Data pooling in stochastic optimization. Management Science URL https://doi.org/10.1287/mnsc.2020.3933. Gupta, Vishal, Paat Rusmevichientong. 2021. Small-data, large-scale linear optimization with uncertain objectives. Management Science 67(1) 220–241. Hastie, Trevor, Robert Tibshirani, Ryan Tibshirani. 2020a. Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons. Statistical Science 35(4) 579 – 592. doi: 10.1214/19-STS733. URL https://doi.org/10.1214/19-STS733. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 40 Hastie, Trevor, Robert Tibshirani, Ryan Tibshirani. 2020b. Best subset, forward stepwise or lasso? analysis and recommendations based on extensive comparisons. Statistical Science 35(4) 579–592. Hu, Yichun, Nathan Kallus, Xiaojie Mao. 2022. Fast rates for contextual linear optimization. Management Science . Ignatiadis, Nikolaos, Stefan Wager. 2019. Covariate-powered empirical bayes estimation. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, R. Garnett, eds., Advances in Neural Informa- tion Processing Systems, vol. 32. Curran Associates, Inc. Ito, Shinji, Akihiro Yabe, Ryohei Fujimaki. 2018. Unbiased objective estimation in predictive optimiza- tion. Jennifer Dy, Andreas Krause, eds., Proceedings of the 35th International Conference on Machine Learning, vol. 80. Stockholm Sweden, 2176–2185. Javanmard, Adel, Andrea Montanari. 2014. Conﬁdence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research 15(1) 2869–2909. Javanmard, Adel, Andrea Montanari. 2018. Debiasing the lasso: Optimal sample size for gaussian designs. Annals of Statistics 46(6A) 2593–2622. Javanmard, Adel, Hamid Nazerzadeh, Simeng Shao. 2020. Multi-product dynamic pricing in high-dimensions with heterogeneous price sensitivity. 2020 IEEE International Symposium on Information Theory (ISIT). 2652–2657. doi:10.1109/ISIT44484.2020.9174296. Kearns, Michael, Dana Ron. 1999. Algorithmic stability and sanity-check bounds for leave-one-out cross- validation. Neural Computation 11(6) 1427–1453. LeVeque, Randall J. 2007. Finite diﬀerence methods for ordinary and partial diﬀerential equations: steady- state and time-dependent problems. SIAM. Li, Xiaocheng, Yinyu Ye. 2019. Online linear programming: Dual convergence, new algorithms, and regret bounds. arXiv preprint arXiv:1909.05499 . Liyanage, Liwan H, J George Shanthikumar. 2005. A practical inventory control policy using operational statistics. Operations Research Letters 33(4) 341–348. Pani, S. Raghavan, Abhishek, Mustafa Sahin. 2017. Large-scale advertising portfolio optimization in online marketing. Working Paper URL http://terpconnect.umd.edu/~raghavan/preprints/lsoapop. pdf. Pollard, David. 1990. Empirical processes: Theory and applications. NSF-CBMS Regional Conference Series in Probability and Statistics, vol. 2. Institute of Mathematical Statistics, i–86. Rusmevichientong, David P. Williamson, Paat. 2006. An adaptive algorithm for selecting proﬁtable keywords for search-based advertising services. Proceedings of the 7th ACM Conference on Electronic Commerce. ACM, 260–269. Sanfridsson, J, J Sparrevik, J Hollenberg, P Nordberg, T Dj¨arv, M Ringh, L Svensson, S Forsberg, A Nord, Magnus Andersson-Hagiwara, et al. 2019. Drone delivery of an automated external deﬁbrillator–a mixed method simulation study of bystander experience. Scandinavian journal of trauma, resuscitation and emergency medicine 27(1) 1–9. Schmoys, David, Shujing Wang. 2019. How to solve a linear optimiza- tion problem on incentive allocation? URL https://eng.lyft.com/ how-to-solve-a-linear-optimization-problem-on-incentive-allocation-5a8fb5d04db1. Gupta, Huang, and Rusmevichientong: Debiasing In-Sample Policy Performance 41 Shalev-Shwartz, Shai, Ohad Shamir, Nathan Srebro, Karthik Sridharan. 2010. Learnability, stability and uniform convergence. The Journal of Machine Learning Research 11 2635–2670. Shao, Jun. 1993. Linear model selection by cross-validation. Journal of the American statistical Association 88(422) 486–494. Smith, James E, Robert L Winkler. 2006. The optimizer’s curse: Skepticism and postdecision surprise in decision analysis. Management Science 52(3) 311–322. Van der Vaart, A. W. 2000. Asymptotic Statistics. No. 3 in Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press, Cambridge, UK. Wager, Stefan, Susan Athey. 2018. Estimation and inference of heterogeneous treatment eﬀects using random forests. Journal of the American Statistical Association 113(523) 1228–1242. Wainwright, Martin J. 2019. High-dimensional statistics: A non-asymptotic viewpoint, vol. 48. Cambridge University Press. Wilder, Bryan, Bistra Dilkina, Milind Tambe. 2019a. Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33. 1658–1665. Wilder, Bryan, Eric Ewing, Bistra Dilkina, Milind Tambe. 2019b. End to end learning and optimization on graphs. Advances in Neural Information Processing Systems. 4672–4683. Zhang, Cun-Hui, Stephanie S. Zhang. 2014. Conﬁdence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B: Statistical Methodology 217–242. e-companion to Debiasing In-Sample Policy Performance ec1 This page is intentionally blank. Proper e-companion title page, with INFORMS branding and exact metadata of the main paper, will be produced by the INFORMS oﬃce when the issue is being assembled. ec2 e-companion to Debiasing In-Sample Policy Performance Online Appendix: Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization Appendix A: Background Results on Empirical Processes In this appendix we collect some results on the suprema of empirical processes that we will require in our proofs. All results are either known or easily derived from known results. Our summary is necessarily brief and we refer the reader to Pollard (1990) for a self-contained exposition. Let Ψ(t) = 1 5 exp(t2). For any real-valued random variable Z, we deﬁne the Ψ-norm ∥Z∥Ψ to be ∥Z∥Ψ ≡ inf {C > 0 : E[Ψ(|Z| /C)] ≤ 1}. Random variables with ﬁnite Ψ-norm are sub-Gaussian random variables. We ﬁrst recall a classical result on the suprema of sub-Gaussian processes over ﬁnite sets. Theorem A.1 (Suprema of Stochastic Processes over Finite Sets) Let f (θ) = (f1(θ), . . . , fK(θ)) ∈ RK be a vector of K independent stochastic processes indexed by θ ∈ Θ. Let F ∈ RK + be a random vector such that |fk(θ)| ≤ Fk for all θ ∈ Θ, k = 1, . . . , K, and suppose there exists a constant M < ∞ such that |{f (θ) : θ ∈ Θ}| ≤ M almost surely. Then, for any R > 1, there exists an absolute constant C such that with probability 1 − e−R sup θ∈Θ ∣ ∣ ∣ ∣ ∣ K∑ k=1 fk(θ) − E [ K∑ k=1 fk(θ) ]∣ ∣ ∣ ∣ ∣ ≤ C · R · ∥∥F ∥2∥Ψ √log M . Proof: The result follows from the discussion leading up to Eq. (7.4) of Pollard (1990) after noting that the entropy integral (Jn(ω) in the notation of Pollard (1990)) is at most 9∥F ∥2√log M given the conditions of the theorem. □ When considering the suprema over potentially inﬁnite sets, we must characterize the “size” of {f (θ) : θ ∈ Θ} more carefully. Recall for any set F, the ϵ-packing number of F is the largest number of points we can select in F such that no two points are within ℓ2 distance ϵ. We denote this packing number by D(ϵ, F). We restrict attention to sets whose packing numbers do not grow too fast. Deﬁnition A.2 (Euclidean Sets) We say a set F is Euclidean if there exists constants A and W such that D(ϵδ, F) ≤ Aϵ −W ∀0 < ϵ < 1, where δ ≡ supf ∈F ∥f ∥. e-companion to Debiasing In-Sample Policy Performance ec3 Furthermore, note that in the special case that Fk ≤ U , Theorem A.1 bounds the suprema by a term that scales like U √K. This bound can be quite loose since fk(θ) typically takes values much smaller than U and is only occasionally large. Our next result provides a more reﬁned bound on the suprema when the pointwise variance of the process is relatively small and the relevant (random) set is Euclidean. We stress the parameters A and W below must be deterministic. Theorem A.3 (Suprema of Stochastic Processes with Small Variance) Suppose that the set {f (θ) : θ ∈ Θ} ⊆ R K is Euclidean with parameters A and W almost surely. Suppose also that i) There exists a constant U such that supθ∈Θ ∥f (θ)∥∞ ≤ U , almost surely, and ii) There exists a constant σ2 such that supθ∈Θ E [∥f (θ)∥2 2] ≤ Kσ2. Then, there exists an absolute constant C such that for any R > 1, with probability at least 1 − e−R, sup θ∈Θ ∣ ∣ ∣ ∣ ∣ K∑ k=1 fk(θ) − E [ K∑ k=1 fk(θ) ]∣ ∣ ∣ ∣ ∣ ≤ CR · V (A, W )√K (σ + U V (A, W ) √K ) , where V (A, W ) ≡ log A+W√ log A . Remark A.4 Notice that when K is suﬃciently large, the term in the parenthesis is dominated by 2σ, and hence the bound does not depend on U . Theorem A.3 is not tight in its dependence on R. See, for example, Talagrand’s Inequality for the suprema of the empirical process (Wainwright (2019)). We prefer Theorem A.3 to Talagrand’s inequality in what follows because it is somewhat easier to apply and is suﬃcient for our purposes. Proof of Theorem A.3. For convenience in what follows, let F be the (random) set {f (θ) : θ ∈ Θ}. Let δ ≡ supf ∈F ∥f ∥2. Our goal will be to apply Theorem A.2 of GR 2021. That theorem shows that there exists an absolute constant C1 such that sup f ∈F ∣ ∣ ∣ ∣ ∣ K∑ k=1 fk − E [ K∑ k=1 fk ]∣ ∣ ∣ ∣ ∣ ≤ C1RV (A, W )∥δ∥Ψ. (A.1) Thus, the remainder of the proof focuses on bounding ∥δ∥Ψ. As an aside, a naive bound ∥δ∥Ψ ≤ U √K, so we know that this value is ﬁnite. In what follows, we seek a stronger bound. Write δ2 = sup f ∈F ∥f ∥ 2 2 ≤ sup f ∈F ∥f ∥2 2 − E [ ∥f ∥2 2] + sup f ∈F E [ ∥f ∥2 2] ≤ sup f ∈F ∥f ∥2 2 − E [ ∥f ∥2 2] + Kσ2 ec4 e-companion to Debiasing In-Sample Policy Performance Let C2 > 0 be a constant to be determined later. Dividing by C2 and taking expectations above shows E [ e δ2 C2 ] ≤ e Kσ2 C2 · E [e ¯Z C2 ] , (A.2) where ¯Z ≡ sup f ∈F {∥f ∥ 2 2 − E [ ∥f ∥2 2]} = sup f ∈F { K∑ k=1 f 2 j − E [ f 2 j ] } Importantly, ¯Z is again the suprema of an empirical process, namely for the “squared” elements. Pollard (1990) provides bounds on ¯Z in terms of the entropy integral of the process. Speciﬁcally, let f 2 denote the vector whose jth element is f 2 j . Let F 2 = {f 2 : f ∈ F}. Then the entropy integral of the squared process is deﬁned to be J ≡ 9 ∫ δ 0 √log D(x, F 2)dx, where δ ≡ supf ∈F ∥f 2∥2. Then, in the discussion just prior to Eq. (7.4) of Pollard (1990), it is proven that E [e ¯Z/∥ ¯J∥Ψ] ≤ 25. (A.3) Hence, to bound the right side of Eq. (A.2), we will next bound ∥ ¯J∥Ψ. This in turn will allow us to bound ∥δ∥Ψ and invoke Theorem A.2 of GR 2021. To this end, observe that for any f , g ∈ F, we have ∥f 2 − g2∥2 = K∑ k=1(f 2 j − g2 j )2 = K∑ k=1(fj + gj)2(fj − gj)2 ≤ 4U 2∥f − g∥ 2. Hence, D(ϵ, F 2) ≤ D ( ϵ 2U , F) . Write J ≡ 9 ∫ δ 0 √log D(x, F 2)dx ≤ 9 ∫ δ 0 √log D ( x 2U , F)dx. = 2U · 9 ∫ δ 2U 0 √log D (x, F)dx. where the last equality is a change of variables. We now claim we can upper bound this last expression by replacing the upper limit of integration with δ. Indeed, if δ 2U ≤ δ, then because the integrand is nonnegative, we only increase the integral. If, δ 2U > δ, then note that ∫ δ 2U δ √log D(x, F)dx = 0, e-companion to Debiasing In-Sample Policy Performance ec5 since D(x, F) = 1 for all x ≥ δ. Thus, in either case we can replace the upper limit of integration, yielding ¯J ≤ 18U ∫ δ 0 √log D(x, F)dx. Recall the entropy integral of the original process is given by J ≡ 9 ∫ δ 0 √log D(x, F)dx. Hence, J ≤ 2U J. Moreover, Theorem A.2 of GR 2021 shows that ∥J∥Ψ ≤ C3∥δ∥ΨV (A, W ) for some absolute constant C3. Thus we have successfully bounded ∥ ¯J∥Ψ ≤ 2U C3∥δ∥ΨV (A, W ). Substituting back into Eq. (A.3) shows that E [ e ¯Z 2U C3∥δ∥ΨV (A,W ) ] ≤ 25. Now choose C2 in Eq. (A.2) to be C2 = α2U C3∥δ∥ΨV (A, W ) for some α > 0 to be determined later. Substituting our bound on ¯Z into Eq. (A.2) shows E [ exp ( δ2 α2U C3∥δ∥ΨV (A, W ) )] ≤ e Kσ2 α2U C3∥δ∥ΨV (A,W ) · E [ e ¯Z α2U C3∥δ∥ΨV (A,W ) ] ≤ exp ( Kσ2 α2U C3∥δ∥ΨV (A, W ) ) · 251/α, (A.4) where we have used α > 0 and Jensen’s Inequality to simplify. We now to choose α large enough that the right side is at most 5. Taking logs, it suﬃces to choose α such that log(5) ≥ 1 α (log(25) + Kσ2 2U C3∥δ∥ΨV (A, W ) ) ⇐⇒ α ≥ 2 + Kσ2 2U C3 log(5) · ∥δ∥ΨV (A, W ) . Substituting into Eq. (A.4) shows ∥δ∥2 Ψ ≤ (2 + Kσ2 2U C3 log(5) · ∥δ∥ΨV (A, W ) ) 2U C3∥δ∥ΨV (A, W ) = 4C3U V (A, W )∥δ∥Ψ + Kσ2 log(5) In summary, ∥δ∥Ψ is at most the largest solution to the quadratic inequality y2 − by − c ≤ 0, ec6 e-companion to Debiasing In-Sample Policy Performance where b = 4C3U V (A, W ) and c = Kσ2 log(5) . Bounding the largest root shows y ≤ b 2 + √b2 + 4c 2 ≤ b 2 + b + 2 √c 2 (Triangle-Inequality) = b + √c. Or in other words, ∥δ∥Ψ ≤ 4C3U V (A, W ) + σ√K, where we upper bounded ( √log(5)) −1 ≤ 1. Now simply substitute into Eq. (A.1) and collect constants to complete the proof. □ A.1. Method of Bounded Diﬀerences Excluding an Exceptional “Bad” Set In our analysis, we utilize an extension of McDiarmid’s inequality due to Combes (2015). Recall, McDiarmid’s inequality shows that for a random vector Z ∈ X with independent components and function f : X → R such that |f (x) − f (y)| ≤ n∑ i=1 ciI {xi ̸= yi} ∀(x, y) ∈ X 2, (A.5) for some c ∈ Rn, we have that P {|f (Z) − E [f (Z)]| ≥ t} ≤ 2 exp (− 2t2 ∑n i=1 c 2 i ) . The next result extends McDiarmid’s inequality to a setting where Eq. (A.5) only holds for all (x, y) ∈ Y 2 where Y ⊆ X is a certain “good” set: Theorem A.5 (Combes (2015)) Let Z ∈ X be a random vector with independent components and f : X ↦→ R be a function such that |f (x) − f (y)| ≤ n∑ j=1 cjI {xj ̸= yj} ∀(x, y) ∈ Y, for some vector c ∈ Rn, where Y ⊆ X . Let c = ∑n i=1 ci, and p = P {X ̸∈ Y}. Then, for any t > 0 P (|f (Z) − E [f (Z) | Z ∈ Y]| ≥ t + p¯c) ≤ 2 (p + exp { − 2t2 ∑n i=1 c 2 i }) In particular, this implies that for any ϵ > 2p, with probability at least 1 − ϵ, |f (Z) − E [f (Z) | Z ∈ Y]| ≤ p¯c + ∥c∥2 √ log ( 2 ϵ − 2p ). Remark A.6 In the special case that Y = X , then p = 0, the theorem recovers McDiarmid’s inequality. e-companion to Debiasing In-Sample Policy Performance ec7 Appendix B: Properties of the Variance Gradient Correction (VGC) First, we state the relevant portion of Danskin’s Theorem for reference. See (Bertsekas 1997, Section B.5) for a proof of a more general result: Theorem B.1 (Derivative Result of Danskin’s Theorem). Let Z ⊆ Rm be a compact set, and let φ : Rn × Z ↦→ R be continuous and such that φ(·, z) : Rn ↦→ R is convex for each z ∈ Z. Additionally, deﬁne Z(x) = {¯z : φ(x, ¯z) = max z∈Z φ(x, z)} . Consider the function f : Rn ↦→ R given by f (x) = max z∈Z φ(x, z). If Z(x) consists of a unique point ¯z and φ(·, ¯z) is diﬀerentiable at x, then f is diﬀerentiable at x, and ∇f (x) = ∇xφ(x, ¯z), where ∇xφ(x, ¯z) is the vector with coordinates ∂φ(x, ¯z) ∂xi , i = 1, . . . , n. The remainder of the section contains proofs of the results in Section 3. B.1. Proof of Theorem 3.2 This section contains the omitted proofs leading to the proof of Theorem 3.2. We ﬁrst relate ﬁnite diﬀerence approximations of the subgradients of V (z + tej) to their true values. Lemma B.1 (Subgradients Bound Finite Diﬀerence Approximation) For any z ∈ Rn and t ∈ R, we have ajxj(z + tej)t ≤ V (z + tej) − V (z) ≤ ajxj(z)t. Proof: Let f (t) = V (z + tej). Recall that f (t) is concave and f ′(t) = ajxj(z + tej) by Dan- skin’s Theorem. Hence, by the subgradient inequality for concave functions, f (t) ≤ f (0) + f ′(0)t and f (0) ≤ f (t) − tf ′(t), and thus, tf ′(t) ≤ f (t) − f (0) ≤ tf ′(0). This is equivalent to ajxj(z + tej)t ≤ V (z + tej) − V (z) ≤ ajxj(z)t, which is the desired result. □ Equipped with Lemma B.1, the proof of Lemma 3.3 is nearly immediate. Proof of Lemma 3.3: The bounds in Lemma B.1 show that ajt(xj(z) − xj(z + tej)) ≥ 0. If aj ≥ 0, it follows that t(xj(z) − xj(z + tej)) ≥ 0 for all t, which shows that t ↦→ xj(z + tej) is non-increasing. Similarly, if aj < 0, it follows that t(xj(z) − xj(z + tej)) ≤ 0 for all t, which shows that t ↦→ xj(z + tej) is non-decreasing. □ Before proving Theorem 3.2, we establish the following intermediary result on the error of a non- randomized forward step, ﬁnite diﬀerence. ec8 e-companion to Debiasing In-Sample Policy Performance Lemma B.2 (Forward Step Finite Diﬀerence Error) Fix some j such that aj ̸= 0 and 0 < h < 1/e. Then, ∣ ∣ ∣ ∣ E [ ξjxj(Z) − V (Z + h √νjξjej) − V (Z) h √νjaj(θ) ] ∣ ∣ ∣ ∣ ≤ 4h log ( 1 h √νmin ) In other words, the forward ﬁnite step diﬀerence introduces a bias of order ˜O(h). Proof: From Lemma B.1, we see that the term inside the expectation can be upper-bounded by the non-negative term ξj [ xj(Z) − xj(Z + h √νjξjej)] . Hence, E [ ξjxj(Z) − V (Z + h √νjξjej) − V (Z) h√ νjaj(θ) ] ≤ E [ ξj (xj(Z) − xj(Z + h √νjξjej) )] . To simplify notation, let g(t) = E [xj (Z−j + ξjej) |ξj = t] where Z−j is identical to Z but has a 0 at the jth component. Then, ∣ ∣E [ ξj (xj(Z) − xj(Z + h √νjξjej))]∣ ∣ = ∣ ∣ ∣ ∣ ∫ ∞ −∞ t [ g(t) − g(t + h √νjt)] φj(t) dt ∣ ∣ ∣ ∣ where φj(t) is the density for N (0, 1/νj). To bound the integral, choose a constant U > 0 (which we optimize later) and break the integral into three regions, (−∞, −U ), (−U, U ), (U, ∞). This yields the upper bound ∣ ∣ ∣ ∣ ∣ ∫ ∞ −∞ t (g(t) − g(t + th √νj) ) φj(t) dt ∣ ∣ ∣ ∣ ∣ ≤ ∫ U −U U ∣ ∣g(t) − g(t + th √νj)∣ ∣ φj(t) dt ︸ ︷︷ ︸ (a) + ∫ −U −∞ |t| φj(t) dt + ∫ ∞ U |t| φj(t) dt ︸ ︷︷ ︸ (b) . We ﬁrst bound (a). As the ﬁst step, we attempt to remove the absolute value. From Lemma B.1, g(·) is a monotone function. We claim that for |t| < U , ∣ ∣g(t) − g(t + h √νjt)∣ ∣ ≤ ∣ ∣g (t − U h √νj) − g (t + U h√νj)∣ ∣ , (B.1) since (t − U h √νj, t + U h √νj) always contains the interval (t, t + h √νjt). Let b = { U h√ νj if aj > 0, −U h √νj otherwise, so that ∣ ∣g(t − U h √νj) − g(t + U h √νj) ∣ ∣ = g(t − b) − g(t + b). Then, ∫ ∞ −∞ ( g(t − b) − g(t + b)) φj(t) dt = ∫ ∞ −∞ g(t + b) (φj(t + 2b) − φj(t)) dt, (Change of variables) = ∫ ∞ −∞ g (t + b) ∫ t+2b t −νjzφj(z) dz dt, (since φ′ j(z) = −νjzφj(z)) e-companion to Debiasing In-Sample Policy Performance ec9 ≤ νj ∫ ∞ −∞ |z| φj(z) ∫ z z−2b g (t + b) dt dz, (Fubini’s Theorem) ≤ 2 |b| νj ∫ ∞ −∞ |z| φj(z)dz, (Since |g(t + b)| ≤ 1 for all t) = 4b √ νj 2π ≤ 2 √ 2 π U hνj. To summarize, we have shown that the term (a) satisﬁes ∫ U −U U ∣ ∣g(t) − g(t + th√νj)∣ ∣ φj(t) dt ≤ 2 √ 2 π U 2hνj To bound (b), we see that ﬁrst see that ∫ −U −∞ |t| φj(t) dt + ∫ ∞ U |t| φj(t) dt = 2 ∫ ∞ U tφj(t) dt = 2 √ 1 2πνj exp { −U 2νj 2 } where the ﬁrst equality holds by symmetry. Putting the bounds of (a) and (b) together, we have ∣ ∣ ∣ ∣ ∫ ∞ −∞ t [ g(t) − g(t + th√νj)] φj(t) dt ∣ ∣ ∣ ∣ ≤ 2 √ 2 π U 2hνj + 2 √ 1 2πνj exp { −U 2νj 2 } We approximately balance the two terms by letting U 2 = 2 νj log ( 1 h √νj ). Substituting and simplify- ing yields 4 √ 2 π h log ( 1 h √νj ) + h √ 2 π ≤ 4 √ 2 π h log ( 1 h √νmin ) + h √ 2 π . To simplify, note that h < 1/e and νmin ≤ 1 implies that log ( 1 h √νj ) ≥ 1. Hence, combining the two terms and simplifying provides a bound of 10 √ 1 2π h log ( 1 h √νmin ) . Note that 10/√2π ≤ 4 to complete the proof. □ We can now prove Theorem 3.2. Proof of Theorem 3.2: Notice that if aj(θ) = 0, then the jth term contribute nothing to the bias because x(Z) is independent of Zj, so E [ξjxj(Z)] = 0 = E [Dj(Z)]. Hence, we focus on terms j where aj(θ) ̸= 0. Decompose the jth term as E [ξjxj(Z) − Dj (Z)] = E [ ξjxj(Z) − V (Z + h √νjξjej) − V (Z) h √νjaj(θ) ] ︸ ︷︷ ︸ (a) + E [ E [ V (Z + h √νjξjej) − V (Z) h √νjaj(θ) − V (Z + δjej) − V (Z) h √νjaj(θ) ∣ ∣ ∣ ∣ Z]] ︸ ︷︷ ︸ (b) ec10 e-companion to Debiasing In-Sample Policy Performance We ﬁrst bound (b). Canceling out the V (Z) yields 1 h √νjaj(θ) E [V (Z + h √νjξjej) − V (Z + δjej)] . From our previous discussion, V (Z + h √νjξjej) ∼d V (Z + δjej), whereby E [ V (Z + h √νjξjej) − V (Z + δjej)] = 0. Lemma B.2 bounds (a) by 4h log ( 1 h √νmin ). Summing over j gives us our intended bound. □ B.2. Properties of VGC We next establish smoothness properties of the VGC. Proof of Lemma 3.7. We begin with i). We ﬁrst claim that θ ↦→ V (z, θ) is Lipschitz continuous with parameter Ln(1 + ∥z∥∞). To this end, write V (z, ¯θ) − V (z, θ) = (r (z, ¯θ) − r (z, θ))⊤ x(z, ¯θ) − r (z, θ) ⊤ (x(z, θ) − x(z, ¯θ)) ︸ ︷︷ ︸ ≤0 by optimality of x(z, θ) ≤ ∣ ∣ ∣(r (z, ¯θ) − r (z, θ))⊤ x(z, ¯θ) ∣ ∣ ∣ ≤ ∥r(z, ¯θ) − r(z, θ)∥1∥x(z, ¯θ)∥∞ ≤ ∥r(z, ¯θ) − r(z, θ)∥1 (since x(z, ¯θ) ∈ X ⊆ [0, 1] n) ≤ n∑ j=1 ∣ ∣aj( ¯θ) − aj(θ) ∣ ∣ |zj| + ∣ ∣bj( ¯θ) − bj(θ)∣ ∣ ≤ n∑ j=1 (L∥z∥∞∥ ¯θ − θ∥ + L∥ ¯θ − θ∥) = Ln · (1 + ∥z∥∞)∥ ¯θ − θ∥. Reversing the roles of θ and ¯θ yields an analogous bound, and, hence, ∣ ∣ ∣r (z, ¯θ)⊤ x(z, ¯θ) − r (z, θ)⊤ x(z, θ)∣ ∣ ∣ ≤ Ln (1 + ∥z∥∞) ∥ ∥ ¯θ − θ∥ ∥ This proves the ﬁrst statement. Next, we claim for any z, ∣ ∣ ∣ ∣ 1 aj( ¯θ) V (z, ¯θ) − 1 aj(θ) V (z, θ)∣ ∣ ∣ ∣ ≤ 2nL amin ( amax amin ∥z∥∞ + amax + bmax amin ) ∥ ¯θ − θ∥. (B.2) Write ∣ ∣ ∣ ∣ 1 aj( ¯θ) V (z, ¯θ) − 1 aj(θ) V (z, θ)∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ aj(θ)V (z, ¯θ) − aj( ¯θ)V (z, θ) aj(θ)aj( ¯θ) ∣ ∣ ∣ ∣ ≤ ∣ ∣ ∣ ∣ V (z, ¯θ) − V (z, θ) aj( ¯θ) ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ V (z, θ)(aj(θ) − aj( ¯θ)) aj(θ)aj( ¯θ) ∣ ∣ ∣ ∣ , ≤ Ln(1 + ∥z∥∞)∥ ¯θ − θ∥ amin + |V (z, θ)| L∥ ¯θ − θ∥ a2 min , e-companion to Debiasing In-Sample Policy Performance ec11 where the ﬁrst inequality follows by adding and subtracting aj(θ)V (θ) in the numerator, and the second inequality follows from the Lipschitz continuity of aj(θ) and V (z, θ) (Assumption 3.6). Next note that |V (z, θ)| ≤ ∥r(z, θ)∥1∥x(z, θ)∥∞ ≤ ∥a(θ) ◦ z∥1 + ∥b(θ)∥1 ≤ n∥z∥∞amax + nbmax. Substituting above and simplifying proves Eq. (B.2) We can now prove the lemma. Fix a component j. Then, Dj(z, ¯θ) − Dj(z, θ) = E [ 1 h √νjaj( ¯θ) (V (Z + δjej, ¯θ) − V (Z, ¯θ)) | Z = z ] − E [ 1 h √νjaj(θ) (V (Z + δjej, θ) − V (Z, θ)) | Z = z] = 1 h √νj E [ 1 aj( ¯θ) V (Z + δjej, ¯θ) − 1 aj(θ) V (Z + δjej, θ) | Z = z] + 1 h √νj E [ 1 aj(θ) V (Z, θ) − 1 aj( ¯θ) V (Z, ¯θ) | Z = z] . Hence, by taking absolute values and applying Eq. (B.2) twice we obtain ∣ ∣Dj(z, ¯θ) − Dj(z, θ)∣ ∣ ≤ 2nL hamin ( amax amin E [∥Z + δjej∥∞ | Z = z] + amax + bmax amin ) ∥ ¯θ − θ∥ + 2nL hamin ( amax amin ∥z∥∞ + amax + bmax amin ) ∥ ¯θ − θ∥, where we have passed through the conditional expectation. Finally, note that E [∥Z + δjej∥∞ | Z = z] ≤ ∥z∥∞ + E [|δj|] ≤ ∥z∥∞ + √ h2 + 2h/νj by Jensen’s inequality. We simplify this last expression by noting for h < 1/e, h2 < h, so that √h2 + 2h/νj ≤ √h √1 + 2/νmin ≤ 2 √ h νmin , using νmin ≤ 1. Thus, E [∥Z + δjej∥∞ | Z = z] ≤ ∥z∥∞ + 2√ h νmin . Substituting above and collecting terms yields 4nL hamin ( amax amin ∥z∥∞ + amax + bmax amin + amax amin √ h νmin ) ∥ ¯θ − θ∥. (B.3) We can simplify this expression by letting C3 ≥ 4 amin · max ( amax amin , amax + bmax amin , amax amin ) . Then Eq. (B.3) is at most C3nL h ( ∥z∥∞ + 1 + √ h νmin ) ∥ ¯θ −θ∥2 ≤ C3nL h (∥z∥∞ + 2 √νmin ) ∥ ¯θ −θ∥2 ≤ 2C3nL h √νmin (∥z∥∞ + 1) ∥ ¯θ −θ∥2, ec12 e-companion to Debiasing In-Sample Policy Performance where we have used the bounds on the precisions (Assumption 3.1) and h < 1/e to simplify. Letting C1 = 2C3 proves the ﬁrst part of the theorem. To complete the proof, we require a high-probability bound on ∥Z∥∞. Since Z − µ is sub- Gaussian, such bounds are well-known (Wainwright 2019), and we have with probability 1 − e−R, ∥Z∥∞ ≤ Cµ + ∥Z − µ∥∞ ≤ Cµ + C4√νmin √log n√R, for some universal constant C4. Substitute this bound into our earlier Lipschitz bound for an arbitrary z, and use the Assumption 3.1, h < 1/e, and R > 1 to collect terms and simplify. We then sum over the n terms of D(Z, θ) to complete the proof for i). We now bound ii). Focusing on the jth component of D(Z, (θ, h)) and writing Dj (Z, (θ, h)) ≡ Dj (Z, h, δh j , θ) = E [ V (Z + δh j ej, θ) − V (Z, θ) aj(θ)h √νj ∣ ∣ ∣ ∣ Z] , we see that Dj (Z, h, δh j , θ) − Dj (Z, h, δh j , θ) = Dj (Z, h, δh j , θ) − Dj (Z, h, δh j , θ) ︸ ︷︷ ︸ (a) + Dj (Z, h, δh j , θ) − Dj (Z, h, δh j , θ) ︸ ︷︷ ︸ (b) . To bound (a) and (b), we see from the proof of Lemma 3.8 that, ∣ ∣ ∣ ∣ V (Z, θ) − V (Z + Y ej, θ) aj(θ) ∣ ∣ ∣ ∣ ≤ |Y | . (B.4) We ﬁrst bound (a). We see ∣ ∣Dj (Z, h, δh j , θ) − Dj (Z, h, δh j , θ)∣ ∣ = ∣ ∣ ∣ ∣ h − h hh ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣E [ V (Z + δh j ej, θ) − V (Z, θ) √νj ∣ ∣ ∣ ∣ Z]∣ ∣ ∣ ∣ ≤ ∣ ∣h − h ∣ ∣ h 2 min ∣ ∣ ∣ ∣E [ |δh j | √νj ∣ ∣ ∣ ∣ Z]∣ ∣ ∣ ∣ , by Eq. (B.4) ≤ ∣ ∣h − h ∣ ∣ h 2 min 1 √νmin √ 3h √νmin ≤ √3 ∣ ∣h − h ∣ ∣ h 2 minν3/4 min , where the second to last inequality applies the inequality E [ |δh j | ] = E [√|δh j |2] ≤ √E [ |δh j |2] ≤ √ 3h √νmin . We then bound (b). We see ∣ ∣ ∣Dj (Z, h, δh j , θ) − Dj (Z, h, δh j , θ)∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣E [ V (Z + δh j ej, θ) − V (Z + δh j ej, θ) aj(θ)h √νj ∣ ∣ ∣ ∣ ∣ Z ]∣ ∣ ∣ ∣ ∣ = 1 h √νj ∣ ∣ ∣E [ f ( δh j ) − f (δh j )∣ ∣ ∣ Z]∣ ∣ ∣ ≤ 1 h √νj W2 (δh j , δh j ) , by Wainwright (2019, pg. 76) e-companion to Debiasing In-Sample Policy Performance ec13 The Wasserstein distance between two mean-zero Gaussians is known in closed form: W2 (δh j , δh j ) = ∣ ∣ ∣ ∣ ∣ √ h2 + 2h √νj − √ h 2 + 2h √ νj ∣ ∣ ∣ ∣ ∣ ≤ √ √ √ √ ∣ ∣ ∣ ∣ ∣h2 − h 2 + 2 (h − h ) √νj ∣ ∣ ∣ ∣ ∣ ≤ √(2 + 2 √νj ) ∣ ∣h − h ∣ ∣, where the ﬁrst inequality comes from the common inequality ∣ ∣ ∣√a − √ b∣ ∣ ∣ ≤ √|a − b|. Thus, ∣ ∣ ∣Dj (Z, h, δh j , θ) − Dj (Z, h, δh j , θ)∣ ∣ ∣ ≤ 1 hmin√νmin √(2 + 2 √νmin ) ∣ ∣h − h ∣ ∣. Collecting constants of the bounds of (a) and (b), we obtain our result. □ We now show the that the components of VGC is bounded. Proof of 3.8: We see V (Z + δjej, θ) − V (Z, θ) aj(θ)h √νj = 1 aj(θ)h √νj ( r(Z, θ)⊤ (x(Z + δjej, θ) − x(Z, θ)) ︸ ︷︷ ︸ ≤0 by optimality of x(Z,θ) + aj(θ)δjxj(Z + δjej, θ)) ≤ δjxj(Z + δjej, θ) h √νj ≤ |δj| h √νmin . By an analogous argument, V (Z, θ) − V (Z + δjej, θ) aj(θ)h √νj ≤ |δj| h √νmin . Taking the conditional expectation, we see |Dj(z)| ≤ E [∣ ∣ ∣ ∣ V (Z + δjej, θ) − V (Z, θ) aj(θ)h √νj ∣ ∣ ∣ ∣ ∣ ∣ ∣Z] ≤ E [ |δj| h √νmin ] ≤ √3 ν3/4 min√ h . where the ﬁrst inequality holds by Jensen’s inequality and the last inequality holds by Jensen’s inequality as E [|δj|] ≤ E [√δ2 j ] ≤ √E [ δ2 j ] = √h2 + 2h/ √νj ≤ √ 3h √νmin . □ B.3. Bias Under Violations of Assumption 2.4 In cases where the precisions νj are not known but estimated by a quantity ˜νj, we can construct the VGC in the same fashion, but replacing instances of νj with ˜νj, giving us, ∑ j:aj ̸=0 1 h √˜νjaj(θ) E [(V (µ + ξ + ˜δjej) − V (µ + ξ) )∣ ∣ ∣ ∣ ∣Z ] where ˜δj ∼ N (0, h2 + 2h/ √˜νj). The bias of this VGC is similar to Theorem 3.2, except that it picks up an additional bias term due to the approximation error incurred from ˜νj, which we quantify in the following lemma. ec14 e-companion to Debiasing In-Sample Policy Performance Lemma B.3 (Bias of VGC with Estimated Precisions) Suppose Assumption 3.1 holds. Let ˜νj be an estimate of νj and let ˜δj ∼ N (0, h2 + 2h/ √˜νj) and assume νmin ≤ minj ˜νj. For any 0 < h < 1/e, there exists a constant C dependent on νmin such that ∣ ∣ ∣ ∣ ∣ ∣E   n∑ j=1 ξjxj(Z) − n∑ j:aj ̸=0 E [ V (Z + ˜δjej) − V (Z) aj(θ)h √˜νj ∣ ∣ ∣ ∣ ∣Z ]  ∣ ∣ ∣ ∣ ∣ ∣ ≤ C · nh log ( 1 h ) + C √h ∑ j:aj ̸=0 ( ∣ ∣ ∣ν1/2 j − ˜ν1/2 j ∣ ∣ ∣ + √∣ ∣ ∣ν1/2 j − ˜ν1/2 j ∣ ∣ ∣ ) Proof of Lemma B.3 Move the inner conditional expectation outwards and consider a sample path with a ﬁxed Z. Let Dj(t) ≡ V (Z+tej )−V (Z) aj h √˜νj if aj ̸= 0 and 0 otherwise, so that E [Dj(˜δj) | Z] is the jth component of the VGC with the estimated precisions and √ ˜νj νj E [Dj(δj) | Z] is the jth component of the VGC with the correct νj. Fix some jth where aj ̸= 0. Note, ξjxj(Z) − Dj(˜δj) = (ξjxj(Z) − V (Z + δjej) − V (Z) ajh √νj ) + (√ ˜νj νj Dj(δj) − Dj(˜δj) ) The expectation of the ﬁrst term was bounded in Theorem 3.2, so we focus on the expectation of the second. We see √ ˜νj νj Dj(δj) − Dj(˜δj) = (√ ˜νj νj ) (Dj(δj) − Dj(˜δj)) ︸ ︷︷ ︸ (a) + ( 1 − √ ˜νj νj ) Dj(˜δj) ︸ ︷︷ ︸ (b) . To bound the expectation of (a), we see ﬁrst see t ↦→ h √˜νjDj(t) is 1−Lipschitz because ∂ ∂t h √˜νjDj(t) = ∂ ∂t V (Z + tej) − V (Z) aj = 1 aj ajxj(Z) = xj(Z) by Danskin’s theorem and because xj(Z) is between 0 and 1. Thus, by (Wainwright 2019, pg. 76) ∣ ∣ ∣E [ h √˜νj (Dj(δj) − Dj(˜δj))∣ ∣ ∣ Z]∣ ∣ ∣ ≤ W2 (δj, ˜δj) , where W2 (δj, ˜δj) is the Wasserstein distance between two mean-zero Gaussians δj and ˜δj, which is known in closed form: W2 ( δj, ˜δj) = ∣ ∣ ∣ ∣ ∣ √ h2 + 2h ˜ν1/2 j − √ h2 + 2h ν1/2 j ∣ ∣ ∣ ∣ ∣ ≤ √ √ √ √ ∣ ∣ ∣ ∣ ∣ 2h ˜ν1/2 j − 2h ν1/2 j ∣ ∣ ∣ ∣ ∣ ≤ √ √ √ √2h ∣ ∣ ∣ ∣ ∣ ν1/2 j − ˜ν1/2 j νmin ∣ ∣ ∣ ∣ ∣ where νmin ≤ minj {min {νj, ˜νj}}. To bound the expectation of (b), we see E [ ∣ ∣ ∣Dj(˜δj)∣ ∣ ∣∣ ∣ ∣ Z] ≤ E   ∣ ∣ ∣aj ˜δj∣ ∣ ∣ |aj|˜ν1/2 j h ∣ ∣ ∣ ∣ ∣ ∣ Z   = 1 ˜ν1/2 j h √ √ √ √ 2 π ( h2 + 2h ˜ν1/2 j ) ≤ √ 2 π ( 3 νminh ) e-companion to Debiasing In-Sample Policy Performance ec15 where the ﬁrst equality holds by directly evaluating the expectation and the last inequality holds because h < 1/e and νmin ≤ 1. Putting it all together, we see ∣ ∣ ∣ ∣ ∣E [ n∑ j=1 ξjxj(Z) − n∑ j=1 V (Z + ˜δjej) − V (Z) ajh √˜νj ]∣ ∣ ∣ ∣ ∣ ≤ ∣ ∣ ∣ ∣ ∣E [ n∑ j=1 ξjxj(Z) − √ ˜νj νj Dj(δj) ]∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ n∑ j=1 E [ E [ √ ˜νj νj Dj(δj) − Dj(˜δj) ∣ ∣ ∣ ∣ ∣ Z ]]∣ ∣ ∣ ∣ ∣ ≤ n∑ j=1 C · h log ( 1 h ) + ∑ j:aj ̸=0 1 h √νmin √ √ √ √2h ∣ ∣ ∣ ∣ ∣ ν1/2 j − ˜ν 1/2 j νmin ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ν1/2 j − ˜ν1/2 j νmin ∣ ∣ ∣ ∣ ∣ √ 2 π ( 3 hνmin ) ≤ n∑ j=1 C · h log ( 1 h ) + ∑ j:aj ̸=0 √2 ∣ ∣ ∣ν 1/2 j − ˜ν1/2 j ∣ ∣ ∣ √hν3/2 min + ∣ ∣ ∣ν1/2 j − ˜ν1/2 j ∣ ∣ ∣ √ 6 π √ hν 3/2 min where the ﬁrst inequality follows from triangle inequality and the second from applying Theorem 3.2 and our bounds on (a) and (b). Collecting constants we obtain our intended result. □ We now highlight when ξj are not Gaussian but only sub-Gaussian. Let Lemma B.4 (Bias VGC with Gaussian assumption violated) Suppose Assumption 3.1 holds. Let ξj be a mean-zero, sub-Gaussian random variable with variance proxy at most σ2 and admits a density density φ(·). Additionally, let ¯ξj ∼ N (0, 1/ √νj with density ¯φ(·). Then, there exists a dimension independent constant C, such that ∣ ∣ ∣ ∣ ∣ ∣E   n∑ j=1 ξjxj(Z) − n∑ j:aj ̸=0 E [ V (Z + δjej) − V (Z) aj(θ)h √νj ∣ ∣ ∣ ∣ ∣Z ]  ∣ ∣ ∣ ∣ ∣ ∣ ≤ n ( σ√2π − log ( ∥ ∥φ − ¯φ∥ ∥ 1 4 )) ∥ ∥φ − ¯φ∥ ∥ 1 + Cnh log ( 1 h ) + n∑ j:aj ̸=0 W2(ξj, ¯ξj) Proof of Lemma B.4 Let ¯Z be Z, but with the jth component replaced by ¯Zj = µj + ¯ξj and let Dj(t) = 1 ajh √νj E [ V (Z + (δj + t − ξj) ej) − V (Z + (t − ξj) ej)| Z] . We see |E [ξjxj(Z) − Dj(ξj)]| ≤ ∣ ∣E [ E [ ξjxj(Z) − ¯ξjxj( ¯Z)∣ ∣ Z −j]]∣ ∣ + ∣ ∣E [ ¯ξjxj( ¯Z) − Dj( ¯ξj)]∣ ∣ + ∣ ∣E [ E [ Dj( ¯ξj) − Dj(ξj) ∣ ∣ Z −j]]∣ ∣ By Lemma C.2 GR 2021, we see ∣ ∣E [ ξjxj(Z) − ¯ξjxj( ¯Z)∣ ∣ Z −j]∣ ∣ ≤ T ∥ ∥φ − ¯φ ∥ ∥1 + 4 exp (− T 2 2σ2 ) (T + σ√2π) . ec16 e-companion to Debiasing In-Sample Policy Performance To optimize T , we ﬁrst upperbound the latter term as follows 4 exp (− T 2 2σ2 ) (T + σ√2π) = 4 exp (− T 2 2σ2 + log (T + σ√2π)) ≤ 4 exp (− T 2 2σ2 + (T + σ√2π − 1 )) , since log t < t − 1 = 4 exp (− T 2 2σ2 + 2T + σ√2π − 1 − T ) ≤ 4 exp (σ√2π − 1 − T ) where the last inequality used the fact that the quadratic − T 2 2σ2 + 2T is maximized at T ∗ = 4σ2. Substituting the upperbound, we see T ∥ ∥φ − ¯φ∥ ∥ 1 + 4 exp (− T 2 2σ2 ) (T + σ√2π) ≤ T ∥ ∥φ − ¯φ ∥ ∥1 + 4 exp (σ√2π − 1 − T ) We see the right hand side is minimized at T ∗ = σ√2π − 1 − log ( ∥φ− ¯φ∥ 1 4 ). Thus, we see ∣ ∣E [ ξjxj(Z) − ¯ξjxj( ¯Z)∣ ∣ Z −j]∣ ∣ ≤ ( σ√2π − log ( ∥ ∥φ − ¯φ ∥ ∥1 4 )) ∥ ∥φ − ¯φ ∥ ∥ 1 By Theorem 3.2, we see ∣ ∣E [ ¯ξjxj( ¯Z) − Dj( ¯Z)∣ ∣ Z −j]∣ ∣ ≤ Ch log ( 1 h ) . Finally, since t ↦→ h √νjDj(t) is 1−Lipschitz from Lemma B.3, we see that ∣ ∣E [ Dj( ¯Z) − Dj(Z)∣ ∣ Z −j]∣ ∣ ≤ W2(ξj, ¯ξj). Putting it all together, we see |E [ξjxj(Z) − Dj(ξj)]| ≤ ( σ√2π − log ( ∥ ∥φ − ¯φ∥ ∥ 1 4 )) ∥ ∥φ − ¯φ∥ ∥ 1 + Ch log ( 1 h ) + W2(ξj, ¯ξj). Summing over the j terms, we obtain our result □ B.4. Proof of Theorem 3.5. Before proving the theorem, we require the following lemma. Lemma B.5 (A χ2-Tail Bound) Consider δ = (δ1, . . . , δn)⊤ where δj is deﬁned as in the deﬁni- tion of the VGC (Eq. (3.3)) and 0 < h < 1/e. Suppose Assumption 3.1 holds. Then, E [ ∥δ∥2 2I { ∥δ∥2 2 > 18hn √νmin }] ≤ 36hn √νmin e−n. e-companion to Debiasing In-Sample Policy Performance ec17 Proof of Lemma B.5. Let Y1, . . . , Yn be independent standard normals. Observe that since h < 1/e, the variance of δj is at most h2 + h √νj ≤ 2h/√νmin. Then, for t > 1 P (∥δ∥ 2 2 > 2hn √νmin (1 + t)) ≤ P ( 2h √νmin n∑ j=1 Y 2 j > 2hn √νmin (1 + t) ) = P ( 1 n n∑ j=1 Y 2 j > 1 + t ) ≤ e−nt/8, (B.5) where the last inequality follows from (Wainwright 2019, pg. 29). Next, by the tail formula for expectation, E [ ∥δ∥2 2I { ∥δ∥2 2 > 18hn √νmin }] = ∫ ∞ 0 P (∥δ∥2 2I { ∥δ∥2 2 > 18hn √νmin } > t ) dt = ∫ 18hn√ νmin 0 P (∥δ∥ 2 2 > 18hn √νmin ) dt + ∫ ∞ 18hn√ νmin P (∥δ∥ 2 2 > t ) dt ≤ 18hn √νmin e−n + ∫ ∞ 18hn√ νmin P ( ∥δ∥2 2 > t) dt (Applying Eq. (B.5)) ≤ 18hn √νmin e−n + 2hn √νmin ∫ ∞ 8 P (∥δ∥2 2 > 2hn √νmin (1 + s)) ds ≤ 9hn √νmin e−n + 2hn √νmin ∫ ∞ 8 e −ns/8ds (Applying Eq. (B.5)) = 18hn √ νmin e−n + 16h √νmin e −n Rounding up and combining proves the theorem. □ We can now prove the theorem. Proof of Theorem 3.5. Proceeding as in the main body, we bound each of the three terms of the out-of-sample estimator error (Eq. (3.10)). Before beginning, note that under Assumption 3.1, Var(δj) ≤ 3h √νmin . We use this upper bound frequently. We start with Eq. (3.10b). Consider the kth non-zero element of the sum. By deﬁnition of DR, ∣ ∣ ∣DR(Z, δ, ˜U ) − DR(Z, δk, ˜U )∣ ∣ ∣ = ∣ ∣ ∣ ∣ δk h √νkak xk(Z + δk ˜Ukek) − δk h √νkak xk(Z + δk ˜Ukek)∣ ∣ ∣ ∣ ≤ 1 h √νkak (|δk| + ∣ ∣δk∣ ∣) Hence, squaring and taking expectations, E [(DR(Z, δ, ˜U ) − DR(Z, δk, ˜U ))2] ≤ 2 h2νka2 k (E [ δ2 k] + E [δ2 k]) ≤ 12 hν 3/2 minamin . ec18 e-companion to Debiasing In-Sample Policy Performance Summing over k shows Eq. (3.10b) ≤ 6n hν3/2 minamin . We now bound Eq. (3.10c). Again, consider the kth non-zero element. By deﬁnition, ∣ ∣ ∣DR(Z, δ, ˜U ) − DR(Z, δ, ˜U k)∣ ∣ ∣ = |δk| h √νkak ∣ ∣xk(Z + δkUkek) − xk(Z + δkU kek)∣ ∣ ≤ 2 |δk| h √νminamin Hence, E [(DR(Z, δ, ˜U ) − DR(Z, δ, ˜U k))2] ≤ 4 h2νmina 2 min E [ δ2 k] ≤ 12 hν3/2 mina 2 min Summing over k shows Eq. (3.10c) ≤ 6n hν3/2 mina2 min . Finally, we bound Eq. (3.10a). For convenience, let Wk ∈ Rn be the random vector with compo- nents Wkj = xj(Z + δj ˜U ej) − xj(Z k + δj ˜U ej). Then, proceeding as in the main text, we have (DR(Z) − DR(Z k) )2 ≤ ∥δ∥2 2 h2a2 minνmin · n∑ j=1 (xj(Z + δj ˜U ej) − xj(Z k + δj ˜U ej))2 ≤ ∥δ∥2 2 h2a2 minνmin · ∥Wk∥ 2 2 Notice that E [∥δ∥ 2 2] = O (nh/νmin). We upper bound this expression by splitting on cases where ∥δ∥2 2 > 18hn √νmin or not. Note this quantity is much larger than the mean, so we expect contributions when ∥δ∥2 2 is large to be small. Splitting the expression yields (DR(Z) − DR(Z k))2 ≤ ∥δ∥2 2 h2a2 minνmin · ∥Wk∥ 2 2I { ∥δ∥ 2 2 > 18hn √νmin } + ∥δ∥2 2 h2a2 minνmin · ∥Wk∥ 2 2I { ∥δ∥ 2 2 ≤ 18hn √νmin } ≤ n h2a2 minνmin ∥δ∥2 2I { ∥δ∥2 2 > 18hn √νmin } + 18n ha 2 minν 3/2 min · ∥Wk∥ 2 2 Next take an expectation and apply Lemma B.5 to obtain E [(DR(Z) − DR(Z k))2] ≤ 36n2 ha 2 minν3/2 min e−n + 18n ha 2 minν3/2 min · E [ ∥Wk∥ 2 2] Finally summing over k shows Eq. (3.10a) ≤ 36n3 ha 2 minν3/2 min e −n + 18n3 ha 2 minν3/2 min · 1 n2 n∑ k=1 E [ ∥Wk∥ 2 2] Finally, we combine all three terms in Eq. (3.10) yielding Var(D(Z)) ≤ 6n hν3/2 minamin + 6n hν 3/2 mina 2 min + 36n3 2ha 2 minν3/2 min e −n + 18n3 2ha 2 minν 3/2 min · 1 n2 n∑ k=1 E [ ∥Wk∥2 2] ≤ C2 h max(n3−α, n), by collecting the dominant terms. □ e-companion to Debiasing In-Sample Policy Performance ec19 B.5. Implementation Details As mentioned, in our experiments we utilize a second-order forward ﬁnite diﬀerence approximation. Namely, instead of approximating the derivative using a ﬁrst-order approximation as in Eq. (3.6), we approximate ∂ ∂λ V (Z + λξjej)∣ ∣ ∣ ∣λ=0 = 1 2h √νjaj ( 4V (Z + h √νjξjej) − V (Z + 2h √νjξjej) − 3V (Z)) + O(h 2). The coeﬃcients in this expansion can be derived directly from a Taylor Series. We then use ran- domization to replace the unknown hξj and 2hξj term as before. The jth element of our estimator becomes Dj(Z) ≡ E [ 1 2h √νjaj (4V (Z + δh j ej) − V (Z + δ2h j ej) − 3V (Z))∣ ∣ ∣ ∣ Z] . where δh j ∼ N (0, h2 + 2h √ νj ) and δ2h j ∼ N (0, 4h 2 + 4h √νj ). As mentioned, one can always compute the above conditional expectation by Monte Carlo sim- ulation. In special cases, a more computationally eﬃcient method is to utilize a parametric pro- gramming algorithm to determine the values of x(Z + tej) as t ranges over R. Importantly, for many classes of optimization problems, including, e.g., linear optimization and mixed-binary lin- ear optimization, x(Z + tej) is piecewise constant on the intervals (ci, ci+1), taking value xi, for i = 1, . . . , I, with c0 = −∞ and cI = ∞. In this case, E [ V (Z + δjej)| Z] = I−1∑ i=0 r(Z)⊤xi · ∫ ci+1 ci φδj (t) dt + √ σ2 2π exp ( (cj − Zj)2 2σ2 ) (B.6) where φδj (·) is the pdf of δj. These integrals can then be evaluated in closed-form in terms of the standard normal CDF. A similar argument holds for E [ V (Z + δ2h j ej)∣ ∣ Z] . We follow this strategy in our case study in Section 5. Appendix C: Problems that are Weakly Coupled by Variables C.1. Convergence of In-Sample Optimism In this section we provide a high-probability bound on sup x0∈X 0 sup θ∈Θ ∣ ∣ξ⊤x(Z, θ, x0) − E [ ξ⊤x(Z, θ, x0)]∣ ∣ . As a ﬁrst step, we bound the cardinality of X Θ,X 0(Z) ≡ {(x0, x1(Z, θ, x0), . . . , xK(Z, θ, x0)) : θ ∈ Θ, x0 ∈ X 0} ⊆ R n. Lemma C.1 (Cardinality of Lifted, Decoupled Policy Class) Under the assumptions of Theorem 4.3, there exists an absolute constant C such that log ∣ ∣ ∣X Θ,X 0(Z)∣ ∣ ∣ ≤ dim(φ) · log (CK ∣ ∣X 0∣ ∣ X 2 max) ec20 e-companion to Debiasing In-Sample Policy Performance Proof: We adapt a hyperplane arrangement argument from Gupta and Kallus (2021). We summa- rize the pertinent details brieﬂy. For any x0 ∈ X 0, xk(Z, θ, x0) is fully determined by the relative ordering of the values {φ(θ)⊤gk(Z k, xk, x0) : xk ∈ X k(x0)} . This observation motivates us to consider the hyperplanes in Rdim(φ) Hk,Z,x0(xk, ¯xk) = {φ(θ) : φ(θ)⊤ ( gk(Z k, xk, x0) − gk(Z k, ¯xk, x0)) = 0 } (C.1) for all xk, ¯xk ∈ Ext (X k(x0)), k = 1, . . . , K, and x0 ∈ X 0. On one side of Hk,Z,x0(xk, ¯xk), xk is preferred to ¯xk in the policy problem Eq. (4.3), on the other side ¯xk is preferred, and on the hyperplane we are indiﬀerent. Thus, if we consider drawing all such hyperplanes in Rdim(φ), then the vector x(Z, θ, x0) is constant for all φ(θ) in the relative interior of each induced region. Hence, ∣ ∣ ∣X Θ,X 0(Z)∣ ∣ ∣ is at most the number of such regions. Gupta and Kallus (2021) prove that the number of such regions is at most (1 + 2m)dim(φ) where m is number of hyperplanes in Eq. (C.1). By assumption, m ≤ KX 2 max |X 0|, and, hence, |X Θ(Z, x0)| ≤ (1 + 2K |X 0| X 2 max)dim(φ) . Collecting constants yields the bound. □ We can now prove our high-probability tail bound. Lemma C.2 (Bounding In-sample Optimism) Under the assumptions of Theorem 4.3, there exists a constant C (depending on νmin) such that, for any R > 1, with probability 1 − e −R sup θ∈Θ,x0∈X 0 ∣ ∣ξ⊤x(Z, θ, x0) − E [ ξ⊤x(Z, θ, x0)]∣ ∣ ≤ CSmaxR√Kdim(φ) log(K |X 0| Xmax) Proof: By triangle inequality, sup θ∈Θ,x0∈X 0 ∣ ∣ξ⊤x(Z, θ, x0) − E [ξ⊤x(Z, θ, x0)]∣ ∣ ≤ sup x0∈X 0 ∣ ∣(ξ0)⊤x0 − E [ (ξ0)⊤x0]∣ ∣ + sup x0∈X 0,θ∈Θ ∣ ∣ ∣ ∣ ∣ K∑ k=1 ∑ j∈Sk ξjxj(Z, θ, x0) − E [ ξjxj(Z, θ, x0) ]∣ ∣ ∣ ∣ ∣ Consider the ﬁrst term. For a ﬁxed x0, this is a sum of sub-Gaussian random variables each with parameter at most 1/ √νmin. We apply Hoeﬀding’s inequality to obtain a pointwise bound for a ﬁxed x0 and then take a a union bound over X 0. This yields for some absolute constant c, P { sup x0∈X 0 ∣ ∣ ∣ ∣ ∣ ∑ j∈S0 ξjx0 j − E [ ξjx 0 j ] ∣ ∣ ∣ ∣ ∣ > t } ≤ 2|X 0| exp (− cνmint 2 |S0| ) , Rearranging shows that, with probability at least 1 − exp {−R}, sup x0∈X 0 ∣ ∣ ∣ ∣ ∣ ∑ j∈S0 ξjx0 j − E [ξjx 0 j ] ∣ ∣ ∣ ∣ ∣ ≤ √ |S0|R cνmin log (2|X 0|) e-companion to Debiasing In-Sample Policy Performance ec21 For the second component, we apply Theorem A.1. We bounded the cardinality X Θ,X 0(Z) in Lemma C.1. Consider the vector ( ∑ j∈S1 ξjxj(Z; θ, x0), . . . , ∑ j∈SK ξjxj(Z; θ, x0) ) . This vector has independent components. We next construct an envelope F (Z) for it and bound the Ψ-norm of F (Z). Since 0 ≤ xj(Z, θ, x0) ≤ 1, we take F (Z) = ( ∑ j∈S1 |ξj| , . . . , ∑ j∈SK |ξj| ) . Note that ( ∑ j∈Sk |ξj| )2 (a) ≤ |Sk| ∑ j∈Sk ( ζj √νj )2 ≤ Smax νmin ∑ j∈Sk ζ 2 j where ζj ∼ N (0, 1). Inequality (a) uses ∥t∥1 ≤ √d ∥t∥2 for t ∈ Rd. Plugging into ∥∥F (ξ)∥2∥Ψ we have ∥∥F (Z)∥2∥ Ψ ≤ √ |Smax| νmin · ∥ ∥ ∥ ∥ ∥ ∥ √ √ √ √ K∑ k=1 ∑ j∈Sk ζ 2 j ∥ ∥ ∥ ∥ ∥ ∥ Ψ (b) ≤ √ |Smax| νmin · 2|Smax|K. Inequality (b) follows from Lemma A.1 iv) of GR 2021. Applying Theorem A.1, combining the bounds of our two components, and collecting constants, we obtain our result. □ C.2. Convergence of VGC Deﬁne V (Z, θ, x0) and D(Z, θ, x0) analogously to V (Z, θ) and D(Z, θ) with x(Z, θ) replaced by x(Z, θ, x0) throughout. The next step in our proof provides a high-probability bound on sup θ∈Θ,x0∈X 0 ∣ ∣D(Z, θ, x0) − E [ D(Z, θ, x0)]∣ ∣ . We ﬁrst establish a pointwise bound for a ﬁxed θ, x0. Lemma C.3 (Pointwise Convergence of VGC for a ﬁxed x0) Under the assumptions of Theorem 4.3, for ﬁxed θ, x0, there exists a constant C (that depends on νmin) such that, for any R > 1, we have with probability 1 − 2 exp(−R), |D(Z, θ) − E [D(Z, θ)]| ≤ C |Smax| √ KR h ec22 e-companion to Debiasing In-Sample Policy Performance Proof: By deﬁnition, D(Z, θ, x0) = ∑ j∈S0 aj (θ)̸=0 Dj(Z, θ, x0) + K∑ k=1 E     ∑ j∈Sk aj (θ)̸=0 V (Z + δjej, θ, x0) − V (Z, θ, x0) aj(θ)h √νj ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ Z     . (C.2) Consider the ﬁrst term. Since x0 is ﬁxed (deterministic), Dj(Z, θ, x0) = 1 aj(θ)h √νj E [V (Z + δjej, θ, x0) − V (Z, θ, x0)∣ ∣ ∣Z] = 1 h √νj E [ δjx 0 j ] = 0. This equality holds almost surely. Hence, it suﬃces to focus on the second term in Eq. (C.2). Importantly, the second term is a sum of K independent random variables for a ﬁxed x0. We next claim that each of these random variables is bounded. For any j such that aj(θ) ̸= 0, let Si be the subproblem such that j ∈ Si. Then, write V (Z + δjej, θ, x0) − V (Z, θ, x0) aj(θ)h √νj = 1 aj(θ)h √νj ( rk(Z, θ, x0)⊤ (xk(Z + δjej, θ, x0) − xk(Z, θ, x0)) ︸ ︷︷ ︸ ≤0 by optimality of xk(Z,θ,x0) + aj(θ)δjxj(Z + δj, ej, θ, x0)) ≤ δjxj(Z + δjej, θ, x0) h √νj ≤ |δj| h √νj . By an analogous argument, V (Z, θ, x0) − V (Z + δjej, θ, x0) aj(θ)h √νj ≤ |δj| h √νj . Hence, ∣ ∣Dj(Z, θ, x0)∣ ∣ ≤ E [∣ ∣ ∣ ∣ V (Z + δjej, θ, x0) − V (Z, θ, x0) aj(θ)h √νj ∣ ∣ ∣ ∣ ∣ ∣ ∣Z] ≤ 1 h √νj E [δj] ≤ 1 h √νj · √h ν 1/4 j ≤ 1 √hν3/4 min . (C.3) Applying Hoeﬀding’s inequality to Eq. (C.2) and collecting constants shows P {|D(Z; θ) − E [D(Z; θ)]| ≥ t} ≤ 2 exp (− h · t2 C0K |Smax| 2 ) for some constant C0 (depending on νmin). Thus, with probability 1 − ϵ, we see |D(Z; θ) − E [D(Z; θ)]| ≤ √ C0 · K |Smax| 2 h log ( 2 ϵ ) Combining constants completes our proof. □ e-companion to Debiasing In-Sample Policy Performance ec23 We now bound sup θ∈Θ ∣ ∣D(Z, θ, x0) − E [ D(Z, θ, x0)]∣ ∣ for a ﬁxed x0. The key idea is to use the Lipschitz continuity of θ ↦→ D(Z, θ, x0) to cover the set Θ. Lemma C.4 (Uniform Convergence of VGC for a ﬁxed x0) Under the assumptions of Theorem 4.3 and for H ≡ [hmin, hmax], there exists a constant C (that depends on νmin, Cµ, L) such that for any R > 1, we have with probability 1 − 2e−R, sup θ∈ ¯Θ |D(Z, θ) − E [D(Z, θ)]| ≤ CSmax √ KR hmin √ √ √ √log n · log N (√ hmin Kn2 , Θ ) N ( hmin K , H). Proof. Using the full notation D(Z, (θ, h)), we ﬁrst write the supremum to be over θ ∈ Θ and h ∈ H. Let Θ0 be a ε1-covering of Θ and let H be a ε2-covering of H ≡ [hmin, hmax]. Then, sup θ∈Θ h∈H |D(Z, (θ, h)) − E [D(Z, (θ, h))]| ≤ sup θ∈Θ0 h∈H |D(Z, (θ, h)) − E [D(Z, (θ, h))]| (C.4a) + sup θ,θ:∥θ−θ∥≤ε1 h∈H ∣ ∣D(Z, (θ, h)) − D(Z, (θ, h))∣ ∣ + sup θ,θ:∥θ−θ∥≤ε1 h∈H ∣ ∣E [D(Z, (θ, h)) − D(Z, (θ, h))]∣ ∣ (C.4b) + sup θ∈Θ0 h,h:∥h−h∥≤ε2 ∣ ∣D(Z, (θ, h)) − D(Z, (θ, h)) ∣ ∣ + sup θ∈Θ0 h,h:∥h−h∥≤ε2 ∣ ∣E [ D(Z, (θ, h)) − D(Z, (θ, h)) ]∣ ∣ (C.4c) We bound Eq. (C.4b) and Eq. (C.4c) using Lemma 3.7. For Eq. (C.4b), there exists a constant C1 such that with probability at least 1 − e−R ∣ ∣D(Z, (θ, h)) − D(Z, (θ, h)) ∣ ∣ ≤ C1ε1n2 h √R log n. Similarly, there exists C2, C3 and C4 (depending on νmin, L, Cµ, amin, amax, bmax) such that ∣ ∣E [ D(Z, (θ, h)) − D(Z, (θ, h)) ]∣ ∣ ≤ C2ε1n2 h (E [∥z∥∞] + 1) ≤ C3ε1n2 h (√log n + Cµ) ≤ C4ε1n2 h √log n, where the second inequality uses a standard bound on the maximum of n sub-Gaussian random variables, and we have used Assumption 3.1 to simplify. Combining and taking the supremum over h shows Eq. (C.4b) ≤ C5ε1n2 hmin √R log n, for some constant C5. For Eq. (C.4c), there exists a constant C6 such that ∣ ∣D(Z, (θ, h)) − D(Z, (θ, h)) ∣ ∣ ≤ C6ε 1/2 2 n hmin . ec24 e-companion to Debiasing In-Sample Policy Performance Since the same holds for the expectation of the same quantity, we see Eq. (C.4c) ≤ 2C6ε 1/2 2 n hmin , Similarly, using Lemma C.3 and applying a union bound over elements in Θ and H shows with probability at least 1 − e−R, Eq. (C.4a) ≤ C7Smax √ KR hmin log (N (ε1, Θ)N (ε2, H)), for some constant C7. Choosing ε1 = Smax√Khmin n2 , ε2 = S2 maxKhmin n2 , we see Eq. (C.4a) + Eq. (C.4b) + Eq. (C.4c) ≤ C8Smax √ KR hmin log (N (ε1, Θ)N (ε2, H)), Finally, we obtain our result by simplifying the above bound slightly since n = ∑K k=0 |Sk| ≤ KSmax, and hence, ε1 = KSmax√Khmin Kn2 ≥ √ hmin Kn2 ε2 = S2 maxK 2hmin Kn2 ≥ hmin K . Substituting the lower-bounds, we obtain our intended result. □ We can now prove Theorem 4.3. Proof of Theorem 4.3: We proceed to bound each term on the right side of Eq. (4.1). To bound Eq. (4.1a), observe by deﬁnition of our lifted policy class and Lemma C.2, we have, with probability at least 1 − e−R, that sup θ∈ ¯Θ ∣ ∣ξ⊤x(Z, θ) − E [ ξ⊤x(Z, θ)]∣ ∣ ≤ sup θ∈ ¯Θ,x0∈X 0 ∣ ∣ξ⊤x(Z, θ, x0) − E [ ξ⊤x(Z, θ, x0)]∣ ∣ ≤ CSmaxR√Kdim(φ) log(K |X 0| Xmax). To bound Eq. (4.1b), let H ≡ [hmin, hmax]. Then, by applying the union bound to Lemma C.4 with R ← R + log(1 + |X 0|) we have that with probability at least 1 − 2e−R, sup θ∈Θ x0∈X 0 |D(Z, θ) − E [D(Z, θ)]| ≤ C1Smax √ K(R + log(|1 + X 0|) hmin √log n · log (N (ε1, Θ) N (ε2, H)) ≤ C2Smax √ KR log(|1 + X 0|) hmin √log n · log (N (ε1, Θ) N (ε2, H)), for some constants C1 and C2 and where ε1 = √ hmin Kn2 and ε2 = hmin K . Finally, to bound Eq. (4.1c), use Theorem 3.2 and take the supremum over h ∈ H to obtain Eq. (4.1c) ≤ C5hmaxKSmax log(1/hmin). Substituting these three bounds into Eq. (4.1) and collecting constants proves the theorem. □ e-companion to Debiasing In-Sample Policy Performance ec25 Appendix D: Problems that are Weakly Coupled by Constraints D.1. Properties of the Dual Optimization Problem Throughout the section, we use the notation ⟨ℓ, u⟩ to denote the interval [min(ℓ, u), max(ℓ, u)]. Recall our dual formulation λ(z, θ) ∈ arg min λ≥0 L(λ, z, θ), where L(λ, z, θ) ≡ b⊤λ + 1 n n∑ j=1 [ rj(zj, θ) − A⊤ j λ]+ . Since L(λ) is non-diﬀerentiable, its (partial) subgradient is not-unique. We identify a particular subgradient by ∇λL(λ, z, θ) = b − 1 n n∑ j=1 I {rj(zj, θ) > A⊤ j λ } Aj. The following identity that characterizes the remainder term in a ﬁrst order Taylor-series expan- sion of L(λ) with this subgradient. Lemma D.1 (A Taylor Series for L(λ)) For any λ ∈ Rm + , z ∈ Rn, θ ∈ Θ, L(λ2, z, θ)−L(λ1, z, θ) = ∇λL(λ1, z, θ)⊤(λ2 −λ1)+ 1 n n∑ j=1 I { rj(zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ } ∣ ∣rj(z, θ) − A⊤ j λ2∣ ∣ . Proof of Lemma D.1: Since z and θ are ﬁxed, drop them from the notation. Using the deﬁni- tions of L and ∇λL, we see it is suﬃcient to prove that for each j, [rj − A⊤ j λ2] + − [rj − A⊤ j λ1] + + I {rj > A⊤ j λ1} A⊤ j (λ2 − λ1) = I { rj ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ } ∣ ∣rj − A⊤ j λ2∣ ∣ (D.1) First notice that if A⊤ j λ1 = A⊤ j λ2, then both sides of Eq. (D.1) equal zero. Further, if rj ̸∈ ⟨A⊤ j λ1, A⊤ j λ2⟩, then both sides are again zero. Thus, we need only considering the case where rj ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩. We can conﬁrm the identity directly by considering the cases where A⊤λ1 < A⊤λ2 and A⊤λ1 > A⊤λ2 separately. □ The following result is proven in Lemma D.3 of GR 2021. We reproduce it here for completeness. Lemma D.2 (Dual Solutions Bounded by Plug-in) If X is s0-strictly feasible, then i) ∥λ(z, θ)∥1 ≤ 2 ns0 ∥r(z, θ)∥1 ii) E [∥λ(z, θ)∥1] ≤ 2 ns0 E [∥r(z, θ)∥1] Proof of Lemma D.2: By optimality, L(λ(z, θ), z, θ) ≤ L(0, z, θ) ≤ 1 n ∥r(z, θ)∥1. Since λ(z, θ) ≥ 0, it follows that ∥λ(z, θ)∥1 = e⊤λ(z, θ). Thus, ∥λ(z, θ)∥1 ≤ max λ≥0 e⊤λ s.t. b⊤λ + 1 n n∑ j=1 (rj(zj, θ) − A⊤ j λ)+ ≤ 1 n ∥r(z, θ)∥1. ec26 e-companion to Debiasing In-Sample Policy Performance We upper bound this optimization by relaxing the constraint with penalty 1/s0 > 0 to see that ∥λ(z, θ)∥1 ≤ max λ≥0 e ⊤λ + 1 s0 ( 1 n ∥r(z, θ)∥1 − b⊤λ − 1 n n∑ j=1 (rj(zj, θ) − A⊤ j λ)+) = max λ≥0 e⊤λ + 1 s0 ( 1 n ∥r(z, θ)∥1 − b⊤λ − 1 n n∑ j=1 max xj ∈[0,1] xj (rj(zj, θ) − A⊤ j λ )) ≤ max λ≥0 e⊤λ + 1 s0 ( 1 n ∥r(z, θ)∥1 − b⊤λ − 1 n n∑ j=1 x 0 j (rj(zj, θ) − A⊤ j λ )) = max λ≥0 (e − 1 s0 b + 1 ns0 Ax0)⊤ λ + 1 ns0 (∥r(z, θ)∥1 − r(z, θ)⊤x0) . By s0-strict feasibility, 1 n Ax0 + s0e ≤ b ←→ e − 1 s0 b + 1 ns0 Ax0 ≤ 0. Hence, λ = 0 is optimal for this optimization problem. Thus, for all θ ∈ Θ, ∥λ(z, θ)∥1 ≤ 1 ns0 (∥r(z, θ)∥1 − r(z, θ)⊤x0) ≤ 2 ns0 ∥r(z, θ)∥1. This proves i). Applying the expectation to both sides completes the proof. □ D.2. Constructing the Good Set To construct the set of Z where approximate strong convexity holds or the “good” set, we ﬁrst deﬁne the following constants: λmax ≡ 2 s0 (amax (Cµ + 4 √νmin ) + bmax ) , (D.2a) φmin ≡ √νmin amax√2π exp (− νmax(amaxCµ + bmax + CAλmax)2 2a2 min ) . (D.2b) Λn = {(λ1, λ2) ∈ Rm + × R m + : ∥λ1∥1 ≤ λmax, ∥λ2∥1 ≤ λmax, ∥λ1 − λ2∥2 ≥ 4/n}, and (D.2c) Tn = {(λ, θ, Γ) ∈ Rm + × Θ × R : ∥λ∥1 ≤ λmax, Γ ≥ 1 n } . (D.2d) These values depend on the constants deﬁned in Assumption 4.4 and Assumption 4.5. We now deﬁne the “good” set, En ≡ { z : (∇λL(λ1, z, θ) − ∇λL(λ2, z, θ))⊤(λ1 − λ2) ≥ φminβ∥λ1 − λ2∥2 2 − ∥λ1 − λ2∥ 3/2 2 V 2 log(V ) log2 n √n ∀(λ1, λ2) ∈ Λn, ∀θ ∈ Θ, 1 n n∑ j=1 I {∣ ∣rj(Zj, θ) − A⊤ j λ ∣ ∣ ≤ Γ } ≤ Γ √νmax + Γ 1/2V log(V ) log2 n √n , ∀(λ, θ, Γ) ∈ Tn ∥z∥1 ≤ nCµ + 2n √νmin , ∥z∥∞ ≤ log n } , e-companion to Debiasing In-Sample Policy Performance ec27 For clarity, we stress that φmin > 0 and λmax > 0 are dimension independent constants. We show in the next section that P (Z ̸∈ En) = ˜O(1/n). Thus, the event {Z ∈ En} happens with high-probability, and we will perform our subsequent probabilistic analysis conditional on this “good” set. D.3. Bounding the “Bad” Set The purpose of this section is to bound P (Z /∈ En). Since, En consists of four conditions, we treat each separately. The last two conditions on ∥Z∥1 and ∥Z∥∞ can be analyzed using standard techniques for sub-Gaussian random variables. Lemma D.3 (Bounding ∥Z∥1) Under Assumption 3.1, P (∥Z∥1 > nCµ + 2n √νmin ) ≤ e−n/32. Proof of Lemma D.3: Note that E [|Zj − µj|] ≤ 1 √νmin by Jensen’s inequality. Furthermore, because each Zj is sub-Gaussian with variance proxy 1 νmin , we have by Lemma A.1 of GR 2021 that ∥ |Zj − µj| ∥Ψ = ∥Zj − µj∥Ψ ≤ 2 √νmin . Thus, |Zj − µj| − E [|Zj − µj|] is a mean-zero sub-Gaussian random variable with variance proxy at most 16 νmin . Finally, observe ∥Z∥1 ≤ nCµ + ∑n j=1 |Zj − µj|. Hence, P (∥Z∥1 > nCµ + 2n √νmin ) ≤ P ( 1 n n∑ j=1 |Zj − µj| > 2 √νmin ) ≤ P ( 1 n n∑ j=1 |Zj − µj| − E [|Zj − µj|] > 1 √νmin ) ≤ e − n 32 , by the usual bound on the sum of independent sub-Gaussian random variables. □ Lemma D.4 (Bounding ∥Z∥∞) Under Assumption 3.1, there exists a dimension independent constant n0 such that for all n ≥ n0, P (∥Z∥∞ > log n) ≤ 1 n2 . Proof of Lemma D.4: By Wainwright (2019), E [∥Z∥∞∥] ≤ C1√log n, for some dimension inde- pendent constant C1. Moreover, by (Wainwright 2019, Example 2.29), ∥Z∥∞ − E [∥Z∥∞] is sub- Gaussian with variance proxy at most 1/νmin. Hence, P (∥Z∥∞ > log n) ≤ P (∥Z∥∞ − E [∥Z∥∞] > log n − C1√log n) . ec28 e-companion to Debiasing In-Sample Policy Performance For n suﬃciently large, log n−C1√log n ≥ 1 2 log n. Hence, for n suﬃciently large, this last probability is at most P (∥Z∥∞ − E [∥Z∥∞] > 1 2 log n) ≤ exp (− νmin log2 n 8 ) ≤ n− νmin log n 8 . For n suﬃciently large, the exponent is at most −2, proving the lemma. □ We next establish that the inequality bounding the behavior over Tn hold with high probability. As a preparation, we ﬁrst bound the supremum of a particular stochastic process over this set. Lemma D.5 (Suprema over Tn) Recall the deﬁnition of Tn in Eq. (D.2d). Under Assump- tions 3.1, 3.6 and 4.6, there exist dimension independent constants C and n0 such for all n ≥ n0, we have that for any R > 1, with probability at least 1 − e−R, sup (λ,θ,Γ)∈Tn ∣ ∣ ∣ ∣ ∣ n∑ j=1 I {∣ ∣rj(Zj, θ) − A⊤ j λ ∣ ∣ ≤ Γ } − P (∣ ∣rj(Zj, θ) − A⊤ j λ∣ ∣ ≤ Γ) ∣ ∣ ∣ ∣ ∣ Γ−1/2 ≤ CRV log V √n. Proof of Lemma D.5: Our goal will be to apply Theorem A.3. As a ﬁrst step, we claim that for a ﬁxed λ, θ, Γ, there exists a dimension independent constant C1 such that P (∣ ∣rj(Zj, θ) − A⊤ j λ ∣ ∣ ≤ Γ) ≤ √ νjC1Γ. To prove the claim, notice that this quantity is the probability that a Gaussian random variables lives in an interval of length 2Γ. Upper bounding the density of the Gaussian by its value at its mean shows the probability is at most √ νj 2π . Thus, P (∣ ∣rj(Zj, θ) − A⊤ j λ∣ ∣ ≤ Γ ) ≤ 2Γ√ νj 2π ≤ Γ √νj. (D.3) This upperbound further implies that there exists a dimension independent constant C2 such that the parameter “σ2” in Theorem A.3 is at most C2, because the indicator squared equals the indicator. We also take the parameter “U ” to be √n since Γ ≥ 1 n . Thus, to apply the theorem it remains to show the set F ≡ {(I {∣ ∣rj(Zj, θ) − A⊤ j λ ∣ ∣ ≤ Γ})n j=1 : (λ, θ, Γ) ∈ Tn} is Euclidean and compute its parameters. Consider the set F1 ≡ {(I {rj(Zj, θ) − A⊤ j λ ≤ Γ })n j=1 : (λ, θ, Γ) ∈ Tn} . e-companion to Debiasing In-Sample Policy Performance ec29 By Assumption 4.6, this set has VC-dimension at most V , and, hence, also has pseudo-dimension at most V . The same is true of the set F2 ≡ {(I {A⊤ j λ − rj(Zj, θ) ≤ Γ })n j=1 : (λ, θ, Γ) ∈ Tn} . Since F = F1 ∧ F2, by (Pollard 1990, Lemma 5.1) there exists an absolute constant C2 such that F has pseudo-dimension at most C2V . By Theorem A.3 of GR 2021, F is Euclidean with parameters A = (C2V )6C2V and W = 4C2V . The relevant complexity parameter “V (A, W )” is then at most 6C2V log(C2V ) + 4C2V √6C2V log(C2V ) ≤ C3√V log V , for some dimension independent constant C3. Theorem A.3 now bounds the suprema by C4RV log(V )√n, completing the proof. □ Equipped with Lemma D.5, we can now show the relevant condition holds with high-probability. Lemma D.6 (Bounding Away from Degeneracy) Recall the deﬁnition of Tn in Eq. (D.2d). Under Assumptions 3.1, 3.6 and 4.6 there exists a dimension independent constant n0 such that for all n ≥ n0, with probability at least 1 − 1/n we have that 1 n n∑ j=1 I {∣ ∣rj(Zj, θ) − A⊤ j λ∣ ∣ ≤ Γ } ≤ Γ √νmax + Γ 1/2V log(V ) log2 n √n , ∀(λ, θ, Γ) ∈ Tn. Proof of Lemma D.6 Apply Lemma D.5 with R = log n to conclude that with probability at least 1 − 1/n, for all (λ, θ, Γ) ∈ Tn simultaneously, we have ∣ ∣ ∣ ∣ ∣ n∑ j=1 I {∣ ∣rj(Zj, θ) − A⊤ j λ ∣ ∣ ≤ Γ} − P (∣ ∣rj(Zj, θ) − A⊤ j λ∣ ∣ ≤ Γ )∣ ∣ ∣ ∣ ∣ ≤ CΓ 1/2V log(V ) log(n) √n. Then observe that as in the proof of Eq. (D.3), P (∣ ∣rj(Zj, θ) − A⊤ j λ∣ ∣ ≤ Γ ) ≤ Γ√νmax. Finally, for n suﬃciently large, C ≤ log n. Rearranging then completes the proof. □ Remark D.7 We describe the condition in Lemma D.6 as “Bounding Away from Degeneracy” because rj(Zj, θ) − A⊤ j λ is the reduced cost of the jth component at the dual solution λ. Hence, the lemma asserts that there are not too many reduced costs that are less than 1/n. It remains to establish that the approximate strong convexity condition over Λn holds with high probability. As preparation, we again bound the suprema of a particular stochastic process. Lemma D.8 (Suprema over Λn) Under Assumptions 3.1, 3.6 and 4.6, there exists a dimension independent constant C such that for any R > 1, with probability at least 1 − e −R, we have sup (λ1,λ2)∈Λn,θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 (I {rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ } − P (rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ ) ) ∣ ∣A⊤ j (λ1 − λ2) ∣ ∣ ∥λ1 − λ2∥ 3/2 2 ∣ ∣ ∣ ∣ ∣ ≤ CRV 2 log(V )√n. ec30 e-companion to Debiasing In-Sample Policy Performance Proof of Lemma D.8: Our strategy will be to apply Theorem A.3. To this end, we ﬁrst claim that there exists a dimension independent constant φmax such that for any ﬁxed (λ1, λ2) ∈ Λn and θ ∈ Θ P (rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ ) ≤ φmax∥λ1 − λ2∥1. To prove the claim, notice that this is the probability that a Gaussian random variable lives in an interval of length at most ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ≤ CA∥λ1 − λ2∥1. Upper bounding the Gaussian density by the square root of its precision proves the claim. We next argue that this claim implies that there exists a dimension independent constant C1 such that the parameter “σ2” in Theorem A.3 is at most C1. Indeed, an indicator squared is still the same indicator. Scaling by ∣ ∣A⊤ j (λ1 − λ2)∣ ∣2 ∥λ1 − λ2∥ 3 2 ≤ C 2 Am ∥λ1 − λ2∥2 , and then averaging over j proves that σ2 is at most C1m. We can take the parameter “U ” to be CA√n since ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ∥λ1 − λ2∥3/2 2 ≤ CA√ m∥λ1 − λ2∥−1/2 2 ≤ CA√mn, because (λ1, λ2) ∈ Λn. Thus, to apply Theorem A.3 we need only show that the set F ≡    ( I {rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩} ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ∥λ1 − λ2∥3/2 2 )n j=1 : (λ1, λ2) ∈ Λn, θ ∈ Θ    is Euclidean and determine its parameters. To this end, ﬁrst consider the sets F1 ≡ {(I {rj(Zj, θ) ≥ A⊤ j λ1})n j=1 : λ1 ∈ Rm + , θ ∈ Θ } F2 ≡ {(I {rj(Zj, θ) ≤ A⊤ j λ2})n j=1 : λ2 ∈ Rm + , θ ∈ Θ } F3 ≡ {(I {rj(Zj, θ) ≥ A⊤ j λ2})n j=1 : λ2 ∈ Rm + , θ ∈ Θ} F4 ≡ {(I {rj(Zj, θ) ≤ A⊤ j λ1})n j=1 : λ1 ∈ Rm + , θ ∈ Θ} By Assumption 4.6, each of these sets has VC-dimension at most V (they are indicator sets for functions with pseudo-dimension at most V ). Now deﬁne the set F5 ≡ {(I {rj(Zj, θ) ∈ ⟨A⊤ j λ1, λ2⟩})n j=1 : (λ1, λ2) ∈ Λn} , and notice that F5 ⊆ (F1 ∧ F2) ∨ (F3 ∧ F4) . e-companion to Debiasing In-Sample Policy Performance ec31 Hence, by (Pollard 1990, Lemma 5.1), there exists an absolute constant C2 > 1 such that F5 has pseudodimension at most C2V . By Theorem A.3 of GR 2021, F5 is thus Euclidean with parameters A = (C2V )6C2V and W = 4C2V . Now consider the set F6 =    ( A⊤ j (λ1 − λ2) ∥λ1 − λ2∥3/2 2 )n j=1 : (λ1, λ1) ∈ Λn    , and notice F6 ⊆ {(A⊤ j λ )n j=1 : λ ∈ Rm}. This latter set belongs to a vector space of dimension at most m, and hence has pseudo-dimension at most m ≤ V . Thus, by Theorem A.3 of GR 2021, it is Euclidean with parameters at most A = V 6V and W = 4V . To conclude, notice that F is the pointwise product of F5 and F6. Hence, by (Pollard 1990, Lemma 5.3), we have that F is Euclidean with parameters A = (C3V )C3V · C C3V 3 and W = C3V for some absolute constant C3. In particular, the relevant complexity parameter “V (A, W )” for F is at most C4√ V log(V ) for some dimension independent parameter C4. Applying Theorem A.3 now shows that suprema of the lemma is at most C5R(√V log V )2m√mn, for some dimension independent C5. Since m ≤ V , this completes the lemma. □ Equipped with Lemma D.8, we can prove the approximate strong convexity condition holds with high probability. Lemma D.9 (Approximate Strong Convexity with High Probability) Under Assump- tions 3.1, 3.6 and 4.6, there exists a dimension independent constant n0 such that for all n ≥ n0, we have with probability at least 1 − 1 n that the following inequality holds simultaneously for all (λ1, λ2) ∈ Λn and θ ∈ Θ: (∇λL(λ1, z, θ) − ∇λL(λ2, z, θ))⊤(λ1 − λ2) ≥ φminβ∥λ1 − λ2∥2 2 − ∥λ1 − λ2∥ 3/2 2 V 2 log(V ) log2 n √n . Proof of Lemma D.9: By choosing R = log n, Lemma D.8 shows that there exists a dimension independent constant C1 with probability at least 1 − 1/n sup (λ1,λ2)∈Λn,θ∈Θ 1 n n∑ j=1 (I {rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ } − P (rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ ) ) ∣ ∣A⊤ j (λ1 − λ2) ∣ ∣ ∥λ1 − λ2∥3/2 ≥ −CV 2 log(V )√n log n. ec32 e-companion to Debiasing In-Sample Policy Performance This inequality implies that for any (λ1, λ2) ∈ Λn and θ ∈ Θ, 1 n n∑ j=1 I {rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ } ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ≥ 1 n n∑ j=1 P ( rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩) ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ − C∥λ1 − λ2∥3/2V 2 log(V ) log n √n . Thus our ﬁrst goal will be to bound the summation on the right side. Isolate the jth term. The probability P (rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩) is the probability that a Gaussian random variable lives in an interval of length at most ∣ ∣A⊤ j (λ1 − λ2)∣ ∣. Moreover, the endpoints of this interval are most ∣ ∣A⊤ j λi∣ ∣ ≤ CAλmax for i = 1, 2, since (λ1, λ2) ∈ Λn. It follows that these endpoints are no further than aj(θ)µj + bj(θ) + CAλmax from the mean of the relevant Gaussian. Thus, we can lower bound the density of the Gaussian on this interval. This reasoning proves P (rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ ) ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ≥ (A⊤ j (λ1 − λ2) )2 · √νj aj(θ)√2π exp (− νj(aj(θ)µj + bj(θ) + CAλmax)2 2aj(θ)2 ) ≥ φmin ( A⊤ j (λ1 − λ2))2 . Averaging over j shows 1 n n∑ j=1 P (rj(Zj, θ) ∈ ⟨A⊤ j λ1, A⊤ j λ2⟩ ) ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ≥ φmin(λ1 − λ2)⊤ 1 n n∑ j=1 AjA⊤ j (λ1 − λ2) ≥ φminβ∥λ1 − λ2∥ 2 2, by Assumption 4.5. Substitute above, and notice if n0 = eC, then log n ≥ C to complete the proof. □ Finally, a simple union bound gives Lemma D.10 (Z ∈ En with High Probability) Under Assumptions 3.1, 3.6 and 4.6 there exists a dimension independent constant n0 such that for all n ≥ n0, P (Z ∈ En) ≥ 1 − 4 n . Proof. Combine Lemmas D.3, D.4, D.6 and D.9 and apply a union bound. D.4. Properties of the Good Set In this section, we argue that for data realizations z ∈ En, our optimization problems satisfy a number of properties, and, in particular, the dual solutions and VGC satisfy a bounded diﬀerences condition. We start by showing that small perturbations to the data z still yield dual solutions that are bounded. Note, any z ∈ En satisﬁes the assumptions of the next lemma. e-companion to Debiasing In-Sample Policy Performance ec33 Lemma D.11 (Bounded Duals) Suppose Assumptions 3.1 and 3.6 hold and ∥t∥∞ ≤ 3 √n √νmin and z satisﬁes ∥z∥1 ≤ nCµ + 2n √νmin . Then, for all j = 1, . . . , n, sup θ∈Θ ∥λ(z + tjej, θ)∥1 ≤ λmax. Proof of Lemma D.11: Write ∥λ(z + tjej, θ)∥1 ≤ 2 ns0 ∥r(z + tjej, θ)∥1 (Lemma D.2) ≤ 2 ns0 (amax∥z∥1 + amax |tj| + bmaxn) (Deﬁnition of r(·, θ)) ≤ 2 s0 (amax (Cµ + 2 √νmin + 3 √nνmin ) + bmax ) (by assumptions on ∥z∥1 and ∥t∥∞) ≤ λmax, since 3/ √n ≤ √3 ≤ 2. Taking the supremum of both sides over θ ∈ Θ completes the proof. □ We next establish a bounded diﬀerences condition for the dual solution λ(z, θ). Lemma D.12 (Bounded Diﬀerences for the Dual) Suppose Assumptions 3.1 and 3.6 hold and that z ∈ En and ∥λ(z, θ)∥ ≤ λmax. Then, there exists a dimension independent constant C such that ∥λ(z, θ) − λ( ¯z, θ)∥2 ≤ CV 3 log2(V ) log4 n n n∑ j=1 I {zj ̸= ¯zj} . Proof of Lemma D.12: To declutter the notation, deﬁne f1(λ) ≡ L(λ, z, θ), λ1 ≡ λ(z, θ), f2(λ) ≡ L(λ, z, θ), λ2 ≡ λ(z, θ). Furthermore, let Ij = ⟨A⊤ j λ1, A⊤ j λ2⟩. Notice if ∥λ1 − λ2∥2 ≤ 4/n, the inequality is immediate for C = 4 since m ≥ 1. Hence, we assume throughout that ∥λ1 − λ2∥2 > 4/n. Using Lemma D.1 we have that f1(λ2) − f1(λ1) = ∇f1(λ1)⊤(λ2 − λ1) + 1 n n∑ j=1 I {rj(zj) ∈ Ij} ∣ ∣rj(zj) − A⊤ j λ2∣ ∣ ≥ 1 n n∑ j=1 I {rj(zj) ∈ Ij} ∣ ∣rj(zj) − A⊤ j λ2∣ ∣ , (D.4) where the inequality uses f1(·) is convex and λ1 is an optimizer. Analogously, we have that f2(λ1) − f2(λ2) ≥ 1 n n∑ j=1 I {rj(¯zj) ∈ Ij} ∣ ∣rj(¯zj) − A⊤ j λ1∣ ∣ . (D.5) ec34 e-companion to Debiasing In-Sample Policy Performance Adding Eqs. (D.4) and (D.5) yields f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) ≥ 1 n n∑ j=1 (I {rj(¯zj) ∈ Ij} ∣ ∣rj(¯zj) − A⊤ j λ1∣ ∣ + I {rj(zj) ∈ Ij} ∣ ∣rj(zj) − A⊤ j λ2∣ ∣) Isolate the jth term on the right. To lower bound this term, note that when zj ̸= ¯zj, ∣ ∣ ∣I {rj(¯zj) ∈ Ij} ∣ ∣rj(¯zj) − A⊤ j λ1∣ ∣ − I {rj(zj) ∈ Ij} ∣ ∣rj(zj) − A⊤ j λ1∣ ∣ ∣ ∣ ∣ ≤ I {rj(¯zj) ∈ Ij} ∣ ∣rj(¯zj) − A⊤ j λ1∣ ∣ + I {rj(zj) ∈ Ij} ∣ ∣rj(zj) − A⊤ j λ1∣ ∣ (a) ≤ 2 ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ ≤ 2CA∥λ1 − λ2∥2, where inequality (a) follows because each indicator is non-zero only when the corresponding r is in the interval ⟨A⊤ j λ1, A⊤ j λ2⟩. Hence, substituting above and rearranging yields f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) + 2CA∥λ1 − λ2∥2 · 1 n n∑ j=1 I {zj ̸= ¯zj} ≥ 1 n n∑ j=1 I {rj(zj) ∈ Ij} (∣ ∣rj(zj) − A⊤ j λ2∣ ∣ + ∣ ∣rj(zj) − A⊤ j λ1∣ ∣) = 1 n n∑ j=1 I {rj(zj) ∈ Ij} ∣ ∣A⊤ j (λ1 − λ2)∣ ∣ , (D.6) = (∇λL(λ1, z, θ) − ∇λL(λ2, z, θ))⊤(λ1 − λ2) ≥ φminβ∥λ1 − λ2∥ 2 2 − V 2 log(V ) · log2 n √n · ∥λ1 − λ2∥ 3/2 2 , (since z ∈ En, (λ1, λ2) ∈ Λn)) where Eq. (D.6) follows because when the indicator is non-zero, rj(zj) is between A⊤ j λ1 and A⊤ j λ2. To summarize the argument so far, we have shown that f1(λ2)−f1(λ1) + f2(λ1) − f2(λ2) + 2CA∥λ1 − λ2∥2 1 n n∑ j=1 I {zj ̸= ¯zj} (D.7) ≥ φminβ∥λ1 − λ2∥2 2 − V 2 log(V ) log2 n √n ∥λ1 − λ2∥ 3/2 2 . The next step of the proof upper bounds the left side. By deﬁnition of f1(·), f2(·), f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) = 1 n n∑ j=1 ([ rj(zj) − A⊤ j λ2]+ − [ rj(zj) − A⊤ j λ1]+ + [ rj(¯zj) − A⊤ j λ1]+ − [ rj(¯zj) − A⊤ j λ2]+) . e-companion to Debiasing In-Sample Policy Performance ec35 The jth term is non-zero only if zj ̸= ¯zj. In that case, [ rj(zj) − A⊤ j λ2]+ − [ rj(zj) − A⊤ j λ1]+ + [ rj(¯zj) − A⊤ j λ1]+ − [ rj(¯zj) − A⊤ j λ2]+ ≤ 2 ∣ ∣A⊤ j (λ2 − λ1)∣ ∣ ≤ 2CA∥λ1 − λ1∥2 ≤ 2CA∥λ1 − λ1∥2. Summing over j for which zj ̸= ¯zj and substituting into the left side of Eq. (D.7) yields, 4CA∥λ1 − λ1∥2 · 1 n n∑ j=1 I {zj ̸= ¯zj} ≥ φminβ∥λ1 − λ2∥2 2 − V 2 log(V ) log2 n √n ∥λ1 − λ2∥3/2 2 . (D.8) To simplify this expression, recall that by assumption ∥λ1 − λ2∥2 ≥ 4 n =⇒ √n∥λ1 − λ2∥ 1/2 2 ≥ 1. Hence we can inﬂate the left side of Eq. (D.8) by multiplying by √n∥λ1 − λ2∥1/2 2 and then rear- ranging to obtain φminβ∥λ1 − λ2∥2 2 ≤ 4CA∥λ1 − λ2∥3/2 · 1 √ n n∑ j=1 I {zj ̸= ¯zj} + V 2 log(V ) log2 n √n ∥λ1 − λ2∥ 3/2 2 ≤ C1V 2 log(V ) log2 n √n n∑ j=1 I {zj ̸= ¯zj} ∥λ1 − λ2∥3/2 2 , for some dimension independent constant C1. Dividing both sides by ∥λ1 − λ2∥ 3/2 2 and combining constants yields ∥λ1 − λ2∥ 1/2 2 ≤ C2V 2 log(V ) log2 n √n n∑ j=1 I {zj ̸= ¯zj} , (D.9) for some dimension independent constant C2. Multiply Eq. (D.9) by V log(V ) log2 n√n ∥λ1 − λ2∥2 to see that V log(V ) log2 n √n ∥λ1 − λ2∥3/2 2 ≤ C2V 3 log2(V ) log4 n n n∑ j=1 I {zj ̸= ¯zj} ∥λ1 − λ2∥2. Substitute this upper-bound to Eq. (D.8), yielding 4CA∥λ1 − λ1∥2 · 1 n n∑ j=1 I {zj ̸= ¯zj} ≥ φminβ∥λ1 − λ2∥2 2 − C2V 3 log2(V ) log4 n n n∑ j=1 I {zj ̸= ¯zj} ∥λ1 − λ2∥2. Now divide by ∥λ1 − λ2∥2 and rearrange to complete the proof. □ We now use this result to show the VGC is also Lipschitz in the Hamming distance. The key to the following proof is that that strong-duality shows V (z, θ) = nL(λ(z, θ), z, θ). ec36 e-companion to Debiasing In-Sample Policy Performance Lemma D.13 (Bounded Diﬀerences for VGC) Let z, z ∈ En. Suppose Assumptions 3.1 and 3.6 hold. Then, there exists a dimension independent constant C, such that for any n such that log n nh ≤ 1, we have that |D(z, θ) − D(z, θ)| ≤ C h V 3 log2(V ) log4(n) n∑ i=1 I {zi ̸= zi} Proof of Lemma D.13: Notice if z = ¯z the lemma is trivially true. Hence, throughout, we assume z ̸= ¯z. Since θ is ﬁxed throughout, we also drop it from the notation. As a ﬁrst step, we will prove the two inequalities V (z + δjej) − V (z) ≥ (rj(z) + ajδj − A⊤ j λ(z + δjej))+ − (rj(zj) − A⊤ j λ(z + δjej))+ , (D.10a) V (z + δjej) − V (z) ≤ (rj(z) + ajδj − A⊤ j λ(z))+ − (rj(zj) − A⊤ j λ(z))+ . (D.10b) To prove the ﬁrst inequality, write V (z + δjej) − V (z) = nL (λ (z + δjej) , z + δjej) − nL (λ (z) , z) (D.11) ≤ nL (λ (z) , z + δjej) − nL (λ (z) , z) = (rj(zj) + ajδj − A⊤ j λ(z))+ − (rj(zj) − A⊤ j λ(z))+ where the inequality holds by the sub-optimality of λ(z) for L (λ, z + δjej), and the last equality holds since all terms except the jth in the summation of the Lagrangian cancel out. A similar argument using the sub-optimality of λ(z + δjej) for L(λ, z + δjej) proves the lower bound. The next step of the proof establishes that there exists a dimension independent constant C1 such that E [V (z + δjej) − V (z) − (V (z + δjej) − V (z))] ≤ C1 ( V 3 log2(V ) log4 n n n∑ i=1 I {zi ̸= zi} ) I {zj = zj} + C1√h I {zj ̸= zj} (D.12) As suggested by the bound, we will consider two cases depending on whether zj = ¯zj. Case 1: zj ̸= ¯zj. Notice the inequalities Eq. (D.10) apply as well when z is replaced by ¯z. Hence, applying the upper bound for the ﬁrst term and the lower bound for the second term shows V (z + δjej) − V (z) − (V (z + δjej) − V (z)) ≤ (( rj(zj) + ajδj − A⊤ j λ(z) )+ − ( rj(zj) − A⊤ j λ(z) )+) − (( rj(¯zj) + ajδj − A⊤ j λ(z + δjej) )+ − ( rj(¯zj) − A⊤ j λ(z + δjej))+) ≤ 2 |aj| |δj| , e-companion to Debiasing In-Sample Policy Performance ec37 because t ↦→ t + is a 1-Lipschitz function. Take expectations of both sides, using Jensen’s inequality and upper bounding the variance of δj shows E [V (z + δjej) − V (z) − (V (z + δjej) − V (z))] ≤ 2amaxE [|δj|] ≤ 2amax √3h ν1/4 min . Collecting constants proves the inequality when zj ̸= ¯zj. Case 2: zj = ¯zj. Proceeding as in Case 1, V (z + δjej) − V (z) − (V (z + δjej) − V (z)) ≤ (rj(zj) + ajδj − A⊤ j λ(z))+ − (rj(zj) − A⊤ j λ(z))+ − ((rj(¯zj) + ajδj − A⊤ j λ(z + δjej))+ − (rj(¯zj) − A⊤ j λ(z + δjej)) )+) ≤ 2 ∣ ∣A⊤ j (λ(z) − λ( ¯z + δjej)) ∣ ∣ ≤ 2CA∥λ(z) − λ( ¯z)∥ + 2CA∥λ(z) − λ(z + δjej)∥, where the second inequality follows again because t ↦→ t+ is a contraction, but we group the terms in a diﬀerent order, and the last inequality follows from the triangle-inequality and the Cauchy- Schwarz inequality. We can bound the ﬁrst term by invoking Lemma D.12 yielding V (z + δjej) − V (z) − (V (z + δjej) − V (z)) (D.13) ≤ C2V 3 log2(V ) log4 n n n∑ i=1 I {zi ̸= ¯zi} + 2CA∥λ(z) − λ(z + δjej)∥, for some dimension independent constant C2. Taking expectations shows that to prove a bound, it will suﬃce to bound E [∥λ(z) − λ(z + δjej)∥]. To this end, consider splitting the expectation based on whether |δj| ≥ 3 √ n νmin . If |δj| ≤ 3√ n νmin , then by Lemma D.11, ∥λ(z + δjej)∥1 ≤ λmax. Hence we can invoke Lemma D.12 again yielding E [ ∥λ(z) − λ(z + δjej)∥I { |δj| ≤ 3 √ n νmin }] ≤ C3V 3 log2(V ) log4 n n P (|δj| ≤ 3√ n νmin ) ≤ C3V 3 log2(V ) log4 n n . Next, assume |δj| ≥ 3√ n νmin . Write ∥λ(z) − λ(z + δjej)∥ ≤ λmax + 2 ns0 ∥r(z + δjej)∥1 (by Lemma D.2) ≤ λmax + 2 ns0 ∥r(z)∥1 + 2 |aj| ns0 ∥ |δj| (by def. of r(·)) ≤ λmax + C4 + 2amax ns0 |δj| , ec38 e-companion to Debiasing In-Sample Policy Performance for some dimension independent constant C4, because z ∈ E implies that ∥r(z)∥/n is bounded by a (dimension-independent) constant. Thus, for some dimension independent constant C5 we have E [ ∥λ(z) − λ(z + δjej)∥I { |δj| > 3√ n νmin }] ≤ E [(C5 + C5 n |δj| ) I { |δj| > 3√ n νmin }] ≤ E [(C5 + C5 n ) |δj| I { |δj| > 3√ n νmin }] = E [ 2C5 |δj| I { |δj| > 3 √ n νmin }] , where the ﬁnal inequality uses 3 √ n νmin ≥ 1 since n ≥ 3. Integration by parts with the Gaussian density shows there exists a dimension independent constant C6 such that E [ |δj| I { |δj| > 3√ n νmin }] ≤ C6√he−nh/√νmin ≤ C6√he−nh ≤ C6 n , because nh > log n and h < 1 by assumption. Combining the two cases shows that E [∥λ(z) − λ(z + δjej)∥] ≤ C7V 3 log2(V ) log4 n n for some dimension independent constant C7. Taking the expectation of Eq. (D.13), substituting this bound and collecting constants proves V (z + δjej) − V (z) − (V (z + δjej) − V (z)) ≤ C8V 3 log2(V ) log4 n n n∑ j=1 I {zj ̸= ¯zj} for some constant C8. Combining with Case 1 establishes Eq. (D.12). Now, by symmetry, Eq. (D.12) holds with the roles of z and z reversed. Hence, Eq. (D.12) also holds after taking the absolute values of both sides. We can now write, |D(z) − D(z)| ≤ n∑ j=1 1 h √νj |aj| ∣ ∣E [V (z + δjej) − V (z) − (V (z + δjej) − V (z))]∣ ∣ ≤ C1 h √νminamin V 3 log2(V ) log4 n n n∑ i=1 I {zi ̸= zi} n∑ j=1 I {zj = zj} + C1√hνminamin n∑ j=1 I {zj ̸= zj} ≤ C9 h V 3 log2(V ) log4(n) n∑ i=1 I {zi ̸= zi} , for some constant C9. This completes the proof. □ Finally, we show that θ ↦→ λ(z, θ) is also smooth on En, at least locally. e-companion to Debiasing In-Sample Policy Performance ec39 Lemma D.14 (Local Smoothness of Dual Solution in θ) Suppose z ∈ En and that Assump- tions 3.1, 3.6 and 4.6 hold. Then, there exist dimension independent constants C and n0 such that for any n ≥ n0 and any ¯θ such that ∥ ¯θ − θ∥ ≤ 1 n , we have that ∥ ∥λ(z, θ) − λ(z, ¯θ)∥ ∥ 2 ≤ CV 2 log V log5/4 n √n Proof of Lemma D.14: The proof is similar to that of Lemma D.12. To declutter the notation, deﬁne f1(λ) ≡ L(λ, z, θ), λ1 ≡ λ(z, θ) f2(λ) ≡ L(λ, z, ¯θ), λ2 ≡ λ(z, ¯θ) Furthermore, let Ij = 〈A⊤ j λ1, A⊤ j λ2〉. If ∥λ1 − λ2∥2 ≤ 4V 2 log V log n√ n , then the lemma holds trivially for C = 4. Hence, for the remainder, we assume ∥λ1 − λ2∥2 > 4V 2 log V log n√n . In particular, by Lemma D.11, this implies (λ1, λ2) ∈ Λn. Using Lemma D.1, we have that f1(λ2) − f1(λ1) = ∇f1(λ1)⊤(λ2 − λ1) + 1 n n∑ j=1 I {rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ ≥ 1 n n∑ j=1 I {rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ where f1(·) is convex and λ1 is an optimizer. Similarly, we have that f2(λ1) − f2(λ2) ≥ 1 n n∑ j=1 I {rj(zj, ¯θ) ∈ Ij} ∣ ∣rj(zj, ¯θ) − A⊤ j λ1∣ ∣ . Adding yields f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) (D.14) ≥ 1 n n∑ j=1 I {rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ + I { rj(zj, ¯θ) ∈ Ij} ∣ ∣rj(zj, ¯θ) − A⊤ j λ1∣ ∣ . We would like to combine the jth summand to simplify. To this end, adding and subtracting I{rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ yields, I {rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ + I {rj(zj, ¯θ) ∈ Ij} ∣ ∣rj(zj, ¯θ) − A⊤ j λ1∣ ∣ = I {rj(zj, θ) ∈ Ij} ( ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ + ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ) (D.15a) + I {rj(zj, ¯θ) ∈ Ij} ∣ ∣rj(zj, ¯θ) − A⊤ j λ1∣ ∣ − I {rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ (D.15b) We simplify Eq. (D.15a) by noting that when the indicator is non-zero, ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ + ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ = ∣ ∣A⊤ j (λ2 − λ1)∣ ∣ ec40 e-companion to Debiasing In-Sample Policy Performance Hence, Eq. (D.15a) = I {rj(zj, θ) ∈ Ij} ∣ ∣A⊤ j (λ2 − λ1)∣ ∣ . We rewrite Eq. (D.15b) as Eq. (D.15b) = I {rj(zj, ¯θ) ∈ Ij} ( ∣ ∣rj(zj, ¯θ) − A⊤ j λ1∣ ∣ − ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ) + ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ (I {rj(zj, ¯θ) ∈ Ij} − I {rj(zj, θ) ∈ Ij} ) (a) ≥ −I { rj(zj, ¯θ) ∈ Ij} ∣ ∣rj(zj, ¯θ) − rj(zj, θ)∣ ∣ + ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ (I {rj(zj, ¯θ) ∈ Ij} − I {rj(zj, θ) ∈ Ij} ), (b) ≥ − ∣ ∣rj(zj, ¯θ) − rj(zj, θ)∣ ∣ − ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ I {rj(zj, ¯θ) ̸∈ Ij, rj(zj, θ) ∈ Ij} , (c) ≥ −L∥θ − ¯θ∥2(|zj| + 1) − ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ I {rj(zj, ¯θ) ̸∈ Ij, rj(zj, θ) ∈ Ij} , (d) ≥ − L n (|zj| + 1) − ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ I { rj(zj, ¯θ) ̸∈ Ij, rj(zj, θ) ∈ Ij} , where inequality (a) is the triangle inequality, inequality (b) rounds the indicators, inequality (c) follows from the Lipschitz assumptions on aj(θ) and bj(θ), and inequality (d) uses ∥θ − ¯θ∥ ≤ 1 n . Finally note that when the last indicator is non-zero, ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ≤ ∣ ∣A⊤ j (λ2 − λ1)∣ ∣ ≤ 2CA√mλmax. Substituting this bound above and the resulting lower bound on Eq. (D.15b) into Eq. (D.15) proves I {rj(zj, θ) ∈ Ij} ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ + I {rj(zj, ¯θ) ∈ Ij} ∣ ∣rj(zj, ¯θ) − A⊤ j λ1∣ ∣ (D.16) ≥ I {rj(zj, θ) ∈ Ij} ∣ ∣A⊤ j (λ2 − λ1)∣ ∣ − L n (|zj| + 1) − 2CA√mλmaxI { rj(zj, ¯θ) ̸∈ Ij, rj(zj, θ) ∈ Ij} . We can further clean up the last indicator by noting that I {rj(zj, ¯θ) ̸∈ Ij, rj(zj, θ) ∈ Ij} =⇒ Either ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ≤ ∣ ∣rj(z, θ) − rj(z, ¯θ)∣ ∣ or ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ ≤ ∣ ∣rj(z, θ) − rj(z, ¯θ)∣ ∣ . Moreover, because z ∈ E, we can use the Lipschitz assumptions on aj(θ) and bj(θ) and the fact that ∥θ − ¯θ∥ ≤ 1 n to write |rj(z, θ) − rj(z, θ)| ≤ 2L∥θ − ¯θ∥ log n ≤ 2L log n n . Thus, I {rj(zj, ¯θ) /∈ Ij, rj(zj, θ) ∈ Ij} ≤ I { ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ≤ 2L log n n } + I { ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ ≤ 2L log n n } . e-companion to Debiasing In-Sample Policy Performance ec41 Making this substitution into Eq. (D.16), averaging over j, and substituting this bound into Eq. (D.14) shows f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) ≥ 1 n n∑ j=1 I {rj(zj, θ) ∈ Ij} ∣ ∣A⊤ j (λ2 − λ1)∣ ∣ − L n2 n∑ j=1(|zj| + 1) − 2CA√mλmax 1 n n∑ j=1 I { ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ≤ 2L log n n } + I { ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ ≤ 2L log n n } = (∇λL(λ1, z, θ) − ∇λL(λ2, z, θ))⊤ (λ1 − λ2) − L n (∥z∥1/n + 1) − 2CA√mλmax 1 n n∑ j=1 I { ∣ ∣rj(zj, θ) − A⊤ j λ1∣ ∣ ≤ 2L log n n } + I { ∣ ∣rj(zj, θ) − A⊤ j λ2∣ ∣ ≤ 2L log n n } ≥ φminβ∥λ1 − λ2∥2 2 − V 2 log(V ) log2 n √n ∥λ1 − λ2∥3/2 2 − 2L n (Cµ + 2/√νmin) − 8CA√mλmax ( L √νmax log n n + √2LV log(V ) log5/2 n n ) , because 2L log n n ≥ 1 n by Assumption 3.1 and z ∈ En. Using Assumption 3.1 to further simplify, we have thus far shown that for some dimension independent constant C2, f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) ≥ φminβ∥λ1 − λ2∥ 2 2 − V 2 log(V ) log2 n √n ∥λ1 − λ2∥3/2 2 (D.17) − C2V 2 log V log5/2 n n We next proceed to upper bound left side of this inequality. By deﬁnition of f1(·), f2(·), f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) = 1 n n∑ j=1 ([ rj(zj, θ) − A⊤ j λ2]+ − [rj(zj, θ) − A⊤ j λ1]+ + [ rj(zj, ¯θ) − A⊤ j λ1]+ − [ rj(zj, ¯θ) − A⊤ j λ2]+) . Focusing on the jth term, we see [rj(zj, θ) − A⊤ j λ2]+ − [ rj(zj, θ) − A⊤ j λ1]+ + [ rj(zj, ¯θ) − A⊤ j λ1]+ − [rj(zj, ¯θ) − A⊤ j λ2]+ ≤ 2 ∣ ∣rj(zj, θ) − rj(zj, ¯θ)∣ ∣ ≤ 2L∥θ − ¯θ∥2(|zj| + 1), ≤ 2L n (|zj| + 1), where the penultimate inequality uses Assumption 3.6. Averaging over j, we see f1(λ2) − f1(λ1) + f2(λ1) − f2(λ2) ≤ 2L n ( ∥z∥1 n + 1 ) ≤ C3 n ec42 e-companion to Debiasing In-Sample Policy Performance for some constant C3, since z ∈ En. Substitute into Eq. (D.17) to see that C3 n ≥ φminβ∥λ1 − λ2∥ 2 2 − ∥λ1 − λ2∥3/2 2 V 2 log(V ) log2 n √n − C2V 2 log V log5/2 n n Rearranging and collecting constants shows φminβ∥λ1 − λ2∥ 2 2 − ∥λ1 − λ2∥3/2 2 V 2 log(V ) log2 n √n ≤ C4V 2 log V log5/2 n n , for some dimension-independent constant C4. We can also lower bound the left side by recalling ∥λ1 − λ2∥2 > V 2 log V log n √n =⇒ n1/4∥λ1 − λ2∥1/2 2 log1/2(n) V 2 log(V ) > 1. Hence, inﬂating the second term on the left yields φminβ∥λ1 − λ2∥2 2 − log3/2 n n1/4 ∥λ1 − λ2∥ 2 2 ≤ C5V 2 log V log5/2 n n . For n suﬃciently large, the ﬁrst term on the left is at least twice the second. Rearranging and taking square roots shows ∥λ1 − λ2∥2 ≤ C6√V 2 log V log5/4 n √n . Recalling that V ≥ 2 proves the theorem. □ D.5. Pointwise Convergence Results To prove our theorem, we require the uniform convergence of the in-sample optimism to expectation and the uniform convergence of VGC to its expectation. In this section, we ﬁrst establish several pointwise convergence results to assist with this task. Our main workhorse will be Theorem A.5 where En deﬁnes the good set on which our random variables satisfy a bounded diﬀerences condition. As a ﬁrst step, we will show that the dual solutions converge (for a ﬁxed θ) to their expectations. In preparation, we ﬁrst bound the behavior of the dual on the bad set. Lemma D.15 (Dual Solution Conditional Expectation Bound) Suppose Assumptions 3.1 and 3.6 both hold. Let E1,n ≡ {z : ∥z∥1 ≤ nCµ + 2n √νmin } . Then, there exists a dimension independent constant C, such that E [λi(Z, θ)I {Z ∈ E c 1,n}] ≤ C exp (− n C ) e-companion to Debiasing In-Sample Policy Performance ec43 Proof of Lemma D.15: We ﬁrst bound E [∥Z∥1 I {Z ∈ E c 1,n}] = ∫ ∞ 0 P (∥Z∥1 I {Z ∈ E c 1,n} ≥ t ) dt = ∫ nCµ+ 2n√νmin 0 P (∥Z∥1 > nCµ + 2n √νmin ) dt + ∫ ∞ nCµ+ 2n√νmin P (∥Z∥1 ≥ t) dt ≤ (nCµ + 2n √νmin ) e−n/32 + ∫ ∞ nCµ+ 2n√νmin P (∥Z∥1 ≥ t) dt By inspection, there exists a dimension independent constant C1 such that the ﬁrst term is at most C1e−n/C1. To analyze the second term, recall ∥Z∥1 ≤ nCµ + ∑n j=1 |Zj − µj|. Hence, ∫ ∞ nCµ+ 2n√ νmin P (|Z|1 ≥ t) dt = ∫ ∞ nCµ+ 2n√ νmin P ( 1 n n∑ j=1 |Zj − µj| ≥ 1 n t − Cµ ) dt = ∫ ∞ nCµ+ 2n√ νmin P ( 1 n n∑ j=1 |Zj − µj| − E [|Zj − µj|] ≥ 1 n t − Cµ − 1 νmin ) dt, since E [|Zj − µj|] ≤ 1 √νmin by Jensen’s inequality. Now make the change of variable s = t n − Cµ − 1 √ νmin to obtain n ∫ ∞ ν−1/2 min P ( 1 n n∑ j=1 |Zj − µj| − E [|Zj − µj|] ≥ s ) ds ≤ n ∫ ∞ ν−1/2 min e − s2νminn 32 ds because |Zj − µj| − E [|Zj − µj|] is a mean-zero, sub-Gaussian random variable with variance proxy at most 16 νmin . (See Lemma D.3 for clariﬁcation.) Making another change of variables proves this last integral is equal to 4 √νminn ∫ ∞ √n/4 e −t2/2dt, ≤ 16 n√νmin e − n 32 This value is also at most C2e −n/C2 for some constant C2. In summary, we have shown that there exists a dimension independent constant C3 such that E [ ∥Z∥1 I { Z ∈ E c 1,n}] ≤ C3e −n/C3. Now to prove the lemma, recall by Lemma D.2, λi(Z, θ) ≤ ∥λ(Z, θ)∥1 ≤ 2 s0n ∥r(Z, θ)∥1 ≤ 2 s0n (amax ∥Z∥1 + bmaxn) (D.18) where the second inequality holds by Assumption 3.6. Multiplying by I {Z ∈ E c 1,n} and taking expectations hows, E [λi(Z, θ)I {Z ∈ E c 1,n}] ≤ C4 n e−n/C4 + C4P (Z ∈ E c 1,n) ≤ C4 n e−n/C4 + C4e −n/32, by Lemma D.3. Collecting constants proves the lemma. □ ec44 e-companion to Debiasing In-Sample Policy Performance We now use Theorem A.5 to prove that the dual solution concentrates at its expectation for any ﬁxed θ ∈ Θ. Lemma D.16 (Pointwise Convergence Dual Solution) Fix some θ ∈ Θ and i = 1, . . . , m. Under Assumptions 3.1, 3.6 and 4.6 There exists dimension independent constants C and n0, such that for all n ≥ n0eR, the following holds with probability 1 − exp (−R) |λi(Z, θ) − E [λi(Z, θ)]| ≤ CV 3 log2 V log4 n √n √R. Proof of Lemma D.16: The proof will use the dual stability condition (Lemma D.12) to apply Theorem A.5. Since θ is ﬁxed throughout, we drop it from the notation. By triangle inequality, |λi(Z) − E [λi(Z)]| ≤ |λi(Z) − E [ λi(Z)| Z ∈ En]| ︸ ︷︷ ︸ (a) + |E [ λi(Z)| Z ∈ En] − E [λi(Z)]| ︸ ︷︷ ︸ (b) . (D.19) We ﬁrst bound (b) by a term that is O(1/n). We see that E [ λi(Z)| Z ∈ En] − E [λi(Z)] = (E [ λi(Z)| Z ∈ En] − E [ λi(Z)| Z /∈ En] )P {Z /∈ En} ≤ C1 n + E [λi(Z)I {Z /∈ En}] , where we used Lemmas D.10 and D.11 to bound the ﬁrst term and C1 is a dimension independent constant. To bound the second term, deﬁne the set E0 ≡ { z : ∥z∥1 ≤ nCµ + 2n √νmin } . Notice, En ⊆ E0. Then write, E [λi(Z)I {Z /∈ En}] = E [λi(Z) (I {Z ∈ E c 0} + I {Z ∈ E0 ∩ E c n})] ≤ C2 exp (− n C2 ) + λmaxP {Z ∈ E c n} (Lemmas D.11 and D.15) ≤ C3 exp (− n C3 ) + C2 n (Lemma D.10), for dimension independent constants C2 and C3. Collecting terms shows that there exists a dimension independent constant C3 such that for n suﬃciently large, Term (b) of Eq. (D.19) = |E [λi(Z) | Z ∈ En] − E [λi(Z)]| ≤ C3 n . We now bound Term (a) by leveraging Theorem A.5. First note that for any Z, ¯Z ∈ En, we have ∣ ∣λi(Z) − λi( ¯Z)∣ ∣ = √∣ ∣λi(Z) − λi( ¯Z)∣ ∣2 ≤ √ √ √ √ m∑ i=1 ∣ ∣λi(Z) − λi( ¯Z)∣ ∣2 = ∥ ∥λ(Z) − λ( ¯Z) ∥ ∥2 . e-companion to Debiasing In-Sample Policy Performance ec45 Thus, by Lemma D.12, we see that ∣ ∣λi(Z) − λi( ¯Z)∣ ∣ ≤ C4V 3 log2 V · log4(n) n n∑ j=1 I {Zj ̸= ¯Zj} , and, hence, λi(·) satisﬁes the bounded diﬀerences condition on En. By Lemma D.10, P {Z ̸∈ En} ≤ C5 n . By the assumptions, n > 4C5e R =⇒ e−R > 2C5 n . Theorem A.5 then shows that with probability at least 1 − e−R, λi(Z) − E [λi(Z) | Z ∈ En] ≤ C5C4V 3 log2(V ) log4 n n + C4V 3 log2(V ) log4 n √n √ log ( 2 e−R − 2C5/n ) ≤ C6V 3 log2(V ) log4 n √n √ log ( 2 e−R − 2C5/n ) ≤ C6V 3 log2(V ) log4 n √n √ log ( 4 e−R ) ≤ C7V 3 log2(V ) log4 n √n √ R, where the third inequality again uses n > 4C5eR, and the remaining inequalities simply collect constants and dominant terms. To summarize, by substituting the two bounds into the upperbound of |λi(Z) − E [λi(Z)]| in Eq. (D.19), we obtain that with probability at least 1 − e−R |λi(Z) − E [λi(Z)]| ≤ C8V 3 log2(V ) log4 n √n √ R + C8 n . Collecting terms completes the proof. □ Proof of Lemma 4.9: Since θ is ﬁxed, we drop it from the notation. By triangle inequality, |D(Z) − E [D(Z)]| ≤ |D(Z) − E [ D(Z)| Z ∈ En]| + |E [ D(Z)| Z ∈ En] − E [D(Z)]| . (D.20) We bound the latter term ﬁrst. Since D(Z) is bounded by Lemma 3.8, we see |E [ D(Z)| Z ∈ En] − E [D(Z)]| =∣ ∣ ∣E [ D(Z)| Z ∈ En] − E [ D(Z)| Z /∈ En] ∣ ∣ ∣P {Z /∈ En} ≤ C1 n√h for some dimension independent constant C1 by using Lemma D.10. We now bound the ﬁrst term. We use Theorem A.5. Recall from Lemma D.13 that ∣ ∣D(Z) − D(Z)∣ ∣ ≤ C2 log4 n · V 3 log2 V · 1 h n∑ j=1 I {Zj ̸= Z j} ec46 e-companion to Debiasing In-Sample Policy Performance for Z, Z ∈ En and from Lemma D.10 P {Z ̸∈ En} ≤ C1 n . Finally if n > 4C1eR, then 2C1/n < 1 2 e−R, and we have by Theorem A.5 that |D(Z) − E [D(Z) | Z ∈ En]| ≤ C3V 3 log2(V ) log4(n) h + C3V 3 log2(V ) log4(n)√n h √ log ( 2 e−R − 2C1/n ) ≤ C4V 3 log2(V ) log4(n)√n h √ log ( 2 e−R − 2C1/n ) ≤ C5V 3 log2(V ) log4(n)√n h √R, where the last line again uses n > 4C1eR. Returning to the initial upper bound Eq. (D.20), we apply our two bounds to see |D(Z) − E [D(Z)]| ≤ C6V 3 log2(V ) log4(n) √n h √R + C6 n√ h By Assumption 3.1, h < 1 < n implies that √n h ≥ 1 n√h . Hence, collecting dominant terms completes the proof. □ D.6. Uniform Convergence of Dual Solutions The goal of this section is to extend our previous pointwise results to uniform results over all θ ∈ Θ. Let Θ be a minimal 1 n -covering of Θ. Then, for every θ ∈ Θ there exists ¯θ ∈ Θ such that ∥θ − ¯θ∥2 ≤ 1 n . Lemma D.17 (Uniform Convergence Dual Solution) Under the assumptions of Theo- rem 4.7, there exists dimension independent constants C and n0 such that for any R > 1 and any n ≥ n0eR, the following holds with probability 1 − 2e−R: sup θ∈Θ ∥λ(Z, θ) − E [λ(Z, θ)] ∥∞ ≤ CV 2 log2 V log m √ R log N ( 1 n , Θ ) log4 n √n Proof of Lemma D.17: By triangle inequality, sup θ∈Θ ∥λ(Z, θ) − E [λ(Z, θ)] ∥∞ ≤ sup θ∈Θ ∥λ(Z, θ) − λ(Z, ¯θ)∥∞ ︸ ︷︷ ︸ (a) + sup θ∈Θ ∥E [ λ(Z, θ) − λ(Z, ¯θ)] ∥∞ ︸ ︷︷ ︸ (b) + sup ¯θ∈ ¯Θ ∥λ(Z, ¯θ) − E [ λ(Z, ¯θ)] ∥∞ ︸ ︷︷ ︸ (c) We bound each term separately. First we bound Term (a). If Z ∈ En, then, from Lemma D.14, and bounding the ℓ∞-norm by the ℓ2-norm, sup θ∈Θ ∥λ(Z, θ) − λ(Z, ¯θ)∥∞ ≤ C1V 3 log2 V log5/4 n √n e-companion to Debiasing In-Sample Policy Performance ec47 for some dimension independent constant C1. By Lemma D.10, this occurs with probability at least 1 − 4/n. Next, we bound (b). Telescoping the expectation as before, we have for any i = 1, . . . , m that E [ λi(Z, θ) − λi(Z, ¯θ) ] = E [λi(Z, θ) − λi(Z, ¯θ)∣ ∣ ∣Z ∈ En] P {Z ∈ En} + E [λi(Z, θ) − λi(Z, ¯θ)∣ ∣ ∣Z /∈ En] P {Z /∈ En} We can bound the ﬁrst term using Lemmas D.10 and D.14. To bound the second term, deﬁne the set E1,n ≡ { z : ∥z∥1 ≤ nCµ + 2n √νmin } , and recall that En ⊆ E1,n. Observe that E [λi(Z, θ)∣ ∣ ∣Z /∈ En] P (Z /∈ En) = E [λi(Z, θ)I {Z /∈ En}] ≤ E [λi(Z, θ) (I {Z /∈ E1,n} + I {Z ∈ E1,n, Z /∈ En})] ≤ E [λi(Z, θ)I {Z /∈ E1,n}] + λmaxP (Z /∈ En) (Lemma D.11) ≤ C2 exp (− n C2 ) + C2λmax n (Lemmas D.10 and D.15), for some dimension independent constant C2. Combining these observations shows that E [λi(Z, θ) − λi(Z, ¯θ)] ≤ C3V 3 log2 V log5/4 n √n + C3 exp (− n C3 ) + C3λmax n ≤ C4V 3 log2 V log5/4 n √n where C3 and C4 are dimension-independent constants. Taking the supremum over θ ∈ Θ and over i = 1, . . . , m, bounds Term (b). Finally, we bound Term (c). We see that sup ¯θ∈ ¯Θ,0≤i≤m ∣ ∣λi(Z, ¯θ) − E [ λi(Z, ¯θ)]∣ ∣ ≤ C5V 3 log2(V ) log4 n √n √ R log (m · N ( 1 n , Θ )) by applying Lemma D.16 and taking the union bound over the ∣ ∣Θ∣ ∣ ≤ N ( 1 n , Θ) elements in Θ by the m choices of i. Taking a union bound over the probabilities that bounds hold on Terms (a) and (c) and adding term (b) shows that there exists a dimension independent constant C such that with probability 1 − e −R − 4/n sup θ∈Θ ∥λ(Z, θ) − E [λ(Z, θ)] ∥∞ ≤ CV 3 log2(V ) log m √ R log N ( 1 n , Θ ) log4 n √n . Finally, note that if n > 4e−R, this last probability is at least 1 − 2e−R to complete the proof. □ ec48 e-companion to Debiasing In-Sample Policy Performance D.7. Uniform Convergence of In-Sample Optimism In this section, we construct a high-probability bound for sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 ξjxj(Z, θ) − E [ξjxj(Z, θ)] ∣ ∣ ∣ ∣ ∣ where we recall that Z = µ + ξ. Note for convenience we have scaled this by 1 n . Constructing the bound requires decomposing the in-sample optimism into several sub- components. We outline the subcomponents by providing the proof to Lemma D.18 ﬁrst. For convenience, in this section only, we use the notation λ(θ) ≡ E [λ(Z, θ)] as shorthand. Lemma D.18 (Uniform In-sample Optimism for Coupling Constraints) Let N (ε, Θ) be the ε−covering number of Θ. Under the assumptions of Theorem 4.7, there exists dimension inde- pendent constants C and n0 such that for any R > 1 and n ≥ n0e R, the following holds with probability 1 − 6 exp(−R). sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 ξjxj(Z; θ) − E [ξjxj(Z; θ)] ∣ ∣ ∣ ∣ ∣ ≤ CV 3 log3 V √ log N ( 1 n , Θ) · R log4(n) √n Proof of Lemma D.18. By triangle inequality, we see, sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 ξjxj(Z, θ) − E [ξjxj(Z, θ)] ∣ ∣ ∣ ∣ ∣ ≤ sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 ξj (xj(Z, θ) − I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)})∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ Rounding Error + sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 ξj (I { rj(Zj, θ) ≥ A⊤ j λ(Z, θ) } − I {rj(Zj, θ) ≥ A⊤ j λ(θ)})∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ Dual Approximation Error + sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 ξjI { rj(Zj, θ) ≥ A⊤ j λ(θ)} − E [ ξjI {rj(Zj, θ) ≥ A⊤ j λ(θ)}]∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ ULLN for Dual Approximation + sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 E [ξj (I {rj(Zj, θ) ≥ A⊤ j λ(θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)})]∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ Expected Dual Approximation Error + sup θ∈Θ ∣ ∣ ∣ ∣ ∣ 1 n n∑ j=1 E [ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − xj(Z, θ))] ∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ Expected Rounding Error . e-companion to Debiasing In-Sample Policy Performance ec49 For Rounding and Expected Rounding Error, we have ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξj ( xj(Z, θ) − I { rj(Zj, θ) ≥ A⊤ j λ(Z, θ) })∣ ∣ ∣ ∣ ∣ ≤ ∥ξ∥∞ ∥ ∥xj(Z, θ) − I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)}∥ ∥ 1 ≤ ∥ξ∥∞ m where the ﬁrst inequality follows Holder’s inequality and the second inequality holds by comple- mentary slackness. Note, P {∥ξ∥∞ ≥ t} ≤ n∑ j=1 P {|ξj| ≥ t} ≤ n∑ j=1 2 exp { − νmint 2 2 } . Moreover, E [∥ξ∥∞] ≤ C1√log n for some dimension independent constant C1. Thus, with proba- bility at least 1 − e −R, we have Rounding Error + Expected Rounding Error ≤ ∥ξ∥∞ m n + E ∥ξ∥∞ m n = m n (∥ξ∥∞ + E ∥ξ∥∞) ≤ C2m n √R log n, for some dimension independent constant C2. We bound the Dual Approximation Error terms in Lemma D.20 with our uniform bounds on the dual solutions from Lemma D.17 below, proving that with probability at least 1 − 4e−R, Dual Approximation Error + Expected Dual Approximation Error (D.21) ≤ C3R √ V n + C3V 2 log2 V log(m) · √ R log N ( 1 n , Θ ) · log4(n) √n (D.22) ≤ C4V 2 log2 V log(m) R √n · √ log N ( 1 n , Θ) log4(n) √n (D.23) for some dimension independent constants C3 and C4. We bound the ULLN for Dual Approximation term in Lemma D.19 below to prove that ULLN for Dual Approximation ≤ C5R √ V n with probability 1 − exp(−R). Taking a union bound over all probabilities and summing all bounds yields the result. □ Lemma D.19 (ULLN for Dual Approximation) Under Assumptions 3.1 and 4.6, there exists a dimension independent constant C such that for any R > 1, with probability at least 1 − e−R, sup θ∈Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξjI {rj(Zj, θ) ≥ A⊤ j λ(θ)} − E [ ξjI { rj(Zj, θ) ≥ A⊤ j λ(θ)}]∣ ∣ ∣ ∣ ∣ ≤ C · R√V n ec50 e-companion to Debiasing In-Sample Policy Performance Proof of Lemma D.19: We ﬁrst note that sup θ∈Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξjI { rj(Zj, θ) ≥ A⊤ j λ(θ)} − E [ ξjI {rj(Zj, θ) ≥ A⊤ j λ(θ)}]∣ ∣ ∣ ∣ ∣ ≤ sup θ∈Θ,λ∈Rm ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξjI { rj(Zj, θ) ≥ A⊤ j λ} − E [ ξjI {rj(Zj, θ) ≥ A⊤ j λ}]∣ ∣ ∣ ∣ ∣ and the summation is a sum of centered independent random variables. We will apply Theorem A.1 to the last expression. Speciﬁcally, we consider the envelope F (Z) = (|Zj|)n j=1. Then, we have ∥∥F (Z)∥2∥Ψ (a) ≤ ∥ ∥ ∥ ∥ ∥ζ∥2√νmin ∥ ∥ ∥ ∥ Ψ (b) ≤ √ 2n νmin = C1√n for some dimension independent constant C1. We see (a) holds by letting ζj = √νjξj and (b) holds by Lemma A.1 iv) from GR2020. Next, ∣ ∣ ∣{(ξjI {rj(Zj, θ) ≥ A⊤ j λ})n j=1 : θ ∈ Θ, λ ∈ R m}∣ ∣ ∣ ≤ ∣ ∣ ∣{(I { rj(Zj, θ) ≥ A⊤ j λ })n j=1 : θ ∈ Θ, λ ∈ Rm}∣ ∣ ∣ , and by Assumption 4.6, the latter set has VC-dimension V and hence cardinality at most 2 V . Thus, we see that with probability 1 − e−R, that sup θ∈Θ,λ∈Rm ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξjI { rj(Zj, θ) ≥ A⊤ j λ } − E [ξjI {rj(Zj, θ) ≥ A⊤ j λ}]∣ ∣ ∣ ∣ ∣ ≤ C2R√V n for some absolute constant C2. □ We next provide bounds for the Dual Approximation Error terms in the proof of Lemma D.18. Lemma D.20 (Dual Approximation Error) Assume Assumptions 3.1 and 4.6 hold. Then, there exists dimension independent constants C and n0 such that for any R > 1 and n > n0eR, we have with probability at least 1 − 4e −R, the following two inequalities hold simultaneously: sup θ∈Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I { rj(Zj, θ) ≥ A⊤ j λ(θ)})∣ ∣ ∣ ∣ ∣ ≤ CR√ V n, sup θ∈Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 E [ ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(θ)})]∣ ∣ ∣ ∣ ∣ ≤ CV 2 log3(V ) · √ R log N ( 1 n , Θ ) · log4(n)√n. Proof of Lemma D.20: First observe that under the conditions of the theorem, Lemma D.17 implies that for some dimension independent constant C1, with probability at least 1 − 2e−R, sup θ∈Θ ∥λ(Z, θ) − λ(θ)∥2 ≤ C1V 2 log2(V ) log(m) · √ R log N ( 1 n , Θ) · log4(n) √n ︸ ︷︷ ︸ ≡δ , e-companion to Debiasing In-Sample Policy Performance ec51 where we have by bounding the ℓ2-norm by √ m times the ℓ∞ norm. Deﬁne the right side to be the constant δ as indicated. We will restrict attention to the events where both the above inequality holds and also Z ∈ En. By the union bound and Lemma D.10, this event happens with probability at least 1 − 2e−R − 4/n. For n > 4eR, this probability is at least 1 − 3e−R. Now write ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(θ)})∣ ∣ ∣ ∣ ∣ (D.24) ≤ n∑ j=1 ∣ ∣ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(θ)})∣ ∣ ≤ n∑ j=1 |ξj| I { rj(Zj, θ) − A⊤ j λ(θ) ∈ [−CAδ, CAδ]} , because ∥λ(Z, θ) − λ(θ)∥2 ≤ δ. Furthermore, when the indicator is non-zero, we can bound |ξj| ≤ 1 amin (CAλmax + CAδ + bmax) + Cµ ≤ C2(1 + δ), (D.25) for some dimension independent C2. By Lemma D.2, ∥λ(θ)∥1 ≤ 2 ns0 E∥r(Z, θ)∥1 ≤ λmax and thus we can upper bound, sup θ∈Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξj ( I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(θ)})∣ ∣ ∣ ∣ ∣ ≤ C2(1 + δ) sup θ∈Θ,∥λ∥1≤λmax ∣ ∣ ∣ ∣ ∣ n∑ j=1 I {rj(Zj, θ) − A⊤ j λ ∈ [−CAδ, CAδ] } − P { rj(Zj, θ) − A⊤ j λ ∈ [−CAδ, CAδ] }∣ ∣ ∣ ∣ ∣ ︸ ︷︷ ︸ (i) + C2(1 + δ) sup θ∈Θ P { rj(Zj, θ) − A⊤ j λ(θ) ∈ [−CAδ, CAδ] } ︸ ︷︷ ︸ (ii) To bound the supremum (i), we will apply Theorem A.1. Not that the vector e is a valid envelope. To bound the cardinality of F ≡ {(I {rj(Zj, θ) − A⊤ j λ ∈ [−CAδ, CAδ] })n j=1 : λ ∈ Rm + , θ ∈ Θ} , consider the two sets F1 ≡ {( I {rj(Zj, θ) − A⊤ j λ ≥ −CAδ})n j=1 : λ ∈ Rm + , θ ∈ Θ} , F2 ≡ {( I {rj(Zj, θ) − A⊤ j λ ≤ CAδ})n j=1 : λ ∈ R m + , θ ∈ Θ} . ec52 e-companion to Debiasing In-Sample Policy Performance Under Assumption 4.6, both sets have pseudo-dimension at most V . Furthermore, F = F1 ∧ F2. Hence, by Pollard (1990), there exists an absolute constant C3 such that the pseudo-dimension of F is at most C3V , and hence its cardinality is at most nC3V . Thus, applying Theorem A.1 shows that there exists a constant C4 such that with probability at least 1 − e −R, Term (i) ≤ C4(1 + δ)R√V n log n. To evaluate Term (ii), we recognize it as the probability as the probability that a Gaussian random variable lives in an interval of length 2CAδ. Upper bounding the Gaussian density by its value at the mean shows P {rj(Zj, θ) − A⊤ j λ ∈ [−CAδ, CAδ] } ≤ 2CA √ νmax 2π δ ≤ C5δ. (D.26) Thus, Term (ii) ≤ C6(1 + δ)δ. Combining our bounds, we see that with probability at least 1 − 4e −R, sup θ∈Θ ∣ ∣ ∣ ∣ ∣ n∑ j=1 ξj ( I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(θ) })∣ ∣ ∣ ∣ ∣ ≤ C7(1 + δ)R√V n + C7(1 + δ)δ ≤ C7R√V n, by substituting the value of δ and only retaining the dominant terms. This proves the ﬁrst result of the lemma. To prove the second result of the lemma, note that ∣ ∣ ∣ ∣ ∣ n∑ j=1 E [ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I {rj(Zj, θ) ≥ A⊤ j λ(θ)})]∣ ∣ ∣ ∣ ∣ ≤ n∑ j=1 E [∣ ∣ξj (I {rj(Zj, θ) ≥ A⊤ j λ(Z, θ)} − I { rj(Zj, θ) ≥ A⊤ j λ(θ)})∣ ∣] ≤ C2(1 + δ) n∑ j=1 P (rj(Zj, θ) − A⊤ j λ(θ) ∈ [−CAδ, CAδ] ) ≤ nC8(1 + δ)δ, where the second inequality uses the bound on |ξj| (Eq. (D.25)) and the last inequality follows from argument leading to Eq. (D.26) above. Substituting the value of δ, using the assumption that V ≥ m, and retaining only the dominant terms completes the proof. □ e-companion to Debiasing In-Sample Policy Performance ec53 D.8. Uniform Convergence of VGC Lemma D.21 (Uniform VGC for Coupling Constraints) Let N (ε, Θ) be the ε−covering number of Θ and the assumptions under Theorem 4.7 hold. There exists dimension independent constants C and n0 such that for n ≥ n0eR the following holds with probability 1 − Ce −R, sup θ∈ ¯Θ |D(Z, θ) − E [D(Z, θ)]| ≤ C · V 2 log2 V · √R · n log (n · N (n−3/2, Θ)) · log4 n hmin Proof of Lemma D.21 We follow a similar strategy to Lemma C.4 and again consider the full notation version of the VGC, D(Z, (θ, h)), and take the supremum over θ ∈ Θ and h ∈ H where H ≡ [hmin, hmax]. Let Θ0 be a minimal n−3/2-covering of Θ. In particular, for any θ ∈ Θ there exists a ¯θ ∈ Θ0 such that ∥ ∥ ¯θ − θ∥ ∥ 2 ≤ n−3/2. Similarly, let ¯H be the n−1-covering of H. By telescoping, sup θ∈Θ ∣ ∣D(Z, (θ, h)) − E [ D(Z, (θ, h))]∣ ∣ ≤ sup ¯θ∈Θ0 h∈H ∣ ∣D(Z, ( ¯θ, h)) − E [D(Z, ( ¯θ, h))]∣ ∣ ︸ ︷︷ ︸ (i) (D.27) + sup θ,θ:∥θ−θ∥≤n−3/2 h∈H ∣ ∣D(Z, (θ, h)) − D(Z, ( ¯θ, h)) ∣ ∣ + sup θ,θ:∥θ−θ∥≤n−3/2 h∈H ∣ ∣E [ D(Z, ( ¯θ, h))] − E [D(Z, (θ, h))] ∣ ∣ ︸ ︷︷ ︸ (ii) + sup ¯θ∈Θ0 h,h:∥h−h∥≤n−1 ∣ ∣D(Z, ( ¯θ, h)) − D(Z, ( ¯θ, h))∣ ∣ + sup ¯θ∈Θ0 h,h:∥h−h∥≤n −1 ∣ ∣E [ D(Z, ( ¯θ, h)) ] − E [ D(Z, ( ¯θ, h)) ]∣ ∣ ︸ ︷︷ ︸ (iii) We bound Term (i) by taking a union bound over the N (n−3/2, Θ) elements of ¯Θ and N (n−1, H) elements of H in combination with the pointwise bound from Lemma 4.9. This shows that with probability at least 1 − 4e−R sup ¯θ∈Θ0 h∈H ∣ ∣D(Z, ¯θ) − E [D(Z, ¯θ)]∣ ∣ ≤ C1V 2 log2(V )√R log (N (n−3/2, Θ) N (n−1, H)) √n log4(n) hmin . Terms (ii) and (iii) of Eq. (D.27) can be bounded as follows. First, for (ii), we see by Lemma 3.7 that for ∥θ − θ∥ ≤ n−3/2 there exists a constant C1 such that ∣ ∣D(Z, (θ, h)) − D(Z, ( ¯θ, h))∣ ∣ ≤ C1L h √ R νmin · n1/2√log n with probability 1 − exp(−R). Similarly, there exists C2, C3 and C4 (depending on νmin, L, Cµ, amin, amax, bmax) such that ∣ ∣E [ D(Z, (θ, h)) − D(Z, (θ, h)) ]∣ ∣ ≤ C2n1/2 h (E [∥z∥∞] + 1) ≤ C3n1/2 h (√ log n + Cµ) ≤ C4n1/2 h √log n, ec54 e-companion to Debiasing In-Sample Policy Performance where the second inequality uses a standard bound on the maximum of n sub-Gaussian random variables, and we have used Assumption 3.1 to simplify. Combining the two terms and taking the supremum over h ∈ [hmin, hmax], we see there exists a constant C5 (depending on C1 and C4), such that (ii) ≤ C5√Rn log n hmin Finally, for (iii) we see by Lemma 3.7 that for ∥h − h∥ ≤ n−1, there exists an absolute constant C6 such that, (iii) ≤ C6√n hminνmin Combining, we see there exists dimension independent constants C7 and C8 such that with prob- ability 1 − 5e−R, sup θ∈Θ h∈H |D(Z, θ) − E [D(Z, θ)]| ≤ C7V 2 log2(V )√Rn log (N (n−3/2, Θ) N (n−1, H)) log4(n) hmin + C7 √Rn log n hmin ≤ C8 · V 2 log2 V · √R · n log (n · N (n−3/2, Θ)) · log4 n hmin where the last inequality holds as N (n−1, H) ≤ n. This completes the proof. □ To obtain uniform bounds, we characterize the complexity of the policy class through the n−3/2 covering number of the parameter space Θ. As an example to demonstrate the size of the covering number, consider the case where Θ is a compact subset of Rp with a ﬁnite diameter Γ. Applying Lemma 4.1 of Pollard (1990) and that the ϵ−packing number bounds the ϵ−covering number, we see that N (n−3/2, Θ) ≤ (3n3/2Γ )p. Combining this bound with Eq. (D.27), we obtain the following corollary. Corollary D.22 (Uniform Convergence for Finite Policy Class) Let Θ be a compact sub- set of Rp with a ﬁnite diameter Γ. There exists dimension independent constants C, n0 such that for n ≥ n0 the following holds with probability 1 − C exp {−R}, sup θ∈Θ 1 n |D(Z, θ) − E [D(Z, θ)]| ≤ C log4 n · V 2 log V · 1 hmin √ R n · p log (3n3/2Γ) This corollary shows that the complexity of the policy class depends on the number of parameters of the plug-in policies. We see from Section 2.2 that p for many common policy classes does not depend on n, implying that the convergence of the VGC estimator to its expectation follows the rate from Corollary D.22 up to log terms. For example, p for mixed eﬀect policies depends on the dimension of Wj which reﬂects the information available, such as features, for each µj. This is typically ﬁxed even as the number of observations n may increase. This implies that for many policy classes, the estimation error converges to 0 as n → ∞. e-companion to Debiasing In-Sample Policy Performance ec55 D.9. Proof of Theorem 4.7 We can now prove Theorem 4.7. Proof of Theorem 4.7: We proceed to bound each term on the right side of Eq. (4.1). To bound Eq. (4.1a), we have by Lemma D.18, that with probability at least 1 − e−R, that sup θ∈ ¯Θ ∣ ∣ξ⊤x(Z, θ) − E [ ξ⊤x(Z, θ)]∣ ∣ ≤ CV 3 log3 V √ n · log N ( 1 n , Θ) · R log4(n). To bound Eq. (4.1b), let H ≡ [hmin, hmax]. Then, by Lemma D.21, we have that for some dimension independent constant C1 that with probability at least 1 − C1e−R, sup θ∈ ¯Θ |D(Z, θ) − E [D(Z, θ)]| ≤ C1 · V 2 log2 V · √R · n log (n · N (n−3/2, Θ)) · log4 n hmin . Finally, to bound Eq. (4.1c), use Theorem 3.2 and take the supremum over h ∈ H to obtain Eq. (4.1c) ≤ C2hmaxn log(1/hmin). Substituting these three bounds into Eq. (4.1) and collecting constants proves the theorem. □","libVersion":"0.3.2","langs":""}
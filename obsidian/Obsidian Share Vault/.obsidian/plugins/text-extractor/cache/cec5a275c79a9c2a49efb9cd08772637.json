{"path":"dailies/attachments/PDFAnnotationTestDoc.pdf","text":"ECE 5630: Information Theory for Data Transmission, Security and Machine Learning 3/5/20 Lecture 10: Mutual Information Scriber: Isay Katsman, Net ID: isk22 Lecturer: Prof. Ziv Goldfeld Assistant Editor: Kia Khezeli 10.1 Mutual Information Deﬁnition 10.1 (Mutual Information) Let (X, Y ) ∼ PXY ∈ P(X × Y). The mutual information between X and Y is deﬁned as I(X; Y ) := DKL(PXY ||PX ⊗ PY ), where PX and PY are the X and Y marginals of PXY and PX ⊗ PY is the induced product measure. Remark 10.1 (Comments) Note the following: (i) Mutual information is a fundamental measure of dependence between random variables: it is invari- ant to invertible transformations of the random variables, nulliﬁes if and only if random variables are independent, and emerges as a solution to operational data compression and transmission questions. (ii) We interpret I(X; Y ) as the amount of information that X and Y convey about each other. Proposition 10.1 (Basic Properties of Mutual Information) Mutual information satisﬁes the follow- ing properties: 1. I(X; Y ) ≥ 0 with equality if and only if X ⊥⊥ Y . 2. I(X; Y ) = DKL(PY |X ||PY |PX ). 3. I(X; Y ) = I(Y ; X). 4. I(X; Y ) ≥ I(X; f (Y )) for any deterministic function, with equality if and only if f is a bijection. 5. I(X, Y ; Z) ≥ I(X; Z). Note that I(X, Y ; Z) = DKL(PXY Z||PXY ⊗ PZ). Proof: 1. Clear by deﬁnition (derives from non-negativity of KL divergence for probability measures). 2. Let QXY = PX ⊗ PY and observe that QX = PX and QY |X = PY . From the chain rule for KL divergences, we have DKL(PXY ||QXY ) = DKL(PX ||QX ) + DKL(PY |X ||QY |X |PX ) Thus, DKL(PXY ||PX ⊗ PY ) = 0 z }| { DKL(PX ||PX ) +DKL(PY |X ||PY |PX ) = DKL(PY |X ||PY |PX ). 3. Let g(x, y) = (y, x) and consider the transition kernel induced by g. Passing PX,Y and PX ⊗ PY through g produces PY,X and PY ⊗ PX , respectively. Applying the KL divergence DPI to this setup we obtain Df (PXY ||PX ⊗ PY ) ≥ Df (PY X ||PY ⊗ PX ). Reversing the role of X and Y completes the proof. 1 2 4. The proof follows by the mutual information DPI. As will be shown in the next lecture, if X → Y → Z forms a Markov chain, then I(X; Y ) ≥ I(X; Z) with equality if and only if X → Z → Y . Clearly, X → Y → f (Y ), and if f is a bijection, then we also have X → f (Y ) → Y . 5. Let g(x, y, z) = (x, z) and consider the induced transition kernel. Passing PX,Y,Z and PX,Y ⊗ PZ through g produces PX,Z and PX ⊗ PZ, respectively. Applying the KL divergence DPI produces the result. ■ Proposition 10.2 (Mutual Information and Entropy) 1. I(X; X) = { H(X), discrete X, ∞, otherwise. 2. For discrete X: I(X; Y ) = H(X) + H(Y ) − H(X, Y ) = H(X) − H(X|Y ) = H(Y ) − H(Y |X). 3. For continuous X: I(X; Y ) = h(X) + h(Y ) − h(X, Y ) = h(X) − h(X|Y ) = h(Y ) − h(Y |X). Proof: 1. We consider discrete and continuous cases separately. Finally, we extend the derivation for the contin- uous case to arbitrary non-discrete case. (i) Discrete: From the deﬁnition I(X, X) = DKL(PX|X ||PX |PX ) where PX|X (·|x) = δx(·). Note that δx ≪ PX , for any x ∈ supp (PX ). Then, I(X; X) = DKL(PX|X ||PX |PX ) = ∑ x∈X pX (x)DKL(PX|X (·|x) | {z } δx(·) ||PX ) = ∑ x∈X pX (x) ∑ x′∈X δx(x′) log δx(x′) pX (x′) = ∑ x∈X pX (x) log 1 pX (x) = H(X). (ii) Continuous: Assume PX ≪ λ where λ is the Lebesgue measure. From the deﬁnition I(X; X) = DKL(PXX ||PX ⊗ PX ). We will show that PXX ̸≪ PX ⊗ PX , thereby implying that KL divergence diverges, as claimed. Deﬁne the diagonal set ∆ := {(x, x) : x ∈ X }. Then, PXX (∆) = ∫ ∆ dPXX (x, x) = ∫ X ∫ X 1 {x=x′}dPXX (x, x) = ∫ X dPX (x) ∫ X 1 {x=x′}dPX|X (x′|x) = ∫ X dPX (x) ∫ X 1 {x=x′}dδx(x′) = ∫ X δx(x)dPX (x) = 1. However, PX ⊗ PX (∆) = ∫ ∆ dPX ⊗ PX (x, x ′) = ∫ X ∫ X 1 {x=x′}dPX ⊗ PX = ∫ X dPX (x) ∫ X 1 {x=x′}dPX (x ′) = ∫ X PX (x)dPX (x) = 0, where the last equality follows from the fact that PX (x) = 0 for all x ∈ X because PX ≪ λ. Thus, PXX ̸≪ PX ⊗ PX as PXX (∆) > 0 while PX ⊗ PX (∆) = 0. (iii) Non-discrete: The continuous distribution argument trivially extends to an arbitrary non-discrete scenario. In particular, deﬁne A := {x ∈ X : PX ({x}) > 0} and ∆A := {(x, x) : x ∈ Ac}. Repeating the above proof for ∆A instead of ∆ produces the general result. 3 2. By deﬁnition, I(X; Y ) = ∑ x,y PXY (x, y) log PXY (x,y) PX (x)PY (y) and PXY (x, y) = PY (y)PX|Y (x|y). Then, I(X; Y ) = ∑ x,y PXY (x, y) log PY (y)PX|Y (x|y) PX (x)PY (y) = ∑ x,y PXY (x, y) log 1 PX (x) − ∑ x,y PXY (x, y) log 1 PX|Y (x|y) = H(X) − H(X|Y ). By repeating the above argument using PXY (x, y) = PX (x)PY |X (y|x) we get I(X; Y ) = H(Y )−H(Y |X). Additionally recall from the deﬁnition of conditional entropy that H(Y |X) = H(X, Y ) − H(X), so we have I(X; Y ) = H(Y ) − H(Y |X) = H(Y ) + H(X) − H(X, Y ). 3. The derivation for the continuous case is analogous to the discrete case, and is thus omitted. ■ Remark 10.2 (Illustration) The relationship between mutual information and entropy is illustrated in Fig- ure 1. H(X) H(Y ) H(X|Y ) H(Y |X)I(X; Y ) Figure 1: The relationship between mutual information and entropy. Example 10.1 • Binary Symmetric Channel (BSC): Let X ∼ Ber(1/2) and Y = X ⊕ Z (addition modulo 2) where Z ∼ Ber(ϵ), with ϵ ∈ [0, 1/2] independent of X. The BSC is depicted in Figure 2. 1 0 1 0 1 − ϵ ϵ ϵ 1 − ϵ Figure 2: Binary symmetric channel with ﬂip parameter ϵ. First observe that Y = { X ⊕ 0, Z = 0, X ⊕ 1, Z = 1. = { X, w.p. 1 − ϵ, 1 − X, w.p. ϵ. 4 To ﬁnd I(X; Y ), we compute H(Y ) and H(Y |X), separately. For H(Y ), we ﬁrst ﬁnd the PMF of Y . Consider: PY (0) = PX (0) · PY |X (0|0) + PX (1) · PY |X (0|1) = 1 2 (1 − ϵ) + 1 2 ϵ = 1 2 . Thus, Y ∼ Ber(1/2), and so H(Y ) = Hb(1/2) = 1. For H(Y |X), we have H(Y |X) = ∑ x∈{0,1} PX (x)H(Y |X = x) = ∑ x∈{0,1} pX (x)H(X ⊕ Z|X = x). By independence of X and Z, we have H(X ⊕ Z|X = x) = H(x ⊕ Z|X = x) = H(x ⊕ Z) = H(Z), where the last equality follows from the fact that entropy is invariant to bijection. Then, H(Y |X) = ∑ x∈{0,1} pX (x)H(X ⊕ Z|X = x) = ∑ x∈{0,1} pX (x)H(Z) = H(Z) = Hb(ϵ). This gives us that I(X; Y ) = 1 − Hb(ϵ) for the BSC. Figure 3 depicts the mutual information as a function of ϵ. ϵ I(X; Y ) 1/2 1 1 Figure 3: Mutual information of a BSC as a function of its parameter ϵ. Notice that, in the “worst” case, ϵ = 1/2 and we have I(X; Y ) = 0, i.e., we cannot pass any information through the BSC. • Bivariate Gaussian: Let (X, Y ) ∼ N ([ 0 0 ] , [1 ρ ρ 1 ]) . Recall that for a d-dimensional Gaussian we have h(N (µ, Σ) ) = 1 2 log((2πe)d det K). Thus I(X; Y ) = h(X) + h(Y ) − h(X, Y ) = 1 2 log(2πe) + 1 2 log(2πe) − 1 2 log((2πe) 2(1 − ρ 2)) = 1 2 log 1 1 − ρ2 Note that I(X; Y ) = ∞ when ρ = 1. One could equivalently see that for X = Y , we have I(X; Y ) = ∞ from Proposition 10.1. Moreover, ρ = 0 implies that X and Y are uncorrelated. For Gaussian random variables uncorrelation is equivalent to independence, which, in turn, is equivalent to I(X; Y ) = 0.","libVersion":"0.3.1","langs":""}
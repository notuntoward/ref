{"path":"lit/lit_sources/Galib22DeepExtremaDeepLearning.pdf","text":"DeepExtrema: A Deep Learning Approach for Forecasting Block Maxima in Time Series Data Asadullah Hill Galib 1 , Andrew McDonald 1 , Tyler Wilson 1 , Lifeng Luo 2 and Pang-Ning Tan 1 1Department of Computer Science & Engineering, Michigan State University 2Department of Geography, Michigan State University {galibasa, mcdon499, wils1270, lluo, ptan}@msu.edu Abstract Accurate forecasting of extreme values in time se- ries is critical due to the signiﬁcant impact of ex- treme events on human and natural systems. This paper presents DeepExtrema, a novel framework that combines a deep neural network (DNN) with generalized extreme value (GEV) distribution to forecast the block maximum value of a time se- ries. Implementing such a network is a challenge as the framework must preserve the inter-dependent constraints among the GEV model parameters even when the DNN is initialized. We describe our ap- proach to address this challenge and present an ar- chitecture that enables both conditional mean and quantile prediction of the block maxima. The ex- tensive experiments performed on both real-world and synthetic data demonstrated the superiority of DeepExtrema compared to other baseline methods. 1 Introduction Extreme events such as droughts, ﬂoods, and severe storms occur when the values of the corresponding geophysical vari- ables (such as temperature, precipitation, or wind speed) reach their highest or lowest point during a period or surpass a threshold value. Extreme events have far-reaching conse- quences for both humans and the environment. For example, four of the most expensive hurricane disasters in the United States since 2005—Katrina, Sandy, Harvey, and Irma—have each incurred over $50 billion in damages with enormous death tolls [US GAO, 2020]. Accurate forecasting of the ex- treme events [Wang and Tan, 2021] is therefore crucial as it not only helps provide timely warnings to the public but also enables emergency managers and responders to better assess the risk of potential hazards caused by future extreme events. Despite its importance, forecasting time series with ex- tremes can be tricky as the extreme values may have been ignored as outliers during training to improve the generaliza- tion performance of the model. Furthermore, as current ap- proaches are mostly designed to minimize the mean-square prediction error, their ﬁtted models focus on predicting the conditional expectation of the target variable rather than its extreme values [Bishop, 2006]. Extreme value theory (EVT) offers a statistically well-grounded approach to derive the Figure 1: Types of extreme values in a given time window. limiting distribution governing a sequence of extreme val- ues [Coles et al., 2001]. The two most popular distribu- tions studied in EVT are the generalized extreme value (GEV) and generalized Pareto (GP) distributions. Given a prediction time window, GEV governs the distribution of its block max- ima, whereas GP is concerned with the distribution of excess values over a certain threshold, as shown Figure 1. This pa- per focuses on forecasting the block maxima as it allows us to assess the worst-case scenario in the given forecast time win- dow and avoids making ad-hoc decisions related to the choice of excess threshold to use for the GP distribution. Unfortu- nately, classical EVT has limited capacity in terms of mod- eling highly complex, nonlinear relationships present in time series data. For example, [Kharin and Zwiers, 2005] uses a simple linear model with predictors to infer the parameters of a GEV distribution. Deep learning methods have grown in popularity in recent years due to their ability to capture nonlinear dependencies in the data. Previous studies have utilized a variety of deep neural network architectures for time series modeling, includ- ing long short-term memory networks [Sagheer and Kotb, 2019; Masum et al., 2018; Laptev et al., 2017], convolu- tional neural networks [Bai et al., 2018; Zhao et al., 2017; Yang et al., 2015], encoder-decoder based RNN [Peng et al., 2018], and attention-based models [Zhang et al., 2019; Aliabadi et al., 2020]. However, these works are mostly fo- cused on predicting the conditional mean of the target vari- able. While there have some recent attempts to incorporate EVT into deep learning [Wilson et al., 2022; Ding et al., 2019; Polson and Sokolov, 2020], they are primarily focused on modeling the tail distribution, i.e., excess values over a threshold, using the GP distribution, rather than forecasting the block maxima using the GEV distribution. Furthermore,arXiv:2205.02441v1 [cs.LG] 5 May 2022 instead of inferring the distribution parameters from data, some methods [Ding et al., 2019] assume that the parameters are ﬁxed at all times and can be provided as user-speciﬁed hyperparameters while others [Polson and Sokolov, 2020] do not enforce the necessary constraints on parameters of the ex- treme value distribution. Incorporating the GEV distribution into the deep learning formulation presents many technical challenges. First, the GEV parameters must satisfy certain positivity constraints to ensure that the predicted distribution has a ﬁnite bound [Coles et al., 2001]. Maintaining these constraints throughout the training process is a challenge since the model parameters depend on the observed predictor values in a mini-batch. An- other challenge is the scarcity of data since there is only one block maximum value per time window. This makes it hard to accurately infer the GEV parameters for each window from a set of predictors. Finally, the training process is highly sen- sitive to model initialization. For example, the random ini- tialization of a deep neural network (DNN) can easily vio- late certain regularity conditions of the GEV parameters es- timated using maximum likelihood (ML) estimation. An im- proper initialization may lead to ξ estimates that defy the reg- ularity conditions. For example, if ξ < −0.5, then resulting ML estimators may not have the standard asymptotic prop- erties [Smith, 1985], whereas if ξ > 1, then the conditional mean is not well-deﬁned. Without proper initialization, the DNN will struggle to converge to a feasible solution with ac- ceptable values of the GEV parameters. Thus, controlling the initial estimate of the parameters is difﬁcult but necessary. To overcome these challenges, we propose a novel frame- work called DeepExtrema that utilizes the GEV distribu- tion to characterize the distribution of block maximum values for a given forecast time window. The parameters of the GEV distribution are estimated using a DNN, which is trained to capture the nonlinear dependencies in the time series data. This is a major departure from previous work by [Ding et al., 2019], where the distribution parameters are assumed to be a ﬁxed, user-speciﬁed hyperparameter. DeepExtrema reparameterizes the GEV formulation to ensure that the DNN output is compliant with the GEV positivity constraints. In addition, DeepExtrema offers a novel, model bias offset mechanism in order to ensure that the regularity conditions of the GEV parameters are satisﬁed from the beginning. In summary, the main contributions of the paper are: 1. We present a novel framework to predict the block max- ima of a given time window by incorporating GEV dis- tribution into the training of a DNN. 2. We propose a reformulation of the GEV constraints to ensure they can be enforced using activation functions in the DNN. 3. We introduce a model bias offset mechanism to ensure that the DNN output preserves the regularity conditions of the GEV parameters despite its random initialization. 4. We perform extensive experiments on both real-world and synthetic data to demonstrate the effectiveness of DeepExtrema compared to other baseline methods. 2 Preliminaries 2.1 Problem Statement Let z1, z2, · · · , zT be a time series of length T . Assume the time series is partitioned into a set of time windows, where each window [t − α, t + β] contains a sequence of predictors, xt = (zt−α, zt−α+1, · · · , zt), and target, ˜yt = (zt+1, zt+2, · · · , zt+β). Note that β is known as the fore- cast horizon of the prediction. For each time window, let yt = maxτ ∈{1,··· ,β} zt+τ be the block maxima of the tar- get variable at time t. Our time series forecasting task is to estimate the block maxima, ˆyt, as well as its upper and lower quantile estimates, ˆyU and ˆyL, of a future time window based on current and past data, xt. 2.2 Generalized Extreme Value Distribution The GEV distribution governs the distribution of block max- ima in a given window. Let Y = max{z1, z2, · · · , zt}. If there exist sequences of constants at > 0 and bt such that P r(Y − bt)/at ≤ y → G(y) as t → ∞ for a non-degenerate distribution G, then the cumulative dis- tribution function G belongs to a family of GEV distribution of the form [Coles et al., 2001]: G(y) = exp { − [ 1 + ξ( y − µ σ )]−1/ξ} (1) The GEV distribution is characterized by the following pa- rameters: µ (location), σ (scale), and ξ (shape). The expected value of the distribution is given by ymean = µ + σ ξ [ Γ(1 − ξ) − 1 ] (2) where Γ(x) denotes the gamma function of a variable x > 0. Thus, ymean is only well-deﬁned for ξ < 1. Furthermore, the p th quantile of the GEV distribution, yp, can be calculated as follows: yp = µ + σ ξ [ (− log p)−ξ − 1 ] (3) Given n independent block maxima values, {y1, y2, · · · , yn}, with the distribution function given by Equation (1) and assuming ξ ̸= 0, its log-likelihood function is given by: ℓGEV (µ, σ, ξ) = −n log σ − ( 1 ξ + 1) n∑ i=1 log(1 + ξ yi − µ σ ) − n∑ i=1 (1 + ξ yi − µ σ ) −1/ξ (4) The GEV parameters (µ, σ, ξ) can be estimated using the maximum likelihood (ML) approach by maximizing (4) sub- ject to the following positivity constraints: σ > 0 and ∀ i : 1 + ξ σ (yi − µ) > 0 (5) In addition to the above positivity constraints, the shape parameter ξ must be within certain range of values in order Figure 2: Proposed DeepExtrema framework for predicting block maxima using GEV distribution. for the ML estimators to exist and have regular asymptotic properties [Coles et al., 2001]. Speciﬁcally, the ML estima- tors have regular asymptotic properties as long as ξ > −0.5. Otherwise, if −1 < ξ < −0.5, then the ML estimators may exist but will not have regular asymptotic properties. Finally, the ML estimators do not exist if ξ < −1 [Smith, 1985]. 3 Proposed Framework: DeepExtrema This section presents the proposed DeepExtrema frame- work for predicting the block maxima of a given time win- dow. The predicted block maxima ˆy follows a GEV dis- tribution, whose parameters are conditioned on observations of the predictors x. Figure 2 provides an overview of the DeepExtrema architecture. Given the input predictors x, the framework uses a stacked LSTM network to learn a rep- resentation of the time series. The LSTM will output a latent representation, which will used by a fully connected layer to generate the GEV parameters: (µ, σ, ξu, ξl) = LST M (x) (6) where µ, σ, and ξ’s are the location, shape, and scale param- eters of the GEV distribution. Note that ξu and ξl are the estimated parameters due to reformulation of the GEV con- straints, which will be described in the next subsection. The proposed Model Bias Offset (MBO) component per- forms bias correction on the estimated GEV parameters to en- sure that the LSTM outputs preserve the regularity conditions of the GEV parameters and generate a feasible value for ξ ir- respective of how the network was initialized. The GEV pa- rameters are subsequently provided to a fully connected layer to obtain point estimates of the block maxima, which include its expected value ˆy as well as upper and lower quantiles, ˆyU and ˆyL, using the equations given in (2) and (3), respectively. The GEV parameters are then used to compute the negative log-likelihood of the estimated GEV distribution, which will be combined with the root-mean-square error (RMSE) of the predicted block maxima to determine the overall loss func- tion. Details of the different components are described in the subsections below. 3.1 GEV Parameter Estimation Let D = {(xi, yi)}n i=1 be a set of training examples, where each xi denotes the predictor time series and yi is the corre- sponding block maxima for time window i. A na¨ıve approach is to assume that the GEV parameters (µ, σ, ξ) are constants for all time windows. This can be done by ﬁtting a global GEV distribution to the set of block maxima values yi’s us- ing the maximum likelihood approach given in (4). Instead of using a global GEV distribution with ﬁxed parameters, our goal is to learn the parameters (µi, σi, ξi) of each window i using the predictors xi. The added ﬂexibility enables the model to improve the accuracy of its block maxima predic- tion, especially for non-stationary time series. The estimated GEV parameters generated by the LSTM must satisfy the two positivity constraints given by the in- equalities in (5). While the ﬁrst positivity constraint on σi is straightforward to enforce, maintaining the second one is harder as it involves a nonlinear relationship between yi and the estimated GEV parameters, ξi, µi, and σi. The GEV pa- rameters may vary from one input xi to another, and thus, learning them from the limited training examples is a chal- lenge. Worse still, some of the estimated GEV parameters could be erroneous, especially at the initial rounds of the training epochs, making it difﬁcult to satisfy the constraints throughout the learning process. To address these challenges, we propose a reformulation of the second constraint in (5). This allows the training process to proceed even though the second constraint in (5) has yet to be satisﬁed especially in the early rounds of the training epochs. Speciﬁcally, we relax the hard constraint by adding a small tolerance factor, τ > 0, as follows: ∀ i : 1 + ξ σ (yi − µ) + τ ≥ 0. (7) The preceding soft constraint allows for minor violations of the second constraint in (5) as long as 1+ ξ σ (yi −µ) > −τ for all time windows i. Furthermore, to ensure that the inequality holds for all yi’s, we reformulate the constraint in (7) in terms of ymin = mini yi and ymax = maxi yi as follows: Theorem 1. Assuming ξ ̸= 0, the soft constraint in (7) can be reformulated into the following bounds on ξ: − σ ymax − µ (1 + τ ) ≤ ξ ≤ σ µ − ymin (1 + τ ) (8) where τ is the tolerance on the constraint in 5. Proof. For the lower bound on ξ, set yi to be ymax in (7): 1 + ξ σ (ymax − µ) + τ ≥ 0 =⇒ ξ σ (ymax − µ) ≥ − (1 + τ ) =⇒ ξ ≥ − σ (ymax − µ) (1 + τ ) To obtain the upper bound on ξ, set yi to be ymin in (7): 1 + ξ σ (ymin − µ) + τ ≥ 0 =⇒ ξ σ (µ − ymin) ≤ (1 + τ ) =⇒ ξ ≤ σ (µ − ymin) (1 + τ ) Following Theorem 1, the upper and lower bound constraints on ξ in (8) can be restated as follows: σ µ − ymin (1 + τ ) − ξ ≥ 0 ξ + σ ymax − µ (1 + τ ) ≥ 0 (9) The reformulation imposes lower and upper bounds on ξ, which can be used to re-parameterize the second constraint in (5). Next, we describe how the reformulated constraints in (9) can be enforced by DeepExtrema in a DNN. Given an input xi, DeepExtrema will generate the fol- lowing four outputs: µi, P1i, P2i, and P3i. A softplus acti- vation function, sof tplus(x) = log (1 + exp (x)), which is a smooth approximation to the ReLU function, is used to en- force the non-negativity constraints associated with the GEV parameters. The scale parameter σi can be computed using the softplus activation function on P1i as follows: σi = sof tplus(P1i) (10) This ensures the constraint σi ≥ 0 is met. The lower and upper bound constraints on ξi given by the inequalities in (9) are enforced using the softplus function on P2i and P3i: σi µi − ymin (1 + τ ) − ξu,i = sof tplus(P2i) σi ymax − µi (1 + τ ) + ξl,i = sof tplus(P3i) (11) By re-arranging the above equation, we obtain ξu,i = σi µi − ymin (1 + τ ) − sof tplus(P2i) ξl,i = sof tplus(P3i) − σi ymax − µi (1 + τ ) (12) DeepExtrema computes the upper and lower bounds on ξi using the formulas in (12). During training, it will minimize the distance between ξu,i and ξl,i and will use the value of ξu,i as the estimate for ξi. Note that the two ξi’s converge rapidly to a single value after a small number of training epochs. 3.2 Model Bias Offset (MBO) Although the constraint reformulation approach described in the previous subsection ensures that the DNN outputs will satisfy the GEV constraints, the random initialization of the network can produce estimates of ξ that violate the regularity conditions described in Section 2.2. Speciﬁcally, the ML- estimated distribution may not have the asymptotic GEV dis- tribution when ξ < −0.5 while its conditional mean is not well-deﬁned when ξ > 1. Additionally, the estimated loca- tion parameter µ may not fall within the desired range be- tween ymin and ymax when the DNN is randomly initialized. Thus, without proper initialization, the DNN will struggle to converge to a good solution and produce acceptable values of the GEV parameters. One way to address this challenge is to repeat the random initialization of the DNN until a reasonable set of initial GEV parameters, i.e., ymin ≤ µ ≤ ymax and −0.5 < ξ < 1, is found. However, this approach is infeasible given the size of the parameter space of the DNN. A better strategy is to con- trol the initial output of the neural network in order to pro- duce an acceptable set of GEV parameters, (µ, σ, ξ) during initialization. Unfortunately, controlling the initial output of a neural network is difﬁcult given its complex architecture. We introduce a simple but effective technique called Model Bias Offset (MBO) to address this challenge. The key insight here is to view the GEV parameters as a biased output due to Figure 3: Model Bias Offset (MBO) to ensure the initial estimates of the GEV parameters are reasonable even when the DNN is randomly initialized. the random initialization of the DNN and then perform bias correction to alleviate the effect of the initialization. To do this, let µdesired, σdesired, and ξdesired be an acceptable set of initial GEV parameters. The values of these initial parameters must satisfy the regularity conditions −0.5 < ξdesired < 1, σdesired > 0, and ymin ≤ µdesired ≤ ymax. This can be done by randomly choosing a value from the preceding range of ac- ceptable values, or more intelligently, using the GEV param- eters estimated from a global GEV distribution ﬁtted to the block maxima yi’s in the training data via the ML approach given in (4), without considering the input predictors (see the discussion at the beginning of Section 3.1). We ﬁnd that the latter strategy works well in practice as it can lead to faster convergence especially when the global GEV parameters are close to the ﬁnal values after training. When the DNN is randomly initialized, let µ0, σ0, ξu,0, and ξl,0 be the initial DNN output for the GEV parameters. These initial outputs may not necessarily fall within their respec- tive range of acceptable values. We consider the difference between the initial DNN output and the desired GEV param- eters as a model bias due to the random initialization: µbias = µ0 − µdesired σbias = σ0 − σdesired ξu,bias = ξu,0 − ξu,desired ξl,bias = ξl,0 − ξl,desired (13) The model bias terms in (14) can be computed during the initial forward pass of the algorithm. The gradient calculation and back-propagation are disabled during this step to prevent the DNN from computing the loss and updating its weight with the unacceptable GEV parameters. After the initial it- eration, the gradient calculation will be enabled and the bias terms will be subtracted from the DNN estimate of the GEV parameters in all subsequent iterations t: µt → µt − µbias σt → σt − σbias ξu,t → ξu,t − ξu,bias ξl,t → ξl,t − ξl,bias (14) Observe that, when µt is set to µ0, then the debiased output µt − µbias will be equal to µdesired. By debiasing the output of the DNN in this way, we guarantee that the initial GEV pa- rameters are reasonable and satisfy the GEV regularity con- ditions. 3.3 Block Maxima Prediction Given an input xi, the DNN will estimate the GEV param- eters needed to compute the block maxima ˆyi along with its upper and lower quantiles, ˆyU,i and ˆyL,i, respectively. The quantiles are estimated using the formula given in (3). The GEV parameters are provided as input to a fully connected network (FCN) to generate the block maxima prediction, ˆyi. DeepExtrema employs a combination of the negative log-likelihood function (−ℓGEV (µ, σ, ξ)) of the GEV dis- tribution and a least-square loss function to train the model. This enables the framework to simultaneously learn the GEV parameters and make accurate block maxima predictions. The loss function to be minimized by DeepExtrema is: L = λ1 ˆL + (1 − λ1) n∑ i=1(yi − ˆyi) 2 (15) where ˆL = −λ2 ℓGEV (µ, σ, ξ) + (1 − λ2) ∑n i=1(ξu,i − ξl,i)2 is the regularized GEV loss. The ﬁrst term in ˆL corresponds to the negative log-likelihood function given in Equation (4) while the second term minimizes the difference between the upper and lower-bound estimates of ξ. The loss function L combines the regularized GEV loss ( ˆL) with the least-square loss. Here, λ1 and λ2 are hyperparameters to manage the trade-off between different factors of the loss function. 4 Experimental Evaluation This section presents our experimental results comparing DeepExtrema against the various baseline methods. The code and datasets are available at https://github.com/galib19/ DeepExtrema-IJCAI22-. 4.1 Data Synthetic Data As the ground truth GEV parameters are often unknown, we have created a synthetic dataset to evaluate the performance of various methods in terms of their ability to correctly infer the parameters of the GEV distribution. The data is generated assuming the GEV parameters are functions of some input predictors x ∈ R6. We ﬁrst generate x by random sampling from a uniform distribution. We then assume a non-linear mapping from x to the GEV parameters µ, σ, and ξ, via the following nonlinear equations: µ(x) = wT µ (exp (x) + x) σ(x) = wT σ (exp (x) + x) ξ(x) = wT ξ (exp (x) + x) (16) where wµ, wσ, and wξ are generated from a standard normal distribution. Using the generated µ, σ, and ξ parameters, we then randomly sample y from the GEV distribution governed by the GEV parameters. Here, y denotes the block maxima as it is generated from a GEV distribution. We created 8,192 block maxima values for our synthetic data. Real-world data We consider the following 3 datasets for our experiments. Negative Log-likelihood DeepExtrema Ground Truth Global GEV Estimate 4410 4451 4745 Table 1: Negative log-likelihood of DeepExtrema with respect to ground truth and global parameter estimation using synthetic data Hurricane: This corresponds to tropical cyclone intensity data obtained from the HURDAT2 database [Landsea and Franklin, 2013]. There are altogether 3,111 hurricanes span- ning the period between 1851 and 2019. For each hurricane, wind speeds (intensities) were reported at every 6-hour inter- val. We consider only hurricanes that have at least 24-time steps at minimum for our experiments. For each hurricane, we have created non-overlapping time windows of length 24 time steps (6 days). We use the ﬁrst 16 time steps (4 days) in the window as the predictor variables and the block maxima of the last 8 time steps (2 days) as the target variable. Solar: This corresponds to half-hourly energy use (kWh) for 55 families over the course of 284 days from Ausgrid database [Aus, 2013]. We preprocess the data by creating non-overlapping time windows of length 192 time steps (4 days). We use the ﬁrst 144 time steps (3 days) in the window as the predictor variables and the block maxima of the last 48 time steps (1 day) as the target variable. Weather: We have used a weather dataset from the Kag- gle competition [Muthukumar, 2017]. The data is based on hourly temperature data for a city over a ten-year period. We use the ﬁrst 16 time steps (16 hours) in the window as the predictor variables and the block maxima of the last 8 time steps (8 hours) as the target variable. 4.2 Experimental Setup For evaluation purposes, we split the data into separate train- ing, validation, testing with a ratio of 7:2:1. The data is stan- dardized to have zero mean and unit variance. We compare DeepExtrema against the following baseline methods: (1) Persistence, which uses the block maxima value from the pre- vious time step as its predicted value, (2) fully-connected network (FCN), (3) LSTM, (4) Transformer, (5) DeepPIPE [Wang et al., 2020], and (6) EVL [Ding et al., 2019]. We will use the following metrics to evaluate the performance of the methods: (1) Root mean squared error (RMSE) and correla- tion between the predicted and ground truth block maxima and (2) Negative log-likelihood (for synthetic data). Finally, hyperparameter tuning is performed by assessing the model performance on the validation set. The hyperparameters of the baseline and the proposed methods are selected using Ray Tune, a tuning framework with ASHA (asynchronous succes- sive halving algorithm) scheduler for early stopping. 4.3 Experimental Results Results on Synthetic Data In this experiment, we have compared the performance of DeepExtrema against using a single (global) GEV param- eter to ﬁt the data. Based on the results shown in Table 1, DeepExtrema achieves a signiﬁcantly lower negative log- likelihood of 4410 compared to the negative log-likelihood Figure 4: Comparison between actual and predicted block maxima of hurricane intensities for DeepExtrema. Figure 5: Comparison between actual and predicted block maxima of hurricane intensities for EVL [Ding et al., 2019]. for global GEV estimate, which is 4745. This result supports the assumption that each block maxima comes from different GEV distributions rather than a single (global) GEV distri- bution. The results also suggest that the negative log likeli- hood estimated by DeepExtrema is lower than that for the ground truth. Methods Hurricanes Ausgrid W eather RMSE Correlation RMSE Correlation RMSE Correlation Persistence 28.6 0.6 0.84 0.65 4.16 0.96 FCN 14.14 0.87 0.69 0.65 2.5 0.97 LSTM 13.31 0.88 0.65 0.64 2.53 0.97 T ransformer 13.89 0.88 0.68 0.62 2.43 0.98 DeepPIPE 13.67 0.87 0.71 0.59 2.59 0.94 EVL 15.72 0.83 0.75 0.54 2.71 0.90 DeepExtrema 12.81 0.90 0.63 0.67 2.27 0.97 Table 2: Performance comparison on real world data. Results on Real-world Data Evaluation on real-world data shows that DeepExtrema outperforms other baseline methods used for comparison for all data sets (see Table 2). For RMSE, DeepExtrema gen- erates lower RMSE compared to all the baselines on all 3 datasets, whereas for correlation, DeepExtrema outper- forms the baselines on 2 of the 3 datasets. To demonstrate how well the model predicts the extreme values, Figure 4 shows a scatter plot of the actual versus Figure 6: 90% conﬁdence interval of the hurricane intensity predic- tions for DeepExtrema, sorted in increasing block maxima values. Ground truth values are shown in red. predicted values generated by DeepExtrema on the test set of the hurricane intensity data. The results suggest that DeepExtrema can accurately predict the hurricane intensi- ties for a wide range of values, especially those below 140 knots. DeepExtrema also does a better job at predicting the high intensity hurricanes compared to EVL [Ding et al., 2019], as illustrated in Figure 5. Figure 6 shows the 90% conﬁdence interval of the predictions. Apart from the point and quantile estimations, DeepExtrema can also estimate the GEV parameter values for each hurricane. Ablation Studies The hyperparameter λ1 in the objective function of DeepExtrema denotes the trade-off between regularized GEV loss and RMSE loss. Experimental results show that the RMSE of block maxima prediction decreases when λ1 in- creases (see Table 3). This validates the importance of incor- porating GEV theory to improve the accuracy of block max- ima estimation instead of using the mean squared loss alone. Hyperparameter RMSE of Block maxima Hurricanes Ausgrid Weather λ1 = 0.0 13.28 0.68 2.52 λ1 = 0.5 13.03 0.63 2.41 λ1 = 0.9 12.81 0.64 2.27 Table 3: Effect of λ1 on RMSE of block maxima prediction. 5 Conclusion This paper presents a novel deep learning framework called DeepExtrema that combines extreme value theory with deep learning to address the challenges of predicting ex- tremes in time series. We offer a reformulation and re- parameterization technique for satisfying constraints as well as a model bias offset technique for proper model initial- ization. We evaluated our framework on synthetic and real- world data and showed its effectiveness. For future work, we plan to extend the formulation to enable more complex deep learning architectures such as those using an attention mech- anism. In addition, the framework will be extended to model extremes in spatio-temporal data. 6 Acknowledgments This research is supported by the U.S. National Science Foun- dation under grant IIS-2006633. Any use of trade, ﬁrm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government. References [Aliabadi et al., 2020] Majid Moradi Aliabadi, Hajar Emami, Ming Dong, and Yinlun Huang. Attention-based recurrent neural network for multistep-ahead predic- tion of process performance. Computers & Chemical Engineering, 140:106931, 2020. [Aus, 2013] Solar home electricity data, Dec 2013. [Bai et al., 2018] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An Empirical Evaluation of Generic Convolu- tional and Recurrent Networks for Sequence Modeling. arXiv:1803.01271 [cs], April 2018. arXiv: 1803.01271. [Bishop, 2006] Christopher M Bishop. Pattern recognition. Springer, New York, 2006. [Coles et al., 2001] Stuart Coles, Joanna Bawa, Lesley Tren- ner, and Pat Dorazio. An introduction to statistical model- ing of extreme values, volume 208. Springer, 2001. [Ding et al., 2019] Daizong Ding, Mi Zhang, Xudong Pan, Min Yang, and Xiangnan He. Modeling extreme events in time series prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discov- ery & Data Mining, pages 1114–1122, 2019. [Kharin and Zwiers, 2005] Viatcheslav V Kharin and Fran- cis W Zwiers. Estimating extremes in transient climate change simulations. Journal of Climate, 18(8):1156–1173, 2005. [Landsea and Franklin, 2013] Christopher W Landsea and James L Franklin. Atlantic hurricane database uncer- tainty and presentation of a new database format. Monthly Weather Review, 141(10):3576–3592, 2013. [Laptev et al., 2017] Nikolay Laptev, Jason Yosinski, Li Er- ran Li, and Slawek Smyl. Time-series extreme event fore- casting with neural networks at uber. In International con- ference on machine learning, volume 34, pages 1–5, 2017. [Masum et al., 2018] Shamsul Masum, Ying Liu, and John Chiverton. Multi-step time series forecasting of electric load using machine learning models. In International con- ference on artiﬁcial intelligence and soft computing, pages 148–159. Springer, 2018. [Muthukumar, 2017] J Muthukumar. Weather dataset, Dec 2017. [Peng et al., 2018] Chenglei Peng, Yang Li, Yao Yu, Yu Zhou, and Sidan Du. Multi-step-ahead host load pre- diction with gru based encoder-decoder in cloud comput- ing. In 2018 10th International Conference on Knowledge and Smart Technology (KST), pages 186–191. IEEE, 2018. [Polson and Sokolov, 2020] Michael Polson and Vadim Sokolov. Deep learning for energy markets. Applied Stochastic Models in Business and Industry, 36(1):195– 209, 2020. [Sagheer and Kotb, 2019] Alaa Sagheer and Mostafa Kotb. Time series forecasting of petroleum production using deep lstm recurrent networks. Neurocomputing, 323:203– 213, 2019. [Smith, 1985] Richard L. Smith. Maximum likelihood es- timation in a class of nonregular cases. Biometrika, 72(1):67–90, 1985. [US GAO, 2020] US GAO. Natural disasters: Economic ef- fects of hurricanes katrina, sandy, harvey, and irma. https: //www.gao.gov/products/gao-20-633r, 2020. Accessed: 2021-04-09. [Wang and Tan, 2021] Ding Wang and Pang-Ning Tan. Jo- han: A joint online hurricane trajectory and intensity fore- casting framework. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1677–1685, 2021. [Wang et al., 2020] Bin Wang, Tianrui Li, Zheng Yan, Guangquan Zhang, and Jie Lu. Deeppipe: A distribution- free uncertainty quantiﬁcation approach for time series forecasting. Neurocomputing, 397:11–19, 2020. [Wilson et al., 2022] Tyler Wilson, Pang-Ning Tan, and Lifeng Luo. Deepgpd: A deep learning approach for mod- eling geospatio-temporal extreme events. In Proceedings of the 36th AAAI Conference on Artiﬁcial Intelligence, 2022. [Yang et al., 2015] Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, and Shonali Krishnaswamy. Deep convolutional neural networks on multichannel time series for human activity recognition. In Twenty-fourth international joint conference on artiﬁcial intelligence, 2015. [Zhang et al., 2019] Xuan Zhang, Xun Liang, Aakas Zhiyuli, Shusen Zhang, Rui Xu, and Bo Wu. At-lstm: An attention-based lstm model for ﬁnancial time series pre- diction. In IOP Conference Series: Materials Science and Engineering, volume 569, page 052037. IOP Publishing, 2019. [Zhao et al., 2017] Bendong Zhao, Huanzhang Lu, Shangfeng Chen, Junliang Liu, and Dongya Wu. Convolutional neural networks for time series classiﬁca- tion. Journal of Systems Engineering and Electronics, 28(1):162–169, 2017.","libVersion":"0.3.1","langs":""}
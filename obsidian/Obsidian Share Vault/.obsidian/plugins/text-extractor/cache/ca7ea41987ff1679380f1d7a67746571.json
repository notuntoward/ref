{"path":"lit/lit_sources/Kallus23StochasticOptimizationForests.pdf","text":"Stochastic Optimization Forests Nathan Kallus*1 Xiaojie Mao* 2 1Cornell University, New York, NY 10044, kallus@cornell.edu 2School of Economics and Management, Tsinghua University, Beijing 100084, China, maoxj@sem.tsinghua.edu.cn We study contextual stochastic optimization problems, where we leverage rich auxiliary observations (e.g., product characteristics) to improve decision making with uncertain variables (e.g., demand). We show how to train forest decision policies for this problem by growing trees that choose splits to directly optimize the downstream decision quality, rather than split to improve prediction accuracy as in the standard random forest algorithm. We realize this seemingly computationally intractable problem by developing approximate splitting criteria that utilize optimization perturbation analysis to eschew burdensome re-optimization for every candidate split, so that our method scales to large-scale problems. We prove that our splitting criteria consistently approximate the true risk and that our method achieves asymptotic optimality. We extensively validate our method empirically, demonstrating the value of optimization-aware construction of forests and the success of our eﬃcient approximations. We show that our approximate splitting criteria can reduce running time hundredfold, while achieving performance close to forest algorithms that exactly re-optimize for every candidate split. Key words : Contextual stochastic optimization, Decision-making under uncertainty with side observations, Random forests, Perturbation analysis History : First posted version: July, 2020. This version: January, 2022. 1. Introduction In this paper we consider the contextual stochastic optimization (CSO) problem, z∗(x) ∈ arg min z∈Z E [c(z; Y ) | X = x] , (1) Z = { z ∈ Rd : hk(z) = 0, k = 1, . . . , s, hk(z) ≤ 0, k = s + 1, . . . , m } , (2) wherein, having observed contextual features X = x ∈ X ⊆ Rp, we seek a decision z ∈ Z to minimize average costs, which are impacted by a yet-unrealized uncertain variable Y ∈ Y. Equation (1) is essentially a stochastic optimization problem (Shapiro et al. 2014) where the distribution of the uncertain variable is given by the conditional distribution of Y | X = x. Crucially, this corresponds to using the observations of features X = x to best possibly control total average costs over new realizations of pairs (X, Y ); that is, E[c(z∗(X); Y )] = minz(x):Rp→Z E[c(z(X); Y )]. * Alphabetical order. 1arXiv:2008.07473v6 [math.OC] 16 Mar 2022 2 Kallus and Mao: Stochastic Optimization Forests x1 ≤ 4 x2 ≤ 6 τ1(x) = 1 τ1(x) = 2 τ1(x) = 3 (a) A depth-3 tree. When the condition in a branching node holds, we take the left branch. 0 2 4 6 8 0 2 4 6 8 1(x) = 1 1(x) = 2 1(x) = 3 (b) Each tree gives a partition of Rd, where each region cor- responds to a leaf of the tree. 0 2 4 6 8 0 2 4 6 8 (c) Darker regions fall into the same region as x = (0, 0) for more trees in a forest. Figure 1 A forest of trees, F = {τ1, . . . , τT }, parameterizes a forest policy ˆz(x) for CSO as in Eq. (3). Stochastic optimization can model many managerial decision-making problems in inventory man- agement (Simchi-Levi et al. 2005), revenue management (Talluri and Van Ryzin 2006), ﬁnance (Cornuejols and T¨ut¨unc¨u 2006), and other application domains (Kleywegt and Shapiro 2001, Shapiro et al. 2014). And, CSO in particular captures the interplay of such decision models with the availability of rich side observations of other variables (i.e., covariates X) often present in modern datasets, which can help signiﬁcantly reduce uncertainty and improve performance compared to unconditional stochastic optimization (Bertsimas and Kallus 2014). Since the exact joint distribution of (X, Y ), which speciﬁes the CSO in Eq. (1), is generally unavailable, we are in particular interested in learning a well-performing policy ˆz(x) based on n independent and identically distributed (i.i.d.) draws from the joint distribution of (X, Y ): Data : D = {(X1, Y1), . . . , (Xn, Yn)}, (Xi, Yi) ∼ (X, Y ) i.i.d. The covariates X may be any that can help predict the value of the uncertain variable Y aﬀecting costs so that we can reduce uncertainty and improve performance. A common approach is to ﬁrst make predictions using models that are trained without consideration of the downstream decision-making problem and then solve optimization given their plugged-in predictions. However, this approach completely separates prediction and optimization. Since all predictive models make errors, especially when learning a complex object such as the conditional distribution of Y given X, the error trade-oﬀs of this approach may be undesirable for the end task of decision-making. In this paper we aim to learn eﬀective forest-based CSO policies that integrate prediction and optimization. To make a decision at a new query point x, a forest policy uses a forest F = {τ1, . . . , τT } of trees τj to reweight the sample to emphasize data points i with covariates Xi “close” to x. Each tree, Kallus and Mao: Stochastic Optimization Forests 3 τj : Rp → {1, . . . , Lj}, is a partition of R p into Lj regions, where the function τj takes the form of a binary tree with internal nodes splitting on the value of a component of x (see Figs. 1a and 1b). We then reweight each data point i in the sample by the frequency wi(x) with which Xi ends up in the same region (tree leaf) as x, over trees in the forest (see Fig. 1c). Using these weights, we solve a weighted sample analogue of Eq. (1). That is, a forest policy has the following form, where the forest F constitutes the parameters of the policy ˆz(x): ˆz(x) ∈ arg min z∈Z n∑ i=1 wi(x)c(z; Yi), wi(x) := 1 T T∑ j=1 I [τj(Xi) = τj(x)] ∑n i′=1 I [τj(Xi′) = τj(x)] . (3) Bertsimas and Kallus (2014) considered using a forest policy where the forest F is given by running the random forest (RandForest) algorithm (Breiman 2001). The RandForest algorithm, however, builds trees that target the prediction problem of learning E [Y | X = x], rather than the CSO prob- lem in Eq. (1). Namely, it builds each tree τj by, starting with all of R p, recursively subpartitioning each region R0 ⊆ Rp into the two subregions R0 = R1 ∪R2 that minimize the sum of squared distance to the mean of data in each subregion (i.e., ∑ j=1,2 minz∈Rd ∑ i:Xi∈Rj ∥z − Yi∥ 2 2). For prediction, ran- dom forests are notable for adeptly handling high-dimensional feature data non-parametrically as they only split on variables relevant to prediction, especially compared to other methods for gen- erating localized weights wi(x) like k-nearest neighbors and Nadaraya–Watson kernel regression. However, for CSO they might miss signals more relevant to the particular optimization structure in Eq. (1), deteriorating downstream policy performance in the actual decision-making problem. Athey et al. (2019) proposed a Generalized Random Forest (GenRandForest) algorithm to esti- mate roots of conditional estimating equations, which can be repurposed for unconstrained CSO problems by solving their ﬁrst order optimality conditions. Their splitting criteria are based on approximating the mean squared errors of equation root estimates, which again may fail to capture signals more important for the particular cost function in Eq. (1) when optimization is one’s aim. In this paper, we design new algorithms to construct decision trees and forests that directly target the CSO problem in Eq. (1). Speciﬁcally, we choose tree splits to optimize the cost of resulting decisions instead of standard impurity measures (e.g., sum of squared errors), thereby incorporating the general cost function c(z; Y ) and constraints Z into the tree construction. A similar idea was suggested in endnote 2 of Bertsimas and Kallus (2014) but is dismissed because it would be too computationally cumbersome to use this to evaluate many candidate splits in each node of each tree in a forest. In this paper, we solve this task in a computationally eﬃcient manner by leveraging a second-order perturbation analysis of stochastic optimization, resulting in eﬃcient and eﬀective large-scale forests tailored to the decision-making problem of interest that lead to strong performance gains in practice. 4 Kallus and Mao: Stochastic Optimization Forests Our contributions are as follows. We formalize the oracle splitting criterion for recursively parti- tioning trees to target the CSO problem and then use second-order perturbation analysis to show how to approximate the intractable oracle splitting criterion by extrapolating from the given region, R0, to the candidate subregions, R1, R2, provided that the CSO problem is suﬃciently smooth. We do this in Section 2 for the unconstrained setting and in Section 3 for the constrained setting. Speciﬁcally, we consider both an approach that extrapolates the optimal value and an approach that extrapolates the optimal solution. Crucially, our perturbation approach means that we only have to solve a stochastic optimization problem at the root region, R0, and then we can eﬃciently extrapolate to what will happen to average costs for any candidate subpartition of the root, allow- ing us to eﬃciently consider many candidate splits. Using these new eﬃcient approximate splitting criteria, we develop the stochastic optimization tree (StochOptTree) algorithm, which we then use to develop the stochastic optimization forest (StochOptForest) algorithm by running the former many times. The StochOptForest algorithm ﬁts forests to directly target the downstream decision- making problem of interest, and then uses these forests to construct eﬀective forest policies for CSO. In Section 4, we empirically demonstrate the success of our StochOptForest algorithm and the value of forests constructed to directly consider the downstream decision-making problem. In Section 5 we provide asymptotic optimality results for StochOptForest. In Section 6 we oﬀer a dis- cussion of and comparison to related literature and in Section 7 we oﬀer some concluding remarks. We extend our results to stochastically-constrained CSO problems in Appendix A, develop variable- importance measures in Appendix B, and provide additional empirical results in Appendix C. We defer all proofs to Appendix H. 1.1. Running Examples of CSOs We will have a few running examples of CSOs. Example 1 (Multi-Item Newsvendor). In the multi-item newsvendor problem we must choose the order quantities for d products, z = (z1, . . . , zd), before we observe the random demand for each of these, Y = (Y1, . . . , Yd), in order to control holding and backorder costs. Whenever the order quantity for product l exceeds the demand for the product we pay a holding cost of αl per unit. And, whenever the demand exceeds the order quantity, we pay a backorder cost of βl per unit. The total cost is c(z; y) = ∑d l=1 max{αl(zl − yl), βl(yl − zl)}. (4) Negating and adding a constant we can also consider this equivalently as the sale revenue up to the smaller of zl and yl, minus ordering costs for zl units. The order quantities may be unrestricted (in which case the d problems decouple). They may be restricted by a capacity constraint, Z = { z ∈ Rd : d∑ l=1 zl ≤ C, zl ≥ 0, l = 1, . . . , d } , Kallus and Mao: Stochastic Optimization Forests 5 where C is a constant that stands for the inventory capacity limit. Covariates X in this problem may be any that can help predict future demand. For example, for predicting demand for home video products, Bertsimas and Kallus (2014) use data from Google search trends, data from online ratings, and past sales data. Example 2 (Variance-based Portfolio Optimization). Consider d assets with random future returns Y = (Y1, . . . , Yd), and decision variables z = (z1, . . . , zd) that represent the fraction of investment in each asset in a portfolio of investments, constrained to be in the simplex ∆d = {z ∈ Rd : ∑d l=1 zl = 1, zl ≥ 0, l = 1, . . . , d}. Then the return of the portfolio is Y ⊤z. We want the portfolio z(x) to minimize the variance of the return given X = x. This can be formulated as a CSO by introducing an additional unconstrained auxiliary optimization variable zd+1 ∈ R and letting c(z; y) = (y⊤z1:d − zd+1)2 . (5) We can either let Z = ∆ d × R or relax nonnegativity constraints to allow short selling. More generally we may consider optimizing a linear combination of the conditional mean and variance of the return, which corresponds to a CSO with the following cost function: c(z; y) = (y⊤z1:d − zd+1)2 − ρy⊤z1:d, ρ > 0. (6) Covariates X in this problem may be any that can help predict future returns. Examples include past returns, stock fundamentals, economic fundamentals, news stories, etc. Example 3 (CVaR-based Portfolio Optimization). When the asset return distributions are not elliptically symmetric, Conditional Value-at-Risk (CVaR) may be a more suitable risk measure than variance (Rockafellar et al. 2000). We may therefore prefer to consider minimizing the CVaR at level α given X = x, deﬁned as CVaRα(Y ⊤z | X = x) = min w∈R E [ 1 α max {w − Y ⊤z, 0 } − w | X = x] . This again can be formulated as a CSO by introducing an additional unconstrained auxiliary optimization variable zd+1 ∈ R and letting c(z; y) = 1 α max {zd+1 − y⊤z1:d, 0 } − zd+1. (7) We can analogously incorporate the simplex constraint or relax the nonnegativity constraint as in Example 2. We can also optimize a weighted combination of the diﬀerent criteria (mean, variance, CVaR at any level); we need only introduce a separate auxiliary variable for variance and for CVaR at each level considered. 6 Kallus and Mao: Stochastic Optimization Forests Example 4 (Prediction of Conditional Expectation). While the above provides exam- ples of actual decision-making problems, the problem of prediction also ﬁts into the CSO framework as a special case. Namely, if Y ∈ R d, c(z; y) = 1 2 ∥z − y∥ 2 2, and Z = Rd is unconstrained, then we can see that z∗(x) = E [Y | X = x]. This can be understood as the best-possible (in squared error) prediction of Y in a draw of (X, Y ) where only X is revealed. Fitting forest models to predict E [Y | X = x] is precisely the target task of random forests, which use squared error as a splitting criterion. We further compare to other literature on estimation using random forests in Section 6.1. A key aspect of handling general CSOs, as we do, is dealing with general cost functions and con- straints and targeting the expected cost of our decision rather than the error in estimating z∗(x). 2. The Unconstrained Case We begin by studying the unconstrained case as it is simpler and therefore more instructive. Throughout this section, we let Z = Rd. We extend to the more general constrained case in Sec- tion 3. To develop our StochOptForest algorithm, we start by considering the StochOptTree algo- rithm, which we will then run many times to create our forest. To motivate our StochOptTree algorithm, we will ﬁrst consider an idealized splitting rule for an idealized policy, then consider approximating it using perturbation analysis, and then consider estimating the approximation using data. Each of these steps constitutes one of the next subsections. 2.1. The Oracle Splitting Rule Given a partition, τ : Rp → {1, . . . , L}, of R p into L regions, consider the policy zτ (x) ∈ arg minz∈Z E [c(z; Y )I [τ (X) = τ (x)]] that, for each x, optimizes costs only for (X, Y ) where X falls in the same region as x. Note that this policy is hypothetical and not implementable in practice given just the data as it involves the true joint distribution of (X, Y ). We wish to learn a partition τ described by a binary decision tree with nodes of the form “xj ≤ θ?” such that it leads to a well-performing policy zτ (x), that is, has small risk E [c(zτ (X); Y )]. Finding the best τ over all trees of a given depth is generally a very hard problem, even if we knew the distributions involved. To simplify it, suppose we ﬁx a partition τ and we wish only to reﬁne it slightly by taking one of its regions, say R0 = τ −1(L), and choosing some j ∈ {1, . . . , p}, θ ∈ R to construct a new partition τ ′ with τ ′(x) = τ (x) for x /∈ R0, τ ′(x) = L for x ∈ R1 = R0 ∩ {x ∈ Rp : xj ≤ θ}, and τ ′(x) = L + 1 for x ∈ R2 = R0 ∩ {x ∈ Rp : xj > θ}. That is, we further subpartition the region R0 into the subre- gions R1, R2. We would then be interested in ﬁnding the choice of (j, θ) leading to minimal risk, E [c(zτ ′(X); Y )] = E [c(zτ ′(X); Y )I [X /∈ R0]] + E [c(zτ ′(X); Y )I [X ∈ R1]] + E [c(zτ ′(X); Y )I [X ∈ R2]]. Notice that the ﬁrst term is constant in the choice of the subpartition and only the second and Kallus and Mao: Stochastic Optimization Forests 7 third terms matter in choosing the subpartition. We should therefore seek the subpartition that leads to the minimal value of Coracle(R1, R2) = ∑ j=1,2 E [c(zτ ′(X); Y )I [X ∈ Rj]] = ∑ j=1,2 minz∈Z E [c(z; Y )I [X ∈ Rj]] , (8) where the last equality holds because the tree policy zτ ′ makes the best decision within each region of the new partition. We call this the oracle splitting criterion. Searching over choices of (j, θ) in some given set of possible options, the best reﬁnement of τ is given by the choice minimizing this criterion. If we start with the trivial partition, τ (x) = 1 ∀x, then we can recursively reﬁne it using this procedure in order to grow a tree of any desired depth. When c(z; y) = 1 2 ∥z − y∥ 2 2 and the criterion is estimated by replacing expectations with empirical averages, this is precisely the regression tree algorithm of Breiman et al. (1984), in which case the estimated criterion is easy to compute as it is simply given by rescaled within-region variances of Yi. For general c(z; y), however, computing the criterion involves solving a general stochastic optimization problem that may have no easy analytical solution (even if we approximate expectations with empirical averages) and it is therefore hard to do quickly for many, many possible candidates for (j, θ), and correspondingly it would be hard to grow large forests of many of these trees. 2.2. Perturbation Analysis of the Oracle Splitting Criterion Consider a region R0 ⊆ Rp and its candidate subpartition R0 = R1 ∪ R2, R1 ∩ R2 = ∅. Let vj(t) = min z∈Z f0(z) + t (fj(z) − f0(z)) , zj(t) ∈ arg min z∈Z f0(z) + t (fj(z) − f0(z)), (9) where fj(z) = E [c(z; Y ) | X ∈ Rj] , j = 0, 1, 2, t ∈ [0, 1]. The optimization objective function in Eq. (9) is obtained from perturbing the objective function f0 (z) in the region R0 towards the objective function fj (z) in a subregion Rj for j = 1, 2. The perturbation magnitude is quantiﬁed by the parameter t ∈ [0, 1]. Note that the optimal values of fully perturbed problems (i.e., t = 1) in two subregions determine the oracle splitting criterion: Coracle(R1, R2) = p1v1(1) + p2v2(1), where pj = P (X ∈ Rj) , (10) and ideally we would use these values to evaluate the quality of the subpartition. But we would rather not have to solve the stochastic optimization problem involved in Eq. (9) at t = 1 repeatedly for every candidate subpartition. Instead, we would rather solve the single problem v1(0) = v2(0), i.e., the problem corresponding to the region R0, and try to extrapolate from there what happens as we take t → 1, i.e., the limiting problem corresponding to each subregion Rj for each candidate split. To solve this, we consider the perturbation of the problem vj(t) at t = 0 as we increase it 8 Kallus and Mao: Stochastic Optimization Forests inﬁnitesimally and use this to approximate vj(1). As long as the distribution of Y | X ∈ R0 is not too diﬀerent from that of Y | X ∈ Rj, this would be a reasonable approximation. First, we note that a ﬁrst-order perturbation analysis would be insuﬃcient. We can show that under appropriate continuity conditions and if arg minz∈Z f0(z) = {z0} is a singleton, we would have vj(t) = (1 − t)f0(z0) + tfj(z0) + o(t). 1 We could use this to approximate vj(1) ≈ fj(z0) by plugging in t = 1 and ignoring the higher-order terms, which makes intuitive sense: if we only perturb the objective slightly, the optimal solution is approximately unchanged and we only need to evaluate its new objective value. This would lead to the approximate splitting criterion p1v1(1) + p2v2(1) ≈ p1f1(z0) + p2f2(z0). However, since p1f1(z0) + p2f2(z0) = p0f0(z0), this is ultimately unhelpful as it does not at all depend on the choice of subpartition. Instead, we must conduct a ﬁner, second-order perturbation analysis in order to understand the eﬀect of the choice of subpartition on risk. The next result does this for the unconstrained case. Theorem 1 (Second-Order Perturbation Analysis: Unconstrained). Fix j = 1, 2. Sup- pose the following conditions hold: 1. f0(z) and fj(z) are twice continuously diﬀerentiable; 2. The inf-compactness condition: there exist constants α and t0 ∈ (0, 1] such that the sublevel sets { z ∈ Rd : f0(z) + t (fj(z) − f0 (z)) ≤ α} are nonempty and uniformly bounded for t ∈ [0, t0); 3. f0(z) has a unique minimizer z0 over Rd, and ∇2f0(z0) is positive deﬁnite; Then vj(t) = (1 − t)f0(z0) + tfj(z0) − 1 2 t2∇fj(z0)⊤ (∇2f0(z0))−1 ∇fj(z0) + o(t 2), (11) zj(t) = z0 − t (∇ 2f0(z0))−1 ∇fj(z0) + o(t). (12) Theorem 1 gives the second order expansion of the optimal value vj(t) and the ﬁrst order expan- sion of any choice of zj(t) that attains vj(t) around t = 0. These expansions quantify the impact on the optimal value and optimal solution when inﬁnitesimally perturbing the objective function in region R0 towards that in a subregion Rj. One crucial condition of Theorem 1 is that the objec- tive functions are suﬃciently smooth (condition 1). This condition holds for any subpartition if we assume that E [c(z; Y ) | X] is almost surely twice continuously diﬀerentiable, which is trivially satisﬁed if the cost function c(z; Y ) is a almost surely twice continuously diﬀerentiable function of z. However, even if c(z; Y ) is nonsmooth (e.g., Examples 1 and 3), E [c(z; Y ) | X] may still be suﬃciently smooth if the distribution of Y | X is continuous (see examples in Section 2.3 below). In particular, one reason we deﬁned the oracle splitting criterion using the population expectation 1 This is, for example, a corollary of Theorem 1, although weaker continuity conditions would be needed for this ﬁrst-order statement. We omit the details as the ﬁrst-order analysis is ultimately not useful. Kallus and Mao: Stochastic Optimization Forests 9 rather than empirical averages is that for many relevant examples such as newsvendor and CVaR only the population objective may be smooth while the sample objective may be nonsmooth and therefore not amenable to perturbation analysis. Condition 2 ensures that if we only slightly perturb the objective function f0, optimal solutions of the resulting perturbed problem are always bounded, and never escape to inﬁnity. This means that without loss of generality we can restrict our attention to a compact subset of Rd. This compactness condition and the smoothness condition (condition 1) together ensure the existence of optimal solutions for any optimization problem corresponding to t ∈ [0, t0). In addition, this condition is crucial for ensuring z(t) → z0 as t → 0 (Bonnans and Shapiro 2000, Proposition 4.4). One suﬃcient condition for this is that any optimal solution z∗(X) in Eq. (1) is almost surely bounded, e.g., when conditional quantiles of all item demands in Example 1 are almost surely bounded. Finally, the regularity condition (condition 3) is obviously satisﬁed if f0(z) is strictly convex, which is implied if either E [c(z; Y ) | X] or c(z; Y ) is almost surely strictly convex. Condition 3 may be satisﬁed even if the cost function c(z; Y ) is not strictly convex: e.g., it holds for the newsvendor problem (Example 1) when the density of Yl | X ∈ R0 is positive at z0 for all l = 1, . . . , d. 2.3. Approximate Splitting Criteria Theorem 1 suggests two possible approximations of the oracle splitting criterion. Approximate Risk Criterion. If we use Eq. (11) to extrapolate to t = 1, ignoring the higher- order terms, we arrive at vj(1) ≈ fj(z0) − 1 2 ∇fj(z0)⊤ (∇2f0(z0))−1 ∇fj(z0). Taking a weighted average of this over j = 1, 2, we arrive at an approximation of the oracle splitting criterion Coracle in Eq. (8). Since p1f1(z0) + p2f2(z0) = p0f0(z0) is constant in the subpartition, we may ignore these terms, leading to the following criterion: Capx-risk(R1, R2) = − 1 2 ∑ j=1,2 pj∇fj(z0)⊤ (∇ 2f0(z0))−1 ∇fj(z0). (13) By strengthening the conditions in Theorem 1, we can in fact show that this approximation becomes arbitrarily accurate as the partition becomes ﬁner. Theorem 2. Suppose the following conditions hold for both j = 1, 2: 1. Condition 1 of Theorem 1. 2. Condition 2 of Theorem 1 holds for all t ∈ [0, 1]. 3. f0(z)+t (fj(z) − f0 (z)) has a unique minimizer z0 and ∇2 (f0(z) + t (fj(z) − f0 (z))) is positive deﬁnite at this unique minimizer for all t ∈ [0, 1]. 10 Kallus and Mao: Stochastic Optimization Forests 4. E [c(z; Y ) | X = x] is twice Lipschitz-continuously diﬀerentiable in x. Then ∣ ∣Coracle(R1, R2) − p0f0(z0) − Capx-risk(R1, R2)∣ ∣ = o(D2 0), where D0 = supx,x′∈R0 ∥x − x′∥2 is the diameter of R0. Again, note that p0f0(z0) is constant in the choice of subpartition. Approximate Solution Criterion. Since vj(1) = fj(zj(1)), we can also approximate vj(1) by approximating zj(1) and plugging it in. Using Eq. (12) to extrapolate zj(t) to t = 1 and ignoring the higher-order terms, we arrive at the following approximate criterion: Capx-soln(R1, R2) = ∑ j=1,2 pjfj (z0 − (∇2f0(z0))−1 ∇fj(z0)) . (14) Notice this almost looks like applying a Newton update to z0 in the minz fj(z) problem, namely, the solution that optimizes the second order expansion of fj(z) at z0. However, a naive Newton update will require to invert the Hessian for fj, which varies across diﬀerent candidate splits. In contrast, the criterion Capx-soln requires the Hessian for f0, meaning we only have to invert a Hessian once for all candidate subpartitions. For unconstrained CSO problems in this section, we may also apply the GenRandForest algo- rithm in Athey et al. (2019) to solve their ﬁrst order optimality condition. The GenRandForest algorithm uses a similar way to approximate optimal solutions in split subregions. It chooses splits to maximize the diﬀerence between approximate solutions in two subregions induced by each can- didate split, as their proposition 1 shows that this approximately minimizes the total mean squared errors of the resulting estimated optimal solutions. In contrast, by using Capx-soln, we choose splits to minimize the expected cost of the approximate optimal solutions, thereby directly targeting the ultimate objective in CSO problems. More importantly, we tackle the constrained case (Section 3) while the GenRandForest algorithm cannot. In Section 4 and Appendix C.1, we show the impact of both of these diﬀerences can be signiﬁcant in practice when optimization is the aim. In the following theorem, we show that the approximate solution criterion also becomes arbi- trarily accurate as the partition becomes ﬁner. Theorem 3. Suppose the assumptions of Theorem 2 hold. Then ∣ ∣Coracle(R1, R2) − Capx-soln(R1, R2)∣ ∣ = o(D2 0). Kallus and Mao: Stochastic Optimization Forests 11 Revisiting the Running Examples. The approximate criteria above crucially depend on the gradients ∇f1(z0), ∇f2(z0) and Hessian ∇2f0(z0). We next study these quantities for some examples. Example 5 (Derivatives with Smooth cost). If c(z; y) is itself twice continuously diﬀer- entiable for every y, then under regularity conditions that enable the exchange of derivative and expectation (e.g., |(∇c (z; Y ))ℓ| ≤ W for all z in a neighborhood of z0 with E [W | Xi ∈ Rj] < ∞), we have ∇fj(z0) = E [∇c (z0; Y ) | Xi ∈ Rj] and ∇2f0(z0) = E [∇2c (z0; Y ) | Xi ∈ R0]. Example 1, Cont’d (Derivatives in Multi-Item Newsvendor). In many cases, c(z; y) is not smooth, as in the example of the multi-item newsvendor cost in Eq. (4). In this case, it suﬃces that the distribution of Y | X ∈ Rj is continuous for gradients and Hessians to exist. Then, we can show that (∇fj(z0))l = (αl + βl)P (Yl ≤ z0,l | X ∈ Rj) − βl and (∇2f0(z0))ll = (αl + βl)µ0,l(z0) for l = 1, . . . , d, j = 1, 2, and (∇2f0(z0))ll′ = 0 for l ̸= l′, where µ0,l is the density function of Yl | X ∈ R0. So, ∇2f0(z0) is invertible as long as µ0,l(z0) > 0 for l = 1, . . . , d. Example 2, Cont’d (Derivatives in Variance-based Portfolio Optimization). The cost function in Eq. (5) is an instance of Example 5 (smooth costs). Using block notation to separate the ﬁrst d decision variables from the ﬁnal single auxiliary variable, we verify in Proposition 10 that ∇fj(z0) = 2 [ E [Y Y ⊤ | X ∈ Rj] z0,1:d − E [Y | X ∈ Rj] z0,d+1 z⊤ 0,1:d (E [Y | X ∈ R0] − E [Y | X ∈ Rj]) ] , (15) ∇2f0(z0) = 2 [ E [Y Y ⊤ | X ∈ R0] −E [Y | X ∈ R0] −E [Y ⊤ | X ∈ R0] 1 ] . (16) Notice ∇2f0(z0) is invertible if and only if the covariance matrix Var (Y | X ∈ R0) is invertible. Example 3, Cont’d (Derivatives in CVaR-based Portfolio Optimization). Like the newsvendor cost in Eq. (4), the CVaR cost in Eq. (7) is not smooth either. Again we assume that the distribution of Y | X ∈ Rj is continuous. Then, when z0 ̸= 0 (Proposition 11 in Appendix G), ∇fj(z0) = 1 α [ −E [ Y I [ Y 0 ≤ qα 0 (Y 0) ] | X ∈ Rj] P (qα 0 (Y 0) − Y 0 ≥ 0 | X ∈ Rj) − α ] , Y 0 := Y ⊤z0,1:d (17) ∇2f0(z0) = µ0 (qα 0 (Y 0)) α [ E [ Y Y ⊤ | Y 0 = qα 0 (Y 0), X ∈ R0] −E [ Y | Y 0 = qα 0 (Y 0), X ∈ R0] −E [Y ⊤ | Y 0 = qα 0 (Y 0), X ∈ R0] 1 ] . (18) where µ0 is the density function of Y 0 given X ∈ R0, and qα 0 (Y 0) as the α-level quantile of Y 0 given X ∈ R0. Notice that the Hessian matrix ∇2f0(z0) may not necessarily be invertible. This arises due to the homogeneity of returns in scaling the portfolio, so that second derivatives in this direction may vanish. This issue is corrected when we consider the constrained case where we ﬁx the scale of the portfolio (see Section 3).2 2 Indeed the unconstrained case for the portfolio problem is in fact uninteresting: the zero portfolio gives minimal variance, and CVaR may be sent to inﬁnity in either direction by inﬁnite scaling. 12 Kallus and Mao: Stochastic Optimization Forests Re-optimizing auxiliary variables. In Examples 2 and 3, z contains both auxiliary variables and decision variables, and we construct the approximate criteria based on gradients and Hessian matrix with respect to both sets of variables. A natural alternative is to re-optimize the auxil- iary variables ﬁrst so that the objective only depends on decision variables, and then evaluate the corresponding gradients and Hessian matrix. That is, if we partition z = (zdec, zaux), then we can re-deﬁne fj(zdec) = minzaux E [c((zdec, zaux); Y ) | X ∈ Rj] and zdec 0 = arg minzdec f0(zdec). The pertur- bation analysis remains largely the same by simply using the gradients and Hessian matrix for the redeﬁned fj(zdec) at zdec 0 . This leads to an alternative approximate splitting criterion. However, evaluating the gradients ∇fj(zdec 0 ) for j = 1, 2 would now involve repeatedly ﬁnding the optimal solution arg minzaux E [c((zdec 0 , zaux); Y ) | X ∈ Rj] for all candidate splits. See Appendix E for details. Since the point of our approximate criteria is to avoid re-optimization for every candidate split, this alternative is practically relevant only when re-optimizing the auxiliary variables is very computationally easy. For example, in Example 2, zdec corresponds to the ﬁrst d variables and re-optimizing the auxiliary (d + 1)th variable amounts to computing the mean of Y ⊤zdec 0 in each subregion, which can be done quite eﬃciently as we vary the candidate splits. 2.4. Estimating the Approximate Splitting Criteria The beneﬁt of our approximate splitting criteria, Capx-soln(R1, R2), Capx-risk(R1, R2) in Eqs. (13) and (14), is that they only involve the solution of z0. Thus, if we want to evaluate many diﬀerent subpartitions of R0, we need only solve for z0 once, compute (∇2f0(z0)) −1 once, and then only re-compute ∇fj(z0) for each new subpartition. Still, this involves quantities we do not actually know since all of these depend on the joint distribution of (X, Y ). We therefore next consider the estimation of these approximate splitting criteria from data. Given estimators ˆH0, ˆh1, ˆh0 of ∇2f0(z0), ∇f1(z0), ∇f2(z0), respectively (see examples below), we can construct the estimated approximate splitting criteria as ˆCapx-risk(R1, R2) = − ∑ j=1,2 nj n ˆh ⊤ j ˆH −1 0 ˆhj, (19) ˆCapx-soln(R1, R2) = ∑ j=1,2 1 n ∑n i=1 I [Xi ∈ Rj] c (ˆz0 − ˆH −1 0 ˆhj; Yi) , (20) where nj = ∑n i=1 I [Xi ∈ Rj]. Under appropriate convergence of ˆH0, ˆh1, ˆh2, these estimated criteria respectively converge to the population approximate criteria Capx-risk(R1, R2) and Capx-soln(R1, R2) in Section 2.3, as summarized by the following self-evident proposition. Proposition 1. If ∥ ˆH −1 0 − (∇2f0(z0))−1 ∥F = op(1), ∥ˆhj − ∇fj(z0)∥2 = Op(n−1/2) for j = 1, 2, then ˆCapx-risk(R1, R2) = Capx-risk(R1, R2) + Op(n−1/2). Kallus and Mao: Stochastic Optimization Forests 13 If also ∣ ∣ ∣ 1 n ∑n i=1 I [Xi ∈ Rj] c (ˆz0 − ˆH −1 0 ˆhj; Yi) − pjfj (z0 − (∇ 2f0(z0))−1 ∇fj(z0) )∣ ∣ ∣ = Op(n−1/2) for j = 1, 2, then ˆCapx-soln(R1, R2) = Capx-soln(R1, R2) + Op(n−1/2). If we can ﬁnd estimators that satisfy the conditions of Proposition 1, then together with Theorems 2 and 3, we will have shown that the estimated approximate splitting criteria can well approximate the oracle splitting criterion when samples are large and the partition is ﬁne. It remains to ﬁnd appropriate estimators. General Estimation Strategy. Since the gradients and Hessian to be estimated are evaluated at a point z0 that is itself unknown, a general strategy is to ﬁrst estimate z0 and then estimate the gradients and Hessian at this estimate. This is the strategy we follow in the examples below. Speciﬁcally, we can ﬁrst estimate z0 by its sample analogue: ˆz0 ∈ arg min z∈Z ̂p0f0(z), where ̂p0f0(z) := 1 n n∑ i=1 I [Xi ∈ R0] c(z; Yi). (21) Under standard regularity conditions, the estimated optimal solution ˆz0 above is consistent (see Lemma 1 in Appendix G). Then, given generic estimators ˆH0(z) of ∇2f0(z) at any one z and similarly estimators ˆhj(z) of ∇fj(z) for j = 1, 2, we let ˆH0 = ˆH0(ˆz0) and ˆhj = ˆhj(ˆz0). Examples of this follow. Revisiting the Running Examples. We next discuss examples of possible estimates ˆH0, ˆhj that can be proved to satisfy the conditions in Proposition 1 (see Propositions 12 to 15 in Appendix G for details). All of our examples use the general estimation strategy above. Example 5, Cont’d (Estimation with Smooth Cost). If c(z; y) is itself twice continu- ously diﬀerentiable in z for every y, we can simply use ˆH0(ˆz0) = 1 n0 ∑n i=1 I [Xi ∈ R0] ∇2c (ˆz0; Yi) and ˆhj(ˆz0) = 1 nj ∑n i=1 I [Xi ∈ Rj] ∇c (ˆz0; Yi). In Proposition 15 in Appendix G, we show that these satisfy the conditions of Proposition 1 thanks to the smoothness of c(z; y). Example 2 is one example of this case, which we discuss below. Example 4 is another example. In particular for the squared error cost function in Example 4 (c(z; y) = 1 2 ∥z − y∥2 2), we show in Proposition 9 in Appendix G that, using the above ˆH0, ˆhj, we have 1 n n∑ i=1 I [Xi ∈ R0] c(ˆz0; Yi) + ˆCapx-risk(R1, R2) = ˆCapx-soln(R1, R2) = ∑ j=1,2 nj 2n d∑ l=1 Var({Yi,l : Xi ∈ Rj}), which is exactly the splitting criterion used for regression by random forests, namely the sum of squared errors to the mean within each subregion. Notice the very ﬁrst term is constant in R1, R2. 14 Kallus and Mao: Stochastic Optimization Forests Example 1, Cont’d (Estimation in Multi-Item Newsvendor). In the previous section we saw that the gradient and Hessian depend on the cumulative distribution and density functions, respectively. We can therefore estimate the gradients by ˆhj,ℓ(ˆz0) = αl+βl nj ∑n i=1 I [Xi ∈ Rj, Yl ≤ ˆz0,l] − βl, and the Hessian using, for example, kernel density estimation: ˆH0,ll(ˆz0) = αl+βl nj b ∑n i=1 I [Xi ∈ R0] K((Yi,l − ˆz0,l)/b), where K is a kernel such as K(u) = I [ |u| ≤ 1 2 ] and b is the bandwidth, and ˆH0,ll′(ˆz0) = 0 for l ̸= l′. We show the validity of these estimates in Proposition 12 in Appendix G. Example 2, Cont’d (Estimation in Variance-based Portfolio Optimization). With ˆz0 = {ˆz0,1, . . . , ˆz0,d, ˆz0,d+1} given by solving the problem Eq. (21), the gradient and Hessian in Eqs. (15) and (16) can be estimated by their sample analogues: ˆhj(ˆz0) = 2 [ 1 nj ∑n i=1 I [Xi ∈ Rj] YiY ⊤ i ˆz0,1:d − 1 nj ∑n i=1 I [Xi ∈ Rj] Yi ˆz0,d+1 ˆz⊤ 0,1:d ( 1 n0 ∑n i=1 I [Xi ∈ R0] Yi − 1 nj ∑n i=1 I [Xi ∈ Rj] Yi) ] , ˆH0(ˆz0) = 2 [ 1 n0 ∑n i=1 I [Xi ∈ R0] YiY ⊤ i − 1 n0 ∑n i=1 I [Xi ∈ R0] Yi − 1 n0 ∑n i=1 I [Xi ∈ R0] Y ⊤ i 1 ] . These estimators are in fact speciﬁc examples of the general smooth case in Example 5, so they too can be analyzed by Proposition 15 in Appendix G. Example 3, Cont’d (Estimation in CVaR-based Portfolio Optimization). It is straightforward to estimate the gradient in Eq. (17): ˆhj(ˆz0) = 1 α [− 1 nj ∑n i=1 I [Y ⊤ i ˆz0,1:d ≤ ˆqα 0 (Y ⊤ ˆz0,1:d), Xi ∈ Rj] Yi 1 nj ∑n i=1 I [Y ⊤ i ˆz0,1:d ≤ ˆqα 0 (Y ⊤ ˆz0,1:d), Xi ∈ Rj] − α ] (22) where ˆqα 0 (Y ⊤ ˆz0,1:d) is the empirical α-level quantile of Y ⊤ ˆz0,1:d based on data in R0. The Hessian matrix in Eq. (18) is more challenging to estimate, since it involves many conditional expectations given the event Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d). In principle, we could estimate these nonparametrically by, for example, kernel smoothing estimators (e.g., Chen and Leng 2015, Fan and Yao 1998, Loubes et al. 2019, Yin et al. 2010). For simplicity and since this is only used as an approximate splitting criterion anyway, in our empirics we can consider a parametric approach instead, which we will use in our empirics in Section 4.1: if Y | X ∈ R0 has a Gaussian distribution N (m0, Σ0), then E [ Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] = m0 + Σ0z0,1:d (z⊤ 0,1:dΣ0z0,1:d)−1 (qα 0 (Y ⊤z0,1:d) − m⊤ 0 z0,1:d), (23) Var (Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0) = Σ0 − Σ0z0,1:d ( z⊤ 0,1:dΣ0z0,1:d)−1 z⊤ 0,1:dΣ0, (24) and E [Y Y ⊤ | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] can be directly derived from these two quantities. We can then estimate these quantities by plugging in ˆz0 for z0, the empirical mean estimator of Y for m0, the empirical variance estimator of Y for Σ0, and the empirical α-level quantile of Y ⊤ ˆz0,1:d Kallus and Mao: Stochastic Optimization Forests 15 Algorithm 1 Recursive procedure to grow a StochOptTree (unconstrained case) 1: procedure StochOptTree.Fit(region R0, data D, depth, id) 2: ˆz0 ← Minimize(∑ (Xi,Yi)∈D I [Xi ∈ R0] c(z; Yi), z ∈ Z) ◃ Solve Eq. (21) 3: ˆH0 ← Estimate ∇2f0(z) at z = ˆz0 4: CandSplit ← GenerateCandidateSplits(R0, D) ◃ Create the set of possible splits 5: ˆC ← ∞ 6: for (j, θ) ∈ CandSplit do ◃ Optimize the estimated approximate criterion 7: (R1, R2) ← (R0 ∩ {x ∈ Rp : xj ≤ θ}, R0 ∩ {x ∈ Rp : xj > θ}) 8: (ˆh1, ˆh2) ← Estimate ∇f1(z), ∇f2(z) at z = ˆz0 9: C ← ˆCapx-risk/apx-soln(R1, R2) ◃ Compute the criterion using ˆH0, ˆh1, ˆh2, D 10: if C < ˆC then ( ˆC, ˆj, ˆθ) ← (C, j, θ) 11: if Stop?((ˆj, ˆθ), R0, D, depth) then 12: return (x ↦→ id) 13: else 14: LeftSubtree ← StochOptTree.Fit(R0 ∩ {x ∈ Rp : xj ≤ θ}, D, depth + 1, 2id) 15: RightSubtree ← StochOptTree.Fit(R0 ∩ {x ∈ Rp : xj > θ}, D, depth + 1, 2id + 1) 16: return (x ↦→ xj ≤ θ ? LeftSubtree(x) : RightSubtree(x)) for qα 0 (Y ⊤z0,1:d), all based only on the data in R0. Finally, we can estimate µ0 (qα 0 (Y ⊤z0,1:d)) by a kernel density estimator 1 n0b ∑n i=1 I [Xi ∈ R0] K ((Y ⊤ i ˆz0,1:d − ˆqα 0 (Y ⊤ ˆz0,1:d)) /b). Although the Gaus- sian distribution may be misspeciﬁed, the resulting estimator is more stable than and easier to implement than nonparametric estimators (especially considering that it will be used repeatedly in tree construction) and it can still approximate the relative scale of entries in the Hessian matrix reasonably well. In Section 4.1, we empirically show that our method based on these approximate estimates works well even if the Gaussian model is misspeciﬁed. If it happens to be correctly spec- iﬁed, we can also theoretically validate that the estimator satisﬁes the conditions of Proposition 1 (see Proposition 14 in Appendix G). 2.5. The Stochastic Optimization Tree and Forest Algorithms With the estimated approximate splitting criteria in hand, we can now describe our StochOptTree and StochOptForest algorithms. Speciﬁcally, we will ﬁrst describe how we use our estimate approx- imate splitting criteria to build trees, which we will then combine to make a forest that leads to a CSO decision policy ˆz(x) as in Eq. (3). StochOptTree Algorithm. We summarize the tree construction procedure in Algorithm 1. We will extend Algorithm 1 to the constrained case in Section 3. This procedure partitions a 16 Kallus and Mao: Stochastic Optimization Forests generic region, R0, into two children subregions, R1, R2, by an axis-aligned cut along a certain coordinate of covariates. It starts with solving the optimization problem within R0 according to Eq. (21), and then ﬁnds the best split coordinate ˆj and cutoﬀ value ˆθ over a set of candidate splits by minimizing 3 the estimated approximate risk criterion in Eq. (19) or the estimated approximate solution criterion in Eq. (20). Once the best split (ˆj, ˆθ) is found, R0 is partitioned into the two subregions accordingly, and the whole procedure continues on recursively until a stopping criterion. There are a few subroutines to be speciﬁed. First, there is the optimization of ˆz0. Depending on the structure of the problem, diﬀerent algorithms may be appropriate. For example, if c(z; y) is the maximum of several linear functions, a linear programming solver may be used. More generally, since the objective has the form of a sum of functions, methods such as stochastic gradient descent (aka stochastic approximation; Nemirovski et al. 2009) may be used. Second, there is the estimation of ˆH0, ˆh1, ˆh2, which was discussed in Section 2.4. Third, we need to generate a set of candidate splits, which can be done in diﬀerent ways. The original RandForest algorithm (Breiman 2001) randomly selects a pre-speciﬁed number of distinct coordinates j from {1, . . . , p} without replacement, and considers θ to be all midpoints in the Xi,j data, which exhausts all possible subpartitions along each selected coordinate. Another option is to consider a random subset of cutoﬀ values, possibly enforcing that the sample sizes of the corresponding two children nodes are balanced, as in Denil et al. (2014). This approach not only enforces balanced splits, which is important for statistical guarantees (see Theorem 5), but it also reduces the computation time. Finally, we need to decide when to stop the tree construction. A typical stopping criterion is when each child region reaches a pre-speciﬁed number of data points (e.g., Breiman 2001). Depth may also additionally be restricted. Note that in an actual implementation if the stopping criterion would have stopped regardless of the split chosen, we can short circuit the call and skip the split optimization. Notice that ˆz0 and ˆH0 need only be computed once for each recursive call to StochOpt- Tree.Fit, while ˆh1, ˆh2 need to be computed for each candidate split. All estimators ˆhj discussed in Section 2.4 take the form of a sample average over i ∈ Rj, for j = 1, 2, and therefore can be easily and quickly computed for each candidate split. Moreover, when candidate cutoﬀ values consist of all midpoints of the sample values in the jth coordinate, such sample averages can be eﬃciently updated by proceeding in sorted order, where only one datapoint changes from one side of the split to the other at a time, similarly to how the original random forest algorithm maintains within-subpartition averages of outcomes and their squares for each candidate split. Notably, the tree construction computation is typically dominated by the step of searching best splits. This step can be implemented very eﬃciently with our approximate criteria, since 3 Ties can be broken arbitrarily. Kallus and Mao: Stochastic Optimization Forests 17 Algorithm 2 Procedure to fit a StochOptForest 1: procedure StochOptForest.Fit(data D, number of trees T ) 2: for j = 1 to T do 3: I tree j , I dec j ← Subsample({1, . . . , |D|}) 4: τj ← StochOptTree.Fit(X , {(Xi, Yi) ∈ D : i ∈ I tree j }, 1, 1) ◃ Fit tree using the sub-dataset I tree j 5: return {(τj, I dec j ) : j = 1, . . . , T } Algorithm 3 Procedure to make a decision using StochOptForest 1: procedure StochOptForest.Decide(data D, forest {(τj, I dec j ) : j = 1, . . . , T }, target x) 2: w(x) ← Zeros(|D|) ◃ Create an all-zero vector of length |D| 3: for j = 1, . . . , T do 4: N (x) ← {i ∈ I dec j : τj(Xi) = τj(x)} ◃ Find the τj-neighbors of x among the data in I dec j 5: for i ∈ N (x) do wi(x) ← wi(x) + 1 |N (x)|T ◃ Update the sample weights 6: return Minimize(∑ (Xi,Yi)∈D wi(x)c(z; Yi), z ∈ Z) ◃ Compute the forest policy Eq. (3) they only involve estimation of gradients and simple linear algebra operations (Section 2.3). Only one optimization and Hessian computation is needed at the beginning of each recursive call. In particular, we do not need to solve optimization problems repeatedly for each candidate split, which is the central aspect of our approach and which enables the construction of large-scale forests. StochOptForest Algorithm. In Algorithm 2, we summarize the algorithm of building forests using trees constructed by Algorithm 1. It involves an unspeciﬁed subsampling subroutine. For each j = 1, . . . , T , we consider possibly subsampling the data on which we will ﬁt the jth tree (I tree) as well as the data which we will later use to generate localized weights for decision-making (I dec). There are diﬀerent possible ways to construct these subsamples. Following the original random forest algorithm, we may set I tree j = I dec j equal to a bootstrap sample (a sample of size n with replacement). Alternatively, we may set I tree j = I dec j to be sampled as a fraction of n without replacement, which is an approach adopted in more recent random forest literature as it is more amenable to theoretical analysis and has similar empirical performance (eg., Mentch and Hooker 2016, Scornet et al. 2015). Alternatively, we may also sequentially sample I tree j , I dec j without replacement so the two are disjoint (e.g., take a random half of the data, then further split it at random into two). The property that the two sets are disjoint, I tree j ∩ I dec j = ∅, is known as honesty 18 Kallus and Mao: Stochastic Optimization Forests and it is helpful in proving statistical consistency of random forests (Athey et al. 2019, Denil et al. 2014, Wager and Athey 2018).4 Final Decision. In Algorithm 3, we summarize the algorithm of making a decision at new query points x once we have ﬁt a forest, that is, compute the forest policy, Eq. (3). Although the tree algorithm we developed so far, Algorithm 1, is for the unconstrained case, we present Algorithm 3 in the general constrained case. In a slight generalization of Eq. (3), we actually allow the data weighted by each tree to be a subset of the whole dataset (i.e., I dec j ), as described above. Namely, the weights wi(x) computed by Algorithm 3 are given by wi(x) = 1 T T∑ j=1 I [i ∈ I dec j , τj(Xi) = τj(x)] ∑n i′=1 I [ i ∈ I dec j , τj(Xi′) = τj(x) ] , (25) which is slightly more general than Eq. (3). Algorithm 3 then optimizes the average cost over the data with sample weights given by wi(x). Note that under honest splitting, for each single tree, each data point is used in either placing splits or constructing weights, but not both. However, since each tree uses an independent random subsample, every data point will participate in the construction of some trees and also the computation of weights by other trees. Therefore, all observations contribute to both forest construction and the weights in the ﬁnal decision making. In this sense, despite appearances, honest splitting is not “wasting” data. The weights {wi(x)}n i=1 generated by Algorithm 3 represent the average frequency with which each data point falls into the same terminal node as x. The measure given by the sum over i of wi(x) times the Dirac measure at Yi can be understood as an estimate for the conditional distribution of Y | X = x. However, in contrast to non-adaptive weights such as given by k-nearest neighbors or Nadaraya–Watson kernel regression (Bertsimas and Kallus 2014), which non-parametrically estimate this conditional distributional generically, our weights directly target the optimization problem of interest, focusing on the aspect of the data that is relevant to the optimization problem, which makes our weights much more eﬃcient. Moreover, in contrast to using weights given by standard random forests, which targets prediction with minimal squared error, our weights target the right downstream optimization problem. 3. The Constrained Case In this section, we develop approximate splitting criteria for training forests for general CSO problems with constraints as described at the onset in Eq. (1). Namely, in this section we let Z = {z ∈ Rd : hk(z) = 0, k = 1, . . . , s, hk(z) ≤ 0, k = s + 1, . . . , m} be as in Eq. (2). The oracle criterion 4 We may similarly use the ≈ 1/e fraction of the data not selected by the bootstrap sample to construct I dec j to achieve honesty, but this is again uncommon as it is diﬃcult to analyze. Kallus and Mao: Stochastic Optimization Forests 19 we target remains Coracle(R1, R2) as in Eq. (8) with the crucial diﬀerence that now Z need not be Rd and may be constrained as above. We then proceed as in Section 2: we approximate the oracle criterion in two ways, then we estimate the approximations, and then we use these estimated splitting criteria to construct trees. Since the perturbation analysis in the presence of constraints is somewhat more cumbersome, this section will be more technical. But the high level idea remains the same as the simpler unconstrained case in Section 2. 3.1. Perturbation Analysis of the Oracle Splitting Criterion Again, consider a region R0 ⊆ R d and its candidate subpartition R0 = R1 ∪ R2, R1 ∩ R2 = ∅. We deﬁne vj(t), zj(t), fj(t) as in Eq. (9) with the crucial diﬀerence that now Z is constrained. The oracle criterion is given by Coracle(R1, R2) = p1v1(1) + p2v2(1), as before. We again approximate v1(1), v2(1) by computing v1(t), v2(t) at t = 0 (where they are equal and do not depend on the subpartition) and then extrapolating from there by leveraging second order perturbation analysis. We present our key perturbation result for this below. Theorem 4 (Second-Order Perturbation Analysis: Constrained). Fix j = 1, 2. Suppose the following conditions hold: 1. f0(z), fj(z) are twice continuously diﬀerentiable. 2. The problem corresponding to f0(z) has a unique minimizer z0 over Z. 3. The inf-compactness condition: there exist constants α and t0 ∈ (0, 1] such that the sublevel set {z ∈ Z : f0(z) + t (fj(z) − f0 (z)) ≤ α} is nonempty and uniformly bounded over t ∈ [0, t0). 4. z0 is associated with a unique Lagrangian multiplier ν0 that also satisﬁes the strict comple- mentarity condition: ν0,k > 0 if k ∈ Kh(z0), where Kh(z0) = {k : hk(z0) = 0, k = s + 1, · · · , m} is the index set of active at z0 inequality constraints. 5. The Mangasarian-Fromovitz constraint qualiﬁcation condition at z0: ∇h1(z0), . . . , ∇hs(z0) are linearly independent, and ∃dz s.t. ∇hk(z0)dz = 0, k = 1, . . . , s, ∇hk(z0)dz < 0, k ∈ Kh(z0). 6. Second order suﬃcient condition: d ⊤ z ( ∇2f0(z0) + m∑ k=1 ν0,k∇2hk(z0) ) dz > 0 ∀dz ∈ C(z0) \\ {0}, where C(z0) is the critical cone deﬁned as follows: C(z0) = { dz : d ⊤ z ∇hk(z0) = 0, for k ∈ {1, . . . , s} ∪ Kh(z0)} . 20 Kallus and Mao: Stochastic Optimization Forests Let d j∗ z be the ﬁrst part of the (unique) solution of the following linear system of equations: [ (∇2f0(z0) + ∑m k=1 ν0,k∇2hk(z0)) ∇HKh ⊤(z0) ∇⊤HKh(z0) 0 ] [d j z ξ ] = [ −(∇fj(z0) − ∇f0(z0)) 0 ] , (26) where ∇⊤H(z0) ∈ Rm×d is the matrix whose kth row is (∇hk(z0))⊤, and ∇⊤HKh(z0) ∈ Rs+|Kh(z0)| consists only of the rows corresponding to equality and active inequality constraints. Then vj(t) = (1 − t)f0(z0) + tfj(z0) (27) + 1 2 t 2 { d j∗⊤ z ( ∇2f0(z0) + m∑ k=1 ν0,k∇2hk(z0) ) d j∗ z + 2d j∗⊤ z (∇fj(z0) − ∇f0(z0)) } + o(t 2), zj(t) = z0 + tdj∗ z + o(t). (28) Due to the presence of constraints, the approximations of optimal value vj(t) and optimal solu- tion zj(t) in Theorem 4 require more complicated conditions than those in Theorem 1. In particular, we need to incorporate constraints in the inf-compactness conditions (condition 3) and second order suﬃcient condition (condition 6), impose uniquenss and strict complementarity regularity conditions for the Lagrangian multiplier (condition 4), and assume a constraint qualiﬁcation con- dition (condition 5). The coeﬃcient matrix on the left hand side of the linear system of equations in Eq. (26) is invertible due to the second order suﬃcient condition in condition 6 (see Bertsekas 1995, Proposition 4.2.2), which ensures that d j∗ z uniquely exists. These regularity conditions guar- antee that the optimal value vj(t) and optimal solution zj(t) vary smoothly with perturbations to the optimization objective, and they rule out problems whose optimal solution may change non-smoothly. For example, optimal solutions to linear programming problems may change to completely diﬀerent vertices under even tiny perturbations to linear objectives. 5 Nevertheless, Theorem 4 may still apply to some problems with linear costs and nonlinear constraints such as the quadratically constrained problems in Section 6.2 of Elmachtoub and Grigas (2017). Concretely, the constraints in Examples 1 to 3 all ensure that the decision variables are bounded. So the inf-compactness condition (condition 3) is satisﬁed when there is no additional auxiliary variable (e.g., the newsvendor problem), or when the auxiliary variables at CSO optimal solutions are almost surely bounded. (e.g., conditional expectation or conditional quantiles of optimal port- folio returns in Example 2 or 3, respectively) Moreover, since these constraints are all simple aﬃne constraints, in Appendix G Proposition 16, we verify that they satisfy a stronger linear indepen- dence constraint qualiﬁcation condition than condition 5, which ensures the unique existence of 5 In the context of such linear problems, Elmachtoub et al. (2020) propose to optimize the oracle criterion by exhaustive search. As noted before, this is computationally burdensome, and indeed their focus is on smaller-scale models, with particular beneﬁts to interpretability. In Proposition 17 in Appendix G, we formally argue that their criterion coincides with what we called the oracle criterion in Eq. (8) in the case of linear costs. Kallus and Mao: Stochastic Optimization Forests 21 Lagrangian multiplier v0 for the solution z0 (condition 4). Since our problems in Examples 1 to 3 are all convex, the second order suﬃcient condition (condition 6) can ensure z0 to be the unique optimal solution (condition 2). This second order suﬃcient condition trivially holds when the Hes- sian matrix is positive deﬁnite and the constraints are aﬃne, e.g., under the conditions we discuss in Section 2.3 for Examples 1 and 2. In contrast to these conditions, the strict complementary slackness in condition 4 is generally more diﬃcult to verify exactly. However, even if it does not hold exactly, splitting criteria based on the approximations in Eqs. (27) and (28) may still capture signals relevant to CSO problems, especially compared to RandForest, which completely ignores the optimization problem structure. Note that Theorem 1 is a special case of Theorem 4 without constraints (m = 0). Indeed, without the constraints, the regularity conditions for the Lagrangian multiplier and constraint qualiﬁcation condition are vacuous, and conditions 3 and 6 reduce to the inf-compactness and positive deﬁnite Hessian matrix conditions in Theorem 1 respectively. And, without constraints, the linear equation system in Eq. (26) consists only of the part corresponding to d j z and the solution exactly coincides with the linear term in Eq. (14). Theorem 4 can itself be viewed as a special case of our Theorem 6 in Appendix A, where we tackle CSO problems with both deterministic and stochastic constraints. 3.2. Approximate Splitting Criteria Analogous to Theorem 1 for unconstrained problems, Theorem 4 for constrained problems also motivates two diﬀerent approximations of the oracle splitting criterion Coracle(R1, R2) = p1v1(1) + p2v2(1). Extrapolating Eq. (27) and Eq. (28) to t = 1 and ignoring the high order terms gives an approximate risk and approximate solution criterion, respectively: Capx-risk(R1, R2) = 1 2 ∑ j=1,2 pjd j∗⊤ z ( ∇2f0(z0) + m∑ k=1 ν0,k∇2hk(z0) ) d j∗ z + ∑ j=1,2 pjd j∗⊤ z (∇fj(z0) − ∇f0(z0)) , (29) Capx-soln(R1, R2) = ∑ j=1,2 pjfj (z0 + d j∗ z ) , (30) where in the approximate risk criterion, Capx-risk(R1, R2), we again omit from the extrapolation the constant term ∑ j=1,2 pj (fj(z0)) = p0f0(z0), as it does not depend on the choice of subpartition. Estimating the Approximate Splitting Criteria. We next discuss a general strategy to estimate our more general approximate splitting criteria in Eqs. (29) and (30) that handle con- straints. First, we start by estimating z0 by its sample analogue as in Eq. (21), where crucially now Z is constrained. Then we can estimate the gradients of fj at z0 for j = 0, 1, 2 and the Hessians of f0 at z0 in the very same way that gradients of fj and Hessians of f0 were estimated in Section 2.3, namely, estimating them at ˆz0, which is now simply solved with constraints. Gradients and Hessians 22 Kallus and Mao: Stochastic Optimization Forests of hk at z0 can be estimated by simply plugging in ˆz0, since the functions hk are known determin- istic functions. We can estimate Kh(z0) by Kh(ˆz0), i.e., the index set of the inequality constraints that are active at ˆz0. Next, we can estimate ν0 by solving ̂∇f 0(ˆz0) + ∑m k=1 νk∇hk(ˆz0) = 0 subject to νk ≥ 0 for k ∈ Kh(ˆz0) and νk = 0 for k ∈ {s + 1, . . . , m} \\ Kh(ˆz0), or alternatively by using a solver for Eq. (21) that provides associated dual solutions. Finally, we can estimate d j∗ z by solving Eq. (26) with estimates plugged in for unknowns. With all of these pieces in hand, we can estimate our approximate criteria in Eqs. (29) and (30). Revisiting the Running Examples. In Section 2.4, we discussed how to estimate gradients and Hessians of the objectives of our running examples. Now we revisit the examples and discuss their constraints. The nonnegativity and capacity constraints in Example 1 can be written as h1(z′) = ∑d l=1 zl ≤ C, hl+1(z′) = −zl ≤ 0, l = 1, . . . , d, and the simplex constraint in Examples 2 and 3 as h1(z) = ∑d l=1 zl = 1, hl+1(z) = −zl ≤ 0, l = 1, . . . , d. These are all deterministic linear constraints: their gradients are known constants and their Hessians are zero. 3.3. Construction of Trees and Forests It is straightforward to now extend the tree ﬁtting algorithm, Algorithm 1, to the constrained case. First, we note that in line 2 that solves for ˆz0, we use a constrained feasible set Z. Then, we update line 3 to estimate ∇f0(z0), ∇2f0(z0), ∇hk(z0), ∇2hk(z0), Kh(z0), ν0. Next, we update line 8 to estimate ∇fj(z0), dj∗ z . And, ﬁnally, we update line 9 to use the general splitting criteria in Eqs. (29) and (30) where we plug in these estimates for the unknowns. A crucial point that is key to the tractability of our method even in the presence of constraints is that the only step that requires any re-computation for each candidate split is the estimation of ∇fj(z0), dj∗ z . As in the unconstrained case, estimators for ∇fj(z0) usually consist of very simple sample averages over the data in the region Rj so they can also be very quickly computed. Moreover, only the right-hand side deﬁning d j∗ z in Eq. (26) varies with each candidate split, so the equation can be presolved using an LU decomposition or a similar approach. Therefore, we can easily and quickly consider many candidate splits, and correspondingly grow large-scale forests. Algorithm 2 for ﬁtting the forest remains the same, since the only change in ﬁtting is in the consideration of tree splits. And, Algorithm 3 was already written in the general constrained setting and so also remains the same. In particular, after growing a forest where tree splits take the constraints into consideration and given this forest, we impose the constraints in Z when computing the ﬁnal forest-policy decision, ˆz(x). 4. Empirical Study In this section we study our algorithm and baselines empirically to investigate the value of optimization-aware construction of forest policies and the success of our algorithm in doing so. Kallus and Mao: Stochastic Optimization Forests 23 1.0 1.2 1.4 1.6 100 200 400 800 Sample size nRelative risk Method StochOptForest (apx−risk) StochOptForest (apx−soln) GenRandForest (modified) RandForest RandSplitForest Constraint yes no (a) Relative risks of diﬀerent forest policies (lower relative risk means better performance). 0.00 0.25 0.50 0.75 1.00 1 2 3 4 5 6 7 8 9 10 Covariate IndexNormalized Importance (b) Feature Importance. Figure 2 Results for the CVaR portfolio optimization problem. Method n = 100 n = 200 n = 400 StochOptTree (oracle) 41.41 (5.43) 165.03 (15.77) 695.88 (83.91) StochOptTree (apx-risk) 0.26 (0.08) 0.68 (0.36) 1.68 (0.54) StochOptTree (apx-soln) 0.22 (0.05) 0.70 (0.20) 2.24 (0.33) Table 1 Average running time in seconds over 10 repetitions (and standard deviations) of constructing one tree for diﬀerent algorithms in the CVaR optimization problem. We focus on constrained CSO problems with CVaR objectives, including one simulated portfolio optimization problem and one real-data shortest path problem. In Appendices C.1, C.4 and C.5, we show additional experimental results for unconstrained multi-item newsvendor problems (Exam- ple 1) and constrained variance-based portfolio optimization problems (Example 2). 4.1. CVaR Portfolio Optimization We ﬁrst apply our method to the CVaR portfolio optimization problem (see Example 3). We consider d = 3 assets and p = 10 covariates. The covariates X are drawn from a stan- dard Gaussian distribution, and the asset returns are independent and are drawn from the conditional distributions Y1 | X ∼ 1 + 0.2 exp(X1) − LogNormal (0, 1 − 0.5I [−3 ≤ X2 ≤ −1]), Y2 | X ∼ 1 − 0.2X1 − LogNormal (0, 1 − 0.5I [−1 ≤ X2 ≤ 1]), and Y3 | X ∼ 1 + 0.2|X1| − LogNormal (0, 1 − 0.5I [1 ≤ X2 ≤ 3]). We seek an investment policy z(·) ∈ Rd that for each x aims to achieve smallest risk CVaR0.2 (Y ⊤z(x) | X = x), or equivalently the 0.8-CVaR of the portfolio loss −Y ⊤z (x), while satisfying the simplex constraint, i.e., Z = { z ∈ R d : ∑d l=1 zl = 1, zl ≥ 0} . 24 Kallus and Mao: Stochastic Optimization Forests We compare our StochOptForest algorithm using either the apx-risk or apx-soln approximate criterion for constrained problems (Eqs. (29) and (30)) to ﬁve benchmarks, where all algorithms are identical except for their splitting criterion. The ﬁrst two benchmarks are our StochOptFor- est algorithm using apx-risk and apx-soln criteria that (mistakenly) ignore the constraints (i.e., Eqs. (13) and (14)). The third benchmark is a modiﬁed 6 GenRandForest algorithm (Athey et al. 2019) applied to the ﬁrst order optimality condition for the CVaR optimization problem with- out the simplex constraint, as GenRandForest is designed for unconstrained problems. The fourth benchmark is the regular RandForest, which uses the squared error splitting criterion in Example 4 and targets the predictions of asset mean returns, and the ﬁfth is the RandSplitForest algorithm, which chooses splits uniformly at random (without using the portfolio return data). For our approx- imate criteria (both constrained and unconstrained) and the GenRanForest criterion, we use the parametric Hessian estimator in Eqs. (23) and (24) (which is misspeciﬁed in this example). We do not compare to StochOptForest with the oracle splitting criterion since it is too computationally intensive as we investigate further below (see Table 1). In all forest algorithms, we use an ensemble of 500 trees. To compute ˆz0 in our StochOptTree algorithm (Algorithm 1 line 2) as well as to compute the ﬁnal forest policy for any forest, we formulate the constrained CVaR optimization problem as a linear programming problem (Rockafellar et al. 2000) and solve it using Gurobi 9.0.2. We evaluate each forest policy ˆz(·) by its relative risk compared to the optimal z∗(·), namely the raio of E [CVaR0.2 (Y ⊤ ˆz(x) | X) | D] over E [CVaR0.2 (Y ⊤z∗(x) | X)], which we approximate using a very large testing dataset. See Appendix C.2 for more details. Figure 2a shows the distribution of the relative risk over 50 replications for each forest algo- rithm across diﬀerent training set size n ∈ {100, 200, 400, 800}. The dashed boxes corresponding to “Constraint = no” indicate that the associated method does not take constraints into account when choosing the splits, which applies to all four benchmarks. (Note that all methods consider constraints in computing a the ﬁnal forest-policy decision, ˆz(x).) We can observe that our Sto- chOptForest algorithms with approximate criteria that incorporate constraints achieve the best relative risk over all sample sizes, and their relative risks decrease considerably when the training set size n increases. In contrast, the relative risks of all benchmark methods decrease much more slowly. Therefore, both failing to target the cost function structure (GenRandForest,7 RandForest, 6 Note we cannot apply the original GenRandForest algorithm to solve unconstrained CVaR optimization: every step of tree construction requires computing the optimal unconstrained solution in the region R0 to be partitioned, which however does not exist because without constraints the CVaR objectives can be made arbitrarily small. We thus have to slightly modify the GenRandForest algorithm to compute the optimal constrained solution in every region to be partitioned, from which we then compute the GenRandForest splitting criterion for the ﬁrst order optimality condition of unconstrained CVaR optimization. We furthermore regularize the Hessian matrix as it is not generally invertible, as discussed after Eq. (18), which would make the GenRandForest splitting criterion undeﬁned. See Appendix C.2. 7 GenRandForest criterion partly captures the cost function structure as it incorporates the corresponding ﬁrst order optimality condition information, but it chooses splits to maximize the discrepancy of approximate solutions in the induced subregions, rather than optimize their decision costs directly. Kallus and Mao: Stochastic Optimization Forests 25 and RandSplitForest) and failing to take constraints into account (all ﬁve benchmark methods) can signiﬁcantly undermine the ultimate decision-making quality. In contrast, our StochOptForest algorithms based on the approximate criteria eﬀectively account for both so they perform much better. Moreover, our results show that even though the normal distribution assumption used to derive our Hessian estimator (Eqs. (23) and (24)) is wrong in our experiment, our proposed forest policies still achieve superior performance, which illustrates the robustness of our methods. To further understand these results, we also consider feature importance measures based on each forest algorithm. In Appendix B, we extend the impurity-based feature importance measures (Hastie et al. 2001) to our StochOptForest method. Recall there are p = 10 covariates, and the ﬁrst two determine the distributions of asset returns. The ﬁrst covariate inﬂuences the conditional mean of return distributions more, while the second one inﬂuences more the distribution tails. In Fig. 2b, we visualize the feature importance measures for our proposed method and RandForest when n = 800. The importance measures are normalized for each method so that the most important feature has an importance value equal to 1. We can observe that our StochOptForest methods (incorporating constraints) value the second covariate more than the ﬁrst one, which shows the importance of signals in the return distribution tails for CVaR optimization. In contrast, the RandForest algorithm puts more importance on the ﬁrst covariate, validating that it is designed to target the prediction of asset mean returns. There do not exist feature importance measures for the GenRandForest algorithm. Instead, we show its average frequency of splitting on each covariate in Appendix C.2 Fig. 7. We observe that the GenRandForest method splits on noise covariates (i.e., the 3rd to 10th covariate) more frequently than our proposals, which may partly explain its inferior performance. We also consider the average running time of our proposed algorithm in Table 1. We compare our StochOptTree algorithm with approximate criteria incorporating constraints to the oracle splitting criterion (using empirical expectations). We consider 10 repetitions, in each of which we apply each tree algorithm with the same speciﬁcations to construct a single tree on the same training data with varying size n ∈ {100, 200, 400}. We run this experiment on a MacBook with 2.7 GHz Intel Core i5 processor. We can see that the running time of our StochOptTree algorithm with apx-risk criterion is hundreds of times faster than the StochOptTree algorithm with the oracle criterion that must solve the constrained CVaR optimization problems for each candidate split. The computational gains of our approximate criteria relative to the oracle criterion also grow with larger sample size n (from around 200 times faster at n = 100 to more than 400 times faster at n = 400), as the CVaR optimization problem becomes slower to solve. Since the StochOptForest algorithm with the oracle criterion is extremely slow, we can only evaluate its performance in a small-scale experiment in Fig. 8 in Appendix C.2. Focusing on 26 Kallus and Mao: Stochastic Optimization Forests (a) The downtown Los Angeles region (enclosed by the black lines) for the shortest path problem. −25% 0% 25% 0.5 1 1.5 2 Sample Size (Years)% of Realized ImprovementMethod StochOptForest (apx−risk) StochOptForest (apx−soln) GenRandForest (modified) RandForest RandSplitForest Constraint yes no (b) Percentages of realized improvement by diﬀerent forest policies for the CVaR shortest-path problem. A higher percentage is better. Figure 3 Set up and results for the CVaR shortest path problem using Uber Movement data. constructing small forests of only 50 trees with n up to 400, we ﬁnd the performance of the oracle criterion is marginally better than our approximate criteria. However, our approximate criteria are much more computationally eﬃcient, which enables us to leverage larger datasets for better performance. In Appendix C.2, we also show that similar results hold for portfolio optimization with a linear combination of CVaR and mean return as the objective (Fig. 9) and for CVaR optimization with asset returns drawn from normal distributions (Fig. 10). We include additional empirical results on minimizing the variance of investment portfolios (see Example 2) in Appendix C.4, and show that the performance of our approximate criteria is close to the oracle criterion. 4.2. CVaR Shortest Path Problem Using Uber Movement Data We next demonstrate our methods in a shortest path problem, using traveling times data in Los Angeles (LA) collected from Uber Movement (https://movement.uber.com). We focus on 45 census tracts in downtown LA (see Fig. 3a), collecting historical data of average traveling times from each of these census tracts to its neighbors during ﬁve periods in each day (AM Peak, Midday, PM Peak, Evening, Early Morning) in 2018 and 2019. This results in 3650 observations of traveling times Yj for j = 1, . . . , 93 edges on a graph with 45 nodes. We consider p = 197 covariates X including weather, period of day and other calendar features, and lagged traveling times. We aim to go from an eastmost census tract (Aliso Village) to a westmost census tract (MacArthur Park) in this region (green and and red marks in Fig. 3a, receptively), through a path between them, encoded by z ∈ {0, 1}d with d = 93, where zj indicates whether we travel on edge j. In particular, we consider the CSO problem z∗(x) ∈ arg minz(·)∈Z CVaR0.8 (Y ⊤z (x) | X = x) where Z is given by standard ﬂow preservations constraints, with a source of +1 at Aliso Village and a sink of Kallus and Mao: Stochastic Optimization Forests 27 −1 at MacArthur Park. See Appendix C.3 for more details about data collection, optimization formulation, and other experiment speciﬁcations. We again compare diﬀerent forest algorithms as we do in Section 4.1, but to reduce computation we only train them up to 100 trees. We consider four diﬀerent sample sizes ranging from 0.5-year to the whole 2-year data. For each sample size, we randomly split the corresponding dataset into two halves as training data Dtrain and testing data Dtest respectively. Note that the distribution of Y | X is unknown, so we can no longer benchmark the performance of each forest policy ˆz (·) trained on Dtrain against the CSO optimal policy z∗(·) as in Section 4.1. Instead, we compare their percentages of realized improvement, termed the coeﬃcient of prescriptiveness in Bertsimas and Kallus (2014). Namely, we consider the ratio between each method’s improvement over the context-free sample average approximation (SAA), which ﬁnds a single solution ˆzSAA to optimize the average cost on the whole training data, over the improvement over SAA of the (infeasible) perfect-information shortest path, which in each test sample computes the shortest path for the observed travel time Y . Notice that more eﬀective forest policies have higher percentages of realized improvement, but even known-distributions optimal policy z∗(·) to Eq. (1) cannot generally achieve 100% realized improvement as the covariates do not perfectly predict travel times. In Fig. 3b, we show the results across 50 realizations of random train-test splits. We observe that as the sample sizes increase, all methods tend to perform better. In particular, our StochOptForest algorithm with either the apx-risk or apx-soln criterion (incorporating constraints) outperforms all benchmarks across all sample sizes, with the clearest improvement seen using the apx-risk criterion and in smaller datasets. Overall the results show that incorporating the optimization problem structure in the tree construction can lead to improvements, when optimization is the aim. 5. Asymptotic Optimality In this section, we prove that under some regularity conditions, our forest policy asymp- totically attains the optimal risk, namely, E [c(ˆzn(x); Y ) | X = x] converges in probability to minz∈Z E [c(z; Y ) | X = x] as n → ∞ for any x ∈ X . It is well known that forests algorithms with adaptively constructed trees are extremely diﬃcult to analyze, so some simplifying regularity conditions are often needed to make the theoretical analysis tractable (Biau and Scornet 2016). In this section, we assume the tree regularity conditions introduced by Athey et al. (2019), Wager and Athey (2018). Assumption 1 (Regular Trees). The trees constructed satisfy the following regularity condi- tions for constants ω ∈ (0, 0.2], π ∈ [0, 1), and an integer kn > 0: 1. Every tree split puts at least a fraction ω of observations in the parent node into each child node. Every leaf node in every tree contains between kn and 2kn − 1 observations. 28 Kallus and Mao: Stochastic Optimization Forests 2. For an index set J ⊆ {1, . . . , p} such that E [c(z; Y ) | X] = E [c(z; Y ) | XJ ] for all z ∈ Z, for each leaf of each tree and for each j ∈ J , the average probability of splitting along feature xj is bounded below by π/p, averaging over nodes on the path from the root to the leaf and marginalizing over any randomization of candidate splits (and conditioning on the data). 3. Each tree grows on a subsample of size sn drawn randomly without replacement from the whole training data, and it is honest, i.e., I tree j ∩ I dec j ̸= ∅ with |I tree j | + |I dec j | = sn for j = 1, . . . , T . Condition 1 in Assumption 1 speciﬁes that the stopping criterion must ensure a minimal leaf size and that all candidate splits be balanced in that they put at least a constant fraction of observations in each child node. Without this condition, even when sample size n is large, some imbalanced splits may run out of data so quickly that some leaves are not suﬃciently partitioned and thus too large. As a result, the estimation bias of the objective function may fail to vanish even when n → ∞. Condition 2 requires the trees to split along every relevant direction at suﬃcient frequency, which ensures that the leaves of the trees become small in all relevant dimensions of the feature space as n gets large. Relevant features are described by those such that the random cost of any decision is mean-independent of X given only these relevant features, which is trivially satisﬁed for J = {1, . . . , p}. Condition 3 speciﬁes that we use subsample splitting, i.e., the data used to construct each tree (I tree j ) and the data used to construct localized weights from this tree for ﬁnal decision-making (I dec j ) are disjoint. This so-called honesty property plays a critical role in the theoretical analysis of forest algorithms but it may be largely technical. In Section 4, we empirically show that our forest policies appear to achieve asymptotic optimality even without using honest subsample splitting. In Appendix C.6, we further illustrate in Fig. 14 that StochOptForest with no subsample splitting (i.e., I dec j = I tree j ) performs better than the honest version with splitting, which can be explained as honest trees using fewer data for tree construction and decision-making. In the following assumption, we further impose some regularity conditions on the cost function c(z; y) and the distribution of Y | X. Assumption 2 (Distribution Regularity). Fix x ∈ X and assume the following conditions: 1. The marginal distribution of X has a density, its support X is compact, and the density is bounded away from 0 and ∞ on X . 2. There exist a constant α and a compact set C ⊆ Z such that, {z ∈ Z : E [c(z; Y ) | X = x] ≤ α} ⊆ C and {z ∈ Z : ∑n i=1 wi(x)c(z; Yi) ≤ α} ⊆ C for wi(x) in Eq. (25) almost surely eventually. 3. There exists a function b(y) such that for any z, z′ ∈ C, y ∈ Y, |c(z; y) − c(z′; y)| ≤ b(y)∥z − z′∥2. Moreover, there exists a positive constant ˜C such that E [b(Y ) | X = x] ≤ ˜C < ∞. 4. There exist constants Lc, Lb such that supz∈C supx′∈X ∣ ∣E [c(z; Y ) | XJ = xJ ] − E [ c(z; Y ) | XJ = x ′ J ]∣ ∣ ≤ Lc∥ ∥xJ − x ′ J ∥ ∥2 and supx′∈X ∣ ∣E [b(Y ) | XJ = xJ ] − E [ b(Y ) | XJ = x ′ J ]∣ ∣ ≤ Lb∥ ∥xJ − x ′ J ∥ ∥2. Kallus and Mao: Stochastic Optimization Forests 29 5. There exist positive constants η, η′, C such that sup z∈C E [ eη|c(z;Y )−E[c(z;Y )|X=x]| | X = x ] ≤ C < ∞, E [eη′|b(Y )−E[b(Y )|X=x]| | X = x ] ≤ C < ∞. One important condition in Assumption 2 is that the cost function c(z; y) is Lipschitz-continuous in z on the compact set C. In the following proposition, we validate that Examples 1 to 3 all satisfy this condition. Proposition 2. For any z, z′ ∈ C: 1. The cost function c(z; y) = ∑d l=1 max{αl(zl − yl), βl(yl − zl)} for the newsvendor problem in Example 1 satisﬁes that |c(z; y) − c(z′; y)| ≤ √ d max{αl, βl}∥z − z′∥2. 2. The cost function c(z; y) = (y⊤z1:d − zd+1) 2 for the variance-based portfolio optimization prob- lem in Example 2 satisﬁes that |c(z; y) − c(z′; y)| ≤ 4 √2(sup˜z∈C ∥˜z∥2) max{1, ∥y∥2 2}∥z − z′∥2. 3. The cost function c(z; y) = 1 α max {zd+1 − y⊤z1:d, 0 } − zd+1 for the CVaR optimization problem in Example 3 satisﬁes that |c(z; y) − c(z′; y)| ≤ (∥y∥2 + 1 + 1 α ) ∥z − z′∥2. Under the assumptions above, we can prove that the forest policy is asymptotically optimal. Theorem 5. Let x ∈ X be ﬁxed. If Assumptions 1 and 2 hold at the given x and if kn → ∞, sn/kn → ∞, log T /kn → 0, and T kn/sn → 0, then sup z∈C ∣ ∣ ∣ ∣ ∣ n∑ i=1 wi(x)c(z; Yi) − E [c(z; Y ) | X = x] ∣ ∣ ∣ ∣ ∣ p → 0. (31) It follows that any choice ˆzn(x) ∈ arg minz∈Z ∑n i=1 wi(x)c(z; Yi) satisﬁes that as n → ∞, ∣ ∣ ∣E [c(ˆzn(x); Y ) | X = x] − min z∈Z E [c(z; Y ) | X = x] ∣ ∣ ∣ p → 0. (32) Theorem 5 provides asymptotic optimality of ˆzn(x) point-wise in x. The result can straightfor- wardly be extended to be uniform in x if we simply assume the conditions in Assumption 2 hold for all x ∈ X with common constants. 6. Discussion In this section we oﬀer some discussions. First, we discuss how our work is related to and diﬀers from work on estimation using localized weights and forests in particular. Then we discuss other related work on CSO and on integrating prediction and optimization. We discuss additional related literature about tree models and perturbation analysis in Appendix F. 30 Kallus and Mao: Stochastic Optimization Forests 6.1. Comparison to Estimation The idea of using localized weights to estimate parameters given covariate values has a long his- tory in statistics and econometrics, including applications in local maximum likelihood (Fan et al. 1998, Tibshirani and Hastie 1987), local generalized method of moments (Lewbel 2007), local esti- mating equation (Carroll et al. 1998) and so on. These early works typically use non-adaptive localized weights like nearest-neighbor weights or Nadaraya-Watson kernel weights, which only use the information of covariates. Recently, some literature propose to use forest-based weights for local parameter estimation (e.g., Athey et al. 2019, Meinshausen 2006, Oprescu et al. 2019, Scornet 2015), which generalizes the original random forest algorithm for regression and classiﬁca- tion problems (Breiman 2001) to other estimation problems where the estimand depends on the X-conditional distribution. These forest-based weights are derived from the proportion of trees in which each observation falls in the same terminal node as the target covariate value. Since those trees are adaptively constructed using label data as well, random forest weights are shown to be more eﬀective in modeling complex heterogeneity in high dimensions than non-adaptive weights. Recent literature has studied the statistical guarantees of random forests in estimating conditional expectation functions (see reviews in Biau and Scornet 2016, Wager and Athey 2018), or more general parameters deﬁned by local estimating equations (Athey et al. 2019, Oprescu et al. 2019). Among the statistical estimation literature above, closest to our work is Athey et al. (2019), who propose the GenRandForest algorithm to estimate roots of conditional estimating equations. This is closely related to our decision making problem, because the optimal solution of unconstrained CSO is also the root of a conditional estimating equation given by the ﬁrst order optimality condition. For example, the optimal solutions of conditional newsvendor problem in Example 1 without constraints are conditional quantiles, which are also considered by Athey et al. (2019) under the conditional estimating equation framework. For computational eﬃciency, Athey et al. (2019) also propose a gradient-based approximation for roots in candidate subpartitions (see discussions below Eq. (14)), and then ﬁnd the best split that maximizes the discrepancy of the approximate roots in the subregions, thereby approximately minimizing the total mean squared error of the estimated roots (Athey et al. 2019, Proposition 1). In contrast, our paper has a fundamentally diﬀerent goal: we target decision-making risk (expected cost) rather than estimation risk (accuracy). In our apx-risk and apx-soln criteria, we directly approximate the optimal average cost itself and use this to choose a split, rather than esti- mation error of the solution. In Appendix C.1, we provide one empirical example of unconstrained newsvendor problem where the heterogeneity of optimal solution estimation is drastically diﬀerent from the heterogeneity of the optimal decision-making, which illustrates the beneﬁt of targeting decision quality when the decision problem, rather than the estimation problem, is of interest. Kallus and Mao: Stochastic Optimization Forests 31 Moreover, our methods uniquely accommodate constraints, which are prevalent in decision-making problems but rare in statistical estimation problems. For constrained CSO, the optimal solution cannot be characterized by local estimating equations so the GenRandForest algorithm is not appli- cable. In Section 4, we provided empirical examples of constrained CVaR optimization problems where the taking into account constraints is key to constructing good trees. 6.2. CSO and Integrating Prediction and Optimization Our paper builds on the CSO framework, and the general local learning approach, i.e., estimating the objective (and stochastic constraints in Appendix A) by weighted averages with weights reﬂect- ing the proximity of each covariate observation to the target value. Bertsimas and Kallus (2014), Hanasusanto and Kuhn (2013), Hannah et al. (2010) propose the use of nonparametric weights that use only the covariate observations X and do not depend on observations of the uncertain variable Y , such as Nadaraya-Watson weights. Bertsimas and Kallus (2014) formally set up the CSO framework, propose a wide variety of machine learning methods for local weights construction, and provide rigorous asymptotic optimality guarantees. In particular, they additionally propose weights based on decision trees and random forests that incorporate the uncertain variable infor- mation, and show their superiority when the covariate dimension is high. However, their tree and forest weights are constructed from standard regression algorithms that target prediction accuracy instead of downstream decision quality, primarily because targeting the latter would be too compu- tationally expensive. Our paper resolves this computational challenge by leveraging approximate criteria that can be eﬃciently computed. Optimization problems that have unknown parameters, such as an unknown distribution or a conditional expectation, are often solved by a two-stage approach: the unknown parameters are estimated or predicted, then these are plugged in, and then the approximated optimization problem is solved. The estimation or prediction step is often done independently of the optimization step, targeting standard accuracy measures such as mean squared error without taking the downstream optimization problem into account. However, all predictive models make errors and when prediction and optimization are completely divorced, the error tradeoﬀs may be undesirable for the end task of decision-making. To deal with this problem, recent literature propose various ways to tailor the predictions to the optimization problems. Elmachtoub and Grigas (2017) study a special CSO problem where c(z; y) = y⊤z is linear and constraints are deterministic and known. In this special case, the parameter of interest is the conditional expectation E [Y | X = x], which forms the linear objective’s coeﬃcients. They propose to ﬁt a parametric model to predict the coeﬃcients by minimizing a convex surrogate loss of the suboptimality of the decisions induced by predicted coeﬃcients. Elmachtoub et al. (2020) study the 32 Kallus and Mao: Stochastic Optimization Forests same linear CSO problem and instead predict the coeﬃcients nonparametrically by decision trees and random forests with suboptimality as the splitting criterion. In Appendix G Proposition 17, we show this criterion is equivalent to what we termed the oracle splitting criterion in Eq. (8) in the case of linear costs. Since this involves full re-optimization for each candidate split, they are limited to very few candidate splits, suggesting using one per candidate feature, and they consider a relatively small number of trees in their forests. In contrast, we consider the general CSO problem and use eﬃcient approximate criteria, which is crucial for large-scale problems and training large tree ensembles. Hu et al. (2021) also study linear CSO problems and they show both theoretically and empirically that with correctly speciﬁed models, integrated approaches may perform worse than the simpler predict-then-optimize approach. Our paper demonstrates the beneﬁt of a forest- based integrated approach in nonlinear CSO problems, where a predict-then-optimize approach would have to learn the whole conditional distribution, not just the conditional expectation. Donti et al. (2017) study smooth convex optimization problems with a parametric model for the conditional distribution of the uncertain variables (in both objective and constraints) given covari- ates, and ﬁt the parametric models by minimizing the decision objective directly using gradient descent methods on the optimization risk instead of the log-likelihood. Wilder et al. (2019) fur- ther extend this approach to nonsmooth problems by leveraging diﬀerentiable surrogate problems. However, unless the cost function depends on the uncertain variables linearly, the stochastic opti- mization problem may involve complicated integrals with respect to the conditional distribution model. In contrast, our paper focuses on nonparametric forest models that cannot be trained by gradient-based methods, and we can straightforwardly target the CSO using localized weights. Notz (2020) consider convex optimization problems with nondiﬀerentiable cost functions, and propose a subgradient boosting algorithm to directly learn a decision policy. While this approach can handle complex objectives, it can only accommodate very simple constraints like box constraints, as it is diﬃcult to impose complex constraints on boosting decision policies. In contrast, our approach based on the CSO framework can readily handle general constraints. 7. Concluding Remarks In CSO problems, covariates X are used to reduce the uncertainty in the variable Y that aﬀects costs in a decision-making problem. The remaining uncertainty is characterized by the conditional distribution of Y | X = x. A crucial element of eﬀective algorithms for learning policies for CSO from data is the integration of prediction and optimization. One can try to ﬁt generic models that predict the distribution of Y | X = x for every x and then plug this in place of the true conditional distribution, but ﬁtting such a model to minimize prediction errors without consideration of the downstream decision-making problem may lead to ill-performing policies. In view of this, we studied Kallus and Mao: Stochastic Optimization Forests 33 how to ﬁt forest policies for CSO (which use a forest to predict the conditional distribution) in a way that directly targets the optimization costs. The na¨ıve direct implementation of this is hopelessly intractable for many important managerial decision-making problems in inventory and revenue management, ﬁnance, etc. Therefore, we instead developed eﬃcient approximations based on second-order perturbation analysis of stochastic optimization. The resulting algorithm, StochOptForest, is able to grow large-scale forests that directly target the decision-making problem of interest, which empirically leads to signiﬁcant improvements in decision quality over baselines. References Athey S, Tibshirani J, Wager S (2019) Generalized random forests. The Annals of Statistics 47(2):1148–1178. Bartlett PL, Long PM, Lugosi G, Tsigler A (2020) Benign overﬁtting in linear regression. Belkin M, Hsu D, Ma S, Mandal S (2019) Reconciling modern machine-learning practice and the classical bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences 116(32):15849–15854. Belkin M, Hsu D, Mitra P (2018) Overﬁtting or perfect ﬁtting? risk bounds for classiﬁcation and regression rules that interpolate. Bertsekas D (1995) Nonlinear programming. Athena Scientiﬁc 48. Bertsimas D, Gupta V, Kallus N (2018a) Data-driven robust optimization. Mathematical Programming 167(2):235–292. Bertsimas D, Gupta V, Kallus N (2018b) Robust sample average approximation. Mathematical Programming 171(1-2):217–282. Bertsimas D, Kallus N (2014) From predictive to prescriptive analytics. arXiv preprint arXiv:1402.5481 . Bertsimas D, Kallus N (2016) The power and limits of predictive approaches to observational-data-driven optimization. arXiv preprint arXiv:1605.02347 . Biau G, Devroye L (2015) Lectures on the Nearest Neighbor Method. Biau G, Scornet E (2016) A random forest guided tour. Test 25(2):197–227. Bonnans JF, Shapiro A (2000) Perturbation Analysis of Optimization Problems (New York: Springer). Breiman L (2001) Random forests. Machine learning 45(1):5–32. Breiman L, Friedman J, Stone CJ, Olshen RA (1984) Classiﬁcation and regression trees (CRC press). Carroll RJ, Ruppert D, Welsh AH (1998) Local estimating equations. Journal of the American Statistical Association 93(441):214–227. Chen YC, Miˇsi´c VV (2020) Decision forest: A nonparametric approach to modeling irrational choice. Available at SSRN 3376273 . Chen YC, Miˇsi´c VV (2021) Assortment optimization under the decision forest model. Available at SSRN 3812654 . 34 Kallus and Mao: Stochastic Optimization Forests Chen Z, Leng C (2015) Local linear estimation of covariance matrices via cholesky decomposition. Statistica Sinica 1249–1263. Ciocan DF, Miˇsi´c VV (2020) Interpretable optimal stopping. Management Science 0(0):null, URL http: //dx.doi.org/10.1287/mnsc.2020.3592. Cornuejols G, T¨ut¨unc¨u R (2006) Optimization methods in ﬁnance (Cambridge University Press). Denil M, Matheson D, Freitas ND (2014) Narrowing the gap: Random forests in theory and in practice. Proceedings of The 31st International Conference on Machine Learning, 665–673. Donti P, Amos B, Kolter JZ (2017) Task-based end-to-end model learning in stochastic optimization. Advances in Neural Information Processing Systems, 5484–5494. Elmachtoub AN, Grigas P (2017) “smart” predict, then optimize. arXiv preprint arXiv:1710.08005 . Elmachtoub AN, Liang JCN, McNellis R (2020) Decision trees for decision-making under the predict-then- optimize framework. arXiv preprint arXiv:2003.00360 . Elmachtoub AN, McNellis R, Oh S, Petrik M (2017) A practical method for solving contextual bandit problems using decision trees. arXiv preprint arXiv:1706.04687 . Fan J, Farmen M, Gijbels I (1998) Local maximum likelihood estimation and inference. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60(3):591–608. Fan J, Yao Q (1998) Eﬃcient estimation of conditional variance functions in stochastic regression. Biometrika 85. F´eraud R, Allesiardo R, Urvoy T, Cl´erot F (2016) Random forest for the contextual bandit problem. Artiﬁcial Intelligence and Statistics, 93–101. Ferreira KJ, Lee BHA, Simchi-Levi D (2016) Analytics for an online retailer: Demand forecasting and price optimization. Manufacturing & Service Operations Management 18(1):69–88. Geurts P, Ernst D, Wehenkel L (2006) Extremely randomized trees. Machine learning 63(1):3–42. Giordano R, Stephenson W, Liu R, Jordan M, Broderick T (2019) A swiss army inﬁnitesimal jackknife. Chaudhuri K, Sugiyama M, eds., Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, 1139–1147 (PMLR). Hanasusanto GA, Kuhn D (2013) Robust data-driven dynamic programming. Advances in Neural Informa- tion Processing Systems, 827–835. Hannah L, Powell W, Blei DM (2010) Nonparametric density estimation for stochastic optimization with an observable state variable. Advances in Neural Information Processing Systems, 820–828. Hastie T, Montanari A, Rosset S, Tibshirani RJ (2020) Surprises in high-dimensional ridgeless least squares interpolation. Hastie T, Tibshirani R, Friedman JH (2001) The Elements of Statistical Learning. Kallus and Mao: Stochastic Optimization Forests 35 Hong L, Liu G (2009) Simulating sensitivities of conditional value at risk. Management Science 55:281–293. Hu Y, Kallus N, Mao X (2021) Fast rates for contextual linear optimization. Jiang H (2017) Uniform convergence rates for kernel density estimation. Precup D, Teh YW, eds., Proceed- ings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 1694–1703 (International Convention Centre, Sydney, Australia: PMLR). Kallus N (2017) Recursive partitioning for personalization using observational data. International Conference on Machine Learning, 1789–1798. Kleywegt AJ, Shapiro A (2001) Stochastic optimization. Handbook of industrial engineering 2625–2649. Koh PW, Liang P (2017) Understanding black-box predictions via inﬂuence functions. Precup D, Teh YW, eds., Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 1885–1894 (International Convention Centre, Sydney, Australia: PMLR). Lewbel A (2007) A local generalized method of moments estimator. Economics Letters 94(1):124–128. Loubes JM, Marteau C, Sol´ıs M (2019) Rates of convergence in conditional covariance matrix with nonpara- metric entries estimation. Communications in Statistics - Theory and Methods 1–23. Louppe G (2015) Understanding random forests: From theory to practice. Meinshausen N (2006) Quantile regression forests. Journal of Machine Learning Research 7(Jun):983–999. Mentch L, Hooker G (2016) Quantifying uncertainty in random forests via conﬁdence intervals and hypothesis tests. The Journal of Machine Learning Research 17(1):841–881. Miˇsi´c VV (2020) Optimization of tree ensembles. Operations Research . Nemirovski A, Juditsky A, Lan G, Shapiro A (2009) Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization 19(4):1574–1609. Notz PM (2020) Explainable subgradient tree boosting for prescriptive analytics in operations management. Available at SSRN 3567665 . Oprescu M, Syrgkanis V, Wu ZS (2019) Orthogonal random forest for causal inference. Chaudhuri K, Salakhutdinov R, eds., Proceedings of the 36th International Conference on Machine Learning, vol- ume 97 of Proceedings of Machine Learning Research, 4932–4941 (Long Beach, California, USA: PMLR). Rockafellar RT, Uryasev S, et al. (2000) Optimization of conditional value-at-risk. Journal of risk 2:21–42. Scornet E (2015) Random forests and kernel methods. Scornet E, Biau G, Vert JP (2015) Consistency of random forests. Ann. Statist. 43(4):1716–1741, URL http://dx.doi.org/10.1214/15-AOS1321. Shapiro A, Dentcheva D, Ruszczy´nski A (2014) Lectures on stochastic programming: modeling and theory (SIAM). 36 Kallus and Mao: Stochastic Optimization Forests Simchi-Levi D, Chen X, Bramel J (2005) The logic of logistics. Theory, Algorithms, and Applications for Logistics and Supply Chain Management . Stephenson W, Broderick T (2020) Approximate cross-validation in high dimensions with guarantees. Chi- appa S, Calandra R, eds., Proceedings of the Twenty Third International Conference on Artiﬁcial Intel- ligence and Statistics, volume 108 of Proceedings of Machine Learning Research, 2424–2434 (Online: PMLR). Talluri KT, Van Ryzin GJ (2006) The theory and practice of revenue management (New York: Springer). Tibshirani R, Hastie T (1987) Local likelihood estimation. Journal of the American Statistical Association 82(398):559–567. Van der Vaart AW (2000) Asymptotic statistics (Cambridge university press). Wachsmuth G (2013) On licq and the uniqueness of lagrange multipliers. Operations Research Letters 41(1):78–80, ISSN 0167-6377, URL http://dx.doi.org/https://doi.org/10.1016/j.orl.2012.11. 009. Wager S, Athey S (2018) Estimation and inference of heterogeneous treatment eﬀects using random forests. Journal of the American Statistical Association 113(523):1228–1242. Wilder B, Dilkina B, Tambe M (2019) Melding the data-decisions pipeline: Decision-focused learning for combinatorial optimization. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 1658–1665. Wilson A, Kasy M, Mackey L (2020) Approximate cross-validation: Guarantees for model assessment and selection. arXiv preprint arXiv:2003.00617 . Yin J, Geng Z, Li R, Wang H (2010) Nonparametric covariance model. Statistica Sinica 20:469–479. Zhou Z, Athey S, Wager S (2018) Oﬄine multi-action policy learning: Generalization and optimization. arXiv preprint arXiv:1810.04778 . Kallus and Mao: Stochastic Optimization Forests 37 Supplemental Material for Stochastic Optimization Forests Appendix A: Contextual Stochastic Optimization with Stochastic Constraints In Section 3, we analyzed CSO problems with only deterministic constraints. In this section, we further extend our results and methods to CSO problems with both deterministic and stochastic constraints. Speciﬁcally, we consider CSO problems given by z∗(x) ∈ arg min z∈Z(x) E [c(z; Y ) | X = x] , (33) Z(x) =    z ∈ Rd : gk(z; x) = E [Gk(z; Y ) | X = x] = 0, k = 1, . . . , s, gk(z; x) = E [Gk(z; Y ) | X = x] ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    , where the stochastic constraints (those given by {gk(z; x)} m k=1) depend on the unknown distribution of Y | X = x and need to be learned from data as well. Note that the constraint set Z(x) now varies with x due to the stochastic constraints. Analogously, we consider forest policies of the following form: ˆz(x) ∈ arg min z∈ ˆZ(x) n∑ i=1 wi(x)c(z; Yi), wi(x) := 1 T T∑ j=1 I [τj(Xi) = τj(x)] ∑n i′=1 I [τj(Xi′) = τj(x)] (34) ˆZ(x) =   z ∈ Rd : ∑n i=1 wi(x)Gk(z; Yi) = 0, k = 1, . . . , s,∑n i=1 wi(x)Gk(z; Yi) ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    . Notice that ˆZ(x) ̸= Z(x) so, unlike the deterministic case, ˆz(x) may violate the constraints of the CSO problem, i.e., ˆz (x) ̸∈ ˆZ (x). In Appendix A.5 we further discuss the nuances and challenges of handling stochastic constraints and the beneﬁts of our approach as well as possible robust variants. Examples of stochastic constraints. Example 1, Cont’d (Stochastic Constraints in Multi- Item Newsvendor). A typical example for stochastic constraints in the multi-item newsvendor problem is the following stochastic aggregate service level constraint: Z(x) = { z ∈ Rd : E [ d∑ l=1 max{Yl − zl, 0} | X = x ] ≤ C ′, zl ≥ 0, l = 1, . . . , d } , (35) where C ′ is a constant that stands for the maximal allowable average number of customers experiencing a stock out across items. Examples 2 and 3, Cont’d (Stochastic Constraints in Portfolio Optimization). For another example, we may impose the following mean return constraint with a minimum return of R in the portfolio optimization: Z(x) = {z ∈ Rd+1 : E [ Y ⊤z1:d | X = x] ≥ R, z1:d ∈ ∆d} . (36) More generally we can also include in the constraints any number of criteria or weighted combinations of criteria (mean, variance, CVaR at any level); we need only introduce a separate auxiliary variable for variance and for CVaR at each level considered. These would all constitute stochastic constraints. 38 Kallus and Mao: Stochastic Optimization Forests A.1. Perturbation Analysis In this section, we develop approximate splitting criteria for training forests for general CSO problems described in Eq. (33). We extend the oracle splitting criterion in Eq. (8) to accommodate additional stochastic constraints: Coracle(R1, R2) = ∑ j=1,2 minz∈Zj E [c(z; Y )I [X ∈ Rj]] , (37) Zj =    z : gj,k(z) = E [Gk(z; Y ) | X ∈ Rj] = 0, k = 1, . . . , s, gj,k(z) = E [Gk(z; Y ) | X ∈ Rj] ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    . Again, consider a region R0 ⊆ Rd and its candidate subpartition R0 = R1 ∪ R2, R1 ∩ R2 = ∅. We deﬁne the following family of optimization problems for t ∈ [0, 1]: vj(t) = min z∈Zj (t) f0(z) + t (fj(z) − f0 (z)), zj(t) ∈ arg min z∈Zj (t) f0(z) + t (fj(z) − f0 (z)), j = 1, 2, (38) where Zj(t) =   z : g0,k(z) + t (gj,k(z) − g0,k(z)) = 0, k = 1, . . . , s, g0,k(z) + t (gj,k(z) − g0,k(z)) ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    , j = 1, 2, fj(z) = E [c(z; Y ) | X ∈ Rj] , gj,k(z) = E [Gk(z; Y ) | X ∈ Rj] , j = 0, 1, 2, k = 1, . . . , m. Note that here only the stochastic constraints (and not the deterministic constraints) are interpolated by t, since only stochastic constraints vary from R0 to R1, R2. The oracle criterion is again given by Coracle(R1, R2) = p1v1(1) + p2v2(1). In the following theorem, we present a general perturbation analysis that enables us to approximate v1(1), v2(1) in presence of both deterministic and stochastic constraints. Theorem 6 (Second-Order Perturbation Analysis: Stochastic and Deterministic Constraints). Fix j = 1, 2. Suppose the following conditions hold: 1. f0(z), fj(z), g0,k(z), gj,k(z) for k = 1, . . . , m are twice continuously diﬀerentiable. 2. The problem corresponding to f0(z) has a unique minimizer z0 over Zj(0). 3. The inf-compactness condition: there exist constants α and t0 > 0 such that the constrained level set {z ∈ Zj(t) : f0(z) + t (fj(z) − f0 (z)) ≤ α} is nonempty and uniformly bounded over t ∈ [0, t0). 4. z0 is associated with a unique Lagrangian multiplier (λ0, ν0) that also satisﬁes the strict complementarity condition: λ0,k > 0 if k ∈ Kg(z0) and ν0,k′ > 0 if k′ ∈ Kh(z0), where Kg(z0) = {k : g0,k(z0) = 0, k = s+1, · · · , m} and Kh(z0) = {k′ : hk′(z0) = 0, k′ = s ′ + 1, · · · , m ′} are the index sets of active at z0 inequality constraints corresponding to t = 0. 5. The Mangasarian-Fromovitz constraint qualiﬁcation condition at z0: ∇zg0,k(z0), k = 1, . . . , s are linearly independent, ∇zhk′(z0), k′ = 1, . . . , s′ are linearly independent, and ∃dz s.t. ∇zg0,k(z0)dz = 0, k = 1, . . . , s, ∇zg0,k(z0)dz < 0, k ∈ Kg(z0), ∇zhk′(z0)dz = 0, k′ = 1, . . . , s′, ∇zhk′(z0)dz < 0, k′ ∈ Kh(z0). Kallus and Mao: Stochastic Optimization Forests 39 6. Second order suﬃcient condition: d⊤ z L(z0; λ0, ν0)dz > 0 ∀dz ∈ C(z0) \\ {0}, where L(z; λ, ν) is the Lagrangian for the problem corresponding to t = 0, i.e., L(z; λ, ν) = f0(z) + ∑m k=1 λkg0,k(z) + ∑m′ k′=1 νk′hk′(z) and the critical cone C(z0) is deﬁned as follows: C(z0) = {dz : d⊤ z ∇g0,k(z0) = 0, for k ∈ {1, . . . , s} ∪ Kg(z0) d⊤ z ∇hk′(z0) = 0, for k′ ∈ {1, . . . , s′} ∪ Kh(z0) } . Deﬁne Gj(z0) ∈ Rm as a column vector whose kth element is gj,k(z0), and GKg j (z0) ∈ Rs+|Kg (z0)| as only elements corresponding to equality and active inequality constraints. We analogously deﬁne Deﬁne H(z0) ∈ Rm as a column vector whose kth element is hk(z0), and HKh(z0) ∈ Rs+|Kh(z0)| as only elements corresponding to equality and active at z0 inequality constraints. Then vj(t) = (1 − t)f0(z0) + tfj(z0) + tλ⊤ 0 (Gj(z0) − G0(z0)) + o(t2) + 1 2 t2 {d j∗⊤ z ∇2 zzL(z0; λ0, ν0)d j∗ z + 2d j∗⊤ z (∇fj(z0) − ∇f0(z0) + (∇G⊤ j (z0) − ∇G⊤ 0 (z0) ) λ0)} , (39) z(t) = z0 + td j∗ z + o(t), (40) where d j∗ z is the ﬁrst part of the unique solution of the following linear system of equations:   ∇2 zzL(z0; λ0, ν0) ∇GKg 0 ⊤(z0) ∇HKh ⊤(z0) ∇⊤GKg 0 (z0) 0 0 ∇⊤HKh(z0) 0 0     dj z ξ η   (41) =    − (∇fj(z0) − ∇f0(z0)) − (∇G⊤ j (z0) − ∇G⊤ 0 (z0)) λ0 − (GKg j (z0) − GKg 0 (z0) ) 0    . Theorem 6 looks very similar to Theorem 4 except that we need to account for the presence of stochastic constraints in all conditions, and also in the ﬁnal perturbation result. Note that if we remove the requirement on stochastic constraints in the conditions, and set gj,k(z) = 0 for j = 1, 2, k = 1, . . . , m in Eqs. (39) to (41), then we recover the conclusion in Theorem 4. A.2. Approximate Splitting Criteria Eqs. (39) and (40) in Theorem 6 motivate the following two diﬀerent approximate splitting critera: Capx-risk(R1, R2) = 1 2 ∑ j=1,2 pjd j∗⊤ z ∇2 zzL(z0; λ0, ν0)d j∗ z (42) + ∑ j=1,2 pjd j∗⊤ z (∇fj(z0) − ∇f0(z0) + (∇G⊤ j (z0) − ∇G⊤ 0 (z0)) λ0) , Capx-soln(R1, R2) = ∑ j=1,2 pjfj (z0 + d j∗ z ) , (43) where in the approximate risk criterion, Capx-risk(R1, R2), we omit from the extrapolation the term ∑ j=1,2 pj (fj(z0) + λ ⊤ 0 (Gj(z0) − G0(z0))) = p0 (f0(z0) − λ ⊤ 0 G0(z0)), which does not depend on the choice of subpartition. 40 Kallus and Mao: Stochastic Optimization Forests Algorithm 4 Procedure to make a decision using StochOptForest 1: procedure StochOptForest.Decide(data D, forest {(τj, I dec j ) : j = 1, . . . , T }, target x) 2: w(x) ← Zeros(|D|) ◃ Create an all-zero vector of length |D| 3: for j = 1, . . . , T do 4: N (x) ← {i ∈ I dec j : τj(Xi) = τj(x)} ◃ Find the τj-neighbors of x among the data in I dec j 5: for i ∈ N (x) do wi(x) ← wi(x) + 1 |N (x)|T ◃ Update the sample weights 6: ˆZ(x) ←    z ∈ Rd : ∑ (Xi,Yi)∈D wi(x)Gk(z; Yi) = 0, k = 1, . . . , s, ∑ (Xi,Yi)∈D wi(x)Gk(z; Yi) ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    7: return Minimize(∑ (Xi,Yi)∈D wi(x)c(z; Yi), z ∈ ˆZ(x)) ◃ Compute the forest policy Eq. (3) A.3. Estimating the Approximate Splitting Criteria To estimate the approximate splitting criteria in Eqs. (42) and (43), we still estimate z0 by its sample analogue ﬁrst: ˆz0 ∈ arg min z∈ ˆZ0 ̂p0f0(z), where ̂p0f0(z) := 1 n n∑ i=1 I [Xi ∈ R0] c(z; Yi), (44) ˆZ0 =    z : 1 n ∑n i=1 Gk(z; Y )I [Xi ∈ R0] = 0, k = 1, . . . , s, 1 n ∑n i=1 Gk(z; Y )I [Xi ∈ R0] ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    . Then we can estimate the gradients of fj, gj,k, hk′ at z0, Hessians of f0, g0,k, hk′ at z0, the Lagrangian multipli- ers λ0, ν0, and the index sets Kg(z0), Kh(z0) of active inequality constraints, and d j∗ z as we do in Section 2.4, namely, by estimating all of them at ˆz0. With all of these pieces in hand, we can ﬁnally estimate our approximate criteria in Eqs. (42) and (43). Revisiting the Running Examples. We now illustrate the estimation of gradients and Hessians for stochastic constraints using Eqs. (35) and (36) as examples. The aggregate service level constraint in Eq. (35) has the same structure as the objective function in Example 1 and so estimating the corresponding gradients and Hessians can be done in the same way as estimating the objective gradients and Hessians as in Section 2.4. The minimum mean return constraint in Eq. (36) corresponds to G1(z; Y ) = R − Y ⊤z ≤ 0. Then ∇2gj,1(z0) is zero and we can estimate gj,1(z0) and ∇gj,1(z0) using simple sample averages, as in Example 5, Cont’d in Section 2.4. A.4. Construction of Trees and Forests It is now possible to extend the tree ﬁtting algorithm, Algorithm 1, to the general CSO problem in Eq. (33). We now solve ˆz0 in line 2 using Eq. (44) instead, i.e., using the estimated constraint set ˆZ0. Then we update line 3 to estimate λ0, ν0, Kg(z0), Kh(z0), ∇f0(z0), ∇2f0(z0), ∇g0,k(z0), ∇2g0,k(z0), ∇hk(z0), ∇2hk(z0), and update line 8 to estimate ∇fj(z0), ∇gj,k(z0), d j∗ z . And, ﬁnally, we update line 9 to use the splitting criteria Eqs. (42) and (43) with these estimates. Again, Algorithm 2 for ﬁtting the forest remains the same, since changing the optimization problem only involves how to choose tree splits but not how to combine the tree. Finally, with the extra stochastic constraints, we need to use Algorithm 4 instead of the previous Algorithm 3 for the ﬁnal decision making. The only diﬀerence is that we use the forest weights to approximate the constraint set ˆZ(x) to solve for the ﬁnal forest-policy decision. Kallus and Mao: Stochastic Optimization Forests 41 A.5. Challenges with Stochastic Constraints in CSO Infeasibility of Stochastic Constraints. In presence of stochastic constraints, we may run into infeasible problems. Consider the portfolio optimization problem with the constraint Z(x) = {z ∈ Rd+1 : E [Y ⊤z1:d | X = x] ≥ R, z1:d ∈ ∆d} as an example. Note that if the return requirement is positive, R > 0, and the conditional mean return for every asset given X = x is negative, i.e., E [Yl | X = x] < 0, l = 1, . . . , d, then the constraint set Z(x) is empty since we constrain the decisions z1, . . . , zd to be all nonnegative. Thus the conditional portfolio optimization problem with this constraint set can become infeasible for some point x, even if the unconditional mean return for every asset is positive so the unconditional stochastic optimization counterpart is still feasible. This appears as an intrinsic challenge with conditional stochastic constraints. However, in some cases, infeasibility may not be an issue, and our forest algorithm can still provide quality decision rules. For example, in Appendix A.6, we show that our forest policies still perform well for mean-variance portfolio optimization that allows shortselling, i.e., Z(x) = { z ∈ Rd+1 : E [Y ⊤z1:d | X = x] ≥ R, ∑d l=1 zl = 1} . This problem is often feasible, since we no longer enforce the nonnegativity constraints that may be at odd with the conditional mean return constraint. Violations of Stochastic Constraints. Because the stochastic constraints are not known, we need to estimate conditional expectations of Gk(z; Y ) at X = x to approximate the constraint set Z(x) for every query point x. This is much harder than estimating z0, fj(z0), ∇2f0(z0), ∇fj(z0), etc., for a given ﬁxed R0, R1, R2. It is akin to the diﬀerence between estimating a marginal expectation and estimating a whole regression function. This means that even when given a ﬁxed forest, we may still need to solve nontrivial estimation subproblems ﬁrst for ﬁnal decision-making. If the constraint set is not approximated accurately, then the resulting decisions may violate the stochastic constraints very often. In this setting, our approach in constructing a policy was to use the forest weights to also approximate the stochastic constraints (see Eq. (34) or line 6 in Algorithm 4), and our approach in constructing the forest was to consider an oracle splitting criterion that enforces only the approximate constraints (Eq. (37)). Note that for this reason, the oracle splitting criterion might not necessarily encourage splitting on constraint- determining covariates. Instead, our focus is on considering stochastic constraints in the splitting criterion for the purpose of approximately assessing the change in risk at constrained solutions. Therefore, we may be concerned that using forest weights to approximate stochastic constraints may not estimate the constraints well, and the resulting forest policy may often violate the stochastic constraints. For this reason, our approach may be most relevant when violation of the stochastic constraints can be tolerated. Despite the potential weakness of constraint violation, our approach seems to be a reasonable proxy that still works well in practice (provided that infeasibility is tolerable). See Appendix A.6 for experiments where the constraints and objective even involve completely diﬀerent covariates. Considering more robust variations on our approach in the presence of stochastic constraints to reduce constraint violation may constitute fruitful future research. Indeed, an inherent issue is that the risk of constraint violation is not clearly deﬁned – were it inﬁnite making decisions from data is hopeless, and were it well-deﬁned we may be able to directly address it in the objective. A possible future direction is the 42 Kallus and Mao: Stochastic Optimization Forests enforcement of stochastic constraints with high probability with respect to the sampling process by using distributionally robust constraints, as done for example by Bertsimas et al. (2018a,b) in non-conditional problems. This may be considered both in the construction of a forest policy given a forest as well as in the construction of the forest itself. A crucial diﬀerence with non-conditional problems is that in addition to the variance of estimating expectations from a ﬁnite sample, which the referenced works tackle, we would also need to consider the inevitable bias of estimating a conditional expectation at X = x from a sample where the event X = x is never observed. While the ﬁnite-sample variation may be easier to characterize and introduce robustness for, characterizing the latter bias may involve substantive structural assumptions on how the distribution of Y | X = x changes with small perturbations to x. And, controlling for such perturbations non-adaptively (e.g., by bounding bias using a Lipschitz assumption) may be very susceptible to the curse of covariate dimensionality. A.6. Experiments: Mean-Variance Portfolio Optimization In this section, we apply our methods to the mean-variance portfolio optimization problem (see also Example 2): we seek an investment policy z(·) ∈ Rd that for each x aims to achieve small risk Var (Y ⊤z(x) | X = x) while satisfying a budget and mean return constraint, i.e., Z(x) = Z(x; R) = { z ∈ Rd : ∑d l=1 zl(x) = 1, E [Y ⊤z(x) | X = x] ≥ R} . We consider d = 3 assets and p = 10 covariates. The covari- ates X are drawn from a standard Gaussian distribution, and the asset returns are independent and are drawn from the conditional distributions Y1 | X ∼ Normal (exp(X1), 5 − 4I [−3 ≤ X2 ≤ −1]), Y2 | X ∼ Normal (−X1, 5 − 4I [−1 ≤ X2 ≤ 1]), and Y3 | X ∼ Normal (|X1|, 5 − 4I [1 ≤ X2 ≤ 3]). We compare our StochOptForest algorithm with either the apx-risk and apx-soln approximate criterion for problems with both deterministic constraints and stochastic constraints (Appendix A.2) to four benchmarks: our StochOptForest algorithm with apx-risk and apx-soln criteria that ignore the constraints in the forest construction (Section 2.3), the regular random forest algorithm RandForest (which targets the predictions of asset mean returns), and the RandSplitForest algorithm that chooses splits uniformly at random. We do not compare to StochOptForest with the oracle splitting criterion as it is too computationally inten- sive, as we investigate further below. In all forest algorithms, we use an ensemble of 500 trees where the tree speciﬁcations are the same as those in Appendix C.1. To compute ˆz0 in our StochOptTree algorithm (Algorithm 1 line 2) as well as to compute the ﬁnal forest policy for any forest, we use Gurobi 9.0.2 to solve the linearly-constrained quadratic optimization problem. For each n ∈ {100, 200, 400, 800}, we repeat the following experiment 50 times. We ﬁrst draw a training set D of n to ﬁt a forest policy ˆz(·) using each algorithm. Then we sample 200 query points x0. For each query point x0, we evaluate the condi- tional risk Var (Y ⊤ ˆz(x0) | X = x0, D) using the true conditional covariance matrix Var (Y | X = x0). Note that the forest policy ˆz(·) may not perfectly satisfy the stochastic constraint for conditional mean return, i.e., ˆR(x0) = E [Y ⊤ ˆz(x0) | X = x0, D] may be smaller than the pre-speciﬁed threshold R. We therefore bench- mark its performance against the minimum conditional risk with mean return equal to that of ˆz(·), namely, z∗(x0; ˆR(x0)) = arg minz∈Z(x0; ˆR(x0)) Var (Y ⊤z | X = x0), which we compute by Gurobi using the true condi- tional mean and covariance as input. We then average these conditional risks over the 200 query points x0 Kallus and Mao: Stochastic Optimization Forests 43 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1 2 3 4 5 100 200 400 800 Sample size nRelative risk Constraint yes no Method StochOptForest (apx−soln) StochOptForest (apx−risk) RandForest RandSplitForest (a) Relative risk of diﬀerent forest policies (relative to optimal risk with similar mean return). 1 2 3 4 5 6 7 8 9 10 0.00 0.05 0.10 0.15 Splitting frequencyCovariate index (b) Splitting fre- quency. ● ● ● ● ● ● 0.0 0.1 0.2 0.3 100 200 400 800 Sample size nViolation of Conditional Constraints (c) Conditional constraint violation. 0.000 0.002 0.004 0.006 100 200 400 800 Sample size nViolation of Marginal Constraints (d) Marginal constraint violation. Figure 4 Results for mean-variance portfolio optimization CSO problem. to estimate E [Var (Y ⊤ ˆz(X) | X, D) | D] and E [ Var (Y ⊤z∗(X; ˆR(X)) | X, D) | D] . We deﬁne the relative risk of each forest algorithm for each replication as the ratio of these two quantities. Figure 4a shows the distribution of the relative risk over replications for each forest algorithm across diﬀerent n. The dashed boxes corresponding to “Constraint = no” indicate that the associated method does 44 Kallus and Mao: Stochastic Optimization Forests not take constraints into account when choosing the splits, which applies to all four benchmarks. We can observe that our StochOptForest algorithms with approximate criteria that incorporate constraints achieve the best relative risk over all sample sizes, and their relative risks decrease considerably when the training set size n increases. In contrast, the relative risks of RandForest, RandSplitForest, and StochOptForest with the constraint-ignoring apx-soln criterion decrease very slowly when n increases. Interestingly, in this example, the performance of StochOptForest algorithm with the constraint-ignoring apx-risk criterion performs similarly to our proposed algorithms that do take constraints into account when choosing splits. Figure 4b shows the average frequency of each covariate being selected to be split on in all nodes of all trees constructed by each algorithm over all replications when n = 800. We note that RandForest, RandSplitForest, and StochOptForest with the constraint-ignoring apx-soln criterion split much less often on the covariate X2 that governs the conditional variances of asset returns. Since the conditional variances directly determine the objective function in the mean-variance problem, this roughly explains the inferior performance of these methods. We further evaluate how well the estimated policy ˆz(·) from each forest algorithm satisﬁes the mean return constraint. In Fig. 4c, we present the distribution of the average magnitude of violation for the conditional mean return constraint, i.e., E[max{R − ˆR(X), 0} | D], over replications for each forest algorithm. We can observe that for small n, all methods have similar average violations, while for large n (n ≥ 400), RandForest appears to achieve the smallest average violation, closely followed by StochOptForest with apx-risk criteria (incorporating constraints or not). This is consistent with the fact that these methods split more often on the covariate X1 that governs the conditional mean returns, as seen in Fig. 4b. However, this relative advantage of RandForest in terms of conditional constraint violation is greatly overshadowed by its bad risk, even relative to its more constrained mean return (Fig. 4a). More generally, this seeming advantage in constraint satisfaction is largely due to the fact that RandForest is specialized to predict the conditional mean function well, which fully determines the constraint. For stochastic constraints involving a nonlinear function of Y , we expect RandForest will not satisfy the constraints well just as it fails to do well in the objective here or in Appendix C.1. (See also Appendix A.5.) In Fig. 4d, we further evaluate the violation magnitude for the marginal mean return constraint, i.e., max{E[R − ˆR(X)], 0}, where the expectation inside is averaged over all 50 replications. We note that the violations for all algorithms are extremely small. This means that the marginal mean return constraint implied by the conditional constraint, i.e., E [Y ⊤ ˆz(X) | D] ≥ R, is almost satisﬁed for all algorithms. In Fig. 12 in Appendix C.5, we also evaluate the performance of StochOptForest algorithm with the oracle criterion in a small-scale experiment, and show that the performance of either apx-soln or apx-risk criterion for constrained problems is close to the oracle criterion, despite the fact they are much faster to compute. In Fig. 13 in Appendix C.5, we additionally show that similar results also hold for diﬀerent mean return constraint thresholds R. Appendix B: Variable Importance Measures In Sections 4 and 5, we show both empirically and theoretically that our StochOptForest algorithm can achieve good decision-making performance for CSO problems. However, sometimes we may not only seek Kallus and Mao: Stochastic Optimization Forests 45 quality decisions, but also hope to identify which covariates are important in determining these decisions. In this case, measuring the importance of each covariate is very useful. In prediction tasks, the standard random forest algorithm provides two common ways to measure variable importance: impurity-based importance measure and permutation-based importance measure. Below we ﬁrst describe these two impurity measures for the regular random forest algorithm, and then based on this we motivate variable importance measures for our StochOptForest algorithm. The impurity-based importance measure is also called the Mean Decrease in Impurity (MDI; see Section 6.1.2, Louppe 2015), which is based on the impurity measure used in tree splitting, e.g., entropy or Gini index for classiﬁcation trees and variance for regression trees. The MDI of each covariate is a weighted sum of impurity decreases for all tree nodes that split on this covariate, averaged over all trees in a forest. To formalize it, ﬁx a forest consisting of trees τ1, . . . , τT and for each internal node t in each tree τi (denoted as t ∈ τi with slight abuse of notation), denote its splitting covariate as ˆjt, the number of data points reaching the node as nt, and the impurity decrease due to this split as ∆I(ˆjt, t). Then the MDI importance measure for a covariate Xj can be written as MDI (j) = 1 T T∑ i=1 ∑ t∈τi I [ ˆjt = j] ˆpt∆I(ˆjt, t). (45) where ˆpt = nt n estimates the probability of an observation reaching the node t. Another importance measure is based on a permute-and-predict procedure using out-of-bag samples (Hastie et al. 2001, Section 15.3.2). Suppose we hope to measure the importance of a covariate Xj based on a given random forest. Then for each tree in this random forest, we ﬁrst record its prediction accuracy on the out-of-bag samples (i.e., samples that were not used to build this tree), and then compute its prediction accuracy again after randomly permuting the Xj observations in the out-of-bag samples. Then we measure the importance of Xj by the decrease in accuracy due to permuting this covariate, averaged over all trees in the random forest. It is natural to consider extending these two types of variable importance measures to our StochOptForest algorithm. First, consider a direct analogue of the permutation-based importance measure in the decision- making setting: we evaluate the increase in decision cost due to permuting each covariate in each tree, and average them over all trees. However, to compute the decisions for out-of-bag samples and evaluate their costs, we need to solve optimization problems in all leaf regions of each decision tree. This can be very time consuming when the trees are deep (so they have many leaf regions) and when there are a large number of trees. Therefore, permutation-based importance measures may often be too computationally intensive for our proposed algorithm. Instead, we focus on impurity-based variable importance measures for our proposed algorithm, as it only requires quantities that are already computed in the tree construction process. Recall that the impurity- based variable importance measures for the random forest algorithm uses the same impurity measure as that in the tree splitting criterion (e.g., Gini index, entropy, or variance). This motivates us to view our proposed tree splitting criteria as the impurity measures. We ﬁrst consider the oracle splitting criterion in Eq. (8). To formalize its variable importance measure, ﬁx an internal node t0 of a tree τi that splits on the covariate 46 Kallus and Mao: Stochastic Optimization Forests ˆjt0, denote its two children nodes as t1 and t2, and denote the probability of an observation reaching these nodes as pt0 , pt1 , pt2 respectively (which can be easily estimated by the fractions of samples reaching these nodes). Viewing these three nodes as regions R0, R1, R2 respectively, a natural way to measure the impurity decrease due to the split ˆjt0 is ∆I oracle(ˆjt0 , t0) = v0 (0) − ( pt1 pt0 v1 (1) + pt2 pt0 v2 (1) ) = v0 (0) − 1 pt0 Coracle (R1, R2) (46) When using the approximate risk criterion, we note that pt0f0 (z0) + Capx-risk(R1, R2) approximates Coracle (R1, R2) (see Theorem 2), so naturally the impurity decrease under the the apx-risk criterion is ∆I apx-risk(ˆjt0 , t0) = − 1 pt0 Capx−risk (R1, R2) . (47) When using the apx-sol criterion, we note that Capx-soln(R1, R2) approximates Coracle (R1, R2) (see Theo- rem 3), so naturally ∆I apx-soln(ˆjt0 , t0) = v0 (0) − 1 pt0 Capx−soln (R1, R2) . (48) Depending on which criterion is used in the StochOptForest, we can estimate the corresponding impurity decrease measure in Eqs. (46) to (48) and plug it into Eq. (45) to quantify the variable importance of each covariate. Finally, since the importance measures are relative, we normalize them by assigning the largest a value of 1 and scaling the others accordingly. Appendix C: Additional Experimental Details C.1. Multi-item Newsvendor We here consider an experiment on an unconstrained multi-item newsvendor problem (see Example 1). We consider d = 2 products and p-dimensional covariates X drawn from a standard Gaussian dis- tribution. The conditional demand distributions are Y1 | X ∼ TruncNormal(3, exp(X1)) and Y2 | X ∼ TruncNormal(3, exp(X2)), where TruncNormal(µ, σ) is the distribution of W | W ≥ 0 where W is Gaussian with mean µ and standard deviation σ. The holding costs are α1 = 5, α2 = 0.05 and the backorder costs are β1 = 100, β2 = 1. We begin by comparing forest policies using diﬀerent algorithms to construct the forest. We compare our StochOptForest algorithm with either the apx-soln or apx-risk approximate splitting criterion to three benchmarks. All forest-constructing algorithms we consider are identical except for their splitting criterion. One benchmark is StochOptForest with the brute-force oracle splitting criterion, which uses the empirical counterpart to Eq. (8) (i.e., E is replaced with 1 n ∑n i=1) and fully re-optimizes for each candidate split. A second benchmark is the standard random forest (RandForest) algorithm, which uses the squared error splitting criterion (Example 4). Finally, since z∗(x) is the vector of conditional 95% quantiles of (Y1, Y2) | X, we also consider the GenRandForest algorithm for quantile regression (Example 2 and Section 5 of Athey et al. 2019; see also Section 6.1). For all forest-constructing algorithms, we use 500 trees, each tree is constructed on bootstrap samples (I tree j = I dec j ), candidate splits are all possible splits with at least 20% of observations in each child node, and the minimum node size is 10. Kallus and Mao: Stochastic Optimization Forests 47 ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1.0 1.5 2.0 2.5 100 200 400 800 Sample size nRelative risk Method StochOptForest (oracle) StochOptForest (apx−soln) StochOptForest (apx−risk) GenRandForest RandForest (a) Relative risk of diﬀerent forest policies. 1 2 3 4 5 6 7 8 9 10 0.00 0.05 0.10 0.15 0.20 Splitting frequencyCovariate index (b) Splitting fre- quency. 0.00 0.25 0.50 0.75 1.00 1 2 3 4 5 6 7 8 9 10 Covariate indexNormalized Importance (c) Feature Importance. Figure 5 Results for the multi-item newsvedor CSO problem with varying n and ﬁxed p = 10. To compare these diﬀerent algorithms, we let p = 10 and for each n in {100, 200, 400, 800} we repeat the following experiment 50 times. We ﬁrst draw a training set D of size n to ﬁt a forest policy, ˆz(·), using each of the above algorithms. Then we sample 200 query points x0. For each such x0 and for each policy ˆz(·), we compute ˆz(x0) and then take the average of c(ˆz(x0); y) over 2000 values of y drawn from the conditional distribution of Y | X = x0. We also compute the average of c(z∗(x0); y) over these. We average these over the 200 query points x0. This gives estimates of E [c(ˆz(X); Y ) | D] and E [c(z∗(X); Y )]. The relative risk for each algorithm and each replication is the ratio of these. In Fig. 5a, we plot the distribution of relative risk over replications for each forest algorithm and n. The ﬁrst thing to note is that for n ≥ 400, the performance of our approximate splitting criteria appear identical to the oracle criterion, as predicted by Theorems 2 and 3 and Proposition 1. The second thing to note is that RandForest and GenRandForest have relative risks that are on average roughly 10–16% worse than our StochOptForest algorithm. 48 Kallus and Mao: Stochastic Optimization Forests 1 2 3 4 StochOptForest (apx−soln) StochOptForest (apx−risk) RandSplitForest 5−NN 10−NN 50−NN MethodRelative risk p 5 10 20 40 80 (a) Relative risk for varying p and ﬁxed n = 800: StochOptForest vs. non-adaptive weighting. 1 2 3 4 StochOptForest (apx−soln) StochOptForest (apx−risk) RandSplitForest 5−NN 10−NN 50−NN MethodRelative riskp 10 100 200 250 (b) High dimensional setting with ﬁxed n = 200. Figure 6 Results for the multi-item newsvedor CSO problem with ﬁxed n and varying p. One way to roughly understand these results is to consider how often each algorithm splits on each covariate. Recall there are p = 10 covariates, the ﬁrst and the second determine the distribution of the two products, respectively. The ﬁrst product, however, has higher costs by a factor of 100. Therefore, to have a well-performing forest policy, we should ﬁrst and foremost have good forecasts of the demand of the ﬁrst product, and hence should split very ﬁnely on X1. Secondarily, we should consider the second product and X2. This is exactly what StochOptForest does. To visualize this, in Fig. 5b, we consider how often each variable is chosen to be split on in all the nodes of all the trees constructed by each forest algorithm over all replications with n = 400. We notice that our StochOptForest algorithms indeed split most often on X1, while in contrast algorithms focusing on estimation (RandForest and GenRandForest) split equally often on X2. More practically, in CSO problems generally, how important variables are for estimating optimal decisions is diﬀerent than how they impact decision costs and the latter is of course most crucial for eﬀective decision making. StochOptForest targets this by directly constructing trees that target their decision risk rather than estimation accuracy. In Fig. 5c, we also plot the impurity-based variable importance measure for each forest algorithm (see Appendix B). We do not include the GenRandForest algorithm there since Athey et al. (2019) does not provide any variable importance measure. Overall the results in Fig. 5c are consistent with those in Fig. 5b: our proposed algorithms value X1 the most, while the RandForest algorithm attaches equal importance to both X1 and X2, which again conﬁrms that our proposed method capture signals more relevant to the optimization problem. Finally, we comment on how StochOptForest handles high dimensional features eﬀectively. We ﬁrst con- sider n = 800 and vary p in {5, 10, 20, 40, 80}. We compare to non-adaptive weighting methods for CSO, which Kallus and Mao: Stochastic Optimization Forests 49 construct the local decision weights wi(x) without regard to the data on Y or to the optimization prob- lem (Bertsimas and Kallus 2014). Speciﬁcally, we consider two non-adaptive weighting schemes: k-nearest neighbors (kNN), where wi(x) = 1/k for the Xi that are the k nearest to x, and random-splitting forest (RandSplitForest), where trees are constructed by choosing a split uniformly at random from the candidate splits (this is the extreme case for the Extremely Randomized Forests algorithm, Geurts et al. 2006). We plot the relative risks (computed similarly to the above) for each algorithm and p in Fig. 6a. As we can see, non-adaptive methods get worse with dimension due to the curse of dimensionality, while the risk of our StochOptForest algorithms remains stable and low. In Fig. 6b, we consider a more challenging setting where the covariate dimension can be as large as or larger than the sample size: we ﬁx n = 200 and increase the covariate dimension from p = 10 to p = 250. We can observe that the performance of our proposed meth- ods does deteriorate when the covariate dimension is very high, but they still signiﬁcantly outperform the non-adaptive methods. Interestingly, when the dimension grows from p = 200 to p = 250, the performance of all methods slightly improve, which is somewhat inconsistent with the conventional wisdom of “curse of dimensionality.” We do not have very good explanations for this phenomenon, but we conjecture that this may be related to counter-intuitive behaviors of interpolating estimators in supervised learning (Bartlett et al. 2020, Belkin et al. 2019, 2018, Hastie et al. 2020). For example, it was observed that in linear regression, when the regressor dimension exceeds the sample size, further increasing the dimension may actually improve the out-of-sample prediction performance as long as we focus on the minimum-norm solution. Studying this phenomenon in an optimization context is out of the scope of this paper and we leave it for future study. C.2. More details for CVaR Portfolio Optimization Additional details for Section 4.1. For all algorithms in Section 4.1, the forest speciﬁcations are the same as those in Appendix C.1: each forest consists of 500 trees, each tree is constructed on bootstrap samples (I tree j = I dec j ), candidate splits are all possible splits with at least 20% of observations in each child node, and the minimum node size is 10. To evaluate the the relative risks of diﬀerent forest policies, we follow the testing data generation process in Appendix C.1. We ﬁrst sample 200 query points x0 from the marginal distribution of X. For each such x0 and for each policy ˆz(·), we compute CVaR0.2 (Y ⊤ ˆz(x0) | X = x0) based on 2000 values of y drawn from the conditional distribution of Y | X = x0. We also compute CVaR0.2 (Y ⊤z∗(x0) | X = x0) based on the same data. Then we average these over the 200 query points x0 to estimate E [CVaR0.2 (Y ⊤ ˆz(x) | X) | D] and E [CVaR0.2 (Y ⊤z∗(x) | X)]. The relative risk for each algorithm and each replication is the ratio of these, which we plot in Fig. 2a. Computing the benchmark splitting criteria that ignore the constraints (our approximate criteria that mistakenly ignore the constraints and the GenRandForest algorithm) requires inverting Hessian estimates. But Hessian estimates for the CVaR objective may often not be invertible. When this happens, we add 0.001 times an identity matrix of conformable size to the Hessian estimates so we can invert them and these splitting criteria that ignore the constraints can still run. In contrast, our proposed approximate criteria that incorporate constraints require inverting the left hand side coeﬃcient matrices in Eq. (26). These matrices are usually invertible thanks to the constraint gradients therein. 50 Kallus and Mao: Stochastic Optimization Forests 0.00 0.05 0.10 0.15 1 2 3 4 5 6 7 8 9 10 Covariate indexSplitting Frequency Constraint yes no Method StochOptForest (apx−risk) StochOptForest (apx−soln) RandForest GenRandForest (modified) Figure 7 The average frequency of splitting on each covariate by diﬀerent forest policies. Feature Splitting Frequency. In Fig. 7, we show how often each variable is chosen to be split on in all the nodes of all trees constructed by several forest algorithms over all replications with n = 800. This complements the variable importance measures shown in Fig. 2b, oﬀering an alternative way to understand the behaviors of each forest algorithm. We can observe that the RandForest algorithm splits on the ﬁrst covariate more frequently than any other covariate, as it targets the conditional mean asset returns that are inﬂuenced more by the ﬁrst covariate. This is in line with the observation in Fig. 2b that the RandForest algorithm attaches more importance to the ﬁrst covariate. Moreover, we note that our proposed criteria choose to split on both of the ﬁrst two covariates very frequently as both of them inﬂuence the conditional asset return distributions. At the same time, according to Fig. 2b, splits on the second covariate result in much larger criterion decreases. Finally, we observe that the GenRandForest algorithm also splits on the signal covariates (i.e., the ﬁrst two covariates) more frequently than any of the noise covariates (i.e., the 3rd to 10th covariate), but compared to our proposed methods, the GenRandForest algorithm does still waste more splits on the noise covariates. Performance of StochOptForest (oracle) We further evaluate the performance of the StochOptForest (oracle) algorithm for CVaR optimization (Section 4.1), but because this algorithm has extremely slow running time (see Table 1), we can only do so for a very small-scale experiment. In this experiment, we apply each forest algorithm to construct an ensemble of 50 trees with the same tree speciﬁcations as those in Section 4.1. In Fig. 8, we show the relative risk of each forest policy over 50 repetitions for diﬀerent training data size n ∈ {100, 200, 400}. We can observe that again our StochOptForest algorithms considerably outperform other benchmark methods that do not take the cost structure or constraint structure of CVaR optimization problem into account. Moreover, we observe that when n = 400, the StochOptForest algorithm with the oracle criterion tends to perform better than our approximate criteria. However, this observation may be limited to only this small-scale experiment, and we cannot evaluate whether the our approximate criteria and the oracle criterion perform similarly for larger sample size because the StochOptForest algorithm with the oracle criterion is too slow. Kallus and Mao: Stochastic Optimization Forests 51 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ● 1.00 1.25 1.50 1.75 2.00 100 200 400 Sample size nRelative risk Constraint yes no Method StochOptForest (oracle) StochOptForest (apx−soln) StochOptForest (apx−risk) RandForest RandSplitForest Figure 8 Comparing StochOptForest(oracle) with other forest methods in small-scale experiments of CVaR Optimization. ● ●● ● ● ● ●● ● ●● ● ● ● ● 1.00 1.25 1.50 1.75 2.00 0 1 2 ρRelative risk Constraint yes no Method StochOptForest (apx−soln) StochOptForest (apx−risk) RandForest RandSplitForest Figure 9 Relative risk of diﬀerent forest policies for portfolio optimization with a weighted combination of CVaR and mean as objective. The weight of mean is ρ. Linear Combination of CVaR and Mean Return as Objective In Fig. 9, we apply the forest algo- rithms to optimize a linear combination of CVaR and mean returun: CVaR0.1(Y ⊤z1:d | X) − ρE [Y ⊤z1:d | X] for ρ ∈ {0, 1, 2} and n = 400. All other speciﬁcations are the same as those in Section 4.1. We observe that across all ρ values, our StochOptForest methods with constraints-aware approximate criteria perform the best. 52 Kallus and Mao: Stochastic Optimization Forests ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ●● ● ● ● ● ● 1 2 3 4 100 200 400 800 Sample size nRelative risk (a) Relative risk of diﬀerent forest poli- cies. ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● 1 2 3 4 100 200 400 Sample size nRelative risk Constraint yes no Method StochOptForest (oracle) StochOptForest (apx−soln) StochOptForest (apx−risk) RandForest RandSplitForest (b) Evaluating StochOptForest(Oracle) policies. Figure 10 CVaR Optimization for asset return data drawn from Gaussian distributions. Data from Gaussian Distribution In Fig. 10, we present results for CVaR optimization with the asset returns drawn from Gaussian distributions. The experiment setup is the same as that in Section 4.1, except that now the data are drawn from the same Gaussian distributions in Appendix A.6, namely, the covariates X are drawn from a standard Gaussian distribution, and the asset returns are independent and are drawn from the conditional distributions Y1 | X ∼ Normal (exp(X1), 5 − 4I [−3 ≤ X2 ≤ −1]), Y2 | X ∼ Normal (−X1, 5 − 4I [−1 ≤ X2 ≤ 1]), and Y3 | X ∼ Normal (|X1|, 5 − 4I [1 ≤ X2 ≤ 3]). In Fig. 10a, we again compare the StochOptForest algorithm with our approximate criteria to other benchmarks, using the same tree and forest speciﬁcations as we do in Section 4.1. In Fig. 10b, we evaluate the oracle criterion for small forests consisting of 50 trees. We can observe that the results are qualitatively the same as those in Section 4.1 based on asset return data drawn from asymmetric lognormal distributions. C.3. More Details for CVaR Shortest Path Problems In Section 4.2, we solve a shortest path problem with a conditional CVaR objective using real data from Uber Movement (https://movement.uber.com/). Uber Movement provides historical traveling times from one basic geographical unit to another in many major cities worldwide during ﬁve periods in each day (AM Peak, 7am to 10am; Midday, 10am to 4pm; PM Peak, 4pm to 7pm; Evening, 7pm to 12am; Early Morning, 12am to 7am). These traveling times are estimated from all Uber trips that passed the two basic geographical units during the corresponding time. The meaning of a basic geographical unit may vary across diﬀerent cities. In Section 4.2, we focus on Los Angeles where the basic geographical unit is the census tract. In particular, we consider a region in downtown Los Angeles consisting of 45 census tracts, which is depicted in Fig. 3a. We aim to go from an eastmost census tract (green mark, roughly Aliso Village) to a westmost census tract (red mark, roughly MacArthur Park). We collected traveling time observations for d = 93 edges during each Kallus and Mao: Stochastic Optimization Forests 53 of the ﬁve periods in each day of 2018 and 2019, where each edge represents a path from one census tract to one of its neighbors in the region of interest. We denote the corresponding traveling times as Y ∈ Rd. Our goal is to choose a path between the departure point to the destination, denoted by z ∈ {0, 1}d, to minimize CVaR0.8 (Y ⊤z | X = x0) for each covariate value x0 of interest. Optimization Formulation. This shortest path problem can be represented by a directed graph consist- ing of 45 nodes and 93 edges. We denote the set of nodes as N with the 1st node as the departure point and the 45th node as the destination. If there exists an edge from a node i ∈ N to a node j ∈ N , we denote it as i → j, and denote the set of all 93 edges as A. Then for each decision z ∈ {0, 1}d, we can index its coordinates by zi→j for i, j ∈ N such that i → j ∈ A. Then, zi→j = 1 means that we decide to travel along the edge i → j and zi→j = 0 means otherwise. In terms of the notations above, we can write the CVaR shortest path problem as follows: z∗ (x) ∈ arg min z∈Z CVaR0.8 (Y ⊤z | X = x) , Z =   z ∈ Rd : zi→j ≥ 0 for any i → j ∈ A∑ j:i→j∈A zi→j − ∑ i:j→i∈A zj→i = 1 if i = 1∑ j:i→j∈A zi→j − ∑ i:j→i∈A zj→i = −1 if i = 45∑ j:i→j∈A zi→j − ∑ i:j→i∈A zj→i = 0 for any i ∈ N \\ {1, 45}    . (49) Note that we do not enforce integer constraints. Data Speciﬁcations. We consider four diﬀerent sample sizes: half-year data (2019.07.01 to 2019.12.31), one-year data (2019.01.01 to 2019.12.31), one-and-half-year data (2018.07.01 to 2019.12.31), and two-year data (2018.01.01 to 2019.12.31). We consider p = 197 covariates including weather (Temperature, Wind Speed, Precipitation, Visibility in Miles), period dummy variables (AM Peak, Midday, PM Peak, Evening, Early Morning), weekday dummy variables, month dummy variables, 1-day-lag traveling times along all edges, and 7-day-lag traveling times along all edges. Forest Speciﬁcations. In the experiment in Section 4.2, all forests use the same speciﬁcations except for the tree splitting criterion. In particular, they all consist of 100 trees, where each tree is constructed on bootstrap samples (I tree j = I dec j ) and the minimum node size is 10. To reduce computation, in every step of tree construction, we do not consider all possible splits. Instead, we generate candidate splits by ﬁrst randomly selecting 65 covariates out of the total 197 covariates (i.e., around 1/3 of covariates8), then randomly drawing 365 cutoﬀ values from all possible ones for each of these selected covariate, and ﬁnally restricting to the subset of these splits that results in at least 20% of observations in each child node. As in the CVaR portfolio optimization experiment in Section 4.1, whenever we need to invert a numerically singular matrix estimate, we add 0.001 times an identity matrix of conformable size to the matrices to be inverted, as in Appendix C.2 . Unlike Section 4.1, this becomes an issue also for our criteria that do consider constraints. Indeed, the CVaR shortest path problem has integer-valued optimal solutions so it is not particularly smooth, thus the second order perturbation analysis in Theorem 4 may not strictly hold. Nevertheless, the perturbation analysis still provides a principled way to incorporate optimization 8 This is the default choice in the ordinary random forest algorithm for regression problems. 54 Kallus and Mao: Stochastic Optimization Forests problem structure into tree splitting criteria while remaining computationally eﬃcient. (Moreover note that the estimated Hessians in these singular matrices are based on probably-misspeciﬁed Gaussian assumptions so they are approximations anyways). For the apx-soln criterion, this may lead to approximate solutions that slightly violate the ﬂow preservation constraints in Eq. (49) so we project the approximate solutions back onto their aﬃne hull, which is fast operation. But we do not modify the apx-risk criterion any further. C.4. Minimum-variance Portfolio Optimization ● ● ● ● ● ● ● ● ● ● ● ● ● 1 2 3 4 5 6 100 200 400 Sample size nRelative risk Constraint yes no Method StochOptForest (oracle) StochOptForest (apx−soln) StochOptForest (apx−risk) RandForest RandSplitForest (a) Comparing StochOptForest(oracle) policies with other forest policies in small-scale experiments. 1 2 3 4 5 6 7 8 9 10 0.00 0.05 0.10 0.15 0.20 0.25 Splitting frequencyCovariate index (b) Splitting fre- quency. ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● 2 4 6 100 200 400 800 Sample size nRelative risk (c) Comparing diﬀerent forest policies in large-scale experiments. Figure 11 Results for minimum-variance portfolio optimization without stochastic constraints. Kallus and Mao: Stochastic Optimization Forests 55 Method n = 100 n = 200 n = 400 StochOptTree (oracle) 10.24 (1.51) 32.60 (7.63) 90.40 (13.67) StochOptTree (apx-risk) 0.10 (0.04) 0.23 (0.04) 0.76 (0.09) StochOptTree (apx-soln) 0.08 (0.01) 0.28 (0.05) 1.48 (0.51) Table 2 Mean running time (in seconds) of constructing one tree for diﬀerent algorithms in minimum-variance portfolio optimization over 10 repetitions. Numbers in parentheses indicate the standard deviation of running time over repetitions. In Fig. 11, we compare diﬀerent forest policies for minimizing Var(Y ⊤z1:d | X = x) with constraint set Z = {z ∈ Rd+1 : z1:d ∈ ∆d}. The experiment setup and forest speciﬁcations are the same as those in Section 4.1. We only show results for return data drawn from Gaussian distributions described in Appendix A.6, and the results for asymmetric lognormal distributions described in Section 4.1 are similar so we omit them here. In Fig. 11a, we compare the StochOptForest algorithm with the oracle criterion on a small-scale exper- iment where forests consist of 50 trees and training data size ranges from 100 to 400. We ﬁnd that the performance of our apx-risk criterion is very close to the oracle criterion, despite that our apx-risk criterion is much faster to compute. All StochOptForest algorithms that account for the optimization structure achieve better performance than the benchmark methods RandForest, RandSplitForest, and StochOptForest with the constraint-ignoring apx-soln criterion. Interestingly, the StochOptForest algorithm with the constraint- ignoring apx-risk criterion performs quite well, although it fails to incorporate the constraint structure. However, we still recommend using approximate criteria that incorporate the constraints, since they consis- tently perform well across diﬀerent optimization problems and ignoring the constraints may undermine the performance. For example, in the CVaR optimization experiments in Section 4.1, we ﬁnd that ignoring the constraints in approximate criteria can considerably hurt their performance. Moreover, in Fig. 11b we show the average feature splitting frequencies of diﬀerent forest algorithms. We can observe that all well-performing methods frequently split on X2 that determines the conditional variance of asset returns and thus the objective function, while those ill-performing methods typically split on X2 much less often. This partly explains the observations in Fig. 11a. In Fig. 11c, we also evaluate diﬀerent tree algorithms on larger-scale experiments with forests consisting of 500 trees and sample size up to n = 800, which again shows the superior performance of our proposed methods. Finally, we show the running time of each tree algorithm for minimum-variance portfolio optimization in Table 2. We can observe that the StochOptTree algorithm with the apx-risk criterion is more than 100 times faster than the StochOptTree algorithm with the oracle criterion for all sample sizes. C.5. Mean-variance Portfolio Optimization In this section, we provide more experimental results on the mean-variance portfolio optimization in Appendix A.6. In Fig. 12, we compare the performance of StochOptForest with oracle criterion with other methods, in particular StochOptForest with our apx-risk and apx-sol approximate criteria. Because of the tremendous computational costs of StochOptForest (oracle), here we compare forests consisting of 50 trees and consider 56 Kallus and Mao: Stochastic Optimization Forests ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● 1 2 3 4 5 100 200 400 Sample size nRelative risk Constraint yes no Method StochOptForest (oracle) StochOptForest (apx−soln) StochOptForest (apx−risk) RandForest RandSplitForest (a) Relative risk of diﬀerent forest policies. 1 2 3 4 5 6 7 8 9 10 0.00 0.05 0.10 0.15 0.20 Splitting frequencyCovariate index (b) Splitting fre- quency. Figure 12 Comparing StochOptForest(oracle) with other forest methods in small-scale experiments for mean- variance portfolio optimization. n up to 400. We note that the performance of our approximate criteria is very similar to the oracle criterion, and the results for all other methods are similar to those in Fig. 4a. In Fig. 13, we show additional results for the experiments in Appendix A.6. More concretely, ?? presents the relative risks of diﬀerent forest policies when training set size n = 400 and the conditional mean return constraint threshold R varies in {0.1, 0.3, 0.5}. We can see that the performance comparisons are very stable across diﬀerent thresholds R. C.6. Honest Forests In Fig. 14, we evaluate the performance of honest forests that use independent datasets to construct trees and form tree weights respectively (see Assumption 1), and dishonest forests that use the same datasets to construct trees and tree weights (see Section 4). The speciﬁcations of experiments in Fig. 14a and Fig. 14b are the same as those in Section 4.1 and Appendix C.1 respectively, except that here each tree is constructed from a subsample of size (1 − 1 e )n ≈ 0.63n drawn randomly without replacement from the whole training data. This fraction is the expected size of distinct data points in a bootstrap sample. We can observe that honest forests tend to be outperformed by the dishonest counterparts, especially for large n in the CVaR optimization problem and for small n in the newsvendor problem. Kallus and Mao: Stochastic Optimization Forests 57 ● ● ● ● ●● ● ● ● ● ● ● ●●● ●●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 2 4 6 0.1 0.3 0.5 Mean Return Threshold RRelative risk Figure 13 Additional results for mean-variance portfolio optimization experiments in Appendix A.6: relative risks for diﬀerent return constraint thresholds. ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● 1.00 1.25 1.50 1.75 2.00 100 200 400 800 Sample size nRelative risk (a) CVaR Optimization. ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● 1.0 1.5 2.0 2.5 3.0 100 200 400 800 Sample size nRelative risk Method StochOptForest (apx−soln) StochOptForest (apx−risk) Honesty yes no (b) Newsvendor problem. Figure 14 Honest forests vs dishonest forests in CVaR optimization and newsvendor problem. Appendix D: Perturbation Analysis In this section we review perturbation analysis of stochastic optimization and use these tools to prove Theorems 1 and 4. 58 Kallus and Mao: Stochastic Optimization Forests D.1. A heuristic argument We ﬁrst give a heuristic argument for Theorem 4 by applying the implicit function theorem to the KKT system. This argument does not treat many regularity conditions rigorously, but it is simple and instructive. We defer our review of the more general and rigorous analysis developed by Bonnans and Shapiro (2000) to Appendices D.2 and D.3 below. Consider a constrained version of the perturbation formulation in Eq. (9): min z∈Z f0(z) + t (fj(z) − f0(z)), where Z = {z ∈ Rd : hk(z) = 0, k = 1, . . . , s, hk(z) ≤ 0, k = s + 1, . . . , m } . Assume that the problem above corresponding to t = 0 has a unique optimal solution z0 with a unique Lagrangian multiplier v0. Let Kh(z0) = {k : hk(z0) = 0, k = s + 1, · · · , m} be the index set of inequality constraints active at z0 and assume the strict complementarity condition, namely, v0,k > 0 if and only if k ∈ Kh (z0) or k ≤ s. Under the Mangasarian-Fromovitz constraint qualiﬁcation condition (condition 5 in Theorem 4), z0 and v0 can be characterized by the following Karush–Kuhn–Tucker (KKT) system: ∇f0 (z0) + ∑ k∈{1,...,s}∪Kh(z0) ν0,k∇hk (z0) = 0, (50) hk (z0) = 0, k ∈ Kh (z0) . (51) Now consider the problem above with t ̸= 0. Assume that it has an optimal solution zj (t) with a Lagrangian multiplier νj (t). When t is very close to 0, we may conjecture that zj (t) is close to z0 so that the inequalities active at zj (t) are still given by Kh (z0), vj,k (t) > 0 if and only if k ∈ Kh (z0) or k ∈ {1, . . . , s}, and zj (t) and νj (t) are also characterized by the corresponding KKT system: Γj (zj (t) , νj (t) , t) = 0 (52) where Γj (z, ν, t) = [ ∇f0 (z) + t (∇fj (z) − ∇f0 (z)) + ∑ k∈{1,...,s}∪Kh(z0) νk∇hk (z) HKh (z) ] . Above HKh (z) is a column vector whose elements are hk (z) for k ∈ {1, . . . , s} ∪ Kh (z0). Note that Eqs. (50) and (51) imply Γj (z0, ν0, 0) = 0. Moreover, the Jacobian matrix of Γj (z, ν, t) at (z0, ν0, 0) is ∇⊤ z,νΓj (z0, ν0, 0) = [ (∇2f0(z0) + ∑m k=1 ν0,k∇2hk(z0)) ∇HKh ⊤(z0) ∇⊤HKh(z0) 0 ] When this Jacobian matrix is invertible, the implicit function theorem ensures that for t close enough to 0, there exist unique and continuously diﬀerentiable zj (t) and νj(t) such that Γj (zj (t) , νj (t) , t) = 0, and dj∗ z = ∂ ∂t z (t) |t=0 and ξj = ∂ ∂t νj (t) |t=0 are solutions to the following linear equation system: [ (∇2f0(z0) + ∑m k=1 ν0,k∇2hk(z0)) ∇HKh ⊤(z0) ∇⊤HKh (z0) 0 ] [ dj∗ z ξj ] = ∂ ∂t Γj (z0, ν0, 0) = [− (∇fj(z0) − ∇f0(z0)) 0 ] . (53) This implies that zj (t) = z0 + td j∗ z + o(t), which is exactly the conclusion in Eq. (28) in Theorem 4. Kallus and Mao: Stochastic Optimization Forests 59 Moreover, by the fact that (zj (t) , vj (t)) forms a KKT pair, the optimal value vj (t) has the following formulation: vj (t) = f0(zj (t)) + t (fj(zj (t)) − f0(zj (t))) + ∑ k∈{1,...,s}∪Kh(z0) νj,k (t) hk (zj (t)) . We can then use the chain rule to derive the ﬁrst and second order derivatives of vj (t) at t = 0 in terms of derivatives of zj (t) at t = 0 and gradients of f0, fj. Proposition 3. Suppose that zj (t) , νj (t) are twice continuously diﬀerentiable at t = 0. Then ∂ ∂t vj (t) |t=0 = fj(z0) − f0(z0), ∂2 ∂t2 vj (t) |t=0 = d j∗⊤ z ( ∇2f0(z0) + m∑ k=1 ν0,k∇2hk(z0) ) d j∗ z + 2d j∗⊤ z (∇fj(z0) − ∇f0(z0)) . Note Proposition 3 agrees with the conclusion of Theorem 4 in Eq. (27). The above argument is largely heuristic as it makes many assumptions without justiﬁcations, like the preservation of the active index set in the perturbed problems, the KKT formulation for the perturbed solu- tions, and the twice continuous diﬀerentiability of the primal and dual solutions to the perturbed problems, etc.. In Appendices D.2 and D.3, we summarize a more rigorous and more general perturbation analysis. D.2. General Perturbation Analysis In this section, we give an overview of the second order perturbation analysis based on results in Bonnans and Shapiro (2000). Consider the following generic parameterized problem: for z, u in ﬁnitely dimensional vector spaces Z, U respectively, minz f (z, u) s.t. gk(z, u) = 0, k = 1, . . . , s, gk(z, u) ≤ 0, k = s + 1, . . . , m, (54) where both f (z, u) and gk(z, u) are twice continuously diﬀerentiable in both z and u. We denote the ﬁrst and second order derivatives of f w.r.t (z, u) as operators Df (z, u) and D2f (z, u) repsectively: Df (z, u)(dz, du) = Dzf (z, u)(dz) + Duf (z, u)(du) = d ⊤ z ∇zf (z, u) + d ⊤ u ∇uf (z, u) D2f (z, u)((dz, du), (dz, du)) = Dzzf (z, u)(dz, dz) + Dzuf (z, u)(dz, du) + Duzf (z, u)(du, dz) + Duuf (z, u)(du, du) = d ⊤ z ∇zzf (z, u)dz + d ⊤ u ∇uuf (z, u)du + 2d⊤ z ∇zuf (z, u)du. We can similarly denote the partial derivatives of f w.r.t z and u by Dzf (z, u) and Duf (z, u) respectively. Derivatives for gk can be deﬁned analogously. Consider the parabolic perturbation path u(t) = u0 + tdu + 1 2 t 2ru + o(t 2) for t > 0 and some elements du, ru such that u(t) ∈ ∈ U, and denote the associated optimization problem as Pu(t) with optimal value as V (u(t)). We assume that the unperturbed problem Pu(0) has a unique optimal solution, which we denote as z∗, and we also denote z∗(t) as one optimal solution of the perturbed problem Pu(t). We aim to derive the second order taylor expansion of V (u(t)), and the ﬁrst order taylor expansion of z∗(t). We ﬁrst introduce several useful notations. We deﬁne the Lagrangian of the parameterized problem as L(z, u; λ) = f (z, u) + m∑ k=1 λkgk(z, u), 60 Kallus and Mao: Stochastic Optimization Forests and the associated Lagrangian multiplier set for any (z, u) as Λ(z, u) = {λ : DzL(z, u; λ) = 0, and λk ≥ 0, λkgk(z, u) = 0, k = s + 1, . . . , m}. For any feasible point z for the unperturbed problem (i.e., t = 0), we deﬁne K(z, u0) = {k : gk(z, u0) = 0, k = s + 1, . . . , m} as the index set of inequality constraints that are active at z, and further deﬁne the index sets for active inequality constraints whose langrangian multipliers are strictly positive or 0 respectively: K+(z, u0, λ) = {k ∈ K(z, u0) : λk > 0}, K0(z, u0, λ) = {k ∈ K(z, u) : λk = 0}. (55) Consider a solution path of form z(t) = z∗ + tdz + 1 2 t2rz + o(t2) for some elements dz, du such that z(t) ∈ Z. If z(t) is feasible for the perturbed problem Pu(t), then we can apply second order taylor expansion to f (z(t), u(t)) as follows: f (z(t), u(t)) = f (z∗, u0) + tDf (z∗, u0)(dz, du) + 1 2 t2 [ Df (z∗, u0)(rz, ru) + D2f (z∗, u0)((dz, du), (dz, du)) ] + o(t 2). (56) This heuristic expansion motivates two sets of optimization problems that are useful in approximating the optimal value of the perturbed problems. The ﬁrst set optimization problem is a LP corresponding to the linear approximation term and its dual9: V (PL) =    mindz Df (z∗, u0)(dz, du) s.t Dgk(z∗, u0)(dz, du) = 0, k = 1, . . . , s Dgk(z∗, u0)(dz, du) ≤ 0, k ∈ K(z∗, u0) , (57) V (DL) = max λ∈Λ(z∗,u0) DuL(z∗, λ, u0)du. (58) Given a feasible point dz of the problem PL, we denote the corresponding set of active inequality constraints in the problem PL as KPL(z∗, u0, dz) = {k ∈ K(z∗, u0) : Dgk(z∗, u0)(dz, du) = 0} We denote the sets of optimal primal and dual solutions to Eqs. (57) and (58) as S(PL) and S(DL) respec- tively. Equation (5.110) in Bonnans and Shapiro (2000) shows that S(PL) has the following form: for any λ ∈ S(DL), S(PL) = {dz : Dgk(z∗, u0)(dz, du) = 0, k ∈ {1, . . . , s} ∪ K+(z∗, u0, λ), Dgk(z∗, u0)(dz, du) ≤ 0, k ∈ K0(z∗, u0, λ) } . The second set of optimization problems is a QP problem corresponding to the second order approximation term and its dual: V (PQ) = min dz ∈S(PL) V (PQ(dz)), V (DQ) = min dz ∈S(PL) V (DQ(dz)) (59) 9 The exact dual problem of problem PL in Eq. (57) actually uses a diﬀerent constraint for λ than that used in Eq. (58). In the proof of Proposition 4, we show that using these two constraint sets results in the same optimal value, which is stated without proof in Bonnans and Shapiro (2000). So we also call the problem in Eq. (58) as the dual of problem PL in Eq. (57). Kallus and Mao: Stochastic Optimization Forests 61 where V (PQ(dz)) =    minrz Df (z∗, u0)(rz, ru) + D2f (z∗, u0)((dz, du), (dz, du)) s.t Dgk(z∗, u0)(rz, ru) + D2gk(z∗, u0)((dz, du), (dz, du)) = 0, k = 1, . . . , s Dgk(z∗, u0)(rz, ru) + D2gk(z∗, u0)((dz, du), (dz, du)) ≤ 0, k ∈ KPL(z∗, u0, dz) , (60) V (DQ(dz)) = max λ∈S(DL) DuL(z∗, u0; λ)ru + D2L(z∗, u0; λ)((dz, du), (dz, du)). (61) The constraints on dz in problem PL (Eq. (57)) and constraints on rz in problem PQ(d(z)) (Eq. (60)) ensure that path of the form z(t) = z∗ + tdz + 1 2 t2rz + o(t2) is (approximately) feasible for the perturbed problem problem, so that the expansion in Eq. (56) is valid. In the following proposition, we characterize the optimization problems above when assuming the Lagrangian multiplier associated with the optimal solution in the unperturbed problem is unique, i.e., Λ(z∗, u0) is a singleton {λ ∗}. Proposition 4. If Λ(z∗, u0) = {λ∗} and V (PL), V (PQ) are both ﬁnite, then V (PL) = V (DL) = DuL(z∗, u0; λ∗)du, V (P Q(dz)) = V (DQ(dz)) = DuL(z∗, u0; λ∗)ru + D2L(z∗, u0; λ ∗)((dz, du), (dz, du)). The following theorem derives the second order expansion of V (u(t)) under regularity conditions. Theorem 7 (Theorem 5.53 in Bonnans and Shapiro (2000)). Suppose the following conditions hold: 1. f (z, u) and gk(z, u) for k = 1, . . . , m are twice continuously diﬀerentiable in both z ∈ Z and u ∈ U in a neighborhood around (z∗, u0); 2. The unperturbed problem corresponding to t = 0 (or equivalently problem Pu0 ) has a unique optimal solution z∗; 3. Mangasarian-Fromovitz constraint qualiﬁcation condition is satisﬁed at z∗: Dzgk(z∗, u0), k = 1, . . . , s are linearly independent, ∃dz, s.t. Dzgk(z∗, u0)dz = 0, k = 1, . . . , s, Dzgk(z∗, u0)dz < 0, k ∈ K(z∗, u0); 4. The set of Lagrangian multipliers Λ(z∗, u0) for the unperturbed problem is nonempty; 5. The following strong form of second order suﬃcient condition is satisﬁed for the unperturbed problem: sup λ∈S(DL) DzzL(z∗, λ, u0)(dz, dz) > 0, ∀dz ∈ C(z∗, u0; λ) \\ {0}, where C(z∗, u0; λ) is the critical cone deﬁned as follows: C(z∗, u0; λ) = {dz : Dzgk(z∗, u0)(dz) = 0, k ∈ {1, . . . , s} ∪ K+(z∗, u0, λ), Dzgk(z∗, u0)(dz) ≤ 0, k ∈ K0(z∗, u0, λ) } . 6. The inf-compactness condition: there exist a constant α and a compact set C ⊆ Z such that the sublevel set {z : f (z, u) ≤ α, gk(z, u) = 0, k = 1, . . . , s, gk(z, u) ≤ 0, k = s + 1, . . . , m.} is nonempty and contained in C for any u within a neighborhood of u0. 62 Kallus and Mao: Stochastic Optimization Forests Then the following conclusions hold: 1. V (PL) and V (PQ) are both ﬁnite, and the optimal value function V (u(t)) for the perturbed problem in Eq. (54) can be expanded as follows: V (u(t)) = V (u0) + tV (PL) + 1 2 t 2V (PQ) + o(t2). 2. If the problem PQ has a unique solution d ∗ z, then any optimal solution z∗(t) of the perturbed problem in Eq. (54) satisﬁes that z∗(t) = z∗ + td ∗ z + o(t). We now show that the optimal solutions of problems PQ and DQ have a simple formulation under regularity conditions about the dual optimal solution of the unperturbed problem. Proposition 5. Under conditions in Proposition 4, if further Λ(z∗, u0) = {λ∗}, i.e., z∗ is associated with a unique Lagrangian multiplier λ ∗, and the strict complementarity condition holds, i.e., the Lagrangian multipli- ers associated with all inequality constraints active at z∗ are strictly positive (or equivalently K+(z∗, u0, λ ∗) = K(z∗, u)), then V (PQ) = V (DQ) equals the optimal value of the following optimization problem: mindz DuL(z∗, u0; λ∗)ru + D2L(z∗, u0; λ ∗)((dz, du), (dz, du)) s.t. Dgk(z∗, u0)(dz, du) = 0, k ∈ {1, . . . , s} ∪ K(z∗, u0). Proposition 5 shows that under the asserted regularity conditions, the second order approximation term V (P Q) is the optimal value of a simple quadratic programming problem with equality constraints, which can be solved very eﬃciently provided that z∗, λ ∗ are known. According to Wachsmuth (2013), one condition to ensure a unique Lagrangian multiplier is the following linear independence constraint qualiﬁcation (LICQ) condition: Dzgk(z∗, u0), k ∈ {1, . . . , s} ∪ K(z∗, u) are linearly independent. (62) Actually, this LICQ condition is also stronger than the Mangasarian-Fromovitz constraint qualiﬁcation con- dition in Theorem 7. D.3. Additively perturbed problems in ﬁnite-dimensional space and its connection to approximate criteria Additive perturbations. Consider the following optimization problem: for z ∈ Rd and t > 0, v(t) =    minz∈Rd f (z) + tδf (z) s.t. gk(z) + tδgk (z) = 0, k = 1, . . . , s, gk(z) + tδgk (z) ≤ 0, k = s + 1, . . . , m hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′, (63) where f , gk, hk are all twice continuously diﬀerentiable in z ∈ Z with gradients and Hessian matrices denoted by ∇ and ∇2 respectively. Moreover, δf and δgk are all diﬀerentiable with gradients denoted by ∇. We deﬁne the Lagrangian for the problem above as follows: L(z, t; λ, ν) = f (z) + tδf (z) + m∑ k=1 λk(gk(z) + tδgk (z)) + m′ ∑ k′=1 νk′hk′(z) Kallus and Mao: Stochastic Optimization Forests 63 where λ and ν are the Lagrangian multipliers associated with the constraints involving {gk} m k=1 and {hk′} m k′=1 respectively. For any feasible point z of the unperturbed problem (i.e., t = 0), we denote the index sets of active (unperturbed) inequality constraints z as Kg(z) and Kh(z) respectively: Kg(z) = {k : gk(z) = 0, k = s + 1, . . . , m}, Kh(z) = {k′ : hk′(z) = 0, k′ = s ′ + 1, . . . , m′}. Note that the problem above in Eq. (63) is a special case of the problem in Eq. (54) with perturbation path u(t) = t, i.e., u0 = 0, du = 1, ru = 0. Thus we can apply Theorem 7 and Proposition 5 to prove the following theorem. Theorem 8. Suppose the following conditions hold: 1. f (z), gk(z), hk′(z) for k = 1, . . . , m and k′ = 1, . . . , m′ are twice continuously diﬀerentiable, and δf , δgk for k = 1, . . . , m are continuously diﬀerentiable; 2. The unperturbed problem corresponding to t = 0 has a unique optimal primarxy solution z∗ that is asso- ciated with a unique Lagrangian multiplier (λ∗, ν ∗), and (λ ∗, ν ∗) satisﬁes the strict complemetarity condition, i.e., λ ∗ k > 0 for k ∈ Kg(z∗) and ν ∗ k > 0 for k ∈ Kh(z∗); 3. Mangasarian-Fromovitz constraint qualiﬁcation condition is satisﬁed at z∗: ∇gk(z∗), k = 1, . . . , s are linearly independent and ∇hk′(z∗), k′ = 1, . . . , s′ are linearly independent, ∃dz, s.t. ∇gk(z∗)dz = 0, k = 1, . . . , s, ∇gk(z∗)dz < 0, k ∈ Kg(z∗), ∇hk′(z∗)dz = 0, k′ = 1, . . . , s′, ∇hk′(z∗)dz < 0, k′ ∈ Kh(z∗); 4. Second order suﬃcient condition: d ⊤ z ∇2L(z∗, 0; λ ∗, ν ∗)dz > 0, ∀dz ∈ C(z∗) \\ {0}, where C(z∗) is the critical cone deﬁned as follows: C(z∗) = {dz : d⊤ z ∇gk(z∗) = 0, k ∈ {1, . . . , s} ∪ Kg(x∗), d ⊤ z ∇hk′(z∗) = 0, k′ ∈ {1, . . . , s′} ∪ Kh(x∗) } . 5. The inf-compactness condition: there exist a constant α, a positive constant t0, and a compact set C ⊆ Z such that the sublevel set    z : f (z) + tδf (z) ≤ α, gk(z) + tδgk (z) = 0, k = 1, . . . , s, gk(z) + tδgk (z) ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′.    is nonempty and contained in C for any t ∈ [0, t0). Then v(t) = v(0) + v′(0)t + 1 2 t2v′′(0) + o(t2) where v′(0) = δf (z∗) + m∑ k=1 λ∗ kδgk (z∗) 64 Kallus and Mao: Stochastic Optimization Forests and v′′(0) = min dz d ⊤ z ∇2L(z∗, 0; λ∗, ν ∗)dz + 2d ⊤ z ( ∇δf (z∗) + s∑ k=1 λ∗ k∇δgk (z∗) ) (64) s.t. d ⊤ z ∇gk(z∗) + δgk (z∗) = 0, k ∈ {1, . . . , s} ∪ Kg(x ∗) d ⊤ z ∇hk(z∗) = 0, k′ ∈ {1, . . . , s′} ∪ Kh(x∗). Moreover, if the optimization problem in Eq. (64) has a unique optimal solution d ∗ z, then any optimal solution z∗(t) of the perturbed problem in Eq. (63) satisﬁes that z∗(t) = z∗ + td ∗ z + o(t). (65) According to Eq. (62), a suﬃcient condition for the uniqueness of the Lagrangian multiplier is the following: ∇gk(z∗), k ∈ {1, . . . , s} ∪ Kg(z∗) are linearly independent, ∇hk′(z∗), k′ = {1, . . . , s′} ∪ Kh(z∗) are linearly independent. (66) D.3.1. Proving Theorem 6 In Appendix A, we aim to approximate vj(t) = minz∈Zj (t) (1 − t)f0(z) + tfj(z), zj(t) ∈ arg minz∈Zj (t) (1 − t)f0(z) + tfj(z) for t ∈ [0, 1], j = 1, 2, where Zj(t) =   z : (1 − t)g0,k(z) + tgj,k(z) = 0, k = 1, . . . , s, (1 − t)g0,k(z) + tgj,k(z) ≤ 0, k = s + 1, . . . , m, hk′(z) = 0, k′ = 1, . . . , s′, hk′(z) ≤ 0, k′ = s ′ + 1, . . . , m′    . Note this is a special example of Eq. (63) with δf = fj − f0 and δgk = gj,k − g0,k for k = 1, . . . , m. Then applying Theorem 8 directly gives Theorem 6. D.3.2. Proving Theorem 1 If there are no constraints, i.e., we consider the problem v(t) = min z∈Rd f (z) + tδf (z). (67) Then Theorem 8 reduces to the following corollary. Corollary 1. Suppose the following conditions hold for f : 1. f (z) is twice continuously diﬀerentiable, and δf (z) is continuously diﬀerentiable; 2. there exists a constant α, a positive constant t0 ∈ (0, 1] and compact set C ⊆ Rd such that the sublevel set {z ∈ Rd : f (z) + tδf (z) ≤ α} is nonempty and contained in C for any t ∈ [0, t0); 3. f (z) has a unique minimizer over Rd (denoted as z∗), and ∇2f (z∗) is positive deﬁnite. Then v(t) in Eq. (67) satisﬁes that v(t) = v(0) + tδf (z∗) − 1 2 t2∇δf (z∗) ⊤ (∇2f (z∗) )−1 ∇δf (z∗) + o(t 2) (68) and any optimal solution z∗(t) of the perturbed problem in Eq. (67) satisﬁes that z∗(t) = z∗ − t (∇2f (z∗) )−1 ∇δf (z∗) + o(t) Note applying this corollary with f = f0, δf = fj − f0, and ∇δf (z∗) = ∇fj(z∗) − ∇f0(z∗) = ∇fj(z∗) gives Theorem 1. Kallus and Mao: Stochastic Optimization Forests 65 D.4. Stronger Diﬀerentiability Results. All results above are based on directional diﬀerentiability, which cannot quantify the magnitude of approx- imation errors of the second order perturbation analysis. Here we show in the context of unconstrained problems that under stronger regularity conditions, we can also bound the approximation errors by the magnitude of perturbation δf . We will then use this to prove Theorems 2 and 3 in Appendix H.4. Consider the following optimization problem denoted as P (f ) for f : Rd ↦→ R, v(f ) = min z∈Rd f (z), z∗(f ) ∈ arg min z∈Rd f (z) (69) We restrict f to the twice continuously diﬀerentiable function class F with norm deﬁned as ∥f ∥F = max{sup z |f (z)|, sup z ∥∇f (z)∥2, sup z ∥∇ 2f (z)∥F}, (70) where ∥∇f (z)∥2 is the Euclidean norm of gradient ∇f (z) and ∥∇2f (z)∥F is the Frobenius norm of the Hessian matrix ∇2f (z). We consider P (f0) as the unperturbed problem and P (f0 + δf ) as the target perturbed problem that we hope to approximate. Eq. (68) gives the ﬁrst and second order functional directional derivatives of v at f0: v′(f0; δf ) := lim t↓0 v(f0 + tδf ) − v(f0) t = δf (z∗(f0)) v′′(f0; δf ) := lim t↓0 v(f0 + tδf ) − v(f0) − tv′(f0; δf ) 1 2 t2 = −∇δf (z∗(f0))⊤ (∇2f0(z∗(f0)))−1 ∇δf (z∗(f0)) We aim to expand v(f0 + δf ) and z∗(f0 + δf ) in the functional space with approximation errors bounded by the magnitude of δf . Theorem 9. If f0(z), δf (z) are both twice continuously diﬀerentiable, condition 2 in Corollary 1 is satisﬁed for t ∈ [0, 1], and for any t ∈ [0, 1], f0(z)+tδf (z) has a unique minimizer z∗(f0 +tδf ) and ∇2 (f0 + tδf ) (z∗(f0 + tδf )) is positive deﬁnite, then v(f0 + δf ) = v(f0) + δf (z∗(f0)) − 1 2 ∇δf (z∗(f0))⊤ (∇2f (z∗(f0)))−1 ∇δf (z∗(f0)) + o(∥δf ∥ 2 F ), z∗(f0 + δf ) = z∗(f0) − (∇2f (z∗(f0)))−1 ∇δf (z∗(f0)) + o(∥δf ∥F ). Based on Theorem 9, we can bound the approximation errors of the proposed criteria in Section 2.3 by o(∥fj − f0∥ 2 F ) for j = 1, 2. Note that the Lipschitzness condition (condition 4 in Theorem 2) implies that ∥fj − f0∥F = O(D2 0). Therefore, the approximation errors of the proposed criteria are o(D2 0). See Theorems 2 and 3 and their proofs in Appendix H.4. Appendix E: Optimization with Auxiliary Variables In Examples 2 and 3, the cost function involves unconstrained auxiliary variables zaux in addition to the decision variables zdec. In this setting, we can do perturbation analysis in two diﬀerent ways. These two diﬀerent approaches lead to diﬀerent approximate criteria that are equivalent under inﬁnitesimal perturba- tions but give diﬀerent extrapolations and diﬀer in terms of computational costs. For convenience, we focus on unconstrained problems with a generic cost function c(zdec, zaux; y). We denote the region to be split as R0 ⊆ Rd, and its candidate subpartition as R0 = R1 ∪ R2, R1 ∩ R2 = ∅. 66 Kallus and Mao: Stochastic Optimization Forests Re-optimizing auxiliary variables. In the ﬁrst approach, we acknowledge the auxiliary role of zaux and deﬁne fj by proﬁling out zaux ﬁrst: fj(zdec) = minzaux E [ c(zdec, zaux; Y ) | X ∈ Rj] for j = 0, 1, 2. We assume that for each ﬁxed zdec value, E [ c(zdec, zaux; Y ) | X ∈ Rj] has a unique minimizer, which we denote as zaux j (zdec). We also denote zdec j as the minimizer of fj(zdec) and zaux j as zaux j (zdec j ). The following proposition derives the gradient and Hessian matrix. Proposition 6. Consider fj(zdec) = minzaux E [c(zdec, zaux; Y ) | X ∈ Rj] for j = 0, 1, 2. Suppose that for each zdec value and j = 1, 2, E [c(zdec, zaux; Y ) | X ∈ Rj] has a unique minimizer zaux j (zdec). Moreover, we assume that f0(zdec) has a unique minimizer zdec 0 , and f0, f1, f2 are twice continuously diﬀerentiable. Then ∇fj (zdec 0 ) = ∂ ∂zdec E [ c(zdec 0 , zaux j (zdec 0 ) ; Y ) | X ∈ Rj] , ∇2f0 (zdec 0 ) = ∂2 (∂zdec) ⊤ ∂zdec E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] − ∂2 ∂zdec (∂zaux) ⊤ E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] { ∂2 ∂zaux (∂zaux)⊤ E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] }−1 ∂2 ∂zaux (∂zdec) ⊤ E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] . It is straightforward to show that Examples 2 and 3 satisfy conditions in Proposition 6 under regularity conditions. So we can apply the gradients and Hessian matrix in Proposition 6 to derive the apx-risk criterion and the apx-soln criterion. However, in order to estimate the gradients, we need to compute zaux j (zdec 0 ) for every candidate split by repeatedly minimizing E [ c(zdec 0 , zaux; Y ) | X ∈ Rj] with respect to zaux. This can be too computationally expensive in practice. Merging auxiliary variables with decision variables. In the second way, we merge the auxiliary variables with the decision variables, and deﬁne ˜fj(zdec, zaux) = E [ c(zdec, zaux; Y ) | X ∈ Rj] for j = 0, 1, 2. The following proposition derives the gradients and Hessian matrix with respect to both decision variables and auxiliary variables. Proposition 7. Consider ˜fj(zdec, zaux) = E [c(zdec, zaux; Y ) | X ∈ Rj] for j = 0, 1, 2. Suppose that ˜f0(zdec, zaux) has a unique minimizer (zdec 0 , zaux 0 ), and ˜f0, ˜f1, ˜f2 are twice continuously diﬀerentiable. Then ∇ ˜fj (zdec 0 , zaux 0 ) = [ ∂ ∂zdec E [c(zdec 0 , zaux 0 ; Y ) | X ∈ Rj] ∂ ∂zaux E [c(zdec 0 , zaux 0 ; Y ) | X ∈ Rj] ] , ∇2 ˜f0 (zdec 0 , zaux 0 ) =   ∂2 (∂zdec) ⊤∂zdec E [c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] ∂2 (∂zaux)⊤∂zdec E [c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] ∂2 (∂zdec) ⊤∂zaux E [c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] ∂2 ∂zaux(∂zaux)⊤ E [c(zdec 0 , zaux 0 ; Y ) | X ∈ R0]   . Note that approximate criteria based on the formulations in Proposition 6 and Proposition 7 are both legitimate, and they are equivalent under inﬁnitesimal perturbations according to Theorem 1. Using the apx-risk criterion as an example, the following proposition further investigates the relationship between the approximate criterion based on Proposition 6 and that based on Proposition 7. Proposition 8. Let Capx-risk(R1, R2) and ˜Capx-risk(R1, R2) be the apx-risk criterion based on {f0, f1, f2} given in Proposition 6 and { ˜f0, ˜f1, ˜f2} given in Proposition 7 respectively. Then Capx-risk(R1, R2) = − ∑ j=1,2 pj ( ∂ ∂zdec ˜fj (zdec 0 , zaux j (zdec 0 )))⊤ {(∇2 ˜f0 (zdec 0 , zaux 0 ))−1 [ zdec 0 , zdec 0 ] } ∂ ∂zdec ˜fj (zdec 0 , zaux j (zdec 0 )) , ˜Capx-risk(R1, R2) = − ∑ j=1,2 pj ( ∂ ∂zdec ˜fj (zdec 0 , zaux 0 ))⊤ {(∇2 ˜f0 (zdec 0 , zaux 0 ))−1 [ zdec 0 , zdec 0 ] } ∂ ∂zdec ˜fj (zdec 0 , zaux 0 ) + R, Kallus and Mao: Stochastic Optimization Forests 67 where (∇2 ˜f0 (zdec 0 , zaux 0 ) )−1 [zdec 0 , zdec 0 ] is the block of the inverse matrix (∇2 ˜f0 (zdec 0 , zaux 0 ))−1 whose rows and columns both correspond to zdec 0 , and R is an adjustment term that only depends on ∇ ˜fj (zdec 0 , zaux 0 ) and ∇2 ˜fj (zdec 0 , zaux 0 ). Proposition 8 shows that evaluating Capx-risk(R1, R2) requires computing zaux j (zdec 0 ) repeatedly for all candidate splits, while evaluating ˜Capx-risk(R1, R2) only requires computing (zdec 0 , zaux 0 ) once. To compen- sate for the fact that ˜Capx-risk(R1, R2) does not re-optimize the decision variable for each candidate split, ˜Capx-risk(R1, R2) also has an additional adjustment term R. In Examples 2 and 3, we use ˜Capx-risk(R1, R2) to reduce computation cost, as this is the main point of using approximate criteria. Appendix F: Other Related Literature Applications of tree models in other decision making problems. In the CSO problem, given a realization y, the eﬀect of decisions z on costs is assumed known (i.e., c(z; y)). This may not apply if decisions aﬀect uncertain costs in an a priori unknown way, such as the unknown eﬀect of prices on demand or of pharmacological treatments on health indicators. In these applications, data consists only of observations of the realized costs for a single decision and not counterfactual costs for other decisions, known as partial or bandit feedback, which requires additional identiﬁcation assumptions such as no unobserved confounders (Bertsimas and Kallus 2016). Kallus (2017), Zhou et al. (2018) apply tree methods to prescribe from a ﬁnite set of interventions based on such data using decision quality rather than prediction error as the splitting criterion. Since they consider a small ﬁnite number of treatments, the criterion for each candidate split is rapidly computed by enumeration. When decisions are continuous, various works use tree ensembles to regress cost on a decision variable (e.g., Sec. 3 of Bertsimas and Kallus 2014, Ferreira et al. 2016, among others) and then search for the input to optimize the output. This is generally a hard optimization problem, to which Miˇsi´c (2020) study mixed-integer optimization approaches. Elmachtoub et al. (2017), F´eraud et al. (2016) similarly use decision trees and random forests for online decision-making in contextual bandit problems. Chen and Miˇsi´c (2020) propose to use trees and forests to nonparametrically model irrational customer choices. Under the forest choice model, Chen and Miˇsi´c (2021) further develops mixed-integer optimization algorithms to ﬁnd the assortment that maximizes expected revenue. Ciocan and Miˇsi´c (2020) study optimal stopping problems with applications in option pricing, and propose algorithms to construct approximately optimal tree policies that are easy to interpret. Applications of perturbation analysis in machine learning. Perturbation analysis studies the impact of slight perturbations to the objective and constraint functions of an optimization problem on the optimal value and optimal solutions, which is the foundation of our approximate splitting criteria. We refer readers to Bonnans and Shapiro (2000) for a general treatment of perturbation analysis for smooth optimization, and to Shapiro et al. (2014) for its application in statistical inference for stochastic optimization. In machine learning, perturbation analysis has been successfully applied to approximate cross-validation for model eval- uation and tuning parameter selection. Exact cross-validation randomly splits the data into many folds, and then repeatedly solves empirical risk minimization (ERM) using all but one fold data, which can be computationally prohibitive if the number of folds is large. In the context of parametric models, recent works 68 Kallus and Mao: Stochastic Optimization Forests propose to solve the ERM problem only once with full data, and then apply a one-step Newton update to the full-data estimate to approximate the estimate when each fold of data is excluded (e.g., Giordano et al. 2019, Stephenson and Broderick 2020, Wilson et al. 2020). Koh and Liang (2017) employ similar ideas to quantify the importance of a data point in model training by approximating the parameter estimate change if the data distribution is inﬁnitesimally perturbed towards the data point of interest. All of these works only focus on unconstrained optimization problems. Appendix G: Supplementary Lemmas and Propositions Lemma 1 (Convergence of ˆz0). Suppose the following conditions hold: 1. supz | ̂p0f0(z) − p0f0(z)| a.s. −→ 0 as n → ∞. 2. f0 is a continuous function and f0(z) has a unique minimizer z0 over Rd. 3. For large enough n, arg minz ̂p0f0(z) is almost surely a nonempty and uniformly bounded set. Then ˆz0 a.s. −→ z0 as n → ∞. Proposition 9 (Estimation for Squared Cost Function.). When c(z; y) = 1 2 ∥z − y∥ 2, 1 n n∑ i=1 I [Xi ∈ R0] c(ˆz0; Yi) + 1 2 ˆCapx-risk(R1, R2) = ˆCapx-soln(R1, R2) = ∑ j=1,2 nj 2n d∑ l=1 Var({Yi,l : Xi ∈ Rj, i ≤ n}), Proposition 10 (Gradient and Hessian for Example 2). For the cost function c(z; y) in Eq. (5) and fj(z) = E [c(z; Y ) | X ∈ Rj], we have ∇fj(z0) = 2 [ E [Y Y ⊤ | X ∈ Rj] z0,1:d − E [Y | X ∈ Rj] z0,d+1 z⊤ 0,1:d (E [Y | X ∈ R0] − E [Y | X ∈ Rj]) ] , ∇2f0(z0) = 2 [E [Y Y ⊤ | X ∈ R0] −E [Y | X ∈ R0] −E [Y ⊤ | X ∈ R0] 1 ] . Proposition 11 (Gradient and Hessian for Example 3). Consider the cost function c(z; y) in Eq. (7) and fj(z) = E [c(z; Y ) | X ∈ Rj]. If Y has a continuous density function and z0 ̸= 0, then ∇fj(z0) = 1 α [ −E [Y I [Y ⊤z0,1:d ≤ qα 0 (Y ⊤z0,1:d)] | X ∈ Rj] P (qα 0 (Y ⊤z0,1:d) − Y ⊤z0,1:d ≥ 0 | X ∈ Rj) − α ] , ∇2f0(z0) = µ0 (qα 0 (Y ⊤z0,1:d)) α [E [Y Y ⊤ | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] −E [Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] −E [Y ⊤ | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] 1 ] , where qα 0 (Y ⊤z0,1:d) is the α-quantile of Y ⊤z0,1:d given X ∈ R0 and µ0 is the density function of Y ⊤z0,1:d given X ∈ R0. If further Y | X ∈ R0 has Gaussian distribution with mean µ0 and covariance matrix Σ0, then E [ Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] = m0 + Σ0z0,1:d (z⊤ 0,1:dΣ0z0,1:d)−1 (qα 0 (Y ⊤z0) − m ⊤ 0 z0,1:d), Var (Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0) = Σ0 − Σ0z0,1:d (z⊤ 0,1:dΣ0z0,1:d)−1 z⊤ 0,1:dΣ0. Lemma 2 (Suﬃcient conditions for Proposition 1). Under conditions in Lemma 1, if we further assume the following conditions: 1. P (X ∈ Rj) > 0 for j = 0, 1, 2; 2. ∇2f0(z) is continuous, and ∇2f0(z0) is invertible; 3. there exist a compact neighborhood N around z0 such that supz∈N ∥ ˆH0(z) − ∇ 2f0(z)∥ = op(1), supz∈N ∥ˆhj(z) − ∇fj(z)∥ = Op(n−1/2), and {I [Xi ∈ R0] c(z; Yi) : z ∈ N } is a Donsker class; Kallus and Mao: Stochastic Optimization Forests 69 4. conditions in Lemma 1 hold; Then ∥ ˆH −1 0 (ˆz0) − (∇2f0(z0))−1 ∥ = op(1), ∥ˆhj(ˆz0) − ∇fj(z0)∥ = Op(n−1/2) for j = 1, 2. Proposition 12 (Regularity conditions for Estimators in Example 1). Consider the esstimates (ˆhj(ˆz0) ) l = 1 nj ∑n i=1 I [Xi ∈ Rj, Yl ≤ ˆz0,l] and ( ˆH0(ˆz0) ) l = (αl + βl) 1 nj b ∑n i=1 I [Xi ∈ Rj] K((Yi,l − ˆz0,l)/b) given in Example 1. Suppose the following conditions hold: 1. P (X ∈ Rj) > 0 for j = 0, 1, 2; 2. The density function µl(z) is H¨older continuous, i.e., there exist a constant 0 < a ≤ 1 such that |µl(z) − µl(z′)| ≤ ∥z − z′∥ a for z, z′ ∈ Rd, and µl(z) > 0 for z in a neighborhood around z0; 3. the bandwidth b satisﬁes that b ≥ log n/n and b → 0 as n → ∞; 4. conditions in Lemma 1 hold. Then (ˆhj(ˆz0) ) l and ( ˆH0(ˆz0) ) l given in Example 1 satisfy the conditions in Proposition 1. Proposition 13 (Regularity conditions for Estimators in Example 2). Consider the estimators ˆhj(ˆz0) and ˆH0(ˆz0) given in Example 2: ˆhj(ˆz0) = 2 [ 1 nj ∑n i=1 I [Xi ∈ Rj] YiY ⊤ i ˆz0,1:d − 1 nj ∑n i=1 I [Xi ∈ Rj] Yi ˆz0,d+1 ˆz⊤ 0,1:d ( 1 n0 ∑n i=1 I [Xi ∈ R0] Yi − 1 nj ∑n i=1 I [Xi ∈ Rj] Yi) ] , ˆH0(ˆz0) = 2 [ 1 n0 ∑n i=1 I [Xi ∈ R0] YiY ⊤ i − 1 n0 ∑n i=1 I [Xi ∈ R0] Yi − 1 n0 ∑n i=1 I [Xi ∈ R0] Y ⊤ i 1 ] . If conditions in Lemma 1 and condition 1 in Lemma 2 hold and Var (Y | X ∈ R0) is vertible, then ˆhj(ˆz0) and ˆH0(ˆz0) satisfy conditions in Proposition 1. Proposition 14 (Regularity conditions for Estimators in Example 3). Consider the estimator ˆhj(ˆz0) and ˆH0(ˆz0) given in Example 3: ˆhj(ˆz0) = [− 1 nj ∑n i=1 I [Y ⊤ i ˆz0,1:d ≤ ˆqα 0 (Y ⊤ ˆz0,1:d), Xi ∈ Rj] Yi 1 nj ∑n i=1 I [Y ⊤ i ˆz0,1:d ≤ qα 0 (Y ⊤ ˆz0,1:d), Xi ∈ Rj] − α ] ˆH0(ˆz0) = ˆµ0(ˆqα 0 (Y ⊤ ˆz0)) α [ ˆM2 − ˆM1 − ˆM1 1 ] where ˆM1 = ˆm0 + ˆΣ0 ˆz0 (ˆz⊤ 0,1:d ˆΣ0 ˆz0,1:d)−1 (ˆqα 0 (Y ⊤ ˆz0,1:d) − ˆm ⊤ 0 ˆz0,1:d) ˆM2 = ˆM1 ˆM ⊤ 1 + ˆΣ0 − ˆΣ0 ˆz0,1:d (ˆz⊤ 0,1:d ˆΣ0 ˆz0,1:d)−1 ˆz⊤ 0,1:d ˆΣ0 If the conditions in Lemma 1 and condition 1 in Lemma 2 holds, the density function of Y ⊤z0 is positive at qα 0 (Y ⊤z0) and it also satisﬁes the H¨older continuity condition, i.e., condition 2 in Proposition 12, and also the bandwidth satisﬁes the condition 3 in Proposition 12, then ˆhj(ˆz0) = ∇fj(z0) + Op(n−1/2) and ∥ ˆH0(ˆz0) − ∇2f0(z0)∥ → 0. Proposition 15 (Regularity Conditions for Estimators in Example 5). Suppose that c(z; y) is twice continuously diﬀerentiable in z for every y and conditions in Lemma 1 and condition 1 in Lemma 2 hold, then the the conditions in Proposition 1 are satisﬁed for estimates ˆH0(ˆz0) = 1 n0 ∑n i=1 I [Xi ∈ R0] ∇2c (ˆz0; Yi) and ˆhj(ˆz0) = 1 nj ∑n i=1 I [Xi ∈ Rj] ∇c (ˆz0; Yi) given in Example 5. 70 Kallus and Mao: Stochastic Optimization Forests Below we introduce the linear independence constraint qualiﬁcation condition for deterministic constraints only. See Eq. (66) for a more complete condition with both deterministic constraints and stochastic con- straints. Definition 1 (Linear Independence Constraint Qualification). Consider constraints Z = {z ∈ Rd : hk(z) = 0, k = 1, . . . , s, hk(z) ≤ 0, k = s + 1, . . . , m } and the index set of inequality constraints active at a point z0 ∈ Z denoted as Kh(z0) = {k : hk(z0) = 0, k = s + 1, · · · , m}. The linear independence constraint qualiﬁcation condition is satisﬁed at z0 ∈ Z if {∇hk (z0) : k ∈ {1, . . . , s} ∪ Kh(z0)} are linearly independent. According to Wachsmuth (2013), the linear independence constraint qualiﬁcation (LICQ) condition is a suﬃcient condition for the Mangasarian-Fromovitz constraint qualiﬁcation condition (condition 5 in The- orem 4). Moreover, when the LICQ condition is satisﬁed at a optimal solution z0, then it has a unique Lagrangian multiplier v0 such that (z0, v0) satisfy the Karush–Kuhn–Tucker conditions (condition 4 in The- orem 4). In the proposition below, we show that the LICQ condition is satisﬁed for any z ∈ Z for the constraints Z given in Examples 1 to 3, so conditions 4 and 5 in Theorem 4 are satisﬁed for these examples. Proposition 16. Example 1 with the constraints Z = { z ∈ Rd : ∑d l=1 zl ≤ C, zl ≥ 0, l = 1, . . . , d} and Examples 2 and 3 with the simplex constraint Z = {z ∈ Rd+1 : ∑d l=1 zl = 1, zl ≥ 0, l = 1, . . . , d} all satisfy the linear independence constraint qualiﬁcation condition in Deﬁnition 1 at any z ∈ Z. Finally, we point out the splitting criterion considered in Elmachtoub et al. (2020) is what we termed the oracle criterion in Eq. (8). Proposition 17. When c (z; y) = y⊤z, the Smart Predict-then-Optimize (SPO) criterion in Elmachtoub et al. (2020) is equivalent the oracle splitting criterion in Eq. (8). Appendix H: Omitted Proofs H.1. Proofs for Appendix D Proof for Proposition 3 Recall that vj (t) = f0(zj (t)) + t (fj(zj (t)) − f0(zj (t))) + ∑ k∈ ˜Kh(z0) νj,k (t) hk (zj (t)) , where ˜Kh (z0) = {1, . . . , s} ∪ Kh (z0). Taking the derivatives w.r.t t based on the chain rule, we have ∂ ∂t vj (t) = fj (zj (t)) − f0 (zj (t)) + ∑ k∈ ˜Kh(z0) ( ∂ ∂t νj,k (t) ) hk (zj (t)) +  ∇f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇hk (zj (t)) + t (∇fj (zj (t)) − ∇f0 (zj (t)))   ∂ ∂t zj (t) . Kallus and Mao: Stochastic Optimization Forests 71 Therefore, ∂ ∂t vj (t) |t=0 = fj (z0) − f0 (z0) +  ∇f0 (z0) + ∑ k∈ ˜Kh(z0) ν0,k∇hk (z0)   ∂ ∂t zj (t) |t=0 + ∑ k∈ ˜Kh(z0) ( ∂ ∂t νj,k (t) |t=0 ) hk (z0) = fj (z0) − f0 (z0) , where the second equation holds because ∇f0 (z0) + ∑ k∈ ˜Kh(z0) ν0,k∇hk (z0) = 0 according to Eq. (51), and hk (z0) = 0 for any k ∈ ˜Kh (z0) by the deﬁnition of Kh (z0). Further taking the second order derivatives, we have ∂2 ∂t2 vj (t) = 2 (∇fj (zj (t)) − ∇f0 (zj (t))) ∂ ∂t zj (t) + ∑ k∈ ˜Kh(z0) ( ∂2 ∂t2 νj,k (t) ) hk (zj (t)) + 2 ∑ k∈ ˜Kh(z0) ( ∂ ∂t νj,k (t)) ∇⊤hk (zj (t)) ( ∂ ∂t zj (t)) +   ∇f0 (zj (t)) + ∑ k∈ ˜Kh(zj (t)) νj,k (t) ∇hk (zj (t)) + t (∇fj (zj (t)) − ∇f0 (zj (t)))    ∂2 ∂t2 zj (t) + ( ∂ ∂t zj (t) )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   ( ∂ ∂t zj (t) ) . Evaluating the above at t = 0 gives ∂2 ∂t2 vj (t) |t=0 = 2 (∇fj (z0) − ∇f0 (z0)) d j∗ z + 2 ∑ k∈ ˜Kh(z0) ( ∂ ∂t νj,k (t) |t=0 ) ∇⊤hk (z0) dj∗ z + (d j∗ z )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   dj∗ z . According to Eq. (53), we know that 2 ∑ k∈ ˜Kh(z0) ( ∂ ∂t νj,k (t) |t=0 ) ∇⊤hk (z0) d j∗ z = 2 ∑ k∈ ˜Kh(z0) ξj∇⊤hk (z0) d j∗ z = − 2 (∇fj(z0) − ∇f0(z0)) ⊤ dj∗ z − 2 (dj∗ z )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   d j∗ z . Moreover, note that by the ﬁrst equation in Eq. (53), we have (dj∗ z )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   d j∗ z + (d j∗ z )⊤ ∇HKh ⊤(z0)ξj = − (∇fj(z0) − ∇f0(z0))⊤ d j∗ z , and by the second equation in Eq. (53), we have (dj∗ z )⊤ ∇HKh ⊤(z0)ξj = 0. Thus (d j∗ z )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   dj∗ z = − (∇fj(z0) − ∇f0(z0))⊤ d j∗ z . 72 Kallus and Mao: Stochastic Optimization Forests It follows that ∂2 ∂t2 vj (t) |t=0 = − (d j∗ z )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   d j∗ z = (d j∗ z )⊤  ∇2f0 (zj (t)) + ∑ k∈ ˜Kh(z0) νj,k (t) ∇2hk (zj (t))   dj∗ z + 2 (∇fj(z0) − ∇f0(z0))⊤ d j∗ z . Proof for Proposition 4 Note that the Lagrangian multiplier set Λ(z∗, u0) for the unperturbed problem can be written as follows: Λ(z∗, u0) =    λ : λk ≥ 0 if k ∈ K(z∗, u0), λk = 0, if k ∈ {s + 1, . . . , m} \\ K(z∗, u0), λk ∈ R if k ∈ {1, . . . , s}, Dzf (z∗, u0) + ∑m k=1 λkDzgk(z∗, u0) = 0    . Similarly, we can deﬁne the Lagrangian for the problem PL: LPL(dz; λ) = Df (z∗, u0)(dz, du) + s∑ k=1 λkDgk(z∗, u0)(dz, du) + ∑ k∈K(z∗,u0) λkDgk(z∗, u0)(dz, du). The multiplier set for any dz feasible for the problem PL as follows: ΛPL(dz) =   λ : λk ≥ 0 if k ∈ KPL(z∗, u0, dz), λk = 0, if k ∈ K(z∗, u0) \\ KPL(z∗, u0, dz) λk ∈ R if k ∈ {1, . . . , s} ∪ ({s + 1, . . . , m} \\ K(z∗, u0)) , Dzf (z∗, u0) + ∑m k=1 λkDzgk(z∗, u0) = 0    . Then by duality of linear program, for any d∗ z ∈ S(P L), V (PL) = max λ LPL(d∗ z; λ). We know that any λ ∗ PL ∈ ΛP L(d∗ z) attains the maximum above. For any λ ∈ ΛPL(d∗ z) or λ ∈ Λ(z∗, u0), by the fact that Dzf (z∗, u0) + ∑m k=1 λkDzgk(z∗, u0) = 0 , we also have LPL(d∗ z; λ) = DuL(z∗, λ, u0)du. Moreover, ΛPL(d∗ z) diﬀers with Λ(z∗, u0) only in two aspects: (1) for λ ∈ Λ(z∗, u0), λk = 0 for k ∈ {s + 1, . . . , m} \\ K(z∗, u0), but for λ ∈ ΛPL(d ∗ z), λk ∈ R for k ∈ {s + 1, . . . , m} \\ K(z∗, u0), which does not matter because LPL(dz; λ) does not depend on λk for k ∈ {s + 1, . . . , m} \\ K(z∗, u0); (2) for λ ∈ Λ(z∗, u0), λk ≥ 0 for k ∈ K(z∗, u0) \\ KPL(z∗, u0, d ∗ z), but for λ ∈ ΛPL(d∗ z), λk = 0 for k ∈ K(z∗, u0) \\ KPL(z∗, u0, d ∗ z), which does not matter as well because Dgk(z∗, u0)(d ∗ z, du) = 0 for k ∈ K(z∗, u0) \\ KPL(z∗, u0, d ∗ z) so that λk for k ∈ K(z∗, u0) \\ KPL(z∗, u0, d ∗ z) do not inﬂuence LPL(d∗ z; λ) as well. This means that for any λ∗ PL ∈ ΛP L(d ∗ z), there always exists λ′ ∈ Λ(z∗, u0) such that LPL(d∗ z; λ∗ PL) = maxλ LPL(d ∗ z; λ) = LPL(d ∗ z; λ ′). Therefore, for any d∗ z ∈ S(PL), V (PL) = max λ LPL(d∗ z; λ) = max λ∈Λ(z∗,u0) DuL(z∗, λ, u0)du = V (DL), which justiﬁes the dual formulation in Eq. (58). By the deﬁnition of Lagrangian multiplier set, S(DL) = ΛPL(d∗ z) for any d ∗ z ∈ S(PL). Now we consider the Lagrangian of the problem PQ(dz): LPQ(rz; λ) = Df (z∗, u0)(rz, ru) + D2f (z∗, u0)((dz, du), (dz, du)) + ∑ k∈KPL(z∗,u0,dz )∩{1,...,s} λk (Dgk(z∗, u0)(rz, ru) + D2gk(z∗, u0)((dz, du), (dz, du))) . Kallus and Mao: Stochastic Optimization Forests 73 Note that PQ(dz) is a linear program, and by the strong duality, we have that for any r∗ z ∈ S(PQ(dz)) V (PQ(dz)) = V (DQ(dz)) = max λ LPQ(r∗ z ; λ). The set of Lagrangian multipliers that attain the maximum above is ΛPQ(dz )(r∗ z ) =   λ : λk ≥ 0 if k ∈ KPQ(z∗, u0, rz), λk = 0, if k ∈ KPL(z∗, u0, dz) \\ KPQ(z∗, u0, rz) λk ∈ R if k ∈ {1, . . . , s} ∪ ({s + 1, . . . , m} \\ KPL(z∗, u0, dz)) , Dzf (z∗, u0) + ∑ k∈KPL(z∗,u0,dz )∩{1,...,s} λkDzgk(z∗, u0) = 0    , where KPQ(z∗, u0, rz) is the index set of active inequality constraints in the problem P Q(dz), i.e., KPQ(z∗, u0, rz) = {k ∈ KPL(z∗, u0, dz) : Dgk(z∗, u0)(rz, ru) + D2gk(z∗, u0)((dz, du), (dz, du)) = 0}. Thus for any λ ∈ ΛPQ(dz ), V (PQ(dz)) = V (DQ(dz)) = LPQ(r∗ z ; λ) = DuL(z∗, u0; λ)ru + D2L(z∗, u0; λ)((dz, du), (dz, du)). Again, ΛPQ(dz )(r∗ z ) diﬀers with S(DL) = ΛPL(d∗ z) only in aspects that do no inﬂuence the value of LPQ(r∗ z ; λ). So for any λ ∈ ΛPQ(dz ), there always exists λ′ ∈ S(DL), such that LPQ(r∗ z ; λ) = LPQ(r∗ z ; λ ′). Therefore, V (PQ(dz)) = V (DQ(dz)) = LPQ(r∗ z ; λ) = sup λ∈S(DL) LPQ(r∗ z ; λ). This proves the dual formulation in Eq. (61). It follows that if the optimal dual solution of the unperturbed problem is unique, i.e., Λ(z∗, u0) = {λ∗}, then V (PL) = V (DL) = DuL(z∗, λ ∗, u0)du. Since V (P Q) is ﬁnite, S(P L) ̸= ∅. By strong duality, we have ∅ ̸= S(DL) ⊆ Λ(z∗, u0) = {λ∗}, thus we must have S(DL) = {λ ∗}. Therefore, V (PQ(dz)) = V (DQ(dz)) = max λ∈S(DL) DuL(z∗, λ, u0)ru + D2L(z∗, λ, u0)((dz, du), (dz, du)) = DuL(z∗, λ ∗, u0)ru + D2L(z∗, λ ∗, u0)((dz, du), (dz, du)). Proof for Proposition 5. Under the asserted strict complementarity condition, K0(z∗, u0, λ ∗) = ∅ and K+(z∗, u0, λ ∗) = K(z∗, u0), so S(PL) = {dz : Dgk(z∗, u0)(dz, du) = 0, k ∈ {1, . . . , s} ∪ K(z∗, u0) } . According to Proposition 4, we have V (PQ) = V (DQ) = min dz ∈S(P L) DuL(z∗, λ ∗, u0)ru + D2L(z∗, λ ∗, u0)((dz, du), (dz, du)). The asserted conclusion then follows. Proof for Theorem 8. Note that the optimization problem in Eq. (63) corresponds to perturbation path u(t) = t, i.e., u0 = 0, du = 1, ru = 0. By assuming unqiue Lagrangian multipliers (λ ∗, ν ∗), we have V (P L) = V (DL) = ∇tL(z∗, 0; λ∗, ν ∗)du = δf (z∗) + ∑ k λ∗ kδgk (z∗). 74 Kallus and Mao: Stochastic Optimization Forests Moreover, because du = 1, ru = 0, V (P Q) = V (DQ) = min dz ∈S(P L) d ⊤ z ∇2 zzL(z∗, 0; λ∗, ν ∗)dz + 2d ⊤ z ∇2 ztL(z∗, 0; λ∗, ν ∗) + ∇2 ttL(z∗, 0; λ∗, ν ∗) Note that ∇2 ttL(z∗, 0; λ ∗, ν ∗) = 0, ∇2 tzL(z∗, 0; λ ∗, ν ∗) = ∇δf (z∗) + s∑ k=1 λ∗ k∇δgk (z∗). Then Eq. (64) follows from the fact that the constraints in DQ now reduces to the following: { d⊤ z ∇z [gk(z) + tδgk (z)] + du∇t [gk(z) + tδgk (z)] } |(z,t)=(z∗,0) =d ⊤ z [∇gk(z) + t∇δgk (z)] |(z,t)=(z∗,0) + ∇t [gk(z) + tδgk (z)] |(z,t)=(z∗,0) =d ⊤ z ∇gk(z∗) + δgk (z∗), [ d ⊤ z ∇zhk(z) + du∇thk(z) ] |(z,t)=(z∗,0) = d ⊤ z ∇hk(z∗). Proof for Corollary 1 Note that under the asserted conditions, conditions 1, 2, 4, 5 in Theorem 8 hold, and condition 3 in Theorem 8 degenerates and thus holds trivially. Note that v′(0) in Theorem 8 now reduces to δf (z∗), and v′′(0) = min dz d ⊤ z ∇2f (z∗)dz + 2d ⊤ z ∇δf (z∗). Under the condition that ∇2f (z∗) is positive deﬁnite (and thus invertible), we have that the optimization problem in the last display has a unique solution d∗ z = − (∇2f (z∗))−1 ∇δf (z∗). Consequently, v′′(0) = −∇δf (z∗)⊤ (∇2f (z∗) )−1 ∇δf (z∗). Proof for Theorem 9. Consider the function φ(t) = v(f0 + tδf ). Given the asserted conditions, for any t ∈ [0, 1], f0(z) + tδf (z) satisﬁes the conditions in Corollary 1, thus results in Corollary 1 imply that φ(t) is twice diﬀerentiable: φ′(t) = v′(f0 + tδf ; δf ) = δf ; φ′′(t) = v′′(f0 + tδf ; δf ) = −∇δf (z∗(f0 + tδf ))⊤ (∇2(f0 + tδf )(z∗(f0 + tδf )))−1 ∇δf (z∗(f0 + tδf )). We now argue that φ′′(t) is also continuous in t ∈ [0, 1]. Since condition 2 in Corollary 1 is satisﬁed for t ∈ [0, 1], there exist a compact set N such that that z∗(f0 + tδf (z)) ∈ N for t ∈ [0, 1]. Note that supz∈N |f0(z) + tδf (z) − f0(z)| → 0 as t → 0 by the fact that δf (z) is bounded over N . Then according to Theorem 5.3 in Shapiro et al. (2014), z∗(f0 + tδf ) → z∗(f0) as t → 0. Similarly, supz∈N ∥∇ 2(f0 + tδf )(z) − ∇ 2f0(z)∥F → 0 as t → 0. This convergence together with the continuity of ∇2f0(z) and ∇2δf (z) imply that ∥∇ 2(f0 + tδf )(z∗(f0 + tδf )) − ∇ 2f0(z∗(f0))∥F → 0. It then follows from the invertibility of ∇2(f0 + tδf )(z∗(f0 + tδf )) for any t ∈ [0, 1] that ∥ (∇2(f0 + tδf )(z∗(f0 + tδf )))−1 − (∇2f0(z∗(f0))) −1 ∥F → 0. Moreover, by the continuity of ∇δf , we have that ∥∇δf (z∗(f0 + tδf )) − ∇δf (z∗(f0))∥2 → 0 as t → 0. These together show that φ′′(t) is continuous in t ∈ [0, 1]. Kallus and Mao: Stochastic Optimization Forests 75 Now that φ(t) is twice continuously diﬀerentiable over [0, 1], there exists t′ ∈ [0, 1] such that φ(1) = φ(0) + φ′(0) + 1 2 φ′′(t′), where φ′(0) = v′(f ; δf ) and φ′′(t′) = v′′(f + t ′δf ; δf ). Or equivalently, v(f0 + δf ) = v(f0) + v′(f ; δf ) + v′′(f0; δf ) + v′′(f0 + t′δf ; δf ) − v′′(f0; δf ). Denote R1 = ∇δf (z∗(f0 + tδf )) − ∇δf (z∗(f0)) and R2 = (∇2 (f0 + tδf ) (z∗(f0 + tδf ))) −1 − (∇2f0(z∗(f0))) −1. It is straightforward to verify that v′′(f0 + t′δf ; δf ) − v′′(f0; δf ) = ∇δf (z∗(f0))⊤ (∇2f0(z∗(f0)))−1 ∇δf (z∗(f0)) − ∇δf (z∗(f0 + tδf ))⊤ (∇2 (f0 + tδf ) (z∗(f0 + tδf )))−1 ∇δf (z∗(f0 + tδf )) = R⊤ 1 (∇2f0(z∗(f0)) )−1 R1 + 2R1 (∇2f0(z∗(f0)) )−1 ∇δf (z∗(f0)) + ∇δf (z∗(f0 + tδf ))⊤R2∇δf (z∗(f0 + tδf )). As δf → 0, we have supz∈N | (f0 + tδf ) (z) − f0(z)| → 0, so that Theorem 5.3 in Shapiro et al. (2014) again implies that |z∗(f0 + tδf ) − z∗(f0)| → 0. It follows that there exist a constant β ∈ [0, 1] such that R1 = ∇2δf (βz∗(f0 +tδf )+(1−β)z∗(f0))(z∗(f0 +tδf )−z∗(f0)) = o(∇2δf (βz∗(f0 +tδf )+(1−β)z∗(f0))) = o(∥δf ∥F ). Similarly, we can also prove that ∥R2∥F → 0 as δf → 0. It follows that as δf → 0, v′′(f0 + t′δf ; δf ) − v′′(f0; δf ) = o(∥δf ∥ 2 F ). Therefore, v(f0 + δf ) = v(f0) + δf (z∗) − 1 2 ∇δf (z∗) ⊤ (∇2f (z∗) )−1 ∇δf (z∗) + o(∥δf ∥ 2 F ). Similarly, we can prove that z∗(f0 + δf ) = z∗(f0) − (∇2f (z∗) )−1 ∇δf (z∗) + o(∥δf ∥F ). H.2. Proofs for Appendix E Proof for Proposition 6. By ﬁrst order optimality condition, for any zdec, ∂ ∂zaux E [ c(zdec, zaux j (zdec) ; Y ) | X ∈ Rj] = 0. It follows that ∇fj (zdec 0 ) = ∂ ∂zdec E [ c(zdec 0 , zaux j (zdec 0 ) ; Y ) | X ∈ Rj] . Note that ∇2f0 (zdec 0 ) = ∂2 (∂zdec) ⊤ ∂zdec E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] + ∂2 ∂zdec (∂zaux) ⊤ E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] ∂ (∂zdec) ⊤ zaux 0 (zdec 0 ) . Moreover, under the asserted smoothness condition and invertibility condition, the implicit function theorem futher implies that ∂ (∂zdec)⊤ zaux 0 (zdec 0 ) = − { ∂2 ∂zaux (∂zaux) ⊤ E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] }−1 × ∂2 ∂zaux (∂zdec) ⊤ E [ c(zdec 0 , zaux 0 ; Y ) | X ∈ R0] , which in turn proves the formula for ∇2fj (zdec 0 ) in Proposition 6. 76 Kallus and Mao: Stochastic Optimization Forests Proof for Proposition 8 The conclusion follows directly from the following facts that can be easily veriﬁed: ∇fj (zdec 0 ) = ∂ ∂zdec ˜fj (zdec 0 , zaux j (zdec 0 )) , ∇2f0 (zdec 0 ) = (∇2 ˜f0 (zdec 0 , zaux 0 ))−1 [ zdec 0 , zdec 0 ] . H.3. Proofs for Appendix G Proof for Lemma 1 The conclusion directly follows from Theorem 5.3 in Shapiro et al. (2014). Proof for Proposition 9 Note that ˆz0 = arg min z 1 n n∑ i=1 I [Xi ∈ R0] ∥z − Yi∥ 2 = 1 n0 ∑ i I [Xi ∈ R0] Yi. Analogously, we deﬁne ˆzj = 1 nj ∑ i I [Xi ∈ Rj] Yi. Note that ∇c(z; y) = z − y and ∇2c(z; y) = I. Thus the gradient and Hessian estimates are ˆhj(ˆz0) = 1 nj ∑ i I [Xi ∈ Rj] (ˆz0 − Yi) = ˆz0 − ˆzj, ˆH0(ˆz0) = I. It follows that ˆCapx-soln(R1, R2) = ∑ j=1,2 1 n n∑ i=1 I [Xi ∈ Rj] c (ˆz0 − ˆH −1 0 ˆhj; Yi) = ∑ j=1,2 1 2n n∑ i=1 I [Xi ∈ Rj] c (ˆzj; Yi) = 1 2n ∑ j=1,2 n∑ i=1 I [Xi ∈ Rj] ∥Yi − ˆzj∥ 2 2 = 1 2 ∑ j=1,2 nj n ( 1 nj n∑ i=1 I [Xi ∈ Rj] ∥Yi − ˆzj∥ 2 2 ) = 1 2 ∑ j=1,2 nj n d∑ l=1 Var ({Yi,l : Xi ∈ Rj, i ≤ n}) and 1 2 ˆCapx-risk(R1, R2) + 1 2 ∑ j=1,2 1 n ∑ i I [Xi ∈ Rj] ∥Yi − ˆz0∥2 2 = 1 2 ∑ j=1,2 nj n d∑ l=1 [ 1 nj ∑ i I [Xi ∈ Rj] (Yi,l − ˆz0,l) 2 − (ˆz0,l − ˆzj,l) 2] = 1 2 ∑ j=1,2 nj n d∑ l=1 [ 1 nj ∑ i I [Xi ∈ Rj] (Yi,l − ˆzj,l + ˆzj,l − ˆz0,l) 2 − (ˆz0,l − ˆzj,l) 2] = 1 2 ∑ j=1,2 nj n d∑ l=1 [ 1 nj ∑ i I [Xi ∈ Rj] (Yi,l − ˆzj,l) 2 + 1 nj ∑ i I [Xi ∈ Rj] (Yi,l − ˆzj,l)(ˆzj,l − ˆz0,l) ] = 1 2 ∑ j=1,2 nj n d∑ l=1 1 nj ∑ i I [Xi ∈ Rj] (Yi,l − ˆzj,l)2 = ∑ j=1,2 nj 2n d∑ l=1 Var ({Yi,l : Xi ∈ Rj, i ≤ n}) Proof for Proposition 10. Note that in Proposition 10, ∇c(z; y) = 2 [ yy⊤z1:d − zd+1y zd+1 − z⊤ 1:dy ] , ∇2c(z; y) = 2 [ yy⊤ −y −y⊤ 1 ] , and also z0,d+1 = arg min zd+1∈R E [ (Y ⊤z1:d − zd+1) 2 | X ∈ R0]∣ ∣z1:d=z0,1:d = z⊤ 0,1:dE [Y | X ∈ R0] . It follows that ∇fj(z0) = E [∇c(z0; Y ) | X ∈ Rj] = 2 [ E [Y Y ⊤ | X ∈ Rj] z0,1:d − E [Y | X ∈ Rj] z0,d+1 z⊤ 0,1:d (E [Y | X ∈ R0] − E [Y | X ∈ Rj]) ] and ∇2f0(z0) = E [∇2c(z; Y ) | X ∈ R0] = 2 [ E [Y Y ⊤ | X ∈ R0] −E [Y | X ∈ R0] −E [Y ⊤ | X ∈ R0] 1 ] Kallus and Mao: Stochastic Optimization Forests 77 Proof for Proposition 11 Recall that fj(z) = E [ 1 α (zd+1 − Y ⊤z1:d) I [zd+1 − Y ⊤z1:d ≥ 0 ] − zd+1 | X ∈ Rj ] , and also z0,d+1 = qα 0 (Y ⊤z0). Under the assumption that Y has a continuous density function and z ̸= 0, Lemma 3.1 in Hong and Liu (2009) implies that ∂ ∂z1:d fj(z) = − 1 α E [ Y I [ zd+1 − Y ⊤z1:d ≥ 0 ] | X ∈ Rj] , ∂ ∂zd+1 fj(z) = 1 α P (zd+1 − Y ⊤z1:d ≥ 0 | X ∈ Rj) − 1. Before deriving the Hessian, we ﬁrst denote µj(ul, yl) as the joint density of ∑ l′̸=l zl′Yl′ and Yl given X ∈ Rj. It follows that for l = 1, . . . , d, ∂2 ∂2zl f0(z0) = − 1 α ∂ ∂zl E [ YlI [ Y ⊤z1:d ≤ zd+1] | X ∈ R0] ∣ ∣ ∣ ∣z=z0 = − 1 α ∂ ∂zl ∫ ∫ zd+1−zlyl −∞ ylµl(ul, yl) dul dyl ∣ ∣ ∣ ∣z=z0 = − 1 α ∫ (−yl) ylµl(zd+1 − zlyl, yl) dyl ∣ ∣ ∣ ∣z=z0 = − µ0 (qα 0 (Y ⊤z0)) α {−E [ Y 2 l | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0]} = µ0 (qα 0 (Y ⊤z0)) α E [ Y 2 l | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0] . Similarly we can prove that for l, l′ = 1, . . . , d ∂2 ∂zl∂zl′ f0(z0) = µ0 (qα 0 (Y ⊤z0)) α E [ YlYl′ | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0] . In contrast, for l = 1, . . . , d, ∂2 ∂zd+1∂zl f0(z0) = ∂ ∂zl ( 1 α P (zd+1 − Y ⊤z1:d ≥ 0 | X ∈ R0) − 1 )∣ ∣ ∣ ∣z=z0 = 1 α ∂ ∂zl ∫ ∫ zd+1−zlyl −∞ µl(ul, yl) dul dyl ∣ ∣ ∣ ∣z=z0 = 1 α ∫ −ylµl(zd+1 − zlyl, yl) dyl ∣ ∣ ∣ ∣z=z0 = − µ0 (qα 0 (Y ⊤z0)) α E [ Yl | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0] , and ∂2 ∂2zd+1 f0(z0) = ∂ ∂zd+1 ( 1 α P (zd+1 − Y ⊤z1:d ≥ 0 | X ∈ R0) − 1 )∣ ∣ ∣ ∣z=z0 = 1 α ∂ ∂zd+1 ∫ ∫ zd+1−zlyl −∞ µl(ul, yl) dul dyl ∣ ∣ ∣ ∣z=z0 = 1 α ∫ µl(zd+1 − zlyl, yl) ∣ ∣ ∣ ∣z=z0 = µ0 (qα 0 (Y ⊤z0)) α . 78 Kallus and Mao: Stochastic Optimization Forests When Y | X ∈ R0 has Gaussian distribution with mean m0 and covariance matrix Σ0, then (Y, Y ⊤z0,1:d) given X ∈ R0 is also has a Gaussian distribution N ([ m0 m ⊤ 0 z0,1:d ] , [ Σ0 Σ0z0,1:d z⊤ 0,1:dΣ0 z⊤ 0,1:dΣ0z0,1:d ]) It follows that Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d) also has a Gaussian distribution with the following conditional mean and conditional variance: E [ Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0] = m0 + Σ0z0,1:d (z⊤ 0,1:dΣ0z0,1:d)−1 (qα 0 (Y ⊤z0) − m ⊤ 0 z0,1:d), Var (Y | Y ⊤z0,1:d = qα 0 (Y ⊤z0,1:d), X ∈ R0) = Σ0 − Σ0z0,1:d (z⊤ 0,1:dΣ0z0,1:d)−1 z⊤ 0,1:dΣ0. Proof for Lemma 2 Under the conditions in Lemma 1, we have that ˆz0 → z0 almost surely, which implies that there exist a neighborhood N around z0 such that ˆz0 ∈ N almost surely for suﬃciently large n. Since supz∈N ∥ ˆH0(z) − ∇ 2f0(z)∥F = op(1) and ∇2f0(z) is continuous, we have that ∥ ˆH0(ˆz0) − ∇ 2f0(z0)∥F = op(1) (Shapiro et al. 2014, Proposition 5.1). By the fact that ∇2f0(z0) is invertible and the con- tinuous mapping theorem, we also have that ˆH0(ˆz0) is diﬀerentiable with high probability and ∥ ˆH −1 0 (ˆz0) − (∇2f0(z0))−1 ∥F = op(1). Since {(x, y) ↦→ I [x ∈ R0] c(z; y) : z ∈ N } is a Donsker class, √n ( ̂p0f0(·) − p0f0(·) ) converges to a Gaussian process (Van der Vaart 2000, Sec. 19.2). By Slutsky’s theorem, this means that √n ( ˆf0(·) − f0(·)) converges to a Gaussian process as well. Then according to Theorem 5.8 in Shapiro et al. (2014), if √n ( ˆf0(·) − p0f0(·) ) converges to a Gaussian process as well and ∇2f0(z0) is invertible, then √n(ˆz0 − z0) also converges to a Gaussian distribution, which implies that ˆz0 − z0 = Op(n−1/2). It follows that we have the following holds almost surely: ∥ˆhj(ˆz0) − ∇fj(z0)∥2 ≤ ∥ˆhj(ˆz0) − ∇fj(ˆz0)∥2 + ∥∇fj(ˆz0) − ∇fj(z0)∥2 ≤ sup z∈N ∥ˆhj(z0) − ∇fj(z0)∥2 + ∥∇fj(ˆz0) − ∇fj(z0)∥2 ≤ sup z∈N ∥ˆhj(z0) − ∇fj(z0)∥2 + ∥∇2fj(z0)∥F∥ˆz0 − z0∥2, (71) which implies that ∥ˆhj(ˆz0) − ∇fj(z0)∥ = Op(n−1/2). Proof for Proposition 12. Note that we only need to verify the conditions in Lemma 2 and that ∣ ∣ ∣ 1 n ∑n i=1 I [Xi ∈ Rj] c (ˆz0 − ˆH −1 0 (ˆz0)ˆhj(ˆz0); Yi) − fj (z0 − (∇2f0(z0))−1 ∇fj(z0))∣ ∣ ∣ = Op(n−1/2) for j = 1, 2. Recall that (∇fj(z0))l = (αl + βl)P (Yl ≤ z0,l | X ∈ Rj) − βl and (∇2f0(z0))ll = (αl + βl)µ0,l(z0) for l = 1, . . . , d, j = 1, 2, and (∇2f0(z0))ll′ = 0 for l ̸= l′, where µ0,l is the density of Yl | X ∈ R0. Note that ∇2f0(z) is continuous, and ∇2f0(z0) is invertible under the asserted conditions. Note that the indicator function class {(x, yl) ↦→ I [x ∈ Rj, yl ≤ zl] : zl ∈ R} is a Donsker class (Van der Vaart 2000, Ex. 19.6). Therefore, supz∈Rd | (ˆhj(z) ) l − (∇fj(z0))l| = Op(n−1/2). Using the Theorem 2 in Jiang (2017) for the uniform convergence of kernel density estimator, we can straightforwardly show that under the asserted Holder continuity condition for µk and rate condition for bandwidth b, the Hessian estimator satisﬁes that supz ∣ ∣ ∣( ˆH0(z)) ll − (∇2f0(z0))ll∣ ∣ ∣ = op(1) for l = 1, . . . , d. Kallus and Mao: Stochastic Optimization Forests 79 Moreover, note that c(z; y) = d∑ l=1 max{αl(zl − yl), βl(yl − zl)} = d∑ l=1 βl(yl − zl) − (αl + βl)(yl − zl)I [yl ≤ zl] . Here the function classes {yl ↦→ βl(yl − zl) : z ∈ Rd} and {yl ↦→ (αl + βl)(yl − zl) : z ∈ Rd} are linear function classes with ﬁxed dimension, so they are Donsker classes (Van der Vaart 2000, Ex 19.17). Moreover, {yl ↦→ I [yl ≤ zl] : zl ∈ Rd} is also a Donsker class (Van der Vaart 2000, Ex. 19.6). It follows that the function class {(x, y) ↦→ I [x ∈ Rj] c(z; y) : z ∈ Rd} is also a Donsker class, according to Ex 19.20 of Van der Vaart (2000). Similar to proving ∥ˆhj(ˆz0) − ∇fj(ˆz0)∥ = Op(n−1/2) in Lemma 2 (see Eq. (71)), we can prove that for j = 1, 2, ∣ ∣ ∣ ∣ ∣ 1 n n∑ i=1 I [Xi ∈ Rj] c (ˆz0 − ˆH −1 0 (ˆz0)ˆhj(ˆz0); Yi) − fj (z0 − (∇2f0(z0))−1 ∇fj(z0) )∣ ∣ ∣ ∣ ∣ = Op(n−1/2). Proof for Proposition 13. Since Example 2 is a special example of Example 5, the conclusions in Propo- sition 13 directly follow from Proposition 15. Proof for Proposition 14. Recall that ˆqα 0 (Y ⊤ i ˆz0) is the empirical quantile of Y ⊤ ˆz0 based on data in R0. Equivalently, ˆqα 0 (Y ⊤ i ˆz0) is the (approximate) minimizer of the following optimization problem: min β∈R 1 n ∑n i=1 (α − I [Y ⊤ i ˆz0 − β ≤ 0]) (Y ⊤ i ˆz0 − β) I [Xi ∈ R0] 1 n ∑n i=1 I [Xi ∈ R0] Since {y ↦→ y⊤z − β : z ∈ Rd, β ∈ R} is a Donsker class (Van der Vaart 2000, Ex. 19.17) and so is {y ↦→ I [y⊤z − β] : z ∈ Rd, β ∈ R}. This implies that sup β∈R,z∈Rd ∣ ∣ ∣ ∣ ∣ 1 n n∑ i=1 (α − I [ Y ⊤ i z − β ≤ 0 ]) ( Y ⊤ i z − β) I [Xi ∈ R0] − E [(α − I [ Y ⊤ i z − β ≤ 0 ]) (Y ⊤ i z − β) I [Xi ∈ R0]] ∣ ∣ ∣ ∣ ∣ → 0. Together with ˆz0 → z0 and the continuity of E [(α − I [Y ⊤ i z − β ≤ 0]) (Y ⊤ i z − β) I [Xi ∈ R0]] in z, this implies that sup β∈R ∣ ∣ ∣ ∣ ∣ 1 n n∑ i=1 (α − I [Y ⊤ i ˆz0 − β ≤ 0 ]) ( Y ⊤ i ˆz0 − β) I [Xi ∈ R0] − E [( α − I [ Y ⊤ i z0 − β ≤ 0 ]) (Y ⊤ i z0 − β) I [Xi ∈ R0] ] ∣ ∣ ∣ ∣ ∣ → 0. Moreover, 1 n ∑n i=1 I [Xi ∈ R0] → P (X ∈ R0) by Law of Large Number. It follows from Theo- rem 5.5 in Shapiro et al. (2014) that ˆqα 0 (Y ⊤ i ˆz0) converges to the set of minimizers of E [(α − I [Y ⊤ i z − β ≤ 0]) (Y ⊤ i z − β) | Xi ∈ R0]. Since the density function of Y ⊤z0 at qα 0 (Y ⊤z0) is positive, min- imizer of E [(α − I [Y ⊤ i z − β ≤ 0]) (Y ⊤ i z − β) | Xi ∈ R0] is unique. Therefore, ˆqα 0 (Y ⊤ i ˆz0) converges to qα 0 (Y ⊤ i z0). Since {y ↦→ y⊤z − β : z ∈ Rd, β ∈ R} is a Donsker class (Van der Vaart 2000, Ex. 19.17), obviously {(x, y) ↦→ I [y⊤z − β, x ∈ Rj] : z ∈ Rd, β ∈ R} and thus {(x, y) ↦→ I [y⊤z − β ≤ 0, x ∈ Rj] y : z ∈ Rd, β ∈ R} are also Donsker classes. Morever, we already prove that ˆqα 0 (Y ⊤ i ˆz0) converges to qα 0 (Y ⊤ i z0), and obviously nj n = 1 n ∑n i=1 I [Xi ∈ Rj] → P (X ∈ Rj). Therefore, − 1 αnj n∑ i=1 I [ Y ⊤ i ˆz0 ≤ ˆqα 0 (Y ⊤ ˆz0), Xi ∈ Rj] Yi = − 1 α E [ I [ Y ⊤ i z0 ≤ qα 0 (Y ⊤z0) ] Yi | Xi ∈ Rj] + Op(n−1/2). Similarly, we can show that 1 nj n∑ i=1 1 α I [ Y ⊤ i ˆz0,1:d ≤ qα 0 (Y ⊤ ˆz0), Xi ∈ Rj] − I [Xi ∈ Rj] = 1 α P (qα 0 (Y ⊤z0) − Y ⊤z0,1:d ≥ 0 | X ∈ Rj) − 1 + Op(n −1/2). 80 Kallus and Mao: Stochastic Optimization Forests Therefore, ˆhj(ˆz0) = ∇fj(z0) + Op(n−1/2). Under the Gaussian assumption, E [ Y | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0] = m0 + Σ0z0 (z⊤ 0 Σ0z0)−1 (qα 0 (Y ⊤z0) − m⊤ 0 z0) Var (Y | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0) = Σ0 − Σ0z0 (z⊤ 0 Σ0z0)−1 z⊤ 0 Σ0. Since both are continuous in z0, m0, Σ0, qα 0 (Y ⊤z0), so when we plug in the empirical estimators that converge to the true values, the estimator for E [Y | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0], Var (Y | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0), and E [Y Y ⊤ | Y ⊤z0 = qα 0 (Y ⊤z0), X ∈ R0] are all consistent. Similar to the proof of Proposition 12, we can show that under the asserted Holder continuity condition and the rate condition on bandwidth b, 1 n0b n∑ i=1 I [Xi ∈ R0] K ((Y ⊤ i ˆz0 − qα(Y ⊤ ˆz0) ) /b ) = µ0 (qα 0 (Y ⊤z0) ) + op(1). Then by the upper boundedness of µ0, we have that ∥ ˆH0(ˆz0) − ∇ 2f0(z0)∥F = op(1). Proof for Proposition 15. Since ∇2f (z) is continuous at z = z0 and ˆz0 → z0 almost surely according to Lemma 1, there exist a suﬃciently small compact neighborhood N around z0 such that the minimum singular value of ∇2f (z), denoted as σmin(∇2f (z)), is at least 2 3 σmin (∇2f (z0)) for any z ∈ N , and for n large enough ˆz0 ∈ N almost surely. Recall that ∇2f0(z) = E [∇2c (z; Yi) | Xi ∈ R0] and ∇fj(z) = E [∇c (z; Yi) | Xi ∈ Rj]. Since ∇2c(z; y) is continuous for all y and N is compact, the class of functions (of y) {y ↦→ ∇2c(z; y) : z ∈ N } is a Glivenko-Cantelli class (Van der Vaart 2000, Example 19.8), which implies the uniform convergence supz∈N ∥ ˆH0(z) − ∇2f0(z)∥ a.s. → 0. Without loss of generality, we can also assume for large enough n that ˆH0(z) is invertible for z ∈ N , and σmin ( ˆH0(z) ) ≥ 1 2 σmin(∇2f (z)) ≥ 1 3 σmin (∇2f (z0)) for z ∈ N . Since ∇c(z; y) is continuously diﬀerentiable, and N is compact, ∇c(z; y) is Lipschitz in z on N . It follows that {y ↦→ ∇c(z; y) : z ∈ N } is a Donsker class (Van der Vaart 2000, Example 19.7). This implies that n1/2 (ˆhj(·) − ∇fj(·)) converges to a Gaussian process G(·) over z ∈ N . Therefore, n1/2 (ˆhj(·) − ∇fj(·) ) = Op(1), and supz∈N ∥ˆhj(z) − ∇fj(z)∥2 = Op(n−1/2). Note that the Donsker property of {y ↦→ ∇c(z; y) : z ∈ N } also implies that it is a Glivenko-Cantelli class, so that supz∈N ∥ˆhj(z) − ∇fj(z)∥2 a.s. → 0. By the fact that for z ∈ N , σmin ( ˆH0(z) ) ≥ 1 3 σmin (∇2f (z0)), ∥ˆhj(z) − ∇fj(z)∥2 a.s. → 0 and ∇fj(z) is bounded on N , we have that there exist another compact set N ′ such that for suﬃciently large n, z − ˆH −1 0 (z)ˆhj(z) ∈ N ′ for any z ∈ N . Since c(z; y) is continuously diﬀernetiable, it is also Lipschitz in z on N ′. Again this means that {y ↦→ c(z; y) : z ∈ N ′} is a Donsker class, so that sup z∈N ′ | 1 n n∑ i=1 I [Xi ∈ Rj] c(z; Yi) − E [I [X ∈ Rj] c(z; Y )] | = Op(n−1/2). Therefore, supz∈N ∣ ∣ ∣ 1 nj ∑n i=1 I [Xi ∈ Rj] c(ˆr(z); Yi) − E [I [X ∈ Rj] c(ˆr(z); Y )] ∣ ∣ ∣ = Op(n −1/2) for ˆr(z) := z − ˆH −1 0 (z)ˆhj(z). Since ˆr(z) → z0 − (∇2f (z0))−1 ∇f (z0) almost surely, we have that ∣ ∣ ∣ ∣ ∣ 1 n n∑ i=1 I [Xi ∈ Rj] c(ˆz0 − ˆH −1 0 (ˆz0)ˆhj(ˆz0); Yi) − E [ I [X ∈ Rj] c(z0 − (∇2f (z0))−1 ∇f (z0); Y )] ∣ ∣ ∣ ∣ ∣ = Op(n −1/2). Kallus and Mao: Stochastic Optimization Forests 81 Proof of Proposition 16 The constraints in Example 1 can be rewritten as Z = { z ∈ Rd : hk (z) = −zk ≤ 0, hd+1 (z) = d∑ l=1 zl − C ≤ 0 } . Note that at any z ∈ Z, there are at most d active inequality constraints, and their gradients have to be linearly independent. So it satisﬁes the LICQ condition at any z ∈ Z. Similarly, we can prove the LICQ condition for Examples 2 and 3 with the simplex constraint. Proof of Proposition 17 Fix a parent region R0 and a split that partitions it into two subregions R1 and R2 (with sample sizes n1, n2 respectively). According to Eq. (4) in Elmachtoub et al. (2020), the SPO splitting criterion for the given split can be written as follows: CSPO (R1, R2) = 2∑ j=1 nj n  min z∈Z 1 nj ∑ i:Xi∈Rj Y ⊤ i z − 1 nj ∑ i:Xi∈Rj min z∈Z Y ⊤ i z   = 2∑ j=1  min z∈Z 1 n ∑ i:Xi∈Rj Y ⊤ i z   − ( 1 n n∑ i=1 min z∈Z Y ⊤ i z ) . Note that the second term above does not depend on the split so using the SPO criterion to choose splits is equivalent to using only the ﬁrst term to choose splits. It is easy to see that the ﬁrst term is exactly our oracle splitting criterion with all unknown expectations replaced by sample averages: ˆCoracle(R1, R2) = ∑ j=1,2 min z∈Z ˆE [c(z; Y )I [X ∈ Rj]] = 2∑ j=1  min z∈Z 1 n ∑ i:Xi∈Rj Y ⊤ i z   . H.4. Proofs for Section 2 Proof for Lemma 1 The conclusion follows from Theorem 5.3 in Shapiro et al. (2014) when the population optimization problem has a unique optimal solution. Proof for Theorem 2 Conditions 1, 2, 3 imply that conditions in Theorem 9 are satisﬁed for both δf = f1 − f0 and δf = f2 − f0. Therefore, Theorem 9 implies that for j = 1, 2, min z∈Rd fj(z) = fj(z0) − 1 2 ∇fj(z0) ⊤ (∇2f0(z0) )−1 ∇fj(z0) + o(∥fj − f0∥F ) where ∥fj − f0∥F = max{supz |fj(z) − f0(z)|, supz ∥∇fj(z) − f0(z)∥2, supz ∥∇ 2fj(z) − f0(z)∥F}. By the Lisp- chitzness condition, we have ∥fj − f0∥F = O(D2 0). Therefore, Coracle(R1, R2) = ∑ j=1,2 pj min z∈Rd fj(z) = ∑ j=1,2 pj (fj(z0) − 1 2 ∇fj(z0)⊤ (∇2f0(z0) )−1 ∇fj(z0) ) + o(D2 0) = p0f0(z0) + 1 2 Capx-risk(R1, R2) + o(D2 0). Proof for Theorem 3 By Theorem 9 with δf = fj − f0 respectively, zj(1) = z0 − (∇2f0(z0))−1 ∇fj(z0) + Rj, where Rj = o(∥fj − f0∥F ). 82 Kallus and Mao: Stochastic Optimization Forests It follows from mean-value theorem that there exist a diagonal matrix Λj whose diagonal entries are real numbers within [0, 1] such that vj(1) = fj(zj(1)) = fj (z0 − (∇2f0(z0))−1 ∇fj(z0) + Rj) = fj (z0 − (∇2f0(z0))−1 ∇fj(z0) ) + R⊤ j ∇fj ( z0 − (∇2f0(z0))−1 ∇fj(z0) + ΛjRj) . We can apply mean-value theorem once again to ∇fj to get ∇fj (z0 − (∇2f0(z0))−1 ∇fj(z0) + ΛjRj) = ∇fj(zj(1)) + O((I − Λj) ⊤ Rj) = O((I − Λj)⊤ Rj), where the last equality follows from the ﬁrst order necessary condition for optimality of zj(1). It follows that vj(1) = fj (z0 − (∇2f0(z0))−1 ∇fj(z0) ) + O(R⊤ j (I − Λj)Rj) = fj (z0 − (∇2f0(z0))−1 ∇fj(z0) ) + o(∥fj − f0∥2 F ) = fj (z0 − (∇2f0(z0))−1 ∇fj(z0) ) + o(D2 0) Therefore Coracle(R1, R2) = ∑ j=1,2 pjfj(zj(1)) = ∑ j=1,2 pjfj (z0 − (∇2f0(z0) )−1 ∇fj(z0)) + o(D2 0) = Capx-soln(R1, R2) + o(D2 0). Proof for Proposition 1. Under the asserted condition, ˆH −1 0 − ∇ 2f −1 0 (z0) = op(1), ˆhj = ∇fj(z0) = Op(n−1/2) for j = 1, 2. It follows that ˆCapx-risk(R1, R2) = − ∑ j=1,2 ˆh ⊤ j ˆH −1 0 ˆhj = − ∑ j=1,2 (h ⊤ j (z0) + Op(n−1/2) ) ( H −1 0 (z0) + op(1) ) (h ⊤ j (z0) + Op(n−1/2) ) = − ∑ j=1,2 h ⊤ j (z0)H −1 0 (z0)hj(z0) + 2h ⊤ j (z0)H −1 0 (z0)Op(n −1/2) + op(n−1/2) = Capx-risk(R1, R2) + Op(n−1/2). Note, the condition that ∣ ∣ ∣ 1 n ∑n i=1 I [Xi ∈ Rj] c (ˆz0 − ˆH −1 0 ˆhj; Yi) − fj(z0)∣ ∣ ∣ = Op(n−1/2) for j = 1, 2 directly ensures that ˆCapx-soln(R1, R2) = Capx-soln(R1, R2) + Op(n−1/2). H.5. Proofs for Section 5 For brevity we deﬁne c(z; x) = E [c(z; Y ) | X = x] and ˆc(z; x) = ∑n i=1 wi(x)c(z; Yi). Under Assumption 2, X is compact. Without loss of generality, we assume X ⊆ [0, 1]p. Lemma 3. Let x ∈ X be ﬁxed and {wij(x) = I[i∈Idec j ,τj (Xi)=τj (x)] ∑n i′ =1 I[i∈Idec j ,τj (Xi′ )=τj (x)] , i = 1, . . . , n} be the weights derived from the jth tree. Under Assumptions 1 and 2, if further kn → ∞ and log T = o(kn), then for any z ∈ C, as n → ∞, sup 1≤j≤T ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − c(z; Xi)) ∣ ∣ ∣ ∣ ∣ p → 0, sup 1≤j≤T ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (b(Yi) − E [b(Yi) | Xi]) ∣ ∣ ∣ ∣ ∣ p → 0. Kallus and Mao: Stochastic Optimization Forests 83 Proof for Lemma 3. In this proof, we ﬁx j = 1, . . . , T and implicitly condition on I tree j . Conditionally on {Xi : i ∈ I dec j }, wij(x) for i = 1, . . . , n are all ﬁxed, and these weights satisfy that wij(x) ≥ 0, ∑n i=1 wij(x) = 1 and maxi wij(x) = [ 1 kn , 1 2kn−1 ]. According to Lemma 12.1 in Biau and Devroye (2015), we have that for any ϵ ≤ min{1,2C} max{η,η′} and any z ∈ C, P (∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − c(z; Xi)) ∣ ∣ ∣ ∣ ∣ ≥ ϵ | X1, . . . , Xn ) ≤ exp (− knϵ2η2 8C ) , P (∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (b(Yi) − E [b(Yi) | Xi]) ∣ ∣ ∣ ∣ ∣ ≥ ϵ | X1, . . . , Xn ) ≤ exp ( − knϵ2η′2 8C ) . It follows that P ( sup j ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − c(z; Xi)) ∣ ∣ ∣ ∣ ∣ ≥ ϵ | X1, . . . , Xn ) ≤ exp ( log T − knϵ2η2 8C ) , P ( sup j ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (b(Yi) − E [b(Yi) | Xi]) ∣ ∣ ∣ ∣ ∣ ≥ ϵ | X1, . . . , Xn ) ≤ exp (log T − knϵ2η′2 8C ) . This means that as n → ∞, sup j ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − c(z; Xi)) ∣ ∣ ∣ ∣ ∣ → 0, sup j ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (b(Yi) − E [b(Yi) | Xi]) ∣ ∣ ∣ ∣ ∣ → 0. Lemma 4. If the assumptions in Lemma 3 hold, sn/kn → ∞ and T = o(sn/kn), then as n → ∞, sup z∈C ∣ ∣ˆc(z; x) − c(z; x)∣ ∣ p → 0. Proof for Lemma 4. Note that ∣ ∣ˆc(z; x) − c(z; x)∣ ∣ ≤ 1 T T∑ j=1 ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − E [c(z; Yi) | Xi = x]) ∣ ∣ ∣ ∣ ∣ ≤ 1 T T∑ j=1 ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − E [c(z; Yi) | Xi]) ∣ ∣ ∣ ∣ ∣ + 1 T T∑ j=1 ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (E [c(z; Yi) | Xi] − E [c(z; Yi) | Xi = x]) ∣ ∣ ∣ ∣ ∣ . (72) Denote Rj(x) as the leaf of the jth tree that contains x. For a set S ⊆ Rp, deﬁne diamJ (S) = supx,x′∈S (∑ j∈J (xj − x′ j) 2)1/2. Following the same arguments as in Lemma 2 of Wager and Athey (2018), we have that P (diamJ (Rj(x)) ≥ ϵ) ≤ √p exp (−C1 log3 (sn/(2kn − 1)) ) . This implies that when sn/kn → ∞ and T = o(sn/kn), P ( sup 1≤j≤T diamJ (Rj(x)) ≥ ϵ) ≤ √pT exp (−C1 log3 (sn/(2kn − 1)) ) → 0. Note that wij(x) > 0 only for i such that Xi ∈ Rj(x). This and the Lipschitz continuity of c(z; x) in x together imply that sup z∈C ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (E [c(z; Yi) | Xi] − E [c(z; Yi) | Xi = x]) ∣ ∣ ∣ ∣ ∣ ≤ Lc sup j diamJ (Rj(x)) → 0. (73) 84 Kallus and Mao: Stochastic Optimization Forests It follows that sup z∈C 1 T T∑ j=1 ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (E [c(z; Yi) | Xi] − E [c(z; Yi) | Xi = x]) ∣ ∣ ∣ ∣ ∣ → 0. Now consider a ϵ/ ˜C−cover of C, which we denote as {z1, . . . , zM } with M ≤ K ˜C d ( diam(C) ϵ )d for a positive constant K. This induces brackets of type [c(zk; Y ) − ϵ ˜C b(Y ), c(zk; Y ) + ϵ ˜C b(Y )] for the function class {y ↦→ c(z; y) : z ∈ C}. Note that for any z ∈ C, n∑ i=1 wij(x) (c(z; Yi) − E [c(z; Yi) | Xi]) ≤ max 1≤k≤M { n∑ i=1 wij(x) ( c(zk; Yi) + ϵ ˜C b(Yi) ) − n∑ i=1 wij(x)E [ c(zk; Yi) + ϵ ˜C b(Yi) | Xi ]} + ϵ ˜C n∑ i=1 wij(x)E [b(Yi) | Xi] By Lemma 3, we have sup j max 1≤k≤M { n∑ i=1 wij(x) (c(zk; Yi) + ϵ ˜C b(Yi)) − n∑ i=1 wij(x)E [ c(zk; Yi) + ϵ ˜C b(Yi) | Xi ]} p → 0. Moreover, sup j ϵ ˜C n∑ i=1 wij(x)E [b(Yi) | Xi] = ϵ ˜C n∑ i=1 wij(x)E [b(Yi) | Xi = x] + ϵ ˜C n∑ i=1 wij(x) (E [b(Yi) | Xi] − E [b(Yi) | Xi = x]) ≤ ϵ + ϵ ˜C Lb sup j diamJ (Rj(x)) → ϵ. Thus as n → ∞, sup j,z n∑ i=1 wij(x) (c(z; Yi) − E [c(z; Yi) | Xi]) ≤ ϵ Similarly, we can prove that as n → ∞, inf j,z n∑ i=1 wij(x) (c(z; Yi) − E [c(z; Yi) | Xi]) ≥ −ϵ By the arbitrariness of ϵ, we have sup j,z ∣ ∣ ∣ ∣ ∣ n∑ i=1 wij(x) (c(z; Yi) − E [c(z; Yi) | Xi]) ∣ ∣ ∣ ∣ ∣ → 0. (74) Eqs. (72) to (74) together imply that sup z∈C ∣ ∣ˆc(z; x) − c(z; x)∣ ∣ p → 0. Proof for Theorem 5. By condition 2 of Assumption 2, we have that for suﬃciently large n, ˆzn ∈ C and arg minz∈Z c(z; x) ⊆ C almost surely. Let us ﬁx z0 ∈ arg minz∈Z c(z; x). The conclusion follows from ∣ ∣ ∣ ∣c(ˆzn; x) − min z∈Z c(z; x) ∣ ∣ ∣ ∣ = |c(ˆzn; x) − c(z0; x)| ≤ ∣ ∣c(ˆzn; x) − ˆc(ˆzn; x) ∣ ∣ + ∣ ∣ˆc(ˆzn; x) − c(z0; x) ∣ ∣ ≤ 2 sup z∈C |z; x) − c(z; x)| → 0. Here the last inequality follows from the facts that ∣ ∣c(ˆzn; x) − ˆc(ˆzn; x) ∣ ∣ ≤ sup z∈C |z; x) − c(z; x)| and that ∣ ∣ˆc(ˆzn; x) − c(z0; x) ∣ ∣ = { ˆc(ˆzn; x) − c(z0; x) ≤ ˆc(z0; x) − c(z0; x) ≤ supz∈C ∣ ∣ˆc(z; x) − c(z; x) ∣ ∣ if ˆc(ˆzn; x) ≥ c(z0; x) c(z0; x) − ˆc(ˆzn; x) ≤ c(ˆzn; x) − ˆc(ˆzn; x) ≤ supz∈C ∣ ∣ˆc(z; x) − c(z; x) ∣ ∣ if ˆc(ˆzn; x) < c(z0; x) . Kallus and Mao: Stochastic Optimization Forests 85 Proof for Proposition 2. Statement 1. Consider cl(z; y) = max{αl (zl − yl) , βl (yl − zl)}. Note cl(z; y) − cl(z′; y) = βl (zl − z′ l) if z′ l, zl ≤ yl and cl(z′; y) − cl(z; y) = αl (zl − z′ l) if z′ l, zl ≥ yl. Denote ∆l = |zl − z′ l|, ∆zl = |zl − yl| and ∆z′ l = |z′ l − yl|. When zl ≤ yl, yl ≤ z′ l, obviously ∆l = ∆zl + ∆z′ l , and |cl(z; y) − cl(z′; y)| = ∣ ∣ ∣βl∆zl − αl∆z′ l ∣ ∣ ∣ ≤ max{αl, βl} max{∆zl , ∆z′ l } ≤ max{αl, βl}∆l. Similarly we can show that when zl > yl, yl > z′ l, |cl(z; y) − cl(z′; y)| ≤ max{αl, βl}∆l. Therefore, |c(z; y) − c(z′; y)| ≤ √d max{αl, βl}∥z − z′∥2. Statement 2. Letting C ′ = sup˜z∈C ∥˜z∥, note that for any z, z′ ∈ C, |c(z; y) − c(z′; y)| = ∣ ∣(y⊤z1:d − zd+1)2 − (y⊤z′ 1:d − z′ d+1)2∣ ∣ ≤ ∣ ∣y⊤ (z1:d + z′ 1:d) − (zd+1 + z′ d+1)∣ ∣ ∣ ∣y⊤ (z1:d − z′ 1:d) − (zd+1 − z′ d+1)∣ ∣ ≤ (∥y∥2∥zd+1 + z′ d+1∥2 + ∣ ∣zd+1 + z′ d+1∣ ∣ ) ∥ ∥ ∥ ∥ [ y 1 ]∥ ∥ ∥ ∥ 2∥z − z′∥2 ≤ 2C ′ (∥y∥2 + 1) √∥y∥ 2 2 + 1∥z − z′∥2 ≤ 4√2C ′ max{1, ∥y∥ 2 2}∥z − z′∥. Statement 3. Note that for any z, z′ ∈ C, |c(z; y) − c(z′; y)| = ∣ ∣ ∣ ∣ 1 α max { zd+1 − y⊤z1:d, 0} − 1 α max { z′ d+1 − y⊤z′ 1:d, 0} ∣ ∣ ∣ ∣ + ∣ ∣zd+1 − z′ d+1∣ ∣ ≤ 1 α ∣ ∣zd+1 − y⊤z1:d − z′ d+1 + y⊤z′ 1:d∣ ∣ + ∣ ∣zd+1 − z′ d+1∣ ∣ ≤ (∥y∥2 + 1 + 1 α ) ∥z − z′∥2.","libVersion":"0.3.2","langs":""}
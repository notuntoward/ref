{"path":"lit/sources/Huang20ModelingComplexSpatial.pdf","text":"Modeling Complex Spatial Patterns with Temporal Features via Heterogenous Graph Embedding Networks Yida Huang 1∗, Haoyan Xu 1∗†, Ziheng Duan 1∗ Anni Ren 2, Jie Feng 1, Xiaoqian Wang 3† 1Zhejiang University 2Nanjing University of Aeronautics and Astronautics 3Purdue University stevenhuang@zju.edu.cn, haoyanxu@zju.edu.cn, duanziheng@zju.edu.cn anniren.edu@gmail.com, zjucse fj@zju.edu.cn, joy.xqwang@gmail.com Abstract Multivariate time series (MTS) forecasting is an important problem in many ﬁelds. Accurate forecasting results can ef- fectively help decision-making. Variables in MTS have rich relations among each other and the value of each variable in MTS depends both on its historical values and on other variables. These rich relations can be static and predictable or dynamic and latent. Existing methods do not incorporate these rich relational information into modeling or only model certain relation among MTS variables. To jointly model rich relations among variables and temporal dependencies within the time series, a novel end-to-end deep learning model, termed Multivariate Time Series Forecasting via Heteroge- nous Graph Neural Networks (MTHetGNN) is proposed in this paper. To characterize rich relations among variables, a relation embedding module is introduced in our model, where each variable is regarded as a graph node and each type of edge represents a speciﬁc relationship among variables or one speciﬁc dynamic update strategy to model the latent depen- dency among variables. In addition, convolutional neural net- work (CNN) ﬁlters with different perception scales are used for time series feature extraction, which is used to generate the feature of each node. Finally, heterogenous graph neu- ral networks are adopted to handle the complex structural information generated by temporal embedding module and relation embedding module. Three benchmark datasets from the real world are used to evaluate the proposed MTHetGNN and the comprehensive experiments show that MTHetGNN achieves state-of-the-art results in MTS forecasting task. Introduction Multivariate time series (MTS) are generally composed of multiple single-dimensional time series of the same object, such as the observation of the same object by multiple sen- sors, the trafﬁc ﬂow of each block in the same area, or the exchange rate information of different countries. People are very interested in analyzing historical time series to get pre- dictions about future trends. MTS have the following two major characteristics: (1) Each single-dimensional time se- ries has internal temporal dependency pattern; (2) MTS have rich spatial relations among different variables. Therefore, ∗Equal contribution with order determined by rolling the dice. †Correspondence to Haoyan Xu and Xiaoqian Wang. the modeling of multi-dimensional time series should con- sider these two sources of information and provide proba- bilistic explanations, so that predictions can be made rea- sonably. Regarding each time series in MTS as a variable, interde- pendency among variables is useful information to exploit. Each variable in MTS depends both on its historical val- ues and on other variables. For example, the activity of sun shows periodic pattern in historic observations. The future trafﬁc ﬂow of a certain street is easier to be predicted by the trafﬁc information of the neighboring area, while the impact from the area farther away is relatively slight. If such priori relation information can be considered, it is more conducive to the interaction among variables with dependency which can be an effective guideline for forecasting. Apart from these priori interdependency information which is available and helpful for MTS forecasting, there also exits relations among variables that are unknown or changing over time, implicitly exhibited. However, existing methods cannot ex- ploit latent and rich interdependencies among variables efﬁ- ciently and effectively. Over the years, researchers have adopted different tech- niques and assumptions to model MTS. Many machine learning methods have been applied for MTS forecasting. Autoregressive integrated moving averate model (ARIMA) is a popular machine learning model which can be applied ﬂexibly to various types of time series with a high compu- tational cost. VAR extends the autoregressive (AR) model to multivariate time series, thus it totally ignores the rela- tions among time series. Classical MTS forecasting mod- els consider statistic information of historical measurement and make the prediction, the farther the forecast is, the less accuracy is the forecast. When deep learning meth- ods are adopted for MTS forecasting, recurrent neural net- works are often used. Due to the lack of ability to model long-term time series, GRU cell and highway structure are applied, boosting the performance of RNN model. Further deep learning model LSTNet and MLCNN, both consider the long-term dependency and short-term variance of time series, but they take the assumption that the time series vari- ables have the same effect to each other, thus they can not model the pairwise dependencies among variables explic-arXiv:2008.08617v1 [cs.LG] 19 Aug 2020 itly. Recently, researches found it promising to model mul- tivariate time series using graph neural networks (Xu et al. 2020; Wu et al. 2020), which is a novel and effective way as the authors suggest. In graph theory, a graph is a structure containing a set of objects in which objects may in some way ’related’. Objects are called vertices or nodes and the relation between nodes are called edges. Time series vari- ables are considered as nodes while the interrelations among them are regarded as edges, the information of which is stored in graph structure and is then processed and ana- lyzed in graph neural networks. Graphs with featured nodes and weighted edges contain rich structural information, thus modeling MTS as graph is promising and reasoning. How- ever, TEGNN, MTGNN can only reveal one type of relation, lacking the ability to model both static and dynamic relations in time series. The classical machine learning methods and mentioned deep learning methods can not fully explore the abundant implicit relations among time series. In this work, a novel framework, termed Multivariate Time Series Forecasting with Heterogeneous Graph Neu- ral Network (MTHetGNN) is proposed and applied for MTS forecasting tasks. MTHetGNN embeds each rela- tion/interdependency into each graph structure and fuses all graph structures with temporal features. Relation em- bedding module considers both static, prior and dynamic, latent relations among variables, characterizing the global relations (such as similarity and causality) and local dy- namic relations among time series respectively. In addition, convolution neural networks (CNN) are used for temporal feature extraction. Finally, heterogeneous graph neural net- works take the output of the former modules and tackle the complex structural embedding of graph structure generated by MTS for forecasting task. Thus our major contributions are: • We ﬁrst propose a heterogeneous graph network based framework that are compatible to take full advantage of rich relations among variables of MTS. • We construct a relation embedding module to explore the relations among time series from both dynamic and static way. • We conduct extensive experiments on MTS benchmark datasets, and the experimental results prove that the per- formance of the proposed method is better than state-of- the-art models. Related Work Graph Network Embedding Nowadays, neural networks have been employed for repre- senting graph structured data, such as social networks and knowledge bases. Extending the word2vec(Mikolov et al. 2013), an unsupervised algorithm DeepWalk(Perozzi, Al- Rfou, and Skiena 2014) is designed to learn representations of nodes in graph based on random walk. Unsupervised net- work embedding algorithms LINE (Tang et al. 2015) and node2vec (Grover and Leskovec 2016) are popular after that. Apart from that, Convolution Neural Network and Recur- rent Neural Network are also deﬁned and employed on graph data. Originated from Graph Signal Processing (Ortega et al. 2018), classical convolutions are extended to spectral do- main, which is space and time consuming. Further research (Defferrard, Bresson, and Vandergheynst 2016) approximate the spectral convolution using K-hops polynomials, reduc- ing the time complexity effectively. Finally, GCN (Kipf and Welling 2016), a scalable approach chose the convolutional architecture via a localized approximation with Chebyshef Polynomial, which is an efﬁcient variant and can operate on graphs directly. However, these methods can only implement on undirected graphs. Previously in form of recurrent neural networks, Graph Neural Networks (GNNs) are proposed to operate on directed graphs. Heterogeneous Network Embedding Conventional methods on processing heterogeneous net- works usually start with extracting typed structural features, which involves the concept of meta structure over the net- work schema (Dong et al. 2020), while the deﬁnition of meta path requires related domain knowledge and experience. Preliminary Task Formulation In this work, we explore the task of multivariate time series forecasting. Formally, given a time series Xi = xi1, xi2, ..., xiT , where xit ∈ Rn is the observation with n variables at time stamp t from the i th sample. T is the number of time stamps contained in a sample. The task is to predict the future value xt+h where h denotes the hori- zon ahead of the current time stamp. Considering all sam- ples X = X1, X2, ..., Xs where s is the number of samples, and the ground truth forecasting value Y = Y1, Y2, ..., Ys, we aims to model the mapping from X to Y using objective function. Heterogeneous Graph network Formally, a heterogeneous graph network is deﬁned as a di- rected graph G = (V, E). The heterogeneous network in- formation are represented with type mapping function τ : V → Tv and ϕ(e) : E → Te (Dong et al. 2020). Tv and Te denote the sets of vertex types and edge types, where |Tv| + |Te| > 2. When modeling multivariate time series as a graph G, the ith time series is regarded as node Vi in G belonging to the same node type Tv (Xu et al. 2020). We aim to use heterogeneous graph neural networks to embed static relation T s e and dynamic relation T d e shown between time series. Thus the multivariate time series could be rep- resented as G = {G1, G2, ..., G|Te|} from a graph perspective considering heterogeneity where Gi denotes graph with type T i v. The Framework Model Architecture We ﬁrst narrate our proposed model MTHetGNN, i.e., Multivariate Time Series Forecasting via Heterogenous Graph Neural Networks in detail, which is a framework for modeling multivariate time series from a graph perspective Figure 1: The general architecture of our model MTHetGNN. MTHetGNN contains three modules and they jointly learn in an end-to-end fashion. Temporal embedding module captures temporal features as node features. Relation embedding module captures static, prior and dynamic, latent spatial relations among variables. Heterogeneous graph embedding module exploits and fuses rich spatial patterns with temporal features for better forecasting. with compatible modules. An overview of MTHetGNN is illustrated in Figure 1. MTHetGNN contains three compo- nents: Temporal Embedding Module, Relation Embedding Module and Heterogeneous Graph Embedding Module. To capture temporal features from time series, we adopt CNN with multi receptive ﬁelds in temporal embedding module, which could be replaced by methods like RNN and its vari- ants. The relation embedding module captures different in- ternal static and dynamic relations among variables in MTS. Taking the above two modules’ output, heterogeneous graph embedding models can exploit rich spatial dependencies in graph structures to model heterogeneity in time series. The three modules jointly learn in an end-to-end fashion to ex- ploit and fuse priori information, dynamic latent relations and temporal features. The following sections detail the three modules we design. Temporal Embedding Module This module aims to capture temporal features by applying multiple CNN ﬁlters. As shown in Figure 1, CNN ﬁlters with different receptive ﬁelds are applied on multivariate time se- ries, thus features under different periods are extracted from raw signals. Time series may have multiple meaningful periods with possible temporal patterns. For example, similar trends of trafﬁc ﬂow in a certain district are observed through week days. This kind of temporal patterns are unknown when de- signing the network structure. Thus the strategy of using ﬁl- ters with multiple sizes from convolution neural networks are applied. The CNN kernel sizes should be carefully cho- sen to capture ﬁne-grained features shown in different tem- poral periods. Follow the concept of inception suggested in (Szegedy et al. 2015), we use p CNN ﬁlters with kernel sizes (1 × ki)(i = 1, 2, 3, ..., p) to scan through input time series x to capture features at multiple time scales. Here set of con- volution ﬁlters with kernel sizes of [1 × 3, 1 × 5, 1 × 7] are used to capture features at multiple time scales. Relation Embedding Module The relation embedding module learns graph adjacency ma- trix to model the internal relations among time series. We model implicit relations in MTS variables using both static and dynamic strategies. From the static perspective, we use correlation coefﬁcient and transfer entropy to model static linear relationships and implicit causality among vari- ables. From the dynamic perspective, we adopt dynamic graph learning concept and learn the graph structure adap- tively, modeling time-varying graph structure. By using the above three strategies, varies adjacency matrices are gener- ated and then fed into heterogeneous graph neural networks to interpret the relations between graph nodes in both static and dynamic way. Recall that for all samples X = {X1, X2, ..., Xs}, the similarity adjacency matrix Asim ∈ R n×n is generated by: Asim = Similarity(X ) (1) where Similarity is a distance metric which measures the pairwise similarity scores between time series. Ex- isted work to measure distance include the Euclidean Distance, the Landmark Similarity and Dynamic T ime W arping(DT W ), etc. Here we adopt correlation coef f icient to measure the global correlation among time series, offering a priori knowledge of overal linear relation. Thus element in Asim is generated by: Asim ij = Cov(Xi, Xj) √ D(Xi)√ D(Xj) (2) where Cov(Xi, Xj) is the covariance between Xi and Xj, D(Xi) and D(Xj) is the variance of time series Xi and Xj respectively. The causality adjacency matrix Acas ∈ R n×n is gener- ated by: Acas = Causality(X ) (3) where Causality is a metric to measure causality be- tween time series. Various efforts have been made to mea- sure causal inference among variables, such as Grager, ..... , etc. Here we propose the use of Transfer Entropy (TE) to process non-stationary time series, a pseudo-regression rela- tionship will be produced in which pairwise time series be considered causal if they have an overall trend caused by common factors. The causality mentioned here is not strict, but the value is helpful for predicting. Given graph variables X and Y , the transfer entropy of variable A to B is deﬁned as: TB→A = H(At+1|At) − H(At+1|At, Bt) (4) in which the concept of conditional entropy is used. Let mt represent the value of variable m at time t, and m (k) t = [mt, mt−1, ..., mt−k+1]. H(Mt+1|Mt) is the condi- tional entropy representing the information amount of Mt+1 under the condition that the variable Mt is known: H(Mt+1|Mt) = ∑ p (mt+1, m (k) t ) log2 p (mt+1|m(k) t ) (5) by calculate transfer entropy between time series, the ele- ment in causality adjacency matrix Acas is calculated as: Acas ij = TXi→Xj − TXj →Xi (6) The third strategy adopts the concept of dynamic evolving networks(Skarding, Gabrys, and Musial 2020). In a certain period of time, the time series are persist to establish a rel- atively stable graph network, and node properties like node degree can be updated in traning process. We propose a dy- namic relation embedding strategy, which learns the adja- cency matrix ADA ∈ Rn×n adaptively to module relations that shows in the given time series sample Xi, denoted as: Ady = Evolve(Xi) (7) Given the input time series Xk = x1, x2, ..., xn ∈ Rn×T from the kth sample with length T , where xi, xj denote the ith, jth time series. We ﬁrst calculate the distance matrix D between sampled time series: Dij = exp(σ(distance(xi, xj))) ∑n p=0 σ(exp(σ(distance(xi, xp))) (8) where distance denotes the distance metric. Then the dy- namic adjacency matrix Ady ∈ R n×n can be calculated as: Ady = σ(DW ) (9) where W is learnable model parameters, σ is an activation function. Normalization is applied to the output of each strategy respectively to form three normalized adjacency matrix. What’s more, to improve training efﬁciency, reduce the ef- fect of noise, amplify the effective relations and make the model more robust, threshold is set to make all the adjacency matrices sparse: Ar ij = { Ar ij Ar ij >= threshold 0 Ar ij < threshold (10) Heterogeneous Graph Embedding Module This module could be viewed as a graph based aggregation method, which fuses temporal features and spatial relations between time series to get forecasting results. Our model is primarily motivated by rGCNs (Schlichtkrull et al. 2018) which learns an aggregation function that is representation-invariant and can operate on large-scale relational data. We adopt the idea of fusing node embeddings of each heterogeneous graph with atten- tion mechanism. We propose the following propagation function: H (l+1) = σ ( H (l)W l 0 + ∑ r∈R sof tmax(αr) ˆArH (l)W (l) r ) (11) where H (l) is the matrix of node embedding in the lth layer, H (0) = X; α(r) is the weight coefﬁcient of each het- erogeneous graph, and sof tmax(αr) = exp(αr) ∑|R| i=1 exp(αi) . W (l) o and W (l) r are layer-speciﬁc weight matrix. σ is a nonlinear activation function, usually being Relu. We use attention mechanism to draw global dependencies between input time series and output forecasting results. Ad- ditive attention and dot-product attention are two commonly used attention functions. Additive attention function uses a feedforward neural network with a hidden layer. The input layer is a horizontal splicing of two vectors, and the activa- tion function of the output layer is sigmoid to indicate the correlation between the two vectors. Matrix operations are supported in dot-product attention, which makes the calcula- tion faster and space saving. Inspired by scaled dot-product attention mechanism (Vaswani et al. 2017),we can extend the use of the dot-product operation to compute the attention coefﬁcients between heterogeneous graph neural network. Noting that the aggregation function of graph structure used in this module is identical to GCN (Kipf and Welling 2016), which uses the 1 th order Chebyshev polynomials to approximation of spectral convolution, assuming only the direct neighbor of a node is reachable at each layer. ((As GCN faces oversmoothing problem, the information from farther neighbor nodes may not be sufﬁciently integrated.)) Other form of graph information propagation methods could be considered to replace the GCN layer. For example, GAT (Vaswani et al. 2017) layer considers the importance of dif- ferent neighbor nodes with attention mechanism. MixHop (Abu-El-Haija et al. 2019) layer fuses information from dif- ferent hops of localized spectral ﬁlters, keeping a balance between local and neighborhood information. We leave this for future work. Objective Function Objective function using L2 loss is used in many forecasting tasks: minimize(L2) = 1 k k∑ i n∑ j (yij − ˆyij)2 (12) where k is the training size and n is the variables in time series. ˆy is the prediction and y is the ground truth value. And researchers have found that objective function using L1 loss has a stable gradient for different inputs, which can reduce the impact of outliers while avoiding gradient explo- sions: minimize(L1) = 1 k k∑ i n∑ j |yij − ˆyij| (13) We use the Adam optimizer in our traning, and decide which objective function to use by the performance shown on validation set. Experiments We conduct experiments on MTHetGNN model on 3 bench- mark datasets and compare the performance of MTHetGNN with 5 baseline methods for multivariate time series fore- casting tasks. Our codes will be available online soon 1. Data We use three benchmark datasets which are commonly used in multivariate time series forecasting. Datasets information are details as following: • Trafﬁc 2: The trafﬁc highway occupancy rates measured by 862 sensors in San Francisco Bay Area from 2015 to 2016 by California Department of Transportation. • Solar-Energy 3: Continuous collected Solar energy data from the National Renewable Energy Laboratory, which contains the solar energy output collected from 137 pho- tovoltaic power plants in Alabama in 2007. • Exchange-Rate: The exchange rate data containing the daily exchange rates from eight countries, including UK, Japan, New Zealand, Canada, Switzerland, Singapore, Australia and China, ranging from 1990 to 2016. 1Our codes will be released as well upon the acceptance of this paper. 2http://pems.dot.ca.gov 3http://www.nrel.gov/grid/solar-power-data.html Methods for Comparison We evaluate the performance of MTHetGNN model with other ﬁve baseline models on multivariate time series fore- casting task. The overview of baseline methods are summa- rized as bellow: • VAR-MLP: A machine learning model mentioned in (Zhang 2003) which is the combination of Multilayer Per- ception(MLP) and Autoregressive model. • RNN-GRU: A Recurrent Neural Network adopting GRU cell (Dey and Salemt 2017). • LSTNet (Lai et al. 2018): A deep learning method, which uses Convolution Neural Network and Recurrent Neural Network to discover both short and long term patterns for time series. • MLCNN (Cheng, Huang, and Zheng 2019): A deep neu- ral network which fuses forecasting information of differ- ent future time. • MTGNN (Wu et al. 2020): A graph neural network de- signed for multivariate time series forecasting. • TEGNN (Xu et al. 2020): A novel deep learning frame- work to tackle forecasting problem of graph structure gen- erated by multivariate time series considering causal rele- vancy. Metrics The following conventional evaluation metrics are used to evaluate all methods: Relative Squared Error (rse), Relative Absolute Error (rae), and Empirical Correlation Coefﬁcient (CORR). For rse and rae, lower value is better, while for corr higher is better. Experimental Details On three bench mark datasets, data are split into training set, validation set and testing set in a ratio of 6 : 2 : 2, then we use the model with the best performance based on rse, rae and corr metrics on validation set for testing. We conduct grid search strategy over adjustable hyperparame- ters for all methods. The windowsize T for all methods are the same under the same setting. For LSTNet, the hidden CNN and RNN layer is chosen from {20, 50, 100, 200}, the length of recurrent-skip is set to 24. For MTGNN, the mix- hop propagation depth is set to 2, the activation saturation rate of graph learning layer is set to 3. For RNN, the hid- den RNN layer is chosen from {10, 20, 50, 100}, the dropout rate is chosen from {0.1, 0.2, 0.3}. For TEGNN and MTHet- GNN, the hidden graph convolutional networks is chosen from {5, 10, 15, ..., 100}. The Adam algorithm is used to optimize the parameters for MTHetGNN. More detailed pa- rameter settings are illustrated in our code. Effectiveness Table 1 summarizes the evaluation results of MTHetGNN and other baseline methods on three benchmark datasets under different settings. Following the settings of LSTNet (Lai et al. 2018), we test the model performance on fore- casting future values {Xt+3, Xt+6, Xt+12, Xt+24}, which Table 1: MTS forecasting results measured by RSE/RAE/CORR score over three datasets. Dataset Exchange-Rate Solar T rafﬁc horizon horizon horizon horizon horizon horizon horizon horizon horizon horizon horizon horizon Methods Metrics 3 6 12 24 3 6 12 24 3 6 12 24 V A R RSE 0.0186 0.0262 0.0370 0.0505 0.1932 0.2721 0.4307 0.8216 0.5513 0.6155 0.6240 0.6107 RAE 0.0141 0.0208 0.0299 0.0427 0.0995 0.1484 0.2372 0.4810 0.3909 0.4066 0.4177 0.4032 CORR 0.9674 0.9590 0.9407 0.9085 0.9819 0.9544 0.9010 0.7723 0.8213 0.7826 0.7750 0.7858 R N N - G R U RSE 0.0200 0.0262 0.0366 0.0527 0.1909 0.2686 0.4270 0.4938 0.5200 0.5201 0.5320 0.5428 RAE 0.0157 0.0209 0.0298 0.0442 0.0946 0.1432 0.2302 0.2849 0.3625 0.3708 0.3669 0.3844 CORR 0.9772 0.9688 0.9534 0.9272 0.9832 0.9660 0.9112 0.8808 0.8436 0.8459 0.8316 0.8232 L S T N E T RSE 0.0216 0.0277 0.0359 0.0482 0.1940 0.2755 0.4332 0.4901 0.4769 0.4890 0.5110 0.5037 RAE 0.0171 0.0226 0.0295 0.0404 0.0999 0.1510 0.2413 0.2997 0.3161 0.3291 0.3435 0.3441 CORR 0.9749 0.9678 0.9534 0.9353 0.9825 0.9633 0.9065 0.8673 0.8730 0.8657 0.8534 0.8537 M L C N N RSE 0.0172 0.0449 0.0519 0.0438 0.1794 0.2983 0.3673 0.5191 0.4924 0.4992 0.5214 0.5353 RAE 0.0129 0.0334 0.0422 0.0375 0.0844 0.1342 0.1873 0.3131 0.3376 0.3243 0.3766 0.3825 CORR 0.9780 0.9610 0.9550 0.9407 0.9814 0.9642 0.9210 0.8513 0.8629 0.8416 0.8320 0.8255 M T G N N RSE 0.0194 0.0253 0.0345 0.0447 0.1767 0.2342 0.3088 0.4352 0.4178 0.4774 0.4461 0.4535 RAE 0.0156 0.0206 0.0283 0.0376 0.0837 0.1171 0.1627 0.2563 0.2435 0.2670 0.2739 0.2651 CORR 0.9782 0.9711 0.9564 0.9370 0.9852 0.9727 0.9511 0.8931 0.8960 0.8665 0.8794 0.8810 T E G N N RSE 0.0178 0.0245 0.0363 0.0449 0.1824 0.2612 0.3289 0.4733 0.4421 0.4433 0.4508 0.4692 RAE 0.0135 0.0195 0.0306 0.0388 0.0851 0.1312 0.1766 0.2821 0.2651 0.2616 0.2740 0.2855 CORR 0.9815 0.9732 0.9566 0.9352 0.9847 0.9676 0.9379 0.8854 0.8853 0.8820 0.8743 0.8671 M T H E T G N N RSE 0.0173 0.0238 0.0327 0.0430 0.1668 0.2175 0.2872 0.3862 0.4142 0.4303 0.4376 0.4500 RAE 0.0132 0.0190 0.0266 0.0361 0.0788 0.1111 0.1514 0.2217 0.2349 0.2490 0.2592 0.2661 CORR 0.9824 0.9746 0.9604 0.9415 0.9872 0.9772 0.9583 0.9210 0.8975 0.8887 0.8828 0.8776 means future value from 3 to 24 days over the Exchange- Rate data, 30 to 240 minutes over the Solar-Energy data, and 3 to 24 hours over the Trafﬁc data, s thus horizon are set to {3, 6, 12, 24} for three benchmark datasets respectively. As shown in table 1, the best results under 4 different horizon with 3 evaluation metrics are set bold, among which MTHet- GNN has records set bold. Clearly, the proposed MTHetGNN model consistently shows state-of-the-art results on all datasets. TEGNN, MT- GNN and MTHetGNN use graph structure to model time se- ries, the strong representing ability of graph neural networks make these three models behave better than other baseline methods to some extent. It is noteworthy that MTHetGNN outperforms the strong graph-based baseline TEGNN, espe- cially on datasets containing plenty variables, indicating the strong information aggregation capabilities heterogeneous graph networks shows under the same neural network depth. This is partly because TEGNN model captures the causality of multivariate time series while MTHetGNN focus on het- erogeneity. Measuring transfer entropy on the whole time series makes the measurement of causality more accurate. However, macroscopic observations will ﬁlter out the ﬂuc- tuations in a single time series segment, thus transfer entropy matrix cannot fully represent the relationship between vari- ables in different time segments. Considering this, MTHet- GNN not only takes the static relations among time series into account, but also considers the dynamic correlations shown in a shorter time segment, fully exploiting the hetero- geneity of time series. Detailed analysis are shown in fol- lowing sections. (a) (b) Figure 2: Running time for train and test process for all methods on solar dataset when horizon is 3. Figure 2.a records the time for training per epoch, ﬁgure 2.b shows the time for testing per epoch. Efﬁciency MTHetGNN has three graph embedding paths, which in- creases the complexity of the model to a certain extent. But the node feature matrix generated by temporal embedding module is shared by all the three paths. And the adjacency matrix AT E and ACO can be calculated in the ofﬂine mod- eling stage, which has little effect on the model complexity. In order to verify the time complexity of the MTHetGNN model, we test the training and testing time of the MTHet- GNN model and other methods on the exchange rate dataset. As shown in Figure 2, the MTHetGNN model mines time se- ries multi relations while having relatively high computing efﬁciency. Ablation Study In this subsection, we conduct ablation studies on Exchange- Rate dataset to understand the contributions of heteroge- neous graph network in MTHetGNN model. There are two main types of settings, type1,2,3 removes the heterogeneous graph part and use one relation extracting way respectively, type4 replaces the attention part with the average operation. The detailed setting of each variant model is as followed: • type1: Only the Transfer Entropy matrix is used to inte- grate neighbor information in each layer. • type2: Only the Correlation Coefﬁcient matrix is used to integrate neighbor information in each layer. • type3: Only the Dynamic matrix is used to integrate neighbor information in each layer. • type4: The MTHetGNN model without attention compo- nent, in which the adjacency matrix obtained by three strategies are averaged to a single matrix. (a) (b) (c) Figure 3: Performance of MTHetGNN and four variants on three benchmark dataset after training 100 epochs. The ex- periment settings are the same for these methods. The results are shown in ﬁgure 3. We notice that MTHet- GNN can model the time series trend more precisely than each variant model, which indicates the effectiveness of both heterogeneous network embedding and attention mech- anism in modeling MTS. As is shown, using heteroge- neous graph instead of each relation graph raises the rse metric of 5.1%, 7.2%, 4.5% respectively for type1,2,3. It is not surprising given the motivation of using heterogeneous graph. Type1,2,3 each only considers relation of the time se- ries in one perspective, while MTHetGNN adopts the con- cept of heterogeneous graph and fuses the relations in both static and dynamic way. The difference in results between MTHetGNN and type4 indicates the effectiveness of atten- tion mechanism as mentioned in the former section. By adopting attention mechanism, MTHetGNN can treat each relation graph with different weight, in accordance with the importance of each type of relation altered in training pro- cess. Figure 4: Performance of MTHetGNN under different set- tings of heterogeneous graph neural networks, the hidden size of GNN layers is varying while other hyperparameters remain the same. Meanwhile, we change the network parameters of the het- erogeneous graph network and test the performance of the MTHetGNN model under different parameter settings on exchange rate dataset. Figure 4 shows that the MTHetGNN model does not rely on speciﬁc parameters, being relatively not sensitive to parameter changes, showing its effectiveness and stability. Conclusion In this paper, we propose a novel heterogenous graph em- bedding network based framework(MTHetGNN) for multi- variate time series forecasting. MTHetGNN can exploit and fuse rich spatial relation information and temporal features generated by MTS. Experiments on three real-world datasets show that our model outperforms 5 baselines in terms of three metrics. There are several directions to go for the future work: (1) It is promising to explore more effective dynamic graph embedding updating strategies to learn complex latent spa- tial dependencies among variables; (2) Heterogeneous graph neural models with stronger representation ability can be in- corporated into our framework to characterize rich spatial re- lations and temporal features, so as to improve prediction ac- curacy and make MTHetGNN more robust; (3) Our method and other current deep learning methods can only model spa- tial and temporal patterns within MTS slices(window size) and cannot model the relation between MTS Slices. It’s in- teresting to explore whether the idea of dynamic graph re- current neural networks can be adopted in MTS forecasting. Acknowledgments We thank Yunsheng Bai, Pengyu Song, Runjian Chen and Zhou Zhao for valuable discussions. References Abu-El-Haija, S.; Perozzi, B.; Kapoor, A.; Alipourfard, N.; Lerman, K.; Harutyunyan, H.; Steeg, G. V.; and Galstyan, A. 2019. MixHop: Higher-Order Graph Convolutional Ar- chitectures via Sparsiﬁed Neighborhood Mixing. Cheng, J.; Huang, K.; and Zheng, Z. 2019. Towards Better Forecasting by Fusing Near and Distant Future Visions. Defferrard, M.; Bresson, X.; and Vandergheynst, P. 2016. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in neural information pro- cessing systems, 3844–3852. Dey, R.; and Salemt, F. M. 2017. Gate-variants of gated re- current unit (GRU) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS), 1597–1600. IEEE. Dong, Y.; Hu, Z.; Wang, K.; Sun, Y.; and Tang, J. 2020. Het- erogeneous Network Representation Learning. IJCAI. Grover, A.; and Leskovec, J. 2016. node2vec: Scalable Fea- ture Learning for Networks. Kipf, T. N.; and Welling, M. 2016. Semi-supervised classi- ﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907 . Lai, G.; Chang, W.-C.; Yang, Y.; and Liu, H. 2018. Model- ing long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, 95– 104. Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef- ﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 . Ortega, A.; Frossard, P.; Kovaˇcevi´c, J.; Moura, J. M.; and Vandergheynst, P. 2018. Graph signal processing: Overview, challenges, and applications. Proceedings of the IEEE 106(5): 808–828. Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowl- edge discovery and data mining, 701–710. Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg, R.; Titov, I.; and Welling, M. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, 593–607. Springer. Skarding, J.; Gabrys, B.; and Musial, K. 2020. Foun- dations and modelling of dynamic networks using Dy- namic Graph Neural Networks: A survey. arXiv preprint arXiv:2005.07496 . Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; and Rabinovich, A. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, 1–9. Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web, 1067–1077. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At- tention Is All You Need. Wu, Z.; Pan, S.; Long, G.; Jiang, J.; Chang, X.; and Zhang, C. 2020. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. Xu, H.; Huang, Y.; Duan, Z.; Wang, X.; Feng, J.; and Song, P. 2020. Multivariate Time Series Forecasting with Transfer Entropy Graph. Zhang, G. P. 2003. Time series forecasting using a hybrid ARIMA and neural network model. Neurocomputing 50: 159–175.","libVersion":"0.3.1","langs":""}
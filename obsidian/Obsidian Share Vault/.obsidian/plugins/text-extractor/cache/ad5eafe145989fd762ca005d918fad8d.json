{"path":"lit/lit_sources/Buhler20DatadrivenMarketSimulator.pdf","text":"A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD Abstract. Neural network based data-driven market simulation unveils a new and ﬂexible way of modelling ﬁnancial time series, without imposing assumptions on the underlying stochastic dynamics. Though in this sense generative market simulation is model-free, the concrete mod- elling choices are nevertheless decisive for the features of the simulated paths. We give a brief overview of currently used generative modelling approaches and performance evaluation met- rics for ﬁnancial time series, and address some of the challenges to achieve good results in the latter. We also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach. While most generative models tend to rely on large amounts of training data, we present here a gen- erative model that works reliably even in environments where the amount of available training data is notoriously small. Furthermore, we show how a rough paths perspective combined with a parsimonious Variational Autoencoder framework provides a powerful way for encoding and evaluating ﬁnancial time series in such environments where available training data is scarce. Finally, we also propose a suitable performance evaluation metric for ﬁnancial time series and discuss some connections of our Market Generator to deep hedging. Contents 1. Introduction 2 Generative Modelling and Market Generators 3 Practical use and applications of Market Generators 3 Some approaches to numerical data simulation in ﬁnance: Classical and new 4 2. Challenges of ﬁnancial time-series simulation: Classical and new 5 2.1. A reminder of speciﬁc stylised facts and evaluation metrics 7 2.2. Challenges for similarity metrics for ﬁnancial time-series by generative models 8 2.3. On signatures, their advantages in encoding path valued data, and their meaning 10 3. Main results: Our methodology and its background and motivation 12 3.1. Methodology: An overview of the main steps 13 3.2. Background and explanations to our generative modelling methodology 14 4. Numerical Results 18 4.1. Numerical experiments with historical data of S&P 18 4.2. Numerical experiments with synthetic paths from rough volatility models 21 5. Conclusions 22 Appendix A. Signatures 22 A.1. Signatures and their properties 22 A.2. Lead-lag transformation 23 Appendix B. Variational Autoencoders 24 References 25 Date: June 26, 2020. 1991 Mathematics Subject Classiﬁcation. 60G20, 60G22, 60L10, 60L20, 60L90, 91G60, 91G70, 91G80, 91G99. Opinions expressed in this paper are those of the authors, and do not necessarily reﬂect the view of JP Morgan. The algorithm developed in this paper is available in the Github repository provided with this paper Github:Marketsimulator . 1arXiv:2006.14498v1 [q-fin.ST] 21 Jun 2020 2 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD 1. Introduction Recent advances of deep learning applied to quantitative ﬁnance [5, 10, 12, 17, 33, 35, 57, 67] have demonstrated the potential prowess of deep learning algorithms in the context of pricing and hedging derivatives. While the neural network based hedging engine presented in [10] rendered a convincing hedging performance when the market scenarios provided for training came from classical stochastic models (such as the Black-Scholes and Heston models including various forms of market frictions), the approach is in fact inherently model agnostic. The collection of sample paths provided to the network in the training phase enable the latter to generate optimal response strategies with respect to market distributions resembling the ones presented during the training phase. This and similar model-agnostic neural network based ﬁnancial applications drove interest towards such market scenario generators that are ﬂexible and highly realistic, ideally even model- free and directly data driven. Indeed, the accurate modelling and eﬃcient numerical simulation of stochastic market movements and ﬁnancial time series has been a central theme throughout the past decades of ﬁnancial modelling. In practice however, even an overly complex model family does not necessarily include the target function or the true data-generating process (or a close approximation thereof) when the latter evolves over time. In this work we take a leap beyond classical stochastic models and present a statistically driven market simulator based on generative modelling: Lately, tremendous progress has been made in training neural networks as powerful function approximators through backpropagation. This progress has brought about frameworks which can use backpropagation-based function approximators to build generative models. Such models are based on the idea of transforming samples of latent variables to data samples via diﬀerentiable functions, which are approximated by a neural network. Indeed, the emergence of generative modelling techniques in various machine learning applications opened up new hori- zons for even more ﬂexible and directly data-driven simulations of market paths, however these possibilities come along with a new set of challenges which brings new unexplored questions to light that we endeavour to identify and address in this paper. The contributions presented in this work are the following: We develop a powerful, ﬂexible, non-parametric generative model for ﬁnancial time series based on signatures of paths, which is capable of eﬃciently operating in an environment where a very low amount of training data is available. We demonstrate the prowess of this algorithm and its ability to produce realistic synthetic data to a high precision—that can be conditioned on various market indicators—in various numerical examples on historical-, as well as on simulated data. The paper is organised as follows: We motivate our work by outlining in Section 1 how potential applications of generative modelling based market simulation (market generators) can go beyond the currently widespread standard applications of numerical simulations of market paths. These applications incldude data anonymisation techniques, or the identiﬁcation of relevant properties (anomalies and outliers) of available data, as well as sophisticated backtesting and risk manage- ment strategies. Section 1 contains a brief summary of such applications, for a more detailled study see [45]. In Section 2 we contrast the main diﬀerences of traditional path generation of sto- chastic processes with the generative approach and discuss new challenges that arise if numerical time series are simulated by generative models rather than traditional means. We also point out how some of the traditionally common indicators describing if a ﬁnancial time series is a realistic reﬂection of market paths may— for generative models—not be fully suﬃcient as performance evaluation metrics. To address the aforementioned new set of challenges we develop an approach that uses the signatures of historical path segments as input data. The concept of signatures was coined in the context of Rough Paths (see [23, 47, 49]) and provides an eﬃcient and parsimonious tool kit to encode the most essential information contained in (continuously observed or discretely sampled) market paths. In Section 3 we present our main results and modelling setup. Section 3.1 gives a brief overview of our general methodology summarized in ﬁve main steps and Section 3.2 provides more details on the modelling choices in each of the steps. To demonstrate the eﬃciency of our proposed approach numerically in Section 4, we generate synthetic market scenarios using variational autoencoders both in the signature-based setting and in the standard returns-based setting. The superiority A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 3 of the signature based framework is displayed both from a computational and from a theoretical point of view. In order to address the non-stationary nature of ﬁnancial markets, in our framework the generated new market scenarios can be conditioned on various market indicators. The latter conditioning can then also be used to build sample paths of arbitrary length (see Section 3.1 (Step 3 ) and Section 4). It is apparent in the majority of (deep) neural network learning based pricing, hedging, forecasting or even generative algorithms that these applications often heavily rely on the availability of large training datasets, which are not always readily available. The particular achievement presented in this work is to remove this limitation: We demonstrate the ability of our approach to generate new samples from a particularly small observable dataset to a high precision, which can in turn be conditioned on speciﬁc market indicators and conditions and used to feed other applications with the necessary amount of training samples. The algorithm developed in this paper is available in the Github repository provided with this paper Github:Marketsimulator. Generative Modelling and Market Generators. The emergence of DNN-based ﬁnancial applications is one of the driving factors that directed the interest towards highly realistic market simulators: A key factor for training these deep networks to a suﬃcient accuracy is the the availability of suﬃciently large, representative training datasets. In the example of deep hedging it is easy to see that the quality and quantitiy of available training data impacts the hedging performance, since unrealistic training data can lead to large losses (resulting from wrongly hedged positions) when the algorithms are subsequently applied to real life data. Though the surge of machine learning came hand in hand with the explosion of available data, in more situations than not, the amount of available training data is insuﬃcient rather than large or the data available for training is not representative of the market, and numerical generation of additional data is necessary. Similarly, failing to train on unseen rare events may leave the application without an adequate response strategy in case the market moves to unexplored territories. This gave rise to an increase in interest for realistic numerical simulation of ﬁnancial markets. Deep neural networks provide a powerful tool to approximate complex distributions and this capacity, together with the increases in available computational power and speed has opened new horizons in all areas of modelling, including market simulation. The fundament of the prowess is laid by the universal approximation properties of neural networks [36, 37], which establish that any function or distribution with suﬃcient regularity can be approximated by a suﬃciently large neural network. Generative models capture probability distributions by approximating these via neural networks from learned samples, from which new synthetic data samples can be drawn. The generative model is trained such that it is representative the underlying distribution of the given dataset with respect to some loss function referred to as performance evaluation metric. Generative modelling originates from more traditional applications of machine learning and the adaptation of these techniques to ﬁnancial setting has its bespoke challenges. Identifying and addressing a these challenges is one of the contributions of this work. Deﬁnition 1.1 (Market Generator). We refer to generative models in a ﬁnancial time series context as Market Generators. That is, to neural networks that are designed to approximate the underlying distribution of an underlying market from a data sample given in form of a time series, so as to generate new data variations of the learned distribution. Possible applications that call for generative simulation of ﬁnancial markets include: Practical use and applications of Market Generators. There are several situations in which it is beneﬁcial to rely on simulated data samples that are statistically indistinguishable from a given original dataset. This section outlines a very brief summary of some potential applications of generative modelling based market simulation (market generators) that can go beyond the currently widespread standard applications of numerical simulations of market paths. For a more detailed study see [45] and [33] for an application to anomaly detection. (i) For data anonymisation: When the available data is conﬁdential, it is desirable to generate anonymised datasets that are representative of the true underlying distribution of the data but cannot be traced back to their origin. Financial data and medical data are often 4 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD proprietary, or conﬁdential. When testing investment strategies or the eﬀectiveness of a treatment it is imperative not to be able to trace back the datasets to the individual client or patient. Scenario (i) already showcases some of the essential challenges in this context: Evaluating whether the produced data is representative of the distribution that the observed data stems from, de- pends on the distributional properties (evaluation metrics) that we control for. Also the level of anonymity achieved by this procedure is a highly interesting question on its own. The study presented in [45] is devoted to understanding these questions in more detail.An even more chal- lenging situation arises if the size available data sample to train the generative model is very small to begin with: (ii) Small original training datasets: When there are natural restrictions on the number of available original samples (constraints on number of experiments, legal restrictions on the access to data), the available training data may not be suﬃcient to train the neural network application at hand, e.g. the hedging engine. Clearly, the more complex the application, the more data samples are needed to train it. In such cases also the training of generative models is challenging, due to the low number of available original samples. In this case, the generative neural network faces the same challenges, it has to be trainable on a very low number of data samples. Generative models for sparse data environments therefore need to be as parsimonious and easily trainable as possible. Once such a generative network is available, the more complex neural network applications can also be trained, using the intermediate step of the market generator that produces the necessary amount of training samples for the latter, which are statistically indistinguishable from the original data. Further practical applications of market generators include (but are not limited to) the following use cases: (iii) Backtesting: When developing a trading strategy, carrying out a backtest to measure how the strategy would perform in a realistic environment is of crucial importance. However, using historical data may result in overﬁtting of the trading strategy. Having a market simulator capable of generating realistic, independent samples of market paths would allow a more robust backtest less prone to overﬁtting. (iv) Risk management of portfolios – be it of ﬁnancial derivatives or trading strategies – is of utmost importance. A realistic market simulator can be used to generate synthetic paths to estimate various risk metrics, such as Value at Risk (VaR). In this paper we present a powerful generative modelling technique for time series generation in the situation that is speciﬁcally designed for the type of sparse data environments (ii) described above: Financial time series generation, when the number of available original samples is notoriously small. Some approaches to numerical data simulation in ﬁnance: Classical and new. • Classical modelling: Numerical simulation of ﬁnancial time series has a long history in related literature, far preceding the recent surge in ﬁnancial machine learning research: (i) Classical approaches include for example classical stochastic market models and autoregressive models and variations of these. Among their advantages are their tractability and the several decades worth of experience in understanding their math- ematical properties. Clearly the advantage thereof is a more straightforward suitabil- ity to currently prevalent risk-management frameworks. Disadvantages may include a relative inﬂexibility, which can result in modelling inconsistencies. (ii) Given the increase in computational powers today, more modelling ﬂexibility can be gained within the realm of “models with a classical ﬂavour” by adding complexity and further parameters to the models, taking their weighted averages with weights calibrated to the current market conditions, in the spirit of [20]. This line of thought opens new avenues that interpolate between model-based and model-free approaches [3], with their own set of challenges which we will not follow further in this paper. Such an experiment is presented in [35], similar ideas are further developed in [63]. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 5 With several decades worth of understanding the asymptotic and stochastic properties of models and with the steady increase of available computational powers, the evolution of (classical) models moved toward more and more complex and realistic models. Neverthe- less, in recent years we have witnessed several situations where classical models have been challenged by market reality. As one of the consequences, the failure of classical models to fully explain the behaviour of asset prices has led to situations where these models have failed to prescribe the right response strategies. A reality that quantitative investors are painfully aware of. While classical modelling techniques nevertheless undoubtedly continue to have their merits, ML-based technologies oﬀer a possible alternative to more closely mimic the behaviour of markets in more ﬂexible data-driven ways. • Data-driven modern generative modelling: Approaches to generative modelling are based on the common principle of generating new synthetic data samples whose distri- bution resembles the distribution of some reference dataset. One of the most striking diﬀerences of modern generative modelling to classical generation of synthetic data is that the explicit knowledge of the underlying data generating distribution is no longer required. Therefore, instead of implementing (an approximaion of) some known distribution or tran- sition density, generative models often approximate the underlying distribution implicitly, by drawing samples from the latter and comparing their similarity to the original dataset with respect to certain similarity metrics. This is in particular true for so called diﬀerential generator networks where a transformation map is learned through backpropagation from an initial source of randomness to a target distribution. The two most commonly used generative diﬀerential network-based approaches are Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN) and variations thereof. A leap away from strictly classical modelling but directly generalising these is the work [51] which in a special case simpliﬁes to the Heston Model and in another case to a GARCH(1,1) model. Kondratyev and Schwarz propose in [44] a restricted Boltzmann Ma- chine (RBM) for time-series generation, controlling for the autocorrelation function and quantiles of the generated time series. Boltzmann Machines are among the ﬁrst generative models introduced to learn arbitrary distributions over binary vectors. Recent contribu- tions to this stream of literature using GANs as generative models include the following: [8, 12, 32, 64, 65, 66]. To date we are not aware of approaches using VAEs for this purpose. GANs are unquestionably the most popular diﬀerential generator networks, though they are typi- cally data-hungry, and it is often diﬃcult to guarantee their convergence and stability. Variational Autoencoders maximize the likelihood of observing the given (original) data samples under the generated samples (2.3) and are particularly well-adapted 1 to the presented scarce data envi- ronment. Recent theoretical connections between autoencoders and latent variable models have indeed brought autoencoders to the forefront of generative modeling (see [26]). Therefore, in this paper we focus on generative modelling based on Variational Autoencoders (and their conditional reﬁnement) and highlight their prowess in the context of ﬁnancial time-series gen- eration. More details can be found in Section 3.2 and Appendix B. 2. Challenges of financial time-series simulation: Classical and new Currently, many of the available neural network-based generative models originate from static applications (such as image processing) and therefore, several available performance evaluation metrics for generative models have been developed to measure some form of marginal distribu- tions. The incorporation of of a time-series aspect of the data poses additional challenges, one of which is that these static performance evaluation metrics may not always be straightforward to 1The variational autoencoder approach is elegant, theoretically pleasing and simple to implement [26, Chapter 20]. 6 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD generalise to time series data (see more about this in Secion 2.1.1). Simulated ﬁnancial time series data is commonly addressed by capturing speciﬁc universal features of the time series, commonly referred to as stylized facts. Below we recall a number of stylised facts that traditional stochastic models are typically aimed to reﬂect. Though for classical stochsatic models, these stylized facts are often formulated in terms of the distribution of returns, this returns-based viewpoint (though still interesting) may not be the ideal choice to convey a suﬃciently full picture for distributions of synthetic market paths that come from market simulators using generative modelling. Some fur- ther issues arising from dealing with speciﬁc properties of sequential data in general are presented in [54] (recalled below for convenience). These can be addressed in a uniﬁed manner by using signatures [23, 47, 49], which is the method of choice for generative modelling that we advocate in this paper. In Section 2.3 below we collect some compelling reasons to do so and also argue why a signature-based approach is tailor-made for ﬁnancial applications such as pricing and hedging. We also indicate how a signatures-based objective function can be translated hedging performance of portfolios (Section 2.2.1). Problem formulation. Let us ﬁrst ﬁx some notations that will be used throughout the paper. In the following, let S(t) denote the price of a ﬁnancial asset (stock, exchange rate or index) and let X(t) = ln S(t) the corresponding log price. Then the log return (at scale t) is denoted as r(t, ∆t) := X(t + ∆t) − X(t),(2.1) where the time scale ∆t (which we specify in Section 3.1 for our methodology and experiments) has a scale ranging from a day to a month2 Autocorrelation for a time lag τ > 0 is denoted by corr ( r(t + τ, ∆t) , r(t, ∆t) ).(2.2) A numerical generation with an increased emphasis on the accuracy of the data generating process then aims at generating synthetically M ∈ N returns-sequences of length k for a suitable k ∈ N ( ̃r1 (t1, ∆t) . . . ̃r1 (tk, ∆t)), . . . , ( ̃rM (t1, ∆t) . . . ̃rM (tk, ∆t)),(2.3) such that the generated set of k-sequences (̃ri (t1, ∆t) . . . ̃ri (tk, ∆t)) of returns reﬂects the prop- erties of the observed k-sequences of returns (r1 (t1, ∆t) . . . r1 (tk, ∆t)), . . . , (rN (t1, ∆t) . . . rN (tk, ∆t)),(2.4) as accurately as possible, where N ∈ N denotes the number of observations in the original dataset. Small data environments: Note that typically, the number of generated samples M ̸= N need not be identical to the number of original samples N . A small data environment (see scenario (ii) in Section 1) would correspond to the situation where M >> N , with N relatively small from a statistical or from a standard deep learning perspective3. Such an example is the daily stock data or of leading indices (S&P500, DAX, FTSE) where the number of data samples (dataset covers 5000 days worth of daily data) available for training is of orders of magnitude smaller than the amount of data normally needed in most neural network applications. In such situations the challenge is to eﬃciently extract the most relevant information from a small amount of available samples in a very simple generative network. Clearly, the term above referring to the accuracy of the modelling, is yet to be speciﬁed. In Section 2.1 below, we recall a collection of properties that are widely accepted to be universal features (stylized facts) of time series of the form (2.1) and (2.4); and in Section 2.1.1 we brieﬂy recall corresponding metrics that are commonly used to test for the presence of these styled facts. 2In fact, ∆t can range from a few seconds to a month, generally including certain high frequency applications as well. From a mathematical perspective there is no reason for us to exclude shorter scales, but in this analysis we focus on scarce data situations, hence we restrict one day to be the smallest unit. 3 Similarly, application (i) in Section 1 could correspond to environments where M ≈ N are approximately the same. This kind of situation would also arise when simulating tick data for high frequency trading. Also Scenarios (iii) and (iv) of Section 1 can occur with M ≈ N or M >> N . A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 7 2.1. A reminder of speciﬁc stylised facts and evaluation metrics. Time series data of ﬁnancial markets exhibits a set of stylised facts of ﬁnancial markets, that a realistic ﬁnancial market model is commonly expected to reﬂect. Below, we included a brief reminder to the most common ones, for more details see [16]. - Non-stationarity: Financial time series are typically non-stationary, that is, past returns do not necessarily behave like future returns. The stationarity assumption states that for any set of time instants t1, . . . , tk and any time lag τ > 0 the joint distribution of returns is the same as the joint distribution of the lagged returns: (r(t1, ∆t) . . . r(tk, ∆t)) ∼ (r(t1 + τ, ∆t) . . . , r(tk + τ, ∆t)).(2.5) This property is not guaranteed to hold for the returns process in calendar time, see [16]. - Heavy tails and aggregational Gaussianity: Asset returns have (power-law-like) heavier tails than normal distribution, and have a distribution that is more peaked than the normal distribution. However, as the time scale ∆t increases, the distribution looks more and more Gaussian. - Absence of autocorrelations of asset returns, but slow decay of autocorrelation in absolute returns: Asset returns are uncorrelated (except for very short intraday timescales) but not independent. The autocorrelation (2.2) function of absolute returns |r(t, ∆t)| decays slowly, as a function of the time lag τ following a power-law. - Volatility clustering and multifractal structure: Phases of high/low activity tend to be followed by phases of high/low activity, see also [24, 50]. - Leverage eﬀect: Asset returns exhibits a leverage eﬀect i.e. a negative correlation between the volatility of asset returns and the returns process. As mentioned above, the sequential nature of the data is usually modelled by an ad-hoc selection of stylised facts and measured by corresponding evaluation metrics (see Section 2.1.1) in the case of classical models. Due to the the lack of an established consensus for similarity metrics for sample paths, to date this is also the case for generative models: The most straightforward optimisation routine is to target a number of essential stylized facts of the data in the training and in the evaluation of the generative procedure. Handcrafted combinations of stylised facts and corresponding performance evaluation metrics can be included in the optimisation routine used in a speciﬁc structure, alongside with further objectives with regards to a speciﬁc application. Therefore, in such cases the chosen combination of optimisation objectives is often very speciﬁc to the application at hand and does not transfer easily to other applications. This is referred to in [54] as the non-universality of the approach for ﬁnancial time series. 2.1.1. Performance evaluation metrics. To date, the most commonly used evaluation scores in- clude (but are not limited to) the following: (1) Distributional metrics: Such metrics target the cumulative distribution functions, in order to ensure that the distributions of the generated samples closely match the historical ones, often visualised by QQ-plots. To assess the goodness of ﬁt, the diﬀerence between the historical sample distribution versus the distribution of the generated samples is measured with respect to a suitable metric D Db∈B ∣ ∣F1,n(b) − F2,m(b) ∣ ∣,(2.6) where n, m denote the number of original and generated samples respectively, and B is (a suitable discretisation of) the sample space. (2) Tail behaviour scores: Targeting properties of the underlying distribution that control the tail behaviour, higher order moments such as skewness and kurtosis. 1 Nx Nx∑ j=1 ∣ ∣skew(xj) − skew([ˆx(1) j , . . . , ˆx(m) j ]) ∣ ∣; 1 Nx Nx∑ j=1 ∣ ∣kurt([xj) − kurt(ˆx(1) j , . . . , x(m) j ]) ∣ ∣(2.7) (3) Correlation-, and cross-dependence scores: To detect serial autocorrelation in the time-series, and analogously for multidimensional time-series, cross-correlation scores. 8 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD The above examples already suggest a wealth of possible metrics and evaluation scores to measure the quality of generated market paths and the list does not end here, depending on the application one has in mind: While the above evaluation scores check eﬀectively for some of the most relevant stylized facts, we might be interested in other properties of the generated data as well: We may want to optimize for example with respect to the expected payoﬀs of vanilla options, or of a portfolio of options. Or we may want to optimize the generative process with respect to the P&L of the hedged portfolio under an appropriate risk measure. Having deep hedging in mind as an application for the generative model, we may aim to include hedging objectives in the optimisation either directly (which may be computationally expensive) or indirectly. For the latter we refer to [1] for some possibilities and we refer to Section 2.2.1 for a brief explanation on how our suggested performance evaluation metric (see (Step 5 ) of Section 3) links back to hedging strategies. 2.2. Challenges for similarity metrics for ﬁnancial time-series by generative models. Traditional distributional metrics and divergences provide ample means to measure distances between distributions. But determining appropriate similarity metrics on the level of a stochastic process—or its data representation through a time series—is a challenge of a diﬀerent nature. This section is devoted to the questions: What are potential challenges in determining appropriate metrics to measure the similarity of distributions on path space, or to determine whether two sets of sample paths originate from the same underlying distribution? Generalisations of the static case (similarity of marginal distributions) to the dynamic one (similarity metrics on path space) are not straightforward. Similarly, evaluating the “goodness” of a generative model for sample paths of ﬁnancial time-series is a challenge in particular. With view to our goal of proposing eﬀective performance evaluation metrics for market generators we list a number of these challenges that are addressed in this paper: (1) The potential non-universality (as described in Section 2.1.1 above) of features to control for in the generated time-series. If the chosen set of optimisation objectives is bespoke to one application, the generated time series may not carry over easily to other applications. (2) The underlying distribution of the data generating model is often not known explicitly in generative models (See Section 1 above) and only controlled implicitly through the generated data samples and their similarity to the original data. Therefore, two-sample tests may be better suited as performance evaluation metrics than distributional metrics and divergences (KL-divergence, Fisher Information metric, Wasserstein metric). The latter may only be applicable after inferring from the available data sample to the underlying distribution. (3) Established distributional metrics (and two-sample tests) for marginal distributions can be generalised to (ﬁnite) multivariate marginals. However, generalising these metrics to path space is not straightforward, one of the diﬃculties being that the pathspace C 0([0, 1], Rd) is inﬁnite dimensional and non-locally compact, see [14]. (4) Non-continuous observation of the original data. Usually only discrete time observations of sample paths are available. While for classical models this is less problematic, for generative models an appropriate feature set needs to be speciﬁed. Modelling sample paths as a learning problem of vector-valued data is problematic for several reasons: the number n and position of observation time points might change from sample to sample. Also, in some applications (for example high- frequency data) n can get very large. Finally, from a hedging perspective it is delicate (and often not suﬃcient) to match marginal distributions on a ﬁnite number of observation dates only. As shown in [9] one can construct examples that are indistinguishable from one another on a ﬁnite set of marginals in the physical measure, but lead to arbitrarily diﬀerent hedging strategies and option prices. (5) Non-stationarity of the target distribution. Financial time-series are typically non-stationary and underlying distributions change with market conditions. For generative models this has two implications in particular: It is beneﬁcial to design a conditional version of the market generator, which allows to produce samples that are conditioned on speciﬁc market states, see Section 3 for more details. Furthermore, training a conditional generative model may amplify the data scarcity A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 9 issue as the availability of suﬃcient number of representative training samples becomes coupled with market conditions. 2.2.1. Two-sample tests and Maximum Mean Discrepancy metrics as performance evaluation met- rics. If the distributions of the original data generating process (ri (t1, ∆t) . . . ri (tk, ∆t)) and the generated series (̃ri (t1, ∆t) . . . ̃ri (tk, ∆t)) are known explicitly, a number of established similar- ity metrics can be computed eﬃciently. In practice however, this underlying distribution of the original data is (often) only available through the given samples and not known explicitly. For generative models, the same holds for synthetic samples as well as summarised in (2) above. If at least one of these data samples is small, metrics on the level samples rather than on the level of distributions are preferable. Two-sample tests provide a ﬂexible framework to compare the empirical distributions of two given data samples according to the following principle: If X and Y are random variables with respec- tive probability measures p and q deﬁned on some common state space; given i.i.d. observations {x1 . . . , xm} and {y1, . . . , yn} from p and q respectively, can we decide whether p ̸= q? Typically these tests are designed to determine whether two given data samples were generated by a common underlying distribution or not, but do not specify what that common distribution is4. A number of related tests addressing matter (2), have recently become more popular in generative modelling, see [25]: In [25], the authors determine if two samples are drawn from diﬀerent distri- butions p and q based on the largest diﬀerence in expectations over predeﬁned set of functions in the unit ball of a Reproducing Kernel Hilbert Space (RKHS) H. MMD[H, p, q] := sup f ∈H (EX∼p [f (X)] − EY ∼q [f (Y )]) . Where the space H is rich enough to distinguish between metrics p and q. Therefore the metric is called the maximum mean discrepancy (MMD). Though this does not yet address the matter (1) of non-universality, using these metrics in the training phase have rendered more stable convergence results in several studies and they can be computed eﬃciently. Therefore, MMD quickly entered the neural network arena5 in generative modelling. This is the performance evaluation metric we use for our generative algorithm, see (3.1) in Section 3.2.5. Generalising Maximum Mean Discrepancy metrics to path space faces the same challenges as the ones summarised in (3). In a recent paper [14], Chevyrev and Oberhauser develop a Maximum Mean Discrepancy based two-sample that relies on a feature map from stochastic analysis called the “signature” of a path, which tackles point (3) simultaneously with the issue (1) of non- universality. In fact, this test is based on a notion that is reminiscent of the role of moment generating functions on path space and hence characterises the distribution of the stochastic process uniquely, see Appendix A for details. Furthermore, it follows from [2, 46], that in addition to the advantages above, signatures also provide the right framework to match hedging objectives and hence to bypass matter (4). In fact, the insight that the similarity on the level of signatures (hence passing the signature based MMD test described in Section 3.2.5) can be linked back to similarity in hedging performance is a consequence of the continuity propertiy of the Itˆo-Lyons map [46]. This states that the mapping from the driving path controlled diﬀerential equation to its solution is continuous (in fact Lipschitz) in a suitable rough path norm. As a consequence, if the signatures of two driving paths are close, then so are the solutions of the corresponding controlled diﬀerential equations: Since the performance of a hedging strategy can be written in form of a controlled diﬀerential equation (the performance is an Itˆo integral of the hedging strategy against the price process) then if two price paths have similar signatures (i.e. pass the signature based MMD test described in Section 3.2.5), it follows that they will have similar performance under the same hedging strategy. 4 One of the best known such nonparametric two-sample tests is the popular two-sample Kolmogorov-Smirnov test, but it has some shortcomings: It may need a large number of samples and it is diﬃcult to generalise to higher dimensions. 5In [58] MMD has been established in the context of Wasserstein Auto-Encoders and [59] introduce MMD-based GANs. By making fundamental connections to optimal transport distances in the data space, MMD measureses- tablish the theory proving the correctness of this generative procedure. 10 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD 2.3. On signatures, their advantages in encoding path valued data, and their meaning. The challenges that generative modelling of ﬁnancial data streams faces—raised in the beginning of Section 2—are not limited to the choice of appropriate performance evaluation metrics only. They also extend to training: An eﬃcient parsimonious encoding (feature map) of ﬁnancial data streams also enhances the training of such generative models. Furthermore, an encoding that anticipates typical properties of the data and irregularities of sampling, makes training more robust with respect to data quality. The framework of rough paths and signatures [23, 29, 47, 49, 54] lends itself well to these aims as well: They provide a means to encode ﬁnancial data streams parsimoniously and eﬃciently and they a powerful framework to address further challenges of path valued data as well. Therefore, we do not only use the signature framework for performance evaluation but also resort to these as a feature map in the generative model itself. Deﬁnition 2.1 (Signature of a path). Let X : [0, T ] → Rd be a continuous path of bounded variation. Then the signature of X is deﬁned by the sequence of iterated integrals given by X<∞ T := (1, X1 t , . . . , Xn T , . . .)(2.8) where Xn T := ∫ 0<u1<...<uk<T dXu1 ⊗ . . . ⊗ dXuk ∈ (Rd)⊗n(2.9) where ⊗ denotes the tensor product. Similarly, given N ∈ N, the truncated signature of order N is deﬁned by X≤N T := (1, X1 T , . . . , XN T ).(2.10) Remark 2.2. If the path X has bounded variation – which is the case of discrete data – the integrals above can be deﬁned using Riemann-Stieltjes integrals. As mentioned in the beginning of Section 2, a particular challenge in the context of synthetic gen- eration of market paths, is that the distribution in question is deﬁned on the inﬁnite-dimensional space of paths C 0([0, 1], Rd), while the available generative modelling tools are ﬁnite-dimensional. Thus the inﬁnite-dimensionality of path space is not only a challenge for the choice of suitable similarity metrics or performance evaluation metrics but also for feature extraction of the data and training. A solution to this issue is to project this inﬁnite-dimensional space to a suitable ﬁnite-dimensional space where standard methods for generative models may be used. However, mapping this inherently inﬁnite dimensional space in an optimal way to a ﬁnite dimen- sional one presents a challenge and the choice of projection is not trivial: The most straightforward choice would be to sample the path on a ﬁxed, discrete time grid re- turn by return as in (2.1) for example (while accounting for the joint distribution of these), and learn the projected probability measure using standard generative models. This approach would not fully capture the sequential nature of ﬁnancial data and would fail to eﬀectively capture the probability measure on the original path space. Moreover, if we project this inﬁnite-dimensional object down to a ﬁnite-dimensional space by sampling on a discrete time grid, the projection is not a “natural one” as the original distribution on an intrinsically inﬁnite-dimensional space captures much richer information about the process. The latter can have signiﬁcant consequences on hedging as outlined in [9] and in the end of Section 2.2.1 above. A more eﬀective projection is to use the signature or log-signature 6 to project an inﬁnite dimen- sional encoding (2.8) of the path space to a ﬁnite N -dimensional one as in (2.10). The signature of a path is a transformation of the original continuous path into a sequence of statistics, an inﬁnite dimensional vector (2.8) of signature entries (2.9). These statistics fully characterise the original path up to time parametrisation (see [30, 6] and Appendix A for details) and furthermore they oﬀer a faithful and parsimonious description of it already in its ﬁrst few entries (dimensions) of the signature vector (2.10). The error made by the truncation at level N decays with factorial speed as O(1/N !), see [49]. 6See Deﬁnition A.3 in Appendix A for the latter. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 11 Moreover, the ﬁrst several signature entries—i.e. the ﬁrst terms in the vectors (2.9) resp. (2.10)— have clear ﬁnancial interpretations: The ﬁrst term captures drift – i.e. the increment of a price path over a period of time. The second term indicates the volatility over the period of time (through the L`evy area). Higher order terms capture ﬁner aspects of the path that end up fully characterising the latter. An ordering, reminiscent of principal components from the most relevant towards ﬁner properties of the path. See the Appendix A for more details. 2.3.1. On returns-based versus signature-based data generation in a pricing and hedging context. It clear from the previous section that the signature transform is a highly eﬃcient way of encoding the most relevant information contained in a stochastic path. We demonstrate in our numerical results below that learning the (truncated) signature of a set of paths leads to a more eﬃcient learning (i.e. training converges with fewer training samples already) than learning the multidimensional distribution of the process on a discrete grid. The projection on the ﬁnite dimensional space of truncated signatures is not only numerically more eﬃcient than the ﬁnite dimensional projection on a discrete time-grid. The signature-based projection encodes a richer and more relevant wealth of information about the path (which also allows us to control for option prices and hedging strategies [2, 55]), while in the latter projection some essential information may be lost, which can have ﬁnancial consequences on option prices and hedging strategies: In fact [9] shows that when sampling returns distributions of a stochastic process on a discrete time grid, even statistically indistinguishable sets of paths in the historical measure can lead to arbitrarily diﬀerent option prices. If however the paths are sampled on the level of signatures, this ambiguity of option prices does not occur. The ﬁndings of [2, 9] therefore indicate that not only is the signature a more eﬃcient way of encoding sample paths but one that that removes the ambiguity of the corresponding option prices and provides a meaningful control over hedging performance. See the end of Section 2.2.1 above for the last statement. 2.3.2. Further advantages of signatures. Further advantages of working with signatures for mod- elling functions of data streams have been discussed and presented in [47, 54]. These advantages include (but are not limited to) the following properties: The expected signature of a stochastic process determines the law of the process uniquely. With that, the expected signature plays a similar role on path space as the moment generating function for distributions (this provides a basis of the performance evaluation metric developed in [14] for path space). They permit a model-free data-driven modelling: The framework does not impose any assumptions on the underlying stochstic dynamics. Signatures provide a ﬂexible basis of functions for a functional on path space. But while Fourier transforms and wavelets have a similar role approximating curves as a linear combination of basis functions, signatures do so in a model free, unparametrised way (since a path by path characterisation is possible). The signature transform is traightforward to implement: Today there are readily available (and constantly improving) powerful python packages and libraries 7 to transform data streams to signatures and algorithms to transform signatures back to paths of datastreams. For the inverse transform, one possible algorithm is developed in this paper (see Section 3.1) and provided in the Github repository Github:Marketsimulator. The framwork is invariant under translation8 and time-parametrisation . Therefore, in order to encode price paths in business time rather than calendar time we apply the lead-lag transformation 9 (see Section A.2 for more details). 7Such as esig, tosig and iisignature libraries, see https://github.com/bottler/iisignature as well as [41] for more background. 8Signatures are constructed from increments of the path. As such, they are also invariant under translation: all reference to absolute values of the path is lost. One may ﬁnd settings where it is important to make reference to absolute values of the path; e.g. to ensure that the asset price remain positive. In such cases, the so-called visibility transform can be used. 9 One of the commonly used method for measuring similarity between two temporal sequences, which may vary in speed is Dynamic time warping (DTW). A well known application has been automatic speech recognition, to cope with diﬀerent speaking speeds. In ﬁnance a similar consideration suggests measuring business time rather than calendar time of the process. 12 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD Furthermore, signatures are robust to irregular sampling (which becomes relevant for tick- data), missing data and towards highly oscillatory data as well: In particular, they provide a consistent framework for unbounded variation paths, which may arise by Donsker-type theorems in the high-frequency limit. Functions of such paths have to be treated with care, as for example the quadratic variation, or the solution of nonlinear ﬁltering or of stochastic diﬀerential equations, do not depend continuously on the underlying path. Signatures appear naturally when describing the behaviour of functions of non-smooth paths, cf. [14]. 2.3.3. From signatures to log-signatures. In the present work, our generative model we are not tar- geting the signature directly, instead we ﬁrst use a bijection to log-signatures (See Deﬁnition A.3 in Appendix A for the latter). Generative modelling on signature space directly may be problematic, which results from the fact the signature space is not linear: Therefore, small perturbations of the signature (as one might expect to obtain from the output of a generative model on signature space) of a path will in general not correspond to the signature of some other path. In fact there may not exist any path with a signature that results from the perturbation. We solve this issue by working with the so-called log-signature instead, which also characterises the price path, but now spans a linear space. We refer the reader to [13, Section 1.3.5] for a detailed discussion of the log-signature, as a full reminder of its formal characterisation and properties is out of the scope of this paper. For more information on the background and implementation of signatures and log- signatures, available python packages and libraries for signatures, log-signatures and the lead-lag transformation see the related works [13, 41, 53, 56] such as a brief reminder in the Appendix A below. Recent machine learning applications using signature inputs include [8, 12, 27, 47, 53, 54]. 3. Main results: Our methodology and its background and motivation Problem setting and main results and contributions: Given a ﬁnancial data stream 10 (be it a market index S&P, DAX or a numerically generated series) we work with the one available realisation of the evolution of this stream. From the observed path observed on a time horizon of several years we intend to infer the underlying data generating process in order to produce further samples from that distribution11. We train a variational autoencoder (VAE) to reproduce the underlying distribution of an observed spot index on diﬀerent time horizons up to a month. Note that our methodology is by no means limited to one-month time horizons and in the postprocessing step (Step 4) of our methodology we propose a way to generate data on longer time horizons (here up to several years) by appropriately concatenating paths. (a) As a ﬁrst step we demonstrate that a returns generation in the classical sense is possible by variational autoencoders on various time-scales (daily, weekly, monthly returns). This experiment follows the spirit of other currently available market generation approaches, and supports our choice of VAE as a parsimonious generative model. We then also go beyond simple returns generation in the following ways: (b) To accommodate to inherent inﬁnite-dimensional nature of the problem, we propose a generative process directly on path space, based on the signatures (see Section 2.3 above) of the paths. We then demonstrate that the generated paths do not only ﬁt all the observed marginal returns distributions (obtained in the approach above) but also the joint distributions of returns on these time-scales and other essential features of the data. (c) We ﬁne-tune the generative process by allowing conditioning on various market indicators to account for the non-stationarity of the time series. With this conditioning we reﬁne our (returnes-based or signature-based) variational autoencoder (VAE) to a conditional variational autoencoder (CVAE), and generate paths conditional on various market states. The latter conditioning also allows us to paste path segments conditioned on the signature of the previous path segment, and thereby generating paths of arbitrary length. 10We assume the data stream to be univariate for notational simplicity but it is straightforward to extend the methodology can be extended to multivariate data streams too. 11For this we temporarily make the customary stationarity assumptions which we will later relax. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 13 3.1. Methodology: An overview of the main steps. Our algorithm and numerical experi- ments can be subdivided into the following ﬁve main steps. We ﬁrst give a brief overview of these and provide in Section 3.2 more explanations on each of the steps. (Step 1) Data extraction from time series: In our experiments given a sample path from a data stream, we subdivide the the full time series in equal length intervals 12. Here we subdivide into segments of: (i) 1 day, (ii) 5 days, corresponding to a business week, and (iii) 20 days corresponding to a month. (Step 2) Preprocessing the data: To obtain training data from the resulting path segments, we (a) calculate log-returns r(t, ∆t) = X(t + ∆t) − X(t) with appropriate ∆t, to generate (i/a) daily log-returns, that is ∆t = 1 day, (ii/a) weekly log returns, that is ∆t = 5 days and (iii/a) monthly log returns, that is ∆t = 20 days, from the data. (b) convert the obtained data samples into log signatures (for the paths of length (b/ii) 5 days and (b/iii) 20 days) applying the lead-lag transformation. This will enable the pathwise generation process described in point (b) above. A mathematical back- ground and motivation for these transformations is given in Appendix A. (Step 3) Creating and training the VAE network: After splitting the historical data into training/testing/and validation sets (a) we train a variational autoencoder VAE(a) on the daily, weekly and monthly returns for (i/a), (ii/a), (iii/a). The output of this VAE is on the level of returns. (b) We train a variational autoencoder VAE(b) on the log-signatures of weekly and monthly sample paths for (ii/b), (iii/b) (see Sections A and A.1). The output of this generative VAE(b) is then given in form of log-signatures. (c) In the reﬁned version we also calculate and store relevant market conditions such as current level13 of volatility, current level of the index, signature of the previous path segment. These values will then be used in the CVAE (Conditional Variational Autoencoder) to generate new data points conditional on these indicators. (Step 4) Postprocessing of the outputs of the VAEs: At this stage of the generative pro- cess we either convert back the generated log signatures into paths or use the generated (log-) signatures directly. In fact, in case the purpose of the generative process to provide training data for neural networks with pricing and hedging objectives, both options are available: Either: (4.b) Signatures can be used for option pricing directly as suggested in [55], Or: (4.a) In order to invert signatures into paths, we suggest in Appendix A one possible method do so 14. In fact, inverting signatures into paths is also instrumental to allow comparing the performance of the generative networks on the level of returns distri- butions, as described in the second table of the following point. (Step 5) Performance evaluation: We evaluate the performance of each of the above approaches to the generative process and compare the outputs (both to the original data samples and to one-another) with respect to diﬀerent similarity metrics and conclude that the signature- based generation outperforms the returns-based approach. In fact in case the purpose of time-series generation lies in the context of pricing and hedging derivatives, the suitable similarity metrics are indeed the signature-based similarity metrics (see Sections 3.2.5 and 2.2.1 for details). This follows directly from ﬁndings of [9]. 12For the numerically generated data in our experiments, we directly generate sample paths of the above length (i), (ii) and (iii), but we could have applied the same method as above. 13That is, the instantaneous level at the start each path segment. 14Note that developing the computationally most eﬃcient ways of this task are mathematically highly nontrivial, and subject of ongoing research [18]. 14 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD 3.2. Background and explanations to our generative modelling methodology. 3.2.1. (Step 1) Data extraction from time series stream. It directly follows from the non- stationarity of ﬁnancial time series that ‘past returns do not reﬂect future performance’ [16]. Therefore, strictly speaking, the data of the evolution of an index over time amounts to one single observation of a realised path. However as a starting point to any per- form statistical analysis of market data one needs several observations of the underlying quantity to which one of the most basic requirements is ‘the existence of some statistical properties of the data under study which remain stable over time’ [16]. To extract multiple observations from this data stream, one divides this path into segments. This (as usual) calls for some sort of stationarity assumption, though ﬁnancial time-series are known to be typically non-stationary. In our experiments given a sample path from a data stream, we subdivide the the full time series in equal length intervals: (i) 1 day, (ii) 5 days, and (iii) 20 days. In dividing the observed path into segments, there are diﬀerent considerations to balance with one-another: • The longer the path segments, the fewer path segments are available 15. • The longer the path segments the less severe the violation non-stationarity assumption in the obtained training data. 3.2.2. (Step 2) Returns-based (a) versus signature-based (b) data generation: If one wants to create a generative model that learns to generate samples from a distribution on path space, one has to decide what representation of the path will be used: a returns-based (a) or signature-based (b) projection from path space to a ﬁnite dimensional space. This representation has to be an eﬀective representation of the path, and it should be rich enough to capture the distribution of the paths. See Section 2.3 for a discussion of com- pelling reasons to use the truncated signature projection for this purpose. Given that the signature of a path uniquely determines the path [7] and that the expected signature uniquely determines the distribution of the paths [15], it is natural to use the signature to represent the path. When the truncated signature is used, the representation of the path is essentially a vector on some high-dimensional space. This transformation of paths to signatures can then be applied to our sample of paths, in order to then use traditional generative models such as Variational Autoencoders. However, the signature of a path is a group-like element [49, Deﬁnition 2.18] whereas the generated signatures wont. There- fore, it is more convenient to apply the generative model to log-signatures, because once we compute the tensor-algebra exponential of the generated log-signatures the resulting element will be group-like [49, Theorem 2.23]. In this case, the output of the generative model will be in form of log signatures as well and may have to be inverted to paths in a post-processing step (see Step 4 ). In fact, the generation of paths in (log-)signature space is more eﬃcient than maximixing a corresponding ﬁt in returns for a simulated stochastic process in the sense that the generative process achieves a higher precision already with less training data. This can be seen in comparative performance metrics (Step 5) in the numerical experiments: There, a comparison with purely returns optimised VAE shows the proof of concept that signatures work better. See Section 4.1.1 for the corresponding numerical results. More precisely, the comparison of the weekly 5-dimensional joint dis- tribution with the weekly signature paths we show that the signature-based generation outperform the weekly joint distribution-of-returns based gneration. The reason for this is that signatures are an eﬃcient feature map to encode the most relevant information captured in time series data. 15 A 20 year daily observation timeline results in ∼ 250 sample paths with monthly (20 days) path segments, or ∼ 1000 weekly (5 days) path segments or in ∼ 5000 daily returns. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 15 In a pricing and hedging context there is an even more compelling reason to use sig- natures as a feature map for the time series: Not only does a signature-based generation yield a faster training and more stable convergence, but in a pricing and hedging context it eliminates pricing ambiguities while pure returns-based generation does not as Brigo ex- plains in [9] in a framework consistent with statistical analysis of historical volatility that can lead to arbitrarily diﬀerent options prices. Previously, [2, 9] had shown that implied volatility is linked with a purely pathwise lift of the stock dynamics, conﬁrming the idea that while historical volatility is a statistical quantity, implied volatility is a pathwise one. See also the end of Section 2.2.1 for a hedging perspective. 3.2.3. (Step 3) Network Architecture and Training of the VAE and CVAE. The Achitec- ture of the VAE and CVAE networks such as the market indicators that serve as condi- tioning variables for the CVAE are summarized in this section below. For full details of the model and training we refer the reader to the Github repository Github:Marketsimulator. The encoder: The encoder network has one hidden layer and two latent layers, with 50 nodes on the hidden layer. The activation function is a leaky (parametric) ReLU with parameter α = 0.3. The decoder: The decoder network has ne hidden layer with 50 units and activation function leaky (parametric) ReLU with parameter α = 0.3. The output layer with sig- moid activation function. Conditional Variational Autoencoder, adapting to speciﬁc market conditions. In order to further accommodate to the non-stationarity of the data, we now reﬁne the VAE to certain speciﬁc market conditions: a Conditional Variational Autoencoder (CVAE). The market conditions we consider here are the following: (a) Level of the instantaeons volatility at the start of the path. (b) Level of the index at the beggining of the path. (c) Log-signature of the previous path: This last condition is in fact more reﬁned than the ﬁrst condition, in fact it is more restrictive. If we control for the log-signature of the previous path, we automatically control for the volatility. See Section 2.3 for more details on the ﬁnancial interpretation of the elements of the signature vectors. Motivation for VAE as our generative model. Here, we brieﬂy motivate our choice for VAEs over the other generative modelling approaches: VAEs aim at maximizing the lower bound of the log-likelihood of the observed data, see Appendix B for details. With that they are parsimonious, theoretically clear to explain and also easy to implement and to interpret. In summary, VAEs are stable and ﬂexible generators for scarce data environ- ments. Furthermore, VAE frameworks work consistently well under diﬀerent diﬀerential operators and architectures including recurrent networks, [26, Chapter 20]. This ﬂexibility may prove useful in later applications. Arguably, the most popular diﬀerential generator networks today are GANs. In fact, the performance evaluation metric presented in (Step 5 ) can be seen as a non-automated discriminator applied as a one-step veriﬁcation that the generated samples are indistin- guishable from the original ones. Our choice for the VAE approach in the current context is based on the following consid- erations. • The relative unpopularity of VAEs stems from image processing and its relative weak- ness in that context are irrelevant for time-series applications. In fact, the relative unpopularity of VAEs compared to GANs in image processing originated from the fact that generated samples from VAEs trained on images tend to be somewhat blurry. This could be attributed to the fact that the model (maximising the likelihood of the observed dataset) may attribute a high probability to (nearby) points other than the ones in the training set. This however would be no drawback of the VAE in a time-series setting. 16 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD • VAEs require considerably less data for training than GANs. A GAN aims at achiev- ing an equilibrium between a Generator and Discriminator. Achieving the equilib- rium between generator and discriminator networks of GANs is in most practical scenarios a highly delicate matter and often sensitive to hyper-parameter tuning (cf [26, p. 692]). Recently much research activity was devoted to addressing this issue, and reformulations of GANs have been proposed that are guaranteed to converge, if provided with enough training samples. Since in this application we are speciﬁcally targeting scenarios where training data is scarce, we opt for generative variant of Au- toencoders (the nonlinear generalisation of PCA, see [26]) for generative modelling in our current context. 3.2.4. (Step 4) Postprocessing of the VAE outputs. When the Market Generator is a VAE(b)-type of generator, that is the outputs of the generative process are given in form of log-signatures or conditional log-signatures (conditioned on the log-signature of the previous path segment), then: (a) We can leave the output of the (C)VAE in the form (log-)signatures and apply it directly to pricing: Signatures can be used directly for pricing vanilla products or exotic derivatives as proposed in [55] and this approach has several advan- tages in particular for computation heavy, path-dependent applications. Therefore, if we have these algorithms in place, for many applications (including XVA oriented computations) it is advantageous to leave the output of our generative model on the log-signature level and apply directly pricing algorithms for signatures to that. (b) Alternatively, we can convert back the the output of the VAE(b) from signatures to sample paths: In this paper we develop an evolutionary algorithm for this purpose. Other alternatives are currently a topic of active research [18]. The idea behind inversion of signatures to paths is based on the following idea: The signature of a path uniquely determines the path itself [30, 6] – in other words, if we know the signature of a path, we know the path itself. However, the task of retrieving the path from its signature (or log-signature)in a computationally eﬃcient way is a highly non-trivial task [18]. In this paper, given that stock prices discrete (multiples of the pip size), we used an evolutionary algorithm to retrieve a path whose log-signature is close to the generated one. Evolutionary algorithms aim to solve certain optimisation problems by mimicking to some extent biological evolution. In the context of signature inversion, we start with an initial population of random paths, and we iteratively (i) select the paths whose signatures are closest to the target signature, and (ii) breed these paths and introduce mutations to generate a new generation of paths. After suﬃciently many iterations, we end up with a population of paths whose signature are close to the target signature we aim to invert. (c) Concatenation of paths for longer time-horizons: Recall that in VAE(b/ii) the outputs of the generative process are log-signatures of weekly paths and in VAE(b/iii) the outputs are log-signatures of monthly paths. Recall also, that the longer the paths, the less samples we have available. However, in some scenarios we might be interested in obtaining longer sample paths than the outputs of the generative model: Say, gen- erating signatures of monthly (or even longer) paths from the outputs of VAE(b/ii) the weekly signature-based generative model. This can be done, using the multiplica- tive algebra structure of the signature space. We demonstrate this by assembling monthly signature outputs VAE(b/iii) to yearly paths in our numerical experiments, see Figure 1 below. Converting from returns (on diﬀerent time scales) to paths: If the Market Gen- erator is a VAE(a)-type of generator, that is, the output of the VAE is in form of returns, one may be interested in producing sample paths from these (i) daily,(or the (ii) weekly or (iii) monthly returns) generated returns. We do so in our numerical experiments in order A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 17 Figure 1. Here, we demonstrate that the method can be applied to generate paths on longer horizons than one month (here up to one year) by concatenating generated paths with the CVAE where we consecutively condition the output of the monthly log-signature CVAE generation on the log signature of the previous segments (generated) path’s signature. See item (c) of (Step 4) in Section 3.2.4. to compare the performance of the signature-based generation VAE(b) with the perfor- mance of returns-based generation VAE(a), see Step 5 below, in particular the Section Performance evaluatio on the level of signatures. In order to construct path from the generated VAE(a/i) returns, we take a Monte-Carlo-type approach by assembling daily generated increments to a full sample path. (Step 5) Evaluating the generated paths. In this step we apply performance evaluation metrics to the output of the generator networks to evaluate certain relevant characteristics of the generated distribution whether they reﬂect the true distribution of the original data. These evaluation metrics are diﬀerent from the objective function of the generative process itself. These ﬁnal performance evaluation tests resemble the role of the Discriminator network in a GAN, with the diﬀerence that in our algorithm, performance evaluation is manual and only happens once. 3.2.5. Distances for time-series and sample paths: A computationally eﬃcient MMD met- ric for laws of stochastic processes. When it comes to assessing the quality of a set of generated paths, being able to compute the distance between the laws of two stochastic processes becomes imperative. A naive metric based on the marginals of the two processes is bound to fail, as two stochastic processes can have identical marginals but very diﬀerent laws. Instead, a metric that considers the entire law of the stochastic process is needed. Moreover, this metric has to be computable in order to make it practical. In [14], the authors propose a (computationally eﬃcient) MMD for laws of stochastic processes based on signature kernels. As an application, they use this metric to develop a two-sample test for stochastic processes. In the context of this paper, this statistical test can be used to evaluate the quality of our market generator. To asses whether a generative model is able to generate paths that are realistic with respect to a sample of real paths Y1, . . . , Yn, we sample from the generative model n ∈ N, paths X1, . . . , Xn and we apply the two- sample test proposed in [14]. More speciﬁcally, we compute the signature-based MMD test statistic T (X1, . . . , Xn; Y1, . . . , Yn) T (X1, . . . , Xn; Y1, . . . , Yn) := 1 n(n − 1) ∑ i,j;i̸=j k(Xi, Xj) − 2 n2 ∑ i,j k(Xi, Yj) + 1 n(n − 1) ∑ i,j;i̸=j k(Yi, Yj),(3.1) 18 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD where k(·, ·) is the so-called signature kernel (see [14, Proposition 4.2]). Then, given a ﬁxed conﬁdence level α ∈ (0, 1), we compute the threshold cα := 4 √ −n−1 log α. The generative model will be said to be realistic with a conﬁdence α if T 2 U < cα. Performance evaluation on the level of signatures: Gen. data↓ / Real data→ Weekly paths (b/ii) Monthly paths (b/iii) Returns (a/i,ii,iii) Ret (a/i) ⇒ paths ⇒(b/ii) Ret (a/i) ⇒ paths⇒(b/iii) Log-Signatures (b/ii) Direct (b/ii)×4 ⇒ Direct on (b/iii) Log-Signatures (b/iii) (b/ii)×4 ⇒ Direct on (b/iii) Direct Comments: The vertical column denotes the generated data and the horizontal row the original data. In particular Signatures (b/ii) dentotes that the output of the generative model was on the level of weekly signatures. Therefore, to compare (b/ii) weekly generated signatures with (b/iii) signatures monthly paths, one concatenates four weekly signatures (see: product of signatures in Appendix A) and compares on the monthly signature level. Clearly, if we generated data on the level of returns, we can compare this with the returns distribution of the original data direcly (see table below). But for a comparison on the level of signatures, as a ﬁrst step one builds random paths, by sampling from the gener- ated returns for each new increment, then calculates the corresponding signature of the thus generated paths. This process is encoded in the notation Ret (a/i) ⇒ paths ⇒(b/ii). Performance evaluation on the level of returns / marginal distributions: Gen. data↓ / Real data→ Daily data (a/i) Weekly paths (a/ii) Monthly paths (a/iii) Returns (a/i,ii,iii) Direct with (a/i) Direct with (a/ii) Direct with (a/iii) Inverted Sig. (b/ii) (b/ii)⇒paths⇒(a/i) (b/iii)⇒ paths⇒(a/ii) (b/ii)⇒paths×4 ⇒(a/iii) Inverted Sig. (b/iii) (b/iii)⇒paths ⇒ (a/i) (b/iii)⇒ paths ⇒ (a/ii) (b/iii) ⇒ paths ⇒ (a/iii) Comments: If both the generated data is in form of returns (a/i,ii,iii), then comparisons of the empirical marginal distributions are direct. If, however, the data generation was on the level of signatures (b/ii,iii) then one ﬁrst has to invert the signatures back to paths for a comparison. This is encoded in the entry (b/iii)⇒paths ⇒ (a/i). Note that process of inverting signatures can be slow (the longer the paths the slower) and eﬃcient algorithms for that are subject of onoing research, see for example [18]. 4. Numerical Results 4.1. Numerical experiments with historical data of S&P. To demonstrate the accuracy generated log-signatures and images of the produced paths, we provide in this section images of 2D projections and the resulting generated paths and corresponding returns for optical demon- stration. The rigorous numerical demonstration of the accuracy of the paths via the Maximum Mean Discrepancy inspired signature moments method can be found in Section 4.1.1 below. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 19 Figure 2. The (unconditional) VAE: The image shows projections of gener- ated weekly signatures (b/ii). Since the log-signatures we generate in this pro- cedure are high-dimensional objects, we display here their projections on various two-dimensonal subspaces, indicated on the vertical axis. Figure 3. The (unconditional) VAE: The image shows projections of gener- ated monthly signatures (b/iii) on various two-dimensonal subspaces. Figure 4. This is the image of generated paths inverted from log-signatures. 20 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD Figure 5. The conditional VAE: The image shows projections of weekly sig- natures generated by the CVAE, projected on various 2dim subspaces. Here we condition on an instantaneous volatility of 5% of the process measured a the start of the interval. Figure 6. The conditional VAE: The image shows projections of weekly sig- natures generated by the CVAE, projected on various 2dim subspaces. Here we condition on the current level of the process a the start of the interval. More speciﬁcally, we condition on the spot price being S(0) = 2000. 4.1.1. Performance evaluation scores. We conduct the signature-based MMD two-sample test (3.1) described in Section 3.2.5 for the scenarios described above. In the table below, we in- clude 1.) the conﬁdence level at which the the signature based MMD test changes from the result A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 21 “the two samples come from the same distribution” to the result ”the two samples come from dif- ferent distributions”. Clearly, if the the test can be passed on a higher conﬁdence level, it indicates a higher similarity of the generated samples to the original ones. As a comparison we also performed a classical two sample test on the level of marginals, summa- rized in the table below: We include 2.) the corresponding conﬁdence level for the statistic for the Kolmogorov-Smirnov test applied to the marginal distribution at ﬁnal time of the time horizon. We display the results for weekly and the monthly paths of the unconditional (uc) variational autoencoder, as well as one example for the conditional variational autoencoder, where we condi- tion the samples of the instantaneous volatility being close to the 5% level. MMD signature conﬁdence level K-S test p-value Weekly signature paths (uc) 99.998% 0.57 Monthly signature paths (uc) 99.987% 0.46 Weekly signature paths (conditional) 99.997%% 0.63 Of course we could carry on here to include further statistics that control for properties of stylised facts from Section 2.1 as it was done in previous works. However we recall here that the signature based MMD test of [14] described in Section 3.2.5 completely characterizes the law of a stochastic process and therefore we omit further test measures here for brevity. Furthermore, below we also include the comparison of generated unconditional weekly paths that were learned as a 5-dimensional joint distribution of returns, which we compare with the weekly paths generated by the log-signature based generator. In the table below, we display for these two sets of weekly generated samples (left side) the result of the signature-based MMD test at 99.95% conﬁdence level for the full path, as well as (right side) the result of the Kolmogorov-Smirnov test at 99.95% conﬁdence level applied to the daily marginal distributions from day 1 (denoted by K-S d1), to day 5 (denoted by K-S d5) of the week. At 99.95% conf. level MMD signature test K-S d1 K-S d2 K-S d3 K-S d4 K-S d5 VAE signatures Passes Passes Passes Passes Passes Passes VAE multidim. distr. Fails Passes Passes Fails Fails Passes We observe that the the latter greatly outperforms the former in terms of quality of generated paths with the same amount of training for both methods. 4.2. Numerical experiments with synthetic paths from rough volatility models. Finally we demonstrate our experiments also on a fractional stochastic volatility model, the rough Bergomi model introduced in 2015 by Bayer, Friz and Gatheral [4], which is a natural extension of the rough fractional stochastic volatility (RSFV) model in [24] to the setting of pricing. Rough stochastic volatility models have the advantage that they capture well some of the essential stylized facts of ﬁnancial time-series. dXt = − 1 2 Vtdt + √VtdWt X0 = log(S0) Vt = ξ0 E(2νCH Vt), V0, v, ξ0 > 0 Vt = ∫ t 0 (t − u)H−1/2dZu, H ∈ (0, 1/2) ⟨Z, W ⟩t = ρt, ρ ∈ (−, 1, 1), where X := log(S), and E(·) denotes the stochastic exponential and CH := 2HΓ(3/2−H) Γ(H+1/2)Γ(2−2H) . We include this experiment in order to demonstrate that our method already works optimally on the small number of paths available from S&P 500 data. We demonstrate this by numerically generating a simulated training dataset from the rough Bergomi model with model parameters inspired by the calibration results in [35]. In the ﬁrst experiment generate the same number of paths in the rough Bergomi model as the number of paths available from the S&P training data (small dataset) and another training dataset with a signiﬁcantly larger number of simulated paths (large dataset).We then train the Variational Autoencoder on the log-signatures of the simulated paths ﬁrst on the small dataset and then on the large dataset and assess the quality of the 22 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD Figure 7. Left had side: Projections of signatures generated by the VAE trained on 250 (small dataset) monthly rough Bergomi paths. Right had side: Projections of signatures generated by the VAE trained on 5000 (large dataset) monthly rough Bergomi paths. generated paths by observing the value of the test statistic of the MMD two-sample test on the same conﬁdence level for the two diﬀerent outputs. Remark 4.1. Since among classical stochastic volatility models rough volatility models are partic- ularly realistic ones, we also assessed the quality of paths generated by Rough Fractional Stochastic Volatility comparing them with the original S&P 500 monthly paths via the signature based MMD test statistic results, which can be found in the code accompanying this work, published on github. 5. Conclusions Our experiments show that the generative model works just as well but not signiﬁcantly better if we have more original data in the training phase to calibrate the network parameters. Providing more training data does not signiﬁcantly improve the learning process of the generative model. This demonstrates that our Variational Autoencoder based training is ideally suited for the scarce data environment at hand, operating eﬃciently with the little data that we have available for training (fewer than ∼ 250 samples). While returns-based optimisation works signiﬁcantly better if it is provided with more training data (tested with numerically generated training data), the signatures-based training delivers convincing results already for the small amount of data from the S&P paths and does not signiﬁcantly improve if more (numerical) training samples are provided. Appendix A. Signatures A.1. Signatures and their properties. In this section we recall properties of signatures that are used in this paper. The signature of a path is a transformation of path space. Moreover, certain properties of signatures make them good feature sets for machine learning: Theorem A.1 (Uniqueness:[6]). Under certain assumptions (see [6]) a path is uniquely determined by its signature. Thus the signature map is a faithful transformation, in the sense that distinct paths have distinct signatures. Therefore, signatures are feature maps that do not lose any information about the original path. Furthermore, for stochastic processes X : [0, T ] → Rd, the expected signature will play a similar role to the moments of a random variable on a ﬁnite-dimensional vector space: under certain assumptions, it characterises its law. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 23 Figure 8. Lead-lag transformation of a price path. The ﬁgure on the left shows the lead and lag components of the path, and the ﬁgure on the right shows the lag component plotted against the lead component. Theorem A.2 (Expected signature characterises the law of a process:[15]). Let X : [0, T ] → Rd be a stochastic process on a probability space (Ω, F, P) such that its signature X<∞ 0,T is a.s. well- deﬁned. Assume that its expected signature E[X<∞ T ] is well-deﬁned. Under certain assumptions (see [15]) E[X<∞ T ] uniquely determines the law of the stochastic process X. Hence, if one is interested in learning to generate samples from a certain stochastic process, one can instead learn to generate signatures of samples of the stochastic process. By the theorem above, this would be suﬃcient to completely characterise the law of the process, as the expected signature characterises the law. On the other hand, by the uniqueness of signature, knowing the signature of a path is essentially equivalent to knowing the path. This is precisely the approach we will follow in this paper: we will learn how to generate signatures of paths. However, generating signatures directly is not an easy task because of the intrinsic structure of signatures. In other words, if a generative model is built to generate signatures directly, the generated object may not be the signature of any path. To avoid this, we will learn how to generate log-signatures instead: Deﬁnition A.3 (Log-signature). Let X : [0, T ] → Rd be a path such that its signature X<∞ 0,T is well-deﬁned. The log-signature is then deﬁned by log X<∞ T := −X<∞ T + 1 2 (X<∞ T ) ⊗2 − 1 3 (X<∞ T ) ⊗3 + . . . + (−1)n 1 n (X<∞ T ) ⊗n + . . . , which can be shown to be well-deﬁned ([49]). Taking the logarithm of the signature is an invertible operation – one can exponentiate it to retrieve the signature. Therefore, no information is lost or gained when considering log-signatures. Log-signatures are deﬁned on a certain free-Lie algebra and in fact, any element of this free-Lie algebra is the log-signature of a certain path (see [49] for more details). Hence, in this paper we will learn how to generate log-signatures of market paths, so that we can guarantee that the outputs of the ML generative model are indeed log-signatures. A.2. Lead-lag transformation. In [22], the authors introduce a transformation of path space called the lead-lag or the Hoﬀ transformation. The authors showed that this transformation is able to capture the volatility of a path and, due to the importance of volatility in ﬁnance, we opted to use this transformation. Let D = {ti} n i=0 ⊂ [0, T ], and let {Xti}ti∈D ⊂ Rd be a d-dimensional sample. The lead-lag transformation of {Xti}ti∈D is deﬁned below. 24 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD Deﬁnition A.4 (Lead-lag transformation). The lead-lag transformation of {Xti}ti∈D is deﬁned by the 2d-dimensional continuous path X D = (X D,b, X D,f ) : [0, T ] → R2d given by X D t = (X D,b, X D,f ) :=    (Xtk , Xtk+1) , t ∈ [ 2k 2nT , 2k+1 2nT ) , (Xtk , Xtk+1 + 2(t − (2k + 1)) (Xtk+2 − Xtk+1)) , t ∈ [ 2k+1 2nT , 2k+ 3 2 2nT ) , (Xtk + 2(t − (2k + 3 2 )) (Xtk+1 − Xtk ) , Xtk+2) , t ∈ [ 2k+ 3 2 2nT , 2k+2 2nT ) . The component X D,b is the backward or lag component, and X D,f is the forward or lead component. The signature of the lead-lag transformation will be denoted by XD,<∞ 0,T . Figure 8 shows the lead-lag transformation of a certain path. As the name suggests, the lead component is leading the lag component. As it was shown in [22], the relationship between the lead and lag component is able to capture the volatility of the path. For instance, if the sample {Xti}ti∈D ⊂ Rd comes from a d-dimensional continuous semimartingale, when the size of the mesh tends to 0 the authors in [22] showed that XD,<∞ 0,T converges to a certain rough path that incorporates information about the quadratic variation – i.e. volatility – of the semimartingale. Appendix B. Variational Autoencoders In this section we brieﬂy recall the basics of Variational Autoencoders (henceforth VAEs), our choice of generative model. More details and background information on VAEs can be found in [26, 40, 42]. VAEs have been recently itroduced in the pioneering 2014 article of Kingma and Welling [40]. A particularly applealing feature of that work is, as they emphasize, that their results can by construction be applied to nonstationary settings, such as time-series data [40, Section 2]. In the section below, we lay out also further reasons of our motivation for choosing VAEs as a generative model for our Market Generator. The basic mechanism of variational autoencoder essentially rely on a Maximum Likelihood idea (see [26]), adjusting the generative process via backpropagation to maximize (lower bound of) the probability of observing the given training samples. P (X) = ∫ P (X|z; θ)P (z)dz(B.1) It is common practice to choose the initial distribution as a d-dimensional Gaussian. P (X|z; θ) = N (f (z; θ, σ2 ∗ I)), where σ is to be set as a hyperparamter P (z) = N (z|0, I) (B.2) The basic functioning of a VAE is the following: Given one random variable z with one distri- bution, we can create another random variable X = g(z) with very diﬀerent distribution: The deterministic function g is then learned from the data through the function approximation capac- ity of the neural network. If using these expressions (B.2), we can ﬁnd a (continuously diﬀerentiable) expression for P (X) in (B.1), then we can optimize the model using stochastic gradient descent to update the network parameters θ. By gradient descent, we gradually make the training data more probable under the generative model. The equation (B.1) implies two tasks for VAEs to solve (i) how to deﬁne the latent variable space Z (what information they represent and what is the structure between its di- mensions) making sure that the latent variables capture the relevant information in the generative process and (ii) taking the integral over z in (B.1). Regarding problem (i) VAEs avoid explicitly describing the dependencies between dimensions of z, assume no latent structure, instead, they rely on the inherent property of neural networks as functional approximators. Finally, it is worth mentioning that Variational Autoencoders are called “autoencoders” because the training objec- tive (B.1) that derives from this setup has an encoder and decoder structure that resembles a traditional autoencoder, the non-linear generalisation of PCA. For more details and background on Variational Autoencoders see the original work [40] and for a gentle introduction see [42]. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 25 References [1] A. Antonov, J. F. Baldeaux, and R. Sesodia. Quantifying model performance. Preprint, SSRN:3299615, 2018. [2] J. Armstrong, C. Bellani, D. Brigo, and T. Cass. Option pricing models without probability. Preprint, arXiv:1808.09378, 2018. [3] D. Bartl, S. Drapeau, J. Ob l´oj and J. Wiesel. Data driven robustness and uncertainty sensitivity analysis. In preparation, 2020. [4] C. Bayer, P. Friz, J. Gatheral. Pricing under rough volatility. Quantitative Finance, 16(6), p. 887-904, 2016 [5] C. Bayer, B. Horvath, A. Muguruza, B. Stemper,and M. Tomas. On deep pricing and calibration of (rough) stochastic volatility models. Preprint, arXiv:1908.08806, 2019. [6] Boedihardjo, H., Geng, X., Lyons, T. and Yang, D., 2016. The signature of a rough path: uniqueness. Advances in Mathematics, 293, pp.720-737. [7] H. Boedihardjo and X. Geng. The uniqueness of signature problem in the non-Markov setting. Stochastic Processes and their Applications, Vol 25(12), pp 4674-4701, 2015. [8] P. Bonnier, P. Kidger, I. Perez Arribas, C. Salvi and T. Lyons. Deep Signature Transforms. ArXiv:1905.08494, 2019. [9] D. Brigo. Probability-free models in option pricing: statistically indistinguishable dynamics and historical vs. implied volatility. Options: 45 Years after the publication of the Black-Scholes-Merton Model, Conference paper, 2019. [10] H. B¨uhler, L. Gonon, J. Teichmann, and B.Wood. Deep hedging. Quantitative Finance, 19(8) p. 1271-1291, 2019. [11] O. Bousquet, S. Gelly, I. Tolstikhin, C.-J. Simon-Gabriel, and B. Sch¨olkopf. From optimal transport to gener- ative modeling: the VEGAN cookbook. CoRR, abs/1705.07642, May 2017. [12] C. Cuchiero, W. Khosrawi, J. Teichmann. A generative adversarial network approach to calibration of local stochastic volatility models. Preprint, arxiv.org:2005.02505, 2020. [13] I. Chevyrev, and A. Kormilitzin. A primer on the signature method in machine learning. Preprint, arXiv:1603.03788, 2016. [14] I. Chevyrev and H. Oberhauser. Signature moments to characterize laws of stochastic processes. Preprint, arxiv.org:1810.10971, 2018. [15] I. Chevyrev and T. Lyons. Characteristic functions of measures on geometric rough paths. The Annals of Probability Vol 44(6), pp. 4049-4082, 2016. [16] R. Cont. Empirical properties of asset returns: stylized facts and statistical issues. Quantitative Finance, (1): 223-236, 2001. [17] R. Cont and S. B. Hamida. Recovering Volatility from Option Prices by Evolutionary Optimization. Finance Concepts Working Papers, 4(1) 2004. [18] J. Chang. Eﬀective algorithms for inverting the signature of a path. Doctoral dissertation, University of Oxford, 2018. [19] C. Doersch, A. Gupta, M. Hebert and J. Walker. An uncertain future: Forecasting from static images using variational autoencoders. 2012. [20] M. Duembgen and L. C. G. Rogers. Estimate nothing. Quantitative Finance, 14(12), p. 2065-2072, 2014. [21] C. Esteban, S. L. Hyland, and G. Ra asch, Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs, ArXiv:1706.02633, 2018. [22] G. Flint, B. Hambly, and T. Lyons, 2016. Discretely sampled signals and the rough Hoﬀ process. Stochastic Processes and their Applications, 126 (9), pp.2593-2614. [23] P. Friz, M. Hairer. A Course on Rough Paths: With an Introduction to Regularity Structures. Cham: Springer International Publishing, 2014. [24] J. Gatheral, T. Jaisson, M. Rosenbaum. Volatility is rough, Quantitative Finance, 18:6, 933-949, 2018. [25] A. Gretton, K. M. Bogwardt, M. Rasch, B. Sch¨onkof, A.J. Smola A Kernel Two-Sample Test. Journal of Machine Learning Research, 13 723-773, 2012. [26] I. Goodfellow, Y. Bengio and A. Courville. Deep Learning. MIT Press, 2016. [27] F. Gressmann, F. J. Kir´aly, B. Mateen, and H. Oberhauser. Probabilistic Supervised Learning. May 2018. [28] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola. A kernel two-sample test. J. Machine Learning Research, 13:pp 723-773, March 2012. [29] L. G. Gyurk´o, T. Lyons, M. Kontkowski, J. Field. Extracting information from the signature of a ﬁnancial data stream, Preprint, ArXiv:1307.7244 2014. [30] B. Hambly and T. Lyons Uniqueness for the signature of a path of bounded variation and the reduced path group. Annals of Mathematics, Vol 171(1), pp. 109-167, 2010. [31] P. R. Hansen, A. Lunde. A forecast comparison of volatility models: does anything beat a Garch(1,1)?, Journal of Applied Econometrics, 20(7), 2005. [32] P. Henry-Labordere. Generative models for ﬁnancial data. Preprint, SSRN 3408007, 2019. [33] P. Henry-Labordere. (Martingale) optimal transport and anomaly detection with neural networks: A primal- dual algorithm. arXiv:1904.04546, 2019. [34] G. Hinton. Introduction to Neural Networks and Machine Learning Lecture notes (CSC321), Lecture 15: Mixture of experts. [35] B. Horvath, A. Muguruza, M. Tomas. Deep learning volatility. Preprint, arXiv:1901.09647, 2019. 26 HANS B ¨UHLER, BLANKA HORVATH, TERRY LYONS, IMANOL PEREZ ARRIBAS, AND BEN WOOD [36] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359-366, 1989. [37] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):251-257, 1991. [38] A. Jentzen, B. Kuckuck, A. Neufeld, and P. von Wurstemberger. Strong error analysis for stochastic gradient descent optimization algorithms. Preprint, arXiv:1801.09324, 2018. [39] A. Koshiyama, N. Firoozye, and P. Treleaven. Generative adversarial networks for ﬁnancial trading strategies ﬁne-tuning and combination. Preprint arXiv:1901.01751, 2019. [40] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. Foundations and Trends in Machine Learn- ing, arXiv:1312.6114.pdf, 2014. [41] Patrick Kidger and Terry Lyons. Signatory: diﬀerentiable computations of the signature and logsignature transforms, on both CPU and GPU. Preprint, arXiv:2001.00706, 2020. [42] D. P. Kingma and M. Welling. An Introduction to Variational Autoencoders. Foundations and Trends in Machine Learning, 2019. [43] A. S. Koshiyama, N. Firoozye, and P. C. Treleaven, Generative adversarial networks for ﬁnancial trading strategies ﬁne-tuning and combination, Preprint arXiv:1901.01751, 2019. [44] A. Kondratyev, C. Schwarz. The market generator. Preprint, SSRN:3384948, 2019. [45] A. Kondratyev, C. Schwarz, B. Horvath, Data anonymisation, outlier detection and ﬁghting overﬁtting with Restricted Boltzmann Machines. Preprint, SSRN:3526436, 2020. [46] T. Lyons. Diﬀerential equations driven by rough signals. Revista Matemtica Iberoamericana, 14(2), pp.215-310, 1998. [47] T. Lyons. Rough paths, signatures and the modelling of functions on streams, Preprint, arXiv:1307.7244, 2014. [48] D. Levin, T. Lyons, H. Ni. Learning from the past, predicting the statistics for the future, learning an evolving system, arXiv preprint arXiv:1309.0260 2016. [49] T. Lyons, M. Caruana, and T. L´evy. Diﬀerential equations driven by rough paths . Springer Berlin Heidelberg, 2007. [50] R. Leonarduzzi, G. Rochette, J.-P. Bouchaud and S. Mallat. Maximum Entropy Scattering Models for Financial Time Series. Conference paper: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) DOI: 10.1109/ICASSP.2019.8683734, 2019. [51] R. Luo, W. Zhang, X. Xu, and J. Wang. A Neural Stochastic Volatility Model, Preprint, arXiv:1712.00504, 2018. [52] H. Ni, S. Liao, L. Szpruch, M.Wiese, B. Xiao. Conditional Sig-Wasserstein GANs for Time Series Generation. Preprint, 2020. [53] H. Ni, S. Liao, T. Lyons, W. Yang. Learning stochastic diﬀerential equations using RNN with log signature features. Preprint, arXiv:1908.08286, 2019. [54] H. Oberhauser and F. Kir´aly. Kernels for sequentially ordered data. Journal of Machine Learning Research, 20(31):1-45, 2019. [55] I. Perez Arribas. Derivatives pricing using signature payoﬀs, Preprint, arXiv:1809.094662018. [56] J. Reizenstein, B. Graham. The iisignature library: eﬃcient calculation of iterated-integral signatures and log signatures. arxiv:1802.08252, 2018. [57] J. Ruf and W. Wang. Neural networks for option pricing and hedging: a literature review. Preprint, arXiv:1911.05620, 2019. [58] R. M. Rustamov. Closed-form expressions for maximum mean discrepancy with applications to Wasserstein auto-encoders. Preprint, arxiv:1901.03227, 2019. [59] D. J. Sutheraland, H. Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola and A. Gretton. Generative models and model criticism via optimized maximum mean discrepancy. 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [60] S. Takahashi, Y. Chen, and K. Tanaka-Ishii, Modeling ﬁnancial time-series with generative adversarial net- works, Physica A: Statistical Mechanics and its Applications 527, p. 121261, Aug. 2019. [61] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Scholkopf. Wasserstein autoencoders. In International Conference on Learning Representations, 2018. [62] T. Trimborn, P. Otte, S. Cramer, M. Beikirch, E. Pabich and M. Frank. SABCEMM-A Simulator for Agent- Based Computational Economic Market Models. Preprint, arXiv:1801.01811, 2018. [63] M. S. Vidales, D. Siska, and L. Szpruch. Unbiased deep solvers for parametric pdes. Preprint arXiv:1810.05094, 2018. [64] M. Wiese, L. Bai, B. Wood, H. B¨uhler. Deep Hedging: Learning to simulate equity options markets. Preprint, arXiv:1911.01700, 2019. [65] M. Wiese, R. Knobloch, R. Korn, P. Kretschmer. Quant GANs: Deep Generation of Financial Time Series. Preprint, arXiv:1907.06673, 2019. [66] T. Xu, L. K. Wenliang, M. Munn, B. Acciaio. COT-GAN: Generating Sequential Data via Causal Optimal Transport. Preprint, arXiv:2006.08571, 2020. [67] K. Zhang, G. Zhong, J. Dong, S. Wang, Y. Wang, Stock Market Prediction Based on Generative Adversarial Network, Procedia Computer Science 147, pp. 400406, 2019. A DATA-DRIVEN MARKET SIMULATOR FOR SMALL DATA ENVIRONMENTS 27 J.P. Morgan, London E-mail address: hans.buehler@jpmorgan.com Department of Mathematics, King’s College London and The Alan Turing Institute E-mail address: blanka.horvath@kcl.ac.uk and b.horvath@imperial.ac.uk Mathematical Institute, University of Oxford and The Alan Turing Institute E-mail address: terry.lyons@maths.ox.ac.uk Mathematical Institute, University of Oxford and The Alan Turing Institute E-mail address: imanol.perez@maths.ox.ac.uk J.P. Morgan, London E-mail address: ben.wood@jpmorgan.com","libVersion":"0.3.2","langs":""}
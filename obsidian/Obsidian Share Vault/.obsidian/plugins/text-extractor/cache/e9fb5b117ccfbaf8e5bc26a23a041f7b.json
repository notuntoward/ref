{"path":"lit/lit_notes_OLD_PARTIAL/CognitiveCreator23word2VecNLPgtwy.pdf","text":"4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 1/21 Member-only story Word2Vec: NLP’s Gateway to Word Embeddings Explore the world of Word2Vec and its significance in NLP Cognitive Creator · Follow 8 min read · Oct 5, 2023 51 Introduction: Navigating the Language of Word2Vec In the world of Natural Language Processing (NLP), it’s really important to grasp how words are represented. Word2Vec, a game-changing technique, has become a key part of NLP. It’s like a big step forward in how machines understand and use words. Open in app Search Write 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 2/21 Image by Author In this article, we embark on a comprehensive journey to unravel the intricacies of Word2Vec, exploring its history, what it entails, why it’s essential, when to employ it, where it finds application, its advantages, drawbacks, step-by-step implementation, and concluding with a reflection on its significance. History: Tracing the Origins of Word2Vec To comprehend Word2Vec’s significance, we must trace its roots. Developed by Tomas Mikolov at Google in 2013, Word2Vec marked a significant leap in word embedding techniques. It sought to address a fundamental problem in NLP: how to represent words as numerical vectors that capture semantic meaning. What is Word2Vec: Unpacking the Technique 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 3/21 Word2Vec is a method for converting words into dense, real-valued vectors while preserving their semantic relationships. It operates on the principle that words with similar meanings tend to occur in similar contexts. There are two main architectures of Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts a word from its context, while Skip-gram predicts the context words from a given word. 1. Continuous Bag of Words (CBOW) Continuous Bag of Words (CBOW) is a natural language processing model that aims to predict a target word based on the context of surrounding words in a given text. In other words, it learns to understand the meaning of a word by considering the words that typically appear with it in a specified context window. CBOW is a type of word embedding model used to represent words as dense vectors in a continuous vector space. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 4/21 Image by Author let’s explain Continuous Bag of Words (CBOW) in a mathematical way: Step 1: Input Encoding Each word in the vocabulary is represented as a one-hot encoded vector: 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 5/21 For a vocabulary of size V, a target word t is represented as one-hot(t), where one-hot(t) is a binary vector of size V with a 1 at the index corresponding to the target word. Step 2: Word Embedding Layer Word embeddings are represented by a matrix E of size V x N, where N is the dimension of word embeddings. The embedding for a target word t is obtained as e(t) = E * one-hot(t). Step 3: Aggregation of Context Embeddings Context words are represented as one-hot encoded vectors and are averaged to create a context vector h: h = (1/|c|) * ΣE * one-hot(c), where |c| is the number of context words. Step 4: Hidden Layer The context vector h is passed through a hidden layer with weights W and bias b, introducing non-linearity: h’ = f(W * h + b), where f is an activation function like ReLU or sigmoid. Step 5: Output Layer The hidden representation h’ is used to predict the probability distribution over words in the vocabulary. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 6/21 The output layer uses the softmax function to calculate predicted probabilities for each word: y = softmax(V * h’), where V is the output weight matrix. Step 6: Loss Calculation The loss function measures the dissimilarity between predicted word probabilities (y) and the true distribution of the target word. Step 7: Backpropagation and Training The model’s parameters (E, W, and V) are updated using backpropagation and gradient descent to minimize the loss. Step 8: Word Embeddings After training, the rows of the embedding matrix E represent word embeddings, capturing word meanings in a dense vector space. CBOW learns word embeddings by predicting a target word from its context words. The model’s parameters are updated during training to create meaningful word representations in a lower-dimensional space. 2. Skip-gram Skip-gram is a natural language processing technique used for word embedding, where it aims to predict the context words (words that appear nearby) of a given target word in a text corpus. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 7/21 Image by Author Now let’s explain Skip-gram in a mathematical way: Step 1: Input Encoding Each word in the vocabulary is represented as a one-hot encoded vector: 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 8/21 For a vocabulary of size V, a target word t is represented as one-hot(t), where one-hot(t) is a binary vector of size V with a 1 at the index corresponding to the target word. Step 2: Word Embedding Layer Word embeddings are represented by a matrix E of size V x N, where N is the dimension of word embeddings. The embedding for a target word t is obtained as e(t) = E * one-hot(t). Step 3: Hidden Layer The embedding e(t) for the target word t is passed through a hidden layer with weights W and bias b to obtain a hidden representation h: h = f(W * e(t) + b), where f is an activation function like ReLU or sigmoid. Step 4: Output Layer The hidden representation h is used to predict the probability distribution over words in the vocabulary. The output layer uses the softmax function to calculate predicted probabilities for each word: y = softmax(V * h), where V is the output weight matrix. Step 5: Loss Calculation 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 9/21 The loss function measures the dissimilarity between predicted word probabilities (y) and the true distribution of the context words surrounding the target word. Step 6: Backpropagation and Training The model’s parameters (E, W, and V) are updated using backpropagation and gradient descent to minimize the loss. Step 7: Word Embeddings After training, the rows of the embedding matrix E represent word embeddings, capturing word meanings in a dense vector space. Skip-gram learns word embeddings by predicting context words from a target word. The model’s parameters are updated during training to create meaningful word representations in a lower-dimensional space. Unlike CBOW, Skip-gram focuses on predicting context words given a target word, which makes it useful for various NLP tasks such as word analogy and semantic similarity. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 10/21 Image by Author Why Word2Vec Matters: The Significance of Word Embeddings Word2Vec’s significance lies in its ability to provide contextually rich word representations. These embeddings facilitate several NLP tasks, including sentiment analysis, machine translation, information retrieval, and more. They bridge the gap between human language and machine learning models, enhancing the latter’s ability to understand and process textual data. When to Employ Word2Vec: Temporal Relevance 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 11/21 Word2Vec is particularly valuable when dealing with large text corpora, where it can capture semantic relationships efficiently. It shines in scenarios where understanding context and word similarity is crucial, making it a versatile tool across various NLP applications. Where Word2Vec Finds Application: Diverse Domains Word2Vec finds applications in diverse domains. In healthcare, it aids in medical text analysis. In finance, it enhances sentiment analysis for stock prediction. E-commerce platforms use it for recommendation systems, while content recommendation engines rely on Word2Vec to personalize user experiences. Pros and Cons: Weighing the Advantages and Limitations Pros: 1 . Semantic Understanding: Word2Vec captures semantic relationships, enabling models to understand word similarity and context. 2 . Efficiency: It generates compact, efficient representations, reducing memory and computational requirements. 3 . Versatility: Word2Vec’s embeddings can be pre-trained or fine-tuned for specific NLP tasks, making it adaptable. Cons: 1 . Contextual Limitations: Word2Vec does not capture complex contextual nuances found in newer models like BERT. 2 . Out-of-Vocabulary (OOV) Words: It struggles with words not present in the training data. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 12/21 Implementation Step by Step: Mastering Word2Vec To implement the Word2Vec process in Python, you’ll need to use the Gensim library, which provides an easy-to-use interface for training Word2Vec models. First, make sure you have Gensim installed. You can install it using pip: pip install gensim Now, let’s implement the Word2Vec process step by step: Step 1: Data Preparation In this step, you need to gather a large text corpus relevant to your domain. For this example, we’ll use a small sample text. # Sample text corpus corpus = [ \"Word embeddings are essential for NLP tasks.\", \"Word2Vec is a popular word embedding technique.\", \"Text preprocessing is necessary to clean the data.\", \"Training Word2Vec models can be done with Gensim.\", \"Word embeddings capture semantic relationships.\" ] Step 2: Preprocessing In this step, you’ll tokenize and clean the text by removing punctuation and stopwords. You can use libraries like NLTK or spaCy for this purpose. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 13/21 import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize import string # Download NLTK stopwords (if not already downloaded) nltk.download('stopwords') nltk.download('punkt') # Tokenize and clean the text def preprocess_text(text): # Tokenize the text tokens = word_tokenize(text) # Remove punctuation and convert to lowercase tokens = [word.lower() for word in tokens if word.isalpha()] # Remove stopwords stop_words = set(stopwords.words('english')) tokens = [word for word in tokens if word not in stop_words] return tokens # Preprocess the corpus preprocessed_corpus = [preprocess_text(text) for text in corpus] Image by Author Step 3: Training Now, you’ll choose between CBOW and Skip-gram architectures and train the Word2Vec model on your preprocessed data. In this example, we’ll use the Skip-gram architecture. 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 14/21 from gensim.models import Word2Vec # Train Word2Vec model model = Word2Vec(sentences=preprocessed_corpus, vector_size=100, window=5, sg=1, sentences : The preprocessed corpus. vector_size : The dimensionality of the word vectors. window : The maximum distance between the current and predicted word within a sentence. sg : 1 for Skip-gram, 0 for CBOW. min_count : Ignores all words with a total frequency lower than this. Step 4: Embedding Generation You can retrieve word embeddings for words in your vocabulary using the trained Word2Vec model. # Retrieve the word embedding for a word word_embedding = model.wv['word'] print(\"Word Embedding for 'word':\", word_embedding) 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 15/21 Image by Author Step 5: Utilization You can use the generated embeddings in various NLP tasks such as text classification, sentiment analysis, or similarity search. # Example: Finding similar words similar_words = model.wv.most_similar('embedding') print(\"Words similar to 'embedding':\", similar_words) Image by Author This example provides a basic implementation of Word2Vec. Depending on your specific domain and dataset, you may need to adjust parameters and 4/9/24, 10:11 AM Word2V ec: NLP ’s Gateway to Word Embeddings | by Cognitive Creator | Medium https://cognitivecreator.medium.com/word2vec-nlps-gateway-to-word-embeddings-6256e6a61afe 16/21 preprocessing steps for optimal results. Conclusion: Word2Vec’s Impact on NLP Word2Vec stands as a testament to the remarkable strides made in NLP. Its ability to transform words into meaningful vectors has unlocked new possibilities in language understanding and processing. While it may have its limitations, its role in empowering NLP models to decipher the intricacies of human language cannot be understated. As we continue to delve deeper into the world of NLP, Word2Vec remains a fundamental stepping stone, guiding us toward more advanced and nuanced word representations. It exemplifies how a simple idea can have a profound impact on the way we interact with and harness the power of language in the digital age. To access the notebook file containing the code for this article, you need to visit my GitHub repository. Click here to proceed. Word2vec NLP Word Embeddings Skip Gram Cbow","libVersion":"0.3.2","langs":""}
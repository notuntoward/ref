{"path":"lit/lit_sources.backup/OpenAI24tokenizer.pdf","text":"4/10/24, 4:58 PM OpenAI Platform https://platform.openai.com/tokenizer 1/2 Tokenizer Learn about language model tokenization OpenAI's large language models (sometimes referred to as GPT's) process text using tokens, which are common sequences of characters found in a set of text. The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens. You can use the tool below to understand how a piece of text might be tokenized by a language model, and the total count of tokens in that piece of text. It's important to note that the exact tokenization process varies between models. Newer models like GPT-3.5 and GPT-4 use a different tokenizer than previous models, and will produce different tokens for the same input text. Tokens 37 Characters 132 GPT-3.5 & GPT-4 GPT-3 (Legacy) “She wanted to read ‘I can’t’ as a single word, but the tokenizer insisted on splitting it into three tokens: ‘I,’ ‘can,’ and ‘t’.” Clear Show example “She wanted to read ‘I can’t’ as a single word, but the tokenizer insisted on splitting it into three tokens: ‘I,’ ‘can,’ and ‘t’.” Overview Documentation API reference Log in Sign up 4/10/24, 4:58 PM OpenAI Platform https://platform.openai.com/tokenizer 2/2 A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words). If you need a programmatic interface for tokenizing text, check out our tiktoken package for Python. For JavaScript, the community-supported @dbdq/tiktoken package works with most GPT models. Text Token IDsOverview Documentation API reference Log in Sign up","libVersion":"0.3.2","langs":""}
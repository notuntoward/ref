{"path":"work/VPP ideas/Generative AI talk/attachments/Graphics to Borrow-20240320192452449.webp","text":"LoRA can even outperform full finetuning training only 2% of the parameters # Trainable | WikiSQL MNLI-m SAMSum —— Model&Method | Parameters }ﬁm %) Acc. (%) RIRIRL <— ROUGE scores ull finetuning P Sdvusannel B, b S tas . S e crtaismiadioi TSGPT3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5 Only tune bias vectors —» GPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5 = GPT-3 (PreEmbed) 32M 63.1 88.6 48.3/24.2/40.5 Prompt tuning P GPT-3 (PreLayer) 202M | 70.1 89.5 50.8/27.3/43.5 Prefix tuning GPT-3 (Adapter') 7.1M 71.9 89.8 53.0/28.9/44.8 GPT-3 (Adapter'’) 40.1IM 732 91.5 53.2/29.0/45.1 GPT-3 (LoRA) 4.M 734 91.7 53.8/29.8/45.9 GPT-3 (LoRA) ’ 37.7M ‘ 74.0 91.6 53.4/29.2/45.1 Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full fine-tuning. The results on WikiSQL have a fluctuation around +0.5%, MNLI-m around +0.1%, and SAMSum around £0.2/20.2/%£0.1 for the three metrics.","libVersion":"0.3.2","langs":"eng"}
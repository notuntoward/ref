{"path":"lit/lit_notes_OLD_PARTIAL/Nassar22tableFormerStructTransfrmr.pdf","text":"TableFormer: Table Structure Understanding with Transformers. Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar IBM Research {ahn,nli,mly,taa}@zurich.ibm.com Abstract Tables organize valuable content in a concise and com- pact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph’s, etc, since they enhance their predictive capabilities. Unfortu- nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header conﬁgurations, multiline rows, different variety of separa- tion lines, missing entries, etc. As such, the correct iden- tiﬁcation of the table-structure from an image is a non- trivial task. In this paper, we present a new table-structure identiﬁcation model. The latter improves the latest end-to- end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two signiﬁcant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program- matic PDF’s directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves signiﬁcantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables. 1. Introduction The occurrence of tables in documents is ubiquitous. They often summarise quantitative or factual data, which is cumbersome to describe in verbose text but nevertheless ex- tremely valuable. Unfortunately, this compact representa- tion is often not easy to parse by machines. There are many implicit conventions used to obtain a compact table repre- sentation. For example, tables often have complex column- and row-headers in order to reduce duplicated cell content. Lines of different shapes and sizes are leveraged to separate content or indicate a tree structure. Additionally, tables can also have empty/missing table-entries or multi-row textual table-entries. Fig. 1 shows a table which presents all these issues. Figure 1: Picture of a table with subtle, complex features such as (1) multi-column headers, (2) cell with multi-row text and (3) cells with no content. Image from PubTabNet evaluation set, ﬁlename: ‘PMC2944238 004 02’. Recently, signiﬁcant progress has been made with vi- sion based approaches to extract tables in documents. For the sake of completeness, the issue of table extraction from documents is typically decomposed into two separate chal- lenges, i.e. (1) ﬁnding the location of the table(s) on a document-page and (2) ﬁnding the structure of a given table in the document. The ﬁrst problem is called table-location and has been previously addressed [31, 39, 20, 22, 24, 27, 8] with state- of-the-art object-detection networks (e.g. YOLO and later on Mask-RCNN [9]). For all practical purposes, it can be 4614 considered as a solved problem, given enough ground-truth data to train on. The second problem is called table-structure decompo- sition. The latter is a long standing problem in the com- munity of document understanding [6, 4, 15]. Contrary to the table-location problem, there are no commonly used ap- proaches that can easily be re-purposed to solve this prob- lem. Lately, a set of new model-architectures has been pro- posed by the community to address table-structure decom- position [38, 37, 19, 21]. All these models have some weak- nesses (see Sec. 2). The common denominator here is the reliance on textual features and/or the inability to provide the bounding box of each table-cell in the original image. In this paper, we want to address these weaknesses and present a robust table-structure decomposition algorithm. The design criteria for our model are the following. First, we want our algorithm to be versatile and handle different languages without being trained on them. In this way, we can obtain the structure of any table, irregardless of the lan- guage. Second, we want our algorithm to leverage as much data as possible from the original PDF document. For pro- grammatic PDF documents, the text-cells can often be ex- tracted much faster and with higher accuracy compared to OCR methods. Last but not least, we want to have a di- rect link between the table-cell and its bounding box in the image. To meet the design criteria listed above, we developed a new model called TableFormer and a synthetically gener- ated table structure dataset called SynthTabNet1. In partic- ular, our contributions in this work can be summarised as follows: • We propose TableFormer, a transformer based model that predicts tables structure and bounding boxes for the table content simultaneously in an end-to-end ap- proach. • Across all benchmark datasets TableFormer signif- icantly outperforms existing state-of-the-art metrics, while being much more efﬁcient in training and infer- ence to existing works. • We present SynthTabNet a synthetically generated dataset, with various appearance styles and complex- ity. • An augmented dataset based on PubTabNet [38], FinTabNet [37], and TableBank [18] with generated ground-truth for reproducibility. The paper is structured as follows. In Sec. 2, we give a brief overview of the current state-of-the-art. In Sec. 3, we describe the datasets on which we train. In Sec. 4, we introduce the TableFormer model-architecture and describe 1https://github.com/IBM/SynthTabNet its results & performance in Sec. 5. As a conclusion, we de- scribe how this new model-architecture can be re-purposed for other tasks in the computer-vision community. 2. Previous work and State of the Art Identifying the structure of a table has been an outstand- ing problem in the document-parsing community, that mo- tivates many organised public challenges [6, 4, 15]. The difﬁculty of the problem can be attributed to a number of factors. First, there is a large variety in the shapes and sizes of tables. Such large variety requires a ﬂexible method. This is especially true for complex column- and row head- ers, which can be extremely intricate and demanding. A second factor of complexity is the lack of data with regard to table-structure. Until the publication of PubTabNet [38], there were no large datasets (i.e. > 100K tables) that pro- vided structure information. This happens primarily due to the fact that tables are notoriously time-consuming to an- notate by hand. However, this has deﬁnitely changed in re- cent years with the deliverance of PubTabNet [38], FinTab- Net [37], TableBank [18] etc. Before the rising popularity of deep neural networks, the community relied heavily on heuristic and/or statistical methods to do table structure identiﬁcation [3, 7, 11, 5, 14, 29]. Although such methods work well on constrained ta- bles [12], a more data-driven approach can be applied due to the advent of convolutional neural networks (CNNs) and the availability of large datasets. To the best-of-our knowl- edge, there are currently two different types of network ar- chitecture that are being pursued for state-of-the-art table- structure identiﬁcation. Image-to-Text networks: In this type of network, one predicts a sequence of tokens starting from an encoded image. Such sequences of tokens can be HTML table tags [38, 18] or LaTeX symbols[10]. The choice of sym- bols is ultimately not very important, since one can be trans- formed into the other. There are however subtle variations in the Image-to-Text networks. The easiest network archi- tectures are “image-encoder → text-decoder” (IETD), sim- ilar to network architectures that try to provide captions to images [33]. In these IETD networks, one expects as output the LaTeX/HTML string of the entire table, i.e. the sym- bols necessary for creating the table with the content of the table. Another approach is the “image-encoder → dual de- coder” (IEDD) networks. In these type of networks, one has two consecutive decoders with different purposes. The ﬁrst decoder is the tag-decoder, i.e. it only produces the HTM- L/LaTeX tags which construct an empty table. The second content-decoder uses the encoding of the image in combi- nation with the output encoding of each cell-tag (from the tag-decoder) to generate the textual content of each table cell. The network architecture of IEDD is certainly more elaborate, but it has the advantage that one can pre-train the 4615 tag-decoder which is constrained to the table-tags. In practice, both network architectures (IETD and IEDD) require an implicit, custom trained object-character- recognition (OCR) to obtain the content of the table-cells. In the case of IETD, this OCR engine is implicit in the de- coder similar to [25]. For the IEDD, the OCR is solely em- bedded in the content-decoder. This reliance on a custom, implicit OCR decoder is of course problematic. OCR is a well known and extremely tough problem, that often needs custom training for each individual language. However, the limited availability for non-english content in the current datasets, makes it impractical to apply the IETD and IEDD methods on tables with other languages. Additionally, OCR can be completely omitted if the tables originate from pro- grammatic PDF documents with known positions of each cell. The latter was the inspiration for the work of this pa- per. Graph Neural networks: Graph Neural networks (GNN’s) take a radically different approach to table- structure extraction. Note that one table cell can consti- tute out of multiple text-cells. To obtain the table-structure, one creates an initial graph, where each of the text-cells becomes a node in the graph similar to [34, 35, 2]. Each node is then associated with en embedding vector coming from the encoded image, its coordinates and the encoded text. Furthermore, nodes that represent adjacent text-cells are linked. Graph Convolutional Networks (GCN’s) based methods take the image as an input, but also the position of the text-cells and their content [19]. The purpose of a GCN is to transform the input graph into a new graph, which re- places the old links with new ones. The new links then represent the table-structure. With this approach, one can avoid the need to build custom OCR decoders. However, the quality of the reconstructed structure is not comparable to the current state-of-the-art [19]. Hybrid Deep Learning-Rule-Based approach: A pop- ular current model for table-structure identiﬁcation is the use of a hybrid Deep Learning-Rule-Based approach similar to [28, 30]. In this approach, one ﬁrst detects the position of the table-cells with object detection (e.g. YoloVx or Mask- RCNN), then classiﬁes the table into different types (from its images) and ﬁnally uses different rule-sets to obtain its table-structure. Currently, this approach achieves state- of-the-art results, but is not an end-to-end deep-learning method. As such, new rules need to be written if different types of tables are encountered. 3. Datasets We rely on large-scale datasets such as PubTabNet [38], FinTabNet [37], and TableBank [18] datasets to train and evaluate our models. These datasets span over various ap- pearance styles and content. We also introduce our own synthetically generated SynthTabNet dataset to ﬁx an im- Figure 2: Distribution of the tables across different table dimensions in PubTabNet + FinTabNet datasets balance in the previous datasets. The PubTabNet dataset contains 509k tables delivered as annotated PNG images. The annotations consist of the table structure represented in HTML format, the tokenized text and its bounding boxes per table cell. Fig. 1 shows the ap- pearance style of PubTabNet. Depending on its complexity, a table is characterized as “simple” when it does not contain row spans or column spans, otherwise it is “complex”. The dataset is divided into Train and Val splits (roughly 98% and 2%). The Train split consists of 54% simple and 46% com- plex tables and the Val split of 51% and 49% respectively. The FinTabNet dataset contains 112k tables delivered as single-page PDF documents with mixed table structures and text content. Similarly to the PubTabNet, the annotations of FinTabNet include the table structure in HTML, the to- kenized text and the bounding boxes on a table cell basis. The dataset is divided into Train, Test and Val splits (81%, 9.5%, 9.5%), and each one is almost equally divided into simple and complex tables (Train: 48% simple, 52% com- plex, Test: 48% simple, 52% complex, Test: 53% simple, 47% complex). Finally the TableBank dataset consists of 145k tables provided as JPEG images. The latter has anno- tations for the table structure, but only few with bounding boxes of the table cells. The entire dataset consists of sim- ple tables and it is divided into 90% Train, 3% Test and 7% Val splits. Due to the heterogeneity across the dataset formats, it was necessary to combine all available data into one homog- enized dataset before we could train our models for practi- cal purposes. Given the size of PubTabNet, we adopted its annotation format and we extracted and converted all tables as PNG images with a resolution of 72 dpi. Additionally, we have ﬁltered out tables with extreme sizes due to small 4616 amount of such tables, and kept only those ones ranging between 1×1 and 20×10 (rows/columns). The availability of the bounding boxes for all table cells is essential to train our models. In order to distinguish be- tween empty and non-empty bounding boxes, we have in- troduced a binary class in the annotation. Unfortunately, the original datasets either omit the bounding boxes for whole tables (e.g. TableBank) or they narrow their scope only to non-empty cells. Therefore, it was imperative to introduce a data pre-processing procedure that generates the missing bounding boxes out of the annotation information. This pro- cedure ﬁrst parses the provided table structure and calcu- lates the dimensions of the most ﬁne-grained grid that cov- ers the table structure. Notice that each table cell may oc- cupy multiple grid squares due to row or column spans. In case of PubTabNet we had to compute missing bounding boxes for 48% of the simple and 69% of the complex ta- bles. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes. As it is illustrated in Fig. 2, the table distributions from all datasets are skewed towards simpler structures with fewer number of rows/columns. Additionally, there is very limited variance in the table styles, which in case of Pub- TabNet and FinTabNet means one styling format for the majority of the tables. Similar limitations appear also in the type of table content, which in some cases (e.g. FinTab- Net) is restricted to a certain domain. Ultimately, the lack of diversity in the training dataset damages the ability of the models to generalize well on unseen data. Motivated by those observations we aimed at generating a synthetic table dataset named SynthTabNet. This approach offers control over: 1) the size of the dataset, 2) the table structure, 3) the table style and 4) the type of content. The complexity of the table structure is described by the size of the table header and the table body, as well as the percentage of the table cells covered by row spans and column spans. A set of carefully designed styling templates provides the basis to build a wide range of table appearances. Lastly, the table content is generated out of a curated collection of text corpora. By controlling the size and scope of the synthetic datasets we are able to train and evaluate our models in a variety of different conditions. For example, we can ﬁrst generate a highly diverse dataset to train our models and then evaluate their performance on other synthetic datasets which are focused on a speciﬁc domain. In this regard, we have prepared four synthetic datasets, each one containing 150k examples. The corpora to gener- ate the table text consists of the most frequent terms appear- ing in PubTabNet and FinTabNet together with randomly generated text. The ﬁrst two synthetic datasets have been ﬁne-tuned to mimic the appearance of the original datasets but encompass more complicated table structures. The third Tags Bbox Size Format PubTabNet ✓ ✓ 509k PNG FinTabNet ✓ ✓ 112k PDF TableBank ✓ ✗ 145k JPEG Combined-Tabnet(*) ✓ ✓ 400k PNG Combined(**) ✓ ✓ 500k PNG SynthTabNet ✓ ✓ 600k PNG Table 1: Both “Combined-Tabnet” and ”Combined- Tabnet” are variations of the following: (*) The Combined- Tabnet dataset is the processed combination of PubTabNet and Fintabnet. (**) The combined dataset is the processed combination of PubTabNet, Fintabnet and TableBank. one adopts a colorful appearance with high contrast and the last one contains tables with sparse content. Lastly, we have combined all synthetic datasets into one big uniﬁed syn- thetic dataset of 600k examples. Tab. 1 summarizes the various attributes of the datasets. 4. The TableFormer model Given the image of a table, TableFormer is able to pre- dict: 1) a sequence of tokens that represent the structure of a table, and 2) a bounding box coupled to a subset of those tokens. The conversion of an image into a sequence of to- kens is a well-known task [36, 17]. While attention is often used as an implicit method to associate each token of the sequence with a position in the original image, an explicit association between the individual table-cells and the image bounding boxes is also required. 4.1. Model architecture. We now describe in detail the proposed method, which is composed of three main components, see Fig. 4. Our CNN Backbone Network encodes the input as a feature vec- tor of predeﬁned length. The input feature vector of the encoded image is passed to the Structure Decoder to pro- duce a sequence of HTML tags that represent the structure of the table. With each prediction of an HTML standard data cell (‘<td>’) the hidden state of that cell is passed to the Cell BBox Decoder. As for spanning cells, such as row or column span, the tag is broken down to ‘<’, ‘rowspan=’ or ‘colspan=’, with the number of spanning cells (attribute), and ‘>’. The hidden state attached to ‘<’ is passed to the Cell BBox Decoder. A shared feed forward network (FFN) receives the hidden states from the Structure Decoder, to provide the ﬁnal detection predictions of the bounding box coordinates and their classiﬁcation. CNN Backbone Network. A ResNet-18 CNN is the backbone that receives the table image and encodes it as a vector of predeﬁned length. The network has been modi- ﬁed by removing the linear and pooling layer, as we are not 4617 Figure 3: TableFormer takes in an image of the PDF and creates bounding box and HTML structure predictions that are synchronized. The bounding boxes grabs the content from the PDF and inserts it in the structure. Input Image Tokenised Tags Multi-Head Attention Add & Normalisation Feed Forward Network Add & Normalisation Linear Softmax CNN BACKBONE ENCODER [30, 1, 2, 3, 4, … 3, 4, 5, 8, 31] Positional Encoding Positional Encoding Add & Normalisation Add & Normalisation Multi-Head Attention Add & Normalisation Feed Forward Network Linear Linear Attention Network MLP Linear SigmoidTransformer Encoder Networkx2 Encoded Output Encoded Output Predicted Tags Bounding Boxes & Classiﬁcation Transformer Decoder Network x4 CELL BBOX DECODER Masked Multi-Head Attention Figure 4: Given an input image of a table, the Encoder pro- duces ﬁxed-length features that represent the input image. The features are then passed to both the Structure Decoder and Cell BBox Decoder. During training, the Structure Decoder receives ‘tokenized tags’ of the HTML code that represent the table structure. Afterwards, a transformer en- coder and decoder architecture is employed to produce fea- tures that are received by a linear layer, and the Cell BBox Decoder. The linear layer is applied to the features to predict the tags. Simultaneously, the Cell BBox Decoder selects features referring to the data cells (‘<td>’, ‘<’) and passes them through an attention network, an MLP, and a linear layer to predict the bounding boxes. performing classiﬁcation, and adding an adaptive pooling layer of size 2828. ResNet by default downsamples the im- age resolution by 32 and then the encoded image is provided to both the Structure Decoder, and Cell BBox Decoder. Structure Decoder. The transformer architecture of this component is based on the work proposed in [32]. After extensive experimentation, the Structure Decoder is mod- eled as a transformer encoder with two encoder layers and a transformer decoder made from a stack of 4 decoder lay- ers that comprise mainly of multi-head attention and feed forward layers. This conﬁguration uses fewer layers and heads in comparison to networks applied to other problems (e.g. “Scene Understanding”, “Image Captioning”), some- thing which we relate to the simplicity of table images. The transformer encoder receives an encoded image from the CNN Backbone Network and reﬁnes it through a multi-head dot-product attention layer, followed by a Feed Forward Network. During training, the transformer de- coder receives as input the output feature produced by the transformer encoder, and the tokenized input of the HTML ground-truth tags. Using a stack of multi-head attention lay- ers, different aspects of the tag sequence could be inferred. This is achieved by each attention head on a layer operating in a different subspace, and then combining altogether their attention score. Cell BBox Decoder. Our architecture allows to simul- taneously predict HTML tags and bounding boxes for each table cell without the need of a separate object detector end to end. This approach is inspired by DETR [1] which em- ploys a Transformer Encoder, and Decoder that looks for a speciﬁc number of object queries (potential object detec- tions). As our model utilizes a transformer architecture, the hidden state of the <td>’ and ‘<’ HTML structure tags be- come the object query. The encoding generated by the CNN Backbone Network along with the features acquired for every data cell from the Transformer Decoder are then passed to the attention net- work. The attention network takes both inputs and learns to provide an attention weighted encoding. This weighted at- 4618 tention encoding is then multiplied to the encoded image to produce a feature for each table cell. Notice that this is dif- ferent than the typical object detection problem where im- balances between the number of detections and the amount of objects may exist. In our case, we know up front that the produced detections always match with the table cells in number and correspondence. The output features for each table cell are then fed into the feed-forward network (FFN). The FFN consists of a Multi-Layer Perceptron (3 layers with ReLU activa- tion function) that predicts the normalized coordinates for the bounding box of each table cell. Finally, the predicted bounding boxes are classiﬁed based on whether they are empty or not using a linear layer. Loss Functions. We formulate a multi-task loss Eq. 4.1 to train our network. The Cross-Entropy loss (denoted as ls) is used to train the Structure Decoder which predicts the structure tokens. As for the Cell BBox Decoder it is trained with a combination of losses denoted as lbox. lbox consists of the generally used l1 loss for object detection and the IoU loss (liou) to be scale invariant as explained in [26]. In comparison to DETR, we do not use the Hungarian algo- rithm [16] to match the predicted bounding boxes with the ground-truth boxes, as we have already achieved a one-to- one match through two steps: 1) Our token input sequence is naturally ordered, therefore the hidden states of the table data cells are also in order when they are provided as in- put to the Cell BBox Decoder, and 2) Our bounding boxes generation mechanism (see Sec. 3) ensures a one-to-one mapping between the cell content and its bounding box for all post-processed datasets. The loss used to train the TableFormer can be deﬁned as following: lbox = λiouliou + λl1 l = λls + (1 − λ)lbox (1) where λ ∈ [0, 1], and λiou, λl1 ∈ R are hyper-parameters. 5. Experimental Results 5.1. Implementation Details TableFormer uses ResNet-18 as the CNN Backbone Net- work. The input images are resized to 448×448 pixels and the feature map has a dimension of 28×28. Additionally, we enforce the following input constraints: Image width and height ≤ 1024 pixels Structural tags length ≤ 512 tokens. (2) Although input constraints are used also by other methods, such as EDD, ours are less restrictive due to the improved runtime performance and lower memory footprint of Table- Former. This allows to utilize input samples with longer sequences and images with larger dimensions. The Transformer Encoder consists of two “Transformer Encoder Layers”, with an input feature size of 512, feed forward network of 1024, and 4 attention heads. As for the Transformer Decoder it is composed of four “Transformer Decoder Layers” with similar input and output dimensions as the “Transformer Encoder Layers”. Even though our model uses fewer layers and heads than the default imple- mentation parameters, our extensive experimentation has proved this setup to be more suitable for table images. We attribute this ﬁnding to the inherent design of table im- ages, which contain mostly lines and text, unlike the more elaborate content present in other scopes (e.g. the COCO dataset). Moreover, we have added ResNet blocks to the inputs of the Structure Decoder and Cell BBox Decoder. This prevents a decoder having a stronger inﬂuence over the learned weights which would damage the other prediction task (structure vs bounding boxes), but learn task speciﬁc weights instead. Lastly our dropout layers are set to 0.5. For training, TableFormer is trained with 3 Adam opti- mizers, each one for the CNN Backbone Network, Structure Decoder, and Cell BBox Decoder. Taking the PubTabNet as an example for our parameter set up, the initializing learn- ing rate is 0.001 for 12 epochs with a batch size of 24, and λ set to 0.5. Afterwards, we reduce the learning rate to 0.0001, the batch size to 18 and train for 12 more epochs or convergence. TableFormer is implemented with PyTorch and Torchvi- sion libraries [23]. To speed up the inference, the image undergoes a single forward pass through the CNN Back- bone Network and transformer encoder. This eliminates the overhead of generating the same features for each decoding step. Similarly, we employ a ’caching’ technique to preform faster autoregressive decoding. This is achieved by storing the features of decoded tokens so we can reuse them for each time step. Therefore, we only compute the attention for each new tag. 5.2. Generalization TableFormer is evaluated on three major publicly avail- able datasets of different nature to prove the generalization and effectiveness of our model. The datasets used for eval- uation are the PubTabNet, FinTabNet and TableBank which stem from the scientiﬁc, ﬁnancial and general domains re- spectively. Our method was not evaluated on ICDAR 2013 due to its very small size as it is composed of 156 tables, which is not enough to train a deep neural network. We also share our baseline results on the challenging SynthTabNet dataset. Throughout our experiments, the same parameters stated in Sec. 5.1 are utilized. 4619 5.3. Datasets and Metrics The Tree-Edit-Distance-Based Similarity (TEDS) met- ric was introduced in [38]. It represents the prediction, and ground-truth as a tree structure of HTML tags. This simi- larity is calculated as: TEDS (Ta, Tb) = 1 − EditDist (Ta, Tb) max (|Ta| , |Tb|) (3) where Ta and Tb represent tables in tree structure HTML format. EditDist denotes the tree-edit distance, and |T | rep- resents the number of nodes in T . 5.4. Quantitative Analysis Structure. As shown in Tab. 2, TableFormer outper- forms all SOTA methods across different datasets by a large margin for predicting the table structure from an image. All the more, our model outperforms pre-trained methods. During the evaluation we do not apply any table ﬁltering. We also provide our baseline results on the SynthTabNet dataset. It has been observed that large tables (e.g. tables that occupy half of the page or more) yield poor predictions. We attribute this issue to the image resizing during the pre- processing step, that produces downsampled images with indistinguishable features. This problem can be addressed by treating such big tables with a separate model which ac- cepts a large input image size. Model TEDS Dataset Simple Complex All EDD [38] PTN 91.1 88.7 89.9 GTE [37] PTN - - 93.01 Davar-Lab [13] PTN 97.88 94.78 96.36 TableFormer PTN 98.5 95.0 96.75 EDD [38] FTN 88.4 92.08 90.6 GTE [37] FTN - - 87.14 GTE [37] (FT) FTN - - 91.02 TableFormer FTN 97.5 96.0 96.8 EDD [38] TB 86.0 - 86.0 TableFormer TB 89.6 - 89.6 TableFormer STN 96.9 95.7 96.7 Table 2: Structure results on PubTabNet (PTN), FinTabNet (FTN), TableBank (TB) and SynthTabNet (STN). FT: Model was trained on PubTabNet then ﬁnetuned. Cell Detection. Like any object detector, our Cell BBox Detector provides bounding boxes that can be improved with post-processing during inference. We make use of the grid-like structure of tables to reﬁne the predictions. A de- tailed explanation on the post-processing is available in the supplementary material. As shown in Tab. 3, we evaluate our Cell BBox Decoder accuracy for cells with a class la- bel of ‘content’ only using the PASCAL VOC mAP metric for pre-processing and post-processing. Note that we do not have post-processing results for SynthTabNet as images are only provided. To compare the performance of our pro- posed approach, we’ve integrated TableFormer’s Cell BBox Decoder into EDD architecture. As mentioned previously, the Structure Decoder provides the Cell BBox Decoder with the features needed to predict the bounding box predictions. Therefore, the accuracy of the Structure Decoder directly inﬂuences the accuracy of the Cell BBox Decoder. If the Structure Decoder predicts an extra column, this will result in an extra column of predicted bounding boxes. Model Dataset mAP mAP (PP) EDD+BBox PubTabNet 79.2 82.7 TableFormer PubTabNet 82.1 86.8 TableFormer SynthTabNet 87.7 - Table 3: Cell Bounding Box detection results on PubTab- Net, and FinTabNet. PP: Post-processing. Cell Content. In this section, we evaluate the entire pipeline of recovering a table with content. Here we put our approach to test by capitalizing on extracting content from the PDF cells rather than decoding from images. Tab. 4 shows the TEDs score of HTML code representing the structure of the table along with the content inserted in the data cell and compared with the ground-truth. Our method achieved a 5.3% increase over the state-of-the-art, and com- mercial solutions. We believe our scores would be higher if the HTML ground-truth matched the extracted PDF cell content. Unfortunately, there are small discrepancies such as spacings around words or special characters with various unicode representations. Model TEDS Simple Complex All Tabula 78.0 57.8 67.9 Traprange 60.8 49.9 55.4 Camelot 80.0 66.0 73.0 Acrobat Pro 68.9 61.8 65.3 EDD 91.2 85.4 88.3 TableFormer 95.4 90.1 93.6 Table 4: Results of structure with content retrieved using cell detection on PubTabNet. In all cases the input is PDF documents with cropped tables. 4620 論文ファイル 参考文献 出典 ファイル数 英語 日本語 英語 日本語 電気情報通信学会年総合大会 情報処理学会第回全国大会 第回人工知能学会全国大会 自然言語処理研究会第〜回 から収集した論文 計 Figure 5: One of the beneﬁts of TableFormer is that it does not rely on language information, as an example, the left part of the illustration demonstrates TableFormer predictions on previously unseen language (Japanese). Additionally, we see that TableFormer is robust to variability in style and content, right side of the illustration shows the example of the TableFormer prediction from the FinTabNet dataset. Figure 6: An example of TableFormer predictions (bounding boxes and structure) from generated SynthTabNet table. 5.5. Qualitative Analysis We showcase several visualizations for the different components of our network on various “complex” tables within datasets presented in this work in Fig. 5 and Fig. 6 As it is shown, our model is able to predict bounding boxes for all table cells, even for the empty ones. Additionally, our post-processing techniques can extract the cell content by matching the predicted bounding boxes to the PDF cells based on their overlap and spatial proximity. The left part of Fig. 5 demonstrates also the adaptability of our method to any language, as it can successfully extract Japanese text, although the training set contains only English content. We provide more visualizations including the intermediate steps in the supplementary material. Overall these illustra- tions justify the versatility of our method across a diverse range of table appearances and content type. 6. Future Work & Conclusion In this paper, we presented TableFormer an end-to-end transformer based approach to predict table structures and bounding boxes of cells from an image. This approach en- ables us to recreate the table structure, and extract the cell content from PDF or OCR by using bounding boxes. Ad- ditionally, it provides the versatility required in real-world scenarios when dealing with various types of PDF docu- ments, and languages. Furthermore, our method outper- forms all state-of-the-arts with a wide margin. Finally, we introduce “SynthTabNet” a challenging synthetically gen- erated dataset that reinforces missing characteristics from other datasets. References [1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- 4621 end object detection with transformers. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, edi- tors, Computer Vision – ECCV 2020, pages 213–229, Cham, 2020. Springer International Publishing. [2] Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanx- uan Yin, and Xian-Ling Mao. Complicated table structure recognition. arXiv preprint arXiv:1908.04729, 2019. [3] Bertrand Couasnon and Aurelie Lemaitre. Recognition of Ta- bles and Forms, pages 647–677. Springer London, London, 2014. [4] Herv´e D´ejean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. IC- DAR 2019 Competition on Table Detection and Recognition (cTDaR), Apr. 2019. http://sac.founderit.com/. [5] Basilios Gatos, Dimitrios Danatsas, Ioannis Pratikakis, and Stavros J Perantonis. Automatic table detection in document images. In International Conference on Pattern Recognition and Image Analysis, pages 609–618. Springer, 2005. [6] Max G¨obel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1449–1453, 2013. [7] EA Green and M Krishnamoorthy. Recognition of tables using table grammars. procs. In Symposium on Document Analysis and Recognition (SDAIR95), pages 261–277. [8] Khurram Azeem Hashmi, Alain Pagani, Marcus Liwicki, Di- dier Stricker, and Muhammad Zeshan Afzal. Castabdetec- tors: Cascade network for table detection in document im- ages with recursive feature pyramid and switchable atrous convolution. Journal of Imaging, 7(10), 2021. [9] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. [10] Yelin He, X. Qi, Jiaquan Ye, Peng Gao, Yihao Chen, Bing- cong Li, Xin Tang, and Rong Xiao. Pingan-vcgroup’s so- lution for icdar 2021 competition on scientiﬁc table image recognition to latex. ArXiv, abs/2105.01846, 2021. [11] Jianying Hu, Ramanujan S Kashi, Daniel P Lopresti, and Gordon Wilfong. Medium-independent table detection. In Document Recognition and Retrieval VII, volume 3967, pages 291–302. International Society for Optics and Photon- ics, 1999. [12] Matthew Hurst. A constraint-based approach to table struc- ture derivation. In Proceedings of the Seventh International Conference on Document Analysis and Recognition - Volume 2, ICDAR ’03, page 911, USA, 2003. IEEE Computer Soci- ety. [13] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Icdar 2021 competition on scientiﬁc literature parsing. In International Conference on Document Analysis and Recog- nition, pages 605–617. Springer, 2021. [14] Thotreingam Kasar, Philippine Barlas, Sebastien Adam, Cl´ement Chatelain, and Thierry Paquet. Learning to detect tables in scanned document images using line information. In 2013 12th International Conference on Document Analy- sis and Recognition, pages 1185–1189. IEEE, 2013. [15] Pratik Kayal, Mrinal Anand, Harsh Desai, and Mayank Singh. Icdar 2021 competition on scientiﬁc table image recognition to latex, 2021. [16] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97, 1955. [17] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sag- nik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. Babytalk: Understanding and generat- ing simple image descriptions. IEEE Transactions on Pat- tern Analysis and Machine Intelligence, 35(12):2891–2903, 2013. [18] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Tablebank: A benchmark dataset for table detection and recognition, 2019. [19] Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye, and Xianhui Liu. Gfte: Graph-based ﬁnancial table extraction. In Alberto Del Bimbo, Rita Cucchiara, Stan Sclaroff, Gio- vanni Maria Farinella, Tao Mei, Marco Bertini, Hugo Jair Escalante, and Roberto Vezzani, editors, Pattern Recogni- tion. ICPR International Workshops and Challenges, pages 644–658, Cham, 2021. Springer International Publishing. [20] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Vik- tor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolﬁ, Christoph Auer, Kasper Dinkla, and Peter Staar. Ro- bust pdf document conversion using recurrent neural net- works. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35(17):15137–15145, May 2021. [21] Rujiao Long, Wen Wang, Nan Xue, Feiyu Gao, Zhibo Yang, Yongpan Wang, and Gui-Song Xia. Parsing table structures in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 944–952, 2021. [22] Shubham Singh Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig. Tablenet: Deep learn- ing model for end-to-end table detection and tabular data ex- traction from scanned document images. In 2019 Interna- tional Conference on Document Analysis and Recognition (ICDAR), pages 128–133. IEEE, 2019. [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 32, pages 8024–8035. Curran Asso- ciates, Inc., 2019. [24] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure. Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 572–573, 2020. [25] Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. Rethinking table recognition using graph neural networks. 4622 In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 142–147. IEEE, 2019. [26] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized in- tersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 658–666, 2019. [27] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Den- gel, and Sheraz Ahmed. Deepdesrt: Deep learning for detec- tion and structure recognition of tables in document images. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 1162– 1167, 2017. [28] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Den- gel, and Sheraz Ahmed. Deepdesrt: Deep learning for de- tection and structure recognition of tables in document im- ages. In 2017 14th IAPR international conference on doc- ument analysis and recognition (ICDAR), volume 1, pages 1162–1167. IEEE, 2017. [29] Faisal Shafait and Ray Smith. Table detection in heteroge- neous documents. In Proceedings of the 9th IAPR Interna- tional Workshop on Document Analysis Systems, pages 65– 72, 2010. [30] Shoaib Ahmed Siddiqui, Imran Ali Fateh, Syed Tah- seen Raza Rizvi, Andreas Dengel, and Sheraz Ahmed. Deeptabstr: Deep learning based table structure recognition. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1403–1409. IEEE, 2019. [31] Peter W J Staar, Michele Dolﬁ, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning plat- form to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD, KDD ’18, pages 774–782, New York, NY, USA, 2018. ACM. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Il- lia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish- wanathan, and R. Garnett, editors, Advances in Neural In- formation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. [33] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du- mitru Erhan. Show and tell: A neural image caption gen- erator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. [34] Wenyuan Xue, Qingyong Li, and Dacheng Tao. Res2tim: reconstruct syntactic structures from table images. In 2019 International Conference on Document Analysis and Recog- nition (ICDAR), pages 749–755. IEEE, 2019. [35] Wenyuan Xue, Baosheng Yu, Wen Wang, Dacheng Tao, and Qingyong Li. Tgrnet: A table graph reconstruction network for table structure recognition. arXiv preprint arXiv:2106.10598, 2021. [36] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4651–4659, 2016. [37] Xinyi Zheng, Doug Burdick, Lucian Popa, Peter Zhong, and Nancy Xin Ru Wang. Global table extractor (gte): A frame- work for joint table identiﬁcation and cell structure recogni- tion using visual context. Winter Conference for Applications in Computer Vision (WACV), 2021. [38] Xu Zhong, Elaheh ShaﬁeiBavani, and Antonio Ji- meno Yepes. Image-based table recognition: Data, model, and evaluation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision – ECCV 2020, pages 564–580, Cham, 2020. Springer Interna- tional Publishing. [39] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Pub- laynet: Largest dataset ever for document layout analysis. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1015–1022, 2019. 4623","libVersion":"0.3.2","langs":""}
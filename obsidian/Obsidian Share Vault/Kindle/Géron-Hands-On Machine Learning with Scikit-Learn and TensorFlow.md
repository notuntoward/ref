---
kindle-sync:
  bookId: "13162"
  title: "Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems"
  author: Aurélien Géron
  asin: B06XNKV5TS
  lastAnnotatedDate: 2021-10-19
  bookImageUrl: https://m.media-amazon.com/images/I/91S7m84AG-L._SY160.jpg
  highlightsCount: 355
created date: 2024-11-30T20:34:28-08:00
modified date: 2024-11-30T20:34:28-08:00
---
# Hands-On Machine Learning with Scikit-Learn and TensorFlow
## Metadata
* Author: [Aurélien Géron](https://www.amazon.comundefined)
* ASIN: B06XNKV5TS
* Reference: https://www.amazon.com/dp/B06XNKV5TS
* [Kindle link](kindle://book?action=open&asin=B06XNKV5TS)

## Highlights
StratifiedShuffleSplit — location: [1080](kindle://book?action=open&asin=B06XNKV5TS&location=1080) ^ref-15697

StratifiedShuffleSplit

Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (p. 52). O'Reilly Media. Kindle Edition.

---
Figure 2-14. Standard correlation coefficient of various datasets (source: Wikipedia; public domain image) — location: [1165](kindle://book?action=open&asin=B06XNKV5TS&location=1165) ^ref-18635

Nice graph showing clear x/y relationships with zero correlation.

---
tail-heavy distribution, so you may want to transform — location: [1201](kindle://book?action=open&asin=B06XNKV5TS&location=1201) ^ref-29235

Tail heavy distributions can (I assume) be made more normal by log tranformation.  I guess this might fit some modeling assumptions better.

---
the median of that attribute: — location: [1262](kindle://book?action=open&asin=B06XNKV5TS&location=1262) ^ref-5325

scikit-learn has a built-in median-substitution missing data imputer.  But using it seems like a bad idea.

---
Estimators. — location: [1285](kindle://book?action=open&asin=B06XNKV5TS&location=1285) ^ref-63357

---
fit() — location: [1286](kindle://book?action=open&asin=B06XNKV5TS&location=1286) ^ref-31893

---
Transformers. — location: [1289](kindle://book?action=open&asin=B06XNKV5TS&location=1289) ^ref-59481

---
transform() — location: [1291](kindle://book?action=open&asin=B06XNKV5TS&location=1291) ^ref-41009

---
fit_transform() — location: [1293](kindle://book?action=open&asin=B06XNKV5TS&location=1293) ^ref-50900

---
Predictors. — location: [1295](kindle://book?action=open&asin=B06XNKV5TS&location=1295) ^ref-42865

---
predict() — location: [1298](kindle://book?action=open&asin=B06XNKV5TS&location=1298) ^ref-6975

---
score() — location: [1299](kindle://book?action=open&asin=B06XNKV5TS&location=1299) ^ref-7535

---
hyperparameters — location: [1302](kindle://book?action=open&asin=B06XNKV5TS&location=1302) ^ref-6242

---
learned parameters are — location: [1303](kindle://book?action=open&asin=B06XNKV5TS&location=1303) ^ref-10383

---
underscore suffix — location: [1303](kindle://book?action=open&asin=B06XNKV5TS&location=1303) ^ref-9461

---
factorize() — location: [1321](kindle://book?action=open&asin=B06XNKV5TS&location=1321) ^ref-45852

---
create one binary attribute per category: — location: [1330](kindle://book?action=open&asin=B06XNKV5TS&location=1330) ^ref-57757

---
one-hot encoding, — location: [1331](kindle://book?action=open&asin=B06XNKV5TS&location=1331) ^ref-47443

---
OneHotEncoder — location: [1333](kindle://book?action=open&asin=B06XNKV5TS&location=1333) ^ref-18802

---
CategoricalEncoder — location: [1354](kindle://book?action=open&asin=B06XNKV5TS&location=1354) ^ref-2405

---
TransformerMixin — location: [1386](kindle://book?action=open&asin=B06XNKV5TS&location=1386) ^ref-28382

---
BaseEstimator — location: [1387](kindle://book?action=open&asin=B06XNKV5TS&location=1387) ^ref-42616

---
normalization) — location: [1429](kindle://book?action=open&asin=B06XNKV5TS&location=1429) ^ref-36533

---
Standardization — location: [1432](kindle://book?action=open&asin=B06XNKV5TS&location=1432) ^ref-35669

---
There is nothing in Scikit-Learn to handle Pandas DataFrames, — location: [1464](kindle://book?action=open&asin=B06XNKV5TS&location=1464) ^ref-34216

---
FeatureUnion — location: [1496](kindle://book?action=open&asin=B06XNKV5TS&location=1496) ^ref-51482

---
full_pipeline.fit_transform( — location: [1506](kindle://book?action=open&asin=B06XNKV5TS&location=1506) ^ref-14151

Call fit_transform() on train; call transform() on test.

---
K-fold cross-validation: it — location: [1572](kindle://book?action=open&asin=B06XNKV5TS&location=1572) ^ref-14880

---
cross-validation features expect a utility function (greater is better) — location: [1582](kindle://book?action=open&asin=B06XNKV5TS&location=1582) ^ref-20220

---
GridSearchCV — location: [1644](kindle://book?action=open&asin=B06XNKV5TS&location=1644) ^ref-21497

---
can treat some of the data preparation steps as hyperparameters. — location: [1699](kindle://book?action=open&asin=B06XNKV5TS&location=1699) ^ref-65056

---
RandomizedSearchCV — location: [1708](kindle://book?action=open&asin=B06XNKV5TS&location=1708) ^ref-36483

---
(call transform(), not fit_transform()!), — location: [1751](kindle://book?action=open&asin=B06XNKV5TS&location=1751) ^ref-39969

---
full_pipeline.transform( — location: [1757](kindle://book?action=open&asin=B06XNKV5TS&location=1757) ^ref-18083

---
precision — location: [2000](kindle://book?action=open&asin=B06XNKV5TS&location=2000) ^ref-2255

---
TP — location: [2002](kindle://book?action=open&asin=B06XNKV5TS&location=2002) ^ref-23012

---
FP — location: [2002](kindle://book?action=open&asin=B06XNKV5TS&location=2002) ^ref-15830

---
recall, — location: [2004](kindle://book?action=open&asin=B06XNKV5TS&location=2004) ^ref-17057

---
sensitivity — location: [2006](kindle://book?action=open&asin=B06XNKV5TS&location=2006) ^ref-36303

---
true positive rate (TPR): — location: [2006](kindle://book?action=open&asin=B06XNKV5TS&location=2006) ^ref-32041

---
FN — location: [2008](kindle://book?action=open&asin=B06XNKV5TS&location=2008) ^ref-15316

---
F1 score, in — location: [2024](kindle://book?action=open&asin=B06XNKV5TS&location=2024) ^ref-24057

---
decision_function() — location: [2056](kindle://book?action=open&asin=B06XNKV5TS&location=2056) ^ref-54123

---
="decision_function") — location: [2074](kindle://book?action=open&asin=B06XNKV5TS&location=2074) ^ref-30916

---
precision_recall_curve() — location: [2075](kindle://book?action=open&asin=B06XNKV5TS&location=2075) ^ref-19324

---
ROC curve plots the true positive rate (another name for recall) against the false positive rate. The — location: [2121](kindle://book?action=open&asin=B06XNKV5TS&location=2121) ^ref-28990

---
FPR — location: [2123](kindle://book?action=open&asin=B06XNKV5TS&location=2123) ^ref-34377

---
specificity. — location: [2125](kindle://book?action=open&asin=B06XNKV5TS&location=2125) ^ref-22734

---
(AUC). — location: [2147](kindle://book?action=open&asin=B06XNKV5TS&location=2147) ^ref-22713

---
prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise. — location: [2154](kindle://book?action=open&asin=B06XNKV5TS&location=2154) ^ref-20339

---
one-versus-all (OvA) strategy — location: [2203](kindle://book?action=open&asin=B06XNKV5TS&location=2203) ^ref-63697

---
one-versus-the-rest). — location: [2204](kindle://book?action=open&asin=B06XNKV5TS&location=2204) ^ref-63438

---
one-versus-one (OvO) strategy. — location: [2206](kindle://book?action=open&asin=B06XNKV5TS&location=2206) ^ref-22807

---
which class wins the most duels. — location: [2208](kindle://book?action=open&asin=B06XNKV5TS&location=2208) ^ref-48356

---
output multiple classes for each instance. — location: [2359](kindle://book?action=open&asin=B06XNKV5TS&location=2359) ^ref-10518

---
each label can be multiclass — location: [2394](kindle://book?action=open&asin=B06XNKV5TS&location=2394) ^ref-45141

---
millions of parameters — location: [4930](kindle://book?action=open&asin=B06XNKV5TS&location=4930) ^ref-39096

---
billions of instances — location: [4930](kindle://book?action=open&asin=B06XNKV5TS&location=4930) ^ref-20545

---
millions of features — location: [4930](kindle://book?action=open&asin=B06XNKV5TS&location=4930) ^ref-38626

---
Python API — location: [4941](kindle://book?action=open&asin=B06XNKV5TS&location=4941) ^ref-56319

---
(tensorflow.contrib.learn), — location: [4942](kindle://book?action=open&asin=B06XNKV5TS&location=4942) ^ref-6457

---
compatible with Scikit-Learn. — location: [4942](kindle://book?action=open&asin=B06XNKV5TS&location=4942) ^ref-50203

---
(tensorflow.contrib.slim) — location: [4946](kindle://book?action=open&asin=B06XNKV5TS&location=4946) ^ref-55983

---
simplify — location: [4946](kindle://book?action=open&asin=B06XNKV5TS&location=4946) ^ref-29808

---
tensorflow.contrib.keras) — location: [4948](kindle://book?action=open&asin=B06XNKV5TS&location=4948) ^ref-57576

---
Pretty Tensor — location: [4949](kindle://book?action=open&asin=B06XNKV5TS&location=4949) ^ref-55912

---
main Python API offers much more flexibility — location: [4949](kindle://book?action=open&asin=B06XNKV5TS&location=4949) ^ref-41179

---
C++ implementations — location: [4951](kindle://book?action=open&asin=B06XNKV5TS&location=4951) ^ref-58425

---
automatic differentiating — location: [4954](kindle://book?action=open&asin=B06XNKV5TS&location=4954) ^ref-11556

---
autodiff). — location: [4954](kindle://book?action=open&asin=B06XNKV5TS&location=4954) ^ref-21932

---
TensorBoard — location: [4955](kindle://book?action=open&asin=B06XNKV5TS&location=4955) ^ref-18087

---
cloud service to — location: [4956](kindle://book?action=open&asin=B06XNKV5TS&location=4956) ^ref-20785

---
does not actually perform any computation, — location: [4995](kindle://book?action=open&asin=B06XNKV5TS&location=4995) ^ref-52269

---
just creates a computation graph. In — location: [4996](kindle://book?action=open&asin=B06XNKV5TS&location=4996) ^ref-56453

---
session — location: [4997](kindle://book?action=open&asin=B06XNKV5TS&location=4997) ^ref-8455

---
devices — location: [4998](kindle://book?action=open&asin=B06XNKV5TS&location=4998) ^ref-32229

---
better way: — location: [5008](kindle://book?action=open&asin=B06XNKV5TS&location=5008) ^ref-12341

---
with — location: [5012](kindle://book?action=open&asin=B06XNKV5TS&location=5012) ^ref-27581

---
automatically closed at the end — location: [5015](kindle://book?action=open&asin=B06XNKV5TS&location=5015) ^ref-31179

---
global_variables_initializer() — location: [5017](kindle://book?action=open&asin=B06XNKV5TS&location=5017) ^ref-17640

---
InteractiveSession. — location: [5025](kindle://book?action=open&asin=B06XNKV5TS&location=5025) ^ref-16855

---
don’t need a with block — location: [5026](kindle://book?action=open&asin=B06XNKV5TS&location=5026) ^ref-34813

---
do need to close — location: [5027](kindle://book?action=open&asin=B06XNKV5TS&location=5027) ^ref-717

---
construction phase), — location: [5033](kindle://book?action=open&asin=B06XNKV5TS&location=5033) ^ref-4024

---
execution phase). — location: [5034](kindle://book?action=open&asin=B06XNKV5TS&location=5034) ^ref-48994

---
creating a new Graph — location: [5044](kindle://book?action=open&asin=B06XNKV5TS&location=5044) ^ref-55665

---
temporarily making it the default — location: [5045](kindle://book?action=open&asin=B06XNKV5TS&location=5045) ^ref-5822

---
duplicate nodes. — location: [5054](kindle://book?action=open&asin=B06XNKV5TS&location=5054) ^ref-37416

---
reset — location: [5055](kindle://book?action=open&asin=B06XNKV5TS&location=5055) ^ref-27429

---
tf.reset_default_graph(). — location: [5055](kindle://book?action=open&asin=B06XNKV5TS&location=5055) ^ref-40040

---
ops — location: [5086](kindle://book?action=open&asin=B06XNKV5TS&location=5086) ^ref-41555

---
Constants — location: [5087](kindle://book?action=open&asin=B06XNKV5TS&location=5087) ^ref-10208

---
variables — location: [5088](kindle://book?action=open&asin=B06XNKV5TS&location=5088) ^ref-49852

---
source ops). — location: [5088](kindle://book?action=open&asin=B06XNKV5TS&location=5088) ^ref-15198

---
multidimensional arrays, — location: [5089](kindle://book?action=open&asin=B06XNKV5TS&location=5089) ^ref-46501

---
tensors — location: [5089](kindle://book?action=open&asin=B06XNKV5TS&location=5089) ^ref-11281

---
the Python API tensors are simply represented by NumPy ndarrays. — location: [5090](kindle://book?action=open&asin=B06XNKV5TS&location=5090) ^ref-11904

---
transpose(), — location: [5096](kindle://book?action=open&asin=B06XNKV5TS&location=5096) ^ref-10592

---
matmul(), — location: [5098](kindle://book?action=open&asin=B06XNKV5TS&location=5098) ^ref-38672

---
matrix_inverse() — location: [5098](kindle://book?action=open&asin=B06XNKV5TS&location=5098) ^ref-2887

---
will automatically run this on your GPU card if you have one — location: [5123](kindle://book?action=open&asin=B06XNKV5TS&location=5123) ^ref-58456

---
When using Gradient Descent, — location: [5132](kindle://book?action=open&asin=B06XNKV5TS&location=5132) ^ref-7842

---
important to first normalize the input — location: [5132](kindle://book?action=open&asin=B06XNKV5TS&location=5132) ^ref-40921

---
random_uniform() — location: [5139](kindle://book?action=open&asin=B06XNKV5TS&location=5139) ^ref-15159

---
assign() — location: [5141](kindle://book?action=open&asin=B06XNKV5TS&location=5141) ^ref-23000

---
(n_epochs — location: [5144](kindle://book?action=open&asin=B06XNKV5TS&location=5144) ^ref-14202

---
(mse). — location: [5145](kindle://book?action=open&asin=B06XNKV5TS&location=5145) ^ref-37573

---
symbolic differentiation — location: [5184](kindle://book?action=open&asin=B06XNKV5TS&location=5184) ^ref-25542

---
would not necessarily be very efficient. — location: [5185](kindle://book?action=open&asin=B06XNKV5TS&location=5185) ^ref-65333

---
autodiff feature comes to the rescue: it can automatically and efficiently compute the gradients — location: [5199](kindle://book?action=open&asin=B06XNKV5TS&location=5199) ^ref-25418

---
gradients() — location: [5203](kindle://book?action=open&asin=B06XNKV5TS&location=5203) ^ref-35094

---
four main approaches to computing gradients automatically. — location: [5207](kindle://book?action=open&asin=B06XNKV5TS&location=5207) ^ref-47742

---
reverse-mode autodiff, — location: [5208](kindle://book?action=open&asin=B06XNKV5TS&location=5208) ^ref-17268

---
are many inputs and few outputs, — location: [5208](kindle://book?action=open&asin=B06XNKV5TS&location=5208) ^ref-43265

---
computes all the partial derivatives of the outputs with regards to all the inputs — location: [5209](kindle://book?action=open&asin=B06XNKV5TS&location=5209) ^ref-40218

---
replace X and y at every iteration with the next mini-batch. — location: [5240](kindle://book?action=open&asin=B06XNKV5TS&location=5240) ^ref-18922

---
placeholder nodes. — location: [5241](kindle://book?action=open&asin=B06XNKV5TS&location=5241) ^ref-19184

---
typically used to pass the training data to TensorFlow during training. — location: [5242](kindle://book?action=open&asin=B06XNKV5TS&location=5242) ^ref-26625

---
placeholder() — location: [5244](kindle://book?action=open&asin=B06XNKV5TS&location=5244) ^ref-41688

---
feed_dict — location: [5247](kindle://book?action=open&asin=B06XNKV5TS&location=5247) ^ref-51866

---
specifies the value of A. — location: [5248](kindle://book?action=open&asin=B06XNKV5TS&location=5248) ^ref-60422

---
fetch the mini-batches one by one, then provide the value of X and y via the feed_dict parameter — location: [5275](kindle://book?action=open&asin=B06XNKV5TS&location=5275) ^ref-52900

---
Saver — location: [5299](kindle://book?action=open&asin=B06XNKV5TS&location=5299) ^ref-26403

---
save() — location: [5300](kindle://book?action=open&asin=B06XNKV5TS&location=5300) ^ref-7938

---
restore() — location: [5320](kindle://book?action=open&asin=B06XNKV5TS&location=5320) ^ref-46168

---
tf.train.import_meta_graph(). — location: [5331](kindle://book?action=open&asin=B06XNKV5TS&location=5331) ^ref-59887

---
at the very end of the construction phase: — location: [5355](kindle://book?action=open&asin=B06XNKV5TS&location=5355) ^ref-20110

---
FileWriter — location: [5363](kindle://book?action=open&asin=B06XNKV5TS&location=5363) ^ref-10049

---
events file. — location: [5366](kindle://book?action=open&asin=B06XNKV5TS&location=5366) ^ref-8327

---
name scopes — location: [5417](kindle://book?action=open&asin=B06XNKV5TS&location=5417) ^ref-25682

---
a Python dictionary — location: [5517](kindle://book?action=open&asin=B06XNKV5TS&location=5517) ^ref-6272

---
containing all the variables in their model, and pass it around to every function. — location: [5517](kindle://book?action=open&asin=B06XNKV5TS&location=5517) ^ref-39741

---
a class for each module — location: [5518](kindle://book?action=open&asin=B06XNKV5TS&location=5518) ^ref-19037

---
shared variable as an attribute — location: [5519](kindle://book?action=open&asin=B06XNKV5TS&location=5519) ^ref-54207

---
TensorFlow offers another option, — location: [5528](kindle://book?action=open&asin=B06XNKV5TS&location=5528) ^ref-1274

---
get_variable() — location: [5532](kindle://book?action=open&asin=B06XNKV5TS&location=5532) ^ref-28219

---
variable_scope(). — location: [5535](kindle://book?action=open&asin=B06XNKV5TS&location=5535) ^ref-42052

---
creates the threshold variable within the relu() function upon — location: [5587](kindle://book?action=open&asin=B06XNKV5TS&location=5587) ^ref-31575

---
ANNs frequently outperform other ML techniques on very large and complex problems. — location: [5679](kindle://book?action=open&asin=B06XNKV5TS&location=5679) ^ref-4208

---
only slightly different from the ones used in the 1990s, — location: [5682](kindle://book?action=open&asin=B06XNKV5TS&location=5682) ^ref-56066

---
small tweaks — location: [5683](kindle://book?action=open&asin=B06XNKV5TS&location=5683) ^ref-5068

---
huge positive impact. — location: [5683](kindle://book?action=open&asin=B06XNKV5TS&location=5683) ^ref-33040

---
artificial neuron: — location: [5711](kindle://book?action=open&asin=B06XNKV5TS&location=5711) ^ref-35088

---
binary (on/off) — location: [5711](kindle://book?action=open&asin=B06XNKV5TS&location=5711) ^ref-13634

---
activates — location: [5712](kindle://book?action=open&asin=B06XNKV5TS&location=5712) ^ref-41159

---
certain number of its inputs are active. — location: [5712](kindle://book?action=open&asin=B06XNKV5TS&location=5712) ^ref-26599

---
possible to build a network of artificial neurons that computes any logical proposition — location: [5713](kindle://book?action=open&asin=B06XNKV5TS&location=5713) ^ref-43361

---
Perceptron — location: [5725](kindle://book?action=open&asin=B06XNKV5TS&location=5725) ^ref-18758

---
linear threshold unit (LTU): — location: [5729](kindle://book?action=open&asin=B06XNKV5TS&location=5729) ^ref-15954

---
Heaviside step function — location: [5736](kindle://book?action=open&asin=B06XNKV5TS&location=5736) ^ref-48925

---
can be used for simple linear binary classification. — location: [5738](kindle://book?action=open&asin=B06XNKV5TS&location=5738) ^ref-30824

---
Perceptron — location: [5742](kindle://book?action=open&asin=B06XNKV5TS&location=5742) ^ref-18758

---
single layer of LTUs, — location: [5743](kindle://book?action=open&asin=B06XNKV5TS&location=5743) ^ref-35535

---
connected to all — location: [5743](kindle://book?action=open&asin=B06XNKV5TS&location=5743) ^ref-23573

---
input neurons: — location: [5745](kindle://book?action=open&asin=B06XNKV5TS&location=5745) ^ref-48793

---
output whatever input they are fed. — location: [5745](kindle://book?action=open&asin=B06XNKV5TS&location=5745) ^ref-30019

---
bias neuron, — location: [5746](kindle://book?action=open&asin=B06XNKV5TS&location=5746) ^ref-54150

---
Hebb’s rule. — location: [5752](kindle://book?action=open&asin=B06XNKV5TS&location=5752) ^ref-6667

---
“Cells that fire together, wire — location: [5754](kindle://book?action=open&asin=B06XNKV5TS&location=5754) ^ref-60730

---
together.” — location: [5754](kindle://book?action=open&asin=B06XNKV5TS&location=5754) ^ref-62612

---
Hebbian learning); — location: [5755](kindle://book?action=open&asin=B06XNKV5TS&location=5755) ^ref-23971

---
does not reinforce connections that lead to the wrong output. — location: [5757](kindle://book?action=open&asin=B06XNKV5TS&location=5757) ^ref-22538

---
decision boundary of each output neuron is linear, — location: [5765](kindle://book?action=open&asin=B06XNKV5TS&location=5765) ^ref-12501

---
Perceptron convergence theorem. — location: [5769](kindle://book?action=open&asin=B06XNKV5TS&location=5769) ^ref-52031

---
Perceptron — location: [5770](kindle://book?action=open&asin=B06XNKV5TS&location=5770) ^ref-18758

---
to prefer Logistic Regression over Perceptrons. — location: [5789](kindle://book?action=open&asin=B06XNKV5TS&location=5789) ^ref-45911

---
some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. — location: [5795](kindle://book?action=open&asin=B06XNKV5TS&location=5795) ^ref-19254

---
Multi-Layer Perceptron (MLP). — location: [5796](kindle://book?action=open&asin=B06XNKV5TS&location=5796) ^ref-30409

---
can solve the XOR — location: [5797](kindle://book?action=open&asin=B06XNKV5TS&location=5797) ^ref-45659

---
hidden layers, — location: [5804](kindle://book?action=open&asin=B06XNKV5TS&location=5804) ^ref-35917

---
output layer — location: [5805](kindle://book?action=open&asin=B06XNKV5TS&location=5805) ^ref-44786

---
two or more hidden layers, — location: [5807](kindle://book?action=open&asin=B06XNKV5TS&location=5807) ^ref-41119

---
deep neural network (DNN). — location: [5808](kindle://book?action=open&asin=B06XNKV5TS&location=5808) ^ref-10071

---
Gradient Descent using reverse-mode autodiff (Gradient — location: [5813](kindle://book?action=open&asin=B06XNKV5TS&location=5813) ^ref-39844

---
each training instance, — location: [5815](kindle://book?action=open&asin=B06XNKV5TS&location=5815) ^ref-19126

---
computes the output of every neuron in each consecutive layer — location: [5815](kindle://book?action=open&asin=B06XNKV5TS&location=5815) ^ref-31837

---
forward pass, — location: [5815](kindle://book?action=open&asin=B06XNKV5TS&location=5815) ^ref-42748

---
computes how much each neuron in the last hidden layer contributed to each output neuron’s error. — location: [5817](kindle://book?action=open&asin=B06XNKV5TS&location=5817) ^ref-32531

---
measures the error gradient across all the connection weights — location: [5819](kindle://book?action=open&asin=B06XNKV5TS&location=5819) ^ref-50277

---
last step — location: [5821](kindle://book?action=open&asin=B06XNKV5TS&location=5821) ^ref-50323

---
Gradient Descent step on all the connection weights — location: [5822](kindle://book?action=open&asin=B06XNKV5TS&location=5822) ^ref-56421

---
replaced the step function with the logistic function, — location: [5826](kindle://book?action=open&asin=B06XNKV5TS&location=5826) ^ref-54148

---
hyperbolic tangent function — location: [5831](kindle://book?action=open&asin=B06XNKV5TS&location=5831) ^ref-57554

---
tends to make each layer’s output more or less normalized — location: [5834](kindle://book?action=open&asin=B06XNKV5TS&location=5834) ^ref-49699

---
ReLU function — location: [5835](kindle://book?action=open&asin=B06XNKV5TS&location=5835) ^ref-15683

---
not differentiable — location: [5837](kindle://book?action=open&asin=B06XNKV5TS&location=5837) ^ref-16172

---
make Gradient Descent bounce around). — location: [5838](kindle://book?action=open&asin=B06XNKV5TS&location=5838) ^ref-2747

---
works very well — location: [5838](kindle://book?action=open&asin=B06XNKV5TS&location=5838) ^ref-27637

---
does not have a maximum output value — location: [5839](kindle://book?action=open&asin=B06XNKV5TS&location=5839) ^ref-7066

---
feedforward neural network (FNN). — location: [5849](kindle://book?action=open&asin=B06XNKV5TS&location=5849) ^ref-42823

---
DNNClassifier — location: [5862](kindle://book?action=open&asin=B06XNKV5TS&location=5862) ^ref-49258

---
two hidden layers — location: [5864](kindle://book?action=open&asin=B06XNKV5TS&location=5864) ^ref-48028

---
300 neurons, — location: [5864](kindle://book?action=open&asin=B06XNKV5TS&location=5864) ^ref-52716

---
100 neurons) — location: [5864](kindle://book?action=open&asin=B06XNKV5TS&location=5864) ^ref-45799

---
40,000 training iterations — location: [5877](kindle://book?action=open&asin=B06XNKV5TS&location=5877) ^ref-15111

---
50 instances. — location: [5877](kindle://book?action=open&asin=B06XNKV5TS&location=5877) ^ref-25983

---
tensorflow.contrib — location: [5888](kindle://book?action=open&asin=B06XNKV5TS&location=5888) ^ref-52601

---
experimental code — location: [5889](kindle://book?action=open&asin=B06XNKV5TS&location=5889) ^ref-13519

---
based on the ReLU — location: [5891](kindle://book?action=open&asin=B06XNKV5TS&location=5891) ^ref-35643

---
activation_fn — location: [5891](kindle://book?action=open&asin=B06XNKV5TS&location=5891) ^ref-54890

---
softmax function, — location: [5892](kindle://book?action=open&asin=B06XNKV5TS&location=5892) ^ref-28602

---
cross entropy — location: [5894](kindle://book?action=open&asin=B06XNKV5TS&location=5894) ^ref-55904

---
only be able to explore a tiny part of the hyperparameter space — location: [6109](kindle://book?action=open&asin=B06XNKV5TS&location=6109) ^ref-21341

---
much better to use randomized search — location: [6110](kindle://book?action=open&asin=B06XNKV5TS&location=6110) ^ref-32435

---
Oscar, — location: [6112](kindle://book?action=open&asin=B06XNKV5TS&location=6112) ^ref-41030

---
deep networks have a much higher — location: [6118](kindle://book?action=open&asin=B06XNKV5TS&location=6118) ^ref-27119

---
parameter efficiency — location: [6118](kindle://book?action=open&asin=B06XNKV5TS&location=6118) ^ref-47869

---
complex functions using exponentially fewer neurons — location: [6119](kindle://book?action=open&asin=B06XNKV5TS&location=6119) ^ref-34394

---
lower hidden layers model low-level structures — location: [6124](kindle://book?action=open&asin=B06XNKV5TS&location=6124) ^ref-34274

---
converge faster — location: [6126](kindle://book?action=open&asin=B06XNKV5TS&location=6126) ^ref-19460

---
generalize to new datasets. — location: [6127](kindle://book?action=open&asin=B06XNKV5TS&location=6127) ^ref-17476

---
many problems — location: [6131](kindle://book?action=open&asin=B06XNKV5TS&location=6131) ^ref-6974

---
one or two hidden layers — location: [6132](kindle://book?action=open&asin=B06XNKV5TS&location=6132) ^ref-59394

---
97% — location: [6132](kindle://book?action=open&asin=B06XNKV5TS&location=6132) ^ref-16277

---
MNIST — location: [6132](kindle://book?action=open&asin=B06XNKV5TS&location=6132) ^ref-29741

---
one hidden — location: [6133](kindle://book?action=open&asin=B06XNKV5TS&location=6133) ^ref-49617

---
98% — location: [6133](kindle://book?action=open&asin=B06XNKV5TS&location=6133) ^ref-16790

---
two hidden layers — location: [6133](kindle://book?action=open&asin=B06XNKV5TS&location=6133) ^ref-48028

---
Very complex tasks, — location: [6135](kindle://book?action=open&asin=B06XNKV5TS&location=6135) ^ref-17239

---
dozens of layers — location: [6135](kindle://book?action=open&asin=B06XNKV5TS&location=6135) ^ref-59710

---
even hundreds, — location: [6135](kindle://book?action=open&asin=B06XNKV5TS&location=6135) ^ref-6492

---
need a huge amount of training data. — location: [6136](kindle://book?action=open&asin=B06XNKV5TS&location=6136) ^ref-58111

---
common practice — location: [6143](kindle://book?action=open&asin=B06XNKV5TS&location=6143) ^ref-60921

---
form a funnel, — location: [6144](kindle://book?action=open&asin=B06XNKV5TS&location=6144) ^ref-42510

---
fewer and fewer neurons at each layer — location: [6144](kindle://book?action=open&asin=B06XNKV5TS&location=6144) ^ref-20671

---
not as common now, — location: [6146](kindle://book?action=open&asin=B06XNKV5TS&location=6146) ^ref-20372

---
use the same size for all hidden layers — location: [6146](kindle://book?action=open&asin=B06XNKV5TS&location=6146) ^ref-11609

---
increasing the number of layers than the number of neurons per layer. — location: [6148](kindle://book?action=open&asin=B06XNKV5TS&location=6148) ^ref-46769

---
simpler approach — location: [6149](kindle://book?action=open&asin=B06XNKV5TS&location=6149) ^ref-11888

---
more layers and neurons than you actually need, — location: [6150](kindle://book?action=open&asin=B06XNKV5TS&location=6150) ^ref-50592

---
early stopping — location: [6150](kindle://book?action=open&asin=B06XNKV5TS&location=6150) ^ref-16310

---
dropout, — location: [6152](kindle://book?action=open&asin=B06XNKV5TS&location=6152) ^ref-17980

---
In most cases you can use the ReLU — location: [6156](kindle://book?action=open&asin=B06XNKV5TS&location=6156) ^ref-61290

---
Gradient Descent does not get stuck as much on plateaus, — location: [6158](kindle://book?action=open&asin=B06XNKV5TS&location=6158) ^ref-14775

---
softmax activation function is generally a good — location: [6161](kindle://book?action=open&asin=B06XNKV5TS&location=6161) ^ref-40213

---
classification — location: [6161](kindle://book?action=open&asin=B06XNKV5TS&location=6161) ^ref-23249

---
when — location: [6161](kindle://book?action=open&asin=B06XNKV5TS&location=6161) ^ref-20659

---
not mutually exclusive — location: [6162](kindle://book?action=open&asin=B06XNKV5TS&location=6162) ^ref-26350

---
two classes), — location: [6162](kindle://book?action=open&asin=B06XNKV5TS&location=6162) ^ref-32449

---
you generally want to use the logistic function. — location: [6162](kindle://book?action=open&asin=B06XNKV5TS&location=6162) ^ref-44847

---
representations of the input — location: [9819](kindle://book?action=open&asin=B06XNKV5TS&location=9819) ^ref-38423

---
codings, — location: [9820](kindle://book?action=open&asin=B06XNKV5TS&location=9820) ^ref-37654

---
useful for dimensionality reduction — location: [9821](kindle://book?action=open&asin=B06XNKV5TS&location=9821) ^ref-63486

---
unsupervised pretraining of deep neural networks — location: [9823](kindle://book?action=open&asin=B06XNKV5TS&location=9823) ^ref-55033

---
capable of randomly generating new data — location: [9824](kindle://book?action=open&asin=B06XNKV5TS&location=9824) ^ref-27321

---
generative model. — location: [9825](kindle://book?action=open&asin=B06XNKV5TS&location=9825) ^ref-29839

---
work by simply learning to copy their inputs to their outputs. — location: [9827](kindle://book?action=open&asin=B06XNKV5TS&location=9827) ^ref-55441

---
constraining the network — location: [9827](kindle://book?action=open&asin=B06XNKV5TS&location=9827) ^ref-27043

---
limit the size of the internal representation, — location: [9828](kindle://book?action=open&asin=B06XNKV5TS&location=9828) ^ref-9854

---
add noise to the inputs — location: [9828](kindle://book?action=open&asin=B06XNKV5TS&location=9828) ^ref-63638

---
recover the original — location: [9829](kindle://book?action=open&asin=B06XNKV5TS&location=9829) ^ref-23251

---
prevent the autoencoder from trivially copying — location: [9829](kindle://book?action=open&asin=B06XNKV5TS&location=9829) ^ref-50205

---
hailstone sequence). — location: [9839](kindle://book?action=open&asin=B06XNKV5TS&location=9839) ^ref-31390

---
encoder — location: [9851](kindle://book?action=open&asin=B06XNKV5TS&location=9851) ^ref-30178

---
recognition network) — location: [9852](kindle://book?action=open&asin=B06XNKV5TS&location=9852) ^ref-763

---
decoder — location: [9852](kindle://book?action=open&asin=B06XNKV5TS&location=9852) ^ref-14552

---
generative network) — location: [9853](kindle://book?action=open&asin=B06XNKV5TS&location=9853) ^ref-59524

---
reconstructions — location: [9857](kindle://book?action=open&asin=B06XNKV5TS&location=9857) ^ref-34683

---
internal representation has a lower dimensionality than the input data (it — location: [9862](kindle://book?action=open&asin=B06XNKV5TS&location=9862) ^ref-64534

---
undercomplete. — location: [9862](kindle://book?action=open&asin=B06XNKV5TS&location=9862) ^ref-13738

---
cannot trivially copy — location: [9863](kindle://book?action=open&asin=B06XNKV5TS&location=9863) ^ref-4966

---
forced to learn — location: [9864](kindle://book?action=open&asin=B06XNKV5TS&location=9864) ^ref-38061

---
only linear activations — location: [9868](kindle://book?action=open&asin=B06XNKV5TS&location=9868) ^ref-58667

---
ends up performing Principal Component Analysis — location: [9869](kindle://book?action=open&asin=B06XNKV5TS&location=9869) ^ref-29299

---
multiple hidden layers. — location: [9906](kindle://book?action=open&asin=B06XNKV5TS&location=9906) ^ref-40926

---
stacked autoencoders — location: [9906](kindle://book?action=open&asin=B06XNKV5TS&location=9906) ^ref-49683

---
deep autoencoders). — location: [9907](kindle://book?action=open&asin=B06XNKV5TS&location=9907) ^ref-47912

---
one must be careful not to make the autoencoder too powerful. — location: [9908](kindle://book?action=open&asin=B06XNKV5TS&location=9908) ^ref-7594

---
typically symmetrical with regards to the central hidden layer — location: [9911](kindle://book?action=open&asin=B06XNKV5TS&location=9911) ^ref-17878

---
tie the weights of the decoder layers to the weights of the encoder layers. — location: [9971](kindle://book?action=open&asin=B06XNKV5TS&location=9971) ^ref-62600

---
easier to just define the layers manually. — location: [9979](kindle://book?action=open&asin=B06XNKV5TS&location=9979) ^ref-47550

---
much faster to train one shallow autoencoder at a time, — location: [10034](kindle://book?action=open&asin=B06XNKV5TS&location=10034) ^ref-45645

---
first phase — location: [10038](kindle://book?action=open&asin=B06XNKV5TS&location=10038) ^ref-23133

---
first autoencoder — location: [10038](kindle://book?action=open&asin=B06XNKV5TS&location=10038) ^ref-41703

---
reconstruct the inputs. — location: [10038](kindle://book?action=open&asin=B06XNKV5TS&location=10038) ^ref-25623

---
second phase, — location: [10038](kindle://book?action=open&asin=B06XNKV5TS&location=10038) ^ref-42717

---
second autoencoder — location: [10038](kindle://book?action=open&asin=B06XNKV5TS&location=10038) ^ref-2876

---
reconstruct the output of the first autoencoder’s hidden layer. — location: [10038](kindle://book?action=open&asin=B06XNKV5TS&location=10038) ^ref-7619

---
simplest approach — location: [10042](kindle://book?action=open&asin=B06XNKV5TS&location=10042) ^ref-50405

---
use a different TensorFlow graph for each phase. — location: [10042](kindle://book?action=open&asin=B06XNKV5TS&location=10042) ^ref-65490

---
simply copy the weights and biases from each autoencoder — location: [10044](kindle://book?action=open&asin=B06XNKV5TS&location=10044) ^ref-5170

---
Another approach — location: [10046](kindle://book?action=open&asin=B06XNKV5TS&location=10046) ^ref-30821

---
single graph — location: [10046](kindle://book?action=open&asin=B06XNKV5TS&location=10046) ^ref-2744

---
just train an autoencoder using all the training data, then reuse its encoder layers to create a new neural network — location: [10163](kindle://book?action=open&asin=B06XNKV5TS&location=10163) ^ref-5822

---
other kinds of constraints — location: [10166](kindle://book?action=open&asin=B06XNKV5TS&location=10166) ^ref-51506

---
coding layer to be just as large as the inputs, or even larger, — location: [10167](kindle://book?action=open&asin=B06XNKV5TS&location=10167) ^ref-17554

---
overcomplete autoencoder. — location: [10168](kindle://book?action=open&asin=B06XNKV5TS&location=10168) ^ref-62726

---
add noise to its inputs, training it to recover the original, noise-free inputs. — location: [10174](kindle://book?action=open&asin=B06XNKV5TS&location=10174) ^ref-7032

---
stacked denoising autoencoders. — location: [10180](kindle://book?action=open&asin=B06XNKV5TS&location=10180) ^ref-16421

---
Gaussian — location: [10181](kindle://book?action=open&asin=B06XNKV5TS&location=10181) ^ref-8798

---
or — location: [10181](kindle://book?action=open&asin=B06XNKV5TS&location=10181) ^ref-20961

---
randomly switched off inputs, — location: [10181](kindle://book?action=open&asin=B06XNKV5TS&location=10181) ^ref-16950

---
tf.shape(X), — location: [10203](kindle://book?action=open&asin=B06XNKV5TS&location=10203) ^ref-51986

---
dropout version, which is more common, — location: [10204](kindle://book?action=open&asin=B06XNKV5TS&location=10204) ^ref-59716

---
adding an appropriate term to the cost function, — location: [10232](kindle://book?action=open&asin=B06XNKV5TS&location=10232) ^ref-60149

---
only 5% significantly active neurons in the coding layer. — location: [10233](kindle://book?action=open&asin=B06XNKV5TS&location=10233) ^ref-60674

---
measure — location: [10236](kindle://book?action=open&asin=B06XNKV5TS&location=10236) ^ref-48628

---
sparsity — location: [10236](kindle://book?action=open&asin=B06XNKV5TS&location=10236) ^ref-40834

---
layer at each training iteration. — location: [10236](kindle://book?action=open&asin=B06XNKV5TS&location=10236) ^ref-13416

---
average activation of each neuron in the coding layer, — location: [10236](kindle://book?action=open&asin=B06XNKV5TS&location=10236) ^ref-54514

---
penalize the neurons that are too active — location: [10238](kindle://book?action=open&asin=B06XNKV5TS&location=10238) ^ref-50217

---
sparsity loss — location: [10239](kindle://book?action=open&asin=B06XNKV5TS&location=10239) ^ref-10085

---
Kullback–Leibler divergence — location: [10241](kindle://book?action=open&asin=B06XNKV5TS&location=10241) ^ref-23141

---
P — location: [10245](kindle://book?action=open&asin=B06XNKV5TS&location=10245) ^ref-28784

---
Q, — location: [10245](kindle://book?action=open&asin=B06XNKV5TS&location=10245) ^ref-3997

---
Kullback–Leibler divergence — location: [10247](kindle://book?action=open&asin=B06XNKV5TS&location=10247) ^ref-23141

---
target probability — location: [10248](kindle://book?action=open&asin=B06XNKV5TS&location=10248) ^ref-30543

---
p — location: [10248](kindle://book?action=open&asin=B06XNKV5TS&location=10248) ^ref-28784

---
actual probability — location: [10249](kindle://book?action=open&asin=B06XNKV5TS&location=10249) ^ref-37954

---
q — location: [10249](kindle://book?action=open&asin=B06XNKV5TS&location=10249) ^ref-29041

---
KL divergence between the target sparsity p and the actual sparsity q — location: [10250](kindle://book?action=open&asin=B06XNKV5TS&location=10250) ^ref-24043

---
activations of the coding layer must be between 0 and 1 — location: [10279](kindle://book?action=open&asin=B06XNKV5TS&location=10279) ^ref-65197

---
use the logistic activation function for the coding layer: — location: [10280](kindle://book?action=open&asin=B06XNKV5TS&location=10280) ^ref-46572

---
speed up convergence: — location: [10284](kindle://book?action=open&asin=B06XNKV5TS&location=10284) ^ref-51207

---
sigmoid_cross_entropy_with_logits() — location: [10287](kindle://book?action=open&asin=B06XNKV5TS&location=10287) ^ref-14928

---
one of the most popular — location: [10305](kindle://book?action=open&asin=B06XNKV5TS&location=10305) ^ref-15014

---
variational autoencoders. — location: [10305](kindle://book?action=open&asin=B06XNKV5TS&location=10305) ^ref-28669

---
probabilistic — location: [10306](kindle://book?action=open&asin=B06XNKV5TS&location=10306) ^ref-4460

---
outputs are partly determined by chance, — location: [10307](kindle://book?action=open&asin=B06XNKV5TS&location=10307) ^ref-37425

---
even after training — location: [10307](kindle://book?action=open&asin=B06XNKV5TS&location=10307) ^ref-42595

---
generative — location: [10308](kindle://book?action=open&asin=B06XNKV5TS&location=10308) ^ref-51502

---
similar to RBMs — location: [10310](kindle://book?action=open&asin=B06XNKV5TS&location=10310) ^ref-12493

---
easier to train and the sampling process is much faster — location: [10310](kindle://book?action=open&asin=B06XNKV5TS&location=10310) ^ref-27786

---
encoder produces a mean coding — location: [10314](kindle://book?action=open&asin=B06XNKV5TS&location=10314) ^ref-6470

---
μ — location: [10315](kindle://book?action=open&asin=B06XNKV5TS&location=10315) ^ref-23179

---
standard deviation — location: [10315](kindle://book?action=open&asin=B06XNKV5TS&location=10315) ^ref-28987

---
σ. — location: [10315](kindle://book?action=open&asin=B06XNKV5TS&location=10315) ^ref-42113

---
sampled randomly from a Gaussian — location: [10315](kindle://book?action=open&asin=B06XNKV5TS&location=10315) ^ref-58440

---
coding space — location: [10324](kindle://book?action=open&asin=B06XNKV5TS&location=10324) ^ref-28324

---
latent space) — location: [10325](kindle://book?action=open&asin=B06XNKV5TS&location=10325) ^ref-63969

---
cost function. — location: [10328](kindle://book?action=open&asin=B06XNKV5TS&location=10328) ^ref-42866

---
two parts. — location: [10328](kindle://book?action=open&asin=B06XNKV5TS&location=10328) ^ref-57813

---
reconstruction loss — location: [10329](kindle://book?action=open&asin=B06XNKV5TS&location=10329) ^ref-12266

---
latent — location: [10330](kindle://book?action=open&asin=B06XNKV5TS&location=10330) ^ref-50314

---
loss — location: [10330](kindle://book?action=open&asin=B06XNKV5TS&location=10330) ^ref-23234

---
KL divergence — location: [10332](kindle://book?action=open&asin=B06XNKV5TS&location=10332) ^ref-50456

---
target distribution — location: [10332](kindle://book?action=open&asin=B06XNKV5TS&location=10332) ^ref-49102

---
actual distribution of the codings. — location: [10332](kindle://book?action=open&asin=B06XNKV5TS&location=10332) ^ref-20306

---
common variant is — location: [10343](kindle://book?action=open&asin=B06XNKV5TS&location=10343) ^ref-39072

---
train the encoder to output γ = log(σ2) — location: [10343](kindle://book?action=open&asin=B06XNKV5TS&location=10343) ^ref-39063

---
somewhat overshadowed unsupervised learning, — location: [10429](kindle://book?action=open&asin=B06XNKV5TS&location=10429) ^ref-50061

---
Contractive autoencoder (CAE)8 — location: [10432](kindle://book?action=open&asin=B06XNKV5TS&location=10432) ^ref-49464

---
derivatives of the codings with regards to the inputs are small. — location: [10434](kindle://book?action=open&asin=B06XNKV5TS&location=10434) ^ref-17124

---
similar inputs must have similar codings. — location: [10435](kindle://book?action=open&asin=B06XNKV5TS&location=10435) ^ref-37814

---
Stacked convolutional autoencoders9 — location: [10436](kindle://book?action=open&asin=B06XNKV5TS&location=10436) ^ref-7422

---
convolutional layers. — location: [10439](kindle://book?action=open&asin=B06XNKV5TS&location=10439) ^ref-24947

---
Generative stochastic network (GSN)10 — location: [10439](kindle://book?action=open&asin=B06XNKV5TS&location=10439) ^ref-19152

---
added capability to generate data. — location: [10442](kindle://book?action=open&asin=B06XNKV5TS&location=10442) ^ref-63638

---
Winner-take-all (WTA) autoencoder11 — location: [10443](kindle://book?action=open&asin=B06XNKV5TS&location=10443) ^ref-45231

---
only the top k% activations for each neuron — location: [10445](kindle://book?action=open&asin=B06XNKV5TS&location=10445) ^ref-37609

---
sparse convolutional — location: [10447](kindle://book?action=open&asin=B06XNKV5TS&location=10447) ^ref-58435

---
Generative Adversarial Network (GAN)12 — location: [10448](kindle://book?action=open&asin=B06XNKV5TS&location=10448) ^ref-44826

---
“discriminator,” — location: [10450](kindle://book?action=open&asin=B06XNKV5TS&location=10450) ^ref-15018

---
“generator.” — location: [10451](kindle://book?action=open&asin=B06XNKV5TS&location=10451) ^ref-5625

---
competition leads to increasingly realistic fake data, — location: [10451](kindle://book?action=open&asin=B06XNKV5TS&location=10451) ^ref-56976

---

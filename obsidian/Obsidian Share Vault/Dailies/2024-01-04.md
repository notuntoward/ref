---
modified date: 2024-07-31T18:46:58-07:00
created date: 2024-12-17T08:51:31-08:00
---
# Talk: Leveraging conformal prediction for calibrated probabilistic time series forecasts
[Inge van den Ende-Leveraging conformal prediction for calibrated probabilistic time series forecasts - YouTube](https://www.youtube.com/watch?v=--WcrDRtrYk)
### Unconditional conformal prediction
Predicts a symmetric coverage range that is constant, regardless of input data.  The only thing that changes is the point forecast about which the coverage range is placed.

Although the talk wasn't super clear, it might be a useful intro, as long as you don't take it too seriously.

- Procedure
	- train a point forecast model
	- predict on a calibration set
	- compute unconditional cdf of the *absolute value* errors
	- pick of the absolute error range within which points fall: the delta in +- delta around the point forecast
- Has a "statistical guarantee of valid coverage" but how can that be, since this is still a forecast?
- It's simple but unconditional symmetry has all the problems you'd expect, like the potential for non-zero forecasted probability of negative power, or power greater than generator capacity
- Python libs: [[Taquet22modelAgnostIntervMAPIE|MAPIE]] and crepes (crepes is easier, I think she said)
### Conformal Quantile Regression
Starts with conditional quantile regression, which is then calibrated. QR is "asymptotically consistent"; conformal adds that "guarantee of valid coverage"

### Procedure
- Train QR (gets conditionality on inputs)
- predict upper and lower quantiles of some interval 
	- e.g. 30%-70% 
	- Symmetric in probability but not necessarily in amplitude
- Calibrate "per interval of prob. pred."
	- test correct # of points are within this interval
		- list difference between actuals and "closest line of the interval".  I guess this means upper or lower bound of the interval, whichever is closest
	- This was very unclear
		- symmetrically adjust?
		- this doesn't seem conditional
		- how do a full distribution?  pick off upper and lower pairs?
		- Is there a different adjustment for upper and lower quantile in interval?
		- She sounded confused at this point.
### Quantile Crossing
- yes, does happen, she says, so must be doing multiple interval pairs?
- quantiles fixed by sorting (sounds like what I used to do)
- OR, she computes 3 sample rolling average, which is somehow supposed to make the quantiles monotonic non-decreasing (I think she means).  This might have been a two step process.
# Talk: A Taste of Conformal Prediction
^81965e
[Emmanuel Cand√®s - A Taste of Conformal Prediction - YouTube](https://www.youtube.com/watch?v=YzTzN3RyFrk)

Another overview of conformal prediction.  Is much more theoretical yet it covers a lot of ground. 
### Definitions 
^c78ebb
(finally comprehended after viewing other videos, including Candes's talk in [[2024-01-05#Talk Conformal inference in 2022|Conformal inference in 2022]])
- **conformity score**: some measure of prediction error for each unseen, non-training set calibration test point.  Bigger means bigger error.
- conformity **quantile**: the quantile at some probability of the calibration set conformity scores.
- **exchangeable**: In practice, I believe this means that future test points will have the same conformity score distribution as the calibration set i.e. you have enough calibration points to accurately predict the quantile of choice, and there is not concept shift between the train, calibration and test sets.  If this is true, then conformal prediction set accuracy will match the conformity score quantile.  
	- But then I'm not impressed.  One of the main stated conformal benefits is that you get a guaranteed accuracy, unlike with quantile regression, where you get predictions that are only asymptotically correct. In practice, is conformal accuracy really any more certain? 
	- Maybe it's better to say that conformal methods can calibrate quantiles, but with the same data size and concept shift problems.  
	- Possibly, they are just a second, independent look at where a quantile should be, and that they are different enough from quantile regression to cancel out some quantile regression errors?
- **prediction set**: what a conformal algorithm predicts.  It's the range of points predicted to have predictions within the conformity quantile.  Or, for classification, the set of classes within that quantile.
### Interesting points
- For some reason full conformal prediction requires training a model for every candidate in the possible set of points you predict.  But he has a [Jackknife+](https://valeman.medium.com/jackknife-a-swiss-knife-of-conformal-prediction-for-regression-ce3b56432f4f) algorithm which cuts down on computation and works pretty well.
- Has examples showing how conformal quantile predictions are better than quantile forecasting of some kind.
- Again talks about exchangeability, which again, I did not understand.
- For multi-class prediction, can predict which of N points belong are in the set of likely values.  **USEFUL FOR VDER PEAK HOUR PREDICTIONS?**
- Guy has a big [paper on conformalized quantile regression](http://papers.neurips.cc/paper/8613-conformalized-quantile-regression.pdf) (2019)
- Calibrations in 2019 don't appear to be adaptive, just like in the other talk I watched today.  But [Improved conformalized quantile regression](https://arxiv.org/abs/2207.02808) does adapt, as does the simple trick in the next bullet.
- A very simple way to update quantile predictions without model training.  Kind of works with concept shift.  Worth looking at.
# Talk: A Tutorial on Conformal Prediction. Part 1: Introduction
[A Tutorial on Conformal Prediction - YouTube](https://www.youtube.com/watch?v=nql000Lu_iE)

This is a simple talk explaining [[#Conformal Classification|Conformal Classification]] and [[#Conformalized Quantile Regression|Conformalized Quantile Regression]].  Conformal Classification would be and easy way to predict a VDER peak hour of day that expands and contracts as the model becomes more or less uncertain. They have [[#<span style="color ff0000">TODO</span> Talk A Tutorial on Conformal Prediction Part 2 Conditional Coverage and Diagnostics|part 2]]  and [[#<span style="color ff0000">TODO</span> Talk Tutorial on Conformal Prediction Part 3 Beyond Conformal Prediction|part 3]] talks below.

### Conformal Classification
#### Method: Conformal Classification
^52e98b

For conformal classification, a blackbox prediction algorithm produces a score for each possible class.  Then, scores are computed over a separate calibration set, and for each prediction, the score, 0<=S(X,y)<=1, for the correct class is collected.   Finally, a predicted set of classes is produced, where the probability of the true answer being inside the set is in the range \[(1-alpha) and 1-alpha + 1/(n+1)\], where alpha is a desired confidence probability e.g. 0.9 for a fairly certain prediction, and n is the number of samples in the calibration set.  This is done by computing a threshold, qhat = ceil((n+1)(1-alpha)/n), and then selecting the set of classes with scores <= qhat.  i.e. the smallest set of points with scores summing to alpha, I think.

#### Why it works
The reason why this works is that it doesn't matter in which order the scores appear: exchangeability, I think, and something to do with [symmetry](https://youtu.be/nql000Lu_iE?t=1154).  This isn't true (I remember for another talk), in cases like sampling without replacement.

#### A better score function
The trick is to pick the right score function.  Regardless, you'll get the right coverage, and adaptive sets sets of varying size, reflecting difficulty.    But the set sizes can be bigger or smaller depending upon the score function choice.

The approach describe in [[#Method Conformal Classification|Method Conformal Classification]], has the smallest average prediction set size of any method, doesn't have great set size adaptivity.  A better method discussed here, use scores (softmax outputs, probably) of [all classes](https://youtu.be/nql000Lu_iE?t=1480), instead of just the true class.

Instead, for each prediction, compute E, the summed prob in the sorted outputs needed to be collected in order to capture the true output's score.  This is the "conformal score".

Then, get the threshold from the alpha probability quantile of all E's.  During test, the prediction set the number of sorted class scores so that their sum exceeds this threshold.  In part 2 of their talk, they say [this method is APS](https://youtu.be/TRx4a2u-j7M?t=464).

### Conformalized Quantile Regression

A way of calibrating continuous value prediction intervals using the outputs of models predicting the lower and upper quantiles of some prediction interval.

####  Method: Conformalized Quantile Regression
- for each sample, the conformal score, E, is the difference between the actual value and the top QR-predicted quantile (at prob 1-alpha/2,  for a desired coverage, alpha).  
- Compute qhat, the alpha_th quantile of the E's found in the calibration set.
- Shrink the predicted interval by moving the upper and lower quantiles by the same amount: ![[2024-01-04-20240104162206187.webp|207]]
- [ ] **Why is this right?**
	- why symmetry?
	- why use alpha twice?
	- why not compute a correction factor for each of the upper and lower quantiles?
# TODO: Talk: A Tutorial on Conformal Prediction. Part 2: Conditional Coverage and Diagnostics
[A Tutorial on Conformal Prediction Part 2: Conditional Coverage and Diagnostics - YouTube](https://www.youtube.com/watch?v=TRx4a2u-j7M)

From the same guys as in previous talk.  They also have a part 3.

# TODO: Talk: Tutorial on Conformal Prediction. Part 3: Beyond Conformal Prediction
[Tutorial on Conformal Prediction Part 3: Beyond Conformal Prediction - YouTube](https://www.youtube.com/watch?v=37HKrmA5gJE)

From the same guys as in previous tow talks



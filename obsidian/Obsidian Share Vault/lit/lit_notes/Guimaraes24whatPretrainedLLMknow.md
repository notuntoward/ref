---
category: literaturenote
tags: ml/genAI
citekey: Guimaraes24whatPretrainedLLMknow
status: unread
dateread: 
ZoteroTags: obsLitNote
aliases:
  - "Pre-trained language models: What do they know?"
  - "Pre-trained language models: What do"
publisher: WIREs Data Mining and Knowledge Discovery
citation key: Guimaraes24whatPretrainedLLMknow
DOI: 10.1002/widm.1518
created date: 2024-07-22T16:11:52-07:00
modified date: 2024-07-22T16:11:52-07:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/2ZVDY86Z)  | [**DOI**](https://doi.org/10.1002/widm.1518)  | [**URL**](https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1518) | [[Guimaraes24whatPretrainedLLMknow.pdf|PDF]]
>
> 
> **Abstract**
> Large language models (LLMs) have substantially pushed artificial intelligence (AI) research and applications in the last few years. They are currently able to achieve high effectiveness in different natural language processing (NLP) tasks, such as machine translation, named entity recognition, text classification, question answering, or text summarization. Recently, significant attention has been drawn to OpenAI's GPT models' capabilities and extremely accessible interface. LLMs are nowadays routinely used and studied for downstream tasks and specific applications with great success, pushing forward the state of the art in almost all of them. However, they also exhibit impressive inference capabilities when used off the shelf without further training. In this paper, we aim to study the behavior of pre-trained language models (PLMs) in some inference tasks they were not initially trained for. Therefore, we focus our attention on very recent research works related to the inference capabilities of PLMs in some selected tasks such as factual probing and common-sense reasoning. We highlight relevant achievements made by these models, as well as some of their current limitations that open opportunities for further research. This article is categorized under: Fundamental Concepts of Data and Knowledge > Key Design Issues in Data Mining Technologies > Artificial Intelligence
> 
> 
> **FirstAuthor**:: Guimarães, Nuno  
> **Author**:: Campos, Ricardo  
> **Author**:: Jorge, Alípio  
~    
> **Title**:: "Pre-trained language models: What do they know?"  
> **Date**:: 2024-01-01  
> **Citekey**:: Guimaraes24whatPretrainedLLMknow  
> **ZoteroItemKey**:: 2ZVDY86Z  
> **itemType**:: journalArticle  
> **DOI**:: 10.1002/widm.1518  
> **URL**:: https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1518  
> **Journal**:: WIREs Data Mining and Knowledge Discovery  
> **Volume**:: 14  
> **Issue**:: 1  
> **Book**:: WIREs Data Mining and Knowledge Discovery  
> **Publisher**::   
> **Location**::    
> **Pages**:: e1518  
> **ISBN**::   
> **ZoteroTags**:: obsLitNote
>**Related**:: 

> Guimarães, Nuno, et al. “Pre-Trained Language Models: What Do They Know?” _WIREs Data Mining and Knowledge Discovery_, vol. 14, no. 1, 2024, p. e1518. _Wiley Online Library_, [https://doi.org/10.1002/widm.1518](https://doi.org/10.1002/widm.1518).
%% begin Obsidian Notes %%
___
==Delete this and write here.==
==Don't delete the `persist` directives above and below.==
___
%% end Obsidian Notes %%



%% Import Date: 2024-04-03T11:55:10.594-07:00 %%

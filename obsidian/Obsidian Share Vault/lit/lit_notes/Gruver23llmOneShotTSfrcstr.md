---
category: literaturenote
tags: ml/genAI
citekey: Gruver23llmOneShotTSfrcstr
status: unread
dateread: 
ZoteroTags: hasCode, /unread
aliases:
  - Large Language Models Are Zero-Shot Time Series Forecasters
  - Large Language Models Are Zero-Shot
publisher: ""
citation key: Gruver23llmOneShotTSfrcstr
DOI: 10.48550/arXiv.2310.07820
created date: 2024-04-07T22:28:13-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/D55ZS487)  | [**DOI**](https://doi.org/10.48550/arXiv.2310.07820)  | [**URL**](http://arxiv.org/abs/2310.07820) | [[Gruver23LargeLanguageModels.pdf|PDF]]
>
> 
> **Abstract**
> By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.
> 
> 
> **FirstAuthor**:: Gruver, Nate  
> **Author**:: Finzi, Marc  
> **Author**:: Qiu, Shikai  
> **Author**:: Wilson, Andrew Gordon  
~    
> **Title**:: "Large Language Models Are Zero-Shot Time Series Forecasters"  
> **Date**:: 2023-10-11  
> **Citekey**:: Gruver23llmOneShotTSfrcstr  
> **ZoteroItemKey**:: D55ZS487  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2310.07820  
> **URL**:: http://arxiv.org/abs/2310.07820  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: hasCode, /unread
>**Related**:: 

> Gruver, Nate, et al. _Large Language Models Are Zero-Shot Time Series Forecasters_. arXiv:2310.07820, arXiv, 11 Oct. 2023. _arXiv.org_, [https://doi.org/10.48550/arXiv.2310.07820](https://doi.org/10.48550/arXiv.2310.07820).
%% begin Obsidian Notes %%
___
==Delete this and write here.==
==Don't delete the `persist` directives above and below.==
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Gruver23llmOneShotTSfrcstr
> 
> Friendly blog post: ([Sen and Zhou, 2024](zotero://select/library/items/DQWJPSHE))
> 
> NeurIPS 2023. Code available at: https://github.com/ngruver/llmtime
> 
> <small>ğŸ“ï¸ (modified: 2024-02-04) [link](zotero://select/library/items/NHTPURMA) - [web](http://zotero.org/users/60638/items/NHTPURMA)</small>
>  
> ---




%% Import Date: 2024-04-07T22:28:29.425-07:00 %%

---
category: literaturenote
tags: ml/genAI
citekey: Guo23CuriousDeclineLLM
status: unread
dateread: 
ZoteroTags: genAI, obsLitNote
aliases:
  - "The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text"
  - The Curious Decline of Linguistic
publisher: ""
citation key: Guo23CuriousDeclineLLM
DOI: 10.48550/arXiv.2311.09807
created date: 2024-04-05T12:58:13-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/6JL8HPA7)  | [**DOI**](https://doi.org/10.48550/arXiv.2311.09807)  | [**URL**](http://arxiv.org/abs/2311.09807) | [[Guo23CuriousDeclineLLM.pdf|PDF]]
>
> 
> **Abstract**
> This study investigates the consequences of training large language models (LLMs) on synthetic data generated by their predecessors, an increasingly prevalent practice aimed at addressing the limited supply of human-generated training data. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we developed a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive fine-tuning experiments across various natural language generation tasks. Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of LLMs.
> 
> 
> **FirstAuthor**:: Guo, Yanzhu  
> **Author**:: Shang, Guokan  
> **Author**:: Vazirgiannis, Michalis  
> **Author**:: Clavel, ChloÃ©  
~    
> **Title**:: "The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text"  
> **Date**:: 2023-11-16  
> **Citekey**:: Guo23CuriousDeclineLLM  
> **ZoteroItemKey**:: 6JL8HPA7  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2311.09807  
> **URL**:: http://arxiv.org/abs/2311.09807  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: genAI, obsLitNote
>**Related**:: 

> Guo, Yanzhu, et al. _The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text_. arXiv:2311.09807, arXiv, 16 Nov. 2023. _arXiv.org_, [https://doi.org/10.48550/arXiv.2311.09807](https://doi.org/10.48550/arXiv.2311.09807).
%% begin Obsidian Notes %%
___
==Delete this and write here.==
==Don't delete the `persist` directives above and below.==
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Guo23CuriousDeclineLLM
> 
> AI performance declines when AI fed AI output, as it would when the internet is clogged w/ AI generated content. Â Good table.
> 
> Same thing for stable diffusion images: ([Hataya et al., 2023](zotero://select/library/items/KS633PSR))
> 
> - google trying to filter AI generated stuff: ([Will Shanklin, 2024](zotero://select/library/items/8SIJDUN2))
> 
> Comment: Work in progress
> 
> <small>ğŸ“ï¸ (modified: 2024-03-05) [link](zotero://select/library/items/9ZRGDVYP) - [web](http://zotero.org/users/60638/items/9ZRGDVYP)</small>
>  
> ---




%% Import Date: 2024-04-05T12:58:20.822-07:00 %%

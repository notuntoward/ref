---
category: literaturenote
tags:
  - ml/genAI
citekey: Jain24liveCodeBenchLeaderboard
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - LiveCodeBench Leaderboard
  - LiveCodeBench Leaderboard
publisher: ""
citation key: Jain24liveCodeBenchLeaderboard
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/3BPQUND9)   | [**URL**](https://livecodebench.github.io/leaderboard.html) | [[Jain24liveCodeBenchLeaderboard.pdf|PDF]] | [[Jain24LiveCodeBenchLeaderboardScrnShot.pdf|PDF]]
>
> 
> 
> **FirstAuthor**:: Jain, Namen  
~    
> **Title**:: "LiveCodeBench Leaderboard"  
> **Date**:: 2024-01-01  
> **Citekey**:: Jain24liveCodeBenchLeaderboard  
> **ZoteroItemKey**:: 3BPQUND9  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://livecodebench.github.io/leaderboard.html  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Jain, Namen. _LiveCodeBench Leaderboard_. 2024, [https://livecodebench.github.io/leaderboard.html](https://livecodebench.github.io/leaderboard.html).
%% begin Obsidian Notes %%
___
The leaderboard for [[Jain24LiveCodeBenchHolisticNoCntam|LiveCodeBench: Holistic and Contamination Free]]

Points that are probably in the paper above, but easier to pick out from the website summary:
- No contamination: for model released on date D, [[Jain24liveCodeBenchLeaderboard.pdf#page=2&annotation=334R|evaluate it on problems released after D]]
- [[Jain24liveCodeBenchLeaderboard.pdf#page=3&annotation=295R|closed api- access models outperform the open models]]
- evidence for [[Jain24liveCodeBenchLeaderboard.pdf#page=4&annotation=328R|lack of diverse fine-tuning data being employed by the open source community]]
- "holistic": tested on code generation, execution, self repair, test output prediction.
- "Pass@1" means [[Jain24LiveCodeBenchHolisticNoCntam|fraction of programming problems which passed on all criteria]]
- AI Explained youtuber [says](https://youtu.be/pal-dMJFU6Q?t=94) that HumanEval coding benchmark is deeply flawed b/c some training data is likely in its eval set.  Was it just that model (Llamma3).  Would LiveCodeBench just not have that problem b/c it considers it?
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Jain24liveCodeBenchLeaderboard
> 
> The leaderboard for ([Jain et al., 2024](zotero://select/library/items/TZZX25Z5))
> 
> <small>📝️ (modified: 2024-04-11) [link](zotero://select/library/items/NMZBQNAC) - [web](http://zotero.org/users/60638/items/NMZBQNAC)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:25.742-07:00 %%

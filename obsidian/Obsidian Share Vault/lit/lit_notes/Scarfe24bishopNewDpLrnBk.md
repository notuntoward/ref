---
category: literaturenote
tags: 
citekey: Scarfe24bishopNewDpLrnBk
status:
  - unread
dateread: 
ZoteroTags: obsLitNote
aliases:
  - Prof. Chris Bishop's NEW Deep Learning Textbook!
  - Prof. Chris Bishop's NEW Deep
publisher: ""
citation key: Scarfe24bishopNewDpLrnBk
DOI: ""
created date: 2024-04-24T18:54:35-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/EFRP5EVC)   | [**URL**](https://www.youtube.com/watch?v=kuvFoXzTK3E)
>
> 
> **Abstract**
> Professor Chris Bishop is a Technical Fellow and Director at Microsoft Research AI4Science, in Cambridge. He is also Honorary Professor of Computer Science at the University of Edinburgh, and a Fellow of Darwin College, Cambridge. In 2004, he was elected Fellow of the Royal Academy of Engineering, in 2007 he was elected Fellow of the Royal Society of Edinburgh, and in 2017 he was elected Fellow of the Royal Society. Chris was a founding member of the UK AI Council, and in 2019 he was appointed to the Prime Minister’s Council for Science and Technology.  At Microsoft Research, Chris oversees a global portfolio of industrial research and development, with a strong focus on machine learning and the natural sciences. Chris obtained a BA in Physics from Oxford, and a PhD in Theoretical Physics from the University of Edinburgh, with a thesis on quantum field theory.   Chris's contributions to the field of machine learning have been truly remarkable. He has authored (what is arguably) the original textbook in the field - 'Pattern Recognition and Machine Learning' (PRML) which has served as an essential reference for countless students and researchers around the world, and that was his second textbook after his highly acclaimed first textbook Neural Networks for Pattern Recognition.   Recently, Chris has co-authored a new book with his son, Hugh, titled 'Deep Learning: Foundations and Concepts.' This book aims to provide a comprehensive understanding of the key ideas and techniques underpinning the rapidly evolving field of deep learning. It covers both the foundational concepts and the latest advances, making it an invaluable resource for newcomers and experienced practitioners alike.  Buy Chris' textbook here: https://amzn.to/3vvLcCh  More about Prof. Chris Bishop: https://en.wikipedia.org/wiki/Christo... https://www.microsoft.com/en-us/resea...  Support MLST: Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, early-access + exclusive content and lots more.     / mlst   Donate: https://www.paypal.com/donate/?hosted... If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail  TOC: 00:00:00 - Intro to Chris 00:06:54 - Changing Landscape of AI 00:08:16 - Symbolism 00:09:32 - PRML 00:11:02 - Bayesian Approach 00:14:49 - Are NNs One Model or Many, Special vs General 00:20:04 - Can Language Models Be Creative 00:22:35 - Sparks of AGI 00:25:52 - Creativity Gap in LLMs 00:35:40 - New Deep Learning Book 00:39:01 - Favourite Chapters 00:44:11 - Probability Theory 00:45:42 - AI4Science 00:48:31 - Inductive Priors 00:58:52 - Drug Discovery 01:05:19 - Foundational Bias Models 01:07:46 - How Fundamental Is Our Physics Knowledge? 01:12:05 - Transformers 01:12:59 - Why Does Deep Learning Work? 01:16:59 - Inscrutability of NNs 01:18:01 - Example of Simulator 01:21:09 - Control
> 
> 
> **FirstDirector**:: Scarfe, Tim  
~    
> **Title**:: "Prof. Chris Bishop's NEW Deep Learning Textbook!"  
> **Date**:: 2024-04-10  
> **Citekey**:: Scarfe24bishopNewDpLrnBk  
> **ZoteroItemKey**:: EFRP5EVC  
> **itemType**:: videoRecording  
> **DOI**::   
> **URL**:: https://www.youtube.com/watch?v=kuvFoXzTK3E  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: obsLitNote
> **Related**:: 

> _Prof. Chris Bishop’s NEW Deep Learning Textbook!_ Directed by Tim Scarfe, 2024. _YouTube_, [https://www.youtube.com/watch?v=kuvFoXzTK3E](https://www.youtube.com/watch?v=kuvFoXzTK3E).
%% begin Obsidian Notes %%
___

# Video Notes

- [00:16:06](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=967#t=16:06.60) GPT-4 better than specialist models e.g. MS used source code to train special programming ML.  But gigantic model w/ lots of irrelevant data is actually better at programming.  
- [00:16:59](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=1019#t=16:59.10) model trained to do math does better at math when given "trained on?" irrelevant data, like Wikipedia.  So there are things we don't really understand.
	- But it seems that larger more general model outperforms a specific model
	- But some science models are good.  Maybe better w/ language
- [00:19:19](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=1160#t=19:19.61) "By virtue of being forced to compress human languages, they've become reasoning engines."
- [00:39:12](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=2352#t=39:12.16) favorite chapters: diffusion, transformers, and in general, how to integrate all the generative frameworks: GANS, variational decoders, normalizing flows.
- [00:40:26](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=2427#t=40:26.78) might add RE learning in a 2nd edition of this book
- [00:49:21](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=2962#t=49:21.57) [[Sutton19bitterLesson|The Bitter Lesson]]: more data beats prior knowledge.  He says he reads it periodically.
	- [00:51:51](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3111#t=51:51.23) LLMs can use a lot of human created data "bitter lesson really kicks in there"
	- [00:52:38](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3158#t=52:38.28) "no free lunch theorem" says you can't learn from data without some form of inductive bias.  I looked it up, and more abstractly, I think it said something like "there's no model that's best in all situations."
	- ML very good at accumulating experience from data, like an experienced person ^617a74
	- says human experience can be an "inductive bias" that's very harmful for machine learning
		- [00:52:37](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3158#t=52:37.88) Transformer is a very light form of inductive bias
		- [00:54:08](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3249#t=54:08.79) it's brittle, a machine can see far more examples than a human can in a lifetime
			- I say, consider the 20K years of reading it would take to get through GPT training data [[Patel23WillScalingWork]]
			- LLM can be more systematic, avoid "recency bias," etc.
	- BUT, he says not so true for science.  Reason: 
		- there is a "rigorous inductive bias," sort of basic laws, like conservation of energy, conservation of momentum.
		- data scarcity in sciences.
			- [00:58:30](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3511#t=58:30.76) he says drug discovery is good b/c can explore huge *combinatorial space* of properties (I think using an NN emulator of physics, but I'd have to go back and check): 10^60 drug molecules
			- so do combinatorial part "in silico" (instead of "in situ")
		- SO, he says science work is a rich, beautiful area to work in.
		- [ ] TODO: get the drug discovery details down
# Drug discovery and ML
- [00:59:06](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3547#t=59:06.93) small "molecule paradigm": small synthetic molecules bind w/ certain target proteins, is where we are today
	- must do that bind
	- but also obey other constraints: absorbed, metabolized, excreted, non-toxic
	- the screening process can be done in ML, I think he said, shrinks search space
- [01:01:02](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3663#t=1:01:02.89) tuberculosis
	- 1.2M people killed in 2022
	- b/c bacteria is evolving resistance
	- know the "pocket" that want to bind to
	- [01:01:48](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3709#t=1:01:48.56) 'smiles' describe molecule as 1D string
		- treat them like tokens
		- predict next element of smile string
		- so speaks the "foundation model of molecules"
		- amino acid (I think is the "smiles" part)
		- but also need the geometry inductive bias
		- I kinda lost it here...
	- now can increase the binding efficiency (2X, I think he said) thanks to modern deep learning
- [01:06:06](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=3966#t=1:06:06.38) There's a language of molecules, a language materials
	- build models w/ broader exposure can improve this
# Transformers

- [01:12:25](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4345#t=1:12:25.31) surprised in transformer is last word in deep learning, but haven't reached its end
- interesting research possible in alternate architectures
# [01:13:04](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4384#t=1:13:04.36) why does DL work?
- [01:13:59](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4440#t=1:13:59.82) models w/ way more params than # data points would be insane to stats
	- odd phenom that train error goes to zero yet test error keeps decreasing
	- something about stoch descent the finds the right zero-valued minima 
	- many open questions, like in neuroscience
		- [01:15:23](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4524#t=1:15:23.54) why do "seemingly overparameterized models" that don't overfit but have good generalization
		- lots of stories to tell about this
		- but need to make them predictive
# Guardrails
- thinks we're heading in good dir to risks
# Example of emulator for a physics ML model
- [01:18:01](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4682#t=1:18:01.62) fusion Tokomak
- changing plasma cross-sectional shape would improve perf
- use NN for feedback control
	- had strong physical inductive bias, describing shape of shape
	- takes 2-3 minutes to solve this equ
	- needed 20 KHz control loop solutions
	- so trained NN to solve this equ.
		- 2 layer
		- a few thousand params
		- NN was actually partly analog so fast enough for tech of the day
# Control and planning problems and agents

- [01:21:30](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4890#t=1:21:30.41) robotics, etc. is a wide open frontier
- [01:22:01](https://www.youtube.com/watch?v=kuvFoXzTK3E&t=4921#t=1:22:01.47) type I, type II (Khanamen) thinking fast/slow sim and eval.  
- That's the future.

___
%% end Obsidian Notes %%



%% Import Date: 2024-04-24T18:54:48.021-07:00 %%


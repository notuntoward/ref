---
category: literaturenote
tags: ml/genAI
citekey: Woo24trainTSfrcstTransfrmrs
status: unread
dateread: 
ZoteroTags: /unread
aliases:
  - Unified Training of Universal Time Series Forecasting Transformers
  - Unified Training of Universal Time
publisher: ""
citation key: Woo24trainTSfrcstTransfrmrs
DOI: ""
created date: 2024-04-07T22:21:48-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/WYFI65CI)   | [**URL**](http://arxiv.org/abs/2402.02592) | [[Woo24trainTSfrcstTransfrmrs.pdf|PDF]]
>
> 
> **Abstract**
> Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.
> 
> 
> **FirstAuthor**:: Woo, Gerald  
> **Author**:: Liu, Chenghao  
> **Author**:: Kumar, Akshat  
> **Author**:: Xiong, Caiming  
> **Author**:: Savarese, Silvio  
> **Author**:: Sahoo, Doyen  
~    
> **Title**:: "Unified Training of Universal Time Series Forecasting Transformers"  
> **Date**:: 2024-02-04  
> **Citekey**:: Woo24trainTSfrcstTransfrmrs  
> **ZoteroItemKey**:: WYFI65CI  
> **itemType**:: preprint  
> **DOI**::   
> **URL**:: http://arxiv.org/abs/2402.02592  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: /unread
>**Related**:: 

> Woo, Gerald, et al. _Unified Training of Universal Time Series Forecasting Transformers_. arXiv:2402.02592, arXiv, 4 Feb. 2024. _arXiv.org_, [http://arxiv.org/abs/2402.02592](http://arxiv.org/abs/2402.02592).
%% begin Obsidian Notes %%
___
==Delete this and write here.==
==Don't delete the `persist` directives above and below.==
___
%% end Obsidian Notes %%



%% Import Date: 2024-04-07T22:23:43.842-07:00 %%

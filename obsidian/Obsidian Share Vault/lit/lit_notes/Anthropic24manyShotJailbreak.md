---
category: literaturenote
tags:
  - ml/genAI
citekey: Anthropic24manyShotJailbreak
status:
  - ?read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Many-shot jailbreaking
  - Many-shot jailbreaking
publisher: ""
citation key: Anthropic24manyShotJailbreak
DOI: ""
created date: 2024-04-13T19:57:31-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/XEV48TDN)   | [**URL**](https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email) | [[Anthropic24manyShotJailbreak.pdf|PDF]]
>
> 
> **Abstract**
> Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.
> 
> 
> **FirstAuthor**:: Anthropic  
~    
> **Title**:: "Many-shot jailbreaking"  
> **Date**:: 2024-04-02  
> **Citekey**:: Anthropic24manyShotJailbreak  
> **ZoteroItemKey**:: XEV48TDN  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Anthropic. “Many-Shot Jailbreaking.” _Anthrop\c_, 2 Apr. 2024, [https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email](https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email).
%% begin Obsidian Notes %%
___

You can override an LLMs safety protections by asking for “bad” information over and over.  The reason has something to do with this is how harmless dialogs progress, with answer accuracy improving with the number of Q’s (verify my understanding of this).  It’s said that this makes models with increasingly large context windows (> 1M) more vulnerable.  Interesting that Anthropic itself published this, and showed how it broke their own model.

Good graph.  For talk, file under

- context window
- one shot (“shot” not quite used in the same sense, though)
- AI danger
- model complexity consequences
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Anthropic24manyShotJailbreak
> 
> You can override an LLMs safety protections by asking for “bad” information over and over.  The reason has something to do with this is how harmless dialogs progress, with answer accuracy improving with the number of Q’s (verify my understanding of this).  It’s said tha this makes models with increasingly large context windows (> 1M) more vulnerable.  Interesting that Anthropic itself published this, and showed how it broke their own model.
> 
> Good graph.  For talk, file under
> 
> - context window
> - one shot (“shot” not quite used in the same sense, though)
> - AI danger
> - model complexity consequences
> 
> <small>📝️ (modified: 2024-04-13) [link](zotero://select/library/items/GAVYNQW4) - [web](http://zotero.org/users/60638/items/GAVYNQW4)</small>
>  
> ---




%% Import Date: 2024-04-13T19:58:44.579-07:00 %%

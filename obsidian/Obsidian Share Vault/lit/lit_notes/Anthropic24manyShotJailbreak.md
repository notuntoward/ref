---
category: literaturenote
tags:
  - ml/genAI
citekey: Anthropic24manyShotJailbreak
status:
  - ?read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Many-shot jailbreaking
  - Many-shot jailbreaking
publisher: ""
citation key: Anthropic24manyShotJailbreak
DOI: ""
created date: 2024-04-13T19:57:31-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/XEV48TDN)   | [**URL**](https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email) | [[Anthropic24manyShotJailbreak.pdf|PDF]]
>
> 
> **Abstract**
> Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.
> 
> 
> **FirstAuthor**:: Anthropic  
~    
> **Title**:: "Many-shot jailbreaking"  
> **Date**:: 2024-04-02  
> **Citekey**:: Anthropic24manyShotJailbreak  
> **ZoteroItemKey**:: XEV48TDN  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Anthropic. â€œMany-Shot Jailbreaking.â€ _Anthrop\c_, 2 Apr. 2024, [https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email](https://www.anthropic.com/research/many-shot-jailbreaking?utm_source=substack&utm_medium=email).
%% begin Obsidian Notes %%
___

You can override an LLMs safety protections by asking for â€œbadâ€ information over and over. Â The reason has something to do with this is how harmless dialogs progress, with answer accuracy improving with the number of Qâ€™s (verify my understanding of this). Â Itâ€™s said that this makes models with increasingly large context windows (> 1M) more vulnerable. Â Interesting that Anthropic itself published this, and showed how it broke their own model.

Good graph. Â For talk, file under

- context window
- one shot (â€œshotâ€ not quite used in the same sense, though)
- AI danger
- model complexity consequences
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Anthropic24manyShotJailbreak
> 
> You can override an LLMs safety protections by asking for â€œbadâ€ information over and over. Â The reason has something to do with this is how harmless dialogs progress, with answer accuracy improving with the number of Qâ€™s (verify my understanding of this). Â Itâ€™s said tha this makes models with increasingly large context windows (> 1M) more vulnerable. Â Interesting that Anthropic itself published this, and showed how it broke their own model.
> 
> Good graph. Â For talk, file under
> 
> - context window
> - one shot (â€œshotâ€ not quite used in the same sense, though)
> - AI danger
> - model complexity consequences
> 
> <small>ğŸ“ï¸ (modified: 2024-04-13) [link](zotero://select/library/items/GAVYNQW4) - [web](http://zotero.org/users/60638/items/GAVYNQW4)</small>
>  
> ---




%% Import Date: 2024-04-13T19:58:44.579-07:00 %%

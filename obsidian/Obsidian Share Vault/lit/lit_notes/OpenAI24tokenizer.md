---
category: literaturenote
tags: ml/genAI
citekey: OpenAI24tokenizer
status: unread
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Tokenizer
  - Tokenizer
publisher: ""
citation key: OpenAI24tokenizer
DOI: ""
created date: 2024-04-10T18:04:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/6ECLDZVM)   | [**URL**](https://platform.openai.com) | [[OpenAI24tokenizer.pdf|PDF]]
>
> 
> **Abstract**
> Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.
> 
> 
> **FirstAuthor**:: OpenAI  
~    
> **Title**:: "Tokenizer"  
> **Date**:: 2024-04-01  
> **Citekey**:: OpenAI24tokenizer  
> **ZoteroItemKey**:: 6ECLDZVM  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://platform.openai.com  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> OpenAI. “Tokenizer.” _OpenAI_, Apr. 2024, [https://platform.openai.com](https://platform.openai.com).
%% begin Obsidian Notes %%
___
> [!note]- Zotero Note (1)
> OpenAI24tokenizer
> 
> Fun website showing how OpenAI tokenizes. Shows how the words are split, how words w/ same spelling get different tokens (bear and bear, verb/noun, so POS parsing), prefixes and punctuation gets coded (codes are displayed, so I could try to look them up, and make a slide showing the difference.  
>   
> For example:
> 
> > “She wanted to read ‘I can’t’ as a single word, but the tokenizer insisted on splitting it into three tokens: ‘I,’ ‘can,’ and ‘t’.”
> 
> See that GPT4, GPT3.5 has fewer tokens (37) than GPT3 (54) for this sentence:
> 
> Here’s another:
> 
> > "While hiking, we spotted a large brown bear lumbering through the trees. We had to bear right at the next fork to avoid its path."
> 
> There, bear and bear get different tokens.  Interesting how “While is grouped.
> 
> I could make some slides with OpenAI’s free tokenizer package, [[tiktoken]].  There’s a [python notebook](zotero://select/library/items/H24M82K9) for this in OpenAI’s [openai-cookbook](https://github.com/openai/openai-cookbook/tree/main) GitHub repo.
> 
> <small>📝️ (modified: 2024-04-10) [link](zotero://select/library/items/H24M82K9) - [web](http://zotero.org/users/60638/items/H24M82K9)</small>
>  
> ---
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> OpenAI24tokenizer
> 
> Fun website showing how OpenAI tokenizes. Shows how the words are split, how words w/ same spelling get different tokens (bear and bear, verb/noun, so POS parsing), prefixes and punctuation gets coded (codes are displayed, so I could try to look them up, and make a slide showing the difference.  
>   
> For example:
> 
> > “She wanted to read ‘I can’t’ as a single word, but the tokenizer insisted on splitting it into three tokens: ‘I,’ ‘can,’ and ‘t’.”
> 
> See that GPT4, GPT3.5 has fewer tokens (37) than GPT3 (54) for this sentence:
> 
> Here’s another:
> 
> > "While hiking, we spotted a large brown bear lumbering through the trees. We had to bear right at the next fork to avoid its path."
> 
> There, bear and bear get different tokens.  Interesting how “While is grouped.
> 
> I could make some slides with OpenAI’s free tokenizer package, [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file).  There’s a [python notebook](zotero://select/library/items/H24M82K9) for this in OpenAI’s [openai-cookbook](https://github.com/openai/openai-cookbook/tree/main) GitHub repo.
> 
> <small>📝️ (modified: 2024-04-10) [link](zotero://select/library/items/H24M82K9) - [web](http://zotero.org/users/60638/items/H24M82K9)</small>
>  
> ---




%% Import Date: 2024-04-10T18:05:34.660-07:00 %%

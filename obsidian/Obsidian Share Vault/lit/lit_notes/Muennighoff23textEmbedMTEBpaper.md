---
category: literaturenote
tags:
  - ml/embed
  - ml/genAI
citekey: Muennighoff23textEmbedMTEBpaper
status:
  - ?read
dateread: 
ZoteroTags: ""
aliases:
  - "MTEB: Massive Text Embedding Benchmark"
  - "MTEB: Massive Text Embedding Benchmark"
publisher: ""
citation key: Muennighoff23textEmbedMTEBpaper
DOI: 10.48550/arXiv.2210.07316
created date: 2024-04-08T21:13:58-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/IMI2BG74)  | [**DOI**](https://doi.org/10.48550/arXiv.2210.07316)  | [**URL**](http://arxiv.org/abs/2210.07316) | [[Muennighoff23MTEBMassiveText.pdf|PDF]]
>
> 
> **Abstract**
> Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.
> 
> 
> **FirstAuthor**:: Muennighoff, Niklas  
> **Author**:: Tazi, Nouamane  
> **Author**:: Magne, Loïc  
> **Author**:: Reimers, Nils  
~    
> **Title**:: "MTEB: Massive Text Embedding Benchmark"  
> **Date**:: 2023-03-19  
> **Citekey**:: Muennighoff23textEmbedMTEBpaper  
> **ZoteroItemKey**:: IMI2BG74  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2210.07316  
> **URL**:: http://arxiv.org/abs/2210.07316  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: 
>**Related**:: 

> Muennighoff, Niklas, et al. _MTEB: Massive Text Embedding Benchmark_. arXiv:2210.07316, arXiv, 19 Mar. 2023. _arXiv.org_, [https://doi.org/10.48550/arXiv.2210.07316](https://doi.org/10.48550/arXiv.2210.07316).
%% begin Obsidian Notes %%
___
Describes data, tasks, and metrics for the [[HuggingFace24leaderboardMTEB|MTEB Leaderboard]].  A bit of this paper might have been extracted to [[Muennighoff23textEmbedMTEBpaper|MTEB: Massive Text Embedding Benchmark]]

**Scores**
- [ ] ? How was task model effect was separated from the effect of embedding model, since different embeddings require different models: See [[SwimmTeam24embedInML#Downstream models]]
- Each task has it's own scoring metric
- [ ] ? there's an "MTEB Score".  Is this an average of scores across all tasks, normalized somehow?

Comment: 24 pages, 14 tables, 6 figures
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Comment: 24 pages, 14 tables, 6 figures
> 
> <small>📝️ (modified: 2024-04-08) [link](zotero://select/library/items/6X3UKNF3) - [web](http://zotero.org/users/60638/items/6X3UKNF3)</small>
>  
> ---




%% Import Date: 2024-04-08T21:14:16.590-07:00 %%

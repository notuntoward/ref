---
category: literaturenote
tags: ml/genAI
citekey: Pandey24beyondChinchillaOptScalel
status: unread
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - MosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference
  - MosaicML Announces Beyond Chinchilla-Optimal for
publisher: ""
citation key: Pandey24beyondChinchillaOptScalel
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/XJ6MPFZU)   | [**URL**](https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/) | [[Pandey24beyondChinchillaOptScalel.pdf|PDF]]
>
> 
> **Abstract**
> MosaicML has unveiled their latest research, titled "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws.
> 
> 
> **FirstAuthor**:: Pandey, Mohit  
~    
> **Title**:: "MosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference"  
> **Date**:: 2024-01-02  
> **Citekey**:: Pandey24beyondChinchillaOptScalel  
> **ZoteroItemKey**:: XJ6MPFZU  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Pandey, Mohit. “MosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference.” _Analytics India Magazine_, 2 Jan. 2024, [https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/](https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/). 

%% begin Obsidian Notes %%
___
Oh, Chinchilla is only a trade between training tokens and training tokens (I think).  The proposal here is to also include inference costs.

Note that Ambitious student(?) plants to train LLamma2 (1.1T params) on 3T tokens, about the ratio estimated from a Sam Altman statement

**Link all this up! and correct my earlier chinchilla misunderstanding.**

The paper this article talks about is: [[Sardana23beyondChinchillaOptInference|Beyond Chinchilla-Optimal: Accounting for Inference]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Pandey24beyondChinchillaOptScalel
> 
> Oh, Chinchilla is only a trade between training tokens and training tokens (I think).  The proposal here is to also include inference costs.
> 
> Note that Ambitious student(?) plants to train LLamma2 (1.1T params) on 3T tokens, about the ratio estimated from a Sam Altman statement
> 
> **Link all this up! and correct my earlier chinchilla misunderstanding.**
> 
> The paper this article talks about is: ([Sardana and Frankle, 2023](zotero://select/library/items/8SFHN778))
> 
> <small>📝️ (modified: 2024-04-11) [link](zotero://select/library/items/B7HFHAAM) - [web](http://zotero.org/users/60638/items/B7HFHAAM)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:26.009-07:00 %%

---
category: literaturenote
tags: ml/genAI
citekey: Pandey24beyondChinchillaOptScalel
status: unread
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - MosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference
  - MosaicML Announces Beyond Chinchilla-Optimal for
publisher: ""
citation key: Pandey24beyondChinchillaOptScalel
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/XJ6MPFZU)   | [**URL**](https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/) | [[Pandey24beyondChinchillaOptScalel.pdf|PDF]]
>
> 
> **Abstract**
> MosaicML has unveiled their latest research, titled "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws.
> 
> 
> **FirstAuthor**:: Pandey, Mohit  
~    
> **Title**:: "MosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference"  
> **Date**:: 2024-01-02  
> **Citekey**:: Pandey24beyondChinchillaOptScalel  
> **ZoteroItemKey**:: XJ6MPFZU  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Pandey, Mohit. â€œMosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference.â€ _Analytics India Magazine_, 2 Jan. 2024, [https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/](https://analyticsindiamag.com/mosaicml-announces-beyond-chinchilla-optimal-for-llm-scaling-laws-in-inference/). 

%% begin Obsidian Notes %%
___
Oh, Chinchilla is only a trade between training tokens and training tokens (I think). Â The proposal here is to also include inference costs.

Note that Ambitious student(?) plants to train LLamma2 (1.1T params) on 3T tokens, about the ratio estimated from a Sam Altman statement

**Link all this up! and correct my earlier chinchilla misunderstanding.**

The paper this article talks about is: [[Sardana23beyondChinchillaOptInference|Beyond Chinchilla-Optimal: Accounting for Inference]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Pandey24beyondChinchillaOptScalel
> 
> Oh, Chinchilla is only a trade between training tokens and training tokens (I think). Â The proposal here is to also include inference costs.
> 
> Note that Ambitious student(?) plants to train LLamma2 (1.1T params) on 3T tokens, about the ratio estimated from a Sam Altman statement
> 
> **Link all this up! and correct my earlier chinchilla misunderstanding.**
> 
> The paper this article talks about is: ([Sardana and Frankle, 2023](zotero://select/library/items/8SFHN778))
> 
> <small>ğŸ“ï¸ (modified: 2024-04-11) [link](zotero://select/library/items/B7HFHAAM) - [web](http://zotero.org/users/60638/items/B7HFHAAM)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:26.009-07:00 %%

---
category: literaturenote
tags:
  - ml/genAI
citekey: Villalobos22outOfDataLLM
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning
  - Will we run out of
publisher: ""
citation key: Villalobos22outOfDataLLM
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/MAT7EU3N)   | [**URL**](http://arxiv.org/abs/2211.04325) | [[Villalobos22WillWeRun.pdf|PDF]]
>
> 
> **Abstract**
> We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.
> 
> 
> **FirstAuthor**:: Villalobos, Pablo  
> **Author**:: Sevilla, Jaime  
> **Author**:: Heim, Lennart  
> **Author**:: Besiroglu, Tamay  
> **Author**:: Hobbhahn, Marius  
> **Author**:: Ho, Anson  
~    
> **Title**:: "Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning"  
> **Date**:: 2022-10-25  
> **Citekey**:: Villalobos22outOfDataLLM  
> **ZoteroItemKey**:: MAT7EU3N  
> **itemType**:: preprint  
> **DOI**::   
> **URL**:: http://arxiv.org/abs/2211.04325  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Villalobos, Pablo, et al. _Will We Run out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning_. arXiv:2211.04325, arXiv, 25 Oct. 2022. _arXiv.org_, [http://arxiv.org/abs/2211.04325](http://arxiv.org/abs/2211.04325).
%% begin Obsidian Notes %%

___
Yes, 
- of [[Villalobos22WillWeRun.pdf#page=1&annotation=457R|high-quality language]] data, by 2026.
- [[Villalobos22WillWeRun.pdf#page=1&annotation=493R|low quality language data and image]] data
	- language: [[Villalobos22WillWeRun.pdf#page=1&annotation=471R|between 2030 and 2050]]
	- image: [[Villalobos22WillWeRun.pdf#page=1&annotation=471R|between 2030 and 2050]]
- So, model progress might [[Villalobos22WillWeRun.pdf#page=1&annotation=480R|might slow down if data efÔ¨Åciency is not drastically improved]] or new data found

___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Villalobos22outOfDataLLM
> 
> Yes, of high quality text data, by 2026.
> 
> <small>üìùÔ∏è (modified: 2024-04-11) [link](zotero://select/library/items/VRIHTCDK) - [web](http://zotero.org/users/60638/items/VRIHTCDK)</small>
>  
> ---


## Annotations%% begin annotations %%



### Imported: 2024-04-12 6:44 pm



<mark style="background-color: #086ddd">Quote</mark>
> high-quality language

<mark style="background-color: #ffd000">Quote</mark>
> likely before 2026.

<mark style="background-color: #086ddd">Quote</mark>
> low quality language 2

<mark style="background-color: #ffd000">Quote</mark>
> etween 2030 and 2050

<mark style="background-color: #ffd000">Quote</mark>
> 2030 and 2060

<mark style="background-color: #086ddd">Quote</mark>
> images)

<mark style="background-color: #086ddd">Quote</mark>
> might slow down if data efficiency is not drastically improved

<mark style="background-color: #086ddd">Quote</mark>
> r new sources


%% end annotations %%



%% Import Date: 2024-04-12T18:47:26.217-07:00 %%

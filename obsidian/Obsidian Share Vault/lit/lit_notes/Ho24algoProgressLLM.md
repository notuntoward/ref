---
category: literaturenote
tags:
  - ml/genAI
citekey: Ho24algoProgressLLM
status:
  - ?read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Algorithmic Progress in Language Models
  - Algorithmic Progress in Language Models
publisher: ""
citation key: Ho24algoProgressLLM
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/DHE2VWHK)   | [**URL**](https://epochai.org/blog/algorithmic-progress-in-language-models) | [[Ho24algoProgressLLM.html|HTM]]
>
> 
> **Abstract**
> We study how algorithmic improvements and increases in computational power have improved the performance of language models from 2014 to 2024. We find that the progress from new algorithms surpasses what we’d expect from merely increasing our computing resources, occurring at a pace equivalent to doubling computational power every 5 to 14 months.
> 
> 
> **FirstAuthor**:: Ho, Anson  
~    
> **Title**:: "Algorithmic Progress in Language Models"  
> **Date**:: 2024-03-12  
> **Citekey**:: Ho24algoProgressLLM  
> **ZoteroItemKey**:: DHE2VWHK  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://epochai.org/blog/algorithmic-progress-in-language-models  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Ho, Anson. “Algorithmic Progress in Language Models.” _Epoch_, 12 Mar. 2024, [https://epochai.org/blog/algorithmic-progress-in-language-models](https://epochai.org/blog/algorithmic-progress-in-language-models).
%% begin Obsidian Notes %%
___

I can’t quite make sense of this page/paper.  It seems to say that both algorithms are important to the progress of LLM performance, and that they are insignificant recently, compared to the gains due to increased compute (in spite of Fig. 3, which shows the opposite, AFIK).

_Anyway_…

- Transformers are estimated to have advanced progress by 2 years
- The Chinchilla scaling laws have improved it by 8-16 months.
- TODO: figure this out.
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Ho24algoProgressLLM
> 
> I can’t quite make sense of this page/paper.  It seems to say that both algorithms are important to the progress of LLM performance, and that they are insignificant recently, compared to the gains due to increased compute (in spite of Fig. 3, which shows the opposite, AFIK).
> 
> _Anyway_…
> 
> - Transformers are estimated to have advanced progress by 2 years
> - The Chinchilla scaling laws have improved it by 8-16 months.  AFIK these seem to be about optimal profits if you are paying for both inference and training.  You get more profit if you have the right amount of training data?
> - TODO: figure this out.
> 
> <small>📝️ (modified: 2024-04-11) [link](zotero://select/library/items/W5JCHZPZ) - [web](http://zotero.org/users/60638/items/W5JCHZPZ)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:25.616-07:00 %%

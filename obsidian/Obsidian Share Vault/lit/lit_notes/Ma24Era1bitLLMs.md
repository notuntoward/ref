---
category: literaturenote
tags:
  - ml/genAI
citekey: Ma24Era1bitLLMs
status:
  - read
dateread: 
ZoteroTags: /unread, todo, obsLitNote
aliases:
  - "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
  - "The Era of 1-bit LLMs:"
publisher: ""
citation key: Ma24Era1bitLLMs
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/N8982IUT)   | [**URL**](https://arxiv.org/abs/2402.17764v1) | [[Ma24Era1bitLLMs.pdf|PDF]]
>
> 
> **Abstract**
> Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
> 
> 
> **FirstAuthor**:: Ma, Shuming  
> **Author**:: Wang, Hongyu  
> **Author**:: Ma, Lingxiao  
> **Author**:: Wang, Lei  
> **Author**:: Wang, Wenhui  
> **Author**:: Huang, Shaohan  
> **Author**:: Dong, Li  
> **Author**:: Wang, Ruiping  
> **Author**:: Xue, Jilong  
> **Author**:: Wei, Furu  
~    
> **Title**:: "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"  
> **Date**:: 2024-02-27  
> **Citekey**:: Ma24Era1bitLLMs  
> **ZoteroItemKey**:: N8982IUT  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://arxiv.org/abs/2402.17764v1  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: /unread, todo, obsLitNote
>**Related**:: 

> Ma, Shuming, et al. â€œThe Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits.â€ _arXiv.Org_, 27 Feb. 2024, [https://arxiv.org/abs/2402.17764v1](https://arxiv.org/abs/2402.17764v1).
%% begin Obsidian Notes %%
___
A 1.58 bit LLM matches a 16 bit floating point LLM (transformer, I think) in terms of compelexity. Â Doesnâ€™t multiply, but manipulates signs ==> something needed other than a GPU.

Google uses 8 or fewer bits for its TPUs: [[Sato17googleFirstTPU|An in-depth look at Googleâ€™s first Tensor Processing Unit (TPU)]]

Good slides images, combine with  [[Sato17googleFirstTPU|An in-depth look at Googleâ€™s first Tensor Processing Unit (TPU)]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Ma24Era1bitLLMs
> 
> A 1.58 bit LLM matches a 16 bit floating point LLM (transformer, I think) in terms of compelexity. Â Doesnâ€™t multiply, but manipulates signs ==> something needed other than a GPU.
> 
> Google uses 8 or fewer bits for its TPUs: ([Sato and Young, 2017](zotero://select/library/items/9VHYI2U3))
> 
> Good slides images, combine with ([Sato and Young, 2017](zotero://select/library/items/9VHYI2U3))
> 
> <small>ğŸ“ï¸ (modified: 2024-03-15) [link](zotero://select/library/items/IYHT8S3Q) - [web](http://zotero.org/users/60638/items/IYHT8S3Q)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:25.819-07:00 %%

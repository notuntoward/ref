---
category: literaturenote
tags: 
citekey: Cahyawijaya24humanValuesVecLLM
status: unread
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - High-Dimension Human Value Representation in Large Language Models
  - High-Dimension Human Value Representation in
publisher: ""
citation key: Cahyawijaya24humanValuesVecLLM
DOI: 10.48550/arXiv.2404.07900
created date: 2024-04-24T15:56:18-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/27UZKMBG)  | [**DOI**](https://doi.org/10.48550/arXiv.2404.07900)  | [**URL**](http://arxiv.org/abs/2404.07900) | [[Cahyawijaya24humanValuesVecLLM.pdf|PDF]]
>
> 
> **Abstract**
> The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.
> 
> 
> **FirstAuthor**:: Cahyawijaya, Samuel  
> **Author**:: Chen, Delong  
> **Author**:: Bang, Yejin  
> **Author**:: Khalatbari, Leila  
> **Author**:: Wilie, Bryan  
> **Author**:: Ji, Ziwei  
> **Author**:: Ishii, Etsuko  
> **Author**:: Fung, Pascale  
~    
> **Title**:: "High-Dimension Human Value Representation in Large Language Models"  
> **Date**:: 2024-04-11  
> **Citekey**:: Cahyawijaya24humanValuesVecLLM  
> **ZoteroItemKey**:: 27UZKMBG  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2404.07900  
> **URL**:: http://arxiv.org/abs/2404.07900  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
> **Related**:: 

> Cahyawijaya, Samuel, et al. _High-Dimension Human Value Representation in Large Language Models_. arXiv:2404.07900, arXiv, 11 Apr. 2024. _arXiv.org_, [https://doi.org/10.48550/arXiv.2404.07900](https://doi.org/10.48550/arXiv.2404.07900)
%% begin Obsidian Notes %%
___

Somehow high dimensional vectors are extracted which represent human values (what is good; what is bad, I suppose).  

I have no idea…

But this is supposed to make it cheap to align LLMs without expensive reannotation of good and bad responses, as in reinforcement learning, I think it says.

Again, I have no idea…

Related: [[Bhargava24controlTheoryLLM|LLM Control Theory: What's the]]: control theory speculated to help fine high level emotional characteristics in activation space, control them w/ prompts.
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Cahyawijaya24humanValuesVecLLM
> 
> Somehow high dimensional vectors are extracted which represent human values (what is good; what is bad, I suppose).  
> 
> I have no idea…
> 
> But this is supposed to make it cheap to align LLMs without expensive reannotation of good and bad responses, as in reinforcement learning, I think it says.
> 
> Again, I have no idea…
> 
> <small>📝️ (modified: 2024-04-17) [link](zotero://select/library/items/YC6LEHWM) - [web](http://zotero.org/users/60638/items/YC6LEHWM)</small>
>  
> ---




%% Import Date: 2024-04-24T15:57:27.831-07:00 %%

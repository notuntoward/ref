---
category: literaturenote
tags:
  - ml/genAI
citekey: Jain24LiveCodeBenchHolisticNoCntam
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"
  - "LiveCodeBench: Holistic and Contamination Free"
publisher: ""
citation key: Jain24LiveCodeBenchHolisticNoCntam
DOI: 10.48550/arXiv.2403.07974
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/TZZX25Z5)  | [**DOI**](https://doi.org/10.48550/arXiv.2403.07974)  | [**URL**](http://arxiv.org/abs/2403.07974) | [[Jain24LiveCodeBenchHolisticNoCntam.pdf|PDF]]
>
> 
> **Abstract**
> Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model
> 
> 
> **FirstAuthor**:: Jain, Naman  
> **Author**:: Han, King  
> **Author**:: Gu, Alex  
> **Author**:: Li, Wen-Ding  
> **Author**:: Yan, Fanjia  
> **Author**:: Zhang, Tianjun  
> **Author**:: Wang, Sida  
> **Author**:: Solar-Lezama, Armando  
> **Author**:: Sen, Koushik  
> **Author**:: Stoica, Ion  
~    
> **Title**:: "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"  
> **Date**:: 2024-03-12  
> **Citekey**:: Jain24LiveCodeBenchHolisticNoCntam  
> **ZoteroItemKey**:: TZZX25Z5  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2403.07974  
> **URL**:: http://arxiv.org/abs/2403.07974  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Jain, Naman, et al. _LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code_. arXiv:2403.07974, arXiv, 12 Mar. 2024. _arXiv.org_, [https://doi.org/10.48550/arXiv.2403.07974](https://doi.org/10.48550/arXiv.2403.07974).
%% begin Obsidian Notes %%
___

*The* paper describing the benchmark. Â Corresponding leaderboard and easy-to-read summary at [[Jain24liveCodeBenchLeaderboard|LiveCodeBench Leaderboard]]

- Evaluated on [[Jain24LiveCodeBenchHolisticNoCntam.pdf#page=5&annotation=1684R|Pass@1]], a [[Jain24LiveCodeBenchHolisticNoCntam.pdf#page=5&annotation=1687R|metric measured as the fraction of the problems for which the model was able to generate a program passing all tests]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Jain24LiveCodeBenchHolisticNoCntam
> 
> The paper describing the benchmark. Â Corresponding leaderboard and easy-to-read summary at ([Jain, 2024](zotero://select/library/items/3BPQUND9))
> 
> Comment: Website - https://livecodebench.github.io/
> 
> <small>ğŸ“ï¸ (modified: 2024-04-11) [link](zotero://select/library/items/JLPX4QQU) - [web](http://zotero.org/users/60638/items/JLPX4QQU)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:25.694-07:00 %%

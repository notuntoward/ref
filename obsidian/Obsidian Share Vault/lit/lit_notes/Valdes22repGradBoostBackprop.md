---
category: literaturenote
tags: 
citekey: Valdes22repGradBoostBackprop
status: unread
dateread: 
ZoteroTags: ""
aliases:
  - "Representational Gradient Boosting: Backpropagation in the Space of Functions"
  - "Representational Gradient Boosting: Backpropagation in"
publisher: IEEE Transactions on Pattern Analysis and Machine Intelligence
citation key: Valdes22repGradBoostBackprop
DOI: 10.1109/TPAMI.2021.3137715
created date: 2024-06-25T11:49:39-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/JNLBD8QH)  | [**DOI**](https://doi.org/10.1109/TPAMI.2021.3137715)  | [**URL**](https://ieeexplore.ieee.org/document/9661330) | [[Valdes22repGradBoostBackprop.pdf|PDF]]
>
> 
> **Abstract**
> The estimation of nested functions (i.e., functions of functions) is one of the central reasons for the success and popularity of machine learning. Today, artificial neural networks are the predominant class of algorithms in this area, known as representational learning. Here, we introduce Representational Gradient Boosting (RGB), a nonparametric algorithm that estimates functions with multi-layer architectures obtained using backpropagation in the space of functions. RGB does not need to assume a functional form in the nodes or output (e.g., linear models or rectified linear units), but rather estimates these transformations. RGB can be seen as an optimized stacking procedure where a meta algorithm learns how to combine different classes of functions (e.g., Neural Networks (NN) and Gradient Boosting (GB)), while building and optimizing them jointly in an attempt to compensate each other’s weaknesses. This highlights a stark difference with current approaches to meta-learning that combine models only after they have been built independently. We showed that providing optimized stacking is one of the main advantages of RGB over current approaches. Additionally, due to the nested nature of RGB we also showed how it improves over GB in problems that have several high-order interactions. Finally, we investigate both theoretically and in practice the problem of recovering nested functions and the value of prior knowledge.
> 
> 
> **FirstAuthor**:: Valdes, Gilmer  
> **Author**:: Friedman, Jerome H.  
> **Author**:: Jiang, Fei  
> **Author**:: Gennatas, Efstathios D.  
~    
> **Title**:: "Representational Gradient Boosting: Backpropagation in the Space of Functions"  
> **Date**:: 2022-12-01  
> **Citekey**:: Valdes22repGradBoostBackprop  
> **ZoteroItemKey**:: JNLBD8QH  
> **itemType**:: journalArticle  
> **DOI**:: 10.1109/TPAMI.2021.3137715  
> **URL**:: https://ieeexplore.ieee.org/document/9661330  
> **Journal**:: IEEE Transactions on Pattern Analysis and Machine Intelligence  
> **Volume**:: 44  
> **Issue**:: 12  
> **Book**:: IEEE Transactions on Pattern Analysis and Machine Intelligence  
> **Publisher**::   
> **Location**::    
> **Pages**:: 10186-10195  
> **ISBN**::   
> **ZoteroTags**:: 
> **Related**:: 

> Valdes, Gilmer, et al. “Representational Gradient Boosting: Backpropagation in the Space of Functions.” _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 44, no. 12, Dec. 2022, pp. 10186–95. _IEEE Xplore_, [https://doi.org/10.1109/TPAMI.2021.3137715](https://doi.org/10.1109/TPAMI.2021.3137715).
%% begin Obsidian Notes %%
___
Somehow uses backprop do do gradient boosting.  A way to get [[LightGBM]] into [[Contextual Optimization]]?

Related: [[Zhang21LrnMultiLayerBDTbackprop]]

==Delete this and write here.==
==Don't delete the `persist` directives above and below.==
___
%% end Obsidian Notes %%



%% Import Date: 2024-06-25T11:49:48.328-07:00 %%

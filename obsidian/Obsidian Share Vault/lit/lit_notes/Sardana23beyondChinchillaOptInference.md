---
category: literaturenote
tags: ml/genAI
citekey: Sardana23beyondChinchillaOptInference
status: unread
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws"
  - "Beyond Chinchilla-Optimal: Accounting for Inference"
publisher: ""
citation key: Sardana23beyondChinchillaOptInference
DOI: 10.48550/arXiv.2401.00448
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/8SFHN778)  | [**DOI**](https://doi.org/10.48550/arXiv.2401.00448)  | [**URL**](http://arxiv.org/abs/2401.00448) | [[Sardana23ChinchillaOptimalAccountingInference.pdf|PDF]]
>
> 
> **Abstract**
> Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular DeepMind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal.
> 
> 
> **FirstAuthor**:: Sardana, Nikhil  
> **Author**:: Frankle, Jonathan  
~    
> **Title**:: "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws"  
> **Date**:: 2023-12-31  
> **Citekey**:: Sardana23beyondChinchillaOptInference  
> **ZoteroItemKey**:: 8SFHN778  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2401.00448  
> **URL**:: http://arxiv.org/abs/2401.00448  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Sardana, Nikhil, and Jonathan Frankle. _Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws_. arXiv:2401.00448, arXiv, 31 Dec. 2023. _arXiv.org_, [https://doi.org/10.48550/arXiv.2401.00448](https://doi.org/10.48550/arXiv.2401.00448).
%% begin Obsidian Notes %%
___
The paper for [[Pandey24beyondChinchillaOptScalel|MosaicML Announces Beyond Chinchilla-Optimal for LLM Scaling Laws in Inference]]


Comment: 8 pages, 2 figures, To appear in the 3rd NeurIPS Workshop on Efficient Natural Language and Speech Processing (ENLSP), 2023
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Sardana23beyondChinchillaOptInference
> 
> The paper for ([Pandey, 2024](zotero://select/library/items/XJ6MPFZU))
> 
> Comment: 8 pages, 2 figures, To appear in the 3rd NeurIPS Workshop on Efficient Natural Language and Speech Processing (ENLSP), 2023
> 
> <small>📝️ (modified: 2024-04-11) [link](zotero://select/library/items/QB389Q9T) - [web](http://zotero.org/users/60638/items/QB389Q9T)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:26.159-07:00 %%

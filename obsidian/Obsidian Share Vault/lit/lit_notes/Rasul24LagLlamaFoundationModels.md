---
category: literaturenote
tags: ml/genAI
citekey: Rasul24LagLlamaFoundationModels
status: unread
dateread: 
ZoteroTags: /unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning
aliases:
  - "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting"
  - "Lag-Llama: Towards Foundation Models for"
publisher: ""
citation key: Rasul24LagLlamaFoundationModels
DOI: 10.48550/arXiv.2310.08278
created date: 2024-04-07T22:52:59-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/6YMD8D3K)  | [**DOI**](https://doi.org/10.48550/arXiv.2310.08278)  | [**URL**](http://arxiv.org/abs/2310.08278) | [[Rasul24LagLlamaFoundationTSfrcst.pdf|PDF]]
>
> 
> **Abstract**
> Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.
> 
> 
> **FirstAuthor**:: Rasul, Kashif  
> **Author**:: Ashok, Arjun  
> **Author**:: Williams, Andrew Robert  
> **Author**:: Ghonia, Hena  
> **Author**:: Bhagwatkar, Rishika  
> **Author**:: Khorasani, Arian  
> **Author**:: Bayazi, Mohammad Javad Darvishi  
> **Author**:: Adamopoulos, George  
> **Author**:: Riachi, Roland  
> **Author**:: Hassen, Nadhir  
> **Author**:: Biloš, Marin  
> **Author**:: Garg, Sahil  
> **Author**:: Schneider, Anderson  
> **Author**:: Chapados, Nicolas  
> **Author**:: Drouin, Alexandre  
> **Author**:: Zantedeschi, Valentina  
> **Author**:: Nevmyvaka, Yuriy  
> **Author**:: Rish, Irina  
~    
> **Title**:: "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting"  
> **Date**:: 2024-02-08  
> **Citekey**:: Rasul24LagLlamaFoundationModels  
> **ZoteroItemKey**:: 6YMD8D3K  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2310.08278  
> **URL**:: http://arxiv.org/abs/2310.08278  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: /unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning
>**Related**:: 

> Rasul, Kashif, et al. _Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting_. arXiv:2310.08278, arXiv, 8 Feb. 2024. _arXiv.org_, [https://doi.org/10.48550/arXiv.2310.08278](https://doi.org/10.48550/arXiv.2310.08278).
%% begin Obsidian Notes %%
___
==Delete this and write here.==
==Don't delete the `persist` directives above and below.==
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Rasul24LagLlamaFoundationModels
> 
> LLM forecast model.  Lots of tutorials on it, plus a GitHub
> 
> Comment: First two authors contributed equally. All data, models and code used are open-source. GitHub: https://github.com/time-series-foundation-models/lag-llama
> 
> <small>📝️ (modified: 2024-04-07) [link](zotero://select/library/items/TC9IF9UG) - [web](http://zotero.org/users/60638/items/TC9IF9UG)</small>
>  
> ---




%% Import Date: 2024-04-07T22:53:10.432-07:00 %%

---
category: literaturenote
tags: 
citekey: Epoch23MachineLearningTrends
status: unread
dateread: 
ZoteroTags: obsLitNote
aliases:
  - Key Trends and Figures in Machine Learning
  - Key Trends and Figures in
publisher: ""
citation key: Epoch23MachineLearningTrends
DOI: ""
created date: 2024-04-15T16:33:38-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/N5N44KSZ)   | [**URL**](https://epochai.org/trends) | [[Epoch23MachineLearningTrends.html|HTM]]
>
> 
> **Abstract**
> Explore Epoch’s trends section of key numbers and data visualizations in Artificial Intelligence and machine learning, showcasing the change and growth in AI over time.
> 
> 
> **FirstAuthor**:: Epoch,   
~    
> **Title**:: "Key Trends and Figures in Machine Learning"  
> **Date**:: 2023-04-11  
> **Citekey**:: Epoch23MachineLearningTrends  
> **ZoteroItemKey**:: N5N44KSZ  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://epochai.org/trends  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: obsLitNote
>**Related**:: 

> Epoch. “Key Trends and Figures in Machine Learning.” _Epoch_, 11 Apr. 2023, [https://epochai.org/trends](https://epochai.org/trends).
%% begin Obsidian Notes %%
___

Great trend graphs, and numbers, but I can’t tell if it’s still up to date in 4/2024.

Lots of articles, and the whole website should be looked into.

### Most notable

- year most public high-quality human data will be used in some training run: 2024!
- will we run out of training data?  “Yes?”  I need to read the linked-to paper
    - there’s a lot of contradictory stuff about this on this page.  Maybe the paper will make sense of it?
    - maybe this page was generated by AI?
    - largest training set known (DBRX): 9 trillion words
    - internet data: 100T words
    - etc.
- For optimal results, need 20 tokens/param (page says it’s chinchilla scaling laws, and to see Hoffmann 2022, which doesn’t give this ### in its abstract).
- Gemini Ultra cost **$630M (**all costs) to develop.  It likely used the most train compute too:  [[Epoch24trainComputeVsTime|Training Compute of Notable machine learning Systems Over Time]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Epoch23MachineLearningTrends
> 
> Great trend graphs, and numbers, but I can’t tell if it’s still up to date in 4/2024.
> 
> Lots of articles, and the whole website should be looked into.
> 
> ### Most notable
> 
> - year most public high-quality human data will be used in some training run: 2024!
> - will we run out of training data?  “Yes?”  I need to read the linked-to paper
>     
>     - there’s a lot of contradictory stuff about this on this page.  Maye the paper will make sense of it?
>     - maybe this page was generated by AI?
>     - largest training set known (DBRX): 9 trillion words
>     - internet data: 100T words
>     - etc.
> - For optimal results, need 20 tokens/param (page says it’s chinchilla scaling laws, and to see Hoffmann 2022, which doesn’t give this ### in its abstract).
> - Gemini Ultra cost **$630M (**all costs) to develop.  It likely used the most train compute too: ([Epoch, 2024](zotero://select/library/items/EUYLH282))
> 
> <small>📝️ (modified: 2024-04-11) [link](zotero://select/library/items/LR8HL7UR) - [web](http://zotero.org/users/60638/items/LR8HL7UR)</small>
>  
> ---




%% Import Date: 2024-04-15T16:34:02.341-07:00 %%

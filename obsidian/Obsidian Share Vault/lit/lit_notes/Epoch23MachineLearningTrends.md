---
category: literaturenote
tags: 
citekey: Epoch23MachineLearningTrends
status: unread
dateread: 
ZoteroTags: obsLitNote
aliases:
  - Key Trends and Figures in Machine Learning
  - Key Trends and Figures in
publisher: ""
citation key: Epoch23MachineLearningTrends
DOI: ""
created date: 2024-04-15T16:33:38-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/N5N44KSZ)   | [**URL**](https://epochai.org/trends) | [[Epoch23MachineLearningTrends.html|HTM]]
>
> 
> **Abstract**
> Explore Epochâ€™s trends section of key numbers and data visualizations in Artificial Intelligence and machine learning, showcasing the change and growth in AI over time.
> 
> 
> **FirstAuthor**:: Epoch,   
~    
> **Title**:: "Key Trends and Figures in Machine Learning"  
> **Date**:: 2023-04-11  
> **Citekey**:: Epoch23MachineLearningTrends  
> **ZoteroItemKey**:: N5N44KSZ  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://epochai.org/trends  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: obsLitNote
>**Related**:: 

> Epoch. â€œKey Trends and Figures in Machine Learning.â€ _Epoch_, 11 Apr. 2023, [https://epochai.org/trends](https://epochai.org/trends).
%% begin Obsidian Notes %%
___

Great trend graphs, and numbers, but I canâ€™t tell if itâ€™s still up to date in 4/2024.

Lots of articles, and the whole website should be looked into.

### Most notable

- year most public high-quality human data will be used in some training run: 2024!
- will we run out of training data? Â â€œYes?â€ Â I need to read the linked-to paper
    - thereâ€™s a lot of contradictory stuff about this on this page. Â Maybe the paper will make sense of it?
    - maybe this page was generated by AI?
    - largest training set known (DBRX): 9 trillion words
    - internet data: 100T words
    - etc.
- For optimal results, need 20 tokens/param (page says itâ€™s chinchilla scaling laws, and to see Hoffmann 2022, which doesnâ€™t give this ### in its abstract).
- Gemini Ultra costÂ **$630M (**all costs) to develop. Â It likely used the most train compute too:  [[Epoch24trainComputeVsTime|Training Compute of Notable machine learning Systems Over Time]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Epoch23MachineLearningTrends
> 
> Great trend graphs, and numbers, but I canâ€™t tell if itâ€™s still up to date in 4/2024.
> 
> Lots of articles, and the whole website should be looked into.
> 
> ### Most notable
> 
> - year most public high-quality human data will be used in some training run: 2024!
> - will we run out of training data? Â â€œYes?â€ Â I need to read the linked-to paper
>     
>     - thereâ€™s a lot of contradictory stuff about this on this page. Â Maye the paper will make sense of it?
>     - maybe this page was generated by AI?
>     - largest training set known (DBRX): 9 trillion words
>     - internet data: 100T words
>     - etc.
> - For optimal results, need 20 tokens/param (page says itâ€™s chinchilla scaling laws, and to see Hoffmann 2022, which doesnâ€™t give this ### in its abstract).
> - Gemini Ultra cost **$630M (**all costs) to develop. Â It likely used the most train compute too: ([Epoch, 2024](zotero://select/library/items/EUYLH282))
> 
> <small>ğŸ“ï¸ (modified: 2024-04-11) [link](zotero://select/library/items/LR8HL7UR) - [web](http://zotero.org/users/60638/items/LR8HL7UR)</small>
>  
> ---




%% Import Date: 2024-04-15T16:34:02.341-07:00 %%

---
category: literaturenote
tags:
  - ml/genAI
citekey: Erdil24optimAllocInfrncTrn
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Optimally Allocating Compute Between Inference and Training
  - Optimally Allocating Compute Between Inference
publisher: ""
citation key: Erdil24optimAllocInfrncTrn
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/RUNJHRAU)   | [**URL**](https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training) | [[Erdil24optimAllocInfrncTrn.html|HTM]]
>
> 
> **Abstract**
> If it is feasible to trade off inference and training compute, we find that it is optimal for AI labs to spend similar amounts on training and inference.
> 
> 
> **FirstAuthor**:: Erdil, Ege  
~    
> **Title**:: "Optimally Allocating Compute Between Inference and Training"  
> **Date**:: 2024-03-29  
> **Citekey**:: Erdil24optimAllocInfrncTrn  
> **ZoteroItemKey**:: RUNJHRAU  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Erdil, Ege. “Optimally Allocating Compute Between Inference and Training.” _Epoch_, 29 Mar. 2024, [https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training](https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training).
%% begin Obsidian Notes %%
___

This may explain why Chinchilla scaling law is an advancement in AI performance.  Also, as an often asked tidbit, that training tokens are about 3X more expensive than generated inference tokens (guestimated from a Sam Altman statement about tokens/year).  So you can compare the cost of yearly train and yearly inference operation.

Also, a table of ways to cut inference cost to improve performance of the given trained model.  This may explain why people have declared that GPT3 or whatever was undertrained.

I should probably read this.
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Erdil24optimAllocInfrncTrn
> 
> This may explain why Chinchilla scaling law is an advancement in AI performance.  Also, as an often asked tidbit, that training tokens are about 3X more expensive than generated inference tokens (guestimated from a Sam Altman statement about tokens/year).  So you can compare the cost of yearly train and yearly inference operation.
> 
> Also, a table of ways to cut inference cost to improve performance of the given trained model.  This may explain why people have declared that GPT3 or whatever was undertrained.
> 
> I should probably read this.
> 
> <small>📝️ (modified: 2024-04-11) [link](zotero://select/library/items/3DM3FF2I) - [web](http://zotero.org/users/60638/items/3DM3FF2I)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:25.538-07:00 %%

---
category: literaturenote
tags:
  - ml/genAI
citekey: Erdil24optimAllocInfrncTrn
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - Optimally Allocating Compute Between Inference and Training
  - Optimally Allocating Compute Between Inference
publisher: ""
citation key: Erdil24optimAllocInfrncTrn
DOI: ""
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/RUNJHRAU)   | [**URL**](https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training) | [[Erdil24optimAllocInfrncTrn.html|HTM]]
>
> 
> **Abstract**
> If it is feasible to trade off inference and training compute, we find that it is optimal for AI labs to spend similar amounts on training and inference.
> 
> 
> **FirstAuthor**:: Erdil, Ege  
~    
> **Title**:: "Optimally Allocating Compute Between Inference and Training"  
> **Date**:: 2024-03-29  
> **Citekey**:: Erdil24optimAllocInfrncTrn  
> **ZoteroItemKey**:: RUNJHRAU  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Erdil, Ege. â€œOptimally Allocating Compute Between Inference and Training.â€ _Epoch_, 29 Mar. 2024, [https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training](https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training).
%% begin Obsidian Notes %%
___

This may explain why Chinchilla scaling law is an advancement in AI performance. Â Also, as an often asked tidbit, that training tokens are about 3X more expensive than generated inference tokens (guestimated from a Sam Altman statement about tokens/year). Â So you can compare the cost of yearly train and yearly inference operation.

Also, a table of ways to cut inference cost to improve performance of the given trained model. Â This may explain why people have declared that GPT3 or whatever was undertrained.

I should probably read this.
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Erdil24optimAllocInfrncTrn
> 
> This may explain why Chinchilla scaling law is an advancement in AI performance. Â Also, as an often asked tidbit, that training tokens are about 3X more expensive than generated inference tokens (guestimated from a Sam Altman statement about tokens/year). Â So you can compare the cost of yearly train and yearly inference operation.
> 
> Also, a table of ways to cut inference cost to improve performance of the given trained model. Â This may explain why people have declared that GPT3 or whatever was undertrained.
> 
> I should probably read this.
> 
> <small>ğŸ“ï¸ (modified: 2024-04-11) [link](zotero://select/library/items/3DM3FF2I) - [web](http://zotero.org/users/60638/items/3DM3FF2I)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:25.538-07:00 %%

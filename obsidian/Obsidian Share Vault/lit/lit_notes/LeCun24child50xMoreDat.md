---
category: literaturenote
tags:
  - ml/genAI
citekey: LeCun24child50xMoreDat
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - I've made that point before...
  - I've made that point before...
publisher: ""
citation key: LeCun24child50xMoreDat
DOI: ""
created date: 2024-05-05T09:43:25-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/U7NTFFTP)   | [**URL**](https://www.linkedin.com/posts/yann-lecun_ive-made-that-point-before-llm-1e13-activity-7156484065603280896-QH63/) | [[LeCun24child50xMoreDat.pdf|PDF]]
>
> 
> 
> **FirstAuthor**:: LeCun, Yann  
~    
> **Title**:: "I've made that point before..."  
> **Date**:: 2024-02-01  
> **Citekey**:: LeCun24child50xMoreDat  
> **ZoteroItemKey**:: U7NTFFTP  
> **itemType**:: webpage  
> **DOI**::   
> **URL**:: https://www.linkedin.com/posts/yann-lecun_ive-made-that-point-before-llm-1e13-activity-7156484065603280896-QH63/  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
> **Related**:: 

> LeCun, Yann. “I’ve Made That Point Before...” _LinkedIn_, Feb. 2024, [https://www.linkedin.com/posts/yann-lecun_ive-made-that-point-before-llm-1e13-activity-7156484065603280896-QH63/](https://www.linkedin.com/posts/yann-lecun_ive-made-that-point-before-llm-1e13-activity-7156484065603280896-QH63/).
%% begin Obsidian Notes %%
___
4 year old child has 50X more training data than current LLMs all available data would take 170 k years to read -- in  [[LeCun24animalSmarterLessDat]], he says that current LLMs trained-on text data that would take 20,000 years to read.

Says video is more redundant than text but that’s good for self-supervised learning, and that YouTube accumulated 32 k hours of video per hour.

- Do his 170k and 20k reading times agree with used and available high quality text in [[Villalobos22outOfDataLLM]] ?
- Similar to 
	- [[Wodecki24yannLeCunDitchGenAI]]
	- [[LeCun24animalSmarterLessDat]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> LeCun24child50xMoreDat
> 
> 4 year old child has 50X more training data than current LLMs all available data would take 170 k years to read -- in ([LeCun, 2024](zotero://select/library/items/CCFV2YSY)), he says that current LLMs trained-on text data that would take 20,000 years to read.
> 
> Says video is more redundant than text but that’s good for self-supervized learning, and that youtube accumulated 32khours of video per hour.
> 
> - Do his 170k and 20k reading times agree with used and available high quality text in ([Villalobos et al., 2022](zotero://select/library/items/MAT7EU3N))?
> - Similar to ([LeCun, 2024](zotero://select/library/items/CCFV2YSY)) and ([Wodecki, 2024](zotero://select/library/items/SUISKUY6))
> 
> <small>📝️ (modified: 2024-05-05) [link](zotero://select/library/items/XMHRMKH7) - [web](http://zotero.org/users/60638/items/XMHRMKH7)</small>
>  
> ---




%% Import Date: 2024-05-05T09:44:09.419-07:00 %%

---
category: literaturenote
tags:
  - ml/genAI
citekey: Rein23GPQAbenchmrkGradLevGglePrf
status:
  - read
dateread: 
ZoteroTags: todo, obsLitNote
aliases:
  - "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
  - "GPQA: A Graduate-Level Google-Proof Q&A"
publisher: ""
citation key: Rein23GPQAbenchmrkGradLevGglePrf
DOI: 10.48550/arXiv.2311.12022
created date: 2024-04-12T18:44:27-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/GPK7TZGE)  | [**DOI**](https://doi.org/10.48550/arXiv.2311.12022)  | [**URL**](http://arxiv.org/abs/2311.12022) | [[Rein23GPQAGraduateLevelGoogleProof.pdf|PDF]]
>
> 
> **Abstract**
> We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.
> 
> 
> **FirstAuthor**:: Rein, David  
> **Author**:: Hou, Betty Li  
> **Author**:: Stickland, Asa Cooper  
> **Author**:: Petty, Jackson  
> **Author**:: Pang, Richard Yuanzhe  
> **Author**:: Dirani, Julien  
> **Author**:: Michael, Julian  
> **Author**:: Bowman, Samuel R.  
~    
> **Title**:: "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"  
> **Date**:: 2023-11-20  
> **Citekey**:: Rein23GPQAbenchmrkGradLevGglePrf  
> **ZoteroItemKey**:: GPK7TZGE  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2311.12022  
> **URL**:: http://arxiv.org/abs/2311.12022  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: todo, obsLitNote
>**Related**:: 

> Rein, David, et al. _GPQA: A Graduate-Level Google-Proof Q&A Benchmark_. arXiv:2311.12022, arXiv, 20 Nov. 2023. _arXiv.org_, [https://doi.org/10.48550/arXiv.2311.12022](https://doi.org/10.48550/arXiv.2311.12022).
%% begin Obsidian Notes %%
___

An AI benchmark, so tough that PhD expert types get only [[Rein23GPQAGraduateLevelGoogleProof.pdf#page=2&selection=44,55,44,70|65%]] right; high-skilled non-experts w/ 30 mins to look stuff up on the web get 34%; GPT4 got 39% in 2023.

Best LLM score was **41.5** on 4/19/24 (GeminiPro 1.5,  [[Ghoshal24dominanceLlama3|Meta eyes LLM dominance with]]) so much better "highly skilled" humans but considerably worse than experts who had PhDs in the area being tested.

- [ ] One of the leaderboards in this group of web pages I just stored in zotero (evening 4/11/14) uses this one, I think. Â Look for the one that has a pdf of the actual leader board scores for today.

Would be interesting to look at these questions myself.
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Rein23GPQAbenchmrkGradLevGglePrf
> 
> An AI benchmark, so tough that PhD expet types get only 65% right; high-skilled experts w/ 30 mins on the web get 34%; GPT4 got 39% in 2023.
> 
> **TODO**: One of the leaderboards in this group of web pages I just stored in zotero (evening 4/11/14) uses this one, I think. Â Lood for the one that has a pdf of the actual leader board scores for toda.
> 
> Would be interesting to look at these questions myself.
> 
> Comment: 28 pages, 5 figures, 7 tables
> 
> <small>ğŸ“ï¸ (modified: 2024-04-11) [link](zotero://select/library/items/KNMPH3YG) - [web](http://zotero.org/users/60638/items/KNMPH3YG)</small>
>  
> ---




%% Import Date: 2024-04-12T18:47:26.097-07:00 %%

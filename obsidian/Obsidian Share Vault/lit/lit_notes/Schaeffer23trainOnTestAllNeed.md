---
category: literaturenote
tags:
  - ml/genAI
citekey: Schaeffer23trainOnTestAllNeed
status:
  - unread
dateread: 
ZoteroTags: obsLitNote
aliases:
  - Pretraining on the Test Set Is All You Need
  - Pretraining on the Test Set
publisher: ""
citation key: Schaeffer23trainOnTestAllNeed
DOI: 10.48550/arXiv.2309.08632
created date: 2024-05-06T14:14:13-07:00
modified date: 2024-12-17T08:51:31-08:00
---

> [!info]- : [**Zotero**](zotero://select/library/items/DNKQRBSD)  | [**DOI**](https://doi.org/10.48550/arXiv.2309.08632)  | [**URL**](http://arxiv.org/abs/2309.08632) | [[Schaeffer23PretrainingTestSet.pdf|PDF]]
>
> 
> **Abstract**
> Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL} (pronounced ``fictional") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.
> 
> 
> **FirstAuthor**:: Schaeffer, Rylan  
~    
> **Title**:: "Pretraining on the Test Set Is All You Need"  
> **Date**:: 2023-09-13  
> **Citekey**:: Schaeffer23trainOnTestAllNeed  
> **ZoteroItemKey**:: DNKQRBSD  
> **itemType**:: preprint  
> **DOI**:: 10.48550/arXiv.2309.08632  
> **URL**:: http://arxiv.org/abs/2309.08632  
> **Journal**::   
> **Volume**::   
> **Issue**::   
> **Book**::   
> **Publisher**::   
> **Location**::    
> **Pages**::   
> **ISBN**::   
> **ZoteroTags**:: obsLitNote
> **Related**:: 

> Schaeffer, Rylan. _Pretraining on the Test Set Is All You Need_. arXiv:2309.08632, arXiv, 13 Sept. 2023. _arXiv.org_, [https://doi.org/10.48550/arXiv.2309.08632](https://doi.org/10.48550/arXiv.2309.08632).
%% begin Obsidian Notes %%
___
Tiny LLM beats big ones by training on test.  Point is (I think) that we don’t know if that’s what OpenAI, etc. is doing too, even if accidentally.

Comment: 3 pages, satire

Referenced by [[Patel23WillScalingWork]]
___
%% end Obsidian Notes %%

> [!note]- Zotero Note (1)
> Schaeffer23trainOnTestAllNeed
> 
> Tiny LLM beats big ones by training on test.  Point is (I think) that we don’t know if that’s what OpenAI, etc. is doing too, even if accidentally.
> 
> Comment: 3 pages, satire
> 
> <small>📝️ (modified: 2024-05-06) [link](zotero://select/library/items/SYUN94LF) - [web](http://zotero.org/users/60638/items/SYUN94LF)</small>
>  
> ---




%% Import Date: 2024-05-06T14:14:34.316-07:00 %%

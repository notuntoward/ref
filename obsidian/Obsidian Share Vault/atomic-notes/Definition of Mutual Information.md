---
date: 2025-02-07 01:48:46
tags: atomized
created date: 2025-02-06T17:49:08-08:00
modified date: 2025-02-06T17:49:08-08:00
---
# Definition of Mutual Information
For two random variables \( (X,Y) \) with joint distribution \( P_{(X,Y)} \) and marginal distributions \( P_{X} \) and \( P_{Y} \), mutual information is defined as:
\[
I(X;Y) = D_{KL}(P_{(X,Y)} \| P_{X} \otimes P_{Y})
\]
where \( D_{KL} \) denotes the Kullback-Leibler divergence.
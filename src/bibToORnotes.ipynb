{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "* parse my markdownish headlining\n",
    "  - 1st level heading: single line of text, no leading starts, maybe short too, no leading spaces\n",
    "  - 1st level heading if already has a leading star\n",
    "  - 2nd level heading: leading `o ` , say one line long\n",
    "  - 2nd level heading '--' and had leading '*'\n",
    "  - 2nd level heading if leading '**'\n",
    "  - 3rd level heading: leading ` -`\n",
    "  - 3rd level heading: leading '***'\n",
    "  - 4th level heading: leading ' --' and no leading star\n",
    "  - 4th level heading: if leading '****'\n",
    "  - 5th level          leading ' ----`\n",
    "* TODO: but org-roam-ui removes *'s, seems to render markdown bullets?, so '-' becomes a bullet?\n",
    "* org-roam-ui ignores #+OPTIONS: H:0 num:0\n",
    "* TODO: some way to control org-roam UI's org rendering?\n",
    "\n",
    "BUT check a few notes to see how consistent I was\n",
    "  \n",
    "* Convert British spelling to American?\n",
    "I only found one example where this split a tag: \"optimization\".\n",
    "quick hack:\n",
    "https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english\n",
    "https://stackoverflow.com/questions/18840640/python-2-7-find-and-replace-from-text-file-using-dictionary-to-new-text-file\n",
    "\n",
    "* make org-id from note's \"date created\" field if available.  If not, use paper year + something.  Also, just use fractions of UTC clock from current computer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "from dateutil.parser import parse\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\ref\\energy.bib\n"
     ]
    }
   ],
   "source": [
    "refDir = Path(\"~/ref\").expanduser()\n",
    "bibDirBase = refDir\n",
    "\n",
    "bibInNm = \"deepSolarDOE\"\n",
    "bibInNm = \"newTechAdopt\"\n",
    "bibInNm = \"energy\"\n",
    "\n",
    "bibInFNm = bibDirBase / f\"{bibInNm}.bib\"\n",
    "noteOutDir = Path(\"~/share/tmp_org_roam_test\").expanduser()\n",
    "noteOutDir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for colapsing/replacing whitespace\n",
    "matchWhiteSpace_regexp = re.compile(r\"\\s+\")\n",
    "# for de-orgifying orig .bib comments\n",
    "matchLeadStars_regexp = re.compile(r\"\\s+\")\n",
    "\n",
    "print(bibInFNm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read source .bib as string so can handle parsing in a separate step, avoiding\n",
    "# error throwing on unquoted .bib fields to right of '='.\n",
    "# (If bibtexparser were to write them out, they would be restored as in the\n",
    "# original bib file).\n",
    "\n",
    "with open(bibInFNm, encoding=\"utf8\") as bibtex_file:\n",
    "    bibtex_str = bibtex_file.read()\n",
    "\n",
    "bp = BibTexParser(interpolate_strings=False)\n",
    "bib_database = bp.parse(bibtex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write bib fields to org-roam notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = wordnet.morphy(\"test\")\n",
    "except:\n",
    "    print(\"Installing nltk data\")\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    print(\"Done installing nltk data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping 287 of 3322 unique keywords\n"
     ]
    }
   ],
   "source": [
    "# Collect all bibtex keys in the .bib file, and make roam_tags strings\n",
    "bibkeysAll = set()\n",
    "keywordsCntAll = defaultdict(lambda: 0)\n",
    "keywordsItem = defaultdict(lambda: [])\n",
    "remPunctTable = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# TODO: is the try below neeeded?  Isnt' \n",
    "# try:\n",
    "#     _ = wordnet.morphy(\"test\")\n",
    "# except:\n",
    "#     print(\"Installing nltk data\")\n",
    "#     import nltk\n",
    "#     nltk.download('wordnet')\n",
    "#     print(\"Done installying nltk data\")\n",
    "\n",
    "# Collect all bibkeys and (normalized) keywords for each item\n",
    "for bibitem in bib_database.entries:\n",
    "    bibkey = bibitem['ID']\n",
    "    bibkeysAll.add(bibkey)\n",
    "\n",
    "    if \"keywords\" in bibitem:\n",
    "        # convert various .bib keyword list formats to OR format\n",
    "        for kw_phrase in re.split(', |; |,|;', bibitem['keywords']):\n",
    "            if len(kw_phrase) > 0:\n",
    "                # normalize to lowercase, no punctuation, singular, no spaces\n",
    "                kw_phrase = kw_phrase.translate(remPunctTable).lower().split()\n",
    "                for idx, word in enumerate(kw_phrase):\n",
    "                    if (word_singular := wordnet.morphy(word)) is not None:\n",
    "                        kw_phrase[idx] = word_singular\n",
    "\n",
    "                kw_phrase = \"_\".join(kw_phrase)\n",
    "                keywordsItem[bibkey] += [kw_phrase]\n",
    "                keywordsCntAll[kw_phrase] += 1\n",
    "\n",
    "# Remove rare keywords and then make a roam_tags string for each bibkey\n",
    "nOccurMin = 3\n",
    "keepKeywords = set({kw: count for kw, count in keywordsCntAll.items()\n",
    "                    if count >= nOccurMin}.keys())\n",
    "\n",
    "roam_tags_str = dict()\n",
    "for bibkey in bibkeysAll:\n",
    "    kws = keywordsItem[bibkey]\n",
    "    kwsItemKeep = keepKeywords.intersection(kws)\n",
    "    roam_tags_str[bibkey] = f'#+roam_tags: {\" \".join(kwsItemKeep)}'\n",
    "\n",
    "print(f\"keeping {len(keepKeywords)} of {len(keywordsCntAll)} unique keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Translate selected .bib entries to org-roam note syntax, and write notes\n",
    "\n",
    "def fix_comment_stars(commentStr):\n",
    "    \"\"\"Replace leading stars with 'o's.  Bib used '*'s which may\n",
    "    result in inadvertently collapsed .org headlines\"\"\"\n",
    "\n",
    "    outlns = []\n",
    "    for ln in commentStr.split('\\n'):\n",
    "        noLstars = ln.lstrip('*')\n",
    "        outlns += [\"o\" * (len(ln) - len(noLstars)) + noLstars]\n",
    "\n",
    "    return \"\\n\".join(outlns)\n",
    "\n",
    "\n",
    "def bibitem_to_OR_note(bibitem):\n",
    "    \"\"\"Makes org-roam note text from a .bib file item.  The note contains \n",
    "    the .bib comment, abstract and metadata.  The filename, OR title and key\n",
    "    are the .bib citekey.\"\"\"\n",
    "\n",
    "    bibkey = bibitem['ID']\n",
    "\n",
    "    noteLns = []\n",
    "    if \"title\" in bibitem:\n",
    "        noteLns += [f\"={bibitem['title']}=\"]\n",
    "\n",
    "    authChunks = []\n",
    "    if \"author\" in bibitem:\n",
    "        authChunks += [bibitem['author']]\n",
    "\n",
    "    if \"year\" in bibitem:\n",
    "        authChunks += [f\"({bibitem['year']})\"]\n",
    "\n",
    "    if len(authChunks) > 0:\n",
    "        authStr = \" \".join(authChunks)\n",
    "        noteLns += [f\"/{authStr}/\"]\n",
    "\n",
    "    if \"comment\" in bibitem:\n",
    "        commentTxt = bibitem['comment']\n",
    "        words = re.findall(r'\\w+', commentTxt)\n",
    "        words = set([w.translate(remPunctTable) for w in words])\n",
    "\n",
    "        for citedKey in words.intersection(bibkeysAll):\n",
    "            linkStr = f\"[[file:{citedKey}.org][{citedKey}]]\"\n",
    "            commentTxt = commentTxt.replace(citedKey, linkStr)\n",
    "\n",
    "        noteLns += [\"\", fix_comment_stars(commentTxt)]\n",
    "\n",
    "    if \"abstract\" in bibitem:\n",
    "        noteLns += [\"\", \"* Abstract\", bibitem['abstract']]\n",
    "\n",
    "    noteLns += [\"\", \"* Org-Roam Metadata\", f\"#+title:{bibkey}\"]\n",
    "\n",
    "    if \"timestamp\" in bibitem:\n",
    "        try:\n",
    "            dt = parse(bibitem['timestamp'])\n",
    "            timeStr = dt.strftime('%Y-%m-%d %a')\n",
    "        except:\n",
    "            timeStr = \"?\"\n",
    "\n",
    "        noteLns.append(f\"#+created: [{timeStr}]\")\n",
    "\n",
    "    # TODO: instead of a roam key man an org-id for this note, and make any links to it point use the org-id\n",
    "    #       See cell below for how to make the org-id\n",
    "    # TODO: figure out why ORv2 has a ROAM_REFS entry that has a cite:<bibtexkey> thing in it.\n",
    "    noteLns += [f\"#+roam_key: cite:{bibkey}\",\n",
    "                roam_tags_str[bibkey]]\n",
    "\n",
    "    return noteLns\n",
    "\n",
    "\n",
    "# Write a separate org-roam note file for each .bib entry\n",
    "for bibitem in bib_database.entries:\n",
    "    noteLns = bibitem_to_OR_note(bibitem)\n",
    "\n",
    "    noteOutFNm = noteOutDir / f\"{bibitem['ID']}.org\"\n",
    "    with open(noteOutFNm, 'w', encoding=\"utf8\") as fh:\n",
    "        # print(f\"Writing notes to {noteOutFNm}\")\n",
    "        for ln in noteLns:\n",
    "            fh.write('%s\\n' % ln)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220524T092246.069718\n"
     ]
    }
   ],
   "source": [
    "# the org-id format with (org-id-method 'ts)\n",
    "# TODO: get ID date from bibtex data created field or year of publication + something else.  Use computers's fractional time.\n",
    "from datetime import datetime\n",
    "my_date = datetime.now()\n",
    "# %Y%m%dT%H%M%S.%6N\n",
    "# TODO: do OR's ID's have the 'T' in the middle?\n",
    "print(my_date.strftime('%Y%m%dT%H%M%S.%f%z'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype code below.  Don't run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pybtex parser string examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from pybtex.database.input import bibtex\n",
    "# bibitem['keywords']=\"CRPS, Diagnostic tools  , Evaluation framework, Ignorance Score, Probabilistic solar forecasting, Scoring rules\"\n",
    "# bibkws = bibitem['keywords'].split(\", \")\n",
    "# bibkws\n",
    "\n",
    "# s = 'Probabilistic    solar forecasting'\n",
    "# kws_underscore = []\n",
    "# for kw in bibitem['keywords'].split(\", \"):\n",
    "#     kws_underscore += [matchWhiteSpace_regexp.sub(\"_\", kw.strip())]\n",
    "\n",
    "# keywords = \" \".join(kws_underscore)\n",
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibtex = \"\"\"@STRING{ jean = \"Jean\"}\n",
    "\n",
    "# @ARTICLE{Cesar2013,\n",
    "#   author = jean # { César},\n",
    "#   title = {An amazing title},\n",
    "#   year = {2013},\n",
    "#   month = jan,\n",
    "#   volume = {12},\n",
    "#   pages = {12--23},\n",
    "#   journal = {Nice Journal},\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# bp = BibTexParser(interpolate_strings=False)\n",
    "# bib_database = bp.parse(bibtex)\n",
    "# bib_database.entries[0]\n",
    "# as_text(bib_database.entries[0]['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BibTeX::Parser\n",
    "I wasn't able to get this to not collapse whitespace, so below is the hack to get around that.  But it turned out that bibtexparser could do the job w/o the hack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# bibIn = bibtex.Parser().parse_file(bibInFNm)\n",
    "\n",
    "# # get keys and whether or not it has a comment\n",
    "# bibkeyFields = dict()\n",
    "# for bibkey, bibitem in bibIn.entries.items():\n",
    "#     if 'comment' in bibitem.fields:\n",
    "#         bibkeyFields[bibkey] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keysWithComments = list(bibkeyFields.keys())\n",
    "# nextKey = keysWithComments.pop(0)\n",
    "# p = re.compile(f\"^@.*{nextKey},\")\n",
    "# lnum=1\n",
    "# with open(bibInFNm, encoding=\"utf8\") as fileHandler:\n",
    "#     try:\n",
    "#         for ln in fileHandler:\n",
    "#             ln = ln.rstrip()\n",
    "#             if p.match((ln)):\n",
    "#                 print(lnum, ln)\n",
    "#                 nextKey = keysWithComments.pop(0)\n",
    "#                 p = re.compile(f\"^@.*{nextKey},\")\n",
    "#             lnum += 1\n",
    "#     except:\n",
    "#         print(f\"trouble with line: {ln}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93d68e46dea65209a5c21138396ab318ec94a0c39b7a7507db7e5d41daed361b"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
